Late-Breaking Work: Extending User Capabilities

#chi4good, CHI 2016, San Jose, CA, USA

Toward Real-time Brain Sensing
for Learning Assessment:
Building a Rich Dataset
Shelby Keating

Anil Motupali

Abstract

Drexel University

Arizona State University

Philadelphia, PA 19104, USA

Tempe, AZ 85287, USA

sak347@drexel.edu

amotupal@asu.edu

Erin Walker

Erin Solovey

Arizona State University

Drexel University

Tempe, AZ 85287, USA

Philadelphia, PA 19104, USA
ets36@drexel.edu

By integrating real-time brain input into personalized
learning environments, it would be possible to capture
a learner’s changing cognitive state and adapt the
learning experience appropriately. Working toward this
goal, we aim to develop a robust system that can classify a user’s cognitive state during a learning activity,
using brain data collected with functional near-infrared
spectroscopy, an emerging non-invasive neuroimaging
tool. This paper describes preliminary steps we have
taken toward this objective as well as the underlying
vision and research goals. This work has implications
for online education as well as the growing fields of
brain-computer interfaces and physiological computing.

eawalke1@asu.edu

Author Keywords
educational data mining; functional near-infrared spectroscopy; machine learning; brain-computer interfaces.

ACM Classification Keywords
H.5.m. Information interfaces and presentation (e.g., HCI):
Miscellaneous
Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for
third-party components of this work must be honored. For all other uses,
contact the Owner/Author. Copyright is held by the owner/author(s).
CHI'16 Extended Abstracts, May 07-12, 2016, San Jose, CA, USA
ACM 978-1-4503-4082-3/16/05.
http://dx.doi.org/10.1145/2851581.2892496

Introduction
With the growth of online and computer-based learning
environments, there now exists an unprecedented
amount of data on how students solve problems and
build knowledge. Many educational technologies collect
fine-grained clickstream data on student activities within a learning environment, including the steps students
take to solve problems, correctness of their actions,
hints requested, and additional scaffolding received.
Large data mining efforts using this data have provided
substantial knowledge on how students learn, including
optimal practice schedules for learning materials [2],

1698

Late-Breaking Work: Extending User Capabilities

skills students require to solve a particular problem
[14], and the range of off-task behaviors that students
may engage in [5]. However, processes related to deep
learning, such as reflecting on errors and confronting
misconceptions, often occur at times when students are
thinking and thus, during a pause in the log data. Of
course, pauses during use of educational systems can
also be indicative of many other cognitive or motivational states such as conversing with a teacher, engaging in off-task behavior, daydreaming, or simply focusing on the wrong problem features [7,18,28]. The predictive power of pauses varies across datasets based on
when they occur, what happens after, and its length. If
pauses during learning could be better understood and
characterized, it might be possible to build predictive
models of when students are learning from online environments.

Figure 1. fNIRS brain sensing
setup

It may be possible to leverage brain data and other
physiological sensing to disambiguate what is occurring
during pauses in log data. There are indications that
fMRI can be used to detect when students are thinking
deeply about a problem [1], and EEG can be used to
predict learners’ scores on an assessment [16], their
correctness and confidence in their answers [22], and
aspects of their mental state (e.g., workload, engagement, and distraction) [40]. In addition, physiological
sensors have been explored for forming better predictions of learning from educational data. Eyetracking has
been used along with log data to differentiate between
where successful and unsuccessful students attend to
on the interface [35,32], predict self-explanation [17],
and indicate boredom and curiosity [27]. Other researchers have attempted to predict states such as distraction, confidence, frustration, and excitement based
on collections by physiological sensors [13,3,42].

#chi4good, CHI 2016, San Jose, CA, USA

This project combines two complementary techniques - educational data mining and functional near-infrared
spectroscopy (fNIRS) brain imaging – with the overall
goal of understanding the cognitive processes that occur during pauses while using learning environments.
fNIRS is an emerging non-invasive neuroimaging tool
[15] that has been used to measure cognitive state in
real-time while participants complete computer-based
tasks [39]. It is now a realistic tool for researchers,
being less expensive, more portable and more comfortable to wear [38]. The continuous nature of physiological measurements allow us to fill in pauses in log data
with insights from brain data, with the goal of differentiating different types of pauses to better understand
the learner’s cognitive state. The detailed educational
log data provides contextual information about what
events occur before and after the pauses that will enable better interpretations of the brain activity. By combining brain and log data, we can explore critical moments and better understand what is occurring during
individual use of a learning environment.
In this late-breaking work paper, we first present the
theoretical foundations for our approach, describing the
types of cognitive states we wish to identify and how
brain data might help us do so. We then discuss the
steps we have taken so far as well as future work.

Background
Educational Data Mining
Our project aims to increase the potential for robust
learning: learning that is retained over time, transfers
to other situations, and enables future learning in new
contexts [29]. There are several obstacles for the use
of a learning environment to result in robust learning.
For example, some students “game the system” at-

1699

Late-Breaking Work: Extending User Capabilities

#chi4good, CHI 2016, San Jose, CA, USA

tempting to get correct answers by using system features in unintended ways rather than by building up
knowledge in the problem domain [6] often times by
repeated help requests or systematically guessing. During mind wandering, another form of disengagement,
students engage in internal non-task thoughts [37,33],
limiting the knowledge they develop. Even motivated
students may actively attend to their work in the learning environment but be unable to master the required
skill, continuing to make repeated errors after many
attempts (called wheel-spinning) [19].

researchers often use additional contextual features
surrounding the pauses to get more accurate reads on
what cognitive states occur within the pauses, spanning
the instructional events triggering the response times
[4], student knowledge of the relevant skill [7], or
problem difficulty [11]. Despite the use of these contextual features, there are still inconsistencies across
different analyses, and often features that are predictive in one analysis will not be predictive in a second. In
addition, it is likely that students cycle through different
cognitive states within a given pause [36].

A large focus in educational data mining research has
been to predict student learning outcomes based on
data on student problem-solving processes. Predictions
can be made about robust learning outcomes [10,8,12,
20] and non-productive behaviors such as gaming [21,
6,34], mind-wandering [31], and wheel-spinning [19].
In these predictive models, timing information is heavily used. For example, Gowda et. al [20] used 25 features, 12 which involved timing and 4 which involved
pauses, to predict whether student learning was shallow. Pauses in system logs can represent either positive
learning events (i.e. the student thinking deeply about
the topic) or undesirable cognitive states (i.e. mindwandering or wheel-spinning). Some predictive models
interpret fast response times as indicators of low effort
and slow times as indicators of engagement in deeper
processing [4,11,25]; others identify long response
times as indicators of disengagement or distraction
[20,10] and short times as indicators of fluency [8,9].

Real-time Brain Sensing in Natural Settings with fNIRS
The variance described above motivates our ongoing
brain sensing work, as we want to better understand
what occurs during pauses that account for differences
between problems, students, and natural variation
within a student’s learning behaviors. Real-time brain
sensing has the potential to fill some gaps and is becoming a realistic tool for HCI researcher. In fNIRS
neuroimaging, sensors are placed and secured on the
head with a headband (Fig. 1). The sensors use nearinfrared light to detect hemodynamic changes associated with neural activity in the brain during tasks [15].

Understanding what occurs during pauses and using
them for predictions is further complicated by the fact
that a “typical” pause length varies across problems,
sessions, and students [26]. Educational data mining

Because fNIRS primarily detects light that travels 1-3
cm into the cortex, most fNIRS research focuses on the
easily accessible frontalpolar cortex (FPC) which lies
behind the forehead. The brain processes that activate
the FPC are mainly high level executive functioning,
and have been shown to play a part in memory, problem solving, judgment, planning, coordinating, controlling and executing behavior. Several researchers have
found that the FPC is highly activated during the development of expertise, and less activated once reached
[40,29,23]. From this work, there is strong evidence

1700

Late-Breaking Work: Extending User Capabilities

Figure 2. ASSISTments system with
Scaffolding

#chi4good, CHI 2016, San Jose, CA, USA

that the FPC will have activation patterns that are detectable during various aspects of learning.

5.

Most work on FPC activation was done using fMRI or
positron emission tomography (PET) imaging which is
not realistic for real-time, ecologically valid measurement. In contrast, fNIRS avoids many restrictions of
other techniques, most notably it has significantly lower
costs and far fewer mobility restrictions. Additionally,
PET requires hazardous material ingestion and fMRI is
not practical for use with computers due to the strong
magnetic fields. Unlike these more intrusive measures,
EEG has been used successfully in brain-computer interface research. However, it typically requires a longer
set up time, is more susceptible to motion artifacts, and
has lower spatial resolution than fNIRS.

6.

Research Questions
We see potential in combining brain data with educational data mining for learning assessment and are currently building a large, rich dataset to explore this and
build a foundation for future research. There are many
research questions that can be explored with our dataset:
1. Can we detect cognitive states in learning contexts
that have been detected in non-learning contexts?
2. Can we replicate findings from log data analysis
with our population and learning environment?
3. What is occurring within the brain data during moments of interest in intelligent tutoring log data,
represented by pause-related features typically associated with cognitive states? Does the activity
appear to be congruent with what we expect to be
going on? Across what percent of similar pauses?
4. How can the brain data collected augment predictions made using log data?

How does brain activity within pauses vary between
students, and within the same students across multiple sessions?
How do our models interact with self-reported cognition and motivation?

Preliminary Work
We have taken several steps toward our research
goals. First, we conducted a proof-of-concept study in
which we collected preliminary fNIRS data from 11 participants who used a basic e-learning platform for fraction addition. The proof-of-concept study demonstrated
that we could successfully integrate brain data features
with log data features. Next, we transitioned to using
the ASSISTments platform (Figure 2) to better suit our
needs [24]. We built up a repository of developmental
math problems, those that prepare student for more
advanced topics, then ran a pilot study with 3 students
for the purpose of testing various developmental math
topics within ASSISTments. We aimed to discover
which topics would be appropriate for our target population of non-STEM college students such that the students would be able to learn the topic during a tutored
Intelligent Tutoring System (ITS) session. We included
problems on geometry, probability, functions, algebraic
simplification, factoring, square roots, inequalities, and
linear equations. Of these topics, all subjects got problems on geometry, probability, algebraic simplification,
and square roots incorrect on their first attempt; however, during the interview period after the ITS session,
the probability question was the only one that the participants rated as difficult. The students stated that
once they remembered how to do the geometry and
simplification problems they were easy. Based on these
results, it was decided that probability and algebraic
simplification would be the topics covered in our study.

1701

Late-Breaking Work: Extending User Capabilities

The probability topic will ideally provide insight into
periods when a student does now know how to do a
problem and should be learning. The simplification
problems will give insight into when a student needs to
remember how to complete a problem or already knows
how. By using these two topics, there are various cognitive and motivational states that can be accessed and
predicted based on the collected log and brain data.
Figure 3. fNIRS Dashboard for
visualization and analysis of
fNIRS data.

In addition to running the pilot study and preparing for
upcoming mass data collection, we have also been developing a tool to extract learning data and visualize
brain data (Figure 3). The team has been developing
programs to extract and analyze important information
from ASSISTments log data and developing a toolbox
to analyze and visualize brain data from fNIRS. Together, this toolbox and these programs will provide a more
in-depth view into a learner’s state and will eventually
be available for use by other researchers.

Building a Rich Dataset
With these preliminary steps complete, we are now in
our data collection phase. We are iteratively collecting
data from groups of 20 participants, recruited from students in developmental mathematics classes at Drexel
University. The goals of this collection are to 1) collect
controlled data with known characteristics in wellstudied tasks and 2) collect data during realistic learning experiences to uncover predictive features in the
data and improve understanding of learning outcomes.
Procedure
The core data collection procedure is as follows. After
obtaining informed consent, participants are introduced
to ASSISTments [24] and undergo a short training. We
then place the fNIRS sensors on their forehead along

#chi4good, CHI 2016, San Jose, CA, USA

with peripheral physiological sensors, including cardiovascular, respiratory, Galvanic skin response and eye
tracking. Once the sensors are arranged, the user session proceeds through calibration tasks, a 15 minute
pre-test consisting of conceptual checks, problems of
mixed difficulty levels, challenge problems, and transfer
problems. The participant then completes a 40-minute
intervention problem set consisting of probability and
simplification problems of varying difficulty in random
order. The intervention problem set is “tutored”, in that
students will be able to request help and get feedback
on their problem-solving. The problems are organized
so that 10 minutes are spent on each topic at a time.
The student then completes 10 minutes of tutored
“challenge” problems, which are of higher difficulty
than the intervention problems. The challenge problems
are designed to learn how well students can use their
developing knowledge to master a new topic. Finally,
students take an untutored posttest isomorphic to the
pretest. At the end of the session, the fNIRS as well as
all physiological sensors are removed and the student
undergoes a debriefing and completes a questionnaire.

Planned Analysis
Our data collection phase is ongoing. Below we describe
some of our plans for analysis to answer our main research questions.
Exploratory Analyses
The large dataset we create will allow us to build robust
machine learning classification models of cognitive
states such as elevated cognitive workload, which has
been demonstrated to be detectable by fNIRS [38,39].
A second necessary component of our research is determining the degree to which we can replicate previous educational data mining findings within our re-

1702

Late-Breaking Work: Extending User Capabilities

Core Analyses
We will identify time ranges in
the log data associated with the
predictive features, as well as the
other theoretically significant
features we have surveyed. In an
exploratory fashion we will examine brain activity during those
time ranges and determine interesting activity features. We will
evaluate how consistent brain
activity is across like regions,
where it is different, and potentially why. As part of this process, we will explicitly look for
variations in brain activity between students. This analysis
should help us isolate the most
detectable cognitive processes
using the intersection of brain
and log data within our particular
context. We will include the brain
features extracted in our learning
predictive models by combining
them with intelligent tutoring log
features both absent of and in
conjunction with pause information. We will focus on brain
features that appear indicative of
the detectable cognitive and motivational states and see if the
brain data improves our model
accuracy.

search context. We will extract features and identify
feature combinations typically associated with cognitive
states of interest such as wheel-spinning and mindwandering, and use our toolkit to tag those pauses with
that additional cognitive state information. Then, following the methodologies presented in previous papers,
we will test a series of statistical and machine learning
models for conducting feature selection and predicting
learning based on log data. Our dependent variable will
be the deep and shallow learning outcomes surveyed.
Secondary Analyses
As we begin to understand the cognitive and motivational states that are possible to detect within the brain
data, we will add additional measures to try to extract
further ground truth assessments of those states (i.e.
we may ask students to report how distracted they
were during a particular problem, or include a measure
of cognitive load at the end of each session). In addition, we will include self-reported measures of the cognitive states we are examining [18]. During a learning
activity, students are periodically asked to respond to
(Y/N) questions about whether their mind is wandering.
This self-report validation will provide an additional
source of validation for the conclusions we draw regarding individual students’ learning process.

Conclusion
The combination of educational data mining and brain
sensing techniques has the potential to facilitate the
detection of critical cognitive and motivational states
during use of an online learning environment. We have
conducted several pilot studies in order to explore this
potential integration, and are currently engaged in a
large-scale data collection. This data collection will lead
to the construction of a data set that will allow us to

#chi4good, CHI 2016, San Jose, CA, USA

answer important research questions relating to the
nature of cognitive states during learning experiences
and the design of highly adaptive learning environments that can respond to those cognitive states. We
look forward to engaging with the CHI community to
discuss our approach and ongoing results.

Acknowledgments
The authors would like to thank the following people:
Stevie Mari Hawkins, Gloria Houseman, Calan Farley,
Paritosh Gupta, Ruixue Liu, the Drexel Advanced Interaction Research Lab students, Robert Jacob and the
Tufts HCI Lab, Missy Cummings and the MIT Humans
and Autonomy Lab, Neil Heffernan, Cristina Heffernan.

References
1.

2.
3.

4.
5.

6.

Anderson, J. R., Betts, S., Ferris, J. L., & Fincham, J.
M. (2011). Cognitive and metacognitive activity in
mathematical problem solving: prefrontal and parietal
patterns. Cognitive, Affective, & Behavioral Neuroscience, 11(1), 52-67.
Anderson, J. R. & Pavlik, P.I. (2008). Using a model to
compute the optimal schedule of practice. Journal of
Experimental Psychology: Applied, 14(2), 101.
Arroyo, I., Cooper, D. G., Burleson, W., Woolf, B. P.,
Muldner, K., & Christopherson, R. (2009, July). Emotion Sensors Go To School. In AIED (Vol. 200, pp. 1724).
Arroyo, I., Mehranian, H., & Woolf, B. P. (2010). Effort-based tutoring: An empirical approach to intelligent tutoring. In Educational Data Mining 2010.
Baker, R. S. (2007). Modeling and understanding students' off-task behavior in intelligent tutoring systems.
In Proceedings of the SIGCHI conference on Human
factors in computing systems (pp. 1059-1068). ACM.
Baker, R. S., Corbett, A. T., & Koedinger, K. R.
(2004). Detecting student misuse of intelligent tutoring systems. In Intelligent tutoring systems (pp. 531540). Springer Berlin Heidelberg.

1703

Late-Breaking Work: Extending User Capabilities

7.

8.

9.

10.

11.

12.
13.

14.

15.

Baker, R. S., D'Mello, S. K., Rodrigo, M. M. T., &
Graesser, A. C. (2010). Better to be frustrated than
bored: The incidence, persistence, and impact of
learners’ cognitive–affective states during interactions
with three different computer-based learning environments. International Journal of Human-Computer
Studies, 68(4), 223-241.
Baker, R. S., Gowda, S., & Corbett, A. (2010). Automatically detecting a student's preparation for future
learning: Help use is key. In Educational Data Mining
2011.
Baker, R. S., Gowda, S. M., & Corbett, A. T. (2011,
January). Towards predicting future transfer of learning. In Artificial Intelligence in Education (pp. 23-30).
Springer Berlin Heidelberg.
Baker, R. S., Gowda, S. M., Corbett, A. T., & Ocumpaugh, J. (2012, January). Towards automatically
detecting whether student learning is shallow. In Intelligent Tutoring Systems (pp. 444-453). Springer
Berlin Heidelberg.
Beck, J. E. (2004). Using response times to model
student disengagement. In Proceedings of the
ITS2004 Workshop on Social and Emotional Intelligence in Learning Environments (pp. 13-20).
Beck, J. & Want, Y. (2012). Incorporating Factors Influencing Knowledge Retention into a Student Model.
In Educational Data Mining 2012.
Blanchard, N., Bixler, R., Joyce, T., & D’Mello, S.
(2014, January). Automated Physiological-Based Detection of Mind Wandering during Learning. In Intelligent Tutoring Systems (pp. 55-60). Springer International Publishing.
Cen, H., Koedinger, K., & Junker, B. (2006, January).
Learning factors analysis–a general method for cognitive model evaluation and improvement. In Intelligent
tutoring systems (pp. 164-175). Springer Berlin Heidelberg.
Cen, H., Koedinger, K., & Junker, B. (2006, January).
Learning factors analysis–a general method for cognitive model evaluation and improvement. In Intelligent
tutoring systems (pp. 164-175). Springer Berlin Heidelberg.

#chi4good, CHI 2016, San Jose, CA, USA

16. Chaouachi, M., Heraz, A., Jraidi, I., & Frasson, C.
(2009). Influence of Dominant Electrical Brainwaves
on Learning Performance. In World Conference on ELearning in Corporate, Government, Healthcare, and
Higher Education (Vol. 2009, No. 1, pp. 2448-2454).
17. Conati, C., & Merten, C. (2007). Eye-tracking for user
modeling in exploratory learning environments: An
empirical evaluation. Knowledge-Based Systems,20(6), 557-574.
18. D’Mello, S. & Bixler, R. (2014). Toward Fully Automated Person-Independent Detection of Mind Wandering.
In User Modeling, Adaptation, and Personalization (pp.
37-48). Springer International Publishing.
19. Gong, Y. & Beck, J. E. (2013). Wheel-spinning: Students who fail to master a skill. In Artificial Intelligence in Education (pp. 431-440). Springer Berlin
Heidelberg.
20. Gowda, S. M., Baker, R. S., Corbett, A. T., & Rossi, L.
M. (2013). Towards automatically detecting whether
student learning is shallow. International journal of artificial intelligence in education, 23(1-4), 50-70.
21. Heffernan, N. T. & Walonoski, J. A. (2006). Detection
and analysis of off-task gaming behavior in intelligent
tutoring systems. In Intelligent Tutoring Systems (pp.
382-391). Springer Berlin Heidelberg.
22. Heraz, A., & Frasson, C. (2009). Predicting Learner
Answers Correctness through Brainwaves Assesment
and Emotional Dimensions. In AIED (pp. 49-56).
23. Jenkins, I. H., Brooks, D. J., Nixon, P.D., Frackowiak,
R. S. J., Passingham, R. E. (1994). Motor Sequence
Learning: A study with positron emission tomography.
Journal of Neuroscience 14(6): 3775-3790.
24. Heffernan, N. & Heffernan, C. (2014). The ASSISTments Ecosystem: Building a Platform that Brings Scientists and Teachers Together for Minimally Invasive
Research on Human Learning and Teaching. IJAIED.
24(4), 470-497.
25. Hershkovitz, A., & Nachmias, R. (2008). Developing a
log-based motivation measuring tool. In Educational
Data Mining 2008.

1704

Late-Breaking Work: Extending User Capabilities

26. Hershkovitz, A., & Nachmias, R. (2009). Consistency
of students' pace in online learning. In Educational Data Mining 2009.
27. Jaques, N., Conati, C., Harley, J. M., & Azevedo, R.
(2014). Predicting Affect from Gaze Data during Interaction with an Intelligent Tutoring System. InIntelligent Tutoring Systems (pp. 29-38). Springer International Publishing.
28. Kalyuga, S. (2007). Enhancing instructional efficiency
of interactive e-learning environments: A cognitive
load perspective. Educational Psychology Review,19(3), 387-399.
29. Koedinger, K.R., Pavlik, P., McLaren, B.M., & Aleven,
V. (2008). Is it Better to Give than to Receive? The
Assistance Dilemma as a Fundamental Unsolved Problem in the Cognitive Science of Learning and Instruction. In B. C. Love, K. McRae, & V. M. Sloutsky (Eds.),
Proceedings of the 30th Annual Conference of the
Cognitive Science Society (pp. 2155-2160). Austin,
TX: Cognitive Science Society.
30. Leff, D. R., Elwell, C. E., Orihuela-Espina, F., Atallah,
L., Delpy, D. T., Darzi, A. W., Yang, G.Z. (2008).
Changes in prefrontal cortical behavior depend upon
familiarity on a bimanual co-ordination task: An fNIRS
study. NeuroImage 39. 805-813.
31. Litman, D. & Drummond, J. (2010). In the zone: Towards detecting student zoning out using supervised
machine learning. In Intelligent Tutoring Systems (pp.
306-308). Springer Berlin Heidelberg.
32. Mayer, R. E. (2010). Unique contributions of eyetracking research to the study of learning with
graphics. Learning and instruction, 20(2), 167-171.
33. Mills, C., D’Mello, S., Lehman, B., Bosch, N., Strain,
A., & Graesser, A. (2013, January). What Makes
Learning Fun? Exploring the Influence of Choice and
Difficulty on Mind Wandering and Engagement during
Learning. In Artificial Intelligence in Education (pp.
71-80). Springer Berlin Heidelberg.
34. Muldner, K., Burleson, W., Van de Sande, B., &
VanLehn, K. (2011). An analysis of students’ gaming

#chi4good, CHI 2016, San Jose, CA, USA

35.

36.

37.

38.

39.

40.

41.

42.

behaviors in an intelligent tutoring system: predictors
and impacts. User modeling and user-adapted interaction, 21(1-2), 99-135.
Najar, A. S., Mitrovic, A., & Neshatian, K. (2014). Utilizing eye tracking to improve learning from examples.
In Universal Access in Human-Computer Interaction.
Universal Access to Information and Knowledge (pp.
410-418). Springer International Publishing.
Shih, B., Koedinger, K. R., & Scheines, R. (2011). A
response time model for bottom-out hints as worked
examples. Handbook of educational data mining, 201212.
Smallwood, J., Fishman, D. J., & Schooler, J. W.
(2007). Counting the cost of an absent mind: Mind
wandering as an underrecognized influence on educational performance. Psychonomic Bulletin & Review,
14(2), 230-236.
Solovey, E. T., Girouard, A., Chauncey, K., Hirshfield,
L. M., Sassaroli, A., Zheng, F., Fantini, S., Jacob, R. J.
K. (2009). Using fNIRS Brain Sensing in Realistic HCI
Settings: Experiments and Guidelines. Proc User Interface Software and Technology (UIST).
Solovey, E. T., Schermerhorn, P., Scheutz, M., Sassaroli, A., Fantini, S., & Jacob, R. J. K. (2012). Brainput:
Enhancing Interactive Systems with Streaming fNIRS
Brain Input. Proc. ACM CHI'12 Conf.
Stevens, R. H., Galloway, T., & Berka, C. (2007).
EEG-related changes in cognitive workload, engagement and distraction as students acquire problem
solving skills. In User Modeling 2007 (pp. 187-196).
Springer Berlin Heidelberg.
Strange, B. A., Henson, R. N. A., Friston, K. J., & Dolan, R. J. (2001). Anterior prefrontal cortex mediates
rule learning in humans. Cerebral Cortex, 11(11),
1040-1046.
Woolf, B., Burleson, W., Arroyo, I., Dragon, T.,
Cooper, D., & Picard, R. (2009). Affect-aware tutors:
recognising and responding to student affect. International Journal of Learning Technology, 4(3-4), 129164.

1705

Computer-Supported Collaborative Learning (2011) 6:279–306
DOI 10.1007/s11412-011-9111-2

Designing automated adaptive support to improve
student helping behaviors in a peer tutoring activity
Erin Walker & Nikol Rummel & Kenneth R. Koedinger

Received: 9 July 2010 / Accepted: 10 January 2011 /
Published online: 8 February 2011
# International Society of the Learning Sciences, Inc.; Springer Science+Business Media, LLC 2011

Abstract Adaptive collaborative learning support systems analyze student collaboration as
it occurs and provide targeted assistance to the collaborators. Too little is known about how
to design adaptive support to have a positive effect on interaction and learning. We
investigated this problem in a reciprocal peer tutoring scenario, where two students take
turns tutoring each other, so that both may benefit from giving help. We used a social
design process to generate three principles for adaptive collaboration assistance. Following
these principles, we designed adaptive assistance for improving peer tutor help-giving, and
deployed it in a classroom, comparing it to traditional fixed support. We found that the
assistance improved the conceptual content of help and the use of interface features. We
qualitatively examined how each design principle contributed to the effect, finding that peer
tutors responded best to assistance that made them feel accountable for help they gave.
Keywords Adaptive collaborative learning support . Adaptive scripting . Reciprocal peer
tutoring . Intelligent tutoring . In vivo experimentation

Introduction
Through participation in collaborative activities students socially construct knowledge
(Schoenfeld 1992). They can elaborate on their existing knowledge and build new
knowledge when they articulate their reasoning (Ploetzner et al. 1999), integrate other
group members’ reasoning (Stahl 2000), reflect on misconceptions, and work toward a
shared understanding (Van den Bossche et al. 2006). However, for collaboration to be
E. Walker (*) : K. R. Koedinger
Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA, USA
e-mail: erinwalk@andrew.cmu.edu
K. R. Koedinger
e-mail: koedinger@cmu.edu
N. Rummel
Institute of Education, Ruhr-Universität Bochum, Bochum, Germany
e-mail: nikol.rummel@rub.de

280

E. Walker, et al.

effective at engaging these processes, students need to display positive collaborative
behaviors (Johnson and Johnson 1990), and they generally do not do so without assistance
(Lou et al. 2001).
Small-group collaboration can be supported in several ways: through the use of human
facilitation to guide the interaction (Hmelo-Silver 2004; Michaels et al. 2008), precollaboration training (Prichard et al. 2006; Saab et al. 2007), or scripting of the
collaborative interaction by giving students designated roles and activities to follow
(Fischer et al. 2007; Kollar et al. 2006). While human facilitation can indeed be effective, it
is resource intensive, as it requires an expert facilitator to guide each group’s discussion.
Training and scripting are less resource intensive, but students may not be capable of or
motivated to follow the instructions given (Ritter et al. 2002). In a face-to-face
collaboration context, there is no way for these techniques to ensure that they do so. An
increase in the presence of computer-mediated collaborative activities in the classroom has
changed the way collaboration can be structured, as script elements can be embedded in the
interface: Roles can manifest themselves through the types of collaborative actions students
can perform using the system, phases can be strictly enforced, and prompts can take the
form of sentence classifiers or starters, where students complete open-ended sentences such
as “I agree, because…” However, this increase in support comes with a potential
decrease in motivation, as this level of support can overstructure collaboration for
students who already know how to collaborate (Kollar et al. 2005). Further, students
often fail to comply with script elements such as sentence starters (Lazonder et al. 2003),
perhaps because they do not know how to use them effectively or are not motivated to do
so. For example, if students repeatedly use sentence classifiers to type something offtopic, such as “I agree because… I’m getting hungry,” this is unlikely to contribute to a
beneficial interaction.
A promising new method for facilitating computer-supported collaborative activities in
the classroom is by providing students with adaptive collaborative learning support
(ACLS). In theory, this approach should be more effective than fixed support alone, as
students would always receive a level of assistance appropriate to their collaborative skill,
and the intelligent system could verify that students are, in fact, improving their
collaboration (Rummel and Weinberger 2008). Studies comparing automated adaptive
support to fixed support have indeed been promising (Baghaei et al. 2007, Kumar et al.
2007), but research into ACLS is still at an early stage. In designing ACLS people have
mainly adapted individual learning paradigms, such as providing explicit feedback directly
to the unproductive collaborator (see Soller et al. 2005, for a review). For example, the
system COLLECT-UML responds to a lack of elaboration by saying: “You seem to just
agree and/or disagree with other members. You may wish to challenge others’ ideas and ask
for explanation and justification” (Baghaei et al. 2007). This form of feedback has been
demonstrated to be successful in individual learning (e.g., Koedinger et al. 1997), as
students can immediately reflect on how the feedback applies to their current activity and
make appropriate changes to their behavior. However, it may be less appropriate for
collaboration, and, in fact, Kumar et al. (2007) found that students tended to ignore
adaptive prompts while collaborating. Students may ignore adaptive feedback because it
violates Gricean maxims of the conversation (e.g., appears irrelevant to the task; Bernsen et
al. 1997) or disrupts the perceived safety of the collaborative context (Nicol and
Macfarlane-Dick 2006; Van den Bossche et al. 2006). Two recent studies have
demonstrated that socially sensitive features of adaptive support are indeed important for
getting positive outcomes from the support (Chaudhuri et al. 2009; Kumar et al. 2010).
Given the complex set of options and interactions involved in collaboration support, any

Computer-Supported Collaborative Learning

281

null effects found in comparing adaptive to fixed support might be due to limitations in the
support design, and not because adaptive support is ineffective per se. It is therefore
important to explore the full design space of support possibilities.
This project focuses on how to design adaptive support to improve the quality of
collaborative student interaction. We investigate this broader question in the context of a
system that we have developed for supporting help-giving behaviors in a reciprocal peer
tutoring scenario for algebra. Our overall program of research has leveraged a paradigm
evolved from the in vivo experiments described by Koedinger et al. (2009). In vivo
experiments lie at the intersection of psychological experimentation and design-based
research, as defined by Collins (1999) and later expanded upon by Barab and Squire
(2004). Like psychological experimentation, an in vivo experiment involves the
manipulation of a single independent variable and the use of fixed procedures to test a
set of hypotheses. In addition, like design-based research, an in vivo experiment takes place
in real-world contexts that involve social interaction, and characterizes the relationships
between multiple process variables and outcome variables.
We would argue that for in vivo experimentation to be successful it can be helpful to
incorporate further elements of design-based research outside those used in a single in vivo
experiment: the use of participant co-design and analysis to develop a profile of what is
occurring and inform flexible, iterative design revisions (Beyer and Holtzblatt 1997).
Iterated in vivo experimentation, where we use a design-based research process to create an
intervention, deploy the intervention using an in vivo experiment, and then interpret the
effects through a design-based lens, may be a more effective way of theory building than
executing an in vivo experiment in isolation. This concept of iterated in vivo
experimentation is similar to that of action research (Brydon-Milier et al. 2003), with a
few key differences. Under iterated in vivo experimentation, theory-building is a driving
force in the research agenda, in addition to effecting social change, and the use of both
quantitative and qualitative methods are advocated.
In this paper, we discuss our four phase program of iterated in vivo experimentation for
adaptively supporting the quality of peer tutor help-giving. First, we used a humancomputer interaction design method called Speed Dating (Davidoff et al. 2007), which led
to the identification of three design principles for adaptive collaboration assistance in this
context (Phase 1: Needs Identification). Based on these principles, we augmented an
existing peer tutoring system with adaptive support, including reflective prompts triggered
by elements of the help given by peer tutors (Phase 2: Assistance Design). We deployed
the system in an in vivo experiment, and quantitatively analyzed its effects, comparing
the adaptive assistance to parallel fixed assistance for effects on student help-giving
(Phase 3: In Vivo Experiment). We then returned to a design-based methodology to
qualitatively examine how our three design principles more directly influenced the
student interactions (Phase 4: Contrasting Cases). We conclude this paper by discussing
the theoretical and design implications of our results, placing our system in the context of
other ACLS systems.
Reciprocal peer tutoring
We explore how to design adaptive support to improve help-giving behaviors among peers.
Help-giving is an important part of many collaborative activities, and is a key element of
the productive interactions identified by Johnson and Johnson (1990) that contribute to
learning from collaboration. In giving help, even novice students benefit; they reflect on
their peer’s error and then construct a relevant explanation, elaborating on their existing

282

E. Walker, et al.

knowledge and generating new knowledge (Roscoe and Chi 2007). Thus, improving helpgiving is likely to benefit the help giver. Further, supporting help-giving might have indirect
benefits for the help receiver, as students tend to benefit most from receiving help that:
arrives when they reach an impasse, allows them to self-explain, and, if necessary, provides
an explanation that is conceptual and targets their misconceptions (VanLehn et al. 2003;
Webb 1989; Webb and Mastergeorge 2003).
Unfortunately, most students do not exhibit positive helping behaviors spontaneously
(Roscoe and Chi 2007). Thus, during collaboration students may fail to help each other
well or even at all. Specifically, students are often more inclined to give each other
instrumental help (e.g., “subtract x”). They rarely provide conceptual, elaborated help that
explains why, in addition to what, and references domain concepts (e.g., “subtract x to
move it to the other side”). This tendency decreases the likelihood that either student
engages in elaborative knowledge-construction processes and benefits from the
interaction (Webb and Mastergeorge 2003). Promoting the conceptual content of student
help has benefits for the interaction as a whole (Fuchs et al. 1997), and is the major focus
of our peer tutoring support.
One technique for facilitating student help-giving is by means of employing a reciprocal
schema, where first one student is given artificial expertise in a particular domain and is
told to regulate the problem solving of a second student, and then the roles are reversed and
the second student becomes the expert (Dillenbourg and Jermann 2007). As part of their
role, the expert must monitor their partner’s problem solving and offer appropriate help
when it is needed. Examples of this class of collaborative activities are dyadic activities
such as reciprocal teaching by Palincsar and Brown (1984), mutual peer tutoring by King et
al. (1998), and reciprocal peer tutoring by Fantuzzo et al. (1989). Several of these activities
have been successful at increasing student learning in classroom environments compared to
individual and unstructured controls (Fantuzzo et al. 1989; King et al. 1998; Fuchs et al.
1997). They have been effective for both low and high ability students, but only when
further support is provided to students in order to assist them in helping each other
effectively. For example, Fuchs et al. 1997 trained students to deliver conceptual
elaborated mathematical explanations, and showed that their mathematical learning was
significantly better than elaborated help training alone (without specific emphasis on
conceptual content).
To explore the potential of adaptive support for help-giving, we have developed a
reciprocal peer tutoring environment as an addition to the Cognitive Tutor Algebra (CTA), a
successful intelligent tutoring system for individual learning in high-school Algebra
(Koedinger et al. 1997). We designed the environment to be used in a scenario where first
students prepare to tutor each other on different problems, and then are seated at different
computers in the same classroom and alternate being tutors and tutees. The environment
encourages students to collaborate in a shared workspace, and talk to each other using a
chat window. In this way, it has much in common with the successful Virtual Math Teams
project, where groups of students get together to discuss mathematics using a shared
workspace and unstructured chat (Stahl 2009). Our system also draws from other adaptive
systems that support peer help, such as IHelp, where computer agents use peer learner and
helper profiles to negotiate tutoring partnerships between students (Vassileva et al. 2003).
However, unlike IHelp, our system supports peer tutors as they tutor, attempting to improve
the conceptual, elaborated help they provide. Thus, the environment also borrows from
single-user systems that have students tutor a virtual peer, some of which include an
adaptive agent that helps the student be a better tutor (e.g., Leelawong and Biswas 2008).
Our research stands out in its extended development of synchronous automated adaptive

Computer-Supported Collaborative Learning

283

support of complex human-human interaction, in the context of peer tutoring. In the next
section, we discuss Phase 1 of our iterated in vivo experimentation program: our
exploration of potential designs for adaptive interaction support for peer tutoring.

Phase 1: Needs identification
The first phase of our work centered on an exploration of potential designs for adaptive
interaction support for peer tutoring. We generated several different ideas for adaptive
support, and then used the Speed Dating method (Davidoff et al. 2007) to gather student
reactions. From this process we derived three principles of ACLS design.
Ideation
Drawing inspiration from existing forms of support for individual and collaborative
learning, we generated several ideas for adaptively supporting reciprocal peer tutoring that
went beyond the individual learning model of presenting explicit feedback to the
collaborator who needs support.
Reflective prompts
One idea for delivering adaptive support to collaborators is to mimic the support that human
facilitators provide in face-to-face groups. In accountable talk as described by Michaels et
al. (2008), a teacher directs a classroom using several different reflective “moves”, such as
asking a student to expand on an utterance (“Why do you think that?”). Instead of
presenting a single student with very explicit feedback, it may be beneficial to present all
students involved in the interaction with questions that prompt further reflection and
reasoning. While other adaptive systems have presented feedback publicly to both users
(e.g., Constantino-Gonzalez et al. 2003), it is rare for adaptive systems to pose open-ended
reflective prompts. While it is true that there are valid technical reasons for this design
decision, the ability of systems to analyze open-ended responses is increasing (McNamara
et al. 2007), and looking to the future, it is important to explore the potential of these
prompts. Further, posing these prompts at appropriate times may be beneficial for triggering
cognitive processes (Chi et al. 1994), even if the system does not follow up on student
responses.
Peer-mediated feedback
Some effective fixed collaborative learning scripts attempt to get individual students to
elicit certain responses from their partners; for example, by having students ask their partner
a series of questions at increasing levels of depth (e.g., King et al. 1998). In our second
idea, peer-mediated feedback, the system provides interaction guidance to the partner of the
student whose behavior we would like to change. For example, if one student is not selfexplaining their problem-solving steps, we can prompt their partner: “Did you understand
what your partner did? If not, ask them why.” Students who receive the feedback are thus
encouraged to self-regulate their learning, prompting them to request the help they need
from their partner. For the students whose behavior we would like to change, receiving a
prompt from a partner might feel more natural and comprehensible than receiving computer
feedback.

284

E. Walker, et al.

Adaptive resources
Adaptive resources, instead of explicitly telling students how to improve their behavior,
provide students with resources to help them to make the necessary changes. This approach
is drawn from adaptive hypermedia, where the information that is available to students
changes in accordance with their knowledge (Brusilovsky 2001). In a fixed support
approach developed by Fuchs et al. (1997), students were trained in delivering conceptual
mathematical explanations, using an alternating program of video clips and classroom
discussion. In an adaptive system, a video related to each collaboration skill could be
presented when a student may be thinking of applying the skill (for example, while
preparing explanations for a given problem), and additional materials surrounding the video
could incorporate specific information about the current problem for collaborating students.
While this specific approach has rarely been used in ACLS systems, visualization systems
have been developed that mirror back to students’ aspects of their collaborative
performance (Soller et al. 2005). Augmenting these systems to present more information
to students about reaching ideal performance might be a fruitful area of research.
Speed dating process
Our next step was to use these assistance concepts as a basis for exploring user perceptions
relating to adaptive support. We applied a design method called Speed Dating (Davidoff et
al. 2007), which takes a sketch-based approach to give the designer insight into user
needs. The aspect of Speed Dating we leveraged involves the use of focus groups to
discuss several potential design scenarios in rapid succession. We sketched 12 scenarios
for adaptively supporting a reciprocal peer tutoring activity, based both on the ideas
described above and on traditional ACLS. The support sketches varied according to the
collaborative situation that triggered the support, with four sketches designed to support
peer tutors unsure how to give help, four sketches designed to prevent peer tutors from
giving over-enthusiastic help, and four sketches designed to prevent peer tutors from
giving simplistic instructions. Each scenario leveraged particular aspects of the ideas
described in the previous section. Fig. 1 shows a sample scenario that we presented to
students representing peer-mediated feedback: In response to the peer tutor giving
unasked-for help, the tutee is told to ask his partner to let him try the step before helping.
We then assembled three groups of volunteer high school students with four students per
group. Groups were pulled out of class and interviewed at the school. We presented the
12 support sketches to students, and asked for their reactions to each idea.
Accountability and efficacy design principles
Two motivational influences reappeared in student discussions: feelings of accountability
for tutee learning, and a desire for tutoring efficacy. Students appeared to take their
potential role as peer tutors very seriously, saying when considering a tutoring error:
“Maybe he’s going to be messed up—I wouldn’t want that to happen” (Group 1). They
wanted to feel like good tutors and be perceived as good tutors, responding very positively
to a scenario where the computer offered public praise in the chat window: “I really like the
one where the computer joins in on the IM… You gave that person good advice, both
students see it” (Group 1). Based on this analysis, students who do not feel like capable
tutors may disengage with the activity or simply give their partners the answer in order to
increase their feelings of efficacy.

Computer-Supported Collaborative Learning

285

Fig. 1 Speed Dating scenario. In this scenario, the tutee is encouraged to self-regulate their own learning by
asking the peer tutor to refrain from helping until the tutee has tried the step. Students were presented with 12
scenarios in rapid succession and asked discussion questions

There are two main implications of these motivational factors with respect to designing
assistance provided to peer tutors: First, assistance could be designed to leverage the
feelings of accountability already present in tutoring interactions in order to encourage peer
tutors to give help in effective ways (Accountability Design Principle). For example,
presenting interaction feedback and praise publicly in the chat window where both students
can see it might encourage peer tutors to apply the advice. Second, it is necessary for
assistance in general, and in particular for assistance designed to increase accountability, to
avoid threatening peer tutors’ beliefs that they are capable tutors, but instead to increase
their sense of control over the situation (Efficacy Design Principle). Any assistance given
by the computer should avoid undermining the peer tutor’s control over the interaction, and
for this reason, students overwhelmingly rejected the idea of commentary on peer tutor
actions being given to the tutee, saying that this was “like your teacher talking over your
shoulder” (Group 2). Instead, students preferred assistance that put computers and peer
tutors on more equal footing, such as reflective prompts delivered by computers in the chat
window (“the computer’s asking—I kind of like that… I think the computer should just go
ahead and do it in the chat window”—Group 3). By positioning computers and peer tutors
as collaborators (see Chan and Chou 1997, for examples of this strategy in individual
learning), we may be able to preserve tutoring efficacy, increasing peer tutor motivation to
give good help.
Relevance design principle
When exploring student perceptions of different support designs, we also found that
students particularly focused on how relevant the help appeared to be to their task, and how
little it disrupted their interaction. On a broad level, it was clear that students wanted to get
system feedback that they could use (“If it [the computer] says something we needed to

286

E. Walker, et al.

know then it would be ok”—Group 2). By and large, students cited cognitive help on how
to solve the problems as useful feedback, but surprisingly, what they wanted to receive was
not simply a hint targeted at the next problem step. Students said that the adaptive hints
were not always very informative (“the hint—doesn’t really tell you much”—Group 2), and
admitted that therefore they would be likely to take advantage of the hints facility (“You
could just be clicking the hint button, to like, get the answers”—Group 3). Instead, students
stated that they preferred hints that gave both the high-level concepts relevant to each
problem step, and specific illustrations of the concepts. One student even suggested support
that “give[s] you an example problem, but explains the steps to you and explains how they
get the answer” (Group 3). Despite all this discussion about the usefulness of cognitive
feedback, there was nearly no talk about the usefulness of interaction feedback, suggesting
that students perceived interaction feedback as less relevant than cognitive feedback. As
we consider interaction feedback highly relevant to student learning, this finding may
be problematic.
This analysis leads us to recognize the importance of designing adaptive support so
that it appears relevant to students (Relevance Design Principle) in order to motivate
them to incorporate the assistance into their own interactions. As students believe that
cognitive support is relevant but do not recognize the relevance of interaction support, it
might be that any interaction feedback given to students should be linked to cognitive
feedback, to make the interaction feedback more concrete and immediately applicable.
Telling students: “You should explain why to do a step in addition to what to do. For
example, on the next step your tutee should be trying to isolate the y” might make the
help seem more relevant than simply just telling them “You should explain why to do a
step in addition to what to do.” Another technique for making the collaboration support
more relevant to student interaction is by clearly linking the support to what peer tutors
themselves want to do. For example, help on how to give an explanation would be
perceived as maximally relevant when students are actively trying to give their partner an
explanation.
Discussion
By generating diverse ideas for support, and then using a needs-validation method called
Speed Dating, we generated three design principles for supporting students in collaborating
with each other: accountability, efficacy, and relevance. At first glance, these principles may
not appear surprising: Effects of accountability and need for efficacy have been documented
in previous peer tutoring literature (Fantuzzo et al. 1989; Robinson et al. 2005), and the
need for relevance is well-known (Bernsen et al. 1997). However, one of the surprising
elements of the Speed Dating analysis was that the computer is ascribed a social rather than
functional role when interacting with the peer tutor. This result is not necessarily predicted
by the literature, which suggests that in a computer-mediated context, people react
differently to humans and computers (Rosé and Torrey 2005). The fact that the computer
support in this context might conflict with the peer tutor’s role as “teacher”, threatening
peer tutor feelings of being a good tutor, is interesting and important for the support design.
Although previous work discusses how efficacy is important to peer tutors, it is not clear
from this literature that efficacy, among many other motivational factors, should be a
primary consideration in the design of computer support. In addition, the design exercise
further revealed particular implications of the three design principles for accepting or
rejecting certain varieties of assistance. For example, students’ opinion that feedback to the
peer tutor should never be delivered solely to the tutee is an important insight into how

Computer-Supported Collaborative Learning

287

manipulating the target of the support might affect feelings of efficacy. It also argues for
rejecting peer-mediated feedback delivered to the tutee as an option for assistance. The
insights gleaned from the Speed Dating activity formed the basis for our design of adaptive
support for peer tutor help giving, which is described in the next section.

Phase 2: Assistance design
In our previous research, we constructed APTA (the Adaptive Peer Tutoring
Assistant) to support students in tutoring each other on literal equation solving. As
part of this system, we developed adaptive cognitive support to facilitate peer tutor
reflection on errors and improve the correctness of peer tutor help (Walker et al.
2009). We used the design principles introduced above and student suggestions during the
Speed Dating activity to refine the initial assistance scenarios, yielding three broad forms
of assistance:
1. Hints on demand. Given when the peer tutor does not know how to proceed.
2. Adaptive resources. Provided when the peer tutor needs support in constructing help.
3. Reflective prompts. Delivers feedback on help peer tutors have already given.
By incorporating a variety of adaptive help-giving assistance, we could examine how
different assistance affected motivation, interaction, and learning. We generated prototypes,
and conducted four iterations of think-aloud sessions, creating higher-fidelity versions with
each iteration, until we arrived at our current system.
Previous version of APTA
In APTA, students work on literal equation problems where they are given an equation like
“ax + by = c” and a prompt like, “Solve for x”. Students are seated at different computers,
and at any given time, one student is the peer tutor and the other is the tutee. Tutees can

Fig. 2 Tutee’s problem-solving interface. The tutee solves problems using the menu, chats with their partner
in the chat window, and receives feedback in the solver and skillometer

288

E. Walker, et al.

perform operations on the equation with a menu-based interaction used in the common,
individual version of the Cognitive Tutor Algebra (CTA). See Fig. 2 for a screenshot of the
tutee’s interface. Using the menus, students can select operations like “subtract from both
sides”, and then type in the term they would like to subtract (#2 in Fig. 2). For some
problems, the computer then performs the result of the operation and displays it on the
screen (#3 and #4 in Fig. 2); for others, the tutee must type in the result of the operation
themselves. The peer tutors can see the tutee’s actions on their computer screen, but are not
able to perform actions in the problem themselves (see Fig. 3 for a screenshot of the peer
tutor’s interface, #5). Instead, the peer tutor can mark the tutee’s actions right or wrong (#6
in Fig. 3), and raise or lower tutee skill assessments in the skillometer window (#1, Fig. 3).
Students can discuss the problem in a chat window (#1 in Fig. 2 and #4 in Fig. 3).
To facilitate the discussion in the chat window, we included a common form of fixed
scaffolding: sentence classifiers. This form of fixed scaffolding is thought to be
pedagogically beneficial by making positive collaborative actions explicit in the interface
and encouraging students to consider the type of utterance they wish to make (Weinberger
et al. 2005). We asked peer tutors to label their utterances using one of four classifiers:
“ask why”, “explain why wrong”, “give hint”, and “explain what next” (#8 in Fig. 3).
Students had to select a classifier before they typed in an utterance, but they could also
choose to click a neutral classifier (“other”). For example, if students wanted to give a

Fig. 3 Peer tutor’s interface. Square labels represent possible peer tutor actions in the interface. Round labels
represent the support peer tutors receives from the adaptive system

Computer-Supported Collaborative Learning

289

hint, they could click “give hint” and then type “subtract x”. Their utterance would appear
as: “tutor hints: subtract x” to both students in the chat window. Tutees were also asked to
self-classify each utterance as one of three categories: a “ask for help”, “explain yourself”,
or “other”.
We attempted to trigger peer tutor reflective processes by providing tutors with
adaptive domain assistance that supported them in identifying and reflecting on tutee
errors. We intended that this assistance have the additional benefit of ensuring that the
tutee received more correct help than they otherwise would have. We implemented
cognitive help for the peer tutor from the intelligent tutoring system in two cases. First,
the peer tutor could request a hint from the CTA and relay it to the tutee. Second, if the
peer tutor marked something incorrectly in the interface (e.g., they marked a wrong step
by the tutee correct), the intelligent tutor would highlight the answer in the interface,
and present the peer tutor with an error message. Hints and error messages were
composed of a prompt to collaborate (e.g., “Your partner is actually wrong. Here is a
hint to help you explain their mistake.”), and the domain help the tutees would have
received had they been solving the problem individually (e.g., “Subtract x to move to
the other side.”). Further, if both students agreed the problem was done, and were
incorrect, the peer tutor would be notified and told to ask for a hint about how to complete
the problem. Students were not allowed to move to the next problem until the current
problem was successfully completed.
Including adaptive help-giving assistance in APTA
In the current system we augmented this cognitive assistance with three types of helpgiving assistance, designed based on the principles identified in Phase 1. The first type of
assistance, hints on demand, is used for instances when the peer tutor (for convenience, we
will call her Sara) does not know how to help the tutee (we will call him James). There may
be moments where James has asked for help, and Sara does not know what the next step to
the problem is or how best to explain it. In this case, Sara would click on a hint button,
found in the top right corner of the interface (#3 in Fig. 3), and receive a multi-level hint on
both how to solve the problem and how to help her partner. The hint opens with a
collaborative component (“Remember to explain why your partner should do
something, not just what they should do”), and then contains the cognitive component
that the tutee would have originally received had they been using the CTA individually
(“You can subtract qcv from both sides of the equation to eliminate the constant value of
qcv [qcv –qcv = 0]“; see #9, Fig. 3). If Sara still doesn’t understand what to do and clicks
next hint, both the collaborative and the cognitive component become more specific, until
the cognitive component ultimately reveals the answer to Sara. The collaborative
component uses several strategies to encourage students to give more conceptual help,
and is adaptively chosen based on the current problem-solving context (e.g., it varies
depending on whether the tutee has most recently taken a correct step or an incorrect
step). Sara is intended to integrate the cognitive assistance for how her tutee, James,
should proceed in the problem with the collaborative assistance on what kind of help she
should give. In this case, Sara might use the information she received to tell James
“Eliminate the constant value of qcv”. This hint does not reveal the answer to the tutee,
but includes relevant and correct domain content.
There may be cases where even after examining the adaptive hints, Sara is still unsure
how to use the hints to give the tutee appropriate feedback (e.g., how to give help that refers
to information James already knows). We designed the adaptive resources to further assist

290

E. Walker, et al.

the peer tutor in constructing good help. When Sara clicks the “give hint” sentence
classifier to prepare to compose a hint to her partner (#8 in Fig. 3), she is presented with a
resource (#2 in Fig. 3), with content tailored to the current problem type, which provides
examples of what a good hint would be within the context of this problem type. We had
four separate sets of resources mapping to each type of sentence classifier (one for “ask
why”, one for “explain why not”, one for “give hint”, and one for “explain next step”). As
the resource presents several sample hints for the whole problem, Sara has to actively
process the resource in order to determine which kind of hint might apply to the
information she has to convey. We expected that Sara would use the adaptive hints and
resources together to construct help.
Once Sara has given help to her partner, she might receive a reflective prompt in the chat
window that appears simultaneously to both students and targets peer tutor help-giving
skills that need improvement (#7 in Fig. 3). For example, if Sara is a novice tutor she may
give a novice hint like “then subtract” rather than a conceptual hint like “to get rid of qcv,
you need to perform the inverse operation on that side of the equation.” In that case, the
computer uses its assessment of Sara’s help-giving skill to say in the chat window (visible
to both Sara and James), “James, do you understand the reason behind what Sara just said?”
This utterance is designed to get both James and Sara reflecting on the domain concepts
behind the next step, and to remind Sara that she should be giving help that explains why in
addition to what. Prompts could be addressed to the peer tutor (e.g., “Tutor, can you
explain your partner’s mistake?”) or the tutee (e.g., “Tutee, do you know what mistake
you made?”), and were adaptively selected based on the computer assessment of helpgiving skills (see below). They contained both praise and hedges, such that the
computer’s voice never publicly threatened the peer tutor’s voice. Students also
received encouragement when they displayed a particular help-giving skill (e.g., “Good
work! Explaining what your partner did wrong can help them not make the same
mistake on future problems”). In addition to receiving prompts related to the help
given, there were prompts encouraging students to use sentence classifiers more
effectively (e.g., “The buttons underneath the chat [e.g., “Give Hint”] can help you let
your partner know what you’re doing”). Only one reflective prompt was given at a
time, and parameters were tuned so that students received an average of one prompt for
every three peer tutor actions. There were several different prompts for any given
situation, so students rarely received the same prompt twice.
In order to decide when to give students reflective prompts, we built a model for good
peer tutoring which assessed whether students displayed four help-giving skills: help in
response to tutee errors and requests, help that targets tutee misconceptions, help that is
conceptual and elaborated, and the use of sentence classifiers to give help. Our main focus
was on supporting peer tutors in giving conceptual elaborated help, and we discussed how
the use of sentence classifiers might facilitate that by encouraging peer tutors to reflect
more on the help they give. Similarly, by encouraging peer tutors to target tutee
misconceptions, we encouraged them to reflect and elaborate more on the concepts
involved in solving the problem in general. Finally, by encouraging peer tutors to give help
when tutees need it, we actively discouraged them from giving instrumental help after
tutees take correct steps, a common approach that students take. While these skills should
also benefit tutee learning from peer tutoring, our primary focus for the time being was on
the elaborative processes triggered by peer tutor help-giving, and the learning that might
ensue. To assess peer tutor performance, the model used a combination of several inputs.
First, it used CTA domain models to see if tutees had recently made an error (and thus if
they needed help). Next, it used student interface actions, including tutor self-classifications

Computer-Supported Collaborative Learning

291

of chat actions as prompts, error feedback, hints, or explanations, to determine what the
students’ intentions were when giving help. Finally, it used TagHelper (Rosé et al. 2008), a
toolkit for automated text classification that has been successfully deployed in educational
contexts, to build a machine classifier trained on previous study data (Cohen’s kappa = .82
for the previous dataset). The classifier could automatically determine whether students
were giving help, and whether the help was conceptual. Based on a combination of these
three sources of information, we used a simple computational model composed of 15 rules
to assess each peer tutor action taken. We used Bayesian knowledge tracing (Corbett and
Anderson 1995) to update a running assessment of peer tutor mastery of conceptual help,
targeted help, timely help, and appropriate use of sentence classifiers. If, after any given
peer tutor action, a given tutor skill fell within a predefined threshold for that skill, students
were given a reflective prompt targeting that skill. We defined thresholds both for positive
and negative feedback.
Discussion
We expected APTA to have several positive cognitive and motivational effects. We
designed adaptive support to give students relevant knowledge on how they can improve
the conceptual content of their help when they can apply it. Additionally, as students using
APTA received a lot of assistance that directly encourages students to use classifiers
appropriately, we expected that the adaptive assistance would also have a positive effect on
how often and how accurately students used classifiers. We were further interested in
exploring how the way we designed the adaptive support motivated students to interact with
and apply the support, based on the principles derived in Phase 1. We hypothesized that we
had engaged the Relevance Design Principle in the hints on demand, where we gave a
collaboration hint in conjunction with a context-relevant cognitive hint. By seeing
immediately how the collaboration hint applies to the cognitive hint, students may perceive
the collaboration hint as more relevant. This principle is also applied in the adaptive
resources, which are directly linked to student choice of sentence classifier, ideally
motivating students to interpret and apply the resources. The Accountability Design
Principle was engaged in the adaptivity of the conceptual resources and in the reflective
prompts. The reflective prompts were presented publicly, putting more of an onus on the
peer tutor to follow the prompt. This feeling of accountability was likely augmented by the
way the resources changed as students select different classifiers, suggesting to students that
they should be putting thought into the help they give. We employed the Efficacy Design
Principle by formulating prompts in ways that provide positive feedback and add to what
the peer tutor is trying to do, rather than contradict it.

Phase 3: In vivo experiment
We deployed APTA in a classroom experiment to examine the influence of the adaptive
support on peer tutor help-giving behaviors. In order to determine whether it was indeed the
way the support was designed that was producing a change in student behavior, we
compared it to fixed support that provided the same collaborative instruction, but did not
include adaptive elements. In this section, we describe a quantitative analysis of the effects
of the adaptive support as compared to fixed support on interaction and learning. In the
following section (Phase 4), we discuss a qualitative analysis exploring to what extent our
designs for accountability, efficacy, and relevance had the desired impact.

292

E. Walker, et al.

Fig. 4 Peer tutor’s interface in fixed support condition. Conceptual resources are not connected to sentence
classifiers, domain assistance is in the form of fixed problem solutions, reflective prompts are randomly
delivered between problems, and the students can request randomly selected collaboration tips

Study conditions
The adaptive support condition included the adaptive resources, and reflective prompts
described in the previous section. Furthermore, it included the traditional CTA hints on
demand, and the cognitive hints and feedback from the previous version of APTA. The fixed
support condition contained the same support content as the adaptive system, but the
content was not presented adaptively (see Fig. 4). To create a fixed parallel to the adaptive
cognitive support, where peer tutors were given domain hints and feedback, we provided
students with annotated solutions to the current problem (#2 in Fig. 4), a technique that had been
used as part of other successful peer tutoring scripts (e.g., Fantuzzo et al. 1989). With this fixed
assistance, peer tutors could consult the problem solutions at any time, but would not receive
feedback on whether their help was correct or whether the current problem was completed. To
parallel the hints on demand, we gave students access to a “Random Tip” button that yielded
multi-level randomly selected tips (#4 in Fig. 4). While the overall content of tips was the same
as the hints on demand, the tips were randomly selected rather than chosen adaptively. The
random tips did not contain any adaptive cognitive content. For adaptive resources, we gave
students access to the same resources as they had in the adaptive condition, but the resources
did not change based on the sentence classifiers students selected—instead, students had to
select which resource they wanted to view without additional guidance (#1 in Fig. 4). Finally,
instead of receiving reflective prompts in the chat window, we gave students reflective
collaborative tips between each problem, with parallel content to the reflective prompts present

Computer-Supported Collaborative Learning

293

in the adaptive condition (#3 in Fig. 4). Each student was presented with 5 randomly
chosen reflective statements after each problem was complete such as “Good work!
Remember, hinting or explaining the reason behind a step can help your partner learn
how to do the step correctly.” We chose that form of support also because it is common
for students using a collaborative script to receive reflective prompts at fixed intervals.
This approach was a reasonable way to provide students with similar content to the
adaptive condition. A summary of support is shown in Table 1.
Method
Participants
Participants were 104 high-school students (54 male, 50 female) from two high schools,
currently enrolled in Algebra 1, Algebra 2, or Pre-Calculus. Both high schools used the
individual version of the CTA as part of regular classroom practice so students were used to
working with the tutors. The literal equation solving unit that we used was a review unit for
the students, and one that they had already covered in Algebra 1. Nevertheless, the concepts
in the unit were difficult for the students to understand, and teachers were in favor of
reviewing the unit. Students from each class were randomly assigned to one of the two
conditions, and to either the initial role of tutor or tutee (later they switched roles). For the
purposes of this analysis, we are interested in those students who interacted with the system
as a tutor, and thus excluded 27 students who only took on the role of tutees; that is, they
were absent on one or both supported tutoring days and were tutees on the days they were
present. We further excluded one student who was partnered with a teacher when tutoring,
and two students who played the role of tutor in both collaboration periods. A total of 74
students were included in the analysis.
Procedure
The study took place over the course of a month, spread across six 45-minute
classroom periods. During the first period, students took a 15-minute pretest measuring
Table 1 Assistance differences in adaptive and fixed systems
Assistance Type

Adaptive System

Fixed System

Cognitive Feedback

Must finish a problem before
moving on. Receive feedback
on marking actions.

Move to next problem when
students believe current
problem is complete. Seek
out feedback on marking
actions by accessing problem
solutions.

Peer-Mediated Hints

Receive integrated cognitive &
interaction hint

Seek out a cognitive hint by
accessing problem solutions.
Receive list of interaction tips
at the end of each problem.

Conceptual Resources

Get resource linked to selected
sentence classifiers.

Select resources and use
classifiers independently.

Reflective Prompts

Receive reflective prompts based
on dialog.

Receive reflective prompts
while waiting for next problem
to load.

294

E. Walker, et al.

domain learning. Then, in the second period, students spent 45 min in a preparation
phase, solving problems individually using the CTA. Students worked on one of two
problem sets, focusing on either factoring in literal equation solving or distributing in
literal equation solving. The third and fourth periods were collaboration periods, where
students were given partners, and tutored them on the problems they had solved in the
second period, with either adaptive or fixed support. Students were given different
partners for each of the two collaboration periods. They were paired with students who
were in the same condition, but who had solved a different problem set during the
preparation phase. Within these constraints, we assigned pairs randomly, with the
exception of not pairing students teachers explicitly told us would not get along. Within
a pair, students were randomly assigned to the tutor or tutee role during the first
collaboration period, and then they took on the opposite role during the second
collaboration period. In the fifth period, students collaborated with new partners without
any adaptive support (as an assessment of their collaborative abilities) and in the sixth
period, between 2 and 3 weeks after the completion of the study, students took a
posttest to assess their domain learning.
Measures
To assess students’ individual learning we used counterbalanced pretests and
posttests, each containing 10 conceptual items, 5 procedural items, and 2 items that
demanded a verbal explanation. Some of the conceptual items had multiple parts. The
tests were developed by the experimenter, but adapted in part from Booth and
Koedinger’s measures of conceptual knowledge in Algebra (2008). Tests were
approved by the classroom teacher, and were administered on paper. We scored answers
on these tests by marking whether students were correct or incorrect on each item part,
computing the scores for each item out of 1, and then summing the item scores to get a
total score.
In order to analyze student collaborative process, we logged all semantic actions students
took within the system, including tutee problem-solving actions, sentence classifiers
selected by both students, and chat actions made by both students. Along with the student
actions, we logged computer tutor responses, which includes both the system’s evaluation
of the action and the assistance students received. Using this data, we computed the number
of problems viewed by each student, and the number of problems correctly solved (in the
fixed condition, students could move to the next problem without having correctly solved
the previous one). We calculated the number of errors viewed by students when they took
on the peer tutoring role, and the number of times peer tutors used each type of sentence
classifier. Finally, we computed peer tutor exposure to the assistance in our system,
including the number of times they received reflective prompts and the number of times
they requested a cognitive hint.
We segmented the dialog by chat messages (creating a new segment every time students
hit enter), and two raters coded the chat data on several dimensions. We computed interrater reliability on 20% of the data, and the remainder of the data was coded by one rater
and checked by a second. All disagreements were resolved through discussion. First, each
help segment was coded for whether it constituted previous-step help, that is, help relating
to an action tutees had already taken (e.g., “no need to factor because there is only one g”;
Cohen’s kappa = 0.83), or whether it was next-step help, that is, help relating to a future
action in the problem (e.g., “how would you get rid of 2h?”; Cohen’s kappa = 0.83).
Finally, each help segment was coded for whether it contained a concept (e.g., “add ax” is

Computer-Supported Collaborative Learning

295

purely instrumental help, while “add ax to cancel out the –ax” is conceptual). Cohen’s
kappa for conceptual help was 0.72.
Quantitative results
We used quantitative interaction and learning data to determine if the peer tutor’s help
quality increased because of the assistance they received, and if an increase in help quality
translated into a learning improvement.
Overall interaction context
First, to get a sense of the context of student interaction, we examined whether there were
systematic high-level differences between the two conditions in the way students solved
problems and gave help. We used a MANOVA with condition as the independent variable to
evaluate the differences between conditions for the following variables: problems viewed,
problems completed correctly, tutee errors viewed by tutors, and help given by tutors. The
analysis revealed significant differences between conditions (Pillai’s Trace = 0.30, F [1, 72]=
7.68, p=0.001). Table 2 displays the results of one-way ANOVAs for each dependent variable.
The students in the fixed condition saw significantly more problems than students in the
adaptive condition (row 1). Students in the fixed condition could skip past problems that gave
them trouble (and occasionally did not realize they had made a mistake), while students in the
adaptive condition had to overcome every impasse they reached. However, both conditions
completed similar numbers of problems correctly (row 2), and the total number of tutee errors
viewed by peer tutors was not significantly different across conditions (row 3). Finally, the
amount of help given by peer tutors was not significantly different across conditions (row 4).
The ratio between errors viewed by the peer tutor and help given was roughly 4:3 in the
adaptive condition and 1:1 in the fixed condition. In the following, we present count data of
particular aspects of student interaction, and use negative binomial regression to test the
relationship between variables. Unless otherwise noted, we will perform statistical tests on the
raw data counts, but to better illustrate what occurred, we may also present ratios between the
count data and context variables like errors viewed or total amounts of help.
Amount of conceptual help
Our main goal in the design of the adaptive assistance was to improve the quality of help
given in the adaptive condition. This goal was operationalized as improving the amount of
conceptual help given, since conceptual help is an indicator of the elaborative processes in

Table 2 Differences in problem-related actions across conditions
Context variables

Adaptive
M

Fixed

ANOVA results

SD

M

SD

F(1, 74)

p

Problems seen

7.90

4.51

10.43

5.76

4.483

0.038

Problems completed

7.26

4.42

7.60

4.37

0.113

0.738

15.71
11.92

8.56
6.23

12.63
12.17

8.41
8.87

2.444
0.019

0.122
0.890

Errors viewed
Help given

296

E. Walker, et al.

Table 3 Differences in help quality between conditions, as measured by the amount of conceptual help
given and the way peer tutors used classifiers
Interaction variables

Adaptive
M

Fixed
SD

M

Mann-Whitney results
SD

U

p
0.015

Conceptual help (n=74)

2.67

2.83

1.34

2.14

468.5

Classifiers used (n=74)

7.95

6.77

4.28

5.78

371.5

0.001

56.8%

33.7%

31.0%

34.4%

348.00

0.001

% help given with classifiers (n=71)

peer tutoring, and a predictor of learning gains for both students. The effects of condition
on conceptual help were significant (see Table 3, row 1). In total, roughly 20% of the help
was conceptual in the adaptive condition, nearly double the percentage of help that was
conceptual in the fixed condition (10%).
Frequency and accuracy of classifier use
In addition to improving the quality of student help-giving, we intended that the adaptive
help would improve student use of interface features, and in particular, encourage students
to use the sentence classifiers while chatting. As described in the Introduction section,
sentence classifier use is theoretically related to help quality, and thus should be related to
the amount of conceptual help that students give. Further, the more appropriately students
use classifiers, the better intelligent systems are determining the content of student chat.
Thus, one hypothesis we had was that students would use help-related classifiers (i.e., not
the neutral “other” classifier) more frequently in the adaptive than in the fixed condition,
regardless of the content of their utterances. This hypothesis was supported by the data (see
Table 3, row 2). Students used roughly 2 classifiers for every 3 errors in the adaptive
condition, compared to 1 classifier for every 3 errors in the fixed condition. However, while
this measure reflected how often students used classifiers, it did not reflect the student’s
purpose in using the classifiers. We also predicted that when peer tutors gave help to tutees,
they would be more likely to label their utterance with one of the help-related classifiers
than the “other” classifier. The percentage of help given using help-related classifiers was
significantly greater in the adaptive condition than in the fixed condition (see Table 3 row
3), suggesting that students used classifiers appropriately more often in the adaptive
condition. The percentage of non-help chats given using help-related classifiers were
not significantly different between conditions, suggesting that it was not increased
classifier use overall that was driving the effect.
We further explored the relationship between condition, sentence classifiers used, and
conceptual help given. The number of classifiers used and conceptual help given were
correlated (r[72]=0.442, p<0.01), but it was not clear whether condition had separate
effects on classifiers used and conceptual help given, or whether the number of classifiers
used influenced the amount of conceptual help given (as suggested by prior research on
sentence classifiers). To explore these separate possibilities, we conducted a regression
analysis to predict the amount of conceptual help given controlling for the number of
classifiers used. We used student condition, the number of sentence classifiers used, and the
amount of help given overall as predictor variables. The model was indeed statistically
significant (χ2(3, N=74)=33.287, p<0.001). Condition was a significant predictor of
conceptual help given (β=0.687, χ2(1, N=74)=5.84, p=0.016), as was the amount of help

Computer-Supported Collaborative Learning

297

given (β=0.087, χ2(1, N =74)=22.97, p<0.001). Classifiers used were marginally
predictive (β=0.042, χ2(1, N=74)=3.78, p=0.052). Based on these results, when help
given and classifiers used are held constant, the adaptive condition is responsible for about
1.98 more instances of conceptual help than the fixed condition. This analysis suggests that
while both help given and classifiers used were predictive of conceptual help given,
condition had an independent effect.
Learning outcomes
Finally, we looked at whether learning outcomes varied between the two conditions.
The adaptive condition had a mean pretest score of 33.53% (SD=25.11%) and posttest
score of 40.55% (SD=21.50%). The fixed condition had a mean pretest score of 39.13%
(SD=23.92%) and posttest score of 47.10% (SD=26.28%). We conducted a two-way
repeated-measures ANOVA with condition as a between-subjects variable and test-time as
a within-subjects variable. We used only students who had participated in the pretest,
posttest, and an intervention phase as a peer tutor. All students learned (F [1, 49]=11.97,
p=0.001), but there were no significant learning differences between conditions (F [1,
49]=0.048, p=0.828).

Phase 4: Case studies
The adaptive support improved two aspects of peer tutor help given compared to the
fixed condition: conceptual content and use of sentence classifiers. We next investigated
to what extent the positive influence of the adaptive support on help-giving was related
to the hypothesized desired effects on student motivational factors, following the design
principles identified in Phase 1. We present one case representative of the positive
effects of accountability on student help-giving, and one case representative of the
negative effects of a lack of perceived relevance. We use both cases to discuss the
influence of efficacy on student help-giving. While there are likely many contextual
factors contributing to the influence of support on student help-giving, we limit our
discussion here to the three identified principles, in order to better follow up on the
Phase 1 results.
A case of accountability & elaborative processing
With this case study of Dyad 1, we illustrate how feelings of accountability to be good
tutors engendered by the adaptive support encouraged dyads to engage in elaborative
processing. In this dyad, the peer tutor scored 55% on the posttest, and the tutee scored
20%. The interaction occurred on the second tutoring day, and concerned the problem kj –
mk = fr, solve for k. It was the second problem the dyad had seen that day, but the first with
this form. Over the course of the interaction, the different assistance types increased the
peer tutor’s accountability to knowledge and to reasoning – that is, her effort to give the
correct answer and to give a conceptual explanation for her answer. The interaction begins
with the tutee asking for help (see Table 4, row 1). When the peer tutor clicks on the
sentence classifier “explain next step” to compose her response, the peer tutor receives a
resource on how to construct good explanations. On first glance, the resource appears to
have little effect, as only 10 seconds pass between the time the resource is presented in the
interface and the time the peer tutor’s response is submitted, and the peer tutor gives

298

E. Walker, et al.

Table 4 Case study demonstrating the positive effects of adaptive support on student interaction. Support
may trigger student feelings of accountability
Solve for k: kj – mk = fr
1

Tutee: [both] What should I do first?
[self]

chooses “explain next step” classifier

2

Tutor:

3

Agent: [tutor] gives resource on explanations

4

Tutor:

5

Agent: [both] Tutee, do you understand the reason behind what the tutor just said?

[both] Add mk to both sides.

6

Tutee: [both] adds mk to both sides of the problem

Solve for k: kj = fr + mk
7 Tutee: [both] Does it matter that there’s a k on the right side?
[both] marks the “add mk” step correct

8

Tutor:

9

Agent: [tutor] highlights step

10

[tutor] This step is wrong. Give your partner some advice on what to do next

11 Tutor:

[tutor] chooses “comments” classifier

12

[both] Wait!!! I completely messed up… the computer wants you to subtract kj from
both sides, because of the other k in the problem. sorry = (

13 Tutee: [both] haha, it’s alright, these problems are so simple but confusing.

instrumental help (“add mk to both sides”; row 4). However, the simple presentation of this
resource begins to establish the expectation that peer tutors are expected to put thought into
the help that they give. A second type of assistance is presented immediately after the peer
tutor has delivered her instrumental help: the computer says in the chat window, where both
collaborators can see it (“Tutee, did you understand the reason behind what the tutor just
said?”; row 5). Not only is the computer prompting the tutee to reflect, but it is also
publicly reminding the peer tutor that help should include an explanation in addition to an
instruction, further increasing the peer tutor’s accountability for giving elaborated help. In
fact, the tutee responds to this prompt with evidence of deep processing (row 7): “Does it
matter that there’s a k on the right side?” The tutee is reflecting on features of the problem
that are relevant for attaining the problem solution. After the tutee has in fact added mk, and
the peer tutor has marked the step wrong, the computer further enforces the peer tutor’s
accountability to provide help by saying privately to the peer tutor: “This step is wrong.
Give your partner some advice on what to do next.” At this point, the peer tutor’s response
represents a breakthrough in the peer tutor’s helping behaviors. The peer tutor responds
with a conceptual statement, saying “the computer wants you to subtract kj from both sides,
because of the other k in the problem” (row 12). This statement explains what the tutee
should do, explains why, and alludes to the concept that all ks in this problem have to be on
the same side, suggesting that the peer tutor is reflecting on the next step and elaborating on
her knowledge. It is the first conceptual statement made by this particular peer tutor. This
insight on the part of the tutor, and articulation of the insight to the tutee, had benefits for
both parties. The error that Dyad 1 made during this problem required them to master the
concept that to solve for a given variable all instances of the variable need to be moved to
the same side of the equation. Both the tutor and the tutee in the dyad got a similar problem
correct on the individual posttest, suggesting that as a result of this interaction, they had
mastered the discussed concept.

Computer-Supported Collaborative Learning

299

Table 5 Case study demonstrating the problem with perceived relevance of adaptive support. While the peer
tutor perceived domain support as relevant, he did not use the interaction support
Solve for q: (6 t – wr)/vt = (qt + qv)/vt
1

Tutee: [both]

2

Tutor:

3

Agent: [tutor]

A good hint explains why in addition to what. What can you do to both sides to get
the q by itself? In (qv + qt)/vt, qv + qt is divided by vt. How do you undo
division? Multiply both sides by vt.

4

Tutor:

chooses "give hint" classifier

5

Agent: [tutor]

gives resource on hints

6

Tutor:

Multiply both sides by vt

7

Agent: [both]

Tutee, can you talk about why you took that last step?

8

Tutee: [both]

What last step? The simplifying fractions?

[self]
[both]

9 Tutor: [both]
10 Agent: [tutor]
11 Tutor:

It won't let me get rid of the v and t. Help me.

[agent] requests hint

marks the simplifying fractions step right
This step is wrong. Get your partner to think about what to do next.

[agent] requests hint

12 Agent: [tutor]

Remember to explain what your partner did wrong, in addition to what to do next.
What can you do to both sides to get the q by itself? In (qv + qt)/vt, qv + qt is divided
by vt. How do you undo division? Multiply both sides by vt.

13 Tutor:

chooses "give hint" classifier

[self]

14 Agent: [tutor]

gives resource on hints

15 Tutor:

Delete the last 3 steps and multiply both sides by vt

[both]

A case of support relevance & shallow processing
While peer tutors appeared to find adaptive help on how to solve the problem extremely
relevant, they did not have a similar response to adaptive assistance on how to give good
help, potentially leading them to process the problem shallowly. The case of Dyad 2 in
Table 5, who engaged in suboptimal interaction, is from the first tutoring day (the third
study period); the dyad was solving the problem 6t – qt = wr + qv. This was their ninth
problem of the day, but the first problem they had encountered where they had to move two
variable instances to the same side. The peer tutor had scored 23% on the pretest, and the
tutee had scored 38%. The following dialogue begins when the tutee had reached the
equation 6 t - wr = qt + qv, but then incorrectly divided both sides by vt instead of v + t. The
tutee triggers the exchange using a question that shows the tutee is reflecting on the
situation (“It won’t let me get rid of the v and t. Help me”; row 1). The peer tutor asks for a
hint, but then only transfers the instrumental component of the hint to the tutee (“Multiply
both sides by vt”; row 6), suggesting that while the peer tutor feels that the domain help is
relevant, he doesn’t perceive the conceptual scaffolding as relevant. As in the previous
scenario, the computer prompts the tutee for further explanation (“eagle, can you talk about
why you took that last step?”; row 7), but this only serves to confuse the students further
(“what last step?”; row 8), suggesting that the vague wording of the prompts may be a
liability in this case. After getting more content-related feedback, and another hint, the peer
tutor relays the hint to the tutee again (row 15). After this exchange, the tutee realizes his
error and proceeds to solve the problem, without interacting further with the peer tutor. This

300

E. Walker, et al.

lack of communication has effects on the posttest results: For Dyad 2 to solve this problem
correctly, they needed to master the concept that to isolate the x in an expression like x(a +
b) you need to divide by (a + b). Neither member of the dyad got the related conceptual
question right on the posttest. Not surprisingly, the peer tutor in this interaction came out of
the session unsure of how to use the computer help, saying in the following period when he
was the tutee: “yeahh the tutor is confusing cuz it gives youu all this stuff to write about but
I had no clue what to write when i was the tutor.” In summary: When this peer tutor gave
help, he ignored the collaborative components of the hint he received and focused on the
cognitive component, which contradicts what we had intended with our design.
Peer tutor self-efficacy: Transfer of control
We had also attempted to design feedback to maintain the peer tutors’ sense of tutoring selfefficacy. To a certain extent, the design appeared successful, and in some cases, the support
we gave helped peer tutors to take control of the situation. For example, after one tutee
added ax to both sides in the problem ax – y=8, and the peer tutor marked it right, the peer
tutor received the feedback: “This step is not right. Tell your partner what mistake they
made. Here is a hint to help you tutor your partner. Since a*x is positive, you should
subtract to remove it from the left side. Erase your last step and subtract a*x from both
sides.” In consequence, the peer tutor changed their response, marking the step wrong, and
then smoothly gave the conceptual hint “It’s a positive ax you wouldn’t add u would
subtract.” This peer tutor was adept at using the cognitive tutor hints to give their partner
guidance, and while the peer tutor didn’t acknowledge his error, he did give error feedback
to the tutee. However, sometimes students would attribute hints to the computer in order to
indicate their uncertainty and to convey to their peer tutee a sense that they (peer tutor and
tutee) are in the same boat. A good example of this phenomenon is in the first case study,
where the peer tutor both attributes the hint to the computer, and apologizes for the
confusion (“wait!!! I completely messed up… the computer wants you to subtract kj from
both sides, because of the other k in the problem. sorry = (“). Interestingly, the peer tutor
gives a much more elaborated hint than the one she had received from the computer, but
still attributes the hint to the computer, probably to indicate her own lack of confidence in
the solution. The same students from Dyad 1 verbally expressed similar sentiments at
several points, bonding over their own inexpertise: The peer tutor said, “wow… this is so
confusing!” The peer tutee replied, “I’m glad I’m not the only one who’s confused!
hahaha”. Those two students went on to be successful at solving the problem. Against our
designs, it appeared that the peer tutor indicating uncertainty and attributing help to the
computer was beneficial for the tutor-tutee relationship, suggesting that in some cases,
constant maintenance of peer tutoring efficacy was not as necessary as the peer tutor being
able to transfer control to the computer, shifting between expert and novice.

General discussion
In this paper, we described a four-phase design process for developing adaptive assistance for
help-giving in a peer tutoring context. First, we used Speed Dating, a human-computer
interaction design method, to generate three principles for designing adaptive support for
collaborating students. On the basis of the three principles we developed three types of helpgiving assistance for our peer tutoring system: hints on demand, conceptual resources, and
reflective prompts. We evaluated the resulting system in an in vivo experiment, and found that

Computer-Supported Collaborative Learning

301

compared to a fixed support condition, the adaptive assistance improved the conceptual content
of student help and their use of sentence classifiers. Case analyses of process data of two
dyads from the adaptive condition suggested that while we successfully designed to increase
student accountability, we were not as successful at increasing the perceived relevance of the
adaptive collaboration support. In this section, we discuss the theoretical conclusions and
design implications of our empirical results, and the promise of iterated in vivo
experimentation.
Theoretical conclusions
Our results add to the small but growing body of evidence that adaptive support can
improve the quality of student collaboration. Previous research in the effects of adaptive
support compared to a fixed control has found that adaptive support can increase student
learning (Kumar et al. 2007), but little is known about how adaptive support affects
collaborative process. Our research provides direct evidence of the effects of adaptive
support on a specific facet of student interaction: tutor help-giving. We found that the
adaptive support led students to increase the conceptual help content of their utterances
compared to fixed support. Help-giving is one of the positive aspects of collaboration
specified by Johnson and Johnson (1990), and conceptual content is widely recognized as
an important component of help-giving (Fuchs et al. 1997; Webb and Mastergeorge 2003).
Thus, our intervention (and in particular, the use of adaptive reflective prompts) could
potentially be applied to other collaborative scenarios that would benefit from an
improvement in the conceptual quality of student help-giving in text-based chat (such as
the Virtual Math Teams system; Stahl 2009). Our system also used simple adaptive prompts
to improve the way peer tutors used sentence classifiers for their help-giving, in that they
chose to use help-related sentence classifiers more often and more accurately. Applying a
similar method to other systems that incorporate sentence classifiers and starters may make
those other interventions more effective. For example, introducing reflective prompts into
the GroupLeader system may improve the difficulties the system developers have
encountered in getting students to use sentence starters accurately (Israel and Aiken
2007). We did not find effects of adaptive support on student learning, compared to fixed
support, but our study was a short-term study, and attrition between the intervention and the
posttest was rather large. In theory, well-designed adaptive support will, in the end, have a
positive effect on student learning.
Our design principles, informed by our qualitative results, contributed to the research
into how students are motivated by peer tutoring. While most studies have looked broadly
at how reward structures increase student accountability (e.g., Fuchs et al. 1997), they have
not examined how this mechanism might be working during the interaction. Our design
work in Phase 1 and the qualitative analyses in Phase 4 support the conclusions of previous
experimental manipulations by demonstrating that students feel accountable to be good peer
tutors to their partners, and that this accountability increases when relevant and public
support is given to tutors (i.e., when peer tutor responsibility is primed). With the increase
in accountability, students put more effort into constructing help and applying the assistance
they received to their help, potentially engaging in more cognitive elaborative processes.
This result suggests that it may not be the adaptivity of the support that is creating these
results, but the perceived adaptivity. In other words, a different fixed control that takes the
form of random prompts in the chat window may have the same effect as adaptive support,
if students perceive the prompts as adaptive and are thus motivated to feel more
accountable for the help they construct.

302

E. Walker, et al.

Given this analysis, and the effects of the support on student help behavior but not
on learning outcomes, it is still an open question whether the effort and expense
required to develop adaptive systems for collaboration is worth the result. Our
classroom study had several limitations, including that the sample size was relatively
small, and that the experimental manipulation included different types of support, which
may have made the system unnecessarily complicated for students to use. Another
study is necessary to tease apart to what extent adaptivity has cognitive benefits (i.e.,
students get the support at the correct time and thus benefit more), and to what extent it
has motivational benefits (i.e., students feel accountable for incorporating support
because they believe it is adaptive). Ideally, this study would have a larger sample size,
take place in a more controlled environment, and vary only one type of adaptive
support. If the results of this future study suggest that adaptive support only has
motivational benefits, and that the adaptiveness itself does not lead to learning, then it
is likely that research on supporting collaboration should focus elsewhere. On the other
hand, if the results of this future study suggest that adaptivity is important and adaptive
support has learning benefits, research on achieving adaptivity of support should indeed
be encouraged, despite the expense.
Design implications
Our results also have direct implications toward defining a design space for ACLS that
can inspire future research. As described in the introduction, most ACLS support is
very similar in the way it provides feedback to students. This similarity can be
conceptualized on two dimensions: the feedback is usually directly delivered to the
ineffective collaborator, and it is explicit with respect to telling the ineffective
collaborator what they did wrong and how to fix it. The assistance used as part of needs
identification in Phase 1 and incorporated in our system in Phase 2 covers a much broader
design space. The hints on demand we provide has a direct and explicit component, but
also a peer-mediated component: we expect the peer tutor to benefit from receiving
domain help and communicating it to the tutee. The conceptual resources are directly
presented to the peer tutor, but they are implicit in nature: The peer tutor is expected to
read the content and determine how to use it to generate good help in that particular
situation. The reflective prompts we included in the system are situated somewhere in the
middle of the design space, they are presented to both students, and worded in a general
way.
While it is difficult to directly compare different assistance types in our experimental
design, we can draw some links to research questions about how varying explicitness and
directness might impact the adoption of support. First, how does directness influence
accountability? In our study, students appeared to feel more accountable to use support
meant for their partner (i.e., the cognitive hints) or support that was publically delivered (i.
e., the reflective prompts) than support that was delivered directly and only to the peer tutor
(i.e., the collaborative portion of the hints, and the adaptive resources). Future studies could
tease out the effects of directness from the effects of support type. A second research
question might be: What effects does explicitness have on relevance and efficacy? While
students appeared to find the most explicit support to be the most relevant, particularly in
Phase 1, peer tutors also resented any support that undermined their ability to tutor. In
general, by developing a sense of how different features of adaptive collaboration support
affect how students react, we can design effective adaptive support tailored to particular
student populations and contexts.

Computer-Supported Collaborative Learning

303

Iterated in vivo experimentation
To conclude, we would argue that the iterated in vivo experimentation design process we
have described in this paper represents a fruitful combination of design research and
controlled experimentation. It is true that this approach has certain drawbacks; we lack the
full ecological context and deep evaluation that design research brings, and we lack the full
control provided by psychological experimentation. However, this approach has unique
value in that it effectively finds a balance between tradeoffs commonly found in learning
sciences research. Our approach balanced experimental control and ecological validity by
allowing us to draw conclusions about how adaptive support affects student behavior while
maintaining a holistic perspective. For example, our choice to evaluate multiple types of
adaptive assistance simultaneously represented a loss in experimental control, as we varied
multiple dimensions of support. However, it allowed us to examine the effects of each type
of assistance through an analysis of the process data, and thus draw richer conclusions
about how support might affect student motivation. Similarly, our use of mixed methods in
combining quantitative data with qualitative analyses allowed us to link accountability to
the quality of student help-giving in a way that would be difficult had we not combined the
approaches. Without the qualitative data it would have been difficult to determine why
student use of conceptual help improved; but without the quantitative data, it would have
been difficult to determine how differences between isolated cases mapped to systematic
differences between conditions. Finally, one of the contributions of our particular process is
that, through our use of human-computer interaction design methods such as Speed Dating,
we treated the students as “users” instead of just “learners”. We designed for use by
analyzing how students perceived and reacted to the support, instead of solely examining
which support was likely to lead to the most learning. This approach served as a good
precursor to designing a form of assistance students do not typically receive, and letting us
know what to expect in terms of how students interact with that kind of assistance. By
designing a system that responded to student motivational needs, we hoped to ultimately
have a more positive effect on their learning as well. In summary, as a result of our
approach of iterated in vivo experimentation, we have made theoretical contributions to the
literature on adaptive support for collaborative learning, and defined a space for future
experimental and design research.
Acknowledgments This project is supported by the Pittsburgh Science of Learning Center which is funded
by the National Science Foundation award number SBE-0836012. Thanks to Thomas Harris, Tristan Nixon,
and Steve Ritter for their support concerning the use of the Carnegie Learning Cognitive Tutor Algebra code,
and to Gail Kusbit, Christy McGuire, and the classroom teachers for their motivated involvement in the
project. Finally, thanks to Carolyn Rosé, Dejana Diziol, Ido Roll, Ruth Wylie and Amy Ogan for their
comments at various stages.

References
Baghaei, N., Mitrovic, A., & Irwin, W. (2007). Supporting collaborative learning and problem solving in a
constraint-based CSCL environment for UML class diagrams. International Journal of ComputerSupported Collaborative Learning, 2(2–3), 159–190.
Barab, S., & Squire, K. (2004). Design-based research: Putting a stake in the ground. Journal of the Learning
Sciences, 13(1), 1–14.
Bernsen, N., Dybkjær, H., & Dybkjær, L. (1997). What should your speech system say? Computer, 30(12),
25–31. Dec. 1997.

304

E. Walker, et al.

Beyer, H., & Holtzblatt K. (1997). Contextual Design: A Customer-Centered Approach to Systems Designs.
Academic Press.
Booth, J. L., & Koedinger, K. R. (2008). Key misconceptions in algebraic problem solving. In B. C. Love, K.
McRae, & V. M. Sloutsky (Eds.), Proceedings of the 30th Annual Conference of the Cognitive Science
Society (pp. 571–576). Austin: Cognitive Science Society.
Brusilovsky, P. (2001). Adaptive hypermedia. User Modeling and User-Adapted Interaction, 11(1/2), 111–
127.
Brydon-Milier, M., Greenwood, D., & Maguire, P. (2003). Why action research? Action Research, 1(1), 9–
28.
Chan, T. W., & Chou, C.-Y. (1997). Exploring the design of computer supports for reciprocal tutoring.
International Journal of Artificial Intelligence in Education, 8(1), 1–29.
Chaudhuri, S., Kumar, R., Howley, I., & Rosè, C. P. (2009). Engaging collaborative learners with helping agents.
In V. Dimitrova, R. Mizoguchi, B. du Bulay, & A. Graesser (Eds.), Proceedings of the 14th Intl. Conf. on
Artificial Intelligence in Education (AIED 2009) (pp. 365–372). Amsterdam: Ios Press.
Chi, M. T. H., DeLeeuw, N., Chiu, M.-H., & LaVancher, C. (1994). Eliciting self-explanations improves
understanding. Cognitive Science, 18, 439–477.
Collins, A. (1999). The changing infrastructure of education research. In E. C. Lagemann & L. S. Shulman
(Eds.), Issues in education research: Problems and possibilities (pp. 289–298). San Francisco: JosseyBass Publishers.
Constantino-Gonzalez, M., Suthers, D. D., & de los Santos, J. G. (2003). Coaching Web-based collaborative
learning based on problem solution differences and participation. International Journal of Artificial
Intelligence In Education, 13, 2–4 (Apr. 2003), 263–299.
Corbett, A. T., & Anderson, J. R. (1995). Knowledge tracing: Modeling the acquisition of procedural
knowledge. User Modeling and User-Adapted Interaction, 4, 253–278.
Davidoff, S., Lee, M.K., Dey, A.K., & Zimmerman, J. (2007). Rapidly exploring application design through
speed dating. In J. Krumm, G. D. Abowd, A. Seneviratrne, & T. Strang (Eds.), UbiComp 2007:
Ubiquitous Computing. Proceedings of the 9th International Conference, UbiComp 2007, Innsbruck,
Austria, September 16–19, 2007 (pp. 429–446). Germany: Springer-Verlag.
Dillenbourg, P., & Jermann, P. (2007). Designing integrative scripts. In F. Fischer, H. Mandl, J. Haake & I.
Kollar (Eds.), Scripting computer-supported communication of knowledge - cognitive, computational
and educational perspectives (pp. 275–301). New York: Springer.
Fantuzzo, J. W., Riggio, R. E., Connelly, S., & Dimeff, L. A. (1989). Effects of reciprocal peer tutoring on
academic achievement and psychological adjustment: A component analysis. Journal of Educational
Psychology, 81(2), 173–177.
Fischer, F., Mandl, H., Haake, J., & Kollar, I. (2007). Scripting computer-supported collaborative learning—
cognitive, computational, and educational perspectives. Computer-supported collaborative learning
series. New York: Springer.
Fuchs, L., Fuchs, D., Hamlett, C., Phillips, N., Karns, K., & Dutka, S. (1997). Enhancing students’ helping
behavior during peer-mediated instruction with conceptual mathematical explanations. The Elementary
School Journal, 97(3), 223–249.
Hmelo-Silver, C. E. (2004). Problem-based learning: What and how do students learn? Educational
Psychology Review, 16(3), 235–266.
Israel, J., & Aiken, R. (2007). Supporting collaborative learning with an intelligent web-based system.
International Journal of Artificial Intelligence in Education, 17, 340.
Johnson, D. W., & Johnson, R. T. (1990). Cooperative learning and achievement. In S. Sharan (Ed.),
Cooperative learning: Theory and Research (pp. 23–37). NY: Praeger.
King, A., Staffieri, A., & Adelgais, A. (1998). Mutual peer tutoring: Effects of structuring tutorial interaction
to scaffold peer learning. Journal of Educational Psychology, 90, 134–152.
Koedinger, K., Anderson, J., Hadley, W., & Mark, M. (1997). Intelligent tutoring goes to school in the big
city. International Journal of Artificial Intelligence in Education, 8, 30–43.
Koedinger, K. R., Aleven, V., Roll, I., & Baker, R. (2009). In vivo experiments on whether supporting
metacognition in intelligent tutoring systems yields robust learning. In D. J. Hacker, J. Dunlosky, & A.
C. Graesser (Eds.), Handbook of Metacognition in Education (pp. 897–964). The Educational
Psychology Series. New York: Routledge.
Kollar, I., Fischer, F., & Hesse, F. W. (2006). Collaboration scripts—A conceptual analysis. Educational
Psychology Review, 18(2), 159–185.
Kollar, I., Fischer, F., & Slotta, J. D. (2005). Internal and external collaboration scripts in web-based science
learning at schools. In T. Koschmann, D. Suthers, & T.-W. Chan (Eds.), Proceedings of the International
Conference on Computer Support for Collaborative Learning 2005 (pp. 331–340). Mahwah: Lawrence
Erlbaum Associates.

Computer-Supported Collaborative Learning

305

Kumar, R., Rosé, C. P., Wang, Y. C., Joshi, M., & Robinson, A. (2007). Tutorial dialogue as adaptive
collaborative learning support. In R. Luckin, K. R. Koedinger, & Greer J. (Eds.), Proceedings of
Artificial Intelligence in Education (pp. 383–390). IOS Press.
Kumar, R., Ai, H., Beuth, J. L., & Rosé, C. P. (2010). Socially-capable conversational tutors can be effective
in collaborative-learning situations. Pittsburgh: Intl. Conf. on Intelligent Tutoring Systems.
Lazonder, A. W., Wilhelm, P., & Ootes, S. A. W. (2003). Using sentence openers to foster student interaction
in computer-mediated learning environments. Computers and Education, 41, 291–308.
Leelawong, K., & Biswas, G. (2008). Designing learning by teaching agents: The Bettys Brain System.
International Journal of Artificial Intelligence in Education, 18(3), 181208.
Lou, Y., Abrami, P. C., & d’Apollonia, S. (2001). Small group and individual learning with technology: A
meta-analysis. Review of Educational Research, 71(3), 449–521.
McNamara, D. S., O’Reilly, T., Rowe, M., Boonthum, C., & Levinstein, I. B. (2007). iSTART: A webbased tutor that teaches self-explanation and metacognitive reading strategies. In D. S. McNamara
(Ed.), Reading Comprehension Strategies: Theories, Interventions, and Technologies (pp. 397–421).
Mahwah: Erlbaum.
Michaels, S., O’Connor, C., & Resnick, L. B. (2008). Deliberative discourse idealized and realized: Accountable
talk in the classroom and in civic life. Studies in the Philosophy of Education, 27(4), 283–297.
Nicol, D. J., & Macfarlane-Dick, D. (2006). Formative assessment and self-regulated learning: A model and
seven principles of good feedback practice. Studies in Higher Education, 31(2), 199–218.
Palincsar, A. S., & Brown, A. L. (1984). Reciprocal teaching of comprehension-fostering and
comprehension-monitoring activities. Cognition and Instruction, 2, 117–175.
Ploetzner, R., Dillenbourg, P., Preier, M., & Traum, D. (1999). Learning by explaining to oneself and to
others. In P. Dillenbourg (Ed.), Collaborative Learning: Cognitive and Computational Approaches (pp.
103 – 121). Elsevier Science Publishers.
Prichard, J. S., Stratford, R. J., & Bizo, L. A. (2006). Team-skills training enhances collaborative learning.
Learning and Instruction, 16(3), 256–265.
Ritter, S., Blessing, S. B., & Hadley, W. S. (2002). SBIR Phase I Final Report 2002. Department of
Education. Department of Education RFP ED: 84-305 S.
Robinson, D. R., Schofield, J. W., & Steers-Wentzell, K. L. (2005). Peer and cross-age tutoring in
math: Outcomes and their design implications. Educational Psychology, 17, 327–361.
Roscoe, R. D., & Chi, M. (2007). Understanding tutor learning: Knowledge-building and knowledge-telling
in peer tutors’ explanations and questions. Review of Educational Research., 77(4), 534–574.
Rosé, C. P., & Torrey, C. (2005). Interactivity versus expectation: Eliciting learning oriented behavior with
tutorial dialogue systems. In Proceedings of Interact, Springer Press.
Rosé, C., Wang, Y.-C., Cui, Y., Arguello, J., Stegmann, K., Weinberger, A., et al. (2008). Analyzing
collaborative learning processes automatically: Exploiting the advances of computational linguistics in
computer-supported collaborative learning. International journal of computer-supported collaborative
learning, 3(3), 237–271. doi:10.1007/s11412-007-9034-0.
Rummel, N., & Weinberger, A. (2008). New challenges in CSCL: Towards adaptive script support. In G.
Kanselaar, Jonker, V., Kirschner, P.A., & Prins, F. (Eds.), Proceedings of the Eighth International
Conference of the Learning Sciences (ICLS 2008), Vol 3 (pp. 338–345). International Society of the
Learning Sciences
Saab, N., Van Joolingen, W. R., & Van Hout-Wolters, B. (2007). Supporting communication in a
collaborative discovery learning environment: The effect of instruction. Instructional Science, 35,
73–98.
Schoenfeld, A. H. (1992). Learning to think mathematically: Problem-solving, metacognition, and sense
making in mathematics. In D. Grouws (Ed.), Handbook for research on mathematics teaching and
learning (pp. 334–370). New York: Macmillan.
Soller, A., Jermann, P., Mühlenbrock, M., & Martinez, A. (2005). From mirroring to guiding: A review of
state of the art technology for supporting collaborative learning. International Journal of Artificial
Intelligence in Education, 15(4), 261–290.
Stahl, G. (2000). A model of collaborative knowledge building. In B. Fishman & S. O'Connor-Divelbiss
(Eds.), Fourth international conference of the learning sciences (pp. 70–77). Mahwah: Erlbaum.
Stahl, G. (2009). Social practices of group cognition in virtual math teams. In S. Ludvigsen, Lund, A., &
Säljö, R. (Eds.), Learning in social practices. ICT and new artifacts: Transformation of social and
cultural practices. Pergamon.
Van den Bossche, P., Gijselaers, W., Segers, M., & Kirschner, P. (2006). Social and cognitive factors driving
teamwork in collaborative learning environments. Small Group Research, 37, 490–521.
VanLehn, K., Siler, S., Murray, C., Yamauchi, T., & Baggett, W. (2003). Why do only some events cause
learning during human tutoring? Cognition and Instruction, 21(3), 209–249.

306

E. Walker, et al.

Vassileva, J., McCalla, G., & Greer, J. (2003). Multi-agent multi-user modeling in I-Help. User modeling and
user-adapted interaction: The Journal of Personalization Research, 13, 179–210. doi:10.1023/
A:1024072706526.
Walker, E., Rummel, N., & Koedinger, K. R. (2009). Integrating collaboration and intelligent tutoring data in
the evaluation of a reciprocal peer tutoring environment. Research and Practice in Technology Enhanced
Learning, 4(3), 221–251.
Webb, N. M. (1989). Peer interaction and learning in small groups. International Journal of Educational
Research, 13, 21–40.
Webb, N. M., & Mastergeorge, A. (2003). Promoting effective helping behavior in peer directed groups.
International Journal of Educational Research, 39, 73–97.
Weinberger, A., Ertl, B., Fischer, F., & Mandl, H. (2005). Epistemic and social scripts in computer-supported
collaborative learning. Instructional Science, 33(1), 1–30.

ICLS 2010 • Volume 2

interact. The tutor's different tutoring dynamics with different tutees highlight how tutoring
mechanisms can differ across working pairs. Future research can illuminate how tutor differences
influence tutoring dynamics. Third, the statistical discourse analysis systematically shows how recent
actions affect subsequent actions at the micro-level and suggests future applications at the macro-level
to address temporal issues. This study suggests that online moderators and online students would
likewise mutually influence one another, and that online artifacts can also play a substantial role in
directing attention and influencing participation. Specifically, online moderators can benefit from
access to online students' computer actions and video displays of their non-verbal behaviors to adapt to
their needs accordingly.

Figure 1. Model predicting 4 types of tutee participation behaviors. All arrows indicate significant
effects Solid arrows indicate positive effects and dashed lines indicate negative effects. Thick arrows
indicate larger effects.

Figure 2. Model predicting 7 types of tutor behaviors. All arrows indicate significant effects Solid
arrows indicate positive effects, dashed lines, negative effects. Thick arrows indicate larger effects.

Automated Adaptive Support for Peer Tutoring in High-School Mathematics
Erin Walker, Nikol Rummel and Kenneth R. Koedinger
Reciprocal peer tutoring is a type of small-group work where two students of similar abilities take turns
tutoring each other. It has been shown to improve domain learning of students involved (Fantuzzo,

151 • © ISLS

ICLS 2010 • Volume 2

Riggio, Connelly & Dimeff, 1989), most likely because students who tutor other students benefit from
the reflective and elaborative processes involved in observing problem-solving steps and providing
explanations (Roscoe & Chi, 2007). However, for students to benefit from the tutee role they must
receive help that is conceptual (Fuchs et al., 1997) and that gives the tutee correct information about the
domain (Webb, 1989). Many previous efforts at assisting peer tutoring have focused on structuring the
tutoring process. For example, King, Staffieri, and Adelgais (1998) attempt to increase the conceptual
content of the interaction by having students ask each other a series of questions at different levels of
depth, and Fantuzzo et al. (1989) support the correctness of the interaction by having students compare
tutee problem-solving steps to problem solutions. While these approaches have been successful,
adaptive support for the peer tutor may be an improvement over fixed support in two ways. First, it
would be able to provide individually tailored interaction support, for example by detecting when peer
tutor help was not conceptual enough, and giving relevant feedback. Second, it would be able to
provide context-sensitive domain support, adaptively alerting peer tutors to tutee errors rather than
simply providing a resource for peer tutors to consult.
We have developed adaptive support for peer tutoring by augmenting the Cognitive Tutor
Algebra (Koedinger, Anderson, Hadley, & Mark, 1997), a successful intelligent tutoring system for
individual learning, with peer tutoring activities. In our extended system, students work on literal
equation solving problems, where they are given a prompt like, “Solve for x” and an equation like “ax
+ by = c”. Each student gets the opportunity to tutor a partner by observing the tutee solve problems,
marking problem steps, and giving help in a chat window. We provided peer tutors with two types of
adaptive support: one which assisted the peer tutor in giving correct help (domain component), and one
which assisted the peer tutor in giving conceptual help (interaction component). For the domain
component, we adapted the hints and feedback already present in the Cognitive Tutor Algebra for a
collaborative context. As tutees took steps in the problem, peer tutors were asked to mark the tutees’
steps as correct or incorrect. If peer tutors made an error (e.g., marked a “correct” step “incorrect”), the
system indicated the error by highlighting it in the interface, and then gave peer tutors the domain
feedback students would typically receive if they were solving the problem individually, along with a
prompt to communicate it to the tutee (see Figure 1a). As tutees solved the problem, the peer tutor
could request a domain hint from the computer tutor at any time. While this domain help provided peer
tutors with information on which problem-solving steps were correct and why, it did not provide
explicit guidance or feedback on how students should help their partner, and thus we added an
additional interaction component to the assistance. Prior to composing a chat message, students were
asked to select a sentence classifier labeling their message as a prompt (“Ask Why”), error feedback
(“Explain Why Wrong”), hint (“Give Hint”), or explanation (“Explain What Next”). Upon submitting
the help, an intelligent tutor for collaboration used a combination of the self-classification, an
automated assessment of the help quality, and the domain context (whether tutees had just made an
error or not) to make an estimate of the level of student help-giving skills. Based on this assessment,
the computer gave context-sensitive reflective prompts in the chat window that were seen by both the
peer tutor and the tutee (see Figure 1b). We expected that the adaptive support would lead peer tutors to
give more correct help by alerting them to the domain errors that they made, and lead peer tutors to
give more conceptual help by alerting them when more conceptual help would be necessary.
We conducted a classroom study with 77 participants comparing two conditions: adaptive
assistance (40 participants) and fixed assistance (37 participants). In the fixed condition, students had
access to problem solutions and tips for good collaboration. The support contained the same content as
the assistance in the adaptive condition but did not vary based on student actions. We found that
adaptive support did indeed have a positive effect on student help. When peer tutors made marking
errors, they corrected their error significantly more often in the adaptive than in the fixed condition
(adaptive M = 65.8%, SD = 26.6%; fixed M = 7.5%, SD = 14.6%; F(1,69 = 127.6), p < 0.001). Further,
students gave significantly more conceptual help in the adaptive than in the fixed condition (adaptive M
= 2.58, SD = 2.75; fixed M = 1.38, SD = 2.14; Mann-Whitney U = 525.5; p = 0.05). Overall, it
appeared that adaptive support was more effective than fixed support at improving student interaction.
We further examined why the adaptive support may have had a positive effect on student
interaction using qualitative observations. Interestingly, when students received reflective prompts they
would rarely acknowledge them, and often would not appear to incorporate the advice into their next
utterance. This behavior was in stark contrast to their use of domain hints given by the computer, which
were often immediately repeated to tutees in a form stripped of conceptual content (e.g., tutors might
receive the hint “subtract x to get it to the other side”, and simply say “subtract x”). This pattern of
behaviors suggested either that students perceived domain help as more integral to the task than
interaction help, or that they perceived it as easier to implement than interaction help. Further, it
seemed that students felt free to ignore advice from a computer in a way that they would not from a
human being, potentially because our computer agent was not as responsive to specific student

152 • © ISLS

ICLS 2010 • Volume 2

utterances as a human would be. Given these observations, it is interesting that student behavior
improved in the adaptive condition compared to the fixed condition. One factor that qualitatively
appeared to mediate this process was student feelings of accountability for their partner’s learning. It is
possible that even the limited responsiveness of the computer to the peer tutor behavior, in combination
with the reflective prompts being posted publicly (i.e., in view of the peer tutee), triggered feelings of
social responsibility which led peer tutors to give help more thoughtfully. Accountability also played a
role in how peer tutors attributed the help they give tutees. Peer tutors would occasionally frame their
help as coming from the computer (e.g., “it wants you to subtract x”), placing peer tutors and tutees in
the position of interpreting the help together. Allowing peer tutors to take on a novice role compared to
the computer may be a secondary advantage of the assistance provided. In future work, we hope to
explore why adaptive support has a beneficial effect on student interaction, and what features of
adaptive support augment this effect.

Figure 1. The adaptive domain support and adaptive interaction support given to collaborating students.

Human guidance of synchronous discussions: A nascent school practice
Baruch Schwarz and Christa Asterhan

Although small group methods have been shown to have positive effects on student achievement
(e.g., O'Donnell, 2006; Slavin, 1995), simply placing students in small groups does not guarantee
learning gains which depend on the quality and depth of discussions, such as the extent to which
students give/receive help, share knowledge, build on each others' ideas and justify their own, and the
extent to which students recognize and resolve contradictions between their own and others'
perspectives (Asterhan & Schwarz, 2009; Webb & Palincsar, 1996). Teachers should then help to avoid
detrimental practices and to facilitate beneficial ones. However, little is known about how the teacher
can foster small group learning. Influencing student interaction through teacher's discourse is
particularly underrepresented in research (Webb, 2009). Several studies (e.g., Chiu, 2004; Webb, 2009)
have found that beyond the question of what type of teacher prompts are more effective (direct or
indirect, explicit or implicit) , a key element in determining the effectiveness of teacher interventions is
whether the teacher's help is tied to students' ideas.
Due to the complexity of this task (Yackel, 2002), some researchers have preferred to adopt a
phenomenological approach to observe how extraordinary teachers facilitate group learning in specific
contexts (e.g., Hmelo and Barrows, 2006, 2008 in a PBL context; and Zhan, Scardamalia, Reeve and
Messina, 2009, for long-term classroom learning in small groups with Knowledge Forum). These
studies show that small-group facilitation is complex but possible and open new research directions: (a)
How can 'normal' teachers face the challenge of ascertaining student thinking during small group work
to base their interventions? (b) How can ‘regular’ teachers successfully monitor and support several
discussion groups at the same time? Our goal is to show that it is possible to provide a suitable
environment that tackles these challenges in the context of a program for fostering critical thinking
through collective argumentation.
Overall, software tools have limited ability to provide adaptive scaffolding (Puntambekar &
Hübscher, 2005). Collaborative scripts are not adaptive either. In the context of a-synchronous
discussions in on-line group work in post-secondary e-courses, guidance has been studied and referred
to as e-moderation (e.g., Salmon (2004). Human guidance of synchronous group discussions seems to
be more appropriate, but has not been sufficiently studied yet. Studying guidance of synchronous
discussions is then a new adventure but two ideas of e-moderation of e-courses should be retained:

153 • © ISLS

Poster Presentation

CSCW 2017, February 25–March 1, 2017, Portland, OR, USA

Scalable Crowd Ideation Support
through Data Visualization, Mining, and
Structured Workflows
Victor Girotto

Abstract

Arizona State University

As the size of innovation communities increases,
methods of supporting their creativity need to scale as
well. Our research proposes the integration of three
scalable techniques into a crowd ideation system: 1)
data visualization, 2) structured microtask workflows,
and 3) data mining, with the goal of supporting users in
convergent and divergent ideation processes. In
addition, these techniques do not work in isolation, but
instead support each other. Our vision is to create a
system that intelligently supports users’ ideation in a
crowd context while maintaining their agency and
facilitating exploration and decision-making.

Tempe, AZ 85281, USA
victor.girotto@asu.edu
Erin Walker
Arizona State University
Tempe, AZ 85281, USA
erin.a.walker@asu.edu
Winslow Burleson
New York University
New York, NY 10012, USA
wb50@nyu.edu

Author Keywords
Crowdsourcing; ideation; creativity; data visualization;
data mining; microtasks;
Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights
for third-party components of this work must be honored. For all other
uses, contact the Owner/Author.

ACM Classification Keywords
H.5.3. Group and Organization Interfaces: Computersupported cooperative work.

Copyright is held by the owner/author(s).

Introduction & Related Work

CSCW '17 Companion, February 25 - March 01, 2017, Portland, OR, USA
ACM 978-1-4503-4688-7/17/02.
http://dx.doi.org/10.1145/3022198.3026349

Collaboration is occurring at an ever-increasing scale.
As communities are formed and grow around shared
passions, the possibility of generating innovation

183

Poster Presentation

CSCW 2017, February 25–March 1, 2017, Portland, OR, USA

through them also increases. For example, crowds are
used as a source of ideas ranging from T-shirts
(https://www.threadless.com) to tough innovation
challenges (https://www.innocentive.com). While in
individual or smaller groups a commonly used
technique for generating such ideas is brainstorming, it
may not necessarily scale well to these large
communities, as the sheer number of ideas generated
can overload those who are generating or selecting
promising ideas.

Figure 1: Matrix visualization of
the solution space. Each column
and row in the matrix represents
a different idea category. The
darker the color, the more ideas
have been developed within a
category. Below, a bar chart
compares the performance of the
user (green) vs. the average
performance of the crowd
(yellow) in a given category.

Currently, however, there are well-known techniques
for dealing with large datasets: 1) data mining, which
allows for automatic identification of patterns in the
data; 2) data visualization, which facilitates visual
exploration of the data; and 3) microtask workflows,
which breaks down bigger tasks into smaller, more
manageable chunks. All three approaches bring forth
complementary strengths in dealing with large
datasets. Therefore, we seek to integrate them
naturally into a system that supports the task of idea
generation and selection.
There have been several attempts at supporting group
or crowd ideation, such as employing human facilitators
[3] or using carefully structured processes [10]. Closer
to our approach, the IdeaHound system integrated
classification tasks into the interface affordance of
spatially organizing ideas into clusters [9]. This allowed
the system to calculate similarity metrics between the
ideas, enabling three forms of support interventions:
supplying the user with diverse examples, similar
examples, and a visualization of the solution space.
Our approach differs from most of the aforementioned
research in that it seeks to employ the same group of

users for both idea and inspiration generation. In this
sense, we are in agreement with the goals of the
IdeaHound system of being near real-time and selfsustainable [9]. We also similarly seek a hybrid humancomputer approach for scalable support. However, this
approach differs from theirs in that it makes microtasks
explicit rather than implicit, contextualizing them as a
form of inspiration and contribution to the ideation
process. There is also a greater focus on supporting
exploration as well as cognitive and social processes
through focused coordinated visualizations rather than
accurate model generation. Finally, we hope to use the
techniques described here not only for ideation, but
also for iteration and selection of ideas.
This research will contribute to the advancement of
crowd ideation systems by investigating the effects of
data visualization and microtasks in crowd idea
generation, improvement, and selection, as well as
modeling and adaptive support of crowd ideators.

Approach
Data visualization
The most visible form of support to the user will be a
set of coordinated visualizations designed for
highlighting important aspects of the ideation process
according to best practices established by creativity
research. One inspiration for this is research on
supporting serendipity through visualization (e.g. [1]).
The first proposed visualization is one of the solution
space (Figure 1). Such a visualization could answer
questions such as: how many and which idea
categories have been developed so far? Which
categories have received more attention than others?
Are there ideas that overlap two categories? This

184

Poster Presentation

CSCW 2017, February 25–March 1, 2017, Portland, OR, USA

should help users in different ways. Seeing categories
of ideas different than those explored by him/herself, a
user can generate more ideas than without that support
[7]. Seeing how much attention some categories have
received can help users direct their efforts to areas that
have not yet been fully explored, perhaps decreasing
redundant ideas. Finally, making overlaps between
categories explicit can direct users to the process of
idea combination (e.g. [10]).
One could also elicit social comparison processes
through a visualization, answering user questions such
as: how is my performance in comparison to the rest of
the group? Which areas have I contributed the most to?
Am I one of the strongest contributors to a particular
category of ideas? This way, one could avoid issues
such as social loafing, while promoting a healthy
upwards comparison [8]. This information could be
conveyed very simply through bar charts comparing the
user’s and average crowd’s performance.
Microtasks
A common way of inspiring ideators is to show them
ideas generated by other people. We propose that
instead of simply showing other ideas, we also embed a
microtask with them, such as rating their originality.
This could have the effect of improving attention to the
idea, which could increase the likelihood of it actually
inspiring the user [7]. In initial studies, we have found
some evidence supporting this hypothesis. By
contextualizing tasks as a form of inspiration, users
may be motivated to do them.
At the same time that users may be inspired by the
ideas they see, they are also contributing with more
information on each idea. In the previous example, this

extra information would be an originality rating for the
idea, which could help when the crowd starts the
process of converging into the best ideas. If users
performed similarity comparisons, the end result could
be a semantic model of the ideas, as done in the
IdeaHound system [9].
Finally, microtasks can also help improve the
underlying models used in both visualization and data
mining (as explained below). For example, the first
visualization suggested a taxonomy of ideas. While
there are ways of generating this taxonomy
automatically, they will not be perfect. Microtasks
inspired by previous taxonomy-creation workflows (e.g.
[4]) could improve these models.
Data Mining
Underlying the previous two items is the notion of
automatic categorization of ideas as the basis of the
visualization and of idea suggestions. Techniques such
as LDA can be used to generate topics based on a text
corpus. While the result has some noise, there are
ways of employing visualizations similar to the one
proposed here to aid users in understanding the models
[5]. Microtasks could then be used to clean them up.
Additionally, user models could also be constructed,
measuring statistics such as likelihood of user being
stuck, fixated, or in other important mental states,
allowing the system to intervene accordingly. We plan
on leveraging advancements in Intelligent Tutoring
Systems (ITS) [6] to model support for ideators.
While microtasks could improve attention to ideas, it is
important that the ideas they present are helpful for
users. If they are far from their knowledge or interests,
for example, they may not be able to benefit from

185

Poster Presentation

CSCW 2017, February 25–March 1, 2017, Portland, OR, USA

them. Research on recommender systems could help to
alleviate this issue. For example, a collaborative
filtering model [2] could be built based on the
categories users contributed to, thus allowing new
categories of ideas to be suggested to users who share
similar patterns.

Conclusion

Figure 2: The different
components described in this
paper and their interactions. The
user interacts directly with the
visualization (for exploration) and
the microtasks (which inspire the
user, while also contributing to
fine-tuning the data mining
component. Meanwhile, the data
mining component is modelling
the user based on his or her
performance, and is improved
based on the microtasks. It also
informs the visualization (e.g.
providing it with the idea
categories) and the microtask
selection (which ideas or tasks
should be shown to a particular
user at a given moment?)

Modern techniques to handle large scale data seem ripe
to intersect with known creativity enhancing practices
in crowd contexts. While no single technique seems to
solve all of the issues, a synergy between techniques
seems promising. Figure 2 describes the interactions
between these different components. This works aims
to build on current research to further understand how
they can be integrated in order to fully develop the
crowd innovation potential.

Acknowledgments
This research was funded by the CAPES Foundation,
Ministry of Education of Brazil, Brasília - DF 70040-020,
Brazil.

References
1. Eric Alexander, Joe Kohlmann, Robin Valenza,
Michael Witmore, and Michael Gleicher. 2014.
Serendip: Topic model-driven visual exploration of
text corpora. In Visual Analytics Science and
Technology (VAST), 2014 IEEE Conference on, 173–
182.
2. J. Bobadilla, F. Ortega, A. Hernando, and A.
Gutiérrez. 2013. Recommender systems survey.
Knowledge-Based Systems 46: 109–132.
3. Joel Chan, Steven Dang, and Steven P Dow. 2016.
Improving Crowd Innovation with Expert Facilitation.
1221–1233.

4. Lydia B. Chilton, Greg Little, Darren Edge, Daniel S.
Weld, and James A. Landay. 2013. Cascade:
Crowdsourcing taxonomy creation. In Proceedings of
the SIGCHI Conference on Human Factors in
Computing Systems, 1999–2008.
5. Jason Chuang, Christopher D. Manning, and Jeffrey
Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of
the International Working Conference on Advanced
Visual Interfaces, 74–77.
6. Michel C. Desmarais and Ryan S. J. d. Baker. 2012.
A review of recent advances in learner and skill
modeling in intelligent learning environments. User
Modeling and User-Adapted Interaction 22, 1–2: 9–
38.
7. Bernard A. Nijstad, Wolfgang Stroebe, and Hein FM
Lodewijkx. 2002. Cognitive stimulation and
interference in groups: Exposure effects in an idea
generation task. Journal of experimental social
psychology 38, 6: 535–544.
8. Paul B. Paulus and Vincent R. Brown. 2003.
Enhancing ideational creativity in groups: Lessons
from research on brainstorming. In Group creativity:
Innovation through collaboration. Oxford University
Press.
9. Pao Siangliulue, Joel Chan, Steven P. Dow, and
Krzysztof Z. Gajos. 2016. IdeaHound: Improving
Large-scale Collaborative Ideation with CrowdPowered Real-time Semantic Modeling. 609–624.
10.Lixiu Yu and Jeffrey V. Nickerson. 2011. Cooks or
cobblers?: crowd creativity through combination. In
Proceedings of the SIGCHI conference on human
factors in computing systems, 1393–1402.

186

NATURALNESS AND RAPPORT IN A PITCH ADAPTIVE LEARNING COMPANION
Nichola Lubold 1, Heather Pon-Barry 2, Erin Walker 1
1

2

CIDSE, Arizona State University, Tempe, AZ, USA
Department of Computer Science, Mount Holyoke College, South Hadley, MA, USA
One application where a socially responsive dialogue system
is potentially very impactful is the learning companion [4]. A
learning companion provides a support system for students
with the goal of improving learning by providing both taskrelated feedback and motivational support. Learning
companions, based on the theory that learning is influenced
by social interactions [32], require social sensitivity to
influence students’ socio-motivational factors and increase
student learning. In the case of learning companions, the
ability to be socially responsive positively impacts learning
(e.g., [14]).

ABSTRACT
Observed frequently in human-human interactions,
entrainment is a social phenomenon in which speakers
become more like each other over the course of a
conversation. Acoustic-prosodic entrainment occurs when
individuals adapt their acoustic-prosodic speech features,
such as pitch and intensity. Correlated with communicative
success, naturalness, and conversational flow as well as social
variables such as rapport, a dialogue system which
automatically entrains has the potential to improve verbal
interactions by increasing rapport, naturalness, and
conversational flow. In an application like the learning
companion, such a socially responsive dialogue system may
improve learning and motivation. However, it is not clear
how to produce entrainment in an automatic dialogue system
in ways that produce the effects seen in human-human
dialogue. In this paper, we take the first steps towards
implementing a spoken dialogue system which can entrain.
We propose three methods of pitch adaptation based on
analysis of human entrainment, and design and implement a
system which can manipulate the pitch of text-to-speech
output adaptively. We find a clear relationship between
perceptions of rapport and different forms of pitch
adaptations. Certain adaptations are perceived as
significantly more natural and rapport-like. Ultimately,
adapting by shifting the pitch contour of the text-to-speech
output by the mean pitch of the user results in the highest
reported measures of rapport and naturalness.

In this paper, we are interested in utilizing acoustic-prosodic
features of speech to improve the social responsiveness of a
learning companion’s dialogue system. We explore the
phenomenon acoustic-prosodic entrainment as a possible
mechanism. Acoustic-prosodic entrainment is where two
speakers adapt their acoustic-prosodic features including
their tone, intensity, and speaking rate to mirror one another
[17]. Correlated with a number of factors including
communicative success, conversational flow, and social
factors like rapport, acoustic-prosodic entrainment is critical
to the naturalness and flow of dialogue [1, 19, 23]. A dialogue
system which automatically entrains has the potential to
improve verbal interactions by increasing the above factors.
In a learning companion, such a dialogue system may
improve learning and motivation as well.
To this end, we design and implement a system which adapts
the pitch of a text-to-speech (TTS) output real-time, to
accommodate to the pitch of the user. We implement three
different forms of pitch adaptation, inspired by how human
conversational partners entrain. We collect data from four
individuals interacting with each form of pitch adaptation,
and then, using crowd-sourced analysis via Amazon
Mechanical Turk, we compare the different adaptations to the
baseline TTS on rapport and naturalness perceived by thirdparty observers. We find the most effective adaptation is on
pitch mean where the standard TTS pitch contour is
maintained but the contour is adapted to the average pitch of
the user. These findings provide insight into how pitch
adaptations are perceived overall and how to proceed with
creating effective entrainment in the dialogue system of a
learning companion.

Index Terms— pitch, adaptation, dialogue system,
naturalness, rapport
1. INTRODUCTION
Spoken dialogue systems are a part of mainstream society,
from automated answering systems to the advent of voiced
personal assistants such as Siri, Google Now, and more
recently, Cortana. With advances made in automatic speech
recognition (ASR), we’ve seen the ability of these
technologies to hold a conversation improve considerably. As
dialogue systems become more pervasive, there is an
increasingly important role for socially responsive dialogue
systems that can effectively socially engage the user.

978-1-4799-7291-3/15/$31.00 ©2015 IEEE

103

ASRU 2015

In the below sections, we motivate this work in relation to
entrainment, rapport, and learning, and describe related work
on manipulating acoustic prosodic features. In Section 2, we
describe in more detail the pitch adaptations performed and
our hypotheses. Section 3 outlines the intended application
and the dialogue interface we built to implement the pitch
adaptations. Section 4 outlines the process and results of
evaluating the naturalness and rapport of these adaptations.
We discuss the implications of these results in Section 5,
concluding with a brief discussion and thoughts on future
work in Section 6.

that individuals interacting with a virtual agent which
entrained on intensity and speaking rate unconsciously
trusted that agent more than an agent which did not entrain
on these features. This provides support that entrainment
triggers social responses in line with traditional humancomputer interaction theory, which suggests that humans
respond socially to computers in similar ways as they respond
to other humans [22].
For this work, we take a similar approach to Levitan in
adapting acoustic-prosodic features on a turn-by-turn basis
but our focus is on pitch. While pitch has been looked at for
improving the naturalness of text-to-speech [6, 31, 33], it has
received less attention as feature for automated entrainment.
Given the history of entrainment on pitch, we hope to confirm
that humans respond to an entraining computer in the same
way they respond to entraining humans by finding that social
variables correlated with human-human entrainment on pitch
can be affected by computer adaptation. As the best way to
entrain on pitch is not yet clear, we evaluate the success of
different pitch adaptations in producing meaningful
entrainment.

1.1 Entrainment, Rapport, & Learning
Entrainment, known also as accommodation, occurs when
dialogue partners adapt their behavior to each other during an
interaction. Entrainment can be gestural, via gaze or facial
expressions [15], word-based or lexical [8], or speech-based
[26]. In this paper, we are interested specifically in acousticprosodic entrainment, when two speakers adapt their
acoustic-prosodic speech features to one another, and we
focus on one acoustic-prosodic feature – pitch.
Entrainment
on
pitch
prominently
differentiates
communicative success [1] and entrainment measures
derived from pitch features are significantly higher in positive
interactions [16]. In addition, entrainment on pitch is linked
to both rapport and learning [30, 20]. In theories of learning,
it has been proposed that all learning is social [32] and
feelings of rapport have been shown to impact how much
students learn from interactions with learning companions
[25]. By entraining on pitch, a learning companion may
increase rapport, and the increased rapport may improve
learning gains. While other forms of automated entrainment
may also benefit learners, our focus here is on assessing an
optimal strategy for pitch adaptation.

2. PITCH ADAPTATION METHODOLOGY
Three forms of pitch adaptation are proposed, inspired by
observations of how human conversation partners entrain.
Prosodic entrainment is often measured along multiple
dimensions. We focus on proximity. Proximity measures how
near the acoustic-prosodic features of two speakers are, on a
turn-by-turn basis. It is the most frequent form of entrainment
observed in turn-by-turn analyses, compared to other acoustic
features and types of entrainment. Proximal entrainment on
pitch specifically has been linked to greater rapport,
communicative success, and positivity [19, 1, 15] so the
adaptations we explore here focus on pitch.
The three proposed methods of pitch adaptation operate at the
turn-level. The system adapts its pitch based on the estimated
pitch values from the previous speaker’s turn, as opposed to
the longer dialogue history. Figure 1 illustrates the pitch
contour of a sample waveform alongside three adaptations.

1.2 Manipulating Acoustic-Prosodic Features
Manipulating the acoustic-prosodic features of the text-tospeech output of a dialogue system to influence entrainment
has precedence in past work. These efforts were focused on
features which are easy to manipulate, such as intensity and
speaking rate [28, 29, 5]. In these scenarios, the acousticprosodic features were adjusted in order to transform the
overall dialogue output without regard to the human speaker.
The results show that humans will entrain to a computer,
adapting their own voice to the computer. The manipulations
did not explore the effect of computer adaptation.

The first method of pitch adaptation is mirror partner. With
mirror partner, we adapt the text to speech output to the entire
pitch contour of the speaker’s previous turn by replacing the
original contour of the TTS with the contour of the speaker.
To account and control for differences in utterance length, we
resize the speaker’s utterance to be the same length as the
proposed text to speech output prior to applying the speaker’s
contour to the output. This approach to adaptation would
maximize the level of entrainment, following the metrics
used in past work [20].

Adapting the acoustic-prosodic features of the output of a
spoken dialogue system to a user is a more recent innovation.
In her thesis on entrainment in human-human and humancomputer dialogue [18], Rivka Levitan appears to be the first
to look at adapting text-to-speech on a turn-by-turn basis
based on the user’s acoustic-prosodic features. Levitan found

Shift+contour is an alternative method of pitch adaptation
that maintains the contour of the original TTS but shifts it up

104

Waveform

control
(no adaptation)

mirror partner

shift+contour

shift+flatten

Fig. 1: Spectrograms and pitch contours of the synthesized waveforms (original + three with pitch adaptation).
or down to match the mean pitch of the speaker. While
mirroring the shape of a partner’s pitch contour might
strengthen automated measures of entrainment, there is the
possibility of “over-adaptation” and of a mismatch between
pitch contour and syntactic and semantic structure. Since
entrainment on pitch mean has been found to be correlated
with learning and rapport, above and beyond other attributes
of pitch, shift+contour only adapts the pitch mean.

the web application is a collection of variable equation
problems (i.e. “Solve 4x +3y = 80 for x”). The application
presents each problem separately and includes steps to reach
a solution. The problems are ordered in increasing order of
difficulty. Quinn and an example of the web interface display
for step one of a sample problem are found in Figure 2.

We introduce a third adaptation called shift+flatten. This
adaptation serves as a minimum manipulation baseline in
respect to the other two approaches. Still adapting on a single
feature, pitch mean, we flatten the pitch contour of the TTS
to the pitch mean of the user. The TTS output maps to the
average pitch of the student. As this adaptation is intuitively
the least realistic, we would not expect it to produce more
rapport than the other two conditions. Thus, it serves as a
baseline comparison for the more sophisticated pitch
adaptations we propose, in addition to the control, the
original synthesized waveform with no adaptation on pitch.

Fig. 2: Quinn and an example of the web interface display for
step one of a sample problem.
Before teaching Quinn, students are given a sample problem
to practice how to teach the problem. They are then
introduced to Quinn and shown how to use the interface. To
teach Quinn the problem, the student clicks on the
microphone displayed on the page which enables real-time
speech. They then proceed to walk Quinn verbally through
the steps displayed on the screen. The speech interaction is
real-time, and the dialogue is recorded as the student speaks.
After explaining each step, the students are instructed to
pause, giving Quinn a chance to respond. A sample of the
dialogue taken from the present study is given below; ‘Q’
represents Quinn and ‘S’ represents the student.

2.2 Hypotheses
We hypothesize the pitch adaptations will result in more
rapport than the basic text-to-speech. Specifically, we
hypothesize mirror partner will produce more rapport than
shift+contour or the basic text-to-speech baseline. The third
adaptation, shift+flatten, will generate the least rapport.
While we are interested in how rapport differs for different
pitch adaptations, we want to ensure that the adaptations are
perceived to be as natural as the original synthesized
waveform. We hypothesize that mirror partner and
shift+contour will not be significantly different from the
baseline text-to-speech. We also hypothesize that
shift+flatten will be significantly less natural.

S:

We will divide both sides by negative six

Q: Can you explain why we divide?
S:

On the left hand side, we have negative 6y. We
need to have it equal just y so we need to get rid
of the negative six. The easiest way is to divide.
Q: Thank you explaining! I get it now. So we divide.
Then what?

3. LEARNING COMPANION APPLICATION
We implement a virtual learning companion to analyze the
effect of the pitch adaptations. Students interact using spoken
language with a virtual entraining agent, referred to as Quinn,
and a web application. Quinn is present throughout the
interaction on a tablet device. Quinn’s facial expressions are
animated when speaking, and neutral otherwise. Underlying

3.1 Dialogue Interface
To explore the effect of pitch adaptations on perceptions of
naturalness and rapport, we build a dialogue interface which
can manipulate the pitch of the text-to-speech output. We

105

Fig. 3: Dialogue system for Quinn. The darker boxes indicate components belonging to the pitch adaptation module.
Table 1: Dialogue and turn statistics for corpus

designed the system in a modular fashion so the pitch
adaptation module can be introduced independently into
other systems. Speech recognition was performed using the
Web Speech API specification1. The dialogue manager was
developed using Artificial Intelligence Markup Language
(AIML) developed by Richard Wallace [34]. AIML is an
XML-compliant pattern matching language. We utilized the
tool PandoraBots2 to develop the AIML. The text-to-speech
output is produced with the Microsoft Speech API. The
feature extraction and pitch adaptations3 are implemented
using Praat [2]. The basic dialogue system design and
technologies utilized are illustrated in Figure 3. The darker
boxes indicate components of the pitch adaptation module.

Dialogue length (min)
Number of turns
Turn length (sec)

Taking the dialogue corpus collected in section 5.1, we
manually select 40 exchanges from each of the student-Quinn
dialogues. An exchange is considered to be two adjacent turns
by different speakers (i.e. the student and Quinn). We select
ten exchanges for the baseline text-to-speech and ten
exchanges for each of the three types of adaptation, focusing
on those exchanges with maximum coherency and minimal
pausing or silence, eliminating any exchanges were speech
recognition may have failed. The ten exchanges are evenly
split between two scenarios. In the first scenario, Quinn is the
first speaker in the exchange. In the second scenario, Quinn
is the second speaker and is responding. With a total of 40
exchanges per student, we utilized Amazon Mechanical Turk
(AMT), a popular resource for crowdsourcing research tasks
including annotations, transcripts, and subjective analysis [3].
We use AMT to obtain 10 random, perceptual evaluations per
exchange for a total of 400 evaluations per student or 1600
evaluations. Using third party ratings such as those collected
through AMT is a standard technique in the evaluation of
naturalness and social features of dialogue systems [13]. In
addition, avoiding first-person ratings allowed us to present
all dialogue approaches to each of the four individuals
without worrying about how their perceptions of one
approach might affect their ratings of a different approach.
Through AMT, individuals, referred to as workers, were
asked to listen to each exchange and answer a series of
questions4 regarding the speakers. Each worker has access to

5.1 Spoken Dialogue Data Collection
We collect 32 dialogues from four individuals. In each study,
an undergraduate college student interacts with Quinn using
the web application to teach Quinn how to solve eight
variable equation math problems. For two problems, Quinn
speaks with a non-transformed baseline speech. For the
remaining six problems, Quinn alternates the type of
adaptation for each problem. Two full problems are given for
each type of adaptation; we collect each problem as a separate
dialogue for a total of 8 dialogues per student. Statistics for
the collected corpus are shown in Table 1. The gender of
Quinn’s voice was chosen to match the gender of the student.
The four case studies are gender balanced with two males and
two females. The gender of the speaker drove the gender of
Quinn’s voice. If the student was a female, then Quinn was
female. If the student was male, Quinn was male.

2

Std. Dev.
2.1
10
4.6

5.2 Amazon Mechanical Turk (AMT) Evaluation

5. NATURALNESS & RAPPORT
EVALUATION

1

Mean
5.4
30
10.8

3

https://dvcs.w3.org/hg/speech-api/raw-file/tip/speechapi.html
www.pandorabots.com

Link to Praat script implementing the adaptations:
http://www.public.asu.edu/~nlubold/Research/pitchadapt.praat
4
http://www.public.asu.edu/~nlubold/Research/sampleHit.html

106

evaluate 160 exchanges (40 per student). To evaluate
naturalness, we use Mean Opinion Score or MOS [12]. With
MOS, workers are asked to evaluate the quality of the voice
on a Likert scale of 1-5, where 1 is very poor and 5 is
completely natural. Workers evaluated both the human
speaker and Quinn on this scale.

Table 2: Means and standard deviations for naturalness on
each pitch adaptation
control
mirror partner
shift+contour
shift+flatten

For evaluating rapport, we adopt a subset of questions from
the rapport scale utilized by [9] and [19]. Workers are asked
the following two questions about the relationship between
the speakers on a Likert scale of 1-5, where 1 is “not at all”
and 5 is “a lot.” In the questions below, Alex refers to the
student and Quinn refers to the virtual agent. We selected
these questions because they target a shared feeling between
speakers. The responses are averaged for one rapport rating.

Mean
2.22
1.79
2.39
1.85

Std. Dev.
1.36
1.11
1.33
1.15

Rapport – To identify differences in how rapport is
perceived for each of the pitch adaptations, we perform a oneway ANOVA with adaptation type as a factor and rapport as
the dependent variable. Table 3 gives the means and standard
deviations. We find statistically significant differences
among the types of adaptations, F(3, 1599) = 5.63, p < 0.001.
Our hypothesis was that mirror partner would result in the
most rapport, followed by shift+contour. Shift+flatten, we
expected to be the lowest. Interestingly, we see shift+contour
is on par with mirror partner. Both are indicating higher,
equivalent degrees of rapport over the control. Using Tukey
post hoc tests to analyze which of the pitch adaptations are
significantly different, we find shift+contour generates
significantly higher perceptions of rapport than shift+flatten
(p < 0.01). Differences between shift+contour, mirror partner,
and control are not significant.

1) Alex and Quinn understood each other
2) There is a sense of closeness between Alex and Quinn
In total, 174 workers provided evaluations of the audio. 12%
or 21 workers rated 30% or more of the possible 160
exchanges they had access to while 40% of the workers
listened to and rated only one exchange. In analyzing the
results below, we treat each rating as the unit of analysis.
5.3 Results
To analyze the effect of the pitch adaptations in terms of
rapport and naturalness and evaluate our hypotheses, we run
a basic statistical analysis of the relationship between type of
adaptation, naturalness, and rapport. A subsequent in-depth
analysis of the individual participants and differences in
ratings reveals a connection between social content of
exchanges, adaptation type, and degree of rapport perceived.

Table 3: Means and standard deviations for rapport on each
pitch adaptation

5.3.1. Naturalness & Rapport

Given that rapport may have been influenced by several
aspects of the human-agent dialogue beyond pitch adaptation,
we pursue in the next section a more in depth analysis of the
exchanges, to identify whether there are any further
conclusions we can draw regarding the social factors
introduced by the pitch adaptations.

control
mirror partner
shift+contour
shift+flatten

Naturalness – We perform a one-way analysis of variance
(ANOVA) with the type of adaptation (mirror partner,
shift+contour, shift+flatten, and control) as a factor and
naturalness as the dependent variable. Table 2 gives the
means and standard deviations for each condition. The
ANOVA analysis indicates statistically significant
differences among type of adaptations, F(3, 1599) = 19.9, p <
0.001. Tukey post hoc tests indicate that shift+contour was
perceived as significantly more natural than either mirror
partner (p < 0.001) or shift-flatten (p < 0.001). We expected
to find mirror partner and shift+contour were as natural as the
control. We find mirror partner is perceived to be much less
natural, on par with shift+flatten. We also find shift+contour
is not significantly different from the control, where no
adaptation is performed (p = 0.52). These results lead us to
conclude that in pursing implementing an automatically
entraining system, shift+contour, adapting pitch by shifting
the TTS contour, is the most natural of the adaptations
reviewed and is as natural as a non-manipulated TTS output.

Mean
3.56
3.73
3.74
3.35

Std. Dev.
1.17
1.07
1.07
1.15

5.3.2. Identifying Moderating Factors
We examine the average ratings for each student who
participated, as shown in Table 4. We find that for 3 of the 4
students, the raters perceived more rapport in the exchanges
where Quinn adapted by the shift+contour adaptation than in
any other condition. Listening to these recordings, we
identify an imbalance in terms of content spoken. In most
scenarios, Quinn and the student engaged in conversation like
the transcript in Section 5.1. In other cases, Quinn would
introduce an off-topic statement. For example,
Q: This is not very fun, are we almost done?
S: Math can be fun! But yeah…we're almost done

107

Student 1
Student 2
Student 3
Student 4

gender
F
F
M
M

control
3.61
3.58
3.64
3.79

Average Rapport
mirror
s-contour
3.68
3.74
3.70
3.78
3.65
3.80
3.36
3.29

s-flatten
3.38
3.59
3.55
3.25

control
2.14
2.36
2.28
2.50

Average Naturalness
mirror
s-contour
1.69
2.41
1.94
2.39
1.74
2.27
1.87
2.05

s-flatten
1.74
1.93
1.83
1.68

Table 4: Descriptive statistics for each student; bold values indicate the highest rapport/naturalness score for that student
There is support in past work that off-task social conversation
increases rapport, particularly in educational dialogues [25].
Given the possibility the raters are considering the content of
exchanges in their evaluations of rapport, we annotate the
exchanges as either social (off-topic and not about the
problem), or not social (on-topic and about the problem). In
addition, we consider that we designed Quinn to entrain to the
previous turn made by the student. In the exchanges rated, we
counter balanced between exchanges where the rater would
hear Quinn speak first and scenarios where the rater would
hear the student speak first. In the latter, the turn to which
Quinn is adapting is audible. We suspect the raters are
perceiving more differences in the rapport produced when
they can hear the speech to which Quinn is adapting.

6. DISCUSSION & CONCLUSION
In reviewing the results above, we find that in terms of
rapport produced, differences between the pitch adaptations
become the most notable when we incorporate social/nonsocial annotations. We find that in social exchanges, shiftcontour produces significantly more rapport than the other
adaptations and the control. This suggests that in future work,
we should consider an adaptation such as shift-contour and
that it may be prudent to pick and choose when an agent
entrains. This is supported by prior work on rapport and
entrainment which shows that off-task social conversation
increases rapport and that humans entrain more in off-task,
social dialogues [21].

5.3.3 Statistical Analysis Incorporating Speaking Order and
Social Context

While we do find support for our hypothesis that shift+flatten
produces the least rapport, our hypothesis regarding mirror
partner producing more rapport than shift+contour or the
control is not supported by the results. Listening to the
exchanges, this result is mostly likely due to our original
concern that mirror partner results in mismatches between
pitch contour and syntactic and semantic structure. This is
supported by the finding that mirror partner is significantly
less natural. Considering mirror partner did receive very low
naturalness scores, the rapport perceived for this adaptation
is relatively high. This suggests that overcoming the issues
with syntactic and semantic structure with a more nuanced
adaptation accounting for contextual dependencies is
necessary if we wish to explore mirror partner in the future.

To explore the effect of the social exchanges versus nonsocial exchanges as well as the order in which Quinn speaks,
we run a 3-way ANOVA with rapport as the independent
variable, including the type of adaptation, whether Quinn
speaks first or second, and the social/not-social annotations
as factors. The ANOVA analysis indicates statistically
significant interactions between all combinations of factors
except for the highest order interaction (all 3 factors). Fscores and p-values are shown in Table 5.
Table 5: 3-way ANOVA with rapport as dependent variable
Factor
Type of Adaptation
Social/Non-Social Exchange
Quinn Speaks First/Second
Adaptation x Social Exchange
Adaptation x Quinn Speaking
Social Exchange x Quinn Speaking
3-Way Interaction

F-Score
6.8
1.3
3.6
6.1
7.5
12.7
2.0

p
< 0.001
0.25
0.06
< 0.001
< 0.001
< 0.001
0.11

We conclude that adapting to the speaker does appear to be
have an effect on naturalness and rapport and we find that
shifting the contour by pitch mean is one form of adaptation
we can accomplish in a manner which sounds as natural as
current text-to-speech technologies while significantly
increasing perceptions of rapport. Future work includes
extending these findings and running a more extensive
analysis focusing specifically on the effects of adaptation on
pitch mean in regards to learning with a learning companion
and assessing effects on self-reported rapport. We also intend
to explore additional acoustic-prosodic features for
adaptation and additional adaptation models where we can
incorporate more refined techniques based on phoneme
length and pauses as well as other forms of entrainment, such
as convergence, where individuals adapt over the course of a
conversation.

Finding significant 2-way interactions for all combinations of
factors, we run pairwise comparisons for further analysis. In
social exchanges, the type of adaptation results in
significantly different levels of rapport. When Quinn speaks
second, shift+contour has significantly higher rapport than
the control (p = 0.03) and shift+flatten (p < 0.001). The
difference with mirror partner is nearly significant (p = 0.08).
Pitch adaptation in non-social exchanges or when Quinn
speaks first appear to have less of an effect.

108

10. REFERENCES

[14] Kumar, R., Ai, H., Beuth, J. L., & Rosé, C. P. Socially capable
conversational tutors can be effective in collaborative learning
situations. In Intelligent tutoring systems (pp. 156-164).
Springer Berlin Heidelberg, 2010.

[1] Borrie, S. A., & Liss, J. M. Rhythm as a coordinating device:
entrainment with disordered speech. Journal of Speech,
Language, and Hearing Research, 57(3), 815-824, 2014.

[15] Lakin, J. L., Jefferis, V. E., Cheng, C. M., & Chartrand, T. L.
The chameleon effect as social glue: Evidence for the
evolutionary significance of nonconscious mimicry. Journal
of nonverbal behavior, 27(3), 145-162, 2003.

[2] Boersma, Paul & Weenink, David. Praat: doing phonetics by
computer [Computer program]. Version 5.4.12, retrieved 10
July 2015 from http://www.praat.org/
[3] Buhrmester, M., Kwang, T., & Gosling, S. D. Amazon's
Mechanical Turk a new source of inexpensive, yet highquality, data?. Perspectives on psychological science, 6(1), 35, 2011.

[16] Lee, C. C., Black, M., Katsamanis, A., Lammert, A. C.,
Baucom, B. R., Christensen, A. & Narayanan, S. S.
Quantification of prosodic entrainment in affective
spontaneous spoken interactions of married couples. In
INTERSPEECH (pp. 793-796), 2010.

[4] Chou, C. Y., Chan, T. W., & Lin, C. J. Redefining the learning
companion: the past, present, and future of educational agents.
Computers & Education, 40(3), 255-269. 2003.
[5] Coulston, R., Oviatt, S., & Darves, C. "Amplitude
convergence in children’s conversational speech with
animated personas." Proceedings of the 7th International
Conference on Spoken Language Processing. Vol. 4. 2002.

[17] Levitan, R., Gravano, A., Willson, L., Benus, S., Hirschberg,
J., & Nenkova, A. “Acoustic-prosodic entrainment and social
behavior.” In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computational
Linguistics: Human language technologies. Association for
Computational Linguistics, 2012.

[6] Eide, Ellen M., and Robert E. Donovan. "Methods for
generating pitch and duration contours in a text to speech
system." U.S. Patent No. 6,101,470, 2000.

[18] Levitan, R. Acoustic-Prosodic Entrainment in Human-Human
and Human-Computer Dialogue (Doctoral dissertation,
Columbia University), 2014.

[7] Frager, S., & Stern, C. Learning by teaching. The Reading
Teacher, 403-417, 1970.

[19] Lubold, N., & Pon-Barry, H. “Acoustic-Prosodic Entrainment
and Rapport in Collaborative Learning Dialogues”. In
Proceedings of the 2014 ACM workshop on Multimodal
Learning Analytics Workshop and Grand Challenge, ACM, 512, 2014.

[8] Friedberg, H., Litman, D., & Paletz, S. B. Lexical entrainment
and success in student engineering groups. In Spoken Language
Technology Workshop (SLT), 2012 IEEE (pp. 404-409). IEEE,
2012.

[20] Lubold, N., & Pon-Barry, H. “A comparison of acousticprosodic entrainment in face-to-face and remote collaborative
learning dialogues.” In Spoken Language Technology
Workshop (SLT), 2014 IEEE (pp. 288-293). IEEE, 2014.

[9] Gratch, J., Wang, N., Gerten, J., Fast, E., and Duffy, R.
“Creating rapport with virtual agents.” In Intelligent Virtual
Agents, Springer, 125-138, 2007.

[21] Lubold, N., Walker, E., & Pon-Barry, H. “Relating
Entrainment, Grounding, and Topic of Discussion in
Collaborative Learning Dialogues.” In Proceedings of
Computer Supported Collaborative Learning, 2015.

[10] Inden, B., Malisz, Z., Wagner, P., & Wachsmuth, I.
Timing and entrainment of multimodal backchanneling
behavior for an embodied conversational agent. In
Proceedings of the 15th ACM on International
conference on multimodal interaction (pp. 181-188).
ACM, 2013.

[22] Reeves, B., & Nass, C. How people treat computers,
television, and new media like real people and places (p.
119). CSLI Publications and Cambridge university press,
1996.

[11] Iio, T., Shiomi, M., Shinozawa, K., Takaaki, A., Hagita, N., &
Shimohara, K. Entrainment between speech and gestures in
human-robot interaction. In SICE Annual Conference 2010,
Proceedings of (pp. 2769-2774). IEEE, 2010.

[23] Nenkova, A., Gravano, A., & Hirschberg, J. “High frequency
word entrainment in spoken dialogue.” In Proceedings of the
46th Annual Meeting of the Association for Computational
Linguistics on Human Language Technologies: Short Papers.
Association for Computational Linguistics, 2008.

[12] ITU-T Recommendation P.85. Telephone transmission quality
subjective opinion tests. A method for subjective performance
assessment of the quality of speech voice output devices,
1994.

[24] Obayashi, F., Shimoda, H., & Yoshikawa, H. Construction and
evaluation of CAI system based on learning by teaching to
virtual student. In Proceedings of the World Multiconference
on Systemics, Cybernetics and Informatics. Vol. 3, 94-99,
2000.

[13] Jurcıcek, F., Keizer, S., Gašic, M., Mairesse, F., Thomson, B.,
Yu, K., & Young, S. "Real user evaluation of spoken dialogue
systems using Amazon Mechanical Turk." Proceedings of
INTERSPEECH. Vol. 11. 2011.

109

[25] Ogan, A., Finkelstein, S., Mayfield, E., D'Adamo, C.,
Matsuda, N., & Cassell, J. “Oh dear Stacy! Social Interaction,
Elaboration, and Learning with Teachable Agents.” In
Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems (pp. 39-48). ACM, 2012.
[26] Reitter, D., Keller, F., & Moore, J. D. A computational
cognitive model of syntactic priming. Cognitive science,
35(4), 587-637, 2011.
[27] Stylianou, Y. Applying the harmonic plus noise model in
concatenative speech synthesis. Speech and Audio
Processing, IEEE Transactions on, 9(1), 21-29, 2001.
[28] Suzuki, N., Takeuchi, Y., Ishii, K., & Okada, M. Effects of
echoic mimicry using hummed sounds on human-computer
interaction. Speech Communication, 40(4), 559–573, 2003.
[29] Suzuki, N., and Katagiri, Y. "Prosodic alignment in human–
computer interaction." Connection Science 19.2, 131-141,
2007.
[30] Thomason, J., Nguyen, H. V., & Litman, D. “Prosodic
entrainment and tutoring dialogue success.” In Artificial
Intelligence in Education. Springer Berlin Heidelberg, 2013.
[31] Violante, L., Zivic, P. R., & Gravano, A. “Improving speech
synthesis quality by reducing pitch peaks in the source
recordings”. In HLT-NAACL, 502-506, 2013.
[32] Vygotsky, L. S. Mind and society: The development of higher
mental processes, 1978.
[33] Watanabe, Tomio. "Effects of pitch adaptation in prosody on
human-machine verbal communication." Advances in Human
Factors/Ergonomic, 20, 269-274, 1995.
[34] Wallace, R. (2003). The elements of AIML style. Alice AI
Foundation.

110

Crowd-powered Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

The Effect of Peripheral Micro-tasks on Crowd Ideation
Victor Girotto1
Erin Walker1
1
School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, AZ, USA
{victor.girotto, erin.a.walker}@asu.edu
ABSTRACT

2

Winslow Burleson2
Rory Meyers College of Nursing
New York University
New York, NY, USA
wb50@nyu.edu

In our work, we leverage a similar micro-tasks paradigm to
achieve creative solutions in response to complex problems.
Creativity thrives on diversity and exploration. It is about
creating something that is both novel, breaking away from
common knowledge or practices, but at the same time being
appropriate or useful [11]. From designing T-shirts
(www.threadless.com) to solving tough technical challenges
(www.innocentive.com), there are many examples of the
crowd performing tasks that rely on their creativity.

Research has explored different ways of improving crowd
ideation, such as presenting examples or employing
facilitators. While such support is usually generated through
peripheral tasks delegated to crowd workers who are not part
of the ideation, it is possible that the ideators themselves
could benefit from the extra thought involved in doing them.
Therefore, we iterate over an ideation system in which
ideators can perform one of three peripheral tasks (rating
originality and usefulness, similarity, or idea combination)
on demand. In controlled experiments with workers on
Mechanical Turk, we compare the effects of these secondary
tasks to simple idea exposure or no support at all, examining
usage of the inspirations, fluency, breadth, and depth of ideas
generated. We find tasks to be as good or better than
exposure, although this depends on the period of ideation and
the fluency level. We also discuss implications of inspiration
size, homogeneity, and frequency.

Why explore the creativity of the crowds? The first reason is
that a great number of people will generate a great number
of ideas. Furthermore, the heterogeneity of the crowd can
increase the potential of ideas being sparked that otherwise
wouldn’t [8]. However, there are also issues that need to be
carefully considered in a system that tries to tap into the
crowd’s creativity. Issues such as cognitive interference or
social loafing can increase together with the number of
ideators [8]. Therefore, crowd ideation needs to be carefully
designed in order to improve, not hinder the creative output.

Author Keywords

Crowdsourcing; ideation; creativity; microtasks.

A popular method used for generating ideas is typically
brainstorming, which seeks to increase the number of ideas
generated by encouraging intensive exploration of ideas
while restricting criticism [25]. In the crowd context, just like
in smaller groups, people have tried to enhance idea
generation during brainstorming sessions in different ways,
many times employing other individuals or workers, outside
of ideation, to do tasks whose output will benefit ideators.
We call these tasks peripheral tasks. The result of their work
is then presented in some way to crowd ideators. For
example: Yu et al. [31] had workers generate problem
schemas, and subsequently used them to enhance ideation
performance of other workers in a subsequent study.
However, the extra cognitive effort that is required to
perform these tasks could potentially benefit ideators as
much as just using their results.

ACM Classification Keywords

H.5.3. Group and Organization Interfaces: Computersupported cooperative work.
INTRODUCTION

With the advent of crowdsourcing, people can now
collectively accomplish a wide range of tasks that could not
otherwise be done by a single human or computer. One
approach to crowdsourcing that has stood out is the use of
micro-task markets such as Amazon’s Mechanical Turk
(MTurk) [14]. In this approach, many workers perform small
tasks that together approximate the quality of experts. Using
micro-task markets, researchers have been able to achieve
good results on a wide variety of tasks [2,5].

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
CHI 2017, May 06-11, 2017, Denver, CO, USA
© 2017 ACM. ISBN 978-1-4503-4655-9/17/05…$15.00
DOI: http://dx.doi.org/10.1145/3025453.3025464

This paper, therefore, examines the effect that performing
peripheral tasks has on ideation. More specifically, it embeds
three types of peripheral tasks—rating, similarity, and
combination—into an online brainstorming session. We
explore the following questions:
1.
2.

1843

How does performing peripheral micro-tasks affect
ideation performance?
Do different types of peripheral tasks affect ideation
differently? If so, how?

Crowd-powered Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

Exploration of these questions could allow ideation systems
to move from passive to active forms of inspiration and
support, resulting in more data collection during an ideation
session, aiding in convergent tasks such as idea selection. A
similar approach has been explored by Siangliulue and
colleagues through the IdeaHound system [28]. IdeaHound
allows users to physically cluster semantically related ideas
together in a virtual workspace. This organization enables
the system to infer a semantic model of the ideas. Our
approach differs in that it makes this data collection explicit
rather than implicitly building it in the UI interactions of the
system. In other words, rather than inferring semantic
relatedness by examining how ideators cluster ideas together,
we explicitly ask them to judge the similarity of two ideas.
Our focus, however, is on how doing these tasks affects
ideation performance, rather than examining their result.

By moving from interactive co-located groups to electronic
communication media, production blocking and other issues
can be lessened [7]. For example, individuals in an ideation
group that uses an instant messaging channel for
collaboration don’t have to wait their turn to speak, and can
choose to attend to others’ ideas as they desire. Furthermore,
as communication technologies advance and the world
grows increasingly connected, ever increasing group sizes
become more feasible and the possibility of synergy between
the participants’ ideas can also increase [7,8]. In other words,
by being exposed to more ideas, it becomes more likely that
users will see concepts that may spark new ideas.
One common feature of group brainstorming sessions is to
hear ideas developed by others and build on them. Exposure
to other ideas may have different effects depending on the
type of exposure, with the possibility of either stimulating or
hindering creativity. On the one hand, exposure to a diverse
set of ideas can increase breadth of ideation, while exposure
to a homogenous set of ideas can increase the depth of
exploration within each semantic category [24]. On the other
hand, exposure to ideas might lead to conformity and fixation
effects, where ideators take the main concepts of the ideas
they were exposed to and use them in their solutions
[13,19,30]. This cognitive interference may affect the
exploration of the solution space, causing answers to be more
like each other. There is evidence that the nature of the
external influence as well as how you attend to it defines its
effect on you. Paying too much attention to the superficial
details of the example ideas may lead to fixation [13,19],
while a higher-level view of ideas, possibly in the form of
analogies or schemas, can improve idea generation [31,32].

In the remainder of this paper, we review literature related to
creativity and crowdsourcing. We then describe a system
built to allow ideators to perform small tasks during ideation,
and describe metrics for its evaluation, including a tree-based
representation of individual ideators’ performance. We then
describe four experiments, evaluating how their combined
results answer the questions above. Generally, we find that
tasks are just as useful as simple idea exposure, with rating
and combination tasks even outperforming it in certain
situations. We also explore how inspiration size, frequency,
and homogeneity affects ideation.
RELATED WORK

Creativity: Convergent and Divergent Processes
Creativity can be defined, at the most basic level, as the
production of something original and appropriate [11]. While
there are many different theories of the underlying nature of
creative processes and products [17], our specific interest is
in the dichotomy between divergent and convergent
processes. Divergent processes are those that generate a wide
variety of ideas, thus increasing the solution space [6,17]. On
the other hand, convergent processes are those that involve
the selection of a particular number of the best ideas, seeking
to reduce ambiguity and the size of the possible solution
space [6,17]. Both processes are necessary for creativity:
generating variability (divergence) without effectively
exploring and evaluating your ideas (convergence) can lead
to lost opportunities or disastrous changes [6].

Perhaps the clearest way to understand the effect of external
stimuli comes from the Search for Ideas in Associate
Memory (SIAM) model [23,24]. This model assumes the
existence of two kinds of memory: a low-capacity short-term
memory, i.e. working memory (WM) and a high-capacity
long-term memory (LTM). It proposes that idea generation
involves these two memories in two stages: first, an ideator
retrieves a concept along with its features from LTM into the
WM (e.g. the concept hotel has the feature “has rooms” [23]),
and then generates ideas based it. When the ideator
continuously fails at generating new ideas based on the
current concept, he or she may activate another concept from
the search queue and try again. This queue is comprised of
items such as the problem definition or previously generated
ideas. When an idea is shown to an ideator, it can be added
to the search queue if it is sufficiently attended to [24].
Therefore, it can be said that an ideator had a great breadth
of ideation if he or she explored many concepts and great
depth if he or she developed many ideas within a concept.

Brainstorming
Much of the effort in research and practice in improving
creativity has focused on supporting divergent processes
through brainstorming. Brainstorming was popularized by
Osborn in the 1950’s, and consists of a few simple rules, such
as holding back on criticism and building on the ideas of
others [25]. There are in fact benefits to this approach, as
attending to the ideas of others can be inspiring [19,23]. But
it is vulnerable to factors such as evaluation apprehension,
free riding, or perhaps more influentially, production
blocking—that is, not being able to share or generate new
ideas while someone is sharing theirs [9].

Peripheral Ideation Tasks
Much of the support for crowd ideation sessions comes from
peripheral tasks done by other MTurk crowd workers. These
tasks can be classified along four main categories: rating,
combination, inspiration design, and problem abstraction. In

1844

Crowd-powered Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

Combination
Combination tasks revolve around combining characteristics
of two ideas, with the goal of generating a third idea.
Combination engages divergent processes that can produce
better, more diverse ideas [16]. In a study by Yu & Nickerson
[33], for example, workers were presented with two different
ideas, generated by other workers, for the design of a chair.
Their task was to design a new chair that combined aspects
of the two other ideas. The resulting design was then passed
to a different set of workers for further processing.
Combinations could also employ higher representations of
ideas—schemas—as explored by Yu, Kittur, & Kraut, who
used them in both exploration and generation of ideas
[31,32]. Schemas can facilitate analogical reasoning when
combining ideas, focusing ideators on the core principles of
an idea rather than on its surface features.

crowd ideation contexts, workers outside of the ideation are
typically tasked with performing them. However, we argue
that these tasks have promise for improving the quality of
brainstorming when executed by the ideators themselves. We
now review the rating and combination tasks, which are
explored in depth in this paper. They were chosen due to their
simplicity and for their usage of external ideas, allowing for
a cleaner comparison with simple idea exposure. We leave
the remaining two categories for future work.
Rating
Rating an idea or artifact on one or more dimensions is one
of the most common forms of peripheral tasks. We are
concerned with two types of rating: the nature of the idea
(originality and usefulness), or its similarity to other ideas.
Rating can support ideation by, for example, presenting the
most creative or diverse sets of ideas as inspiration to other
ideators [24]. Comparisons are either relative—rate ideas in
terms of each other—or absolute—rate ideas individually on
a given scale. For example, Siangliulue et al. [27] looked into
rating ideas based on their similarity in two different ways:
for the first, they presented workers with three ideas, asking
them to choose which of two ideas was more similar to
another. They also tasked workers with rating 30 pairs of
ideas on their similarity, using a 7-point scale. Other ways
ratings tasks have been performed are to ask workers to rate
ideas in terms of novelty and quality (or similar dimension),
both on a 7-point scale [4,28,32].

While, to our knowledge, there are no other examples of
combination tasks in crowdsourced ideation within a
microtask platform such as MTurk, the microtasks literature
provides other instances of similar tasks, which demonstrate
the feasibility of assigning workers to combine two or more
different objects. On a general level, the CrowdForge
framework [15] defines three types of tasks: partition, in
which tasks are broken up in smaller pieces; map, in which a
task is processed by one or more workers; and reduce, in
which the result of multiple worker’s tasks are merged into a
single output. One of their examples demonstrates how
workers could write an article by breaking it down into an
outline, assigning workers to write down facts for each topic
in an outline, and having workers merge those facts into
paragraphs. In the case of ideation tasks, the merge step
could be defined as combining two ideas. More specifically,
however, combination of results from different workers is a
common feature in MTurk workflows, generally for quality
assurance purposes (e.g. [2,5]).

What could be the effect of performing rating tasks on an
ideator’s performance? By rating others’ ideas, ideators
would be exposed to a small number of raw ideas which, as
we have already reviewed, may produce either stimulation or
fixation effects. These tasks would require users to think
critically about the examples in order to rate them in terms
of their similarity, usefulness, or originality. Attending to the
stimuli provided by external ideas is a requirement for the
idea to influence ideation [10]. In fact, the SIAM model
proposes that an idea will only be added to the search cue if
sufficiently attended to [23]. Therefore, having users actively
engage in analyzing other ideas may promote a larger
effect—either of stimulation or interference—than if just
passively being exposed to them. But this may depend on the
nature of the idea and how it is displayed [24,29], as rating
others’ ideas may positively affect the creativity, diversity,
depth, and quantity of ideas generated.

Thus, our hypothesized effect of combination tasks on
ideation relates to that of rating: users are exposed to new
ideas and have to attend to them to perform the combination.
However, unlike the rating tasks, this is not an entirely
convergent process, but also includes a divergent step of
generating a new idea [16]. This may counterbalance any
fixation that can happen while attending to the ideas that are
to be combined. On the other hand, the depth of the produced
ideas might decrease due to this increased divergence.
SYSTEM AND WORKFLOW

A negative effect may come from engaging in criticism or
judgment during brainstorming, which usually discourages
criticism as it may lead to evaluation apprehension.
However, there is evidence that this effect does not account
for most productivity loss in brainstorming [9]. In fact,
criticism has even been found to improve performance [21].
Furthermore, while rating an idea’s originality and
usefulness may produce such effect, other rating tasks such
as rating the similarity may not yield such effects due to their
non-judgmental nature.

We developed an online ideation system that enables the
creation of timed asynchronous ideation sessions, as well as
a mechanism for seeing other people’s ideas upon request via
an inspiration button, thus allowing ideators to pull
inspirations whenever they choose to do so. This is in line
with previous approaches (e.g. [4,29]). An alternative to this
approach would be to push inspirations at regular time
intervals. This would ensure that every ideator was exposed
to the same number of inspirations, allowing a clearer
comparison of the effect of the different types of tasks.

1845

Crowd-powered Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

However, one of our goals was to see if embedding tasks into
inspirations would detract from users’ interest in using the
inspiration mechanism or decrease performance. A push
approach would hinder us from exploring this. Furthermore,
the SIAM model predicts that pushing inspirations could
negatively affect performance [22], since it could interrupt a
users’ train of thought. In fact, Siangliulue et al. found issues
with fluency using a push approach [29]. Therefore, we
allowed users to request inspirations on demand.

with a thank you message, a user ID (used for payment), and
a link to a short post-session survey.
For every study in this paper, the problem that ideators were
tasked to ideate on was: “Mechanical Turk currently lacks a
dedicated mobile app for performing HITs on smartphones
(iPhone, Androids, etc.) or tablets (e.g., the iPad).
Brainstorm N features for a mobile app to Mechanical Turk
that would improve the worker's experience when
performing HITs on mobile devices. Be as speciﬁc as
possible in your responses.” This task, suggested by
Krynicki [18], was chosen because it has been successfully
used in previous studies [3,18] and MTurk users have
knowledge about the issue and may be motivated to
contribute to it, as it could increase their opportunities for
engaging with HITs and improving their income. Both
motivation and knowledge are key to creativity [1].

The system is comprised of four main parts (Figure 1).
Although the figure depicts the system in its final iteration,
its overall structure as described in this section was
maintained throughout the sessions, with a few incremental
differences that will be pointed out in each experiment’s
section. At the top (A), the system displays instructions, the
problem definition, and a timer. On the left is the ideation
panel. It consists of a form for entering an idea along with a
list of user defined tags associated with it (B), and a list of
the user’s previously submitted ideas and tags (C). On the
right side is the inspiration panel (D). When the button is
clicked, the user is presented with a set of ideas and
depending on their condition, a task associated with them.
This mechanism draws randomly from a pool of ideas
generated in previous experiments.

METRICS

For each study, we report the following metrics:
 Fluency: number of ideas generated by the user.
 Number of inspirations: number of times the user clicked
the inspiration button.
 Inspiration influence: a user’s average similarity between
an idea and the most similar of its preceding inspirations.
More central to our interests, however, are metrics of breadth
and depth, which we extracted from an ideation tree,
described below. Tree representations have been previously
suggested to measure or visualize ideation outcome [12,20],
and the semantics of the different branches of a tree can
reflect the usual discrete categorization of ideas traditionally
used in creativity research [26], while their depth can
represent the notion of ideation within one category [23].
This tree is built from a chronological list of user actions—
they either add a new idea or request an inspiration. In the
tree, similarity between ideas is measured using Latent
Semantic Analysis (LSA) [3]. For this paper, our LSA corpus
was built on 5640 ideas generated to solve the same problem
that we explore in this paper. This corpus comes both from
our own studies (2115 ideas) and the corpus shared by the
authors of [4] (3525 ideas). Figure 2 shows the tree and idea
pool in five different points in time during a user’s ideation:
1. We add the first user idea as the child of a dummy node;
2. For the second user generated idea, we compare it either
with every node that is already in the tree, or with every
inspiration previously seen. If the LSA similarity to any
of those is greater than a given threshold (we used 0.5),
we add it as a child of the most similar node. In this point
in time, idea 2 was most similar to idea 1, and is added as
its child;
3. At the third point in time, the user has generated a third
idea. Again, we compare it to every node already in the
tree. In this case, none of the similarities exceeded the
threshold, so this idea is added as a new child of the root
node, representing an estimated new category of ideation.

Figure 1. Screenshot of the ideation system as used in the final
experiment, comprised of the following parts: A) Problem
description and timer; B) idea submission input; C) list of the
users’ submitted ideas; D) inspiration panel.

When users access the system, they first see a page stating
how much time the session lasts, and asking them to move
forward only if absolutely sure that they can commit their
full attention for the specified amount of time. Following
that, users would see another page describing the system,
including the inspiration mechanism (if any), and how to use
it. Upon finishing the instructions, users begin the ideation
session. After the timer is done, the system presents users

1846

Crowd-powered Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

Figure 2. Snapshots of the ideation tree and idea pool in five different points of time in a given user’s ideation.
Also note that between t2 and t3 the ideator has requested
an inspiration, which is added to the idea pool but not to
the tree;
4. The user’s 4th idea is compared to every node in the idea
pool. In this case, a previously seen inspiration is the most
similar. Therefore, we add it as a child of the inspiration
node, which we then add as a new child of the root node;
5. Finally, another idea has been generated by the user. It is
compared to previous ideas and inspirations, and is added
as a child to the most similar node, which is the first one.

quickly be done by any worker, and they are very effective
for supporting convergence processes. We used an earlier
version of the system than the one described in the system
section, which differed as follows: the input box for the idea
was on the top panel, along with the problem description;
users did not have to input tags for their ideas; Lastly, the
inspiration box did not have any instructions regarding how
inspirations could be used. This study had three conditions:
1.

From this tree, we extract the two metrics:
 Breadth: the number of children in the root node. These
were the ideas that, at the time they were added, were not
similar enough to be considered a continuation of another
idea, therefore creating a new branch of ideas. For
example, in Figure 2 at t5, the breadth would be 3.
 Depth: the number of nodes in the branch with the most
number of nodes. For example, in Figure 2 at t5, the depth
would be 3.

2.

3.

As a check on this measure, we additionally calculated the
metrics described by Chan et al. [4], also built using Latent
Semantic Analysis (LSA). In [4], breadth was the mean
distance between each pair of ideas generated by a user.
Depth was the maximum similarity between the ideas
generated by a user. Our metrics are significantly correlated
with these, at r = 0.650, p < 0.001 for breadth, and r = 0.564,
p = 0.001 for depth. And while there may be concerns
relative to the metric’s sensitivity to the threshold value, we
have found that threshold changes in either direction do not
result in drastic changes in the results. For example,
changing it to 0.7 resulted in a mean difference of 2.3
(SD=2.2) in breadth. Therefore, we believe that this metric is
both valid and capable of more accurately representing the
notions of breadth and depth.

Control: This condition is equivalent to a nominal
group in typical brainstorming settings. There is no
inspiration panel, and thus no external stimulus. Users
type their ideas, and can see the list of their own ideas.
Exposure: In this condition, the inspiration panel is
visible. When the inspiration button is clicked, it
displays one idea from the pool of past ideas, without
any task associated with it. The idea disappears when the
user clicks the “done” button.
Rating: This condition is similar to the exposure
condition. However, when the inspiration button is
clicked, in addition to the idea, users also received a task
prompting them to rate the inspiration idea in 2
dimensions: originality and practicality (Figure 3). After
submitting the rating, the idea disappears.

EXPERIMENT 1: RATING TASK

For the first experiment, we chose to start our exploration
with rating tasks. Due to their simplicity, they can easily and

Figure 3. Rating task interface.

1847

Crowd-powered Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

We published a MTurk HIT that directed workers to our
system. 60 workers participated in this study (at least 1000
completed HITs, approval > 95%, US only), but 1 (exposure
condition) was excluded from the analysis due to an
abnormal number of inspirations requested (142). In total,
559 ideas were generated. Each worker ideated for 18
minutes, filled out a small survey at the end of the session,
and was compensated $2. Workers also received an ideation
qualification on MTurk (awarded after every experiment).
Subsequent experiments reported here required workers to
not have this qualification, thus ensuring participants were
unique for each session.

being randomly drawn from the pool of ideas, which will
likely create a heterogeneous set of examples. Past work has
shown that a heterogeneous set of examples will improve
diversity of ideas [24,27]. Having no clear advantage could
mean a problem either in the intervention (e.g. it is too
simple) or in how users performed it (e.g. they did not attend
to it). There may also have been confusion on how users
should use the ideas in the rating task. For example, a user
declared feeling that the inspiration they got would invalidate
using that idea: “I think it hindered me more than it helped
because it just provided an example that I then couldn't use”.
Perhaps guidelines might be effective in helping users better
use the inspirations.

Results

Condition

Workers

Ideas /
Worker

Insp. /
Worker

Baseline

19

10.37 (4.16)

-

Exposure

19

9.53 (4.62)

12.16 (10.64)

Rating

21

8.62 (5.02)

6.67 (6.80)

EXPERIMENTS 2A & 2B: SIMILARITY CHOICE TASK

In experiment 1, we found no clear advantages over the
baseline, even though there was a larger influence in the
exposure condition. Given these results, we decided to
change the task to similarity comparison, the number of ideas
displayed, and to add a clarification on how they could use
the inspirations (see the text at the top of Figure 4).

Table 1. Fluency and inspiration metrics for experiment.

Tables 1 and 2 summarize the metrics across the different
conditions. A one-way ANOVA test shows no significant
difference in fluency, F(2,56) = 0.713, p = 0.495. There was
a marginally significant difference in number of inspirations
requested across the two conditions, F(1,38) = 3.855, p =
0.057. We do, however, find a difference in inspiration
influence between the exposure and rating conditions,
F(1,38) = 9.855, p = 0.003.
Condition

Breadth

Depth

Influence

Baseline

5.37 (2.54)

4.74 (3.69)

-

Exposure

6.40 (3.20)

3.25 (1.74)

0.23 (0.90)

Rating

5.10 (2.70)

3.29 (1.52)

0.12 (0.12)

Table 2. Breadth, depth, and influence for experiment 1.

We calculated a Mixed Generalized Linear Model (GLM)
with breadth as outcome variable, condition as factor, and the
fluency as covariate. We included the interaction between
condition and fluency in the model. We found a marginally
significant interaction effect between condition and fluency,
F(2, 56) = 3.078, p = 0.054, and no main effect of condition
on breadth, F(2,56) = 1.374, p = 0.262.
As the depth of user ideas followed a negative binomial
distribution rather than a normal distribution, we conducted
a negative binomial regression with depth as outcome,
condition as factor, and fluency as covariate. The interaction
between fluency and condition was included in the model.
We found no significant interaction effect or main effect of
condition on depth, Wald Chi-Square = 4.099, p = 0.129.

Figure 4. The task panel for condition 3. Users were shown a
seed idea along with 6 other ideas, and were asked to click on
the most similar one (in this case, the user clicked on the dark
blue idea), as well as rating their degree of relationship.

Discussion for Study 1

While we see only a marginal effect of the interaction
between condition and fluency on breadth, we find no clear
advantage in any condition. The fact that breadth seemed to
be more affected than depth may spring from inspirations

Experiment 2A

In this experiment, the task condition presents the user with
one seed idea along with 6 other ideas, asking him or her to

1848

Crowd-powered Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

choose the most similar to the seed (Figure 4). The number
of ideas was chosen to maximize the possibility of similar
ideas being shown, as well as to explore the result of a more
dramatic increase in the number of ideas shown per
inspiration. We expected this to yield a stronger influence on
ideators’ breadth, as they would be exposed to more ideas.
We also hypothesized that similarity comparisons would
force to user to think more abstractly about the ideas in order
to find common features between them, thus reducing
fixation and possibly improving breadth.

requested for both inspiration conditions—close to 2. It is
likely that the great number of ideas per inspiration either
overwhelmed users or provided them with what they judged
to be enough inspiration for a long stretch of time.
Experiment 2B: Smaller Inspirations, Controlled Pool

In experiment 2A, we found no meaningful difference across
conditions, likely due to the very small number of
inspirations requested in both experimental conditions.
Therefore, we reduced the number of ideas per inspiration to
3. We also controlled the pool of ideas. One of the authors
went through the existing idea pool and generated 40
different groups of 3 ideas. The goal was to create sets of
ideas that shared similar elements, making the choice task
easier, while at the same time having different features. For
example, the idea “Notifications such as sound or vibration
when a new hit is available” was grouped with “sounds
effects so people know when to do the surveys and also tools
to see how well they are doing” and “The app would have
alarms, bells, or sounds to notify of particular work or
requesters”. The three ideas in the group would always come
together, but in random order.

This second experiment followed the same method as the
first, with the two key differences above. 60 workers
participated in this study (at least 1000 completed HITs,
approval > 95%, US only). In total, 492 ideas were
generated. Each worker ideated for 18 minutes, filled out a
small survey at the end of the session, and was paid $2.
Results

Condition

Workers

Ideas /
Worker

Insp. /
Worker

Baseline

20

7.45 (5.51)

-

Exposure

22

8.50 (4.34)

2.00 (2.19)

Similarity

18

8.61 (2.87)

2.28 (1.77)

The method for this experiment changed in one significant
way: we increased ideation time from 18 to 25 minutes, to
collect more data on how ideation changes as fluency
increases. We also increased the target number of users per
condition (see Table 5). 89 workers participated in this study
(at least 1000 completed HITs, approval > 98%, US only. In
total, 863 ideas were generated. Workers also filled out a
small survey at the end. Each worker was paid $3.50.

Table 3. Fluency and inspiration metrics for experiment 2A.

Tables 3 and 4 summarize the metrics for this experiment. A
one-way ANOVA test shows no difference in fluency,
F(2,57) = 0.416, p = 0.661, or number of inspirations
requested between the exposure and task conditions, F(1,28)
= 0.261, p = 0.612. Finally, similarly to the last study, we
found a difference in inspiration influence. This time,
however, the task condition displayed a higher influence than
the exposure, F(1,28) = 4.59, p = 0.039 (see Table 4) .

Condition

Workers

Ideas /
Worker

Insp. /
Worker

Baseline

35

8.43 (4.74)

-

Exposure

27

9.53 (5.95)

12.59 (12.29)

Similarity

27

9.15 (5.11)

5.74 (5.18)

Condition

Breadth

Depth

Influence

Baseline

4.90 (3.43)

2.40 (1.31)

-

Exposure

6.36 (2.95)

2.05 (0.785)

0.11 (0.03)

Table 5. Fluency and inspiration metrics for experiment 2B.

Similarity

5.78 (2.15)

2.28 (0.82)

0.13 (0.03)

Tables 5 and 6 summarize the metrics for this experiment. A
one-way ANOVA shows a significant difference in fluency,
F(2,86) = 3.528, p = 0.034. A post hoc Tukey test shows a
significant difference between baseline and exposure
conditions, p = 0.031, but no difference between baseline and
task (p = 0.854) or exposure and task (p = 0.139). There was
also a significant difference in number of inspirations
between the exposure and similarity conditions, F(1,52) =
7.119, p = 0.01. However, this time no significant differences
were found in inspiration influence, F(1,47) 2.019, p = 0.162.

Table 4. Breadth, depth, and influence for experiment 2A.

We calculated a Mixed GLM with breadth as outcome
variable, condition as factor, and fluency as covariate,
finding no significant difference, F(2,57) = 1.962, p = 0.150.
As in the last study, we conducted a negative binomial
regression for depth. With condition as factor and number of
ideas as covariate, we found no significant difference, Wald
Chi-Square = 2.108, p = 0.348.
Discussion for study 2A

Unlike the first study, the task condition yielded a
significantly higher influence than the exposure condition,
but this did not translate into an improvement in ideation
breadth or depth. In general, all three conditions appeared to
be very similar with respect to breadth and depth, despite the
small but significant difference in influence. This is not so
surprising when you consider the low number of inspirations

Condition

Breadth

Depth

Influence

Baseline

5.31 (2.99)

3.17 (2.62)

-

Exposure

8.07 (3.79)

3.33 (1.68)

0.14 (0.04)

Similarity

6.33 (3.75)

2.78 (1.45)

0.16 (0.04)

Table 6. Breadth, depth, and influence for experiment 2B.

1849

Crowd-powered Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

We calculated a Mixed GLM with breadth as outcome,
condition as factor, and fluency as covariate, including the
interaction between condition and fluency. We found a
marginally significant interaction between condition and
fluency, F(2,83) = 2.88, p = 0.062, but no main effect of
condition on breadth, F(2,83) = 1.269, p = 0.286.

The exposure condition also saw more inspiration requests.
We also found baseline high fluency ideators outperforming
the others in overall depth, low fluency baseline
outperforming task in 2nd half breadth, and low fluency
exposure outperforming baseline in 1st half depth. In other
words, the inspirations not only did not help, but actually
hindered the depth of high fluency ideators. It is possible that
the closely related nature of the inspirations promoted
fixation for them, thus detracting from their second half
depth. Finally, for low fluency ideators, we see exposure
helping them in first half depth, but we see tasks detracting
from their second half breadth.

For depth, a negative binomial regression with condition as
factor, fluency as covariate, and including the interaction
between fluency and condition found a significant
interaction between fluency and condition, Wald Chi-Square
= 10.003, p = 0.007, but no significant main effect of
condition, Wald Chi-Square = 4.550, p = 0.103. A pairwise
comparison shows a difference only for high fluency ideators
(1 SD above the mean). In this case, those in the control
condition (M=6.31, SE=0.850) performed significantly
above both exposure (M=3.84, SE=0.425, p = 0.009) and
similarity (M=3.35, SE=0.584, p = 0.004) conditions.

EXPERIMENT 3: COMPARISON ACROSS TASK TYPES

We conducted a final study in order to compare the two
previous task types with a new one: combination.
Combination tasks involve not only convergent processes,
but also a divergent one—the generation of the new,
combined idea [16]. While this can happen naturally during
ideation, this task explicitly forces it to happen. Therefore,
we expect a positive impact of combination on breadth. We
also reverted back to completely random inspiration
retrieval. The method remained the same as the one
employed in experiment 2B, with the difference being that
there are five conditions (control, exposure, 3 task types).

We also divided both halves of the ideation and analyzed
their breadth and depth separately. This was done since the
effect of inspirations on users is likely not constant across the
session, as they will likely be able to generate more ideas by
themselves at the beginning of the session than at the end,
when inspirations may be more useful. Thus, looking at the
metrics over the entire session may wash out some effects.

150 workers participated in this study (at least 1000
completed HITs, approval > 98%, US only), but 7 workers
were not included in the analysis, as they either wrote
unrelated ideas (n=1), generated unrelated tags (e.g. “tags 1”,
n=4), or didn’t complete the post session questionnaire
(n=2). In total, 1480 ideas were generated. Workers ideated
for 25 minutes, and filled out a small survey at the end of the
session. Each worker was paid $3.50.

A Mixed GLM with breadth for the first and second halves
(run separately) as outcome variables, with condition as
factor, and fluency as covariate yielded no main effect of
condition on the first half breadth, F(2,85) =2.704, p = 0.073.
On the second half, however, it yielded a main condition
effect, F(2,83) = 3.527, p = 0.034, as well as a significant
interaction on condition and fluency, F(2,83) = 6.957, p =
0.03. In pairwise comparisons, a difference was seen for low
fluency ideators (1 SD below the mean), where the control
condition (M=1.54, SE=0.322) was significantly superior to
the task condition (M=0.38, SE=0.381, p = 0.022), but was
not significantly different than the exposure condition
(M=1.06, SE=0.447, p=0.386).
For the first half depth metric, a negative binomial regression
with condition as factor, fluency as covariate, and including
the interaction between condition and fluency yielded a
significant main effect, Wald Chi-Square = 6.48, p = 0.039,
and a significant interaction between condition and number
of ideas, Wald Chi-Square = 7.46, p = 0.024. For low fluency
ideators (1 SD below the mean), we see the exposure
condition (M=1.99, SE=0.348) significantly outperform the
control condition (M=1.05, SE=0.22, p = 0.042), but it was
not significantly different to the task condition (M=1.74,
SE=0.409, p=0.638). No pairwise differences were seen for
high fluency ideators. The second half presented no
significant interaction or main condition effect, Wald ChiSquare = 3.362, p = 0.186.

Condition

Workers

Ideas /
Worker

Insp. /
Worker

Baseline

29

11.38 (7.178)

-

Exposure

28

10.57 (6.143)

7.70 (6.92)

Rating

27

8.48 (4.136)

4.28 (4.86)

Similarity

31

11.52 (6.45)

8.77 (5.36)

Combine

28

9.57 (5.647)

3.16 (2.51)

Table 7. Fluency and inspiration metrics for experiment 3.

Tables 7 and 8 summarize the metrics for this experiment. A
one-way ANOVA shows no significant difference in fluency
across conditions, F(4,142) = 1.276, p = 0.283. A one-way
ANOVA for number of inspirations between conditions
shows a significant difference in the number of inspirations
requested, F(3,110) = 8.022, p < 0.001. A post hoc Tukey
test shows that the exposure and similarity conditions were
significantly higher than the rating and combination
conditions (p < 0.05), but not from each other. As for
influence, a one-way ANOVA shows no significant
difference, F(3,105) = 1.285, p = 0.283.

Discussion for study 2B

To summarize this study, we found a significant difference
in fluency only between the exposure condition over control.

1850

Crowd-powered Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

Condition

Breadth

Depth

Influence

Baseline

7.86 (4.086)

3.28 (2.52)

-

Exposure

8.00 (4.830)

2.61 (1.52)

0.14 (0.05)

Rating

6.48 (3.887)

2.56 (1.76)

0.15 (0.07)

Similarity

8.74 (4.289)

2.58 (1.52)

0.12 (0.04)

Combine

8.07 (4.48)

2.07 (1.15)

0.13 (0.05)

SD above the mean (16 ideas). No significant difference is
seen for low fluency ideators. For high fluency ideators,
however, we see that that the three task conditions
significantly outperformed the baseline (p < 0.001). When
compared to the exposure condition, however, only the rating
and combination conditions significantly outperformed it.

Table 8. Breadth, depth, and influence for experiment 3.

A Mixed GLM with breadth as outcome variable, condition
as factor, fluency as covariate, and including the interaction
between condition and fluency yielded a significant
interaction of condition and number of ideas on breadth,
F(4,133) = 3.736, p = 0.006, but no main effect of condition,
F(4,133) = 1.823, p = 0.128. For average fluency ideators (10
ideas), a pairwise comparison shows a significant difference
between the control (M=7.20, SE=0.39) and combine
(M=8.38, SE=0.39) conditions, p = 0.037. There are also
significant differences for high fluency ideators (1 SD above
the mean, fluency = 16 ideas), in which the control condition
was outperformed by all other conditions (pexposure = 0.018,
prating = 0.010, psimilarity = 0.046, pcombine < 0.001), but they
were not significantly different among themselves. Figure 5
depicts the regression lines for the different conditions.

Figure 6. Marginal means and std. error for breadth (when
fluency is 4 and 16) during the second half of ideation.

For overall depth, the control condition trended higher than
the others, but a negative binomial regression with depth as
outcome variable, condition as factor, and fluency as
covariate found no effect of condition, Wald Chi-Square =
5.456, p = 0.244. The same model, but separately testing first
and second half depth as outcome variables also yielded no
significant differences, 1st half Wald Chi-Square = 1.469, p
= 0.832, 2nd half Wald Chi-Square = 3.422, p = 0.49.
DISCUSSION

Through four experiments, we explored the integration of
peripheral microtasks as part of an ideation session.
Experiment 1 compared the rating task with simple exposure,
finding very little influence. Experiments 2A and 2B
increased the number of ideas and evaluated similarity tasks,
pointing to limitations with quantity and homogeneity of
inspiration ideas. Finally, experiment 3 compared all three
task types together. We now discuss the main points from the
combined results.
Tasks Performed as Good as Exposure, Outperforming it
in Some Cases

Figure 5. Regression lines for breadth by fluency.

Again, we calculated the breadth metric for each half, and
found that a Mixed GLM with first half breadth as outcome
variable, condition as factor, and fluency as covariate yielded
no significant effect of condition, F(4,137) = 1.342, p =
0.257. However, using the second half breadth as outcome
variable and including an interaction between condition and
fluency yields a significant interaction between condition
and fluency, F(4,133) = 7.197, p < 0.001, and a main effect
of condition, F(4,133) = 2.725, p = 0.032. Figure 6 shows the
marginal means for second half breadth across the different
conditions, with fluency fixed at 1 SD below (4 ideas) and 1

Experiment 3 shows combination tasks outperforming the
baseline for average fluency ideators. As for high fluency
ideators, we find all conditions outperforming the baseline.
However, when we isolate the second half of ideation, we
find significant differences between the types of inspiration.
The rating and combination tasks significantly outperformed
the exposure condition, while similarity significantly
outperformed control. One explanation for this difference is
that these tasks were more cognitively demanding than the
similarity and exposition inspirations. But unique
characteristics of the tasks may explain them further. While

1851

Crowd-powered Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

The Homogeneity of Idea Sets Can Influence the Effects

usual brainstorming rules discourage criticism of ideas [25],
there is evidence that criticism may foster exploration [21],
which could partially account for the better performance of
the rating task. Alternatively, it is possible that the rating
scales provided users with a structure that guided them in
generating ideas or evaluating inspirations. As for the
combination task, this result was in line with our expectation,
as the task also involves a divergent step [16], which could
foster breadth of exploration.

While most results were seen in breadth, we see a different
pattern in experiment 2B, where the inspiration idea sets
were manipulated to be more homogenous. In it, we see
exposure outperforming control in first half depth, and
control outperforming task in second half breadth. This could
be explained partially by the homogenous nature of the ideas,
as previously discussed. This indicates that the nature of the
inspiration sets is highly influential in the outcome [27].
Therefore, the effect of different levels of homogeneity and
task types should be explored in future work.

Fewer Effective Inspirations May Be Better than Many
Ineffective Ones

It is interesting to note that the two most effective conditions
had the lowest number of inspiration requests. This may lead
to the conclusion that the cognitive load of an inspiration
may be more important than the number of times it is used.
In other words, fewer but more effective inspirations can be
better than having many less effective inspirations. An
alternative explanation is that since these users requested
fewer inspirations, they had more time to ideate, thus
increasing breadth. However, since the fluency was not
different across conditions, this is an unlikely explanation.

Some limitations of this investigation must be noted, with the
first being the metrics. While the tree-based metric is
consistent with previous practices and results, it needs
further evaluation. A comparison with similar trees built by
human experts would shed light on its performance.
Alternatively, graph-based metrics could also be devised in
order to better represent the inherent uncertainties in
automated textual analysis (e.g. ideas could be linked to
more than just one parent idea, with edge weights
representing their similarity). Furthermore, we do not
explore measures of creativity, whereas past research has
used MTurk workers to do that [3,29,33]. However, we
found workers to have very low degrees of agreement among
themselves, and therefore do not report these measures.
There are also limitations to the pull approach. While it
allowed us to compare the performance in a natural setting,
the numerical differences in inspiration requests limit our
ability to clearly determine the effects of the different tasks.
Finally, we do not explore the results of the tasks (e.g. the
quality of the ratings). While this exploration is outside of
the scope of this paper, past results are encouraging in the
potential of peripheral crowd work to yield useful outcomes
such as a semantic model of ideas [28].

Inspiration Effects Depend on Timing and Fluency

The studies, especially experiment 3, highlight that
inspirations may influence different users at different times.
On experiment 3, for example, we see significant differences
only for average or high fluency ideators. This is not
surprising, as low fluency ideators may simply not be
engaged enough to attend to the task or the inspirations,
regardless of condition. Furthermore, results were mainly
seen on the second half of ideation. This is intuitive, since at
later points in time ideators are more likely to be running out
of ideas [23], and thus may be more susceptible to the
inspirations. This suggests that a “one size fits all” approach
does not work. It may prove useful for crowd ideation
support systems to restrain inspirations for a latter phase of
ideation, or to initially target fluency improvement. Research
that looks into adaptive support could prove fruitful.

CONCLUSION

In this paper, we have analyzed the effect of performing three
different types of tasks normally done by other crowd
workers: rating, similarity, and combination. We ran four
subsequent experiments on MTurk to evaluate how they
compare to idea exposure or individual ideation. Using
breadth and depth metrics based on an ideation tree, we
found the performance of task inspirations to be as good or
better than simple idea exposure. We also found that the
effect of inspirations depends on the fluency of ideators and
the period in which it is used. Finally, we saw indications
that the homogeneity of inspirations influences the outcome.
Therefore, this paper provides some support and guidance in
explicitly embedding microtasks into ideation, which will
not only be generating information useful for convergent
processes, but will also aid ideators in improving the
divergence of their idea generation.

Very Simple or Complex Inspirations Have no Effect

Studies 1 and 2A, while exploring two different types of
tasks, shed light on lower and upper limits when concerning
the number of ideas that can be presented for each
inspiration. With both one (experiment 1) and seven
(experiment 2A) ideas per inspiration, we see no significant
difference between conditions. On the lower end, this lack of
effect happened despite a considerable number of inspiration
requests. This could be due to the simplicity of the
inspirations not fostering attention to the ideas, or to users
not knowing how to use the inspirations, as previously
discussed. On the higher end, the lack of effect likely
happened due to the low number of requests. At the end, we
have found better effects with inspirations containing three
ideas each. This could, however, vary depending on the
inspiration type (e.g. a combination task of size 6 could be
considerably more demanding than a similarity task of size
6), or even nature of ideas (homogenous idea sets may be less
cognitively demanding, allowing more ideas per inspiration).

ACKNOWLEDGMENTS

The authors would like to thank all of the Mechanical Turk
workers who participated in the studies. This research was
funded by the CAPES Foundation, Ministry of Education of
Brazil, Brasília - DF 70040-020, Brazil.

1852

Crowd-powered Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

REFERENCES

Robert E. Kraut. 2011. Crowdforge: Crowdsourcing
complex work. In Proceedings of the 24th annual ACM
symposium on User interface software and technology, 43–
52.
Retrieved
August
4,
2015
from
http://dl.acm.org/citation.cfm?id=2047202
16. Nicholas W. Kohn, Paul B. Paulus, and YunHee Choi.
2011. Building on the ideas of others: An examination of the
idea combination process. Journal of Experimental Social
Psychology
47,
3:
554–561.
https://doi.org/10.1016/j.jesp.2011.01.004
17. Aaron Kozbelt, Ronald A. Beghetto, and Mark A.
Runco. 2010. Theories of creativity. The Cambridge
handbook of creativity: 20–47.
18. Filip Krynicki. 2014. Methods and models for the
quantitative analysis of crowd brainstorming. Retrieved
April
5,
2016
from
https://uwspace.uwaterloo.ca/handle/10012/8347
19. Richard L. Marsh, Joshua D. Landau, and Jason L.
Hicks. 1996. How examples may (and may not) constrain
creativity. Memory & cognition 24, 5: 669–680.
20. Brent A. Nelson, Jamal O. Wilson, David Rosen, and
Jeannette Yen. 2009. Refined metrics for measuring ideation
effectiveness. Design Studies 30, 6: 737–743.
https://doi.org/10.1016/j.destud.2009.07.002
21. Charlan J. Nemeth, Bernard Personnaz, Marie
Personnaz, and Jack A. Goncalo. 2004. The liberating role of
conflict in group creativity: A study in two countries.
European Journal of Social Psychology 34, 4: 365–374.
https://doi.org/10.1002/ejsp.210
22. Bernard A. Nijstad, Michael Diehl, and Wolfgang
Stroebe. 2003. Cognitive Stimulation and Interference in
Idea-Generating Groups. In Group Creativity: Innovation
Through Collaboration. Oxford University Press.
23. Bernard A. Nijstad and Wolfgang Stroebe. 2006. How
the group affects the mind: A cognitive model of idea
generation in groups. Personality and social psychology
review 10, 3: 186–213.
24. Bernard A. Nijstad, Wolfgang Stroebe, and Hein FM
Lodewijkx. 2002. Cognitive stimulation and interference in
groups: Exposure effects in an idea generation task. Journal
of experimental social psychology 38, 6: 535–544.
25. Alex F. Osborn. 1963. Applied imagination; principles
and procedures of creative problem-solving. Scribner, New
York.
26. Jonathan A. Plucker and Matthew C. Makel. 2010.
Assessment of Creativity. In The Cambridge Handbook of
Creativity, James C. Kaufman and Robert J. Sternberg (eds.).
Cambridge University Press, Cambridge, 48–73. Retrieved
November
29,
2016
from
http://ebooks.cambridge.org/ref/id/CBO9780511763205A0
13
27. Pao Siangliulue, Kenneth C. Arnold, Krzysztof Z.
Gajos, and Steven P. Dow. 2015. Toward Collaborative
Ideation at Scale: Leveraging Ideas from Others to Generate
More
Creative
and
Diverse
Ideas.
937–945.
https://doi.org/10.1145/2675133.2675239
28. Pao Siangliulue, Joel Chan, Steven P. Dow, and

1. Teresa M. Amabile. 1983. The social psychology of
creativity: A componential conceptualization. Journal of
personality and social psychology 45, 2: 357.
2. Michael S. Bernstein, Greg Little, Robert C. Miller,
Björn Hartmann, Mark S. Ackerman, David R. Karger,
David Crowell, and Katrina Panovich. 2010. Soylent: a word
processor with a crowd inside. In Proceedings of the 23nd
annual ACM symposium on User interface software and
technology, 313–322. Retrieved January 11, 2016 from
http://dl.acm.org/citation.cfm?id=1866078
3. Joel Chan, Steven Dang, and Steven P. Dow. 2016.
Comparing Different Sensemaking Approaches for LargeScale Ideation. Retrieved March 11, 2016 from
http://joelchan.me/files/2016-chi-sensemaking-ideation.pdf
4. Joel Chan, Steven Dang, and Steven P. Dow. 2016.
Improving Crowd Innovation with Expert Facilitation.
5. Lydia B. Chilton, Greg Little, Darren Edge, Daniel S.
Weld, and James A. Landay. 2013. Cascade: Crowdsourcing
taxonomy creation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, 1999–
2008.
Retrieved
January
11,
2016
from
http://dl.acm.org/citation.cfm?id=2466265
6. Arthur Cropley. 2006. In praise of convergent thinking.
Creativity research journal 18, 3: 391–404.
7. Alan R. Dennis and Joseph S. Valacich. 1993. Computer
brainstorms: More heads are better than one. Journal of
applied psychology 78, 4: 531.
8. Alan R. Dennis and Mike L. Williams. 2003. Electronic
Brainstorming: Theory, Research, and Future Directions. In
Group creativity: Innovation through collaboration. Oxford
University Press.
9. Michael Diehl and Wolfgang Stroebe. 1987.
Productivity loss in brainstorming groups: Toward the
solution of a riddle. Journal of Personality and Social
Psychology 53, 3: 497–509. https://doi.org/10.1037/00223514.53.3.497
10. Karen Leggett Dugosh, Paul B. Paulus, Evelyn J.
Roland, and Huei-Chuan Yang. 2000. Cognitive stimulation
in brainstorming. Journal of Personality and Social
Psychology 79, 5: 722–735. https://doi.org/10.1037/00223514.79.5.722
11. Beth A. Hennessey and Teresa M. Amabile. 2010.
Creativity. Annual Review of Psychology 61, 1: 569–598.
https://doi.org/10.1146/annurev.psych.093008.100416
12. Alex Ivanov and Dianne Cyr. 2006. The Concept Plot: a
concept mapping visualization tool for asynchronous webbased brainstorming sessions. Information Visualization 5,
3: 185–191. https://doi.org/10.1057/palgrave.ivs.9500130
13. D. G. Jansson and S. M. Smith. 1991. Design Fixation.
Design Studies 12, 1: 3–11.
14. Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk. In
Proceedings of the SIGCHI conference on human factors in
computing systems, 453–456. Retrieved August 17, 2015
from http://dl.acm.org/citation.cfm?id=1357127
15. Aniket Kittur, Boris Smus, Susheel Khamkar, and

1853

Crowd-powered Systems

CHI 2017, May 6–11, 2017, Denver, CO, USA

Krzysztof Z. Gajos. 2016. IdeaHound: Improving Largescale Collaborative Ideation with Crowd-Powered Real-time
Semantic
Modeling.
609–624.
https://doi.org/10.1145/2984511.2984578
29. Pao Siangliulue, Joel Chan, Krzysztof Z. Gajos, and
Steven P. Dow. 2015. Providing Timely Examples Improves
the Quantity and Quality of Generated Ideas. 83–92.
https://doi.org/10.1145/2757226.2757230
30. Steven M. Smith. 2003. The constraining effects of
initial ideas. In Group creativity: Innovation through
collaboration, Paul B. Paulus and Bernard A. Nijstad (eds.).
Oxford University Press, New York, NY, US, 15–31.

31. Lixiu Yu, Aniket Kittur, and Robert E. Kraut. 2014.
Distributed analogical idea generation: inventing with
crowds.
1245–1254.
https://doi.org/10.1145/2556288.2557371
32. Lixiu Yu, Aniket Kittur, and Robert E. Kraut. 2014.
Searching for analogical ideas with crowds. 1225–1234.
https://doi.org/10.1145/2556288.2557378
33. Lixiu Yu and Jeffrey V. Nickerson. 2011. Cooks or
cobblers?: crowd creativity through combination. In
Proceedings of the SIGCHI conference on human factors in
computing systems, 1393–1402. Retrieved October 12, 2015
from http://dl.acm.org/citation.cfm?id=1979147

1854

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

Self-Experimentation for Behavior Change:
Design and Formative Evaluation of Two Approaches
Jisoo Lee
School of Arts, Media and
Engineering
Arizona State University
jisoo.lee@asu.edu

Erin Walker
School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
erin.a.walker@asu.edu

Winslow Burleson
Rory Meyers College of Nursing
New York University
wb50@nyu.edu

Matthew Kay
Computer Science & Engineering | dub
University of Washington
mjskay@cs.washington.edu

Matthew Buman
School of Nutrition and Health
Promotion
Arizona State University
mbuman@asu.edu

Eric B. Hekler
School of Nutrition and Health
Promotion
Arizona State University
ehekler@asu.edu

ABSTRACT

productivity, and wellbeing [39,50]. For example, daily
brushing is important for oral health [1] and regular physical activity can reduce risk of cardiovascular disease, obesity, and colon cancer [50]. Patients with Type 2 diabetes are
recommended a number of behaviors such as monitoring
glucose, taking medications, physical activity, and eating
low sugar diets [17]. However, it is common for individuals
to struggle with initiating and sustaining health behaviors
[60]. This issue has inspired a large effort in the humancomputer interaction (HCI) community to generate plausible solutions for supporting behavior change [e.g., 10].

Desirable outcomes such as health are tightly linked to behaviors, thus inspiring research on technologies that support
people in changing those behaviors. Many behavior-change
technologies are designed by HCI experts but this approach
can make it difficult to personalize support to each user’s
unique goals and needs. This paper reports on the iterative
design of two complementary support strategies for helping
users create their own personalized behavior-change plans
via self-experimentation: One emphasized the use of interactive instructional materials, and the other additionally
introduced context-aware computing to enable user creation
of “just in time” home-based interventions. In a formative
trial with 27 users, we compared these two approaches to an
unstructured sleep education control. Results suggest great
promise in both strategies and provide insights on how to
develop personalized behavior-change technologies.

Many of these behavior-change technologies (see related
work) are designed, implemented, and evaluated by experts.
An alternative and complementary approach for supporting
more personalized and precise behavior change could be to
help individuals create their own behavior change plans.
Behavior change plans are the approaches a person takes to
initiate and maintain a desired behavior, including the use
of behavior-change techniques from the scientific literature
but also, plausibly, other self-created approaches. This selfcreation approach is linked to the Quantified Self (QS)
movement, where individuals work to better understand
themselves through self-tracking/self-study, including
methods that they create [9,35]. Choe et al. [9] found that
“Q-Selfers often described the process of seeking answers
as self-experimentation. When used in an academic context,
self-experimentation means participating in one’s own experiments when recruiting other participants is not feasible.
However, in QS, the goal of self-experimentation is not to
find generalizable knowledge, but to find meaningful selfknowledge that matters to individuals.“ A number of studies have been conducted in the HCI community focused on
providing improved resources for self-experimentation,
such as data collection and interpretation tools [36].

Author Keywords

Behavior change; self-experimentation; just-in-time interventions; context-aware computing
ACM Classification Keywords

H.5.2. User Interfaces: User-Centered Design; Theory &
Methods; J.4 Computer Applications; Social & Behavioral
Sciences.
INTRODUCTION

Extensive evidence suggests the importance of people’s
sustained engagement in behaviors to improve health,
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice
and the full citation on the first page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee. Request permissions
from Permissions@acm.org.
CHI 2017, May 06 - 11, 2017, Denver, CO, USA
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-4655-9/17/05…$15.00
DOI: http://dx.doi.org/10.1145/3025453.3026038

In this paper, we explore theoretically grounded mechanisms for supporting users’ self-experimentation. Karkar et
al. define self-experimentation as requiring three phases:
formulating a hypothesis, testing the hypothesis with N-of-1
6837

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

trial designs, and examining the results of the study [24].
Our work extends the concept of self-experimentation to the
systematic study of the behavior-change plans one could
use to initiate and maintain health behaviors, which we label self-experimentation for behavior change. We investigated two approaches for facilitating self-experimentation
for behavior change. First, we designed interactive instructional materials to support users in the creation of
and experimentation with behavior-change plans. This approach focuses on giving users tools to design and implement behavior-change plans compatible with their goals
and lifestyle. Second, we used end-user programmable
sensing and feedback to support the design of “just-intime” (JIT) interventions, which provide triggers to engage in a desired behavior during states when a person has
both the opportunity to engage in the behavior and the receptivity to interact with the system [48]. Just-in-time interventions are a logical target for self-experimentation for
behavior change because JIT strategies are often contextsensitive and idiosyncratic. For example, if a person is trying to improve diet, a JIT intervention requires insights on
when, where, with whom, and in what state (e.g., stresseating) a person may be in when eating too much to define
the JIT states when a prompt would actually be helpful.

reflection on daily experiences, by providing abstract body
figures that represented movement and arousal levels
throughout the day. MAHI [42] provided a website where
diabetes patients and their educators communicated via
diaries, with an explicit goal of fostering improved reflective skills among the patients. Li asserted the usefulness of
users’ exploration of multiple types of contextual and behavioral information in a single interface to support identification of factors that affect behavior [36]. Bentley and his
colleagues [3] created a system that automatically finds
correlations between a variety of contextual factors (weight,
sleep, step count, etc.) and people’s health and wellbeing.
Another popular strategy for supporting behavior change
involves goal-setting and self-monitoring. In UbiFit [10]
users were invited to establish a weekly goal for various
activities (Cardio, Strength, Flexibility) and then provided
feedback via the growth of a virtual garden. Fish'n'Steps
[38] invites users to set their daily step goal, gathers player’s step counts, and presents users’ activity achievements
via changes to a virtual character such as growth and facial
expressions. In Kunini [6], players set goals that required
them to run specific distances or paces before a specific
date. More recently, Rabbi et al. [53] explored a systemdriven personalization approach to goal-setting within
MyBehavior. Specifically, MyBehavior gives suggestions
on physical activity and dietary behavior based on continuously collected information on each user’s behavior. In addition to goal-setting and self-monitoring, HCI has adopted
a broad range of concepts from behavioral theory, including
just-in-time information (e.g., Nawyn et al. [49]), priming
(e.g., Consolvo et al. [10]), social validation (e.g., Toscos et
al. [57]), and behavioral economics (e.g., Lee et al. [34]).

In this paper, we first describe prior work in HCI focused
on behavior-change technologies. Next, we describe our
iterative design process in the creation of our interactive
instructional materials and our context-aware JIT intervention system. We then report on a 7-week formative evaluation, which tests these two approaches for improving sleep
relative to a sleep education control. We hypothesized that
both self-experimentation for behavior change approaches
would produce significantly improved sleep relative to an
unstructured self-experimentation/education-only control
condition. The key contributions of our study include:

These examples involve HCI researchers encapsulating
behavior-change techniques into a technical system for users. Behavior-change techniques are “observable, replicable, and irreducible component[s] of a [behavioral] intervention designed to alter or regulate behavior; that is, a
technique is proposed to be an ‘active ingredient’ (e.g.,
feedback, self-monitoring, and reinforcement)” [45]. While
users may benefit from this exposure to behavior-change
techniques, they may feel a lack of agency in the implementation of these techniques, or may feel as though the techniques are not relevant to their individual needs. For example, King et al. [26] developed three smartphone apps focused on improving mid-life and older adults’ physical activity that used different behavior-change techniques (e.g.
social, analytic, or more game-like). Qualitative work suggested many individuals requested a “mix-and-match” approach at different times, thus reinforcing the need for personalization over time.

1) Empirical results in favor of our structured selfexperimentation for behavior change strategies for improving sleep relative to our unstructured control.
2) Concrete suggestions for personalization of behavior
change interventions via self-experimentation. These
suggestions generalize to other interventions attempting
to scaffold self-experimentation for behavior change.
3) The use of a Bayesian statistical approach to conduct our
formative evaluation, extending previous work [24]. This
concrete use-case of Bayesian statistics for formative
work provides details on what is gained from these analyses and can serve as a template for the HCI community.
RELATED WORK
Behavior-Change Technologies in HCI

HCI has become increasingly interested in studying the use
of computing technology to promote behavior change
[15,21]. A key approach has focused on improving a users’
self-awareness, typically via sensing technologies for selftracking and feedback from data. For example, Affective
Diary [55] facilitated users’ affective interpretation and

Self-Experimentation & Behavior Change

There is great opportunity for creating tools that enable the
end-user in selection and personalization of behaviorchange techniques [21,46]. This aligns with practices Quantified Selfers’ engage in when attempting to find answers

6838

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

for themselves. A number of studies have investigated
Quantified Selfers’ self-tracking needs and methods for
data collection and analysis to empower end-users in their
own self-discovery process [9,35]. To enhance such selfdiscovery, Karkar et al. [24] established a framework for
self-experimentation that includes formulating a hypothesis,
conducting rigorous n-of-1 trials to support evidence-based
decision-making, and reviewing results to facilitate further
learning and study. There framework was developed for
users interested in understanding how various actions (e.g.,
foods eaten) impact a symptom or other clinically-relevant
outcome for that individual (e.g., irritable bowel syndrome
symptoms). More recently, SleepCoacher [13] provides
personalized recommendations related to sleep based on a
person’s self-tracking data (e.g., “…when your bedtime is
consistent you tend to fall asleep faster. For the next {N}
days, try going to bed at a consistent bedtime, around
{N}am/pm”) and to then test that recommendation with a nof-1 trial. Results from this work highlight how experimenting on recommendations can improve sleep. A complementary set of tools could support individuals in the selection
and personalization of behavior-change techniques and
then, through self-experimentation, examine if those techniques help an individual initiate and maintain a behavior,
which we call self-experimentation for behavior change.

on a task). Poppinga et al. [52] identified factors (time of
day, phone position) that provide insights on receptivity and
found that notifications are more likely to be answered before 8:21 a.m. and after 8:20 p.m., but not late at night,
while the phone is in the person’s hand. Theoretically, justin-time notifications are meant to inspire action in that moment and, by extension, rely less on memory and selfcontrol [47]. Further, based on the high likelihood that defining a JIT moment is both highly context-dependent and
idiosyncratic [22], a plausible and, as of yet, under-studied
area, could be in the development of technology that supports end-users in creating JIT interventions.
There is a long tradition in end-user development research
for technologies like JIT interventions [8,37]. End-user development is characterized by the use of techniques that
allow non-technical people to create applications [11]. This
strategy is valuable when the problem space could benefit
from intimate knowledge about activities and environments
to design useful solutions such as context-aware applications [14]. Accordingly, there has been considerable research on end-user development tools for creation of context-aware applications in home environments [14,18].
However, most existing tools for users’ creation of contextaware applications are to support control of appliances or
environmental equipment [14,18]. Little attention has been
given to the provision of toolkits focused on behavior
change, thus establishing the key gap filled by our work.

Self-experimentation for behavior change is distinct and
complementary to Karkar’s self-experimentation for discovery. Karkar’s framework provides a thoughtful strategy
for helping an individual to select which behaviors to
change to produce a desired outcome (e.g., better mood,
reduced stress, reduced symptoms). After a person knows
what they “should” do, a separate process is needed to
study how to change and maintain the targeted behavior
over time. A great deal of prior work highlights that there is
a gap between what individuals intend to do vs. what they
actually do [54], thus establishing the need for selfexperimentation for behavior change. The creation and
formative evaluation of approaches for self-experimentation
for behavior change is the core focus of our work.

ITERATIVE
DESIGN
OF
INSTRUCTIONAL MATERIALS

THE

INTERACTIVE

In this section, we describe the creation and iterative testing
of interactive instructional materials to support users in selfexperimentation for behavior change. More specifically, we
conducted two user studies to create the protocol used in
our formative evaluation trial. For details of each iteration,
see [31,32]. This formative work revealed several design
strategies one could take to support goal-setting, the selection of behavior-change techniques, and self-monitoring.
As a starting point, we define self-experimentation for
behavior change as a process executed by users to formulate, test, and iterate on hypotheses related to how well behavior plans can produce desired behavioral outcomes, including behavioral initiation and maintenance. Formulating,
testing, and iterating on a behavioral plan corresponds to
the three phases of self-experimentation delineated by
Karkar et al. but, in this context, the focus is on testing a
behavioral plan, not on testing if an action influences an
outcome. A behavioral plan includes: 1) a goal; 2) a consciously chosen behavior-change technique(s) that is personalized by and for the individual; and 3) self-monitoring
of the behavioral target to examine if the behavior-change
technique fostered achievement of the goal.

Self-Experimentation & Context-Aware Computing

One interesting strategy for supporting personalized behavior change could involve end-user programming of contextaware computing systems to enable individuals in the creation of their own personalized just-in-time interventions.
Just-in-time adaptive interventions are an emerging class of
behavioral interventions focused on providing support during the “just-in-time” moments when a person is vulnerable
to engaging in negative behaviors and/or whenever opportunities for positive changes arise while also being receptive
to interacting with a system [48]. For instance, SitCoach
[59] is an intervention that provides office workers with a
message to be physically active whenever the computer has
detected 30-minutes of non-active work. For this message
to be useful, an individual needs to both have the opportunity to be active soon after receiving the message and is receptive to receiving the message (e.g., not deep in thought

Based on personal experience with training students in this
approach within a class setting, our formative work [31,32],
and design practices common in HCI, we explicitly did not
include n-of-1 trials (e.g., [28]) within our 7-week trial. The
6839

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

key reason was because we have found that introduction of
n-of-1 trials too early into a design process can have the
unintended consequence of undermining the creative process, which is essential when formulating a hypothesis
about a useful behavioral plan. We discuss this in detail in
the discussion. To simplify the design process, we explicitly
chose to develop highly structured protocols administered
by a research assistant, which eventually could be used to
design an interactive digital tutorial. This protocol was fully
scripted and images were provided in succession via a
presentation. The protocol had five steps: 1) Choosing a
behavior to attempt to change (the target behavior), 2) Setting a goal, 3) Generating ideas for attainment of the goal
by applying behavior-change techniques, 4) Formulating a
final plan, consisting of one or more complementary behavior-change techniques, and 5) Devising self-tracking
measures to determine if the goal was accomplished.

slide). In the second session, users were presented with the
meta-model phrase and each technique they tried the previous week was linked to the meta-model. They were then
asked to self-diagnose if there might be a lack of opportunity, ability, motivation, or triggers when it comes to enacting
their behavioral goal. Following this self-diagnosis, users
were presented with additional behavior-change techniques
for the diagnosed problem domain (e.g., given all of the
opportunity behavior-change techniques). Note that if users
self-diagnosed the problem as including multiple domains
(e.g., both opportunity and motivation), then they were provided with both sets of added behavior-change techniques.
In terms of self-monitoring, our formative work indicated
the value of providing individuals with two types of selfmonitoring; open-ended journaling, which was particularly
valuable for generating hypotheses about behavioral plans,
and phone-based surveys with clear quantification of the
target outcome(s) and any process variables (e.g., stress)
that they thought might influence their outcome. We did not
change this strategy in the second iteration.

Our first test of this protocol included 2 sessions. In the
first, users completed the above five steps. In the second,
one week later, users reviewed results of their implementation of the behavioral plan including reviewing their selftracking data. Results of our first user study revealed two
problems. First, individuals generated under-specified goals
that were not actionable. Second, individuals did not use the
behavior-change techniques provided in the training (e.g.,
they often used the first behavior-change idea they came up
with rather than the evidence-based suggestions).

We conducted a second test, this time using a 3-week protocol to better understand users’ iterative process. As described elsewhere [31,32], these changes resulted in better
specified goals, improved engagement with behaviorchange techniques, and increased iteration in terms of further personalizing the behavior-change techniques chosen.

Based on these results, we refined the protocol in two ways.
First, during goal creation (Step 2), we included the concept
of SMART goals [30]. The SMART (Specific, Measurable,
Actionable, Realistic, and Timely) goal concept is a reinterpretation of Locke and Latham’s goal setting theory [41].
According to this concept, goals that meet each of the acronym’s words (e.g., specific, measurable) will be more useful for behavior change. During Step 2, users generated
SMART goals by: (1) Reflecting on the issues they wanted
to work on, (2) learning about the concept of ‘behavioral
goals,’ in contrast to ‘outcome goals’, and (3) learning
about the concept of a SMART goal with instructions on
how to create one (Figure 1).

Figure 1. Exemplar slides of the prototype guiding users’ behavioral plan creation.
JUST-IN-TIME INTERVENTION TOOL

Our second self-experimentation for behavior change strategy focused on the self-creation and testing of JIT interventions using context-aware computing. We developed a context-sensitive application, which integrates off-the-shelf
hardware and software to support the creation of contextaware JIT interventions. The system enables rapid prototyping of simple rule-based systems that include physical sensing, data storage, and media event components. Scripted
sequences of media events are triggered based on time,
sensed activity, and/or history of behavior.

Second, during step 3 (behavior-change techniques), we
provided an organizing structure to help individuals in the
selection and personalization of behavior-change techniques. We leveraged two existing meta-models, Fogg’s
behavior model [16], and Michie’s COM-B model [44],
which were developed to help professionals create interventions. We simplified these models for the purposes of the
instructional materials to a phrase, “A behavior occurs
when, opportunity, ability, motivation, and a trigger all
align.” As part of this, we provided definitions for each
concept. To support iteration and “self-diagnosis” of factors
that impact behavioral initiation and maintenance, during
the first session, users were provided only a single behavior-change technique from each domain (opportunity, motivation, ability, and trigger, see Figure 1 for one example

We favored a rule-based approach due to its logical simplicity and flexibility across situations [14,18,58]. For example, a rule for detecting meal preparation might be: ‘IF
resident is in the kitchen AND (resident accessed cupboard
AND resident accessed plates OR utensils cabinet) OR resident used an appliance THEN a meal was prepared’ [12].
For sensing, we adopted X10 (www.x10.com) and Insteon
(www.insteon.net) home automation sensors. Currently, the
tool supports X10 wireless open/closed magnetic sensors,
6840

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

X10 wireless motion sensors, and Insteon on/off modules
(see Figure 2). The JIT intervention toolkit also includes
two types of prompting methods: audio content via wireless
speakers and text messages via mobile devices. The audio
prompts can include machine speech of user-inputted text
or it can play user-added/selected sound files (e.g., music or
other mp3 files). The use of sound as a prompt has several
advantages over visual display. First, audio can catch a person’s attention even if the user is not looking at the device.
In addition, sound, especially music, is well known to influence emotions [27,40]. However, the audio prompts are
limited by the need for individuals to be near the speakers.
We additionally used text messages for prompting users for
those times when a person is not near a speaker but has
their phone [51]. The commercially available home automation software, Indigo (www.indigodomo.com), was employed for hardware communication (i.e., Apple computer,
X10 and Insteon sensors, Apple wireless speaker system,
mobile phones) and application programming.

and 3) Sleep Hygiene+JIT Self-Experimentation for Behavior Change Tutorial+JIT Intervention Tool (SH+SBT+JIT).
We hypothesized both self-experimentation for behavior
change approaches would improve sleep quality compared
to the SH control over 7 weeks. Our primary outcome was
the Pittsburgh Sleep Quality Inventory (PSQI) [5], but we
also collected daily self-reported sleep satisfaction and activity-monitor-measured sleep duration. A priori, we did not
anticipate major shifts in sleep duration as the trial was only
7 weeks long, which is often too short for improvements.
Methods
Users

We recruited users with sleep complaints but no diagnosed
sleep disorder. Users were informed that they would be
given a sleep and activity monitor (i.e., the Jawbone UP
Move) for participating in the study. Inclusion criteria included: 1) significant complaints with their sleep; 2) a
smartphone (i.e,. Android or iPhone) to be used to gather
self-tracking data via the app Paco; and 3) no plans to travel
during the 7 weeks of the study. Exclusion criteria included:
1) diagnosed sleep disorder; 2) co-sleeping with someone
else in the same bed/bedroom; and 3) disruptive and uncontrollable sleep schedules, such as night shift workers.
In total, 27 users (14 male, 13 female) were enrolled. Unfortunately, despite random assignment, distribution of ages
was not balanced (see Table 1). While the majority of users
were students (N=19), there was an imbalance in students
vs. non-students (i.e., 8 non-students in SH, 6 in SH+SBT
and 5 in SH+SBT+JIT conditions). Participants’ survey
responses to questions on perceived difficulty with sleep
habits, importance of good sleep, and belief their sleep must
be fixed (on a 7-point Likert scale) during session 1 indicated high motivation to improve sleep, as shown in Table 2.

Figure 2. Sensor Use Examples; X10 motion sensor to detect
users’ taking a book (the left), X10 door sensor to detect users’
opening the refrigerator (the middle), X10 door sensor to detect start/end of the laundry (the right).
FORMATIVE EVALUATION STUDY
Overview

We conducted a formative evaluation of the two selfexperimentation for behavior change approaches on users’
sleep quality. We chose sleep because sleep is an essential
factor that affects an individual’s physical vitality, emotional balance, and productivity, poor sleep is common, and
various factors influence sleep such as other behaviors (e.g.,
bedtime, diet), psychological disturbances, pain, medical
conditions, genetic factors, stress, age, physiological and
cognitive arousal [29]. Further, sleep hygiene includes wellresearched behavioral strategies to improve sleep [29], including: 1) Go to bed and get up at the same time each day;
2) Avoid napping; 3) Have a regular schedule for meals,
medications, chores, and other activities; 4) Avoid stimulants such as caffeine (e.g., coffee, chocolate), nicotine, and
alcohol near bedtime; and 5) Stay away from large meals
near bedtime. Previous research suggests knowledge of
sleep hygiene alone does not translate to improved sleep
[56], thus making sleep hygiene a good control as it provides likely new information that will inspire the sort of
unstructured self-experimentation common by QS’ers.

Age
range

SH

SH-SBT

SH-SBT-JIT

Male

Female

Male

Female

Male

Female

18-20

1

0

0

1

2

0

21-29

4

3

3

2

0

2

30-39

0

1

1

1

2

0

40-49

0

0

0

1

0

1

50-59

0

0

0

0

1

1

Table 1. Users’ age distribution.

We randomly assigned users to one of three conditions: 1)
Sleep Hygiene alone (SH), 2) Sleep Hygiene+SelfExperimentation for Behavior Change Tutorial (SH+SBT),

SH

SH-SBT

SH-SBT-JIT

Difficulty with
sleep habits

5.1 (.9)

5.6 (1.2)

5 (1.6)

Importance of
good sleep

6.2 (.8)

6.1 (.8)

6.2 (1)

Belief sleep
must be fixed

6.1 (.8)

5.8 (.8)

5.9 (1.2)

Table 2. Participants’ motivation (1-strongly disagree to 7
strongly agree; Mean (SD).

6841

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

Protocol & study design

All users met with research personnel for 5 sessions (see
Table 3), received sleep hygiene information, and used the
self-tracking strategies (see measures).
Session 1 Session 2
SelfInitial
tracking
creation
tools setup

Session 3
First
revision

Session 4
Second
revision

Session 5
Wrapping
up

Figure 4. Example ideation of JIT applications.

Sessions 3, 4, & 5 started with a review of self-tracking
data. They completed survey questions about normality of
daily life, sleep, and plan implementation. Then they reported verbally with related cues (e.g., “How was your
sleep?”’ “Tell me how you carried out your plans?”).

Table 3. Study procedure.

The SH group developed a behavioral plan for improving
sleep with only information about sleep hygiene. During
each subsequent section, they were asked to report what
they observed based on self-tracking and then asked to
change their plans as appropriate, again without any additional support. This was done to create the sort of unstructured self-experimentation common by QS’ers, while
controlling for in-person interactions and education. The
SH+SBT and SH+SBT+JIT conditions were trained in selfexperimentation for behavior change based on the interactive instructional materials described earlier. Both of these
self-experimentation groups were provided with worksheets
to generate goals and ideas (see Figure 3).

Measures

Our primary outcome measure was the PSQI [5]. The PSQI
is a well-validated and extensively used measure of overall
sleep quality that is used in a wide range of sleep research.
The PSQI items (i.e., 7 components including subjective
sleep quality, sleep latency, sleep duration, sleep efficiency,
sleep disturbance, use of sleep medication, daytime dysfunction) correspond to the behavioral changes that our
interventions and SH should influence. All users completed
this questionnaire each session.
We also collected daily self-tracked sleep quality via a daily
sleep diary and the wristband activity and sleep tracker, UP
Move made by Jawbone (www.jawbone.com). We consider
these secondary outcomes as the PSQI is a well regarded
measure within the sleep research community. Further, as is
common within behavioral evaluations, the self-tracked
data was conceptualized as part of the intervention whereas
the PSQI remained separate for a clean evaluation.

Figure 3. Users’ worksheets.

The sleep diary, which is used in sleep studies, included
four questions asked every morning: 1) when did you go to
bed? 2) how long did it take for you to fall asleep? 3) when
did you wake up?, and 4) how satisfied were you with your
sleep (rate from 1 to 10, higher scores are better)? Users
installed PACO (www.pacoapp.com), which triggers a reminder inviting a user to answer the survey.

For users in the SH+SBT+JIT condition, the interactive
instruction was customized with content specifically aimed
at JIT interventions. This group was taught the concept of
JIT interventions and context-aware computing, and received JIT examples of behavior-change techniques. For
each example, we used a slideshow to describe the targeted
behavior, the behavior-change technique used, and a series
of images showing how the technique could be implemented in a real-world context via our JIT tool. Users were
asked to generate their ideas of the rules for implementation
within the tool using sticky notes (Figure 4). We introduced
this format to reinforce the rules-based logic and to enable
easy rule changes via post-it note movement.

The Up Move collects a variety of data including total sleep
time. As we did not intend to assess users’ physical activity
level, use of the activity tracking function during the day
was optional but not required. All users were asked to
download the Jawbone app to enable syncing.
All sessions were video recorded and user-generated materials were collected including worksheets, JIT app ideation
using sticky notes, and notes of behavioral plans.

After each session, all participants were asked to write up
their plan in an email and to send it to themselves as a reminder during the 2-week testing period. The SH group was
asked to describe ‘Things to do’, and the other two conditions sent ‘Goals’ and ‘Plans’. For the SH+SBT+JIT group,
initial application descriptions were written by the researcher, and revised by users. The researcher developed
JIT tool applications and installed them at users’ homes.
Participants were told that they could check past selftracked data whenever they found it necessary.

Analysis

We used Bayesian analysis instead of frequentist analysis,
because it offers a more principled way to handle the uncertainty in small formative trials [19,25]. Our primary outcome was sleep quality as measured via the PSQI. Results
from our secondary outcomes (daily sleep satisfaction and
duration) can be found in our online supplement. We estimated the difference between the baseline and the phase 3
6842

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

(final phase) outcomes for each measure. For the PSQI
model, we used the estimated difference from baseline
(measured at session 2) to phase 3 (measured at session 5)
for each user as the response. All models included fixed
effects for condition: the control (SH) and both intervention
conditions (SH+SBT and SH+SBT+JIT).

Control

SH-SBT-JIT

20

15

10

We constructed Bayesian models using robust mixed effects linear regression.1 Inspired by Howard et al. [23], for
each outcome we ran three Bayesian analyses: an uninformed analysis, a skeptical analysis, and an optimistic
analysis. These analyses differ only in the priors we set.
The uninformed model uses uninformed priors, which produce estimates similar to frequentist models, which enables
incorporation of beliefs postulated by a frequentist approach into our final averaged estimate. The skeptical priors
are centered at zero (i.e. they assume it is most likely the
treatment has no effect) and roughly cover the range of
plausible effect sizes (e.g. reductions in PSQI of 4 or more
are rarely seen for interventions like this; effects this size
and larger have very low probability in our skeptical prior).
Our optimistic priors assume the intervention will have a
clinically meaningful effect around the size seen in the literature (i.e. a reduction of about 2 points on the PSQI) but
not larger; thus, even this prior gives low probability to a
reduction in PSQI of 4 points or more. The skeptical and
optimistic priors were set by one of the authors who has
expertise in sleep (see online supplement for full details).

5

PSQI 0
s1

s2

s3

s4

s5

s1

s2

s3

s4

s5

s1

s2

s3

s4

s5

Session

Figure 5. PSQI changes in each group. Each grey line shows
one user. The blue line shows the mean for each session with
95% CI. S2 is our true baseline as measurement only occurred
between s1 and s2 to help individuals generate more realistic
PSQI estimates during s2.

The frequentist models suggest significant within-group
changes for all conditions as indicated by the 95% confidence intervals not including 0 in the left column of Figure
6 (equivalent to p < 0.05). The central effect size estimates
are approximately SH=-2, SH+SBT+JIT=-3, & SH+SBT=
-4. These indicate clinically (and likely unrealistically)
large effect sizes, which suggests that these models may be
subject to a magnitude error, the tendency of small trials to
overestimate effect sizes in frequentist analysis [20]. Results of the between-group comparisons based on the frequentist analyses revealed no significant differences between groups, but there are wide confidence intervals.

As a final step, we average all three Bayesian models,
weighted by the WAIC of each model (the WidelyApplicable Information Criterion, an estimate of out-ofsample prediction error). Models that are better at predicting the data out-of-sample are weighted higher. After
McElreath [43], this approach acknowledges the uncertainty we have in our models (including our prior beliefs), and
is applicable to small-sample studies.

The plausibility of different effect sizes based on different
prior beliefs are shown as probability distributions in light
blue above the credibility intervals in Figure 6 (taller indicates increased probability that the effect is that size, such
as -2). The final averaged Bayesian model suggests a very
high probability (over 99% chance) that all conditions produced a change from baseline to phase 3 that was less than
0 (lower is better for PSQI). Our aggregate model estimates
a 35% chance of reduction of 2 or more and 80% chance of
reduction of 1 or more in the control, suggesting a moderate
effect. Further, there was a high probability of a clinically
significant effect for the SH+SBT and SH+SBT+JIT conditions (a 95% and 92% respective chance of reduction in
PSQI of 2 or more; left column of Figure 6). There is also a
good chance that there was some greater reduction in PSQI
for both interventions than in the control (~90% chance that
the between-group effects are < 0; right column) and an OK
chance that those additional reductions are meaningful
(~75% chance the differences are < -1) but a low chance
that the effects are large (~35% chance that the differences
are <-2). Finally, the estimated difference in PSQI of
SH+SBT+JIT – SH+SBT is 0.05 with a wide 95% credibility interval (-1.73 to 2.45). There is a 73% chance that the
difference in reduction between the two is 1 or less (i.e.,
that they have outcomes of similar clinical significance).

We also report frequentist results to establish a comparator
to statistics commonly reported. Our frequentist models
used linear regression/ mixed effects linear regression 2 to
examine the within-group changes over time and the between-group differences in our three target outcomes.
Outcome Results

Descriptive trends, prior to any statistical analyses of the
PSQI, are visualized in Figure 5. These descriptives suggest
that all groups reduced their PSQI scores (lower scores are
better on the PSQI). These within-group changes for all
conditions were confirmed in all of our models including
our final averaged model.

1

brms package in R [4], which uses the Stan modeling language
[7], and a Student t error distribution in place of a Normal error
distribution for robustness [33].
2

SH-SBT

lme4 package in R [2]
6843

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

Change in PSQI
Model

Control

Frequentist

Satisfying

Posterior
Prior

Skeptical
Bayesian

Additional change in
PSQI for treatments
compared to control

Optimistic
Bayesian
WAICaveraged

Fitting

(phase 3 − baseline | treatment) −
(phase 3 − baseline | control)

Essential

SH-SBT

Frequentist

SH-SBT-JIT

5.9 (1.3)

6 (1.5)

Session 4-Session 2

.8 (1.0)

0.0 (.5)

Session 4

6.0 (1.0)

6.4 (1.1)

Session 4-Session 2

.3 (1.1)

.5 (1.1)

Session 4

6.3 (.9)

6.3 (1.2)

Session 4-Session 2

.9 (1.3)

.6 (1.1)

Table 5. Evaluation on created plans (1-strongly disagree to 7
strongly agree; Mean (SD)).

Skeptical
Bayesian
Optimistic
Bayesian

One key difference between the interventions and control,
which is in line with our goals, was that the behavioral
plans within the SH-SBT and SH-SBT-JIT conditions were
more specific. For example:

WAICaveraged
Frequentist

SH-SBT-JIT

SH-SBT
Session 4

(phase 3 − baseline)

P28 (SH), Session 2
Go to bed between 10 and 11 pm, and wake up between
5:30 and 7 am. In the morning, don’t go back to bed.
Don’t take a nap; Avoid stimulants near bedtime.
P08 (SH-SBT), Session 2
Goals: Go to bed at 11-11:30 pm
Wake up at 8:30 am
Plans: Set up calendar reminder at 10:30 pm to get ready
for bed; Turn off electronics at that time; Start getting
ready for bed; Relax before sleep; Don't use electronics
before bed; Keep computer on desk; Reward: if I go to
bed on time for a week, go shopping
P09 (SH-SBT-JIT), Session 2,
Goals: No phone use near bedtime. 10 PM.
Open curtains in morning. Keep room light.
Plans: Remove chargers earlier than 10 PM, and
transport all devices and chargers into another room,
and finish up any tasks related to the computer within the
guest room, and do not take back any of the devices to
your room; When waking up at 8:00 AM, as a first activity, walk to curtains and open them to ensure a steady
flow of natural light.

Skeptical
Bayesian
Optimistic
Bayesian
WAICaveraged
-6 -5 -4 -3 -2 -1 0 1 2

-6 -5 -4 -3 -2 -1 0 1 2

Difference in PSQI (lower is better)

Figure 6. Results from our PSQI analyses with mode and 95%
confidence intervals (95% quantile credibility intervals for
Bayesian models). Prior and posterior probability distributions are shown where applicable. The uninformed Bayesian
model (omitted) has estimates similar to the frequentist. The
WAIC-averaged model is weighted 26% on the uninformed,
21% on the skeptical, & 53% on the optimistic model.
Process Results
Behavioral plan quality

Across conditions, users selected similar goals based on
sleep hygiene suggestions. Three goals were particularly
popular, ‘Adjusting/sticking to a sleep schedule’ (16 users),
‘Doing relaxing routines near bedtime’ (15 users) and
‘Physical activity’ (12 users). We asked all participants to
rate how well they achieved their goals on a 0 to 10 scale,
and the result shown in Table 4 indicates improvement.
SH

SH-SBT

SH-SBT-JIT

Session 5

6.4 (2.9)

6.8 (1.8)

8.4 (1.2)

Session 5–Session 3

.8 (1.2)

2.9 (2.4)

1.9 (2.0)

This pattern continued as users made revisions in Sessions
3 and 4. In the SH control, users did not typically change
plans but, instead, tried different sleep hygiene strategies.
Specifically, in session 3, 6 users added 1 or 2 new goals
and 4 removed 1 or 2 goals. In session 4, 3 users added 1
new goal and 4 users removed 1 goal. In contrast, in the
SH+SBT & SH+SBT+JIT conditions, users did not change
their targeted sleep hygiene behaviors/goals as often. In the
SH-SBT condition, during session 3, 3 users added 1 new
goal and 2 users removed 1. In session 4, 2 added 1 new
goal and 1 user removed 1 goal. Instead, users in the intervention conditions (SH-SBT and SH-SBT-JIT) made
changes to their behavioral plans including adding or
removing behavior-change techniques or modifying and
further personalizing behavior-change techniques, such as
adding elements that had been poorly defined. For instance,
P08 (SH-SBT) specified items to avoid near bedtime (TV,
movies, phone) in Session 3, which was only labeled ‘elec-

Table 4. Goal achievement (1 to 10 with 10 indicating perfectly
meeting the goal, Mean (SD)).

Participants in the interventions were also asked to rate the
perceived quality of plans on 7-point scales: 1) Overall, I
am satisfied with my goal; 2) The plan fits my lifestyle
well; and 3) The plan will be essential for me to achieve my
goal. Results summarized in Table 5 indicate both improvement and near maxed out scale responses.

6844

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

tronics’ in session 2. In Session 3, 4 of the SH condition
users did not make any changes in their plans, while 2 of
the SH-SBT and 1 of the SH-SBT-JIT did not. In Session 4,
5 users in the SH condition did not change their plans,
while 2 users in the SH-SBT and all of the SH-SBT-JIT
changed their plans.

no liquid after 9 PM, she did not create any triggers. In contrast, P27, who did have improved sleep quality, created
triggers for most of his goals. For exercise in the morning,
he designed his application to play music when he entered
the kitchen and to help him eat smaller meals, he made the
application play music when he entered the kitchen after
work. For no coffee after 4pm, he created a SMS reminder
at 4PM. The only behavior he did not create a trigger for
was going to bed between 10:30 and 11:30PM.

Overall, individuals across conditions that created more
realistic, specified, and personalized plans had greater sleep
improvements, though our sample is too small for firm conclusions. For example, P37 (SH) had more specific solutions compared to others in the SH group. In the revisions,
she gradually modified her solutions to be more realistic
and personalized (initial 6:15 AM wake up time changed to
6:45AM and 7AM; initial warm bath with soothing music
changed to reading or writing journal). Her PSQI score improved by 5 (baseline=8, phase 3=3). In contrast, P10 realized the need to define activities during the nighttime in
Session 3 and added ‘Make a relaxing bedtime routine’, but
with no further details. By session 4, she reported that her
bedtime routine was not more relaxing and her PSQI worsened (baseline=12, phase 3=14).

A second explanation could be under-utilization of contextaware computing. Users mostly developed time-based triggers, such as P15 sending himself an SMS at 11AM on
Sunday, saying “Meal plan.” Action-based conditions that
involve sensors (e.g., when opening the refrigerator between 7 and 8 PM, play sound to invite preparation of
snack/lunch for tomorrow) were limited including 3 users
(P15, P31, and P33) who used only time-based triggers. The
participants that used action-based triggers, in general, had
improved sleep quality. For example, P09 had a targeted
goal of establishing a bedtime routine and, as part of that,
developed a series of triggers focused on supporting the
routine. The first trigger at 9 PM was a text message saying
“charge the devices.” At 10 PM if the smartphone was not
being changed, music would play in the bedroom. If the
phone was charged on time (meaning plugged in prior to 10
PM for 3 nights in a row) AND when a person opened a
box of candy THEN happy music would play. If the phone
had not been charged and the box opened then sad music
played. The user also created a trigger at 8 AM to play happy music to invite them to open the blinds.

JIT-specific results

Most participants found JIT support beneficial, with two
general themes. First, the system reminded participants of
goals. It helped to stop preoccupying activity, which has
been continuing longer than necessary: P19, ‘I usually spent
long time using computer [sic]’, P31, ‘Yeah, like if I was
distracted, playing video game or working on the homework, it was nice to get that text message…and then I realize it’s late [sic]…’ Second, it inspired positive emotions.
For example, P13 (who had the music play when she came
home after work, which was designed to remind her about
prep for the next day) stated, ‘not necessarily about
snack/lunch prep. Now you’re are at home… now [I am]
relaxed[sic]’, P03, ‘I really liked the music when I open the
closet, and on Friday mornings. Though I failed in reaching
the exercise goal, it was just fun, good to hear. [sic]’ Despite this positive perception of the JIT support, our quantitative PSQI results indicate a low likelihood that our JIT
component improved sleep beyond our tutorial.

A third plausible explanation for limited response was some
participants engaging in only limited iteration on their JIT
interventions. For example, P27 added a trigger to not drink
coffee after 4 PM in session 3 after realizing that was important but not specified in session 2. Existing triggers were
also modified. For example, P13 added a trigger to an existing one to support going to bed. Initially, only music played
in the living room at 9:45 PM, but in session 3, another
piece of music played in the bedroom 10 minutes later.
Those individuals that engaged in these small tweaks appeared to benefit most from the JIT intervention, thus suggesting the need for more explicit support in iteration.

One possible explanation may have been a misalignment
between triggers and plans. Sometimes, no triggers were
created for a goal. While most users created triggers for the
majority of their targets, P33 included a trigger for only 1
among his 4 targets, and P24, for 2 targets among her 5/7
(depending on session). Users with the greatest sleep quality improvements in the SH+SBT+JIT group appeared to
have better alignment between their plans and triggers. For
example, P24, who made only minimal sleep quality improvement, only incorporated application responses for one
target behavior among six. For waking up, she designed her
application to play peaceful music at 5:45 AM and switch
to loud rock music at 6AM if she did not awake. For her
other behavioral targets including drinking water, no working in bed, increased exercise, relaxation near bedtime, and

DISCUSSION

Results indicate that all three self-experimentation strategies, including our unstructured control, appeared to improve sleep quality over 7 weeks, with the high likelihood
that our two interventions resulted in a small to moderate
improvement in sleep quality relative to the control. Further, results indicated success with goals in all conditions
with greater improvements in goal achievement in the interventions relative to control. Results also indicated near
maxed out scales of positive perceptions of plans created in
the two intervention conditions by the final session. Taken
together, these results suggest the value of our structured
self-experimentation approaches for creating goals and

6845

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

plans relative to an unstructured self-experimentation control, which was designed to mimic self-experimentation
commonly done by QS’ers. Our results suggest the value of
our tutorials for supporting self-creation of personalized
behavior-change plans. Others should consider building on
our tutorials if there is a need to support users in the selfcreation of personalized behavioral goals and plans (see
online supplement).

Beyond self-experimentation, our work also provides clear
guidance for other HCI researchers interested in using
Bayesian analyses when evaluating their systems. As illustrated in our discussion, the Bayesian estimates give us a
richer—and appropriately more conservative—view of the
systems by allowing us to quantify the probability of clinically significant effect sizes. This more nuanced information can better support decision-making of others. In
particular, we argue that the probability of small to medium
effects justifies continued design work, including development of these protocols within automated online systems
and better support for individuals to create more complex
JIT state trigger rules. These conclusions would have been
hard to draw if using purely frequentist approaches, which,
from a between-group perspective, would have indicated no
effect because of the wide confidence intervals. It is our
hope that this case study could be used as a starting template for others interested in using Bayesian analyses in
formative evaluations; to help, as Kay et al. put it [25],
“free design and engineering researchers from the shackles
of meaningless p-values in small-n studies.”

Our current work placed greatest emphasis on the first part
of the self-experimentation for behavior change framework;
namely formulating a hypothesis about how to change a
behavior. While our approach did include self-tracking and
a systematic way to review those data to iterate on behavioral plans, we did not include a formal n-of-1 trial evaluation of the plans. This was done intentionally as our personal experience teaching behavior change in our courses is
that individuals struggle with what to change, which others
have studied [13,24] and devising a viable personalized
plan on how to change and sustain the behavior. While
technically an n-of-1 trial could have been incorporated
during the 2 weeks in between sessions, we explicitly excluded n-of-1 trials because we wanted participants to formulate a robust hypothesis and not get overly focused on
“the answer” from the trial over other information. Indeed,
the subtle changes made during each iteration were exactly
what we sought. If we had run a longer trial, we would likely have incorporated n-of-1 trials (e.g., [28]) after the week
5 or 7 mark, depending on how confident each participant
was in their goals and plans. We contend that only at that
point would a person have enough experience to translate
an under-specified hunch about a behavioral plan (i.e., what
they often start with) into a well-reasoned hypothesis (i.e.,
something appropriate for an n-of-1 trial). Future work
should flesh out when to use self-experimentation for discovery [13,24], self-experimentation for behavior change
without n-of-1 trials (what we did) and self-experimentation
for behavior change that includes n-of-1 trials.

CONCLUSION

Our results indicate the value of our tutorials for helping
individuals generate personalized behavioral goals and
plans for achieving said goals over a more unstructured
form of self-experimentation. Results from our JIT intervention suggests that these systems work particularly well if
JIT plans are created for each behavioral goal, if contextaware computing is fully used for inferring JIT states, and if
individuals are empowered to iterate. Beyond this, we have
also proposed a unique framework for self-experimentation
for behavior change that is complementary to previous
work [24]. Our Bayesian analyses could also be used as a
starting template for others interested in using Bayesian
analyses in their work, with full details available within our
online supplement. Future work should explore how to help
individuals devise realistic, specific, and personalized behavioral plans; help individuals use more rigorous n-of-1
trials in self-experimentation (including the possibility of a
combined self-experimentation paradigm that includes both
Karkar’s self-experimentation for discovery and our selfexperimentation for behavior change paradigms along with
careful timing on when NOT to use n-of-1 trials); and provide individuals with better training on the concepts of opportunity and receptivity for helping individuals in designing their own JIT triggers via context-aware computing.

While our JIT intervention did produce improved outcomes
relative to control, results indicated no significant advantage beyond our self-experimentation tutorial. With that
said, careful examination of our results suggests several
targets for future work. First, if notifications can be sent
during JIT states, then individuals appear to appreciate
them, thus suggesting the potential for this type of intervention. It appeared that many of the issues arose from the
wide variety exhibited in how individuals programmed their
own JIT interventions. Specifically, individuals that clearly
specified JIT plans for each goal, took full advantage of
context-aware computing (i.e., did not merely use time but
also used the sensors for defining rules), and iterated to
improve their systems, exhibited the greatest improvements.
Future work could build on the basic self-experimentation
protocol we created (see online supplement) but add greater
support for and emphasis of these three points.

ACKNOWLEDGEMENTS

This work was supported by a Google Research Faculty
Award (PI: Hekler). Thanks to Bob Evans from Google for
being our sponsor, for support on the early conceptualization of this work, and for providing the tool, Paco, for our
use.

6846

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

Human Factors in Computing Systems (CHI '08),
1797-1806.

REFERENCES

1.

Thomas Attin and E. Hornecker. 2005. Tooth brushing
and oral health: how frequently and when should tooth
brushing be performed?. Oral health & preventive dentistry, 3, 3: 135-40.

2.

Douglas Bates, Martin Maechler, Ben Bolker, and Steve Walker. 2015. Fitting Linear Mixed-Effects Models
Using lme4. Journal of Statistical Software, 67, 1: 148.

3.

Frank Bentley, Konrad Tollmar, Peter Stephenson,
Laura Levy, Brian Jones, Scott Robertson, Ed Price,
Richard Catrambone, and Jeff Wilson. 2013. Health
Mashups: Presenting Statistical Patterns between Wellbeing Data and Context in Natural Language to Promote Behavior Change. ACM Trans. Comput.-Hum.
Interact. 20, 5, Article 30, 27 pages.

4.

Paul-Christian Buerkner. brms: An R Package for
Bayesian Multilevel Models using Stan. Journal of Statistical Software (in press).

5.

Daniel J. Buysse, Charles F. Reynolds, Timothy H.
Monk, Susan R. Berman, and David J. Kupfer. 1989.
The Pittsburgh Sleep Quality Index: a new instrument
for psychiatric practice and research. Psychiatry research 28, 2: 193-213.

6.

7.

8.

9.

11. Allen Cypher and Daniel Conrad Halbert. Watch what
I do: programming by demonstration. MIT press, 1993.
12. Siddharth Dalal, Majd Alwan, Reza Seifrafi, Steve
Kell, and Donald Brown. 2005. A rule-based approach
to the analysis of elders activity data: Detection of
health and possible emergency conditions. In AAAI
Fall 2005 Symposium, pp. 2545-2552.
13. Nediyana Daskalova, Danaë Metaxa-Kakavouli,
Adrienne Tran, Nicole Nugent, Julie Boergers, John
McGeary, and Jeff Huang. 2016. SleepCoacher: A Personalized Automated Self-Experimentation System for
Sleep Recommendations. In Proceedings of the 29th
Annual Symposium on User Interface Software and
Technology (UIST '16). ACM, New York, NY, USA,
347-358.
14. Anind K. Dey, Timothy Sohn, Sara Streng, and Justin
Kodama. 2006. iCAP: Interactive prototyping of context-aware applications. In International Conference on
Pervasive Computing, 254-271.
15. Brian J. Fogg. 2002. Persuasive Technology: Using
Computers to Change What We Think and Do. Morgan Kaufmann.
16. Brian J. Fogg. 2009. A behavior model for persuasive
design. In Proceedings of the 4th international Conference on Persuasive Technology, 40.

Taj Campbell, Brian Ngo, and James Fogarty. 2008.
Game design principles in everyday fitness applications. In Proceedings of the 2008 ACM conference on
Computer supported cooperative work (CSCW '08),
249-252.

17. Martha M. Funnell, Tammy L. Brown, Belinda P.
Childs, Linda B. Haas, Gwen M. Hosey, Brian Jensen,
Melinda Maryniuk et al. 2009. National standards for
diabetes self-management education. Diabetes care 32,
no. Supplement 1: S87-S94.

Bob Carpenter, Andrew Gelman, Matt Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Michael A.
Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell.
2016. Stan: A probabilistic programming language.
Journal of Statistical Software (in press).

18. Manuel García-Herranz, Pablo A. Haya, and Xavier
Alamán. 2010. Towards a Ubiquitous End-User Programming System for Smart Spaces. J. UCS 16, 12:
1633-1649.

Jeannette S. Chin, Victor Callaghan, and Graham
Clarke. 2006. An end-user programming paradigm for
pervasive computing applications. In 2006 ACS/IEEE
International Conference on Pervasive Services, 325328.

19. Andrew Gelman and David Weakliem. 2009. Of beauty, sex and power: Too little attention has been paid to
the statistical challenges in estimating small effects.
American Scientist 97, 4: 310-316.

Eun Kyoung Choe, Nicole B. Lee, Bongshin Lee,
Wanda Pratt, and Julie A. Kientz. 2014. Understanding
quantified-selfers' practices in collecting and exploring
personal data. In Proceedings of the 32nd annual ACM
conference on Human factors in computing systems
(CHI '14), 1143-1152.

20. Andrew Gelman and John Carlin. 2014. Beyond Power
Calculations: Assessing Type S (Sign) and Type M
(Magnitude) Errors. Perspectives on Psychological
Science 9, 6: 641–651.
21. Eric B. Hekler, Predrag Klasnja, Jon E. Froehlich, and
Matthew P. Buman. 2013. Mind the theoretical gap: interpreting, using, and developing behavioral theory in
HCI research. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI
'13), 3307-3316.

10. Sunny Consolvo, David W. McDonald, Tammy Toscos, Mike Y. Chen, Jon Froehlich, Beverly Harrison,
Predrag Klasnja, Anthony LaMarca, Louis LeGrand,
Ryan Libby, Ian Smith, and James A. Landay. 2008.
Activity sensing in the wild: a field trial of ubifit garden. In Proceedings of the SIGCHI Conference on

22. Eric B. Hekler, Susan Michie, Misha Pavel, Daniel E.
Rivera, Linda M. Collins, Holly B. Jimison, Claire
Garnett, Skye Parral, Donna Spruijt-Metz. 2016. Ad6847

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

vancing Models and Theories for Digital Behavior
Change Interventions. American Journal of Preventive
Medicine, 51,5: 825-832.

ceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems
(CHI EA '15), 2301-2306.

23. George S. Howard, Scott E. Maxwell, and Kevin J.
Fleming. 2000. The proof of the pudding: An illustration of the relative strengths of null hypothesis, metaanalysis, and Bayesian analysis. Psychological methods
5, 3: 315–332.

33. John K Kruschke. 2013. Bayesian estimation supersedes the t test. Journal of Experimental Psychology:
General 142, 2: 573–603.
http://doi.org/10.1037/a0029146
34. Min Kyung Lee, Sara Kiesler, and Jodi Forlizzi. 2011.
Mining behavioral economics to design persuasive
technology for healthy choices. In Proceedings of the
SIGCHI Conference on Human Factors in Computing
Systems (CHI '11), 325-334.

24. Ravi Karkar, Jasmine Zia, Roger Vilardaga, Sonali R.
Mishra, James Fogarty, Sean A. Munson, and Julie A.
Kientz. 2015. A framework for self-experimentation in
personalized health. Journal of the American Medical
Informatics Association: ocv150.

35. Ian Li, Anind K. Dey, and Jodi Forlizzi. 2011. Understanding my data, myself: supporting self-reflection
with ubicomp technologies. In Proceedings of the 13th
international conference on Ubiquitous computing
(UbiComp '11), 405-414.

25. Matthew Kay, Gregory L. Nelson, and Eric B. Hekler.
2016. Researcher-Centered Design of Statistics: Why
Bayesian Statistics Better Fit the Culture and Incentives of HCI. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI
'16), 4521-4532.

36. Ian Li. 2011. Personal Informatics and Context: Using
Context to Reveal Factors that Affect Behavior. Ph.D
Dissertation. Carnegie Mellon University, Pittsburgh,
Pennsylvania..

26. Abby C. King, Eric B. Hekler, Lauren A. Grieco, Sandra J. Winter, Jylana L. Sheats, Matthew P. Buman,
Banny Banerjee, Thomas N. Robinson, and Jesse Cirimele. 2013. Harnessing different motivational frames
via mobile phones to promote daily physical activity
and reduce sedentary behavior in aging adults. PloS
one 8, 4: e62613.

37. Henry Lieberman, Fabio Paternò, Markus Klann, and
Volker Wulf. 2006. End-user development: An emerging paradigm. In End user development, 1-8.
38. James J. Lin, Lena Mamykina, Silvia Lindtner, Gregory Delajoux, and Henry B. Strub. 2006. Fish’n’Steps:
Encouraging physical activity with an interactive computer game. In International Conference on Ubiquitous
Computing, 261-278.

27. Vladimir J. Konečni. 2008. Does music induce emotion? A theoretical and methodological analysis. Psychology of Aesthetics, Creativity, and the Arts 2, 2:
115.

39. Helen Lindner, David Menzies, Jill Kelly, Sonya Taylor, and Marianne Shearer. 2003. Coaching for behaviour change in chronic disease: a review of the literature and the implications for coaching as a selfmanagement intervention. Australian Journal of Primary Health 9, 3: 177-185.

28. Richard L. Kravitz, Naihua Duan, eds, and the DEcIDE
Methods Center N-of-1 Guidance Panel. Design and
Implementation of N-of-1 Trials: A User’s Guide.
AHRQ Publication No. 13(14)-EHC122-EF. Rockville,
MD: Agency for Healthcare Research and Quality;
February 2014. www.effectivehealthcare.ahrq.gov/N1-Trials.cfm.

40. Steven R. Livingstone, Ralf Mühlberger, Andrew R.
Brown, and Andrew Loch. 2007. Controlling musical
emotionality: An affective computational architecture
for influencing musical emotions. Digital Creativity 18,
1: 43-53.

29. Patricia Lacks and Monique Rotert. Knowledge and
practice of sleep hygiene techniques in insomniacs and
good sleepers. Behaviour research and therapy 24, 3:
365-368.
30. Gary P. Latham. 2003. Goal Setting: A Five-Step Approach to Behavior Change. Organizational Dynamics
32, 3: 309-318.

41. Edwin A. Locke and Gary P. Latham. 2002. Building a
practically useful theory of goal setting and task motivation: A 35-year odyssey. American psychologist 57,
9: 705.

31. Jisoo Lee, Erin Walker, Winslow Burleson, and Eric B.
Hekler. 2014. Exploring users' creation of personalized
behavioral plans. In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication (UbiComp '14
Adjunct), 703-706.

42. Lena Mamykina, Elizabeth Mynatt, Patricia Davidson,
and Daniel Greenblatt. 2008. MAHI: investigation of
social scaffolding for reflective thinking in diabetes
management. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 477486.

32. Jisoo Lee, Erin Walker, Winslow Burleson, and Eric B.
Hekler. 2015. Understanding Users' Creation of Behavior Change Plans with Theory-Based Support. In Pro-

43. Richard McElreath. 2016. Statistical rethinking: A
Bayesian course with examples in R and Stan. Vol.
122. CRC Press.
6848

Online Experiments

CHI 2017, May 6–11, 2017, Denver, CO, USA

44. Susan Michie, Maartje M. van Stralen, and Robert
West. 2011. The behaviour change wheel: a new method for characterising and designing behaviour change
interventions. Implementation Science 6, 1: 1.

52. Benjamin Poppinga, Wilko Heuten, and Susanne Boll.
2014. Sensor-based identification of opportune moments for triggering notifications. IEEE Pervasive
Computing 13, 1: 22-29.

45. Susan Michie, Michelle Richardson, Marie Johnston,
Charles Abraham, Jill Francis, Wendy Hardeman, Martin P. Eccles, James Cane, and Caroline E. Wood.
2013. The behavior change technique taxonomy (v1) of
93 hierarchically clustered techniques: building an international consensus for the reporting of behavior
change interventions. Annals of behavioral medicine
46, 1: 81-95.

53. Mashfiqui Rabbi, Min Hane Aung, Mi Zhang, and
Tanzeem Choudhury. 2015. MyBehavior: automatic
personalized health feedback from user behaviors and
preferences using smartphones. In Proceedings of the
2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp '15), 707718.
54. Paschal Sheeran. 2002. Intention—behavior relations:
A conceptual and empirical review. European review
of social psychology 12, 1: 1-36.

46. Susan Michie, Lou Atkins, & Robert West. 2014. The
Behaviour Change Wheel: A Guide To Designing Interventions. Silverback Publishing.

55. Anna Ståhl, Kristina Höök, Martin Svensson, Alex S.
Taylor, and Marco Combetto. 2009. Experiencing the
affective diary. Personal and Ubiquitous Computing
13, 5: 365-378.

47. Mark Muraven and Roy F. Baumeister. 2000. Selfregulation and depletion of limited resources: Does
self-control resemble a muscle?. Psychological bulletin
126, 2: 247.

56. Edward J. Stepanski and James K. Wyatt. 2003. Use of
sleep hygiene in the treatment of insomnia. Sleep medicine reviews 7, 3: 215-225.

48. Inbal Nahum-Shani, Eric B. Hekler, and Donna
Spruijt-Metz. 2015. Building health behavior models to
guide the development of just-in-time adaptive interventions: A pragmatic framework. Health Psychology
34, S: 1209.

57. Tammy Toscos, Anne Faber, Shunying An, and Mona
Praful Gandhi. 2006. Chick clique: persuasive technology to motivate teenage girls to exercise. In CHI '06
Extended Abstracts on Human Factors in Computing
Systems (CHI EA '06), 1873-1878.

49. Jason Nawyn, Stephen S. Intille, and Kent Larson.
2006. Embedding behavior modification strategies into
a consumer electronic device: a case study. In Proceedings of the 8th international conference on Ubiquitous
Computing (UbiComp'06), 297-314.

58. Khai N. Truong, Elaine M. Huang, and Gregory D.
Abowd. 2004. CAMP: A magnetic poetry interface for
end-user programming of capture applications for the
home. In International Conference on Ubiquitous
Computing, 143-160.

50. Miriam E. Nelson, W. Jack Rejeski, Steven N. Blair,
Pamela W. Duncan, James O. Judge, Abby C. King,
Carol A. Macera, and Carmen Castaneda-Sceppa.
2007. Physical activity and public health in older
adults: recommendation from the American College of
Sports Medicine and the American Heart Association.
Circulation 116, 9: 1094.

59. Van Dantzig, Saskia, Gijs Geleijnse, and Aart Tijmen
van Halteren. 2013. Toward a persuasive mobile application to reduce sedentary behavior.Personal and ubiquitous computing 17, 6: 1237-1246.
60. Fang Xu, Machell Town, Lina S. Balluz, William P.
Bartoli, Wilmon Murphy, Pranesh P. Chowdhury, William S. Garvin et al. 2013. Surveillance for certain
health behaviors among States and selected local areas—United States, 2010. MMWR Surveill Summ 62,
1: 1-247.

51. Virpi Oksman and Pirjo Rautiainen. 2003. "Perhaps it
is a Body Part”: How the Mobile Phone Became an
Organic Part of the Everyday Lives of Finnish Children
and Teenagers. Machines that become us: The social
context of communication technology: 293-308.

6849

Int J Artif Intell Educ (2014) 24:33–61
DOI 10.1007/s40593-013-0001-9
R E S E A R C H A RT I C L E

Adaptive Intelligent Support to Improve Peer
Tutoring in Algebra
Erin Walker & Nikol Rummel &
Kenneth R. Koedinger

Published online: 18 October 2013
# International Artificial Intelligence in Education Society 2013

Abstract Adaptive collaborative learning support (ACLS) involves collaborative
learning environments that adapt their characteristics, and sometimes provide intelligent
hints and feedback, to improve individual students’ collaborative interactions. ACLS
often involves a system that can automatically assess student dialogue, model effective
and ineffective collaboration, and provide relevant support. While there is evidence that
ACLS can improve student learning, little is known about why systems that incorporate
ACLS are effective. Does relevant support improve student interactions by providing
just-in-time feedback, or do students who believe they are receiving relevant support feel
more accountable for the collaboration, and thus more motivated to improve their
interactions? In this paper, we describe an adaptive system we have developed to support
help-giving during peer tutoring in high school algebra: the Adaptive Peer Tutoring
Assistant (APTA). To validate our approach, we conducted a controlled study that
demonstrated that our system provided students with more relevant support and was
more effective at improving student learning than parallel nonadaptive conditions. Our
contributions involve generalizable techniques for implementing ACLS that can function adaptively and effectively, and the finding that adaptive support does indeed
improve student learning because of the relevance of the support.
Keywords Intelligent tutoring . Computer-supported collaborative learning . Adaptive
collaborative learning support . Peer tutoring

E. Walker (*)
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University,
Tempe, USA
e-mail: erin.a.walker@asu.edu
N. Rummel
Institute of Educational Research, Ruhr-Universität Bochum, Bochum, Germany
e-mail: nikol.rummel@rub.de
K. R. Koedinger
Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, USA
e-mail: koedinger@cmu.edu

34

Int J Artif Intell Educ (2014) 24:33–61

Introduction
The ability to computationally model human problem-solving has had a profoundly
positive impact on the advancement of effective educational technologies. In intelligent tutoring systems (ITSs), the system develops a model of student knowledge by
comparing students’ problem-solving steps to ideal performance, and then uses that
model to give students individualized support such as hints, feedback, and problem
selection (VanLehn 2006). These systems have been very successful at supporting
individual learners in problem solving, often approaching the effectiveness of expert
human tutoring (VanLehn 2011). For example, the Cognitive Tutor Algebra, an
intelligent tutoring system for high school algebra, has been demonstrated to improve
classroom learning by one standard deviation over traditional instruction (Koedinger
et al. 1997). Recently there has been a movement in ITS research to take a more
holistic view of the learning process and develop “tutors that care” (du Boulay et al.
2010), with a goal of modeling and supporting higher-order skills like metacognition,
motivation, and social interaction (e.g., Roll et al. 2011; Muldner et al. 2010; Ogan
et al. 2011).
Our focus within this movement is on modeling and supporting collaborative
learning. An important aspect of learning is the social construction of knowledge,
where students exchange ideas, reflect on their own misconceptions, and come to a
shared understanding through dialogue with their peers (Schoenfeld 1992).
Collaborative activities in the classroom can facilitate this kind of learning, but only
if students are engaging in particular interactions, such as giving help when their
partners need it (Johnson and Johnson 1990). As students do not engage in these
interactions spontaneously, researchers and practitioners attempt to improve student
collaboration through interventions that structure or script the collaboration, providing students with roles and activities to follow (Fischer et al. 2007; Kollar et al. 2006).
For example, in King’s reciprocal peer tutoring script, students take turns teaching a
partner, and are prompted to ask their partner specific questions at increasing levels of
depth (King et al. 1998). These collaboration scripts tend to be fixed, in that they do
not adapt to individual or group needs. They may overconstrain collaboration for
good collaborators, provide insufficient support for poor collaborators, and there is no
guarantee that students will use them as designed (Kollar et al. 2005; Dillenbourg
2002; Lazonder et al. 2003).
In adaptive collaborative learning support (ACLS), collaborative learning environments adapt their characteristics, and sometimes provide intelligent hints and
feedback, to improve students’ collaborative interactions (Magnisalis et al. 2011).
In theory, ACLS systems are more effective than nonadaptive collaborative learning
systems because they can tailor the support they provide to students’ needs and
abilities (Rummel and Weinberger 2008). In this paper, we describe an ACLS system
we developed to support peer tutoring in high school algebra: The Adaptive Peer
Tutoring Assistant (APTA; Walker et al. 2011). APTA is based on the literal equation
solving unit of the Cognitive Tutor Algebra (CTA), a successful individual intelligent
tutoring system in high school algebra (Koedinger et al. 1997). In APTA, one student
(the tutee) solves problems like “Solve for x; ”, and a second student (the peer tutor)
marks the problem steps right or wrong and discusses the problems with the tutee in a
chat window. Our system provides adaptive support to peer tutors in order to help

Int J Artif Intell Educ (2014) 24:33–61

35

them give more correct and more effective help. In this paper, we focus on the
mechanisms we used to support the peer tutor in giving more effective help (a
help-giving tutor). In constructing our system, we made a technological contribution
by developing techniques to handle the uncertainty inherent in assessing, modeling,
and supporting collaborative dialogue. We made a theoretical contribution by evaluating APTA to investigate the impact of adaptive support on learning from collaboration, improving understanding of when and why adaptive systems are effective.
Implementation of ACLS
Soller et al. (2005), in their review of systems that support collaboration, differentiate
between mirroring systems that reflect the state of students’ collaboration back to the
students, metacognitive tools that provide information about the current and desired
state of the interaction, and guiding systems that propose remedial actions in response
to perceived collaborative weaknesses. Within guiding systems (what we are referring
to as ACLS), Magnisalis, Demetriadis, and Karakostas (2011) make a further distinction between systems that focus on group formation, domain-specific support, or peer
interaction support. For the purposes of this paper, our primary interest is in guiding
systems that provide peer interaction support by analyzing student collaborative
dialogue as it occurs and intervening when appropriate (e.g., Israel and Aiken
2007; Vieira et al. 2004).
Dialogue is a highly important element of learning from peer tutoring, and the
target of the help-giving support implemented in APTA. The simplest way of
supporting collaborative dialogue is to focus on metrics that do not rely on the
content of the dialogue. Several systems count individuals’ utterances as a way of
generating a participation score, and then encourage those who are not participating
to participate more (e.g., Vizcaíno et al. 2000; Rosatelli and Self 2004; Vieira et al.
2004; Kumar et al. 2007). For example, if a student does not participate in a chat
window for more than 50 % of the estimated time the group will be working on a
step, LeCS prompts the student to participate in the discussion by saying, “Would you
like to participate in the discussion?” (Rosatelli and Self 2004). Another set of
guiding systems facilitates dialogue between collaborating students by using sentence
openers or classifiers. Students are asked to classify their own utterances, and then the
sequence of labelled utterances is compared to a model of ideal dialogue in order to
provide students with support (McManus and Aiken 1995; Baker and Lund 2003).
For example, GroupLeader (Israel and Aiken 2007) asks students to select from one
of 46 sentence starters in order to label their utterance. Based on the starter selected,
the system identifies the dialog action students were taking (e.g., “Let’s negotiate
this…” represents the negotiation dialogue act), and students could then be encouraged to make more of a specific type of contribution (Barros and Verdejo 2000).
Particular sequences of dialogue actions can also relate to student behaviors or beliefs
that can serve as targets for support. For example, Tedesco (2003) uses sentence
starters such as “I disagree” and other forms of highly constrained input to detect
conflicts within a group and prompt group members to reflect and elaborate on their
opinions. Unfortunately, students do not consistently select sentence starters that
match the content of their statements, and therefore the inferences the system makes
can be inaccurate (Israel and Aiken 2007; Lazonder et al. 2003). In recent years,

36

Int J Artif Intell Educ (2014) 24:33–61

machine learning technology has been used to supplement sentence classifiers and
content-free metrics by automatically classifying student contributions along a growing
variety of dimensions (Rosé et al. 2008), and using features of the learning environment
such as the way discussion contributions are linked to improve the impact of the
machine learning (Mclaren et al. 2010). For example, Dragon, Florian, Woolf, and
Murray (2010) have demonstrated that they can automatically classify the topic of
student discussion. Recently, these classifiers have become effective at detecting relevant aspects of collaborative dialogue such as authority (Mayfield and Rosé 2011), with
the goal of using these assessments as triggers for support.
Like an individual ITS, ACLS involves three phases: assessing student interactions, modeling ideal interactions, and using a comparison between the two to provide
tailored support (Soller et al. 2005). Unlike many traditional ITSs, ACLS must deal
with ambiguity at each phase, sharing commonalities with the growing body of work
on ITSs in ill-defined domains (Mitrovic and Weerasinghe 2009). With regards to
assessment, much of the benefits of collaborative learning come from the dialogue
between collaborating students, but automatically assessing relevant aspects of student dialogue is not a solved problem. Regarding modeling (i.e., creating a model of
ideal performance), while we know which collaborative interactions are broadly
related to learning, we know less about under what specific circumstances these
positive interactions should be employed. Therefore, when creating a model of
collaboration it is difficult to specify what interactions should occur at what moments
during the collaboration. Finally, in terms of support, little research has been done on
how to provide adaptive support based on uncertain information delivered in the
assessment and modeling phases. A single collaborative action or state is unlikely to
be unilaterally correct or incorrect, and students have many options when choosing
their next action. Traditional ITSs are ill-equipped to deal with this kind of ambiguity.
It is difficult to build an adaptive system that can produce support relevant to student
interaction, and thus ACLS systems are rarely sufficiently developed such that they
are able to be deployed in a natural context.
Magnisalis and colleagues (2011) introduce several necessary research directions
in their review of the state of the art of ACLS, including improving the assessment of
student collaboration, deepening the interaction analysis of the collaboration, and understanding the design space for providing support. In our system, APTA, we advance ACLS
by using multiple channels of information (automatic classifications, self-classifications, and
domain information) to assess student dialogue. We then use a combination of production
rule modeling, constraint-based modeling, and knowledge tracing to assess student peer
tutoring skills. The feedback we give based on these assessments takes into account the
uncertainty inherent in modeling collaboration, giving the peer tutor the benefit of the doubt
with respect to many of their collaborative actions.
Effects of ACLS
Once ACLS is implemented, it tends to be an effective way of supporting collaboration. COLLECT-UML, for example, adaptively supports students in collaborating
on the design of UML class diagrams, and has been shown to lead to greater
knowledge of collaboration over an individual learning system (Baghaei et al.
2007). A previous iteration of our system demonstrated that adaptive support improved

Int J Artif Intell Educ (2014) 24:33–61

37

student peer tutoring in high school algebra over a static resource on how to collaborate
effectively (Walker et al. 2011). There is also evidence that ACLS can improve domain
learning. CycleTalk adaptively supports collaborative dialog in simulation-based learning environments using tutorial dialog agents, and has been shown to be better than fixed
support at increasing domain learning (Kumar et al. 2007). Karakostas and Demetriadis
(2011) provided adaptive domain support to collaborating students in the form of
prompts relating to important concepts students had missed in their discussion. This
adaptive support improved domain learning over a static resource that described all
important domain concepts. Although the research involving actual adaptive systems
and their effects on learning from collaboration is limited, there have additionally been
Wizard of Oz studies where adaptive support within a collaborative learning system has
been provided by a human acting as a computer, and benefits of the adaptive support
have been demonstrated (Gweon et al. 2006; Tsovaltzi et al. 2010).
It is important to understand why ACLS might be effective, so we can better direct
our efforts when building support. Research on collaborative learning has evolved from
investigating whether collaborative learning is better than individual learning, to investigating when collaborative interactions are related to benefits of collaboration, to
understanding how to support positive collaborative interactions to yield optimal outcomes (Dillenbourg et al. 1995). Now that it is possible to support collaborative
interactions adaptively using technology, it becomes important to understand what kinds
of adaptive support are effective for what kinds of collaboration. Most research on
ACLS has assumed that it benefits students because when given relevant support in a
timely manner, students might more easily apply it to their interactions. In fact, the
success of ACLS systems is typically measured by looking at the validity of the
collaborative model used (Suebnukarn and Haddawy 2006) or the applicability of the
feedback messages given (Constantino-González et al. 2003). However, there is another
explanation for why ACLS might be effective: students who believe they are receiving
adaptive support may feel more motivated to engage with the support and take appropriate action, regardless of the actual relevance of the support. Accountability for one’s
partner’s outcomes tends to be an important motivational force in collaboration (e.g.,
Slavin 1996), and in previous work we found results that suggested that if students
believe the computer is monitoring and responding to their actions, they may feel an
increased sense of accountability for those actions and become better collaborators
(Walker et al. 2011). If support relevance is important, the uncertainty inherent in
modeling student collaboration needs to be resolved, but if support relevance is not
important, then designing engaging support becomes the most important goal. By
implementing a truly adaptive system, we can begin to evaluate the impact more relevant
help, delivered by an intelligent system, has on student learning from collaboration.
To demonstrate the effectiveness of our system, we examined whether and why
providing more adaptive support improves learning from collaboration. We compared
three conditions: one in which support is adaptive and students were told it is adaptive
(real adaptive), one in which support is random but students were told it is adaptive
(told adaptive), and one in which support is random and students were told it is
random (real nonadaptive). If it is relevant support that matters, only students who
actually receive adaptive support (i.e., those students in the real adaptive condition)
should improve their collaboration quality and their domain learning. On the other
hand, if accountability is most important, told adaptive students should also improve

38

Int J Artif Intell Educ (2014) 24:33–61

their collaboration quality and domain learning. By distinguishing between these two
explanations for the effectiveness of adaptive collaborative support, we can better
evaluate the effectiveness of the specific adaptivity implemented in our system.
In the remainder of this paper, we first survey research on learning from peer tutoring,
and describe they ways in which APTA, our collaborative learning environment, enables
peer tutors and tutees to interact. We describe the implementation of the ACLS we
developed for the peer tutor. We then present the results of an evaluation of APTA that
explores why adaptive support might be effective in this context. For consistency,
throughout this paper, the student being tutored is referred to as the tutee, the student
doing the tutoring is referred to as the peer tutor, and the intelligent tutoring component
that provides feedback to the peer tutor is referred to as the help-giving tutor.

Context: Peer Tutoring
We investigate adaptive support to collaborative dialogue within the context of a
reciprocal peer tutoring script. In reciprocal peer tutoring, novices are put in pairs, and
take turns tutoring each other. These scenarios have been shown to lead to learning in
classroom environments (Fantuzzo et al. 1989). The core activity in reciprocal peer
tutoring is help-giving, where one student helps another. Help-giving is a large component of many collaborative scenarios, and thus determining how to support students in
giving better help might generalize to many different areas (Johnson & Johnson, 1990).
Peer tutoring can have a positive impact on both the help-giver and help-receiver.
Students of all abilities benefit from giving help (Ploetzner et al. 1999). When students
know they will be tutoring another, they are more motivated to attend to the domain
material. As their partner takes steps and makes errors, they reflect on the steps, noticing
their own misconceptions. As they construct explanations to help their partner, they
elaborate on their existing knowledge and construct new knowledge (Roscoe and Chi
2007). In contrast, for the help-receiver to benefit from reciprocal peer tutoring scenarios, many criteria need to be met: for example, help needs to target misconceptions, be
conceptual, be elaborated, and, for the most part, be correct (Webb and Mastergeorge
2003). In addition, the actions help-receivers take can have a positive impact on their
own learning, for example, by using peer tutor help constructively or by self-explaining
their errors (Webb et al. 1995; Chi et al. 1994).
APTA, our peer tutoring system, is based on the literal equation solving unit of the
Cognitive Tutor Algebra (CTA), a successful individual intelligent tutoring system in
high school algebra (Koedinger et al. 1997). In literal equation solving, students are
given a prompt like “Solve for x,” and then given an equation like “” This domain is
consistently identified by classroom teachers as one that is particularly difficult for
students to master. In this unit, students use menus in an equation solver tool to
manipulate the equation, selecting operations like “add x” or “combine like terms”.
The semantic label for the operation then appears on the right side of the screen. For
certain problems, students have to type the result of the operation in addition to
selecting it. As the students solve the problem, the CTA compares their actions to a
model of correct and incorrect problem-solving behavior. If they make a mistake, they
receive visual feedback in the interface, and often a message describing their
misconception. At any point, students can request a hint on the next step of the

Int J Artif Intell Educ (2014) 24:33–61

39

problem. The CTA monitors student skills, reflects them in a skill display, or
skillometer, and selects problems based on student skill mastery. Students complete
the unit when they have demonstrated mastery on problems at three levels of
difficulty: 1) problems where all variable terms are on the same side, 2) problems
where variable terms are on different sides of the equation, and 3) problems where the
variable terms are in unusual positions (e.g., in the denominator of a fraction).
Our initial version of the peer tutoring script attempted to create interaction conditions
conducive to the display of positive tutoring behaviors. During use of the script, students
were in the same classroom, but tutors and tutees were seated on opposite sides of the room
and asked to communicate with each other solely through a chat window on the computer.
One component of the script involved encouraging the peer tutor to reflect on the correctness
of tutee steps. The tutee solves problems using the equation solver interface in the CTA. The
tutor sees the tutee’s problem-solving steps and the results of her typed-in entries, but cannot
solve the problem herself (see Fig. 1e). Instead, she can mark the tutee’s actions right or
wrong, providing feedback that the tutee sees and can use while continuing to solve the
problem (Fig. 1f). Peer tutors did not have to mark every step before they could move to the
next problem, but could mark whichever steps they feel necessary. Peer tutors receive
feedback on these reflective actions; if they marked a step correct when it is actually
incorrect, or marked a step incorrect when it is actually correct, the computer tutor highlights
the answer in the interface, and presented the peer tutor with an error message, consisting of
a randomly selected prompt to collaborate and the domain help the tutees would have
received had they been solving the problem individually. The peer tutor could request a hint
from the computer tutor at any time, and would receive multi-level hints involving domain
help (including conceptual hints and feedback) and a prompt to collaborate. This feedback
was designed to trigger reflective processes on the part of the peer tutor by encouraging them
to mark steps more frequently. It also served to draw the peer tutors’ attention to misconceptions by letting them know when they have made an error marking a problem step,
further encouraging them to engage in reflection. In addition, the support is intended to lead
peer tutors to provide tutees with more correct feedback on problem steps, which facilitates
both students in building correct procedural knowledge in the domain. The hints that peer
tutors receive are intended to further introduce more correct and conceptual content into the
interaction. As students benefit from peer tutoring because they reflect on domain knowledge and their own misconceptions, and tutees benefit more from peer tutoring when they
receive correct help, we felt that this intervention would improve the domain learning of both
students involved.
A second component of the script involved natural language interaction between
peer tutors and tutees using a chat tool, where, for example, tutees can ask questions
and tutors can give hints and feedback (Fig. 1a). To facilitate the discussion in the
chat window, we included a common form of fixed scaffolding: sentence classifiers.
This form of fixed scaffolding is thought to be pedagogically beneficial by making
positive collaborative actions explicit in the interface and encouraging students to
consider the type of utterance they wish to make (Weinberger et al. 2005). We asked
peer tutors to label their utterances using one of four classifiers: “ask why”, “explain
why wrong”, “give hint”, and “explain what next” (see Fig. 1d). Students had to select a
classifier before they typed in an utterance, but they could also choose to click a neutral
classifier (“other”). For example, if students wanted to give a hint, they could click “give
hint” and then type “subtract x”. Their utterance would appear as: “tutor hints: subtract

40

Int J Artif Intell Educ (2014) 24:33–61

Fig. 1 Peer tutor’s interface in APTA. The peer tutor watches the tutee take problem-solving actions (e),
and marks the actions right or wrong (f). Students can talk in the chat window (a), where they classify their
own utterances (d). They also receive prompts from the computer (b), and can choose to like them, dislike
them, or ignore them (c)

x” to both students in the chat window. Tutees were also asked to self-classify each
utterance as one of three categories: a “ask for help”, “explain yourself”, or “other”. To
further encourage students to chat appropriately, in the tutor’s interface, we linked each
sentence classifier to a brief description of how to use the classifier appropriately. This
description appeared every time the student selected the sentence classifier. Dialogue is
an important component of learning from peer tutoring; Peer tutors engage in elaborative
and generative processes when they compose conceptual explanations and reflect on
tutee questions; and tutees benefit by explaining their own actions and asking tutors
specific questions (Webb and Mastergeorge 2003). Including a chat window gives
students the opportunity to engage in these behaviors.
In the above system as described, there is cognitive support that adapts to what
students are doing, but not direct support for the quality of student collaboration
(which we call help-giving support). If tutees take incorrect steps, peer tutors have
access to hints and feedback on what the correct steps are, and the concepts behind
them. However, peer tutors do not receive adaptive guidance on whether they are
giving good help and how to improve. We hypothesized that introducing adaptive
help-giving support into the system would significantly improve the learning of both
the peer tutor and peer tutee. While it is likely that supporting the interactions of the
tutee more directly would also improve the learning of both parties, as an initial
attempt at introducing this ACLS we focused only on peer tutor actions.

Int J Artif Intell Educ (2014) 24:33–61

41

Adaptive Help-Giving Support
Description
In improving peer tutor help-giving, our goal was to target four peer tutor skills that
exemplify good help in our context:
1. Timely help. Giving help in response to tutee errors and help requests. While this
skill is rarely discussed in research on peer tutoring, interacting with the tutee when
they are struggling is a basic common-sense requirement of effective peer tutoring.
2. Appropriate help. Prompting the tutee to self-explain and giving error feedback
when appropriate. As surveyed above, both prompting tutees to self-explain and
explaining tutee misconceptions can be beneficial for tutee learning (Chi et al.
1994; Webb and Mastergeorge 2003). Reflecting on tutee misconceptions is also
beneficial for peer tutor learning (Roscoe and Chi 2007).
3. Conceptual help. Explaining a problem-solving step using a domain concept.
Building a conceptual explanation is an essential part of peer tutor knowledge
construction (Roscoe and Chi 2007). Receiving these explanations can also be
beneficial for the tutee (Webb and Mastergeorge 2003).
4. Use of classifiers. Using sentence classifiers to accurately label the content of
peer tutor utterances. Sentence classifiers can be beneficial scaffolding on their
own for collaborating students (Weinberger et al. 2005), by enabling students to
reflect on the types of collaborative actions they want to take. In addition, we use
the sentence classifiers to improve our understanding of what the peer tutor is
doing. Ensuring that students use sentence classifiers correctly could potentially
both improve peer tutor learning and the accuracy of our system.
Adaptive support, visible to both students, was provided in the chat window to
help peer tutors give better help. For example, the peer tutor might give an instrumental hint like “then subtract” rather than a conceptual hint like “to get rid of qcv,
you need to perform the inverse operation on that side of the equation.” In that case,
the computer uses an assessment of the peer tutor’s help-giving skill to say in the chat
window (visible to both students), “Tutor, why do you say that? Can you explain
more?” (Fig. 1b). This utterance is designed to get both students reflecting on the
domain concepts behind the next step, and to remind the peer tutor that she should be
giving help that explains why in addition to what. However, the computer assistance
is posed as a question and uses non-critical wording to avoid threatening the authority
of the peer tutor. Students also received encouragement when they displayed a
particular help-giving skill (“Good work! Explaining what your partner did wrong
can help them not make the same mistake on future problems”). Only one reflective
prompt was given at a time, and prompts (in this version of the system) were always
addressed to the peer tutor. Parameters were tuned so that students received an average
of one prompt in the chat window for every three peer tutor actions. There were several
different prompts for any given situation, so students rarely received the same prompt
twice. These prompts were intended to support peer tutors in engaging in the four helpgiving skills described above, leading them to experience more reflective and generative
elaborative processes while providing better help to their tutees.

42

Int J Artif Intell Educ (2014) 24:33–61

A simplified architecture of the system is depicted in Fig. 2. The peer tutor and tutee
interact with separate interfaces. Student actions are sent to a central control module, and
then to the tutoring components of the system. The adaptive help-giving support consists
of an assessment component, a modeling component (that does both model and
knowledge tracing), and a support component. Once a pedagogical decision has been
made, the support component sends the appropriate action back to the control module,
which then directs the response to the student interfaces. In the following subsections we
describe how the help-giving tutoring components were implemented in more detail.
Assessment
The first step in delivering adaptive prompts to the peer tutor was to assess the current
quality of peer tutor help (see the Assessment Component of Fig. 2). The help-giving
tutor used a combination of several inputs. First, it used the CTA assessment of tutee
problem-solving steps. For each step tutees took, the CTA models classified the step
as right or wrong, and our help-giving tutor had access to that data. Second, it used
student self-classifications of chat actions, based on the sentence classifier selected
(e.g., “give hint”). Third, a machine classifier of student help, constructed using
Taghelper Tools (Rosé et al. 2008; currently called LightSIDE), could determine
whether students gave help, what kind of help it was, and whether it was conceptual

Fig. 2 Simplified architecture of APTA, depicting the help-giving tutoring components. First student
actions are assessed, then we model trace to compare the actions to an ideal model and knowledge trace to
update an assessment of tutor skills. Finally, an appropriate prompt is chosen by the support component

Int J Artif Intell Educ (2014) 24:33–61

43

or not. The details of this classifier are described fully in Walker, Walker, Rummel,
and Koedinger (2010); we summarize its functionality in the following paragraphs.
The system classified two aspects of peer tutor dialogue: help type and conceptual help.
First, for help type, we looked at whether peer tutors were giving next-step help, previousstep help, both, or no help at all. Next-step help related to whether the utterance related to a
future action on the problem (e.g., “How would you get rid of 2h?”) while previous-step help
related to an action tutees had already taken (e.g., “No need to factor because there is only
one g”). If the help segment contained both categories, its help type was labeled “both”, and
if it contained neither category (e.g., “on to the next problem”), its help type was labeled
“none”. Using the classified help type in conjunction with the problem-solving context (e.g.,
knowing whether the tutee has just made a correct step, incorrect step, or help request) can
help APTA decide whether peer tutors are giving the appropriate kind of help. Second, for
conceptual content, we looked at whether peer tutors gave help that explains concepts rather
than simply stating what to do next (e.g., “add ax” was purely instrumental help, while “add
ax to cancel out the –ax” was conceptual). Being able to identify this aspect lets APTA know
whether peer tutors are providing enough conceptual help. We used a corpus drawn from a
previous classroom study, where we compared adaptive and fixed support for peer tutoring
(Walker et al. 2011). As part of the study, students participated in two supported peer tutoring
sessions; one in which they acted as the tutor, and one in which they acted as the tutee. There
were a total of 84 tutoring sessions from both conditions, consisting of an average of 21.77
tutor lines of dialogue per session (SD=10.25). Two raters coded tutor utterances for help
type (kappa=0.83) and conceptual content (kappa=0.72). Interrater reliability was computed
on 20 % of the data, and the remainder of the data was coded by one rater and checked by the
second. All disagreements were resolved through discussion. The dialog was segmented by
chat messages, creating a new segment every time students hit enter. In our dataset, 935 tutor
instances were coded as “none”, 764 were coded as “next-step help”, 83 were coded as
“previous-step help”, and 47 were coded as “both”; 1654 instances were coded as nonconceptual help, and 165 were conceptual help.
We created baseline machine classifiers for help type and conceptual content using
Taghelper Tools, state of the art text-classification technology designed for coding collaborative dialogue (Rosé et al. 2008). Taghelper automatically extracts several dialogue features
for use in machine classification, including unigrams, bigrams, line length, and punctuation.
The domain context of the interaction was used to provide additional features for a machine
learning classifier. This context included information directly taken from the students’
problem-solving behavior (e.g., a student has just taken a incorrect step in the problem),
information about how student dialogue relates to the problem-solving context (e.g., a
student has referred to another student’s incorrect step), and information about the
history of the interaction (e.g., a student referred to another student’s incorrect steps
10 times over the course of the interaction). We used a chi-squared feature selection
algorithm to rank the most predictive features, and 10-fold cross validation to train a
support vector machine classifier for help type and conceptual content. On training data,
the classifier had an accuracy of .81 for help type and .66 for conceptual content.
Modeling
The modeling components of APTA consisted of both a model tracing and knowledge
tracing algorithm (see the Modeling component of Fig. 2). The above inputs (machine

44

Int J Artif Intell Educ (2014) 24:33–61

classification, self-classification, and CTA classification) were fed into a production rule
model with 19 rules. Our model is a metacognitive model rather than a cognitive model,
and thus, while our approach was inspired by model tracing, we made accommodations
for the ill-defined nature of the collaborative domain. Instead of being labeled as either
“correct” or “buggy,” rules were divided into four types: effective, somewhat effective,
somewhat ineffective, and ineffective behaviors. These four levels allowed us to
represent ambiguity in the model. The identification of rules and division into types
were conducted through a literature review, and leveraged previous data collected of
how students tutor each other in this particular context. Table 1 presents the rules, their
type, their associated skill, and what classification sources were used as inputs.
Effective behaviors, represented by the “++” in Table 1, are most analogous to correct
behaviors in traditional ITSs. They are paths in our model of good peer tutoring that are
considered to be beneficial for collaboration quality the majority of the time. For example,
explaining a tutee error was considered to be an ideal behavior (rule 9 in Table 1). The tutee
commission of the error was detected by the CTA, and then explanation of the error was
detected using our automated classification algorithm (whenever an utterance was classified
as previous-step help). There are four effective behaviors in the model.
Somewhat effective behaviors, represented by the “+” in the table, were considered to
be probably beneficial for collaboration quality at any given time. An example of a
somewhat effective behavior can be found at rule 2 in Table 1, where if the tutee makes
an error, the peer tutor should give help. This rule is only somewhat effective because
while there are many situations where peer tutor should give help after an error, it is not
necessarily the best course of action in all cases; there may be many situations where tutees
should repair their own error. There were six somewhat effective behaviors in the model.
Somewhat ineffective behaviors, represented by the “-”in the table, were considered to be probably detrimental to collaboration quality. An example of a somewhat
ineffective behavior is rule 13 in the table, where peer tutors give next step help after
an error. While this may be beneficial in cases where tutees are struggling, in many
situations it is likely to be more beneficial if peer tutors address tutee misconceptions
in their help. The model consisted of five somewhat ineffective behaviors.
Finally, ineffective behaviors, represented by the “–”in the table, were behaviors
considered to be detrimental to collaboration quality in most cases (analogous to
buggy rules in traditional ITSs). The model consisted of four ineffective behaviors.
Two of the ineffective behaviors are related to the timely help skill: one where the
tutee commits three errors in a row without a peer tutor response (rule 6), and another
where the tutee makes three help requests without a peer tutor response (rule 4) These
rules are two of the three rules in the model where the model firing is triggered on
peer tutor inaction, rather than on peer tutor action, which is unusual in typical ITSs.
They are indicators that the tutee is in trouble, and the peer tutor, possibly because of
a gap in domain knowledge, is struggling to help.
Rules were represented in a fully configurable xml file, and then parsed as part of a
model tracing engine in the java code. Through this representation, we were able to
modify the rule set on the fly, without having to recompile the code. This implementation strategy is ideal for testing the collaborative model. During piloting, we easily
iterated on the model by adjusting its parameters and the specifics of the rule definitions.
While our approach has much in common with traditional production rule modeling,
there are a few key differences. As in much production rule modeling, our approach

Int J Artif Intell Educ (2014) 24:33–61

45

Table 1 Production rules in APTA. Each rule has an associated skill, and is mapped to effective (++),
somewhat effective (+), somewhat ineffective (−), or ineffective behaviors (−−). The classification column
describes how the elements of the rule are assessed, either by the peer tutor (self), the help-giving agent
(machine), or the Cognitive Tutor Algebra (CTA)
#

Rule

Skill

Type

1

IF tutee makes a help request
THEN peer tutor gives help

Timely

++

IF tutee makes an error
THEN peer tutor gives help

Timely

IF tutee self-explains
THEN peer tutor gives help

Timely

4

IF tutee makes 2 help requests in a row
THEN tutee makes a 3rd help request

Timely

–

5

IF tutee makes a help request
THEN tutee makes an error

Timely

-

6

IF tutee makes 2 errors in a row
THEN tutee makes a third error

Timely

–

7

IF tutee makes a correct stepAND peer tutor
gives next-step help
AND tutee takes another correct step
THEN peer tutor gives next-step help

Timely

-

IF tutee makes an error
THEN prompt for explanation

Appropriate

IF tutee makes an error
THEN give previous-step help

Appropriate

IF tutee makes an error
AND tutee makes a help request
THEN prompt for explanation

Appropriate

IF tutee makes an error
AND tutee makes a help request
THEN give previous-step help

Appropriate

IF tutee makes an error
AND tutee makes a help request
THEN give next-step help

Appropriate

13

IF tutee makes an error
THEN give next-step help

Appropriate

–

CTA

14

IF the peer tutor gives help
THEN help is conceptual

Conceptual

+

self

15

IF the peer tutor gives next-step help
THEN help is not conceptual

Conceptual

-

self

16

IF peer tutor labels help
THEN give help

Classifiers

++

If peer tutor labels no help
THEN don’t give help

Classifiers

2
3

Classification
self
machine

+

CTA
machine

+

self
machine
self
self
self
CTA
CTA
CTA
CTA
machine
CTA
machine

8
9
10

11

12

17

++

CTA
self

++

CTA
machine

+

CTA
self
self

+

CTA
self
machine

-

CTA
self
machine
machine
machine
machine
self
machine

+

self
machine

46

Int J Artif Intell Educ (2014) 24:33–61

Table 1 (continued)
#

Rule

Skill

Type

Classification

18

If peer tutor labels no help
THEN give help

Classifiers

–

self

19

IF peer tutor labels help
THEN don’t give help

Classifiers

-

machine
self
machine

focuses on student actions (in this case, peer tutor actions). Our model of peer tutor
behaviors contains rules for both correct and incorrect behaviors, although we created
four categories: 2 levels of correctness (effective and somewhat effective) and 2 levels of
buggy-ness (somewhat ineffective and ineffective). The rules in our model form a selfcontained peer tutor, in that they can reproduce a subset of peer tutor help-giving
behaviors (although we cannot produce natural language dialogue). On the other hand,
our approach is in many ways more congruent with constraint-based tutoring systems
than with model tracing tutors (Mitrovic et al. 2003). As in constraint-based tutors, the
model that we use is a subset of the domain, and anything not represented in our model is
considered to be correct behavior, rather than incorrect behavior. Most importantly, a
single peer tutor action can cause multiple rules to fire, which is not the case with
traditional model-tracing tutors. These adaptations are acknowledgments to the illdefined nature of supporting collaboration, where we give peer tutors the benefit of
the doubt by not immediately assuming the actions we do not understand are wrong, and
associate several rules for tutoring behaviors with a single collaborative action.
Each production rule contributed to an overall assessment of the degree to which
students had mastered one of the four skills described above: timely help, appropriate
help, conceptual help, and classifier use. Timely help, covered by model rules 1–7,
represented whether the peer tutor gave help when tutees needed it. Appropriate help,
covered by rules 8–13, represented whether peer tutors gave the type of help that
tutees needed. For the purposes of our model, it can be divided into two subskills:
whether peer tutors prompted tutees to self-explain after an error, and whether peer
tutors provided error feedback after an error. Conceptual help, covered by rules 14–
15, represented whether peer tutors gave help that included an explanation. Finally,
use of classifiers, represented by rules 16–19, covered whether peer tutors used
sentence classifiers appropriately.
We used an algorithm based on Bayesian knowledge tracing to update a running
assessment of peer tutor mastery of these four skills (Corbett and Anderson 1994).
Knowledge tracing computes the likelihood that students have mastered a skill for
any particular opportunity to do so (called p[Ln]), based on four parameters: The
probability that peer tutors had mastered the skill before the opportunity (p[Ln-1]), the
probability that peer tutors will learn the skill at the next opportunity (p[T]), the
probability that the peer tutor will exhibit an effective collaborative behaviour even if
they have not mastered the skill (p[G]), and the probability that the peer tutor will
exhibit an ineffective collaborative behaviour even if they have mastered the skill
(p[S]). For each student step, the algorithm first calculates the probability that
students had mastered the skill prior to taking the step, and then the algorithm
calculates the probability that students currently know the skill.

Int J Artif Intell Educ (2014) 24:33–61

47

While this type of knowledge tracing has been used in individual settings for the
assessment of domain knowledge, it has not to our knowledge been used in collaborative
settings. Thus we present a novel application of standard knowledge tracing algorithms. We
made a few modifications to the basic knowledge tracing algorithm to make it more
appropriate for collaborative settings. First, at the beginning of the tutorial session, we set
p(Lo) to 0.9 for the collaborative skills. The system assumes that students know how to
collaborate effectively, unless they repeatedly provide evidence that they do not. This
approach gives students the benefit of the doubt on initial interactions with each other,
assuming that the students know more about collaboration than the system does until a
pattern of interaction suggests that students do indeed need help. Next, according to the
approach in Beck and Sison (2006), we inflated the values of p(G) and p(S), the probabilities
that students behave effectively even if they do not know the skill and ineffectively even if
they do know the skill. We also varied those probabilities based on the valence of the fired
rule (e.g., p(S) was larger for a “somewhat ineffective” rule than for an “ineffective” rule).
This approach takes parameters that were initially meant to represent human error and
incorporates system error as well. p(G) now included the probability that the system
characterizes student responses as effective even if they do not know the skill, and p(S)
now included the probability that the system characterizes student responses as ineffective
even if they have mastered the skill. Essentially, as in Beck and Sison (2006), we redefine
p(G) to be the broader probability of a “false positive”, and p(S) to be the broader probability
of a “false negative”. For example, for the conceptual help skill, we defined p(G)=0.20 and
p(S)=0.30 for effective and ineffective rules. For somewhat effective and somewhat ineffective rules, p(G)=0.25 and p(S)=0.375. The higher values for p(G) and p(S) for somewhat
effective and somewhat ineffective rules means that the skill assessment increases less after a
positive interaction, and also decreases less after a negative interaction.
In Table 2, there is an example model and knowledge trace. The students were solving the
problem “” for w, and the tutee had just subtracted 3n from both sides, which was incorrect
(#1 in the Table 2). The peer tutor then said “factor out n” and labeled it as a “hint”. The
computer classified the chat as next-step help and nonconceptual help (#2 in the table), and
recognized that it came immediately after an incorrect step. This action fires model rules 2,
12, 15, and 16, meaning that the knowledge tracing assessments for all four skills are
updated (#3 in Table 2). The assessment of peer tutor mastery of timely help and use of
classifiers increases, while the appropriate help and conceptual help skills decrease. The
effective and ineffective rules fired lead to more fluctuation in the skills than the somewhat
effective and somewhat ineffective rules.
Support
We used a combination of the model tracing and knowledge tracing results to decide
when to give students reflective prompts in the chat window, based on encoded
support strategies and a pedagogical model (see the Support component of Fig. 2).
The model tracing specified which skills students had exhibited or failed to exhibit
with any particular action (firing particular production rules), and then the knowledge
tracing recomputed the probability that students had mastered a skill. Each rule was
linked to a set of feedback thresholds. If a rule fired, and the skill adjustment
associated with the rule fell within one of the feedback thresholds linked to the rule,
then the rule-threshold combination was added to a list specifying the possible

48

Int J Artif Intell Educ (2014) 24:33–61

Table 2 Modeling and feedback example from Phase 3. The system uses the problem state to model
student collaborative knowledge and select appropriate feedback
(1) Problem State
Problem

Solve for

Last step

Last evaluation

State

-wn+3n=w

w

Subtract 3n

Incorrect

-wn=w-3n

(2) Assessment
Tutor chat

Self labeling

Machine labeling

Machine labeling

Domain context

“Factor out n”

Hint

Next-step help

Non-conceptual

Incorrect step

(3) Model and Knowledge Tracing
Timely

Appropriate

Conceptual

Classifiers

p(Ln-1)

0.903

0.911

0.903

0.794

Rule fired

2

13

15

16

Valence

+

–

-

++

p(Ln)

0.956

0.742

0.81

0.931

(4) Feedback Selection
Rule-threshold

Timely

Appropriate

Conceptual

Classifiers

None

[0.6,1]

[0.7,1]

None

Add to list

No

Yes

Yes

No

Priority

n/a

4

3

n/a

Chosen

No

Yes

No

No

(5) Message Choice
Possible prompts “Tutor, do you know if your partner has made a mistake?”, “Tutor, can you explain
your partner’s mistake?”, “Tutor, is there anything your partner doesn’t understand
right now about the problem?”
Prompt chosen

“Tutor, is there anything your partner doesn’t understand right now about the
problem?”

feedback to send. Each rule-threshold combination was assigned a particular priority,
and once the list was complete, the rule-threshold with the highest priority was chosen to
be the target rule for a reflective prompt. If there was a tie in priority, then the target rulethreshold was randomly chosen out of the tied candidates. Finally, each rule-threshold
had a set of similar prompt messages associated with it, and one of the messages
associated with the rule-threshold target was randomly chosen. The message was either
sent to both students in the chat window or privately to the peer tutor, and this parameter
was linked to the rule-threshold combination. This decision to make multiple messages
for any given situation ensured that students rarely received the same message twice.
Expanding on the example in the previous section, although all the skills were
adjusted, only the values of the appropriate and conceptual help fell within the
feedback threshold, and were added to the list (#4 in Table 2). Because the rule
associated with the targeted skill had the highest priority, it was selected to be
delivered to both students in the chat window. Out of all the possible prompts that
could be chosen (#5 in Table 2), the prompt “Tutor, is there anything your partner
doesn’t understand right now?” was sent to the students.
Both priorities and thresholds were assigned based on a combination of theoretical
model of the relative importance from each rule, piloting, and data from previous
studies. Priorities were assigned on a scale from 1 to 10. As an example, there were

Int J Artif Intell Educ (2014) 24:33–61

49

two feedback thresholds based on rule #6: One was [0, 0.3) with a priority of 1, and one
was [0.3, 7) with a priority of 2. Both thresholds were given extremely high priorities
because the peer tutor likely needs immediate feedback if they are not responding to
three tutor errors in a row. The messages associated with the higher threshold for this rule
(e.g., “Tutor, is your partner still taking the right steps? Make sure both sides of the
equation still equal each other”) tended to have less urgency than the messages associated with the lower threshold (e.g., “Tutor, do you know what your partner should do?
Try asking the computer for a hint”), as a lower skill assessment indicated the peer tutor
needed more explicit support. As a second example, rule #15 had a single feedback
threshold: [0,.7) with a priority of 7. If a student gives high level help, and they are not in
the habit of doing so (i.e., their p[Ln] was under .7), then giving them positive feedback
to reinforce their behavior was a somewhat low priority, but still built into the system. In
contrast, if they were already relatively proficient at the skill (p[Ln]>= .7), we did not
give any positive feedback. We tuned these parameters in piloting so that peer tutors
received one prompt for every three peer tutor actions.
As the primary use of the skill estimates was to trigger feedback, our knowledge
tracing algorithm was designed and tuned more to accomplish this goal than to actually
estimate student collaborative skills. We are aware our approach violates certain assumptions of knowledge tracing (Corbett and Anderson 1994): In our approach, a
student can transfer from a learned to an unlearned state, and more than one skill maps
to a given action. Future work will be to modify the core knowledge tracing algorithm to
incorporate the assumptions we use as part of the peer tutoring model, evaluate the
algorithm against a human-coded assessment of student skills, and improve the algorithm to be a more fine-grained representation of peer tutor skills. However, we think our
approach is an effective first pass at using a knowledge tracing approach to mitigate the
uncertainty inherent in collaborative scenarios, by using the (potentially flawed) skill
estimates to trigger feedback rather than specific peer tutor behaviors.

Data Collection
Hypotheses & Conditions
We conducted a study in order to evaluate whether our adaptive system had a
beneficial effect on peer tutor learning, and what features of the system may have
contributed to its effectiveness. In the introduction, we presented two possible
mechanisms that might link receiving adaptive support to benefitting more from a
peer tutoring interaction. First, students who receive relevant support might more
easily apply it to their interactions, improving the quality of their collaboration and
learning. Second, students who believe they are receiving adaptive support may feel
more motivated to engage with the support and take appropriate action, improving the
quality of their collaboration and learning.
We tested these hypotheses and evaluated the effectiveness of our system by
comparing the help-giving support in the adaptive version of our system (the real
adaptive condition) to two conditions that received non-adaptive computer prompts
in the chat window. In one of the nonadaptive conditions (the real nonadaptive
condition), students were told that the prompts were nonadaptive. In the second

50

Int J Artif Intell Educ (2014) 24:33–61

nonadaptive condition (the told adaptive condition), students were told that the
prompts were adaptive, when they in fact were not. If it is relevant support that
matters, only students who actually receive adaptive support (i.e., those students in
the real adaptive condition) should improve their collaboration quality and their
domain learning. On the other hand, if engagement is most important, and students
in the told adaptive condition believe the system is adaptive, told adaptive students
should also improve their collaboration quality and domain learning real nonadaptive
students. By distinguishing between these two explanations for the effectiveness of
adaptive collaborative support, we can better evaluate the effectiveness of the specific
adaptivity implemented in our system.
Participants
Participants were 130 high-school students (49 males, 81 females) from one high
school, ranging from 7th to 12th grade, and currently enrolled in Algebra 1 (46
students), Geometry (49 students), or Algebra 2 (35 students). While the literal
equation solving unit was one that all students had theoretically received instruction
on in Algebra 1, many students did not remember seeing the material before. The
teacher we were working with identified this unit as challenging for the students. The
study was run at the high school, either immediately after school or on Saturdays.
Thus the study was somewhere in between a lab study and a classroom study: It was
run in a school context and with several students at once, but it was not run during
school hours as part of regular classes. All students were paid 30 dollars for their
participation. Students participated in sessions of up to 9 students at a time (M group
size=7.41, SD=1.35). Each session was randomly assigned to one of the three
conditions, and then within each pair students were randomly assigned to the role
of tutee or tutor. While APTA was designed to be used in a reciprocal scenario, for the
purposes of this evaluation students retained the same role throughout the whole
study; if a student was assigned the role of tutor, he or she tutored throughout the
entire session.
Students came with partners that they had chosen, except in the case of 12 students
who came to the study alone and were assigned to their partners by the researchers. The
results of Ogan, Finkelstein, Walker, Carlson, and Cassell (2012) in their analysis of this
dataset suggested that students who self-selected their partners and thus collaborated
with friends interacted more effectively and learned more than students who have their
partners selected by researchers. We thus excluded the 12 students with researcherselected partners from our analysis, as they were small in number, may have been from a
different population than those with self-selected partners, and had a substantially
different interaction experience. Two dyads were excluded due to logging errors with
the computer prompts. Further, for ease of scheduling, we sometimes assigned an extra
student to a given session (in case somebody did not show up at the assigned time).
There were 8 students who worked alone over the course of the session. Thus, a total of
108 students were included in the analysis. There were 45 same-gender pairs and 8
cross-gender pairs. We did a median split on pretest score to reclassify students as lowability or high-ability, and then counted homogenous and heterogeneous pairs. 31 pairs
were homogenous (two low-ability or two high-ability students) and 22 were heterogeneous (one low ability and one high ability student).

Int J Artif Intell Educ (2014) 24:33–61

51

Procedure
During the study, students took a 20 min pretest. Next, students spent 20 min in a
preparation phase, solving problems individually using the CTA. All students worked on
easier problems in the literal equation solving unit, which consisted of factoring
problems where the variable terms were on the same side of the equation. Students then
spent 30 min in the tutoring phase, with the peer tutor helping their partner with factoring
problems where the variable terms were on both sides of the equation. Students took up
to 10 min to answer several survey questions on their motivational state, and then spent
another 30 min in the tutoring phase. Students took a 20 min domain posttest.
In the tutoring phase, we varied whether students received adaptive support or not
and whether they thought it was adaptive or not. The nonadaptive support was
implemented as follows. We gave students pseudo-random prompts that ensured that
the timing and content of the prompts did not depend on their behavior. Every time
students would have received a reflective prompt were they in the adaptive condition,
they never received a prompt in the fixed condition. However, we ensured that they
received a prompt within the next three turns, essentially yoking the nonadaptive
prompt to the adaptive prompt. We randomly choose the content of the prompt, but
we never choose content that would have been relevant to the yoked adaptive prompt.
All other support across conditions was parallel (i.e., all students received adaptive
correction support).
We manipulated whether we told students that support was adaptive or
nonadaptive prior to the tutoring phase. The adaptive instructions were as follows:
“The computer will watch you tutor, and give you targeted advice when you need it
based on how well you tutor. Both you and your partner will see the help in the chat.”
The nonadaptive instructions were as follows: “From time to time, the computer will
give you a general tip chosen randomly from advice on good collaboration. Both you
and your partner will see the help in the chat.” As students began to use APTA, they
were given further instruction, including directions to indicate how they felt about the
reflective prompts using thumbs up and thumbs down widgets (Fig. 1c). To motivate
the use of these widgets and reaffirm the experimental manipulation, students in the real
and told adaptive conditions were told: “We will use that information to improve the
computer’s ability to track what you’re doing and give you advice you can use.” Students
in the real nonadaptive condition were told: “We will use that information to describe
which pieces of advice can go into the pool of advice we randomly select from.”
Measures
To assess students’ individual learning we used counterbalanced pretests and posttests, each containing 7 conceptual items (some with multiple parts), 5 procedural
items, and 2 items that demanded a verbal explanation. Tests were approved by the
coordinating classroom teacher, and were administered on paper. We scored answers
on these tests by marking whether students were correct or incorrect on each item
part, and then summing the item scores to get a total score.
As a manipulation check, we assessed perceived adaptivity with five items asking
students how adaptive they thought the system was (e.g., for the peer tutor: “The
computer gave advice at times when it was useful”) and how positively they perceived

52

Int J Artif Intell Educ (2014) 24:33–61

the system’s effects (e.g., for the tutee: “The advice the computer gave improved how
well my partner tutored me”). Items were rated on a 7-point likert scale.
All collaborative process variables were logged, including tutee problem-solving actions,
sentence classifiers selected by both students, and chat actions made by both students. Along
with the student actions, we logged computer tutor responses, which included both the
system’s evaluation of the action and the computer assistance students received.
We coded each instance of support delivered by the computer tutor for whether it
was relevant to the current context, as defined by the tutee-tutor interactions spanning
the last instance of tutee dialogue, peer tutor dialogue, and tutee problem step. To be
relevant, negative feedback had to meet three criteria:
1. Not contradict the current situation. E.g., feedback that referred to an error
contradicts the situation if tutees had not made an error.
2. Refer to something students were not currently doing. E.g., feedback that
prompted for more conceptual help would only be relevant if students were not
giving conceptual help.
3. If students were to follow the help, their interaction would be improved, based on
the four skills. E.g., feedback that tells the peer tutor to give help would improve
the interaction if the tutee had asked for help and not received it.
For positive feedback to be relevant, students had to be doing something to merit
positive feedback, and then the advice given by the feedback had to meet the above
criteria #1 and #3. To calculate inter-rater reliability, two raters independently coded
30 % of the data, with a kappa of 0.70. Conflicts were resolved through discussion.

Results
Domain Learning
Our first step was to determine whether students learned more from the real adaptive
support condition than from the other two nonadaptive conditions. We conducted a twoway (condition x role) ANCOVA, controlling for pretest, with posttest as the dependent
variable. Pretest score was significantly predictive of posttest score (F[1,99]=103.73,
p<0.001; see Table 3). There was a significant effect of condition on posttest
(F[2,99]=4.03, p=0.021, eta2=0.075), indicating that the adaptivity of support had a
positive effect on student posttest performance. A planned comparison of the effects of
receiving real adaptive support revealed that it indeed had a significant effect
(F[1,99]=7.73, p=0.006), while a planned comparison of the effects of receiving
support that students were told was adaptive revealed that this manipulation did not
have a significant effect (F[1,99]=0.990, p=0.322). These results suggest that the real
adaptive support we implemented in APTA had a more beneficial effect than
nonadaptive support. Telling students support was adaptive did not have a beneficial
effect on learning compared to telling them support was not adaptive.
While the effect of role on posttest was not significant (F[1,99]=0.194, p=0.661),
there was a significant interaction effect between condition and role (F[2,99]=3.87,
p=0.024, eta2=0.073). Applying the planned comparisons to the interaction effect
revealed that the effects of real adaptivity had significantly differential effects on peer

Int J Artif Intell Educ (2014) 24:33–61

53

Table 3 Mean pre and posttest results for tutors and tutees by condition. Standard deviations are in
parentheses
Condition

Peer Tutor

Peer Tutee

Pretest

Postttest

Pretest

Postttest

Real Adaptive

0.27 (0.14)

0.42 (0.18)

0.28 (0.16)

0.37 (0.22)

Told Adaptive

0.25 (0.13)

0.29 (0.14)

0.27 (0.16)

0.29 (0.16)

Real Nonadaptive

0.30 (0.16)

0.29 (0.18)

0.24 (0.15)

0.35 (0.21)

tutors and tutees (F[1,99]=3.95, p=0.05), as did the effects of told adaptivity
(F[1,99]=7.33, p=0.008). Inspecting student learning across role and condition (see
Table 3), we see that while all students benefit from the real adaptive condition, peer
tutors benefit more from the told adaptive condition than the real nonadpative
condition, but tutees benefit more from the real nonadaptive condition than the told
adaptive condition. The perception that the advice is relevant when it is not, as in the
told adaptive condition, may impede the tutoring abilities of the peer tutor and thus
may lead to less tutee learning.
Support Relevance
Given the encouraging results that students in the real adaptive condition learned
more, we then verified that the support students received in the real adaptive
condition was indeed more adaptive. Indeed, the total number of prompts each pair
received from the adaptive system was not significantly different between conditions
(F[2,50]=0.660, p=0.522; see Table 4). We conducted an ANCOVA with relevant
prompts received as the dependent variable, condition as an independent variable, and
total prompts received as a covariate. While the number of relevant prompts students
received was not significantly different between conditions (F[2,47]=0.057, p=0.944;
see Table 4), total prompts received was predictive of relevant support
(F[1,47]=266.34, p<0.01). The interaction between condition and total support given
was significantly related to relevant prompts received (F[2,47]=17.32; p<0.001).
Inspecting Fig. 3, you can see that as total instances of feedback increase, the greater
the difference in amount of relevant feedback between the real adaptive conditions
and the other two conditions. Over a long period of time, with many instances of

Table 4 Mean support received and relevant support received per group by condition. Perceived adaptivity
by condition and role. Standard deviations are in parentheses
Support Characteristics

Perceived Adaptivity

Condition

Total support

Relevant support

Peer tutor

Tutee

Real Adaptive

15.20 (12.17)

12.60 (11.34)

5.88 (1.82)

4.15 (0.85)

Told Adaptive

17.84 (9.67)

7.63 (5.63)

5.29 (1.40)

3.65 (1.23)

Real Nonadaptive

14.26 (11.86)

5.68 (4.44)

4.60 (1.34)

3.80 (1.01)

54

Int J Artif Intell Educ (2014) 24:33–61

Fig. 3 Graph of relevant support compared to total support by condition. As total support instances
increase, the adaptive condition has more relevant support than the other two conditions

feedback, the differences between adaptive and nonadaptive support become
apparent.
Perceptions of Adaptive Support
We next examined whether student perceptions of adaptive support differed across
conditions, as a check of our “told adaptive” manipulation. We had intended the told
adaptive and real adaptive conditions to perceive support as more adaptive than the
real nonadaptive condition. Thus, we had asked students questions intended to assess
whether they perceived the support they received as adaptive. We conducted a twoway ANCOVA (including both peer tutors and tutees) with perceived adaptivity as
the dependent variable, condition and role as independent variables, and relevant
feedback received as a covariate. There was no significant differences across conditions on the perceived adaptivity measure (F[2,80]=.330, p=0.72). Student perceptions did not appear to be affected by the experimental manipulation of telling them
support was adaptive. However, the amount of relevant support students received did
predict the perceived adaptivity of the system (F[1,80]=34.08; p<0.001). In addition,
there was a significant interaction between role and relevant support (F[1,80]=9.99,
p=0.002). The scatterplot in Fig. 4 reveals that as the amount of relevant support
increases, so do students perceptions of the adaptivity of support, and this effect is
stronger for peer tutors than for tutees. Because peer tutors were the targets of the

Int J Artif Intell Educ (2014) 24:33–61

55

Fig. 4 Graph of relevant support and perceived adaptivity. As relevant support increases, perceived
adaptivity increases. This effect is stronger for peer tutors than tutees

support, it seems logical that their perceptions of the support would be more affected
by its relevance.
Frequencies of Support Given
Finally, to get a better sense of what specific support students received based on our
model, we looked at the real adaptive condition to determine how often each rule
fired, how frequently support was given to students based on the rule, and how
frequently that support was relevant. The means and standard deviations of this
measure are presented in Table 5. Within our model, the most frequent peer tutor
behavioural profile involves giving help after an error that focuses on the next step, is
nonconceptual, and isn’t appropriately labelled with a sentence classifier. Our system
often gave students feedback on these behaviors. Our system also reinforced particular positive behaviors that students engaged in: giving help after a help request,
giving conceptual help, and labelling utterances appropriately with sentence classifiers. The majority of support given in response to each rule was coded as relevant,
except for the cases of prompting for self-explanation and using sentence classifiers
when help is not being given. This result may have been the fault of poorly designed
feedback messages or a failure of the automated classifier to detect help. There are
also some rules in the model that did not fire at all (e.g., 4, 10, 11, and 12). These

56

Int J Artif Intell Educ (2014) 24:33–61

Table 5 Production rules in APTA , along with how frequently they fired, how frequently they triggered
feedback, and how many of those feedback instances were relevant. Each rule has a type representing
whether the rule is mapped to effective (++), somewhat effective (+), somewhat ineffective (−), or
ineffective behaviors (−−)
#

Rule

Type

# Times Rule
Fired M (SD)

Support
Instances
M (SD)

Relevant Support
Instances M (SD)

1

IF tutee makes a help request
THEN peer tutor gives help

++

1.40 (2.38)

0.67 (1.40)

0.47 (0.92)

2

IF tutee makes an error
THEN peer tutor gives help

+

4.27 (2.69)

0 (0)

0 (0)

3

IF tutee self-explains
THEN peer tutor gives help

+

0.20 (0.56)

0 (0)

0 (0)

4

IF tutee makes 2 help requests in a row
THEN tutee makes a 3rd help request

–

0 (0)

0 (0)

0 (0)

5

IF tutee makes a help request
THEN tutee makes an error

-

0.67 (1.40)

0.625 (1.36)

0.625 (1.36)

6

IF tutee makes 2 errors in a row
THEN tutee makes a third error

–

1.33 (2.02)

1.33 (2.02)

1.27 (1.87)

7

IF tutee makes a correct stepAND
peer tutor gives next-step help
AND tutee takes another correct step
THEN peer tutor gives next-step help

-

1.13 (2.07)

0.73 (1.71)

0.67 (1.63)

8

IF tutee makes an error
THEN prompt for explanation

++

0.47 (1.30)

0.13 (0.35)

0.07 (0.26)

9

IF tutee makes an error
THEN give previous-step help

++

0.33 (0.49)

0 (0)

0 (0)

10

IF tutee makes an error
AND tutee makes a help request
THEN prompt for explanation

+

0 (0)

0 (0)

0 (0)

11

IF tutee makes an error
AND tutee makes a help request
THEN give previous-step help

+

0 (0)

0 (0)

0 (0)

12

IF tutee makes an error
AND tutee makes a help request
THEN give next-step help

-

0 (0)

0 (0)

0 (0)

13

IF tutee makes an error
THEN give next-step help

–

4.20 (2.68)

4.13 (2.61)

3.00 (2.48)

14

IF the peer tutor gives help
THEN help is conceptual

+

2.27 (2.25)

1.00 (1.77)

0.93 (1.79)

15

IF the peer tutor gives next-step help
THEN help is not conceptual

-

11.13 (10.44)

5.00 (4.52)

4.33 (4.67)

16

IF peer tutor labels help
THEN give help

++

4.53 (5.34)

0 (0)

0 (0)

17

If peer tutor labels no help
THEN don’t give help

+

0 (0)

0 (0)

0 (0)

18

If peer tutor labels no help
THEN give help

–

9.20 (12.16)

2.73 (5.46)

2.73 (5.46)

19

IF peer tutor labels help
THEN don’t give help

-

0.27 (0.59)

0.27 (0.59)

0.07 (0.26)

Int J Artif Intell Educ (2014) 24:33–61

57

rules may have not fired because we relied on tutees to use sentence classifiers to
label their help requests, and tutees may have failed to do so or done so inaccurately.
Overall, however, the model behaved as expected and provided relevant support to
tutees.

Discussion
In this paper, we discussed the assessment, modeling, and support provided in APTA,
an intelligent tutoring system for peer tutoring. We demonstrated that APTA is indeed
adaptive, in that it provides students with significantly more relevant support than
non-adaptive control conditions, and also that APTA improves student learning over
non-adaptive controls. We also found that student perceptions of the adaptivity of the
system was directly linked to the actual adaptivity of support, making it difficult to
convince students that support was adaptive when, in fact, it was not. Based on these
results, our system was a successful implementation of an ACLS.
Our approach acknowledged the ambiguity inherent in assessing, modeling, and
supporting collaboration. One challenge in supporting collaboration is the difficulty
in automatically assessing student dialogue. APTA uses multiple sources of information to assess collaborative state: a combination of problem-solving information,
student self-classifications of their own chat, and machine classifications of student
chat. The incorporation of problem-solving information into our assessment would
not have been possible without having built our help-giving tutor on top of the
Cognitive Tutor Algebra, but we believe it was a main contributor to the success of
the system. The multiple channels of information allowed us to understand better the
context in which peer tutoring actions were executed, allowing us to interpret those
actions more effectively.
Another challenge in supporting collaboration is the difficulty inherent in modelling collaborative behaviors by representing which interactions should be employed,
and under what contexts. APTA maintains a production-rule style model of effective
and ineffective peer tutor actions that it uses to compare the current collaborative state
to an ideal model. The modeling in APTA draws both from model-tracing tutors and
constraint-based tutors to incorporate the advantages of both approaches. Like modeltracing tutors, APTA focuses on peer tutoring actions and models both correct and
incorrect behaviors. Like constraint-based tutors, APTA assumes that anything outside the model is correct rather than incorrect, and an action can fire multiple rules.
By assuming that anything outside the model is correct, it gives the peer tutors the
benefit of the doubt when they take unexpected tutorial actions. One innovation here
is that APTA employs different levels of correctness, distinguishing, for example,
between effective behaviors and somewhat effective behaviors. Using this technique,
we can mitigate some of the ambiguity in judging the potential benefits of a particular
peer tutor behavior by representing (albeit in a discrete way) the likelihood that the
behaviour is effective.
A final challenge in supporting collaborative learning is understanding how to
provide adaptive support based on the uncertain information delivered in the assessment and modeling phases. APTA uses Bayesian Knowledge Tracing to evaluate peer
tutor skills and provide reflective prompts in a chat window. Our approach is used as

58

Int J Artif Intell Educ (2014) 24:33–61

a trigger for feedback, and was not yet designed to be a fine-grained representation of
student skills. However, it allows us to base feedback on the overall pattern of peer
tutor behaviors rather than specific peer tutor actions, making us more certain that
peer tutors will get positive feedback when they are truly excelling and negative
feedback when they are truly struggling. Future work will be to iterate on the
algorithm so it can more accurately assess collaborative skills.
We demonstrated that the techniques we used produced more relevant help than
nonadaptive techniques, using a human coding of relevant help. Developing our
coding scheme was a challenging and iterative process, in part because any given
instance of support may appear to be relevant in multiple situations. Indeed, the
nonadaptive technique we used still produced a large proportion of relevant help,
even when we purposefully inhibited all responses congruent with our adaptive
model of support. It is possible that with carefully designed feedback messages,
nonadaptive techniques could produce similar amounts of relevant help as adaptive
techniques. Nevertheless, the amount of relevant help affected student perceptions of
the adaptivity of the system, suggesting that students do recognize and respond to
adaptive support. Additionally, as relevant support increased, peer tutors perceived
the system to be more adaptive than tutees. This suggests that in order to reap the full
benefits of relevant support, it should be directed at all parties in the interaction. This
link between relevant support and perceptions of adaptivity may have negated the
effects of our second manipulation, where we told students support was adaptive
when, in fact, it was not.
Our empirical results support some common-sense ideas about the benefits of
adaptive support. We demonstrated that students in the adaptive support conditions
learned more than students in the nonadaptive conditions. As the amount of support
students received increased, the difference between the adaptive condition and the
nonadaptive conditions became more apparent. While if students receive few instances of support it may not be necessary that the support be highly adaptive, as
students use the system over a longer period of time they will be able to distinguish
between adaptive and nonadaptive support. The techniques presented in this paper
bring us closer to implementing an ACLS that can provide students with the long-term
adaptive support they need to collaborate more effectively.
Acknowledgments This work was supported by the Pittsburgh Science of Learning Center, NSF Grant
#SBE-0836012, and a Computing Innovations Fellowship, NSF Grant #1019343. Thanks to Ruth Wylie for
her comments and Sean Walker for his work on the assessment algorithm.

References
Baghaei, N., Mitrovic, A., & Irwin, W. (2007). Supporting collaborative learning and problem solving in a
constraint-based CSCL environment for UML class diagrams. International Journal of ComputerSupported Collaborative Learning, 2(2–3), 159–190.
Baker, M., & Lund, K. (2003). Promoting reflective interactions in a CSCL environment. Journal of
Computer Assisted Learning, 13(3), 175–193.
Barros, B., & Verdejo, M. F. (2000). Analysing student interaction processes in order to improve
collaboration. The DEGREE approach. International Journal of Artificial Intelligence in Education,
11(3), 221–241.

Int J Artif Intell Educ (2014) 24:33–61

59

Beck, J. E., & Sison, J. (2006). Using knowledge tracing in a noisy environment to measure
student reading proficiencies. International Journal of Artificial Intelligence in Education, 16,
129–143.
Chi, M. T., De Leeuw, N., Chiu, M. H., & LaVancher, C. (1994). Eliciting self-explanations improves
understanding. Cognitive Science, 18(3), 439–477.
Constantino-González, M. A., Suthers, D., & Escamilla de los Santos, J. (2003). Coaching web-based
collaborative learning based on problem solution differences and participation. IJAIED, 13, 263–299.
Corbett, A. T., & Anderson, J. R. (1994). Knowledge tracing: Modeling the acquisition of procedural
knowledge. User Modeling and User-Adapted Interaction, 4(4), 253–278.
Dillenbourg, P. (2002). Over-scripting CSCL: The risk of blending collaborative learning with instructional
design. In Kirschner, P. A. (Ed.), Three worlds of CSCL: Can we support CSCL? 61–91.
Dillenbourg, P., Baker, M. J., Blaye, A., & O’Malley, C. (1995). The evolution of research on collaborative
learning. Learning in Humans and Machine: Towards an interdisciplinary learning science. 189–211.
Dragon, T., Floryan, M., Woolf, B., & Murray, T. (2010). Recognizing dialogue content in student
collaborative conversation. In Intelligent Tutoring Systems (pp. 113–122). Berlin: Springer.
du Boulay, B., Avramides, K., Luckin, R., Martinez-Miron, E., Rebolledo-Mendez, G., & Carr, A. (2010).
Towards systems that care: A conceptual framework based on motivation, metacognition and affect.
International Journal of Artificial Intelligence in Education, 20(3), 197–229.
Fantuzzo, J. W., Riggio, R. E., Connelly, S., & Dimeff, L. A. (1989). Effects of reciprocal peer tutoring on academic
achievement and psychological adjustment: A component analysis. Journal of Educational Psychology, 81(2),
173–177.
Fischer, F., Mandl, H., Haake, J., & Kollar, I. (2007). Scripting computer-supported collaborative
learning—cognitive, computational, and educational perspectives. Computer-supported collaborative
learning series. New York: Springer.
Gweon, G., Rose, C., Carey, R., & Zaiss, Z. (2006, April). Providing support for adaptive scripting in an
on-line collaborative learning environment. In Proceedings of the SIGCHI conference on Human
Factors in computing systems (pp. 251–260). ACM.
Israel, J., & Aiken, R. (2007). Supporting collaborative learning with an intelligent web-based system.
International Journal of Artificial Intelligence and Education, 17(1), 3–40.
Johnson, D. W., & Johnson, R. T. (1990). Cooperative learning and achievement. In S. Sharan (Ed.),
Cooperative learning: Theory and research (pp. 23–37). NY: Praeger.
Karakostas, A., & Demetriadis, S. (2011). Enhancing collaborative learning through dynamic forms of support: the
impact of an adaptive domain-specific support strategy. Journal of Computer Assisted Learning, 27(3), 243–258.
King, A., Staffieri, A., & Adelgais, A. (1998). Mutual peer tutoring: Effects of structuring tutorial
interaction to scaffold peer learning. Journal of Educational Psychology, 90, 134–152.
Koedinger, K. R., Anderson, J., Hadley, W., & Mark, M. (1997). Intelligent tutoring goes to school in the
big city. International Journal of Artificial Intelligence in Education, 8, 30–43.
Kollar, I., Fischer, F., & Slotta, J. D. (2005). Internal and external collaboration scripts in web-based science
learning at schools. In T. Koschmann, D. Suthers, & T.-W. Chan (Eds.), Proceedings of the
International Conference on Computer Support for Collaborative Learning 2005 (pp. 331–340).
Mahwah: Lawrence Erlbaum Associates.
Kollar, I., Fischer, F., & Hesse, F. W. (2006). Collaboration scripts—A conceptual analysis. Educational
Psychology Review, 18(2), 159–185.
Kumar, R., Rosé, C. P., Wang, Y. C., Joshi, M., & Robinson, A. (2007). Tutorial dialogue as adaptive
collaborative learning support. In R. Luckin, K. R. Koedinger, & Greer J. (Eds.), Proceedings of
Artificial Intelligence in Education (pp. 383–390). IOS Press.
Lazonder, A. W., Wilhelm, P., & Ootes, S. A. W. (2003). Using sentence openers to foster student
interaction in computer-mediated learning environments. Computers and Education, 41, 291–308.
Magnisalis, I., Demetriadis, S., & Karakostas, A. (2011). Adaptive and intelligent systems for collaborative
learning support: A review of the field. IEEE Transactions on Learning Technologies, 4(1), 5–20.
Mayfield, E., & Rosé, C. P. (2011, June). Recognizing authority in dialogue with an integer linear
programming constrained model. In Proceedings of Association for Computational Linguistics.
7Mclaren, B. M., Scheuer, O., & Mikšátko, J. (2010). Supporting collaborative learning and e-discussions
using artificial intelligence techniques. International Journal of Artificial Intelligence in Education,
20(1), 1–46.
McManus, M. M., & Aiken, R. M. (1995). Monitoring computer-based colaborative problem solving.
Journal of Artificial Intelligence in Education, 6(4), 307–336.
Mitrovic, A., Weerasinghe, A. (2009). Revisiting ill-definedness and the consequences for ITSs. In: The
14th Conference on Artificial Intelligence in Education, pp. 375–382. IOS Press, Marina Del Ray

60

Int J Artif Intell Educ (2014) 24:33–61

Mitrovic, A., Koedinger, K. R., & Martin, B. (2003). A comparative analysis of cognitive tutoring and
constraint-based modeling. In P. Brusilovsky, A. Corbett, & F. D. Rosis (Eds.), Proceedings of the Ninth
International Conference on User Modeling, UM 2003 (Vol. LNAI 2702) (pp. 313–322). Berlin: Springer.
Muldner, K., Burleson, B., VanLehn, K. (2010). “Yes!”: Using tutor and sensor data to predict moments of
delight during instructional activities. In Proceedings of the International Conference on User
Modeling and Adaptive Presentation, 159–170.
Ogan, A., Aleven, V., Kim, J., & Jones, C. (2011). Persistent Effects of Social Instructional Dialog in a
Virtual Learning Environment. In Proc. 15th International Conference on AIED, pp.238-246.
Ogan, A., Finkelstein, S., Walker, E., Carlson, R., & Cassell, J. (2012). Rudeness and Rapport: Insults and
Learning Gains in Peer Tutoring. In Proceedings of the 11th International Conference on Intelligent
Tutoring Systems. ITS ‘12 (pp. 11–21). Berlin: Springer.
Ploetzner, R., Dillenbourg, P., Preier, M., & Traum, D. (1999). Learning by explaining to oneself and to
others. In P. Dillenbourg (Ed.), Collaborative learning: cognitive and computational approaches (pp.
103–121). UK: Elsevier Science Publishers.
Roll, I., Aleven, V., McLaren, B. M., & Koedinger, K. R. (2011). Improving students’ help-seeking skills
using metacognitive feedback in an intelligent tutoring system. Learning and Instruction, 21, 267–280.
Rosatelli, M., & Self, J. (2004). A collaborative case study system for distance learning. International
Journal of Artificial Intelligence in Education, 14(1), 97–125.
Roscoe, R. D., & Chi, M. (2007). Understanding tutor learning: Knowledge-building and knowledgetelling in peer tutors’ explanations and questions. Review of Educational Research., 77(4), 534–574.
Rosé, C., Wang, Y. C., Cui, Y., Arguello, J., Stegmann, K., Weinberger, A., et al. (2008). Analyzing collaborative
learning processes automatically: Exploiting the advances of computational linguistics in computer-supported
collaborative learning. International Journal of Computer-Supported Collaborative Learning, 3(3), 237–271.
Rummel, N., & Weinberger, A. (2008). New challenges in CSCL: Towards adaptive script support. In G. Kanselaar,
Jonker, V., Kirschner, P.A., & Prins, F. (Eds.), Proceedings of the Eighth International Conference of the
Learning Sciences (ICLS 2008), Vol 3 (pp. 338–345). International Society of the Learning Sciences.
Schoenfeld, A. H. (1992). Learning to think mathematically: Problem-solving, metacognition, and sense
making in mathematics. In D. Grouws (Ed.), Handbook for research on mathematics teaching and
learning (pp. 334–370). New York: Macmillan.
Slavin, R. E. (1996). Research on cooperative learning and achievement: What we know, what we need to
know. Contemporary Educational Psychology, 21, 43–69.
Soller, A., Jermann, P., Mühlenbrock, M., & Martinez, A. (2005). From mirroring to guiding: A review of state of the
art technology for supporting collaborative learning. International Journal of Artificial Intelligence in Education,
15(4), 261–290.
Suebnukarn, S., & Haddawy, P. (2006). Modeling individual and collaborative problem-solving in medical
problem-based learning. User Modeling and User-Adapted Interaction, 16(3–4), 211–248.
Tedesco, P. (2003). MArCo: Building an artificial conflict mediator to support group planning interactions.
International Journal of Artificial Intelligence in Education, 13(1), 117–155.
Tsovaltzi, D., Rummel, N., McLaren, B. M., Pinkwart, N., Scheuer, O., Harrer, A., et al. (2010). Extending
a virtual chemistry laboratory with a collaboration script to promote conceptual learning. International
Journal of Technology Enhanced Learning, 2(1), 91–110.
VanLehn, K. (2006). The behavior of tutoring systems. IJAIED, 16(3), 227–265.
VanLehn, K. (2011). The relative effectiveness of human tutoring, intelligent tutoring systems, and other
tutoring systems. Educational Psychologist, 46(4), 197–221.
Vieira, A. C., Teixeira, L., Timóteo, A., Tedesco, P., Barros, F. A., Lester, J. C., et al. (2004). Analyzing online collaborative dialogues: The OXEnTCHÊ-Chat. In F. Paraguaçu (Ed.), Proceedings of the 7th
International Conference on Intelligent Tutoring Systems (pp. 315–324). Germany: Springer.
Vizcaíno, A., Contreras, J., Favela, J., & Prieto, M. (2000). An adaptive collaborative environment to
develop good habits in programming. In G. Gauthier, C. Frasson, & K. VanLehn (Eds.), 5th
International Conference on Intelligent Tutoring Systems, ITS’2000 (pp. 262–271). Berlin: Springer.
Walker, E., Walker, S., Rummel, N., & Koedinger, K. (2010). Using problem-solving context to assess help
quality in computer-mediated peer tutoring. In V. Aleven, J. Kay, & J. Mostow (Eds.), Proceedings of
the International Conference on Intelligent Tutoring Systems (pp. 145–155). Berlin: Springer.
Walker, E., Rummel, N., & Koedinger, K. R. (2011). Designing automated adaptive support to improve
student helping behaviors in a peer tutoring activity. International Journal of Computer-Supported
Collaborative Learning, 6(2), 279–306.
Webb, N. M., & Mastergeorge, A. (2003). Promoting effective helping behavior in peer-directed groups.
International Journal of Education Research, 39, 73–97.

Int J Artif Intell Educ (2014) 24:33–61

61

Webb, N. M., Troper, J. D., & Fall, R. (1995). Constructive activity and learning in collaborative small
groups. Journal of Educational Psychology, 87(3), 406.
Weinberger, A., Ertl, B., Fischer, F., & Mandl, H. (2005). Epistemic and social scripts in computer–
supported collaborative learning. Instructional Science, 33(1), 1–30.

Analyzing Frequent Sequential Patterns of Learning
Behaviors in Concept Mapping
Shang Wang

Erin Walker

Ruth Wylie

School of Computing, Informatics, and School of Computing, Informatics, and Mary Lou Fulton Teachers College
Decision Systems Engineering
Decision Systems Engineering
Arizona State University, Tempe AZ,
Arizona State University, Tempe AZ, Arizona State University, Tempe AZ,
USA
USA
USA
Ruth.Wylie@asu.edu

swang158@asu.edu

Erin.A.Walker@asu.edu

ABSTRACT
Computer-based concept mapping learning environments can
produce large amounts of data on student interactions. The ability
to automatically extract common interaction patterns and
distinguish between effective and ineffective interactions creates
opportunities for researchers to calibrate feedback and assistance
to better support student learning. In this paper, we present an
exploratory workflow that assesses and compares student learning
behaviors with concept maps. This workflow employs a
sequential pattern mining technique to classify interaction patterns
among students and determine specific behavior patterns that lead
to better learning outcomes.

This paper explores the use of data mining methods to
systematically build and analyze models of student behaviors as
they interact with our concept map environment. This paper
approaches student modeling by analyzing similar and different
behavior patterns between various types of student groups.

2. WORKFLOW METHOD
2.1 Data Inputs
The raw data are xml files, where each item in corresponds to a
specific action performed by students on the system. There are 8
fields of information being logged in each student action.
1.

Student ID, identifying the student interacting with the
system.

Keywords
Data mining, sequential pattern mining, student behavior, concept
mapping.

2.

Session ID, denoting the session of the study.

3.

Time, recording the time stamp of the action.

1. INTRODUCTION

4.

Time zone, indicating the time zone of the system.

Concept maps are visual representations of knowledge, with
concept nodes representing concepts in the knowledge structure
and links denoting relationships among concepts. Concept
mapping has been widely used as an active learning tool in
educational contexts and research has shown the positive effect of
concept mapping in helping students organizing and summarizing
knowledge [1][2]. One of the main disadvantages of concept
mapping is the complexity of the task. Learners who lack
expertise often feel overwhelmed and de-motivated [3].

5.

Selection, representing where student is interacting with.
For example, concept map view, textbook view, etc.

6.

Action, denoting the specific student action. For
example, adding a concept node from the textbook,
navigating to a new page, linking two concepts,
hyperlinking navigation, etc.

7.

Input, representing the input of the action. For example,
an input for adding a concept from the textbook would
be “root” and an input for navigating to a new page
would be “page 5”.

8.

Page number, indicating the text page when the action
is performed.

To facilitate students in concept map construction, we designed a
personalized and interactive concept mapping learning
environment integrated within a digital textbook. Students are
able to create maps directly from the textbook, which allows them
to better relate concepts with the textbook content. The system
offers a hyperlinking navigation feature where, after creating the
concept map from the textbook, students are able to click on the
concept nodes and navigate to where these nodes were added from
the textbook. We hypothesize that this feature supports learning
by offering flexibility in comparing and finding connections
between concepts that are located in different pages,
To examine the effect of interactive concept mapping learning
environments, we have conducted a week-long study with 32 high
school students using the system as a substitute for a paper-andpencil based concept mapping activity while they learn about their
current science textbook chapter. Students in the study were
randomly assigned into two conditions: A hyperlinking condition,
where nodes in the concept maps were hyperlinked with the
textbook, and a non-hyperlinking condition. Pre and post tests
were given before and after the study to measure learning
outcomes.

These raw data are generated in real-time and are sent to a server
after each session for further analysis.
Apart from the log files, we also use pre and post test results and
final concept maps for analysis. Pre and post tests consist of 30
multiple choice questions. The test results can be used to classify
students into high and low performance groups and help us
determine specific behavior sequences that distinguish the better
groups from the weaker ones. Similarly, the concepts created by
students enable us to understand how different behavior patterns
affect concept mapping.

2.2 Workflow Model
Action abstraction is the first step of our workflow, in which we
categorize a specific sequence of low granularity actions into
aggregated actions that indicate specific learning behaviors. This
step filters out irrelevant information and combines qualitatively
similar actions (Table 1). For example, a student might flip 10

pages in the textbook quickly when searching for certain sections
in the textbook. Instead of analyzing these 10 navigation actions
separately, we consider them as one aggregated action called
“Quick Search” (QS).
Aggregated
Behavior

Log Action

Quick
(QS)

Students flip several pages quickly to go to
a specific page

Search

Long Stay (LS)

Students don’t perform any actions for a
long period of time

Read
(RA)

and

Add

Students read the textbook and add a
concept node into the concept map

Read
(RL)

and

Link

Students read the textbook and link two
concepts in the concept map

Add and
(AD)

Link

Students add a concept node to the concept
map and quickly link it to another node

Read and Delete
Node (RD)

Students read the textbook and delete a
node from the concept map

Hyperlinking
Navigation (HN)

Students click on a concept node to
navigate to the page where it’s created

Back and Forth
(BF)d

Student navigate between a few pages
back and forth within a short period of
time

Table 1. Student actions and aggregated behaviors
We classify all the student actions into 8 aggregated student
behaviors, which are easier for sequential pattern mining and
student modelling. For example, a back and forth (BF) behavior
could be an indication that the student is comparing two linked
concepts in the concept map. A long stay (LS) behavior might
suggest that the student is spending a lot of effort reading the
textbook or distracted and not motivated.
After this classification, we apply sequential pattern mining
techniques to extract interesting behavior patterns. Research in the
literature has applied sequential pattern mining techniques to a
variety of educational data. Perera and colleagues showed the
importance of leadership and group interactions towards learning
success using k-means clustering to find groups of similar teams
and similar individuals, and employing a modified version of the
Generalized Sequential Pattern (GSP) mining algorithm to extract
student behavior patterns [4]. Martinez et al. applied clustering
and sequential pattern mining techniques to determine the
sequences of actions that characterize high-achieving and lowachieving learners [5].
In our workflow, we plan to use sequential mining techniques to
identify the frequent sequential patterns from the two conditions
for further analysis.

2.3 Workflow Outputs
The first output from the workflow model is a list of sequential
patterns extracted from the log files depending on the minsup.
These patterns represent frequent student behaviors that occurred
during the concept mapping task. After extracting frequent
behavior patterns, we further cluster these patterns based on
different student groups.

1.

Hyperlinking
and
No-hyperlinking:
Comparing
sequential patterns between hyperlinking and nonhyperlinking conditions suggests how hyperlinking
navigation affects student behaviors.

2.

High performance and low performance: Comparing
frequent patterns in these two conditions identifies
certain behavior patterns that distinguish better learning
groups than the lower ones.

3.

Better concept maps and weaker concept maps:
Comparing sequential patterns in these two conditions
would help us understand how behavior patterns affect
the final concept maps created by students.

3. DISCUSSION
We present a workflow that first creates aggregated behaviors
from the log files and then applies sequential pattern mining to
extract behavior patterns from various conditions. Comparisons of
student behaviors between the hyperlinking and non-hyperlinking
condition would help us understand how the hyperlinking feature
affects student navigation. Questions like does the navigational
flexibility in the hyperlinking condition yield more comparison
between concepts located in different pages in the textbook would
be interesting to explore. Comparisons of student behaviors
between different types of student groups would help us examine
specific behavior patterns that lead to high learning outcomes and
better concept maps, which provides opportunities for researchers
to develop feedback or scaffolding methods to support these
behaviors. This work opens doors for teachers or automated
systems to intervene and provide feedback more appropriately. It
also enables researchers to develop concept mapping learning
environment that offers automation to replace the ineffective
behaviors while preserving and supporting behaviors that yield
better learning outcomes.

4. ACKNOWLEDGMENTS
This research was funded by NSF CISE-IIS-1451431 EAGER:
Towards Knowledge Curation and Community Building within a
Postdigital Textbook.

5. REFERENCES
[1] Nesbit, John C., and Olusola O. Adesope. "Learning with
concept and knowledge maps: A meta-analysis." Review of
educational research 76.3 (2006): 413-448.
[2] Novak, Joseph D., and Alberto J. Cañas. "The theory
underlying concept maps and how to construct and use
them." (2008).
[3] Davies, Martin. "Concept mapping, mind mapping and
argument mapping: what are the differences and do they
matter?." Higher education 62.3 (2011): 279-301.
[4] Perera, Dilhan, et al. "Clustering and sequential pattern
mining of online collaborative learning data." Knowledge
and Data Engineering, IEEE Transactions on 21.6 (2009):
759-772.
[5] Martinez-Maldonado, Roberto, et al. "Analysing frequent
sequential patterns of collaborative learning activity around
an interactive tabletop." Proceedings of the International
Conference on Educational Data Mining 2011 (EDM 2011).
2011.

Planning with Action Abstraction and Plan Decomposition Hierarchies
Christel Kemke
University of Manitoba
ckemke@cs.umanitoba.ca
Abstract
Useful and suitable action representations, with accompanying planning algorithms are crucial for the task
performance of many agent systems, and thus a core issue
of research on intelligent agents. An efficient and expressive representation of actions and plans can allow planning systems to retrieve relevant knowledge faster and to
access and use suitable actions more effectively [18]. Two
general approaches have been pursued in the past;
STRIPS-based planners, which construct plans from
scratch, based on primitive action descriptions and planners using pre-defined Plan Decompositions Hierarchies,
also known as Hierarchical Task Networks. In our research, we integrated both an inheritance hierarchy of
actions, using STRIPS-like action descriptions, with a
plan decomposition hierarchy, which consists of pre-defined plan schemata. This combination is suitable for a
richer action and plan representation, and thus an improved planning algorithm. We implemented and tested
this approach for a prototypical example application: the
travel planning domain.

1. Introduction
It is often advantageous to organize descriptions about
concepts in the world in a hierarchical form, by placing
more general, abstract, or complex concepts on the higher
levels and more specific or simpler concepts on the lower
levels. There is more than one way of specifying such a
hierarchical relationship for (action) concepts: they can be
arranged in a taxonomy, e.g. flying and driving as subconcepts of traveling; or in a decomposition hierarchy, e.g.
buying a ticket as part of a complex, higher level traveling
action [9]. In a taxonomy, a general category subsumes
more specific categories. Each category has certain properties, and a subclass inherits the properties of its superclass. In a decomposition hierarchy, subordinate actions
are actions that must be executed in a specific temporal
sequence in order to execute the respective higher-level
action. For example, buying a ticket and going through
security are both subordinate actions of the higher-level
flying-via-plane action, and the ticket must be bought
before security is passed.

Proceedings of the IEEE/WIC/ACM International
Conference on Intelligent Agent Technology (IAT'06)
0-7695-2748-5/06 $20.00 © 2006

Erin Walker
Carnegie-Mellon University
erinwalk@andrew.cmu.edu
Weida used STRIPS-like action descriptions and defined a taxonomy through adding and removing literals in
the condition clauses [15, 16]. Similarly, Knoblock’s ALPINE system generates action abstraction hierarchies by
adding or removing precondition literals [12]. A general
formalization of action taxonomies, which we took as
basis for the integrated planning algorithm, has been described by Kemke [11]
The other form of organizing action concepts into a hierarchy is based on part-of relationships. These hierarchies are known in planning as Plan Decomposition Hierarchies or Hierarchical Task Networks (HTN): a collection of tasks with constraints on the task order, preconditions and postconditions for the tasks [5].
Both types of planners, i.e. domain-independent
STRIPS-based planners as well as domain-dependent
HTN Planners may benefit from a better organized representation of actions: Actions arranged in a taxonomy can
be retrieved more efficiently, and the number of actions
that must be explicitly represented can be reduced significantly. Our goal was thus to link an action taxonomy with
a plan decomposition hierarchy, in order to improve a
planner’s efficiency by making use of the compact action
taxonomy and the pre-structured plan decomposition hierarchy.

2. Knowledge Representation
Although currently the most efficient planners are
domain-independent, research suggests that real-world
problems require some sort of knowledge-based planning.
Wilkins and DesJardins argue that planners using domain
knowledge will be more capable of tackling complex
problems involving complex reasoning [18]. One focus of
our research was thus the representation of domain concepts, objects, actions, and plans.

2.1. Domain Objects and Concepts
A concept refers to a set of domain objects. For example, the concept Transportation(A) is the set of all
transportation vehicles, where A is a variable that can be
replaced by any constant referring to a particular vehicle.
A single instance of Transportation named bus1 is

represented using the predicate Transportation(bus1).
Domain concepts are represented using basic elements,
which refer to concepts, roles, and features. A role describes a particular relationship between any number of
concepts. The role At(Station(X), City(Y)) represents
the fact that station X is located in city Y. A feature is a
function that takes one concept as input and produces
another as output. For example, StartLocation(Train(X), TrainStation(Y)) maps a train station to
each train.
The world state, i.e. the facts that are true about the
world at a given time, is represented by a collection of
instantiated concepts, roles, and features. The truth value
of an instantiated element is positive, if the element is
contained in the current world state. An abstract element,
e.g. Bus(X), evaluates to true, if it can be unified with an
element in the current world state, e.g. bus1, that can
replace the variable X, yielding an element Bus(bus1),
which is true in the in the current world state.

2.2. Taxonomy of Concepts
Abstract concepts of the domain are organized in a
taxonomy. For example, the concept PublicTransportation is defined as a subclass of the Transportation concept and a superclass of the Train concept. Characteristic
for a taxonomy is that a concept can have any number of
subclasses but only one superclass, thus exhibiting a treelike structure.
Determining whether one concept c is a subclass of
another concept c’ involves thus a simple linear search,
starting with the node c and moving up in the hierarchy
towards the root-node:
Algorithm 1: testSubset
Check the set of all superconcepts of c, Super(c)
if c’∈ Super(c) return true and STOP
else return false and STOP

The hierarchy also facilitates generalization, specification, and instantiation of concepts. A concept can be generalized by navigating up in the hierarchy to retrieve one
of its superclass concepts, and it can be specified by navigating down to retrieve a subclass concept. Regarding
instantiation, a variable in a more general abstract concept
can be replaced with an instance of a more specific concept. For example, if Train is a subset of Transportation, the instance Train(train1) can be used to instantiate Transportation(X), and Transportation(train1) will then evaluate to true in the world. The
operations enabled by the concept hierarchy also apply to
roles and features.

3. Action Representations
3.1 Action Concepts
Action concepts are inherently dynamic and specified
through the changes of the world states they cause. Action
concepts contain a list of parameters (features and roles)
that provide a description of the action. In traditional
STRIPS fashion, actions have preconditions, or facts that
must be true for the action to occur, and effects, or facts
that are true in the world after the action occurs. The preconditions and effects are logical formulas: features,
roles, and concepts that are joined by conjunctions and
disjunctions. They relate directly to what is changed in the
world state: When an action is executed, the preconditions
are removed from the world state and the effects are
added. Here is a description of a sample action:
Action 1: Travel
Mode: Transportation
Start Location: Location(D)
End Location: Location(E)
Distance: (0 km, 100000km]
Precondition: AND (At(Person(X), City(B)),
At(Location(D), City(B)))
Postcondition: AND (At(Person(X), City(C)),
At(Location(E), City(C)))

One action a is equal to another action a’ when the action names are the same, each parameter of a is equal to
the corresponding parameter of a’, and the precondition
and effects formulas are equivalent in both actions.

3.2. Action Taxonomy and Subsumption
An action a is a subclass of another action a’, if every
parameter in a’ can be found in a (inherited or modified)
and if the preconditions and effects of a’ are a subset of
those of a.
This is illustrated in the following example with the
action BusTravel, which is a subconcept of Travel:
Action 2: BusTravel
Mode: Bus
StartLocation: BusStop(D)
EndLocation: BusStop(E)
Distance: (0 km, 100km]
Cost: [$.25, $100.00]
Precondition: AND (At(Person(X), City(B)),
At(Bus Stop(D), City(B)),
Can Afford(Person(X), $.25, $100))
Postconditions:
AND (At(Person(X), City(C)),
At(Bus Stop(E), City(C)))

An action can be specialized or generalized based on
the action description. To specialize an action, one of its
parameters can be made more specific, one of the ele-

Proceedings of the IEEE/WIC/ACM International
Conference on Intelligent Agent Technology (IAT'06)
0-7695-2748-5/06 $20.00 © 2006

ments of the precondition or effects formula can be made
more specific, or an element can be added to a conjunction in the precondition or effects formula. To generalize
an action, one of its parameters can be made more general, one of the elements of the precondition or effects
formula can be made more general, or an element can be
added to a disjunction in the precondition or effects formula.
Based on this subsumption relationship, action concepts can be organized in a taxonomy. An action can be
retrieved from or classified in the hierarchy by finding the
node in the hierarchy that is equal to the action or the
most specific superclass of the action: If the located node
is equal to the desired action, then the action has been
located in the hierarchy. Otherwise, the action can be
added to the hierarchy by adding a link between it and its
superclass, and connecting the appropriate subclasses of
the superclass to the action.

3.3 Action Instantiation
The world state contains action descriptions as well as
concepts, roles, and features. An action concept is instantiated by replacing its parameters with instances from
a matching action description in the world state, and then
evaluating the truth value of the instantiated preconditions. If the preconditions are true, then the action can be
executed by removing the preconditions from the world
and adding the action’s effects.

4. Plan Decomposition
Particular action concepts are also organized in plan
decompositions, which detail how one action can be executed by performing a sequence of component actions. A
plan decomposition is a two-level hierarchy, with the first
level consisting of a series of goal nodes, and the second
level consisting of their respective decompositions. Here
is a plan decomposition example:
Action 3: Public Travel
BuyTicket - >
TravelToStation - >
BoardTransportation - >
Ride - >
DisembarkTransportation

The plan decomposition hierarchy is defined by expanding a second level node with a new decomposition, in
a recursive fashion.

Proceedings of the IEEE/WIC/ACM International
Conference on Intelligent Agent Technology (IAT'06)
0-7695-2748-5/06 $20.00 © 2006

5. Planning with the Integrated Hierarchy
In planning with a regular plan decomposition hierarchy, separate plan decompositions are required for more
or less specific goal actions, like flying-in-a-plane, traveling, and traveling-by-train. In the integrated hierarchy,
the full decomposition is only needed for one of them,
and the subsumption links between the actions are sufficient to infer the decompositions for the other two. The
integrated planner is able to locate the goal action in the
plan decomposition hierarchy, find component actions
required to achieve the goal, specify them according to
the parameters of the goal action, and locate each component action in the abstraction hierarchy.
For example, given a traveling-by-plane action, the
planner locates Traveling in the plan decomposition hierarchy, recognizes BuyTicket as a component action,
and then substitutes BuyTicket with BuyTicketOnline
using the abstraction hierarchy.

5.1. A Simple Planning Example
The goal action is to travel a distance of 100km:
Action 4: Travel
Distance: [100km, 100km]

The desired distance is matched to the parameters of
subclasses of Travel (i.e., Bus Travel, Train Travel,
Plane Travel), and Bus Travel is selected because its
definition of the Distance feature most closely matches
the definition of the goal action.
The planner checks for a plan decomposition of BusTravel, and noticing that it does not exist, checks for a
plan decomposition of its superclass: PublicTravel.
Finding that, it decomposes PublicTravel into
BuyTicket, TravelToStation, BoardTransportation, Ride, and DisembarkTransportation. Each of
those actions would get specified based on the parameters
for BusTravel.
For example, the Mode parameter in BuyTicket becomes “Bus”, and the parameter Cost: [$.25, $100.00].
The planner then recognizes that the component actions
have no decompositions, and instantiate and execute each
component action.

5.2. Complex Planning
In most cases, the component actions in the decomposition will be non-primitive, and will serve as additional
goal actions for the planner. Continuing the previous example, the next goal action for the planner is a BuyBusTicket action. Although this action can be located in the
action taxonomy, the closest goal node in the plan decomposition might be a simple Buy action that decomposes into FindSeller - > PayForObject. Those actions

would be specified to include parameters that identify a
bus ticket is being bought, the cost of the ticket, and the
location of the seller. If these actions cannot be decomposed, they will then be instantiated and executed.
This process becomes more complicated in the case of
an action like BoardTransportation, since the three
subclasses of BoardTransportation, i.e. BoardPlane,
BoardTrain, and BoardBus, may have different decompositions. In this case, the planner needs to locate
BoardTransportation in the action hierarchy, navigate
down the hierarchy to find a more specific node with the
appropriate decomposition (e.g. BoardBus), and continue planning from that node.

6. Evaluation
Action Description. The representation for action
concepts uses a set of parameters in addition to preconditions and effects. Many qualities of an action such as its
manner and intention are far easier represented as properties of the action instead of as preconditions and effects
formulas. Detailed preconditions and effects formulas are
particularly useful for the extensive search through a
problem space of basic actions executed by STRIPS planners. However, domain-dependent real-world planning requires information about the action itself [18], and therefore it is appropriate to provide a representation of the
properties of the action.
Action Taxonomy. Organizing the action concepts in
a taxonomy based on the subsumption relationship for actions provides a more efficient representation. Search in a
hierarchy is more efficient than searching an unorganized
list of actions. Further, retrieval of an action from the hierarchy allows information about related actions to be
collected. In a domain-dependent system, information
about the action’s superclasses and subclasses becomes
useful both for constructing plans and for natural language understanding of the action. This extra information
cannot be retrieved, when the actions in a particular domain are not organized in a meaningful fashion.
Because the subsumption relation for actions is formalized, actions can be classified on-the-fly in the taxonomy, which is more efficient than when the superclass
and subclass relationships for the actions must be predefined. Given that domain-dependent taxonomies will
likely be extensive, this classification ability is more efficient because fewer actions have to be defined in the taxonomy. The type of action that can be classified in the
taxonomy is not restricted, and consequently the taxonomy can refer to as broad or as specific a domain as is
needed, and thus makes the system flexible and easily
adaptable to different domains.
Additionally, the taxonomy of actions allows the inheritance of action parameters. Consequently, action parameters can be incompletely defined when they are being

Proceedings of the IEEE/WIC/ACM International
Conference on Intelligent Agent Technology (IAT'06)
0-7695-2748-5/06 $20.00 © 2006

added to the taxonomy or used for planning. In real-world
situations, the complete description of the action is not
always available, and a planning or natural language understanding system must take this incomplete information
into account. Because the taxonomy supports inheritance,
inferences about the parameters of incomplete actions can
be made based on the actions’ positions in the hierarchy
[3].
Finally, the subsumption relationship makes it easy to
create new actions by specifying or generalizing the parameters and formulas. Knoblock uses this technique in
his ALPINE system to create abstraction hierarchies from
action representations, which can then be used for planning [12]. This capability makes fewer action representations required in general. An action can be created at the
desired level of specificity or generality simply by modifying the parameters, and does not have to be explicitly
defined in the taxonomy. Similarly, the taxonomic representation becomes more expressive, because a large number of actions can be created from a small number of action descriptions. For example, in an action representation
without a formal subsumption relationship, the actions
Buy, BuyTicket, and BuyTicketOnline would have
to be explicitly defined. In the action taxonomy, only one
of the actions need to be explicitly defined, and the others
can be generated when they become necessary. When
actions are being used for planning or natural language
understanding, the ability to make them as specific or
general as one would like is practical, and makes the system flexible.
Planning Method. With an action taxonomy linked to
a plan decomposition hierarchy, planning requires fewer,
explicitly defined decompositions. In a similar manner to
Knoblock’s ALPINE system [12], the decompositions can
be defined for more abstract actions, and then the parameters can be specified to create concrete plans. Consequently, a single decomposition for PublicTravel is
required for all forms of public travel, while in a standard
system there are decompositions for BusTravel, TrainTravel, PlaneTravel, BoatTravel, and any other desired form of travel. In the integrated action taxonomy
and decomposition hierarchy, the TrainTravel decomposition can be created as needed by specifying the PublicTravel decomposition, and does not have to be explicitly represented. Another implication of this system is that
a wide variety of decompositions can be generated from a
single decomposition, just as a wide variety of actions can
be generated from a single action.

7. Conclusion and Outlook
We have suggested and described a planning algorithm, which integrates action abstraction and plan decomposition hierarchies. The representation of actions
and plans in this approach is more expressive and efficient, through the use of hierarchical structures as well as
pre-defined decompositions of actions and plans. We have
built a taxonomy of actions in the domain of travel by
applying and expanding the formalism for subsumption
and inheritance of actions [11], implemented classification and retrieval algorithms for the action taxonomy,
and shown how the action taxonomy can be integrated
with a simple plan decomposition hierarchy.
Because the integrated action taxonomy and decomposition hierarchy provides a detailed description of the action concepts used, it seems ideal for natural language
processing. The representation of action concepts is similar to a case frame representation, which can be used to
represent the semantics of simple natural language sentences. It would be worthwhile to further explore this.
Overall, combining an inheritance hierarchy of actions
and a decomposition hierarchy of plans seems a useful
approach to deal with complex, real-world planning problems.

Acknowledgements
The planning algorithm has been designed and implemented by Erin Walker under the supervision of Christel
Kemke as part of an Honours Project at the University of
Manitoba. This work was supported by the Natural Sciences and Engineering Research Council of Canada,
NSERC.

References
[1] A. Artale and E. Franconi. A temporal description logic for
reasoning about actions and plans. Journal of Artificial Intelligence Research, 9:463–506, 1998.
[2] A. Artale, E. Franconi, N. Guarino, and L. Pazzi. Part-whole
relations in object-centered systems: An overview. Data Knowledge Engineering, 20(3):347–383, 1996.
[3] J. Doyle and R. Patil. Two theses of knowledge representation: Language restrictions, taxonomic classifications, and the
utility of representation services. Artificial Intelligence,
48(3):261–298, 1991.
[4] K. Erol, J. Hendler, and D. Nau. Semantics for hierarchical
task network planning. Technical report CS-TR-3239,
UMIACS-TR-94-31, 1994.
[5] Kutluhan Erol, James Hendler, and Dana S. Nau. HTN planning: Complexity and expressivity. In Proc. Twelfth National
Conference on Artificial Intelligence (AAAI-94), vol. 2, pages
1123–1128, Seattle, Washington, USA, 1994. AAAI Press/MIT
Press.

Proceedings of the IEEE/WIC/ACM International
Conference on Intelligent Agent Technology (IAT'06)
0-7695-2748-5/06 $20.00 © 2006

[6] M. L. Ginsberg and D. E. Smith. Reasoning about action I: A
possible worlds approach. In Frank M. Brown, editor, The frame
problem in artificial intelligence: Proc. of the 1987 workshop,
pages 233–258. Morgan Kaufmann, 1987.
[7] N. Guarino. Understanding, building, and using ontologies:
A commentary to using explicit ontologies in kbs development.
International Journal of Human and Computer Studies 46: 293310., 1997.
[8] J. Hoffmann and B. Nebel. The ff planning system: Fast plan
generation through heuristic search. Journal of Artificial Intelligence Research, 14:253–302, 2001.
[9] H. A. Kautz. A formal theory of plan recognition and its
implementation. In J. F. Allen, H. A. Kautz, R. Pelavin, and J.
Tenenberg, editors, Reasoning About Plans, pages 69–125.
Morgan Kaufmann Publishers, San Mateo (CA), USA, 1991.
[10] C. Kemke. About the ontology of actions. Technical Report
MCCS-01328, 2001.
[11] C. Kemke. A formal theory for describing action concepts
in terminological knowledge bases. In Canadian Conference on
AI, pages 458–465, 2003.
[12] C. A. Knoblock. Automatically generating abstractions for
planning, artificial intelligence. Artificial Intelligence, 68(2),
1994.
[13] T. Liebig and D. Roesner. Action hierarchies for the automatic generation of multilingual technical documents. In IJCAI97 Workshop Ontologies and Multilingual NLP, International
Joint Conference on Artificial Intelligence, 1997.
[14] D. S. Nau, S. J. Smith, and K. Erol. Control strategies in
HTN planning: Theory versus practice. In AAAI/IAAI, pages
1127–1133, 1998.
[15] A. Weida and D. Litman. Subsumption and recognition of
heterogeneous constraint networks. In Proc. 10th IEEE Conference on Artificial Intelligence, San Antonio (TX), USA, pages
381–388, 1994.
[16] R. Weida and D. Litman. Terminological reasoning with
constraint networks and an application to plan recognition. In
Principles of Knowledge Representation and Reasoning, pages
282–293, 1992.
[17] D. S. Weld. Recent advances in AI planning. AI Magazine,
20(2):93–123, 1999.
[18] D. Wilkins and M. DesJardins. A call for knowledge-based
planning. In AIPS Workshop on Analysing and Exploiting Domain Knowledge for Efficient Planning, 2000.
[19] W. Woods. Conceptual indexing: A better way to organize
knowledge. Technical Report SMLI TR-97-61, Sun Microsystems Laboratories, 1997.

2016 IEEE 16th International Conference on Advanced Learning Technologies

The Effects of Physical Form and Embodied Action in a Teachable Robot for
Geometry Learning
Erin Walker1, Victor Girotto1, Younsu Kim2
1

Kasia Muldner

School of Computing, Informatics, and Decision
Systems Engineering, 2Teachers College
Arizona State University, Tempe, USA
{erin.a.walker, vaugusto, yskim1}@asu.edu

Institute of Cognitive Science
Carleton University
Ottawa, Canada
kasia.muldner@carleton.ca

In our research, we use the robo-Tangible Activities for
Geometry system (rTAG) to explore the impact of physical
form on students’ perceptions of a teachable agent,
motivation, and learning. In rTAG, middle school students
move within a projected coordinate system and interact with
a robot named Quinn. They solve coordinate geometry
problems [e.g., “Plot the point (3,1)”] by giving Quinn
instructions to take actions such as “Move 3 units.” Quinn
responds to students with cognitive and social prompts.
In this paper, we survey related work, describe the rTAG
system, and then present results from a study where we
compare the rTAG system to two other conditions: a
completely virtual version of TAG, run on a personal
computer (vTAG), and a version of TAG where students
interact in the embodied environment but with a virtual
rather than robotic agent (eTAG). We hypothesize that rTAG
will improve learning over the other two conditions because
the robot will lead students to perceive the agent more
positively and be more engaged. We close the paper with a
discussion of the implications of our finding for future
design of robotic learning environments.

Abstract—A teachable agent is a learning companion that
students teach about a domain they are trying to master. While
most teachable agents have been virtual, there may be
advantages to having students teach an agent with a physical
form (i.e., a robot). The robot may better engage students in
the learning activity, and if students take embodied action in
order to instruct the robot, they may develop deeper
knowledge. In this paper, we investigate these two hypotheses
using the rTAG system, a teachable robot for geometry
learning. In a study with 37 4th-6th grade participants, we
compare rTAG to two other conditions, one where students use
embodied action to teach a virtual agent, and one where
students teach a virtual agent on a personal computer. We find
that while there are no significant learning differences between
conditions, students’ perceptions of the agent are influenced by
condition and prior knowledge.
Keywords - robotic learning environment; teachable agent;
personalized learning

I.

INTRODUCTION

In 1988, Chan and Baskin outlined the idea of a computer
as a learning companion, or a virtual agent that “learns”
alongside a human student. Through the interactions between
the virtual agent and human student, the virtual agent can
improve the human student’s learning [1]. One type of
learning companion is a teachable agent, which simulates the
collaborative activity of peer tutoring, in that the student
teaches the agent about the target domain. Prior work has
demonstrated that there are cognitive and social benefits to
peer tutoring [2], and by extension, to having a student teach
an agent [3]. However, it is not fully understood how to
design a teachable agent to maximize student learning.
To date, the majority of teachable agent systems have
been virtual. However, there may be several potential
advantages to having students teach a robot rather than a
virtual agent. First, the physical presence provided by a
robotic agent strengthens users’ perceptions of having a
social partner more than a virtual agent, and thus may better
socially engage students [4]. Second, students benefit from
learning through embodied, physical interactions [5], which
robotic platforms naturally support. In a robotic
environment, the effects of a robot’s social behaviors may be
heightened; in an embodied environment, students may
develop deep knowledge by linking concrete embodied
representations to the underlying domain formalisms.
2161-377X/16 $31.00 © 2016 IEEE
DOI 10.1109/ICALT.2016.129

II.

BACKGROUND

A. Teachable Agents
Teachable agents have emerged from the body of
research on how students benefit from tutoring other students
(e.g., [2]). The most investigated teachable agent system is
Betty’s Brain, designed to help students learn about causal
modeling [6]. Students teach Betty, their agent, by using
resources such as text and videos to draw causal networks.
Students can ask Betty questions that she will answer based
on the network, and at some point, Betty will take a quiz.
Teaching a computer agent is highly beneficial for the
student doing the teaching: it can lead to more learning than
being taught by a computer agent [6], is nearly as effective as
being taught by a human tutor [7], and can be more effective
than classroom instruction [8]. When teaching an agent,
students notice their own misconceptions and elaborate on
their knowledge [6]. Moreover, students tend to be highly
motivated to teach their agents, feel responsible for them,
and so attend more to instructional material [3].
B. Robotic Learning Environments
Our research builds on the teachable agent paradigm by
exploring the effects of a student teaching a robot. There are
381

III.

many platforms that provide students with the opportunity to
program robots. One of the earliest systems was related to
turtle geometry [9]. Students programmed a robotic logo
turtle to turn and move, and a pen attached to it created
geometric figures. When using programmable robot
platforms like LEGO Mindstorms®, children are often asked
to meet certain challenges, ranging from building a soccer
playing robot to creating an interactive park [10]. These
activities are successful at improving programming and
robotics skills [11], but the evidence on whether mathematics
and science outcomes are improved is less convincing, with
existing quantitative evaluations of learning from robotics
programs yielding mixed results [12].
While having students teach a robot is similar to having
students program a robot, a robotic teachable agent should
initiate interaction in ways that the programmable robot
paradigm does not support. In fact, [13] characterizes the
above approaches as using the robot as a passive tool, and
suggest instead using robots as direct facilitators and
coordinators for the learning activity, such that the robot
responds in socially and cognitively appropriate ways. Use
of a robot as an intelligent mediator is promising, with initial
positive results arising out of instituting robotic learning
companions in various settings [14, 15].
Along these lines, there have been a few examples of
teachable robots for student learning. In [16], children aged
3-6 taught a Nao care-receiving robot how to act out
particular English verbs. These children had improved
vocabulary learning compared to a set of children who did
not interact with the robot. In [17], students aged 6-8 taught a
robot to write handwritten letters. This work is promising,
but the benefits of teachable robots are not yet understood.

RTAG SYSTEM

This paper uses a platform for tangible robotic learning
called rTAG [27]. In rTAG, students are told they are giving
instructions to a robot in order to teach it how to solve pointplotting problems. rTAG is currently comprised of the
following three components (see Fig. 1, left). The problem
space consists of a Cartesian plane projected onto a white
foam mat. The teachable agent, Quinn, is an iPod Touch that
displays facial expressions mounted on a LEGO
Mindstorms® NXT 2.0 robot. Finally, the mobile interface
consists of a second iPod Touch that students use to issue
commands to the robot. Each one of these three components
is a web app running from the same server.
When using rTAG students are informed that they will be
teaching Quinn to solve geometry problems. To issue
commands to Quinn, students must first touch the iPod
Touch that displays its face. When the face is touched, a list
of commands available to the student appears on the mobile
interface. This interface also lets the student see the current
problem description and the steps taken thus far to solve it,
check for correctness of the current solution, and, if the
solution is correct, to move to the following problem. To
illustrate, suppose a student is shown the following problem
on the mobile interface: “Plot the point (2, 1)”. At this point,
Quinn will be at the origin with zero degrees of rotation. The
student might tell Quinn to move 2 units, turn 90 degrees,
move 1 unit, and then plot the point. To issue each
command, the student has to touch Quinn’s face.
When a command is issued, the robot moves to the
correct place within the problem space. Its position is tracked
by four Wii remotes attached to the ceiling, which capture
the light from two sets of LEDs attached to the top of the
robot. The communication between the system’s components
is done through Bluetooth in the case of computer-to-robot
and Wii remotes-to-computer; and through a Wireless LAN
network in the case of computer-to-iPods. Communication
between web apps is done via web sockets.
To facilitate learning from rTAG, the robot provides
cognitive support by generating prompts during and after
problem solving, based on the student’s current problemsolving state. The prompts include hints, self-explanations,
and questions. Example prompts include: “I can’t remember
which way the y-axis goes! Do you? Can you walk along the
y-axis for me?”; “In (4, 0), does the 4 tell me to move on the
x-axis or on the y-axis?” Students are instructed to answer
these prompts aloud.
rTAG also contains preliminary social support. Quinn’s
social behaviors are loosely based on attribution theory, and
are generated in response to rTAG’s feedback, representing
the robot’s reaction to whether it got the correct answer.
Quinn’s response includes an emotion displayed on its iPod
and telling the student how it feels about the outcome
(correct vs. incorrect solution) through a message spoken in a
gender-neutral voice. In the message, Quinn attributes the
outcome to factors along two dimensions: the cause of the
outcome (effort or ability) and the agent responsible for it,
namely itself, the student, or both. For example, Quinn might
say, “Yay! I got that right because you are a good teacher.”

C. Research Hypotheses
Our work explores two hypotheses for why a teachable
robot in an embodied environment might enhance learning.
The first hypothesis is: The social affordances of the robot
will improve learning (H1a) and social perceptions of the
robot (H1b). Physical robots can increase social presence
compared to virtual agents [18, 19], and strengthen users’
perceptions of having a social partner [4]. These positive
social perceptions of one’s learning companion are
considered an important factor for learning in computermediated environments [20, 21]. For example, [22] showed
that students’ positive interactions with a social assistive
robot helped in developing geometric thinking and metacognitive skills. The social interactions fostered by the robot
in rTAG may similarly improve student learning. Our
second hypothesis is: The embodied affordances of the
activity will improve learning (H2a). It has been proposed
that humans give meaning to experience by going from
embodied representations, using sensorimotor information,
to symbolic representations [23]. There is much evidence
that gesturing can facilitate learning [e.g., 24, 25], including
complex gestural movements like sliding, stacking and/or
rolling to mimic the shape of an object [26]. Thus,
embodied action facilitated within our learning environment
might improve student learning.

382

Fig. 1. The three TAG conditions tested in the study. In rTAG (left), students interact with a robot, Quinn, using a mobile interface. In eTAG (center),
students interact using the same mobile interface, and a projected circle responds to students’ instructions. The agent’s face is displayed on a screen to
the side of the problem space. In vTAG (right), students interact with Quinn and the problem space through a computer.

skipped several items on the self-report questionnaire,
leaving 35 participants. Participants were randomly assigned
to condition, leaving 10 students in rTAG, 12 in eTAG, and
13 in vTAG. The study took place both at our research lab
and in a spare room at a school. Students received $20.
Students were introduced to the study, and then spent 5
minutes studying a Geometry cheat sheet that discussed the
principles to be learned in the study. They were then given a
15-minute pretest, followed by a questionnaire on their
attributional style (described below). They received 20
minutes of training on how to use the system. Next, students
were given 45 minutes to solve problems with the system. If
students made three incorrect attempts at a given problem,
they were given a list of steps to correctly answer the
problem, which they followed by themselves. Students
solved a mean of 9.20 problems (SD = 5.89). After the 45
minutes, students took a 15-minute posttest, and then
answered self-report questions on perceptions of their
experience. In total, the study took two hours.

Quinn avoids attributions that may provoke negative
responses (e.g. attributing failure to the student’s ability).
IV.

STUDY METHOD

A. Conditions
To investigate our hypotheses regarding rTAG’s effect
on engagement and learning, we compared three conditions:
1. rTAG. This is the embodied teachable robot condition
described in Section III. Students watched the robot
take action in the physical space and interacted with the
robot using both the iPod Touch mounted on the robot
and the mobile interface (Fig. 1, left).
2. eTAG. This is the embodied teachable agent condition.
Students still observed the problem being solved in a
projected, physical space, but Quinn was represented by
a projected circle rather than a physical robot. To make
eTAG equivalent to rTAG in terms of the
expressiveness of the agent, Quinn’s face was displayed
on a monitor beside the problem space (Fig. 1, center).
Instead of touching Quinn’s face to issue a command,
students were instructed to touch the projected circle
with the mobile interface iPod, and then tap on a
“click” button available in the same device to trigger
the list of commands. Thus, in both the rTAG and
eTAG students had to move to the robot to issue
commands, facilitating embodied action.
3. vTAG. This is the virtual teachable agent condition.
Students interacted with a virtual version of TAG on a
personal computer, consisting of both the coordinate
space and Quinn’s face (Fig. 1, right).
If H1a and H1b were true, one would expect rTAG to
improve learning over eTAG and vTAG due to the social
engagement engendered by rTAG. If H2a were true, one
would expect rTAG and eTAG to improve learning over
vTAG, due to the embodied action in rTAG and vTAG.

C. Measures
Pre- and posttests of domain knowledge were two
isomorphic, counterbalanced forms, each consisting of 11
questions spanning factual knowledge (e.g. labeling the
coordinate system), procedural knowledge (e.g. plotting
points), transfer knowledge (e.g. units in the coordinate
system) and embodied knowledge (e.g. moving direction
when plotting points). Each item was assigned 0 or 1 point
and then the total was summed to create the final score.
Perceptions of the robot were collected using [28]’s
validated measurement tool for human-robot interaction,
assessing perceived animacy (6 items), likability (5 items),
intelligence (5 items), and trustworthiness (3 items). In this
measure, students rate the agent on a series of 5-point scales
(e.g., from foolish to sensible, unpleasant to pleasant, or
artificial to lifelike). All items were averaged to create an
overall measure of students’ social perceptions.

B. Participants & Procedure
Participants were 37 students recruited from the 4th, 5th,
and 6th grade (19 male, 18 female). We excluded one student
who scored 100% on the pretest and one student who

V.

STUDY RESULTS AND DISCUSSION

A. Social Perceptions
First, we examined the effects of condition on student

383

perceptions of the agent, testing H1b. We conducted four
ANCOVAs, each with animacy, likability, intelligence, and
trustworthiness as the dependent variable, condition as an
independent variable, and pretest score as a covariate, as our
initial analyses suggested that prior knowledge may be
influencing student perceptions (see Table 1 for means). We
found that condition had a marginally significant effect on
likability (F(2,29) = 3.26, p = 0.053), and significant effects
on intelligence (F(2,29) = 3.59, p = 0.041), and
trustworthiness (F(2,29) = 4.82, p = 0.016). Effects of
condition on animacy was not significant (F(2,29) = 1.30, p
= 0.29). In addition, the interaction between condition and
pretest score had a significant effect on trustworthiness
(F(2,29) = 4.64, p = 0.018) and a marginal effect on
likability (F(2,29) = 3.18, p = 0.058). We computed
correlations for each condition between pretest and these
two perception variables. For the vTAG condition, pretest
was positively related to trustworthiness (r(11) = 0.511, p =
0.074) and likability (r(11) = 0.457, p = 0.116). For the
eTAG condition, pretest did not seem particularly related to
trustworthiness (r(10) = 0.241, p = 0.450) or likability (r(10)
= -0.016, p = 0.960). For the rTAG condition, pretest score
was negatively related to trustworthiness (r(8) = -0.566, p =
0.088) and likability (r(8) = -0.489, p = 0.151).
Interpreting these results, it seems as though low-prior
knowledge students had more positive perceptions of Quinn
in rTAG, while high-prior knowledge students had more
positive perceptions in vTAG. It may be that high-prior
knowledge students were likely to be more receptive to
familiar technologies, while low prior knowledge students
were more likely to be engaged by novel ones.

where we controlled for pretest score and explored the
effects of students’ social perceptions on their posttest
scores. While no correlations were significant, within the
rTAG condition, we found negative relationships between
posttest score and perceived likability (r(7)= -0.528), and
posttest score and trustworthiness (r(7) = -0.227). This
pattern was stronger in the eTAG condition, with posttest
score negatively related to likability (r(9) = -0.650),
animacy (r(9) = -0.329), intelligence (r(9) = -0.477), and
trustworthiness (r(9) = -0.441). We did not find this pattern
in the vTAG condition, and, in fact, posttest score appeared
to be positively related to perceived intelligence (r(10) =
0.331) and likability (r(10) = 0.202).
Thus, there was little evidence that social perceptions
positively related to learning within the two embodied
conditions. In fact, the added motivational elements may
have increased cognitive load, adding “seductive details”
that distracted the learners [29]. The more students attended
to novel features of the technology, the less they may have
attended to the problem-solving content.
VI.

B. Learning
Next, we conducted a repeated-measures ANOVA to
assess learning (H1a and H2a). Test score was the
dependent variable, test time was used as a within-subjects
variable, and condition was used as a between-subjects
variable (see Table 1). Students improved significantly from
pre to posttest (F(1,32) = 40.76, p < 0.001), but there was no
significant differences between conditions in terms of
learning (F(2,32) = 0.423, p = 0.658).
We had posited a relationship between students’ social
perceptions and their learning. As an exploratory analysis,
we ran a series of partial correlations within each condition

TABLE I.

CONCLUSIONS

In this paper, we examined the effects of a teachable
robot on student perceptions and learning, compared to
virtual and embodied versions of the system. While students
learned from using the system, there were no significant
differences between conditions on learning. There was some
evidence that perceptions influenced learning, although in
unexpected ways. Students with low prior knowledge
appeared to respond more positively to rTAG, suggesting
that the novelty of the system and the robot may have
appealed to them. However, positive perceptions of rTAG
were related to smaller learning gains. Students with more
positive perceptions of rTAG may have been distracted by
its novel elements [29].
Our analysis was limited by our small sample size, which
reduces the generalizability of the findings. In addition, the
short duration of the study makes it difficult to interpret the
results. Two hours may not give students enough time both
to learn how to use the system and to reflect on the problemsolving content. Short-term interactions with technology can
produce a novelty effect, where students’ initial engagement
decreases after the first few interactions [30].
Thus, there is a need for additional research examining

MEANS AND STANDARD DEVIATIONS (IN PARENTHESIS) FOR SOCIAL PERCEPTION AND LEARNING VARIABLES. PRETEST AND POSTTEST
SCORES ARE REPORTED AS PERCENTAGES. SOCIAL PERCEPTION SCORES ARE OUT OF 5.

vTAG

Animacy
3.92 (0.94)

Likability
4.46 (0.96)

Intelligence
4.19 (0.73)

Trustworthiness
4.33 (0.73)

Pretest
38.8 (24.1)

Posttest
57.0 (25.6)

eTAG

3.87 (0.76)

4.62 (0.36)

4.40 (0.55)

4.64 (0.39)

45.8 (32.0)

58.7 (30.1)

rTAG

4.53 (0.67)

4.66 (0.49)

4.56 (0.46)

4.53 (0.67)

40.3 (23.0)

55.9 (17.2)

384

whether low prior knowledge students are socially engaged
by the robot over long periods of time, and if so, how best to
support them in learning. Iteration on the cognitive and
social support within the system may preserve students’
motivation while better directing their attention to salient
problem-solving features. For example, we could investigate
how different types of social statements can lead students to
reflect on problem-solving steps. In addition, use of rTAG
in classroom contexts may yield secondary benefits,
facilitating collaboration and technological literacy in
addition to problem-solving [27]. Teachable robots hold
theoretical promise, but require continued research into
relevant design principles and their potential effects.

[12] F. B. V. Benitti. Exploring the educational potential of
robotics in schools: A systematic review. Computers &
Education, 58(3), 978-988. 2012.
[13] R. Mitnik, M. Nussbaum, and A. Soto. An Autonomous
Educational Mobile Robot Mediator. Autonomous Robots,
25(4), 367–38. 2008.
[14] T. Kanda, T. Hirano, T., D. Eaton, and H. Ishiguro.
Interactive robots as social partners and peer tutors for
children: A field trial. Human-Computer Interaction, 19(1),
61–84. 2004.
[15] J-H. Han, M-H. Jo, V. Jones, and J-H Jo. Comparative study
on the educational use of home robots for children. Journal of
Information Processing Systems, 4(4), pp.159-168. 2008.
[16] F. Tanaka and S. Matsuzoe. Children teach a care-receiving
robot to promote their learning: Field experiments in a
classroom for vocabulary learning. Journal of Human-Robot
Interaction, 1(1). 2012.
[17] D. Hood, S. Lemaignan, S., and P. Dillenbourg. When
children teach a robot to write: An autonomous teachable
humanoid which uses simulated handwriting. In Proc.
Human-Robot Interaction (pp. 83-90). ACM. 2015.
[18] K. M. Lee, Y. Jung, J. Kim, and S. R. Kim. Are physically
embodied social agents better than disembodied social
agents?: The effects of physical embodiement, tactile
interaction, and people's lonliness in human-robot interaction,
Int. Journal of Human-Computer Studies, 64, 962-973. 2006.
[19] J. Li. The benefit of being physically present: A survey of
experiemntal works comparing copresent robots, telepresent
robots, and virtual agents. International Journal of HumanComputer Studies, 77, 23-37. 2015.
[20] C. Tu. On-line learning migration: from social learning theory
to presence theory in CMC environment, Journal of Network
and Computer Application, 23(2), 27-37. 2000.
[21] C. Wei, N. Chen, and N. Kinshuk. A model for social
presence in online classroom, Educational Technology
Research Development, 60 (3), 529-545. 2012.
[22] G. Keren and M. Fridin. Kindergatten social assistive robot
(KinSAR) for children's geometric thiking and metacognitive
development in preschool education: A pilot study.
Computers in Human Behavior, 35, 400-412. 2014.
[23] M. Wilson, M. Six views of embodied cognition.
Psychonomic Bulletin & Review, 9(4), 625–636. 2002.
[24] S. Goldin-Meadow, S. W. Cook, and Z. A. Mitchell.
Gesturing gives children new ideas about math. Psychological
Science, 20(3), 267–272. 2009.
[25] M. Howison, D. Trninic, D. Reinholz, and D. Abrahamson.
The Mathematical Imagery Trainer: from embodied
interaction to conceptual learning. In Proc. SIGCHI
Conference on Human Factors in Computing Systems (pp.
1989-1998). ACM. 2011.
[26] M. Kim, W. M. Roth, and J. Thom. Children’s gestures and
the embodied knowledge of geometry. International Journal
of Science and Mathematics Education, 9(1), 207–238. 2011.
[27] V. Girotto, K. Muldner, C. Lozano, W. Burleson, and E.
Walker. Lesson learned from in-school use of rTAG: A robotangible learning environment. To appear in the 2016
Conference on Human Factors in Computing Systems.
[28] C. Bartneck, D. Kulić, E. Croft, and S. Zoghbi. Measurement
instruments for the anthropomorphism, animacy, likeability,
perceived intelligence, and perceived safety of robots.
International journal of social robotics, 1(1), 71-81. 2009.
[29] H. Astleitner and C. Wiesner. An integrated model of
multimedia learning and motivation. Journal of Educational
Multimedia and Hypermedia,13(1), p.3. 2004.
[30] I. Leite, C. Martinho, and A. Paiva. Social robots for longterm interaction: a survey. International Journal of Social
Robotics, 5(2), 291-308. 2013.

ACKNOWLEDGMENT
We would like to thank Cecil Lozano, Win Burleson,
Esha Naidu, Tyler Robbins, Rachana Rao, and all
participating students in the research. This research was
funded by NSF 1249406: EAGER: A Teachable Robot for
Mathematics Learning in Middle School Classrooms and by
the CAPES Foundation, Ministry of Education of Brazil,
Brasília - DF 70040-020, Brazil.
REFERENCES
[1] T. W. Chan, and A. B. Baskin. Studying with the prince: The
computer as a learning companion. Proc. of the International
Conf. on Intelligent Tutoring Systems (Vol. 94200). 1988.
[2] R. D., Roscoe and M. Chi. Understanding tutor learning:
Knowledge-building and knowledge-telling in peer tutors’
explanations and questions. Review of Educational Research,
77(4), 534-574. 2007.
[3] C. Chase, D. Chin, M. Oppezzo, & D. Schwartz. Teachable
agents and the protégé effect: Increasing the effort towards
learning. Journal of Science Education and Technology,
18(4), 334-352. 2009.
[4] A. Powers, S. Kiesler, S. Fussell, S., and C.
Torrey. Comparing a computer agent with a humanoid robot.
In Proc. of the ACM/IEEE International Conference on
Human-Robot Interaction, Arlington, Virginia, USA. 2007.
[5] C. O'Malley, and D. S. Fraser. Literature Review in Learning
with Tangible Technologies. Literature Review Series, Report
12. Bristol. 2004.
[6] K. Leelawong and G. Biswas. Designing learning by teaching
agents: The Betty's Brain system. International Journal of
Artificial Intelligence in Education, 18(3), 181-208. 2008.
[7] Reif, F. and Scott, L. A. (1999). Teaching scientific thinking
skills: Students and computers coaching each other. American
Journal of Physics, 67, 819.
[8] L. Pareto, T. Arvemo, Y. Dahl, M. Haake, and A. Gulz. A
teachable-agent arithmetic game’s effects on mathematics
understanding, attitude and self-efficacy. In AIED (pp. 247255). Springer Berlin Heidelberg. 2011.
[9] S. Papert. Mindstorms: children, computers, and powerful
ideas, 2nd edn. Basic Books, New York. 1999.
[10] N. Rusk, M. Resnick, R. Berg, and M. Pezella-Granlund. New
Pathways into Robotics: Strategies for Broadening
Participation, Journal of Science Education and Technology,
17(1), 59-69. 2008.
[11] M. Petre and B. Price. Using Robotics to Motivate ‘Back
Door’ Learning. Education and Information Technologies,
9(2), 147–158, 2004.

385

Session: Domestic Computing

UbiComp’13, September 8–12, 2013, Zurich, Switzerland

A Tangible Programming Tool for Creation
of Context-Aware Applications
Erin Walker2
Jisoo Lee1
Luis Garduño2
Winslow Burleson2
1
2
School of Arts, Media + Engineering
School of Computing, Informatics, and Decision Systems
Engineering
Arizona State University, Tempe, AZ 85281
{jisoo.lee, luis.garduno, erin.a.walker, winslow.burleson}@asu.edu
ABSTRACT

improvement of user habits and behaviors by providing the
required triggers at the right time, as evidenced in the work
by Intille et al. [13, 20]. However, these contextual cues
will likely be highly idiosyncratic and therefore require
individuals to conduct self-experimentation to find the best
triggers to foster the desired behavior change of interest.

End-user programming tools, if properly designed, have the
potential to empower end-users to create context-aware
applications tailored to their own needs and lives, in order
to help them break bad habits and change their behaviors.
In this work, we present GALLAG Strip, an easy to use
mobile and tangible tool that allows users to create contextaware applications without the need of programming
experience. It enables programming by physical
demonstration of envisioned interactions with the same
sensors and objects that users will later encounter in their
finished application. After an initial pilot to verify the
usability of GALLAG Strip, we conducted a user study to
evaluate the effects of tangible programming in terms of
ease of use, engagement, and facilitation of the ideation
process. We found that tangibility has both benefits and
drawbacks, and suggest a mixed tangible and non-tangible
approach for better user experience.

There is a need for a system that allows end-users to create
applications that are more relevant to their lives and
ultimately more effective at promoting behavior change. To
address this issue, we developed GALLAG Strip, a visual,
mobile, and tangible programming tool. GALLAG Strip
allows end-users who have little or no programming
experience to create context-aware applications that
integrate sensing and actuating components to work for
their particular needs. We intend to facilitate end-users in
self-experimentation with strategies for creating better
behavioral routines. A number of studies on end-user
programming for context-aware applications suggest the
importance of end-user involvement. Users have intimate
knowledge about their ever-changing activities and
environments. Therefore, they are often better positioned
than a hired programmer to design context-aware software
[7, 8, 24].

Author Keywords

Context-aware computing, end-user programming, mobile
programming, tangible programming.
ACM Classification Keywords

H.5.2 User Interfaces -- Interaction styles.

To allow end-users to create their own applications without
previous programming skills, we employed a visual
programming approach in developing GALLAG Strip. We
were inspired by some previous systems that use a
simplified menu-based or metaphor-based GUI interface [7,
15, 24] to allow end-users to specify applications visually
without requiring them to write any code [7]. Furthermore,
we hypothesized benefits of mobility and tangibility in enduser programming for context-aware applications.

General Terms

Human Factors; Experimentation.
INTRODUCTION

Improvement of individuals’ lifestyles has been one of the
main goals of ubiquitous computing in the home. An
increasingly large number of systems are being developed
in the HCI community to promote behavior change in a
variety of domains such as physical health [16, 20],
affective stability [23] and energy conservation [1].

While most end-user programming tools for context-aware
applications imply desktop computers as their usage
environments, GALLAG Strip is mobile. Its smartphone
based user interface allows users to roam within a sensorinstrumented space while programming their applications.
This design decision was inspired by our observation of
participants’ use of environments while they brainstormed
ideas for context-aware applications, demonstrating the
potential utility of placing users in a location to which a
target behavior is related. This mobile approach is validated
by values of contextual design, rapid prototyping, and in
situ creation acknowledged in the HCI community [21, 22].

Fogg, a pioneer of persuasive technologies, asserts that a
user will not engage in a target behavior without an
appropriate trigger, even if the user has high motivation and
ability [11]. A context-aware application can promote the
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full
citation on the first page. Copyrights for components of this work owned by others
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise,
or republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from permissions@acm.org.
UbiComp’13, September 8–12, 2013, Zurich, Switzerland.
Copyright © 2013 ACM 978-1-4503-1770-2/13/09…$15.00.

http://dx.doi.org/10.1145/2493432.2493483

391

Session: Domestic Computing

UbiComp’13, September 8–12, 2013, Zurich, Switzerland

Going beyond mobility, GALLAG Strip is tangible, in that
its interface enables programming by physical
demonstration of envisioned interactions with the same
sensors and objects that users will later encounter in their
finished application. Users manipulate objects that are part
of their daily lives rather than the models (e.g., abstractive
blocks [14] and miniatures of real-world objects [2]) that
most tangible interface systems have adopted. We call this
approach real-world tangibility. It may be frequently subtle
for a person to identify the contextual cues that trigger their
behaviors, especially when they are habits that occur in an
automatic way [25]. Therefore, we assumed that users may
be better reminded by doing.

define if-then rules with “and” and “or” conditional
operators. Recently, a project called Twine [28] is trying to
bring to market a user-programmable wireless module with
embedded sensors for home automation. Their approach is
a visual, rule-based web editor that allows users to create ifthen rules to program sensor modules. Empowered by
crowdfunding, Twine has raised more than half a million
dollars in just over a month -- an indication of the strong
current demand for sensor-based end-user programming.
Tangible Programming Tools

Several studies have developed tangible tools for contextaware programming. For example, SiteView by Beckmann
and Dey [2], allowed users to build rule-based applications
for home automation through tangible interaction with
physical objects placed on a small-scale floor plan. Their
system used RFID and a top-mounted camera to capture the
rules that users wanted to program, and an environmental
display to show images of how the real environment
(represented by the floor plan) would look when the rules
were applied. Beckmann and Dey describe how their
interface is intuitive and lowers the programming difficulty
for novice users.

While there are a few studies that propose end-user
programming tools integrating real-world tangibility [6,
29], no substantial evaluation has been conducted to
examine its effects on the user’s experience. Therefore, we
conducted a controlled experiment, comparing it with two
other conditions (mobile and non-tangible interface, and
non-mobile and non-tangible interface), regarding ease of
use, engagement, and the user’s ideation experience.
In this paper, we describe GALLAG Strip’s design and user
experience. Then, we present results of a preliminary study
to verify the basic usability of the system. Next, we
describe our controlled experiment, findings, and
limitations. We present design implications for end-user
programming tools employing real-world tangibility.

While use of models is dominant in tangible interface
systems like SiteView [9], some researchers have explored
the use of real environments for context-aware
programming. Chin et al. [6] proposed PiP (Pervasive
interactive Programming). It is an if-then rule system that
lets users show the behaviors that they intend to program
through physical interaction with a sensed environment.
The programming interface in PiP was composed of several
networked devices (e.g., lamp, phone, fridge), and a PCbased GUI called the PiPView which allowed users to
define the devices that can be used and their capabilities.
Users had the choice to program their applications solely
through physical demonstration, through the GUI, or a
combination of both. They report that the majority of the
participants (72%) preferred to program through physical
interactions. Likewise, the HomeMaestro project by
Salzberg [27], allowed users to move about a sensed
environment and interact with physical artifacts to program
context-aware applications using a mobile phone. The
HomeMaestro prototype uses Microsoft’s HomeOS [29]
and focuses mainly on home automation. Our work is
parallel with these two systems as all three employ realworld tangibility as a primary interface method, focusing on
the potential benefits of its intuitive quality. However, there
has been little, if any, effort to examine the effects of this
approach through controlled experiments and in-depth
analysis. Although Chin et al. conducted user evaluation,
only their tangible tool was examined, and to our
knowledge the HomeMaestro project did not perform a
formal evaluation.

RELATED WORK
Visual Programming Tools for End-User Creation of
Context-Aware Applications

There has been considerable research and commercial
efforts to enable users with little or no technical expertise to
prototype context-aware applications. The majority of these
efforts have employed visual programming methods [7, 15,
24, 28] by using either metaphor-based GUIs or simple
input [24]. One such tool is the work by Humble and
Crabtree [15], with their GUI based in the “jigsaw puzzle”
metaphor. They allowed users to connect digital jigsawpuzzle-like components that represented sensors and
devices in various left-to-right combinations to form
expressions. They believed that although their linear
programming model constrained users in terms of
expression possibilities, it allowed for easy reconfiguration
and helped users to have a better sense of the information
flow. Similarly, Truong et al. employed a GUI based in a
magnetic poetry metaphor for CAMP [24], allowing users
to define context-aware applications through the
arrangement of fridge-magnet-like words. They found that
users tended to define their applications in terms of highlevel goals, rather than low-level details like devices and
sensors. With a more traditional PC-based GUI in iCAP [7],
Dey et al. enabled users to create context-aware
applications by selecting menus and dragging and dropping
graphical elements, like objects, activities, locations, people
and time. In their system, users arrange these elements to

Our early user studies demonstrated the need to enhance a
tool by reflecting on users’ actual creation processes.
During informal interviews with six users, we asked about

392

Session: Domestic Computing

UbiComp’13, September 8–12, 2013, Zurich, Switzerland

thheir thought process when creating their own application
ns.
W
We found that they typically
y pictured a paarticular locatio
on
oof their house (e.g.,
(
kitchen, living
l
room), or
o looked aroun
nd
thhe space wheere they weree currently lo
ocated. Anoth
her
ccommon process was that theey thought of what
w they usuallly
ddo during a particular perriod of time (e.g., mornin
ng,
eevening); that is, they meentally placed
d themselves in
ssituations of ev
veryday life. Additionally,
A
we
w observed th
hat
innteracting with
h physical objeects served as a cue to remin
nd
uusers of situations that they wanted
w
to add
dress. With theese
ffindings, takin
ng a mobile and
a
tangible approach
a
to kn
nit
aapplication creeation closely with users’ environment
e
an
nd
bbehaviors seem
med advantageo
ous.

responnse frames (auudio cues). Neext, the user decides to
make tthe applicationn sense when sshe turns the T
TV off and
be rew
warded with ann achievementt sound cue. S
So the user
touchees the record buutton, the dem
monstration screeen goes to
V using the
recordiing mode agaiin, the user turrns off the TV
TV’s rremote control,, and an actionn frame, with aan icon of a
TV turrned off, is apppended at the ennd of the appliication (see
Figure 1). The finaal frame the uuser wants to add is an
achiev ement sound aas a reward foor turning off tthe TV, so
the useer touches thee pause button to switch to edit mode,
then toouches the pluus button to addd a response aand selects
the achhievement sounnd (see Figure 2).

S
SYSTEM DESC
CRIPTION
U
User Experience

W
We developed
d GALLAG Strip,
S
an if-th
hen, rule-baseed,
pprogramming-w
with-demonstraation system with a mobiile
pphone-based GUI. Here we ex
xplain the proccess of creating
ga
nnew GALLA
AG applicatio
on, its struccture, and th
he
ccustomization options
o
availab
ble.
E
Example applic
cation

A
As a sample ap
pplication, imag
gine that a user wants to creaate
aan application that
t will sense when the TV is turned on an
nd
trriggers an audiio cue to remin
nd that reading
g would be bettter
thhan watching TV.
T

F
Figure 1. Addin
ng action framee in recording m
mode.

W
When GALLA
AG Strip startss, the user can
n see the list of
ssystem applicaations that she created previiously. The usser
ccan enable and disable som
me of them according to her
h
nneeds. To add
d a new appliccation, she tou
uches the ‘plu
us’
bbutton and thee demonstratio
on screen is presented. Th
his
sscreen is wheere users dem
monstrate what they want to
pprogram. The demonstration
n screen has two modes: a
rrecording mod
de and an edit mode. When creating a neew
aapplication, th
he demonstratiion screen goes directly in
nto
rrecording modee, where the sy
ystem listens for
f sensor even
nts
trriggered by user actions.

Figure 22. Adding a resp
ponse frame.

When the user finishhes creating thee application, sshe touches
the savve button and tthe system connfigures itself to do what
the useer just program
mmed. After tthe applicationn has been
configuured in the seerver, it is readdy to be run aand can be
tested simply by inteeracting with thhe sensed objeect (i.e., the
TV) aand performinng actions preeviously definned in the
applicaation.

F
Following our example, im
magine that thee demonstratio
on
sscreen is in recording mo
ode and the user starts to
ddemonstrate heer application, so she first turns
t
on the TV
T
((i.e., with the TV’s remote control) and a frame with an
V turned on appears
a
on thee demonstratio
on
iccon of a TV
sscreen. Becausse that is the only
o
event thee user wants th
he
aapplication to listen to at the beginning,
b
the user touches th
he
ppause button to
o stop recordin
ng. When the user
u
touches th
he
ppause button, the
t application
n goes into editt mode, wheree a
uuser can review
w and edit the current appliccation. Now, th
he
uuser wants to add the audio
o cues to remiind her that sh
he
sshould read in
nstead, therefo
ore the user to
ouches the pllus
bbutton to add an
a audio respo
onse, selects th
he sound to plaay,
aand then adds another respon
nse to make th
he system speaak
((i.e., text to speeech) the phrasse: “You should
d read instead of
w
watching TV””. At this poiint, the user has added tw
wo
rresponse framees and has a total of threee frames in her
h
aapplication, on
ne action fram
me (TV turneed on) and tw
wo

Types of frames

We em
mployed a com
mic strip metapphor in our moobile phone
GUI, w
where states arre represented w
with frames. Inn our GUI,
a GAL
LLAG applicattion is represeented through a sequence
of fram
mes. We call thhis sequence of frames the “aapplication
strip” and it can hhave three tyypes of framees: action,
responnse, and time-date.
Actionn frames repressent the user’s actions within the sensed
space aand are shownn as blue framees in the appliccation strip.
These frames have a default text laabel and image depending
on the type of sensorr (see Figure 3)).

393

Session: Domestic Computing

UbiComp’13, September 8–12, 2013, Zurich, Switzerland

Response frames represent actions that the system will
perform and are set by the user. This type of frame is shown
in orange and has a text label and image related to the type
of response selected. Response frames can also have an
additional parameter that is displayed in text above the
frame’s image.

User customization

Action frames are displayed with a default text label and
image depending on the sensor being activated. Users can
customize them by changing the text and taking a picture
with the phone’s built-in camera to make it easier to
understand. Figure 5 shows an example, where a frame is
represented with a captured book image and user-typed
label, changed from a default motion sensor image and text.

Time-date frames are conditions set by the user and they
constrain the application’s execution to a particular time
and/or date. These frames are shown in green and show the
selected date or time as their text label. Time frames
additionally have a parameter to show the selected days of
the week (see Figure 4). Time and date frames can be
combined to create conditions based both on a date and a
time; that is, an application can have up to two date and
time frames.

Design Rationale

We designed GALLAG Strip as a visual programming tool.
A visual representation of a program’s structure is easier to
understand than in text form, especially if the programs are
short [17, 19]. We target creation of relatively small and
simple applications. Application scenarios generated by
users in several prior studies [7, 24] tended to be small and
simple, not containing nested loops or nested conditionals
[18], and therefore are relatively easy to represent visually.
In designing the graphic interface, we employed a comic
strip metaphor (i.e., showing action states in a sequence of
frames), inspired by the work of Modugno and colleagues
in Pursuit [17].
Users define their applications in a linear fashion, using
simple if-then conditions. In developing a tool as an attempt
to support end-user experimentation with behavior change,
we chose to start with simple but essential programming
logic. In our early field studies, we found that participants
quite frequently generated application scenarios that just
involved simple if-then rules [7]. For example, "it plays a 23 minute song every time we walk by the dishwasher, with
the intent of suggesting we clean just until the song ends."
One that is considered fairy necessary next to the simple ifthen rules in context-aware applications is logic for
temporal relationships [12]. However, GALLAG Strip does
not support this functionality, and thus, scenarios like the
following cannot be programmed: “If I keep brushing my
teeth for 2 minutes, it plays a kind of cheerful sound.” As a
first pass, we incorporated simple and essential
functionality to emphasize ease of use, and reserved further
expansion for future work.

Figure 3. Action frames with default text label and image.

Figure 4. Time and date frames.

Implementation

GALLAG Strip has three main components: a physical
sensing system, the GALLAG Strip server, and a mobile
phone-based GUI.
Figure 5. Action frame after being customized.

Users’ interactions with objects and spaces are sensed
through X10 [30] and Insteon [31] home automation
sensors. The current implementation of GALLAG Strip
supports four types of sensors: X10 wireless open/closed
magnetic sensors, X10 wireless motion sensors, Insteon
LampLinc modules and Insteon SynchroLinc modules.

Application strip

The programming model for GALLAG Strip is a linear, ifthen, rule-based one, and it is read from left to right and
from top to bottom. Preceding actions or responses need to
occur in the same sequence as they appear before the
current one can execute. Similarly, time and/or date
condition frames need to evaluate to true in order for the
rest of the application to execute. When reviewing or
editing an application, users can scroll up and down to see
the whole application, as it may not be possible to see the
whole application at once.

X10 open/closed sensors can be used to sense interactions
with a wide variety of objects, like a drawer, a door, a
thermostat and other objects for which position or location
can be changed (see Figure 6a). X10 motion sensors are
used to detect presence of an individual in a specific space.

394

Session: Domestic Computing

UbiComp’13, September 8–12, 2013, Zurich, Switzerland

two prrogramming taasks were simpple (one if-thenn rule, two
actionss, and two responses). By compaaring user
perform
mance on the ttwo tasks, we pplanned to test how much
particippants improvved when aasked to proogram an
applicaation of the ssame complexiity as the onee they just
program
mmed. Thus, the order oof the first annd second
applicaations was couunterbalanced. The third taskk was more
compleex (a time-datte condition, ttwo nested if-then rules,
three aactions and thhree responses)). The last tassk was for
users too program an aapplication of their choosing (free-form
applicaation), with noo restrictions otther than a fifteeen-minute
time lim
mit.

T
They are also
o useful in siituations wherre attaching an
oopen/closed sen
nsor to an objeect is not desireed, like detectin
ng
w
when a user reeaches to grab something fro
om a chest wiith
sseveral objects or detecting if
i a user grabss a book from
ma
sshelf (see Figu
ure 6b). Insteon
n LampLinc and
a SynchroLin
nc
ddevices are useed to detect usser's turning on/off
o
lamps an
nd
eelectric applian
nces such as a TV
T respectively
y (Figure 6c).

a) Open/closed
sensor

b) Motio
on sensor

c)
c LampLinc

Figure 6.
6 Sensors suppo
orted in GALLA
AG Strip.

W
We use comm
mercial home automation software called
IIndigo [32] fo
or communicaation with X10 and Insteo
on
hhardware, as well
w
as a plaatform for run
nning GALLA
AG
aapplications. In
ndigo receivess information from X10 an
nd
IInsteon devices either through RF or thro
ough AC pow
wer
s
command
ds to these dev
vices using theese
liines, and can send
ssame channels.. The GALLA
AG Strip serverr was developed
sso that it prov
vides sensor ev
vent informatio
on to the phon
ne
aand configuress Indigo when
n the user sav
ves a GALLA
AG
aapplication in the mobile phone
p
interfacce. The mobiile
aapplication was developed to
o run on the Windows
W
Phon
ne
[33] mobile operating
o
systeem. We explaain this mobiile
innterface in more detail in thee following section.

Figu
ure 7. Living rooom setting and sensors (yellow
w circles).

U
USABILITY ST
TUDY

W
We performed an initial evalu
uation of GAL
LLAG Strip wiith
nnovice users. The
T purpose off this study was to evaluate th
he
bbasic usability of the system
m for end-user programming
p
of
ccontext-aware applications.
a
Figuree 8. Errors (leftt) and time (right) for each of ffour tasks.

A total of thirteeen subjects vo
olunteered to participate
p
in th
his
sstudy, with agees ranging from
m 21 to 49, six
x male and seven
ffemale. Particcipants had a variety of education
nal
bbackgrounds. They
T
were reequired to hav
ve prior generral
eexposure to sm
martphones. In addition to th
he thirteen stud
dy
pparticipants, two
t
members of our research
r
grou
up
vvolunteered to participate ass expert users.. They were th
he
ssource of bencchmark data to
o compare the performance of
nnovice users crreating applicaations with GA
ALLAG Strip to
eexpert users programming
p
the same app
plications usin
ng
A
AppleScript co
ode and Indig
go configuratiion. The expeert
pparticipants haad one or mo
ore years of exposure
e
to th
he
G
GALLAG sysstem and werre proficient in AppleScriipt
pprogramming. Study session
ns were held in a laborato
ory
ssetting that sim
mulated a livin
ng room, with
h sensors placed
aaround the spacce (see Figure 7).
7

We fouund that averaage number of errors was low
w for all of
the fouur applicationss that users weere asked to prrogram and
errors decreased afteer the first application (see Figure 8).
mpleted the appplications in a reasonable
All parrticipants com
amounnt of time annd improvement was evident for the
secondd application (see Figure 8). Another interesting
findingg was that, eeven though tthe average nnumber of
requireements (and coomplexity) for the free-form aapplication
was hiigher than the third one, the average time tto program
was lower; this shows that GALLAG
each rrequirement w
Strip hhas a low learnning curve, as participants w
were able to
program
m applicationns of increasinng complexity in shorter
amounnts of time.
We tthen compareed novice pperformance to expert
perform
mance. Althouugh both expeerts had to ddebug their
applicaations to make sure they worrked, neither haad errors in
their ffinal applicatiions. Like thhe novices, thhe second
applicaation took leess time thann the first too program.
Howevver, the averagge time the experts required fo
for the third

T
The individuall, one-hour user
u
sessions consisted
c
of an
innitial twenty--minute tutorrial about GALLAG
G
Striip,
ffollowed by fo
our application
n programming
g tasks. The firrst

395

Session: Domestic Computing

UbiComp’13, September 8–12, 2013, Zurich, Switzerland

aand more com
mplex applicatiion was almo
ost double (16
6.3
m
minutes) than the average tim
me that the sttudy participan
nts
nneeded (8.5 miinutes). This finding
fi
demonsstrates that useers
w
without knowlledge about how
h
to prograam a GALLA
AG
aapplication are able to prograam them with GALLAG Strrip
ffaster than expeert programmeers using traditiional tools.

med their
Particippants in thee MM condittion programm
applicaations throughh an equivalennt menu-based GUI on a
mobilee device. The m
menu-based GU
UI for the MM
M condition
differeed from the GU
UI of the MT condition in thhat actions
are addded to the appplication mannually by seleccting them
from a list of availabble sensors andd related sensinng features
(e.g., a motion senssor detecting m
motion or the ends of a
magneetic sensor beinng separated; seee Figure 9).

C
COMPARATIV
VE EXPERIMEN
NT

W
We conducted a second user study
s
to system
matically explo
ore
hhow a real-worrld tangible pro
ogramming method affects usser
eexperience in
n developing context-awaare applications
eemploying conttextual cues ass means for beh
havior change.
Q
Questions

B
Based on ourr preliminary user study experience an
nd
ccomparative stu
udies on tangib
ble and graphical interfaces by
b
H
Horn, et al. [14] and Xie, et
e al. [26], we identified thrree
qquestions explo
oring the effectts of real-world
d tangibility. Q1:
Q
D
Does a com
mbined mobiile and tan
ngible end-usser
pprogramming environment
e
make
m
use of th
he programmin
ng
toool more diffficult? Q2: Does
D
it make end-users mo
ore
eengaged? Q3: Does
D
it facilitatte ideation proccesses?

F
Figure 9. Manuaally adding a motion detected aaction

With thhis new featurre, users in the MM conditionn were able
to creaate an application completelyy through the G
GUI, that is,
withouut needing to pphysically dem
monstrate the aapplication.
In this mode, action fframes and thee desired sensorrs could be
added entirely throuugh menu opptions. Particippants were
given a list of the seensors with thheir IDs and a picture so
they c ould locate thhem in the labb setting, how
wever, they
were nnot required too carry the listt while prograamming. In
both thhe MT and MM
M conditions, pparticipants w
were able to
move aaround while ccreating their appplications.

W
We found that ease of learnin
ng by users of tangible
t
system
ms
is not yet demo
onstrated in ressearch that com
mpares graphiccal
aand tangible interfaces.
i
Forr example, Ho
orn, et al. [14]
rreport that no significant difference was found between
thheir two cond
ditions, and in the study by Xie, et al. [26
6],
pparticipants haad more difficulty in the GUI conditio
on.
M
Moreover, usability of taangible interaactions in reeal
eenvironments has
h only rarely
y been examineed. The effect of
eengagement off GUI and TU
UI conditions was found th
he
ssame in both sttudies by Horn, et al. and Xiee, et al., althoug
gh
itt has long beeen claimed thatt increased eng
gagement is on
ne
oof the principal benefits of taangible interacction [10, 26]. A
sshortcoming in
n relying on exiisting studies to understand th
he
bbenefits of tang
gible programm
ming is that thee target audien
nce
inn most studies has been child
dren.

Particippants in the SM
M condition ussed the same m
menu-based
GUI aas the MM coondition, but on a desktopp computer
throughh a Windoows Phone emulator. Thhus, they
program
mmed their appplications in a stationary m
manner. SM
particippants faced aw
way from the livving room, butt were able
to turnn their heads and look at tthe living rooom setting.
Througgh these condiitions we hopedd to isolate thee effects of
being aable to move within an envvironment (as iin the MM
conditiion) from haaving the m
movement influence the
develooping program (as in the MT ccondition).

M
Mobility in user’s
u
conceptiion of scenarrio ideas mig
ght
ppositively affecct users’ creativ
vity in design by making theem
aaware of theirr surroundingss and giving them
t
immediaate
ccontextual inpu
ut during creattive activities [21]. For effeccts
oof TUI in useer’s ideation and creativity
y, there was no
n
ssignificant diffference in the study by Horrn, et al. On th
he
oother hand, sev
veral studies su
uggest potentiaal benefits due to
loower cognitivee load and a naatural and fam
miliar mechanissm
[6], intuitivenesss [2], and “liv
veness” [9].

Particiipants

A totall of 36 individduals were recrruited through email lists
and Crraigslist. 17 w
were female, aand 19 were m
male. Ages
rangedd from 18 to 339. Participantss were requireed to know
the bassics of how too use a computter and a smarrtphone. 19
particippants (6 menn; 13 womenn) had non-eengineering
backgrrounds (e.g., ddance, industrrial design), aand 17 (13
men; 4 women) hhad engineerring backgrouunds (e.g.,
chemiccal engineeringg, civil engineeering, computeer science).
Particippants were raandomly assignned to one off the three
experim
mental conditiions, for a totaal of 12 particcipants per
conditiion. 5 non-enggineering and 7 engineering bbackground
particippants were in the MT condittion, 6 non-enggineers and
6 enginneers were in tthe MM condittion, and 8 enggineers and
4 non-eengineers weree in the SM conndition.

C
Conditions

W
We compared three conditio
ons: (1) mobilee-tangible (MT
T),
((2) mobile-men
nu (MM), and (3) stationary--menu (SM). We
W
cchose a betweeen-subject deesign, most of
o all to avo
oid
pparticipants’ po
otential bias to
o “please the ex
xperimenter.” In
aall three condittions, participan
nts were asked
d to complete th
he
ssame tasks in the
t same laboratory setting as
a the first stud
dy.
M
MT participantts programmed
d their applicaations using th
he
vversion of GAL
LLAG Strip described
d
abov
ve, by physicallly
innteracting with
h the sensed en
nvironment.

396

Session: Domestic Computing

UbiComp’13, September 8–12, 2013, Zurich, Switzerland

Procedure

discussion of ease of use and engagement with interview
and observational data. We also discuss participants’ idea
generation while using our system through observational
data, interviews, and artifacts produced by participants
while brainstorming. It should be noted that while we
tracked time spent and errors made across tasks, we tested
for and found no significant differences between conditions,
and thus for space considerations we will not address those
results further.

The one and a quarter-hour sessions began with an initial
video tutorial about GALLAG Strip. Participants were then
provided with the programming tool corresponding to their
assigned condition (i.e., a mobile phone for the MT and MM
conditions, and a desktop computer for the SM condition).
Participants were also given a printed list of all the sensors
available, showing a picture of their location and their
assigned sensor ID.
Each participant was shown a set of presentation slides on a
computer describing the three applications that we wanted
them to program: a simple, a complex, and a free-form one.
For the simple application task, participants were asked to
program an application with two actions and two responses:
if you enter the living room and you turn the TV on, then
make the system play the reminder sound and make the
system say “Remember to take your pills”. In the complex
application task, participants were asked to program an
application with one time-date condition, three actions and
three responses: if time is after nine in the morning and you
turn on the AC and then open the front door, then make the
system play the alarm sound, and make the system say
“Turn off the AC”; if you then close the door (i.e., if you
leave the house), then make the system send an SMS to
your mobile phone with the message “You left the AC on!”
For the free-form application, participants were asked to
think about a personal scenario that they would like to
program. They were given a blank piece of paper and they
were asked to think for a couple of minutes and then
describe the chosen scenario. After they gave their
description of their envisioned application, they were asked
to program it without time limitations.

RESULTS
Ease of Use

We conducted a two-way ANOVA with ease of use as the
dependent variable, and engineering background and
condition as factors. While condition was not significantly
related to ease of use (F[2,30] = 1.36, p = 0.271),
engineering background was significantly predictive of ease
of use (F[1,30] = 5.41, p = 0.027). Additionally, we found
that engineering background interacted with condition to
predict ease of use (F[2,30] = 6.23, p = 0.005; see Table 1
for means). Contrasts revealed that non-engineers found the
MT condition to be significantly more difficult than the MM
condition (p = 0.006) and the SM condition (p = 0.003). For
the MT condition, non-engineers perceived the activity as
significantly more difficult than the engineers (p < 0.001).
To ensure that gender was not driving the effect, we then
conducted a two-way ANOVA with ease of use as the
dependent variable, and gender and condition as factors.
Gender did not have a significant effect on ease of use
(F[1,30] = 2.63, p = 0.155), and neither did the interaction
between gender and condition (F[2,30] = 1.27, p = 0.297).

After completing the three programming tasks, participants
were given a questionnaire to complete on their subjective
perceptions of the activity. After participants completed the
questionnaire, we conducted an exit interview to get their
feedback regarding usability and engagement. The
interview lasted an average of fifteen minutes and included
questions about fun and creativity, future use, and potential
effectiveness of the system in improving their lives. As part
of the interview, we asked participants to imagine that they
had the system installed in their home and asked them to
think about as many scenarios as they could that they would
like to program and have available at home.

Condition

Major (Number
of participants)

MT

MM

SM

Measures

Ease of Use

Engagement

Mean

SD

Mean

SD

NonEngineering (5)

4.15

.74

6.07

.76

Engineering (7)

6.12

.82

6.61

.73

NonEngineering (6)

5.58

.63

6.39

.88

Engineering (6)

5.39

.39

5.92

.50

NonEngineering (8)

5.63

.88

5.88

.94

Engineering (4)

4.75

1.27

6.58

.50

Table 1. Descriptive statistics for ease of use and engagement.

Below we present results related to perceived ease of use
and engagement while using the system. Ease of use was
measured using four items on the post-questionnaire. It
asked for user responses on a 7-point Likert Scale
(“Strongly disagree” to “Strongly agree”) to statements
such as “I found the system unnecessarily complex.”
Engagement was measured using three items on the postquestionnaire (e.g., “The experience was fun.”). Questions
were adapted from the System Usability Scale (SUS)
developed by John Brooke [4]. We supplement our

Interviews supported the fact that some non-engineering
MT participants found the activity “inconvenient” (P33) or
“cumbersome” (P35). Part of the difficulty that participants
had with the MT and MM conditions was the size of the
mobile interface. P3 commented “I want to have a bigger
size of screen,” while P17 commented “…the current one
[interface] is a little confusing for a small screen.” One
participant with an engineering background (P8) better
articulated some of the difficulties with the small screen:

397

Session: Domestic Computing

UbiComp’13, September 8–12, 2013, Zurich, Switzerland

“It was bothering to switch between the screen and the
scene while programming. It might be cumbersome since
people may need to modify -- change frequently”.
Observation of participants confirmed that it was at times
awkward to hold the mobile device while triggering the
physical sensors, especially for the non-engineering
participants. It is possible that engineers, more used to
working with technology, had an easier time adapting to the
difficulties that the tangible interface presented than nonengineers.

were not unique to a particular condition and engineering
background.
Ideation Experience
Effects of physical information

Gaining ideas. Participants tended to look around the scene
while developing their own scenario idea in the free-form
task (59% of total participants). Even if they were in the SM
condition, they turned their heads to face the scene. P13
from the SM condition said, “By looking at the room, I was
able to get a picture of the whole area… I'm a visually
oriented person so [I] like to see the whole
picture…looking at the room, I thought of what is in my
proximity, and whether it could be better done if I were
sitting on the chair." Participants also tended to look around
the scene while brainstorming (64% of total participants;
92%, 50%, 50% of the MT, MM, SM participants
respectively).

There is also evidence that the engineers appreciated the
advantages of the MT condition in ways that the nonengineers did not. P1 commented on the benefits of the
tangible environment for debugging: "I can test it, it's like a
preliminary test to see whether sensors are working
properly.” P26 described the advantages as follows: “I
think physically making actions helped me remember -follow a pattern I’ve created.”

Considering intangible aspects. In addition, we found that
the tangible interaction might have interfered with
participant's attention toward intangible elements. In the
free-form task, only one person in the MT condition used
time-date constraints, compared to five in the MM condition
and nine in the SM condition.

It should be noted that using the menu-based tool also
posed its own obstacles for some participants, especially
with the need to use the correct sensor IDs and with
participants not being sure of what sensor state to select.
P36, in the MM condition commented: "…not easy to figure
out what it is with the pictures in the list; I needed to find
sensor ID, and it's a little inconvenient; especially because
I was not sure if I should select 'open' or 'close' for a
particular item." P12 in the MM condition expressed
concern about a potentially lengthy list of sensors when
used in real situations including significantly more items,
which may cause difficulty in finding intended ones.

Intuitiveness

Ordering actions. When participants in the MM and SM
conditions had to mentally construct a series of actions, the
resulting programs tended to be more unnatural. In adding
actions for the phrase “leave the living room”, one
participant (P13; SM condition) added first stepping off the
mat and then stepping on, when the order should have been
the opposite. Another participant (P16, SM condition) made
her application send a SMS message when she closed the
door at first. Then, she realized that it made more sense to
receive a message before she shut the door, rather than
after, as the message was a reminder that told her to turn off
the air-conditioner before leaving for work. P19 (SM
condition) remarked that the most difficult thing to her in
doing the free-form task was to organize her actions (that is,
add actions in the order that she actually does in her daily
life). “It's hard to organize-- a couple of actions, picking up
guitar, Turn on TV, Pick up dumbbells, but hard to order
them…what do I do first?" It is likely that the MT condition
made the organization of ideas more concrete.

Engagement

We then conducted a two-way ANOVA with engagement
as the dependent variable, and engineering background and
condition as factors. There were no significant differences
between the effects of condition (F[2,30] = 0.179, p =
0.837) or engineering background (F[1,30] = 1.023, p =
0.320) on engagement, and no significant interaction effect
(F[2,30] = 2.018, p = 0.15; see Table 1). We again looked
at the effects of gender using a two-way ANOVA with
engagement as the dependent variable, and gender and
condition as factors. Gender did indeed have a significant
effect on engagement (F[1,30] = 5.16, p = 0.03; Men: M =
5.98, SD = 0.768; Women: M = 6.49, SD = 0.708). The
interaction between gender and condition was not
significant (F[2,30] = 0.319, p = 0.729).

Finding circumstance of an action. We further found that
the tangible experience helped people to discover particular
features of the system. P3 in the MT condition got puzzled
since she did not have an idea for what sensor to use for the
phrase, ‘leave the living room’. After wondering for a
while, she decided to just walk to the entrance door hoping
she might be able to discover a clue. She by chance noticed
a sensor-augmented mat that was placed at the edge of the
living room area, and realized that it was an appropriate
item for the phrase.

Interview data revealed the specific aspects of the activity
that people enjoyed. People liked the immediate feedback
they received after programming the application: “Almost
instantly you could use an app. Really very fun!” (P17; MT
condition); “I liked how quick it was to program, and how
quick the responses were to the actions.” (P21; MM
condition). Others liked particular features of the system:
“It’s fun when music plays.” (P19; SM condition); “I liked
the ability for the system to say something out loud” (P27;
MM condition). The most engaging aspects of the system

398

Session: Domestic Computing

UbiComp’13, September 8–12, 2013, Zurich, Switzerland

DISCUSSION & CONCLUSIONS

engineering background. However, the same results did not
appear when we substituted gender for engineering
background in our analyses, although we did find that
women were more engaged than men overall. In spite of
these limitations, we believe the combination of our
qualitative and quantitative analyses provides interesting
design insights that can inform future development of enduser tangible programming environments.

In this paper, we presented GALLAG Strip, a tool designed
to enable end-user programming of context-aware
applications for behavior change, and results of a
comparative experiment to explore the benefits of our
tangible approach. We found that while our system was
usable, people with a non-engineering background
perceived it as less easy to use. In contrast, the participants
who had some degree of programming skills considered it
easier than the other conditions.

It was not always easy to determine whether problems we
noticed were artifacts of our particular interface or tangible
interfaces in general. Our results indicate that we should
iteratively refine the interface to allow for even more
naturalistic physical interactions. In our data, the smallsized screen and switching attention between the scene and
the screen was identified as an issue in the tangible-mobile
tool. We also found that the current interface design should
be improved to minimize interruptions of natural user
actions. Sound could be employed to supplement the visual
interface to give information while the user is manipulating
an object; for example, a sound could be played for a newly
added action tile so that users do not have to look at the
screen to check if it has been added successfully. It might
be a better design to have the user record all actions first
and then allow the user to insert responses between the
recorded actions. The shortcomings of the present interface
design may have influenced the results of our experiment,
and further study is necessary with a tool improved to better
support users’ natural performance.

Richness brought by using real-world everyday objects can
be considered as augmentations of ‘role expression’ and
‘hidden dependency’ dimensions among the Cognitive
Dimensions suggested by Edge and Blackwell [9]. People
are quite familiar with the uses of everyday objects. Their
forms and operations naturally elicit people’s recognition
and action. Furthermore, as objects are located in a living
space, relationships between them build up. For example,
the location of objects inside a container depends on where
the container is placed. By sitting on a chair located at a
particular spot, a user notices a picture frame in front of it.
In our case, such richness of artifacts might have served as
advantage for the participants with an engineering
background as they are more able to handle both the visual
interface on a mobile phone and rich information from an
environment relatively easily.
Despite the difficulty the non-engineering participants had
in using the system, we observed benefits of the tangible
programming such as intuitive ordering of actions and
diverse ideation with rich physical information. On the
other hand, the tangible interface tended to distract the
participants from the intangible elements of the system.
These results imply that it may be useful to only encourage
tangible programming interactions for tasks where the
tangible medium is particularly beneficial. The tangible
feature might best be used to support people’s creation of
scenarios that are primarily related to object use and
actions. In contrast, with a GUI, people might be better
equipped to program applications with non-tangible or
global states such as time or weather. Considering
advantages of each method, we suggest that a mixed
tangible and menu-based approach is appropriate to
encompass user groups of different programming skills and
use cases.

Our work demonstrates benefits and disadvantages of a
combined mobile and tangible programming tool for
context-aware applications for end-users, with a particular
focus on ease of use, engagement, and creativity. We also
showed that our programming tool’s design has a low
learning curve and that it is effective when compared to
traditional ways of programming context-aware
applications (i.e., coding). Our vision is to continue to
develop GALLAG Strip as a means of facilitating
programming of context-aware applications that are
uniquely tailored to the needs of the end-user.
REFERENCES

1. Bang, M., Gustafsson, A., and Katzeff, C. Promoting
new patterns in household energy consumption with
pervasive learning games. Proc. Persuasive 2007,
Springer Berlin Heidelberg (2007), 55–63.

Our comparative experiment had some limitations. First, we
conducted it in a lab setting. The participants may have
showed more naturalistic responses in a place more familiar
to them. Secondly, our sample size was too small to draw
solid conclusions about the interaction between condition
and engineering major. In addition, the numbers of
participants of each educational background and gender
were not evenly distributed over the conditions. An
alternate explanation for the results could be that more
women are non-engineers, and more men are engineers, so
the results that we found might be driven by gender and not

2. Beckmann, C. and Dey, A.K. SiteView: Tangibly
Programming Active Environments with Predictive
Visualization. Intel Research Tech Report, (2003).
3. Blackwell, a. F. and Hague, R. AutoHAN: an
architecture for programming the home. Proc. HCC
2001, IEEE (2001), 150–157.
4. Brooke, J. SUS-A quick and dirty usability scale. in
Jordan, P. W., Thomas, B., Weerdmeester, B. A., &
McClelland, A. L. ed. Usability Evaluation in Industry
y, Taylor and Francis, London (1996),189–194.

399

Session: Domestic Computing

UbiComp’13, September 8–12, 2013, Zurich, Switzerland

5. Burleson, W., Ruffenach, C., Jensen, C., Bandaru, U.K.,
and Muldner, K. Game as life --- life as game. IDC
2009, ACM (2009), 272.

18. Modugno, F. and Myers, B.A. Graphical Representation
and Feedback in a PBD System. In Watch What I Do:
Programming by Demonstration. MIT Press,
Cambridge, MA (1993), 415–422.

6. Chin, J.S., Callaghan, V., and Clarke, G. An End-User
Programming Paradigm for Pervasive Computing
Applications. Proc. ICPS 2006, IEEE (2006), 325–328.

19. Myers, B.A. Taxonomies of Visual Programming and
Program Visualization. Journal of Visual Languages &
Computing 1, 1 (1990), 97–123.

7. Dey, A., Sohn, T., Streng, S., and Kodama, J. iCAP:
Interactive prototyping of context-aware applications.
Proc. PerCom 2006, Springer (2006), 254–271.

20. Nawyn, J., Intille, S., and Larson, K. Embedding
behavior modification strategies into a consumer
electronic device: a case study. Proc. UbiComp 2006,
Springer (2006), 297–314.

8. Dey, A.K., Hamid, R., Beckmann, C., Li, I., and Hsu, D.
a CAPpella: programming by demonstration of contextaware applications. Proc. CHI 2004, ACM (2004), 33–
40.

21. De Sá, M. and Carriço, L. A mobile tool for in-situ
prototyping. Proc. MobileHCI 2009, ACM (2009), 1.

9. Edge, D. and Blackwell, A. Correlates of the cognitive
dimensions for tangible user interface. Journal of Visual
Languages & Computing 17, 4, Elsevier(2006), 366–
394.

22. Seifert, J. Mobidev: a tool for creating apps on mobile
phones. MobileHCI, ACM (2011), 109–112.
23. Ståhl, A., Höök, K., Svensson, M., Taylor, A.S., and
Combetto, M. Experiencing the Affective Diary. Proc.
Personal and Ubiquitous Computing 13, 5, Springer
(2009), 365–378.

10. Fails, J., Druin, A., and Guha, M. Child’s play: a
comparison of desktop and physical interactive
environments. Proc. Interaction design and children,
ACM (2005), 48–55.

24. Truong, K., Huang, E., and Abowd, G.D. CAMP: A
Magnetic Poetry Interface for End-User Programming
of Capture Applications for the Home. Proc. UbiComp
2004, Springer (2004), 143–160.

11. Fogg, B. A behavior model for persuasive design. Proc.
Persuasive 2009, ACM (2009), 1.
12. García-herranz, M., Haya, P., Alamán, X. Towards a
Ubiquitous End – User Programming System for Smart
Spaces. Journal of Universal Computer Science, 16, 12,
(2010), 1633-1649.

25. Wood, W. and Neal, D.T. A new look at habits and the
habit-goal interface. Psychol Rev 114, (2007), 843–863.
26. Xie, L., Antle, A.N., Motamedi, N., and Vt, C. Are
Tangibles More Fun? Comparing Children’s Enjoyment
and Engagement Using Physical, Graphical and
Tangible User Interfaces. Proc. TEI 2008, ACM (2008),
191–198.

13. Ho, J. and Intille, S.S. Using context-aware computing
to reduce the perceived burden of interruptions from
mobile devices. Proc. CHI 2005, ACM (2005), 909-918.
14. Horn, M.S., Solovey, E.T., Crouser, R.J., and Jacob,
R.J.K. Comparing the use of tangible and graphical
programming languages for informal science education.
Proc. CHI 2009, ACM (2009), 975-984.

27. Home Maestro.
http://shaunsalzberg.com/medialab/homemaestro.

15. Humble, J. and Crabtree, A. “Playing with the Bits”
User-configuration of Ubiquitous Domestic
Environments. Proc. UbiComp 2003, Springer (2003),
256–263.

29. HomeOS - Microsoft Research.
http://research.microsoft.com/en-us/projects/homeos/.

28. Twine - Listen to your world, talk to the Internet.
http://supermechanical.com/twine/.

30. X10 - Home Automation.
http://www.x10.com/automation/index.html.

16. Maitland, J. and Chalmers, M. Self-monitoring, selfawareness, and self-determination in cardiac
rehabilitation. Proc. CHI 2010, ACM (2010), 1213–
1222.

31. INSTEON - Wireless Home Control Solutions for
Lighting, Security, HVAC, and A/V Systems.
http://www.insteon.net/.
32. Indigo: Macintosh Home Automation and Control
Server. http://www.perceptiveautomation.com.

17. Modugno, F., Corbett, A.T., and Myers, B. a. Graphical
representation of programs in a demonstrational visual
shell---an empirical evaluation. ACM Transactions on
Computer-Human Interaction 4, 3 (1997), 276–308.

33. Microsoft Windows Phone.
http://www.microsoft.com/windowsphone/.

400

81

Exploring Adaptive Scaffolding in a Multifaceted
Tangible Learning Environment
Elissa Thomas, Victor Girotto, Alex Abreu, Cecil Lozano, Kasia Muldner, Winslow
Burleson, Erin Walker
Computing, Informatics & Decision Systems Engineering, Arizona State University

{eethomas, victor.girotto, alexabreu, cecil.lozano,
katarzyna.muldner, winslow.burleson,
erin.a.walker}@asu.edu

Abstract. The majority of educational software is designed for traditional computers, which allow little opportunity for physical manipulation of an environment. Tangible Activities for Geometry (TAG) provides students a tangible
learning environment. Currently, however, TAG does not employ adaptive scaffolding techniques. Accordingly, we describe how scaffolding techniques and
teachable agent behaviors can be integrated into TAG to improve this tangible
learning environment.

Keywords: adaptive scaffolding, tangible learning environments, teachable
agents

1

Introduction

Open-ended learning environments (OELEs) enable students to actively engage in
problem solving, such as generation, testing and revision of a hypothesis [1]. However, most educational systems target personal computers and their typical WIMP (window, icon, menu, pointing device) setup. These systems rarely allow for embodied
interaction between the student and the learning environment, despite the fact that
students learn a great deal through physically engaging with their environment [2].
The Tangible Activities for Geometry system (TAG) aims to fill this gap, by providing
a tangible OELE where students can move beyond the boundaries of the virtual world
and explore different strategies for solving geometric problems [3].
The current TAG system provides no feedback or adaptation to the userÕs performance. Therefore, our goal with this paper is to propose ways of integrating adaptive
scaffolding techniques into this tangible learning environment (TUI), laying the foundation for studying the effects that they would have in this type of learning environment. The majority of TUIs do not currently possess such capabilities, which allows
us to start exploring this intersection. Here, we will review existing frameworks and
techniques that can be used for scaffolding the user's learning in an adaptive manner
and will describe ways in which they could be applied to our system.

82

2

Description of Current System

In the current implementation of the TAG system, a student solves geometry problems by instructing a teachable agent on the steps needed to solve the problem. Problems include plotting a point in a given quadrant, translating a point, or rotating a
point around a center of origin. While answers are sometimes the same, problems can
often be solved in different ways. The system is comprised of three main components
[3]. The problem space is a Cartesian plane projected on the ground. This is where the
teachable agent and the problem objects, such as lines and points, are displayed. The
interactions with the problem space occur through a hanging pointer that hangs from
the ceiling, functioning as a mouse. Hovering the pointer over the problem space
moves the cursor. Clicking is performed when the user moves the pointer below a
certain height threshold and back up. The feedback for the userÕs interactions on the
problem space is received on the mobile interface, displayed on an iPod Touch. In this
interface, the user is able to select an action that will be performed by the agent, view
the steps already taken, and navigate through problems.

Figure 1: Elements of the TAG system. The problem space (a), where the Cartesian plane is
projected, the hanging pointer (b), used by the student to interact with the problem and the
mobile interface (c), the iPod interface commands are issued to the agent.

3

Review of Existing Pedagogical Techniques

Prior research has explored how various pedagogical techniques impact student learning. A number of these rely on a teachable agent paradigm, where students learn by
tutoring a computerized agent modeled to simulate behaviors of a student tutee. For
instance, reflective knowledge building uses questions and explanations generated by
a teachable agent to prompt students to reflect on their own understanding of various
concepts, and refine their ideas [4]. Agents could also use this technique to introduce
new ideas to a studentÕs existing knowledge [5].

83

Other research has shown that the level of abstraction in the advice provided by a
teachable agent can impact a studentÕs perceptions and performance. Students who
work with agents that give different kinds of feedback, ranging from high-level advice
to concrete, task specific suggestions, performed better than students who interacted
with agents that only used task-specific suggestions [6].
Techniques used in cognitive tutors can also be useful for extending TAG. Cognitive tutors provide the user with feedback on a step-by-step basis, in response to
common errors and with on-demand instructional hints, and adapt the selection of
problems based on user-performance [7]. The challenge is to adapt these techniques to
an open-ended system such as TAG while still encouraging open-ended exploration.

4

Proposed Extensions on the Current System

We propose expanding TAG to employ adaptive scaffolding as a way to increase the
systemÕs effectiveness. Techniques such as reflective knowledge building could be
integrated into our system to improve student learning while also enhancing unique
tangible aspects of our system. For example, if the student is attempting to plot a point
in quadrant II, but moved the agent into quadrant IV, a question from the agent might
prompt the student to recognize that their actions are not leading them to the correct
solution. As another example, after a student solves a problem, the TAG agent could
propose an alternate solution, helping students evolve their ideas, which some students struggle to do in OELEs [8]. As an extension of adaptive scaffolding in a traditional learning environment, students could also be encouraged to try additional tangible interactions that may not have been incorporated into their original solution.
Scaffolding could also be employed through hints given by the agent while a student is working on a problem. In this scenario, the agent uses cues that a student
might be confused, such as a long pause without any activity, and provides a hint to
guide the student in the right direction. Are there unique cues within TUIs that could
be detected to improve an adaptive scaffolding model? To study this, our system
could monitor embodied behaviors exhibited by the student, such as pacing back and
forth or kneeling down on the projected Cartesian plane. Following standard convention, the agent's hints should vary in detail based on the student's performance within
a given problem. Students would initially be provided with high-level feedback from
the teachable agent, allowing them to apply the information given to them by the
agent to the problem domain. If the student continues having trouble, the system can
adaptively adjust the agentÕs hints to be more direct, allowing students to discover the
correct approach, albeit, with less reflection on the metacognitive process. By providing feedback in this manner, we can foster an atmosphere of discovery, which should
help students feel more engaged [2]. Since previous work has shown that increasing
the sociability of an agent improves student perceptions of the system and student
performance [9], hints from the agent could be provided textually through a pop up on
the iPod interface while also being spoken by the agent.
On a less localized scale, adaptive scaffolding could also be applied based on a
studentÕs performance throughout an entire session. Indicators that could be used to

84

measure student performance include the amount of time taken to solve a problem,
the number of correct and incorrect solutions a student has produced, and the number
of steps a student uses as compared to an optimal solution with a minimal number of
steps. Applying this type of adaptive scaffolding in a TUI introduces some unique
challenges. For example, how do we differentiate between students that are struggling
with the problem domain and students that are having trouble understanding how to
use the unique tangible interactions of our system?

5

Conclusion

By proposing a novel set of techniques to augment the TAG system, we aim to provide the appropriate level of scaffolding needed to improve student learning, while
maintaining student engagement when faced with difficulties and failure. The ultimate
goal is to ensure that students receive help when it is needed, but are not hindered
during open-ended exploration. We also hope to learn more about how this scaffolding should be presented to the student on the different dimensions that a TUI provides, exploring the advantages and drawbacks of each type of scaffolding.
References
1. Land S. Cognitive requirements for learning with open-ended learning environments.
Educational Technology Research and Development. 2000, Volume 48, Issue 3, pp 61-78.
2. Walker, E, and Burleson, W. User-Centered design of a teachable robot. Intelligent Tutoring
Systems, 2012.
3. Mulder, K., Lozano, C., Girotto, V., Burleson, W., and Walker, E. Designing a Tangible
Learning Environment with a Teachable Agent. Artificial Intelligence in Education, 2013.
4. Roscoe, D., Wagster, J., and Biswas, G., Using Teachable Agent Feedback to Support
Effective Learning by Teaching, In Proceedings of the 30th Annual Meeting of the
Cognitive Science Society, Washington, DC, 2008.
5. Blair, K., Schwartz, D., Biswas, G., and Leelawong, K. Pedagogical Agents for Learning by
Teaching: Teachable Agents. In Educational Technology & Society: Special Issue on
Pedagogical Agents, 2006.
6. Lester, J. C., Converse, S. A., Kahler, S. E., Barlow, S. T., Stone, B. A., and Bhogal, R. S.
The Persona Effect: Affective Impact of Animated Pedagogical Agents. In Proceedings of
CHI Ô97, 1997.
7. Koedinger, K., Aleven, V. Exploring the Assistance Dilemma in Experiments with Cognitive
Tutors. 2007.
8. Land, S. M. Cognitive Requirements for Learning with Open-Ended Learning Environments.
Educational Technology Research and Development 48.3, 2000.
9. Hershey D. K., Mishra P., and Altermatt, E. All or nothing: Levels of sociability of a
pedagogical software agent and its impact on student perceptions and learning. Journal of
Educational Multimedia and Hypermedia 14.2, 2005.

COMPUTER SUPPORTED COLLABORATIVE LEARNING PRACTICES

CSCL2009 PROCEEDINGS

Beyond Explicit Feedback: New Directions in
Adaptive Collaborative Learning Support
Erin Walker, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, USA, erinwalk@andrew.cmu.edu
Nikol Rummel, University of Freiburg, Germany, rummel@psychologie.uni-freiburg.de
Kenneth R. Koedinger, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, USA, koedinger@cmu.edu
Abstract: Adaptive collaborative learning support (ACLS) may be better than fixed forms of
support at increasing learning from collaboration. While much existing adaptive assistance has
focused on providing explicit feedback directly to the relevant student, we propose a twodimensional design space which explores alternative methods of adaptive assistance that are
implicit, indirect, or both. We investigated the viability of these ideas using data collected in a
classroom evaluation of an ACLS system for peer tutoring which incorporated the design
ideas in a manner that provided cognitive support to peer tutors. In this paper, we discuss how
students interacted with the different forms of feedback, and propose a second iteration of the
assistance that involves collaborative support in addition to domain support.
0B

Introduction

Collaborative activities have been shown to be a good way of improving student learning, but effects are not
found when students do not interact in positive ways (Lou, Abrami, & d’Appolonia, 2001). Thus, researchers
implement collaboration scripts, which support student interaction using clearly defined roles and activities
(e.g., O’Donnel & Dansereau, 1992). Most script support for collaboration that has been implemented so far has
been fixed, and do not change based on student behavior. Adaptive support, which would provide assistance to
students when and where they need it, might improve upon or complement many fixed forms of support
(Rummel & Weinberger, 2008), and has indeed been shown to have a more positive effect on student learning
(Kumar, Rosé, Wang, Joshi, & Robinson, 2007). However, few full systems have been implemented or had their
learning effects evaluated (see Soller, Jermann, Muehlenbrock, & Martinez, 2005, for a review).
One potential reason for the slow progress in the field is that much adaptive collaborative learning
support (ACLS) follows an individual learning model established in intelligent tutoring technology (see Van
Lehn, 2006). Student collaboration is compared to a model of ideal collaboration, and discrepancies are
addressed by providing explicit feedback on the next course of action directly to the student who is collaborating
suboptimally (Soller et al., 2005). For example, COLER (Constantino-Gonzales, Suthers, & de los Santos,
2003) provides explicit advice to nonparticipating students such as, “George, participation is a learning
opportunity. I suggest that you leverage it. Come on, participate! : )”. Similarly, COLLECT-UML (Baghaei &
Mitrovic, 2007) provides individual feedback on a UML modeling task such as "Some relationship types
(associations) in your individual solution are missing from the group diagram. You may wish to share your
work by adding those association(s)/discuss it with other members." This type of feedback might not be the
most effective way of supporting collaborating students, as it favors cognitive processes without attending to
social interactions, potentially distracting or overloading students. In fact, Kumar and colleagues (2007) found
that students tended to ignore adaptive prompts while collaborating. Thus, it might be productive to explore the
effects of other forms of feedback on student interaction and learning.
In this paper, we outline a design space for adaptive feedback involving two dimensions: whether the
action that students should take is explicitly described in the feedback or implicitly arises as a result of the
support (explicit or implicit), and whether it is presented directly to the person it targets or presented indirectly
to another party or through a change in the learning environment (direct or indirect; see Figure 1). So far, most
ACLS systems have been located in the lower right quadrant of Figure 1. We intend to further explore the
possibilities for adaptive support by investigating a design idea in each of the other three quadrants.
1. Adaptive Opportunities modifies the learning environment in order to create learning opportunities for
students. For example, problems could be adaptively assigned to students based on their previous
interactions. Here, the change to the learning path is implicit, and feedback is presented indirectly.
2. Peer-Mediated Feedback encourages students to better self-regulate their learning. For example, if one
student is not explaining a step clearly, we can prompt their partner to ask, “What do you mean by
that?” rather than telling the first student to expand their explanation. This approach is indirect, as it is
not presented directly to the relevant student, but explicit because the next course of action is clear.
3. Adaptive Resources provides resources to students at moments when they need them. For example, a
video related to a given concept could be presented when a student may be thinking of applying the
concept, and additional materials surrounding the video could incorporate specific information about
the current problem or collaborating students. Here, the presentation of the resources is directly to the
relevant student, but the course of action is implicit.

552

© ISLS

DESIGNING FOR CSCL PRACTICES

Figure 1. Design space for adaptive collaborative learning support.
We explore the viability of each idea using data collected from an existing adaptive collaborative learning
system for peer tutoring, APTA (Adaptive Peer Tutor Assistance). APTA is an extension to the Cognitive Tutor
Algebra, a successful intelligent tutoring system for individual learning in Algebra (Koedinger, Anderson,
Hadley, & Mark, 1997). Using APTA, students take turns tutoring each other; the peer tutee solves the problem,
and the peer tutor marks steps right or wrong, and gives the tutee hints and feedback in a chat window. In turn,
the tutee can ask for help and self-explain. The system provides fixed domain support (the peer tutor can view a
worked-out problem solution) and adaptive domain support (if the peer tutor marks something right and it is
actually wrong, the system will intervene with a prompt to collaborate and a cognitive hint). We evaluated
APTA in a classroom study that took place over the course of 2 weeks with 51 collaborating students (Walker,
Rummel, & Koedinger, 2008), and the examples and observations addressed in this paper were drawn from that
study. Each design idea has been realized to differing extents in the domain help provided by the system,
enabling us to learn about how collaborating students might respond to the design ideas. We further discuss the
implications of the initial results for further development of the designs.

Designs for Collaboration Support in the Context of APTA
6B

Adaptive Opportunities

The current implementation of this design idea within the context of APTA is somewhat reflected in the
adaptive instructional support delivered by the tutoring system, which attempts to set up opportunities where
both parties reflect on and repair their misconceptions. To accomplish this goal, the system compares each peer
tutor assessment of tutee actions to the cognitive tutor assessment, and makes peer tutors aware of discrepancies
that arise. Table 1 illustrates an example from our study where the tutor marked a problem step correct, but then
was presented with information from the intelligent system which demonstrated that the step was in fact
incorrect. The peer tutor determined how to repair the error and take the next correct step. Although the outcome
of his reasoning was communicated to the tutee, the process itself was not made transparent, potentially
explaining why the delayed gain of the tutor was 0.375, while the tutee showed a delayed gain of 0.125. In
general, tutors appeared to benefit even from simply viewing tutee errors.
This design idea might more usefully be applied in creating the opportunity for errors to be committed
through the adaptive selection of problems that lead to errors. There were two obstacles preventing errors from
being committed by the tutee. First, many problems were too easy for tutees. Second, some peer tutors were
overzealous in helping tutees, such that tutees had no chance to commit errors. We would see a pattern where a
given tutor would give the tutee an instruction like “factor q”, the tutee would execute the action, and the tutor
would immediately give the next instruction like “divide by a + b”. Therefore, our next step in implementing
this design idea is to create the conditions where errors are made. As in the individual version of the CTA, we
plan to assess the skills that tutees have mastered, and adaptively select problems where tutees are likely to
make errors that both parties can benefit from. Simultaneously, we will assess the peer tutor tendency to provide
unsolicited help before a step has been attempted, and, if it is high, select problems for the tutee that the peer
tutor has not yet mastered. Hopefully, if the peer tutor is struggling with the concepts in the problem, he or she

© ISLS

553

COMPUTER SUPPORTED COLLABORATIVE LEARNING PRACTICES

CSCL2009 PROCEEDINGS

will be less able to simply walk the tutee through the problem, and more joint knowledge construction will
occur. This intervention is potentially advantageous because of its subtlety; students are unlikely to notice the
deliberateness of it, but it has the potential to increase the opportunity for tutees to make errors and therefore the
potential for learning. Adaptively selecting problems to improve learning conditions is an example of guidance
with an indirect presentation, as it is not directly delivered to the student, and implicit instruction, as it does not
make the next interaction steps clear to students.
Table 1. Learning opportunity created by tutee error while solving the equation “3q-xq = x.”
Step Description
Analysis
Tutee selects “factor q”, but types “3q = x”.
The tutee knows what to do, but is not sure how to
complete the step.
Peer tutor approves the calculation, and receives error The peer tutor initially thinks the step is correct, but
feedback from the cognitive tutor.
is made aware from the system that it is an error,
creating a learning opportunity.
The peer tutor tells the tutee “undo that step”, but the The peer tutor understands that the tutee has not
tutee proceeds by dividing by 3. The tutee clicks the solved the problem.
done button, but the peer tutor disagrees.
The students have the following exchange:
The peer tutor identifies the error for the tutee in an
Peer tutor: undo it
unelaborated way.
Tutee: why? U marked it right?
Peer tutor: the step is right but it said you made a
typing error when you factored
The dialog continues until the tutee confirms which step
to undo.
The tutee undoes the step, and the tutor explicitly tells The peer tutor then tells the tutee how to complete
the tutee what to do, after asking for a hint:
the step, correcting his own error.
Now factor out q. It should be q(3 – x) + x.
q(3 – x) = x, sorry

7B

Peer-Mediated Feedback

The current implementation of peer-mediated help in our system lies in the way assistance was presented, where
error feedback and hints on tutee problem-solving actions were presented to the peer tutor. We hoped that the
peer tutor would elaborate on the help and adapt it to the tutee’s needs, improving the learning of the tutee.
Below, Table 2 contains an example drawn from a different pair than Table 1. The peer tutor is told that they are
not actually done with the current problem, and then more successfully communicates hint feedback to the tutee
than the peer tutor from the previous example. In this pair, the tutee had a gain score of 0.375 on the delayed
posttest. Here, the tutee benefitted from committing an error and engaging in a dialog with the tutor.
Table 2. Example of peer-mediated feedback. Students are solving for
They need to simplify the equation.
Step Description
Tutee selects the done button. Peer tutor incorrectly agrees, and
receives feedback from the system.
The tutee says, “do u kno wat i should do”. The tutor looks at the
problem solution.
Students have the following dialog:
Tutor: look at the neg sign on the denominator
Tutee: but wat do i do to get rid of the negative?
Tutor: the neg has to disappear u ll find it in trans
Tutee: will u please just tell me already?
Tutor: i don’t remember what it’s called
The dialog continues until the tutor realizes that he does not actually
know the specific next step.
The peer tutor asks for a hint from the cognitive tutor. She
communicates the help, saying “use common factor”. The tutee
simplifies fractions and then promptly undoes it. The tutor says, “-1”
, and the tutee factors -1. Finally, the tutor says, “now simplify.” The
tutee simplifies and completes the problem.
554

t in the equation: t = (-bh+mn)/(-v-r).
Analysis
Both students are surprised to hear
that they are not done.
The tutee asks for a hint, and the tutor
consults the worked example to help
her.
The tutor begins to give elaborated
help, but lacks the knowledge to fully
identify and explain the step. The
tutor is unsuccessful at helping the
tutee.

The peer tutor uses a hint to provide a
series of procedural instructions to the
tutee.
The
tutee
successfully
completes the problem.

© ISLS

DESIGNING FOR CSCL PRACTICES

This example illustrates an additional place for implementing peer-mediated feedback. One difference
between the examples in Table 1 and Table 2 is that the peer tutor in Table 2 attempted to explain the error,
which was less the case in Table 1. Often, even when peer tutors transferred the system feedback they received
to the tutees, they did not elaborate sufficiently on the feedback. Therefore, we propose to use “reverse”
mediated feedback to the tutee in order to encourage tutors to produce better explanations. For problem steps
where tutees receive help from the tutor, and it is likely that they do not understand the concepts involved with
the help, we plan to deliver indirect explicit feedback to the peer tutee such as: “Wait -- do you understand why
you should subtract x? If not, ask your partner why.” This approach is in contrast to a direct and explicit
feedback approach, where the prompt would generally be given to the peer tutor: “Why don’t you tell your
partner why they should subtract x.” In this proposed “reverse” mediated feedback, it is not so clear that
blocking other tutee actions (e.g., problem-solving actions) as they receive this feedback is the best direction, as
it takes away some tutee control over their environment. How to balance student control with partner confusion
is still an open question. Nevertheless, we envision that this mediated feedback will promote better selfregulation of the collaborative learning and potentially trigger a deeper interaction.
8B

Adaptive Resources

Another attempt to help peer tutors provide good advice to tutees was by providing them with a worked-out
solution to the problem in the interface. As an implementation of the Adaptive Resources design idea, this
approach is limited, because the resource (the problem solution) did not change during problem-solving.
However, looking at how students used this fixed resource might give us better insight into how tutors might
benefit from an adaptive resource. Students appeared to use the problem answers in two ways: to check the
work of their partner and to figure out the next problem step. In fact, the problem solution was consulted
frequently in the course of regular problem-solving so that the peer tutor was always prepared to give help.
Thus, we see an opportunity here to adaptively present resources in order to encourage deeper conceptual
interaction amongst the students. In the process of comparing the tutee actions to the problem answers, some
tutors were able to generate help that contained conceptual information, suggesting that they were engaging in
beneficial knowledge-building processes. Table 3 is an example of a conceptual exchange observed between
students, where the peer tutor involved had a gain score of .625 on the delayed test. Although this exchange is
the type of interaction we were hoping to see, this kind of conceptual help was rare among students.
Table 3. Conceptual interaction about problem ay + by + m = n
Step Description
The tutee factors y. The tutor checks the problem answers (which
say to subtract m from both sides). The tutor marks the problem
step wrong, and the tutee undoes the step.
The students have the following dialogue:
Tutor: ok um what variable is by itsself
Tutor: that is the one you need to get on the other side
Tutee: right now just “n” but i have to get “y” by itself
Tutor: look at the equation ay+by+m ...wat 1 is bby itself
Tutee: m
The tutee adds m. The tutor gives a hint:
Tutor: look at the sign b4 n
The tutee combines like terms. The tutor checks the problem
answers and flags the step. The tutee undoes both steps.
Tutor: look at the sign b4 the m is it a plus or a minus
Tutee: it a plus so i would wnt to minus it from the rest of the
problem

Analysis
The tutor (incorrectly) flags the tutee
because her solution doesn’t match the
problem-solving action
The tutor conceptually explains the first
step as she sees it.

The tutee makes a conceptual error, and
the tutor immediately moves to correct it.
The tutor uses the fixed resource to verify
her thinking, then marks the step wrong.
The tutor continues giving the conceptual
hint. The tutee self-explains her
reasoning.

We intend to explore two types of adaptivity in delivering resources to students: Changing the content
of the resources based on the current problem state, and changing the content of the resources based on an
assessment of student knowledge. There are several different types of resources we can provide to peer tutors
other than a worked out problem example, arranged in order from most general to most specific:
R1. Conceptual description of how to solve the problem rather than the problem steps
R2. Example of a similar problem, but using numbers in place of letters representing constant terms
R3. An annotated worked-example with conceptual explanations for each step
Different levels of help might be appropriate at different times in the problem: The earlier resources might be
better for tutees who have mastered the skills necessary to solve the problem or have not made many attempts at
the problem step, while the later resources might be better for students who have made several incorrect
© ISLS

555

COMPUTER SUPPORTED COLLABORATIVE LEARNING PRACTICES

CSCL2009 PROCEEDINGS

attempts or are not expected to have the skills required for the problem. Additionally, the content of the
resources themselves could be adapted based on information about the current problem-state, skill mastery, or
student interaction. For example, R2 could also display the errors made by the tutee on the problem using
numbers in place of letters, or R3 could derive the conceptual explanations using language that students have
used previously. Making the resources adaptive means that we can provide peer tutors with a wide variety of
different resources over the course of the activity without overloading them, and we can tailor the presentation
of each resource to the particular problem situation and to the abilities of the tutee.
4B

Discussion

In this paper, we have outlined a design space for the delivery of adaptive feedback to collaborating students,
focusing on two dimensions: the explicitness of the feedback content, and the directness of the feedback
presentation. The three ideas that we have generated, each falling into a quadrant of the design space, are not
incompatible with direct feedback, nor are they incompatible with each other. It is likely that each idea is best
applied in particular contexts, and multiple feedback types should be integrated within a single system. In
APTA, it makes sense to use adaptive opportunities to create an amount of errors sufficient for the peer tutor to
benefit from the interaction, mediated feedback to encourage tutors to generate explanations rather than
instructions when tutees make errors, and then adaptive resources to help peer tutors put conceptual elements
into their explanations. In the cases where one feedback type doesn’t work, a second feedback type might be
more appropriate; for example, if mediated feedback isn’t being communicated, it would seem natural to switch
to direct feedback. Determining when and how to apply each kind of feedback is still an open research question,
with the ultimate goal of optimally facilitating computer-supported collaborative learning interaction.
4B

Acknowledgments

This research is supported by the Pittsburgh Science of Learning Center, National Science Foundation Grant
#0354420. Thanks to Amy Ogan, Dejana Diziol, Turadg Aleahmad, and Carolyn Rose.
5B

References

Baghaei, N., Mitrovic, T., and Irwin, W. (2007). Supporting Collaborative Learning and Problem Solving in a
Constraint-based CSCL Environment for UML Class Diagrams. International Journal of ComputerSupported Collaborative Learning, 2 (2-3), 159-190.
Constantino-Gonzalez, M. A., Suthers, D., & Escamilla de los Santos, J. (2003). Coaching web-based
collaborative learning based on problem solution differences and participation. Artificial Intelligence in
Education, 13(2–4), 263–299.
Dillenbourg, P. (2002). Over-scripting CSCL: The risk of blending collaborative learning with instructional
design. In Kirschner, P. A. (Ed.), Three worlds of CSCL: Can we support CSCL? (61-91). Heerlen:
Open Universiteit Nederland.
Koedinger, K., Anderson, J., Hadley, W., & Mark, M. (1997). Intelligent tutoring goes to school in the big city.
International Journal of Artificial Intelligence in Education, 8, 30-43.
Kumar, R., Rosé, C. P., Wang, Y. C., Joshi, M., Robinson, A. (2007). Tutorial dialogue as adaptive
collaborative learning support. In R. Luckin, K. R. Koedinger, & J. Greer (Eds.) Proceedings of
Artificial Intelligence in Education (pp. 383-390). IOS Press.
Lou, Y., Abrami, P., & d’Apollonia, S. (2001). Small group and individual learning with technology: a metaanalysis. Review of Educational Research, 71(3), 141-178.
O’Donnell, A. M.,& Dansereau, D. F. (1992). Scripted cooperation in student dyads: A method for analyzing
and enhancing academic learning and performance. In R. Hertz-Lazarowitz & N. Miller (Eds.),
Interaction in cooperative groups. The theoretical anatomy of group learning (pp. 120–141). New
York: Cambridge University Press.
Rummel, N. & Weinberger, A. (2008). New challenges in CSCL: Towards adaptive script support. In G.
Kanselaar, V. Jonker, P.A. Kirschner, & F. Prins, (Eds.), International perspectives of the learning
sciences: Cre8ing a learning world. Proceedings of the Eighth International Conference of the
Learning Sciences (ICLS 2008), 3 (pp. 338-345). International Society of the Learning Sciences, Inc.
Soller, A., Jermann, P., Muehlenbrock, M. & Martinez, A. (2005). From Mirroring to Guiding: A Review of
State of the Art Technology for Supporting Collaborative Learning. International Journal of Artificial
Intelligence in Education, 15(4), 261-290.
Walker, E., Rummel, N., & Koedinger, K. (2008). To tutor the tutor: Adaptive domain support for peer tutoring.
In B. Woolf, E. Aimeur, R. Nkambou, S. Lajoie (Eds), Proceedings of the 9th International
Conference on Intelligent Tutoring Systems. (pp. 626-635).

556

© ISLS

Learning @ School

#chi4good, CHI 2016, San Jose, CA, USA

Lessons Learned from In-School Use of rTAG:
A Robo-Tangible Learning Environment
Victor Girotto1, Cecil Lozano1, Kasia Muldner2, Winslow Burleson3, Erin Walker1
1

Arizona State University
Tempe, AZ, USA

2

Carleton University,
Ottawa, Ontario, Canada

3

New York University,
New York, NY, USA

victor.girotto@asu.edu, cecil.lozano@asu.edu, kasia.muldner@carleton.ca, wb50@nyu.edu,
erin.a.walker@asu.edu
ABSTRACT

[29]. They can construct knowledge collaboratively in blog
posts or discussion forums [6], explore complex concepts
within a virtual world [2], and even interact with tutoring
systems that analyze problem solving and tailor future
exercises [35]. A recent meta-analysis suggests that over
the past 40 years, classrooms using digital technologies
result in a significant student achievement over classrooms
that do not [33]. These benefits are not restricted to
students: Educational technology enhances teachers’ ability
to prepare students for an increasingly collaborative and
information-oriented work force [11,36].

As technology is increasingly integrated into the classroom,
understanding the facilitators and barriers for deployment
becomes an important part of the process. While systems
that employ traditional WIMP-based interfaces have a wellestablished body of work describing their integration into
classroom environments, more novel technologies generally
lack such a foundation to guide their advancement. In this
paper we present Robo-Tangible Activities for Geometry
(rTAG), a tangible learning environment that utilizes a
teachable agent framing, together with a physical robotic
agent. We describe its deployment in a school environment,
qualitatively analyzing how teachers chose to orchestrate its
use, the value they saw in it, and the barriers they faced
while organizing the sessions with their students. Based on
this analysis, we extract four recommendations that aid in
designing and deploying systems that make use of
affordances that are similar to those of the rTAG system.

To date, the majority of mainstream educational software
has been designed for personal computers and related
devices. While this kind of software can be beneficial, the
WIMP (window, icon, menu, pointing device) paradigm of
personal computing does create an artificial separation
between the input device, system output, and underlying
real-world representation [15]. This paradigm also
encourages a style of interaction where students simply sit
in front of a computer and interact with a virtual
environment on a screen. While tablet and mobile devices
have become more popular in recent years, many
educational apps still apply a similar style of interaction.
Thus, some researchers are beginning to explore the
affordances of more embodied and tangible interactions,
ranging from collaborative activities surrounding an
interactive tabletop [17], to classroom-sized distributed
simulations that teach science [22], to interactive robots that
teach language learning [32]. Preliminary investigations
have highlighted that such technologies can be highly
engaging for students and foster learning, but more work is
needed to understand the utility of these technologies,
particularly in classroom settings.

Author Keywords

Classroom integration; social robot; teachable agents;
embodied learning.
ACM Classification Keywords

K.3.1. Computer Uses in Education
INTRODUCTION

Technology has become an important element of many
classroom environments (e.g., by 2009, 97% of American
teachers had computers in their classroom [10]). Students
routinely type their assignments on word processors or
search for information relevant to their class on the Internet
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
CHI'16, May 07-12, 2016, San Jose, CA, USA
© 2016 ACM. ISBN 978-1-4503-3362-7/16/05…$15.00
DOI: http://dx.doi.org/10.1145/2858036.2858454

The Robo-Tangible Activities for Geometry (rTAG) system
supports physical, embodied interactions with a robot. The
system consists of a Cartesian plane projected onto a white
floor mat where a robotic agent, named Quinn, navigates.
rTAG facilitates students in mastering the domain through
processes related to tangible embodied learning and
learning by teaching [24]. Previous work has focused on
user studies in laboratory environments [23,24] and there is
little in the way of an established body of literature to guide
919

Learning @ School

#chi4good, CHI 2016, San Jose, CA, USA

the translation of rTAG from a laboratory setting to
classroom use.

environments that employ nontraditional interactions in a
school setting.

Understanding the barriers that teachers experience in their
use of different forms of technology in the classroom is
vital for the successful integration of those technologies.
Because of the popularity of WIMP-based forms of
technology, there is a well-established body of work that
examines how to integrate these technologies into
classroom practice. For example, a recent review by
Bingimlas surveys several barriers to integration at the
teacher level (lack of teacher confidence, lack of teacher
competence, resistance to change) and at the institutional
level (lack of time, lack of training, lack of access to
resources, lack of technical support) [4]. Bingimlas’ review
parallels several related analyses that divide barriers into
those constraints imposed on the teachers, and those related
to the teacher’s attitudes and beliefs [1,3,9].

RELATED WORK
Integrating Non-WIMP Environments into Classrooms

Kharrufa et al.’s research on the deployment of digital
tabletops in a classroom [17] identified five themes,
including control (how much a teacher felt in control of the
classroom), and awareness (how aware teachers felt of
what students were doing). Their findings show that the
deployment had both positive and negative responses from
students and teachers. They highlight the importance of
supporting teachers through flexibility, or making the
system flexible enough to adjust to teachers’ plans, and
awareness, or making teachers aware of students’ progress.
Another similar deployment was made by Hayes et al., who
investigated the use of CareLog, a system that aids in the
capture and analysis of student behavior information within
the context of a special education school [12]. Their fivemonth study yielded various design principles focusing on
empowering teachers to make informed decisions. Poole et
al. also discussed the deployment of an ubicomp system
that targeted positive health behaviors within a school [29].
Using diverse sources of data, their qualitative analysis also
resulted in design recommendations (such as to focus on
student-teacher interaction, to be mindful of school
boundaries, and to design for group experiences).

Recommendations for overcoming these barriers related to
traditional WIMP interfaces focus heavily on providing
teachers with resources and training. For example, Hew and
Brush suggest maintaining a shared vision for technology
integration, overcoming scarcity and resources, changing
attitudes and beliefs, and providing professional
development [13]. It is important to note that professional
development should not consist only of instructing teachers
on how to use the system, but should also provide a
pedagogical background that helps in understanding why
the system is effective [3]. To provide this kind of support,
it is necessary to understand how teachers perceive the
value of the system, what barriers to implementation exist,
how they can be overcome, and how teachers can integrate
the systems into their current classroom practices.

As a precursor to the integration of tangible embodied
learning environments into classroom practice, Moher [22]
describes the integration of “embedded phenomena,” which
moves technologies off desktop computers into classrooms.
These embedded phenomena include simulations of
scientific events, such as observations of orbital dynamics
and seismic events occurring on a fault line running through
the classroom, through media presented on tablets affixed
to the walls. Students interact with the simulations over
weeks or months, collecting data and making predictions.
Although the paper’s focus was on describing interactions
with the technology, the authors acknowledge that teachers’
knowledge of individual students was critical to ensuring
that students learn effectively from the simulations. Lui et
al. [20] also described the integration of immersive
simulations in the classroom, and emphasized the necessity
of iterative design and co-design with teachers.

Since basic technology integration within classrooms has
historically suffered from logistics, time, and financial
constraints [25], it is reasonable to think that these issues
hold, or may even increase when integrating more advanced
technologies in the classroom. Teachers may find
pedagogical activities using complex technologies too time
consuming due to the amount of training required to
understand how to use them, or due to the time required to
integrate them into classroom activities.
Our overarching research interest is in improving
understanding of how to facilitate the integration of nonWIMP educational technologies into classroom practice. As
a step towards that, we describe in this paper the design of
the rTAG system, with a specific focus on the elements that
make it suitable for use in classrooms. We then
qualitatively analyze the data collected from the
deployment of the rTAG system in a school context using
Thomas’ General Inductive Approach [34]. This analysis
focuses on the teachers’ viewpoint, identifying: (1) how
they orchestrated the system’s usage; (2) The value that
they saw in this system; and (3) barriers faced by them in
this implementation. From this analysis, we extract design
recommendations that can apply to other learning

These contributions highlight the importance of teachers in
the successful implementation of non-traditional systems in
school environments. This paper aims at further expanding
knowledge on this form of integration, focusing on the
perceptions of the teachers and on the particular affordances
of the rTAG system.
Robotic Learning Environments

rTAG is, at its core, a robotic learning environment. The
system was inspired by Papert’s robotic LOGO system, in
which students used LOGO primitives (commands) to
920

Learning @ School

#chi4good, CHI 2016, San Jose, CA, USA

control robots [26]. However, rTAG uses embodied
interaction and a teachable agent framing in ways that
extend beyond the LOGO paradigm. rTAG also leverages
principles of robotic learning environments to create more
social engagement with the activity, something others are
beginning to explore as well. For example, Saerbeck et al.
[32] used the iCat robot to investigate how a socially
supportive cat influenced the task of language learning. In
contrast to a neutral cat, users of the socially supportive
version learned more and were more motivated. Leite et al.
[19] also relied on the iCat framework, creating a robotic
agent that empathized with human chess players. During a
game of chess, the robot would generate empathetic
messages to the human player, such as “don’t be sad, you
didn’t have better options”. Compared to those who had a
neutral robot, users who worked with the empathetic
version provided higher ratings of degree of companionship
with the robot. Kanda et al. [16] conducted a two-month
trial in an elementary school with a social robot called
Robovie, who could express various social behaviors, such
as calling children by name. The focus of this work was to
explore the possibility of social human-robot relationships;
thus, integration issues were not addressed.

agent tutors notice their own misconceptions and elaborate
on their knowledge as they watch their agents solve
problems [5]. Thus, teaching a computer agent is highly
beneficial for the student doing the teaching: it can lead to
more learning than being taught by a computer agent [18],
is nearly as effective as being taught by a human tutor [30],
and is more effective than classroom instruction [27].
Recently, Hood, Lemaignan, and Dillenbourg extended a
Nao robot so that students could teach it about handwriting
[14], further suggesting that the promise of teachable agent
interactions can be extended to teachable robot scenarios.
THE rTAG SYSTEM
System Overview

rTAG is designed to teach basic geometry concepts to
middle school children [23,24]. It is assembled using
components that may already be common in some school
settings, including iPod Touches, Wii Remotes, a LEGO®
Mindstorms® NXT robot, and a projector.
The system is comprised of three main components. The
first is the problem space, which consists of a Cartesian
plane projected onto a white floor mat. This plane contains
a virtual agent, and can also have zero or more points
plotted onto it. The second component is Quinn, a teachable
and affective agent that is comprised of a LEGO®
Mindstorms® NXT robot and an iPod Touch placed on its
top. This iPod Touch displays Quinn’s face and outputs its
voice, through which it can give affective responses. It also
provides the entry point for interacting with Quinn. The last
component is the mobile interface, which consists of
another iPod Touch, this one held by the student when
interacting with the system. Through the mobile interface,
the user selects commands for Quinn.

Teachable Agent Environments

The second inspiration for rTAG is teachable agent
systems. Teachable agents have emerged from the body of
research showing that students benefit from tutoring other
students [28]. For instance, when students know they will
be tutoring their peers, they are more motivated to attend to
educational material. As their partner takes steps and makes
errors, they reflect, noticing their own misconceptions; as
they give explanations, they elaborate on their knowledge
and construct new knowledge [31].

Before using the system, students are told that they need to
help Quinn learn how to solve geometry problems—
examples of problems are: “Plot point (3, 1)” and “Plot
point (-2, 3)”. To solve these problems, students have to
issue commands to Quinn, so that it will move to the
specified location and plot the point. To give a command to
Quinn, students must first touch its face (the iPod Touch
screen that is on top of the robot). This triggers a pop-up on
the student’s mobile interface, from which he or she can
select an action for Quinn to perform. Actions include move
n units, turn d degrees (counter-clockwise) and plot point.
Therefore, a possible solution plan for the problem “Plot
point (3,1)” could consist of performing the actions move 3,
turn 90, move 1, and finally plot point. Before each action,
the student has to approach Quinn and touch its face in
order to trigger the menu on the mobile interface. Since
Quinn is always moving, this means that the student will
always be moving as well.

Accordingly, developers have designed educational systems
where students teach an agent about the subject they are
learning. The most investigated teachable agent system is
Betty’s Brain, designed to help students learn about causal
modeling [5]. Students teach Betty, their agent, by using
resources such as text and videos to draw causal networks.
Students can ask Betty questions that she will answer based
on the network, and at some point, Betty will take a quiz. In
Betty’s Brain, the teaching mechanism is one of framing--it
is the students that create the causal network, and thus,
when Betty takes a quiz, it is their work that is being tested.
In contrast, other teachable agent platforms such as
SimStudent leverage the co-learning potential of teachable
agents. After each problem-solving step SimStudent takes,
she asks her human tutor if the step is correct, and updates
her knowledge with the response. As a result of this
learning process, SimStudent makes errors that simulate
ones a human student might make, leading human tutors to
reflect on their misconceptions [21].

After plotting the point, the student can check if the
solution is correct or incorrect by tapping on the “Check
Answer” button available on the mobile interface, which
triggers the system’s response with both visual and audio

Studies show that students are highly motivated to teach
their agents, feel responsible for them, and so try harder and
attend more to instructional material [7]. Moreover, peer-to921

Learning @ School

#chi4good, CHI 2016, San Jose, CA, USA

feedback on correctness. Following this feedback, Quinn
smiles for positive (correct) outcomes and frowns for
negative (incorrect) ones, and then makes a social statement
related to the outcome of the problem such as “We worked
hard to solve that problem.”

to Quinn, which looks the same as the iPod interface (Fig. 1
right). It is possible for teachers and students to become
comfortable using the WIMP version, called vTAG, before
transferring to the non-WIMP version. Collectively, the
rTAG and vTAG versions are referred to simply as the
TAG system.

Design Principles

The rTAG system combines robotic learning environments
and teachable agent learning environments. Quinn is a
robotic agent that moves within the physical space.
Students also move within the physical space, and thus
concepts such as differentiating between axes or translating
points may be encoded in movements such as pointing or
walking to a point. Further, the rTAG system uses a
teachable agent framing where users are told that they are
teaching Quinn how to solve the point plotting problems.
As described in the related work section, teachable agents
have been shown to have a positive impact in learning and
engagement, as students attend more to the domain
material, reflect on the knowledge required to solve the
problem, and feel responsible for the agent’s performance.

Figure 1. rTAG (left) and vTAG (right).
rTAG is designed to leverage teacher objectives in a way
that, ideally, engenders positive attitudes and goodwill
towards the system. In addition to targeting the geometry
domain, it targets cross-curricular skills like collaboration
and critical thinking, which are important skills for a
successful life, and are being increasingly worked on by
schoolteachers due to standards such as the Common Core
[8]. It presents an engaging and novel activity that may
motivate students to attend more to learning content. We
return to implications of these features later.

rTAG was designed to explore the potential for installation
in classroom environments. First, it was designed to help
address the barrier of insufficient school resources. A fully
assembled system costs roughly $2000. While this is too
expensive for a typical classroom, it is much more
affordable than most embodied learning environments (e.g.,
US$35,000 for SMALLab [37]), and with optimizations
and price drops on components such as the projector, we
anticipate that it will be possible to further reduce the cost.
Furthermore, the system is built with components that may
already exist in a school, such as LEGO® Mindstorms®
NXT robots, regular laptops, and portable web browsing
devices such as the iPod Touch (Fig. 1 left). In addition,
because the system is a physical installation, it allows
several students to position themselves around the edges,
making observing and engaging with the system more
accessible to large classrooms of students.

DEPLOYMENT

We conducted a week long study to evaluate the impact of
the TAG system in a school setting, inviting teachers from a
California public school district to bring their students to
one or more sessions taking place at a room in the district’s
office in which we had set up several stations of the TAG
system. This district is particularly engaged in integrating
technology and fostering domain-independent skills. This is
clearly visible in their Mission Statement, which highlights
the “4 C’s”: collaboration, communication, critical
thinking, creativity, as well as STEAM initiatives. Both the
district’s superintendent and technology administrator, our
contacts within the school, demonstrated great interest in
this project, which further shows the district’s commitment
to adopting technology. In this district, 79% of students
qualify for free or reduced price meals.

rTAG also includes design features intended to improve
teacher confidence and competence while using the system.
It is built from recognizable subcomponents that many
teachers are already familiar with: LEGO® Mindstorms®,
iPod Touches, and Wii Remotes. As such, its functionality
and design aims at being more interpretable than the black
box approach of some commercial systems.

Twelve teachers, from 4 different schools, scheduled one
session each for their classes. Classes had 25-40 students,
with 8 classes from 3rd, 3 from 4th and 1 from 5th grade.
Five teachers opted to have additional facilitators (parents,
administrators, or teacher interns) in their sessions.

We have built both virtual and physical versions of rTAG,
which creates a bridge between the familiar WIMP version
of the system and the less familiar non-WIMP version of
the system. The virtual version, named vTAG, has the same
functionality as rTAG, but all the interactions and actions
take place through a regular WIMP-based interface. The
screen is divided in 3 sections: on the left is the Cartesian
plane with a circle representing the robot selected with a
mouse click, on the top right is the face of the robot Quinn
and on the bottom right is the interface to give commands

Three researchers travelled to the school one day before the
study to set up the system, but only two remained to
oversee the study. The room was arranged in a semi-circle
with a total of 5 stations, as shown in Fig. 2. There was one
rTAG station located in the middle of the room, 3 virtual
TAG stations (vTAG) and one LEGO® Mindstorms® NXT
922

Learning @ School

#chi4good, CHI 2016, San Jose, CA, USA

2.0 station. We physically separated the vTAG stations to
minimize bias from adjacent vTAG stations. The LEGO®
station had a robot built similarly to Quinn, but without a
face interface. Instead, students used the regular EV3
interface to program it to move. This station, therefore,
represented a more common usage of this kind of robot.
Each station had a concise system manual and solutions to
the problems. Video cameras were arranged around the
room to get both a wide angle view of the room and a closer
take of the students’ interactions with the system, while
minimizing interference or distractions as much as possible.

storyboards to show an ideal student interaction with the
system (in two groups, ~30 minutes), and shared their
designs (~30 minutes).
Finally, two of the five teachers involved in the focus group
participated in a follow up semi-structured reflective
interview. Both teachers watched the footage of their
sessions before the interview.
METHOD

To better understand the affordances and limitations of
integrating the TAG system within a school setting, we
analyzed teacher’s actions and perceptions through three
objectives: 1) understand how they orchestrated their
sessions, which helps us understand the physical and
logistical constraints of the system, 2) understand the value
they saw in the system, giving us insight into what worked
for them, and 3) understand barriers they faced while using
the system, which helps us understand complexities and
limitations of the system. By examining how they used the
system, why they might want to use it, and what obstacles
they faced, we will be able to better understand how to
iterate on the design of the system to make it more suitable
for classroom use.

Figure 2. Physical organization of the room.
Consenting teachers were encouraged to be part of all the
study activities that took place before, during, and after the
immersion session. Before the session, teachers received a
summary of the study activities and the description of the
system combined with a short video showing how it
worked. They were encouraged to send us a new set of
problems for their students to match their learning goals
(none requested this), send us a lesson plan (1 did) and
come to a training session before their scheduled session so
they became familiar with the system (less than half did). In
the one-hour training session, teachers were debriefed about
the goals of the study, the setup and the teaching framing
and learned how to give Quinn commands.

Analysis of the interviews, focus groups, and session
footage was performed using the General Inductive
Approach method outlined by Thomas [34], with the goal
of extracting themes from the data. Two members of the
research team independently followed this approach,
occasionally meeting to converge on the themes that were
identified and the quotes that were related to them. The
process is as follows: 1) initial reading of the data in order
to gain familiarity with it; 2) identifying segments of data
(e.g., interview responses, teacher interactions with
students) that related to our three objectives (orchestration,
value, and barriers); 3) labeling the segments and creating
categories; 4) reducing overlap and redundancy between
categories; and 5) creating a model that incorporates the
most important categories. This procedure followed the
independent parallel coding strategy outlined by Thomas
for checking consistency of qualitative coding [34]. We
now describe the themes that arose from this analysis.

The immersion session had a brief introduction from the
research team, a training phase followed by an immersion
phase, and concluded with a short unstructured debriefing
phase. The training phase was intended to be led by the
teacher, instructing students on how to perform all TAG
actions that they would need to solve problems (e.g., move
and turn Quinn, check the answer). Right before the
immersion phase, the class was notified that teachers were
the ones in charge of the session and experimenters would
be available to help only with technical difficulties. The
debriefing phase consisted of a few questions from the
teacher or experimenter about student impressions of
Quinn, the system, and their general experience. After each
day, the researchers were responsible for shutting down the
system, as well as starting it again on the following day.

RESULTS
Orchestration

Orchestration refers to the activities that the teachers chose
to employ in order to facilitate their use of rTAG, including
activities both before and during the session. It was
analyzed by looking mainly at the session footage, but it
was complemented with the data from the interviews and
focus groups. The goal was to understand how teachers
used the rTAG setup, and if it differed from the more
common vTAG setup. We organized orchestration into five
subcategories: pre-session instruction, session introduction,
student distribution, session management, and rotation of
students. Table 1 summarizes the findings for this category.

After the sessions, the teachers were invited to participate in
a 2-hour focus group activity on the final day of the week.
Five teachers participated in this activity, in which they
reflected on the TAG system (~30 minutes), collaboratively
designed a lesson plan (~30 minutes), developed
923

Learning @ School

#chi4good, CHI 2016, San Jose, CA, USA

For the remainder of this paper, we will refer to specific
sessions by a code in the form X-Y, where X is the day of
the session and Y is the number of the session that happened
on that day. Session 3-2, therefore, is the second session on
the third day. We will also refer to the two focus groups as
FG1 (comprised of T1, T2, and T5), and the second as FG2
(comprised of T3 and T4).
Subcategory

Typical behavior

Atypical behavior

Pre-session
instruction

No instruction;
teach the domain;
teach how to use
the system

None

Session
introduction

Researcher or
teacher handled the
system

Teacher chose
assigned student to
handle the system
(1-1, T1)

Student
distribution

Teacher distributed
students around
stations; preformed teams

Quinn station not
used from the start
(1-1, T1); LEGO®
station not used at
all (1-1, T1)

Session
management

Teachers moved
around the room;
used adult
supervisors

None

vocabulary and group guidelines. FG2, however, proposed
ideas like using YouTube to show how to use the robot and
training team captains who could help other students.
Session Introduction

During the introduction phase (explaining how to use the
system to the class) in the immersion session itself, six
teachers led the session for their students, although three of
them couldn’t remember some details of the system and
required assistance from the researcher. For the remaining
six sessions, the researcher was the one performing the
training. The main difference between how the teachers vs.
researcher introduced the system pertained to time.
Teachers spent approximately five minutes showing
students how to use the system, while the researcher would
spend around fifteen. This finding could relate to the
teachers’ perceptions that students are able to pick up new
technology very fast. T1 says: “With the technology that we
have this school year, I just have found that these kids are
really quick. (…) Literally, I did a five-minute demo in front
of the class with my computer on the projector of how to
make a Google drawing. Five minutes was all they needed,
and they were done. They were off. They were running.
They were making their Google drawings. It was amazing”.
This reflects on the lesson plans and storyboards developed
during the focus group session. The only situation similar to
an in-session introduction was mentioned by FG2, and it
simply stated that teachers should talk about appropriate
behavior and good sportsmanship before the session.

Between stations:
up to the students
(1-1, T1; 2-2, T2;
Student
rotation
2-4, T2); rTAG
managed by
supervisor (1-1, T1)
Table 1. Summary of the orchestration strategies and
exceptions used throughout the sessions.

As for the content of the instruction, one teacher, who had
already taught students the domain before the session, still
focused on reviewing the domain content, and not the
system usage. But regardless of whether the teacher or
researcher introduced the session, the overall strategy was
similar: students gathered around a given station (usually a
vTAG one), where the teacher or researcher would
demonstrate how the system works. One notable exception,
however, occurred in session 1-1, where the teacher (T1)
chose a “very tech savvy” student to man the station while
she explained how the system worked.

Within stations: up
to the students;
between stations:
up to the teacher

Pre-session Instruction

Most teachers did not prepare their students during their
regular classes prior to the session, due to time constraints.
Some of the teachers who did shared that they showed
students the video of the system and Quinn that we sent
them, went over the commands to teach Quinn, and
described some geometry concepts like positive and
negative coordinates, the quadrants, angles, and degrees.
While debriefing after the sessions, teachers reflected on
how they would prepare their students for the session.
Regarding teaching the domain, T5 said: “See, what I did
beforehand, before coming, so they kind of already knew
what to do, I taught them the coordinates”. As for showing
how the system works, T1 planned on integrating her
current practice of demonstrating the system using a
projection onto a whiteboard, where she could freely
annotate the screen. FG1, of which both T1 and T5 were
part of, proposed a lesson plan that followed the same
direction, reinforcing the notion of pre-lesson teaching

Student Distribution

After the introduction phase, teachers would distribute the
students around the different stations. In six out of the
twelve sessions, teachers had already assigned students to
teams before the start of the session. In five of the sessions,
teachers took some extra time to create the teams and then
assigned them to each station. In session 1-1, students could
choose which station to go to.
Usually, all stations were used from the beginning of the
session. However, on session 1-1, T1 decided to hold off on
using the physical system until later, judging that using that
setup upfront would have been “wasted” time. In this
session, rTAG was used only after she gauged that most
students had already used the vTAG station. This strategy is
reflected in FG1’s storyboards, where the rTAG station
924

Learning @ School

#chi4good, CHI 2016, San Jose, CA, USA

would be used only after students had already completed a
few tasks on pen and paper and using the virtual setup. T1
also didn’t make use of the LEGO® station, arguing that it
would be “too much” for “that short amount of time”

session 1-1, a facilitator trusted by the teacher managed the
use of rTAG. Students would sit around the setup, while
one student, chosen by the facilitator, would use the system.
After the student solved a problem, this facilitator would
select another student to solve the next problem.

Session Management

It is interesting to note that even though rTAG is a system
with many novel affordances, it was still used much like the
vTAG stations were (with the notable exception of session
1-1). Teachers normally did not do anything special with
the rTAG station. This warrants further research, but it is
possible that some of the principles behind rTAG, such as
using recognizable subcomponents, may have helped
teachers to perceive it as a more regular system.

During the session, teachers usually moved around the
room, to ensure each group made progress and that all
students had an equal opportunity to try using the system.
Five sessions also employed other adult supervisors, such
as members of the school staff or some students’ parents.
Talking about these helpers, T1 says: “The fact that there
was another adult there that they knew and were
comfortable with, I think helped, whereas if she weren't
there, they probably would have just skirted along the back
and probably never even—would have never even
attempted.” Not all helpers were adults: Some students took
a leadership role, going around the stations to help other
groups. This happened either by initiative of the students, or
in some cases, by explicit leadership assignment from the
teachers. In fact, teachers in FG2 supplemented this goal in
the lesson plans, explicitly assigning some leader students,
who would be responsible for coaching their peers. More
generally, both groups planned for group interactions. This
reinforces the collaboration affordances of this system,
which were deemed valuable by the teachers, especially
given the new Common Core Standards that are being
adopted. T4 stated: “The good thing, I thought, the Common
Core says communication, collaboration. We do some of
that, but this was really good.”

Figure 3. Students using rTAG during a session.
Value

Having examined how the sessions were orchestrated by the
teachers, we now turn to analyzing the value they saw in the
system. The four subcategories we identified are: increase
of engagement, physical robot affordances, technology
exposure, and domain-general skills.

Student Rotation

Rotation of students happened in two levels: within and
between the stations. The first relates to how students
would control which member of their group would be
interacting directly with the station. With the exception of
one session (1-2), in which the teacher had a predefined
order of which student should be interacting with a station,
teachers gave the students freedom to manage this, at most
resorting to some organization by the student leaders in
each group. Students employed various ad hoc strategies:
one group, for instance, used the “rock, papers, scissors”
game to decide who would go first. However, many
students interacted in very fluid ways, such as sharing the
solution generation to a single problem by passing the iPod
Touch around, allowing another student to touch Quinn (on
the rTAG station), or by passing the mouse around (on the
vTAG stations). Rotation between stations was usually
controlled by the teachers. They would rotate the groups
after a given amount of time which varied among sessions,
from 5 to 45 minutes.

Engagement

Teachers saw student engagement as one of the positive
assets of the system. This was very evident from both the
interviews and the footage of the sessions. To illustrate, in
the sessions where a teacher or facilitator asked the students
who wanted to go next on the rTAG station, students would
always promptly raise their hands. Another evidence of
engagement was that whenever time was up for a given
session, those who had not had the chance to interact with
the rTAG station would loudly express their discontent.
Teachers perceived this. For example, T1 said: “They were
excited about him [Quinn]. They just thought he was cool”.
T2 went deeper: “That's what I think is really key, is that if
they aren’t even realizing that they’re learning, that they
think that they just went on this field trip and had fun, but
now they know how to plot points on the positive and
negative side. They just think they went and played with a
robot, which I think is cool.” The focus of the excitement, at
least in this instance, was on the physical robot.

The exception happened in sessions 1-1 (T1), 2-2 (T2), and
2-4 (T2), where students were given the freedom to move
between stations. While all TAG stations were usually
regulated by the students themselves, T1 (session 1-1)
decided to have a tighter control over the rTAG station. On

Robot Affordances

Teachers perceived the physical robot as an important
benefit of the system. T1 emphasizes that the novelty of the

925

Learning @ School

#chi4good, CHI 2016, San Jose, CA, USA

robot would make this a remarkable experience for
students: “I think this was something that the kids are going
to remember because it was separate from the classroom,
different from something they were doing on their netbooks.
The robot was right there in front of them.” T1 also
mentioned a boost in perseverance due to the robot,
especially due to its social attributes: “I think it would
motivate them to persist in a problem and to keep going and
to keep trying to get it the way it was supposed to be and
get Quinn where he was supposed to go. I do think it makes
a difference. I think it's something they can relate to, as
opposed to this faceless no name little box with wheels. I
think it does make a difference”. In fact, T2 contrasts rTAG
with its virtual counterpart: “the reaction’s better with the
physical because it’s like an actual being. It could be their
pet or something. Whereas, on the computer, it’s just so
second nature.” It is possible that this heightened
engagement may be due to a novelty effect, and would
likely decrease over time as they grow accustomed to the
system. Teachers may be aware of this. FG1, for example,
has proposed gamification additions to the system, which
may help to maintain engagement over time.

valuable domain-general skill. This focus on domaingeneral skills is possibly due to the increasing requirement
for compliance to the Common Core State Standards [8].
For students to collaborate, teachers usually divided them
into smaller groups and distributed these groups among the
several available stations, allowing students to organize
themselves within each station. On vTAG, this led to a few
engaged students close to the computers, while others
would mostly just sit back and watch, or wander around the
room. On the physical setup, however, more students
usually tried to participate together with who was currently
using the system. Teacher T2 noted: “I see eight or nine
kids jumping in and trying to help, or looking to see if it’s
time to touch. It’s just they’re more involved, more willing
to maybe offer a solution.” In many sessions, several
students could be seen standing on the foam mat discussing
the problem, constantly passing the iPod Touch around and
taking turns on who would be interacting with Quinn.
Contrasting the vTAG and rTAG setups, we see that most
of the values highlighted by teachers were either present
only on the rTAG setup (for example, the values related to
the robot affordances), or at least heightened by it (for
example, rTAG allowed more room for collaboration.
Teachers also perceived it to be more engaging). T1
explicitly contrasted rTAG to “something they were doing
on their netbooks”, that is, their regular computers.
Nonetheless, vTAG also proved important, as it was
intensively used by most teachers during training, likely
due to the larger screen and increased familiarity.

Technology Exposure

Another common theme among teachers was related to the
value of exposing students to technology. Teachers in this
school aim to make their students proficient in using digital
tools, e.g., most students have their own laptop, and
conduct a great amount of the classroom work in them.
Teacher T1, for example, integrates Khan Academy into a
morning routine: “the kids are all on Khan Academy. One
of our morning activities is the kids’ work on Khan
Academy. Part of Khan Academy, there is a coding section
to it, and I had a group of boys that really wanted to
explore it, and so I said sure. I sent about five of them off
to a little corner in my classroom, and for several weeks,
they explored the coding part of it”. Her goal has been to
“integrate technology into everything that we do as much as
possible”. In this context, the teachers saw great value in
the rTAG system. To them, this was another opportunity of
showing their students some of the affordances made
possible by technology, possibly making them more
proficient in its use, while also leveraging their curiosity.

Barriers

Our analysis also identified perceived barriers faced by the
rTAG system in a school environment. We identified five
subcategories: lack of time for teachers to spend on the
system, lack of teacher training, student intimidation,
technology limitations, and number of students per station.
Lack of Time

Teachers complained about an overall lack of time to
perform activities out of their lesson plans. T2 says: “Do we
have the time? We don’t. We don’t even have the time to
do what we’re supposed to be doing.” This is corroborated
by the fact that only one teacher sent the lesson plan which
we requested from them. T4 states: “Well, I don’t have time
to write a lesson plan, but let me look at what I can do”.
This is relevant for rTAG, as it requires a larger setup and
training overhead when compared to vTAG, possibly
drawing teachers away from using the robotic setup.

Domain-General Skills

Teachers emphasized domain-general skills that can be
acquired through the usage of this system, such as critical
thinking and problem solving. T1 believes “that they have
to be able to analyze what they're doing and problem solve
and decide, what's a more efficient way that I could have
done this. I think that's a huge, huge benefit of this
program.” Further evidence is seen through the
storyboards, where one group explicitly mentioned the goal
of developing the four C’s: collaboration, communication,
critical thinking, and creativity. The other group planned to
attribute the role of leader to some students, giving them the
responsibility of coaching other students, which is another

Lack of Training

Lack of time could also contribute to another barrier, which
is lack of training. In the study we gave teachers two days
to come to training before the sessions started, but many of
them were not present, which made this problem even more
evident. As a result, some teachers could not prepare the
students well in both the domain and the system, and were
926

Learning @ School

#chi4good, CHI 2016, San Jose, CA, USA

not very well acquainted with it themselves. This resulted in
only three out of twelve teachers leading the training
completely by themselves, with the remaining nine—
including some of those who actually came to training—
delegating it partially or completely to the researcher.

design and deployment of robo-tangible learning
environments such as the rTAG system, so that those who
are designing non-WIMP based systems may maximize
their value while minimizing the barriers for their adoption.
Target Multiple Learning Objectives

As exposed in the barriers section, one big issue for
teachers was lack of time. While even simple activities may
already be infeasible, a system like rTAG may suffer from
longer setup times when compared to a simple WIMPbased computer application. Additionally, the system’s
learning curve will probably be steeper for both teachers
and students due to novel or uncommon interaction
methods and technologies. Therefore, it is important for the
system to target multiple learning objectives, including
those that are domain-independent. Doing so should help to
maximize the value of the system for teachers and the
return of their time investment, as it would allow them to
combine multiple activities that target one objective into
one activity that targets multiple. It would also address the
issue that Kharrufa et al. ran into, where teachers reported
that students were not used to proper critical thinking and
collaborative work [17].

Student Intimidation

Teachers also perceived that some students felt intimidated
by the rTAG setup, causing them to prefer the vTAG
stations. T1 reports: “I think the ones that stayed on that
computer with the larger monitor with my student teacher,
those were the ones who were just really afraid and really
not quite understanding it.” This, again, shows the benefit
of having vTAG, where students who may be hesitant to try
the robotic setup can start to use the system on a more
familiar setup. Nonetheless, some were even afraid of the
vTAG setup, actively avoiding using it. T1 continues: “I
think there was a little bit of movement of one of those who
was afraid and didn't really want to try, was getting close to
their turn on one computer, I think they did go to another
computer.” This happened despite the school’s adoption of
technology on the classroom. She says: “There were still
some that just really were almost afraid of it.”

In the case of rTAG, teachers believed that it was a good
way of teaching domain concepts, but also saw value in the
way that the system could facilitate collaboration,
communication, leadership skills, critical thinking, and
problem solving. In addition, teachers found the rTAG
system to have potential for exposing students to new
technologies. rTAG problem sets and related curriculum
should be redesigned to more explicitly facilitate these
higher-order learning objectives. For example, some of the
engineering behind rTAG could be exposed and discussed
as a secondary lesson related to student use of rTAG.

Technology Limitations

The limitations of the technology involved in the system
could impact the workflow of teachers and students. T2
reported having problems when trying to demonstrate the
rTAG system to the students due to the iPod’s small screen
size, since some could not see the little screen. Whereas the
teachers were used to connecting their computers to a
projector to demonstrate systems, they were unable to do so
with the physical setup of rTAG. In addition, previous
experiences led some teachers to worry about the fragility
of the setup. T1 recounts an experience with projected
smart boards, and the fact that if students moved the
projector slightly, there would be significant downtime. T1
says: “When you have the class's attention, and you're
doing something and you're in the middle of a lesson and
something happens, then their attention is gone.”

Emphasize the Collaborative Affordances of the System

One of the positive aspects mentioned by the teachers was
the opportunity for collaboration that rTAG provided. This
was also evident on the storyboards and lesson plans
developed during the focus group session—all of them
included team and collaboration elements. While students
normally work on individual computers in the classroom,
this setup encouraged them to work in groups. This is seen
by contrasting the vTAG and rTAG setups: on the first,
students gathered around the computer. Since there was not
much room for all of them, a few stayed back and simply
watched. On the latter, however, students were able to
gather around most of the projected Cartesian plane,
sometimes even walking around the physical space while
trying to solve the problem together. This result is in
agreement with findings from Poole et al., where group
experiences seemed to foster participation [29]. This
recommendation expands on Poole et al. by emphasizing
that systems with tangible and embodied elements should
explore their affordances to foster collaboration by design,
rather than planning for students to individually use it.

Number of Students per Station

Teachers also reported limitations related to the quantity of
students per station. There were too many students at each
station, relegating some of them to the role of passive
viewers, while only a few members of the group were
actively engaged. T1 suggested that a good number would
be five students per station, since it would allow all students
to have some time with the system, while allowing more
timid students to sit back for a little while to see how the
system works. T2 favored a number closer to ten students.
T4 suggested only three students, arguing that it would give
them more time to perform the tasks.
DESIGN RECOMMENDATIONS

We now turn to four design recommendations based on our
analysis of the data. These recommendations target the
927

Learning @ School

#chi4good, CHI 2016, San Jose, CA, USA

Thus, it follows that rTAG should better leverage the
collaborative affordances of the installation so that many
students can actively use it at a time. We recommend
providing an interface to rTAG that allows multiple
students to give the robot commands, potentially by
facilitating turn-taking or enabling a voting mechanism.

It is important to note some limitations of the data that we
acquired. While we had twelve teachers running sessions,
five of them agreed to participate in the focus group
session. Of those five, only two participated in an interview
session. Therefore, we have much more verbal data from
T1 (whose unique behavior was an outlier) and T2 than we
have from the other teachers. Furthermore, most teachers
could use the system only once for a short session. These
factors affect our ability to generalize the results.
Nonetheless, the higher-level comments made by these two
teachers do not diverge from the goals of the other teachers,
as evidenced through the focus group session, such as
developing problem solving and collaboration skills. This is
compatible with the district’s goals, so it may not be so
visible in places where there is not so much incentive
towards using technology and developing these kinds of
skills. Finally, although our recommendations are more
focused on the context of the rTAG system and the school
we ran our studies in, they have some overlap with the
related work on classroom deployment, as discussed in the
results, which also contributes to supporting our claims.

Optimize Training for the Teachers’ Workflows

Since the system employs a novel interaction method,
training of both teachers and students becomes an important
part of the system’s deployment, not only to ensure they
correctly use the system, but also to enhance their
confidence and reduce apprehension in using the system.
To achieve proper means of training, it is important to
leverage the familiarity with technology they may already
have and to integrate training to their existing workflow.
This is analogous, in ways, to findings from Hayes et al.,
who recommended a minimum disruption of the teacher’s
classroom organization [12]. For the teachers involved with
this current study, they could use a projector hooked up to
their computer to perform the training, just as it was their
habit. This is only possible due to the capabilities of the
TAG system to run in a traditional WIMP interface.

As evidenced in the results section, T1 was unique in her
approach. While this has implications to generalizability, it
can also shed some light into a particular population. T1 is a
white, female teacher with 22 years of experience teaching
grades 1-3. On a scale from 1 to 10 of technological
confidence, she defined herself as an 8+. She employs at
least seven different technologies in her teaching, including
code.org, Khan Academy, and blogs. This shows that she is
extremely confident and engaged in using technology,
maybe more so than her peers, which probably motivated
her different approach for orchestrating the session.

Innovative Use of Commonplace Technology

While the overall interaction with the system may be
unusual and requires training, the familiarity with some of
the system components may help to bring down this barrier,
as well as possibly reducing apprehension of using it. In the
case of rTAG, one of the main input methods was through
an iPod Touch, which is likely familiar to most students.
For example, a student asked: “Is that an app? I wanna go
home and get the app.” The student had a better
understanding of how the system functioned because of his
familiarity with its components. The new application of
known technology could also motivate students to explore
new technology-related possibilities, one of the teachers’
desired outcomes. For administration purposes, repurposing
technology already owned by a school may reduce costs
and facilitate adoption. Something similar was noted by
Moher’s work on Embedded Phenomena [22], where he
chose to use technology already available in the classroom,
but for the purposes of scalability rather than familiarity.

Throughout the sessions, many of the expected outcomes of
the design philosophy behind the rTAG system were clearly
observed. Students developed rapport with Quinn, as it was
expected from having a physical robotic agent that they
could interact with. As expected from the literature
available on barriers for the adoption of technology in the
classroom, teachers demonstrated some constraints such
time, training, and confidence. The design of the TAG
system, however, helped to reduce some of those
constraints to a certain degree. Future iterations of the
system will further develop the system based on the four
design recommendations here suggested to maximally
facilitate classroom deployments.

DISCUSSION & CONCLUSION

Deploying a non-WIMP based system in a school setting
can be a challenging task. While traditional technologies
already have barriers to their deployment, a system such as
rTAG can increase the complexity of in-school integration.
To minimize these barriers, while optimizing the classroom
orchestration, and thus maximizing the value in such
systems, we proposed four design recommendations for the
deployment of non-WIMP (e.g., rTAG) systems in a school
setting: 1) target multiple learning objectives, 2) emphasize
the collaborative affordances, 3) optimize for training, and
4) innovate the use of known system components.

ACKNOWLEDGMENTS

The authors would like to thank Elissa Thomas for her help
with the data collection, as well as the teachers and
administrators of the district. This research was funded by
NSF 1249406: EAGER: A Teachable Robot for
Mathematics Learning in Middle School Classrooms and by
the CAPES Foundation, Ministry of Education of Brazil,
Brasília - DF 70040-020, Brazil.

928

Learning @ School

#chi4good, CHI 2016, San Jose, CA, USA

REFERENCES

1.

2.

Sasha Barab, Michael Thomas, Tyler Dodge, Robert
Carteaux, and Hakan Tuzun. 2005. Making learning
fun: Quest Atlantis, a game without guns. Educational
Technology Research and Development 53, 1: 86–107.

3.

Becta. 2004. A review of the research literature on
barriers to the uptake of ICT by teachers: British
Educational Communications and Technology Agency
(Becta). June.

4.

Khalid Abdullah Bingimlas. 2009. Barriers to the
successful integration of ICT in teaching and learning
environments: A review of the literature. Eurasia
Journal of Mathematics, Science & Technology
Education 5, 3: 235–245.

5.

13. Khe Foon Hew and Thomas Brush. 2007. Integrating
technology into K-12 teaching and learning: Current
knowledge gaps and recommendations for future
research. Educational Technology Research and
Development 55, 3: 223-252.
14. Deanna Hood, Séverin Lemaignan, and Pierre
Dillenbourg. 2015. When Children Teach a Robot to
Write: An Autonomous Teachable Humanoid Which
Uses Simulated Handwriting. ACM Press, 83–90.
15. Hiroshi Ishii and Brygg Ullmer. 1997. Tangible bits:
towards seamless interfaces between people, bits and
atoms. Proceedings of the ACM SIGCHI Conference,
234-241.
16. Takayuki Kanda and Rumi Sato. 2007. A two-month
field trial in an elementary school for long-term
human–robot interaction. Robotics, IEEE Transactions
on 23, 5: 962-971.

Gautam Biswas, Krittaya Leelawong, Daniel Schwartz,
Nancy Vye, and V-TAG. 2005. Learning by teaching:
A new agent paradigm for educational software.
Applied Artificial Intelligence 19, 3-4: 363-392.

6.

John Seely Brown and Richard Adler. 2008. Mind on
Fire: Open education, the long tail and Learning 2.0.
Educause Review 43, 1: 16–32.

7.

Catherine C. Chase, Doris B. Chin, Marily A.
Oppezzo, and Daniel L. Schwartz. 2009. Teachable
Agents and the Protégé Effect: Increasing the Effort
Towards Learning. Journal of Science Education and
Technology 18, 4: 334–352.

8.

Common Core State Standards Initiative. 2010.
Common core state standards for English language arts
& literacy in history/social studies, science, and
technical subjects. Retrieved Sept. 25, 2015 from
http://www.corestandards.org/assets/CCSSI_ELA%20
Standards.pdf

9.

conference on Human factors in computing systems CHI ’08: 685–694.

Abdulkareem E. S. Al-Alwani. 2005. Barriers to
integrating information technology in Saudi Arabia
science education.

17. Ahmed Kharrufa, Madeline Balaam, and Phil Heslop.
2013. Tables in the wild: lessons learned from a largescale multi-tabletop deployment. Proceedings of the
SIGCHI Conference on Human Factors in Computing
Systems, 1021-1030.
18. Krittaya Leelawong and Gautam Biswas. 2008.
Designing learning by teaching agents: The Betty’s
Brain system. International journal of artificial
intelligence in education 18, 3: 181–208.
19. Iolanda Leite, Samuel Mascarenhas, André Pereira,
Carlos Martinho, Rui Prada, and Ana Paiva. 2010.
“Why Can’t We Be Friends?” An Empathic Game
Companion for Long-Term Interaction. Intelligent
Virtual Agents: 315–321.
20. Michelle Lui, Alex C. Kuhn, Alisa Acosta, Chris
Quintana, and James D. Slotta. 2014. Supporting
learners in collecting and exploring data from
immersive simulations in collective inquiry. ACM
Press, 2103–2112.

Peggy A. Ertmer. 1999. Addressing first-and secondorder barriers to change: Strategies for technology
integration. Educational Technology Research and
Development 47, 4: 47-61.

21. Noboru Matsuda, William W. Cohen, Jonathan Sewall,
Gustavo Lacerda, and Kenneth R. Koedinger. 2007.
Predicting students’ performance with simstudent:
Learning cognitive skills from observation.
FRONTIERS IN ARTIFICIAL INTELLIGENCE AND
APPLICATIONS 158: 467.

10. Lucinda Gray, Nina Thomas, Laurie Lewis, and Peter
Tice. 2010. Teachers’ use of educational technology in
US public schools, 2009: First look. National Center
for Education Statistics.
11. Margarete Grimus. 2000. ICT and multimedia in the
primary school. 16th conference on educational uses of
information and communication technologies, 21-25.

22. Tom Moher. 2006. Embedded phenomena: supporting
science learning with classroom-sized distributed
simulations. Proceedings of the SIGCHI conference on
human factors in computing systems, 691-700.

12. Gillian R. Hayes, Lamar M. Gardere, Gregory D.
Abowd, and Khai N. Truong. 2008. CareLog: a
selective archiving tool for behavior management in
schools. Proceeding of the twenty-sixth annual CHI

23. Kasia Muldner, Victor Girotto, Cecil Lozano, Winslow
Burleson, and Erin Walker. 2014. The Impact of a
Social Robot’s Attributions for Success and Failure in
929

Learning @ School

#chi4good, CHI 2016, San Jose, CA, USA

a Teachable Agent Framework Tangible Activities for
Geometry (TAG). International Conference of the
Learning Sciences.

31. Rod D. Roscoe and Michelene T. H. Chi. 2007.
Understanding tutor learning: Knowledge-building and
knowledge-telling in peer tutors’ explanations and
questions. Review of Educational Research 77, 4: 534547.

24. Kasia Muldner, Cecil Lozano, Victor Girotto, Winslow
Burleson, and Erin Walker. 2013. Designing a
Tangible Learning Environment with a Teachable
Agent. Artificial Intelligence in Education, 299–308.

32. Martin Saerbeck, Tom Schut, Cristoph Bartneck, and
Maddy D. Janse. 2010. Expressive robots in education:
varying the degree of social supportive behavior of a
robotic tutor. Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, 1613-1622.

25. Shazia Mumtaz. 2000. Factors affecting teachers’ use
of information and communications technology: a
review of the literature. Journal of Information
Techology for Teacher Education 9, 3: 319–342.

33. Rana M. Tamim, Robert M. Bernard, Eugene
Borokhovskim Philip C. Abrami, and Richard F.
Schmid. 2011. What forty years of research says about
the impact of technology on learning a second-order
meta-analysis and validation study. Review of
Educational Research 81, 1: 4–28.

26. Seymour Papert. 1980. Mindstorms: Computers,
children, and powerful ideas. NY: Basic Books.
27. Lena Pareto, Tobias Arvemo, Ylva Dahl, Magnus
Haake, and Agneta Gulz. 2011. A teachable-agent
arithmetic game’s effects on mathematics
understanding, attitude and self-efficacy. Artificial
Intelligence in Education, 247-255.

34. David R. Thomas. 2006. A General Inductive
Approach for Analyzing Qualitative Evaluation Data.
American Journal of Evaluation 27, 2: 237–246.

28. Rolf Ploetzner, Pierre Dillenbourg, Michael Preier, and
David Traum. 1999. Learning by explaining to oneself
and to others. Collaborative learning: Cognitive and
computational approaches, 103-121.

35. Kurt Vanlehn. 2006. The Behavior of Tutoring
Systems. International journal of artificial intelligence
in education 16, 3: 227-265.
36. Angela F. L. Wong, Choon-Lang Quek, Shanti
Divaharan, Woon-Chia Liu, Jarina Peer, and Michael
D. Williams. 2006. Singapore students’ and teachers’
perceptions of computer-supported project work
classroom learning environments. Journal of Research
on Technology in Education 38, 4: 449-479.

29. Erika Shehan Poole, Andrew D Miller, Yan Xu, Elsa
Eiriksdottir, Richard Catrambone, and Elizabeth D
Mynatt. 2011. The Place for Ubiquitous Computing in
Schools: Lessons Learned from a School-based
Intervention for Youth Physical Activity. Proceedings
of the 13th International Conference on Ubiquitous
Computing: 395–404.

37. Elizabeth Forward lab enables teachers to use technical
tactic dubbed embodied learning. Retrieved Sept. 25,
2015 from http://triblive.com/neighborhoods/257556374/students-learning-tactic-elizabeth-forward-gamesmallab-eighth-madison-arizona

30. Frederick Reif and Lisa A. Scott. 1999. Teaching
scientific thinking skills: Students and computers
coaching each other. American Journal of Physics 67,
9: 819-831.

930

Int J Artif Intell Educ (2015) 25:229–248
DOI 10.1007/s40593-014-0034-8
R E S E A R C H A RT I C L E

Towards Understanding How to Assess Help-Seeking
Behavior Across Cultures
Amy Ogan & Erin Walker & Ryan Baker &
Ma. Mercedes T. Rodrigo & Jose Carlo Soriano &
Maynor Jimenez Castro

Published online: 20 December 2014
# International Artificial Intelligence in Education Society 2014

Abstract In recent years, there has been increasing interest in automatically assessing
help seeking, the process of referring to resources outside of oneself to accomplish a
task or solve a problem. Research in the United States has shown that specific helpseeking behaviors led to better learning within intelligent tutoring systems. However,
intelligent tutors are used differently by students in different countries, raising the
question of whether the same help-seeking behaviors are effective and desirable in
different cultural settings. To investigate this question, models connecting help-seeking
behaviors with learning were generated from datasets from students in three countries –
Costa Rica, the Philippines, and the United States, as well as a combined dataset from
all three sites. Each model was tested on data from the other countries. This study found
that models of effective help seeking transfer to some degree between the United States
and Philippines, but not between those countries and Costa Rica. Differences may be
explained by variations in classroom practices between the sites; for example, greater
collaboration observed in the Costa Rican site indicates that much help seeking
occurred outside of the technology. Findings indicate that greater care should be taken
when assuming that the models underlying AIED systems generalize across cultures
and contexts.
A. Ogan (*)
Carnegie Mellon University, Pittsburgh, USA
e-mail: aeo@andrew.cmu.edu
E. Walker
Arizona State University, Phoenix, USA
R. Baker
Columbia University, New York, NY, USA
M. M. T. Rodrigo : J. C. Soriano
Ateneo de Manila University, Metro Manila, Philippines
M. J. Castro
Universidad de Costa Rica, Brenes, Costa Rica

230

Int J Artif Intell Educ (2015) 25:229–248

Keywords Help seeking . Cross-curricular skills . Assessment . Intelligent tutoring
system

Introduction
The teaching and assessing of cross-curricular skills that cut across domain boundaries,
such as problem-solving and critical thinking, has long been of interest in education
(e.g., Judd, 1908). This interest has led to the decision to emphasize cross-curricular
skills such as problem-solving in the PISA2012 international examinations, and collaborative problem-solving on the upcoming PISA2015 (Beller, 2011). There has been
increasing research on assessing the complex behaviors demonstrated in crosscurricular skills with technology (Martin, 2008), and in modeling them for use in
personalized learning technologies (Csapó, 1999; Greiff, 2012). One such skill that has
received particular attention in recent years in the AIED community and throughout the
learning sciences is how students seek and utilize help.
Help seeking is the process of looking to resources outside of oneself to find
information or strategies that will assist in accomplishing a task or solving a problem
(Ames & Lau, 1982, Karabenick & Knapp, 1991, Nelson-Le Gall, 1985). Early
research regarded help seeking as a negative behavior that indicated student dependency on outside sources (Ames & Lau, 1982, Nelson-Le Gall, 1985). These studies
therefore concentrated on help seeking’s potential negative effects on the student, such
as embarrassment and damage to self-esteem (Nadler & Fisher, 1986, Nelson-Le Gall,
1985). During the eighties, however, the view of help seeking changed (Ames & Lau,
1982, Polson & Richardson, 1988). Educational researchers recognized that it was
important for students to seek help, especially when they encountered ambiguity or
difficulty in schoolwork, in order to be able to continue with the learning process
(Ames & Lau, 1982).
Here, we focus on the role of help seeking within online learning for multiple
reasons. First, help seeking that occurs online is an increasingly important part of
learning, as growing amounts of learning take place online around the world (Nafukho,
2007; O’Lawrence, 2005). Second, online help seeking is likely to be easier to assess
behaviorally than offline help seeking, especially given the recent advances in online
methods for assessing help seeking (cf. Aleven et al., 2006). This increased ease of
assessment becomes a particularly important consideration for conducting assessment
in a cross-cultural context, where it may be difficult to directly observe the help seeking
that is occurring in classrooms. However, as we describe in more detail below, few
studies have investigated our ability to assess help seeking skills across cultures.
In this paper, we therefore consider how the cross-curricular skill of help seeking
differs across cultures. Behaviors for many cross-curricular skills may differ between
cultures; for instance, collaborative behaviors that are considered desirable in one
culture may be considered problematic or even offensive in other cultures (Kim &
Bonk, 2002; Vatrapu, 2008). It has been noted that most of the research into student
interactions within educational software – such as research on help seeking – takes
place in wealthy North American countries, European countries, and Australia/New
Zealand (Blanchard, 2012). As such, relatively little is known about how cultural
differences might affect key features of help seeking with learning technologies.

Int J Artif Intell Educ (2015) 25:229–248

231

There is a need for greater research on how computer-based assessments of specific
skills can account for cultural norms.
Objectives and Research Questions
The main objective of this study is to investigate whether the assessment of effective
help-seeking behavior can be consistent across cultures. To do this, we will use data
mining methods to develop models that predict effective learning from students’ helpseeking behaviors, using data sets from sites in three countries that are expected to vary
along a number of cultural dimensions: Costa Rica, the Philippines, and the U.S.A. We
will then compare between the models found for the three sites to see how effective
help-seeking behaviors are similar or different across cultures. While culture is notoriously difficult to define, we work with the understanding that it entails “a fuzzy set of
basic assumptions and values, orientations to life, beliefs, policies, procedures and
behavioral conventions that are shared by a group of people, and that influence (but do
not determine) each member’s behavior” (Spencer-Oatey, 2008). In this paper, we link
it operationally with the three countries being studied, under the viewpoint that the
cultural differences between these countries are broadly greater than the cultural
differences within them, and that many – though not all – of the differences between
these three countries can be understood in terms of culture.
This study aims to answer the following research questions:
1. Which help-seeking behaviors are effective in online learning within sites located
in Costa Rica, the Philippines, and the United States of America?
2. How well does a model of effective help-seeking behavior generalize across three
cultures?
By understanding the cross-cultural variations in this essential cross-curricular skill,
we can understand whether and how this skill should be assessed differently in different
cultural contexts, and work towards achieving a less culturally biased picture of this
skill’s application in different societies.
Effective vs. Ineffective Help Seeking
Help seeking may be characterized as effective or ineffective. Effective help seeking
takes place when a learner knows when he needs help, what kind of help he needs,
whom to ask for help, and how to ask for the help that he needs. Effective help seeking
may avert possible failure, maintain engagement, and lead to long-term mastery and
autonomous learning (Newman, 2002). Effective help seeking is viewed as an important strategy that contributes to self-regulation and vice versa (Newman, 2002,
Puustinen, 1998, Ryan et al. 1997), enabling the student to continue learning. Selfregulated students control the frequency with which they ask for help, asking only at
appropriate times and avoiding dependence (Puustinen, 1998). Both the overuse of help
and the avoidance of help when it is needed (Ryan et al. 2001) may lead to less
effective learning.
Newman (2002) identifies four specific competencies and motivational resources for
effective and adaptive help seeking:

232

Int J Artif Intell Educ (2015) 25:229–248

(a) cognitive competencies (i.e., knowing when help is necessary, knowing that others
can help, knowing how to ask a question that yields precisely what is needed);
(b) social competencies (i.e., knowing who is the best person to approach for help,
knowing how to carry out a request for help in a socially appropriate way);
(c) personal motivational resources (i.e., personal goals, self-beliefs, and feelings
associated with tolerance for task difficulty; willingness to express to others a
need for help; and a sense of personal agency);
(d) contextual motivational resources (i.e., classroom factors such as goals, grading
systems, collaborative activities, student-teacher interaction, and teacher expectations for the child that facilitate help seeking).
Beyond simply whether a student seeks help when it is needed, it is important to
consider the degree to which the student thinks about the help received (cf. Bielaczyc
et al. 1995), an act termed self-explanation. Self-explanation behaviors have been found
to be associated with positive learning outcomes across different types of learning
contexts and with learning from different types of help (Baker, R.S.J.d et al. 2011; Chi
et al. 1989; Shih et al. 2008; VanLehn et al. 1992).
By contrast, some students abuse help rather than thinking through the help and
trying to learn from it. Help abuse refers to the use of help only to advance in the
curriculum or obtain the solution to a particular problem, rather than thinking through
the subject matter (Aleven et al., 2006); this behavior has also been termed executive
help seeking (Nelson-Le Gall, 1985) and gaming the system (Baker et al. 2004a). This
behavior may prevent students from learning because it bypasses the self-explanation
and self-regulation processes.
Help Seeking in the Traditional Classroom
In the past few decades, there has been considerable research within traditional
classrooms to understand the differences in students’ help-seeking behaviors. A variety
of methods have been employed, including questionnaires (Karabenick & Knapp,
1991; Ryan et al., 1997; Taplin et al. 2001), analysis of student help-seeking choices
during learning (Ames & Lau, 1982), and interviews (Taplin, et al., 2001). Across this
body of research, a considerable amount has been learned about who seeks help; for
instance, Ames & Lau (1982) find that student beliefs about the usefulness of help
sessions influence the choice to attend help sessions. Similarly, (Hofer et al. 1996)
(cited in Aleven et al., 2003) identified more positive opinions of help when it was
perceived as relevant to the task being performed. Karabenick & Knapp (1991) found
evidence that learners who seek more active control of their learning are more likely to
ask for help. However, conflicting reports have been made about whether generally
more successful or generally less successful students are likely to seek help (Ames &
Lau, 1982; Karabenick & Knapp, 1991; Taplin et al. 2001).
Help Seeking within Online Learning
In recent years, the findings derived from studies in traditional classrooms have been
augmented by research on student help seeking within online learning. Unlike research
based on questionnaires and interviews, online learning environments can provide rich

Int J Artif Intell Educ (2015) 25:229–248

233

traces of every action a student makes, allowing a researcher to study not only what a
student does, but the context of the action in terms of the student’s recent experiences,
and the impacts of the student’s decision over the next minutes. While analysis of
student help-seeking behavior occurred before the use of data from online learning
environments (cf. Ames & Lau, 1982), it was very time-consuming and difficult to
analyze behavior at a fine-grained scale over large numbers of students. By contrast,
online learning environments make it easy to collect fine-grained data from hundreds or
thousands of students, and data mining methods make analysis very scalable. In
addition, learning systems such as intelligent tutoring systems (ITS; Koedinger &
Corbett, 2006; Woolf, 2009) often provide students with help of different types and
modalities (Woolf, 2009) including hints, glossary support, scaffolds that break problems down, and directed messages about student misconceptions. Some ITS provide
different levels of help- from general guidance to bottom-out hints (Anderson et al.
1985; Wood & Wood, 1999). In addition, help is sometimes given automatically and
sometimes given upon request by the student, allowing a researcher to analyze a
student’s differential responses to these two situations. This diversity of types and
contexts of help available makes more sophisticated comparisons of different types of
help seeking very feasible.
There has been an increasing amount of research within this paradigm in recent
years. In one of the earliest studies on help seeking within online learning, Wood and
Wood (1999) found that students with more prior knowledge tended to work faster,
make fewer errors, and ask for less help, and that students who requested less help
actually achieved better learning outcomes. A contrasting pattern was found by (Aleven
et al. 2004), who found a positive relationship between help seeking and learning.
(Baker et al. 2011) similarly found that the failure to seek help was correlated with a
lower degree of student preparation for future learning.
However, the way the student uses the help they seek matters. For example, students
who pause to think through the implications of a hint have higher learning gains (Shih
et al., 2008), and time spent on problems where help was sought is positively related to
learning gains (Arroyo and Woolf, 2005). In contrast, intentional misuse of help
features to obtain answers without thinking, sometimes by clicking through hints
rapidly, is associated with poorer learning (Aleven et al., 2004; Baker et al. 2004b).
It also matters which type of help a student uses; students who most frequently use lowlevel help (which is more explicit) tend to show lower learning (Mathews et al. 2008)
than students who focus on help related to concepts.
Help Seeking Across Cultures
One limitation of much of the research in help seeking in online learning environments
is that it has still largely taken place in fairly similar and homogenous populations.
Indeed, most of the data in the studies listed above was drawn exclusively from one
region of the United States (Wood & Wood were using data from Britain; Mathews,
Mitrovic, & Thompson were using data from New Zealand). It is unclear to what
degree these relationships between help seeking and learning are consistent across
different populations and cultures. It is logical to assume that the parameters of
Newman’s help seeking competencies may be affected by the culture in which they
are expressed – for example, the most appropriate source of help to approach in one

234

Int J Artif Intell Educ (2015) 25:229–248

culture may be the expert (e.g., the teacher), while in another, the expert should not be
bothered until other sources are exhausted.
While there has only been a limited degree of research comparing help-seeking
behaviors across cultures, online or offline, some noteworthy examples exist that would
support this claim. For instance, a study comparing help-seeking orientations of Latinos
and non-Latinos in a public urban high school in the United States, found that nonLatinos and English-speaking Latinos exhibited the same desire for academic support,
while Spanish-dominant Latinos exhibited a significantly lower desire for academic
support (Stanton-Salazar et al. 2001). The researchers conjectured that this difference
might be the result of the influence of American culture on the English-proficient
Latinos. Another comparison of learners in the United States and Oman found that
Americans reported less self-regulation of help seeking than Omani students (AlHarthi, 2008). In a recent study by Ogan et al. (2012) within the context of classrooms
using educational software, researchers found that students in Mexico, Costa Rica, and
Brazil using an intelligent tutor tended to work far more collaboratively than their
American counterparts. Software that was intended for individual use became the
object of group activity, and the primary source of help in these cases was not the
educational software but other classmates.
It is still not known, however, whether a) help-seeking skills can be modeled in a
generalizable way, or b) despite differences in the cultural appropriateness of helpseeking behaviors, a standard set of help seeking approaches lead to greater learning.

Method
As previously discussed, the main objective of this study is to investigate whether
effective help-seeking behavior is similar or different across cultures, towards understanding how assessment of this cross-curricular competency might be similar or
different across cultures. In this section, we describe how we created machinelearned models for predicting learning through help-seeking behavior from datasets
from sites in three countries: Costa Rica, Philippines, and U.S.A. The data sets
consisted of log files recording student interactions with a Cognitive Tutor for teaching
generation and interpretation of scatterplots. The data from the three sites has also been
used in previous studies, but the help-seeking behaviors of the students have not
previously been analyzed.
Data Source
This study had two main sources of data: log files from student interaction with a
Cognitive Tutor for Scatterplot generation and interpretation (Baker et al. 2006), and
pre-test and post-test scores that assessed student knowledge before and after interaction with the system. The data was collected as part of a classroom-based study in
which participating mathematics teachers were first provided training with the system
and then asked to conduct each session as if they were including the technology as part
of their typical practice. In each case, class was held in the computer lab during the
regularly scheduled math period. Researchers were available for technical support and
to ensure that the study procedure was kept as consistent as possible across the settings.

Int J Artif Intell Educ (2015) 25:229–248

235

Additionally, at least two researchers were present in the computer lab in each session,
taking field notes as they positioned themselves around the lab in order to observe
computer screens. Field notes captured on- and off-task behavior including collaboration, teachers’ instructional procedures, student impasses, and affective reactions.
In all three sites, the Scatterplot Tutor was used for a total of 80 minutes, after brief
instruction on the domain concepts and a pre-test. In the tutor, students created
scatterplots of data for small data sets, and then interpreted the data sets; the process
of scatterplot creation, from selecting appropriate variables, to choosing axis values, to
plotting points was reified (e.g. each cognitive step was made visible), and computergenerated hints were available at each step. In addition, bug messages were given when
student actions indicated a known misconception, such as the belief that all graphs
should have one categorical variable and one numerical variable. Within this Cognitive
Tutor, an animated agent was incorporated named “Scooter the Tutor”, developed using
graphics from the Microsoft Office Assistant (Microsoft Corporation 1997) but modifying those graphics to enable a wider range of emotions (see Fig. 1). Scooter was designed
to reduce the incentive to game the system, through expressions of negative emotion, and
to help students learn the material that they were avoiding by gaming, by providing
supplementary exercises after gaming episodes. Scooter was found to reduce gaming
behavior in the U.S.A. and improve gaming students’ learning (Baker et al. 2006), but
was not found to be effective at reducing gaming in the Philippines (Rodrigo et al. 2012).
A post-test was given in class after the final tutor session. The pretest and posttest
consisted of two isomorphic tests counterbalanced across students. In each test, the
student had to generate a single scatterplot using provided data and was assessed on
choosing appropriate variables, creating appropriate scales, labeling axes, and correctly
placing points on the plot (Baker et al. 2006).
Efforts were made to choose comparable schools in the three countries. There is no
easy matching of schools between three such different countries, as the overall structure
of how cities and metro regions are organized and populated differs greatly between
countries. Great efforts were instead made to ensure that the three populations represented comparable demographics within their respective countries. The participating
students in all three sites were from the local ethnic majority and were drawn from a

Fig. 1 Scooter the Tutor – looking happy when the student does not game (top-left), giving a supplementary
exercise to a gaming student (right), and angry when the student is believed to have been gaming heavily, or
attempted to game Scooter during a supplementary exercise (bottom-left)

236

Int J Artif Intell Educ (2015) 25:229–248

public school population that was neither unusually wealthy nor experiencing unusual
degrees of poverty relative to the local societal context. The populations in all three
sites had an approximately equal number of male and female students, who were from
the same year in school yet ranged in age from 12 to 14 years. Given the inherent
difficulty in such international comparisons, Table 1 is provided to summarize several
important points of comparison that can contextualize the present results for future
discussion. In Costa Rica, 85 students participated, in the United States, 139 students,
and in the Philippines, 127 students.
Feature Selection
From log files of student actions within the tutoring software, seventeen aspects of
student behavior were distilled. While inspired by extensive prior theory regarding
effective and ineffective help-seeking behavior, specific features were directly drawn
from recent work to study student preparation for future learning in a similar intelligent
tutoring system (Baker et al. 2011).
All features were normalized as a percentage of total actions taken by the student in
the interface. The features were as follows, in Table 2:
The features were chosen in order to encompass a wide array of possible helpseeking behaviors for students interacting with the Scatterplot tutor, and cognitive
tutors more generally. Feature NHP (help avoidance) has been identified in literature
as ineffective help-seeking behavior (Ryan et al. 2001), while its converse, Feature HP
(help use on poorly known skills) is identified as both common and desirable. Features
HPQ and HPL examine whether students pause after seeking help on an unknown skill.
Feature HPQ may indicate hint abuse/executive help seeking/gaming the system.
Feature NHW (not using help when it is not needed) is thought to be effective and
desirable help-seeking behavior because it is indicative of self-regulation (Newman,
2002), while the converse, Feature HW (using help when it is not needed) may be an
indicator of unnecessary student dependence on help.
Features STHW and LTHW examine student pauses when requesting help on a skill
the student already knows. Feature LTHW may suggest that the student is continuing to
Table 1 Comparison of the three populations on math skills, rank on the 2012 Program for International
Student Assessment math test, computer skills and use, city size, and 2008 gross domestic product per capita
in the city of our field site
U.S. site

Philippines site

Costa Rica site

# Participants

139

127

85

Age of participants

12–14

12–14

12–14

Math skills

Learning algebra,
basic geometry

Learning algebra,
linear equations

Learning trigonometry,
mastered basic math

Computer lab

1 computer lab, used 2 computer labs, used in
1 computer lab, used
in writing classes
computer literacy classes
across classes

City population

17,000

2.76 million

10,000

City GDP per capita

$41,500 U.S.D

$2,760 U.S.D

$6,590 U.S.D

Does not participate

407

2012 PISA Score (Avg: 494) 481

Int J Artif Intell Educ (2015) 25:229–248

237

Table 2 List of 17 help-seeking features extracted from log data, along with whether they indicate positive or
negative help-seeking behaviors
Code

Feature name

Feature description

Predicted
relationship to
help seeking

NHP

Help avoidance

Does not request help on poorly known skills
(Aleven et al., 2006)

Negative

HP

Help use on poorly known
skills

Requests help on poorly known skills
(cf. Aleven et al., 2006)

Positive

HPQ

Help Abuse

Requests help on poorly known skill but answers
quickly afterwards (cf. Aleven et al., 2006,
Baker et al. 2004b); subset of feature HP

Negative

HPL

Self-explanation after help
use on poorly known skill

Requests help on a poorly known skill and makes
a long pause; subset of feature HP

Positive

NHW

Help non-use on
well-known skills

Does not request help on an already known skill
(cf. Aleven et al., 2006)

Positive

HW

Help use on well-known
skills

Requests help even when skill is already known

Negative

STHW Short pause after help use on Requests help even when skill is already known,
well-known skill
but takes only a short time before the next
action; subset of feature HW.

Negative

LTHW Long pause after help use on Requests help even when a skill is already known, Positive
well-known skill
but takes a long pause; subset of feature HW.
LTB

Long pauses after bug
message

Takes a long pause after receiving a bug message
(Baker et al. 2011)

Positive

STB

Short pauses after bug
message

Does not take much time before the next action
when receiving a bug message
(Baker et al. 2011)

Negative

LTAH

Long pauses after requesting
hints

Takes a long pause after receiving a hint message
(Baker et al. 2011)

Positive

STAH

Short pauses after requesting
hints

Does not take much time before the next action
when receiving a hint message (Baker
et al. 2011)

Negative

LTHR

Long pauses after requesting
hint and getting current
action right

Takes a long pause after receiving a hint message
and gives a correct response for the problem
step (Shih et al. 2008)

Positive

STHR

Short pauses after requesting
hint and getting current
action right

Does not take much time before the next action
when receiving a hint message, but gives a
correct response to the problem step

Negative

STBH

Short pause before
requesting hints

Spends little time on the action prior to asking
for help

Negative

LTBH

Long pause before
requesting hints

Spends a longer interval on the action prior to
asking for help (Aleven et al., 2006, Wood
& Wood, 1999)

Positive

HF

Help use on first encounter
of skill

Requests a hint on the first time student
encounters a skill

Ambiguous

self-explain, even after being able to perform effectively; this could potentially imply
that the student does not know the skill as well as the system believes, or that the

238

Int J Artif Intell Educ (2015) 25:229–248

student is continuing to consider some aspect of the skill. It could also mean that the
student did not realize the skill applied in this situation. Feature STHW instead may
indicate “gaming the system” on already-known material, perhaps because it is timeconsuming (Baker et al. 2004a).
Features LTB and STB involve bug messages (messages explaining why a student
made an error), which can be considered a form of help automatically provided by the
tutor; these features investigate whether students pause to read and self-explain these
messages. It is important to note that the evidence of self-explanation in these features
is indirect, but prior work has found a relationship between these pauses and learning
(Baker, R.S.J.d et al. 2011). Features LTAH, STAH, LTHR, and STHR examine
whether students pause to self-explain after asking for help (in the case of features
LTHR and STHR, after requesting help and entering the correct answer). Again, the
evidence for self-explanation is indirect, but prior work has found a relationship
between these pauses and learning (cf. Shih et al., 2008).
Features STBH and LTBH examine how long students attempt a problem prior to
requesting help from the system. Taking longer before requesting help may be indicative
of the student trying to understand the problems carefully before asking for help. Finally,
feature HF, hint requests the first time the student encounters a new skill, may either
indicate help abuse or a general desire to understand a problem better before attempting it.
Given this list of features, we can infer which help-seeking behaviors are effective
and ineffective in each context, by attempting to predict student learning, operationalized as the student’s post-test score minus their pre-test score, showing how much
learning students gained. We determined the proportion of student actions that belong
to each feature category, and then create combined models to predict effective help
seeking for each site. Our process for creating models of effective help seeking for each
culture involved several steps: feature engineering (discussed immediately above),
feature selection, feature optimization, model creation, and model evaluation.
Feature Optimization
The first step to using the data features discussed above is to select optimal feature
values, e.g. the value for each feature parameter that best predicts learning (post-test
minus pre-test). For example, feature 1 refers to poorly known skills, but it is necessary
to operationalize “poorly known” in terms of a percentage probability that the student
knows the skill. All features except for feature 17 incorporate parameters, either time or
knowledge (or both).
Knowledge is operationalized within these features using Bayesian Knowledge
Tracing (Corbett & Anderson, 1995), the most widely used knowledge assessment
algorithm within intelligent tutoring systems. Bayesian Knowledge Tracing uses a
Bayesian update process to repeatedly re-estimate recent and current student knowledge based on student correctness and general probabilities of learning, guessing
(obtaining correct answers by luck), and slipping (obtaining incorrect answers despite
knowing the skill), computed across all students in each country. Bayesian Knowledge
Tracing produces an estimate of the probability that a student knows a skill at a given
point in the learning sequence.
For feature optimization, we used the method in (Baker et al. 2011). For each
dataset, we performed a brute-force grid search on all features to find the optimal

Int J Artif Intell Educ (2015) 25:229–248

239

threshold for each feature. The features were distilled from the logs depending on the
thresholds in the grid. For thresholds based on estimates of student knowledge, the grid
size for the search was 0.05 (e.g. increments of 5 % probability). For thresholds
requiring a time interval, the grid size was 0.5 seconds. For each feature and threshold,
we performed a single-parameter linear regression with leave-one-out-cross-validation
(LOOCV; Efron & Gong, 1983), where a model is repeatedly built on all students but
one, and tested on the held-out student; leave-one-out-cross-validation is mathematically equivalent to the Akaike Information Criterion (AIC; Akaike, 1974). Information
criteria and cross-validation approaches are increasingly seen as alternatives to the
classical statistical significance paradigm, as they assess how well the model will
function on new data drawn from the same distribution as the original distribution
(e.g. new students from the same population), rather than just the probability that the
results seen would have arisen if only chance events were occurring (cf. Raftery, 1995).
The cross-validated correlation (computing the correlation between the model’s predicted learning and the student’s actual learning, within new students) is used as the
model goodness criterion. The threshold that showed the highest positive crossvalidated correlation for each feature became the threshold that was used for that
feature during model creation. As a control against over-fitting, features with optimal
thresholds that had negative cross-validated correlation (e.g. the relationship
flips in direction between students) were not considered during the creation
of the full model, as negative cross-validated correlation means the model fails
to generalize to new data.
Model Creation and Evaluation
For each site’s data set, we then created a model using forward selection (Ramsey &
Schafer, 1993), finding the model that best predicts each student’s degree of learning
(post-test minus pre-test). The feature whose single-parameter linear regression model
had the highest cross-validated correlation was added to the model. Then, the feature
that most increased the cross-validated correlation was repeatedly added to the model,
one by one, until no additional feature improved the cross-validated correlation.
A fourth model was created by combining the three individual data sets, representing
a “multinational” model of effective help seeking, in order to test whether it is possible
to have one effective help-seeking model trained from different sets of cultures that
generalizes effectively to individual cultures.
As with the single-feature models used during feature optimization, model goodness
was assessed using cross-validation, to derive an assessment of likely model performance on new students drawn from each population. The four models were then
applied to all data sets, with non-cross-validated correlations used as the measure of
goodness.

Results and Analysis
In this section, we present the final models generated through feature selection for each
site. Each site’s model is then applied to each of the four data sets, to study the
relationship between effective help-seeking behaviors in the different cultures.

240

Int J Artif Intell Educ (2015) 25:229–248

Learning Gains
Pre and post test scores for each site are shown in Fig. 2. A repeated-measures ANOVA
with test time as a within-subjects factor and country as a between subjects factor
shows that students in all three sites had significant learning gains from pre-test to posttest (F (1,348)=236.81); p<0.001), and that there is a significant country by test time
interaction (F (2, 348)=23.51; p<0.001). A one-way ANOVA shows that there are
significant differences between groups at pre-test (F (2,348)=65.06; p<0.001). Tukey
post-hoc comparisons of the three groups indicate that the U.S. site (M=0.634, SD=
0.391) had higher prior knowledge than both other groups (p<0.001), while the Costa
Rica site (M=0.331, SD=0.297) had higher prior knowledge than the Philippines site
(M = 0.181, SD = 0.271), (p < 0.001). Hence, some of the greater gains seen in
Philippines and Costa Rica are due to students in those countries catching up to
students in the U.S.A.
Forward Selection Results
At a surface level, there were few similarities between the features in the final model for
each country, as shown in Table 3. Only two features appeared in the final model of
more than one data set: help avoidance and help abuse. Interestingly, these two features
– among the 17 features used in the data set – were drawn from some of the earliest
work in modeling help seeking within the log files from educational software (cf.
Aleven et al., 2004). Help abuse (HPQ) is negatively associated with learning in both
the Philippines and United States data sets (in the case of the United States data set, this
is a direct replication of Aleven et al., 2004). By contrast, help avoidance (NHP) has a
negative feature coefficient for Costa Rica and a positive coefficient for the combined
data set. This is not just a case of statistical suppression or lack of attention to
collinearity in a model with too many parameters – the relationships go in the same
direction in single-feature linear regression models. This suggests that help avoidance is
associated with positive outcomes in the U.S. and Philippines data sets, but leads to
negative outcomes in the Costa Rica data set.
Within the Costa Rica data set, several features were predictive of learning: help
avoidance (NHP), long pauses after requesting hints (LTAH), long pauses after
0.9
0.8
0.7
0.6
0.5

USA

0.4

Costa Rica

0.3

Philippines

0.2
0.1
0
Pretest

Fig. 2 Pre and posttest scores for each site

Posest

Int J Artif Intell Educ (2015) 25:229–248

241

Table 3 Models of how help-seeking behaviors influence learning in each site. Refer to Table 2 for feature
definitions. Thresholds for each parameter are written in parentheses: PK indicates probability that student
knows the skill, T indicates time in seconds
Site

Learning =

Cross-validated r

Costa Rica

- 0.132

* NHP

(PK <0.15)

+ 7.385

* LTAH

(T >47.5)

- 9.096

* LTHR

(T >41.5)

- 21.847

* HPL

(PK <0.25, T >45.5)

0.462

+ 53.01
Philippines

- 0.763

* HPQ

(PK <0.4, T <1)

+ 0.021

* NHW

(PK >0.95)

- 6.680

* HPQ

(PK <0.25, T <19.5)

- 1.021

* HP

(PK <0.25)

- 2.870

* LTB

(T >57)

+ 5.605

* LTBH

(T >37.5)

+ 0.046

* NHP

(PK <0.05)

- 0.482

* SPB

(T <3.5)

- 0.446

* HPL

(PK <0.35, time >0.5)

+ 8.147

* LTHW

(PK >0.95, T >58.5)

0.126

+ 32.423
U.S.A.

0.350

+ 12.086
Combined

0.216

+ 35.067

requesting hint and getting correct action right (LTHR), and self-explanation after help
use on poorly known skill (HPL). The overall cross-validated correlation of the Costa
Rica model to the Costa Rica data was a relatively high 0.462.
Within the Philippines data set, only two features were predictive of learning: help
non-use on well-known skills (NHW), and help abuse (HPQ). No features overlapped
directly or conceptually with the Costa Rica model. The overall cross-validated correlation of the Philippines model to the Philippines data was a fairly low 0.126.
Within the U.S.A. data set, several features were predictive of learning: help use on
poorly known skills (HP), long pauses after receiving bug messages (LTB), help abuse
(HPQ), and long pauses before requesting hints (LTBH). It is worth noting that help
abuse was found in both the U.S.A. and Philippines models, with help abuse being
associated with poorer learning in both data sets. The overall cross-validated correlation
of the U.S.A. model to the U.S.A. model was 0.350.
The multi-national model fit on all 3 data sets performed fairly poorly, achieving a
cross-validated correlation of 0.216. We re-visit this model below, in studying its
performance on each of the national populations.
Cross-Cultural Evaluation of Models
When each site’s model was applied to each of the other data sets, all of the models
performed less well (see Table 4), except for the U.S.A. model used on the data from

242

Int J Artif Intell Educ (2015) 25:229–248

Table 4 Model Evaluation (non-cross-validated correlation between predicted learning from help-seeking
behaviors, and actual learning). Rows are the models, columns are the data sets applied to
Site

Costa Rica

Philippines

U.S.A.

Combined

Costa Rica

0.534

0.051

0.151

0.099

Philippines

0.004

0.203

0.146

0.119

U.S.A.

−0.085

0.228

0.476

0.081

Combined

−0.073

0.091

0.238

0.286

the Philippines. However, only two models returned negative correlations to student
learning in the new data sets (indicating that the relationships captured point in the
same direction in both contexts). Note that these correlations are not cross-validated, as
we are evaluating the previously constructed models against the full data sets drawn
from each site.
In ten out of the 12 cases where a model is applied to the full data set from a site, the
correlation between the model’s predicted learning and the actual learning was positive,
suggesting successful transfer of these models. A sign test can be applied in this case,
and indicates two-tailed p=0.04; hence, in general the models transfer more than would
be expected solely due to chance.
Accounting for Pretest
Given the differences in pretest scores between groups that were observed earlier, we
investigated whether the variance we see in the models is related to prior knowledge
rather than culture. One way to answer this question is to predict posttest scores, using
pretest score as an additional factor, rather than predicting pre-post gains.
When we ran the same models with these new parameters, pretest scores passed the
brute-force grid search for all data sets/sites. However, pretest was not selected by
forward selection as a predictor of posttest scores in any of the four data sets, and
student behaviors did not disappear from the models when pretest scores were included
as possible predictors. It is also worth noting that the two sites with the most significant
differences in pretest score were the most similar in predictive help-seeking features. As
such, student behaviors seem to be better than pretest scores at predicting posttest
scores, suggesting that the differences seen between cultures in the help-seeking
behaviors associated with learning were not mainly due to pretest differences.

Discussion
We observed distinct differences in the performance of the models across countries.
The extremely poor performance of the U.S. model on the Costa Rica data set implied
that effective help-seeking behaviors are distinctly different between the U.S. and Costa
Rican sites, corresponding to previous findings of differences in how students use
educational software in the U.S. and Costa Rica (Ogan et al., 2012, Walker et al., 2011).
Overall, Ogan et al. (2012) and Walker et al. (2011) found that the Costa Rican students

Int J Artif Intell Educ (2015) 25:229–248

243

were more likely to work collaboratively over extended periods of time, even
abandoning their own computers to work on a problem together before returning to
enter responses, than the American students. The collaborative behaviors seen in Costa
Rican students in previous research may therefore explain the difference in helpseeking behavior seen here, as a more collaborative environment may make other
students the main source of help while using educational software. As such, the types of
help sought from the software and the situations within which it is sought may differ
considerably between the two sites.
By contrast, the U.S. model performs even better on the Philippines data than the
model built on Philippines data does. This successful transfer suggests that many of the
same help-seeking behaviors are associated with effective learning in the two sites.
(The Philippines model also performs about as well in the U.S. as it does in the
Philippines). Baker et al. (2004b) previously showed that American classrooms using
the same system spend only 4 % of their time talking to other students or the teacher
while working; informally, observations of the students in our Philippines site suggested a similar proportion of collaborative time, making the system the primary source
of help. Interestingly, another automated detector has been shown to generalize between students in the U.S. and Philippines; San Pedro et al. (2011) found that a model
of carelessness built on data from the U.S. functioned effectively within data from the
Philippines, and vice-versa. In contrast, Rodrigo et al. (2013) found disengaged
behavior to be different between U.S. and the Philippines (in brief, the U.S. students
went off-task much more frequently, while students in the Philippines gamed the
system more). Together, these findings suggest that even when the appropriateness
and efficacy of some learning practices are consistent across cultures, they may not all
be.
Even if the help-seeking behaviors are in some part distinct across these sites, we
might hypothesize that a combined model that uses all of the data could still be useful
for developing personalized learning systems. We found that the model built for the
combined multi-national data set performed particularly well on the U.S. data set,
moderately on the Philippines data set, and not very well on the Costa Rica data set. If
the U.S. and the Philippines are relatively similar, it makes sense that they would
dominate prediction in a combined data set. In fact, no model except for the model
trained on Costa Rica data did well on the Costa Rica data set. This suggests that
effective help-seeking behavior in Costa Rica is quite different from the other two
countries, making combined models difficult. We again note that past research has
indicated that Costa Rican students collaborate in more extended fashions when using
educational software (Walker et al., 2011; Ogan et al., 2012). Interestingly, several
features in the Costa Rican model show that requesting help and pausing are associated
with positive learning, a difference from other models. It may be that these long pauses
actually represent students seeking help from the software, and discussing the help with
other students. Understanding this difference better will be a valuable area of future
work.
It is also worth noting that the Costa Rica and U.S. models performed relatively
better than the Philippines and combined data sets. It would be possible to conjecture
that the poorer performance of the combined data set is caused by the mixed influences
of the three different cultures. The diverse mix of help-seeking behaviors makes it more
difficult to correctly predict learning for a given data set. In contrast, it is possible that

244

Int J Artif Intell Educ (2015) 25:229–248

Costa Rica and U.S. students behavior is more homogenous, making their learning
performance more predictable according to observable help-seeking behavior. The
Philippines cross-validated r was the lowest among the four cultures. Analyzing the
model, it has the lowest number of features surviving the forward selection process.
The implications of this could be 1) Student help-seeking behavior in the Philippines
may simply be not as correlated to learning compared to other cultures, 2) Help-seeking
behavior in the Philippines may not be as homogenous as in Costa Rica and U.S., and
distinct help-seeking strategies may be adopted by students which lead to different
patterns of behavior being effective for different sub-groups of students, or 3) The
initial feature set was generated based on research that focuses on data from the U.S.,
and there exist other indicators in the Philippines data of successful or unsuccessful
help seeking that have not been captured in these features.
Interpretation within a Cultural Framework
It is possible to speculate on interpretations of these behavioral findings that stem from
differing underlying cultural values. In order to do so, an explanatory framework for
culture should be proposed. As an example, Hofstede’s dimensions is one of the most
frequently cited cultural frameworks given the breadth and extent of his investigations
across nations (e.g., Hofstede et al., 2010). One of these dimensions that is particularly
salient from this framework for explaining educational results is labeled Masculinity, or
“the belief that the society will be driven by competition, achievement and success,
with success being defined by the ‘winner’ or ‘best-in-the-field’”. Respondents to
Hofstede’s cultural dimensions survey in the Philippines and the United States indicated that they placed a relatively strong value on a Masculine society, while Costa Rican
respondents were quite low on this dimension. Perhaps students in the Philippines and
United States viewed work on the ITS as a competition in which getting further ahead
was preferable to helping their classmates succeed, while students in Costa Rica may
have instead viewed the work as an opportunity to “care for one another” rather than
“stand out among the crowd” (a Masculine expression of values).
An alternative interpretation that uses the same framework might be related to the
dimension of Individualism, given the observations that the students in the United
States and the Philippines for the most part worked individually rather than relying on
peers as part of the support. Individualism refers to “the degree of interdependence a
society maintains among its members”, with a very individualist society placing greater
value on autonomy than on group membership. Indeed, Hofstede’s survey demonstrated that on average, Costa Rican respondents were far less individualistic than American
respondents, whose scores were one of the most individualistic countries surveyed in
the world. However, respondents in the Philippines also scored quite similarly to Costa
Rica on this dimension, while their help-seeking behaviors were quite distinct, so
values related to Individualism alone could not completely explain our results.
Instead, a dimension that our observations suggested as relevant is that of Power
Distance, or “the extent to which the less powerful members of institutions and
organizations within a country expect and accept that power is distributed unequally.”
The way this dimension is expressed in the classroom often relates to the relative power
of the teacher to dictate student behaviors and goals among other classroom features.
Hofstede’s Power Distance scores in the Philippines were much higher than those of

Int J Artif Intell Educ (2015) 25:229–248

245

Costa Rica or the United States, and our observations indicated that students may have
remained silently working at the direction of the teachers, to whom great respect is
traditionally accorded. It may be that, due to interactions between underlying values
relating to Power Distance and Individualism, students from our Costa Rican site were
more likely to choose to informally collaborate despite being originally told that this
was individual work, while students from the Philippines site were more likely to heed
the directions of the teacher and work silently and alone.
While Hofstede’s framework can provide various post-hoc interpretations of the
results, we should emphasize that there are many other factors that could be contributing to the different behaviors between sites, and we did not test the relationship
between Hofstede’s dimensions and student behaviors directly. Empirical research
across a larger number of sites would be necessary to determine whether these
dimensions (or others, or the use of a different cultural framework entirely) would best
explain the results.
A final potential interpretation of the results is that these differences stem from some
alternative source of variation such as socio-economic status or religion; or that they
represent a specific culture e.g. bounded within the particular classroom, school, or city.
A feature of our working definition of culture is that it is not limited by national or
geographic boundaries, but rather may comprise groups with shared behaviors and
values at a variety of levels of affiliation. Although in this study we choose to compare
sites within three countries in part to maximize the likelihood that distinct behaviors
would be present, we believe that the findings are equally relevant if the underlying
factor is not national membership. Our intent was not to determine features of effective
help seeking for all Costa Rican students (if such a thing were even possible), but rather
to elucidate the variety of help-seeking behaviors than may be effective for learning,
and address the possibility that modeling and assessing such behaviors may not be
universal. We expect that future work will help to uncover generalizable causes for such
variation through the use of larger samples and additional methods such as questionnaires and quantitative observational protocols.

Conclusion
In this study, we investigated whether a model of effective help seeking can generalize
across cultures, within the context of adaptive educational software. To answer this
question, we created a set of models of help seeking that try to predict learning from
seventeen commonly-reported help-seeking behaviors within educational software.
The model creation process consisted of several steps. First, log data was obtained
from studies of students using the same tutor on scatterplots in three countries: Costa
Rica, Philippines, and U.S.A. Pre- and post-test data were also collected from the
students in order to measure learning. Seventeen help-seeking features (e.g. aspects of
student help-seeking behavior) were then engineered from the log files. The seventeen
features were drawn from features used in past research on help-seeking behavior
within educational software. After optimizing the parameters used in these features,
models were developed that could predict learning in each country from a combination
of features, using linear regression. Four models of effective help seeking were created,
one for each of the sites in the three countries: Costa Rica, Philippines, and U.S.A., and

246

Int J Artif Intell Educ (2015) 25:229–248

one multi-national model created from the combined data set of the three countries. The
resulting models were analyzed to understand the differences in effective help seeking
between sites. The results suggested that effective help seeking looks substantially
different in the Costa Rican site than in the other two countries, suggesting that while
help seeking may be a cross-curricular skill, it can differ considerably between cultures.
The findings in this study indicate that there is a need for more cross-cultural
comparisons of students’ interactions with intelligent tutors, and the relationships
between different student behaviors and learning. It will be particularly important to
understand how cross-curricular skills such as help seeking differ between countries,
inasmuch as there is an increased interest both in transferring educational software
between countries (cf. Walker et al., 2011; Ogan et al., 2012; Nicaud et al. 2006), and in
measuring cross-curricular skills in international comparison examinations (Beller,
2011). Simply using the same normative model in different countries may induce bias
in favor of the cultural values and practices of the countries where the test developers
are based, entrenching an already-present bias in education research in favor of
phenomena present in developed, wealthy countries (cf. Blanchard, 2012). As such,
it will be essential to replicate this type of research in additional countries; we see this
step as a vital prerequisite to the wide deployment of new educational technologies and
pedagogies (and related assessments). Without conducting research of this type, we
may deploy educational approaches that are inappropriate and ineffective in the new
contexts where they are being used.
Acknowledgments We thank the many students, teachers, and school administrators across the world who
made this study possible.

References
Arroyo, I., and Woolf, B. (2005). Inferring Learning and Attitudes from a Bayesian Network of Log File Data.
In Proceedings of the Twelfth International Conference on Artificial Intelligence in Education, pp. 33–40.
Akaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic
Control, 19(6), 716–723.
Aleven, V., McLaren, B. M., Roll, O., & Koedinger, K. (2004). Toward tutoring help-seeking: Applying
cognitive modelling to meta-cognitive Skills. In J. C. Lester, R. M. Vicari, & F. Paraguacu (Eds.),
Intelligent Tutoring Systems, Lecture Notes in Computer Science (Vol. 3220, pp. 227–239). Berlin
Heidelberg: Springer.
Aleven, V., McLaren, B. M., Roll, I., & Koedinger, K. R. (2006). Toward meta-cognitive tutoring: A model of
help seeking with a Cognitive Tutor. International Journal of Artificial Intelligence in Education, 16,
101–128.
Aleven, V., Stahl, E., Schworm, S., Fischer, F., & Wallace, R. M. (2003). Help seeking and help design in
interactive learning environments. Review of Educational Research, 73(2), 277–320.
Al-Harthi, A.S. (2008). Learner self-regulation in distance education: A cross-cultural study. 24th Annual
Conference on Distance Teaching & Learning. Available from: http://www.uwex.edu/disted/conference/
Resource_library/proceedings/08_13597.pdf
Ames, R., & Lau, S. (1982). An attributional analysis of student help-seeking in academic settings. Journal of
Educational Psychology, 74(3), 414–423.
Anderson, J. R., Boyle, F. C., & Reiser, B. J. (1985). Intelligent tutoring systems. Science, 228, 456–462.
Baker, R. S., Corbett, A. T., & Koedinger, K. R. (2004a). Detecting student misuse of intelligent tutoring
systems. In J. C. Lester et al. (Eds.), Intelligent Tutoring Systems,Lecture Notes in Computer Science (Vol.
3220, pp. 531–540). Berlin Heidelberg: Springer.

Int J Artif Intell Educ (2015) 25:229–248

247

Baker, R. S. J. D., Corbett, A. T., Koedinger, K. R., Evenson, E., Roll, I., Wagner, A. Z., Naim, M., Raspat, J.,
Baker, D. J., & Beck, J. (2006). Adapting to when students game an intelligent tutoring system. In M.
Ikeda, K. Ashley, & T. W. Chan (Eds.), Intelligent Tutoring Systems, Lecture Notes in Computer Science
(Vol. 4053/2006, pp. 392–401). Berlin Heidelberg: Springer.
Baker, R. S., Corbett, A. T., Koedinger, K. R., & Wagner, A. Z. (2004b). Off-task behavior in the cognitive
tutor classroom: when students “game the system”. In Proceedings of the SIGCHI conference on Human
factors in computing systems (CHI 04) (pp. 383–390). New York: ACM.
Baker, R.S.J.d., Gowda, S.M., & Corbett, A.T. (2011). Automatically detecting a student’s preparation for
future learning: Help use is key. In Proceedings of the 4th International Conference on Educational Data
Mining, 179–188.
Beller, M. (2011). Technologies in large-scale assessments: New directions, challenges, and opportunities. The
Role of International Large-Scale Assessments: Perspectives from Technology, Economy, and
Educational Research, 25–45.
Bielaczyc, K., Pirolli, P. L., & Brown, A. L. (1995). Training in self-explanation and self-regulation strategies:
Investigating the effects of knowledge acquisition activities on Problem solving. Cognition and
Instruction, 13(2), 221–252.
Blanchard, E. G. (2012). On the WEIRD nature of ITS/AIED conferences: A 10 year longitudinal study
analyzing potential cultural biases, In Proceedings of the 11th International Conference on Intelligent
Tutoring Systems (ITS2012) (pp. 280–285). Greece: Chania.
Chi, M. T. H., Bassok, M., Lewis, M., Reimann, P., & Glaser, R. (1989). Self-explanation: How students study
and use examples in learning to solve problems. Cognitive Science, 13(2), 145–182.
Corbett, A. T., & Anderson, J. R. (1995). Knowledge tracing: Modeling the acquisition of procedural
knowledge. User Modeling and User-Adapted Interaction, 4, 253–278.
Csapó, B. (1999). Improving thinking through the content of teaching. In J. H. M. Hamers, J. E. H. Van Luit,
& B. Csapo (Eds.), Teaching and learning thinking skills (pp. 11–36). Lisse: Swets & Zeitlinger.
Efron, B., & Gong, G. (1983). A leisurely look at the bootstrap, the jackknife, and cross-validation. The
American Statistician, 37(1), 36–48.
Greiff, S. (2012). Assessment and theory in complex problem solving- A continuing contradiction? Australian
Journal of Educational and Developmental Psychology, 2, 49–56.
Hofer, M., Niegemann, H. M., Eckert, A., & Rinn, U. (1996). Pädagogische Hilfen für interaktive
selbstgesteuerte Lernprozesse und Konstruktion eines neuen Verfahrens zur Wissensdiagnose.
[Instructional help for interactive self-directed learning processes and construction of a new procedure
for knowledge diagnosis]. Zeitschrift für Berufs-und Wirtschaftspädagogik: ZBW, 53–67.
Hofstede, G.H., Hofstede, G.J., & Minkov, M. (2010). Cultures and Organizations: Software of the Mind,
Third Edition. McGraw Hill.
Judd, C. H. (1908). The relation of special training and general intelligence. Educational Review, 36, 28–42.
Karabenick, S., & Knapp, J. (1991). Relationship of academic help seeking to the use of learning strategies
and other instrumental achievement behavior in college students. Journal of Educational Psychology,
83(2), 221–230.
Kim, K., & Bonk, C.J. (2002). Cross-cultural comparisons of online collaboration. Journal of ComputerMediated Communication, 8, 0.
Koedinger, K.R., & Corbett, A.T. (2006). Cognitive tutors: Technology bringing learning science to the
classroom. The Cambridge handbook of the learning sciences, 61–78.
Martin, R. (2008) New possibilities and challenges for assessment through the use of technology; In F.
Scheuermann, A.G. Pereira (Eds) Towards a Research Agenda on Computer-Based Assessment, Ch.1.
Luxembourg: Office for Official Publications of the European Communities.
Mathews, M., Mitrovic, T., & Thomson, D. (2008). Analysing high-level help-seeking behavior in ITSs. In
Proceedings of the International Conference on Adaptive Hypermedia and Adaptive Web-based Systems,
312–315.
Microsoft Corporation. (1997). Microsoft Office 97. Seattle, WA: Microsoft Corporation.
Nadler, A., & Fisher, J. D. (1986). The role of threat to self-esteem and perceived control in recipient reaction to help:
Theory development and empirical validation. Advances in Experimental Social Psychology, 19, 81–120.
Nafukho, F. M. (2007). The place of e-learning in Africa’s institutions of higher learning. Higher Education
Policy, 20, 19–43.
Nelson-Le Gall, S. N. (1985). Help-seeking behaviour in learning. Review of Research in Education, 12, 55–90.
Newman, R. S. (2002). How self-regulated learners cope with academic difficulty: The role of adaptive help
seeking. Theory Into Practice, 41(2), 132–138.
Nicaud, J. F., Bitta, M., Chaachoua, H., Inamdar, P., & Maffei, L. (2006). Experiments with Aplusix in four
countries. International Journal for Technology in Mathematics Education, 13(2), 79–88.

248

Int J Artif Intell Educ (2015) 25:229–248

Ogan, A., Walker, E., Baker, R., Rebolledo, G., & Jimenez-Castro, M. (2012). Collaboration in cognitive tutor
use in Latin America: field study and design recommendations. In Proceedings of the 2012 ACM Annual
Conference on Human Factors in Computing Systems (CHI 12) (pp. 1381–1390). New York: ACM.
O’Lawrence, H. (2005). ‘A review of distance learning influences on adult learning: Advantages and
disadvantages. Proceedings of the 2005 Information Science and IT Education Joint Conference, 125–
135.
Polson, M. C., & Richardson, J. J. (1988). Foundations of Intelligent Tutoring Systems. New Jersey: Lawrence
Erlbaum Associates.
Puustinen, M. (1998). Help-seeking behavior in a problem-solving situation: Development of self-regulation.
European Journal of Psychology of Education, 13(2), 271–282.
Raftery, A. E. (1995). Bayesian model selection in social research. Sociological Methodology, 25, 111–164.
Ramsey, F., & Schafer, D. W. (1993). The statistical sleuth: an intermediate course in statistical methods.
Independence, KY: Brooks Cole Publishing.
Rodrigo, M. M. T., Baker, R. S. J. d., & Rossi, L. (2013). Student off-task behavior in computer-based learning
in the Philippines: comparison to prior research in the USA. Teachers College Record, 115(10), 1–27.
Rodrigo, M. M. T., Baker, R. S. J. D., Agapito, J., Nabos, J., Repalam, M. C., Reyes, S. S., & San Pedro, M. C.
Z. (2012). The effects of an interactive software agent on student affective dynamics while using an
intelligent tutoring system. IEEE Transactions on Affective Computing, 3(2), 224–236.
Ryan, A. M., Hicks, L., & Midgley, C. (1997). Social goals, academic goals, and avoiding seeking help in the
classroom. Journal of Early Adolescence, 17(2), 152–171.
Ryan, A. M., Pintrich, P. R., & Midgley, C. (2001). Avoiding seeking help in the classroom: Who and why?
Educational Psychology Review, 13(2), 93–114.
San Pedro, M. O. C., Baker, R. S. J. D., & Rodrigo, M. M. (2011). Detecting carelessness through contextual
estimation of slip probabilities among students using an intelligent tutor for mathematics. In G. Biswas
et al. (Eds.), Artificial Intelligence in Education, Lecture Notes in Computer Science (Vol. 6738/2011, pp.
304–311). Berlin Heidelberg: Springer.
Shih, B., Koedinger, K., & Scheines, R. (2008). A response time model for bottom-out hints as worked
examples. In Proceedings of the 1st International Conference on Educational Data Mining, 117–126.
Spencer-Oatey, H. (2008). Culturally Speaking. Culture, Communication and Politeness Theory (2nd ed.).
London: Continuum.
Stanton-Salazar, R. D., Chavez, L. F., & Tai, R. H. (2001). The help-seeking orientations of Latino and nonLatino urban high school students: a critical-sociological investigation. Social Psychology of Education,
5(1), 49–82.
Taplin, M., Yum, J. C. K., Jegede, O., Fan, R. Y. K., & Chan, M. S. C. (2001). Help-seeking strategies used by
high-achieving and low-achieving distance education students. American Journal of Distance Education,
16(1), 56–69.
VanLehn, K., Jones, M. R., & Chi, M. T. H. (1992). A model of the self-explanation effect. The Journal of the
Learning Sciences, 2(1), 1–59.
Vatrapu, R. K. (2008). Cultural considerations in computer supported collaborative learning. Research and
Practice in Technology Enhanced Learning, 3, 159–201.
Walker, E., Ogan, A., Baker, R.S.J.d., de Carvalho, A., Laurentino, T., Rebolledo-Mendez, G., & Castro, M.J.
(2011). Observations of collaboration in cognitive tutor use in Latin America. In Proceedings of the 15th
International Conference on Artificial Intelligence in Education, 575–577.
Wood, H., & Wood, D. (1999). Help-seeking, learning, and contingent tutoring. Computers & Education, 33,
153–169.
Woolf, B. P. (2009). Building Interactive Intelligent Tutors. Burlington: Elsevier, Inc.

Cognitive Tutoring of Collaboration:
Developmental and Empirical Steps
Towards Realization
Bruce M. McLaren*, Lars Bollen+, Erin Walker*, Andreas Harrer+, Jonathan Sewall*
*Human-Computer Interaction Institute
Carnegie Mellon University
Pittsburgh, PA USA
bmclaren@cs.cmu.edu,
erinwalk@andrew.cmu.edu, sewall@cs.cmu.edu

+Collide Research Group
University Duisburg-Essen
Duisburg, Germany
harrer@collide.info,
bollen@collide.info

Abstract. In this paper, we describe developmental and empirical steps we have taken toward
providing Cognitive Tutoring to students within a collaborative software environment. We have
taken two important steps toward realizing this goal. First, we have integrated a collaborative
software tool, Cool Modes, with software designed to develop Cognitive Tutors (the Cognitive
Tutor Authoring Tool). Our initial integration does not provide tutoring per se but rather acts as a
means to capture data that provides the beginnings of a tutor for collaboration. Second, we have
performed an initial study in which dyads of students used our software to collaborate in solving a
classification / composition problem. This study uncovered five dimensions of analysis that our
approach must use to help us better understand student collaborative behavior and lead to the
eventual development of a Cognitive Tutor for collaboration. We discuss our plans to incorporate
such analysis into our approach and to run further studies.
Keywords: Collaborative learning, Cognitive Tutors, jigsaw design, spatial effects on problem
solving

INTRODUCTION
Intelligent Tutoring Systems (ITS) have long been used to provide one-on-one (machine-to-student) instruction
(Wenger 1987). We are interested, however, in using a software tutor to instruct multiple students collaborating
on a single problem. There have been steps toward providing tutoring in a collaborative environment (e.g.,
Goodman et al. 2003; Suthers 2003; Lesgold et al. 1992), but many difficult challenges remain. For instance,
the space of possible actions among collaborating users is huge for even the simplest of problems, thus making
the analysis of learner behavior much more difficult for collaborative tasks than for the single-student case.
As a step toward addressing the complexities of a collaborative environment, we have created a tutordevelopment methodology that leverages actual problem-solving data not only to guide ITS design, as has been
done in past work (e.g., Koedinger and Terao, 2002), but also to contribute directly to tutor implementation
(McLaren et al. 2004a; McLaren et al. 2004b). Using this approach, called bootstrapping novice data (BND),
groups of collaborating students attempt to solve problems with a computer-based tool. While they work, the
system records their actions in a graphical representation that combines all of the groups' solutions into a single
graph that can be used as the basis for building a tutor and analyzing the collaboration. Our initial BND
implementation is realized through the integration of a collaborative modeling tool, Cool Modes (Collaborative
Open Learning and MODEling System) (Pinkwart 2003), and a tutor authoring environment, the Cognitive
Tutor Authoring Tools (CTAT) (Koedinger et al. 2004).
Our ultimate research aim is to develop better support for collaborative learning through cognitive tutoring.
In this paper, we describe two steps we have taken toward realizing this ambition: (1) an initial implementation
of the BND methodology and (2) a study using the BND approach, including the results and the implications
for further development of the methodology. The study we have performed reveals some interesting aspects of
the way dyads solved a particular collaborative problem. More importantly, it has pointed us in the direction of
improving our implementation of the BND methodology and realizing cognitive tutoring in a collaborative
environment.
In our initial implementation of the BND methodology, depicted in Figure 1, Cool Modes (shown on the
left) provides the graphical user interface, including a shared workspace that all collaborators in a session can
view and update, a palette with objects that users can drag onto the workspace, a chat area, and a private
workspace. Cool Modes sends messages about students' actions (e.g., "create an IS-A link") to CTAT’s

418

Behavior Recorder (also referred to as the "BR" and shown on the right of Figure 1), which stores the actions in
a behavior graph. Edges in the graph represent student actions and paths through the graph represent attempted
solutions to the problem. The current approach keeps track of the number of times actions are taken by the
various collaborating groups and presents these "traversal counts" on the edges of the behavior graph, e.g., 3
student dyads took the action from the "start state" Classification-Composition to State1. Using CTAT, a tutor
author can subsequently transform the generated behavior graph into a Pseudo Tutor, or problem-specific tutor
(Koedinger et al. 2004), by adding or deleting edges, labeling correct or buggy behavior, and adding hints to the
edges. To use the finished graph as a tutor, the BR is switched to "model-tracing" mode in which student
actions are compared to the graph, instead of recorded, and error messages and hints are delivered to the student.
While our ultimate aim is use this approach to provide cognitive tutoring within Cool Modes, as well as other
collaborative environments, our initial focus is somewhat more modest: We want to analyze data that was
collected using the BND methodology to help us better understand both collaborative behavior and how we can
enhance the BND methodology to provide more useful analysis of that behavior. The preliminary study that we
have performed is an example of such an analysis.

Figure 1: The student's view of the integrated Cool Modes (left) and the Behavior Recorder (right) environment.
This shared Cool Modes workspace is from a vehicle classification / composition task that was completed by a
dyad of collaborating students. The behavior graph at right shows the amalgamated solutions of different
collaborating groups of students.

DESCRIPTION OF THE STUDY
The research question in the preliminary study was whether, in a graphical problem-solving domain, an
organized arrangement of objects leads to quicker and better collaborative solutions than a disorganized
arrangement. We also wondered whether student rearrangement of the objects facilitates quicker and better results
and how this rearrangement might be conducted in a collaborative scenario. To explore these questions and test
how the BND methodology might be a useful analysis tool, we assigned 16 students to 8 dyads and asked each
dyad to solve an object-modeling problem using the Cool Modes / BR integrated system (one subject was a
class assistant). The objects in the given problem were vehicles (e.g., "Car") and parts of vehicles (e.g., "Tire").
The student dyads were asked to relate the objects using classification and composition links. The students were
volunteers from a "Modeling Techniques in Computer Science" course at the University of Duisburg, Germany.
Seven of the students (pairs 6, 7, 8 and one in pair 5) had had previous experience with Cool Modes. All
students received approximately 5 minutes of instruction on how to use the system before the experiment. The
student pairs worked at separate workstations, back-to-back in the same room. They shared a single Cool Modes
workspace.
To specify IS-A (i.e., classification) and PART-OF (i.e., composition) links between objects in the
workspace, the students used the Unified Modeling Language, a graphical modeling technique. To stimulate
collaboration we used an unequal resources design akin to jigsaw experiments (Aronson et al. 1978). One
student was provided with IS-A links only and one with PART-OF links only, so that no student could solve
the problem alone. Students communicated by typing statements into a chat box.
The only other

419

communication permitted was the actual composition and repositioning steps taken by the students in the shared
workspace.
The 8 groups were randomly assigned to two experimental conditions. In Condition 1, pairs attempted to
solve a problem in which related objects were close to one another, providing an organized visual display of the
final network. For example, the two abstract classes “Vehicle” and “Vehicle Part” were located near the top of the
visual space, and most of the subclasses were located near their respective super classes. In Condition 2, pairs
solved a problem for which the objects were positioned in the workspace without any clear organizational
principle.

STUDY RESULTS AND ANALYSIS
All 8 dyads completed the task. Students’ solutions can be divided into three categories: good solutions (groups
5 and 8), incomplete solutions (groups 2, 6, and 7), and poor solutions (groups 1, 3, and 4). All three poor
solutions were in the disorganized condition, and two of the incomplete solutions were in the organized
condition. While one of the good solutions was in the organized condition, one was in the disorganized
condition. There were negligible differences in the time required to complete the task between the different
solution categories. The informal results suggested that a clearly organized problem state does not necessarily
lead to quicker and better solutions than a disorganized problem state, but does lead to different types of errors.
It is of course impossible to draw statistical conclusions about the relationship between the solutions and
starting conditions with such a small sample size, but we nevertheless have made some interesting informal
findings. We now describe those findings.
The conceptual distinctions between good, incomplete, and poor solutions were related to the errors
committed in solving the classification / composition problem. In the good solutions, errors reflected
misunderstandings about the meaning of classes. Otherwise, students correctly divided the objects into
subclasses and super classes and correctly placed the inheritance and composition edges. In the incomplete
solutions, students only connected one inheritance or composition link from each class. As a result, they had too
few links, and left out key relationships. In the poor solutions, students would often connect a class to multiple
ancestor nodes of the same lineage. For example, in Group 3, students connected "Car" to both "MotorVehicle"
and "Vehicle." The poor solution groups also typically created too many edges and were logically inconsistent
about the connection decisions they made.
Solutions can also be characterized by the way students tended to move nodes in the shared workspace. In
the good solutions, the students separated the two abstract classes, placing one at the top of the workspace and
the other at the bottom. IS-A and PART-OF links flowed in opposite directions and crossed only when
necessary. Objects were organized in rows that reflected their level of abstraction. In the incomplete solutions,
the two abstract classes were placed relatively close to one another, all links pointed in one direction, and there
were no crossed links. The objects tended to be clustered together and were organized into fewer rows than in
the good solutions, but the rationale behind the organization wasn't as clear. Finally, the poor solutions had the
abstract classes positioned without an obvious rationale. They had much longer edges pointing in all directions
and frequently intersecting with one another. As a result, the poor groups tended to use the entire shared
workspace.
We then analyzed the processes associated with the development of each solution. While working on the
problem, students could take three types of actions: chat actions, "talking" to a partner in a chat window, move
actions, repositioning an object in the shared workspace, and creation/deletion actions, creating or deleting
edges. Solution types showed differences in collaborative and task-oriented behavior. In terms of collaboration,
the students with the good solutions had different approaches. Group 8 worked completely collaboratively.
Members would take turns moving and creating objects and for a given group of objects, one member would
reposition objects while the other would create the edges. Conversely, the members of Group 5 worked
completely in parallel, coordinating their actions only to correct their partner's mistakes. In the incomplete and
poor groups, pair members shared the work. The poor groups were informal in their turn taking, while students
in the incomplete groups would alternate taking the initiative. With the exception of Group 5, groups decided on
their actions using the chat window and ensured that both members agreed on the actions being taken.
The three solution groups coordinated phases of chatting, moving, and creating/deleting differently. In the
good solutions, the approaches of two pairs were dissimilar. Group 8 collaborated by alternating chat phases,
move phases, and creation/deletion phases. Group 5 alternated between move phases and creation/deletion
phases, and primarily communicated visually. In both groups, adjacent phases referred to the same objects and
levels of abstractions, displaying a coherent problem-solving strategy. Both pairs were the only groups to have
more move actions than chat actions. The incomplete groups had fewer move actions, longer phases of chatting,
and fewer deletions. They adopted the inefficient strategy of discussing many future actions, creating a single
edge, and then repeating the discussion. On the other hand, they would consistently reorganize objects before
creating edges, which may have contributed to the tree-like organization of the classes. The poor groups engaged
in long phases of chatting or moving, but showed less coherence between the objects they were discussing,
objects they were creating, and objects they were moving. They deleted a lot of edges and tended to create
particular edges before repositioning them. This disorganized approach probably led to the conceptual and

420

visual disorganization of their final solutions. Another difference was in terms of the selection of objects: Both
the good and the incomplete solutions would focus on objects based on their level of abstraction; they would
manipulate a given superclass and all its subclasses, and then move on to another group of objects. On the other
hand, students in the poor solutions appeared to let the problem organization guide their actions. They would
draw edges based on classes that were close to one other in the shared workspace, rather than classes that were
semantically related. Differences in object selection probably related to differences in the consistency of the final
solutions.
These results indicate that we should focus on five elements when evaluating students' performance (at least
on this particular task): conceptual understanding, visual organization, task coherence, task coordination, and
task selection, (see Table 1). These elements were chosen because they represent different relevant aspects of
student action that appeared to inform groups' solutions. Students within each solution type (good, incomplete,
and poor) tended to make the same types of mistakes within each of these categories.
Conceptual understanding refers to a pair's ability to correctly place the inheritance and composition edges,
while visual organization refers to a pair's ability to visually arrange the classes and edges in an appropriate
manner. These two elements are linked; conceptual understanding of the problem was often reflected in the
visual organization, and therefore conceptual steps tended to parallel organizational steps. For example, students
in the incomplete solution groups appeared to believe that they could create only one inheritance or composition
link extending from each object, and their solutions thus tended to take on rigid tree structures.
Conceptual
Understanding
Good
Solutions

Translation
mistakes

Visual
Organization
Based on
abstractions

Incomplete Overly restricted
Based on a rigid,
Solutions definition of super tree-like structure
class / subclass
relationships
Poor
Solutions

Inconsistent
Disorganized
definition of super
class / subclass
relationships

Task Coherence
Adjacent phases
referred to the same
objects

Task Coordination

Task Selection

Balanced phases and Based on
work distribution
abstractions

Chat phases referred Strict turn-taking
Based on
to far more objects
and overly long chat abstractions
than subsequent
phases
phases
Little
correspondence
between selected
objects in adjacent
phases

More informal turn
taking and overly
long phases
(particularly of
deletion)

Based on
proximity

Table 1: Solution Types and Elements of Analysis
Task coherence, task coordination, and task selection reflect student strategies for collaborating on the
problem. Task coordination refers to skills in coordinating actions, without reference to the content of the
actions. It includes distributing the work among group members, and spending appropriate amounts of time in
each phase. The good groups exhibited successful task coordination, in part because they spent more time
moving objects than talking about them. Task coherence refers to the appropriateness of the content of student
actions. Students in the incomplete groups showed poor task coherence by chatting at length about many
different links and then creating a single link. Task selection refers to a student's ability to set subgoals for
solving the problem by drawing edges between classes in a sensible order. A breakdown in task selection leads
to disorganized and incoherent solutions, as seen in the poor group. These three skills should be assessed by
looking at the problem-solving process.

IMPLICATIONS FOR
CONCLUSIONS

IMPROVING

THE

BND

METHODOLOGY

AND

One of the key goals of this study was to determine how to enhance the BND methodology to provide more
helpful data and analysis. In theory, the BR should be able to facilitate analysis of all the skills from Table 1.
However, these five elements are not supported in the current BR, making it difficult to classify behavior and
provide tutoring support. In an attempt to produce convergent paths in the behavior graphs, we restricted input
to the BR to creation and deletion actions. Unfortunately, analyzing creation and deletion appears too limited to
be useful. Although the BR records sequences of actions at a single level of generality, the nature of this
problem indicates that student skills at different levels of abstraction need to be addressed, and the BR needs to
be able to create hierarchies of behavior graphs.
We intend to address this problem by modifying the BR to support recording at different levels of
abstraction. Single chat, move, or creation/deletion actions, made by a particular user and referring to a particular
object, are at the lowest level of abstraction. Separate behavior paths can be generated for the creation/deletion
and move actions, and used for analyzing the conceptual understanding and visual organization. The middle
level of abstraction involves the analysis of phases of action, or chains of the same type of action, and can deal

421

with task coherence and task coordination. The highest abstraction level addresses sequences of phases, or the
characteristics of the current phase in relation to previous and subsequent phases. Skills related to task selection
will be evaluated and supported at this level. An approach to classifying actions and action sequences in Cool
Modes has been described in Harrer and Bollen (2004), and can be used to process actions at different levels of
abstraction.
Given these changes, we believe the modified BR will be a much more effective tool for analyzing
collaboration and providing tutoring support in future experiments. Besides the preliminary study we have
already performed, we plan to perform two more experiments in the near term. In these studies, we will test how
students collaborate to solve a Petri Net problem. We wish to determine how well the five elements of analysis
we have uncovered generalize to other graphical collaboration tasks and to less structured problems. We then
intend to enhance the BND methodology to allow us to compare and classify student problem solving strategies.
The resulting annotated behavior graphs will provide the basis for cognitive tutor development within Cool
Modes.

ACKNOWLEDGMENTS
We thank Ken Koedinger who was responsible for bringing us all together and for some of the early ideas.
Partial support was provided by Deutsche Forschungsgemeinschaft (DFG) and Kaleidoscope “Network of
Excellence” (IST-507838).

REFERENCES
Aronson, E., Blaney, N., Stephin, C., Sikes, J., & Snapp, M (1978). The Jigsaw Classroom. Sage Publishing
Company. Beverly Hills, CA.
Bollen, L, Harrer, A., & Hoppe, U. (2004). An Integrated Approach for Analysis-Based Report Generation.
Proceedings of Advanced Learning Technologies (ICALT 2004), IEEE Press, Los Alamitos, CA, USA,
pp. 1094-1095.
Goodman, B., Hitzeman, J., Linton, F., & Ross, H. (2003) Towards Intelligent Agents for Collaborative
Learning: Recognizing the Role of Dialogue Participants. In Proceedings of the International Conference
on AI in Education.
Harrer, A. & Bollen, L. (2004) Klassifizierung und Analyse von Aktionen in Modellierungswerkzeugen zur
Lernerunterstützung. In Workshop-Proceedings Modellierung 2004 . Marburg, 2004.
Koedinger, K. R., Aleven, V., Heffernan, N., McLaren, B. M., & Hockenberry, M. (2004) Opening the Door to
Non-Programmers: Authoring Intelligent Tutor Behavior by Demonstration. In Proceedings of ITS,
Maceio, Brazil, 2004.
Koedinger, K. R. & Terao, A. (2002) A cognitive task analysis of using pictures to support pre-algebraic
th
reasoning. In C. D. Schunn & W. Gray (Eds.), Proceedings of the 24 Annual Conference of the
Cognitive Science Society, 542-547.
Lesgold, A., Katz, S., Greenberg, L., Hughes, E., & Eggan, G. (1992) Extensions of Intelligent Tutoring
Paradigms to Support Collaborative Learning. In S. Dijkstra, H. Krammer, & J. van Merrienboer (Eds.),
Instructional Models in Computer-Based Learning Environments. Berlin: Springer-Verlag, 291-311.
McLaren, B. M., Koedinger, K. R., Schneider, M., Harrer, A., & Bollen, L. (2004a) Bootstrapping Novice
Data: Semi-Automated Tutor Authoring Using Student Log Files; Proceedings of the Workshop on
Analyzing Student-Tutor Interaction Logs to Improve Educational Outcomes, ITS 2004
McLaren, B. M., Koedinger, K. R., Schneider, M., Harrer, A., & Bollen, L. (2004b) Toward Cognitive
Tutoring in a Collaborative, Web-Based Environment; Proceedings of the Workshop of AHCW 04,
Munich, Germany, July 2004.
Pinkwart, N. (2003) A Plug-In Architecture for Graph Based Collaborative Modeling Systems. In U. Hoppe, F.
th
Verdejo & J. Kay (eds.): Proceedings of the 11 Conference on Artificial Intelligence in Education, 535536.
Suthers, D. D. (2003). Representational Guidance for Collaborative Learning. In the Proceedings of Artificial
Intelligence in Education (AIED-03), IOS Press, Amsterdam.
Wenger, E. (1987) Artificial Intelligence and Tutoring Systems. Morgan Kaufmann Publishers, Inc.

422

User Model User-Adap Inter (2006) 16:175–209
DOI 10.1007/s11257-006-9007-4
O R I G I NA L PA P E R

Creating cognitive tutors for collaborative learning:
steps toward realization
Andreas Harrer · Bruce M. McLaren · Erin Walker ·
Lars Bollen · Jonathan Sewall

Received: 1 October 2005 / Accepted in revised form: 10 May 2006 /
Published online: 18 August 2006
© Springer Science+Business Media B.V. 2006

Abstract Our long-term research goal is to provide cognitive tutoring of collaboration within a collaborative software environment. This is a challenging goal, as intelligent tutors have traditionally focused on cognitive skills, rather than on the skills
necessary to collaborate successfully. In this paper, we describe progress we have
made toward this goal. Our first step was to devise a process known as bootstrapping
novice data (BND), in which student problem-solving actions are collected and used
to begin the development of a tutor. Next, we implemented BND by integrating a
collaborative software tool, Cool Modes, with software designed to develop cognitive
tutors (i.e., the cognitive tutor authoring tools). Our initial implementation of BND
provides a means to directly capture data as a foundation for a collaboration tutor
but does not yet fully support tutoring. Our next step was to perform two exploratory
studies in which dyads of students used our integrated BND software to collaborate
in solving modeling tasks. The data collected from these studies led us to identify five
dimensions of collaborative and problem-solving behavior that point to the need for
abstraction of student actions to better recognize, analyze, and provide feedback on
collaboration. We also interviewed a domain expert who provided evidence for the
advantage of bootstrapping over manual creation of a collaboration tutor. We discuss

A. Harrer (B) · L. Bollen
University Duisburg-Essen, Duisburg, Germany
e-mail: harrer@collide.info
L. Bollen
e-mail: bollen@collide.info
B. M. McLaren · E. Walker · J. Sewall
Carnegie Mellon University, Pittsburgh, PA, USA
e-mail: bmclaren@cs.cmu.edu
E. Walker
e-mail: erinwalk@andrew.cmu.edu
J. Sewall
e-mail: sewall@cs.cmu.edu

176

User Model User-Adap Inter (2006) 16:175–209

plans to use these analyses to inform and extend our tools so that we can eventually
reach our goal of tutoring collaboration.
Keywords Intelligent tutoring systems · Collaborative learning · Collaboration
modeling · Action-based analysis

1 Introduction
Intelligent tutoring systems (ITS) have long been used to provide one-on-one
(machine-to-student) instruction (e.g., Wenger, 1987; Polson and Richardson 1988).
Cognitive tutors, a particular type of intelligent tutor that supports “guided learning
by doing” (Anderson et al. 1995), have been shown to improve cognitive learning in
domains like algebra and geometry by approximately one standard deviation over
traditional classroom instruction (Koedinger et al. 1997). So far, however, cognitive
tutors have been used only for one-on-one instruction—a computer tutor assisting a
single student—and primarily to support cognitive learning. We, on the other hand,
seek to determine whether a cognitive tutoring approach, and, more specifically, the
type of tutoring approach taken by Anderson et al., can support and improve collaboration as well. In addition, other research questions we aspire to answer with our
work are:
•
•
•

To what extent can a cognitive tutor evaluate collaboration by evaluating problem-solving actions and ignoring specific dialogue between collaborators?
What types of collaborative problems are most amenable to a cognitive tutoring
approach?
Should feedback for collaborating students be immediate, as in traditional cognitive tutors, or is a delayed-feedback approach more appropriate for a collaborative
environment?

Collaboration is recognized as an important forum for learning, as discussed in a recent
book on how people learn (Bransford et al. 2000), and learning research has demonstrated its potential for improving students’ problem solving and learning (Johnson
and Johnson 1990; Slavin 1992). However, collaboration is a complex process, not as
constrained as individual learning. There have been steps toward providing tutoring in
a collaborative environment (e.g., Lesgold et al. 1992; Mühlenbrock and Hoppe 2001;
Constantino-González and Suthers 2002; Constantino-González et al. 2003; Soller
and Lesgold 2003; Goodman et al. 2005; Vizcaino 2005), but many difficult challenges
remain. Development effort, for one, presents a major barrier. For the simpler single-student case, estimates are in the range of 100–1,000 h of development time per
each hour of instruction (Murray 1999); (Anderson et al. 1995) estimated 200 h for
the Algebra Cognitive Tutor. In the collaborative environment, analysis of learner
behavior is much more difficult, in part because the space of possible actions and
conversations among collaborating users is huge for even the simplest of problems.
With the relatively small number of existing tutors for collaborative support, and the
conceptual problems still to answer, reliable estimates for development effort are not
readily available, but it can be assumed that the effort is at least equal to, and likely
much larger than, that of the single-user case. This makes it particularly attractive
to leverage live user data as the basis for creating and fine-tuning the tutors, as we
attempt to do in our work.

User Model User-Adap Inter (2006) 16:175–209

177

Our work leverages the principle of cost-effective example-tracing tutors,1 which
are developed by demonstration rather than by complex rule writing (Koedinger
et al. 2004). Furthermore, instead of having an expert create the tutor by demonstration, we use real data gained from collaborating groups solving problems as the basis
for a tutor. This automatic capturing of data will be complemented by manual and/or
automatic analysis of that data. The ultimate desired result is a “cognitive” model
representing specific successful and unsuccessful modes of collaboration.
To take a step toward addressing the questions raised above, we have developed an
approach called bootstrapping novice data (BND) in which groups of students attempt
to solve problems with a computer-based collaborative tool. While the students work,
the system records their actions in a network representation that combines all collaborating groups’ solutions into a single graph that can be used for analysis and,
eventually, as the basis for a tutor. We have begun experimentation with the BND
approach by integrating a collaborative work environment and a cognitive tutoring
tool (McLaren et al. 2004a, b; Harrer et al. 2005; McLaren et al. 2005). The collaborative modeling tool we used in our initial implementation is Cool Modes (Collaborative
Open Learning and MODEling System) (Pinkwart et al. 2002; Pinkwart 2003), while
the tutoring software is the Cognitive Tutor Authoring Tools (CTAT) (Aleven et al.
2006a, b). There are two stages to our plans—data collection/analysis and tutor creation—the first of which we are well on the way to achieving. We have captured and
analyzed data from live collaboration so that we can better understand how a cognitive
tutor might use that data to diagnose and support student actions in a collaborative
environment. We eventually want to directly use the data we collect as the basis for
the cognitive tutor model of collaboration.
In this paper, we review relevant past work, introduce the concept of bootstrapping novice data, describe how we have implemented the BND methodology, present
empirical work that explores a particular type of collaborative problem and tests
the BND approach, discuss an interview we conducted with a domain expert who
analyzed the trade-offs between bootstrapping and manual creation of a tutor model,
and, finally, present our ideas for extending BND both to improve analysis and to lead
to our ultimate goal of providing tutoring in a collaborative environment.

2 Related research
We are not the first to attempt to provide intelligent tutoring and/or coaching in an
online collaborative environment. On the other hand, this is certainly a nascent area
of research with many interesting research issues remaining open, such as the ones
stated above that are particular to cognitive tutoring. In this section, we discuss some
of the past, related work and compare and contrast this research to our own.
One of the seminal projects to suggest the combination of intelligent tutoring
and collaborative learning was the work of Lesgold et al. (1992). This research was
an extension of the well-known Sherlock project in which Air Force mechanics were
tutored (one-on-one, machine-to-human) in diagnosing failures in complex electronic

1 In Koedinger et al. (2004) these specialized tutors based on examples of actual problem solutions
were referred as “Pseudo Tutors.” However, because we believe this name does not aptly describe the
tutors, we have since renamed them “example-tracing tutors” (Aleven et al. 2006a, b).

178

User Model User-Adap Inter (2006) 16:175–209

systems (Lesgold et al. 1993). An important component of the Sherlock system was
its “reflective follow-up” module, which presented an abstracted replay of a solution
after a problem solving session and prompted the student to ask Sherlock questions
about that solution. Sandra Katz, one of the members of Lesgold’s team, saw this
phase of Sherlock as a particularly good opportunity for students to collaborate with
one another or with Sherlock. The idea behind this work was not so much to directly
tutor collaboration as it occurred, but rather to provide a framework to allow students
(or a student and Sherlock) to critique one another and explain problem solving. In
this sense, it is distinct from our research, as well as much of the research described
below, which is focused more on evaluating and supporting live, computer-mediated
collaboration. In addition, the collaborative features of the Sherlock system were
only designed as extensions to the individual tutoring system, but they were not
implemented or evaluated.
Some of the more recent efforts that have focused on live, computer-mediated
collaboration and have been implemented focus on both student actions and collaborative conversations, in contrast to our approach which focuses on student actions
while ignoring language content. The HabiPro system of Vizcaino (2005), for instance,
analyzes both problem solving and chat actions via a “simulated student” that coaches
students as they collaboratively learn how to write a computer program online. The
“live” student collaborators are not informed that HabiPro is a pedagogical agent;
the program provides hints and feedback in simple natural language statements. The
hypothesis here is that peer coaching may provide more welcome support than a
teacher or tutor. The program focuses on and intervenes to correct three particular
problems: off-topic conversations, passive students, and learning issues. To identify
off-topic conversation, the system matches the language of chat messages against keyword databases containing valid domain words and “playful” (i.e., off-topic) words.
An analysis of the frequency and density of contributions is used to identify passive
students, while mistake frequency and answer “closeness” are used to identify learning
problems. A study by Vizcaino involving 22 student dyads, one-half of which collaborated with the simulated student and one-half of which did not, demonstrated that
HabiPro is moderately successful in correcting the three types of problems. The HabiPro program is a step beyond our work in development in that it already provides live
collaborative tutoring, but it relies on pre-defined and fixed measures of correctness
to advise students, rather than using live student data to directly build the tutor model,
as we are attempting to do.
Another research project that employs the concept of a peer pedagogical agent
and evaluates collaborative dialog is that of Goodman et al. (2005). The pedagogical
agent in their web-based collaborative system follows a discussion between students
and provides feedback when it detects a problem, such as one student dominating
the discussion. Their primary (but not sole) focus is on dialog acts (Searle 1969)
and other characteristics of language communication, rather than on problem-solving
actions, as in our work. They found that neural networks worked better than Hidden
Markov Models (HMMs) in evaluating communication and predicting when intervention might be helpful. Soller and Lesgold (2003) also focus on collaborative conversation and the use of HMMs as an analysis tool. They investigated knowledge sharing
and knowledge construction between students as those students tried to solve objectmodeling problems. Their system identifies speech acts through the technique of “sentence openers” in which collaborators communicate with one another by selecting
from a small and finite set of possible ways to start a sentence (e.g., “Let me explain

User Model User-Adap Inter (2006) 16:175–209

179

it this way. . .”, “I agree because . . .”) (McManus and Aiken 1995). Both of these
projects, unlike the HabiPro work, share our goal of leveraging live student data for
the purpose of tutoring. Their neural networks and HMMs are trained on “good”
and “bad” collaboration for the purpose of identifying these characteristics in live
collaboration. Perhaps our work could benefit from experimenting with some of the
advanced analysis techniques they have employed, although we would apply these
techniques to student actions rather than speech acts, which is their primary focus.
The action-based collaboration analysis of Mühlenbrock and Hoppe (2001), which
focuses primarily on actions in a graphical workspace rather than on dialogue content between participants, is closer to our approach. They use rule-based recognition
of characteristic sequences of problem-related user actions in a shared workspace—
for instance, in a graphical game in which puzzle pieces must be moved. Since this
approach was developed primarily for collaborative face-to-face scenarios, where language communication between the students occurs outside of computer mediation,
it focuses exclusively on actions and not on the discourse/dialogue between collaborators. While our initial approach similarly places emphasis on actions, our project
differs in that we are interested in collaboration achieved completely in a computermediated way (i.e., dyads collaborating strictly over computers). At least initially,
the only dialogue analysis we attempt involves identifying the fact that students are
communicating via “chat actions.” Mühlenbrock and Hoppe’s work is focused more
on analysis techniques and less on tutoring or coaching of collaboration, as we are in
our work.
Like the Mühlenbrock and Hoppe work, as well as our own, COLER (ConstantinoGonzález and Suthers 2002) focuses exclusively on student problem-solving actions,
ignoring the content of language communication between collaborating students. The
task of COLER’s collaborating students is to build database Entity/Relationship
diagrams. A shared workspace contains the amalgamated solution of all the collaborators, and each student has his or her own individual solution workspace. Based
on socio-cognitive conflict theory, COLER operates by finding structural differences
between the students’ evolving individual and group solutions and identifying opportunities to suggest actions based on those differences. For instance, the coach in
COLER might notice that a student, George, has correctly defined a relationship in
his individual diagram that is missing from the shared diagram and suggest that he
volunteer this information to the group. The COLER also evaluates collaborative
features of student interaction, such as lack of participation by a particular student.
As with HabiPro, a key difference of COLER from our work is the fact that it is
not based on a model built from live student data. Rather, it operates by dynamically analyzing differences generated through comparison of the individual and group
solutions.
Our approach contrasts with all of this previous work in its task-independent
nature. The Cool Modes graphical modeling environment has many different plugins, supporting a variety of problem-solving tasks, and our BND approach is agnostic to the particular type of graphical modeling task undertaken. While supporting
collaboration within different tasks has not yet been achieved, we have demonstrated
(and published) bootstrapping in several graphical modelling tasks (McLaren et al.
2004a, b, 2005; Harrer et al. 2005). In principle, some of the prior work might be
applicable to different problems, but all of the prior systems discussed above were
demonstrated in a single task domain.

180

User Model User-Adap Inter (2006) 16:175–209

3 Building a tutor directly from student data: Bootstrapping Novice Data
In the bootstrapping novice data approach we have developed, groups of collaborating
users generate (possibly different) correct and faulty solutions to the same problem.
The logs of the different collaborating groups are directly translated into a single
representation of problem solving, in the form of a graph whose edges represent student actions. The graph starts initially empty. The BND approach not only generates
examples of actual correct and buggy paths taken by students, but also provides
another important piece of information: traversal frequencies of those paths across
all of the collaborating groups. The same actions taken by different collaborating
dyads are identified with the same edges in the graph: as these actions are recognized,
they contribute to incrementing the edge traversal frequencies. After multiple groups
have generated data, the graph contains the actions of all student groups and reveals
the frequency of common paths, both correct and incorrect. In the final step of the
BND process, the tutor author manually updates the problem-solving graph by marking buggy paths, adding hints and bug messages, and adding skills to graph edges. At
this stage, the model is ready to be used for tutoring.
Use of novice data in this manner can help avoid the so-called “expert blind spot”
problem, in which experienced problem-solvers and teachers fail to identify the common errors of novice students (Nathan et al. 2001). The edge traversal counts are good
indicators of which of the correct solution paths might be considered primary, which
secondary, and the counts along incorrect paths provide data to show which errors
occur frequently enough to merit specific buggy messages. The traversal counts can
also help authors identify slips and careless errors (e.g., accidental item selections):
an edge with a traversal count of 1, as compared to much higher counts on alternative
edges, may indicate that an accidental action was taken by a student. Such edges can
be deleted from the graph.
Instead of authors building tutors from scratch, relying on their own experience
or incorporating student data “by hand,” as in traditional ITS development, they can
semi-automatically leverage the empirical data of a wide range of students engaged in
actual problem-solving activity. A tutor author can then create a tutor directly from
the graph by labeling edges with hints and buggy messages. This approach contrasts
markedly with the usual ITS development method in which an expert author first
models “expert” problem solutions. In our approach, the student novices create the
initial solutions. The expert’s judgment as to which novice solutions are correct is, of
course, critical to creating a final version of the tutor. It may also be the case that an
expert will have to augment a model by demonstrating a correct solution (or solutions), if the student novices fail to generate any correct solution paths. But the critical
aspect of BND is how it directly captures and encodes incorrect and inefficient novice solutions, information that is invaluable in building a full ITS for a collaborative
system.
There are two key advantages to the BND approach. First, direct capture of student
data for use in tutor building is a novel and powerful idea. While student data has
been used to guide cognitive tutor design (Koedinger and Terao 2002) and tune
tutor parameters (Corbett et al. 2000), it has not previously been used directly as
input in creating a cognitive tutor. Recently educational data mining approaches have
been used to understand student–tutor interaction (Heiner et al. 2004) and individual
learning (Merceron and Yacef 2005). Our approach, as well as that of Goodman
et al. (2005) and Soller and Lesgold (2003) cited above, makes use of real data

User Model User-Adap Inter (2006) 16:175–209

181

from student collaboration to assist in building a model for tutoring. The potential
timesaving in data collection, data analysis, and system development provided with
a single integrated tool could be significant. Second, given the complexity of collaborative learning, a 2-D visualization, in the form of a graph, may allow for a
better understanding and analysis of collaborative behavior when compared with, for
instance, a non-visual, linear representation, such as production rules. The traversal
frequencies, discussed above, provide additional data to the graph visualization that
could further assist analysis.

4 A realization of BND: the integration of Cool Modes and the Behavior Recorder
An important underpinning of this work is the notion of component-based tutor
development (McArthur et al. 1996; Ritter and Koedinger 1996). Our approach is to
take an existing software application (what we term a “tool”) and integrate it, with
little or no modification, with a tutor or tutor agent. Using off-the-shelf or pre-existing
software as the basis for building tutoring systems could result in substantial timesavings, as compared to the traditional approach of building tutors “from scratch.” This is
particularly important in developing tutors for collaboration (Walker et al. 2006). As
pointed out above, the underlying interaction model in collaboration is much more
complex than in the single-student scenario.
4.1 Cool modes: the collaborative modelling tool
Cool Modes is a collaborative software tool designed to support “conversations” and
shared graphical modelling facilities between collaborative learners (Pinkwart et al.
2002; Pinkwart 2003). The tool is intended to facilitate collaborative problem solving and learning, but provides no tutoring capability. An example of a Cool Modes
problem space is shown in Fig. 1.
Cool Modes provides users with a variety of plug-in objects, such as Petri nets, a
turtle programming environment, a variety of text widgets, a “chat” area, etc., each of
which has its own semantics and underlying representation. All of these objects are
available in a toolbox from which students may drag and drop objects into workspaces
for use. Each Cool Modes user has her own private workspace in which objects can
be privately created and updated, while all users have access to a shared workspace,
which is visible to all collaborators and may be updated by any participant. Cool
Modes is extensible; new objects adhering to a well-defined API may be added to the
toolbox. Communication and translation between different object types is achieved
through reference frames, a set of entities and rules that facilitate semantic mapping
between objects (Pinkwart 2005). Some of these sets are visual languages (Hoppe et al.
2000), which have been designed as formal representations of specific domains. Some
are graphical argumentation and discussion languages, which have been designed for
collaboration and thus can be considered coordinating representations that help to
structure collaboration (Introne and Alterman 2006).
4.2 CTAT: the tutor authoring environment
The graph building and (eventual) tutoring component of the BND integration
is provided by CTAT, an authoring tool for intelligent tutors. Authoring systems

182

User Model User-Adap Inter (2006) 16:175–209

Fig. 1 An example Cool Modes problem space. Here students are collaborating on a nuclear decay
problem in a shared workspace

comprise an important area of ITS research (Murray et al. 2003). Among these tools,
CTAT fits into the “Domain Expert System” category described by Murray (1999).
It supports authors in building cognitive tutors, a form of “model–tracing” tutor
based on cognitive psychology theory (Anderson et al. 1995). As of the winter of
2005–2006, cognitive tutors have been deployed in over 2000 schools in the United
States.2
A cognitive tutor is composed of a problem representation and a set of production rules that model both desired and buggy behavior; it is general enough to tutor
students on a range of problems within a particular domain (e.g., geometry, algebra). Model tracing involves (a) matching actual student behavior during problem
solving with the desired behavior represented by the production rules and (b) identifying deviations from that behavior, either in the form of so-called “buggy rules,”
which model known misconceptions, or in the form of no-model conditions, where no
production rules match the student action. Cognitive tutors are difficult to develop,
typically requiring AI programming expertise.
A specialized type of cognitive tutor also supported within CTAT is an example-tracing tutor, a tutor that behaves much like a regular cognitive tutor, except
that it provides instruction for only a single problem instance and is much easier to
develop. Example-tracing tutors are developed using “programming by demonstration” (Lieberman 2001), an approach that allows authors with no programming skills
to build tutors. An illustration of an example-tracing tutor for fraction addition is
shown in Fig. 2.
2 It should be noted, however, that the Algebra and Geometry Cognitive Tutors in use at schools
across the United States were developed before CTAT was created, so they were not built using
CTAT.

User Model User-Adap Inter (2006) 16:175–209

183

Fig. 2 The behavior recorder (BR) records authors’ actions in any interface created with CTAT’s
specialized GUI widgets

In CTAT, an example-tracing tutor is developed as follows:
•

•

•

•

First, the author builds (or uses) a graphical user interface (GUI) with a set of
CTAT widgets, which are ordinary user interface components augmented for use
with a tutoring system. The GUI for a fraction addition tutor is shown on the right
side of Fig. 2.
Second, the author demonstrates correct, alternative correct, and incorrect actions.
A CTAT tool known as the Behavior Recorder (abbreviated BR) records all of
these actions and stores them in a structure known as a behavior graph, shown
on the left side of Fig. 2. Each edge of the graph represents an action taken by
the student on a particular widget of the GUI. For instance, out of the start state
(labeled “prob-1-fourth-1-fifth”) are two correct actions (“20, F21den” and “20,
F22den”), in which a proper common denominator is entered in either of the
converted fractions, and one incorrect path (“2, F13num”). The student’s action is
represented as a Selection (the GUI widget selected, such as the text area named
“F22den”), Action (the type of action taken, such as “Update Text”), and Input
(the value provided, such as “20”). Each node of the graph represents a state of
the interface after a path of edges from the root to that node has been traversed.
In the case above, since the Tutor Interface shows the denominator “20” in the
second converted fraction but no other student action, state8 is highlighted in the
BR (the boldfaced state label) as the current state.
Third, after the behavior graph has been created by problem demonstration, the
author can annotate the graph by labelling buggy edges (e.g., the incorrect path
represented by “2, F13num” in Fig. 2), hints, feedback messages, and skills associated with the edges.
Finally, the author can test the model by “executing” the example-tracing tutor,
acting like a student or observing actual student use. The whole process typically
iterates, as the author improves the model and fixes problems during testing.

184

User Model User-Adap Inter (2006) 16:175–209

Fig. 3 Bootstrapping novice data: initial integration of Cool Modes and the behavior recorder

4.3 The technical integration
As described above and depicted in Fig. 3, the fundamental idea behind BND is to
use a collaborative tool, in this case Cool Modes, to generate traces of actions created
by the participants in the collaborative groups and have those actions recorded in the
BR, providing the beginnings of a real tutor. Because the BR is a component with a
well-defined message interface (these messages are called “Dormin Messages”), the
initial integration Cool Modes/BR integration only required a translator that took
Cool Modes student actions and transformed them to Dormin messages. In this initial
implementation, we integrated the tool and the tutor in loosely coupled fashion by
translating Cool Modes log files, which are represented in XML format, using XSLT
(McLaren et al. 2004a,b).
Our initial implementation fed the XML-based log files of Cool Modes asynchronously into the BR and thus was not able to support live capture of user actions.
It also provided only one-way communication, which could support recording of
student actions but not tutoring. The only development effort required for this filebased interoperability was the definition of a semantic mapping of user events to
Dormin Messages and the resulting XSL-transformation script to achieve this mapping. Cool Modes did not have to be updated to support this solution, while the BR
had to be extended with the traversal frequency feature (discussed above) to make
the BND approach effective.
In our current, more advanced implementation (Harrer et al. 2005; McLaren et al.
2005), we have integrated Cool Modes and the BR in a real-time, synchronous fashion:
Both tools remain fully operational independently, but now they can exchange messages bi-directionally using Cool Modes’ MatchMaker communication server (Jansen
2003) and a “Tutor Adapter” (see Fig. 4). Now, a student action causes the Cool Modes
client to send an event to the MatchMaker server, which sends the event to the Tutor
Adapter, which in turn forwards the event to the BR. If an author were to create an
example-tracing tutor and switch the BR from recording to tutoring mode, then it

User Model User-Adap Inter (2006) 16:175–209

185

Fig. 4 UML collaboration diagram showing the message flow between Cool Modes and Behavior
Recorder

would respond to incoming events by sending appropriate buggy messages and hints
to the proper student or students.3
The implementation effort to achieve this bidirectional solution consisted of
several changes. While the Cool Modes application itself didn’t need to be extended,
an additional component, the Tutor Adapter, was developed to send MatchMaker
events from the collaborative application(s) to the BR. This component uses the
conceptual mapping already defined for the first version but sends messages through
a socket connection. The same communication is required from the BR; otherwise
no modifications were required. The rationale behind this distributed architecture is
the flexibility gained by running different components on different computers. This
architecture substantially reduces the cost of the computing requirements for running
the entire integrated system, for no single machine is relied upon to provide a lot of
processing power or memory. Since our intent is eventually to use AI and data mining
techniques to support the tutor model (discussed later), it may be desirable to have
a dedicated machine executing these computations, perhaps in parallel for multiple
groups of students.
While it is difficult to precisely estimate the time and cost savings due to implementing BND in a component-based fashion instead of developing either a tutor agent
for Cool Modes or a collaborative application for the BR, we can say the following:
the Cool Modes application on its own was a doctoral thesis project and several student programmers’ work, while CTAT has been under development by a team of 2–4
programmers for just over 3 years. Given this experience, for either alternative approach we estimate at least one-person year for the development effort and probably
much more. Similarly, the MatchMaker communication architecture required several
person months of development time. For a collaborative application with less flexibility, restricted to a specific domain, we would still assume development time of at least
3–4 person months. On the other hand, extending our existing tools as described took
less than a person week of effort on each side, demonstrating the enormous advantage
of component-based development.
Another notable aspect of our implemented integration is that a change in learning
domain only requires a different Cool Modes plug-in. All of our pilot tests and the two
3 It should be noted that we have not yet defined the incoming events in Cool Modes. While technically this is not a difficult task, it is dependent on the more difficult tasks, which we are currently
undertaking, of collaborative analysis and defining the feedback we would want to provide to students.

186

User Model User-Adap Inter (2006) 16:175–209

studies, described in following sections, were realized by using different Cool Modes
plug-ins with no programming effort. The flexibility of this BND implementation
can be seen in the variety of scenarios we tested: a priority planning task (i.e., the
NASA game), fraction addition, and open graphical modeling problems, including
UML diagrams and Entity/Relationship modeling.
Figure 5 depicts the user’s view of one of our experimental scenarios, UML objectoriented modeling (see “Study 1”). Cool Modes, shown on the left, provides the user
interface for the student, which includes a shared workspace that all collaborating
students in a session can view and update. Cool Modes sends messages describing
students’ actions (e.g., “student A created classification link L”) to the BR, shown
on the right, which stores the actions in a behavior graph. Each edge in the graph
represents a single student action, and paths through the graph represent series of
student actions. The behavior graph of Fig. 5 represents multiple collaborations; this
can be seen by the traversal frequencies shown on the edges of the graph.

5 Studies to analyze collaboration using the integrated system
We ran two exploratory studies to assess the information required by the BR, to investigate the types of problems that might be amenable to our approach, and to analyze
two interesting collaborative tasks. Each study involved a graphical modeling problem
and tested the effect of the initial organization of objects on the collaborative problem-solving effort. In Study 1, in which we had students perform a UML modeling
task, we identified five elements of collaboration that were relevant in solving the
problem and demonstrated the need for adding multiple levels of abstraction to the
BR in order to represent those elements. In Study 2, we verified that the five elements
of collaboration were generalizable to a Petri Net modeling problem and explored
how the five elements could be analyzed and tutored using the BR. We believe that

Fig. 5 The user’s view of the integrated Cool Modes (left) and the behavior recorder (right) environment. This shared Cool Modes workspace is from a vehicle classification/composition task

User Model User-Adap Inter (2006) 16:175–209

187

our results can be generalized to many other graphical modeling problems that can
be tackled in Cool Modes, but the proof will be in further studies. We chose these
study tasks to show the potential usage of our approach across different domains
and because the same tasks were the focus of earlier studies (i.e., object-oriented
modeling Soller and Lesgold, 2003; Petri Net modeling Gasevic and Devedzic 2003).
We conducted the studies using students from a course on “Modeling Techniques in
Computer Science” at the University of Duisburg-Essen. All our subjects were majors
in applied computer science or computer engineering.
5.1 Study 1
5.1.1 Study 1 method
We performed study 1 to help identify properties of the collaboration that might be
important and to define the properties in a way that could be easily expressed in
the behavior graph. To facilitate this process, we varied certain properties of the
initial problem in an attempt to evoke a rich variety of collaborative behaviors.
We wished to discover whether, in a graphical problem-solving domain, an organized
visual arrangement of objects leads to quicker and better collaborative solutions than
a disorganized arrangement. We also were interested in whether the rearrangement
of objects facilitates quicker and better results and how this rearrangement might be
conducted in a collaborative scenario.
To explore these questions and test how the BND methodology might be a useful
analysis tool, we assigned 16 students to eight dyads and asked each dyad to solve an
object-modeling problem using the Cool Modes/BR integrated system (one subject
was a class assistant). All the students had knowledge of object-oriented modeling at
the time of the study and participated in this optional exercise as a means to get additional preparation and experience for an exam. Seven of the students (pairs 6–8 and
one in pair 5) had had previous experience with Cool Modes. All students received
approximately 5 min of instruction on how to use the system (especially using the
graphical modelling functionality and the chat) prior to the study. The student pairs
worked at separate workstations, back-to-back in the same room. They shared a single
Cool Modes workspace.
The students were asked to specify IS-A (i.e., classification) and PART-OF (i.e.,
composition) links between pre-defined, given objects in the workspace using the
Unified Modeling Language, a graphical modelling technique. The objects in the
given problem were vehicles (e.g., “Car”) and parts of vehicles (e.g., “Tire”). A paper
handout with detailed instructions for the task accompanied this setup of the collaborative environment. To stimulate collaboration we used an unequal resources design
akin to jigsaw experiments (Aronson et al. 1978). One student was provided with
IS-A links only and one with PART-OF links only, so that no student could solve the
problem alone, and the students could take three types of actions:
•
•
•

Chat actions: “talking” to a partner in a chat window.
Move actions: repositioning an object in the shared workspace.
Creation/deletion actions: creating or deleting links between objects.

The students had approximately 25 min to solve the problem.
The eight groups were randomly assigned to two experimental conditions. In
the ordered or organized condition (Condition 1), the initial presentation showed

188

User Model User-Adap Inter (2006) 16:175–209

related objects visually close to one another, to provide a well-organized display of the
desired final network. For example, the two abstract classes “Vehicle” and “Vehicle
Part” were located near the top of the visual space, and most of the subclasses were
located near their respective super classes. In the scrambled or disorganized condition
(Condition 2), objects were positioned randomly. Groups 1–5 were in the scrambled
condition; groups 6–8 were in the ordered condition. We hypothesized that students
in the ordered condition would use the visual organization as a scaffold in problem
solving and would be less likely to rearrange objects in the visual space.
5.1.2 Study 1 results and analysis
To analyze the results, we looked both at students’ solutions and at their process
for arriving at those solutions. We looked only at elements that could be automatically gathered by the BR, and we tried to examine how actions could be evaluated at
different levels of abstraction and correlated with the quality of the students’ solutions.
Characterizing students’ solutions All eight dyads completed the task. Students’
solutions can be divided into three categories: good solutions (groups 5 and 8), incomplete solutions (groups 2, 6, and 7), and poor solutions (groups 1, 3, and 4). All three
poor solutions were in the disorganized condition, and two of the incomplete solutions
were in the organized condition. While one of the good solutions was in the organized
condition, one was in the disorganized condition. There were negligible differences
in the time required to complete the task among the different solution categories.
The informal results suggested that a clearly organized problem state does not necessarily lead to quicker and better solutions than a disorganized problem state, but it
does lead to different types of errors.
The conceptual distinctions between good, incomplete, and poor solutions were
related to the errors committed in solving the classification/composition problem.
In the good solutions, errors reflected misunderstandings about the meaning of words
in the chosen domain (viz., vehicle parts). Otherwise, students correctly divided the
objects into subclasses and super classes and correctly placed the inheritance and
composition edges. In the incomplete solutions, students only connected one inheritance or composition link from each class. As a result, they had too few links (see
Fig. 6), and left out key relationships. In the poor solutions, students would often connect a class to multiple ancestor nodes of the same lineage (see Fig. 7 for the classes
“Vehicle Part”, “Rim”, and “19” Aluminium Rim”). For example, in group 3, students
connected “Car” to both “MotorVehicle” and “Vehicle.” The poor solution groups
also typically created too many edges and were logically inconsistent about the connection decisions they made.
Solutions can also be characterized by the way students tended to arrange nodes in
the shared workspace. In the good solutions, the students separated the two abstract
classes, placing one at the top of the workspace and the other at the bottom. IS-A
and PART-OF links flowed in opposite directions and crossed infrequently and only
when necessary. Objects were organized in rows that reflected their level of abstraction. In the incomplete solutions, the two abstract classes were placed relatively close
to one another, all links pointed in one direction, and there were no crossed links.
The objects tended to be clustered together and were organized into fewer rows than
in the good solutions, but the rationale behind the organization wasn’t as clear. Finally,
the poor solutions had the abstract classes positioned without an obvious rationale.
They had much longer edges pointing in all directions and frequently intersecting with

User Model User-Adap Inter (2006) 16:175–209

189

Fig. 6 Incomplete solution of the classification/composition problem. Notice that only one edge was
created from each class or object

Fig. 7 Poor solution of the classification/composition problem. Notice that abstract classes are
positioned without an obvious rationale and there are longer intersecting edges

190

User Model User-Adap Inter (2006) 16:175–209

one another (see Fig. 7). As a result, the poor groups tended to use the entire shared
workspace.
Analyzing students’ solution processes We then analyzed the processes associated
with the development of each solution. While working on the problem, students could
take three types of actions: chat actions, move actions, and creation/deletion actions.
Solution types showed differences in collaborative and task-oriented behavior. In
terms of collaboration, the students with the good solutions had different approaches.
Group 8 worked completely collaboratively. Members would take turns moving and
creating objects and, for a given group of objects, one member would reposition
objects while the other would create the edges. In contrast, the members of group 5
worked completely in parallel, coordinating their actions only to correct their partner’s mistakes. In the incomplete and poor groups, pair members shared the work.
The poor groups were informal in their turn taking, while students in the incomplete
groups would alternate taking the initiative. With the exception of group 5, groups
decided on their actions using the chat window and ensured that both members agreed
on the actions being taken.
The three solution groups coordinated phases of chatting, moving, and creating/
deleting differently. In the good solutions, the approaches of two pairs were dissimilar. Group 8 collaborated by alternating chat phases, move phases, and creation/
deletion phases, while group 5 alternated between move phases and creation/deletion phases, and primarily communicated visually. In both groups, adjacent phases
referred to the same objects and levels of abstractions, displaying a coherent problemsolving strategy. Both pairs were the only groups to have more move actions than chat
actions. The incomplete groups had fewer move actions, longer phases of chatting,
and fewer deletions. They adopted the inefficient strategy of discussing many future
actions, creating a single edge, and then repeating the discussion. On the other hand,
they would consistently reorganize objects before creating edges, which may have
contributed to the tree-like organization of the classes. The poor groups engaged in
long phases of chatting or moving, but showed less coherence between the objects
they were discussing, objects they were creating, and objects they were moving. They
deleted a lot of edges and tended to create particular edges before repositioning them.
This disorganized approach probably led to the conceptual and visual disorganization
of their final solutions. Another difference was in terms of the selection of objects:
both the good and the incomplete solutions would focus on objects based on their
level of abstraction; they would manipulate a given super class and all its subclasses,
and then move on to another group of objects. On the other hand, students in the poor
solutions appeared to let the problem organization guide their actions. They would
draw edges between classes that were physically close to one other in the shared
workspace, rather than classes that were semantically related. Differences in object
selection probably related to differences in the consistency of the final solutions.
These results indicate that we should focus on five elements when evaluating
students’ performance on graphical modeling tasks: conceptual understanding,
visual organization, task selection, task coordination, and task coherence (see Table 1).
These elements were chosen because they represent different positive and negative
strategies of student action that appeared to affect the quality of groups’ solutions.
Students within each solution type (good, incomplete, and poor) tended to make the
same types of mistakes within each of these categories.
Conceptual understanding refers to a pair’s ability to correctly place the inheritance
and composition edges, while visual organization refers to a pair’s ability to visu-

User Model User-Adap Inter (2006) 16:175–209

191

Table 1 Solution types and dimensions of analysis in Study 1
Good
(groups 5 and 8)

Incomplete
(groups2, 6, and 7)

Poor
(groups 1, 3, and 4)

Conceptual
understanding

Good—only trivial
mistakes

Visual
organization
Task
coordination

Good—based on
abstractions
Good—good and
alternation of phases
distribution of
work
Good—adjacent
phases referred to
similar objects and
levels of abstraction.
Good—based on
abstractions

Incomplete—only one
link extended from
each class
Overly organized—
had a tree-like structure
Hesitant—long chat
phases, formal turn-taking
structure

Inconsistent—too
many links extended
from each class
Disorganized—had
long, intersecting links
Impulsive—creation
before organization,
informal turn-taking.

Good—adjacent phases
referred to similar
objects and levels of
abstraction.
Good—based on
abstractions

Poor—adjacent phases
referred to different
objects

Task
coherence

Task
selection

Poor—based on visual
proximity

ally arrange the classes and edges according to a schema expressing the relations
intended by the student. These two elements are linked; conceptual understanding of
the problem was often reflected in the visual organization, and therefore conceptual
steps tended to parallel organizational steps. For example, students in the incomplete
solution groups appeared to believe that they could create only one inheritance or
composition link extending from each object, and their solutions thus tended to take
on rigid tree structures.
Task coherence, task coordination, and task selection reflect student strategies
for collaborating on the problem. Task coordination refers to skills in coordinating
actions, without reference to the content of the actions. It includes distributing the
work among group members, and spending appropriate amounts of time in each
phase. The good groups exhibited successful task coordination, in part because they
spent more time moving objects than talking about them. Task coherence refers to the
appropriateness of the content of student actions. Students in the incomplete groups
showed poor task coherence by chatting at length about many different links and then
creating a single link. Task selection refers to a student’s ability to set subgoals for
solving the problem by drawing edges between classes in a sensible order. A breakdown in task selection leads to disorganized and incoherent solutions, as seen in the
poor group. Evaluation of the students’ problem-solving process should assess these
three skills.
We identified these five dimensions by using a data-driven approach to analysis, as
opposed to a theory-driven approach. The results of applying BND methodology are
inherently domain-specific and data-driven: It is used to develop a model based on
student performance on a given task. Because we wished to assess the potential for
BND, during our analysis we only considered elements that can currently be analyzed
and tutored using the BR. We made a conscious decision to look in detail at three
basic elements: the user performing the action, type of the action, and target of the
action. We avoided elements that required in-depth coding, like the content of chat
actions, because those elements cannot currently be automatically identified in the
BR. In general, we wished to determine not only how those basic elements affected

192

User Model User-Adap Inter (2006) 16:175–209

student collaboration in this specific domain, but also how they could be relevant for
discriminating good collaboration from bad collaboration in general.
5.2 Relationship between the five dimensions and other analysis approaches
It is important that despite our data-driven approach, we can show that the process dimensions that we have identified (i.e., task coordination, task coherence, and
task selection) have a theoretical basis in collaborative learning. Spada et al. (2005)
combined a bottom-up analysis with a review of the literature to identify nine dimensions of collaborative process, drawn from major theories of collaboration such as
Clark and Brennan’s (1991) theory of grounding and Stasser and Titus’ (1985) theory
of knowledge sharing. Spada et al. describe sustaining mutual understanding, information pooling, and reaching consensus as processes involving the convergence on
similar concepts during collaboration, which can be assessed by task coherence, which
relates to the objects used in adjacent turns by users. The high-level dimensions of
coordinating communication, technical coordination, and shared task alignment all
deal with the coordination of tasks between users, and can be probed with task coordination, which involves the nature of user turns. Finally, the Spada et al. planning
dimensions of task division and time management relate to task selection, which deals
with how users decompose a task into subgoals. While there is a link between the
Spada et al. dimensions and the dimensions we have identified, there is not a oneto-one mapping, as they operate at a different level of granularity. Our dimensions
are designed to involve easily detectable patterns of student action, while the Spada
et al. dimensions are a result of extensive coding of protocol data. We gain automaticity with our approach but lose the richness of the Spada et al. analysis. Eventually, we
hope to move toward a BR that facilitates a deeper analysis of collaborative protocols.
Nevertheless, it is likely that our dimensions will prove to be relevant for tasks
in other domains, for they function as units of analysis for collaborative process and
have some basis in collaborative learning theory. For example, let us look at how our
dimensions might transfer to collaborative essay writing. In this domain, conceptual
understanding would refer to whether the students stated the key points of the essay
problem. The analogue of visual organization would be the organization of the paragraphs in the essay itself. Task coordination might refer to how students shared the
task of writing the essay, and whether they alternated planning, writing, and proofreading stages appropriately. Task coherence would be whether students referred to
the same elements of the essay in sequences of essay-writing actions (for example, in
adjacent planning and writing phases, or in writing adjacent paragraphs). Finally, task
selection would assess how well students decomposed the task of writing the essay in
the first place. Although essay writing is quite different from graphical modelling, the
dimensions are still useful units of analysis.
5.2.1 How abstraction might play a part in the analysis of the five dimensions
One of the key goals of this first study was to gain insights into how we might enhance
the BND methodology to provide more helpful data and analysis. In theory, the BR
should be able to facilitate analysis of all the skills from Table 1. However, these
five elements are not supported in the current BR, so that it is difficult to classify
behavior and provide tutoring support. When all actions were input, the initial BR
graph showed no common paths. In an attempt to produce convergent paths in the

User Model User-Adap Inter (2006) 16:175–209

193

behavior graphs, we restricted input to the BR to creation and deletion actions.
Unfortunately, analyzing creation and deletion alone appears too limited to be useful.
Although the BR records sequences of actions at a single, detailed level of generality,
the nature of this problem indicates that student skills at different levels of abstraction
need to be addressed, and the BR needs to be able to create hierarchies of behavior
graphs.
Conceptual understanding and visual organization appear to be addressable on an
action-by-action basis. On the other hand, task coordination and task coherence seem
to be best evaluated through the analysis of phases of action, or chains of the same type
of action. A chain of chat actions followed by chain of creation actions would indicate
that, on a task coordination level, students have decided to discuss what objects they
should create and then create some objects. This type of information is difficult, if not
impossible, to extract from an action-by-action representation. Finally, task selection
seems like it could be analyzed in the BR by aggregating multiple phases of action,
which indicate high-level goals.
We took a step toward addressing this problem by modifying the BR to support
recording at different levels of abstraction (see the section “A granularity analysis:
abstracted actions versus individual actions”). Another approach we will consider is
to classify actions and action sequences in Cool Modes as described in Harrer and
Bollen (2004). This approach could be used to process actions at different levels of
abstraction.
5.3 Study 2
5.3.1 Study 2 method
We performed Study 2 to verify that abstraction is important and generalizable across
different graphical modelling problems. We also wished to explore how abstract
actions could be analyzed according to our five dimensions and tutored using the
BR. We again varied the organization of the starting state in an attempt to produce a
greater variety of collaborative actions.
We assigned 16 students to eight dyads and asked each dyad to solve a traffic light
modeling problem using the Cool Modes/BR integrated system. The objects in the
given problem were traffic lights (e.g., “car.red”, “ped.green”) and transitions. The
students were volunteers from the same course as in Study 1 at the University of
Duisburg (“Modeling Techniques in Computer Science”). Five of the students (pairs
2 and 5, one in pair 3) had had previous experience with Cool Modes, and all students
received approximately 5 min of instruction on how to use the system before the
study. The student pairs worked at separate workstations, back-to-back in the same
room. They shared a single Cool Modes workspace, configured in a manner similar to
that shown in Fig. 1 (but without the links between nodes) and each received a paper
handout with the instructions to solve the task.
The students were asked to model the progression of car and pedestrian lights at
a given intersection using Petri Nets (i.e., they were asked to draw links between
traffic lights and transitions, and use tokens to represent an illuminated light). While
working on the problem, students could take four types of actions:
•
•

Chat actions, “talking” to a partner in a chat window.
Move actions, repositioning an object in the shared workspace.

194

User Model User-Adap Inter (2006) 16:175–209

Table 2 Solution types and dimensions of analysis in Study 2

Conceptual
understanding
Visual
organization

Task
coordination

Task
coherence
Task
selection

•
•

Good
(groups 1 and 2 (both
in scrambled
condition))

Mediocre
(Groups 3, 4
(scrambled), and 5
(ordered condition))

Poor
(Groups 6–8 (all
in ordered condition))

Good—mean score 6.5
of possible 9 on
concepts
Easy to follow—very
few intersecting links

Mediocre—mean score
3.7 of possible 9 on
concepts
Real-world—yet the
scrambled groups
created a layout with
relatively few
intersecting links
Less efficient—longer
chat phases; balanced
work distribution

Poor—mean score 1.3
of possible 9 on
concepts
Real-world—many
intersecting links or
(group 7) overly
simplistic square
Inefficient—long chat
with fewer move
actions; more turn-taking

Good—adjacent phases
referred to same objects
Object-focused—chain
of tasks remained
focused on same object
or class

Good—adjacent phases
referred to same objects
Object-focused—chain
of tasks remained
focused on same object
or class

Efficient—fewer chat
actions and phases; less
balanced work
distribution
Good—adjacent phases
referred to same objects
Object-focused—chain
of tasks remained
focused on same object
or class

Creation/deletion actions, creating or deleting edges and tokens.
Simulation actions, firing transitions to move tokens from one light to another.

Each dyad was assigned to one of two possible conditions. In the ordered condition,
the pairs attempted to solve a problem in which related objects were in juxtaposition
to one another, providing a well-organized visual display of the final network. In this
case, the objects were organized like real-world traffic lights, with the car lights on one
side, the pedestrian lights on the other side, and the transitions in the middle. In the
scrambled condition, the pairs solved the same problem, but objects were randomly
positioned in the workspace. The student pairs had 30 min to work collaboratively on
the task.
For a post-test of their knowledge gains, each student individually (without collaboration) had to model another typical topic for the Petri net technique: regulating
competing access to exclusive resources (three processes competing for two processors
in a computer) from an empty workspace. For this task the students had approximately
20 min to solve the problem.

5.3.2 Study 2 results and analysis
Again, we wanted to analyze the results in such a way that the BR could perform the
analysis automatically, without excessive pre-processing. We wanted to validate the
five dimensions we identified in Study 1: conceptual understanding, visual organization, task coordination, task coherence, and task selection. We also wished to assess
the potential for tutoring those dimensions. Table 2 summarizes our analysis of the
Study 2 data.

User Model User-Adap Inter (2006) 16:175–209

195

Fig. 8 Solution with “real-world” arrangement

To evaluate conceptual understanding, six concepts were identified as required by the
problem statement:
(1)
(2)
(3)
(4)
(5)

pedestrian traffic lights should change in the correct order;
car traffic lights should change in the correct order;
there should be no point where the lights stop changing;
depending on the situation, only two or three lights should be on at a time;
there should never be a solution where the pedestrians and the cars are moving
at the same time;
(6) the pedestrian and car lights should change simultaneously.
Solutions were rated on a scale of nine based on these categories. The first two concepts were weighted greater than the following four, because they were more critical
to solving the problem. In this case the scrambled group produced significantly better
solutions than the ordered group (Ms = 5.25 vs. 1.75). Solutions could be further
divided into good (groups 1 and 2, M = 6.5), mediocre (groups 3–5, M = 3.7), and
poor solutions (groups 6–8, M = 1.3). The scrambled group had two good and two
mediocre solutions, and the ordered group had one mediocre and three bad solutions.
The visual organization of the final solutions can be described in terms of two competing schemes: “real-world,” wherein the car and pedestrian lights were separated
and arranged in the red/yellow/green order typical of traffic lights, versus “easyto-follow,” wherein the number of edge crossings was minimized. The “real-world”
arrangement often meant that the best place for the transitions was in the center of the
shared visual space; these solutions were difficult to follow, with too many intersecting
links extending in too many different directions. In the ordered start state, the ideal
solution corresponded to the real world, but was not easy-to-follow (see Fig. 8). Three

196

User Model User-Adap Inter (2006) 16:175–209

Fig. 9 Solution with “easy-to-follow” arrangement

out of the four ordered groups (groups 5, 6, and 8) did not significantly reposition
the objects from their original places in the start state. Consequently, the edges that
they drew between objects intersected often (37% of the time), and were difficult to
follow. Group 7, the other organized group, moved transitions so that the edges they
drew made a square, which reflected their overly simplistic, linear conception of the
problem. On the other hand, all four of the groups in the scrambled condition moved
objects from their initial disorganized state to good final solutions that were relatively
easy to follow. Groups 3 and 4 created a real-life arrangement of the traffic lights, but
put the transitions in locations that made the edges between transitions and objects
easier to follow and separate from one another. They had the best balance between a
real-world arrangement and an easy-to-follow solution, with 31% intersecting links.
Groups 1 and 2 organized their traffic lights in a way that was clear to follow with an
average of 7% intersecting links, but had little real-world correspondence. It appears
that our conception of an “organized” condition may not have been as well founded
for this particular problem, since an easy-to-follow arrangement seemed to relate to
better solutions than a real-world arrangement (see Fig. 9).
The results for the task coordination differed significantly between good and bad
solutions. Good groups had a significantly fewer percentage of chat actions than mediocre and poor groups (Ms = 12, 48, and 44%), and a significantly lower percentage
of chat phases (Ms = 20, 40, and 39%). The good groups and the two mediocre
groups in the scrambled condition also had a significantly higher percentage of move
actions than the ordered groups (Ms = 28 and 8%) and significantly more move
phases (Ms = 23 and 11%). There was some statistical evidence that the ordering
of phases also had an effect on whether groups did well or poorly, with the optimal
sequence of phases being chat->move->creation/deletion->simulation. Further, the

User Model User-Adap Inter (2006) 16:175–209

197

good groups had a less balanced work distribution than the mediocre and poor groups.
The ordered (and therefore less successful) groups split their time between having
one person perform the whole phase (M = 37%), the other person perform the whole
phase (M = 34%), or both people taking action in the phase (M = 28%). The scrambled groups had fewer phases where both people took action (M = 15%), and a less
balanced distribution of individual phases (Ms = 53 and 32%). These results were
surprisingly congruent with the task coordination results for Study 1.
Although task coherence varied between conditions in Study 1, there were few
differences on this dimension between groups in Study 2. Groups referred to an average of 1.8 objects per phase in move phases, creation/deletion phases, and simulation
phases. All groups tended to refer to the same objects across multiple phases.
Task selection also did not differ between groups in this study, but commonalities
between groups provided insight into the collaborative process. Groups structured
their actions based on the transitions from one state of traffic lights to the next.
Creation/deletion actions were chained 79% of the time, in that the current edge
being drawn involved an object used in the previous creation/deletion action. Groups
tended to focus on either the pedestrian or the car lights at a given time; the current creation/deletion action tended to involve the same light class as the previous
creation/deletion action 75% of the time.
Since the focus of this paper is on the analysis of collaboration as a step towards
the creation of tutors for collaboration, we only briefly mention the results of the
individual post-test of Study 2. In the post-test the solutions can be classified again in
the three categories: seven students had a good solution, including all four students
from the good groups (pairs 1 and 2), two students from mediocre groups (one from
pair 3 and one from pair 5), and one student from a poor group (pair 6). Five students
had a mediocre solution, including three students from mediocre groups (one member
each of pairs 3–5) and two students from poor groups (one member each of pairs 6
and 8). Four students had a poor solution. One was from a mediocre group (pair 4),
and three were from poor groups (one member from pair 8 and both members of pair
7). Compared to the classification of the collaborative solution one student improved
by two categories (poor to good), four students improved by one category (two from
mediocre to good, two from poor to mediocre), while only one student had a worse
category (mediocre to poor). This improvement in the results indeed suggests that
collaboration has a positive effect on student performance. We will investigate this
effect in more detail in follow-up studies, to get more reliable information if the
collaboration produces better learning outcomes also for other tasks.
5.4 Additional analyses of Studies 1 and 2
5.4.1 A granularity analysis: abstracted actions versus individual actions
After Study 2 we explored how abstracted actions (versus individual actions) within
the BR might be used to help in building an appropriate example-tracing tutor of
collaboration. In this section, we will discuss how the BR might be used to analyze
results and, eventually, provide tutoring, focusing on conceptual understanding, a
low level of abstraction which might be addressed by tutoring individual actions, and
task coordination, a middle level of abstraction that might be better addressed by
abstraction.

198

User Model User-Adap Inter (2006) 16:175–209

Fig. 10 An Abstract-level behavior graph

To explore tutoring of conceptual understanding, we used the BR to capture individual creation actions, and discovered that two groups (1 and 3) used the same correct
strategy in creating the links necessary to have the traffic lights turn from green to
yellow to red. This path in the graph demonstrated a conceptual understanding of
how Petri Nets can be used to effect transitions. We will ultimately be able to add
hints that encourage students to take this path, leveraging the behavior graph as a
means for tutoring. In likewise fashion, the BR can also be used to identify common
bugs in participants’ action-by-action problem solving. For instance, the BR captured
a common error in groups 1 and 2 of Study 2: each group built a Petri Net, in almost
identical fashion, in which the traffic-red and pedestrian-green lights would not occur
together. In situations like this, the behavior graph could be annotated to mark this
sequence as buggy, thus allowing the tutor to provide feedback should a future student
take the same steps.
On the other hand, it is clear that the level of individual actions is not sufficient
for representing all of the dimensions. For instance, evaluating whether students are

User Model User-Adap Inter (2006) 16:175–209

199

chatting “too much” or alternating phases in an “optimal” way is not easily detected
at the lowest level of abstraction. To explore how we might do more abstract analysis,
we wrote code to pre-process and cluster the Cool Modes logs at a higher level of
abstraction and sent them to the BR. The pre-processing we did was straightforward;
we simply identified groups of consecutive identical actions that were taken by the
students in our dyads. So, for instance, if students 1 and 2 in a dyad exchanged six chat
messages in a row, our pre-processing code would identify this as a (CHAT, 6) cluster.
Figure 10 shows an example of this level of analysis from Study 2.
Instead of individual actions, edges in the graph represent phases of action (see the
“CHAT”, “MOVE”, and “OBJEC” designations on the edges). The number to the
right of each phase in the figure specifies how many instances of that particular action
occurred during consecutive steps, e.g., the first CHAT phase, starting to the left from
the root node, had two individual chat actions. The graph shows the first five phases
of groups 2, 3, 5, and 6. Because the type of phase, the number of actions within each
phase, and who participates (not shown in the figure), is recorded we can analyze
the data and, ultimately, may be able to provide tutor feedback at this level. For
instance, notice that the scrambled groups (2 and 3) incorporated move phases into
their process, while at the same point; the organized groups (5 and 8) only used CHAT
and OBJEC (i.e., creation/deletion) phases. Additionally, groups 5 and 8 began their
collaboration with a lengthy chat phase, and group 5 continued to chat excessively
(23 chat actions by group 5 leading to state22!). This level of data provided to the BR
could help us to understand better the task coordination dimension. In addition, if
provided at student time, the BR could also provide feedback to groups with “buggy”
behavior; for instance, a tutor might have been able to intervene during group 5’s long
chat phase. In future work, we intend to further explore how this, and other levels, of
abstraction can help us address not only the task coordination dimension but also the
task coherence and task selection dimensions
The argument for having different levels of abstraction for analysis and tutoring
with the BND approach is also supported by observations considering the manual
creation of example-tracing tutors. Petri nets have an operational semantics, i.e. they
can be simulated and tested during and after the construction process. Thus the experimentation with subparts of the created model and the configuration of the net by
changing the token values will happen naturally, especially with the students that
favor an experimental approach. An expert creating a tutor manually would have
to pre-define phases of experimentation and re-configuration throughout the whole
modeling process, which would lead to state-explosion for this problem type. This
issue is discussed further in the next section.

5.4.2 Expert interview and complexity analysis: bootstrapping versus manual creation
of example-tracing tutors
As part of Study 1 and to further investigate whether and how the BND approach
might be preferred to “from scratch” development of example-tracing tutors, we
interviewed an expert in the problem domain, a computer scientist with a strong
background in object-oriented modelling. We wanted to better understand what kind
of misconceptions or problems students might have and thus what kind of tutoring
would be required for the classification/composition problem, from the point-of-view
of an expert. We also wanted to uncover inherent problems in having an expert man-

200

User Model User-Adap Inter (2006) 16:175–209

ually define an example-tracing tutor (versus bootstrapping). Finally, we wanted to
validate our ideas about providing abstraction as part of the bootstrapping process.
First, our expert identified what he perceived to be likely student mistakes for this
problem:
•
•

•

leaving out levels of the hierarchies, such as connecting “Car” directly to “Vehicle,”
not going via “Motored Vehicle”;
defining multiple IS-A links to classes of a hierarchy path from one subclass,
ignoring relation transitivity, e.g. connecting “Car” to both “Motored Vehicle” and
“Vehicle” when “Motored Vehicle” is also modeled as a subclass of
“Vehicle”;
providing incorrect directions of both IS-A and PART-OF links.

Given these common errors, a manually defined example-tracing behavior graph
would require at least one additional “buggy” branch for each composition link (i.e.,
for incorrect PART-OF link direction) and up to three additional “buggy” branches
for each classification link (i.e., for missing hierarchy levels, incorrect transitivity, and
incorrect IS-A link direction). However, our preliminary data suggests that, while students do make these types of errors, they are not as frequent as our expert suspected.
More specifically, there appear to be particular objects that more naturally lead to
these misconceptions, thus obviating the need to manually define buggy branches for
all possible combinations. By bootstrapping data for particular problems, we could
build a model that is more likely to be representative of real misconceptions, rather
than what is suspected by an expert, and save effort in manually and comprehensively
building a behavior graph from scratch.
Second, since most students are likely to re-arrange objects for this type of problem, especially in the disorganized condition, we asked our expert what types of
re-arrangement techniques students would likely use to make sense of the problem.
Our expert conjectured that students would take one of two approaches:
•
•

“concept clustering” in the initial stage of the solution to organize the objects
according to a conceptual schema, such as congregating all “Parts” close to one
another;
“cleaning up” in the late stages of the problem solving to reduce the number
of intersecting links or to make the visual arrangement easier to understand or
aesthetically more attractive.

If our expert is correct, such a pre- and post-organization of the visual arrangement,
and all permutations of those arrangements, would need to be added to each path of
the behavior graph. This would be a considerable task to undertake in manual fashion. Thus, this appears to be an opportunity for abstraction techniques, in conjunction
with bootstrapping, to make a difference. To reduce the number of possible variants,
abstraction techniques that ignore exact screen positions and possible permutations
of operations would benefit the designer of a collaboration tutor.
Finally, but most important for our work, aspects of collaboration and communication must be considered. The domain expert provided specific situations in which
he assumed discussion and collaboration between students would occur:
•

Choosing one of the major correct solution variants, i.e. connecting the most
abstract concepts or the least abstract concepts.

User Model User-Adap Inter (2006) 16:175–209

•

201

Clarification of the conceptual understanding of some relations, e.g. if “Bike” has
a “Steering Wheel” as a relaxed interpretation of the concept “Steering device”.

The expert assumed discussion would happen after an action creating an edge. He
explained that when removing edges, students tend to act before and not after an
agreement. This was an interesting and surprising belief, since not all of our study
groups followed this approach. Some of our subjects engaged in intense chatting
before acting at the object level. Thus, the expert’s assumption in this case might be
viewed as an “expert blindspot” (Nathan et al. 2001); in the full specification of the
example-tracing tutor he would not have specified discussion phases before domain
actions. Again, the use of bootstrapping would provide the actual approach students
take, rather than the approach perceived by an expert.
As a complexity analysis of the possible communication of collaborating students,
we evaluated the complexity of a conversational schema, similiar to Winograd and Flores (1986). Our idealized discourse model for dyads only allowed a limited number of
conversational acts for students: proposing an action, agreeing or disagreeing with the
action, challenging and defending the action, and performing the action. Despite the
constraints imposed by the schema, the number of possible communication sequences
grows exponentially and thus cannot be, in practice, fully pre-defined by an expert.
This is even more pronounced in the combination of possible workspace actions and
natural conversation as used in our studies: in a graphical modeling problem, the
combination of typical mistakes, graphical arrangement phases, and communication
phases results in state explosion. Bootstrapping an example-tracing tutor, which will
capture only the most realistic sequences of actions, is preferable to having an expert
try to create one from scratch.
6 Discussion and future directions
In taking steps toward our vision of Bootstrapping Novice Data, we have learned
some important lessons. In considering the results of the current project—and thinking
about next steps—we asked ourselves the following questions.
•
•
•
•

Were the five dimensions valid units of analysis for the collaboration in our studies?
How might the BR be extended to better support collaborative analysis and model
creation?
How successful was our component-based integration and what remains to be
done?
Besides using the bootstrapped data to help build a tutor model, what other data
analyses might be valuable?

In the following sections, we attempt to answer to these questions and, for the questions we cannot fully answer, briefly lay out a roadmap forward.
6.1 The five dimensions of analysis
The dimensions did, indeed, provide a useful analysis framework. The conceptual
understanding dimension was helpful in evaluating problem solutions; in both studies
we were able to identify and rate the dyads based on salient (but different) conceptual features. Visual organization was important in both tasks, and appeared to inform
problem solutions. The task coordination dimension provided the clearest tutoring
guidelines of all the dimensions. The task coherence dimension provided information

202

User Model User-Adap Inter (2006) 16:175–209

about object references in Study 1, but was not as clear of an aid in the analysis of
Study 2. Finally, the task selection dimension was a useful measure in both studies,
but was more valuable in Study 1 due to the greater number of possible strategies. In
a nutshell, it appears that evaluating these five dimensions may be a useful strategy
for tutoring collaboration of graphical modelling problems.
We limited our analysis to elements of the collaboration that (1) could be linked
to solution quality, (2) could be operationalized based on patterns of group actions,
and (3) could be displayed in the BR with a minimum of pre-processing. There is the
potential to extend this analysis. We will use more sophisticated (and fine-grained)
coding schemes for collaborative action (e.g., Spada et al. 2005; Weinberger and
Fischer 2006), to analyze future data. The challenge would then be to translate these
coding schemes into definitions the BR can recognize automatically and in real-time
in order to pre-process actions and transform them into abstract actions that are coded
in the particular scheme we use. We could also use data mining techniques to connect
patterns of action to positive and negative solutions. For this approach, we will need
to collect more data, and a way to interpret the results of this analysis so that we can
add appropriate hints and bug messages. Finally, it may be appropriate to analyze
sequences of actions both at the group level and at the individual level.
Of course, since we limited ourselves in the initial two studies to similar types of
problems, the generality of our dimensions of analysis is unclear. On the other hand,
with the exception of visual organization, it appears that the dimensions would be
applicable to other collaborative problems as well. While general applicability of the
dimensions is certainly an interesting topic of future research, our primary interest
is the type of graphical modelling problems supported by Cool Modes and similar
applications; thus, our studies will continue in this area, at least in the near term.
6.2 Extending the behavior recorder to better support analysis and model creation
Extensions to the BR are clearly required to facilitate the BND approach, as discussed
earlier. For instance, we will develop more sophisticated pre-processing techniques
to provide abstracted, or aggregated, input to the BR. Besides the Goodman et al.
(2005) and Soller and Lesgold (2003) approaches we have already discussed, Stevens
and Soller (2005) have developed an interesting multi-layered approach to clustering,
utilizing item response theory, artificial neural networks, and Hidden Markov Modeling, to analyze patterns in large amounts of subject data. In some of our own previous
work, we investigated the problem of automatically identifying phases by aggregating
similar types of actions (Harrer and Bollen 2004) and may be able to leverage those
techniques in the present work.
In deciding which specific data mining and clustering techniques to try, we will
be guided by the collaborative analysis techniques discussed previously (Spada et
al. 2005). In particular, for each of their nine dimensions (e.g., information pooling,
coordinating communication, technical communication), we will attempt to identify
representative action patterns for which our data mining and/or clustering approach
will search. Taking an approach such as this will ground our technical method in a
well-founded theoretical model, as Spada et al.’s approach relies on such models.
Besides fully automated techniques, we also envision that bootstrapping in the
BR may benefit from a mixed-initiative approach, i.e., interleaving author actions
with bootstrapping and analysis. For instance, one could imagine generalizing a bootstrapped behavior graph by manually defining appropriate feature detector functions

User Model User-Adap Inter (2006) 16:175–209

203

and updating some of the edges in the graph appropriately. As a simple example,
suppose we defined a “short” cluster function (Is-Short-Cluster (5)) which takes a
path length 5 as a parameter and can identify a behavior graph path with less than
or equal to 5 consecutive edges of the same action type. We could then replace an
exact matching value on the corresponding edge in the behavior graph with the new
Is-Short-Cluster function. This would allow us, in turn, to collapse branches in the
behavior graph and increase the frequency counts on those branches.
To implement such a facility, we would need to develop a function library, a function library GUI, and techniques for collapsing states in the BR. We will explore
techniques with real users and real data sets to understand precisely the types of
mixed-initiative techniques that will be most useful during bootstrapping (or in a
post-processing step) and what works to save development time. We also will experiment with using machine learning (ML) techniques to partially automate aspects of
the mixed-initiative process. The general idea is to rely on the ML algorithm to induce
the appropriate functions to apply to behavior graph edges, instead of relying on the
author to select them. For instance, in the example given above, the author might label
consecutive edges with the same skill and let the machine learning algorithm induce
the appropriate matching function. A similar approach, designed to induce production rules from behavior graphs, has already been developed and is being tested at
our lab at CMU (Matsuda et al. 2005).
For non-abstracted evaluation of student actions, we also need to solve the problem
of divergent solution paths for semantically similar sequences of actions. We intend
to modify the BR to merge similar solution paths using an approximate, rather than
exact, comparison between states. Each state in the behavior graph might be represented by a sequence of parameters. For example, a state in the “move” graph might
be represented by values for link length, node cluster, link flow, and organization of
rows, which are particular characteristics that we identified for visual organization. A
move action would update the values of these parameters and the parameters would
then be compared to other states on a qualitative basis. Combinations of parameters should be approximately equal to one another for two states to be declared the
same. As with abstraction, we have done some previous work that may be helpful in
collapsing similar states (Bollen et al. 2004). An important subproblem is that two
paths with the same actions are taken as different from the point at which actions
occur in a different order. This problem could also be addressed by providing more
expressive ordering constraints to a CTAT author, in the case of paths with identical
actions in a different order. We currently have only weak methods for dealing with
path ordering. For instance, we cannot have an unordered group of ordered groups
of actions. One approach is to define hierarchies of unordered and ordered groups of
actions; we are exploring this approach already. We will also need to develop a user
interface to support authors in defining such hierarchies.
Finally, our initial studies showed us that the BR needs to be extended to handle
dynamic instantiation of objects (e.g., the definition of new UML class nodes in Cool
Modes). In particular, it must be capable of handling dynamic object definition across
sessions to unify paths in the graph that diverge when the identity of objects is not recognized. Since Cool Modes uses a consistent, internal naming scheme for objects, we
can leverage this to identify common dynamic objects across sessions using mapping
tables.

204

User Model User-Adap Inter (2006) 16:175–209

6.3 Component-based integration
Another important lesson of our initial work relates to the component-based combination of Cool Modes and CTAT. In principle, doing plug-and-play integration should
be straightforward, but in practice it can be quite challenging. Two tool characteristics
identified in Ritter and Koedinger (1996)—recordability and scriptability—are essential. Being recordable means the tool is capable of capturing individual actions taken
in its user interface (and not complete states). Cool Modes clearly meets the recordability criterion, as apparent from the current implementation and data capture
described in this paper. Being scriptable, on the other hand, means the tool is capable
of playing back user actions in its user interface, typically through a scripting language.
Scriptability is desirable for real-time tutoring and necessary for convenient Behavior
Recorder authoring. For example, to allow the author to immediately test the tutor
at any step, the author should be able to select any state in the behavior graph, and
the system should put the student interface into the corresponding configuration by
replaying the actions along a path to that state. The MatchMaker server of Cool
Modes, as described earlier and shown in Fig. 4, is scriptable. One of our next steps
will be to make the Cool Modes client capable of updating its interface based on the
messages from the BR received via the tutor adapter and MatchMaker component
of the integration architecture. This will also allow us to display in Cool Modes the
hints and bug messages from the BR, so that on-the-fly student tutoring is supported
by our technical integration.
6.4 Further analysis and use of student data
Besides using the student log files for building an initial version of a tutor, we envision
other uses of the data. For instance, we plan to do problem profile analysis (Mark
1998) in which we first have an expert, or group of experts, solve a problem using Cool
Modes, record the steps in a behavior graph, and then have students attempt to solve
the same problem, also recording the steps as in the BND approach described above.
Assuming the expert solution(s) is the correct one, we can then use the resulting
behavior graph to calculate the percentage of novice steps that diverge from the
experts. The greater the divergence from expert behavior, the harder this problem
can be assumed to be. Such data is helpful in designing and ordering problems in a
curriculum, e.g., situating more difficult problems later in the curriculum. This expertversus-novice data can also be used to do skill proficiency analysis. Divergent paths
that occur with a high frequency, according to the traversal counts collected by BND,
likely relate to skills that should be the focus of additional problems and tutoring. If
the author associates skills to edges of the behavior graph after BND data collection,
we will have the data necessary to identify skills that, in general, cause the students
more difficulty. The author could then design new problems that focus on these skills.
In order to improve an example-tracing tutor’s behavior graph over time, we plan
to do Learning factors analysis (Koedinger and Junker, 1999) with the Cool Modes
log data. Learning factors analysis is a process whereby a cognitive model is evaluated
and modified based on how closely student performance matches expected learning
curves. The labeling of edges with skills—essentially the “factors”—will allow us to
subsequently check whether collaborating students gradually reduce their error rate.
If the error rate for a particular skill does not yield a smooth, downward sloping learning curve, this is an indication that the skill has been mis-assigned to particular edges

User Model User-Adap Inter (2006) 16:175–209

205

in the behavior graph. Eventually, we intend to extend CTAT to support authors
in viewing and specifying alternative skill labellings, viewing the resulting learning
curves, and making changes to the skill labels accordingly.
We see our approach possibly supporting simultaneous development of both individual and group models. Bootstrapping is designed to create a plausible collaboration
model based on real data, and individual differences would not be explicitly represented in such a model. Yet the bootstrapping approach, in which a behavior graph is
built, or extended, by each collaborating group, may also provide data to build student
models with individual and collective characteristics (Paiva 1997). For example, one
student might perform excessive re-arrangement activities in an effort to conceptually
organize a task: this is clearly an individual trait that could be supplied to a separate
individual student model. On the other hand, the intensity and length of discussions
may vary substantially from one group to the next, even with the same student participating, and thus may be information that should be represented in a group model.
Separating and appropriately evaluating data for use in generating individual versus
group models is a topic for our future research.
The creation of individual student models and/or collaborative models is also discussed in more detail in other articles in this issue: Even though the tasks supported
by the medical tutoring system in (Suebnukarn and Haddawy 2006) are very different,
their approach shares many common traits: for each medical scenario or case, one
specific domain model is created, somewhat like an example-tracing tutor. They use a
Bayesian Network for the representations of domain and individual student models
as well as group models to address uncertainty in the student input. The same computational representation has been chosen in Read et al. (2006) for student and group
modelling in the linguistic domain. Because the diagnosis cannot be guaranteed for
natural language they combine their approach with interactive evaluation by more
experienced students; this technique is comparable to our mixed-initiative approach
of interactive creation and annotation of an example-tracing tutor.
For our analyzes of collaboration scenarios, we deliberately chose a random assignment to the dyads without taking into account prior knowledge or learning styles. For
our intended full version of collaborative tutoring with the BND approach, it is clearly
a valuable option to assemble groups according to some group formation strategies,
as are discussed in this issue in Read et al. (2006) and Alfonseca et al. (2006). These
components could be plugged into our integration architecture to configure the setup
of the collaboration session in Cool Modes.
7 Conclusion
Tackling the problem of tutoring a collaborative process is non-trivial. The tremendous
variety of possible collaborative behavior makes the task quite difficult. Others have
taken steps toward tutoring or coaching collaboration, but there are still challenges
ahead.
For our part, we have been working on capturing and analyzing collaborative problem solving in a graphical modelling tool (Cool Modes) by capturing actions in the
CTAT Behavior Recorder, a tool for building a special type of cognitive tutor called
an example-tracing tutor. The technique we have devised, called “bootstrapping novice data”, is based on the idea of recording problem solving behavior and then using
the captured demonstration as the basis for creating a model for tutoring students.

206

User Model User-Adap Inter (2006) 16:175–209

The work and empirical results we have presented in this paper have led us to the
conclusion that, to reach our goal of tutoring collaboration, BR analysis needs to take
place at multiple levels of abstraction. The use of techniques such as data mining and
clustering in conjunction with bootstrapping—in a mixed-initiative mode with a tutor
author—will be our next major area of investigation.
Acknowledgments This research was initiated by NSF-DFG travel grants for German-American
researcher exchange in 2004. We thank Kenneth R. Koedinger for his valuable contributions in the
discussion of this project, Tilman Goehnert and Mike Schneider for their support in implementing this
approach, and the students of the course “Modeling Techniques in Computer Science” in Duisburg
for their voluntary participation in our two studies.

References
Aleven, V., McLaren, B.M., Sewall, J., Koedinger, K.R.: The Cognitive Tutor Authoring Tools (CTAT):
Versatile and increasingly rapid creation of tutors. Accepted for presentation at the 8th international conference on intelligent tutoring systems, Jhongli, Taiwan, 26–30 June 2006.
Aleven, V., Sewall, J., McLaren, B.M., Koedinger, K.R.: Rapid authoring of intelligent tutors for realworld and experimental use. Submitted to the 6th IEEE international conference on advanced
learning technologies (ICALT 2006), Kerkrade, The Netherlands, 5–7 July 2006.
Alfonseca, E., Carro, R.M., Martin, E., Ortigosa, A., Paredes, P.: The Impact of Learning Styles on
Student Grouping for Collaborative Learning: A Case Study, in this issue (2006)
Anderson, J.R., Corbett, A.T., Koedinger, K., Pelletier, R.: Cognitive tutors: lessons learned. J. Learning Sci. 4, 167–207 (1995)
Aronson, E., Blaney, N., Stephan, C., Sikes, J., Snapp, M.: The Jigsaw Classroom. Sage Publishing
Company. Beverly Hills, CA (1978)
Bollen, L., Harrer, A., Hoppe, H.U.: An integrated approach for analysis-based report generation. In:
Kinshuk, Chee-Kit Looi, Erkki Sutinen et.al. (eds.) Proceedings of the 4th IEEE International
Conference on Advanced Learning Technologies (ICALT 2004), pp. 1094–1095. Joensuu, Finland
Bransford, J.D., Brown, A.L., Cocking, R.R. (eds.): How People Learn: Brain, Mind, Experience, and
School. National Academy Press, Washington, DC (2000)
Clark, H.H., Brennan, S.E.: Grounding in Communication. In: Resnik, L.B., Levine, J. M.,
Teasley, S. D. (eds.) Perspectives on Socially Shared Cognition, pp. 17–149. American Psychological Association, Washington, DC (1991)
Corbett, A., McLaughlin, M., Scarpinatto, K.C.: Modelling student knowledge: cognitive tutors in
high school and college. User Model User-Adapted Interaction 10, 81–108 (2000)
Constantino-González, M.A., Suthers, D.D., Escamilla de los Santos, J.G.: Coaching web-based collaborative learning based on problem solution differences and participation. Int. J. Artificial
Intelligence Education 13, 263–299 (2003)
Constantino-González, M.A., Suthers, D.D.: Coaching collaboration in a computer-mediated learning
environment. In: Proceedings of the Conference on Computer Supported Collaborative Learning
(CSCL-02) (2002)
Gasevic, D., Devedzic, V.: Software support for teaching Petri Nets: P3. In: Proceedings of the
3rd IEEE International Conference on Advanced Learning Technologies, pp. 300–301. Athens,
Greece, (2003)
Goodman, B.A., Linton, F.N., Gaimari, R.D., Hitzeman, J.M., Ross, H.J., Zarrella, G.: Using Dialogue
Features to Predict Trouble During Collaborative Learning. User Modelling and User-Adapted
Interaction, vol. 15, pp. 85–134. Springer, Berlin (2005)
Harrer, A., Bollen, L.: Klassifizierung und Analyze von Aktionen in Modellierungswerkzeugen zur
Lernerunterstützung. In: Workshop-Proceedings Modellierung 2004, Marburg, 2004
Harrer, A., McLaren, B.M., Walker, E., Bollen, L., Sewall, J. (2005). Collaboration and cognitive tutoring: integration, empirical results, and future directions. In: Proceedings of the 12th International
Conference on Artificial Intelligence and Education (AIED-05), Amsterdam, the Netherlands,
July 2005
Heiner, C., Beck, J.E., Mostow, J.: Lessons on using ITS data to answer educational research questions. In: Proceedings of the ITS2004 Workshop on Analyzing Student-Tutor Interaction Logs to
Improve Educational Outcomes, pp. 1–9 (2004)

User Model User-Adap Inter (2006) 16:175–209

207

Hoppe, H.U., Gassner, K., Mühlenbrock, M., Tewissen, F.: Distributed visual language environments
for cooperation and learning - applications and intelligent support. Group Decision Negotiation
9(3), 205–220 (2000)
Introne, J., Alterman, R.: Using Shared Representations to Improve Coordination and Intent Inference, this issue (2006)
Jansen, M.: Matchmaker - a framework to support collaborative java applications. In: Proceedings of
the 11th International Conference on Artificial Intelligence in Education (AIED-03), IOS Press,
Amsterdam (2003)
Johnson, D.W., Johnson, R.T.: Cooperative learning and achievement. In: S. Sharan (ed.) Cooperative
Learning: Theory and Research, pp. 23–37. Praeger, New York (1990)
Koedinger, K.R., Aleven, V., Heffernan, N., McLaren, B.M., Hockenberry, M.: Opening the door
to non-programmers: authoring Intelligent Tutor Behavior by Demonstration. In: Proceedings
of the 7th International Conference on Intelligent Tutoring Systems (ITS-2004), Maceio, Brazil
(2004)
Koedinger, K.R., Anderson, J.R., Hadley, W.H., Mark, M.A.: Intelligent tutoring goes to school in the
big city. Int. J. Artificial Int. Educ. 8, 30–43 (1997)
Koedinger, K.R., Junker, B.: Learning factors analysis: mining student-tutor interactions to optimize
instruction. Presented at Social Science Data Infrastructure Conference. New York University.
12–13, November, 1999
Koedinger, K.R., Terao, A.: A cognitive task analysis of using pictures to support pre-algebraic
reasoning. In: Schunn, C.D., Gray, W. (eds.) In: Proceedings of the 24th Annual Conference of
the Cognitive Science Society, pp. 542–547 (2002)
Lesgold, A., Katz, S., Greenberg, L., Hughes, E., Eggan, G.: Extensions of intelligent tutoring paradigms to support collaborative learning. In: Dijkstra, S., Krammer, H., van Merrienboer J.
(eds.) Instructional Models in Computer-Based Learning Environments, Springer-Verlag, Berlin,
pp. 291–311 (1992)
Lesgold, A.M., Lajoie, S.P., Bunzo, M., Eggan, G.: SHERLOCK: A coached practice environment for
an electronics troubleshooting job. In: Larkin, J., Chabay, R. (eds.) Computer Assisted Instruction
and Intelligent Tutoring Systems. LEA, Hillsdale, NJ (1993)
Lieberman, H. (ed.): Your Wish is My Command: Programming by Example. Morgan-Kauffman
Publishers (2001)
Mark, M.: Analysis of Protocol Files: PACT Center User’s Manual. Carnegie Mellon University (1998)
Matsuda, N., Cohen, W.W., Koedinger, K.R.: Building Cognitive Tutors with Programming by Demonstration. In: Kramer, S., Pfahringer, B. (eds.) Technical report: TUM-I0510, Proceedings of the
International Conference on Inductive Logic Programming, pp. 41–46. Institut fur Informatik,
Technische Universitat Munchen (2005)
McArthur, D., Lewis, M.W., Bishay, M.: ESSCOTS for learning: transforming commercial software
into powerful educational tools. J. Artificial Int. Educ. 6(1), 3–33 (1996)
McLaren, B.M., Bollen, L., Walker, E., Harrer, A., Sewall, J.: Cognitive tutoring of collaboration:
developmental and empirical steps toward realization. In: Proceedings of the Conference on
Computer Supported Collaborative Learning (CSCL-05), Taipei, Taiwan in May/June 2005.
McLaren, B.M., Koedinger, K.R., Schneider, M., Harrer, A., Bollen, L.: Bootstrapping novice data:
semi-automated tutor authoring using student log files. In: Proceedings of the ITS2004 Workshop
on Analyzing Student-Tutor Interaction Logs to Improve Educational Outcomes (2004a)
McLaren, B.M., Koedinger, K.R., Schneider, M., Harrer, A., Bollen, L.: Toward cognitive tutoring in
a collaborative, web-based environment. In: Matera, M., Comai, S. (eds.) Engineering Advanced
Web Applications, pp. 167–179. Rinton Press, Princeton, NJ (2004b)
McManus, M., Aiken, R.: Monitoring computer-based problem solving. J. Artif. Int. Educ. 6(4), 307–
336 (1995)
Merceron, A., Yacef, K.: TADA-Ed for educational data mining. Interactive Multimedia
Electronic Journal of Computer-Enhanced Learning, 7(1) (2005), http://imej.wfu.edu/articles/2005/1/03/index.asp.
Mühlenbrock, M., Hoppe, H.U.: A collaboration monitor for shared workspaces. In: Proceedings of
the International Conference on Artificial Intelligence in Education (AIED-2001) (2001)
Murray, T., Ainsworth, S., Blessing, S. (eds.): Authoring Tools for Advanced Technology Learning
Environments: Toward Cost-Effective, Adaptive, Interactive, and Intelligent Educational Software. Kluwer Academic Publishers, Printed in the Netherlands (2003)
Murray, T.: Authoring intelligent tutoring systems: an analysis of the state of the art. Int. J. Artif. Int.
Educ. 10, 98–129 (1999)

208

User Model User-Adap Inter (2006) 16:175–209

Nathan, M., Koedinger, K., Alibali, M.: Expert blind spot: when content knowledge eclipses pedagogical content knowledge. Paper presented at the Annual Meeting of the American Educational
Research Association. Seattle (2001)
Paiva, A.: Learner modelling for collaborative learning environments. In: Proceedings of the 8th International Conference on Artificial Intelligence in Education, pp. 215–222. Kobe (Japan), August
(1997)
Pinkwart, N.: Collaborative modeling in graph based environments. Berlin, Germany: dissertation.de
– Verlag im Internet (2005)
Pinkwart, N.: A Plug-In Architecture for graph based collaborative modelling systems. In: Hoppe,
U., Verdejo, F., Kay, J. (eds.) Proceedings of the 11th International Conference on Artificial
Intelligence in Education (AIED-03), pp. 535–536 (2003)
Pinkwart, N., Hoppe, H.U., Bollen, L., Fuhlrott, E.: Group-oriented modelling tools with heterogeneous semantics. In: Cerri, S., Gouarderes, G., Paraguacu, F. (eds.) Proceedings of the 7th
International Conference on Intelligent Tutoring Systems (ITS-2004), pp. 21–30. Maceio, Brazil,
Springer, Berlin (2002)
Polson, M.C., Richardson, J.J.: Foundations of Intelligent Tutoring Systems. Lawrence Erlbaum Associates Publishers (1988)
Read, T., Barros, B., Barcena, E., Pancorbo, J.: Coalescing individual and collaborative learning to
model user linguistic competences, this issue (2006)
Ritter, S., Koedinger, K.R.: An Architecture For plug-in tutor agents. J. Artif. Int. Educ. 7(3/4), 315–347
(1996)
Searle, J.: Dialogue Acts: An Essay in the Philosophy of Language. Cambridge University Press,
London
Slavin, R.E.: When and why does cooperative learning increase achievement? Theoretical and empirical perspectives. In: Hertz-Lazarowitz, R., Miller, N. (eds.) Interaction in cooperative groups:
The theoretical anatomy of group learning, pp. 145–173. Cambridge University Press, New York
(1992)
Soller, A., Lesgold, A.: A computational approach to analyzing online knowledge sharing interaction. In: Proceedings of the 11th International Conference on Artificial Intelligence in Education
(AIED-03), pp. 253–260. Sydney, Australia (2003)
Spada, H., Meier, A., Rummel, N., Hauser, S.: A new method to assess the quality of collaborative
process in CSCL. In: Koschmann, T., Suthers, D., Chan, T.W. (eds.) Proceedings of the Conference on Computer Supported Collaborative Learning (CSCL-05), pp. 622–631. Taipei, Taiwan,
Lawrence Erlbaum, Mahwah, NJ (2005)
Stasser, G., Titus, W.: Pooling of unshared information in group decision making: biased information
sampling during discussion. J. Pers. Soc. Psychol. 48, 1467–1478 (1985)
Stevens, R., Soller, A.: Implementing a layered analytic approach for real-time modelling of students’
Scientific Understanding. In: Proceedings of the 12th International Conference on Artificial Intelligence and Education (AIED-05). Amsterdam, the Netherlands, July 2005 (2005)
Suebnukarn, S., Haddawy, P.: Modeling Individual and Collaborative Problem-Solving in Medical
Problem-Based Learning, this issue. (2006)
Vizcaino, A.: A simulated student can improve collaborative learning. Int. J. Artif. Int. Educ. 15 (2005)
3–40, IOS Press, Amsterdan (2005)
Walker, E., Koedinger, K.R., McLaren, B.M., Rummel, N.: Cognitive tutors as research platforms:
extending and established tutoring system for collaborative and metacognitive experimentation.
Accepted for presentation at the 8th international conference on intelligent tutoring systems,
Jhongli, Taiwan, 26–30 June 2006.
Weinberger, A., Fischer, F.: A framework to analyze argumentative knowledge construction in computer-supported collaborative learning. Comput. Education, 46, 71–95 (2006)
Wenger, E.: Artificial Intelligence and Tutoring Systems. Morgan Kaufmann Publishers Inc, OS Altos,
CA (1987)
Winograd, T., Flores, F.: Understanding Computers and Cognition – A new Foundation for Design.
Ablex Publishing Comp, New Jersey (1986)

Authors’ Vitae
Dr. Andreas Harrer is senior research assistant at the Department of Computer Science at Universität
Duisburg-Essen in Germany. He received his Diploma and Doctoral Title in Computer Science at
Technische Universität München. His main areas of interest are in computer-supported collaborative

User Model User-Adap Inter (2006) 16:175–209

209

learning, intelligent tutoring and learner support, and the development of educational systems from
a software engineering perspective. He is speaker of the special interest group “Artificial Intelligence
and Education” of the “Kaleidoscope” European Network of Excellence. The joint research in this
article combines these research fields and was initiated by a DFG-NSF researcher exchange program.
Dr. Bruce McLaren is a systems scientist at Carnegie Mellon University and the Pittsburgh Science of
Learning Center in the United States. Dr. McLaren is focused on developing educational technologies to support metacognitive learning skills, such as knowing how to collaborate and knowing when
to seek help. He co-manages a team of 8 programmers and research associates in the development
and enhancement of the Cognitive Tutor Authoring Tools (CTAT). Dr. McLaren holds a Ph.D. and
M.S. in Intelligent Systems from the University of Pittsburgh, an M.S. in Computer Science from the
University of Pittsburgh, and a B.S. in Computer Science (cum laude) from Millersville University.
Erin Walker is a Ph.D. candidate in Human Computer Interaction at Carnegie Mellon University. She
received her B.Sc. in Computer Science and Psychology from the University of Manitoba in 2004. Her
primary interests lie in the areas of computer-supported collaborative learning and cognitive tutoring
in ill-defined domains.
Lars Bollen holds a high school teachers degree (Master of Science equivalent) for computer science and physics from University Duisburg-Essen. In 2002, he joined the Collide research group
as a research and teaching assistant. He has been involved in various projects on the national and
international level in the field of CSCL, wireless and mobile technologies in education and collaboration analysis. His PhD thesis research area is the computer-based support for teachers by automatic
reporting facilities from collaborative learning processes, utilizing action and collaboration analysis.
Jonathan Sewall is a project director on the CTAT project at the Human-Computer Interaction Institute of Carnegie Mellon University. He holds a B.A. degree from Dartmouth College in Classics and a
Master of Computer Science from Rice University. Before coming to Carnegie Mellon he held several
positions in government and industry developing or extending systems having to do with low-level
data communications protocols, wide-area networking, distributed systems, data base management,
and planning and scheduling.

Effects of Voice-Adaptation and Social Dialogue on
Perceptions of a Robotic Learning Companion
Nichola Lubold and Erin Walker

Heather Pon-Barry

School of Computing, Informatics, and Decision Systems
Engineering
Arizona State University, Tempe, AZ, USA
nichola.lubold@asu.edu, erin.a.walker@asu.edu

Department of Computer Science
Mount Holyoke College
South Hadley, MA, USA
ponbarry@mtholyoke.edu
speaking; they may speak fast or slow, loudly or softly, high or
low. Modifying the vocal prosody of a robot can convey
emotional states such as anger, happiness, and sadness [5]. We
are interested in creating a robot that conveys social information
in its manner of speaking, adaptively, to enhance the perceived
social responsiveness. To create this voice adaptation, we focus
on a phenomenon known as acoustic-prosodic entrainment.
Acoustic-prosodic entrainment occurs when two speakers adapt
their acoustic-prosodic features including tone, intensity, and
speaking rate to mirror one another. In human-human
interaction, entrainment is correlated with social factors
including communicative success [6], conversational flow [7],
and rapport [8]. A robot that can entrain has the potential to
improve interactions by enhancing these factors.

Abstract— With a growing number of applications involving
social human-robot interactions, there is an increasingly
important role for socially responsive speech interfaces that can
effectively engage the user. For example, learning companions
provide both task-related feedback and motivational support for
students with the goal of improving learning. As a learning
companion’s ability to be socially responsive increases, so do
learning outcomes. This paper presents a socially responsive
speech interface for an embodied, robotic learning companion. We
explore two methods of social responsiveness. The first method
introduces social responses into the dialogue, while the second
method augments these responses with voice-adaptation based on
acoustic-prosodic entrainment. We evaluate the effect of a social,
voice-adaptive robotic learning companion on social variables
such as social presence and rapport, and we compare this to a
companion with only social dialogue and one with neither social
dialogue nor voice-adaptions. We contrast the effects against those
of individual factors, such as gender. We find (1) that social
presence is significantly higher with a social voice-adaptive speech
interface than with purely social dialogue, and (2) that females feel
significantly more rapport and are significantly more persistent in
interactions with a robotic learning companion than males.

We present the design and preliminary evaluation of a
socially responsive, voice-adaptive speech interface for an
embodied, robotic learning companion comprised of a LEGO®
Mindstorms® NXT robot and an iPod-Touch that displays facial
expressions and outputs speech. The primary goal of the
learning companion is to facilitate student learning by providing
both task-related feedback and motivational support. Learning
companions, based on the theory that learning is influenced by
social interactions [9], require social sensitivity to influence
students’ socio-motivational factors and increase student
learning [10]. We explore the learning companion as a robotic
teachable agent. A robotic teachable agent applies the concept
of peer tutoring within a robotic learning companion framework.
With the advantage of physical embodiment, the learning
companion as a robotic teachable agent has the potential to
create more social engagement with the activity, enhance
motivation, and promote learning. Recent work on robotic
teachable agents has shown students respond positively to these
interventions [11, 12].

Keywords—adaptive learning companion, spoken dialogue,
acoustic-prosodic entrainment, social presence, rapport

I.

INTRODUCTION

As robots become increasingly pervasive, filling every
aspect of life, at home, at work, at school, they can offer
continued and individualized support in cases where it is not
always possible to have a constant human companion. With this
growing number of applications involving social human-robot
interactions, there is a growing need for adaptive, socially
responsive speech interfaces. In human-robot interactions,
people tend to consciously treat robots as non-living mechanics
but unconsciously, they engage robots in fundamentally social
ways [1, 2]. When it comes to speech interfaces, people react
similarly, applying automatic and unconscious social responses
[3]. This work presents a socially responsive speech interface
for human-robot interaction motivated by our understanding of
how people interact in spoken human-human interactions.

To explore and evaluate the effect of voice-adaption in this
environment, we analyze the platform under three conditions: a
social condition with social dialogue content in addition to the
educational content, a voice adaptive plus social condition with
the addition of both social dialogue and voice adaptation, and a
control condition with neither social dialogue nor voice
adaptation. Prior work on entrainment and human-robot
interaction suggests males and females respond very differently
to entrainment and to robotic interventions [13]. Given this prior
work, we propose the following three research questions:

In spoken HRI interactions, one approach to enhance a
robot’s social responsiveness is to add social lexical content to a
dialogue [4]. In human-human interaction, people also
communicate social information through their manner of

978-1-4673-8370-7/16/$31.00 © 2016 IEEE

255

1.

How do social variables like social presence and
rapport differ depending on condition and gender?

2.

How does persistence in the interaction differ
depending on condition and gender?

3.

How is learning affected by condition and gender?

name. The social behaviors engaged the students; the students
who interacted with Robovie longer learned more. Breazeal [19]
increased social engagement by detecting emotion and
providing expressive, emotive responses. Tapus and Matarie
[20] demonstrate users prefer interacting with robots that display
similar personalities via dialogue and vocal adaptations. While
they did not introduce social content as we interpret it here, Lee
et al. [4] improved rapport, cooperation, and engagement with a
service robot that personalized its interactions and dialogue.
Kumar, et al. [10] found an intelligent tutoring system which
introduced text-based social dialogue by giving encouragement
and promoting cohesion increased learning gains. Bickmore and
Cassell [21] found adding social content to spoken dialogue can
have a significant impact on a user’s trust of an embodied realestate agent engaging a user in real-time dialogue. In this work,
we explore the effect of social dialogue on rapport and social
presence in spoken interactions with a robotic teachable agent,
where students are teaching the robot rather than the robot acting
as a tutor.

We explore these questions in a 3 (condition) x 2 (gender)
experiment. We find social presence differs significantly
between conditions, with the voice plus social condition
resulting in the highest average social presence. In addition,
females report significantly more rapport than males; we do not
find an effect between rapport and condition. We also find
females persisted longer in the interaction than males but we
observe no differences in learning gains by condition or gender.
In the next section, we provide background on the learning
companion as a robotic teachable agent, prior findings on
acoustic-prosodic entrainment, and relationships to social
presence and rapport. We then outline the platform introduced
for this analysis, discuss the study and procedure, and give an
overview of the results. We conclude with a discussion of the
results and directions for future work.
II.

C. Acoustic-Prosodic Entrainment
Entrainment, known also as accommodation, occurs when
dialogue partners adapt their behavior to each other during an
interaction. Entrainment can be gestural, via gaze or facial
expressions [22], word-based or lexical [23], or speech-based
[24]. Acoustic-prosodic entrainment occurs when two people
adapt their manner of speaking, such as their tone, speaking rate,
or pitch, to one another. Acoustic-prosodic entrainment is
correlated with communicative success, conversational flow,
and social factors like rapport [6, 24, 25]. Explored in-depth in
human-human conversation, entrainment has been found to be
both continuous and dynamic. Speakers will entrain over the
course of a conversation, growing more similar over time, and
they will also fluctuate in similarity dynamically within the
conversation, growing closer and then resetting. Entrainment
has been measured and analyzed both globally, at the
conversation level, and locally, at the turn level, in humanhuman corpora. In prior analysis of turn-level entrainment, we
found individuals entraining on pitch on a turn-by-turn basis had
higher measures of communicative success [26] and rapport
[25].

BACKGROUND

A. Learning Companion as a Robotic Teachable Agent
While learning companions can be implemented in various
forms, we focus on the learning companion as a robotic
teachable agent [13]. Teachable agents are based on the
observed benefits of peer tutoring [14]. When students teach
other students, they are driven to reflect and elaborate on their
knowledge and identify misconceptions [15]. Studies with
teachable agents show that students are highly motivated to
teach their agents and the agents can be highly beneficial to the
student [16].
By utilizing the advantages of the teachable agent
framework in a robotic learning environment, we can leverage
the principles of both to create more social engagement with the
activity, enhance motivation, and promote learning. This
approach is supported by prior work; for example, Saerbeck, et
al. [17] used the iCat robot to investigate how a socially
supportive robotic cat influenced the task of language learning.
Users who interacted with the socially supportive robotic cat
were more motivated and learned more than those who
interacted with a socially neutral robotic cat. Hood, et al. [11]
introduced a teachable NAO robot which children could teach
to write, and they validated this interaction paradigm for both
engaging and educating students. We believe influencing social
variables in the teachable agent framework has a high potential
for resulting in improved learning because increased feelings of
social presence and rapport will positively affect student
motivation. We expect our findings will generalize to other
learning companion environments.

Exploration of entrainment with computer systems has
shown people will entrain to a computer [27] and that
individuals prefer computer voices which are similar to their
own. For example, Nass [3] found that users who were
extroverts preferred a computer voice which displayed
extroverted speech features such as increased intensity and
speaking rate. Levitan [28] found that people unconsciously
trusted a virtual avatar which adapted to the user’s speaking rate
and intensity more than one that did not.
In this work, we provide further insight into humancomputer entrainment by exploring a voice-adaptive speech
interface similar to Levitan [28]. Instead of intensity and
speaking rate, we focus on adapting to pitch. Given analysis of
entrainment on pitch in human-human dialogues, adaptation on
pitch is likely to affect communicative success, conversational
flow, and social factors like rapport. In our prior work [29], we
found a speech interface which adapted to a user’s pitch could
achieve higher 3rd party perceptual ratings of naturalness and

B. Social Dialogue
In HRI as well as spoken dialogue research, the introduction
of social content to otherwise non-social dialogue improves the
interaction. Kanda, et al. [18] conducted a two-month trial in an
elementary school with a social robot called Robovie, who could
express various social behaviors, such as calling children by

256

robot interactions, we propose two primary sets of hypotheses,
one for gender and one for condition, for each of our research
questions, as depicted in Figure 1. Given the effects of voice
adaption and gender on rapport and social presence, we
hypothesize for research question one that the voice adaptive
plus social condition will result in the highest ratings of rapport
and social presence, followed by the social condition, and finally
the control, and that females will feel more rapport and social
presence than males. If we find our first hypotheses are
validated, then we believe based on theories of motivation that
there will be increased motivation with increased rapport [35].
This leads us to hypothesize for our second research question
that there will be greater persistence in teaching in the voice
adaptive plus social condition and females will persist in the
interaction longer. For our third research question, if we find
increased rapport, social presence, and persistence for females
and the voice adaptive plus social condition, the teachable agent
framework suggests we will also find greater learning. We
hypothesize for research question three that the voice adaptive
plus social condition will lead to greater learning, and that
females will learn more than males, since we expect them to
experience greater rapport and persist in the interaction longer.

rapport over other pitch adaptions. We also found that while the
pitch adaptation resulted in more rapport and naturalness, it was
not significantly better than normal text-to-speech output.
D. Gender
It is well established that stereotypes, especially gender
stereotypes, can play a significant role in influencing humanhuman interactions, and there is evidence to suggest this effect
applies to human-robot interactions as well. Utilizing a security
robot, Tay, et al. [30] showed users applied gender stereotypes
to a security robot, with users perceiving a security robot which
a male gender overtones more useful and acceptable.
Schermerhorn, Scheutz, and Crowell [13] found females viewed
robots as more machine-like and responded less socially. These
findings suggest we are likely to see a gender effect in our study
given the robotic interaction; however, whether we will see the
same effect is uncertain if we take into account prior work on
virtual learning companions and prior work on entrainment.
Prior work on socially responsive virtual learning companions
has shown females tend to respond more positively to the
interaction than males and that females also tend to persist in the
interaction longer for virtual learning companions [31]. These
prior works suggest we will see males and females respond
differently to a voice adaptive, robotic learning companion, but
how they will differ is a nuanced question.

III.

DESIGNING A SOCIAL VOICE-ADAPTIVE ROBOTIC
LEARNING COMPANION

Drawing on previous work involving social dialogue and
pitch-adaptations, we designed and built Quinn, a social voiceadaptive teachable robot. Quinn is a teachable robot for the math
domain; students teaching Quinn how to solve variable
equations. Quinn consists of a LEGO Mindstorms base with an
iPod mounted on top of it representing its face. Quinn’s facial
expressions are animated when speaking, and neutral otherwise.
Students interact with Quinn using spoken language and a web
application. The web application contains materials to guide the
students in their teaching of Quinn: there are six variable
equation problems (i.e. “Solve 𝑏𝑥 + 𝑔𝑦 = 14𝑏𝑦 + 6𝑥 for
𝑥”), and six quizzes. The application presents one problem at a
time and includes the worked-out steps to reach a solution. The
problems are ordered in increasing order of difficulty with
particular concepts introduced in each problem and follow-up
quiz. Students walk Quinn through the worked-out problems
using spoken language, explaining each step. Quinn responds
using spoken language. At the end of each problem, students ask

E. Hypotheses
To evaluate the effect of a social voice adaptive robotic
learning companion, we report on two social variables: rapport
and social presence. Rapport is a complex phenomenon
characteristic of many successful interactions. We base our
approach to rapport on Tickle-Degnen and Rosenthal’s [32]
theory of positive emotions, mutual attentiveness, and
coordination, and we utilize a rapport scale developed by
Gratch, et al. [33] to assess rapport. Social presence is also a
complicated social factor with multiple interpretations. We
utilize the human-computer interpretation for our work,
construing social presence as the “perceptual illusion of nonmediation.” In prior work, social presence is correlated with
increased satisfaction, enjoyment, and motivation [34],
implying that the more people feel like a mediated condition is
not mediated, the more successful the interaction can be.
With these interpretations for rapport and social presence
and the background work on entrainment, gender, and human-

Fig. 1: Research questions and hypotheses; the hypotheses from research questions one and two drive the hypothesis for research question three

RQ #1: How do social presence and
rapport differ by condition
and gender?

RQ #2: How does persistence in the
interaction differ depending
on condition and gender?

RQ #3: How is learning affected by
condition and gender?

Condition: voice adaptive plus
social will have more social
presence and more rapport than
the social and control

Condition: voice adaptive plus
social will result in greater
persistence than the social and
control

Condition: voice adaptive plus
social will result in greater
learning than the social and
control

Gender: females will feel more
social presence and more rapport
for the teachable robot than males

Gender: females will persist longer
in the interaction than do males

Gender: females will learn more
than will males

257

responses are supported by human-human dialogue analysis
which categorizes social responses as including positive
dialogue moves, such as compliments [38]. Table 1 includes
non-social response for contrast. Quinn’s social and non-social
responses were aligned to the same number of syllables as much
as possible; both versions included Quinn’s acknowledgement
of the student’s dialogue. In the social condition, Quinn selects
a social response 15–20% of the time, in line with analysis of
human-human social responses in peer tutoring and
collaborative dialogues [8, 10].

Fig. 2: The teachable robotic agent, Quinn, and a sample problem

IV.

Quinn to solve the quiz, step by step. Figure 2 shows an image
of Quinn and a sample problem.

STUDY

A. Conditions
We conducted a between subjects experiment with three
conditions: control, social, and voice-adaptive plus social,
referred to as voice plus social. In each condition, there were 16
participants: 8 females and 8 males. In the control condition,
Quinn had no social responses and no pitch adaptation. In the
social condition, Quinn introduced statements of a social nature
as described in the prior section. In the voice plus social
condition, Quinn introduced social dialogue and adapted the
pitch of its voice based on the student’s voice. The gender of the
synthesized voice was pre-set to match the gender of the
participant. Experimenter instructions and the content of the
activity were held constant for all conditions.

The spoken dialogue system consists of a speech recognition
module that utilizes the Web Speech API specification, a
pattern-matching dialogue manager written using the XMLcompliant Artificial Intelligence Markup Language [36], and a
text-to-speech synthesis module that utilizes the Microsoft
Speech API. Our voice adaptation module takes a synthesized
waveform as input and uses Praat [37] to alter the voice and
output a new waveform. An advantage of this approach is that
the voice adaptation module can be introduced independently
into other dialogue systems.
A. Voice Adaptation
We adopt a method for voice adaptation that manipulates a
single acoustic-prosodic feature—pitch. The pitch adaptation
method preserves the pitch contour of the original text-to-speech
output but shifts the pitch up or down to match the mean pitch
of the previous speaker turn. This method, described in detail in
Lubold, Pon-Barry, and Walker [27], was found to have the
highest ratings of perceived naturalness and rapport among three
different methods of automated pitch adaptations. In the voiceadaptive condition, every turn spoken by Quinn is adapted to the
mean pitch of the user using this method. In human-robot
interactions the effect of adapting on pitch mean has been less
explored. This work contributes insight into how pitch mean can
be utilized as a voice adaptation in a human-robot platform.

B. Participants
We recruited 48 undergraduate students for the experiment
(24 female, 24 male). All students were native English speakers
between ages 18 and 30 and were randomly assigned a
condition. Sessions lasted 90 minutes and students were
compensated $15 upon completion. Students sat a desk with a
Surface Pro tablet in front of them. Quinn sat on the desk next
to the Surface Pro, to the right of them. 5 participants were
excluded for scoring 100% on the pretest, and thus having too
much prior knowledge for the study. Thus, 16 students remained
in the voice plus social condition, 14 students in the social
condition, and 13 students remained in the control condition.

B. Social Responses
Quinn’s social dialogue responses are motivated by the
social interaction strategy proposed by Kumar et al. [10] based
on Bales’ socio-emotional interaction categories [38]. There are
three main categories: showing solidarity, showing tension
release, and agreeing. Examples of social responses Quinn
might give in each category are given in Table 1. These

C. Study Design & Procedures
Students began the study by completing a 10 minute pretest.
Next, each student was given a practice exercise which
contained two worked-out examples of variable equation
problems. The student was asked to explain the problems and
the steps described out loud. After this practice exercise, the
students watched a 4-minute video introducing Quinn and

TABLE 1: CATEGORIES AND DESCRIPTIONS FOR SOCIAL RESPONSES; EXAMPLES OF SOCIAL AND NON-SOCIAL RESPONSES

Category

Description

Solidarity

Compliments

Tension
Release
Agreeing

Social Response

Non-social Response

Ok so we add x. You’re a really great
teacher!

Ok so we add x. I get that we are adding x here.

Being cheerful

Ok so we add x. I’m so happy to be
working with you

Ok so we add x. It makes sense that we would
add x here.

Off-topic

Ok so we add x. Do you like math?

Ok so we add x. I get adding.

Comprehension

I hear what you’re saying. You’re saying
we add x.

We add x. It makes sense that we would add x
here.

258

describing the task. Students were told they should help Quinn
learn how to solve variable equations by walking Quinn through
the six example problems and quizzing Quinn after each
problem to assess Quinn’s knowledge. Students were also
informed they have the option to re-teach Quinn if Quinn
struggles on a quiz. Students worked with Quinn through all six
problems and quizzes. They then completed the post-test, which
took around 10 minutes. They were then given a questionnaire
assessing their motivation during the study and attitudes towards
Quinn. Given time, they were asked some final interview
questions. In total, the study took 90 minutes.

V.

A. Social Presence and Rapport
We compare a social voice-adaptive robotic learning
companion (condition = voice + social) to a social (condition =
social) and to a non-social, non-voice adaptive (condition =
control) robotic learning companion. Our first research question,
introduced in the Introduction, was: “How do social variables
like social presence and rapport differ depending on condition
and gender?” We answer this question with an initial two-way
MANOVA examining social presence and rapport as dependent
variables and gender and condition as independent variables.
The means and standard deviations by gender and condition are
in Table 2. The MANOVA analysis reveals a significant
multivariate main effect for condition, Wilks’ λ = .80, F = 4.41,
p = .02, partial eta squared = .197 and a significant multivariate
main effect for gender, Wilks’ λ = .77, F = 2.54, p = .04, partial
eta squared = .124. However, the interaction between condition
and gender is not significant, Wilks’ λ = .85, F = 1.52, p = .21,
partial eta squared = .124. Given the significance of the
multivariate main effects, we examine univariate main effects
for condition and gender on social presence and rapport. We also
report the univariate results of the interactions.

D. Measures
For measuring rapport and social presence, the follow-up
questionnaire adopted nine Likert-scale questions from prior
literature to assess rapport [33] and eight Likert-scale questions
for social presence (8 questions). The social presence questions
were adopted from the attentional allocation portion of the
Networked Minds Social Presence Inventory [39]. Attentional
allocation is a critical element of social presence [40] and is the
most applicable within our robotic teachable agent scenario. We
average the rapport and social presence questions to create three
representative constructs with an acceptable internal reliability
(Cronbach’s 𝛼 ≥ 0.70).

Univariate analyses for the effect of condition indicate
significant differences related to social presence, F(2, 42) = 4.0,
p = .02. The η2 effect size is 0.17, meaning the condition
explained 14% of the total variability in social presence scores,
which is large effect by conventional standards (Cohen 1988).
Analyzing the pairwise differences for condition on social
presence, significant pairwise differences are found between the
voice plus social condition and the social condition. The voice
plus social condition results in significantly higher ratings of
social presence than the social condition (p = 0.02), but the voice
plus social and control are not significantly different. We do not
see an effect of condition on rapport, F(2, 42) = .16, p = .86.

We assess a measure regarding persistence in the interaction
by collecting the number of times a student retaught Quinn.
Quinn was pre-programmed to get the wrong answer on two of
the quizzes. This re-teaching metric was calculated as the total
number of times the student retaught Quinn, with four possible
values observed: 0, 1, 2, or 3.
Learning gains were assessed with the pretest and posttest
scores. We computed normalized learning gains according to
[41] using (1) to account for prior knowledge. If the posttest
scores were lower than the pretest scores, we used (2).

While this result partially validates our hypothesis (as the
voice plus social condition has higher social presence than the
social condition), we expected the social condition to score
higher. One possibility is the percentage of social turns within
Quinn’s dialogue moderated the effect on social presence.
However, we did not find a significant effect comparing the
percentage of social turns in the two social conditions, F(2, 42)
= 2.51, p = 0.09.

𝑔𝑎𝑖𝑛 = (𝑝𝑜𝑠𝑡𝑡𝑒𝑠𝑡 − 𝑝𝑟𝑒𝑡𝑒𝑠𝑡)⁄(1 − 𝑝𝑟𝑒𝑡𝑒𝑠𝑡) (1)
𝑔𝑎𝑖𝑛 = (𝑝𝑜𝑠𝑡𝑡𝑒𝑠𝑡 − 𝑝𝑟𝑒𝑡𝑒𝑠𝑡)⁄𝑝𝑟𝑒𝑡𝑒𝑠𝑡

RESULTS

(2)

After removing the five participants who scored 100% on
the pretest, we found of the 43 participants remaining, 23 hit a
ceiling on their learning gains (scoring 100% on the posttest).
With 10 individuals at zero gain, 10 individuals who gained in
a normal distribution, and 23 hitting full gain, we determined
analysis would be better served by grouping the learners into
three groups – no gain, some gain and all gain. The results on
learning gains are analyzed in this context.

Univariate analyses for gender reveal significant differences
between males and females in regards to rapport, F(2, 42) = 8.86,
p = 0.006. The η2 effect size is 0.18, meaning gender explains

TABLE 2. MEANS AND STANDARD DEVIATIONS FOR GENDER AND CONDITION ON SOCIAL PRESENCE (LIKERT SCALE 1 – 7), RAPPORT (LIKERT SCALE 1 – 7),
PERSISTENCE (0 – 3), AND LEARNING GAINS (0 – 1). * INDICATES SIGNIFICANCE AT P < 0.05, ** INDICATES SIGNIFICANCE AT P < 0.01

Social Presence

Rapport

Persistence

Learning Gain

M

SD

M

SD

M

SD

M

SD

5.54

.67

5.04

.84

1.6

1.1

.81

.33

Social

4.90

.74

5.21

.84

1.1

1.2

.50

.54

Voice + Social

5.57*

.75

5.30

1.1

1.4

1.2

.56

.45

Males

5.18

.79

4.70

.97

1.0

1.1

.53

.48

Females

5.55

.70

5.60**

.71

1.7*

1.1

.71

.43

Control

259

18% of the total variability in the degree of rapport, which is
large effect by conventional standards [42]. Analyzing the
differences between the genders, females feel more rapport for
Quinn than males, with the total average rapport score of
females, ignoring condition, equal to 5.59 and males equal to
4.78. In regards to social presence, the difference between males
and females approaches significance, F(2, 42) = 3.76, p = 0.06.

Given the significance of re-teaching in relation to gender,
we then explore whether re-teaching is related to learning. We
run Pearson’s chi-squared correlation on the categorical learning
gains described above. We find that there is a significant
correlation between re-teaching and the categorical learning
gains, with Χ 2 (6) =17.9, p = 0.006.
We also assess social presence and rapport in terms of
learning. Running a multinomial regression with rapport and
social presence, we find the model is not significant, Χ 2 (4)
=4.68, p = 0.32. However, in viewing the individual coefficients,
social presence is approaching a significant effect on learning.
For those individuals who gained but did not hit ceiling on their
gain, social presence is 1.38 times higher than for those
individuals who did not gain, p = 0.06.

This result, while partially validating our hypothesis, is also
somewhat surprising given prior work has found females often
do not respond warmly to human-robot interactions [13], liking
the robot less than their male counterparts. One possibility for
the difference in our result may be the larger number of females
we have from technical majors, including engineering and math.
Their technical background and experience may make them
more inclined to feel more rapport for Quinn. We run a followup 2-way ANOVA with major and gender as independent
variables and rapport as a dependent variable. However, we find
no significant relationship between how males and females feel
about Quinn and their majors (p = 0.16), and there is no
significant relationship between major and rapport (p = 0.33).

VI.

DISCUSSION

We find our two hypotheses regarding research question one
regarding how social presence and rapport are effected by
condition and gender partially validated. The social voiceadaptive robotic learning companion has higher social presence
than the social condition and females feel more rapport in
general. We do not see an effect of condition on rapport and we
find the effect of gender on social presence merely suggestive.

Finally, univariate analyses of the interaction between
condition and gender on rapport is not significant, F(2, 42) = .18,
p = .84, as we would expect. However, the interaction between
condition and gender on social presence is approaching
significance, F(2, 42) = 3.02, p = 0.06. We examined the
differences in social presence among the conditions separately
for males and females. The male simple effect test indicated
statistically significant differences among the means, F(2, 37) =
6.73, p = 0.003, η2 = .27, whereas the female simple effect test
was non-significant, F(2, 37) = .31, p = .74, η2 = .02. Simple
pairwise comparisons among the male means indicated the
social condition differed from both the voice plus social (p =
0.001) and the control (p = 0.01) conditions. This indicates
males may be the driving force behind the differences we see
between the conditions on social presence.

We explore one potential explanation for these results in the
potential speech recognition errors made by the dialogue system.
The Web Speech API we utilized for speech recognition uses
Google’s voice recognition service, which has been publicized
as having an error rate of only 8%. To analyze the effect of
speech recognition errors on rapport and social presence, we
focus on the output of the dialogue manager (DM). The DM
selected responses based on pattern matching, keywords, and
context [35]. If the DM could not match the student’s words to
a particular pattern or response, the DM would return one of two
types of responses, either a request for clarification (i.e. “can you
please repeat that?”) or a general acknowledgement (i.e. “ok
sounds good”). Classifying the number of generic responses
Quinn returned when Quinn could not match an exact pattern to
a precise response, we ran an ANCOVA with gender and
condition as independent variables and social presence and
rapport as covariates, with the percentage of turns where Quinn
requested clarification or gave a general acknowledgement as
the dependent variable. We found this did not have a statistically
significant effect on the differences reported by gender and
condition on social presence and rapport, with F=1.1, p=0.41.

B. Persistence
We utilize the re-teaching metric described earlier to answer
our second research question: “How does persistence in the
interaction differ depending on condition and gender?” The
means and standard deviations for persistence by gender and
condition are shown in Table 2. We utilize multinomial logistic
regression to estimate the influence of condition and gender on
persistence in the interaction, given that we measure persistence
in terms of total re-teaching. In our analysis, the overall model
including both condition and gender was not significant, Χ 2 (9)
=12.35, p = 0.19. Looking at the predictors individually, gender
is significant when controlling for condition. The likelihood of
a female persisting in the interaction and re-teaching Quinn was
2.13 times more likely than a male, p = 0.03.

To gain some qualitative insight into the results, we explore
the interview responses collected as a part of the experimental
procedure. Participants were interviewed as time allowed,
resulting in a total of 20 interviews. The interviews were
approximately distributed across gender (11 female, 9 male) and
condition (6 control, 6 social, 8 voice adaptive plus social).

C. Learning Gains
Having grouped the students into three learning groups, we
analyze the learning gains in terms of a multinomial logistic
regression. However, even with this adjustment, the overall
model in the analysis including both condition and gender is not
significant, Χ 2 (6) =6.86, p = 0.33, and we find that none of the
individual predictors are significant.

Analyzing social presence, we are surprised the social
condition scored lower than both the control and the voice
adaptive plus social condition. Given prior work on social
dialogue, we presumed the social condition would have a
positive effect when compared to the control. Turning to the
interview data, we asked participants how they felt about
Quinn’s responses. We find individuals in different conditions
responded with very different views. Participants in the voice

260

because we cannot assume that males and females will respond
similarly simply because it is a social mechanism.

plus social condition said they “liked Quinn’s responses”, Quinn
responded like “a normal every day person,” and “sometimes
Quinn was kind of sassy!” Participants in the social condition
tended to feel Quinn was less focused, “not on the same page,”
“didn’t really seem to listen,” and Quinn’s responses
“sometimes felt like they came from out of nowhere.” The
interview responses from those in the control condition, who
heard no social dialogue, suggest the absence of social dialogue
led to different expectations of Quinn – Quinn was “a robot so
of course it’s not going to respond like a human would.”

In reviewing persistence in terms of persistence, we are
disappointed to find condition does not appear to be having an
effect but given rapport is also not significant by condition, this
is not surprising. We do find females, who report significantly
higher measures of rapport, are also persisting in the interaction
significantly more. This finding supports learning theories on
motivation which suggest rapport can have a positive effect on
motivation [43]. In reviewing the interview data from those
females who retaught Quinn, many gave motivational reasons in
line prior work on teachable agents for why they persisted in
teaching Quinn. For example, one interviewee responded “I felt
responsible for Quinn failing the quiz” and another replied she
retaught Quinn because she “wanted Quinn to succeed and I felt
like it was my fault she wasn’t.” These responses validate
persistence as an intrinsic measure of motivation. Examining
these responses in association with females who also reported
higher rapport, we find that those who said Quinn was now their
friend also commented about being motivated to reteach Quinn,
further supporting the relationship between rapport and
motivation. There did not appear to a major difference between
males and females who did not reteach Quinn.

Comparing these responses, there are several possible
theories as to why those in the social condition may have felt
differently about Quinn’s responses. One possibility is the pitch
adaptation is counter-balancing the adverse effect of the social
dialogue when the social dialogue is presented without nonverbal cues. Prior work suggests users are sensitive to social
dialogue timing and non-verbal cues which accompany social
dialogue, such as facial expressions. When non-verbal cues do
not accompany social dialogue, there can be a mismatch in
expectations leading to dichotomous results. Another possibility
is that individuals are responding differently as a result of the
different social responses Quinn is capable of giving.
In assessing rapport, females felt significantly more rapport
towards the robotic learning companion than males. We review
the interview responses of the males and females to identify any
relevant clues as to how or why females and males are reacting
differently. In the interviews when asked how they would
describe Quinn, three of the females described Quinn as “so
cute” or “very cute.” The males described Quinn literally, as a
robot made of Lego Mindstorms. When asked how they felt
about Quinn, seven of the females replied “yeah I really like
Quinn” or “I liked Quinn!” When asked why they liked Quinn,
females would explain with “we’re best friends now,” “I feel
like Quinn is now my friend,” or “teaching her felt like a
connection.” Four of the male interviewees, when asked how
they felt about Quinn, also replied with “I liked Quinn.” When
asked why, they explained they liked Quinn because he was an
“interesting robot,” “decently complex,” or a “quick learner.”

VII. CONCLUSIONS
As a part of this work, we introduced a platform for
performing voice adaptation and we explored the effect of
adapting to one promising acoustic-prosodic feature, pitch,
incorporating analyses of both gender and contextual social
dialogue into our exploration. We find the voice adaptation with
the addition of social dialogue is significantly higher than pure
social dialogue alone. We also find females react with more
rapport to the interaction and are more persistent in teaching the
robotic learning companion. Interestingly, males have a lower
social response to Quinn, and this is validated by interview
responses. We are limited in our results regarding learning
gains; due to a large number of participants earning 100% on the
posttest; we fail to detect effects of learning. However, we
believe the nature of the interactions can still provide interesting
insights. To further understand the potential design
repercussions, future work includes a deeper process analysis of
the social and voice adaptive conditions to assess the types of
social responses Quinn is evincing and how students are
responding to Quinn’s social responses. In addition, our
implementation of pitch adaptation is naïve – we entrain to every
turn of the user to the absolute mean of the user every time,
which is not realistically representative of the fine-grained and
nuanced phenomenon in human-human conversations. Recent
analysis of human-human data is providing ideas for how we
might operationalize entrainment in future work [43]. We plan
on exploring more nuanced forms of pitch adaptation as well as
other acoustic-prosodic features for voice adaptation, including
intensity, speaking rate, and vocal quality.

These responses give some insight into the differences we
are observing between males and females. The females appear
to be suspending disbelief more readily and easily than the
males. Aligned with the statistical findings on rapport, this
suggests females are viewing Quinn as more of a social
embodied companion than as a robot. Taking into account
background work with virtual learning companions showing
that females respond with more rapport in these types of
domains, these findings suggest domain may play an important
role in how participants of different genders will respond
socially in robotic interactions. These responses also help
illuminate possible reasons for why males are viewing Quinn as
significantly less socially present in the social condition. While
the interaction between gender and condition was not
significant, the male gender differs significantly on measures of
social presence in the social and voice plus social conditions. If
males are viewing Quinn from a more technical, robotic
viewpoint, they may perceive Quinn’s social prompts
differently. This suggests when designing social robots,
awareness of gender is vital in how we apply social mechanisms

ACKNOWLEDGEMENTS
This work is supported in part by the Ira A. Fulton Schools of
Engineering through a Dean's Fellowship and the Google Anita
Borg Memorial Scholarship. We would also like to thank Tyler
Robbins for his help in the data collection process.

261

[22] Lakin, J. L., Jefferis, V. E., Cheng, C. M., & Chartrand, T. L. (2003). The
chameleon effect as social glue: Evidence for the evolutionary
significance of nonconscious mimicry. Journal of nonverbal behavior,
27(3), 145-162.
[23] Friedberg, H., Litman, D., & Paletz, S. B. (2012). Lexical entrainment and
success in student engineering groups. In Spoken Language Technology
Workshop (SLT), 2012 IEEE (pp. 404-409). IEEE.
[24] Reitter, D., Keller, F., & Moore, J. D. (2011). A computational cognitive
model of syntactic priming. Cognitive science, 35(4), 587-637.
[25] Lubold, N., & Pon-Barry, H. (2014). Acoustic-Prosodic Entrainment and
Rapport in Collaborative Learning Dialogues. In Proceedings of the 2014
ACM workshop on Multimodal Learning Analytics Workshop and Grand
Challenge (pp. 5-12). ACM.
[26] Borrie, S. A., Lubold, N., & Pon-Barry, H. (2015). Disordered speech
disrupts conversational entrainment: a study of acoustic-prosodic
entrainment and communicative success in populations with
communication challenges. Frontiers in Psychology, 6, 1187.
[27] Coulston, R., Oviatt, S., & Darves, C. (2002). Amplitude convergence in
children’s conversational speech with animated personas. In Proceedings
of the 7th International Conference on SLP (pp. 2689-2692).
[28] Levitan, R. (2013). Entrainment in Spoken Dialogue Systems: Adopting,
Predicting and Influencing User Behavior. In HLT-NAACL (pp. 84-90).
[29] Lubold, N., Pon-Barry, H., & Walker, E. (2015). Naturalness and Rapport
in a Pitch-Adaptive Learning Companion. In Automatic Speech
Recognition and Understanding (ASRU), 2015 IEEE Workshop. IEEE.
[30] Tay, B. T. C., Park, T., Jung, Y., Tan, Y. K., & Wong, A. H. Y. (2013).
When stereotypes meet robots: The effect of gender stereotypes on
people’s acceptance of a security robot. In Engineering psychology and
cognitive ergonomics. Understanding human cognition (pp. 261-270).
Springer Berlin Heidelberg.
[31] Burleson, W., & Picard, R. W. (2007). Gender-specific approaches to
developing emotionally intelligent learning companions. Intelligent
Systems, IEEE, 22(4), 62-69.
[32] Tickle-Degnen, L., & Rosenthal, R. (1990). The nature of rapport and its
nonverbal correlates. Psychological inquiry, 1(4), 285-293.
[33] Gratch, J., Wang, N., Gerten, J., Fast, E., & Duffy, R. (2007). Creating
rapport with virtual agents. In Intelligent Virtual Agents (pp. 125-138).
Springer Berlin Heidelberg.
[34] Heerink, M., Ben, K., Evers, V., & Wielinga, B. (2008). The influence of
social presence on acceptance of a companion robot by older people.
Journal of Physical Agents, 2(2), 33-40.
[35] Murphy, E., & Rodríguez-Manzanares, M. A. (2009). Teachers’
perspectives on motivation in high-school distance education.
International Journal of E-Learning & Distance Education, 23(3), 1-24.
[36] Wallace, R. (2003). The elements of AIML style. Alice AI Foundation.
[37] Boersma, Paul & Weenink, David. Praat: doing phonetics by computer
[Computer program]. V. 5.4.12, retrieved 07/10/15 http://www.praat.org/
[38] Bales, R. F. (1950). Interaction process analysis; a method for the study
of small groups. Cambridge: Addison-Wesley.
[39] Biocca, F., & Harms, C. (2002). Defining and measuring social presence:
Contribution to the networked minds theory and measure. Proceedings of
PRESENCE, 2002, 1-36.
[40] Baron-Cohen, S., & Swettenham, J. (1996). The relationship between
SAM and ToMM: Two hypotheses. In Theories of theories of mind (pp.
158-169). New York: Cambridge University Press.
[41] Hake, R. R. (2002). Relationship of individual student normalized
learning gains in mechanics with gender, high-school physics, and pretest
scores on mathematics and spatial visualization. In submitted to the
Physics Education Research Conference, Boise, ID.
[42] Cohen, J. (1988). Statistical power analysis: A computer program.
Routledge.
[43] R. Levitan, S. Benus, A. Gravano, & J. Hirschberg. (2015). “Entrainment
and turn-taking in human-human dialogue.” In AAAI Spring Symposium
on Turn-Taking and Coordination in Human-Machine Interaction.

REFERENCES
[1]
[2]

[3]
[4]

[5]

[6]

[7]
[8]

[9]
[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]
[20]

[21]

Fong, T., Nourbakhsh, I., & Dautenhahn, K. (2003). A survey of socially
interactive robots. Robotics and Autonomous Systems, 42(3), 143-166.
Leyzberg, D., Avrunin, E., Liu, J., & Scassellati, B. (2011). Robots that
express emotion elicit better human teaching. In Proceedings of the 6th
International Conference on Human-Robot Interaction (pp. 347-354).
Nass, C. I., & Brave, S. (2005). Wired for Speech: How voice activates
and advances the human-computer relationship. Cambridge: MIT press.
Lee, M. K., Forlizzi, J., Kiesler, S., Rybski, P., Antanitis, J., & Savetsila,
S. (2012, March). Personalization in HRI: A longitudinal field
experiment. In Human-Robot Interaction (HRI), 2012 7th ACM/IEEE
International Conference on (pp. 319-326). IEEE.
Crumpton, J & Bethel, C. (2015). Validation of vocal prosody
modifications to communicate emotion in robot speech. In International
Conference on Collaboration Technologies and Systems (pp.39-46).
Borrie, S. A., & Liss, J. M. (2014). Rhythm as a coordinating device:
entrainment with disordered speech. Journal of Speech, Language, and
Hearing Research, 57(3), 815-824.
Porzel, R., Scheffler, A., & Malaka, R. (2006, January). How entrainment
increases dialogical effectiveness. In Proceedings of the IUI (Vol. 6).
Lubold, N., & Pon-Barry, H. (2014, November). Acoustic-Prosodic
Entrainment and Rapport in Collaborative Learning Dialogues. In
Proceedings of the 2014 ACM workshop on Multimodal Learning
Analytics Workshop and Grand Challenge (pp. 5-12). ACM.
Vygotsky, L. S. (1978). Mind and Society: the Development of Higher
Mental Processes,(edited by Cole, M., et. al).
Kumar, R., Ai, H., Beuth, J. L., & Rosé, C. P. (2010). Socially capable
conversational tutors can be effective in collaborative learning situations.
In Intelligent tutoring systems (pp. 156-164). Springer Berlin Heidelberg.
Hood, D., Lemaignan, S., & Dillenbourg, P. (2015). When children teach
a robot to write: An autonomous teachable humanoid which uses
simulated handwriting. In Proceedings of the Tenth Annual ACM/IEEE
International Conference on Human-Robot Interaction (pp. 83-90).
Tanaka, F., & Matsuzoe, S. (2012). Children teach a care-receiving robot
to promote their learning: Field experiments in a classroom for vocabulary
learning. Journal of Human-Robot Interaction, 1(1).
Schermerhorn, P., Scheutz, M., & Crowell, C. R. (2008). Robot social
presence and gender: Do females view robots differently than males?. In
Proceedings of the 3rd ACM/IEEE international conference on Human
robot interaction (pp. 263-270). ACM.
Ploetzner, R., Dillenbourg, P., Preier, M., & Traum, D. (1999). Learning
by explaining to oneself and to others. Collaborative learning: Cognitive
and computational approaches, 103-121.
Roscoe, R. D., & Chi, M. T. (2007). Understanding tutor learning:
Knowledge-building and knowledge-telling in peer tutors’ explanations
and questions. Review of Educational Research, 77(4), 534-574.
Biswas, G., Leelawong, K., Schwartz, D., Vye, N., & The Teachable
Agents Group at Vanderbilt. (2005). Learning by teaching: A new agent
paradigm for educational software. Applied Artificial Intelligence, 19(34), 363-392.
Saerbeck, M., Schut, T., Bartneck, C., & Janse, M. D. (2010). Expressive
robots in education: varying the degree of social supportive behavior of a
robotic tutor. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems (pp. 1613-1622). ACM.
Kanda, T., Sato, R., Saiwaki, N., & Ishiguro, H. (2007). A two-month
field trial in an elementary school for long-term human–robot interaction.
Robotics, IEEE Transactions on, 23(5), 962-971.
Breazeal, C. (2003). Emotion and sociable humanoid robots.
International Journal of Human-Computer Studies, 59(1), 119-155.
Tapus, A., & Mataric, M. J. (2008). Socially Assistive Robots: The Link
between Personality, Empathy, Physiological Signals, and Task
Performance. In AAAI Spring Symposium: Emotion, Personality, and
Social Behavior (pp. 133-140).
Bickmore, T., & Cassell, J. (2005). Social dialongue with embodied
conversational agents. In Advances in natural multimodal dialogue
systems (pp. 23-54). Springer Netherlands.

262

	

			
		
	

	








 
 !



"#
!$%!&%'"('

!
)%!$$"#
!$)*
&*
&+
$
,
-		

&
./!
0
/!$
1)
&




 	&

-

!$!
2 3(4!

!
)


&

)!)
 3(

!






!
#
3'
&23'4 3()
  !  
&  
  
  !
  56  !
!    
#
  
  
  )  
  


      
!
  
  '  !
!  

    


!#
$

!	



!!$
7
!

8
$



!

		
&  &#
  #
!  -  

      &
    
))
#
  -$  )

! !
 	-

 3
 
 &
1!#  #
 &$    3(  !  
))
#
  &  !
!
!)	-

-
&#
#
!
!


)
8!
)  

  	-

  &      -$!  #
  !))
  
  )  !
!  9  -	  

!
&#

- 3(!

)
-	-
)-!
!




)
) 3(!
&$&#
&#

-
#




!-

7)
7! 3($
8


&

))
#
)-
!!:(
!-#
!

 3(
$#



)-
#
-	!
 3(	
!
)
!!



!
!

!!


)
)
 3(&

!



  !  


!    )  !
!  -

  
  )
11)
  '  #
  
&#
-$&


!!$)!
!$-
!3

)

)!!
!

&!



-

7)
7! 3(: 

#
3'
&
!


)
)
 3(!


&
-


!
!
&-
#
!)

&	
!
!!
$&
3

))
#

!!)!

!
-
-

-#
#
!

		

		





!)
#!!
!!)

!!
#
 %


!&

!-&

))
#
-
!
!
7&
&
#!'!	!
)8
!!


#
&

7!!!

7!!#
$#
&



-



2
&&  3
  ;  *  <<=4(
! 
 ) &
!) 
$

 
 )

2*>>   
$;  ?
))  <<4!	!&
8
 2*! 
  6654
#


!-
&

7! 2())
;'
!<<@4!-!(->


-A$

3'+1A266=4
)


!
!)

!!


7

&

)!)
&$
:!
!	

!!&$)
)
!

	-




!!
!
#
!		
B(#
)7C)
8	
B7E
&$FC(
!-!
!:
!
&!
 
	



!
#


-


)
&$!#&
!!

3'3
$!

8!#



8
#


)

&	
)
#

)
3'-

$	
!	
3
$!!	)
)
3'$
'!
$!#

&

$
#
)

&	

!!
!	

-!&!

!

!	!
#
-


!
!?
 

	
!
!

!
1

!)!
&
!&
))


!	!&


!



!%



!
!#

!
&
!!
!#


!
!
!

)
%

!

751

CSCL 2007

 !






G!! &!#

&

!
#
!  !
 
$
#

)
-!-
!&
	


)
#
3
$
	




G!!-H!
#
!)


G!!	&!3

!!
	-



!!	8
!!!#

7!


-#
!
7

!
!
!


*!    
  
  !
  -
  #
  !
!  8
!!    

  
  )  
  &#



!  )    !  -
  !  
  #
  
!  2
  B'    8
!  !  !
)    !	!  -$
!
!
--
)
&
-!!#

-$!8
!
!	&
!
$!
I
!JC4(
-
#
!
!


)

8
!!)

$H!)!
&
2
B-!
&
!8
!!	
&$



J )


K!	$8
!!--!8
!
L!
#
!	
JC4







-

7)


&#
)
-	


#
3'
&23'4

	


-!
-!
!


!
3'
)

&$)-

&!
!2
	4
-!
!

  
  
  !  
  3'  
)
    -

  #
  
    &#
  !

!&


#!2

4
$
!>



-  
!
  !
  
    &  !  &  #  !
!    !  -




))
!)


(

3&
)
!)

7



3
!!
!!!

-
!

&


!!
!
@8
!!-
)
!
!


8
!!
%!-

561!!
!)-)!1$

&!!
!#
!!!
!-

&$
!


?

!#

!!)!
!
!

!!  !  ))

  
#
!  -
  !
    &
-

1!!    3
  !!  -  
  !
!  -!  !!
    
  E
)
    9$  M  !  
    
!
!)
!$2

!
)

!
!4:!
#


!
#

E
)
")
$

-

!)&
-

1
!!))


!:!
!
E
)
-

-	!)$-


#
3'
&
!$2!F"@5"N(?!F=ON*24
F@F64
3&
7



?))


!&
-

!

&$




	

	

			








%

!
9#
#
%

%!


6
=
M6

1

!	-


1#
#
-)
)

1!
!!#

&
!
$-
&


5

&
%!

%!
!

=6

1!
!



6

1!
!	-



1

!	-


1#
#
-)
)

1!
!!#

&
!
$-
&

				
	
1!
!


			
	
1!
!	-



5



!



!!!
!!=!



-1-$27

!1
4  


1
!
  '
9A'  -  
!1
  !  
  


  
!
  %!
!  !
!  -


!)$


!!
!&

E
)
2*24F
==P66QRF6=N4&

-

!)))


!&
-

!

2!

3&
43)

7
-

&!
-



!)!!&!
#?

!
!




7&$
)
!#
&#
&
#!-
-




-!#

&

!-

-	-

!!
)1
)
-
#
-
&!
#




752

CSCL 2007

!!
#


!-!-
!



$
-

&!
*!

$)
!
!-
!-

$
	-

7&
!
!
$

!!!
!#
&
'!
!

!
!	
&
!-


$3!
!&
&
#))

&
-


-
!2!

3&
4(
!



&
!!
!

E
)





&
!!-
3
#

&
)
&
!

&$$!
E
)
-!-0!
!!	
#

 )
! 

!
&

 N
 #

 

(
!


!	&
!
$!#


!!
  N6S  )  
  &
!  
$  

  

$  &
)
  !	    &
  !
!  -


$!

&$!#
B G	--!
B
	)#
BT!!
 G

!
C )!
!!	&
!
$$
-!#

))&
!-
#
)
$

$&
!
$$&
!))
$
7!


!	!##

-&
#
)
-

!!


3&
'

&
!
)
-!


3
3E
)


	

	


5
<


M=@
M@



=M
=5



5@
6

	




M
@MO
=@
5

	




@M
=5
MM
6@<

		
'!
!

!
!)


-
)
-
!



-! !
$
!
!))$)-


!
))
#
$(
!



!	!&
!
$!#
-
!
!
E
)



)
-
&
!!
!
 
!
&
)&
!!
!

&

$

-
&!#
!

&
!
!
!-&

#

)$!

!	!
8
&$))

&
!'#
)

&	
!-

!

))
#
$
$


!

	
!-!+(->?

-A$

;3'+1A 266=4
&$
:'
-

)
!)-

	
	

	5N5U5<
*>>T
$(;?
))'2<<4))
!)




!    !  H!
:  '  
  $!! 
 	  	 
 	


	 !25455155<
*!(*!?%


9-
;(

26654
1
!
!K
&
!#-!
)1



!

!
		
	

	"2456N15=
  '  ())
  '  ;  '
!  '  2<<@4    

  :  ))
!  )  !  

!))



		
	

	#5M1=
	





#
3!!
!
%)!:
7
!&!
3($!
)&#

#
7



		
	 
	
	$		

	%	&'T
3-T
N15666N

&&
3
T?;*2<<=4!#
#$
&#
!
!
		
	

	 (M6N1M5

!	
	
3!
!
!!
&$
%!&(

)



(*+V65=MM6

753

CSCL 2007

User Model User-Adap Inter (2009) 19:387–431
DOI 10.1007/s11257-009-9069-1
ORIGINAL PAPER

CTRL: A research framework for providing adaptive
collaborative learning support
Erin Walker · Nikol Rummel ·
Kenneth R. Koedinger

Received: 5 August 2008 / Accepted in revised form: 2 September 2009
© Springer Science+Business Media B.V. 2009

Abstract There is evidence suggesting that providing adaptive assistance to
collaborative interactions might be a good way of improving the effectiveness of collaborative activities. In this paper, we introduce the Collaborative Tutoring Research
Lab (CTRL), a research-oriented framework for adaptive collaborative learning support that enables researchers to combine different types of adaptive support, particularly by using domain-specific models as input to domain-general components in order
to create more complex tutoring functionality. Additionally, the framework allows
researchers to implement comparison conditions by making it easier to vary single
factors of the adaptive intervention. We evaluated CTRL by designing adaptive and
fixed support for a peer tutoring setting, and instantiating the framework using those
two collaborative scenarios and an individual tutoring scenario. As part of the implementation, we integrated pre-existing components from the Cognitive Tutor Algebra
(CTA) with custom-built components. The three conditions were then compared in a
controlled classroom study, and the results helped us to contribute to learning sciences
research in peer tutoring. CTRL can be generalized to other collaborative scenarios,
but the ease of implementation relates to the complexity of the existing components
used. CTRL as a framework has yielded a full implementation of an adaptive support
system and a controlled evaluation in the classroom.

E. Walker (B) · K. R. Koedinger
Human-Computer Interaction Institute, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh,
PA 15213, USA
e-mail: erinwalk@andrew.cmu.edu
K. R. Koedinger
e-mail: koedinger@cmu.edu
N. Rummel
Institute of Psychology, University of Freiburg, Engelbergerstr. 41, 79085 Freiburg, Germany
e-mail: rummel@psychologie.uni-freiburg.de

123

388

E. Walker et al.

Keywords Adaptive collaborative learning support · Intelligent collaborative
learning systems · Cognitive tutoring systems · Collaboration modeling ·
Peer tutoring · Classroom evaluation

1 Introduction
Over the past 30 years there has been an evolution in research on how students learn
by collaborating, depicted in Fig. 1 (Dillenbourg et al. 1995). In the conditions stage,
early work compared the effects of collaborative and individual activities, or looked at
how the conditions of collaboration related to learning and attitudinal outcomes (see
Slavin 1996, for a review). However, to better understand the effects of collaboration,
it is important to model collaborative interactions and relate them to outcomes (the
interactions stage). As it grew apparent that students often do not exhibit beneficial
collaborative behaviors spontaneously, it further became relevant to determine how to
support collaboration in order to produce the desired interactions, which would then
hopefully lead to the desired learning outcomes (Strijbos et al. 2004). Much current
collaborative learning research is situated in this fixed support stage, which focuses
on the effects of giving students fixed assistance, including declarative instruction
on how to collaborate (e.g., Saab et al. 2007), examples of good collaboration (e.g.,
Rummel and Spada 2005), and collaboration scripts that provide students with designated roles and activities as they work together (e.g., Fischer et al. 2007). However,
fixed scripts may provide students with too much structure and extraneous collaborative load, particularly for students who are capable of regulating their own learning
(Dillenbourg 2002). On the other hand, pre-collaboration training may provide too
little support for students during the actual collaboration, where students may not
follow the activity as designed (e.g., Ritter et al. 2002). Adaptive support might be a
better way of targeting the individual needs of students (Soller et al. 2005; Kumar et al.
2007). Therefore, there has been a movement toward developing adaptive assistance
for collaboration, where collaborative interactions are modeled as they occur, and the
results of the analysis determine the content of the assistance given (adaptive support
stage).
Although there are many potential learning sciences research questions surrounding the adaptive support of collaboration, this support has proven to be challenging to implement, and evaluations of adaptive support compared to fixed support
have been promising but rare. The problem of delivering adaptive assistance to collaboration can be considered an instantiation of a more general assistance dilemma
(Koedinger and Aleven 2007), where in order to discover how best to deliver assistance to optimize student learning, one must manipulate the amount, type, and timing
of help provided to students. In the case of collaborative learning, there are several
levels on which assistance can be delivered, ranging from assistance on domain skills
to assistance on elaborated verbal interactions. In cases where assistance on multiple
levels might be appropriate at a single time, how best to integrate the different levels is an open question. Non-technological implementations of adaptive support for
collaboration require an experimenter or a teacher to interact with each collaborating group (e.g., Tsovaltzi et al. 2008; Hmelo-Silver 2004; Gweon et al. 2006). This

123

Collaborative Tutoring Research Lab

389

Fig. 1 Four stages of research on collaborative learning. Researchers began by examining how the collaborative setting and positive collaborative behaviors lead to learning. They then explored how to support
collaboration to encourage productive interactions. More recently, the effects of adaptive support have been
investigated

wizard of oz methodology is impractical for large-scale research, let alone classroom
deployment, and creates uncertainty as to whether different facilitators have different
effects. Instead, it may be advantageous to examine the effects of computer-delivered
adaptive support at different and possibly interacting levels. In these settings, task and
language interactions can be automatically collected, guided, and used as input to a
system that delivers adaptive feedback. Unfortunately, such systems take a long time
to develop because of the difficulty of constructing accurate collaborative models and
the challenges of having the system provide non-disruptive feedback to collaborating
students. Further, the effects of adaptive feedback provided by these systems on collaborative interactions and learning outcomes have rarely been evaluated in large-scale
controlled studies, despite the fact that the evaluations that have been conducted have
had promising results. For example, Kumar et al. (2007) found that adaptive support
to collaborating pairs was better than no support to collaborating pairs and adaptive
support to individual learning. In general the adaptive collaborative learning systems
that have been developed have been research prototypes, which mainly demonstrate
how to construct such systems. This agenda for development makes it difficult to adapt
them to create relevant control conditions, deploy them in classroom environments,
or iterate upon them in future studies. Removing some of the technical obstacles to

123

390

E. Walker et al.

implementing adaptive assistance and relevant comparison conditions may encourage
the use of such systems to address educational psychology research questions.
In this paper, we introduce the Collaborative Tutoring Research Lab (CTRL), a
research-oriented framework for adaptive collaborative learning support that facilitates the collection of multiple streams of process data, the development and integration of assistance based on the data, and the implementation of relevant comparison
conditions for experimental control. In the construction of CTRL, we have adopted
ideas from an individual learning perspective on delivering adaptive instruction: cognitive tutors. Cognitive tutors are computer-based instructional systems that compare
student actions to a model of correct and incorrect problem-solving and provide targeted feedback to students when needed. They have been successful at increasing
learning in individual settings (Koedinger et al. 1997) and have evolved from acting as isolated interventions to serving as research platforms. For example, Project
LISTEN’s Reading Tutor supports the incremental addition and evaluation of features, and the collection of rich log data that can later be mined to provide insight into
student learning processes (Beck et al. 2004). CTRL extends the individual tutoring
scenario (one student, one tutor) to a collaborative multi-tutor setting (multiple students and multiple tutors, with different roles or for different purposes). One of the
strengths of our framework is that it focuses on reusability: it facilitates the addition,
removal, and integration of components. In CTRL, adaptive collaborative conditions
can be developed more rapidly by using existing computational models, and comparison conditions can be created by removing particular components of the adaptive
system. We have used CTRL to create an adaptive support condition for a peer tutoring
activity that integrates domain and collaboration assistance, and evaluated the adaptive
collaboration support condition in a controlled classroom study by comparing it to a
fixed collaboration support condition and an individual learning condition. All three
study conditions were implemented following the framework, and the results of the
study increase understanding of the effects of adaptive support on peer tutoring. In this
paper, we review other adaptive collaborative learning systems in Sect. 2. In Sect. 3,
we describe CTRL, and in Sect. 4, we describe the implementation and evaluation of a
specific adaptive collaborative learning system using CTRL. To conclude in Sect. 5, we
outline the scope of CTRL, demonstrating how it can be used to implement other adaptive collaboration scripts that involve more participants, more balanced collaborative
roles, and more complex adaptive tutoring.

2 Background
In this section, we survey related work on adaptive collaborative learning support
(ACLS). The types of systems of primary interest to us are coaching systems, as defined
by Soller et al. (2005) in their review of collaboration support systems. Coaching systems help students who are engaged in computer-mediated collaboration by assessing
the current state of student interaction, comparing the current state to a desired state,
and then offering assistance to the students. Coaching systems have a lot in common
with intelligent tutoring systems, which also support students using the three phases of
assessment, comparison, and assistance, but focus on individual learning. Moreover,

123

Collaborative Tutoring Research Lab

391

intelligent tutoring systems, and cognitive tutors in particular, have moved away from
merely being interventions and toward serving as research platforms to answer learning sciences questions about the effects of adaptive assistance. Our goal is to develop a
similar research platform for ACLS. To this end, we focus our review in this section on
ACLS systems that have been implemented and evaluated. Further, we examine how
cognitive tutor principles and architectures for individual learning might contribute to
our goal.

2.1 Student interactions in adaptive collaborative learning support systems
ACLS systems support both collaborative task actions and computer-mediated conversation (see Table 1 for a summary of interactions enabled by ACLS systems). Often,
student interactions are structured either using micro-scripts, which operate on an
action-by-action basis, or macro-scripts, which operate on the level of phases of activity (see Dillenbourg and Hong 2008, for further discussion). In our work, we are
interested in micro-scripts, or structuring interactions within a phase of collaborative
activity. ACLS systems tend to include a shared workspace where students can work
together toward a domain goal. Micro-scripts are often applied to these shared workspaces by giving students different roles in the workspace or by allowing them only
to act at particular times. For example, as summarized in Table 1, COLER contains
a shared workspace where students can collaboratively construct entity-relationship
diagrams by interacting with coupled nodes and edges (Constantino-González et al.
2003). Students have to indicate their intention to draw in the workspace, and when
one student is drawing the other students cannot. Learning systems that have a shared
workspace also often include a private workspace that contains no coupled objects, so
that students can do individual work. The other primary component of many implemented ACLS systems is a text-based tool that allows students to communicate with
each other in natural language. Within these tools, micro-scripts are often applied
through the use of sentence-starters that students select to begin their utterance (e.g.,
“I would like to explain that…”) or classifiers that student select after typing their utterance (e.g., “Give an Explanation”). As described in Table 1, Group Leader currently
has 46 sentence openers that represent 10 subskills students should be exhibiting while
collaborating, such as “Task Leadership” (Israel and Aiken 2007). Finally, interfaces
may contain widgets such as buttons through which the students can get information
from the intelligent system. For instance, students can request four different types of
help from HabiPro: clues to the solution, a worked example of the current problem,
a worked example to a different problem, and the solution to the problem (Vizcaíno
et al. 2000). Assuming that most current ACLS systems are logging all the actions
that they enable, the systems capture collaborative task actions, verbal interactions,
and meta-interactions that arise as a result of following micro- and macro-interaction
scripts.
The interactions in an ACLS system can be viewed through the lens of “making thinking visible”, which is a principle employed in cognitive tutor development
(Koedinger and Corbett 2006). In cognitive tutors, students are asked to perform several steps to complete each problem-solving task. These steps can be considered as

123

123
Chat counts, keywords

Count dialogue acts, keywords, sequences
of disagreement
Solution quality, individual expertise, help
type requested, chat counts, keywords
Length of time to complete a step, chat counts,
solution
Logical conflict between student utterances

IP, RI, DM, DL

Solution structure, individual contributions,
solution quality
Action counts, action sequences, student
expertise, solution quality
Chat counts, keywords in chat, parsing of chat

TO, RI

RC

RI, DL

IP, TO, RI, DL

RI, DM, RC

RI, TO, DL

IP, RI

IP, RI

Goals

Solution structure, individual contributions

Assessment method

The tutoring goals described are information pooling (IP), reciprocal interaction (RI), dialogue management (DM), task orientation (TO), reaching consensus (RC), and
domain learning (DL)

OXEnTCHÊ (Vieira et al. 2004)

MArCo (Tedesco 2003)

LeCS (Rosatelli and Self 2004)

Editing computer programs using chat, shared
workspace
Case study (phases) in chat (sentence starters),
shared text editor, solution representation
Graphical planning in shared workspace, chat
(dialogue games)
Chat (sentence starters)

Modeling, shared & private workspace, chat
(classifiers)
Modeling, shared & private workspace
(phases), chat (classifiers)
Medical problem-based learning in shared
workspace, chat (unstructured)
Shared workspace (different phases),
unstructured chat
Programming with chat (sentence starters)

COLER (Constantino-González
et al. 2003)
COLLECT-UML
(Baghaei et al. 2007)
COMET (Suebnukarn
and Haddawy 2004)
CycleTalk (Kumar et al. 2007)

Group Leader (Israel and
Aiken 2007)
HabiPro (Vizcaíno et al. 2000)

Task elements

System

Table 1 Tasks and assistance provided in several ACLS systems

392
E. Walker et al.

Collaborative Tutoring Research Lab

393

subgoals in the problem-solving process. When the steps are explicitly represented in
the interface, the subgoals become more salient to students, increasing their learning.
In turn, when students take action in order to meet the subgoals, an adaptive system
gains more insight into students’ cognitive processes than it would if students were
simply providing the answer to the problem. For example, in the PACT geometry tutor
students are asked to solve geometry problems and explain their steps using a menubased interface (Aleven and Koedinger 2002). Self-explanation is both beneficial for
students and helpful for the cognitive tutor in identifying the source of student error.
Scripts imposed on collaborative learning activities can function similarly: in addition to informing students about the communication acts expected, adding sentence
starters to an interface can make a student’s communication intention visible (e.g.,
Israel and Aiken 2007). Private and shared workspaces can make the discrepancy
between an individual’s private reasoning and their group contributions visible (e.g.,
Constantino-González et al. 2003), and thus provide input to an adaptive system.

2.2 Modeling and feedback in adaptive collaborative learning support systems
Current ACLS systems assess collaboration based on targeted aspects of student interactions, compare the assessment to ideal collaborative qualities, and then provide
feedback based on the comparison (see Table 1 for an overview). In many ways, these
ACLS systems are very different from each other. Feedback policies with respect to
both collaboration and domain feedback varies; some feedback is triggered by user
actions (Tedesco 2003), some is triggered by user inaction (Constantino-González
et al. 2003), some is provided on demand (Vizcaíno et al. 2000), and some is only
provided when a user submits a solution (Baghaei et al. 2007). The representation
of ideal student performance also varies between systems, ranging from finite state
machines (Israel and Aiken 2007) to decision trees (Constantino-González et al. 2003)
to constraints (Baghaei et al. 2007). Despite these differences, ACLS systems have
broad commonalities with respect to collaborative skills targeted and how the skills are
assessed in the context of the system. In fact, the types of support provided by ACLS
can be described using a collaboration analysis scheme developed by Meier and colleagues (Meier et al. 2007), where student interaction is rated on 9 dimensions. Some
systems attempt to improve student interaction on Meier and colleagues’ dimension
of information pooling (IP), i.e. how much students share their knowledge with their
groupmates (Constantino-González et al. 2003; Baghaei et al. 2007). As represented
in Table 1, assessment on this dimension is drawn from workspace actions: Student
actions in a public workspace are compared to their actions in a private workspace in
order to evaluate how much of their individual actions they are sharing with the group.
Some systems instead support Meier and colleagues’ dimension of dialogue management (DM), or how students execute conversational acts. Assessment in this area is
based on chat actions; sentence classifiers are used to count utterances of particular
types or even create a model of student dialogue acts and compare it to a sequence
of ideal dialogue acts. Then, drawing from earlier analysis systems such as EPSILON
(Soller 2004), the ACLS system can give feedback to students based on their contributions (e.g., Israel and Aiken 2007). Some of the systems described in Table 1 help

123

394

E. Walker et al.

students in reaching consensus (RC; encouraging students to engage in productive
conflict) by detecting and responding to loops of disagreement. There is also a growing trend toward using machine learning to classify student utterances instead of (or
in addition to) sentence starters, with some success (e.g., Kumar et al. 2007). These
efforts have mostly focuses on task orientation (TO), making sure students stay on
topic. Up until now, we have discussed supporting either workspace actions or chat
actions. Even systems that use metrics of assessment that might apply to both types of
interactions often focus their analysis on either one. For example, a common dimension targeted for assistance is reciprocal interaction (RI), or whether everyone in a
collaborative group is participating. Systems track actions in the shared workspace
(e.g., Constantino-González et al. 2003), chat contributions (e.g., Vieira et al. 2004),
or the length of time since students have contributed last (Rosatelli and Self 2004) in
order to assess this dimension. However, systems do not generally use all three metrics
at once. In addition to providing collaboration feedback on aspects of student interaction, some systems also provide task-related feedback that targets domain learning
(DL). This feedback is generally provided in a manner similar to individual learning
systems. For example, Cycle-Talk (Kumar et al. 2007) engages collaborating students
in tutorial dialogues that are identical to those used for individual learners.
The different models and feedback in a given ACLS system are not often integrated,
pointing toward an opportunity for the advancement of the systems. In particular, different types of collaborative and domain feedback are often kept separate by design,
with each type of feedback appearing to students at different times during the collaboration. For example, GroupLeader (Israel and Aiken 2007) has three types of feedback:
get back on topic, incorporate a single idea per post, or re-evaluate a conflict. In the
system, there is never a case where it is appropriate for the two types of feedback to
be given at the same time, avoiding the issue of how to decide between multiple feedback options. Although this configuration is a good initial policy, as systems begin
to provide more comprehensive support to student collaboration, keeping the feedback separate in this way will not scale. Furthermore, the models and assessment
mechanisms underlying the feedback are often kept separate within ACLS systems.
COLER (Constantino-González et al. 2003), for example, counts workspace actions to
assess individual contributions but ignores chat actions. This separation might make
it difficult to get the full picture of when feedback should be provided. More notably,
task-related models are rarely used to augment collaboration models, even when it
would make sense to do so. COLLECT-UML (Baghaei et al. 2007) provides students
with both task-related feedback on the quality of their group solution and prompts to
contribute elements from their individual solutions to their group solution. However,
the system does not provide information on whether the elements students have not
shared with the group are correct or incorrect. This knowledge would augment the
system’s capabilities to provide relevant feedback: The system could suggest that students only share the correct elements with their group, or even suggest that students
ask their group why an element in their individual solution is incorrect. One system
that does integrate domain and collaboration information is COMET (Suebnukarn and
Haddawy 2004), where the next participant in a collaborative dialogue is selected
based in part on which student has the domain expertise to make a contribution. The
effect of this assistance on users has not been explored. One next step in ACLS design

123

Collaborative Tutoring Research Lab

395

is to provide more complex assistance by integrating different types of models of
interaction and different types of feedback. In particular, integrating domain and collaboration models might have large benefits in providing interaction support that is
sensitive to the problem-solving context.
Research on cognitive tutors (and intelligent tutoring systems more generally) has
recently begun to explore the integration of different forms of assistance, in particular
augmenting task-related feedback with metacognitive feedback. There has been growing recognition that the limitations of intelligent tutoring systems might be addressed
by providing students with metacognitive instruction, with the goal to enable them to
regulate their own learning (Azevedo 2005). One example of an existing metacognitive
tutor is iSTART, a tutor for helping students to acquire reading comprehension strategies (McNamara et al. 2007). iSTART asks students to explain a text to themselves
as they read it, and then provides them with feedback on the type and quality of their
explanations. In some tutoring systems, not only is metacognitive support provided,
but cognitive and metacognitive tutoring are integrated. Output from a domain-specific
cognitive model serves as input to a domain-general metacognitive model, resulting
in more effective metacognitive model and better integrated feedback. One example
of a tutor which uses this technique is the Help Tutor, which is a meta-cognitive tutor
for help-seeking that is designed as a domain-independent addition to any cognitive
tutor (Aleven et al. 2006). The Help Tutor uses both student actions and information
from the cognitive tutor to evaluate student help-seeking while problem-solving. For
example, a student that attempts a problem-solving step (student action) with too low
of a skill assessment for that step (cognitive tutor assessment) has committed a helpseeking error. Only one type of feedback is given at a time; if both the Help Tutor and
the regular cognitive tutor have feedback to give to the student, the feedback source is
chosen based on the type and correctness of the student action. Other researchers have
explored similar methods of augmenting an intelligent tutoring system with agents
that improve student motivation (Del Solato and du Boulay 1995), discourage students from gaming the system (Baker et al. 2006), or facilitate learning by teaching
(Biswas et al. 2005). As collaboration can be thought of as a collection of metacognitive skills, the techniques for integrating metacognitive and cognitive tutoring could
potentially be leveraged to combine collaborative with cognitive tutoring.

2.3 Implementation of adaptive collaborative learning support systems
Many coaching systems (Soller et al. 2005) use a component-based architecture, which
can enable the easy modification of an existing system and the reuse of system modules in novel configurations. In component-based architectures, software is divided
into abstract components that can be specified to suit the developer’s needs and that
can be flexibly integrated with other components using a standard framework (Krueger 1992). At a minimum, the way a system is divided into components has an impact
on reuse, because each component can be enhanced or replaced without having to
modify the other components. As ACLS systems are distributed applications with
multiple users, one common implementation of these systems follows a client-server
architecture, with an interface client provided for each student and a central server

123

396

E. Walker et al.

containing multiple components responsible for managing the collaborative sessions
(e.g., Baghaei et al. 2007; Tedesco 2003; Vizcaíno et al. 2000). Collaboration between
interface clients is often facilitated using a “what you see is what I see” policy, where
objects are coupled in shared workspaces so that an action taken on a coupled object
in one user’s client is broadcasted to the parallel objects in collaborators’ interfaces
(Suthers 2001). Similarly, text-based interaction tends to follow a traditional instant
messaging format, where everything a user submits as an utterance is seen by their
partners (e.g., Vieira et al. 2004). The tutoring functionality of these systems is then
generally located on the server. Many systems subdivide the tutoring module into different components, and although the components are named differently across systems,
the underlying purpose is parallel across systems. ACLS systems generally include
an expert model, which compares student actions to an ideal model of collaboration,
and a feedback model, which contains the logic for how feedback should be delivered
to students (e.g., Kumar et al. 2007; Israel and Aiken 2007; Tedesco 2003). The two
components handle all types of support the system offers. For instance, in the case
of COMET, they would support both information pooling and reciprocal interaction
(Suebnukarn and Haddawy 2004). One or more translator components are sometimes
also included to convert the low-level user actions into high-level representations of
their collaboration that can be input to the expert model (e.g., Kumar et al. 2007;
Israel and Aiken 2007; Vieira et al. 2004). A variation of this approach to developing a
tutoring module is to include both individual expert models and a group expert model
on the server, with the group model being either a parameterization of the individual models (Hoppe 1995) or containing its own specifications for good collaboration
(Baghaei et al. 2007).
Based on this description of components, the reuse facilitated primarily involves
the ability to modify one aspect of tutor functionality without altering other aspects
of tutoring functionality. For example, Kumar et al. (2007) discuss how their expert
model, translator, and feedback model are separate from each other, such that each
component then can be iteratively improved without altering any others. However,
another way to facilitate reuse is by adding new components directly to existing configurations: In COLLECT-UML (Baghaei et al. 2007), group modeling components
are added to augment the individual modeling components already present. Once an
integration framework has been developed for the components, they can be more easily substituted for one another or combined in novel ways. For instance, Mühlenbrock
and colleagues have created an integration framework where individual user interfaces
register with the DALIS server, which then invokes a pre-specified set of support agents
(Mühlenbrock et al. 1998). Essentially, the DALIS server acts as the facilitator in a
federated system (Genesereth 1997). Similarly, LeCS treats tutors as clients, with a
central facilitator managing the interaction between tutor clients and interface clients,
although with no explicit integration framework (Rosatelli and Self 2004). Although
the described designs for reuse can make it easier to increase the sophistication of
a single type of adaptive support, they do not necessarily facilitate the integration
of multiple types of adaptive support and the efficient implementation of comparison conditions. Few ACLS systems specifically include multiple tutor components
which each provide a different level of tutoring. One exception is COLER, which
includes three expert model components: a “Participation Monitor”, a “Difference

123

Collaborative Tutoring Research Lab

397

Recognizer”, and a “Diagram Analyzer” (Constantino-González et al. 2003). This
division of tutoring components by functionality can make it easier to incrementally
add tutoring complexity by integrating multiple tutoring components, particularly if
there is a framework in place so that new tutoring models can be integrated with
existing tutoring models.
Cognitive tutor architectures are structured so that custom-built interface and tutor
components can be integrated with existing components. This type of reusability can
be found in Ritter and Koedinger (1996) component-based framework for facilitating
the development of intelligent tutoring systems. Framework components are divided
into tools and tutors, and a standard protocol for interchanging messages is defined
to make it easier to swap different components in and out. So that off-the-shelf components can be used, the framework also includes a translator component to convert
messages sent from the off-the-shelf components into the standard format, and convert messages sent to the component into a format that it understands. Although Ritter
and Koedinger (1996) demonstrated how the framework could be used with two separate tutoring applications, their emphasis was on the use of off-the-shelf applications
for individual tutoring, rather than on the addition of metacognitive or collaborative
components. However, further iterations of the cognitive tutor (e.g., the Help Tutor)
have experimented with using a similar framework to add metacognitive tutoring; the
Help Tutor module was added to the traditional cognitive tutor, and feedback from
the two tutor modules were integrated as necessary (Aleven et al. 2006). Allowing
multiple tutors, and providing an integration framework for the tutors, might allow us
to provide more complex tutoring to collaborating students.

2.4 Evaluation of adaptive collaborative learning support systems
Much of the evaluation of ACLS systems has been conducted on the technological
aspects rather than on the effects of the assistance on student interactions and learning
outcomes. See Table 2 for a summary of the evaluations that have been conducted
on ACLS systems. In some cases, a technological evaluation meant evaluating the
effectiveness of the collaborative assessment. For example, Mühlenbrock (2004) in
evaluating CARDDALIS described how well the model represented the student interactions. In other cases, it meant evaluating the predictive power of the models used.
COMET used kappa to demonstrate the relationship between expert-constructed group
solutions and system-predicted group solutions, with positive results (Suebnukarn and
Haddawy 2004). Finally, sometimes feedback itself was evaluated. For an evaluation
of COLER, 73% of the advice the system provided to collaborative students was rated
as “worth saying” by an expert. Research that has not focused directly on validating
the system technology has tended to fall under the category of design-related and
usability studies rather than controlled experiments. To inform the development of
the adaptive component of LeCS, data from dyads interacting using the LeCS interface were collected and analyzed (Rosatelli and Self 2004), and after OXenTCHÊ had
been implemented, the usability and the benefits of the assistance were rated by student
users (Vieira et al. 2004). The few full studies that have been conducted using adaptive
systems have been promising. As described in Table 2, to evaluate COLLECT-UML,

123

398

E. Walker et al.

Table 2 Evaluations of ACLS support
System

Evaluation purpose

Evaluation specifics

COLER
(Constantino-González
et al. 2003)
COLLECT-UML
(Baghaei et al.
2007)
COMET
(Suebnukarn
and Haddawy
2004)
CycleTalk (Kumar
et al. 2007)

Feedback validation

Expert ratings of system support, comparison
of expert & system support

Controlled experiment

2 conditions (adaptive collaboration support vs.
no collaboration support), classroom study,
effects on learning and interactions
Predict individual & group solution paths

GroupLeader (Israel and
Aiken 2007; McManus
and Aiken 1996)
HabiPro (Vizcaíno
et al. 2000)
LeCS (Rosatelli
and Self 2004)
MArCO (Tedesco
2003)
OXEnTCHÊ
(Vieira et al.
2004)

Model validation,
usability study

Model validation

Controlled experiment

Model validation
Design-related study
Usability study
Usability study

2 (collaborative, individual) × 3(adaptive,
static, no support) design, classroom study,
effects on learning & interactions
Assess student dialogue acts, single-condition
evaluation of the effects of the system on
learning
Assess need for assistance, off-topic behaviors,
& passivity
Students use a non-adaptive system to inform
design
Students use adaptive and non-adaptive
versions of the system to explore its effects
Usability, student ratings of system assistance

Evaluations range from technical validations of the models behind the systems, usability studies of interactions within the system, and controlled experiments to evaluate student learning

Baghaei et al. (2007) compared an adaptive collaboration support condition to a no
support condition and found that while there were no differences in domain learning
gains, the experimental condition gained more collaborative knowledge. Even more
encouraging was the study conducted by Kumar et al. (2007), which manipulated two
variables: adaptive versus fixed support, and collaborative versus individual learning.
They found that the adaptivity and collaboration interacted to produce a significantly
higher learning result compared to the other conditions. As the technical merits of the
reviewed systems have been established, a logical next step will be to investigate their
potential learning benefits.
In addition to taking principles from intelligent tutor design, building components
on top of an existing tutoring system might accelerate the evaluation process. There
are several obstacles to conducting controlled experiments with adaptive collaborative
learning systems. Large amounts of data are often required to develop the assessment
components of the systems, but the data can be difficult to collect. After expending the
effort it takes to build an adaptive collaborative system, it can be too time-consuming
to build appropriate control conditions for evaluation. Finally, once appropriately calibrated conditions exist, it can be difficult to find enough participants for the study, and
even more difficult to conduct the study in an ecologically valid setting. As intelligent
tutoring systems are older than ACLS, there exists more infrastructure surrounding

123

Collaborative Tutoring Research Lab

399

these systems that can facilitate evaluation studies. The Cognitive Tutor Algebra, for
example, can be found in thousands of schools across the US, and therefore vast
amounts of data are logged every day (Carnegie Learning 2009). Tutor data is often
mined in service of investigating learning science hypotheses and ultimately informing
the improvement of intelligent tutoring systems (Beck et al. 2004). Similarly, it has
become common practice to perform embedded experiments, making small modifications to already deployed tutoring systems (Mostow and Aist 2001). Finally, because
the tutors are so widespread, there are well-established relationships with schools that
can be leveraged to gain access to classrooms and ecologically valid participants.
Taking advantage of these relationships is a main goal of the Pittsburgh Science of
Learning Center, which connects researchers and classrooms, and instruments classrooms so that it is easier to collect data and evaluate learning interventions (PSLC
2009). Developing ACLS systems on top of existing intelligent tutoring systems holds
great promise both in making such systems more available and in using them as a
platform for research on users’ interactions, collaborative learning, and methods of
adaptive support.
2.5 Outlook
Our goal is to build a framework for ACLS that facilitates the representation of rich
interactions, the integration of different tutoring types, and the efficient creation of
valid comparison conditions for controlled studies. Up to this point, ACLS systems
have done a very good job at focusing their support at separate types of interaction,
but have generally not integrated support based on different streams of input. The
architectures that have been developed to make ACLS systems easier to implement
have not emphasized the use of pre-existing tutoring modules as input to custombuilt models, which would increase the potential ability of the tutoring system to
provide assistance. Further, facilitating this integration would make it easier to combine domain-specific task models with domain-general models of good collaboration,
enabling context-sensitive collaborative tutoring across multiple tasks. These architectures have also not explicitly facilitated the creation of comparison conditions, which
would increase the effectiveness of the empirical evaluation of the system in order to
investigate learning sciences research questions. We see an opportunity here to develop
a framework for ACLS that facilitates controlled research of different types of adaptive support, and for this purpose we introduce the Collaborative Tutoring Research
Lab (CTRL). CTRL focuses directly on the interaction between collaborating students
and intelligent support, and would therefore ideally be used in combination with other
approaches. For example, the tool-level integration provided by Freestyler (Hoppe
and Gaßner 2002) or CoolModes (Pinkwart 2003) would be a good complement for
the tutor-level integration we facilitate. Additionally, CTRL would be a good fit as part
of a higher-level integration platform such as SAIL (Berge and Slotta 2007), which
facilitates the authoring, deployment, and assessment of learning activities. The distinct contribution of CTRL is the establishment of an integration framework for preexisting and custom-built components to provide adaptive tutoring to collaborating
students.

123

400

E. Walker et al.

Fig. 2 High level overview of CTRL. CTRL consists of tool, tutor, and translator agents, learner and
research management data stores, and a central control module

3 The Collaborative Tutoring Research Lab (CTRL)
The Collaborative Tutoring Research Lab (CTRL) provides a flexible integration
mechanism for independent components to form an adaptive collaborative learning
support (ACLS) environment. Using the framework, the feedback from different tutors
can be combined, meaning that students can receive complex tutoring based on multiple streams of process data. New tutor components can capitalize on existing tutor
models, increasing the meta-cognitive tutoring possible. For example, a meta-tutor
for sharing information with a teammate would be able to use results provided by a
domain tutor about whether the facts shared were correct. CTRL facilitates the addition
and removal of components in order to create appropriate comparison conditions for
adaptive support. In this section, we outline the basic components involved, the way
they interact with each other, and the way they can be integrated. The actual design,
implementation, and evaluation of a peer tutoring scenario using CTRL is described
in Sect. 4.
A high-level overview of our framework is depicted in Fig. 2. CTRL consists of six
different types of components, based in part on Ritter and Koedinger (1996) description of plug-in tool and tutor components:

123

Collaborative Tutoring Research Lab

1.
2.
3.
4.
5.
6.

401

Tools: Used by the student to take problem-solving actions
Tutors: Provide students with assistance
Translators: Facilitate inter-component communication and the implementation
of collaboration scripts
Learner Management: Stores curriculum information and student model data
Research Management: Stores protocol logs and information about how the components involved can be integrated with each other (session types)
Control Module: Constructs and manages collaborative sessions, both on a problem-to-problem level (session manager) and on an action-to-action level (mediator)

The focus of CTRL is on facilitating interactions between tool, tutor, and translator
components, and we define and discuss each of those components in more detail in
Sect. 3.1. In Sect. 3.2, we describe how the various components communicate with each
other. Sect. 3.3 outlines how the control module interacts with the research management store to allow the flexible integration of components and construction of multiple
collaborative conditions. Learner management is not further described because it is
outside the current scope of our architecture.
3.1 Component functionality
A tool is a piece of software that a student interacts with in order to solve problems
in a particular domain. A tool could be as simple as a text-editor that allows students
to write essays or as complex as a simulation environment for chemistry experiments.
CTRL allows any number of tools to be involved in the learning scenario. Multiple
users can collaborate remotely while each one uses different tool components. There
is not necessarily a one-to-one mapping between students and tools; a single student
could have access to multiple tools (e.g., an instant messaging tool in addition to the
text-editor), and two students could conceivably be using the same tool at the same
computer. However, we assume for the purposes of this discussion that in a condition with multiple users, a tool represents a single user’s interaction with the system
as a whole. Tool components contain the user interface, a domain model, and metaknowledge of tutoring. The interface is the point of interaction between the user and
the system. The domain model is present so that the tool can update its state without input from an additional component. A user can then interact with a tool without
input from any tutoring component, and therefore a tool is not bound to a given tutor.
For example, in a chemistry simulation environment, the interface might allow students to mix different chemicals, and the domain model might calculate and display
the result of mixing the chemicals. Although tools should be able to share domain
models, this behavior is currently not explicitly supported by CTRL, in part because
of our focus on using pre-existing components that already have a domain model.
Tools also contain meta-knowledge so that they can convert feedback from a tutor
agent into a format appropriate for display. Thus, tutors can be used with any tool
because they do not need to send tool-specific messages. When the chemistry simulation tool mentioned above receives a hint message from the tutor, it might display it
in a pop-up dialogue in the interface, while if a collaborative discussion tool receives

123

402

E. Walker et al.

the same message, it might display it as part of the chat interaction. The functionality that we have described is ideal, but it is likely that many pre-developed tools
we may want to use will not incorporate all functionality, and may be difficult to
modify. In these cases, we use translator components to compensate for the missing
functionality.
Translator components are all-purpose facilitators that bridge communication
between other components. They have two general functions. First, they make it
possible to integrate components that do not conform exactly to the framework specification by providing missing functionality (e.g., an implementation of tutoring
meta-knowledge) or by converting individual component messages into the standard
message format. For example, if a particular collaborative discussion tool does not
know how to handle a hint message, a translator would need to be built to convert the abstract message (e.g., giveHint[hint]) into a format the tool understands
(e.g., displayInChat[hint]). This aspect of translator functionality is very much in
line with the translators discussed in Ritter and Koedinger (1996) and Kumar et al.
(2007). Second, translators can impose a structure on the collaborative interaction
by communicating certain actions across tool components (such that a user action
on one component is displayed on all other relevant components) and by triggering
changes related to collaboration scripts to the tool components. For example, a translator could be used to allow some actions made by one student to appear on the other
student’s screen, but not others. This approach, where translators facilitate collaboration, is different from the more traditional object coupling approach in CSCL systems
(Suthers 2001), where students can automatically see all actions made in a shared
workspace. There may be cases during a student interaction where actions that would
generally be collaborative should not be shared (e.g., when one collaborating student makes an error, it may not always be desirable to broadcast the error to group
members). We chose this implementation so that a designer of a learning environment has more control over structuring the interaction between students. Like tools
and tutors, there can be any number of translator components incorporated in a learning scenario. The specific implementation of a given translator would depend on its
function.
Tutor components are any components that provide adaptive support to students,
generally by comparing their actions to a model, providing assistance based on the
model, and assessing skills based on the model. Tutors might range from a domain
tutor for writing grammatical sentences based on a constraint-based model to a metacognitive tutor for proofreading a paper based on a cognitive model. Any number of
tutors can be involved in a learning scenario, and any type of tutor can be used in our
framework. Tutor components should contain an expert model, a feedback model, and
a student model. Like in regular intelligent tutoring system functionality (as described
in VanLehn 2006), the expert model evaluates the student action, the feedback model
determines the sort of feedback that is given, and the student model assesses the
student performance (or in some cases, the group performance). As with tools, any
preexisting tutor components used that do not have the desired functionality can be
augmented with a translator component.

123

Collaborative Tutoring Research Lab

403

3.2 Message protocol
All components communicate with each other using a standardized set of messages,
providing guidelines for the development of new components that can be incorporated into the framework (see Table 3). As components may be running on different
machines, messages are sent remotely. In these messages, details specific to the implementation of individual components are hidden as much as possible and only abstract
semantic content is communicated. In this paragraph, we will enumerate the high-level
representations that form the parameters and return values of the messages sent, and
in the following paragraph we will discuss the types of messages themselves. First, a
Student Interaction, or a step that can be taken by a user in the interface, is represented
using four parameters:
1.
2.
3.
4.

Student—the student taking the action
Selection—the widget being acted upon
Action—the action performed upon the widget
Input—any additional information necessary for the action

For example, a student with an id of jmiller entering 25 in a table might be represented
as (student = jmiller, selection = cell A1, action = enterValue, input = 25). The
concept of a selection-action-input triple can be traced back to Anderson and Pelletier
(1991). A Tutor Response to a student interaction is represented by four parameters:
1.
2.
3.
4.

Tutor—the tutor sending the message
Action Evaluation—the type of message (e.g., correct, incorrect, highlight)
Feedback Message—any message the tutor wants to send
Skill Assessment—the change in student skill values

For example, a domain tutor might approve the student action in cell A1 (indicating
it was correct), send a feedback message for encouragement (e.g., “Keep it up! What
goes in cell A2?”), and increase the value of the relevant skill (e.g., set the skill “entering values in a table” to 60%). As described in Table 3, information that is not a Student
Interaction or Tutor Response (such as current problem details) is communicated as
a set of Properties, which is a conventional data structure containing any number of
attribute-value pairs.
These data structures are then used as parameters and return values for the message
types exchanged between components (see Table 3). For example, when a session is
started a getData message would be used to retrieve relevant curriculum and student
information, and launchComponent messages would be used to start and configure
all the relevant components. While elements of this message protocol are taken from
Ritter and Koedinger (1996), the protocol is more abstract than the protocol that they
defined, in order to facilitate a variety of potential learning environment interactions.
Because the problem-solving interactions are the core messages of CTRL, here we
present an in-depth example of how those messages might be used by the different
components (see Fig. 3). The example includes two tools (representing two collaborating students, Bo and Jan), two tutors (representing a domain and collaborative
tutor), one translator to implement the shared collaborative workspace, one research
management component, and the mediator subsection of the control model. In the

123

404

E. Walker et al.

Table 3 Messages passed between components
Message name

Input

launchComponent Component
properties
quitComponent None

Output

Sending components Receiving
components

Success or failure Session Manager

Tool, Tutor, Translator

Success or failure Session Manager

Tool, Tutor, Translator

Problemselection
properties
changeProblem Problem
properties
processInteraction Interaction

Problem
properties

Session Manager

Tool, Tutor, Translator

Success or failure Session Manager

Tool, Tutor, Translator

None

Tool, Translator

Tutor

scriptInteraction

None

Translator, Tutor

Tool

None

Tutor, Translator

Tool

None

Translator, Tutor

Tool, Translator, Tutor

getNextProblem

Interaction

processFeedback Interaction,
Response
setProperty
Component
property
getValue
Attribute

Value

Translator, Tutor

Tool, Translator, Tutor

putData

Data properties

None

Mediator

getData

None

Data properties

Session Manager

Learner
Management,
Research
Management
Learner
Management,
Research
Management

Messages are used to for session management tasks like moving to the next problem, but also for tutor-tool
interactions within the problem

example, the tool receives input from the user and sends information about the user
action to the control model, using a processInteraction message. Once the control
module receives the message, it logs it, and then redirects it to all components that
should receive it (in this case, the translator and the two tutors). The translator takes
the message and transforms it into a scriptInteraction message in order to reproduce
a student action on another interface, which is sent back to the control module. Meanwhile, the domain (math) tutor evaluates the user action, and sends its feedback to the
control module, which passes it along to the collaboration (chat) tutor using a processFeedback message. The collaboration tutor, using the user action and the feedback as
input, evaluates the action and sends its feedback back to the control module using a
processFeedback message. The control module has now received messages from the
translator, the collaboration tutor and the domain tutor. The control module integrates
the messages, passes the scriptInteraction message along to both tools, and then sends
the feedback message to Jan’s tool, as specified by the integration logic in the control
module. Although not all collaborative scenarios will operate in exactly this way, these
messages form the building blocks for handling interactions between tool, tutor, and
translator components.
We have explicitly chosen to leave some elements necessary for implementing
a computer-supported collaborative learning system unspecified, because they are
outside of our main focus. As the system is distributed, some components of the

123

Collaborative Tutoring Research Lab

405

Fig. 3 Message-passing between two tools, two tutors, and a translator. Each tool represents a collaborating
student. One tutor supports student interaction and one supports problem-solving

system (e.g. the control module) will run on a central server, and some components
(e.g. the tool components) will run on various clients. However, the way components
are distributed may depend on the deployment environment, so we leave it purposefully ambiguous. Also, because components are distributed, all messages need to be
sent remotely, and we leave the implementation of the specific protocols up to the
developer. Finally, to be deployed in a classroom, multiple sessions handling multiple
student pairs need to be run at once, meaning that a server needs to handle client
logins and launching the collaborative sessions. Although we do not outline general
guidelines for accomplishing these goals, we do discuss our implementation of these
features in Sect. 4.
3.3 Component integration
In addition to illustrating how messages are passed between components, there are
several notable elements of the above example that highlight the centrality of the control module during a session. All messages sent go through the control module, which
logs the messages prior to sending them to the relevant components. In this manner,

123

406

E. Walker et al.

the logging of different streams of interaction is combined within a single framework.
Further, the control module is in control of which components are involved, where
messages get sent, and how messages are integrated. Using the control module, a translator component can be built to echo messages from one tool to another, facilitating
collaboration. Additionally, the output of one tutor module can be used as input to a
second tutor module, facilitating the integration of different tutor components. While
CTRL is not the only architectural framework to use a federated system (see Rosatelli
and Self 2004; Mühlenbrock et al. 1998), its contribution is that it focuses specifically on integrating different tutor components and on the efficient implementation of
comparison conditions.
The control module facilitates the integration of different components, helping to
meet our goals of providing complex adaptive functionality and making it easier to
create control conditions. In standard use of the intelligent tutoring system, each individual component has knowledge of where it is sending and receiving messages, and
this configuration works because the system is so simple (the tutor sends messages to
the tool, the tool sends messages to the tutor). With multiple components, a central
body is needed to manage all the communication. The control module uses a representation of the session characteristics in order to determine how to route the messages.
Each condition facilitated by CTRL is represented as a session type stored in research
management. Each session type contains three arrays corresponding to three different
types of components (tool, translator, tutor). Session types also contain a set of logical
rules for how messages are passed between components. These rules can be as simple
as:
IF a message m was sent by any tool
THEN send m to every tutor
However, some rules will need to be more complex, as they should also represent
how to integrate feedback messages from different tutors. For example, if there is a
participation tutor and a domain tutor involved in a session, a rule represented in the
session type might be:
IF step s is incorrect
AND m is a domain feedback message
AND student a has not participated sufficiently
AND n is a participation feedback message
THEN aggregate m and n and send m + n to a.
Rules can involve any information available to the mediator, including the components
involved in the message, the parameters of a particular message, curriculum or student
parameters, and a pre-set priority of the message.
Once a session type has been created, the session manager and mediator can use it as
a guideline for how different components should be interacting. When a collaborative
session is started, the session is associated with a given session type. How this association is made is left open: it can be based on user login, or a particular curriculum, or
even be selected by the user. The details of the particular session type discussed in the
above paragraph are then retrieved from research management and stored locally in
the control module. The session manager iterates through the components involved to

123

Collaborative Tutoring Research Lab

407

send a high-level message (e.g., launching each component). The mediator’s function
is to control the low-level message passing between components by intercepting all
messages sent by a component and directing them to the appropriate targets, following
the rules outlined in the session type. Therefore, based on the session type activated,
the same components can be used in different ways. Adding or removing a component can be as simple as creating a new session type, without the need to modify the
other components involved in the interaction. Of course, depending on the complexity of the rules, authoring session types might be a challenge (particularly for nonprogrammers). In the discussion of an instantiation of CTRL in Sect. 4, we discuss the
potential utility of rule templates for accelerating the authoring of session types.
The central control module also facilitates the creation of an integrated log of collaborative interactions. In CTRL, each semantically meaningful action occurring within
a component is sent to the control module, which transforms the action into xml, and
sends it to a data store in the research management component. In this manner, logs
from each component are automatically integrated and can be reviewed together after
a study without any further processing. The logging protocol of the architecture is
based on the Pittsburgh Science of Learning Center protocol (PSLC 2009), which
records semantic-level messages sent from tool and tutor components. These tool and
tutor logs follow the concept of a transaction described by VanLehn et al. (2007),
where a user action and the tutor response to the action are linked. In our framework,
a processInteraction message is logged as “tool message” to the learner management
module, with the student interaction parameters, a unique id, and a timestamp being
represented in the log (see Fig. 4). A responding processFeedback message is logged
as a “tutor message” to the learner management module, with the student interaction
parameters, tutor response parameters, and a timestamp being captured. The relationship between the tool and tutor messages is also represented, as the tutor message
contains the ID of the tool message that triggered it. Logs include context messages,
which are initiated by the control module, and record information about the problem
being solved, the settings of the learning environment, or the experimental design.
Once a relevant context message has been logged, both tool and tutor messages will
be linked to it, containing the context message id.
Because CTRL is designed for adaptive collaborative learning systems rather than
individual intelligent tutoring systems, the logging supported needs to be broader than
the protocol discussed by VanLehn et al. (2007). Thus, an additional type of message
is supported: a scripting message, logged whenever a module changes the problem
state of a tool. In this case, the student interaction parameters, the timestamp, the relevant context message id, and the relevant tool message id are logged. Second, because
CTRL supports multiple users on multiple tools, it is important not only to record the
user of the message (part of the student interaction parameter), but the collaborative
session of the user, and the role of the user within that session. We incorporate this
information into the context message, which logs the learning environment settings.
Third, because CTRL supports multiple tool responses, the relevant metaphor for analyzing the data is not a single tool-tutor transaction but a chorus of responses to a tool
action. Not only does each tutor response need to be logged, but also the final message
constructed by the mediator to be sent to each tool.

123

408

E. Walker et al.

Fig. 4 Logging format for student-tutor interaction. Logs consist of context messages, tutor messages, tool
messages, and scripting messages

3.4 Outlook
Ideally, CTRL captures rich process data, integrates feedback from multiple tutor
components, and makes it easier to implement comparison conditions. All semantic messages from components are sent to the control module, which creates a log
of all student interactions including verbal interaction, collaborative problem-solving
actions, and the intelligent tutor responses. Multiple pre-existing and custom-built
intelligent tutors can be incorporated into the system by changing the definition of
a session type in the mediator. Domain-general intelligent tutors can use the output
of domain-specific tutors as input into their models. Finally, because components are
designed to be independent, it becomes possible to remove components from collaborative sessions in order to create multiple comparison conditions. In the following
section, we discuss an instantiation of CTRL that demonstrates these features.
4 Instantiation of CTRL
We demonstrated the suitability of CTRL as a research platform by using it as the
foundation for conducting a controlled study on the effects of adaptive support in
the context of a collaborative learning activity. In this section, we first describe how
we designed an ACLS intervention called APTA (Adaptive Peer Tutoring Assistant)
in which we augmented a successful intelligent tutoring system, the Cognitive Tutor
Algebra (CTA), with a peer tutoring activity. Our design drew on previous successful
peer tutoring interventions and included both fixed and adaptive assistance. Second,
we describe how we implemented the adaptive support condition, and two comparison

123

Collaborative Tutoring Research Lab

409

Fig. 5 Individual version of the CTA. Students solve problems in the equation solver, and receive hints
and feedback from the cognitive tutor. © 2009 Carnegie Learning. Used with permission

conditions, using an instantiation of CTRL. Finally, we describe a controlled classroom
study in which we evaluated the adaptive peer tutoring condition. Our results benefitted from having access to process data, having an adaptive intervention that relies on
both domain and collaboration models, and having strong comparison conditions.

4.1 Intervention design: peer tutoring in the context of the Cognitive Tutor Algebra
APTA is designed as an addition to the Cognitive Tutor Algebra (CTA). Figure 5 shows
the literal equation solving unit of the CTA. Students use menus in an equation solver
tool to manipulate the equation, selecting operations like “add x” or “combine like
terms”. The semantic label for the operation then appears on the right side of the
screen. For certain problems, students have to type the result of the operation in addition to selecting it. As the students solve the problem, the CTA compares their actions
to a model of correct and incorrect problem-solving behavior. If they make a mistake,
they receive visual feedback in the interface, and often a message describing their
misconception. At any point, students can request a hint on the next step of the problem. The CTA monitors student skills, reflects them in a skill display, or skillometer,
and selects problems based on student skill mastery. As students may acquire shallow
conceptual knowledge while using tutoring systems, recent efforts have augmented
cognitive tutors with activities that encourage elaboration. There are promising early
results on adding supported collaborative activities to the CTA (Diziol et al. 2008).
We augmented the CTA with a reciprocal tutoring script. When students act as peer
tutors they benefit because they are reflecting on the current state of their knowledge

123

410

E. Walker et al.

and using it to construct new knowledge (Roscoe and Chi 2007b). As these positive
effects are present even if peer tutors have low domain knowledge, researchers implement reciprocal peer tutoring programs, where students of similar abilities take turns
tutoring each other. This type of peer tutoring has been shown to increase academic
achievement in long-term interventions integrated into classroom practice (Fantuzzo
et al. 1989). Unfortunately, such gains are not always seen, possibly because students
do not often exhibit positive peer tutoring behaviors spontaneously (Roscoe and Chi
2007a). Successful interventions have provided peer tutors with assistance in order to
achieve better learning outcomes for both tutors and tutees. For one, this assistance
can target tutoring behaviors. For example, training students to deliver conceptual
mathematical explanations and elaborated help had a significantly positive effect on
tutor learning (Fuchs et al. 1997). However, it is just as critical for assistance to target
the domain expertise of the peer tutors, in order to ensure that students have sufficient
knowledge about the correct solution to a problem. If not, there may be cognitive consequences (tutees cannot correctly solve problems; Walker et al. 2007) and affective
consequences (when students feel that they are poor tutors they become discouraged;
Medway and Baron 1997).
In APTA, we script the interaction to create conditions conducive to the display of
positive tutoring behaviors. The script includes two phases: a preparation phase and a
collaboration phase. In the preparation phase, students solve the problems that they
will be tutoring, using the individual version of the Cognitive Tutor Algebra. After
each problem, they answer a reflection question that prepares them to tutor on the
problem, such as “What is a good explanation to give to your partner about a problem
step?” Including a preparation phase helps to give students the domain knowledge
necessary to later tutor their partner. Also, it may be beneficial for learning in itself,
because the anticipation of tutoring may lead students to feel more accountable for
their knowledge and therefore attend more to the domain content during preparation.
Pair members are each given different sets of problems to solve in the preparation
phase. In the collaboration phase, students then take turns tutoring each other on the
problems that they solved in the preparation phase. For example, if Bob and Sara are
partners, and Sara was the tutor on the first problem, Bob would be the tutor on the
second problem. Sara would solve the second problem just as though she was using
the individual cognitive tutor, by manipulating the menus in the Equation Solver and
typing in the results of a step when necessary. Bob in the role of the tutor cannot take
actions in the problem himself, but he can see every step Sara takes on the problem and
the results of every type-in entry. He can mark her answers right or wrong and monitor
her knowledge by raising or lowering the values of her skillometer bars. These monitoring demands might lead Bob to reflect more on the knowledge required to solve
the problem, and on his own knowledge, by extension. Sara sees every action Bob
takes to correct her or give her feedback on her knowledge. Bob and Sara can interact
with each other in natural language using an instant messaging tool, and we expected
that providing this functionality would facilitate elaborated discussion between the
students. Furthermore, Bob has access to the problem solution in an interface tab, in
order to provide him with fixed domain assistance during tutoring (see Fig. 6).
We used the CTA models to further provide adaptive collaboration assistance to
the peer tutor, using a meta-tutor. In a pilot study with unsupported students using

123

Collaborative Tutoring Research Lab

411

Fig. 6 Peer tutor’s interface. As the tutee solves problems in the equation solver window, the peer tutor
can mark steps right or wrong. The peer tutor can also give tutees feedback by increasing and degreasing
their skill bars. Students can talk in the chat window

the peer tutoring script, we found that peer tutors had difficulty giving correct help to
their tutees, and tutees solved few problems correctly (Walker et al. 2007). Therefore,
we focused our first attempt at adaptive assistance for the peer tutor’s corrections of
the peer tutee’s problem-solving actions. There are three main ways a peer tutor can
provide this type of feedback to the peer tutee:
Path 1. Responding “agree” or “disagree” whenever the tutee clicks the done button
(indicating that the tutee believes that the problem has been solved)
Path 2. Marking a problem step “right” or “wrong” after the tutee has taken that step
Path 3. Providing a hint in the chat window
For Path 1, the ideal model of performance is that whenever the tutee indicates he or
she is done with the problem, the peer tutor clicks “agree”, and whenever the tutee is
not actually done with the problem, the peer tutor clicks “disagree”. Similarly, for Path
2, the ideal model of performance is that the peer tutor marks a step right when it is in
fact correct, and marks a step wrong when it is incorrect. Path 3 is more complicated,
but for the purposes of this discussion the ideal model would simply be that the tutor
provides a correct hint in the chat window. In the context of Path 1 and Path 2, the
meta-tutor provides feedback whenever the peer tutor deviates from the model (e.g.,
whenever a step that is actually correct is marked wrong). All feedback is given to
the peer tutor, with the hope that peer tutors will reflect on their misconceptions and
then deeply process the feedback as they attempt to communicate it to the tutee. In
order to support Path 3, we also make help-on-demand available to the peer tutor. The
peer tutor can ask for a hint at any time, and use it as a basis for assisting the peer
tutee. Both hints and feedback always include a prompt for students to collaborate, and
the domain help peer tutees would have received had they been solving the problem
individually (see Fig. 7). The goal of providing the hints and feedback is not simply to
force the peer tutor to reproduce all help the CTA would have provided. The meta-tutor

123

412

E. Walker et al.

Fig. 7 Feedback delivered to the peer tutor. Once the peer tutor marks a step incorrectly, the step is
highlighted in the interface. The peer tutor then receives feedback containing a prompt to collaborate and
domain help originally designed for individual learners

provides feedback to peer tutors based on their actions, not their inaction; so if the
peer tutee does something wrong and the peer tutor does not respond, no action on the
part of the meta-tutor will be taken.
Our educational psychology research goal was to evaluate the effects of the adaptive
assistance on student collaboration in APTA by comparing it to two comparison conditions. We introduced a close comparison condition where students received fixed
assistance, and therefore only whether students received adaptive support from an
intelligent tutor compared to simply the problem answers was manipulated. We also
used a far comparison condition representing current classroom practice, where students used the cognitive tutor individually as they would during their regular curriculum. We hypothesized that the adaptive support condition would be more effective at
increasing learning than the fixed support condition because the support is provided
to peer tutors only when needed. Further, collaborative learning should be better than
individual learning because students have the opportunity to interact about the domain
material in depth. In the following section we describe how we implemented our three
study conditions using the architecture described in Sect. 3. Then we present the empirical study and its results.
4.2 Implementation of study conditions with CTRL
In this subsection, we first discuss the high-level structure of our implementation of the
three conditions, and then describe in detail how each component was implemented.
All conditions were implemented as instantiations of the CTRL framework, with a
mixture of custom-implemented components and components that were originally
part of the CTA. The adaptive peer tutoring condition included two tool components
(the peer tutor’s interface and the peer tutee’s interface), a translator component (to
echo actions from one tool to the other tool), and two tutor components (a domain

123

Collaborative Tutoring Research Lab

413

tutor component to evaluate the peer tutee’s problem-solving actions, and a meta-tutor
component to evaluate the peer tutor’s collaborative actions). The fixed peer tutoring condition included the same two tools as the adaptive condition, and the translator
component. The individual use condition included the original CTA tool and tutor components, using a tool similar to the peer tutee’s tool and the domain tutor. All conditions
included a learner management component, a research management component, and
a control module to integrate all the components. Conditions were implemented in
Java.
The tool components were implemented based on the equation solver tool already
found in the CTA. Although the CTA was intended to be implemented in line with
Ritter and Koedinger’s (1996) clean separation between tools and tutors, development
constraints led its current state to evolve from this ideal. Therefore, our first step to
being able to use the CTA tool components was refactoring them so that the tool
functionality was separate from the tutor functionality. Because this process entailed
working with existing code, it is important to note that it was time-consuming, and the
refactored product is not as cleanly implemented as it may have been had we started
from scratch. The tool components were then further modified to create the peer tutor’s
and peer tutee’s interfaces.
The translator component was a custom-made component designed to facilitate
collaboration between two users. This component functions by receiving all processInteraction messages and converting them into corresponding scriptInteraction messages before sending them back to the tool components through the control module.
The translator only deals with semantic events, so the shared solver workspace is not a
“what you see is what I see” interface. This decision was made to allow the peer tutee
space to work without interference from the peer tutor. In the CTA, the tool needs
permission from a tutor to effect certain actions (e.g., to create a point on a graph).
Because we want the peer tutee’s interaction to be less restricted than in typical use of
the cognitive tutor, the translator automatically grants that permission. The translator
is constructed based on the CTA tools, and is therefore not a general component for
facilitating collaboration (which, given the goal of working with existing components,
would likely not be possible).
As mentioned above, we implemented two tutor components in the adaptive peer
tutoring condition, one existing CTA component (domain tutor) and one custommade component (meta-tutor). The domain tutor component was taken directly from
the refactored CTA, without any further modifications. The meta-tutor was built fully
from scratch. It consisted of an expert model based on a simple bug rule:
IF a student has taken step x
AND the cognitive tutor response to x is a
AND the cognitive tutor feedback message is m
AND the peer tutor response to x is b
AND a is not equal to b
THEN send feedback to the peer tutor using x, a, b, m
When this bug rule fires, the tutoring model considers the type of problem step (e.g.,
a solver action) and peer tutor response (e.g., the peer tutor marked it incorrectly
wrong) in choosing from a fixed set of collaboration-oriented meta-feedback (e.g.,

123

414

E. Walker et al.

Fig. 8 Message passing logic for adaptive peer tutoring, fixed peer tutoring, and individual use conditions

“Your partner is actually right. Why don’t you talk to them about why they took this
step”). Then, if the domain tutor has appropriate feedback, the meta-tutor appends
the cognitive tutor message to the meta-feedback message. The tutoring model sends
a processFeedback message to highlight the problem step on the peer tutor’s screen
and to present the feedback to the peer tutor. Both the domain tutor and the peer tutor
must respond to the step before the rule can fire, and thus the model is not forcing
the peer tutor to respond to every single tutee step. Hint requests from the peer tutor
work in a similar manner, combining the cognitive tutor hint on the step with a prompt
to collaborate. The meta-tutor is domain-independent, and thus could be effective in
combination with any intelligent tutor, as long as a translator exists to translate the
intelligent tutor messages into an appropriate message format.
In general, components communicate using the CTRL message protocol, and the
way components interact in a given session is defined in the control module. All peer
tutee solver actions, peer tutor correction actions, peer tutor skill ranking actions, and
student chat actions are logged as tool messages by the control module. All cognitive
and meta tutor feedback and hints are logged as tutor messages. See the left hand side
of Fig. 8 for a diagrammatic representation of the message-passing logic in the adaptive support condition (all interactions occur via the mediator). In this configuration,
when the peer tutee takes an action, the echoing translator sends the action to the peer
tutor’s screen. In addition, the cognitive tutor evaluates the action, and sends the evaluation to the meta-tutor. All these interactions occur via the mediator. When the peer
tutor takes an action, it is sent to the echo translator, which echoes the action onto the
peer tutee’s screen, and to the meta-tutor, which compares the peer tutor evaluation to
the cognitive tutor evaluation. If a bug rule fires, the meta-tutor sends feedback to the
peer tutor. The peer tutor can also request a hint from the meta-tutor, which has stored
the cognitive tutor hint for that step. The right hand side of Fig. 8 shows the message
passing logic for the other two conditions: fixed peer tutoring and individual use.
As the logic of which components are involved in the session and how they communicate exists in the control module, it is simple to use the module to implement

123

Collaborative Tutoring Research Lab

415

the relevant conditions. The components involved were defined in the same manner
as in the CTRL framework, where all the components involved in a session and their
component types are enumerated. However, instead of the message passing logic being
defined in a rule-based manner, it was initially defined in the form of several message
groups, each comprising an originating component, a target component, and a priority
(represented pictorially in Fig. 8). Message groups can be considered a template for
automatically authoring simple rules. Upon receiving a message from a component,
the mediator would match the component to all message groups that have that component as an origin, and then send the message to the targets in each relevant message
group. In the case of messages sent to non-tool components, the control module then
waits for a response from all the components that have received messages, before
sending the messages out in the order of the specified priority. We intended to implement more complexity into the message groups, but we soon realized the limitations of
the format for anything more complex than adding action specifications to the group
statements, and consequently decided to transition to a rule-based format in the future.
Three session types were created that corresponded to the three conditions, so it was
simple to switch from one condition to another.
In CTRL, we purposefully did not specify how to pass messages remotely or how
to implement a client-server framework so that multiple people could collaborate at
once. Specifying such a framework is outside the scope of the architecture and might
depend in part on the conditions of the classrooms in which the collaboration is being
implemented (some classrooms do not allow web-based delivery, for example). Within
a single collaborative session, the session manager handles launching, quitting, and
navigating between problems, while the mediator handles the within-problem component exchanges. In the CTA, components had already been designed to send networked
messages using TCP/IP sockets, so this is the protocol we used within the mediator to
send the low-level remote messages. High-level responsibility for managing sessions
was not fully factored, so we used Java RMI to make the remote message calls for
accomplishing these functions. We also used RMI to implement a client-server setup
for running multiple tutoring sessions at once. Once two clients that were part of the
same session had connected to the server, both the session manager and the mediator
were started on the server, and the session type related to the user login was retrieved.
All other components (tools, tutors, and translators) were run on client machines.

4.3 Empirical study
After implementing the adaptive support condition (adaptive peer tutoring), the close
comparison condition (fixed peer tutoring), and the far comparison condition (individual use), we compared the three conditions in a controlled study in a classroom.
A description of the study can be found in Walker etal. (2008). Participants were
62 high-school students from five second-year algebra classes, taught by the same
teacher. The high-school used the individual version of the CTA as part of regular
classroom practice. Students in the collaborative conditions were assigned to pairs
by the classroom teacher, who was advised to group students of similar abilities who
would work well together. Students from each class were randomly assigned to one

123

416

E. Walker et al.

Table 4 Student pretest scores, delayed test scores, and gain score
Pretest score

Delayed test score

Pre-delayed gain

M

SD

M

SD

M

SD

Adaptive peer tutoring

0.82

1.08

2.82

1.78

0.29

0.19

Fixed peer tutoring

0.90

0.88

3.60

2.17

0.30

0.51

Individual learning

1.28

1.60

3.67

1.78

0.26

0.50

of the three conditions. The total number of participants included in the analysis was
39 (11 in the adaptive peer tutoring condition, 10 fixed peer tutoring condition, and 18
in the individual use condition). There were an odd number of students in the adaptive condition because we retained students in the analysis who had an absent partner
during an intervention day but were placed with a new partner in the same condition.
The study took place over three weeks. Early in the first week, students were given
a pretest. The intervention then occurred over two 70 minute class periods, each a
week apart. On both intervention days, students in the peer tutoring conditions spent
the first half of the period in the preparation phase, and the second half taking turns
tutoring each other in the collaboration phase. Students switched roles between tutor
and tutee after every problem. Students in the individual use condition simply used the
CTA as during regular classroom practice. The week after the intervention, students
were given a posttest. Two weeks after the posttest, students were given a delayed
posttest to assess their long-term retention. We logged all tutor actions, tutee actions,
and intelligent tutor responses. The log data allowed us to extract process variables
such as incorrect attempts made by students, help accessed by the peer tutor, help
communicated by the peer tutor, and problems completed.
In the remainder of this section, we look at how the implementation of the three
experimental conditions, facilitated by CTRL, helped us to gain insight into the learning effects of adaptive support for peer tutoring. First, we describe how multiple
streams of interaction data, in connection with outcome measures, provided us with
insight into the peer tutoring process. Next, we examine how the integration of the
domain and meta support may have affected the peer tutor’s behavior. Finally, we
discuss how our comparison conditions provide us with more insight than the experimental condition would have alone. Because this analysis is exploratory and intended
as a demonstration of the goals of CTRL, our discussion includes trends in addition to
significant results. For reference, Table 4 provides the pretest and delayed test scores
of the three conditions. Gain scores were computed using the following formula:
(delayed score − pretest score) / (total points possible − pretest score). For negative
gain scores, the formula we used was: (delayed score − pretest score) / (pretest score).
4.3.1 Benefits of collecting rich log data
To demonstrate the benefits of collecting rich log data, we focus on one particular
result in the adaptive peer tutoring condition: relating student impasses to the learning gains between pretest and delayed test. One might expect that the more mistakes

123

Collaborative Tutoring Research Lab

417

a tutee makes, the less they would gain between the pretest and delayed posttest,
because if they made many mistakes throughout the intervention they probably have
not mastered the material. This hypothesis can be evaluated through the integrated
stream of data collected using the CTRL logging protocols. In the adaptive condition,
the number of incorrect problem solving attempts per problem on the part of the tutee
were negatively correlated with tutee learning (r (9) = −0.561, P = 0.072), as were
the percentage of incorrect attempts to move to the next problem out of total attempts
(r (9) = −0.667, P = 0.025). One might also expect that the more mistakes a tutee
makes, the worse their tutor will do on the delayed posttest, as large numbers of tutee
mistakes may indicate that the tutor lacks the understanding to successfully help their
tutee. However, surprisingly, tutor learning was positively correlated with tutee incorrect problem solving attempts (r (9) = 0.523, P = 0.099) and tutee percent incorrect
done attempts (r (9) = 0.652, P < 0.030). These results, while correlational, suggest
that peer tutors benefit from actively processing tutee errors, which was similarly demonstrated by studies on learning from erroneous worked examples (Große and Renkl
2007). Reaching this insight required integrating problem-solving data, information
about student correctness (using cognitive tutor models), and outcome data.
The relationship between the chat logs, problem-solving logs, and correctness information also provides us with insight that would not have been available had we only
had one source of interaction data. Table 5 displays a student interaction immediately
after the peer tutee has taken an incorrect done action and the peer tutor has incorrectly agreed. The equation the students are working on is “t = f /(1 − .75)”, with
the goal of solving for t. The students must realize here that they need to get rid of
the decimal in the denominator to achieve the answer “t = 4 f ”. The entire exchange
in Table 5 took 10 min. If we were to look only at the left hand column of Table 5,
depicting the student talk, it might appear that productive behaviors are not occurring
at all: the tutor is simply giving the tutee didactic instructions for how to proceed.
Looking at the problem-solving actions does appear to confirm a lack of effort on the
part of the tutee. We see that the tutee is essentially taking a trial and error approach
to completing the problem, executing both tutor suggestions and other viable options,
and then attempting to finish the problem by clicking done. However, throughout this
interaction the tutor consults the problem answers after every tutee action, suggesting
that the tutor is engaged in comparing the student answer to the ideal worked example.
We also notice that this tutor does not make use of the adaptive feedback provided,
which may have helped him in making the comparison. Further, we can see at a glance
which actions are correct or incorrect, and when feedback was given. Using these
multiple streams of data, we can better understand that tutors may be benefiting from
student errors by being encouraged to compare the errors to a correct problem solution. In fact, this tutor had a gain score on the delayed test of 0.33, suggesting that
some of this active processing was beneficial, but also that there was more room for
improvement.
4.3.2 Benefits of integrating domain and collaboration support
The addition of adaptive feedback that incorporates both domain support and collaboration support (via the meta-tutor) can provide us with insight into how providing

123

418

E. Walker et al.

Table 5 Student interaction immediately following an impasse
Chat actions

Problem-solving actions

Computer response

Tutor: checks answers
Tutee: yeah i donno what to do after that step
Tutor: simplify fractions i think
Tutee: simplify fractions

Incorrect (cognitive)

Tutee: undoes simplify fractions
Tutor: checks answers
Tutee: I did that
Tutee: combine like terms

Correct (cognitive)

Tutee: clicks done

Incorrect (cognitive)

Tutor: agrees done

Incorrect (meta)

Tutor: checks answers
Tutee: clicks done

Incorrect (cognitive)

Tutor: agrees done

Incorrect (meta)

Tutor: multiply by 4
Tutor: checks answers
Tutee: both sides
Tutor: no
Tutee: performs multiplication

Incorrect (cognitive)

Tutee: undoes perform multiplication
Tutor: checks answers
Tutor: I mean yes
Tutee: multiplies both sides by 4

Incorrect (cognitive)

adaptive feedback affects the peer tutor’s behavior and tutee learning, especially in
comparison to domain support implemented in a fixed manner. In this section, we use
the rich log and outcome data to make a direct comparison between student use of fixed
assistance and adaptive assistance, and as a result, gain insight into the relative merits
of each assistance type in this context. This comparison would not have been possible
had we not been able to leverage the existing CTA models in order to implement the
adaptive domain support.
In the adaptive condition, peer tutors were given domain feedback about the peer
tutee’s actions and then instructions to communicate the feedback to the tutee. They
also had access to the problem answers as they were tutoring. To make a fair comparison, we looked only at the 9 students who chose to use all forms of assistance
when in the role of the peer tutor. As evident from Table 6, students accessed more
fixed assistance than adaptive assistance. However, there were differences in the relationship between the two types of assistance received by the tutor on the gains of the
student in the peer tutee role. Communicating adaptive assistance was positively correlated with tutee learning (r (7) = 0.786, P = 0.115), while failing to communicate
adaptive assistance was negatively correlated with tutee learning (r (7) = −0.803,

123

Collaborative Tutoring Research Lab

419

Table 6 Amounts of adaptive and fixed assistance communicated and not communicated
Adaptive assistance

Fixed assistance

M

M

SD

SD

Assistance communicated

1.44

1.81

3.56

3.84

Assistance not communicated

1.44

2.01

5.67

6.22

P = 0.102). Surprisingly, communicating domain feedback to the tutee after accessing fixed support (i.e. checking problem answers) was negatively correlated with tutee
learning (r (7) = −0.925, P = 0.024).
We can retrieve from our log data examples from each of the three different relevant
assistance cases (adaptive assistance communicated, adaptive assistance not communicated, and fixed assistance communicated) to illustrate what may be occurring. The
following is an example of the peer tutor receiving feedback on a step, and not communicating it. After marking a step right, the peer tutor received a feedback message
telling him that the step was actually wrong and giving him a hint on the step. At this
point, the peer tutee said: “that doesn’t look right, im sorry I suck at math lol”, and then
“k, nevermind.” The peer tutor did not respond. Next, the peer tutee clicked done, the
peer tutor agreed, and the peer tutor was given another feedback message saying the
problem is not done. This message was also not communicated to the tutee. Thus, not
only were tutees not getting the assistance needed, but they were getting misleading
feedback. To the tutee, it appeared as if the steps were correct, even if they were not.
This example can be compared to the following example where the feedback received
was communicated:
Tutor: undo it
Tutee: why? U marked it right….?
Tutor: The step is right but it said you made a typing error when you factored
Tutee: in which step?
Tutor: the first
Tutee: so u want me to undo it or is it right?
Tutee: k
Tutor: undo it
Not only did the tutor communicate what was incorrect about the current problem
solution, the two students together cleared up a misunderstanding about which aspect
was incorrect. In the final example, fixed assistance was communicated to the tutee:
After the tutee took an incorrect step dividing both sides by q +r , the peer tutor checked
the answers and said, “divide both sides by q + s.” The tutee then promptly undid his
last step and performed the correct step. Here, it is likely that the tutor instruction was
not beneficial, because no explanation was provided for why the first step was wrong
and the second step was right, and the tutee did not have to identify or reflect on his
or her error.

123

420

E. Walker et al.

4.3.3 Use of comparison conditions
Because CTRL enabled us to develop two comparison conditions in addition to the
adaptive support condition, we were able to compare adaptive support for peer tutoring
to fixed support and individual learning. If we had looked at the adaptive peer tutoring
condition independently of the comparison conditions (as many ACLS evaluations
have done so far), we would have found that the learning gains in the adaptive condition appear satisfactorily high, with a mean gain score of 0.29 (S D = 0.19) between
the pretest and the delayed posttest. However, comparing the learning improvement
across all three conditions, we see that the adaptive condition score is not different
from the fixed condition score (M = 0.30, S D = 0.51) or the individual use condition
(M = 0.26, S D = 0.50). In fact, an ANOVA reveals that the gain scores are not significantly different (F(2, 36) = 0.033, P = 0.967). Then, even though the learning
gains across the three conditions are similar, we can examine the different paths students took to learning across the three conditions. For example, the number of problems
completed per hour in the individual condition (M = 47.0, S D = 30.2) was much
higher than the number of problems completed per hour in the fixed support condition
(M = 13.3, S D = 7.71) and the adaptive support condition (M = 17.7, S D = 6.69).
A logical hypothesis may be that students in the individual condition learned by solving many problems quickly but shallowly, whereas in the collaborative conditions,
students learned by solving fewer problems slowly but deeply. In general, it would not
be possible to place the effects of the adaptive support in context without the results
of the comparison conditions.
4.4 Summary
We designed a collaborative peer tutoring script and adaptive domain support for the
peer tutoring, implemented the adaptive support condition and two comparison conditions using CTRL, and conducted a controlled classroom study comparing the three
conditions. As a result, we gained valuable insights on the effects of providing adaptive support to peer tutoring. We were able to use the combination of process data and
outcome data to learn that the more impasses faced by tutees, the more their tutors
showed delayed posttest gains. We used multiple streams of the process data to analyze
why that might be the case. We looked at how the adaptivity of the support related to
whether the assistance was communicated or not, and investigated how those two factors related to the tutee’s learning gains. Finally, we were able to put the results on the
adaptive condition in context by comparing it to the other two conditions. We realized
that even if the learning gains were similar in all conditions the paths to learning might
be different. In conclusion, implementing the experimental conditions of the learning
system with CTRL facilitated the learning sciences research that we conducted.
5 Implications of CTRL
We could have much more sophisticated systems to provide adaptive collaborative
learning support if we had an architecture for plugging together the many excellent

123

Collaborative Tutoring Research Lab

421

and complementary systems that already exist. The main claim made by CTRL is that
several different tool and tutor models can be combined to form a variety of different collaborative scenarios, and then components can be systematically removed to
create experimental control conditions. We have already presented the Adaptive Peer
Tutoring Assistant (APTA), one extended example of a collaboration activity that can
be supported under CTRL. In this section, we present two other hypothetical examples in order to further illustrate the potential use of CTRL. The first example also
involves a collaborative extension of the CTA, but in a different setting: here, students
engage in collaborative problem-solving rather than peer tutoring. We demonstrate
that even if we were to use the same core code elements that we used in the peer
tutoring extension, we could create a remarkably different collaborative experience
for students. In the second example, we take a different approach, demonstrating how
an existing collaboration script could be used as the jumping off point for adaptive
collaborative learning support, rather than an existing intelligent tutoring system. We
describe how the script could be implemented using CTRL and how tutors from other
systems might augment the script. Then, we map out the space of control conditions
made possible by the augmented script. To conclude, we discuss the limitations of
CTRL as a framework.

5.1 Implementing the collaborative problem-solving script in CTRL
In our first example we use a collaboration script developed by Diziol et al. (2008),
called the collaborative problem-solving script. In this script, two students are put in
pairs to work together on a “systems of equations” problem on a single computer.
While collaborating, students receive two types of feedback: domain-specific taskrelated feedback on their problem-solving, and domain-general collaborative feedback encouraging them to engage in effective learning strategies. This script was
shown to improve deep conceptual learning, but was implemented in a face-to-face
computer-supported setting rather than using computer-mediated collaboration, limiting the ability of the provided adaptive support to assess and provide feedback on
the collaboration.
This script could be implemented within CTRL by primarily using existing CTA
components. Each collaborating student would need a tool component, which consists of a graphing widget (already present in the CTA), a spreadsheet widget (already
present in the CTA), and an instant messaging client (already incorporated in APTA).
The tool components would broadcast processInteraction messages using selectionaction-input triples (as described above in Table 3). The collaborative problem-solving
script could also use APTA’s translator to allow graphing and worksheet actions made
by one student to appear on the other student’s screen, by sending scriptInteraction
messages. Finally, there are also two tutor components involved in this scenario: a
cognitive tutor that provides task-related support, and a collaborative tutor that provides support for the student interaction. Like in APTA, the cognitive tutor would be
taken from the existing CTA; unlike APTA, this tutor communicates directly with the
students, by broadcasting problem-solving feedback to both students’ screens using
a processFeedback message. The collaborative tutor would be a simple custom-built

123

422

E. Walker et al.

component that uses input from the CTA models to detect ineffective learning strategies on the part of the student, and respond appropriately. For example, if students
make several errors in a small amount of time, the tutor would classify their strategy
as a “trial-and-error” strategy, and send feedback using a processFeedback message.
All the components could be combined to form the collaborative scenario by adding
a new session type in the mediator, which specifies that each tool component sends
messages to all tutor and translator components, each translator component sends
messages to all the tool components, and each tutor component sends messages to all
tool components.
Reimplementing the collaborative problem-solving script within CTRL would have
several advantages for the expansion and evaluation of the script. First, the collaborative version of the script could be enhanced using components already developed
for the individual version of the CTA. To illustrate we use the example of the Help
Tutor: an addition to the CTA developed by Aleven et al. (2006). The Help Tutor
accepts student process data and knowledge assessments as input (e.g., time between
steps, hint requests, probability that students know a skill). It then provides as output
a classification of their help-seeking behavior, and direct feedback related to negative
help-seeking behavior. The Help Tutor is a natural fit with the collaborative problemsolving script, in that it is a more sophisticated and potentially more effective way
of diagnosing a variety of hint abuse and trial and error strategies that could then be
used as input to the collaborative tutor module. The collaborative tutor module could
then respond to these individual problem-solving errors with feedback prompting the
students to collaborate more effectively in order to overcome their impasses. Improving the collaboration script in this manner would simply require the integration of
the Help Tutor within the CTRL framework; the tutor would need to receive processInteraction messages, send processFeedback messages, and be added to a session
type in the mediator. Second, reimplementing the collaborative problem-solving script
within CTRL makes it easier to generate experimental control conditions to investigate
educational psychology research questions. For example, by adding different session
types to the mediator, the tool component of one partner could be removed to create an
individual learning scenario, feedback from the collaborative tutor could be removed
such that only cognitive assistance is provided, and feedback from the cognitive tutor
could be removed such that only collaborative assistance is provided. Using different combinations of components, researchers can investigate questions such as: What
are the effects of cognitive and collaborative assistance to collaboration compared to
only cognitive assistance to collaboration? What are the effects of adaptive assistance
to collaboration compared to adaptive assistance to individual learning? As a whole,
APTA and the collaborative problem-solving script overlap regarding the CTA tools
and domain models used, but create very different collaborative scenarios.

5.2 Augmenting the learning protocol approach in CTRL
Our next example is intended to show the versatility of CTRL, and uses an existing
collaboration script as its basis rather than an existing intelligent tutoring system.
We base the example on a simple computer-supported collaboration script outlined by

123

Collaborative Tutoring Research Lab

423

Pfister and Mühlpfordt (2002), called the learning protocol approach. In this approach,
three to five students attempt to comprehend and remember the content of a section
of text by discussing it in a chat window. In order to support the cognitive aspects
of reading comprehension, students are required to classify their contributions and
indicate the previous utterance to which their contribution refers, prior to making a
contribution. Further, to support student coordination, the system enforces a strict
turn-taking structure to the conversation, selecting which student should speak next
based on a predefined order. The learning protocol approach is limited in the ways
that many other collaboration scripts are limited, both in that it is not guaranteed that
improved collaboration will result from use of the script and in that aspects of the script
may in fact overstructure collaboration. There is no guarantee that students will take
the referencing or classification activities seriously, and even if they do, they may not
generate contributions of sufficient quality to trigger the desired cognitive elaborative
processes in themselves or others. Further, forcing students to take turns speaking so
rigidly may improve the participation of all students, but it may also decrease student
motivation: some students may not be able to participate when they have something
they to contribute, and others may become discouraged to be forced to comment when
they do not know what to say. Thus, adding adaptive support elements to the script
using the CTRL framework has the potential to greatly improve the learning activity.
In order to add components to the learning protocol script within CTRL, the original
version of the script would have to be refactored into several identical tool components
(one for each collaborating student) and two translator components. Each tool component would be composed of the text to be analyzed and a chat window, including
interface scaffolding (e.g., sentence classifiers and a widget to indicate the reference
of the contribution). Tools would broadcast processInteraction messages for each user
action. One translator would promote cognitive elaboration by sending a processFeedback message whenever students do not classify their utterance and mark the reference
of their utterance before submitting it. A second translator component would handle
the coordination elements of the script, disabling the chat interfaces of the collaborators who do not have the turn to speak, and transferring the turn from one person to
another using setProperty messages. Once these components have been developed, it
is simple to combine them under our conceptual framework by defining a message
group between each tool and each translator (e.g., establish two rules: IF a message m
was sent by any tool THEN send m to every translator, AND IF message m was sent
by any translator THEN send m to every tool).
One key area where adaptive support could augment the script is in evaluating, in real
time, whether student contributions to the discussion are of a good quality; essentially,
are the script scaffolds having the desired effect? Here, we can turn to an individual
intelligent tutoring system for reading comprehension called iSTART (McNamara
et al. 2007) to help adaptively support students while using the collaborative reading
comprehension script. As part of iSTART, students generate self-explanations based
on a particular segment of text, and then the system compares their self-explanation
to the segment and surrounding text to assess its quality. The iSTART tutor model
could also be used to assess student collaborative utterances, by comparing the student contribution to the reference in the discussion that the students themselves have
made. If the contribution is determined to be insufficiently relevant, the student could

123

424

E. Walker et al.

be given feedback, to help them improve their utterance. If iSTART can be refactored
so that the tutor component is isolated, and then the tutor component can be extended
to accept userInteraction messages and then to send processFeedback messages, it
could be integrated with the learning protocol components using CTRL.
Second, adding adaptive components could help mitigate the aspects of the script
that overstructure student interaction. For example, rather than forcing students to
speak in turns, the system could meet the same interaction goals by making sure that
everyone participates relatively equally. To assist this aspect of interaction, we can
turn to another ACLS system, LeCS (Rosatelli and Self 2004). LeCS has a model for
enforcing participation where if a student is silent for greater than 50% of the time estimated to complete discussing the section, the tutor sends a randomly chosen feedback
message encouraging the student to participate. If this model accepted processInteraction messages and broadcasted processFeedback messages, it could be plugged into
CTRL by adding a message group between each tool and this tutor. More interestingly,
instead of sending feedback directly to collaborating students, both the participation
tutor and relevance tutor could be used as input to a custom-built tutor that tracks the
relevance of student statements to particular concepts over time, and then, whenever
possible, prompts students who likely have something to contribute to speak next.
This concept is similar to the adaptive feedback found in COMET (Suebnukarn and
Haddawy 2004). However, because of the framework provided by CTRL, it is easy
to make this addition to the system after the custom-built tutor has been constructed,
for example by changing the logic in the mediator that sends messages from the participation tutor to the students, and instead send those messages to the custom-built
tutor.
Further, once this extended adaptive scenario has been implemented, relevant control conditions could then be generated using CTRL in order to investigate a variety
of research questions surrounding the effects of feedback on collaboration. Simply
by changing the logic in the mediator for components included in a session type and
messages passed between components, the researcher could vary the type of support
provided (cognitive elaboration support, coordination support, or both), and the adaptivity of support for each type (fixed or adaptive). The researcher could also examine
more specific questions, such as whether using domain information to augment support for social coordination is better than providing social coordination support alone.
These comparison conditions would contribute to a systematic investigation of the
current research questions surrounding adaptive collaboration support.

5.3 The scope of CTRL
These two additional examples should contribute to an overall sense of the scope of the
CTRL framework and the types of activities for which it is most appropriate. Earlier in
the paper, we stated that CTRL focuses directly on the interaction between collaborating students and intelligent support. It is appropriate for use in scenarios where a small
number of students have been placed in a group and are collaborating on a particular
task. CTRL is not designed to adaptively assign students to particular groups or tasks;
that is, it is not a tool for manipulating the preconditions of the interaction. However,

123

Collaborative Tutoring Research Lab

425

CTRL can be applied in conjunction with a wide variety of different sets of preconditions, once they have been specified, and there is nothing inherent in CTRL that
restricts the domains for which it is used. CTRL is also not specifically designed for
macro-scripting the interaction (e.g., by specifying a sequence of phases for students
to follow, such as alternation between individual and collaboration phases). Although
it is possible to implement a macro-script using a translator, the challenges of managing a macro-script are not addressed by the design of CTRL, and there may be simpler
ways for doing so within a given adaptive system. Despite this limitation, CTRL can
be applied to manage interactions within the phases of macro-scripts. Finally, although
CTRL could be applied to asynchronous interaction, it was designed with synchronous
interaction in mind, and it is likely there are other frameworks more appropriate for
managing asynchronous communication. Within these parameters (adaptive or fixed
micro-scripting of synchronous interaction between a small number of students given
a particular task), CTRL actively facilitates the implementation of different types of
interactions.
In determining what is necessary for other researchers to use CTRL, it is important to make the distinction between the conceptual framework itself and our specific
implementation using existing CTA components. The mediator component of CTRL is
simple to implement, and one could imagine other researchers adopting this concept in
order to develop their systems (and we would encourage this!). However, the difficult
part of applying CTRL is refactoring the components of existing systems to separate
the tool, translator, and tutor functionality; for example, integrating iSTART with the
learning protocol approach would depend on what would be required to isolate the
tutor component of iSTART. For large and complicated systems whose code has been
developed iteratively and by multiple people, this process can be a challenge. Ideally,
once the code has been refactored, it would not be necessary to make modifications
to the existing components. However, practically, this is not the case; it still can be
difficult to interpret and modify the code relating to the existing tutor components,
as became clear when a colleague of ours encountered difficulties in his attempt to
integrate our code with a simulated student tool. In cases where existing components
are used, we need to do more work towards reducing the need for them to be modified.
However, as more systems are developed with a component-based approach, CTRL
will become more and more effective.

6 Conclusions
We have outlined a conceptual framework called CTRL that supports educational
technologists in developing adaptive support for collaboration and educational psychologists in investigating its effects. The framework enables researchers to integrate
different types of adaptive support and, in particular, allows domain-specific models to
be used as input to domain-general components in order to create more complex tutoring functionality. Additionally, the framework helps researchers to implement comparison conditions by making it easier to vary single aspects of the adaptive intervention
through removing tool or tutor components from a system. We demonstrated the use of
CTRL by first designing adaptive support for a peer tutoring script, then instantiating

123

426

E. Walker et al.

the framework using the adaptive scenario, a fixed support scenario, and an individual tutoring scenario. Implementation was accomplished by combining pre-existing
components from the Cognitive Tutor Algebra (CTA) with custom-built components.
The three conditions were compared in an experimental study, and the results illuminated the relationship between tutee impasses and tutor learning, the different effects
of communicating adaptive and fixed assistance, and the different paths students take
to learning in individual and collaborative conditions. Then, we described how CTRL
could be used as a basis for two different examples of adaptive collaborative learning support. We outlined the limitations of CTRL with respect to defining conditions
of interaction, and discussed how while CTRL as a concept is simple to adopt, the
complexity lies in refactoring existing systems to create the necessary components.
We see one of the main contributions of our work as the development of a framework that supports the integration of pre-existing and custom-built components, with
a particular focus on tutoring components. Using CTRL, we combined a pre-existing
domain model of a tutee’s problem-solving performance with a collaborative model of
a peer tutor’s correction actions, and delivered feedback that included both a prompt
to collaborate and a domain hint. There are difficulties to relying heavily on existing
tutoring systems for components, because it may be necessary to refactor the components or deal with legacy code that is difficult to appropriate for new purposes.
However, in implementing our three experimental conditions, we leveraged CTA logging protocols, interface components, and cognitive models, which would have been
time-consuming to reconstruct from scratch. These components made it possible to
develop a classroom-functional adaptive collaborative learning system, which is currently a rarity. Another concern with relying too much on existing components is that
it might overly constrain the design of adaptive support interventions. It is true that
considering the full design space of adaptive collaborative learning support, our system did not depart very much from the current functionality of the CTA. It substituted
peer tutoring for cognitive tutoring and collaborative domain support for individual
domain support, but we did not explore collaborative scenarios that would involve
tutoring or forms of collaborative support other than collaborative domain support.
In our view, remaining close to the CTA was the first natural step. We plan to tackle
further extensions in future work; particularly since there is much more to be done
within the confines of existing CTA components (e.g., leveraging the student modeling to help the peer tutor understand what the student knows and does not know).
As we develop more components, they will form a basis for the construction of more
general collaborative scenarios. Further, using CTRL, it will eventually be possible to
apply our domain-general collaborative components to provide collaborative tutoring
for other tasks with pre-existing domain models. The other main contribution of our
framework is that it makes it easier to implement comparison conditions, by placing
the integration logic in a control module. In our evaluation, we compared three very
different scenarios: a computer-student intelligent tutoring condition, a student-student peer tutoring condition, and a computer-student-student adaptive peer tutoring
condition. Traditionally, these scenarios would have been implemented in very different manners, rather than using the same architectural framework. Furthermore, adding
a new scenario to function as a comparison condition took very little effort once the
scenario components had been implemented. One limitation of CTRL is that currently

123

Collaborative Tutoring Research Lab

427

only simple integration scenarios are accommodated; it has not yet been tested with
more complex configurations. As the conditions that we attempt to implement evolve
and become more complex, so will the framework.
CTRL, the collaborative tutoring research lab, is an initial step toward supporting research into complex forms of adaptive assistance for collaborative learning.
There have generally between two types of work in this area: Research that attempts
to understand from an educational psychology perspective whether and how adaptive assistance can be effective to promote collaborative learning, and research that
attempts to understand from a technological perspective how to construct models of
collaboration and provide automated adaptive assistance. In the first case, educational
psychologists often lack the technological tools required to implement adaptive systems, and thus conduct wizard-of-oz studies or work with programmers to implement
technologically less than optimal interventions. On the other hand, technologists focus
their energies on determining how to create complex systems, but the output is often
a research prototype that is not generally evaluated to determine its effect on student
collaboration and learning. What we offer in this paper is a way to bridge the gap
between the two approaches, making it easier to move from implementing adaptive
systems to evaluating them, and iterate upon existing adaptive systems to improve the
quality of the support that they can provide. We believe that such a bridge is necessary in order to create adaptive systems that can have a real impact on classrooms;
it does not matter if impressive adaptive systems are being developed if they do not
have a positive effect on collaboration and learning, and psychology experiments may
develop a restricted theory of adaptive assistance if they only experiment with suboptimal, low-tech solutions. It is our hope that the structure of CTRL, and in particular
its integration framework, facilitates more complex forms of support by leveraging
domain-specific models, a more controlled evaluation by allowing the construction of
comparison conditions using pre-existing components, and iteration on the development of adaptive support.
Acknowledgements This research was supported by the Pittsburgh Science of Learning Center, NSF
Grant # 0354420. We thank Bruce McLaren for his valuable contributions during initial stages of the project. Thanks to Jonathan Steinhart, Dale Walters, and Steve Ritter for their support concerning the use of the
Carnegie Learning Cognitive Tutor Algebra code, and to Ido Jamar, Kathy Dickensheets, and the teachers
from CWCTC for their motivated involvement in the project. Finally, thanks to Carolyn Rosé, Dejana Diziol
and Amy Ogan for their comments.

References
Aleven, V., Koedinger, K.R.: An effective meta-cognitive strategy: learning by doing and explaining with
a computer-based Cognitive Tutor. Cogn. Sci. 26(2), 147–179 (2002)
Aleven, V., McLaren, B., Roll, I., Koedinger, K.: Toward meta-cognitive tutoring: a model of help seeking
with a Cognitive Tutor. Int. J. Artif. Intell. Educ. 16, 101–128 (2006)
Anderson, J.R., Pelletier, R.: A development system for model-tracing tutors. In: Birnbaum, L. (eds.) Proceedings of the 1991 International Conference of the Learning Sciences, pp. 1–8. Charlottesville,
AACE, VA (1991)
Azevedo, R.: Computer environments as metacognitive tools for enhancing learning. Educ. Psychol.
40(4), 193–197 (2005)
Baghaei, N., Mitrovic, A., Irwin, W.: Supporting collaborative learning and problem solving in a constraintbased CSCL environment for UML class diagrams. Int. J. Comput. Support. Collab. Learn. 2(2–3),
159–190 (2007)

123

428

E. Walker et al.

Baker, R.S.J.d., Corbett, A.T., Koedinger, K.R., Evenson, S., Roll, I., Wagner, A.Z., Naim, M., Raspat, J.,
Baker, D.J., Beck, J.: Adapting to when students game an intelligent tutoring system. In: Ikeda, M.,
Ashley, K., Chan, T.W. (eds.) Proceedings of 8th International Conference on Intelligent Tutoring
Systems, pp. 392–401. Springer Verlag, Berlin (2006)
Beck, J.E., Mostow, J., Bey, J.: Can automated questions scaffold children’s reading comprehension? In:
Lester, J.C., Vicari R.M., Paraguacu, F. (eds.) Proceedings of 7th International Conference on Intelligent Tutoring Systems, pp. 478–490. Springer Verlag, Berlin (2004)
Berge, O., Slotta, J.: Learning technology standards and inquiry-based learning. In: Koohang, A., Harman, K.
(eds.) Learning Objects and Instructional Design, pp. 327–358. Informing Science Press, Santa
Rosa (2007)
Biswas, G., Leelawong, K., Schwartz, D., Vye, N.: The Teachable Agents Group at Vanderbilt: Learning by
teaching: a new agent paradigm for educational software. Appl. Artif. Intell. 19(3), 363–392 (2005)
Carnegie learning: Carnegie learning, Inc. Math Curricula with Proven Success. Retrieved 1 Oct 2009 from
http://www.carnegielearning.com/ (2009)
Constantino-González, M.A., Suthers, D., Escamilla de los Santos, J.: Coaching web-based collaborative
learning based on problem solution differences and participation. Int. J. Artif. Intell. Educ. 13(2–4),
263–299 (2003)
Del Solato, T., du Boulay, B.: Formalization and implementation of motivational tactics in tutoring systems. Int. J. Artif. Intell. Educ. 6(4), 337–378 (1995)
Dillenbourg, P., Baker, M., Blaye, A., O’Malley, C. : The evolution of research on collaborative learning. In: Spada, H., Reimann, P. (eds.) Learning in Humans and Machine: Towards an Interdisciplinary
Learning Science, pp. 189–211. Elsevier, Oxford (1995)
Dillenbourg, P., Hong, F.: The mechanics of CSCL macro scripts. Int. J. Comput. Support. Collab.
Learn. 3(1), 5–23 (2008)
Dillenbourg, P.: Over-scripting CSCL: the risk of blending collaborative learning with instructional
design. In: Kirschner, P.A. (ed.) Three Worlds of CSCL: Can we Support CSCL? pp. 61–91, Open
Universiteit Nederland, Heerlen (2002)
Diziol, D., Rummel, N., Kahrimanis, G., Guevara, T., Holz, J., Spada, H., Fiotakis, G.: Using contrasting
cases to better understand the relationship between students’ interactions and their learning outcome.
In: Kanselaar, G., Jonker, V., Kirschner, P.A., Prins, F. (eds.) Proceedings of the Eighth International
Conference of the Learning Sciences, pp. 348–349. International Society of the Learning Sciences,
Inc., Utrecht (2008)
Fantuzzo, J.W., Riggio, R.E., Connelly, S., Dimeff, L.A.: Effects of reciprocal peer tutoring on academic achievement and psychological adjustment: a component analysis. J. Educ. Psychol. 81(2),
173–177 (1989)
Fischer, F., Kollar, I., Mandl, H., Haake , J.M. (eds.): Scripting Computer-Supported Collaborative Learning—Cognitive, Computational, and Educational Perspectives. (Springer, New York 2007)
Fuchs, L., Fuchs, D., Hamlett, C.L., Phillips, N.B., Karns, K., Dutka, S.: Enhancing students’ helping
behavior during peer-mediated instruction with conceptual mathematical explanations. Elem. Sch.
J. 97(3), 223–250 (1997)
Genesereth, M.R.: An agent-based framework for interoperability. In: Bradshaw, J.M. (ed.) Software
Agents, pp. 317–345. AAAI Press/MIT Press, Menlo Park (1997)
Große, C.S., Renkl, A.: Finding and fixing errors in worked examples: can this foster learning outcomes’?. Learn. Instr. 17, 612–634 (2007)
Gweon, G., Rose, C., Carey, R., Zaiss, Z.: Providing support for adaptive scripting in an on-line collaborative
learning environment. In: Grinter, R., Rodden, T., Aoki, P., Cutrell, E., Jeffries, R., Olson, G. (eds.)
Proceedings of ACM CHI 2006 Conference on Human Factors in Computing Systems, pp. 251–260.
ACM Press, New Jersey (2006)
Hmelo-Silver, C.E.: Problem-based learning: what and how do students learn?. Educ. Psychol. Rev.
16(3), 235–266 (2004)
Hoppe, H.U.: The use of multiple student modeling to parameterize group learning. In: Greer, J. (ed.)
Proceedings of the 7th International Conference on Artificial Intelligence in Education, pp. 234–241.
Association for the Advancement of Computing in Education (AACE), Charlottesville, VA (1995)
Hoppe, H.U., Gaßner, K.: Integrating collaborative concept mapping tools with group memory and retrieval
functions. In: Stahl, G. (ed.) Proceedings of CSCL 2002, pp. 115–124. Lawrence Erlbaum, Boulder,
CO (2002)

123

Collaborative Tutoring Research Lab

429

Israel, J., Aiken, R.: Supporting collaborative learning with an intelligent web-based system. Int. J. Artif.
Intell. Educ. 17(1), 3–40 (2007)
Koedinger, K.R., Aleven, V.: Exploring the assistance dilemma in experiments with Cognitive Tutors. Educ.
Psychol. Rev. 19(3), 239–264 (2007)
Koedinger, K., Anderson, J., Hadley, W., Mark, M.: Intelligent tutoring goes to school in the big city. Int.
J. Artif. Intell. Educ. 8, 30–43 (1997)
Koedinger, K., Corbett, A.T.: Cognitive tutors: technology bringing learning science to the classroom. In:
Sawyer, R.K. (ed.) The Cambridge Handbook of the Learning Sciences, pp. 61–78. Cambridge
University Press, New York (2006)
Krueger, C.W.: Software reuse. ACM Comput. Surv. 24(2), 131–183 (1992)
Kumar, R., Rosé, C.P., Wang, Y.C., Joshi, M., Robinson, A.: Tutorial dialogue as adaptive collaborative
learning support. In: Luckin, R., Koedinger, K.R., Greer, J. (eds.) Proceedings of the 13th International
Conference on Artificial Intelligence in Education, pp. 383–390. IOS Press, Amsterdam (2007)
McManus, M.M., Aiken, R.M.: Teaching collaborative skills with a group leader tutor. Educ. Inf. Technol. 1, 75–96 (1996)
McNamara, D.S., O’Reilly, T., Rowe, M., Boonthum, C., Levinstein, I.B. : iSTART: a web-based tutor
that teaches self-explanation and metacognitive reading strategies. In: McNamara, D.S. (ed.) Reading
Comprehension Strategies: Theories, Interventions, and Technologies, pp. 397–421. Erlbaum, Mahwah (2007)
Medway, F.J., Baron, R.M.: Locus of control and tutor’s instructional style. Contemp. Educ. Psychol. 2,
298–310 (1997)
Meier, A., Spada, H., Rummel, N.: A rating scheme for assessing the quality of computer-supported collaboration processes. Int. J. Comput. Supp. Collab. Learn. 2, 63–86 (2007)
Mostow, J., Aist, G. : Evaluating tutors that listen: an overview of Project LISTEN. In: Forbus, K., Feltovich,
P. (eds.) Smart Machines in Education, pp. 169–234. MIT/AAAI Press, Menlo Park (2001)
Mühlenbrock, M., Tewissen, F., Hoppe, H.U.: A framework system for intelligent support in open distributed
learning environments. Int. J. Artif. Intell. Educ. 9, 256–274 (1998)
Mühlenbrock, M.: Shared workspaces: analyzing user activity and group interaction. In: Hoppe, H.U.,
Ikeda, M., Ogata, H. (eds.) New Technologies for Collaborative Learning, Kluwer Academic Publishers, Dordrecht (2004)
Pfister, H.R., Mühlpfordt, M.: Supporting discourse in a synchronous learning environment: The learning
protocol approach. In: Stahl, G. (ed.) Proceedings of CSCL 2002, pp. 581–589. Erlbaum, Hillsdale,
NJ (2002)
Pinkwart, N.: A plug-in architecture for graph based collaborative modeling systems. In: Hoppe, U.,
Verdejo, F., Kay, J. (eds.), Proceedings of the 11th International Conference on Artificial Intelligence
in Education, pp. 535–536. IOS Press, Amsterdam (2003)
PSLC: Welcome: Pittsburgh Science of Learning Center. Retrieved 1 Oct 2009 from http://www.learnlab.
org/ (2009)
Ritter, S., Blessing, S.B., Hadley, W.S.: SBIR phase I final report 2002. Department of Education RFP ED:
84-305S (2002)
Ritter, S., Koedinger, K.R.: An architecture for plug-in tutor agents. Int. J. Artif. Intell. Educ. 7(3/4),
315–347 (1996)
Rosatelli, M., Self, J.: A collaborative case study system for distance learning’. Int. J. Artif. Intell.
Educ. 14(1), 97–125 (2004)
Roscoe, R.D., Chi, M.: Understanding tutor learning: knowledge-building and knowledge-telling in peer
tutors’ explanations and questions. Rev. Educ. Res. 77(4), 534–574 (2007a)
Roscoe, R.D., Chi, M.: Tutor learning: the role of instructional explaining and responding to questions. Instr.
Sci. 36(4), 321–350 (2007b)
Rummel, N., Spada, H.: Learning to collaborate: an instructional approach to promoting collaborative
problem-solving in computer-mediated settings. J. Learn. Sci. 14(2), 201–241 (2005)
Saab, N., van Joolingen, W.R., van Hout-Wolters, B.H.A.M.: Supporting communication in a collaborative
discovery learning environment: the effect of instruction. Instr. Sci. 35(1), 73–98 (2007)
Strijbos, J.W., Martens, R.L., Jochems, W.M.G.: Designing for interaction: six steps to designing computersupported group-based learning. Comput. Educ. 42(4), 403–424 (2004)
Slavin, R.E.: Research on cooperative learning and achievement: What we know, what we need to
know. Contemp. Educ. Psychol. 21(1), 43–69 (1996)

123

430

E. Walker et al.

Soller, A., Martinez, A., Jermann, P., Mühlenbrock, M.: From mirroring to guiding: a review of state of the
art technology for supporting collaborative learning. Int. J. Artif. Intell. Educ. 15(4), 261–290 (2005)
Soller, A.: Computational modeling and analysis of knowledge sharing in collaborative distance learning. User Model. User Adapt. Interact. J. Pers. Res. 14(4), 351–381 (2004)
Suebnukarn, S., Haddawy, P.: A collaborative intelligent tutoring system for medical problem-based learning. In: Nuno, N., Charles, R. (eds.) Proceedings of the 9th International Conference on Intelligent
User Interfaces, pp. 14–21. ACM Press, New York, NY (2004)
Suthers, D.: Architectures for computer supported collaborative learning. In: Okamoto, T., Hartley, R.,
Klus, Kinshuk, J.P. (eds.) Proceedings of the IEEE International Conference on Advanced Learning
Technology: Issues, Achievements and Challenges, pp. 25–28. IEEE Computer Society, Los Alamitos,
CA (2001)
Tedesco, P.: MArCo: building an artificial conflict mediator to support group planning interactions. Int. J.
Artif. Intell. Educ. 13(1), 117–155 (2003)
Tsovaltzi, D., Rummel, N., Pinkwart, N., Scheuer, O., Harrer, A., Braun, I. McLaren, B.M.: CoChemEx:
supporting conceptual chemistry learning via computer-mediated collaboration scripts. In: Dillenbourg, P., Specht, M. (eds.) Proceedings of the Third European Conference on Technology Enhanced
Learning, pp. 437–448. Springer, Berlin (2008)
VanLehn, K.: The behavior of tutoring systems. Int. J. Artif. Intell. Educ. 16(3), 227–265 (2006)
VanLehn, K., Koedinger, K. R., Skogsholm, A., Nwaigwe, A., Hausmann, R. G. M., Weinstein, A.,
Billings, B.: What’s in a step? Toward general, abstract representations of tutoring system log data.
In: Conati, C., McCoy, K.F., Paliouras, G. (eds.) User Modeling 2007, 11th International Conference,
pp. 455–459. Springer (2007)
Vieira, A. C., Teixeira, L., Timóteo, A., Tedesco, P., Barros, F. A.: Analyzing on-line collaborative dialogues: The OXEnTCHÊ-Chat. In: Lester, J.C., Vicari, R.M., Paraguaçu, F. (eds.) Proceedings of the
7th International Conference on Intelligent Tutoring Systems, pp. 315–324. Springer-Verlag, Germany
(2004)
Vizcaíno, A., Contreras, J., Favela, J., Prieto, M.: An adaptive collaborative environment to develop good
habits in programming. In: Gauthier, G., Frasson, C., VanLehn, K. (eds.) 5th International Conference
on Intelligent Tutoring Systems, ITS’2000, pp. 262–271. Springer-Verlag, Berlin (2000)
Walker, E., Rummel, N., Koedinger, K.: To tutor the tutor: adaptive domain support for peer tutoring. In:
Woolf, B., Aimeur, E. Nkambou, R., Lajoie, S. (eds.) Proceedings of the 9th International Conference
on Intelligent Tutoring Systems, pp. 626–635. Springer, Berlin (2008)
Walker, E., Rummel, N., McLaren, B., Koedinger, K.: The student becomes the master: Integrating peer
tutoring with cognitive tutoring. In: Chinn, C.A., Erkens, G., Puntambekar, S. (eds.) Proceedings of the
Computer Supported Collaborative Learning (CSCL) Conference 2007, pp. 750–752. International
Society of the Learning Sciences, New Brunswick (2007)

Author Biographies
Erin Walker is a Ph.D. candidate in Human-Computer Interaction at Carnegie Mellon University. She
received her BS with honors in both Computer Science and Psychology from the University of Manitoba
in 2004. Her primary research interests lie in the area of adaptive collaborative learning support, with three
emphases: how to design the support to have a positive influence on student behavior, how to efficiently
implement the support using existing problem-solving models, and what effects the support has on domain
learning outcomes.
Nikol Rummel is an Assistant Professor in the Institute of Psychology at the University of Freiburg, Germany, and an Adjunct Assistant Professor in the Human-Computer Interaction Institute at Carnegie Mellon
University. Dr. Rummel received a Diploma and a Ph.D. in psychology from the University of Freiburg
and a M.Sc. degree in educational psychology from the University of Wisconsin—Madison, USA as a
Fulbright scholar. Before her training in Psychology, she completed two years of teacher training at the
Pädagogische Hochschule (College of Education) Freiburg. Dr. Rummel’s research interests center around
issues of instructional support for learning in computer-supported settings. The focus of recent work at the
Pittsburgh Science of Learning Center was on adaptive support for collaborative learning with the Cognitive
Tutor Algebra.

123

Collaborative Tutoring Research Lab

431

Kenneth R. Koedinger is Professor of Human-Computer Interaction and Psychology at Carnegie Mellon.
His background includes a BS in Mathematics, a MS in Computer Science, a Ph.D. in Cognitive Psychology,
and experience teaching in an urban high school. His research has contributed new principles and techniques for the design of educational software and has produced basic cognitive science research results on
the nature of mathematical thinking and learning. Dr. Koedinger directs the Pittsburgh Science of Learning
Center and is co-founder of Carnegie Learning Inc., a company bringing learning research to schools in the
form of the Cognitive Tutor technology.

123

Work-in-Progress

CHI 2012, May 5–10, 2012, Austin, Texas, USA

Using Need Validation to Design an
Intelligent Tangible Learning
Environment
Erin Walker

Abstract

Arizona State University

Tangible learning environments may be improved if
combined with another successful educational
technology, intelligent tutoring systems. However,
design principles for tangible environments and
intelligent support are often at odds. To reconcile these
differences, we employ a need validation methodology
to understand student needs in an intelligent tangible
learning environment. We found that students seek
activities that provide them with feelings of discovery,
inter-group competition, and an appropriate level of
challenge. In addition, students value physical
movement, interactivity, and perceived relevance to
their learning objectives. We discuss design
implications of these findings for combining the benefits
of tangible learning and intelligent support systems.

699 S. Mill Ave
Tempe, AZ, 85281, USA
erin.a.walker@asu.edu
Winslow Burleson
Arizona State University
699 S. Mill Ave
Tempe, AZ, 85281, USA
burleson@asu.edu

Author Keywords
Tangible learning environments; intelligent support
systems; user-centered design.
Copyright is held by the author/owner(s).
CHI’12, May 5–10, 2012, Austin, Texas, USA.

ACM Classification Keywords
K.3.1 Computer Uses in Education

ACM 978-1-4503-1016-1/12/05.

2123

Work-in-Progress

CHI 2012, May 5–10, 2012, Austin, Texas, USA

Introduction
By identifying effective education technologies and
integrating them into a single powerful system, we may
be able to combine the strengths of multiple
approaches. One technology that could have a large
impact on learning is tangible learning environments
(TLEs), where students interact in a physical space with
digitally augmented devices. In theory, these
environments help learning because they encourage
sensory engagement, active manipulation, and physical
activity [7, 10]. Despite this promise, there has been
little empirical evidence for these environments’
benefits [3]. These environments might not be as
successful as they could be because they provide
students with little explicit support, despite evidence
suggesting that, at least in some domains, explicit
support is important for learning [4].

Intelligent
TLEs

Support

Adding intelligent support components to tangible
environments might be one way of making them more
effective. Intelligent tutoring systems (ITSs) model
student knowledge and problem-solving skills, and use
this model to provide tailored support such as hints,
feedback, and problem selection [11]. These systems
yield dramatic learning gains in science and math
domains in K-12 classrooms [e.g., 5].

Systems
Learning

Self-

Objectives

defined

Support
Provided

Well-defined

Facilitate

Scaffolding,

Using

Hints, and

Tools

Feedback

Table 1. Difference between tangible
learning environments and intelligent
support environments.

ITSs and TLEs often have conflicting design principles
(see Table 1). In articulating the motivation for tangible
learning environments, researchers have taken
constructionist perspectives, where students generate
knowledge through the process of constructing a
meaningful artifact [8]. Encouraging this approach to
learning necessitates that certain assumptions be made
about the learning activity: students define their own
goals for what they want to achieve, and they use

digital tools to support themselves in achieving the
goals [6]. In contrast, intelligent tutoring principles
emphasize that learning objectives should be welldefined, so that appropriate scaffolding can be designed
and student behavior can be easily modeled. Feedback
and help should be given in order to keep students
moving along that well-constrained and pre-defined
path. Ideal intelligent support within a tangible
environment will provide guidance students need to
benefit from the environment without sacrificing the
freedom that tangible environments afford.
Our overall research goal is to determine how to
augment tangible learning environments with intelligent
support to create intelligent tangible learning
environments (ITLEs). Our approach involves three
steps: 1. Use a design space to generate ideas that
combine instructional and motivational principles, 2.
Get user input on design concepts, 3. Analyze data by
focusing on needs, not activities.

Design Space & Concepts
Our first step was to generate several concepts for ITLE
activities that incorporate user needs. We constrained
our ideation in two ways. First, we chose middle and
high school geometry as our learning domain, with
sample tasks ranging from plotting ordered pairs to
proving two triangles are similar. Second, we chose an
iRobot Create as the central piece of technology to
incorporate into our ideas, inspired by Papert’s Tangible
Logo [9]. Off the shelf, the iRobot Create is capable of
running simple programs that allow it to move forward
and backward and turn left and right. As part of
brainstorming, we relaxed most technological
constraints on the robot. We assumed that users could
interact with the robot using gestures or speech, and

2124

Work-in-Progress

CHI 2012, May 5–10, 2012, Austin, Texas, USA

that they and the robot could interact with projected
geometry figures in the environment.
We engaged in structured brainstorming, using a
framework for educational game design developed by
Aleven and colleagues [1] that suggests that while
brainstorming, designers consider both instructional
and motivational principles (which they refer to as
aesthetics). We selected four instructional principles to
explore based on our analysis of the literature on TLEs
and ITSs: tangible representation, embodied
interaction, scaffolded problem-solving, and studentagent relationships. We then chose four motivational
principles described by [1] that were most relevant:
Discovery, challenge, fellowship, and narrative.
We generated 24 scenarios spanning the full range of
concepts related to use of the iRobot that we might be
interested in exploring. Each scenario employed at least
one instructional and one motivational principle that we
had selected. In some cases, the use of the
motivational principle was negative, in that it provided
students with the opposite of what we believed might
be motivating. Each scenario had three storyboard
panels to be read in sequence from left to right, with
captions explaining what was going on. Figure 1 is a
scenario that primarily engages the instructional
principle embodied interaction, as the robot and the
students are interacting with a geometric figure in
physical space. It is a positive example of the
motivational principle discovery, as a surprise figure
appears upon successful completion of the problem. In
the Scenarios, we called the iRobot “Rover.”

You get a series of
coordinates to navigate to.

You plot the coordinates
on the floor and drive
Rover between coordinates.

Once Rover follows the
sequence correctly, a
mystery shape appears.

Figure 1. Scenario representing need of discovery.

Need Validation Method
We used the scenarios as a tool for soliciting user
reactions and ideas by applying need validation, the
first component of Davidoff and colleagues’ Speed
Dating design method [2].
Participants
Three groups participated in need validation for a total
of 11 participants (five in Group 1, three in Group 2,
and three in Group 3). Participants were each paid $20.
Participants were between 13 and 16 years old (M =
14.9), and all but two participants had already taken
geometry. Students within a group knew each other,
having signed up for the study together.
Procedure
Design sessions lasted two hours. We began by
explaining the purpose of the sessions to students and
demonstrating the functionality of the iRobot. Sessions
then consisted of four alternating periods of soliciting

2125

Work-in-Progress

CHI 2012, May 5–10, 2012, Austin, Texas, USA

user reactions and asking users to generate ideas. For
the user reactions phases of the sessions, we focused
on the use of sketches (based on the scenarios
described above) to solicit user feedback. Following
need validation [2], we presented students with each
sketch, and asked a discussion question. Once students
were done discussing a sketch, we presented them with
the next sketch. Sketches were aggregated into themes
(e.g., feedback delivered by the robot), with an
average of six sketches related to a theme. We
presented students with all sketches related to one
theme prior to moving on to a brainstorming phase. For
the user brainstorming phases of the sessions, we used
the sketches students had just seen as a jumping-off
point for brainstorming. For some groups, participants
found it natural to sketch their ideas; for others they
simply brainstormed by discussing ideas with the rest
of the group. Once participants stopped generating
ideas, we moved on to the next user reactions phase.
Analysis
We audiotaped the sessions, and retained student
sketches as data. In our analysis, we looked for two
aspects of student reactions [2]. First, we identified
strong positive and negative reactions to elements of
scenarios that we had purposefully introduced (e.g.,
discovery, fellowship, challenge, and narrative). We
drew links between these strong reactions and student
ideas during brainstorming. Second, we identified
recurring emergent themes we had not purposefully
introduced that students brought up spontaneously
during user reactions and brainstorming.

Results: User Needs
Strong Reactions
Our first method of analysis was to look for strong
positive or negative reactions to elements that we had
introduced into the sketches. The need of discovery
resonated with students, where something previously
hidden was revealed as part of learning activities. We
illustrate this finding with student reactions to a
connect-the-dots scenario, which was designed to
prime the discovery reaction (see Figure 1). When we
designed the scenario, undergraduate pilot subjects
considered it to be one of the least exciting scenarios,
as it simply involved a figure appearing when points
were correctly plotted on the graphs. However, the
reaction of the middle and high school students were
different. Students responded excitedly: “I think it’s a
good idea… it’d be fun, like, to try to get the mysterious
picture, and see what it is.” (P11).
We also presented students with several ideas
attempting to prime their feelings of fellowship. We had
thought that scenarios where students worked together
would be motivating to students. However, the ideas
that resonated the most were the ones that specifically
involved intergroup competition. A sketch that got one
of the most positive reactions was one where groups
would teach their robot different shapes, and than the
robots would face off to see who could draw the most
shapes. In direct relation to the sketch, P5 said, “That’s
cool that different ones would face off, I like that”, with
P3 responding “It would get everyone really excited”.
Students had some of their most emphatic reactions to
scenarios that had examples of interactions that were
too challenging or not challenging enough. Students
had strong negative reactions to scenarios that they felt

2126

Work-in-Progress

CHI 2012, May 5–10, 2012, Austin, Texas, USA

talking about the sketches in general: “We’re at school
7 hours a day, sitting in the classroom with, like, offgray walls… it’s like a prison… You get to like jump up
and move around… that’s like great for your mind.”

Rover executes the
incorrect actions.

Rover begins the
first action but stops.

Rover stops in the
middle, corrects you,
and continues the
program correctly.

Figure 2. Three scenarios for how the robot can deliver
feedback to the student.

supplied little support, such as A in Figure 2. They
complained about doing a lot of work without
perceiving the value, “Hmm, this [has] happened to
me… I did all of this, and I have to figure out where I
went wrong” (P6). On the other hand, students also
reacted to too much feedback, as depicted in C in
Figure 2. They commented: “I don’t think Rover should
tell them what they did, because, they have to, like,
figure it out.” (P9) Student comments relating to
challenge nearly always focused on the motivational
elements rather than on the cognitive elements. They
expressed a desire to demonstrate their knowledge,
going so far as to say: “I’d rather teach it something
rather than having it teach me something” (P1).
Emergent Needs
In addition to students reacting strongly to elements of
the scenarios that we had purposely introduced, there
were three additional needs mentioned repeatedly by
the students. The enjoyment students predicted over
the simple act of physical motion was a theme that
occurred across several scenarios. P5 said, when

Interactivity was another theme that was brought up
repeatedly during the design sessions. Students often
referred to simple forms of interactivity with high
enthusiasm. When discussing the idea of a projected
figure, one student said, “And then you’d probably get
color too… ‘cause graph paper is boring. If it’s
projected, you can try to make it fun” (P8). When
brainstorming, students referred to specific features of
the problem that could become interactive, saying
things like, “Make angles turn colors when measured”,
and “Rover will glow if you’re right or X if you’re
wrong.” Students found value in simple augmentations
to escape the tedium of typical classroom activities.
Relevance was also a theme. Students pushed back
against the learning content contained in the sketches,
saying it was too simple: “[I’m] trying to think how
Rover can be used in more complex ways. (P6)” Later,
P6 added, “I think the first thing we need to decide on
is what aspect they need to learn, and how they’re
going to learn it… We need to borrow a geometry book
for next time, and just take the problems, and find out
how Rover can help with them.” Students identified
several areas of geometry with which they had trouble,
and developed concepts for those scenarios.

Discussion and Conclusions
In this paper, we presented the results of ideationstage design work for combining ITSs and TLEs. Using
a need validation methodology that forms one half of
[2]’s Speed Dating procedure, we found that students

2127

Work-in-Progress

CHI 2012, May 5–10, 2012, Austin, Texas, USA

desired an element of discovery in their activities, intergroup competition, and the appropriate level of
challenge. In addition, we found that incorporating
simpler elements of physicality, interactivity, and
relevance appealed to students. Future work in this
area will involve constructing an ITLE that will
incorporate the elements that students desired most.
These findings present guidelines for how to navigate
tensions between ITSs and TLEs. ITSs demand welldefined learning objectives while TLEs encourage selfdefined objectives. Our results suggest that, at least
that for domains like geometry, students need to be
given objectives that they perceive as relevant. If selfdefined objectives are a key element of the
instructional strategy, our results suggest that
instructional materials communicate to students the
relevance of the learning activity to their current
geometry classes. In addition, ITSs suggest students
should be provided with heavy support instead of tools
that facilitate exploration, as in TLEs. As we discovered
when exploring the need of challenge, students were
quick to reject the idea of minimal feedback, not
wanting to “get stuck.” They were also resistant to
being given too much help. When allowing students the
freedom inherent in TLEs, intelligent support models
can function not by giving feedback with every action,
but by ensuring students feel like the next correct step
is within reach. Overall, our approach provides insight
into how students view their needs when interacting
with an advanced learning technology.

Acknowledgements
This work was funded by the Computing Innovations
Fellowship program, NSF Grant #SBE-1019343.

References
[1] Aleven, V., Myers, E., Easterday, M., and Ogan, A.
"Toward a Framework for the Analysis and Design of
Educational Games," Digitel (2010), 69-76.
[2] Davidoff, S., Lee, M. K., Dey, A. K., and
Zimmerman, J. (2007). Rapidly exploring application
design through speed dating. In Proc. UBICOMP ’07.
Springer-Verlag, Berlin, Heidelberg (2007), 429-446.
[3] Johnson-Glenberg, M. C., Birchfield, D., Savvides,
P. & Megowan-Romanowicz, C. Semi-virtual Embodied
Learning – Real World STEM Assessment. In Serious
Educational Game Assessment: Practical Methods and
Models for Educational Games, Simulations and Virtual
Worlds. Sense Publications, Rotterdam (2010). 225-241
[4] Koedinger, K. R., & Aleven V. Exploring the
assistance dilemma in experiments with Cognitive
Tutors. Ed. Psy. Rev. 19, 3 (2007), 239-264.
[5] Koedinger, K.R., Anderson, J.R., Hadley, W.H., and
Mark, M. Intelligent tutoring goes to school in the big
city. IJAIED 8 (1997), 30–43.
[6] Marshall, P. Do tangible interfaces enhance
learning?. In Proc. TEI’07. ACM Press (2007), 163-170.
[7] O’Malley, C. & Stanton-Fraser, D. Literature Review
in Learning with Tangible Technologies. Literature
Review series, report 12. NESTA Futurelab Publications
(2004).
[8] Papert, S. Situating constructionism.
Constructionism. Norwood, NJ: Ablex (1991). 1-11.
[9] Papert, S. (1999) Mindstorms: children, computers,
and powerful ideas, 2nd edn. Basic Books, New York.
[10] Price, S. and Rogers, Y. (2004) Lets get physical:
the learning benefits of interacting in digitallyaugmented physical spaces. Computers & Education
15(2). 169-185.
[11] VanLehn, K. (2006) The behavior of tutoring
systems. IJAIED 16, 3, 227-265.

2128

New challenges in CSCL: Towards adaptive script support
Nikol Rummel, University Freiburg, Institute of Psychology, Engelbergerstr. 41,
79085 Freiburg, Germany, rummel@psychologie.uni-freiburg.de
Armin Weinberger, Ludwig-Maximilians-Universität (LMU) München, Department of Psychology,
Leopoldstrasse 13, 80802 München, Germany, armin.weinberger@psy.lmu.de
Abstract: Scaffolding learners, i.e. helping learners to attain tasks they could not accomplish
without support, entails the notion of fading, i.e. reducing the scaffolding for learners to
become more and more self-regulated. Fading implies to tailor support for collaboration, such
as collaboration scripts, to the particular needs of the specific collaborators. In computer
supported collaborative learning (CSCL) settings, support can be designed in a very restrictive
and inflexible fashion; at the same time computerized settings open new possibilities for the
realization of adaptive support as they enable automation of analysis and feedback
mechanisms. In this symposium we present new technical approaches and latest empirical
research on possibilities and limitations of adaptive support for learners in CSCL settings.
Much research has demonstrated the potential effectiveness of collaboration for improving students’
problem solving and learning (e.g., Slavin, 1996). Collaborative learners have the possibility to receive help
from their partner, and can engage in elaborated discussions (Teasley, 1995). Unfortunately, research has also
shown that effective collaboration does not happen spontaneously (e.g. Dillenbourg, Baker, Blaye, & O’Malley,
1995; Rummel & Spada, 2005) Collaborative learners often do not engage in productive interactions and thus
miss collaborative learning opportunities. Hence, in order to ensure that students can benefit from learning
collaboratively, it is important that collaborative partners learn how to work together in productive ways.
Particularly at the beginning, some degree of other-regulation, e.g., through guidance, instruction, and training,
is required before learners are enabled to engage in self-regulated effective processes of collaborative learning
(Kollar & Fischer, 2007; Slavin, 1996). One approach that has shown its effectiveness in a variety of contexts is
guiding students’ interaction by a collaboration script (e.g. Kollar, Fischer & Hesse, 2006; O’Donnell, 1999). To
improve students’ interaction, scripts prompt students to engage in cognitive, meta-cognitive, and social
processes that might otherwise not occur. However, concern has been expressed that there may be a danger in
“overscripting” collaborative interaction, i.e., providing too much structure and support for collaboration –
especially for advanced students who are capable of self-regulating their learning activities (Cohen, 1994;
Dillenbourg, 2002). This reproach is particularly true for script approaches that have been developed for
computer-mediated collaboration (e.g., Pfister & Mühlpfordt, 2002). Scripting collaboration inflexibly might
prevent the independent, exploratory thinking required for generative learning or problem-solving, and
consequently decrease students’ motivation. Moreover, following script prescriptions may impose considerable
cognitive load upon learners and thus hamper problem-solving and learning. Taken together, there is reason to
believe that it might be best to scaffold collaboration in an adaptive fashion, providing and fading structured
support for collaboration based on the particular needs of the specific collaborators. Further evidence for the
assumption that adaptive support of collaboration will be most effective for learning comes from research on
cognitive tutors (e.g. Anderson, Corbett, Koedinger, & Pelletier, 1995). A key strength of cognitive tutors is that
they provide just-in-time support, tailored to the needs of the individual student in a particular moment. As soon
as the student makes an error, he or she receives feedback from the system and usually is given some advice
about how to overcome the impasse.
In the long run, this is what an adaptive collaboration approach aims at: a collaboration tutor. However,
this is obviously a highly ambitious goal to achieve. Defining all possible interaction patterns and decisions
collaborative learners could make ahead of time and defining a “best path” or “buggy paths” through the
collaboration space seems to be difficult and perhaps impossible. So far, it is only possible to define positive and
negative collaborative behaviors in general terms. The challenge is to find ways to monitor those behaviors
based on real-time data collected during student collaboration and have the system respond to the collaborating
partners accordingly. While interest in adaptive collaborative learning systems is on the rise in the computersupported collaborative learning (CSCL) community (Soller, Jermann, Muehlenbrock, & Martinez, 2005), little
progress has yet been made on the implementation of adaptive support. In this symposium we present latest
results from projects concerned with developing adaptive support for CSCL environments. Wecker and Fischer
have taken a first step towards adaptiveness by comparing traditional script support to faded scripting in a CSCL
setting under the condition of distributed monitoring. Meier and colleagues conducted a study in which they
compared the effects of adaptive feedback vs. generic feedback vs. no feedback on students’ subsequent
collaboration. Walker, Rummel & Koedinger tackled the challenge of providing intelligent domain support to
the peer tutor in a computer-mediated peer tutoring setting. Against the background of a number of empirical

studies with varying degrees of automation, Rose, Kumar, Gweon, Wang, and Joshi discuss the possibilities of
providing adaptive support on the basis of automated analysis of conversational data. The former three
contributions present experimental studies and exemplify the challenges, but also the gains, of conducting
empirical research on adaptive support. Then the contribution by Rose et al. takes a broader view and opens up
the stage for a discussion of the possibilities and limitations of achieving adaptive collaboration support.

Fading of collaboration scripts: Does it foster the acquisition of applicationoriented knowledge?
Christof Wecker & Frank Fischer, Department of Psychology, Ludwig-Maximilians-Universität (LMU)
München, Germany
To foster the acquisition of knowledge that learners can apply easily to solve problems of fields of
practice, learners are often required to discuss authentic problems in problem-oriented environments for
computer-supported collaborative learning. For example, students of education are asked to analyze cases in
which learners face motivational problems by means of psychological theories such as Weiner’s attribution
theory. This application of theories to solve practical problems requires knowledge of heuristics for applying
theories to cases. For example, learners need to derive diagnoses of problematic traits of behaviour on the basis
of case information and relevant definitions of theoretical concepts. They also need to derive consequences such
as predictions of future developments or suggestions for interventions from these diagnoses based on general regularities. One approach to foster this kind of reasoning during problem-based learning is collaborative argumentation. However, the quality of argumentation in these contexts is typically low (Stegmann et al., 2007). The
instructional approach of collaboration scripts (Kollar, Fischer & Hesse, 2006) was developed as a kind of
socio-cognitive scaffolding to overcome these problems and has proven rather successful in increasing the
quality of argumentation (e. g. Stegmann et al., 2007). The fading of these scaffolds has been suggested as an
important means to enable learners to internalize the information contained in them (e. g. Pea, 2004). It could be
demonstrated that distributed monitoring of a learner’s steps by a learning partner can increase the effectiveness
of fading with respect to the internalization of the target skill (here: argumentation; Wecker & Fischer, 2007). It
is still an open question, however, how the fading of such scripts affects the acquisition of more domain-specific
application-oriented knowledge and whether distributed monitoring interferes with these effects. These
questions were investigated in the present study.

Methods
The participants were 143 students of education in two designs: One was a group design with “no
script”, “script” and “script with fading” conditions. Furthermore, a 2x2-factorial design with the factors
“fading” (no/yes) and “distributed monitoring” (no/yes) was implemented. In discussion boards for dyads,
learners wrote counterarguments to case analyses based on attribution theory. Beforehand, they read texts on the
theory and on argumentation. The script was implemented in the user interface of the discussion board as text
boxes, instructions and explanatory examples. Distributed monitoring was implemented by prompting one of the
two learners to provide feedback to his or her learning partner’s steps in formulating each counterargument. Per
dyad one person who had not provided feedback was included in the analysis. Fading was implemented mainly
by gradually replacing the specific instructions in the script with unspecific ones. First the learners filled in
questionnaires and read the texts on attribution theory and argumentation. Then they collaborated for 80 minutes
in the learning environment. Finally, they completed the post-tests. Application-oriented knowledge was
measured based on the learners’ own case analyses from the post-test, operationalized as the number of
segments containing components necessary for complete case analyses (i. e. proportion of correct diagnoses and
number consequences drawn per diagnosis).

Results
Effects of script and fading on knowledge on application-oriented knowledge. The null hypothesis of
identical means could not be rejected with respect to the proportion of correct diagnoses (F(2; 35) = 2.18; p =
.13; η2 = .11), with a lower mean in the script condition. Yet a significant difference was found with respect to
the number of consequences drawn per diagnosis, with the highest levels in the script and fading condition,
followed by the script condition (F(2; 35) = 3.56; p < .05; η2 = .17).
Effects of fading and distributed monitoring on application-oriented knowledge. For the proportion of
correct diagnoses, a significant main effect fading was found for fading (F(1; 57) = 8.66; p < .01; η2 = .13), with
higher numbers of consequences in the fading conditions. No main effect for distributed monitoring (F(1; 57) =
1.10; n. s.; η2 = .02) and no interaction effect (F(1; 57) < 1; n. s.; η2 = .01) could be detected. No effect
whatsoever was found for the number of consequences drawn per diagnosis (all F(1; 57) < 1; n. s.; η2 = .01).

Discussion
These results indicate that the effects of a script targeted at a specific skill such as argumentation on application-oriented knowledge can be increased by the fading of the script: In the faded script condition, the
balance of diagnoses and consequences in case analyses was more even, thereby yielding more systematic case
analyses. Distributed monitoring does not appear to interfere with the positive effects of fading. This finding
might be explained by the increased availability of cognitive resources for schema induction through the fading
of scripts (cf. Renkl & Atkinson, 2003). Therefore, the fading of scripts in collaborative problem-based learning
seems to be an appropriate means to foster multiple instructional goals related to the acquisition of applicationoriented knowledge.

Teaching students how to improve their collaboration: Assessing
collaboration quality and providing adaptive feedback in a CSCL setting
Anne Meier, Institute of Psychology, University of Freiburg, Germany
Eleni Voyiatzaki, George Kahrimanis, HCI Group, Electrical and Computer Engineering Department.
University of Patras, Greece
Nikol Rummel, Hans Spada, Institute of Psychology, University of Freiburg, Germany
Nikolaos Avouris, HCI Group, Electrical and Computer Engineering Department. University of Patras, Greece
In the present study, we explored the possibility of giving adaptive feedback to students based on an
offline assessment of their collaboration on a joint CSCL task with the help of a multi-dimensional rating
scheme. The rating scheme is based on the one proposed by Meier, Rummel, & Spada (2007). One goal of our
experiment was to test whether the theoretical model underlying our assessment tool would be able to capture
the relevant aspects of students’ collaboration in a CSCL setting that differs strongly from the one in which the
rating scheme had originally been developed (communication channel: chat vs. videoconferencing; task domain:
programming vs. medical decision making; group composition: dyads with homogenous prior knowledge vs.
interdisciplinary dyads). Most importantly, however, we wanted to use this theoretical model to teach students
how to improve their collaboration. This was done by integrating an innovative instructional unit in a regular
computer-science class at the University of Patras, Greece. The core of this instructional unit was an adaptive
feedback component in a sequence of CSCL activities: Students collaborated on two subsequent tasks in the
domain of programming. In between, they received adaptive feedback about their collaboration on the first,
based on a feedback scheme that corresponds to the assessment tool’s dimensions. Effects of two different kinds
of feedback, either generic or adaptive, were assessed in students’ collaboration on the second task in a betweensubjects design. We expected that giving feedback that is adapted to an assessment of students’ collaboration
quality would help students to focus on those aspects of their collaboration that need the most attention, and thus
would be more effective than information about what constitutes successful collaboration in general. Thus, even
though giving adaptive feedback requires instructors to invest more time and effort, we hope to show that it will
also lead to better teaching results. Data analysis in this experiment is still under way, but the first observations
made during its implementation are promising.

Method
46 first-year computer science students of the University of Patras, Greece, participated on a voluntary
basis. 23 homogenous dyads were formed based on the prior knowledge. The same student dyads collaborated
on two tasks, in two consecutive weeks, as part of an introductory programming course. All students interacted
through Synergo (Avouris, Margaritis, Komis, 2004), a network-based, synchronous, collaborative drawing tool
including a shared whiteboard for building the algorithm flow-chart, and a chat for exchanging short messages.
Synergo includes a playback tool which allows a reliable reproduction of the activity using sequential event
logfiles. In that way, evaluators can review the whole activity, navigate through different episodes, and assess
the quality of students’ collaboration. The experiment lasted four weekly lab sessions; the two CSCL tasks were
given two students in the second and third week, respectively. In both tasks, students were asked to
collaboratively build a diagrammatic representation (flow-chart) of an algorithm in Synergo, based on a written
description of its properties and behavior. The first task introduced the loop structure as a new concept. The
second task used the same algorithmic structures, but was more difficult to implement.
The quality of students’ collaboration on the first CSCL task was assessed on the six dimensions of the
adapted rating scheme: collaboration flow (i.e. the degree to which students actions and/or utterances build upon
each other), sustaining mutual understanding (i.e. working towards “common ground”), giving explanations
(i.e. self- and other-directed explanations), argumentation (i.e. engaging in a critical discussion), structuring the
problem solving process (i.e. coordination of activities, including time management), and cooperative
orientation (e.g., constructive handling of disagreements). These dimensions were similar in their purpose and

scope to a subset of the original nine dimensions of the rating scheme by Meier et al. (2007), but had been
defined and illustrated with examples to fit the new CSCL setting.
The corresponding feedback scheme contained a generic description, a positive feedback module and a
negative feedback module for each of the six dimensions. For example, the generic module for “giving
explanation” read: “It is important for you to learn from your partner’s knowledge, and let him learn from you.
Therefore, ask for explanation if you have not completely understood what your partner is doing, and be sure
that you explain understandably your own actions and reasoning.” The positive feedback for the same
dimension was: “Your activities show that you are putting effort into explaining to each other what you are
doing. Keep up with this good practice!”; and the negative feedback was: “Your activities show that you need to
give more explanations of what you are doing in order to improve the quality of your collaboration and your
joint solution.“ Prior to their collaboration on the second task, students received domain specific feedback on
typical mistakes, and feedback on their collaboration according to experimental condition: Dyads in the generic
feedback condition received only the generic feedback for each dimension, in the form of a short instructional
text. They were told that this was general advice on how to collaborate well. Dyads in the adaptive feedback
condition received adaptive feedback in addition to the generic feedback. Adaptive feedback was selected by
determining the two dimensions with the highest ratings and the two dimensions with the lowest ratings.
Ambiguities were resolved by a set of rules and a hierarchy of dimensions. The positive feedback modules were
added to the generic feedback for the two “best” dimensions and the negative feedback modules were added to
the generic feedback for the two “worst” dimensions. Dyads in the control condition received no feedback on
their collaboration, only task-specific feedback. However, they were encouraged to discuss their collaboration
on the first task with each other. Students in all conditions were informed that the feedback was based on careful
observation of their collaboration on the first task. Dyads in all conditions were given 20 minutes to read the
feedback and to discuss what they could do to improve their collaboration in the upcoming task.

First Results and Outlook
Data analysis in this experiment is still under way, so only some preliminary observations can be
reported. First of all, the adaptation of the rating scheme to the new CSCL setting was successful: The two raters
involved in assessing students’ collaboration on the first task report that the dimensions were able to capture the
most important aspects of students’ collaboration, and that it was helpful in selecting the adaptive feedback.
Measures of inter-rater reliability for the rating scheme are currently being obtained in a sample of logfiles from
both tasks and all three conditions. Students accepted the adaptive feedback given to them based on these ratings
as genuine, and discussed it actively. On the other hand, students in the remaining conditions, in particular the
control condition, reported that they found it difficult to think of ways in which they might improve their
collaboration on the second task. Therefore, we are optimistic that our analysis of students’ collaboration on the
second task will confirm our expectations that feedback, in particular of the adaptive kind, was effective in
helping students to collaborate better. We will look in detail into the effects of generic versus adaptive feedback
on students’ discussion of their past and upcoming collaboration, and their actual interaction during the second
task. We see further potential use of our rating and feedback schemes in teaching students how to improve their
collaboration as part of their general curriculum, but also in training teachers to provide adaptive feedback on
students’ collaboration.

Adaptive Domain Support for Computer-Mediated Peer Tutoring
Erin Walker, Human Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA, USA
Nikol Rummel, Institute of Psychology, University of Freiburg, Germany
Kenneth R. Koedinger, Human Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA,
USA
Reciprocal peer tutoring, where two students of similar abilities take turns tutoring each other, has been
shown to promote the domain learning of students involved (Fantuzzo, Riggio, Connelly & Dimeff, 1989). For
tutoring to be effective, the peer tutor must provide conceptual, elaborated help (Fuchs et al., 1997) that
addresses tutee misconceptions (VanLehn, Siler, Murray, Yamauchi, & Bagget, 2003) and ultimately guides the
tutee to a correct solution. Previous efforts at assisting peer tutoring have focused on training peer tutors or
structuring the tutoring process; for example, King, Staffieri, and Adelgais (1998) found that having students
ask each other a series of questions at different levels of depth had a significantly positive effect on tutor
learning. Adaptive support for the peer tutor may be an improvement over these fixed approaches, as it would
provide help tailored to individual peer tutors at the moments when it is most needed. We have been taking steps
in this direction by augmenting the Cognitive Tutor Algebra (Koedinger, Anderson, Hadley, & Mark, 1997), a
successful intelligent tutoring system, with peer tutoring activities. The domain models of the tutoring system,

which typically provide algebraic help, can then be leveraged in order to provide adaptive domain support to the
student collaboration. The following paragraphs describe the design and effects of this adaptive domain support.

Design of Adaptive Domain Support
During typical classroom use of the Cognitive Tutor Algebra, students can ask for a hint from the
intelligent tutoring system at any time, and receive immediate feedback on errors as they solve problems. In the
design of domain support for collaboration, we balanced two competing concerns: students should have similar
access to domain help as in individual use of the intelligent tutor, but the interaction between collaborators
should be encouraged by the tutoring system, not constrained or disrupted. Therefore, just like in individual use,
domain hints are available on demand. However, unlike individual use of the tutor, hints are requested by and
given to the peer tutor, and the person solving the problem (the peer tutee) must interact with the peer tutor via
chat in order to receive the help. In the process of communicating the hint to the peer tutee, the peer tutor could
learn by interpreting the hint and explaining it in a way that addresses the peer tutee’s impasse. Similarly,
system feedback is given based on peer tutor responses rather than tutee problem-solving actions, and is
displayed only to the peer tutor (e.g., when a peer tutor marks a step correct when it is wrong, as in Figure 1).
Both hints and error feedback consist of a prompt for students to interact, followed by the domain help
individual learners would ordinarily receive. This collaborative feedback was implemented through the
development of a separate collaborative tutor module to augment the cognitive tutor module. Every time that
the peer tutee takes an action in the interface, the response by the cognitive tutor is stored in the collaborative
tutor, rather than presented as feedback to the peer tutee. Then, when the peer tutor takes an action in the
interface by marking a step right or wrong, the collaborative tutor compares the peer tutor’s action on that
particular widget to the stored cognitive tutor’s response for the widget. If the two actions fail to match, then the
collaborative tutor sends the peer tutor the feedback shown in Figure 1. Similarly, the collaborative tutor module
stores the hint that the cognitive tutor would have sent for each step, and presents it to the peer tutor on demand,
along with a prompt to collaborate.

Figure 1. Equation solver subsection of peer tutor interface. Full interface also contains chat window.

Method
We conducted a classroom study in five second-year algebra classes in which we compared three
conditions: In the adaptive condition students tutored each other with adaptive support and problem answers
available (17 participants), In the non-adaptive condition students tutored each other simply with problem
answers available (14 participants), and in the individual condition students used the cognitive tutor individually
(20 participants). To evaluate the effects of the adaptive domain support, we examined how student learning and
progression through the intervention problems was affected by student collaboration and type of support. We
hypothesized that collaborating students would solve fewer problems successfully than students working alone,
potentially due to the increased interaction between the peer tutor and the tutee. However, the number of
attempts made per problem should be similar for collaborating students given adaptive support and individual
learners, as both conditions would have access to the same cognitive tutor help. Attempts per problem should be
greater for collaborating students without adaptive support, as those students would not have as much access to
help. The adaptive condition should have a greater effect on learning than the non-adaptive and individual
conditions because it provides the advantages of both collaboration and of domain feedback.

Results and Discussion
In line with our hypotheses, our analyses indicate that number of problems solved per hour in the
individual condition was greater than the number of problems solved per hour by dyads in the non-adaptive
support condition and adaptive support condition (Ms = 47.04, 13.33, 17.65; SDs = 30.24, 7.71, 5.69; F(2,34) =
8.64; p < .01), but surprisingly, the total number of incorrect attempts per problem made by each student over
the intervention was not significantly different between conditions when pretest was considered as a covariate
(Ms = 1.46, 1.81, 2.46; SDs = 1.26, 1.04, 1.87; F(2, 47) = 2.480; p = .1). Although all students learned from
pretest to posttest, there were no significant differences in learning gains between conditions.
The results indicate that students progressed similarly through the problems in all conditions, and
therefore that the domain support given to peer tutors was sufficient for facilitating the tutoring interaction.
However, it is surprising that the attempts per problem in the non-adaptive and adaptive conditions were
equivalent. Further analysis is necessary to look both at problem-solving behaviors at different intervention
phases, and at how students used the help that was provided. In summary, we have leveraged existing domain
models to develop a collaborative tutor that provides adaptive domain support for peer tutoring. This technology
will serve as a promising basis for the development of more sophisticated adaptive scripting.

Open Problems in Dynamic Collaborative Learning Support
Carolyn Penstein Rosé, Rohit Kumar, Gahgene Gweon, Yi-Chia Wang, & Mahesh Joshi
Language Technologies Institute / Human Computer Interaction Institute, Carnegie Mellon University,
Pittsburgh, PA, USA
State-of-the-art forms of collaboration support play a role similar to training wheels on bicycles. As is
well known, however, training wheels must eventually come off. And typically, they are removed by a watchful
parent, who may decide after watching their child fall a few times, to put them back on for a time until the child
has developed further in their own coordination and balance. In a similar vein, the learning sciences literature
tells us that scaffolding should be faded over time (Collins, Brown, & Newman, 1989), and that over-scripting
or unnecessary support may be detrimental to collaboration or demotivating (Dillenbourg, 2002).
This model of a watchful parent requires that the collaborative learning environment is able to track
what is happening in the collaboration between students. Thus, a major goal of our research is to support
collaboration in a way that is responsive to what is happening in the collaboration rather than behaving in a “one
size fits all” fashion. For example, rather than providing prompts to elicit reflection from students whether or not
they have already shown evidence of doing so spontaneously, we aim to monitor the behavior of students, and
prompt positive behaviors that are lacking or discourage negative behaviors we detect. To this end, we have
made substantial progress towards automatically replicating multi-dimensional process analyses of collaborative
learning discussions (Wang, Joshi, & Rosé, 2007; Rosé et al., in press), towards tracking topics discussed in
collaborative discussions (Wang & Rosé, 2007; Kumar, Rosé, Wang, Joshi, & Robinson, 2007), and estimating
level of learning during conversations (Joshi & Rosé, 2007). A running theme through this work has been the
development of a methodology for creatively encoding the raw conversational data in such a way that enables
state-of-the-art machine learning technology to identify consistent and generalizable patterns in the data. In
addition to developing basic technology to support our own work, we have also worked towards providing
resources to other researchers interested in the problem of dynamic collaborative learning support by developing
the publicly available TagHelper tool set (http://www.cs.cmu.edu/~cprose/TagHelper.html) as well as a distance
course called Machine Learning in Practice (http://www.cs.cmu.edu/~cprose/MachineLearningInPractice.html).
With this technology in hand, we have begun to move past the traditional one-size-fits-all non-adaptive
approaches to collaboration support. As part of our experimental approach, we have typically contrasted
individuals and pairs, with and without support, where students communicated with each other and with the
interactive support within a typed chat interface. Our purpose for doing so was to separate the direct effect of the
support on learning from the indirect effect it has on learning by improving collaboration. We have conducted a
series of studies in which we experimentally investigated foundational issues related to the design of dynamic
support for on-line collaborative learning (Gweon, Rosé, Zaiss, & Carey, 2006). One question we started with
was whether the context sensitive support, because it would be offered much less frequently than “one-size-fitsall” support that is administered whether it is needed or not, would result in any significant effect on the
learners’ experience whatsoever. Fortunately, these initial investigations demonstrated that explanation
elicitation prompts delivered strategically, on an as needed basis, were effective for eliciting explanation
attempts as well as increasing learning in a collaborative learning setting. Moving beyond simple prompts to
interactive instructional agents that can engage students in reflective dialogues, we have also run two successful
pilot studies in which we used these dialogue agents to deliver interactive support to collaborative learners when
triggered by an automatic analysis of the collaborative learning discussions as they unfolded (Wang & Rosé,

2007; Kumar et al., 2007). In both of these successful studies, the fully automatic interactive support lead to
significant increases in learning in comparison to a control condition where students worked individually and
did not have the interactive support, in one case achieving an increase in learning gains of 1.24 standard
deviations above that of the control condition.
While our results with dynamic collaborative learning support to date have been encouraging, close
inspection of the conversational data from our recent investigations with that technology indicate that we have
far to go to reach our ultimate goal. For example, although in one study we determined that students with
dynamic support were more likely to engage in transactive forms of conversation (Wang & Rosé, 2007), in
another study (Kumar et al., 2007) we noticed that although the degree to which students engaged in transactive
patterns of discussion correlated with their learning, the condition where students received the dynamic support
for collaborative learning showed marginally lower levels of transactive discussion although the students in that
condition learned significantly more than in the control condition. Furthermore, although the conversational
agents that delivered the support were effective for increasing reflection and learning with students whether or
not they were working collaboratively, students who worked individually appeared to engage more deeply in
their interaction with the agents, whereas students who worked in pairs tended to “talk around” the agents rather
than treating them as participants in the conversation the way the individual students did, although positive
effects of the agents on learning demonstrate that the students are not ignoring the content of the agent’s
utterances. Thus, we see that the three way interaction between pairs of students and the collaborative support
agents is not as effective as it could be. Overall, students benefit most from working together with the support of
the agents, although the agents somewhat degrade the quality of the interaction between students in some cases,
and similarly the collaboration sometimes interferes with the interaction the students have with the agents.
Furthermore, evidence from questionnaire data and informal discussions with students indicate that students
perceive the conversational support agents as an interruption. We suspect that the fact that the support we are
offering is seen as an active part of the interaction rather than a passive part of the environment in which the
interaction takes place changes the way it affects the conversational behavior of the students. While much
valuable work investigating the design space for static forms of support for collaborative learning provides a
valuable starting place for our investigation, a continued effort is needed. Thus, a major focus of our current
work is on investigating the design space for interactive agents for supporting collaborative learning processes,
and more recent results offer evidence that agents with richer conversational behaviors are more effective for
engaging student attention in a collaborative learning context.

References
Anderson, J. R., Corbett, A. T., Koedinger, K. R., & Pelletier, R. (1995). Cognitive tutors: Lessons learned.
Journal of the Learning Sciences, 4(2), 167-207.
Avouris N., Margaritis M., & Komis V. (2004). Modelling interaction during small-group synchronous problem
solving activities: The Synergo approach. 2nd International Workshop on Designing Computational
Models of Collaborative Learning Interaction, ITS 2004, Brazil.
Cohen, E. G. (1994). Restructuring the classroom: Conditions for productive small groups. Review of
Educational Research, 64, 1-35.
Collins, A., Brown, J. S. & Newman, S. E. (1989). Cognitive apprenticeship: Teaching the crafts of reading,
writing, and mathematics. In L. B. Resnick (Hrsg.), Knowing, learning, and instruction (S. 453-494).
Hillsdale, NJ: Erlbaum.
Dillenbourg, P. (2002). Over-scripting CSCL: The risks of blending collaborative learning with instructional
design. In P. A. Kirschner (Ed.), Three worlds of CSCL. Can we support CSCL (pp. 61-91). Heerlen:
Open Universiteit Nederland.
Dillenbourg, P., Baker, M., Blaye, A., & O’Malley, C. (1995). The evolution of research on collaborative
learning. In P. Reimann & H. Spada (Eds.), Learning in humans and machines: Towards an
interdisciplinary learning science (pp. 189-211). Oxford: Elsevier/Pergamon.
Fantuzzo, J. W., Riggio, R. E., Connelly, S., & Dimeff, L. A. (1989). Effects of reciprocal peer tutoring on
academic achievement and psychological adjustment: A component analysis. Journal of Educational
Psychology. 81(2), 173-177.
Fuchs, L., Fuchs, D., Hamlett, C., Phillips, N., Karns, K., & Dutka, S. (1997). Enhancing students’ helping
behavior during peer-mediated instruction with conceptual mathematical explanations. The Elementary
School Journal, 97(3), 223-249.
Gweon, G., Rosé, C. P., Zaiss, Z., & Carey, R. (2006). Providing support for adaptive scripting in an on-line
collaborative learning environment. Proceedings of CHI 06: ACM conference on human factors in
computer systems. New York: ACM Press.
Joshi, M. & Rosé, C. P. (2007). Using transactivity in conversation summarization in educational dialog. To
appear in the Proceedings of the SLaTE Workshop on Speech and Language Technology in Education.

King, A., Staffieri, A., & Adelgais, A. (1998). Mutual peer tutoring: Effects of structuring tutorial interaction to
scaffold peer learning. Journal of Educational Psychology, 90, 134-152.
Koedinger, K. R., Anderson, J. R., Hadley, W. H., & Mark, M. A. (1997). Intelligent tutoring goes to school in
the big city. International Journal of Artificial Intelligence in Education, 8, 30-43.
Kollar, I. & Fischer, F. (2007). Supporting self-regulated learners for a while and what computers can
contribute. Journal of Educational Computing Research, 35(4), 425-435.
Kollar, I., Fischer, F., & Hesse, F. W. (2006). Collaboration scripts - a conceptual analysis. Educational
Psychology Review, 18 (2), 159-185.
Kumar, R., Rosé, C. P., Wang, Y. C., Joshi, M., Robinson, A. (2007). Tutorial dialogue as adaptive
collaborative learning support. Proceedings of the 13th International Conference on Artificial
Intelligence in Education (AI-ED 2007), Amsterdam: IOSPress.
Meier, A., Spada, H., & Rummel, N. (2007). A rating scheme for assessing the quality of computer-supported
collaboration processes. International Journal of Computer-Supported Collaborative Learning, 2 (1),
63-86.
O’Donnell, A. M. (1999). Structuring dyadic interaction through scripted cooperation. In A. M. O’Donnell & A.
King (Eds.), Cognitive perspectives on peer learning (pp. 179-196). Mahwah, NJ: Lawrence Erlbaum
Associates.
Pea, R. D. (2004). The social and technological dimensions of scaffolding and related theoretical concepts for
learning, education, and human activity. Journal of the Learning Sciences, 13(3), 423-451.
Pfister, H.-R., & Mühlpfordt, M. (2002). Supporting discourse in a synchronous learning environment: The
learning protocol approach. In G. Stahl (Ed.), Proceedings of the Computer Support for Collaborative
Learning (CSCL) 2002 Conference (pp. 581-589). Hillsdale, NJ: Lawrence Erlbaum Associates.
Renkl, A. & Atkinson, R. K. (2003). Structuring the transition from example study to problem solving in
cognitive skill acquisition: A cognitive load perspective. Educational Psychologist, 38(1), 15-22.
Rosé, C. P., Wang, Y.C., Cui, Y., Arguello, J., Fischer, F., Weinberger, A., & Stegmann, K. (in press).
Analyzing collaborative learning processes automatically: Exploiting the advances of computational
linguistics in computer-supported collaborative learning. Submitted to the International Journal of
Computer Supported Collaborative Learning.
Slavin, R. E. (1996). Research on Cooperative Learning and Achievement: What we know, what we need to
know. Contemporary Educational Psychology, 21(1), 43-69.
Soller, A., Jermann, P., Muehlenbrock, M. & Martinez, A. (2005). From Mirroring to Guiding: A Review of
State of the Art Technology for Supporting Collaborative Learning. International Journal of Artificial
Intelligence in Education, 15(4), 261-290.
Stegmann, K., Wecker, C., Weinberger, A. & Fischer, F. (2007). Collaborative argumentation and cognitive
processing – An empirical study in a computer-supported collaborative learning environment. In C. A.
Chinn, G. Erkens & S. Puntambekar (Eds.), Mice, minds and sociecty. Proceedings of the Computer
Supported Collaborative Learning (CSCL) Conference 2007, Vol. 8, Part 2 (pp. 661–670).
International Society of the Learning Sciences.
Teasley, S. D. (1995). The role of talk in children’s peer collaborations. Developmental Psychology, 31(2), 207220.
VanLehn, K., Siler, S., Murray, C., Yamauchi, T., & Baggett, W. (2003). Why do only some events cause
learning during human tutoring? Cognition and Instruction, 21(3), 209-249.
Wang, Y. C., Joshi, M., & Rosé, C. P. (2007). A feature based approach for leveraging context for classifying
newsgroup style discussion segments. Proceedings of the Association for Computational Linguistics
Wang, H. C. & Rosé, C. P. (2007). Supporting collaborative idea generation: A closer look using statistical
process analysis techniques. Proceedings of Artificial Intelligence in Education
Wecker, C. & Fischer, F. (2007). Fading scripts in computer-supported collaborative learning: The role of
distributed monitoring. In C. A. Chinn, G. Erkens & S. Puntambekar (Eds.), Mice, minds and sociecty.
Proceedings of the Computer Supported Collaborative Learning (CSCL) Conference 2007, Vol. 8, Part
2 (pp. 763–771). International Society of the Learning Sciences.

Acknowledgments
Meier et al.: This research project was partly funded by the Kaleidoscope Network of Excellence project
Cavicola (Computer-based Analysis and Visualization of Collaborative Learning Activities).
Wecker & Fischer: This research project was funded by Deutsche Forschungsgemeinschaft (DFG).

Work-in-Progress

CHI 2015, Crossings, Seoul, Korea

Understanding Users’ Creation of
Behavior Change Plans with TheoryBased Support
Jisoo Lee

Winslow Burleson

Abstract

School of Arts, Media +

School of Computing,

Engineering, Arizona State

Informatics, and Decision

University, Tempe, AZ 85281

Systems Engineering, Arizona

jisoo.lee@asu.edu

State University, Tempe, AZ

The goal of this project is to develop tools that support
users’ creation of their own behavior-change plans. We
conducted two formative user studies to explore
people’s creation of plans for their own behavioral
goals. Users were provided with minimal support to
facilitate goal-setting, use of behavior-change
techniques, and self-monitoring. In this paper, we
present insights on how to further facilitate
personalization of behavior-change plans.

85281
Erin Walker

winslow.burleson@asu.edu

School of Computing,
Informatics, and Decision

Eric B. Hekler

Systems Engineering, Arizona

School of Nutrition & Health

State University, Tempe, AZ

Promotion, Arizona State

85281

University, Phoenix, AZ 85004

Author Keywords

erin.a.walker@asu.edu

ehekler@asu.edu

Behavior change; Goal-setting; Behavioral plans

ACM Classification Keywords
Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights
for third-party components of this work must be honored. For all other
uses, contact the Owner/Author.
Copyright is held by the owner/author(s).
CHI'15 Extended Abstracts, Apr 18-23, 2015, Seoul, Republic of Korea
ACM 978-1-4503-3146-3/15/04.
http://dx.doi.org/10.1145/2702613.2732870

H.5.m. Information interfaces and presentation (e.g.,
HCI): Miscellaneous.

Introduction
Support for people’s behavior change is a popular topic
in human-computer interaction (HCI) [e.g., 6]. Digital
technologies have the potential to help people achieve
personal goals like exercising or working more
efficiently. The majority of behavior-change
technologies emphasize pre-determined strategies for
fostering behavior change, chosen by the designer (see

2301

Work-in-Progress

CHI 2015, Crossings, Seoul, Korea

related work). It is likely that individuals will have
idiosyncratic needs that cannot be anticipated via prefabricated solutions. This problem will likely only
become more pronounced as behavior-change
technologies are more ubiquitous and individuals use
them for months or years rather than days or weeks.

Figure 1. In Study 1, the
concept of each technique was
delivered with a short sentence of
its key idea and an example
depicted in storyboard-format
(top); in Study 2, we provided
more informative, recorded
narratives for key ideas and
examples, with suggestive
pictures (bottom).

We are exploring an alternative strategy. The long-term
goal is to develop a do-it-yourself self-experimentation
toolkit that includes a “design support tool” focused on
teaching fundamentals in creating personalized
behavior-change plans [9] and an end-user
programmable behavior-change technology that can be
used to facilitate the plans individuals develop [5, 9].
The purpose of this paper is to describe formative work
in developing the design support tool. We conducted
two user studies to better understand common pitfalls
users experience while creating behavior-change plans,
and develop a tool that can address those pitfalls.

Related Work
As the trend progresses towards technology-enriched
environments, researchers have been increasingly
exploring the use of technology to promote behavior
change for topics like health or energy conservation [2,
4], often drawing on behavioral theories [6]. For
example, King et al. [7] previously developed three
smartphone apps focused on improving mid-life and
older adult’s physical activity. The apps included a more
game-like app focused on increasing someone’s
positive emotions for activity, a socially-oriented app
focused on increasing a person’s awareness of the
activity of others, and a rationally-driven app focused
on helping individuals set goals and track progress.
Results from this work indicated success at increasing
physical activity with each but also found preferences

among the users that shifted over time. Many
individuals requested a “mix-and-match” approach at
different times. While these pre-specified tools were
useful, formative interviews reinforced the need for
strategies to facilitate personalization over time.
Contrasted with these pre-specified plans, the
Quantified Self movement [1] reflects the sort of
activities we wish to facilitate but with a broader
segment beyond current Quantified Selfers.
Understanding how best to support this sort of selfexperimentation is still in its infancy [1].

Initial Design Support Tool
One fundamental part of our approach in helping users’
behavior change is to teach them about behaviorchange techniques so that they can apply them in their
own lives. Behavior-change techniques are “observable,
replicable, and irreducible component[s] of a
[behavioral] intervention designed to alter or regulate
behavior; that is, a technique is proposed to be an
‘active ingredient’ (e.g., feedback, self-monitoring, and
reinforcement) [11].” We chose three generic features
of a behavior-change plan to establish a generic
structure: (1) goal-setting, (2) other techniques that
can support meeting a goal (e.g., strategies such as
self-rewarding), and (3) self-monitoring to determine
success and facilitate iteration. We chose goal-setting
and self-monitoring as two required techniques and
then provided other behavior-change techniques that
cover categories identified in Michie’s behavior-change
taxonomy [11] (e.g., self-reward, prompting to action,
seeking social support). The tool guides users to
generate actionable goals, create plans applying a
variety of behavior change techniques, and do selftracking to help observation.

2302

Work-in-Progress

1

Fogg’s model proposes
‘Motivation, Ability, and
Triggers’, while Michie et al’s
COM-B model emphasized
Capability, Opportunity, and
Motivation. We collapsed
Ability and Capability, as they
are similar constructs. We
considered Triggers and
Opportunity to be related but
distinct.

2

Trigger: Define a trigger;
Information or inspiration as
triggers; Counteracting
negative emotional triggers
Opportunity: Find the
opportune/dangerous time
and place; Turn off your
“auto-pilot”; Make it the
“default” option
Ability: Script critical
actions; Shrink the change;
Build habit chains
Motivation: Define your
inspiration; Ride the wave;
Reward yourself.

CHI 2015, Crossings, Seoul, Korea

Study 1
This study was described previously [9] and thus only
briefly described here. We developed a low-fidelity
prototype of the design support tool and conducted a
user study. Participants were a convenience sample
(N=11, 9 females and 2 males, ages from 18-39) of
college/graduate students at a large US university.
Participants came in for three sessions over two weeks.
By design, the sessions were delivered by individuals
with no formal clinical training to ensure training did
not contribute to potential effects of the session design.
In the first session, we supported goal-setting by
asking individuals to think about a “New Year’s
Resolution” that they wanted to work on. They were
then asked to think of smaller sub-goals to make the
behavior more manageable. After the goal was
selected, we asked individuals to generate a plan to
reach their goal, and then prompted them to critically
examine this plan by reflecting on past experience with
this problem. Participants were then provided 13 other
techniques to incorporate into their plan and asked to
choose 3. The researcher then presented two options
for self-tracking: structured or unstructured. Structured
journaling involved creating quantitative questions
(e.g., how stressed are you on a 1-5 scale?) whereas
unstructured was a free-flow of thoughts and ideas. In
both types, participants chose specific times of day to
self-track. In the two subsequent sessions, participants
went through a similar procedure, but focused on
revising their present plans or, if they wanted, creating
new plans. Week two and three also included a brief
interview on their experience the previous week.
We found two issues. First, participant-generated goals
and plans were too vague to be actionable. Second, our

strategy for incorporating evidence-based techniques
did not work. Counter to expectations, providing the
techniques did not enrich their plan but instead
provided participants with an often false label for the
plan they had already decided upon. While participants
changed at least one behavior-change technique
between sessions, very few tweaked the first technique.
In later sessions, participants reported ‘burning out.’
This suggested they may not have been self-diagnosing
the true problems for achieving the goal. In particular,
most emphasized motivation and did not think much
about other factors. This insight led us to provide a
behavior-change framework to support self-diagnosis.

Revised Design Support Tool
Based on these findings, we created a revised
prototype. To facilitate actionable goals, we adopted an
evidence-based goal-setting strategy, the SMART
(Specific, Measurable, Actionable, Realistic, and Timely)
goal concept [8], which is a reinterpretation of Locke
and Latham’s goal setting theory [10]. According to the
SMART concept, goals that meet each of the acronym’s
words (e.g., specific, measurable, actionable) will result
in more effective goals. To support self-diagnosis, we
categorized techniques via meta-models of behavior
that, like our generic plan, could be used across a wide
range of behaviors. We leveraged two existing metamodels, Fogg’s behavior model [3], and Michie’s COM-B
model [12], which were developed to help professionals
create interventions. We organized techniques into four
domains1: Opportunity (availability to engage in a
behavior), Triggers (prompts to perform the behavior),
Ability (having the required skills/attributes to perform
the behavior), and Motivation (drive to do the
behavior). Existing behavior-change techniques were
labeled with each domain.2

2303

Work-in-Progress

CHI 2015, Crossings, Seoul, Korea

Study 2

P6’s plan
(Session 1)
SMART goal: Spend at least 1
hour per day (5 days per week)
writing dissertation
Plan to apply the techniques:
Write at home at desk. Dayn
send reminder text or verbal
every day. Post-it-note on
bathroom mirror. Timer. No
looking at other stuff during
writing time (Facebook, email,
etc.)
(Session 2)
SMART goal:
Spend at least 1 hour per day (5
days per week) in the morning at
home working on dissertation. If
there is an unusual event and I
cannon complete my goal then I
can have a make up day on the
weekend.
Plan to apply the techniques:
Script Critical Actions: Wake up,
alarm clock in room and one
outside of room to make sure I
get out of bed. Then shower and
make breakfast. Look at email
while eating breakfast and set
time to start working. When time
comes close all other things
(facebook, email, news, etc.) and
start. Remember you want to
graduate!

Method
Participants were a convenience sample (N=7; 5 female
and 2 male; with one dropout) of graduate students at
a large US university. Similar to study 1, they were
asked to participate in three sessions focused on
creating behavior-change plans.
In Session 1, participants chose an issue they would
like to work and to choose a ‘behavioral goal.’ Then, we
taught the SMART goal concept and asked participants
to create a SMART goal. Participants were taught one
technique from each of the domains and then asked to
generate a plan on how they would use each technique.
For each behavior-change technique, we provided a
recorded narrative (30-40sec) describing the technique,
including an example. Participants were given a chance
to either incorporate or ignore each technique. In
session two & three, participants reflected on the
quality of their SMART goal and revised as necessary.
In session two, they were taught the framework and
informed that the four techniques taught in session one
were examples of each domain. Participants were asked
to self-diagnosis the most problematic domain for them
(i.e., is this a trigger, opportunity, motivation, or ability
problem?) and then presented two more techniques for
the problem domain. Participants were presented the
same options for self-tracking as Study 1.
In this study we used both qualitative and quantitative
strategies to understand how individuals develop
behavior-change plans. We used a survey to examine
session experience, and conducted a semi-structured
interview after each session to glean insights about the
process. To analyze the data, the lead author listened

to all recordings (both of the interviews and the
sessions) and documented themes that arose.
Results
Compared with Study 1, goals the participants set were
more specific (e.g., ‘Study every night’ vs. ‘Spend at
least 1 hour per day, 5 days per week’), and the
participants reported appreciating the SMART goal.
However, the goals and plans were still not as
specific/actionable as the SMART concept would
prescribe. For instance, many individuals (57%, n=4)
found that they were unsure how best to set both a
specific AND realistic goal. For example, P3 often had
guests or dinner appointments with friends. Based on
this, she said that she would write during the day but
not set a specific time.
Participants appeared to demonstrate better use of the
behavior-change techniques compared to Study 1.
Unlike Study 1, participants reported liking and actively
using the behavior-change techniques when creating
their plans (e.g., P5, ‘It’s good to have all of them at
once’). However, most participants did not understand
how to develop a good trigger (71.5%. n=5) or script a
critical action (71.5%. n=5) during session one. Many
participants set triggering times that were not at the
time when they would engage in the activity (a
requirement for a good trigger). P4, for example, set a
notification on her mobile phone at 12p to remind her
to work at 2p. These small details were not grasped at
first but did start to be understood after one week of
experience. We also found that the example given for a
technique greatly impacted how creative most
individuals were personalization. Specifically, most
participants used the triggering example (i.e.,
notification from the phone) as the only type of trigger.

2304

Work-in-Progress

CHI 2015, Crossings, Seoul, Korea

While this may be fine, it is plausible that they did not
personalize it enough to make the technique useful for
themselves. For example, P2’s “trigger” to be more
empathic was the pressure he felt from his ring when
he shook hands. This was a creative personalization
that was not common, but potentially very valuable.

during session one but after trying it out, found it to be
an essential and important technique. P6 found that
there were a couple of routines she did before starting
her work and carrying these routines out swiftly led her
to successfully start working on her target activity.

DISCUSSION AND CONCLUSIONS
Compared to study 1, there also appear to be improved
understanding on how to iterate on the concepts (which
was supported by better self-reported success in
achieving the goals in study 2 compared to 1). This
seemed strongly influenced by the person’s personal
experience using the plans. Unlike study 1, plans
almost always (86%, n=6) became more elaborate and
personalized to the person’s daily life. For instance, P7
set her target time to go to bed differently for
weekdays and weekends, which was not differentiated
initially, based on her failure the previous weekend. P4
originally set a goal of "work for 2 hours" but during
session two changed it to a more actionable goal of
“practice speaking through presentation twice per day.”
Participants also presented more vivid descriptions on
how they carried out their plans. For instance, while
initially P6 was going to work “in the morning,” it was
changed into “after having breakfast, and checking
emails and news.” Active use of the techniques also
appeared to help them better diagnose the domain to
work on (86%, n=6). For instance, P4 chose ‘Ability’ as
her problematic domain in session two. Upon further
reflection though, she realized it was a motivational
problem as she prioritized friends over work.
Lastly, most participants (71%, n=5) demonstrated far
better understanding of the different techniques after
trying them out. For example, many participants did
not understand the idea of scripting critical actions

Overall, results indicate that our convenience sample
could develop and refine their behavior-change plans
and it appears that plans were better refined in our
second user study. Of particular importance, our
findings suggest that the addition of a SMART goal and
including a meta-model to help understand behaviorchange techniques did enable individuals to more
rapidly self-diagnose and improve upon their behaviorchange plans. This iterative improvement did not occur
during Study 1, and thus is an important finding. While
we did see self-reported improvements, we still found
some problems persisting related to supporting the
development of personalized plans. Specifically, we still
found the continued influence of the examples given on
establishing the perceived range of available options for
a given technique. That said, we did find that using the
meta-model appeared to improve creativity.
A core future direction for our research is to better
understand how to further facilitate the creative
personalization of the techniques. It was clear that
participants’ experiences with the techniques improved
customization. However, we also found participants’
ideation was often constrained by the examples we
provided for each technique. We believe that more
research on facilitating increased creativity and the
techniques will be important for aiding individuals in
coming up with effective, personalized plans. Based on
the positive effects that experience had during the two-

2305

Work-in-Progress

CHI 2015, Crossings, Seoul, Korea

week study, it is plausible that simply giving individuals
enough time (i.e., more than two weeks) to engage
with self-diagnosis and implementation of their plan
might be enough but this requires empirical validation.
Providing more examples, particularly extreme
examples of a technique, is another strategy we plan to
expand upon to facilitate personalization.
Our study had several limitations. This was a
convenience sample of educated individuals, thus
generalizability is limited. Further, a majority of the
sample chose issues related to work (e.g., P3, ‘write a
manuscript’) and thus we did not study some important
behaviors (e.g., smoking cessation). Another limitation
is that the study lasted two weeks. Finally, researcher’s
presence in sessions and involvement in delivering
materials may have biased the individuals’ ideation.
As an initial effort in developing tools that support
users’ creation of their own behavior-change plans, we
conducted a two formative user studies. Overall, we
found that the improvements we implemented between
our first support tools and second did appear to
facilitate iterative improvement between the sessions.

Acknowledgements
This work was supported, in part, by a Google Faculty
Research Award (PI: Hekler).

References
[1] Choe, E. K., Lee, N. B., Lee, B., Pratt, W., & Kientz,
J. A. Understanding quantified-selfers' practices in
collecting and exploring personal data. CHI’14. (2014),
1143-1152.

[2] Consolvo, S., Mcdonald, D. W.,et al. A. Activity
Sensing in the Wild : A Field Trial of UbiFit Garden.
CHI’08, (2008), 1797–1806.
[3] Fogg, B. A behavior model for persuasive design.
Persuasive ’09, (2009).
[4] Grimes, A., et al. Let's play!: mobile health games
for adults. UbiComp’10 (2010). 241-250.
[5] Hekler, E.B., Burleson, W., Lee, J. A DIY selfexperimentation toolkit for behavior change. Presented
via the Personal Informatics in the Wild: Hacking Habits
for Health & Happiness Workshop at CHI’13 (2013).
http://personalinformatics.org/chi2013/hekler
[6] Hekler, E. B., Klasnja, P., Froehlich, J. E., &
Buman, M. P. Mind the theoretical gap: interpreting,
using, and developing behavioral theory in HCI
research. CHI’13, (2013) 3307-3316.
[7] King, A. C., Hekler, E. B., et al. Harnessing
different motivational frames via mobile phones to
promote daily physical activity and reduce sedentary
behavior in aging adults. PLoS One, 8, 4 (2013),
e62613.
[8] Latham, G. P. Goal Setting: A Five-Step Approach
to Behavior Change. Organiz Dyn, 32, 3 (2003), 309318.
[9] Lee, J., Walker, E., Burleson, W., Hekler, E. B.
Exploring Users’ Creation of Personalized Behavioral
Plans. UbiComp’14 Adjunct. (2014).
[10] Locke, E.A. & Latham, G.P. Building a Practically
Useful Theory of Goal Setting and Task Motivation: A
35- Year Odyssey. Am Psychol, 57, 9 (2002), 705-17.
[11] Michie, S., Richardson, M., Johnston, M., Abraham,
C., Francis, J., Hardeman, W., Wood, C. E. The
Behavior Change Technique Taxonomy (v1) of 93
Hierarchically Clustered Techniques: Building an
International Consensus for the Reporting of Behavior
Change Interventions. Ann Behav Med, 46, 1 (2013),
81–95.
[12] Michie, S., van Stralen, M. M., & West, R. The
Behaviour Change Wheel: A New Method for
Characterising and Designing Behaviour Change
Interventions. Implement Sci, 6, 1 (2011), 42.

2306

UBICOMP '14 ADJUNCT, SEPTEMBER 13 - 17, 2014, SEATTLE, WA, USA

Exploring Users’ Creation of
Personalized Behavioral Plans
Jisoo Lee

Winslow Burleson

Abstract

School of Arts, Media +

School of Computing,

Engineering, Arizona State

Informatics, and Decision

University, Tempe, AZ 85281

Systems Engineering, Arizona

jisoo.lee@asu.edu

State University, Tempe, AZ

As an initial effort in developing tools that support
users’ creation of their own behavior-change plans, we
conducted a formative user study. We intended to
explore people’s creation of plans for their own
behavioral goals, with minimal support to facilitate their
goal-setting, implementation of behavior-change
techniques, and self-monitoring. In this paper, we
present lessons that we obtained from this initial study,
and insights on shifts in our design tools for a follow-up
formative study currently underway.

85281
Erin Walker

winslow.burleson@asu.edu

School of Computing,
Informatics, and Decision

Eric B. Hekler

Systems Engineering

School of Nutrition & Health

Arizona State University, Tempe,

Promotion, Arizona State

AZ 85281
erin.a.walker@asu.edu

University, Phoenix, AZ 85004
ehekler@asu.edu

Author Keywords
Behavior change; Goal-setting; Personalized behavioral
plan; Self-tracking

ACM Classification Keywords
H.5.m. Information interfaces and presentation (e.g.,
HCI): Miscellaneous.
Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights
for third-party components of this work must be honored. For all other
uses, contact the Owner/Author. Copyright is held by the
owner/author(s).
UbiComp '14 Adjunct, September 13-17, 2014, Seattle, WA, USA
ACM 978-1-4503-3047-3/14/09.
http://dx.doi.org/10.1145/2638728.2641318

703

Introduction
Previous research suggests that the primary reason
people self-track is to support behavior change, or
better lifestyle choices such as losing weight or
spending less [1][4]. Self-tracking and reflection is one
important behavior change technique, but there are
several other techniques that can be used to increase

UBICOMP '14 ADJUNCT, SEPTEMBER 13 - 17, 2014, SEATTLE, WA, USA

the potency of interventions [5]. At present, the choice
on which other behavior-change techniques to utilize is
often chosen for an individual by the technology they
use. For example, someone using a Fitbit will be
provided with a static goal-setting option (e.g., 10,000
steps per day), an opportunity to share steps with
others via a leader board, and historical data to foster
self-reflection. Not only is there only a limited number
of behavior-change techniques available to the person,
the exact design of the techniques is largely controlled
by the developer of the technology, not the individual
using it. An under-explored topic that is in line with the
current DIY culture within personal informatics and
quantified self communities is the exploration of tools
that could help individuals develop their own
personalized and temporarily adaptive behavior change
solutions. Metaphorically, the current approach is more
akin to giving them a fish (i.e., fully developed
behavior-change intervention) vs. teaching them how
to fish (i.e., tools to support creating and customizing a
personal behavior-change plan).
The purpose of this line of research is to develop tools
that can support users’ in developing their own
personalized and adaptive behavior-change plans. To
achieve this goal, we conducted an initial formative
user study among 11 individuals using a low-fidelity
prototype (i.e., powerpoint presentation) to guide
individuals through the process of developing their own
behavior change solutions over a preliminary two week
period. In this formative study, our goal was to explore
people’s creation of their own behavior change plans for
reaching their behavioral goal, with minimal support
provided to them via design support tools to facilitate:
(1) setting a specific goal; (2) creating personalized
implementations of behavior-change techniques; and (3)

704

supporting self-monitoring. In our first formative study,
we aimed to answer: (1) What is the impact of these
three resources for creating a behavior-change plan,
(2) How might the resources be used differently after
one week of engaging in attempting to enact the
behavior-change plan with self-monitoring? (3) What
difficulties did our participants have with incorporating
these ideas into their behavioral plans? This paper
summarizes lessons learned in this initial study and
provides insights on shifts in our design tools for a
follow-up formative study currently underway.

Study design
Participants were a convenience sample (N=11; 9
female and 2 male) of college students at a large US
university. Ages ranged from 18-39. Participants were
asked to participate in three sessions over a two week
period, and received an Amazon gift card after each
session for participation. By design, the sessions were
delivered by individuals with no formal training in
providing behavioral counseling to ensure clinical
training did not contribute to any potential effects of
the session design. The three sessions largely follow
the same general format with two added components
only in session 1. In session 1 only, participants were
asked to think about a behavioral goal to work on,
provided a prompt to think about a recent New Year’s
resolution (e.g., eat better), and asked to simply write
down their behavioral goals. Following this, in session
1 only, they were then provided with several domains
to consider (health, self-development, work, etc.) to try
and expand their thinking. All material after this point
was common with only minor tweaks made to phrasing
between session 1 and sessions 2/3. In all sessions,
individuals were asked to develop more specific goals
for the behavioral goal (e.g., eat less fried food). After

WORKSHOP: DISPI

this exercise on goal-generation, participants were
asked to select a specific goal to work on for the next
week. After the goal was selected, individuals were
prompted to create a specific behavior plan via the
following prompt: “How will you reach your goal?
Specify what you will do, when you will do it, and how”.
After going through this exercise, they were asked to
tell about their past experience regarding the chosen
goal, which was followed by generating further ideas or
rewriting their current plan ideas, based on their
reflection. After this ideation with the past experience,
a set of 13 behavior change techniques that covers
most categories identified in the behavior-change
techniques taxonomy [5] were taught to the
participants and participants were asked to choose
three techniques to incorporate into their further
ideation. Once they finalized their behavioral plan, the
researcher presented two options for self-tracking:
structured and unstructured. Participants who chose
the structured journaling came up with questions to
answer. The unstructured journaling was a free-flow of
thoughts and ideas. In both types, participants chose
specific times of day to do their journaling.

many vague and hard to enact components. Based on
this, the most common complaint about the strategies
they came up with was that they “burned out” trying to
achieve them. While goals and plans became more
specific during meetings 2 and 3 based on their
previous experience, most individuals still found it
difficult to enact the new behavioral goals.
Dominance of initial ideas
In designing the user study, we assumed that
participants would develop more sophisticated plans as
new support tools were provided to them during the
session (e.g., new behavior-change techniques
provided) and over the sessions. Counter to
expectations, the majority of individuals tended to
enact and stick with the initial idea they generated to
enact at the beginning of each session. That said, past
experience with trying to enact the behavior plan did
tend to elicit changes in techniques used (see below).

Conducting the study and brief analysis of results, we
found four primary issues with our initial support tools.

Natural reflection on the past experience was powerful
When participants were asked to think about their
behavioral plans relative to past experience, they
tended to improve upon their ideas. This was
particularly true during Sessions 2 and 3 as participants
immediately reflected on the past week (or past two
weeks) when asked to revise their goals/plans.

Goals and plans were not specific enough
We found the participant-generated goals and plans too
vague to be actionable by most individuals. Although
the prompt to think of sub-goals was effective in
leading participants to transform goals into more
specific forms, the ideas were not specified enough to
really make them actionable. Related to the plans, the
majority of individuals developed plans that included

Less engaged with behavior-change techniques
While participants expressed that the techniques served
as a helpful reminder of tools that they could utilize, we
found that our provision of techniques was not
successful in leading participants to generate rich ideas.
Frequently, the behavior-change techniques served as
a way for participants to give labels to elements they
had already implemented. Every participant changed at

Initial Results

705

UBICOMP '14 ADJUNCT, SEPTEMBER 13 - 17, 2014, SEATTLE, WA, USA

least one behavior change technique after the first
week, while most changed two or three. Very few
individuals attempted to tweak the technique they
initially chose to try and make it work better.

Implications for Future Research
Based on results from this initial study, the team is
developing a revised protocol that they plan to
implement to try and facilitate more specific
goals/plans, reduced influence of the first idea on what
to implement, increased utilization of self-reflection to
support the behavior-change process, and a rework on
how best to present behavior-change techniques.

opportunity to do it, trigger for enacting it, ability to do
it, and/or motivation to do, is meant to provide
individuals with a model to support self-diagnosis on
the core behavioral driver of the problem and thus
come up with better solutions. The group will continue
to iterate on supporting the creation of personal
behavioral plans based on lessons learned from these
initial formative studies.

Acknowledgement
This work was supported, in part, by a Google Faculty
Research Award (PI: Hekler).

References

As of this writing, the revised protocol is still under
development. That said, the current modified
procedure includes some of the following tweaks. First,
to support better specified goals, participants are
provided with the SMART goal [3] concept (i.e.,
Specific, Measurable, Actionable, Realistic, and Timely).
Reflection on the past experience will be utilized more
often throughout the process as a benchmark for
judging the quality of a goal as well as the
appropriateness of the behavior-change technique
chosen. To help reduce the impact of the dominance of
the first idea, behavior change techniques are now
provided right after setting a “SMART” goal.
To improve upon the teaching of behavior-change
techniques, lessons from Fogg’s behavior model and
Michie’s COM-B model, are combined to provide
individuals with a model for choosing the best behaviorchange technique [2][6]. In line with our goal to
metaphorically “teach how to fish,” the inclusion of this
hybrid model, which emphasizes that a behavioral
problem may be occurring because of a lack of the

706

[1] Choe, E. K., Lee, N. B., Lee, B., Pratt, W., & Kientz, J.
A. Understanding quantified-selfers' practices in collecting
and exploring personal data. Proc. the 32nd annual ACM
conference on Human factors in computing systems. ACM
(2014), 1143-1152.
[2] Fogg, B. A behavior model for persuasive design. Proc.
the 4th International Conference on Persuasive Technology
- Persuasive ’09. ACM (2009).
[3] Latham, G. P. Goal Setting: A Five-Step Approach to
Behavior Change. Organizational Dynamics, 32, 3 (2003).
309-318.
[4] Li, I., Dey, A. K., & Forlizzi, J. Understanding My Data,
Myself : Supporting Self-Reflection with Ubicomp
Technologies. Proc. the 13th international conference on
Ubiquitous computing, ACM (2001), 405–414.
[5] Michie, S., Richardson, M., Johnston, M., Abraham, C.,
Francis, J., Hardeman, W., Wood, C. E. The Behavior
Change Technique Taxonomy (v1) of 93 Hierarchically
Clustered Techniques: Building an International Consensus
for the Reporting of Behavior Change Interventions. Annals
of behavioral medicine: a publication of the Society of
Behavioral Medicine, 46, 1 (2013), 81–95.
[6] Michie, S., van Stralen, M. M., & West, R. The
Behaviour Change Wheel: A New Method for Characterising
and Designing Behaviour Change Interventions.
Implementation Science, 6, 1 (2011), 42.

Session: Promoting Educational Opportunity

CHI 2012, May 5–10, 2012, Austin, Texas, USA

Collaboration in Cognitive Tutor Use in Latin America:
Field Study and Design Recommendations
Amy Ogan1, Erin Walker2, Ryan S.J.d. Baker3, Genaro Rebolledo-Mendez4,
Maynor Jimenez Castro5, Tania Laurentino6, Adriana de Carvalho1
1Carnegie

Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213

4Universidad

Veracruzana
J.M. Morelos No. 44
Xalapa, Veracruz, México

2Arizona

State University
University Drive & Mill Avenue
Tempe, AZ 85287
5 Universidad

de Costa Rica
Sede Montes de Oca,
San José, Costa Rica

3Worcester

Polytechnic Institute
100 Institute Road
Worcester, MA 01609
6SENAI

Institute
Avenida Fernandes Lima
Maceió, Brazil

aeo@cs.cmu.edu, erin.a.walker@asu.edu, rsbaker@wpi.edu, grebolledo@uv.mx, maynorj@gmail.com,
tania.laurentino@gmail.com, dikajoazeirodebaker@gmail.com
ABSTRACT

Technology has the promise to transform educational practices worldwide. In particular, cognitive tutoring systems are
an example of educational technology that has been extremely effective at improving mathematics learning over
traditional classroom instruction. However, studies on the
effectiveness of tutor software have been conducted mainly
in the United States, Canada, and Western Europe, and little
is known about how these systems might be used in other
contexts with differing classroom practices and values. To
understand this question, we studied the usage of mathematics tutoring software for middle school at sites in three Latin
American countries: Brazil, Mexico, and Costa Rica. While
cognitive tutors were designed for individual use, we found
that students in these classrooms worked collaboratively,
engaging in interdependently paced work and conducting
work away from their own computer. In this paper we present design recommendations for how cognitive tutors
might be incorporated into different classroom practices,
and better adapted for student needs in these environments.
Author Keywords

Cognitive tutors; collaborative learning; cultural adaptation
ACM Classification Keywords: H.5.2: User Interfaces -

Graphical user interfaces; K.3.1: Computer Uses in Education
INTRODUCTION

In recent years, school access to computers has greatly increased worldwide and across socioeconomic groups, with
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first page.
To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee.
CHI’12, May 5–10, 2012, Austin, Texas, USA.
Copyright 2012 ACM 978-1-4503-1015-4/12/05...$10.00.

the growth of initiatives like One Laptop Per Child and Intel
classmate PCs. Within these evolving contexts, effective
educational software could make a huge impact: it is easily
deployable, and when amortized across users it is low cost
compared to physical resources like textbooks. Yet, it has
capabilities that far surpass physical resources, such as the
ability to provide structured guidance and support. However, creating this impact is more complex than simply distributing educational software widely. Not only does the
content of the software need to be translated and localized,
but attention needs to be paid to how teacher practices and
student interactions with educational software vary across
contexts. In this paper, we examine the cross-contextual
generalizability of pedagogical assumptions of cognitive
tutors, educational software that has been demonstrated to
be effective at improving learning outcomes. We observe
student and teacher use of the Middle School Mathematics
Cognitive Tutor (CT) in classroom settings in three Latin
American countries to gain insight into how the CT might
be adapted to different settings.
Cognitive tutors are an example educational technology that
has the potential to transform education. A cognitive tutor
assesses skill mastery as a student solves problems, and
provides context-sensitive hints, error feedback, and adaptive problem selection [23]. Their self-paced learning and
tailored content and support provide students with individual attention, and free teachers to act as classroom facilitators
[21]. Cognitive tutors were initially designed to support
problem solving in well-defined domains such as math and
science, and have been demonstrated to improve a number
of studies, particularly for students of low socio-economic
status [13]. In recent years, cognitive tutors have also been
successfully used in ill-defined domains such as developing
intercultural competence [e.g., 18].
In order to study the CT across learning contexts, we conducted a twelve-week multi-context field study of the CT,
observing over seven hundred students in Brazil, Mexico,

1381

Session: Promoting Educational Opportunity

CHI 2012, May 5–10, 2012, Austin, Texas, USA

and Costa Rica. Generally, evaluations of cognitive tutors
have been limited to schools in the United States, Canada,
and Western Europe. However, individuals in the United
States frequently (though not always) differ from students in
the Latin American countries we investigated along several
dimensions, including power distance (equality of power
distribution), tolerance for ambiguity, and individualism
versus collectivism [10]. In addition, access to resources
varies between developing and developed contexts, and these differences affect how technology is used [19]. In taking
a technology that has been successful in one context, and
transporting it to a different context, we can identify which
aspects of cognitive tutors are context-specific and develop
guidelines for more effective use in new contexts.
In this paper, we survey related work on generalizing education technology, and discuss the specific tutor studied. We
then present qualitative observations of CT use in three Latin American settings. Our findings have three themes: the
school conditions in which the tutor was used, the way the
CT was integrated into classroom practice, and collaborative
student use of the CT. From these, we develop design recommendations for the future development of cognitive tutors that are educationally effective across a broader set of
contexts. Our contributions are both a richer understanding
of ways educational technology designed for one context
might be adopted to different contexts, and practical recommendations for how best to accomplish this goal.
EDUCATIONAL TECHNOLOGY IN DEVELOPING CONTEXTS

Warschauer writes, “Technology projects around the world
too often focus on providing hardware and software and
pay insufﬁcient attention to the human and social systems
that must also change for technology to make a difference.”
Access to technology is increasing worldwide, but must be
incorporated into existing social and institutional structures
to have a truly positive impact [25].
Hoadley and colleagues extend Warschauer’s argument by
arguing that desktop technologies can be disruptive, difficult
to use, and impossible to maintain in developing contexts.
Thus, the platform for educational technology is very important for adoption and impact [9]. It has been possible to
reach a broad audience using platforms already popular in
developing regions, such as mobile phones [14, 11] or bargain video game consoles [15]. Existing tools have also
been adapted to be more suitable for developing contexts,
such as making low-cost programmable bricks with local
materials and local construction [22], or adjusting to high
student-to-computer ratios by giving each student a mouse
that controls a cursor on a shared display [16].

these technologies equally successful in different contexts,
such as developing regions? What needs to be adapted to
make them viable across cultural contexts, in both developed
and developing regions? While some completed systems
have been used in multiple cultural contexts [17], work on
adapting these systems to new contexts has focused on translation, localization, and interface design (see [6] for review).
Hence, there is a gap in the literature with respect to understanding how students use existing mature systems across
contexts.
MIDDLE SCHOOL MATHEMATICS COGNITIVE TUTOR

The Middle School Mathematics Cognitive Tutor (CT) was
developed between 1999 and 2004 at Carnegie Mellon University. It spans over 30 units covering different mathematical topics for students in U.S. grades 6-8 (approximately 1114 years old). In our investigation, we used the Scatterplot
unit of the CT [4], after determining with each school that
scatterplots were an appropriate topic for their curriculum.
In this unit, students read a scenario in which a scatterplot
can be used to answer a question about data. For example, a
scenario might describe kids who have a lemonade stand
and want to know whether they sell more cups of lemonade
on hot days. The goal is to answer this question by plotting
two numerical variables (e.g., cups and temperature) on a
graph. In the tutor (see Figure 1), students are scaffolded in
labeling axes, choosing a scale, plotting points, and answering interpretation questions. The CT delivers immediate
corrective feedback on each step, an approach shown to
support learning in an American context [8]. Students can
also request a multi-level hint at any step. The first hint students receive tends to reference the underlying concept, and
subsequent hints increase in specificity. The system assesses
students’ knowledge based on their problem solving and
presents this information as a “skill bar” that increases or
decreases as they solve problems with particular skills.
Target Use of the CT

The design of cognitive tutors, and the algorithms they use
to model learning, assumes that students generally work at

While these hardware solutions are an important component
of bringing educational technology to developing contexts,
educational software is also a necessary area of research. We
now have mature, empirically proven technological methods
of improving learning being used in schools across the United States, from rural or suburban classrooms, to urban classrooms serving historically disadvantaged populations. Are

1382

Figure 1. English-language version of the Middle School
Mathematics Cognitive Tutor. Students must compare two
variables on a graph. They use several scaffolding tools to
complete the task, and receive context-sensitive help.

Session: Promoting Educational Opportunity

CHI 2012, May 5–10, 2012, Austin, Texas, USA

their own individual computers and at their own pace [23].
Students proceed using the CT’s help and feedback as the
teacher circulates around the room providing extra support
to students that need it [4, 21]. The CT’s model of student
knowledge assumes that the student is solving problems
without the assistance of others [7]. Recent additions to
cognitive tutors also assume individual work. For example,
[1]’s model of ideal help-seeking in cognitive tutors focuses
solely on help-seeking within the tutoring environment, and
not on help students seek from their teacher and peers.
Based on published classroom observations, these assumptions are generally met in use of the CT and related software
in American classrooms. Though there are qualitative reports of student collaboration while using the CT [21], it
occupies a relatively low amount of class time. For instance,
in quantitative field observations of student behavior in suburban American middle schools, using this same tutor lesson, students spent only 4% of their time talking on-task to
the teacher or another student. They spent 78% of their time
working on their own [4]. Since that study, the third author
has spent over 500 hours conducting quantitative field observations in American classrooms, including urban, suburban, and rural classrooms. Though the proportion of collaborative behavior in those observations has not been published, it is similar to the proportion seen in [4].
Preparation of the CT for Use Outside of the U.S.

While the Scatterplot CT unit was developed in English, the
students in this investigation were native speakers of Spanish and Portuguese, and generally did not speak English.
Therefore, with local support, all of the text in the tutor was
translated into the local language of instruction, and then
tailored to each particular locale; for example, in Mexico
and Costa Rica, after an initial translation, the tutor text was
iteratively reviewed and revised by local teachers and researchers in each of the sites separately. Also, scenarios in
the unmodified system were in some cases outside of the
scope of students’ cultural experience. The scenarios and
mathematics were examined for culturally appropriate content, and modified as necessary to better fit the local context. For instance, in Brazil, local researchers and teachers
changed the previously mentioned lemonade scenario to
reflect local practices of selling coconut water on the beach.
FIELD STUDY PROCEDURE

Following the iterative translation procedure, the on-site
investigation occurred over the course of twelve weeks, with
four weeks dedicated to each of three sites (Brazil, Mexico,
and Costa Rica). The first step at each site was a meeting
between the international team and local teachers and researchers at the school computer lab, to demonstrate the
software, answer questions about the study and material,
and for teachers to work through a full problem in the tutor
so that they were comfortable using it in class. These meetings were followed by a week of software installation and
piloting with students who were not part of the full study.
As much as possible, teachers supervised the student pilots,

so that they would know what to expect from facilitating a
Cognitive Tutor classroom. International researchers also
gathered information about the school context.
During the following three weeks, each student used the CT
in a classroom setting for eighty minutes. At least two researchers were present in the computer lab in each session,
taking field notes as they positioned themselves around the
lab in order to observe computer screens. Field notes captured on- and off-task behavior including collaboration,
teachers’ instructional procedures, student impasses, and
affective reactions. Diagrams of machine and participant
locations were drawn at intervals throughout CT sessions.
The teachers were asked to conduct every session on their
own, and behave as they would if they were to include the
technology as part of their typical practice. They were told
that researchers would be available for technical support
should any problems arise with the tutors or the computers.
While the study procedure was kept as consistent as possible, given the nature of classroom research in multiple diverse contexts, we adapted it to each context as individual
school conditions dictated. We describe these adaptations as
part of the “Findings: Deployment of the CT” section.
Following tutor use, we conducted in-context guided interviews with participants [20], which began with a set of prepared questions but were allowed to digress into follow-up
questions. Teachers and principals were interviewed individually in order to make them more comfortable in discussing sensitive topics surrounding school conditions. Teacher
interview questions explored the social context of the participants at the site, the procedures and values they held in
their traditional classrooms, and their experiences leading
the CT sessions. Students were interviewed in groups of two
to four at a time; groups were found to facilitate communication for students who were otherwise shy. Student interview questions explored social context, students’ values
about learning, and their experiences with using CT and
other technology. These interviews were recorded and were
conducted in the local language by a member of the research
team who spoke the language. Two groups were taken aside
for a think-aloud procedure while using the CT at each site.
FINDINGS: SCHOOL CONTEXTS

Based on our participant interviews and field notes across
researchers, we can describe the conditions where cognitive
tutors could potentially be deployed, and put the results of
tutor usage in context. Across the three sites, we iteratively
organized our findings into three main themes that affected
our educational technology deployment: socio-economic
context, typical classroom instruction, and technology use.
Socio-Economic Context

In Brazil, we deployed the CT in a middle school in an impoverished area of a large city in the northeast, known both
for resorts and a significant industrial base. Class sizes
ranged from 20-40. Given the economic situation in the
neighborhood of the school, many teachers drove from up to

1383

Session: Promoting Educational Opportunity

CHI 2012, May 5–10, 2012, Austin, Texas, USA

three hours away, and did not always arrive to teach their
class. Students, however, had a significant incentive to attend, as all students received free lunch and snacks during
the day. Even so, as the school was located in a community
in which religion was strong, many students were absent for
up to a month at a time on family religious pilgrimages.

There was low accountability for student absences. When
teachers were absent, there were no substitutes, so students
either sat unattended in their classroom, played games in the
central courtyard, or sat in on other classes. Students wore tshirts, jeans, and sandals that had been given to them by the
school. The computer lab in the school was located behind a
heavily locked and barred door designed to deter robberies.
In Mexico, we deployed the CT in a public middle school
serving a lower and lower-middle class area of a large town
in a Southeastern state. In this school, class sizes varied
from 20 to 46 students. Students wore school uniforms purchased by parents and were provided with textbooks from
the government. Lunch was not provided; rather, students
were divided into two school sessions, a morning session
from 7am until 1pm and afternoon from 1pm until 7pm.
In the Mexican site, as in Brazil, there were no substitute
teachers. Instead, there was a system of aides. When teachers were absent, an aide would sit in the class and give students a prepared problem to do. If too many teachers were
absent, an aide or other teacher periodically checked in on
the class to make sure they were not misbehaving.
In Costa Rica, we deployed the CT in a middle school serving a medium-size town. Students were issued a school uniform shirt, but bought their own pants and shoes. Lunch was
provided to all students in a cafeteria where they ate alongside their teachers, and afterwards all were expected to assist in cleaning their plates and utensils. These meal arrangements, also common at other schools in the area, are
perhaps reflective of the lower Power Distance seen in Costa Rica compared to other Latin American countries [10].
Both students and teachers reported that teacher absences
were rare, although school sessions were sometimes canceled for days due to heavy rains.
The school property itself was surrounded by locked gates
and barbed wire, and had prior issues with armed robberies.
Within the gates, the school walls (and those of other
schools in the area) were covered in murals of jungle wildlife, painted by the students. These murals reflected the
great importance placed on caring for the environment (ecotourism is a significant factor in the Costa Rican economy).
Teaching and Learning Practices

In each of the three sites, we interviewed teachers in order
to understand their typical classroom procedures. These
interviews took place in context, with teachers walking us
through materials that they would typically use to teach. In
each country, standard classroom practice for math instruction had a pattern of teacher demonstration followed by
multiple days of group exercises.

Figure 2. Images from the computer lab at each of the
three sites. From top: Brazil, Mexico (combined with library), and Costa Rica.

At the Brazilian site, classes typically consisted of the
teacher demonstrating a worked example briefly at the beginning of class, followed by students doing group exercises
that came from a list at the end of a textbook. Despite the
students’ age (12-15), teachers reported that students were
still working on basic math skills such as addition, subtrac-

1384

Session: Promoting Educational Opportunity

CHI 2012, May 5–10, 2012, Austin, Texas, USA

Brazil Site

Mexico Site

Costa Rica Site

Math Skills

struggling with basic operations

learning geometry, some
difficulties with basic math

learning trigonometry,
mastered basic math

Computer Skills

minimal exposure

recreational exposure

used in school

Computer Lab

12 computers, unused

49 computers, unused

30 computers, used in classes

City Population

1 million

50,000

10,000

City GDP Per Capita

$3,366USD

$5,417USD

$6,590USD

2009 PISA Test

th

st

57 of 64 countries

51 of 64 countries

N/A

Table 1. Comparison of the three populations on math skills, rank on the 2009 Program for International Student Assessment
math test, computer skills and use, city size, and 2008 gross domestic product per capita in the city of our field site.

tion, multiplication, and division. Teachers informed us that
they strongly encourage collaboration during classwork and
even on assessments, from providing explanations to just
sharing answers. Students were often assessed orally.
At the Mexican site, math classes consisted of an introductory lecture and worked example on a topic followed by 2-4
days of group work on a related exercise taken from materials provided by the government-standardized curriculum.
Students were officially studying geometry, but teachers
informed us that they felt students were underprepared by
their previous courses and needed to review basic math
skills. In class, students frequently talked out loud to one
another off-topic during all classroom activities, including
both groupwork and lectures. Teachers reported that this did
not concern them, as they believed the talk would eventually
turn to math-related topics.
At the Costa Rican site, math classes consisted of a lecture
on a topic followed by a few periods of group work. Students were learning trigonometry, and teachers were confident that students had mastered basic math skills.
Digital Literacy

At the Brazil site, students reported that they were frequent
computer users and that they were proficient in their use.
However, when asked for detail, the majority of students
reported that they did not have computers in their houses,
and used computers at most once a month in internet cafés,
where they used social networking sites and played games.
Observations of these students using computers revealed a
range of ability; some did not know how to operate a mouse
or move a window, others were proficient in searching and
posting pictures on the internet. All interviewees reported
having a television in their home, even though many of their
homes were off the electricity grid. A small percentage of
interviewees had owned a cell phone.
Teachers showed a range of skill with the computers. Some
had never used a computer, while others had one at home.
Teachers who did not have computer skills relied on more
proficient students to support other students with the CT.

The school in Brazil had a computer lab with 22 machines
purchased by the federal government in an initiative to give
every school access to technology. Computer viruses had
infected all of the machines, but were seen as harmless
pranks by administration. Twelve were functioning, but had
no internet access. Computers were not used by classes for a
number of reasons: many teachers had no experience with
technology and were unfamiliar with educational uses of
computers, and because the lab was typically locked due to
security concerns.
At the Mexican site, students were familiar with computers
and most used computers for social networking and listening to music. Half of interviewees reported having a computer in their home, and those who did stated that they used
them to complete homework assignments. Students were
also very familiar with other technologies: all had a television and cell phone and several owned a gaming system
such as Xbox or PlayStation.
Teachers were familiar with computers, but used them only
for preparation for class (e.g., typing up worksheets, searching for curriculum materials). One teacher had set up a blog
for her students to follow. An exception to this rule was a
teacher who reported that he did not trust computers and
never worked with them; as noted below in the “Findings:
Deployment of Cognitive Tutor” section, this teacher avoided attending any of his students’ sessions with the CT.
The combination computer lab/library in Mexico was outfitted with 41 Telmex Intel computers provided by the federal
government, and 8 desktop machines. The school director’s
office had internet. Although they had been in the school for
a year, teachers reported that we were the first group to use
the Telmex computers. Reports differed on why the lab was
not used: the principal was not enthusiastic about technology, there was no educational software, or the teachers
weren’t proficient in teaching with computers. However,
during the study, several teachers stopped to ask if they
could use the computers when we finished.
At the Costa Rican site, students had exposure to computers
at school, and were capable users. Most had a computer in

1385

Session: Promoting Educational Opportunity

CHI 2012, May 5–10, 2012, Austin, Texas, USA

their home or were able to find one for recreational use such
as social networking, and several reported having multiple
machines in their home. Interviewees reported using a video
chat program from home to jointly complete homework
assignments (or answer-share) with their classmates when
they finished school for the day.
Teachers were experienced computer users, and reported
using them for class preparation and social networking.
They owned cell phones and texted frequently during
breaks. The teachers we worked with took part in a program
initiated by the local university, designed to support them in
integrating technology into the classroom, but rarely did so
in practice. They reported that the computer lab schedules
were often full with other courses, and that preparation time
was too great for technology-enhanced instruction.
The school in Costa Rica had two computer labs, provided
by a private foundation. Each lab was supervised by an attendant, and used by students for classes and internet access.
There were currently 30 desktops across the two computer
labs, following a theft in the school the previous year. For
the study, our collaborators brought 10 laptops from a local
university to temporarily replace the stolen machines. These
computers allowed us to conduct our observations in a manner that was closer to typical classroom practices. Lab attendants reported that most courses in the computer lab were
either computer skills classes on making presentations and
documents, or were using the internet for information
search.

teachers believed that being in their regular class was important, and that others would be able to handle the instruction in the computer lab. Additionally, as described above,
there were frequent absences on the part of the teachers, and
no substitutes. This led to teachers from other subject areas
helping during periods that they had free from teaching. On
occasion the principal of the school participated too. If neither teachers nor the principal were present, researchers
supervised students.
At the Mexican site, the principal insisted that all students
have the chance to use the technology. Thus approximately
600 students ages 13-15 and seven teachers participated.
During use of the CT, each class of students was brought in
to the computer lab in place of their typical math period,
with each class participating for two periods of forty
minutes each. Teachers (or, on occasion, knowledgeable
substitutes) were present to conduct their classes for the
duration of the study. The only teacher who was reluctant to
use technology made himself absent for all of the sessions
that his students participated in, and thus his sessions were
given a substitute or supervised by the research team.
At the Costa Rican site, around 90 students and two teachers
participated. The study was supervised by the regular math
teachers, but occurred at a time where classes had just finished for the semester and students only attend exams.
Therefore, we observed student use of the CT in groups of
approximately 20 at a time selected from the same math
class, for single 80 minute periods.
Structure of Cognitive Tutor Session

FINDINGS: DEPLOYMENT OF THE CT
General Reception towards the Cognitive Tutor

Across the three sites, there were several commonalities that
indicated that the CT might be a welcome addition to the
classroom. Each school’s principal expressed interest in
using more educational technology (despite reports to the
contrary from the lab manager at the Mexican site), and was
an active partner in organizing the study. All students appeared excited and motivated by technology, whether or not
they were proficient or frequent users.
In general classroom practice, teachers reported not having
appropriate educational software or enough time to prepare
lesson plans that incorporated technology. Thus, most
teachers were enthusiastic about using the CT, which they
perceived as requiring little additional preparation, and serving as a good supplement to their exercise-based classes.
Student and Teacher Participation

In the Brazilian site, around 100 students ages 12-15 participated. Because the number of computers in the lab was
smaller than the size of a class, groups of twelve students
were pulled from one class at a time to use the CT over two
class periods of forty minutes each. Although the math
teachers were enthusiastic participants in the initial informational meeting, for most of the study they were not present
in the computer lab. Given that their classes were split for
the study between the computer lab and the classroom, the

In the Brazilian site, students filed in excitedly and wanted
to sit beside friends. There was confusion at the beginning
trying to determine how to login to the system, as the names
they used frequently did not match their names on the
school roster. In addition, typing skills were low. As students started the first tutor problem, they were typically confused about the content, and had many questions at each
step about how to proceed. Due to low literacy levels in
addition to low math skills, they had difficulty understanding the tutor hints. Nevertheless they gradually moved
through the first problem with each other’s help, and that of
any aides present. Occasionally the aides requested the researchers’ help when unsure of how to proceed. Some aides
also indicated that if we were going to be watching them,
we ought to share the load of supporting students.
In Mexico, students took their assigned seats, and were able
to help one another login to the tutor. The supervising
teacher then typically walked the students through the problem, either verbally or using a projector to demonstrate.
When teachers felt the students understood, they let the students work on their own, and walked around to help. They
also asked the researchers to help, because the class sizes
were so large that they had trouble getting to everyone who
needed help. During the second session, students started
using the tutor immediately. Because screens on the Intel
PCs were small, and the CT has multiple windows, students

1386

Session: Promoting Educational Opportunity

CHI 2012, May 5–10, 2012, Austin, Texas, USA

had some difficulty navigating from window to window
when working through the problems.
In Costa Rica, students took their assigned seats. As in Mexico, the supervising teacher either used a projector or a
whiteboard to walk students through the problem, and then
let the students work on their own, walking around to help.
When students transitioned to solving problems on their
own, they tended to work in small groups and were more
successful than in the other sites.
In all three sites, certain collaborative patterns of use
emerged when they used the tutor, concerning students
working interdependently, working at locations all over the
classroom, and giving particular kinds of help to each other.
We believe they are particularly relevant for cognitive tutor
design for use in these settings, and describe them below.

Figure 3. The ways in which students moved around the
classroom and interacted with classmates across sites.

Interdependent Pace of Work

this announcement was made because otherwise students
would not work, but at the first sign of difficulty would wait
for the teacher to give help.

I1: Teacher led instruction

I3: Student-led group work

One collaborative pattern we observed in both the Mexican
and Costa Rican sites was synchronized whole-class advancement through the tutor, led by the teacher. In many
introductory sessions, the whole class worked at the same
pace, led by the teacher guiding students step by step
through a problem in the tutor. Teachers seemed to find this
to be a useful technique when students were unfamiliar with
the tutor. The teacher would describe a single step in the
tutor, and then wait as students executed the step on their
own computer. The teachers would then either demonstrate
the step themselves on a projector or provide the students
with the correct answer. Many teachers requested a projector to facilitate this process, often using the only projector in
the school.

A third pattern we observed involving interdependent pace
of work was student-led group problem solving. As students
moved into an individual work phase, their pace of problemsolving often remained interdependent, but in spontaneously
formed small groups seated at adjacent computers. When
one group member would successfully complete a step, they
would inform the other members of their group of the correct action, and the other members of group would then take
the correct step. Between groups, it varied whether one person always took on the explainer role, or whether different
members of the group did. During this type of work, the
teacher circulated around the classroom to help individual
students and groups.

I2: Teacher guided practice

At the Costa Rican site only, students were more likely to
work at an individual pace. However, when they worked
synchronously, groups of two to three would work completely interdependently on the same computer, sharing the
mouse and keyboard (despite the fact that each student had
access to an individual machine).

FINDINGS: COLLABORATIVE PATTERNS OF USE

A related collaborative pattern was whole-class advancement through the tutor, but with opportunities for students to
work independently. As students acquired more expertise in
using the CT, teachers would instruct them to do two or
three steps on their own, but then stop the whole class to
wait for everyone to catch up, saying “Is everybody here?”
In this pattern, students typically did not show exploratory
behavior with the tutor; they would wait patiently for the
teacher to say they could continue, and follow the teacher’s
instructions closely.
When asked about the rationale for structuring the class in
this way, teachers said that is important to keep everyone on
the same page, an easy way to familiarize the students with
the system, and more efficient than repeatedly answering the
same questions from students. Interestingly, in almost all
cases, after this guided period of instruction which generally
consisted of one full problem in the tutor, teachers would
make an announcement to the class proclaiming that now it
was time for students to work on their own, and the teachers
would not be helping (this statement was never true; teachers did help during the self-paced period). Teachers said that

I4: Shared interfaces

Variable Location of Student Work

In addition to problem-solving interdependently, we found
that students frequently helped each other, and thus a lot of
students’ work did not occur at their own computers. Figure
3 aggregates data from all three sites to depict ways in
which students interacted with their peers in order to give or
receive help. Students interacted either from their seats (represented by straight lines) or by moving around the class
(represented by curved lines). Teachers encouraged these
collaborative behaviors as they circulated around the classroom (represented by G in Figure 3). To the observer, classrooms were chaotic, with constant movement by students
and loud cross-class collaborations. We estimate that in the
Brazilian and Mexican sites, roughly 60% of student work
did not occur at their own computer, a much higher proportion than the 4% seen in past research on CT usage in the

1387

Session: Promoting Educational Opportunity

CHI 2012, May 5–10, 2012, Austin, Texas, USA

United States [4]. We divide the variable location of student
work into two patterns of collaboration.
L1: Directed help

One pattern related to location of work was the directed
exchange of help between students. This pattern represents
help given after one student calls out or signals to a specific
friend for help. For example, in Figure 3, D and E represent
help given in response to one student calling out to a friend
for help. In D, the helper moves around the table, while in E
the helper remains in her seat but rotates her laptop.
L2: Spontaneous help

A second pattern related to location of work involved help
that did not appear to be purposefully directed to a specific
friend in need. In cases such as A and H, help-related actions were spontaneous. In H, the student goes from computer to computer looking for the answer she needs. When
students were done with a problem step, they also might
move around the room from classmate to classmate giving
them information (as with student B in Figure 3).
When probed on their helping behaviors, students explained
that everybody needed to finish, and that the academic performance of their whole class was important. Students said
they felt kinship with their classmates, given that they were
often classmates several years in a row.
Content of Help

Despite these commonalities in movement around the classroom, the kinds of help students gave varied between settings. In general, help consisted of a verbal explanation,
telling another student the answer, or even demonstrating
the next correct step by physically taking control of another
person’s computer (or a combination of these).
The verbal content of explanations differed from site to site,
and appeared to be related to the prior knowledge of the
students. We identified three collaborative patterns related
to the verbal content of help. At the Brazilian site, where
students had very low prior knowledge, we observed students circulating around the classroom (as in B or H in Figure 3), giving or seeking the answers to the next problem
step (C1: Answer-based help). In contrast, students at the
Mexican site primarily exchanged help focusing on how to
use the technology rather than problem solving (C2: Technology-based help). At the Costa Rican site, students reported giving full explanations to help their classmates understand the material (C3: Concept-based help), although they
were observed to give answers and acknowledged doing so
when questioned. These differing approaches emphasize the
opportunity in supporting students whose natural inclination
is to collaborate to give more conceptual help.

be necessary to redesign the underlying systems, which assume that students are working for the most part at their
own pace and computer [23].
Collaborative Knowledge Tracing

One of the most important aspects of CTs is their ability to
track the current knowledge level of the student. This
knowledge tracing allows the student and the teacher to
know when the student has achieved mastery on a skill, and
enables the tutor to select appropriate problems for each
student. However, we saw that as students worked in almost
all collaborative patterns (I1, I3, I4, L1, and L2), the answers
entered into a tutor were frequently not reflective of the
knowledge of the student using that machine. We believe it
would be beneficial for the knowledge-tracing algorithm to
view the classroom as a network of connected nodes instead
of a collection of individual users, accounting for when multiple students are completing problems jointly.
To implement this modification, one could explicitly estimate the probability that the student has been told the next
step in the problem. In classical Bayesian KnowledgeTracing [7], skill mastery is estimated based on four parameters, including the probability that students will perform a
step correctly even if they have not mastered the skill
(P[Guess]). It is possible to incorporate other probabilities
in P(Guess) to account for other sources of error [5]; here, it
would be appropriate to include the probability that a student answered correctly due to another student’s help (e.g.,
A in Table 2). This probability could be estimated empirically based on data on the problem-solver’s behavior (as in
the “contextual guess and slip” approach in [cf. 3].
This approach will necessitate the inclusion of data on the
problem-solving pace of all students in the networked classroom. It may be possible to determine over time which students’ performances are linked, by tracking the timing of
different students’ steps. It may similarly be possible to assess whether students are working together (I3) or following
the teacher’s lead as a whole class (I1).

DESIGN RECOMMENDATIONS FOR CT USE IN HIGHLY
COLLABORATIVE SETTINGS

By encouraging teachers to use the CT as they would typically conduct a regular class, we open the door to students
using the CT in a highly collaborative manner. In order to
achieve the full learning benefits of intelligent tutors, it may

1388

A. Knowledge
Tracing

B. Model
Tracing

P(Guess’) =
guess*(1-helped) +
helped

IF don’t know x
AND person y
knows x

Where
helped =
probability student
was helped
guess =
probability student
guessed

C. Adaptive
Scaffolding

THEN ask y
about x
IF know x
AND person y
doesn’t know x
THEN tell y
about x

Table 2. Three redesign proposals for the Middle School
Mathematics Cognitive tutors.

Session: Promoting Educational Opportunity

CHI 2012, May 5–10, 2012, Austin, Texas, USA

It is worth noting that in a different context, these assessments might be seen as identifying an undesirable behavior;
in these classrooms, it is a step towards more accurate assessment for the predominant style of usage.

often interdependent, and work often occurred at classmates’ computers in addition to their own. We propose
guidelines for integrating the CT into similar classrooms
and three augmentations to cognitive tutor design.

Help-Giving

There is a great opportunity for educational technology to
have a positive impact in developing contexts. Access to
technology is increasing, and computers can now be found
in schools in impoverished areas. Our findings suggest that
students are enthusiastic about learning with technology.
Given the structure of the school systems we observed and
the potential lack of teaching resources, intelligent tutors
provide an opportunity to support student learning when a
regular teacher cannot be present in the classroom, or during
periods where students would otherwise be working on paper-based assignments. In terms of content, cognitive tutors
that focus on creating fluency with basic skills, rather than
on advanced units, have the potential to teach or remediate
critical deficiencies in students’ understanding. A large barrier to using educational software in classrooms is the preparation teachers require to structure lessons around the software. Cognitive tutors help to mitigate this obstacle by
providing self-contained lessons. In cases where teachers
cannot be in class, cognitive tutors could assist substitutes or
aides and supplement existing lessons.

As discussed above, existing models of help-usage in tutors
assume that help comes from the tutor [1, 23]. However, in
the settings described here, the source of help was most frequently another student in the class (I3, L1, L2). Students’
collectivist behaviors reflect an opportunity to actively encourage students to seek and give help at appropriate times
during their problem-solving, from appropriate people.
There are asynchronous systems such as iHelp that match
students based on their expertise and preferences [24], but
these systems do not take into account real-time problemsolving progress.
Working in conjunction with a classroom-level knowledgetracing algorithm, if a student is clearly struggling, the system could encourage them to seek help from someone who
has already mastered the relevant skill. On the other hand,
students who have mastered a skill quickly could be encouraged to help others who have not (sample rules for this approach can be found in B in Table 2). This would support
more effective help, by taking advantage of students’ natural
inclinations to collaborate, but by pairing students who
might maximally benefit from working with one another.
Adaptive Scaffolding

The benefits of collaboration, however, are not automatic
[12]. Students often shared only the answer to a problem
step (C1), rather than giving an explanation that their partner could learn from. To ensure that students give constructive help, it may be useful to view the person receiving help
as studying a worked example rather than taking a problemsolving step. Using techniques such as those described
above, students can be inferred to be receiving help. One
approach would then be to introduce scaffolding encouraging the peer learners to provide an elaborated explanation of
the problem-solving step just entered. Many have demonstrated that alternation between self-explanation of worked
examples and problem-solving is an effective learning technique (as in [2]). Hence, this design recommendation has the
potential to benefit both the struggling partner and the partner with greater knowledge. C in Table 2 represents this
concept; students transition between self-explanation and
problem-solving based on whether they are judged to be
receiving help, or working on their own.
DISCUSSION AND CONCLUSIONS

In this paper, we described a project where we deployed one
unit of a mathematics cognitive tutor in three different Latin
American school sites. The samples had varying socioeconomic contexts, typical methods of instruction, and experience with technology use. Thus, there was variation in the
way the CT was integrated into classroom practice. Nevertheless, across all contexts, we found that students collaborated frequently while using the tutor; the pace of work was

CT use in these developing contexts was far more collaborative than the typical use of CT, and we have several hypotheses for why this may be so. In the classrooms we visited,
students had never used cognitive tutors before, and in the
Brazilian and Mexican sites, students had never used computers to learn before. Teachers described students as more
engaged and motivated than normal, which could be attributed to a novelty effect. However, these external effects
are not likely the dominant cause, since increased engagement has also been reported in classrooms that use technology in the U.S. [19]. In addition, students’ variation in basic
math skills and computer experience might mean that they
required more help to use the CT than analogous students in
the U.S., driving them to collaborate more. Following this
line of thinking, the differences we observed between each
Latin American context may have been related to factors
such as prior knowledge and experience with computers. In
the Brazilian site, where students we worked with had lower
prior knowledge than those in the Costa Rican site, we saw
more frequent answer-focused help. Finally, other researchers have theorized that the scarcity of technological resources in developing contexts force students to share resources, and thus collaborate more, which may have played
a factor in the behaviors we observed [19].
It is also difficult to discount the possible influence of cultural factors on use of the CT in these settings. Brazil, Mexico, and Costa Rica are considered to be more collectivist
cultures than the United States, in that they are “…societies
in which people from birth onwards are integrated into
strong, cohesive in-groups, often extended families…which
continue protecting them in exchange for unquestioning

1389

Session: Promoting Educational Opportunity

CHI 2012, May 5–10, 2012, Austin, Texas, USA

loyalty [10].” In all three settings, collaborative work was a
core part of classroom activities. Teachers valued student
collaboration. Students valued helping their classmates and
were comfortable asking classmates for help. Students were
part of a culture that valued group membership and employed collaboration as an integral part of everyday activities. Therefore, the students collaborated extensively while
using a technology primarily designed for individual use.
Regardless of the reasons for the differences observed, this
work contributes insights into the opportunities for intelligent educational software in underserved regions, and into
the ways different contexts might adapt to a proven technology platform. There may be benefits to deploying existing
effective systems in developing contexts, and this direction
of research should be pursued. However, a thorough understanding is necessary of how those contexts might incorporate technology into their current instructional practices, and
which modifications to the basic assumptions of such systems might be necessary. This understanding might also
provide insight for how cognitive tutors may and should be
used in contexts similar to the sites we observed, in countries where intelligent tutoring systems have already been
deployed. Rather than forcing others to conform to a single
model of appropriate technology use, we must understand
how the technology can be best integrated in vastly different
contexts than those for which it was designed.
ACKNOWLEDGEMENTS

Thanks to all of the teachers and school administration who
made this study possible. Sofía Pachaco, Xareni Alvarez,
Etmon Vega, and Ruth Wylie also provided immeasurable
support.
REFERENCES
1. Aleven, V., McLaren, B., Roll, I., & Koedinger, K.: Toward
tutoring help seeking: Applying cognitive modeling to metacognitive skills. In J. C. Lester, R. M. Vicario, & F. Paraguaçu
(Eds.), Proc. ITS 04, 227-239.
2. Atkinson, R. K., Renkl, A., & Merrill, M. M. Transitioning
from studying examples to solving problems: Effects of selfexplanation prompts and fading worked-out steps. J. Ed. Psych,
95 4 (2003), 774–783.
3. Baker, R.S.J.d., Corbett, A.T., & Aleven, V. More Accurate
Student Modeling Through Contextual Estimation of Slip and
Guess Probabilities in Bayesian Knowledge Tracing. In Proc.
ITS ’08, 406-415.
4. Baker, R. S., Corbett, A. T., Koedinger, K. R., & Wagner, A.
Z: Off-task behavior in the Cognitive Tutor classroom: When
students “game the system.” In Proc. CHI 2004, ACM Press
(2004). 383–390.
5. Beck, J. E., & Sison, J.: Using knowledge tracing in a noisy
environment to measure student reading proficiencies.
IJAIED, 16 (2006). 129-143.
6. Blanchard, E. G., & Ogan: A. Infusing Cultural Awareness in
Intelligent Tutoring Systems for a Globalized World. Advances
in Intelligent Tutoring Systems. Springer (2010).

7. Corbett, A.T. & Anderson, J. R.: Knowledge Tracing: Modeling the Acquisition of Procedural Knowledge. UMUAI, 4 (4),
253-278 (1993).
8. Corbett, A.T. & Anderson, J.R. Locus of feedback control in
computer-based tutoring: Impact on learning rate, achievement
and attitudes. In Proc. Chi’2001, ACM Press (2001). 245-252.
9. Hoadley, C. Honwad, S., & Tamminga, K. Technologysupported cross cultural collaborative learning in the developing world. In Proc. ICIC '10, ACM Press, NY, USA (2010).
131-140.
10. Hofstede G. Culture’s consequences: Comparing values, behaviors, institutions, and organizations across nations. Sage
Publications Ltd. (2001).
11. Jain, M., Birnholtz, J., Cutrell, E. & Balakrishnan, R. Exploring display techniques for mobile collaborative learning in developing regions. In MobileHCI 2011. ACM Press.
12. Johnson, D. W. & Johnson, R. T. 1990. Cooperative learning
and achievement. In S. Sharan (Ed.), Cooperative learning:
Theory and research (pp. 23-37). NY: Praeger.
13. Koedinger, K. R. & Corbett, A. T. Cognitive tutors: Technology bringing learning science to the classroom. The Cambridge
Handbook of the Learning Sciences. Cambridge University
Press (2006). 61-78.
14. Kumar, A., Tewari, A., Shroff, G., Chittamuru, D., Kam, M., &
Canny, J. An Exploratory Study of Unsupervised Mobile
Learning in Rural India. In Proc. CHI’10, ACM Press.
15. Lomas, D., Ching, D., Hoadley, C., Patel, K., & Kam, M.
When a Console Game Becomes CSCL: Play, Participatory
Learning and 8-Bit Home Computing in India. In Proc.
ICLS’11.
16. Moraveji, N., Kim, T., Ge, J., Pawar, U. S., Mulcahy, K., &
Inkpen, K. 2008. Mischief: supporting remote teaching in developing regions. In Proc. CHI’08. ACM Press, 353-362.
17. Nicaud J.F., Bittar M., Chaachoua H., Inamdar P., & Maffei L.:
Experiments With Aplusix In Four Countries. I. J. Tech. in
Math Ed., 13, 1 (2006).
18. Ogan, A., Aleven, V., & Jones, C: Advancing development of
intercultural competence through supporting predictions in narrative video. IJAIED, 19(3), 67-288 (2010).
19. Pal, J., Pawar U., Brewer, E. & Toyama,K: The case for multiuser design for computer aided learning in developing regions.
WWW2006, Edinburgh Scotland, May 2006.
20. Patton, M.Q. Qualitative Research and Evaluation, 3rd ed.,
Sage, Thousand Oaks, CA (2002).
21. Schofield, J. W. Computers and classroom culture. New York:
Cambridge University Press (1995).
22. Sipitakiat, A., Blikstein, P. & Cavallo, D. GoGo Board: Augmenting Programmable Bricks for Economically Challenged
Audiences, In Proc. ICLS’04.
23. VanLehn, K. The behavior of tutoring systems. IJAIED, 16, 3
(2006), 227-265.
24. Vassileva, J., McCalla, G., & Greer, J. Multi-Agent Multi-User
Modeling in I-Help. UMUAI, 13, 179-210, (2003).
25. Warschauer, M. Technology and social inclusion: Rethinking
the digital divide. Cambridge, MA: MIT Press (2003).

1390

UBICOMP '14 ADJUNCT, SEPTEMBER 13 - 17, 2014, SEATTLE, WA, USA

Programming Tool of Context-Aware
Applications for Behavior Change
Jisoo Lee

Winslow Burleson

Abstract

School of Arts, Media +

School of Computing,

Engineering, Arizona State

Informatics, and Decision

University, Tempe, AZ 85281

Systems Engineering, Arizona

jisoo.lee@asu.edu

State University, Tempe, AZ

While users often have goals related to developing
better habits (e.g., eating more healthy food,
exercising more frequently), they are typically not very
effective at achieving those goals. We have been
developing a toolkit that provides hardware and
software for users who have no programming
experience to easily invent and test context-aware
applications that can help them make changes in their
behaviors. We have found that this toolkit needs to
balance simplicity of interaction with the facilitation of a
wide range of user experiences. To address this issue,
we identified key temporal rule patterns from a usergenerated collection of behavior change applications,
and created programming elements with which users
can compose applications of those patterns.

85281
Erin Walker

winslow.burleson@asu.edu

School of Computing,
Informatics, and Decision

Eric B. Hekler

Systems Engineering

School of Nutrition & Health

Arizona State University, Tempe,

Promotion, Arizona State

AZ 85281

University, Phoenix, AZ 85004

erin.a.walker@asu.edu

ehekler@asu.edu

Author Keywords
Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact
the Owner/Author. Copyright is held by the owner/author(s).
UbiComp’14 Adjunct, September 13-17, 2014, Seattle, WA, USA
ACM 978-1-4503-3047-3/14/09.
http://dx.doi.org/10.1145/2638728.2638735

Behavior change; Context-aware computing; End-user
programming tools

ACM Classification Keywords
H.5.m. Information interfaces and presentation (e.g.,
HCI): Miscellaneous.

Introduction
Significant work has been carried out to enhance
people's behavior change in various domains,

91

UBICOMP '14 ADJUNCT, SEPTEMBER 13 - 17, 2014, SEATTLE, WA, USA

leveraging Ubiquitous Computing technology. However,
it is mostly provisioned as prefabricated ‘solutions’.
While this research is important, it ignores users’
agency in determining which behaviors they want to
change and how. A complementary pathway in line with
the rising do-it-yourself culture would be to develop
tools that can facilitate self-experimentation with
behavior change strategies for creating solutions to
unique personal needs. With these tools, users will be
able to discover the behavior change solutions that are
most effective for them.
Therefore, we have been developing a toolkit that
provides hardware and software well-suited for users
who have no programming experience to easily invent
and test context-aware applications in an attempt to
promote behavior change for a personally salient,
home-based behavior (i.e., sitting/TV watching,
snacking, or flossing). We focus on users’ realization of
the cues-to-action behavior change strategy with
context-aware computing that enables just-in-time
delivery of adapted information [4]. In addition to
significant persuasive power by intervening at the most
opportune moments, we conceive the crucial role of
contextual cues in habit formation [8]. These context
cues are highly idiosyncratic, and therefore require
individuals to actively engage in creating the solutions
as they know best about their own contexts.

Figure 1. Exemplar use of
magnetic sensors for detecting
user's object use.

Although there has been considerable research on enduser programming tools for creating context-aware
applications in home environments, most tools are to
support control of appliances or environmental
equipment [1][2]. The provision of toolkits focused on
behavior change likely involve addressing user needs

92

that are distinct from the ones currently advanced by
existing smart home control and automation systems.
One of the most salient issues encountered when
developing the toolkit was to create programming tools
that are simple enough for a non-computer scientist yet
allow creation of a wide range of user experiences for
supporting behavior change. We identified key rule
patterns from a user-generated collection of behavior
change applications, and determined the necessary
programming elements with which users can compose
applications of those patterns. In this paper, we
present our approach in developing the toolkit and then
describe the patterns and programming elements.

Approach of Toolkit Development
The toolkit enables rapid prototyping of rule and eventbased systems that include physical sensing, data
storage, and media event components [5]. This is an
exemplar application that can be implemented with the
proposed toolkit. A person who becomes concerned
about always skipping brushing her teeth at night may
have this application idea: “When I enter the bathroom
between 9 and 11pm (opportune as I can immediately
start brushing), I hear a silly sound effect that I set to
prompt me. As I start brushing, a news podcast starts.”
The toolkit includes off-the-shelf X10 sensors for
detection of user’s interactions with objects and spaces
(Figure 1)[5]. For example, a sensor could be attached
to a toothbrush for detecting the start of brushing
teeth. It integrates two prompting channels: first, audio
contents via location-based displays (i.e., wireless
speakers), and secondly, text messages via mobile
devices. The audio content includes machine speech of
user-input text, and play of user-added/selected sound

SESSION: UBICOMP POSTERS

Figure 2. Exemplar composition
(“If TV turned on after 8 PM, a
sound clip played.”) of GaLLaG
Strip

Examples of each pattern
respectively
1. “If I keep brushing my
teeth for 2 minutes, an
applause sound plays”
2. “If I have not washed my
hands in 10 minutes after
coming home, a ‘water’
sound clip plays”
3. "If I brushed my teeth at
three consecutive nights, my
favorite songs play when I
open my chocolate box."
4. “Two minutes later after
an entrance door is closed, I
hear music from the
bathroom inviting me to
washing hands”

files. We developed a first version visual programming
approach, GaLLaG Strip [5], to support this process. By
arranging visual elements in a linear fashion, users
define if-then rules (Figure 2). This approach of starting
with simple but essential logic is in line with previous
research [1][7].

Although our early user studies [6] confirmed benefits
of simple if-then rules proposed by the existing
research (people naturally utilize them in defining
applications), there were interactions in the participantgenerated scenarios that cannot be implemented with
the current functionality of GaLLaG Strip. Based on
frequency of use and significance with respect to core
behavior change techniques, we identified the following
types of rules for eliciting a system response:

P.M.”, or “beginning at 7 P.M for 2 hours”), ‘iCAP’ [1]
allows richer expression by further including logic for
ordering (e.g., “if Tom walks in after Jane finished her
dinner”). With the identified patterns, especially the
first three, we conceived the need to expand the
functionality proposed in the previous studies. Pattern 1
and 3 reveals the need of functionality to track the
degree of performance (e.g., duration, frequency) and
respond according to it, that is, enable users to involve
the self-monitoring strategy. Although we focus on the
application of the cues-to-action technique, it is likely
that other behavior change strategies, such as selftracking, may likely be required in the system. Second,
with Pattern 2 and 3, we learned that users would like
to be prompted when a behavior was missed. Thus, the
second version includes logic for checking whether an
activity is performed in a specified period of time.

1.

When an action continues for specific duration;

Visual Interface Design

2.

If another action has or has not occurred for
specific duration since an action occurred;

3.

If an action has or has not occurred between two
absolute times;

4.

When specific duration passes after an action
occurs, a system response is made.

We determined a set of programming elements that
allow composition of the identified rules. In visually
representing the elements and relationships between
them, we employed a block-based approach. For users’
intuitive understanding, we used different interlocking
shapes that constrain connections. There were a total
of nine possible blocks for the condition of the rules
(that is, blocks placed above the ‘THEN’ block in Figure
3), classified into Objects, Use of Objects, Time, Time
Elapsing, and History. Time order of a rule is
represented by placement of the elements, that is, an
element placed above another one should occur first.
Laying elements horizontally means no specification on
their time order. Blocks of the Time Elapsing and
History categories define time duration of specific
situations, time passed after situations, and time
periods to query include other blocks to describe the

Key Patterns in User-Generated Rules

This finding teaches us that inclusion of temporal
relationships is quite desirable, if not essential, in
behavior change applications. Existing end-user
configuration or programming tools for context-aware
applications are discriminated from each other in terms
of their involvement of time-related logic for conditional
rules. For instance, ‘Play bits’ [3] does not involve any
temporal logic. While ‘CAMP’ [7] only provides logic to
define time periods (e.g., Dinner can be defined to
happen “in the dining room between 7 P.M. and 9

93

UBICOMP '14 ADJUNCT, SEPTEMBER 13 - 17, 2014, SEATTLE, WA, USA

Entrance door

Becomes On
Two conditional blocks; the left
block consisting of one piece
constraining time, and the right
block, multiple pieces representing
timely use of objects

Faucet

Becomes On

situations related. For example, in
Figure 3, the red block (“Elapse 10
min(s)”) wraps the blocks (“Faucet”,
“becomes on”, and “0 time”),
defining what should occur after
“Entrance door” “Becomes On.” If
the user enters the house and,
within 10 minutes, the faucet has
not been turned on, the rule is
satisfied.

Conclusion and Future Work

We developed a toolkit to support
users’ creation of context-aware
applications more relevant to their
Between
Elapse
10
min(s)
lives and ultimately more effective
4:00pm ~ 7:00pm
at promoting behavior change. Our
on-going research for optimization is
THEN
THEN
to figure out functionality essential
for behavior change solutions. In
'THEN' piece with three
this paper, we presented our second
Plays
connections (by default, two)
iteration of the programming tool
design. It extends the previous,
Sound: Water
Pieces defining system response
initial tool by involving elements for
placed under 'THEN' piece
the temporal relationships that we
found from the user-generated
Figure 3. Composition of the
scenarios. In the immediate future we will be
Patter 2 example, “If I have not
conducting a lab study with the current paper-based
washed my hands in 10 minutes
prototype, to examine: (1) Can people easily
after coming home, a ‘water’
understand how to use the tool, and (2) construct what
sound clip plays”
they intend? This study will be followed by further
design modifications, development of working
prototypes, and a field study in natural environments.
In addition to the effort to improve usability and
expressiveness of the programming tool, we anticipate
needs to enhance users’ creativity and knowledge of
0 time(s)

94

behavior change techniques so that they can generate
rich and meaningful solution ideas.

Acknowledgement
This work was supported, in part, by a Google Faculty
Research Award (PI: Hekler).

References
[1] Dey, A., Sohn, T., Streng, S., & Kodama, J. (2006).
iCAP: Interactive prototyping of context-aware
applications. In Pervasive Computing (pp. 254–271).
[2] García-herranz, M., Haya, P., & Alamán, X. (2010).
Towards a Ubiquitous End – User Programming System
for Smart Spaces. J. UCS, 16(12), 1633–1649.
[3] Humble, J., & Crabtree, A. (2003). “Playing with
the Bits” User-configuration of Ubiquitous Domestic
Environments. In UbiComp 2003: Ubiquitous
Computing (pp. 256–263). Springer Berlin Heidelberg.
[4] Intille, S. S. (2006). The goal: smart people, not
smart homes. In Proc. ICOST2006: The International
Conference on Smart Homes and Health Telematics
(pp. 3–6).
[5] Lee, J., Garduño, L., Walker, E., & Burleson, W.
(2013). A tangible programming tool for creation of
context-aware applications. In Proc. the 2013 ACM
international joint conference on Pervasive and
ubiquitous computing (pp. 391–400). ACM.
[6] Lee, J. (2013). Supporting self-experimentation of
behavior change strategies. In Proc. the 2013 ACM
conference on Pervasive and ubiquitous computing
adjunct publication (pp. 361-366). ACM.
[7] Truong, K. N., Huang, E. M., & Abowd, G. D.
(2004). CAMP: A magnetic poetry interface for enduser programming of capture applications for the home.
In UbiComp 2004: Ubiquitous Computing (pp. 143160). Springer Berlin Heidelberg.
[8] Wood, W. and Neal, D.T. (2007). A new look at
habits and the habit-goal interface. Psychol Rev 114,
843– 863.

