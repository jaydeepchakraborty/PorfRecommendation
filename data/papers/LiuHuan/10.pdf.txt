Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/305828246

Areviewofkeytechniquesofvision-based controlforharvestingrobot
ArticleinComputersandElectronicsinAgriculture·September2016
DOI:10.1016/j.compag.2016.06.022

CITATIONS

READS

0
4authors,including: YuanshenZhao ShanghaiJiaoTongUniversity
12PUBLICATIONS10CITATIONS
SEEPROFILE

126

LiangGong ShanghaiJiaoTongUniversity
35PUBLICATIONS84CITATIONS
SEEPROFILE

YixiangHuang ShanghaiJiaoTongUniversity
27PUBLICATIONS163CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

Astudyonkeytechniquesofmulti-armharvestingrobotViewproject

AllcontentfollowingthispagewasuploadedbyYuanshenZhaoon06August2016.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Computers and Electronics in Agriculture 127 (2016) 311­323

Contents lists available at ScienceDirect

Computers and Electronics in Agriculture
journal homepage: www.elsevier.com/locate/compag

Review

A review of key techniques of vision-based control for harvesting robot
Yuanshen Zhao, Liang Gong, Yixiang Huang, Chengliang Liu 
School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China

a r t i c l e

i n f o

a b s t r a c t
Although there is a rapid development of agricultural robotic technologies, a lack of access to robust fruit recognition and precision picking capabilities has limited the commercial application of harvesting robots. On the other hand, recent advances in key techniques in vision-based control have improved this situation. These techniques include vision information acquisition strategies, fruit recognition algorithms, and eye-hand coordination methods. In a fruit or vegetable harvesting robot, vision control is employed to solve two major problems in detecting objects in tree canopies and picking objects using visual information. This paper presents a review on these key vision control techniques and their potential applications in fruit or vegetable harvesting robots. The challenges and feature trends of applying these vision control techniques in harvesting robots are also described and discussed in the review. Ó 2016 Elsevier B.V. All rights reserved.

Article history: Received 11 December 2015 Received in revised form 16 June 2016 Accepted 20 June 2016

Keywords: Harvesting robot Vision-based control Vision information acquisition Fruit recognition Eye-hand coordination

Contents 1. 2. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Vision-based control for harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1. The concept of vision-based control for harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2. The role of vision-based control in harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Vision schemes for harvesting robot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1. Monocular camera scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2. Binocular stereovision scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3. Laser active visual scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4. Thermal imaging scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5. Spectral imaging scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Recognition approaches for harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1. Single feature analysis approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2. Multiple features fusion approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3. Pattern recognition approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Eye-hand coordination in harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1. Open-loop visual control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2. Visual servo control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Examples of fruit harvesting robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Challenges and future trends. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1. Enhancing the vision-based control of harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2. Human­machine collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3. Multi-arms cooperating for robotic harvesting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4. Making the environment more suitable for robotic harvesting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Conclusions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312 312 312 312 312 313 313 314 314 315 316 316 316 317 318 318 318 319 319 319 320 320 320 321 321 321

3.

4.

5.

6. 7.

8.

 Corresponding author.
E-mail address: chlliu@sjtu.edu.cn (C. Liu). http://dx.doi.org/10.1016/j.compag.2016.06.022 0168-1699/Ó 2016 Elsevier B.V. All rights reserved.

312

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311­323

1. Introduction With the development of modern agriculture, the application of robotic and intelligent machines in agriculture has followed several trends in technology advancements. Firstly, increasing cost and decreasing supply of skilled labor force are becoming a huge challenge to the agriculture industry. Traditional farming is highly labor intensive and contains many menial and tedious tasks, it is one of the last industries to use robots (Sistler, 1987). Secondly, food safety is an important issue that requires the use of reliable robotic machines to reduce the risk of contaminations (Edan et al., 2009). Thirdly, sustainable agriculture, which provides enough food while not harming the environment, also needs robotic systems to improve productivity at low cost (Grift et al., 2008). It is evident that wide application of robots can offer a significant benefit to agriculture. The emergence of agricultural robots is accompanied with other industries such as manufacturing and mining that have embraced the robotic revolution. Agricultural robots are perceptive and intelligent machines which are programmed to perform a variety of agricultural tasks such as transplanting, cultivating, spraying, trimming and harvesting (Edan et al., 2009). Considering the economic benefit, high harvesting costs have led the harvesting robot as a research focus (Hayashi et al., 2005). Harvesting robots are designed to sense the complex agricultural environment by various sensors and use that information, together with a goal, to perform the harvesting actions (Edan and Gaines, 1994). Although harvesting robot holds ample promise for the future, currently the overall performance of harvesting robot is often insufficient to compete with manual operation (Grift et al., 2008). The bottleneck to promote the application of harvesting robot lies on the performance of vision-based control. The use of visual information for the control of robotic manipulator is called vision-based control, which began with the work of Shirai and Inoue (1973). The progress of vision control in that era was hindered largely by various technological issues, in particular, extracting information form vision sensors (Corke and Hager, 1998). Since 1990, there has been a marked rise in the interest in this field of vision control, largely fueled by the increasing computing power of personal computers. After that, vision-based control for harvesting robot had ushered in the era of rapid development. Although numerous research results have been reported on development of vision control technology for robotic harvesting, the low successful rate of fruit recognition and inefficiency of eye-hand coordination are the main factors to limit the performance of harvesting robot. Thus, a review of this research field is necessary to promote further developments of vision-based control technology for harvesting robot. This article provides a review of the past and current research and development of vision-based control for harvesting robot. It is aimed to introduce an up-date account of useful methods found in literature to provide solutions to the two key issues: (a) the recognition of target fruit; (b) eye-hand coordination control. The remaining of this paper is organized as follows. In Section 2, a general background to vision-based control is introduced. Representative vision schemes for harvesting robots are presented in Section 3. In Section 4, approaches adopted for fruit target recognitions are discussed. A review of eye-hand coordination techniques is given in Section 5. Section 6 presents some examples of fruit or vegetable harvesting robots. Challenges and future trends for harvesting robots are discussed in Section 7. A conclusion is drawn in Section 8.

2. Vision-based control for harvesting robot 2.1. The concept of vision-based control for harvesting robot Vision-based control for harvesting robot is a framework by which the robot accomplishes the fruit picking task under the guidance of visual information. This framework is constructed with two objectives; fruit recognition and eye-hand coordination (Hashimoto, 2003). Automatic fruit recognition for harvesting robot means identifying and locating the fruit in a natural complex scene. These two tasks are the foundation of picking operation. Eye-hand coordination for harvesting robot is concerned with the interaction between the robot visual perception of the workspace and it actuators (Goncalves and Torres, 2010).

2.2. The role of vision-based control in harvesting robot The idea of robotic harvesting was firstly proposed by Schertz and Brown (1968) in 1960s for citrus harvesting. Compared with the traditional mechanical harvesting approaches using shaker or air blast, robotic harvesting is a precision harvesting approach. Typical fruit or vegetable harvesting robots are built with manipulators, end-effectors, vision systems, and motion systems (Edan et al., 2009). Among these, vision-based control plays an important role of autonomous harvesting. On the contrary to industrial robots which are simple, repetitive, well-defined and known a priori, harvesting robots need to work in an unstructured, uncertain, and varying environment. Vision-based control for harvesting robots is designed to solve the follow difficult problems. Firstly, the manipulated objects of harvesting robots are natural objects which have a high degree of variety in fruit size, shape, color, texture and hardness as a result of environmental and genetic differences. Secondly, the workspace is complex and loosely structured with large variations in illumination and degree of object occlusion. Thirdly, the random location of target fruits requires picking to operate in a three-dimensional continuously changing track. Thus, vision-based control is an attractive approach to meet these challenges.

3. Vision schemes for harvesting robot Fruit or vegetable detection for harvesting robot is conducted by various visual sensors. According to the principle of imaging, the visual sensors used to recognize objects are classified into two-dimension (2D) visual imaging sensors and three-dimension (3D) visual imaging sensors. The 2D images acquired can indicate morphological features of the target fruit such as color, shape and texture. Three-dimensional visual image sensors provide 3D coordinate maps of the entire scene which can give the shape and spatial location of the fruit object. The vision scheme also has relationship with the recognition process. As shown in Fig. 1, for identifying different kinds of objects, it is needed to select available visual sensors cooperating with a certain recognition algorithm. The review of recognition algorithms such as color based analysis, edge detection, K-means clustering, and Bayes classification is given in Section 4. The follow sub-sections contain a critical review of visual sensors used in the past for fruit detection in harvesting robot. The applications, principles, advantages and limitations of various vision schemes for harvesting robots are summarized in Table 1.

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311­323

313

Vision schemes

Algorithms ·Color based analysis ·Shape based analysis ·Texture based analysis ·Neural network approach ·Multi-feature fusion ·Fuzzy clustering means ·Support vector machine 

Objects ·Tomato ·Apple ·Cucumber ·Pepper ·Strawberry ·Citrus ·Eggplant · 

·Monocular ·Binocular stereoscope ·Laser active visual ·Spectral imaging ·Thermal imaging 

Fig. 1. Various vision schemes and recognition algorithms for different kinds of fruits (Li et al., 2011a,b).

Table 1 The classification of applications, principles, advantages and limitations of various vision schemes for harvesting robots. Vision scheme Monocular Binocular stereo Laser active visual Thermal imaging Spectral imaging Applications and principles Identifying target fruits by color, shape, and texture feature Identifying fruits using color shape, and texture features; positing target fruits through the principle of triangulation Identifying fruits using 3D shape feature; positing target fruits through 3D reconstruction approach Identifying target fruits using the differences of infrared radiation between target fruit and background Identifying target fruits using features extracted from invisible wavelengths Advantages Monocular vision system is the simplest and lowest cost The binocular stereo is the most common approach to obtain the 3D position of detected fruit It is an alternative to obtain the 3D position in the condition of light changing and background clustering It is available to detect target fruit in varying illumination condition, especially at night It can detect the green color or overlapped fruits Limitations Only provides 2D information, light change influences the imaging results Sensor calibration is required, image matching is very time consuming; errors in 3D measurement is unavoidable Vision system is required for geo-referencing, complex and large image data are needed. The imaging processing is also a challenge Sensor calibration and atmospheric correction are required, high computation consumption for image processing Imaging processing is very time consuming, the sensor cost is high

3.1. Monocular camera scheme Monocular scheme is a machine vision system consisting of a single camera, which was used in some earliest studies for detecting fruit (Jimenez et al., 2000a,b). The cameras with Charged Coupled Device (CCD) sensors or Complementary Metal Oxide Semiconductor (CMOS) sensors are widely used in monocular schemes. In the MAGALI project, a B/W camera was applied to detect the fruits based on geometric feature (d'Esnon et al., 1987). In later years, the B/W camera was replaced with a color camera to enhance the color contrast between red apples and green leaves. Slaughter and Harrell (1987) also used a digital color camera with a filter of 675 nm wavelength to amplify the contrast between oranges and background. The author reported a detection accuracy of 75%. Zhao et al. (2005) used a color camera to identify apples based on color and texture features and reported an accuracy of 90%. Baeten et al. (2008) developed a monocular vision system combining a camera and a high frequency light source to detect apples in outdoor environment. The author recognized that high frequency light could reduce the illumination influence. There are other vision systems for harvesting robot with several cameras forming a redundancy monocular system. Edan et al. (2000) developed a multiple monocular cameras system which was constructed with two B/W CCD cameras to detect and locate melons in the field. The two B/W cameras mounted on the platform and griper could acquire far scene images and near scene images. The author reported that the use of multiple monocular cameras could improve the recognition accuracy. The major disadvantage of monocular scheme is that images captured by the visual sensor are sensitive to illumination conditions (see Table 2).

3.2. Binocular stereovision scheme The Binocular stereovision scheme is designed with two cameras separated in a certain distance with an angle between them, and they capture the same scene in two images. The threedimensional map of fruit object can be obtained through triangulation (Sun et al., 2011). Buemi et al. (1995, 1996) used a color stereoscopic vision system consisting of two micro cameras in a tomato harvesting robot named Agrobot. The stereovision system mounted on the robot head could be used to navigate and identify the ripe tomato. Shinsuke and Koichi (2005) installed a parallel stereovision system of two cameras in a sweet pepper picking robot. The stereovision system controlled by a camera positioning system could move to a desired location to capture images of sweet peppers. Yang et al. (2007) developed an improved stereovision system based on the Color Layer Growing (CLG) algorithm to reconstruct the 3D model of fruit object. Fruit objects with stereo and self-occlusion in a strong sunlight condition could be detected and located by the improved stereovision system. Xiang et al. (2014) also studied a clustered tomato recognition method based on depth map acquired by a binocular stereo camera. The recognition accuracy of clustered tomatoes was 87.9% at an acquisition distance of 300­500 mm. Li et al. (2011a,b) developed a stereovision system, with optical filters, which was an attempt to capture different waveband images. The recognition test results indicated that the polarizer filtered data is slightly better than neutral density filtered data, and much better than the original image data. Si et al. (2015) also used a stereo camera to detect and locate mature apples in tree canopies. As shown in Fig. 2, the stereo camera was mounted on the slide bar in parallel with a distance of

314

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311­323

Table 2 The comparison of different single feature analysis approaches. Features Color Shape Texture Fruits Tomato Strawberry Apple Peach Pineapple Bitter melon Application situations Artificial light condition Uncontrolled environment Uncontrolled environment Uncontrolled environment Field Single objects Accuracy 96.36% >95% 94% 90% 85% 100% Limitations It is not available to the green color fruit such as cucumber The contour loss is less than 1/2 High computation cost, occlusion is a big challenge Reference Arefi et al. (2011) Wei et al. (2014) Kelman and Linker (2014) Xie et al. (2011) Chaivivatrakul and Dailey (2014)

Fig. 2. Fruit harvesting robot with a binocular stereoscope. (1) Binocular stereoscope, (2) Manipulator, (3) End-effector, (4) Fruit object, (5) mobile platform (Si et al., 2015).

200 mm between the centers of the two camera lens. The author reported that over 89.5% of apples were successfully recognized and the errors were less than 20 mm when the measuring distance was between 400 mm and 1500 mm. The disadvantage of stereovision scheme is its complexity and long computation time due to stereo matching (Hannan and Burks, 2004). 3.3. Laser active visual scheme Although there are several techniques to obtain depth information, but considering some desirable features of the sensed image, the laser active visual is a better choice. The 3D shape of fruit object is measured by scanning the laser beams and the fruit can

be distinguished from other obstacles according to different spectral-reflections. The laser active visual scheme for fruit detection is illustrated in Fig. 3 (Gotou et al., 2003). Both laser beams scan the fruit object simultaneously and the locations of fruit objects and obstacles are recognized through image processing. An infrared laser range-finder was installed in an orange harvesting robot named Agribot working in non-structured environments, Jimenez et al. (2000a,b). The output of the infrared laser range finder includes 3D position, radius, and surface reflectivity of fruit object. Tankgaki et al. (2008) designed a machine vision system equipped with red and infrared laser scanning devices to detect cherry on the tree, which could prevent the influence of the sunlight. Yin et al. (2009a,b) used a laser active visual sensor to measure the turned angles of robot arm and distance between target tomato and end-effector. Zhang et al. (2015) developed a novel apple stem recognition system using the 3D reconstruction technique combined with near-infrared and linear-array structured lighting. The author reported that 97.5% overall recognition accuracy for the 100 samples was obtained by the proposed system and method. Even though the accuracy of laser active visual system is promising, the complexity of the system often limits its practical application. 3.4. Thermal imaging scheme Thermal imaging is also called infrared thermograph which is the visualization of infrared radiation (Li et al., 2014). Because of the physical structure and characteristic, leaves accumulate less heat and radiate faster than fruits, the temperature distributions of the plant canopy with fruit can be applied for fruit detection (Vadivambal and Jayas, 2001). Xu and Ying (2004) used a thermal camera to identify the citrus in a tree canopy. From the analysis result shown in Fig. 4, the temperature distribution along line AB

Infrared laser diode Laser beam (Infrared and red)

Oscillator 35kHz

Timing signal

Red laser diode

Oscillator 17.5kHz

Fruit object

Cold filter

Lock in amplifier

Filter

A/D converter

Preamplifier PSD DC voltage Preamplifier Lock in amplifier Filter Lock in amplifier Filter

Lens

Fig. 3. A scheme of the laser active visual system for fruit recognition (Gotou et al., 2003).

Computer

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311­323

315

19.7 19.1 18.6 18.0 17.5

A

B

16.9 16.3 15.8 15.2 80 160 240 320 400 480 560 640

Fig. 4. The typical test results of thermal image for citrus recognition.

Fig. 5. The model of fruit object detection using hyperspectral image.

across the citrus area, leaves and others showed that the temperature difference between citrus and other objects was more than 1 °C. Bulanon et al. (2008) also used a thermal infrared camera to detect the citrus in day and night. The fruits were successfully segmented in the thermal images using image processing techniques according to the largest temperature difference. Based on prior research, Bulanon et al. (2009) proposed an improved fruit detection approach combined with a thermal image and a visible image. Results showed that the performance of the image fusion approach was better than using the thermal image alone. Even though thermal imaging has advantages on detecting fruits even when fruit and background color are similar, the accuracy of recognition using thermal imaging is affected by the shadow of the tree canopy (Stajnko et al., 2004). 3.5. Spectral imaging scheme The spectral camera is developed to integrate both spectroscopic and imaging techniques into one system to obtain a set of

monochromatic images at a continuum of wavelengths (Zhang et al., 2014). With recent development of spectral imaging, the spectral cameras have been used to recognize fruits. The model of fruit object detection using multispectral image is shown in Fig. 5 (Manolakis et al., 2003). A monochromatic near-infrared camera, equipped with three different optical band pass filters (1064, 1150 and 1572 nm), was used to identify in-field green citrus by Kane and Lee (2007). The author reported an average correct pixel identification of 84.5%. Safren et al. (2007) used a hyperspectral camera to detect green apples. The hyperspectral imaging was capable of giving a wealth of information both in the visible and the near-infrared (NIR) regions and thus offered the potential to detect the green apples. Okamoto and Lee (2009) also proposed a green citrus recognition method using a hyperspectral camera of 369­1042 nm to solve the detection problem arising from similar color between fruits and natural scenes. The test results reflected that 80­89% of the citrus in the foreground of the validation set were identified correctly. By comparing the spectral reflectance difference of cucumber plant (fruit, leaf and flower) from visible

316

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311­323

to infrared (350­1200 nm), sensitive bands of fruit information were obtained by statistical variance analysis (Yuan et al., 2011). 4. Recognition approaches for harvesting robot The recognition algorithm is a key factor affecting the performance of a vision recognition system. Numerous literatures reported that various recognition algorithms have been employed for robotic harvesting of fruits. Those recognition approaches can be classified into single feature analysis approaches, multiple features fusion analysis approaches and pattern recognition approaches. 4.1. Single feature analysis approaches Color is one of the most prominent features used to distinguish the mature fruit from the complex natural background. In the studies of color-based segmentation for fruit recognition, the image pixels are categorized into two groups according to the threshold which decides whether a pixel belongs to the fruit object or to the background. However, the accuracy of segmentation using color feature is sensitive to varing illumination conditions. For alleviating the influence of varying illumination, several color spaces such as HIS, L/a/b/, and LCD are used to extracting color features (Huang and He, 2012; Yin et al., 2009a,b). Arefi et al. (2011) developed a ripe tomato recognition algorithm using a combination of RGB, HSI, and YIQ color spaces and fruit morphological features. The authors argued that the total accuracy was 96.36% when the proposed approach was adopted in a greenhouse with artificial illumination. Mao et al. (2009) employed a Drg-Drb color indexing to segment apples from background and an accuracy of 90% was achieved. Wei et al. (2014) also proposed a fruit recognition method based on an improved OSTU threshold algorithm using a new color feature in the OHTA color space. The OHTA color space was transform from the RGB color model through linear conversion and the extraction accuracy was more than 95%. When the fruits and leaves have similar colors, color-based segmentation methods are not available for recognizing fruits (Reis et al., 2012). Fruit recognition algorithms based on extracting geometric features are universal for detecting spherical fruit such as tomatoes, apples, and citrus (Liu et al., 2007; Xie et al., 2010). Because of independent color features, the shape-based analysis approach is not affected by varying illuminations. Whittaker et al. (1987) proposed a modified Circular Hough Transform algorithm for locating mature tomatoes which were partially hidden from obstacles. The authors recommended that this shape-based analysis algorithm could be valid for situations in which the perimeter of the fruit is partially obscured by leaves or by overlapped tomatoes. Xie et al. (2007, 2011) also put forward a concave spots searching algorithm

based on Hough Transform to improve the accuracy of strawberry recognition. The authors argued that the proposed strawberry recognition method is effective both for single fruit and complex situation when the strawberry contour loss is less than 1/2. Kelman and Linker (2014) also proposed a localization algorithm of mature apples in trees using convexity. Together with the removing 99.8% of the edges initially identified by Canny detector, 94% of the visible apples were correctly detected. The images captured in natural outdoor conditions have some texture differences which can be used to facilitate separation of fruits from their background. Thus, texture feature plays an important role in fruit recognition especially when the fruits are clustered or occluded (Zhao et al., 2005; Kurtulmus et al., 2011a,b; Rakun et al., 2011). To use color, texture, and shape information by histogram based separation, circular Gabor texture features and eigen-fruit approaches were implemented in the fruit recognition algorithm by Kurtulmus et al. (2011a,b). Notice that the application of texture features are always combined with color features and/or geometric features. 4.2. Multiple features fusion approaches In order to increase recognition reliability in uncontrolled environments caused by uneven illumination conditions, partly occluded surfaces and similar background features, some researchers apply multiple features (color, geometry, and texture) fusion algorithms to recognize fruits. Hannan et al. (2009) also developed a machine vision algorithm consisted of color-based segmentation and perimeter-based detection. Yin et al. (2009a,b) proposed a ripe tomato recognition method which is combined with the tomato's shape features and the color features. With the color features extracted from the L/a/b/ color space and the shape feature acquired by a laser ranging sensor, the recognition and localization system for tomato harvesting robot could be used to handle the situations of tomato overlapping and sheltering. Zhao et al. (2005) proposed a texture based edge detection algorithm combined with color properties analysis to recognize on-tree apples. The authors presented that 90% of apples were correctly detected using the recognition approach. Colors, intensity, edge and orientations as the features of the target were considered by Patel et al. (2011) to develop an improved multiple features based algorithm for fruit detection. The authors reported that the detection efficiency was achieved up to 90% using the optimal weights of different features. Lu et al. (2014) also developed a novel method based on fusing the segmentation results of chromatic aberration map (CAM) and luminance map (LM) to recognize the citrus in a tree canopy. Rakun et al. (2011) comprehensively considered three distinct features; color, texture and 3D shape of the fruit object for overcoming low recognition reliability in uncontrolled environments. They

Table 3 The major types of multi-modal images recognition algorithms. Features Color + Geometry Color + Texture Color + Texture + Geometry Color + 3D shape Color + Texture + 3D shape CAM + LM I-component + a/-component Color + Amplitude image Color + Thermal image Fruits Tomato Orange Apple Citrus Peach Apple Apple Citrus Tomato Apple Apple Citrus Vision scheme Camera and Laser ranging sensor Two cameras Camera Camera Camera Camera and Laser ranging sensor Camera Camera Camera Camera and ToF camera Camera and thermal camera Camera and thermal camera Accuracy NR >90 90% 75.3% 90% >90% NR 86.81% 93% >83.67% 74% 74.37% Reference Yin et al. (2009a,b) Hannan et al. (2009) Zhao et al. (2005) Kurtulmus et al. (2011a,b) Patel et al. (2011) Bulanon and Kataoka (2010) Rakun et al. (2011) Lu et al. (2014) Zhao et al. (2016a,b) Feng et al. (2014) Wachs et al. (2009, 2010) Bulanon et al. (2009), Bulanon and Kataoka (2010)

CAM = Chromatic Aberration Map; LM = Luminance Map.

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311­323 Table 4 The main kinds of multi-modal images recognition algorithms. Pattern recognition types Statistical pattern recognition Classification algorithms Linear decision classifier Bayesian classifier X-means clustering PCA Adaboost Fuzzy pattern recognition Soft computing methods FCM Feed-forward neural network Fuzzy neural network SVM Fruits Peach Apple Orange Tomato Citrus Kiwifruit Tomato Tomato Apple Pepper Correct/error rate >89%/NR 80%/3% >75%/6% 88%/NR 75.3%/NR >92.1%/7% NR/16.55% 95.45%/NR >92.31%/NR >74.2%/NR Reference

317

Sites and Delwiche (1988) Bulanon et al. (2004) Slaughter and Harrell (1989) Yamamoto et al. (2014) Kurtulmus et al. (2011a,b) Zhan et al. (2013) Wang et al. (2015) Arefi and Motlagh (2013) Ma et al. (2013) Song et al. (2014)

applied color segmentation to multiple scene images to separate potential regions from background and verify them first with texture analysis and then by reconstructing in the 3D space. The recognition methods based on multi-sensor fusion technology have been used to fruit recognition (Bulanon et al., 2009; Bulanon and Kataoka, 2010; Wachs et al., 2009, 2010; Feng et al., 2014). For overcoming the challenge of recognizing green fruit in the tree canopy, Bulanon et al. (2009) and Wachs et al. (2010) used infrared images and visible images fusion to improve fruit detection. The infrared image captured by thermal infrared camera and visible image captured by a color camera required image registration prior to image fusion. Maximization of the mutual information was employed by Wachs et al. (2009) to find the optimal registration parameters for image fusion. The authors argued that the recognition accuracy of fusion approach (74%) was increased compared to the conventional approach of detection using either color (66%) or IR (52%) alone. The amplitude image acquired by ToF camera and H component image extracted from HSI color space were selected as source images for fusion by Feng et al. (2014). The fusion algorithm which is aimed at enhancing the fruit object area distribution in the fused image could produce more accurate and robust fruit recognition. A summary of the major types of multi-modal based algorithms is given in Table 3. 4.3. Pattern recognition approaches Pattern recognition approaches have long been investigated for application in fruit recognition (see Table 4). Early in the 1970s, Parrish and Goksel (1977) had suggested that pattern recognition approaches could be used for fruit recognition. Two linear classification techniques including a non-parametric linear classifier and linear decision function classifier were evaluated by Sites and Delwiche (1988). The outcome indicated that both classification algorithms produced similar results, and the non-parametric linear classifier was easier to implement. Bulanon et al. (2004) also developed an apple detection method using the linear decision classifier and the trichromatic coefficients as patterns. About 80% of fruit pixels were correctly classified under all lighting conditions with less than 3% error rate. Slaughter and Harrell (1989) used a Bayesian classifier to discriminate oranges from the natural background. The classification model using chrominance and intensity information could correctly classify over 75% of the fruit pixels. In order to solve the overlapping problem in plantlets recognition, Pastrana and Rath (2013) developed a novel pattern recognition approach using an active shape model (ASM). Yamamoto et al. (2014) applied the X-means clustering algorithm on the basis of K-means clustering to determine the optimal number of clusters and to detect individual fruit in a multi-fruit blob. Due to their similarities, fruit detection tasks can be conducted with the similar method for face recognition and detection. Kurtulmus et al. (2011a,b) used an `eigenfruit' approach based on principle compo-

nent analysis (PCA) to detect green citrus under natural illumination. Zhan et al. (2013) used an Adaboost algorithm to recognize the kiwifruit in field and achieved and ideal effect for the segmentation between kiwifruit and trunk, soil and branches. The Adaboost algorithm could combine the strengths of two weak classifiers and mitigate their shortcomings. Zhao et al. (2016a,b) also developed an algorithm combining AdaBoost classifier and color analysis for the automatic detection of ripe tomatoes in greenhouse. It argued that over 96% of ripe tomatoes were correctly detected. These statistical pattern recognition approaches are developed according to the posterior probability of the samples. Thus, more and more attention are being paid on intelligent pattern recognition methods such as artificial neural networks (ANN), support vector machine (SVM), and fuzzy pattern recognition. ANN and SVM are supervised learning algorithms that have the ability to learn from the data through an iterative training process and improve its performance after each iteration. An olive recognition method using neural networks was presented by Gatica et al. (2013). The process of fruit recognition comprised of two stages: the first stage focused on deciding whether or not the candidate identified in the image corresponds to an olive fruit, the second stage focused on olives overlapping within the tree canopy. Arefi and Motlagh (2013) developed an experts system based on wavelet transform and ANN for ripe tomato detection. Totally 90 wavelet features were extracted from each tomato, and a feed-forward neural network was used to distinguish the ripe tomato from its background. An accuracy of 95.45% was obtained from the proposed recognition algorithm. In order to overcome the fuzziness and uncertain factor existing in the color image boundary pixels, a model combining quantum genetic algorithm and fuzzy neural networks was built by Ma et al. (2013). An improved fuzzy neural network could avoid redundant iteration and the tendency to fall into the local minimum of traditional BP neural networks. Ji et al. (2012) introduced a new classification algorithm based on support vector machine to improve the apple recognition accuracy and efficiency. The new classifier had balanced the recognition success rate and the time used in recognition. A statistical classifier, an ANN and a SVM classifier were built and used for detecting peach fruit by Kurtulmus et al. (2011a,b, 2014). Authors reported that 84.6%, 77.9% and 71.2% of the actual fruits were successfully detected, using the three classifiers for the same validation set. For improving the tomatoes identifying accurately, Song et al. (2014) also used a bag-of-words (BoW) model to locate peppers on the plant. The BoW model represented each image by a frequency distribution of its visual vocabularies, which was classified to a fruit class. Wang et al. (2015) presented a Fuzzy Clustering Means (FCM) algorithm to recognize the clustered tomatoes. The superiority of this algorithm was verified according to a comparison with K-means and Otsu threshold. In order to accelerate the computation of the traditional FCM, Xiong et al. (2013) presented

318

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311­323

Table 5 The comparison of two types of eye-hand coordination control. Eye-hand coordination Open-loop visual control Principle Hierarchical controlling based on precision 3D measuring Dynamic interacting between the robot and environment Advantages Control law is simpler; controllability and region of stability are better Calibration is not required; real-time tracking is achieved and it is object friendly Limitations Performance depends on the accuracy of measurement, assembly and calibration Local minima of potential and unexpected camera trajectory

Visual servo control

an improved FCM algorithm based on the fusion of the bicubic interpolation algorithm and the FCM algorithm. The improved recognition algorithm was available for litchi recognition which enabled the recognition system to operate in real-time. 5. Eye-hand coordination in harvesting robot Vision-based robot control has been investigated for more than 30 years. This technology promises substantial advantages when working with the targets whose positions are unknown, or with manipulators which may be inaccurate. Visual information can be used for controlling the robotic manipulator or guiding its motion. The two types of vision-based control applications in robot control loop are called eye-hand coordination and visual navigation. Visual navigation has been widely applied (Khadraoui et al., 1998), while the eye-hand coordination is a bottleneck to improve the performance of harvesting robot. Thus, further developments in vision-based control for harvesting robot are necessary. In this section, an overview of the eye-hand coordination control in harvesting robot is given. Traditionally, eye-hand coordination control systems were based on an open loop control framework which employs the ``looking then moving" mode of operation. The control precision depends directly on the accuracy of the vision system, and the calibration of manipulator and assembly (Yau and Wang, 1996). An alternative to increase the accuracy of these subsystems is to use a visual feedback control loop (Corke and Hager, 1998). This particular vision-based robotic control mode is also called visual servo. The visual servo is a framework to implement ``looking and moving" as a dynamical system. The task of visual servo for harvesting robot is to control the pose of the robot's end-effector using image features which are extracted from the image captured by the camera-in-hand or fixed camera (Hashimoto, 2003). The comparison of these two types of eye-hand coordination control is given in Table 5. 5.1. Open-loop visual control The open-loop visual control mode is built for accurate positioning of the fruit object in 3D workspace. Therefore, vision systems of open loop control may consist of stereo vision or laser range sensor that can measure the spatial distance between the target fruit and the end-effector. Following a precision distance measurement, the trajectory of the manipulator can be planned through calculating the kinematics of the robot. Hence, the manipulator kinematic model and calibration of vision system have to be very precise. In the study of vision-based control for robotic harvesting of cherry tomatoes (Kondo et al., 1996), a open-loop visual control scheme was implemented based on the 3D position detection

acquired by a stereo camera. The end-effector was first sent to a location based on the X, Z position of the fruit and the Y (depth information) of the cluster center. If the fruit could not be reached, the end-effector advanced forward 50 mm (in Y direction) in the next movement. Thus, the author argued that the harvesting success rate was affected by the accuracy of the calculated depth. Inoue et al. (1996) also developed an open-loop visual control scheme based on precise position detection for robotic harvesting. The minimum distance gathering path to effectively touching the target fruit could be calculated from this control model. In order to improve the precision of position measurement, multiple visual sensors are employed in the vision system. Hayashi et al. (2010) developed a machine vision unit equipped with three aligned cameras to enhance the recognition rate. The center camera was used to calculate the inclination of the peduncle, whereas the two cameras mounted on both sides of the center camera form a stereovision system to determine the 3D position of the fruit. Han et al. (2012) also developed the strawberry harvesting robot based on open-loop visual control. The 3D position of target strawberry was acquired by a color stereoscope camera and a laser device. The performance of the vision-based control scheme in field tests showed that the execution time for successful harvest of a single strawberry was less than 7 s. In some cases, the fruit position in the tree canopy would be influenced by wind or manipulator movement. When this situation occurs, the efficiency of open-loop visual control for robotic harvesting is very low. Shen et al. (2011) have researched on the increasing harvest efficiency of the fruit harvest robot in oscillatory conditions. The oscillation frequency was obtained by curve fitting and applying fast Fourier transform to video samples. With the calculated movement duration of the end-effector, it can eliminate the time waiting for the oscillation to decay. Font et al. (2014) also investigated a vision control strategy by combining open-loop visual control and visual closed-loop control. A stereovision camera mounted on a robot arm could acquire the initial fruit location. With the open-loop visual control, the grasper could move quickly to the front of the target fruit. The final picking operation was conducted by iteratively adjusting the vertical and horizontal positions of the gripper through closed-loop visual control. Aiming at solving the positioning problem, Zou et al. (2012) developed a binocular stereo vision system and position principle of the picking manipulator in virtual environment (VE). The stereo vision data was mapped to the manipulator and was guided by accurate positioning in VE. The simulation results in VE could be applied to control harvesting robot operation and to correct the positioning errors in real-time. 5.2. Visual servo control Compared to open-loop visual control, the input of visual servo is continuous and contains dynamic image information. Therefore, frame rates of the video must match the closed-loop bandwidths of

Target Pose

+ -

Error Control law

Angle Velocity Acceleration

Motion Manipulator Video

Pose estimation

Visual processing Visual feedback

Fig. 6. The structure of visual servo (Pan et al., 2000).

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311­323

319

Camera in Hand

Tomato plant Robot controller

Robot

Fixed Camera Control signal Image signal

Field-of-view of CiH

Industry Computer

Global view from Fixed camera

Fig. 7. Schematic diagram of a cooperative vision system for harvesting robot.

vision controllers. As shown in Fig. 6, pose estimation is the core issue of visual feedback control. Input of the visual servo control are the time-varying position errors between the target fruit and end-effector. The advantages of visual servo are that its performance does not relay on the precise kinematic model of the robot and the calibration of the vision system. Mehta and Burks (2014) developed a visual servo system for robotic citrus harvesting. As shown in Fig. 7, a cooperative vision system consisting of a fixed camera and a camera-in-hand (CiH) was incorporated such that the fixed camera and provided a global view of a tree canopy. The CiH, due to its proximity, provided high resolution fruit image. In contrast to the open-loop visual control model the visual servo control strategy used perspective camera geometry to obtain absolute range estimates. The estimated range information can be used to generate a global map of fruit locations, and a rotation controller was developed to orient the robot endeffector towards the target fruit such that the fruit could enter the field of view of the CiH. The performance of the proposed visual servo controller was demonstrated using a 7 DOF robotic manipulator in an artificial environment. Van Henten et al. (2003a,b, 2010) introduced a novel eye-hand coordination approach based on the A/-search algorithm which could assured collision­free motions when the robot harvested cucumbers in a greenhouse. Eye-hand coordination based on the A/-search algorithm had assured an optimal motion path was obtained such that the cucumber picking time can be reduced. Zhao et al. (2011) developed an apple harvesting robot consisting of a manipulator, end-effector and image-based vision servo control system. In the visual control system, the image-based vision servo (IBVS) control method was employed for localization and picking motion for the target fruit. The IBVS was often used to control the manipulator according to image features. The key issue of this method is how to calculate the Jacobian matrix which describes the relationship between camera coordinate and robot coordinate (Harrell et al., 1985). Robot joint motion could be controlled based on feedback from

the position of a target fruit in an image. Vision servo was accomplished by controlling the velocities of each joint according to the vertical and horizontal offsets of a fruit's image position from the image center (Harrell et al., 1990). Moreover, the closed-loop bandwidths of vision controllers could be varied from 1.0 to 1.1 Hz. 6. Examples of fruit harvesting robots Robotics harvesting is not a new phenomenon but with the history of over 30 years. Currently, harvesting robots have not been advanced to the commercialization stage because of their low efficiencies, low intelligence, and high costs. On the other hand, different designs of fruit harvesting robots have emerged in recent years. Examples of major fruit harvesting robots are shown in Table 6. For convenience, the examples of harvesting robot are categorized according the types of target fruit or vegetable. The vision schemes and eye-hand coordination models of the harvesting robots are described in the table. The performances of different vision-based control approaches applied in various harvesting robot are also shown in the table. 7. Challenges and future trends 7.1. Enhancing the vision-based control of harvesting robot With the development of robotic technology and sensor technology, enhancing the performance of fruit harvesting robot can be a positive trend in meeting the challenges. Several suggestions are given to improve the ability of the harvesting robot to deal with the complex working environment (Bac et al., 2014). Firstly, improvements in sensing are required. The sensors currently used have certain shortcomings in the application to fruit recognizing (Gongal et al., 2015). Additionally, the combination of multi-sensor may satisfy the performance required for fruit

320

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311­323

Table 6 A comparison of the development of harvesting robots in the main countries. Products Fruits Robots Apple harvesting robot Vision scheme A camera-in-hand and positioning sensor A camera-in-hand and a laser range sensor Citrus harvesting robot Melon harvesting robot Strawberry harvesting robot Watermelon harvesting robot Kiwifruit harvesting robot Grape harvesting robot Cherry harvesting robot Vegetables Tomato harvesting robot A fixed camera and a camera-in-hand A fixed camera A far-vision CCD and a near-vision CCD A stereovision system and a central camera A stereo camera, a camera and a laser sensor A stereo vision sensor and a camera in hand A stereo vision sensor Eight cameras (four stereo vision systems) A camera-in-hand A red and infrared laser active sensor A binocular stereo vision sensor A stereo vision sensor A fixed camera and a camera-in-hand A near-infrared camera and a camera A camera-in-hand A monochrome camera in hand A camera-in-hand Two ToF cameras, a stereo camera and a camera Two infrared laser sensors A stereo vision camera Eye-hand coordination Open-loop visual control Open-loop visual control Visual servo control Visual servo control Visual servo control Visual servo control Visual servo control Visual servo control Open-loop visual control Open-loop visual control Visual servo control Open-loop visual control Open-loop visual control Visual servo control Visual servo control Open-loop visual control Visual servo control Visual servo control Visual servo control Open-loop visual control Open-loop visual control Open-loop visual control Success rate 77% >90% NR 50% >85% <41.3% NR 100% 66.7% NR NR 66.7% 70% 88.6% 80% 74.4% NR >80% 62.5% 79% NR 82.22% Speed 15s 7.1s <8s 36s <22s 11.5s 7s 12s NR NR NR 14s 3s-5s 37.2s 45s 65.2s <7s 6.7s 64.1s NR NR NR Reference Zhao et al. (2011) Bulanon and Kataoka (2010) Mehta et al. (2014) Harrell et al. (1990) Edan et al. (2000) Hayashi et al. (2010) Han et al. (2012) Sakai et al. (2007) Umeda et al. (1999) Scarfe et al. (2009) Kondo (1991) Tankgaki et al. (2008) Kondo et al. (1996) Ji et al. (2014) Van Henten et al. (2002) Van Henten et al. (2003a, b) Foglia and Reina (2006) Reed et al. (2001) Hayashi et al. (2001, 2002) Hemming et al. (2014) Sakai et al. (2013) Kohan et al. (2011)

Cucumber harvesting robot

Radicchio harvesting robot Mushroom Harvesting robot Eggplant harvesting robot Sweet-pepper harvesting robot Asparagus harvesting robot Rosa harvesting robot

detection and localization (Fernandez et al., 2013). Though given the large number of articles that described a number of fruit recognition algorithms, the development of advanced image processing algorithms is also a challenge for precision fruit recognition. Recently, more and more attentions have been attracted by new visual sensors such as ToF camera, light-field camera, and chlorophyll fluorescence camera. The core of the application considerations of these sensors is how to take advantage of the data acquired. On the other hand, vision control precision and efficiency for fruit harvesting robot need to improve. There are many recent works regarding eye-hand coordination for outdoor operation robot (Mariottini et al., 2006). The control law of calibration-free eye-hand coordination was described byHager et al. (1994). Base on the auto disturbance rejection control (ADRC) strategy, Su proposed an advanced calibration-free eye-hand coordination which has a strong adaptability and robustness (Su et al., 2004). Another new visual servo control approach based on adaptive neural network has been applied for dynamic positioning of underwater vehicles (Gao et al., 2015). 7.2. Human­machine collaboration To date, the commercial application of fruit harvesting robot is still unavailable because of lack of high efficiency and economic justification (Edan, 1999). One of the new approaches to improve the applicability of robotic harvesting is to combine human workers and robots synergistically. This approach for robotic harvesting is to separate the fruit recognition stage from the harvest stage by marking the target fruit a priori. In the Agribot project, a robot was designed and built for a new aided-harvesting strategy, involving a harmonic human­machine task distribution (Ceres et al., 1998). Ji

et al. (2014) introduced an assistant-mark approach to recognize and locate the picking-point of the harvesting robot. Bechar and Oren has defined and implemented the collaboration of a human operator and robot applied to target fruit detection (Bechar and Edan, 2003; Oren et al., 2012). Experimental results indicated that the target recognition system based on human-robot collaboration could increase the target fruit detection rate to 94% and reduce the time needed by 20%. 7.3. Multi-arms cooperating for robotic harvesting Another approach towards the goal of efficient robotic harvesting is the multi-arm robotic harvester. The idea is that a number of manipulators are mounted on the mobile robot platform, and each of the robotic arms is assigned a specific fruit to harvest. Zion from Israel has designed a multi-arm melons harvesting robot which enabled the maximum number of melons to be harvested (Zion et al., 2014). According to the idea of multi-robot cooperation for fruit harvesting, Noguchi et al. (2004) also proposed a master­ slave robot system for field operations. In this multiple robot system, a high level of autonomy on the robots was achieved to allow them to cope with unexpected events and obstacles. 7.4. Making the environment more suitable for robotic harvesting There are major technical challenges in automation due to the uncontrolled environment in combination with the fact that the harvest objects and materials are highly inconsistent in shape and size. The harvesting robot working in the complex natural environment requires a higher degree of skill and a wider range of operating. In order to improve the efficiency of robotic harvesting in the future, collaboration among engineers and agronomists

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311­323

321

is needed. Many engineers, working with agronomists, in the world now have developed advanced robots (Burks et al., 2005; He et al., 2013). Their solution for the problem of low harvest efficiency is making environment robot more user friendly. Trees or plants can be pruned to obtain suitable plant geometry for robotic harvesting and, hence, the picking cycle time can be reduced (Edan et al., 1990). 8. Conclusions In this paper, a broad overview of the development of vision control technology applied in fruit harvesting robot is given. Vision control for fruit harvesting robot includes two key elements, fruit recognition and eye-hand coordination. For recognizing the target fruit in the canopy, different types of visual sensors and image analysis algorithms are equipped in the fruit harvesting robots. Two-dimensional imaging data of the target fruit have been successfully acquired from monocular camera, hyperspectral imaging, and thermal imaging. Three-dimensional surface reconstruction of the target fruit requires data acquisition from binocular stereoscope or structured light sensor. There is a large difference in the image processing algorithm between 2D imaging schemes and 3D imaging schemes. The scheme of fruit recognition for robotic harvest may depend on the species of harvesting fruit. Moreover, the performance of the fruit recognition system is also influenced by many factors such as variable light, occlusions and many others. Therefore, the reliability of recognition methods must consider the environment in which the robot is working, and the proper selection of sensors. In addition to the techniques reviewed, the development of eyehand coordination control for fruit harvesting robot is also described in this paper. We have classified eye-hand coordination into two main categories according to whether the visual control is open-loop or close-loop. There is a large difference in the control principle between open-loop visual control mode and visual servo. The input image of open-loop visual control is a static image. However, the input of visual servo is the video image where frame rates of the video must match the closed-loop bandwidths of vision controllers. The performance of the open-loop visual control relies on the precision calibration of the camera and manipulators. The fruit harvesting robot adopting visual servo can arrive at certain control precision without calibration. With the advancement of current imaging technologies and the development of new control algorithms, more information will be available to aid recognizing the target fruit and speeding up fruit picking. Acknowledgments Financial support from the National High-Tech R&D Program of China (863 Program No. 2013AA102307) and the National Key Technology R&D Programs (2014BAD08B01 and 2015BAF12B02) are gratefully acknowledged. References
Arefi, A., Motlagh, A.M., Mollazade, K., Teimourlou, R.F., 2011. Recognition and localization of ripen tomato based on machine vision. Aust. J. Crop Sci. 5 (10), 1144­1149. Arefi, A., Motlagh, A.M., 2013. Development of an expert system based on wavelet transform and artificial neural networks for the ripe tomato harvesting robot. Aust. J. Crop Sci. 7 (5), 699­705. Baeten, J., Donne, K., Boedrij, S., Bechers, W., Claesen, E., 2008. Autonomous fruit picking machine: a robotic apple harvester. 6th International Conference on Field and Service Robotics, vol. 42, pp. 531­539. Bac, C.W., Heten, E.J.V., Hemming, J., 2014. Harvesting robots for high-value crops: state-of-the-art review and challenges ahead. J. Field Robot 31 (6), 888­991. Bechar, A., Edan, Y., 2003. Human-robot collaboration for improved target recognition of agricultural robots. Ind. Robot 30 (5), 432­436.

Buemi, F., Massa, M., Sandini, G., 1995. Agrobot: a robotic system for greenhouse operations. Robotics Agric. Food Ind. 4, 172­184. Buemi, F., Massa, M., Sandini, G., Costi, G., 1996. The Agrobot project. Adv. Space Res. 18, 185­189. Bulanon, D.M., Burks, T.F., Alchanatis, V., 2008. Study on temporal variation in citrus canopy using thermal imaging for citrus fruit detection. Biosyst. Eng. 101 (2), 161­171. Bulanon, D.M., Burks, T.F., Alchanatis, V., 2009. Image fusion of visible and thermal images for fruit detection. Biosyst. Eng. 103, 12­22. Bulanon, D.M., Kataoka, T., Okamoto, H., Hata, S., 2004. Development of a real-time machine vision system for apple harvesting robot. SICE Ann. Conf., 595­598 Bulanon, D.M., Kataoka, T., 2010. A fruit detection system and an end effector for robotic harvesting of Fuji apples. Agric. Eng. Int.: CIGR J. 2010 (7), 1­14. Burks, T., Villegas, F., Hannan, M., 2005. Engineering and horticultural aspects of robotic harvesting: opportunities and constraints. Horttechnology 15 (1), 79­ 87. Ceres, R., Pons, J.L., Jimenez, A.R., Martin, J.M., Calderon, L., 1998. Design and implementation of an aided fruit-harvesting robot. Ind. Robot 25 (5), 337­346. Chaivivatrakul, S., Dailey, M.N., 2014. Texture-based fruit detection. Precis. Agric. 15 (6), 662­683. Corke, P.I., Hager, G.D., 1998. Vision-based robot control. Control Problems Robotics Automat., 177­192 d'Esnon, G.A., Rabatel, G., Pellenc, R., Joumeau, A., Aldon, M.J., 1987. MAGALI: a selfpropelled robot to pick apples. In: Proceedings of American Society of Agricultural Engineers, Baltimore, Maryland. Edan, Y., Flash, T., Shmulevich, I., 1990. An algorithm defining the motions of a citrus picking robot. J. Agr. Eng. Res. 46, 259­273. Edan, Y., Gaines, E., 1994. Systems engineering of agricultural robot design. IEEE Trans. Syst. Man, Cybern. 24 (8), 1259­1265. Edan, Y., 1999. Food and agriculture robotic. Handbook of Industrial Robotic. Second edition. Chapter 60, 1143­1155. Edan, Y., Rogozin, D., Flash, T., 2000. Robotic melon harvesting. IEEE Trans. Rob. Autom. 16 (6), 831­834. Edan, Y., Han, S.F., Kondo, N., 2009. Automation in agriculture. Springer Handbook of Automation, Part G, 1095­1128. Feng, J., Zeng, L.H., Liu, G., 2014. Fruit recognition algorithm based on multi-source images fusion. Trans. CSAM 45 (2), 73­80. Fernandez, R., Salinas, C., Montes, H., Sarria, J., Armada, M., 2013. Validation of a multisensory system for fruit harvesting robots in lab conditions. First Iberian Robotics Conf. Adv. Intell. Syst. Comput. 252, 495­504. Foglia, M.M., Reina, G., 2006. Agricultural robot for radicchio harvesting. J. Field Robot 23, 363­377. Font, D., Palleja, T., Tresanchez, M., Rucan, D., Moreno, J., Martinez, D., Teixido, M., Palacin, J.T., 2014. A proposal for automatic fruit harvesting by combining a low cost stereovision camera and a robotic arm. Sensors 14, 11557­11579. Gao, J., Proctor, A., Bradley, C., 2015. Adaptive neural network visual servo control for dynamic positioning of underwater vehicles. Neurocomputing 167, 604­ 613. Gatica, G., Best, S., Ceroni, J., Lefranc, G., 2013. Olive fruits recognition using neural networks. Procedia Comput. Sci. 17, 412­419. Goncalves, P.J.S., Torres, M.B., 2010. Learning approaches to visual control of robotic manipulators. In: The Second International Conference on Advanced Cognitive Technologies and Applications, pp. 103­108, Lisbon, Portugal, 21-26 November. Gongal, A., Amatya, S., Karkee, M., 2015. Sensors and systems for fruit detection and localization: a review. Comput. Electr. Agr. 116, 8­19. Gotou, K., Fujiura, T., Nishiura, Y., 2003. 3-D vision system of tomato production robot. Int. Conf. Adv. Intell. Mech., 1210­1215 Grift, T., Zhang, Q., Kondo, N., Ting, K.C., 2008. A review of automation and robotics for the bio-industry. J. Biomech. Eng. 1 (1), 37­54. Hager, G.H., Chang, W.C., Morse, A.S., 1994. Robot feedback control based on stereo vision: towards calibration-free hand-eye coordination. IEEE Int. Conf. Robotic Automat., 2850­2856 Han, K.S., Kim, S.C., Lee, Y.B., Kim, S.C., Im, D.H., Choi, H.K., Hwang, H., 2012. Strawberry harvesting robot for bench-type cultivation. Biosyst. Eng. 37 (1), 65­ 74. Hannan, M.W., Burks, T.F., 2004. Current developments in automated citrus harvesting. In ASAE/CSAE Annual International Meeting. St. Joseph, MI. pp. 043827. Hannan, M.W., Burks, T.F., Bulanon, D.M., 2009. A machine vision algorithm combining adaptive segmentation and shape analysis for orange fruit detection. Agric. Eng. Int.: CIGR J. 6, 1­17. Harrell, R.C., Adsit, P.D., Slaughter, D.C., 1985. Real-time vision-servoing of a robotic tree fruit harvester. Trans. ASAE 85­3550, 1­15. Harrell, R.C., Adsit, P.D., Munilla, R.D., 1990. Robotic picking of citrus. Robotica 8, 269­278. Hashimoto, K., 2003. A review on vision-based control of robot manipulators. Adv. Robotics 17 (10), 969­991. Hayashi, S., Ganno, K., Ishii, Y., 2001. Development of a harvesting end-effector for eggplants. SHITA 13 (2), 97­103. Hayashi, S., Ganno, K., Ishii, Y., Tanaka, I., 2002. Robotic harvesting system for Eggplants. JARQ 36 (3), 163­168. Hayashi, S., Ota, T., Kubota, K., Ganno, K., Kondo, N., 2005. Robotic harvesting technology for fruit vegetables in protected horticultural production. In: Information and Technology for Sustainable Fruit and Vegetable Production, pp. 227­236, 12-16 September.

322

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311­323 Rakun, J., Stajnko, D., Zazula, D., 2011. Detecting fruits in natural scenes by using spatial-frequency based texture analysis and multiview geometry. Comput. Electr. Agric. 76 (1), 80­88. Reed, J.N., Miles, S.J., Butler, J., 2001. Automatic mushroom harvester development. J. Agric. Eng. Res. 78 (1), 15­23. Reis, M.J.C.S., Morais, R., Peres, E., Pereira, C., Contente, O., Soares, S., Valente, A., Baptista, J., Ferreira, P.J.S.G., Cruz, J.B., 2012. Automatic detection of bunches of grapes in natural environment from color images. J. Appl. Logic. 10, 285­290. Safren, Q., Alchanatis, V., Ostrovsky, V., Levi, O., 2007. Detection of green apples in hyperspectral images of apple-tree foliage using machine vision. Trans. ASABE 50 (6), 2303­2313. Sakai, H., Shiigi, T., Kondo, N., Ogawa, Y., 2013. Accurate Position Detecting during asparagus spear harvesting using a laser sensor. EAEF 6 (3), 105­110. Sakai, S., Osuka, K., Maekwaw, T., Umeda, M., 2007. Robust control systems of a heavy material handling agricultural robot: a case study for initial cost problem. IEEE Trans. Control Syst. Tchnol. 15 (6), 1038­1048. Scarfe, A.J., Flemmer, R.C., Bakker, H.H., Flemmer, C.L., 2009. Development of an autonomous kiwifruit picking robot. In: The 4th International Conference on Autonomous Robots and Agents, pp. 380­384, Wellington, New Zealand. Schertz, C.E., Brown, G.K., 1968. Basic considerations in mechanizing citrus harvest. Trans. ASAE, 343­346. Shen, H.L., Zhao, D., Ji, W., 2011. Research on the strategy of advantage of advancing harvest efficiency of fruit harvest robot in the oscillation conditions. Third Int. Conf. Intell. Human-Mach. Syst. Cybernet. 215­218. Shinsuke, K., Koichi, O., 2005. Recognition and cutting system of sweet pepper picking robot in greenhouse horticulture. In: Proceedings of the IEEE International Conference on Mechatronics & Automation, pp. 1807­1812, Nigara Falls, Ontario, Canada, 29 July-1 August. Shirai, Y., Inoue, H., 1973. Guiding a robot by visual feedback in assembling tasks. Patt. Recogn. 5, 99­108. Si, Y.S., Liu, G., Feng, J., 2015. Location of apples in trees using stereoscopic vision. Comput. Electr. Agric. 112 (3), 68­74. Sistler, F.E., 1987. Robotics and intelligent machines in agriculture. IEEE J. Robotic Autom. 3 (1), 3­6. Sites, P.W., Delwiche, M.J., 1988. Computer vision to locate fruit on a tree. Trans. ASAE 31 (1), 257­263. Slaughter, D.C., Harrell, R.C., 1987. Color vision in robotic fruit harvesting. Trans. ASAE 30 (4), 1144­1148. Slaughter, D.C., Harrell, R.C., 1989. Discriminating fruit for robotic harvest using color in natural outdoor scenes. Trans. ASAE 32 (2), 757­763. Song, Y., Glasbey, C.A., Horgan, G.W., 2014. Automatic fruit recognition and counting from multiple images. Biosyst. Eng. 18, 203­215. Stajnko, D., Lakota, M., Hocevar, M., 2004. Estimation of number and diameter of apple fruits in an orchard during the growing season by thermal imaging. Comput. Electron. Agric. 42 (1), 31­42. Su, J.B., Qiu, W.B., Ma, H.Y., 2004. Calibration-free robotic eye-hand coordination based on an auto disturbance-rejection controller. IEEE Trans. Robot. 20 (5), 889­907. Sun, J., Lu, B., Mao, H.P., 2011. Fruits recognition in complex background using binocular stereovision. J. Jiangsu Univ. Nat. Sci. Ed. 32 (4), 423­427. Tankgaki, K., Fujiura, T., Akase, A., 2008. Cherry-harvesting robot. Comput. Electr. Agric. 63, 65­72. Umeda, M., Kubota, S., Iida, M., 1999. Development of ``STORK", a watermelonharvesting robot. Artif. Life Robot. 3, 143­147. Vadivambal, R., Jayas, D.S., 2001. Applications of thermal imaging in Agriculture and food industry-a review. Food Bioprocess Tech. 4, 186­199. Van Henten, E.J., Hemming, J., Tuijl, B.A.J.V., 2002. An autonomous robot for harvesting cucumbers in greenhouses. Auton. Robot. 13 (3), 241­258. Van Henten, E.J., Hemming, J., Tuijl, B.A.J.V., 2003a. Collision-free motion planning for a cucumber picking robot. Biosyst. Eng. 86 (2), 135­144. Van Henten, E.J., Tuijl, B.A.J.V., Hemming, J., 2003b. Field test of an autonomous cucumber picking robot. Biosyst. Eng. 86 (3), 305­313. Van Henten, E.J., Schenk, E.J.L., Willigenburg, G.V., 2010. Collision-free inverse kinematics of the redundant seven-link manipulator used in a cucumber picking robot. Biosyst. Eng. 106, 112­124. Wachs, J.P., Stern, H.I., Burks, T., 2009. Apple detection in natural tree canopies from multimodal image. In: 6th European Conference on Precision Agriculture, pp. 293­302. Wachs, J.P., Stern, H.I., Burks, T., 2010. Low and high-level visual feature-based apple detection from multi-modal images. Precis. Agric. 11, 717­735. Wang, F.C., Xu, Y., Song, H.B., 2015. Study on identification of tomatoes based on fuzzy clustering algorithm. J. Agric. Mech. Res. 10, 24­28. Wei, X.Q., Kun, J., Jin, H.L., 2014. Automatic method of fruit object extraction under complex agricultural background for vision system of fruit picking robot. Optics 125 (12), 5684­5689. Whittaker, A.D., Miles, G.E., Mitchell, O.R., 1987. Fruit location in a partially occluded image. Trans. ASAE 30 (3), 591­596. Xiang, R., Jiang, H.Y., Ying, Y.B., 2014. Recognition of clustered tomatoes based on binocular stereo vision. Comput. Electr. Agric. 106 (8), 75­90. Xie, Z.Y., Zhang, T.Z., Zhao, J.Y., 2007. Ripened strawberry recognition based on Hough transform. Trans. CSAM 38 (3), 106­109. Xie, Z.H., Ji, C.Y., Guo, X.Q., 2010. An object detection method for quasi-circular fruits based on improved Hough transform. Trans. CSAE 26 (7), 157­162. Xie, Z.H., Ji, C.Y., Guo, X.Q., 2011. Detection and location algorithm for overlapped fruits based on concave spots searching. Trans. CSAE 42 (12), 191­196.

Hayashi, S., Shigematsu, K., Yamamoto, S., Kobayashi, K., Kohno, Y., Kamata, J., Kurita, M., 2010. Evaluation of a strawberry-harvesting robot in a field test. Biosyst. Eng. 105, 160­171. He, L., Zhang, Q., Charvet, H.J., 2013. A kont-tying end-effector for robotic hop twining. Biosyst. Eng. 114, 334­350. Hemming, J., Bac, C.W., Tuijl, B.A.J.V., 2014. A robot for harvesting sweet-pepper in greenhouses. In: 2014 International Conference of Agricultural Engineering, pp. 1­8, Zurich, Switzerland. Huang, L.W., He, D.J., 2012. Ripe Fuji apple detection model analysis in natural tree canopy. TELKOMNIKA 10 (7), 1771­1778. Inoue, S., Ojika, T., Harayama, M., Kobayashi, T., Imai, T., 1996. Cooperated operation of plural hand-robots for automatic harvest system. Math. Comput. Simulat. 41, 357­365. Ji, C., Zhang, J., Yuan, T., Li, W., 2014. Research on key technology of truss tomato harvesting robot in greenhouse. AMM 442, 480­486. Ji, W., Zhao, D., Cheng, F.Y., 2012. Automatic recognition vision system guided for apple harvesting robot. Comput. Electr. Agric. 38, 1186­1195. Jimenez, A.R., Ceres, R., Pons, J.L., 2000a. A machine vision system based on a laser rang-finder applied to robotic fruit harvesting. Mach. Vision Appl. 11, 321­329. Jimenez, A.R., Ceres, R., Pons, J.L., 2000b. A survey of computer vision methods for locating fruit on trees. Trans. ASAE 43 (6), 1911­1920. Kane, K.E., Lee, W.S., 2007. Multispectral imaging for in-field green citrus identification. In: ASABE Annual International Meeting, St. Joseph, MI. Paper Number: 073025. Kelman, E., Linker, R., 2014. Vision-based localization of mature apples in tree images using convexity. Biosyst. Eng. 2014 (118), 174­185. Khadraoui, D., Debain, C., Rouveure, R., Martinet, P., Bonton, P., Gallice, J., 1998. Vision-based control in driving assistance of agricultural vehicles. Int. J. Rob. Res. 17 (10), 1040­1054. Kohan, A., Borghaee, A.M., Yazdi, M., Minaei, S., Sheykhdavudi, M.J., 2011. Robotic harvesting of rosa damascene using stereoscopic machine vision. WASJ 12 (2), 231­237. Kondo, N., 1991. Study on grape harvesting robot. Mathematical and Control Applications in Agriculture and Horticulture, a volume in IFAC Workshop Series, pp. 243­246. Kondo, N., Nishitsuji, Y., Ling, P.P., 1996. Visual feedback guided robotic cherry tomato harvesting. Trans. ASAE 39 (6), 2331­2338. Kurtulmus, F., Lee, W.S., Vardar, A., 2011a. Green citrus detection using `eigenfruit', color and circular Gabor texture features under natural outdoor conditions. Comput. Electr. Agr. 78, 140­149. Kurtulmus, F., Lee, W.S., Vardar, A., 2011b. An advanced green citrus detection algorithm using color images and neural networks. J. Agric. Mach. Sci. 7 (2), 145­151. Kurtulmus, F., Lee, W.S., Vardar, A., 2014. Immature peach detection in colour images acquired in natural illumination conditions using statistic classifiers and neural network. Precis. Agric. 15, 57­79. Li, L., Zhang, Q., Huang, D.F., 2014. A review of imaging techniques for plant phenotyping. Sensors 14, 20078­20111. Li, P.L., Lee, S.H., Hsu, H.Y., 2011a. Study on citrus fruit image data separability by segmentation methods. Procedia Eng. 23, 408­416. Li, P.L., Lee, S.H., Hsu, H.Y., 2011b. Review on fruit harvesting method for potential use of automatic fruit harvesting systems. Procedia Eng. 23, 351­366. Liu, J.Z., Li, P.P., Li, Z.G., 2007. A multi-sensory end-effector for spherical fruit harvesting robot. Int. Conf. Automat. Logist., 258­262 Lu, J., Sang, N., Hu, Y., 2014. Detecting citrus fruits with highlight on tree based on fusion of multi-map. Optics 125, 1903­1907. Ma, X.D., Liu, G., Zhou, W., 2013. Apple recognition based fuzzy neural network and quantum genetic algorithm. Trans. CSAM 44 (12), 227­232. Manolakis, D., Marden, D., Shaw, G.A., 2003. Hyperspectral image processing for automatic target detection applications. Lincoln Lab. J. 14 (1), 79­116. Mao, W.H., Ji, B.P., Zhan, J.C., Zhang, X.C., Hu, X.A., 2009. Apple location method for the apple harvesting robot. In: 2nd International Congress on Image and Signal Processing, pp. 17­19. Mariottini, G.L., Prattichizzo, D., Oriolo, G., 2006. Image-based visual servoing for nonholonomic mobile robots with central catadioptric camera. Int. Conf. Robotics Automat., 538­544 Mehta, S.S., Burks, T.F., 2014. Vision-based control of robotic manipulator for citrus harvesting. Comput. Electr. Agric. 102, 146­158. Mehta, S.S., MacKunis, W., Burks, T.F., 2014. Nonlinear robust visual servo control for robotic citrus harvesting. The 19th world congress of the International Federation of Automatic. Control, 8110­8115. Noguchi, N., Will, J., Reid, J., Zhang, Q., 2004. Development of a master-slave robot system for farm operations. Comput. Electr. Agr. 44, 1­19. Okamoto, H., Lee, W.S., 2009. Green citrus detection using hyperspectral imaging. Comput. Electr. Agr. 66, 201­208. Oren, Y., Bechar, A., Edan, Y., 2012. Performance analysis of a human-robot collaborative target recognition system. Robotica 30, 813­826. Pan, Q.L., Su, J.B., Xi, Y.G., 2000. Uncalibrated 3D robotic visual tracking based on stereo vision. ROBOT 22 (4), 293­299. Parrish, E.A., Goksel, J.A.K., 1977. Pictorial pattern recognition applied to fruit harvesting. Trans. ASAE 20 (5), 822­827. Pastrana, J.C., Rath, T., 2013. Novel image processing approach for solving the overlapping problem in agriculture. Biosyst. Eng. 115, 106­115. Patel, H.N., Jain, R.K., Joshi, M.V., 2011. Fruit Detection using improved Multiple Features based Algorithm. IJCA 13 (2), 1­5.

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311­323 Xiong, J.T., Zou, X.J., Wang, H.J., 2013. Recognition of ripe litchi in different illumination conditions based on Retinex image enhancement. Trans. CSAE 29 (12), 170­178. Xu, H.R., Ying, Y.B., 2004. Detection citrus in a tree canopy using infrared thermal imaging. SPIE 5271, 321­327. Yamamoto, K., Guo, W., Yoshioka, Y., Ninomiya, S., 2014. On plant detection of intact tomato fruits using image analysis and machine learning methods. Sensors 14, 12191­12206. Yang, L., Dickinson, J., Wu, Q.M.J., 2007. A fruit recognition method for automatic harvesting. In: 14th International Conference on Mechatronics and Machine Vision in Practice, pp. 152­157. Yau, W.Y., Wang, H., 1996. Robust hand-eye coordination. Adv. Robotic 11 (1), 57­73. Yin, H.P., Chai, Y., Yang, S.X., 2009a. Ripe tomato recognition and localization for a tomato harvesting robotic system. Int. Conf. Soft Comput. Patter Recogn. 557­562. Yin, H.P., Chai, Y., Yang, S.X., 2009b. Ripe tomato extraction for a harvesting robotic system. In: Proceedings of IEEE International Conference on System, Man, and Cybernetics, pp. 2984­2989. Yuan, T., Ji, C., Chen, Y., Li, W., Zhang, J.X., 2011. Greenhouse cucumber recognition based on spectral imaging technology. Trans. CSAM S1, 172­176. Zhan, W.T., He, D.J., Shi, S.L., 2013. Recognition of kiwifruit in field based on adaboost algorithm. Trans. CSAE 29 (23), 140­146.

323

Zhang, B.H., Huang, W.Q., Li, J.B., 2014. Principles, developments and applications of computer vision for external quality inspection of fruits and vegetables: a review. Food Res. Int. 62, 326­343. Zhang, B.H., Huang, W.Q., Wang, C.P., 2015. Computer vision recognition of stem and calyx in apples using near-infrared linear-array structured light and 3D reconstruction. Biosyst. Eng. 139 (12), 25­34. Zhao, D.A., Lv, J.D., Ji, W., 2011. Design and control of an apple harvesting robot. Biosyst. Eng. 110 (2), 112­122. Zhao, J., Tow, J., Katupitiya, J., 2005. On-tree fruit recognition using texture properties and color data. In: IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 263­268. Aug. Zhao, Y.S., Gong, L., Huang, Y.X., Liu, C.L., 2016a. Robust tomato recognition for robotic harvesting using feature images fusion. Sensors 16 (2), 173­185. Zhao, Y.S., Gong, L., Zhou, B., Huang, Y.X., Liu, C.L., 2016b. Detecting tomatoes in greenhouse scenes by combining AdaBoost classifier and colour analysis. Biosyst. Eng. 148 (8), 127­137. Zion, B., Mann, M., Levin, D., Shilo, A., Rubinstein, D., Shmulevich, I., 2014. Harvested-order planning for a multiarm robotic harvester. Comput. Electr. Agric. 103, 75­81. Zou, X.J., Zou, H.X., Lu, J., 2012. Virtual manipulator-based binocular stereo vision positioning system and errors modelling. Mach. Vision Appl. 23, 43­63.

View publication stats

