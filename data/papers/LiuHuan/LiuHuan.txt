Received July 27, 2016, accepted August 10, 2016, date of publication September 7, 2016, date of current version November 18, 2016.
Digital Object Identifier 10.1109/ACCESS.2016.2605759

Interactive Reference Region Based
Multi-Objective Evolutionary Algorithm
Through Decomposition
RUOCHEN LIU, (Member, IEEE), RUINAN WANG, WEN FENG, JUNJUN HUANG,
AND LICHENG JIAO, (Senior Member, IEEE)
Key Laboratory of Intelligent Perception and Image Understanding Key of Ministry of Education, International Research Center for Intelligent Perception and
Computation, Xidian University, Xi’an 710071, China

Corresponding author: R. Liu (ruochenliu@xidian.edu.cn)
This work was supported in part by the National Natural Science Foundation of China under Grant 61373111, Grant 61272279,
Grant 61672405, and Grant 61203303, in part by the Fundamental Research Funds for the Central Universities under
Grant K50511020014, Grant K5051302084, Grant K50510020011, Grant JB150227 and JBG160229,
and in part by the Provincial Natural Science Foundation of Shaanxi of China under Grant 2014JM8321.

ABSTRACT Many evolutionary multi-objective optimization (EMOs) methodologies have been proposed
and shown a great potential in approximating the entire Pareto front. While in real world, what decision
makers (DMs) want is one or several solutions to satisfy their requirements. It has become a hot problem
that dynamically using preference information provided by DMs during the optimization process guides
the search of EMO algorithms. An interactive reference region-based evolutionary algorithm through
decomposition is proposed, denoted as RR-MOEA/D in this paper, which focuses the search on the desire of
DMs to save computational resources. MOEA/D, as a well-known multi-objective optimization algorithm,
is used as a basic framework here. In MOEA/D, by dealing with the sub-problems in the preference region
and ignoring uninterested ones, the solutions obtained can converge to the regions which the DM prefers
on the Pareto front and the computational complexity can be saved to a great extent. At each interaction, a
humanized and simple interactive condition is adopted so that the reference region can be changed in a very
intuitive way if the DM is unsatisfied the results in the interactive process. A rapid interaction is designed
and a set of rough solutions can be obtained quickly whenever the preference information is changed. The
proposed algorithm is tested on several benchmark problems and the experimental results show that the
proposed algorithm can take full use of preference information and successfully converge to the reference
region due to its reasonable and simple interaction mechanism.
INDEX TERMS Interactive preference, multi-objective optimization problem, reference region selection
mechanism.

I. INTRODUCTION

Evolutionary multi-objective optimization algorithms (EMOs)
have been widely applied in many engineering areas. Multiple objectives with conflicts make it impossible to get a
single optimal solution which can optimize all the objectives
at the same time. As a result, the optimal solutions obtained
by EMOs are trade-offs, known as Pareto optimal solutions,
which are characterized by that some improvement in one
objective must lead to deterioration in at least one other
objective.
Without incorporating preference information, an approximation to the entire Pareto front (PF) is obtained by most

VOLUME 4, 2016

EMOs. However, in many practical applications, what a
decision maker (DM) requires may be a single solution or
a set of solutions in a preferred region instead of entire
PF. It is very time-consuming and unnecessary to obtain
the whole PF which DMs are not interested in. Meanwhile,
many preference information based EMOs have been proposed to select the most desirable solutions on the Paretooptimal front, and those can be mainly divided into three categories: prior, interactive and posterior. The apriori approaches
require the DM provides his/her preference information in
advance. Often this preference information is used as a variant of selection mechanism to substitute or compose the

2169-3536 
 2016 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

7331

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

tradition one. A disadvantage of these apriori approaches lies
in that it is very difficult and confused for a DM to express
his/her preference without related knowledge of available
solutions or structure of the search space. On the other
hand, for those posterior approaches, the DMs can choose
those preferred solutions on the basis of the convergence
and spread of solutions. Disadvantages and criticisms for the
posterior methods include high computational costs and time
consumption, and it will become more difficult when dealing with many-objective problems (more than three objectives). The interactive, as one of the preference methods,
is a new trend. It can be described as a DM dynamically
guides the searching process by providing their reference
information in an interactive way until he/she is satisfied with
the outcome. The interactive methods allow DMs to guide
searching and modify their preferences at various stages
during the optimization process. Moreover, ignoring those
regions with little interest and applying reference information as guidance can effectively reduce the computational
costs.
Multi-objective evolutionary algorithm based on decomposition (MOEA/D) [1] is selected as a basic MOEA framework in this paper. MOEA/D decomposes a multi-objective
problem (MOP) into multiple single-objective subproblems
in which each subproblem is a different aggregation of all
the objectives. From this viewpoint, the aggregation can
be described as different preference of objectives, in other
words, different subproblems are generated under different preference condition. Therefore, it is a convenient way
to find the regions where the DM is interested by solving the corresponding subproblems according to the DM’s
preferences.
In this paper, we propose an interactive reference region
based multi-objective evolutionary algorithm through decomposition (denoted as RR-MOEA/D), which focus the search
on the desire of DMs to save computational resources. Once
the preference is given by a DM at interactive stage, the proposed algorithm reduces the computational cost by ignoring
the regions the DM is not interested in and only optimizing the
subproblems the DM prefers. Whereas during optimization
process, weight vectors of subproblems preferred is adapted
dynamically to avoid the solutions we obtain flying away
from the preferred regions. At each interaction, our algorithm
provides a set of current solutions to the DM and the reference
region can be changed in a very intuitive way if the DM
is unsatisfied. Therefore, a rapid interaction is designed in
this paper. A set of rough solutions can be obtained quickly
whenever the preference information change.
The major advantages of the proposed RR-MOEA/D algorithm are enumerated as follows:
1) Reduce the amount of calculation and save the computing resources by focusing the search on the preferred regions;
2) It needs not to calculate any achievement scalarizing
functions (ASF) [2] used in other preference mechanisms;
3) A simple and intuitive way to set the reference information the DM prefers;
7332

4) A fast way to get rough solutions in the reference regions
can avoid wasting time in searching Pareto-optimal solutions
in regions the DM is not preferred;
5) Giving uncertain and inconsistent human decisions as
the DM does in real life are allowed and changing the preference information in the interaction stage is permitted.
The remainder of this paper is structured as follows.
Section 2 introduces the related background. The proposed
algorithm is presented in Section 3 in details. Section 4
presents experimental results and validates the performance
of the proposed algorithm. Section 5 concludes the paper.
II. RELATED BACKGROUND
A. MULTI-OBJECTIVE OPTIMIZATION

Generally, a minimized multi-objective
problem (MOP) can be defined as follows:

optimization

min F(x) = (f1 (x), f2 (x), . . . , fm (x))T
s.t. x ∈ 

(1)

where x = (x1 , x2 , . . . , xn ) is the decision vector, n is the
dimensionality of the decision space, and  is the feasible
region in the decision space. F(x) is a set of objective function
consisting of m objective functions.
Very often, since the objectives in Eq. (1) conflict each
other, it is impossible to get a single optimal solution which
optimizes all the objectives at the same time. As a result, the
optimal solutions we obtained are trade-offs, known as Pareto
optimal solutions. Some important related terms are stated as
follows:
Definition 1 (Pareto-Dominance): Let xa , xb ∈ , xa is
said to Pareto dominate xb , denoted as xa  xb if and only if:
(
∀i = 1, 2, . . . m, fi (xa ) ≤ fi (xb )
(2)
∃j = 1, 2, . . . m, fj (xa ) < fj (xb )
Definition 2 (Pareto-Optimal): A point is Pareto optimal to
Eq. (1) if:
¬∃x ∈  : x  x∗

(3)

The Pareto-optimal set (PS) consists of all the Paretooptimal solutions and the set of all the Pareto optimal objective vectors is called Pareto front (PF) [3].
B. MULTI-OBJECTIVE OPTIMIZATION BASED
ON DECOMPOSITION

Decomposition is a basic strategy in traditional multiobjective optimization. Relying on a decomposition strategy such as the weighted-sum method [3], the Tchebycheff
approach [3] or the boundary intersection approach [4], [5],
a multi-objective problem can be converted into a number of single-objective problems. In 2007, Zhang et al. [1]
proposed a decomposition algorithm based evolutionary
multi-objective algorithm using the Tchebycheff as the
decomposition strategy. Since we use the Tchebycheff
approach in our study, a brief introduction of the Tchebycheff
approach is given in the following.
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

The Tchebycheff method plays an important role not only
in converting a multi-objective problem into a number of
single-objective problems [3] but also in locally approximating the underlying preference utility function [6]. Each subproblem is obtained by using the following equation [1], [3]:
	

 
minimize gte (x λ, z∗ ) = max λi fi (x) − z∗ 
1≤i≤m

subject to x ∈ 

(4)

Where, for i ∈ {1, 2, . . . , m}, z∗i is the ideal point, i.e.,
z∗i = min {fi (x) |x ∈  }. For any weight vector λ, the optimal
solutions of Eq. (4) corresponds to a Pareto-optimal solution
of Eq. (1). All the optimal solutions obtained can approximate
to the entire PF with proper weight vectors.
As shown in Eq. (4), each subproblem can be translated
as a different aggregation of all the objectives and different
aggregation could be constructed as the different preference
for objectives. By setting proper weights, solutions desired by
DMs can be obtained by solving those related subproblems
in the preference regions. Therefore, associating preference
method with decomposition is a very natural thought. As the
most common decomposition based multi-objective evolutionary algorithms, MOEA/D [1] has shown great potential.
Here is a brief introduction of MOEA/D and its variants.
MOEA/D uses the Tchebycheff method as a decomposition method and converts a multi-objective problem into a set
of subproblems and optimizes all the subproblems simultaneously in a single run. Different subproblems in MOEA/D have
different weight vectors. For each weight vector, a neighborhood is defined as a set of its several closest weight vectors.
Offspring of each subproblem is generated by using some
genetic operators and its parents are randomly selected from
its neighborhood. The solution of each subproblem is the best
solution found so far in the neighborhood. That is to say, each
subproblem optimized by its neighboring subproblems only
obtain its optimal solution in evolving process, which makes
that MOEA/D has such advantages as low computational
complexity and fast convergence rate. MOEA/D-DE [7] is
an improved version of MOEA/D, which has three improvements: 1) using a differential evolution (DE) [8] operator
instead of SBX operator [9] in updating process; 2) setting a
probability that parent solutions are selected from the neighborhood; 3) setting a upper limit number of solutions in
neighborhood replaced by each child solution. All these three
improvements aim at maintaining the population diversity.
Since our algorithm works only in reference regions and
has a small population size, keeping diversity is an important task. So, the proposed algorithm is mainly based on
MOEA/D-DE in fact. As another new version of MOEA/D,
MOEA/D-M2M [10] converts a multi-objective problem into
several simple multi-objective problems. For each subproblem, any Pareto based EMOs can be used. MOEA/DD [11]
suggests a hybrid paradigm which combines decomposition
and dominance together. Most recently, an inverse model
based MOEA (IM-MOEA) [12] is proposed, which uses a
set of reference vectors to partition the objective space into a
VOLUME 4, 2016

number of subspaces and then the inverse models which maps
the objective vectors into the decision vectors are built in
each subspace to sample new solutions. More recently, a reference vector guide evolution algorithm for many-objective
optimization (REVA) [13] is proposed, in which reference
vectors is used to decompose the original multi-objective
optimization problem into a number of single-objective subproblems. Additionally, a scalarization approach, termed the
angle penalized distance, is adopted to balance convergence
and diversity of the solutions in the high-dimensional objective space. NSGA-III [14] combines a decomposition strategy
in MOEA/D [1] with non-dominated sorting approach from
NSGA-II [15], and has been able to successfully find a wellconverged and well-diversified set of solutions for manyobjective problems.
C. PREFERENCE INFORMATION BASED EMO

The goal of evolutionary search is to help that the DM
selects the solution which best matches his/her preferences
within a representative set of Pareto optimal solutions. Various preference-based EMOs have been proposed in the last
decade. Two surveys were provided in this topic [16], [17].
EMOs are categorized apriori, interactive, or aposteriori algorithms according to the treatment of preference. An apriori
approach requires the preference information provided by
the DM before the optimization stage and the evolutionary
search is guided by the preference information. On the other
hand, a posteriori approach firstly needs to approximate to
the whole Pareto front, and then the DM selects their preferred solutions. With the character of the user involving in
the optimization process, the interactive approaches offer a
chance for the user to know about the structure of the search
space and express his /her authentic preference correctly,
and computational costs is reduced largely by progressively
focusing on the most relevant areas and ignoring regions of
little interest. We will give a simple review about the three
approaches in the following section.
Some typical apriori methods are listed here:
Wierzbicki [18] proposed a reference point based optimization of classical multi-criterion decision making algorithm
which combines three approaches in a single multi-objective
optimization framework and enables the DMs to refine the
problem definition and to reduce the size of the objective
space iteratively. Jaszkiewicz and Slowinski [19] proposed
a light beam search procedure which connects the reference point with tools of multi-attribute decision analysis.
This algorithm tries to overcome the inconvenience when
the DM chooses the candidates for the best compromise.
Deb et al. [20] hybridized the reference direction strategy
with NSGA-II, in which, a reference direction can be determined by setting a starting point and a reference point,
and reference solutions along the reference direction on PF
can be obtained. Deb and Kumar [21] combined a reference
point based reference information with NSGA-II in order
to find a set of solutions instead of a single optimal solution and used a modified crowding distance to control the
7333

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

density of solutions near reference points. Thiele et al. [22]
proposed a preference based evolutionary approach which
directly uses reference point information in fitness evaluation.
Recently, Molina et al. [23] proposed another definition of
Pareto dominance called g-dominance, in which, solutions
satisfy all aspiration levels and solutions fulfilling none of the
aspiration levels are preferred over those solutions satisfying
some aspiration levels. All these algorithms mentioned above
belong to the apriori method.
For the posteriori approach, in fact, many famous EMOs
like MOEA/D [1], NSGA-II [15], SPEA-II [24], NNIA [25],
and so on belong to it. After those algorithms obtain a
set of compromised solutions which are closest to the true
Pareto front and the DMs pick out the solutions they are
interested in.
An interactive process aims to integrate the preference
information provided by DMs into algorithm in an interactive
way during the optimizing process. That is to say, some tradeoff factors expressing the desires of the DMs are added into
the optimization process to guide the latter search. According
to [26], Tanino et al. [27] suggested the earliest attempt to
incorporate the preference information expressed iteratively
by the DM to evaluate solutions. Fonseca and Fleming [28]
allowed the user to specify aspiration levels in form of a
reference point, and used it to modify the EMO’s rank scheme
in order to focus the search. Kita et al. [29] interleaved generations of a Pareto ranking based evolutionary algorithm
with the rank of the solutions provided by a DM. Todd and
Sen [30] also tried to learn the user’s utility function, instead
of only considering linear weightings of objectives, they
used the preference information provided by the DM to train
an artificial neural network, which is then used to evaluate
solutions in the evolutionary algorithm. Parmee et al. [31]
described the concept of an interactive evolutionary design
system which is related to multi-objective information gathering and subsequent design space redefinition. Instead of
using linear programming to derive a weighting of the objectives most compatible with the pairwise comparisons as in
[32], Phelps and Kö ksalan [33] used two evolutionary algorithms, one is to find the solutions, and another is to determine the most compatible ranking. Kamalian et al. [34] suggested using a posteriori evolutionary multi-objective optimization followed by an interactive evolutionary algorithm.
Reference [35] allowed the user to modify the Pareto ranking computed automatically by changing the rank of some
of the solutions. Thiele et al. [36] also used DM’s preferences interactively expressed in the form of reference points.
Avigad and Moshaiov [37] presented a new interactive concept based multi-objective evolutionary algorithm to handle
Pareto-directed IC-MOPs. Said et al. [38] described an interactive run of r-NSGA-II algorithm in which is changed for
a certain number of generations freely determined by the
DM, and if the running result the DMs are dissatisfied, they
can change the reference point. Deb et al. [39] presented a
progressively interactive EMO approach which applied the
constructed value function to search more preferred solutions.
7334

And an interactive evolutionary algorithm can contribute
to designing elegant object-oriented software which was
discussed by Simons and Parmee [40]. Sun et al. [41] proposed a new surrogate-assisted interactive genetic algorithm
to exploit the uncertainty of subjective fitness evaluations
both in training the surrogates and in managing surrogates.
Ruiz et al. [42] developed an interactive WASF-GA which
is based on a preference-based evolutionary multiobjective
optimization algorithm called WASF-GA [43], it required
the DM to provide preference information at each iteration
simply as a reference point and then the desired number of
solutions is generated to represent the region of interest of the
Pareto optimal front associated to the reference point given.
Among the interactive algorithms, those algorithms based
on decomposition strategy are very few. We just found one
references on this topic. iMOEA/D [44] is an interactive
version of the MOEA/D algorithm [1], in which the preferred
solutions are presented to the decision maker at intermediate generations, then, the searching process is guided to
the neighbor regions of the preferred solution. It is obvious
that both iMOEA/D and the proposed algorithm are based
on MOEA/D although there are some little differences like
initiation and updating of population. The biggest differences
between the two algorithms lie in the way of determining preferred region and interactive method. In iMOEA/D,
a generic polynomial utility function [45] is used to represent
the human DM. During the stage of interactive, P current
solutions are randomly presented to DM for choosing their
favorite one; and the selected solution becomes the center
of preferred weight region in the following optimization process. The detailed descriptions of about the way of determining preferred region and interactive method in the proposed
algorithm is presented in Section 3.
III. THE PROPOSED APPROACH

In this section, the proposed reference region based multiobjective evolutionary algorithm through decomposition, is
presented in details. If the final aim is to choose and implement a solution, then the goal of applying a multi-objective
optimization method is to find a single, most preferred, final
solution. However, in some cases, it may be preferable to find
a set of solutions instead of one. This may be particularly true
in case of robustness considerations when some aspects of
uncertainty, imprecision or inconsistency in data or model are
to be taken into account [26]. For most of preference based
multi-objectives evolution algorithms, the preference information is treated as a variant of special selection mechanism
to substitute or compose the traditional selection process [46],
and the whole population converges close to the reference
point or reference direction [44]. MOEA/D-M2M [10] converts a multi-objective problem into several simple multiobjective problems based on PF or PS shapes and searches
and optimizes each simple multi-objective problem respectively. Inspired by all these, we consider that a reference
region can be treated as a simple multi-objective problem
and thus the search is focused on solving this problem only.
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

As a result, a reference region based preference mechanism is
designed in which the population evolves only in this region
where the DM desires. Provided the reference information, it
is possible to find the preference regions and then initialize
the individuals in the preference regions and search in the
region until approximating properly to the Pareto front. At the
initial stage, MOEA/D [1] is used to generate the initial population. When the preference information is given, namely
one or several reference regions are determined and those new
population are formed in the preference regions corresponding to some subproblems. Whereas in the evolution process,
the subproblems are updated by using MOEA/D-DE until
the DM is satisfied. Here it should be emphasized that each
individual solution belongs to different corresponding subproblem and each subproblem corresponds to certain weight.
Thus collective and dynamic adapting the weight vectors in
the course of evolution can ensure not only the distribution
range but also the convergence in the preference region. At
the interactive stage, a humanized and simple interactive
condition is adopted in this paper. A set of rough solutions are
presented to DM when the interactive condition is met. If the
DM is satisfied with the present solutions, the process would
be terminated; otherwise, the decision maker could change
his/her prefer information, a similar process is started until
the DM obtain the satisfied solutions. Therefore, the DM can
control the preference information in a very intuitive and fast
way.
A. PREFERENCE REGION

The first challenge in the proposed approach is how to determine the preference region. As we know, interaction involves
the DM in the process of optimization for dynamically offering preference information. And the DM dynamically guides
the algorithm to search in the preference region in the interaction processing. Therefore, an effective method should be
found for determining the preference region when the interaction happens.
As a basic algorithm framework in this paper, MOEA/D [1]
is characterized by 1) each subproblem is a independent
single-objective optimization problem and the solutions of
all the subproblems form the entire population; 2) all subproblems in MOEA/D spread uniformly in objective space
and each subproblem and its neighbor subproblems form a
preferred region. That is to say, finding the optimal subproblem which is nearest to the reference direction or the
reference point and these individuals in its neighbor can form
a preference region. In this paper, those reference directions
are selected as the preferred information which are marked
by owning more intuitive and better linked with weight vectors. As a result, the preferred regions can be determined
by two factors in the proposed algorithm: one is the searching direction which is determined by an aspiration and a
reservation point provided by DMs. another is radius which
determines the range of reference region. So, once a DM
offers a searching direction, we can find the optimal weight
vector close to the searching direction and obtain the neighbor
VOLUME 4, 2016

FIGURE 1. The illustration of determining the preference region.

of the optimal subproblem by computing the Euclidean distances between the optimal weight vector and other weight
vectors. By setting the radius and reasonably initializing the
weight vectors in this range, we can get subproblems in the
preference region. Fig. 1 shows this process visually. Here,
we can see that the vector determined by aspiration point and
reservation point is just what we define as the reference direction. The radius determines the scope of reference region, and
then we can find the optimal vector and other subproblems in
this reference region.
Finding an optimal weight vector close to the searching
direction is not trivial. In this paper, we employ the basic
MOEA/D [1] to generate the initial population. To determine
optimal weight vector, we connect each individual to the
aspiration point so as to form a vector and compute the Cosine
value between this vector and the searching direction:
cos(a, r) =

a•r
|a| × |r|

(5)

where vector a is obtained by connecting individual x with the
aspiration point and vector r is the searching direction. The
bigger the Cosine values, the closer to the searching direction
the individual is. We can select the subproblem’s weight
vector with the biggest Cosine value as the optimal weight
vector and re-initialize all the subproblems in the reference
region. Algorithm 1 shows the process of the initializing the
weights and finding initial population in the reference region
in details.
The human decision is characterized by uncertainty and
inconsistency [45]. In the proposed algorithm, the reference
information and the size of population are given by the decision maker. It is a common thing that the parameters the
DM inputs might not well match with each other. Therefore, we should take account of various circumstances. If
the number of subproblems (population size) is too small to
cover entire preferred region, the subproblem in the region
most close to the reference direction will become a priority
(Algorithm 1: line 8-9). In this paper we set the parameter
penalty to 1.5 to control the range of small population (it
can also be changed in some specific application). Otherwise,
7335

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

Algorithm 1 REFREGION
Interation Parameters:
num_v: the number of reference directions
PF: initial population
N: the size of population close to each reference direction
start: the vector of num_v aspiration points
fend: the vector of num_v reservation points
radius: determine the size of reference region
Output:
D0 : initial population in reference region
W0 : initial weight vectors in reference region
1. ref _dir ← fend − start
2. for i = 1, 2, . . . num_v do
3. best_weight i ← max(cos _value(PF, ref _dir))
4. range_weight i ← max(euc_dis(all_weight,
best_weight) < radius)
5. L = length(range_weight i )
6. for j = 1, 2, ....L
7. calculate the interval distance dj between the jth
and the (j + 1)th weight
8.
if L > penalty∗ N (:, i)
9.
Wi ← init_weight(min_euc_dis(range_weighti ,
N (:, i)))
10. else if L < penalty∗ N (:, i) & & L > N (:, i)
11.
Wi = range_weighti
12. while L > N (:, i)
13. find out the weight w with minimum interval
distance d
14.
Wi ← delete w from Wi
15.
L– –
16. end while
17. else
18.
Wi = range_weighti
19. while L < N (:, i)
20. find out the weight w with maximum interval
distance d
21. Wi ← insert a middle weight between w and the
following weight of w in Wi
22. L++
23. end while
24. end if
25. end for
26. Di ← max(cos_value(PF, ref _dir(:, i)), N (:, i))
27. D0 ← (D1 , . . . Dnum_v ), W0 ← (W1 , . . . Wnum_v )
28. end for

the subproblems could be evenly distributed in the preference
region to satisfy requirements (Algorithm 1: line 10-23). Consequently, the algorithm can keep diversity and robustness
as well.
B. Interactive Condition

As one of the preference methods, the interactive ones allow a
DM to dynamically guide the searching process until he/she is
satisfied with the resulting outcome. Whereas a key problem
7336

is when the current solutions are presented to the DM, in
many interactive algorithms, the process of interacting with a
DM will happen periodically [45], [46]. A long time interval
between two interactions may lead to a waste of time in
searching the region the DM does not favor. On the contrary,
it is tiresome to bother the DM excessively and it is also
a tiring work to find several preferred points from a large
number of random points in the beginning. Determining when
the algorithm interacts with the DM is important. We use an
indicator to determine whether the rough result needs to be
presented to the DM which is defined as follows:
δ=

N
i
1 X fti − ft−1
i
N
ft−1

(6)

i=1

Where N is the population size in the reference region, and
fti denotes the objective value of individual i at the tth generation. In this case, the function f is one with the minimum value of the individual i. Further, δ means the average
improvement degree of function value that the current individuals are compared with their parent individuals. The precision of the solutions presented to the DM can be controlled
by setting the value of δ. In other words, the parameter δ is
determined by the DM, when the condition of derta ≤ δ is
satisfied, the interaction happens.
Let us make some detailed illustration about a few aspects
of the Algorithm 1.
Step1: The reference direction(s) must be given by the DM,
in which aspiration points are start and reservation points are
fend.
Step3 to Step4: The two steps are designed to calculate
the best weight corresponding to the reference direction and
estimate the range weight around the preference direction
simultaneously.
Step5 to Step 26 used to determine the initial population
and the initial weight according to the relationship between
the preference region weight.
C. RR-MOEA/D

The overall procedure of the proposed interactive preference region based multi-objective evolutionary algorithm through decomposition (RR-MOEA/D) is described
in Algorithm 2.
Here, we give a more detailed explanation about several
key steps.
Initialization: In order to get uniform distribution of the
initial solutions and to make sure any region the DM prefers
has enough corresponding individuals, the initial population
is produced by limited iterations of MOEA/D instead of
random initialization. In this way, we can easily obtain the
individual and its corresponding weights in the preference
region, and shift quickly into new initial population when
DM’s searching regions are changed.
Updating Strategy: The population in the reference region
is updated by using MOEA/D until the termination criteria
is satisfied. It is worth noticing that the weights need to
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

Algorithm 2 RR-MOEA/D
Step1. Initialization: run pre-determined generations
MOEA/D to generate initial population;
Step2. Parameter input: ask the DM to input his/her
preference information;
Step3. Determining reference region: determine the reference region using the preference information
according to Algorithm 1;
Step4. Updating: update the population in the preference
region by MOEA/D-DE until the stopping
criterion is meet;
Step5. Interaction: If the DM is not satisfied with reference information, go to Step2 to reset the reference
information; If the DM wants to get better results in this
reference region, go to Step4; otherwise, output the results.

be updated every generation. Updating weights needs two
indicators: update direction and update step. The update
direction is determined by the subproblem farthest from the
reference direction to the subproblem nearest from the reference direction. The update step is defined as a unit length
(Algorithm 3: line 3). It is evitable that the population may
flies away from the preferred region by dynamically adjusting
the weights. The details of updating weights are stated in
Algorithm 3.
Interaction: At this stage, the current population is presented to the DM. If the DM is unsatisfied with the result,
the preference information can be reset. A new population
is produced according to the new desire of the DM and
a mixed population combining the current population with
the initial population. The DM can make tentative decisions
before he/she finds his/her desired regions. When the reference region is determined, the DM can reset the precision of
the solutions as the stopping criteria to get the final output
results.
Algorithm 3 Updating Weights
Interation Parameters:
num_v: the number of reference directions
PF: initial population
N: the size of population close to each reference direction
start: the vector of num_v aspiration points
fend: the vector of num_v reservation points
radius: determine the size of reference region
Output:
Wt : weights after updating
1. ref _dir ← fend − start
2. for i = 1, 2, . . . , num_v do
3.
stepi ← 2 × radiusi /N i
4.
best i ← max(cos(Dt , ref _dir(:, i)))
5.
worst i ← min(cos(Dt , ref _dir(:,i)))
6.
update_directioni ← best i − worst i
7.
Wt ← update_weight(update_directioni , stepi , Wt )
8. end for
VOLUME 4, 2016

In the algorithm, the size of the neighborhood is decided by
the size of population in the reference region and the range of
reference region. If a small population or a quite small range
which the DM is interested in, all the subproblems are treated
as a neighborhood. In this case, the algorithm is simplified
due to avoiding calculating the Euclidean distances between
all of the weight vectors.
Given the preference direction(s), RR-MOEA/D can control its searching regions in which the DM is interested.
By optimizing the subproblems in those regions, those solutions can be gotten which are converged to the preferred
regions.
IV. EXPERIMENTS AND RESULTS

In order to validate the performance of the proposed algorithm, we apply it to solving two 2-objective ZDT problems
and two 3-objective DTLZ problems respectively. All experiments were carried out on a personal computer with AMD
A8-5550M APU with Radeon (tm) HD Graphics (2.10GHz)
and 4 GB of RAM, Running Windows7.
A. BENCHMARK FUNCTIONS AND
EXPERIMENTAL SETTING

In our experiments, we use ZDT2and ZDT4 [47] and threeobjective benchmark problems, DTLZ1 and DTLZ2 [21], to
test our algorithm, which are presented in Table 1.
The parameters in RR-MOEA/D are set as follows: the
initial population size of MOEA/D-DE is set to 100 and 300
for the two objective and three objective problems respectively and MOEA/D-DE is executed 50 generations before
inputting the preference information. The algorithm will stop
after 250 generations. The size of population in the preference region N and the parameter of controlling interactive
condition δ depend on the decision maker. Here, in order to
illustrate experimental result, the size of radius is set as 0.005
and N is set to 30 and 60 for the two objective and three
objective problems and the penalty is set as 1.5. The impact
of the parameters provided by the DM on the performance of
the algorithm is analyzed in Section 4.3.
B. EXPERIMENTAL RESULT AND ANALYSIS
1) RESULTS OF TWO-OBJECTIVE TEST PROBLEMS

ZDT2 is a two-objective problem with 30variables and its
PF is non-convex as shown in Fig. 2. Four cases based on
different preference information are analyzed in order to
verify the performance of RR-MOEA/D.
Case 1: The reference direction is defined by two points
(0,0) and (1,1), we suppose the DM prefers the middle region
of the PF in this case.
Case 2: The reference direction is between (0,0) and (1,0),
which means one objective gets more preferences.
Case 3: The same as Case 2 and the aspiration point and
reservation point is (0,0) and (0,1), respectively.
Case 4: To show the performance of algorithm when the
DM sets more than one reference directions, we consider two
7337

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

TABLE 1. Test functions used in the study.

reference directions: D1 is determined by (0, 0) and (1, 2);
and D2 is determined by (0, 0) and (2, 1).
Fig. 2 shows that RR-MOEA/D has a good performance
for all these four cases. For different kind of reference region
required by the DM, the algorithm can obtain a set of reasonable and uniformly distributed solutions in the preferred
region. Also, the algorithm can deal with more than one
reference region simultaneously, and all preferred solutions
can be obtained in each reference region.
Another two objective problem is ZDT4, which is hard due
to its many local optimal solutions and its convex PF with
10 variables. Similar with ZDT2 problem, we also adopt the
four cases above to test ZDT4 problem (see in Fig. 3).
It is obvious in Fig. 3 that the proposed algorithm can deal
with the convex problem as well and get the ideal solutions in
any region the DM prefers. Uniform distribution and convergence in the preference region on ZDT4, a hard problem of
7338

ZDT test problems, show a good performance of the proposed
algorithm.
2) RESULTS OF THREE-OBJECTIVE TEST PROBLEMS

DTLZ1 is a 10-variable, three-objective
problem. Its efficient
P
frontier is known and is given by i∈[1,m] fi = 0.5, as shown
in Fig. 4. Fig. 4(a) shows the obtained solutions with one
reference direction when the reference direction is given by
two points (0, 0, 0) and (0.5, 0.5, 0.5). To show the performance with more than one preference region, Fig. 4(b)
shows the obtained solutions with two reference direction
regions. Two reference directions are: {(0,0,0), (0.5,0.5,0.3)},
{(0,0,0), (0.1,0.1,0.5)}.
DTLZ2 has a convex
PF with 10 variables and its efficient
P
frontier satisfies i∈[1,m] fi2 = 1. Similar with DTLZ1 problem, we use one and two reference directions to show the performance. The first reference direction is from point (0,0,0)
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 2. Preferred solutions obtained by RR-MOEA/D on ZDT2: (a) case 1; (b) case 2; (c) case 3; (d) case 4.

FIGURE 3. Preferred solutions obtained by RR-MOEA/D on ZDT4 : (a) case 1; (b) case 2; (c) case 3; (d) case 4.

to point (0.7, 0.8, 0.5). The two reference directions in
Fig. 5(b) are {(0,0,0), (1,1,0.8)}, {(0,0,0), (0.4,0.4,0.1)},
respectively.
VOLUME 4, 2016

The final results of three-objective problem are shown in
Fig. 4 and Fig. 5. Obviously our algorithm also can well
converge to the true PF within the referred region the decision
7339

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 4. Preferred solutions obtained by RR-MOEA/D on DTLZ1 (a) one reference region; (b) two reference regions.

FIGURE 5. Preferred solutions obtained by RR-MOEA/D on DTLZ2 (a) one reference region; (b) two reference regions.

maker favors. It needs to point out here that the shape of
obtained solutions is determined by the way of initializing
the weights in MOEA/D.
C. DISCUSSION
1) THE DISCUSSION OF INTERACTION PERFORMANCE

As an interactive method, the interaction performance of
the algorithm is vitally importance. A simple interaction is
presented in the proposed algorithm. It is not an easy task

FIGURE 6. Initial solutions obtained in the interactive stage of ZDT2
((0, 0) →(1,1)).
7340

that an exact preference region is required at the beginning,
so the algorithm allows the decision maker to pick up a
reference direction as a trial. When the interaction condition
is met, these current solutions which are rough solutions in
the reference region are presented to the DM. If the DM is
interested in the reference region, our algorithm will go on
with finding the solutions approximate to PF in the reference
region. Otherwise, searching direction can be changed by the
DM. Here is an example to illustrate that we have plenty
ways to change the searching directions. Supposed we set
an initial searching direction from (0, 0) to (1, 1) and the
size of preference region radius = 0.005 for ZDT2 problem,
a set of rough solutions in the reference region are presented
to the DM (see in Fig. 6) when the interaction condition is
satisfied (Eq. 6), in which, δ = 0.001. If the DM is unsatisfied
with the reference region he/she set earlier, they change their
preference information as follows:
Case 1: The DM changes the initial searching region to
a near preference region(e.g. from (0,0) → (1,1) to (0,0) →
(1,1.1)).
Case 2: The reference region is changed far from the
initial reference region by the DM(e.g. from (0,0) →(1,1) to
(0,0) → (0.3,0.8)).
Case 3: A new preference region is added (e.g. from
(0,0) → (1,1) to (0,0) →(0.5,1) and (0,0) → (1,1)).
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 7. Solutions obtained of ZDT2 after different interactions: (a) case 1; (b) case 2; (c) case 3; (d) case 4.
TABLE 2. Mean generations and its standard variance with different δ.

Case 4: The DM changes an initial searching region to
two different searching regions (e.g. from (0,0) → (1,1) to
(0,0) → (0.5,1) and (0,0) → (1,0.5)).

VOLUME 4, 2016

The final results of these four cases are shown in
Fig. 7. If the DM is not satisfied with the searching
region he/she set before, our algorithm also can fast capture

7341

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 8. Solutions obtained on ZDT2 with the radius=0.5: (a) N = 20; (b) N = 50; (c) N = 100.

FIGURE 9. Solutions obtained on ZDT2 with the radius=0.05: (a) N = 20; (b) N = 50; (c) N = 100.

the new preference region and well converge to the PF
within the new preferred region to meet various user
requirements.
7342

A fast way to obtain a number of rough solutions in the
reference region to interact with the DM is another distinguishing characteristic. As mentioned earlier, δ is provided
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 10. Solutions obtained on ZDT2 with the radius=0.005: (a) N = 20; (b) N = 50; (c) N = 100.

FIGURE 11. Solutions obtained on ZDT2 problem with the radius=0.005: (a) penalty=1.0; (b) penalty=1.5;
(c) Penalty=2.0; (d) penalty=3.0.

by the decision maker to control the accuracy of the solution.
According to the interactive condition used in our study,
rapidly getting most solutions in DM’s preferred region is presented to the DM. In Table 2, the mean generations needed are
VOLUME 4, 2016

shown when the interaction happens for ZDT test problems
based on 10 independent runs (δ is set as 0.1, 0.01, 0.001). It
is easy to see that it just needs no more than 20 generations for
the proposed algorithm to find a rough set of solution when
7343

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

δ = 0.001, even when δ = 0.1, the proposed algorithm can
find some rough solutions in no more than 100 generations.
It is obvious that RR-MOEAD is a fast algorithm to meet
the interactive condition and this means the algorithm can
quickly get the rough solutions in the region preferred by the
DM.
2) THE DISCUSSION OF PARAMETERS
(RADIUS, N AND PENALTY)

At interactive stage, RR-MOEA/D allows the DM to input
different preference information to satisfy different preference requirements. In order to analyze the impact of the input
parameters like the population size N , the size of preferred
region i.e. the radius and the penalty on the performance of
the algorithm, we set the population size as 20, 50 and 100
respectively when the radius size is fixed as 0.5, 0.05, and
0.005 for 2-objective ZDT2 problem and perform a series of
experiments. The aspiration point and the reservation point
are set to (0, 0) and (1, 1).
Further, in order to determine the effect of the parameter
penalty on the experiment’s results. We set the penalty as
1.0, 1.5, 2.0 and 3.0 respectively when the value of radius
is fixed as 0.005 and the population is 30 for ZDT2, the
aspiration point and the reservation point are set to (0,0)
and (1,1).
From Fig. 8 Fig. 10, we can see that the larger the radius
is, it is better to set the radius as a large value. When DMs
provide a small value for the parameter radius, it is not
necessary to set a large value for the parameter N . in this
paper, we set radius = 0.005 and N is set to 30 and 60 for
the two objective and three objective problems.
Fig. 11 shows the solutions obtained with different value of
the penalty. We can conclude that if the value of the penalty is
set around 1.5, the entire reference region would be well converged to near the reference direction. Meanwhile, the penalty
being fixed as a greater value, the converged front will deviate
the reference direction. So, in the experimental study, we set
it as1.5. From here, we can draw a conclusion that the more
detailed information the DM provide, the more accurate solutions our algorithm can find. For example, if the smaller value
of radius and penalty are given, the front obtained would be
well converged to the reference direction. No matter what
reference region the DM needs, our algorithm can get a wellspaced front well converged to the region most preferred.
V. CONCLUSIONS

In this paper, an interactive reference region based evolutionary algorithm through decomposition named RR-MOEA/D
is proposed. At the beginning of the proposed algorithm,
MOEA/D is used to generate initial solutions before the
preference desired by the decision maker are given. We can
initialize the suitable subproblems in the reference regions
according to the DM’s requirements. By dealing with the
subproblems in the reference region, the solutions obtained
can converge to the regions of the Pareto front which the DM
prefers.
7344

During the stage of interaction, a simple interactive condition is adopted in this paper. The proposed algorithm can get
a set of rough solutions in one or several reference regions
quickly and avoid wasting time in searching the region which
the DM is not interested in. Solutions obtained are presented
to DM when the interactive condition is met. If the DM is
unsatisfied with these solutions, the new preference information can be reset. Therefore, the decision maker can control
the preference information in a very intuitive way. In addition, RR-MOEA/D allows the DM to reset different kinds of
preference information and it can quickly obtain a set rough
solution according to new preference information.
It should be emphasized that all of subproblems in the
preference region are determined by weight vectors in the
evolving stage. It is evitable that the population converges
to the region on PF where the user is undesired by dynamically adjusting the corresponding weights. If the number of
sub problems (population size) is so small to cover entire
preference region, the subproblems in the region most close
to the reference direction will become a priority. In other
words, RR-MOEA/D allows the DM to give uncertain and
inconsistent human decision as he or she does in real life.
Both MOPs with two and three objectives are adopted
to test the performance of RR-MOEA/D. According to the
results of experimental study, the proposed algorithm can successfully converge to different regions the DM most prefers.
It is obvious that our algorithm can realize a fast and simple
interactive process and copes with different preference information including uncertain and inconsistent human decision.
In our future work, we will try to find a better way to construct
the preference regions. As we known, there are still many
aspects of interaction strategy to worth further exploration.
Dealing with interaction information better and more humanized is our another work.
REFERENCES
[1] Q. Zhang and H. Li, ‘‘MOEA/D: A multiobjective evolutionary algorithm
based on decomposition,’’ IEEE Trans. Evol. Comput., vol. 11, no. 6,
pp. 712–731, Dec. 2007.
[2] K. Sindhya, A. B. Ruiz, and K. Miettinen, ‘‘A preference based interactive evolutionary algorithm for multi-objective optimization: PIE,’’ in
Evolutionary Multi-Criterion Optimization (Lecture Notes in Computer
Science), vol. 6576. Berlin, Germany: Springer-Verlag, 2011, pp. 212–225.
[3] K. Miettinen, Nonlinear Multiobjective Optimization. Norwell, MA, USA:
Kluwer, 1999.
[4] I. Das and J. E. Dennis, ‘‘Normal-boundary intersection: A new method
for generating the Pareto surface in nonlinear multicriteria optimization
problems,’’ SIAM J. Optim., vol. 8, no. 3, pp. 631–657, 1998.
[5] A. Messac, A. Ismail-Yahaya, and C. A. Mattson, ‘‘The normalized normal
constraint method for generating the Pareto frontier,’’ Struct. Multidisciplinary Optim., vol. 25, no. 2, pp. 86–98, Jul. 2003.
[6] R. F. Dell and M. H. Karwan, ‘‘An interactive MCDM weight space
reduction method utilizing a Tchebycheff utility function,’’ Naval Res.
Logistics, vol. 37, no. 2, pp. 263–277, Apr. 1990.
[7] H. Li and Q. Zhang, ‘‘Multiobjective optimization problems with complicated Pareto sets, MOEA/D and NSGA-II,’’ IEEE Trans. Evol. Comput.,
vol. 13, no. 2, pp. 284–302, Apr. 2009.
[8] R. Storn and K. Price, ‘‘Differential evolution—A simple and efficient
heuristic for global optimization over continuous spaces,’’ J. Glob. Optim.,
vol. 11, no. 4, pp. 341–359, 1997.
[9] K. Deb and R. B. Agrawal, ‘‘Simulated binary crossover for continuous
search space,’’ Complex Syst., vol. 9, no. 3, pp. 115–148, 1995.
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

[10] H.-L. Liu, F. Gu, and Q. Zhang, ‘‘Decomposition of a multiobjective optimization problem into a number of simple multiobjective subproblems,’’
IEEE Trans. Evol. Comput., vol. 18, no. 3, pp. 450–455, Jun. 2013.
[11] K. Li, K. Deb, Q. Zhang, and S. Kwong, ‘‘An evolutionary many-objective
optimization algorithm based on dominance and decomposition,’’ IEEE
Trans. Evol. Comput., vol. 19, no. 5, pp. 694–716, Oct. 2015.
[12] R. Cheng, Y. Jin, K. Narukawa, and B. Sendhoff, ‘‘A multiobjective
evolutionary algorithm using Gaussian process-based inverse modeling,’’
IEEE Trans. Evol. Comput., vol. 19, no. 6, pp. 838–856, Dec. 2015.
[13] R. Cheng, Y. Jin, M. Olhofer, and B. Sendhoff, ‘‘A reference vector guided
evolutionary algorithm for many-objective optimization,’’ IEEE Trans.
Evol. Comput., vol. 20, no. 5, pp. 773–791, 2016.
[14] K. Deb and H. Jain, ‘‘An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part I:
Solving problems with box constraints,’’ IEEE Trans. Evol. Comput.,
vol. 18, no. 4, pp. 577–601, Aug. 2014.
[15] K. Deb, S. Agarwal, A. Pratap, and T. Meyarivan, ‘‘A fast and elitist
multiobjective genetic algorithm: NSGA-II,’’ IEEE Trans. Evol. Comput.,
vol. 6, no. 2, pp. 182–197, Apr. 2002.
[16] C. A. C. Coello, ‘‘Handling preferences in evolutionary multiobjective
optimization: A survey,’’ in Proc. Congr. Evol. Comput., Jul. 2000,
pp. 30–37.
[17] L. Rachmawati and D. Srinivasan, ‘‘Preference incorporation in multiobjective evolutionary algorithms: A survey,’’ in Proc. IEEE Congr. Evol.
Comput. (CEC), Jul. 2006, pp. 3385–3391.
[18] A. P. Wierzbicki, ‘‘The use of reference objectives in multiobjective optimization,’’ in Multiple Criteria Decision Making Theory and Application.
Heidelberg, Germany: Springer, 1980, pp. 468–486.
[19] A. Jaszkiewicz and R. Slowiński, ‘‘The ‘light beam search’ approach—An
overview of methodology applications,’’ Eur. Jour. Oper. Res., vol. 113,
no. 2, pp. 300–314, Mar. 1999.
[20] K. Deb and J. Sundar, ‘‘Reference point based multi-objective optimization
using evolutionary algorithms,’’ Int. J. Comput. Intell. Res., vol. 2, no. 3,
pp. 273–286, 2006.
[21] K. Deb and A. Kumar, ‘‘Interactive evolutionary multi-objective optimization and decision-making using reference direction method,’’ in Proc.
9th Annu. Conf. Genet. Evol. Comput. (GECCO), 2007, pp. 781–788.
[22] L. Thiele, K. Miettinen, P. J. Korhonen, and J. Molina, ‘‘A preference-based
evolutionary algorithm for multi-objective optimization,’’ Evol. Comput.,
vol. 17, no. 3, pp. 411–436, 2009.
[23] J. Molina, L. V. Santana, A. G. Hernández-Diaz, C. A. C. Coello, and
R. Caballero, ‘‘g-dominance: Reference point based dominance for multiobjective metaheuristics,’’ Eur. J. Oper. Res., vol. 197, no. 2, pp. 685–692,
Sep. 2009.
[24] E. Zitzler, M. Laumanns, and L. Thiele, ‘‘SPEA2: Improving
the strength Pareto evolutionary algorithm,’’ in Evolutionary
Methods for Design, Optimization and Control With Applications
to Industrial Problems K. C. G. Koglou, D. T. Tsahalis, J. Périaux,
and T. Fogarty, Eds. Heidelberg, Germany: Springer, 2001,
pp. 95–100.
[25] M. G. Gong, L. Jiao, H. Du, and L. Bo, ‘‘Multiobjective immune algorithm
with nondominated neighbor-based selection,’’ Evol. Comput., vol. 16,
no. 2, pp. 225–255, 2008.
[26] J. Branke, K. Deb, and K. Miettinen, Multiobjective Optimization: Interactive and Evolutionary Approaches, vol. LNCS 5252. Berlin, Germany:
Springer-Verlag, 2008.
[27] T. Tanino, M. Tanaka, and C. Hojo, ‘‘An interactive multicriteria decision
making method by using a genetic algorithm,’’ Manage. Sci., vol. 22 no. 6,
pp. 652–663, 1976.
[28] C. M. Fonseca and P. J. Fleming, ‘‘Multiobjective optimization and multiple constraint handling with evolutionary algorithms. I. A unified formulation,’’ IEEE Trans. Syst., Man, Cybern. A, Syst., Humans, vol. 28, no. 1,
pp. 26–37, Jan. 1998.
[29] M. Shibuya, H. Kita, and S. Kobayashi, ‘‘Integration of multi-objective
and interactive genetic algorithms and its application to animation design,’’
in Proc. IEEE Int. Conf. Syst., Man, Cybern. IEEE SMC, Oct. 1999,
pp. 646–651.
[30] D. S. Todd and P. Sen, ‘‘Directed multiple objective search of design spaces
using Genetic Algorithms and neural networks,’’ in Proc. Genetic Evol.
Comput. Conf., 1999, pp. 1738–1743.
[31] I. C. Parmee, D. Cvetković, A. Watson, and C. Bonham, ‘‘Multiobjective
satisfaction within an interactive evolutionary design environment,’’ Evol.
Comput., vol. 8, no. 2, pp. 197–222, Jun. 2000.
VOLUME 4, 2016

[32] H. J. C. Barbosa and A. M. S. Barreto, ‘‘An interactive genetic algorithm with co-evolution of weights for multiobjective problems,’’ in Proc.
Genetic Evol. Comput. Conf., 2001, pp. 203–210.
[33] S. Phelps and M. Köksalan, ‘‘An interactive evolutionary metaheuristic for
multiobjective combinatorial optimization,’’ Manage. Sci., vol. 49, no. 12,
pp. 1726–1738, 2003.
[34] R. Kamalian, H. Takagi, and A. M. Agogino, ‘‘Optimized design of MEMS
by evolutionary multi-objective optimization with interactive evolutionary computation,’’ in Proc. GECCO, Berlin, Germany, vol. 3103. 2004,
pp. 1030–1041.
[35] R. Kamalian, Y. Zhang, H. Takagi, and A. M. Agogino, ‘Evolutionary synthesis of micromachines using supervisory multiobjective interactive evolutionary computation,’’ in Proc. ICMLC, LNAI3930, 2006,
pp. 428–437.
[36] L. Thiele, K. Miettinen, P. J. Korhonen, and J. Molina, ‘‘A preference-based
interactive evolutionary algorithm for multiobjective optimization,’’ Dept.
Inf. Technol. Electr. Eng., Aalto Univ. School Bus., Helsinki, Finland,
Tech. Rep., 2007.
[37] G. Avigad and A. Moshaiov, ‘‘Interactive evolutionary multiobjective
search and optimization of set-based concepts,’’ IEEE Trans. Syst., Man,
Cybern. B, Cybern., vol. 39, no. 4, pp. 1013–1027, Aug. 2009.
[38] L. B. Said, S. Bechikh, and K. Ghedira, ‘‘The r-dominance: A new dominance relation for interactive evolutionary multicriteria decision making,’’
IEEE Trans. Evol. Comput., vol. 14, no. 5, pp. 801–818, Oct. 2010.
[39] K. Deb, A. Sinha, P. J. Korhonen, and J. Wallenius, ‘‘An interactive
evolutionary multiobjective optimization method based on progressively
approximated value functions,’’ IEEE Trans. Evol. Comput., vol. 14, no. 5,
pp. 723–739, Oct. 2010.
[40] C. L. Simons and I. C. Parmee, ‘‘Elegant object-oriented software design
via interactive, evolutionary computation,’’ IEEE Trans. Syst., Man,
Cybern. C, Appl. Rev., vol. 42, no. 6, pp. 1797–1805, Nov. 2012.
[41] X. Sun, D. Gong, Y. Jin, and S. Chen, ‘‘ A new surrogate-assisted interactive genetic algorithm with weighted semisupervised learning,’’ IEEE
Trans. Cybern., vol. 43, no. 2, pp. 685–698, Apr. 2013.
[42] Evolutionary Multi-Criterion Optimization (Lecture Notes in Computer
Science), vol. 9019. Switzerland, Springer International Publishing, 2015,
pp. 249–263.
[43] A. B. Ruiz, R. Saborido, and M. Luque, ‘‘A preference-based evolutionary
algorithm for multiobjective optimization: The weighting achievement
scalarizing function genetic algorithm,’’ J. Global Optim., vol. 62, no. 1,
pp. 101–129, May 2015.
[44] M. G. Gong, F. Liu, W. Zhang, L. Jiao, and Q. Zhang, ‘‘Interactive
MOEA/D for multi-objective decision making,’’ in Proc. 13th Annu. Conf.
Genetic Evol. Comput., 2011, pp. 721–728.
[45] R. Battiti and A. Passerini, ‘‘Brain–computer evolutionary multiobjective
optimization: A genetic algorithm adapting to the decision maker,’’ IEEE
Trans. Evol. Comput., vol. 14, no. 5, pp. 671–687, Oct. 2010.
[46] C. Fonseca and P. J. Fleming, ‘‘Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization,’’ In Proc.
5th Int. Conf. Genet. Algorithms, 1993, pp. 416–423.
[47] E. Zitzler, L. Thiele, and K. Deb, ‘‘Comparison of multiobjective evolutionary algorithms: Empirical results,’’ Evol. Comput., vol. 8, no. 2,
pp. 173–195, Jun. 2000.

RUOCHEN LIU (M’07) received the Ph.D. degree
from Xidian University, Xian, China, in 2005. She
is currently an Associate Professor with the Intelligent Information Processing Innovative Research
Team of the Ministry of Education of China, Xidian University. Her research interests are broadly
in the area of computational intelligence. Her areas
of special interest include artificial immune systems, evolutionary computation, data mining, and
optimization.
7345

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

7346

RUINAN WANG received the B.S. degree from
the Hebei University of Science and Technology, Hebei, China. He is currently pursuing the
M.S. degree from Xidian University. His current
research focuses on multi-objective optimization
and data mining.

JUNJUN HUANG received the B.S. degree from
the Huaiyin Institute of Technology, Huai’an,
China. He is currently pursuing the M.S. degree
with Xidian University. His current research
focuses on multi-objective optimization and image
processing.

WEN FENG received the B.S. degree from Xidian
University, Xi’an, China, where she is currently
pursuing the M.S. degree. Her current research
focuses on multi-objective optimization.

LICHENG JIAO (SM’89) received the Ph.D.
degree from Xian Jiaotong University, Xian,
China, in 1990. He is currently a Professor and the
Dean of the Electronic Engineering School, Xidian
University, China. His current research focuses on
intelligent information processing.

VOLUME 4, 2016

Database, 2016, 18 doi: 10.1093/database/baw127 Database Tool

Database Tool

Skeleton Genetics: a comprehensive database for genes and mutations related to genetic skeletal disorders
Chong Chen1,, Yi Jiang2,, Chenyang Xu1, Xinting Liu2, Lin Hu3, Yanbao Xiang1, Qingshuang Chen1, Denghui Chen2, Huanzheng Li1, Xueqin Xu1 and Shaohua Tang1,3,*
Department of Genetics of Dingli Clinical Medical School, Wenzhou Central Hospital, Wenzhou 325000, China, 2Institute of Genomic Medicine, Wenzhou Medical University, Wenzhou 325035, China, 3 School of Laboratory Medicine and Life Science, Wenzhou Medical University, Wenzhou 325035, China
*Corresponding author: Tel/Fax: 86 577 88070197; Email: tsh006@126.com


Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

1

These authors contributed equally to this work.

Citation details: Chen,C., Jiang,J., Xu,C. et al. SkeletonGenetics: a comprehensive database for genes and mutations related to genetic skeletal disorders. Database (2016) Vol. 2016: article ID baw127; doi:10.1093/database/baw127
Received 2 May 2016; Revised 2 August 2016; Accepted 12 August 2016

Abstract
Genetic skeletal disorders (GSD) involving the skeletal system arises through disturbances in the complex processes of skeletal development, growth and homeostasis and remain a diagnostic challenge because of their clinical heterogeneity and genetic variety. Over the past decades, tremendous effort platforms have been made to explore the complex heterogeneity, and massive new genes and mutations have been identified in different GSD, but the information supplied by literature is still limited and it is hard to meet the further needs of scientists and clinicians. In this study, combined with Nosology and Classification of genetic skeletal disorders, we developed the first comprehensive and annotated genetic skeletal disorders database, named `SkeletonGenetics', which contains information about all GSD-related knowledge including 8225 mutations in 357 genes, with detailed information associated with 481 clinical diseases (2260 clinical phenotype) classified in 42 groups defined by molecular, biochemical and/or radiographic criteria from 1698 publications. Further annotations were performed to each entry including Gene Ontology, pathways analysis, proteinprotein interaction, mutation annotations, diseasedisease clustering and genedisease networking. Furthermore, using concise search methods, intuitive graphical displays, convenient browsing functions and constantly updatable features, `SkeletonGenetics' could serve as a central and integrative database for unveiling the genetic and pathways pre-dispositions of GSD. Database URL: http://101.200.211.232/skeletongenetics/

C The Author(s) 2016. Published by Oxford University Press. V

Page 1 of 8

This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. (page number not for citation purposes)

Page 2 of 8

Database, Vol. 2016, Article ID baw127

Introduction
The skeleton provides the structural framework in humans for muscle attachments, assists movement, protects organs, and maintains the homeostasis of the vascular systems (1). Perturbation of development in bone, cartilage and joints could result in human skeletal dysplasias, which affect approximately 1 in 5000 live births and is one of the common causes of neonatal birth defect in departments of obstetrics (2). Genetic skeletal disorders (GSD), which are a group of disorders involving gene mutations or genetic susceptibility to bone disease, account for most of the human skeletal dysplasias, involving a variety of pathogenic factors and diseases (3). In the Nosology and Classification of Genetic Skeletal Disorders 2015 revision (hereinafter referred to as NCGSD-2015), 436 conditions were included and placed in 42 groups, which were associated with mutations in one or more of 336 unique genes (4). Till now, especially with the impact the next-generation sequencing technology has made on the study of genetic skeletal disorders, a large amount of mutations and novel GSD-related genes have been identified. Genetic skeletal disorders face similar problems to other complex diseases with exceptional genetic heterogeneity and clinical variety (5). Notably, the same skeletal dysplasia gene can lead to substantially different clinical phenotypes or a specific skeletal phenotype can be caused by mutations in different genes (6). Therefore, understanding genotypephenotype correlations and functional diversity remains one of the major challenges for GSD. The NCGSD-2015 provides an overview of the recognized diagnostic entities of GSD by clinical, anatomical site and molecular pathogenesis for clinicians, scientists and the radiology community to diagnose individual cases and describe novel disorders (5). However, the increasing availability of next-generation sequencing technology and other new sequencing platforms will likely result in a rapid identification of novel GSD-related genes and mutations, and novel phenotypes associated with mutations in genes linked to other phenotypes has increased dramatically to date (7, 8). Therefore, the catalog of GSD-related genotype-phenotype has become so large as to surpass the scope of a `Nosology', so the Nosology must be transformed into an automated annotation and query database. Thus, it is crucial to integrate the existing data and present an organized comprehensive mutation repository to construct an integrative, informative and updatable resource for GSD-related genetic pre-dispositions which could greatly facilitate the counseling, diagnosis and therapy for pediatrics and genetics. In this study, we reviewed three dependent databases and public resources related to genetic skeletal disorders and made the first annotated database about GSD, named `SkeletonGenetics', which provides full-scale gene and

mutation information and extensive annotations for GSD. Initially, we retrospectively extracted the basic information for each mutation (disease conditions or syndrome, gene symbol, number of OMIM, hereditary mode, coding sequence change, transcript, ethnicity, age, PubMed ID, etc.) from open publications. Additionally, extensive annotations were performed, which include gene information, Gene Ontology, pathways analysis, protein-protein interaction, mutations clustering and gene-disease network. As a result, 42 groups of GSD, 481 diseases or syndromes, 357 genes, 5884 single nucleotide variations (SNVs), 516 insertions, 1427 deletions and 2260 different phenotypes were included. Taken together, `SkeletonGenetics' is constructed to be a well-organized, internal-standard and comprehensive resource for scientists and clinicians looking for the clinical correlates of mutations, genes and pathways involved in skeletal biology.

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Methods
The standard of data entry group of GSD
The Nosology Group of the International Skeletal Dysplasia Society formulated the table of NCGSD-2015 in September, 2015. The criteria used for genetic skeletal disorders were unchanged from NCGSD-2010 revision. They were (1) significant skeletal involvement, including skeletal dysplasias, skeletal dysostoses, metabolic bone disorders and skeletal malformation and/or reduction; (2) publication and/or listing in OMIM; (3) genetic basis proven by pedigree or very likely based on homogeneity of phenotype in unrelated families; (4) nosologic autonomy confirmed by molecular or linkage analysis and/or by the presence of distinctive diagnostic features and of observation in multiple individuals or families (4). In accordance with the standards of NCGSD-2015, 481 different conditions were placed in 42 groups, associated with one or more of 357 different genes. The grouping results and criteria are presented in Supplementary Table S1.

Data collection and content
Based on the standards of NCGSD-2015, we performed comprehensive searches for GSD-related publications and databases to obtain a complete list of mutation information associated with GSD. Firstly, we retrospectively queried the PubMed database (http://www.ncbi.nih.gov/ pubmed) for genes and diseases retrieved in Entrez with terms `gene or disease symbol [Title/Abstract] AND mutation [Title/Abstract] OR variant [Title/Abstract]. Secondly, other databases were taken as supplementary sources, including Leiden Open Variation Database 3.0 (LOVD3.0,

Database, Vol. 2016, Article ID baw127

Page 3 of 8

http://www.lovd.nl/3.0/home) which contained fanconi anemia relevant genes and the mutations predicted to be benign or which did not segregate with phenotype were excluded (9), Clinvar (http://www.ncbi.nlm.nih.gov/clin var/), which contained pathogenic or likely pathogenic clinical mutation information. Human Phenotype Ontology (HPO, http://human-phenotype-ontology. github.io/) (10) and Online Mendelian Inheritance in Man (OMIM) (11) were used to describe phenotypic information. Patient clinical data have been obtained in accordance with the tenets of the Declaration of Helsinki. Finally, >2000 publications were queried from open resource beginning in 1990. After manually screening the abstracts and full-text of these publications, we excluded those studying diseases other than GSD and eventually retained 1698 publications. We extracted the basic information for each mutation, including disease name, gene symbol, inheritance mode, CDS change, PubMed ID, ethnicity, age, gender and functional study through reading the full-text articles and double-checked these manually. The in-house perl program was used to obtain the correct genomic coordinate for each entry. As of the beginning of 2016, the `SkeletonGenetics' database contains 481 disease conditions, associated with mutations in one or more of 357 different genes, 8225 variations (5884 SNVs, 1427 deletions, 516 insertions and 398 Block substitution), 2260 clinical phenotypes and their detailed information included in 42 groups of GSD from 1698 publications ranging from common, recurrent mutations to `private' mutations found in single families or individuals. The results are presented in Table 1.

Table 1 Data content and statistics of genetic data in SkeletonGenetics
Data type Mutation SNVs Deletions Insertions Block substitution Genes Disease group Disease or syndrome Phenotypes GOs Pathways KEGG pathways Wiki pathways MicroRNA Target PPI Publications Data count 8225 5884 1427 516 398 357 42 481 2260 2535 138 64 74 66 40 1698

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

syndrome, Cornelia de Lange syndrome, which all belong to the Top 5 of the disease list, all have >367 gene mutation information. At the same time, we classified the number of mutations by group: the limb hypoplasia-reduction defects group; the overgrowth syndromes with skeletal involvement group; the lysosomal storage diseases with skeletal involvement (dysostosis multiplex group); the disorganized development of skeletal components group; and the dysostoses with predominant craniofacial involvement group all have >375 mutation information. Finally, according to the information collected, we statistically analyzed the bias by chromosomes, mutation types, mutation effect, gender, ages of onset and inheritance mode.

Information statistics
Specific skeletal phenotype can be caused by mutations in different genes or the same gene can lead to substantially different clinical phenotypes. This leads to the bias of genotype and phenotype of GSD, therefore statistics on the number of mutations for gene, disease and the group become very important. The gene Top 5 with the highest number of mutations in `SkeletonGenetics' is `FBN1', associated with four skeletal diseases (Marfan syndrome, Weill-Marchesani syndrome 2, Geleophysic dysplasia type 2, Acromicric dysplasia) with 639 mutations; followed by `NF1' associated with Neurofibromatosis type 1 with 415 mutations; `NIPBL' associated with Cornelia de Lange syndrome 1 with 406 mutations; `NSD1' associated with Sotos syndrome 1 with 362 mutations; `GNPTAB' associated with two diseases (Mucolipidosis II, alpha/beta type; Mucolipidosis III (Pseudo-Hurler polydystrophy), alpha/ beta type) with 231 mutations. Correspondingly, Marfan syndrom, Fanconi anemia, Neurofibromatosis, Sotos

Functional and enrichment analysis
To further interpret the function and heterogeneity of GSD, `SkeletonGenetics' performs a series of functional analyses, which included enrichment analysis, mutation annotation, mutation spectrum and gene-disease network construction (Figure 1). For the aspect of enrichment analysis, we provided Gene Ontology (GO), KEGG pathways, Wikipathways, MicroRNA Target and ProteinProtein interaction. Meanwhile, we used WebGestalt software to store the gene list in `SkeletonGenetics' (12, 13). Users can click on the `Analysis' page to see whether the gene of interest is involved in any GO terms, function pathways, microRNA target or PPIs. Gene ontology annotation terms involved the biological process, the cellular component and molecular function of three biological processes, the Top5 most statistically

Page 4 of 8

Database, Vol. 2016, Article ID baw127

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Figure 1. Flowchart of the procedure for `SkeletonGenetics'. `SkeletonGenetics' mainly consists of three parts: (i) data extraction based on literature search and GSD-related databases, (ii) annotation of all mutations and genes using ANNOVAR and (iii) enrichment analysis by WebGestalt and genedisease network analysis graphically.

significant (P-values) of biological process terms in `SkeletonGenetics', namely, skeletal system development (P-value: 3.29E-63, number of genes, 87) (14); appendage development (P-value: 6.25E-46, number of genes, 50) (15); limb development (P-value: 6.25E-46, number of genes, 50) (16); appendage morphogenesis (P-value: 5.11E-45, number of genes, 48) (15); and limb morphogenesis (P-value: 5.11E-45, number of genes, 48) (16). The above entries of the most statistically significant GOs terms are all related to skeletal system or component formation, morphological differentiation and location control. Meanwhile, we provided functions pathways, including KEGG pathways and Wikipathways of two annotation methods. A complex series of signaling pathways including TGF-beta signaling pathway (P-value: 1.07E-13, number of genes, 14) (17); Hedgehog signaling pathway (P-value: 5.85E-09, number of genes, 9) (18); WNT signaling pathway and pluripotency (P-value: 1.72E-06, number of genes, 10) (19); Notch signaling pathway (P-value: 4.79E07, number of genes, 7) (20) and metabolic processing (P-value: 1.48E-14, number of genes, 41) are essential for proper skeletogenesis (21), mainly distributed in the cell, extracellular matrix, and transcriptional regulation related to bone, cartilage, and joint formation. Researchers recommend use of a morphogen rheostat model to conceptualize the differential signaling inputs which lead to divergent skeletal phenotypes within a temporal and spatial context

(1). In these terms, we have established some function pathways of the relationship between different gene mutations and groups of bone diseases. The modification of microRNA by degraded target mRNA maintains cellular homeostasis and regulates cell fate transitions during differentiation. These processes are important to ensure proper organogenesis and growth of skeleton. `hsa_CTTTGCA, MIR-527', which ranks first with the P-value of 3.93E-07 among all MicroRNA Target enriched, including the MYCN gene, in which miRNA cluster heterozygous mutations cause Feingold syndrome, a disorder that involves limb malformations, microcephaly, learning disability/mental retardation, hand and foot abnormalities and may include hypoplastic thumbs, clinodactyly of the second and fifth fingers, syndactyly (characteristically between the second and the fourth and fifth toes) and shortened or absent middle phalanges, cardiac and renal malformations, vertebral anomalies and deafness (22). Defining the targets of this miRNAs gene will give a deeper understanding of the pathophysiology and complex genetics of GSD. Finally, PPIs enriched was introduced, `Hsapiens_Module_19', which was the most statistically significant with the P-value of 1.83E-17 among all PPIs, was mainly involved in the collagen group (COL1A1, COL1A2, COL2A1, COL9A1, COL9A2, COL9A3, COL10A1, COL11A1 and COL11A2) (23), the matrix

Database, Vol. 2016, Article ID baw127

Page 5 of 8

metalloproteinase group (MMP2, MMP9 and MMP13) (24) and the Fibrillin-TGFbeta receptor group (causing overgrowth syndromes, including FBN1, FBN2, TGFb1 and TGFb2) (25). The above functional and enrichment analysis results, including classical genetic and epigenetic modifications, is consistent with previous findings that skeletal system development, appendage development, limb development, appendage morphogenesis and limb morphogenesis are related to genetic skeletal disorders.

Mutation annotation
`SkeletonGenetics' performed the detailed annotation information of all mutations to facilitate the users to assess the regarding interest mutations. Firstly, the coordinates of variations, such as SNV, (FBN1, NM_000138 and c.5198G > A) or InDels, (COL2A1, NM_033150 and c.4234_4245del) were converted to the corresponding coordinates on the human reference genome GRC37/hg19 (chr15:48755305-48755305) and (chr12:48367202-4836 7213) by UCSC Genome Browser (26) and the in-house perl program was used to convert coordinates from CDS to genome. Secondly, the general annotation of mutations, such as the effects on protein coding (frameshift, nonframeshift, non-synonymous, splicing, stopgain, stoploss, etc.), amino acid change and the location of the mutation (exonic, intronic, intergenic, region, etc.) were performed by ANNOVAR (27). Additionally, more detailed clinical information was provided about each entry, including PubMed ID, ethnicity, gender (male or female), age-ofonset (death, newborn, days, weeks, months, years) and hereditary mode. Another 27 databases or data sets were linked and annotated, such as seven quick links (NCBI, HGNC, MGI, OMIM, Ensembl, Vega and GeneCards), 15 functional prediction software, dbSNP (28) and 1000 Genomes Project (29), ESP, CG69, ExAC. Phenotype information extracted from the HPO databases and provided the OMIM ID, MGI ID, phenotype or syndrome name (such as WeillMarchesani syndrome 2, dominant), phenotype description (related to search module phenotype button), cosmic70, clinvar_20150330 information, etc.

Therefore, in order to facilitate the search for mutation information and statistics on the bias of the mutation position, we used scalable vector graphics (SVG) to visualize the mutation distribution in each GSD-gene for related syndromes, each simulated fonts including gene position information, gene name (number of exons, transcript ID), and encoded information, with different colors to represent different mutation effects or types, which presented a gene level overview of the summarized mutations. For example, the syndrome of achondrogenesis type 2 (ACG2; LangerSaldino), can be expressed as chr12:48 367 189-48 398 104, Gene: COL2A1 (54 coding exons, NM_001844 or NM_033150) and mutations in exonic or intronic, the variations were more than once distinguished by different colors and fonts from those first identified. Besides, SVG was used to construct a graphical gene-disease network to provide the potential relations of GSD and skeletal-related genes for understanding the complex heterogeneity of GSD. Information about the genedisease network includes the number of GSD genes, mutation information and different disease phenotypes. For example, the COL2A1 related to nine common genetic skeletal disorders, including achondrogenesis type 2 (ACG2; Langer-Saldino), platyspondylic dysplasia, Torrance type, hypochondrogenesis, spondyloepiphyseal dysplasia congenital (SEDC), spondyloepimetaphyseal dysplasia (SEMD) strudwick type, kniest dysplasia, spondyloperipheral dysplasia, mild SED with premature onset arthrosis, SED with metatarsal shortening (formerly Czech dysplasia), stickler syndrome type 1, are depicted by a simple ball (red ball represents gene and blue ball represents disease) and a straight line to construct the genedisease network. Users click on the corresponding graphics and can quickly link to detailed information on mutations and phenotypes.

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Data search and browse
`SkeletonGenetics' provides a quick and concise search box on the home page for searching by five symbols. Firstly, there are three gene symbol search modules, `gene symbol', `gene ID' and `gene transcript'; secondly, mutation position and phenotype information were incorporated to allows users to search by (i) specifying options like gene name (capital letters), gene ID, transcript information (ii) investigating the specified phenotype of the typical skeletal changes related to the position of the forearm of the upper limb, to the description of the variability to the extent of skeletal changes (iii) search mutation position data of more than one gene or mutation, or when it is known that a mutation or gene is located on a particular area or chromosome, a position symbol list will be needed to achieve this fuzzy search. To facilitate users browsing the data, two

Mutation spectrum and genedisease network
The location of the gene mutation is biased, some of which are located in the 50 -UTR region, 30 -UTR region or the mutation-rich region, and some are distributed in single mutation sites, such as 1138G > A mutation in FGFR3 of achondroplasia (30) and 49G > A mutation in AKT1 of proteus syndrome (31), which accounted for 98% and 100% of the total number of mutations, respectively.

Page 6 of 8

Database, Vol. 2016, Article ID baw127

different approaches are provided: (i) browse by disorders (ii) browse by chromosome (Figure 2). The `browse by disorders' option provides 42 groups and 481 conditions of genetic skeletal disorders for users to conveniently retrieve the information about mutations of interest. The genes and mutations related to this group or disease conditions can be easily retrieved by selecting from the list. Additionally, users can browse all the variants that are mapped on the entry chromosome or chromosomal bands in a graphical way in `browse by chromosome', which is linked to the mutations information page.

provide a user-friendly web interface for searching and browsing. Meanwhile, the database was organized in two different table output formats by ANNOVAR software (27) and GOs annotation, KEGG pathways, Wikipathways, MicroRNA Target and ProteinProtein interaction were stored in separate tables. All data can be freely downloaded from the website (Download page). The web client has been successfully tested with Internet Explorer 10, Chrome 48.0, Safari 7.1 and Firefox 2/3 and is implemented independently of the operating system.

Results and discussion
Database organization and web interface
In `SkeletonGenetics', all the data were stored and managed in a MySQL relational database and run on an Apache HTTP server by PHP and JavaScript program to The assignment of genetic skeletal disorders into specific groups has been practiced since the previous versions of the `Nosology' by biochemical, molecular information available, or the group of disorders with similar

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Figure 2. A screenshot of the search, browse and annotation module in `SkeletonGenetics'. Search box at home page for searching by five symbols. `gene symbol', `gene ID' and `gene transcript', `mutations position' and phenotype information. `Browse by chromosome' is used to retrieve all GSDrelated genes mapped on chromosomes, `Browse by disease' is used to retrieve all GSD-related genes'. Annotation module including functional and enrichment analysis, mutation annotation, mutation spectrum and genedisease network.

Database, Vol. 2016, Article ID baw127

Page 7 of 8

phenotypic features defined by common radiographic features or anatomical site. Meanwhile, researchers criticize the previous versions, focusing on their `hybrid' nature, which does not stick to a single systematic approach. Firstly, disorders should be classified on phenotypic similarities. Secondly, they should be reclassified based on the pathway or gene related to the functional abnormality (4). Based on the above principles, and as more and more resources are published on the network resource, we developed a comprehensive database for genes and mutations related to genetic skeletal disorders, `SkeletonGenetics' integrated data types associated with GSD through indepth mining of 1698 publications and extensive functional analysis, which covered a broad range of data including lists of disease grouping and disease names, genes, mutations and mutations spectrum, GO terms, pathways, microRNA target, proteinprotein interaction and genedisease network. Meanwhile, combined with concise search methods, intuitive graphical displays, convenient browsing functions and constantly updatable features, the `SkeletonGenetics' database could serve as a reclassified reference tool and valuable resource for unveiling the genetic and pathway basis of GSD. With the development of the high-throughput sequencing, massive genetic skeletal disorder related genes and mutations have been identified in the past decade, but there is still about 3040% of GSD with undiscovered disease genes (4, 6) because of the restriction of patients and complexity of the gene interaction network. In this study, functional analysis of the known GSD genes in publications mostly involved in specific GO items, such as skeletal system development, appendage development, limb development, appendage morphogenesis and limb morphogenesis or pathways, such as classical FGFs, TGF-beta, Hedgehog, WNT, Notch signaling pathways. Meanwhile, many of the new identified genes interact with known GSD genes (32 35) or key GSD genes could induce the disease state. In SkeletonGenetics, if researchers have found a specific GSD gene, they may link to the `Analysis' page to see whether the new gene is involved in any skeletal-related GO terms, pathways or PPIs. This database would be important and useful for revealing novel GSD-related genes and pathways. Therefore, researchers could focus on the other unknown genes, which were involved in the same skeletal development and homeostasis functions module combined with biochemical, molecular information or the group of disorders with similar phenotypic features. The increasing availability of massive parallel sequencing and other new sequencing technologies will likely result in a rapid and cost-effective identification of many GSDcausing genes and mutations, or novel phenotypes associated with mutations in genes already linked to other

phenotypes. Automatic mining methods will be used in `SkeletonGenetics' for updating GSD-related data, (1) collect the latest disorders, genes or mutation from PubMed or open databases related to GSD; (2) perform more extensive functional and enrichment analysis based on the updated data sets; (3) improve the mutation spectrum, genedisease network and other database functionalities. In SkeletonGenetics, researchers may use the `Submission' page to upload de novo mutations for GSD or new genetic skeletal disorders to keep the database up-to-date and comprehensive. We believe that `SkeletonGenetics' will hopefully have paved the way by setting standards for the recognition and definition of skeletal phenotypes and understanding of the complex heterogeneity of GSD and hope that the continued efforts to improve `SkeletonGenetics' will ultimately help improve diagnosis and treatment of genetic skeletal disorders.

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Supplementary data
Supplementary data are available at Database Online.

Acknowledgements
We thank Drs Lili Zhou, Xiangnan Chen, Jiaojiao Lv, Ping Wang, Yunying Chen, Ke Wu, Zhaotang Luan, Manli Jia, at the Department of Genetics of Dingli Clinical Medical School, Wenzhou Central Hospital and School of Laboratory Medicine and Life Science, Wenzhou Medical University, for the data collecting and testing.

Funding
Natural Science Foundation of Zhejiang province (LY13H200002); Science and technology project of Wenzhou (Y20150092); The Ministry of Health Project (WKJ2011-2-017). This study was supported by the Natural Science Foundation of Zhejiang province in Hangzhou, China (); Science and technology project of Wenzhou in Zhejiang, China(); The Ministry of Health Project in (). Conflict of interest. None declared.

References
1. Baldridge,D., Shchelochkov,O., Kelley,B. et al. (2010) Signaling pathways in human skeletal dysplasias. Annu. Rev. Genomics. Hum. Genet., 11, 189217. 2. Krakow,D. and Rimoin,D.L. (2010) The skeletal dysplasias. Genet. Med., 12, 327341. 3. Mortier,G.R. (2001) The diagnosis of skeletal dysplasias: a multidisciplinary approach. Eur. J. Radiol., 40, 161167. 4. Warman,M.L., Cormier,D.V., Hall,C. et al. (2015) Nosology and classification of genetic skeletal disorders: 2015 revision. Am. J. Med. Genet., 167A, 28692892. 5. Rousseau,F., Bonaventure,J., Legeai,M.L. et al. (1996) Clinical and genetic heterogeneity of hypochondroplasia. J. Med. Genet., 33, 749752.

Page 8 of 8 6. Geister,K.A. and Camper,S.A. (2015) Advances in Skeletal Dysplasia Genetics. Annu. Rev. Genomics Hum. Genet., 16, 199227. 7. Bernier,F.P., Caluseriu,O., Ng,S. et al. (2012) Haploinsufficiency of SF3B4, a component of the pre-mRNA spliceosomal complex, causes Nager syndrome. Am. J. Hum. Genet., 90, 925933. 8. Twigg,S.R. and Wilkie,A.O. (2015) A geneticpathophysiological framework for craniosynostosis. Am. J. Hum. Genet., 97, 359377. 9. Fokkema,I.F., Taschner,P.E., Schaafsma,G.C. et al. (2011) LOVD v.2.0: the next generation in gene variant databases. Hum. Mutat., 32, 557563. 10. Ko  hler,S., Doelken,S.C., Mungall,C.J. et al. (2014) The Human Phenotype Ontology project: linking molecular biology and disease through phenotype data. Nucleic Acids Res., 42, 966974. 11. Hamosh,A., Scott,A.F., Amberger,J.S. et al. (2005) Online Mendelian Inheritance in Man (OMIM), a knowledgebase of human genes and genetic disorders. Nucleic Acids Res., 33, 514517. 12. Wang,J., Duncan,D., Shi,Z. et al. (2013) WEB-based GEne SeT AnaLysis Toolkit (WebGestalt): update 2013. Nucleic Acids Res., 41, 7783. 13. Zhang,B., Kirov,S., and Snoddy,J. (2005) WebGestalt: an integrated system for exploring gene sets in various biological contexts. Nucleic Acids Res., 33, 741748. 14. Pacak,C.A. and Cowan,D.B. (2014) Growth of bone marrow and skeletal muscle side population stem cells in suspension culture. Methods Mol. Biol., 1210, 5161. 15. Hadzhiev,Y., Lele,Z., Schindler,S. et al. (2007) Hedgehog signaling patterns the outgrowth of unpaired skeletal appendages in zebrafish. BMC Dev. Biol., 7, 75. 16. Rodriguez,L.J., Tomas,A.R., Johnson,A. et al. (2013) Recent advances in the study of limb development: the emergence and function of the apical ectodermal ridge. J. Stem Cells, 8, 7998. 17. Rahman,M.S., Akhtar,N., Jamil,H.M. et al. (2015) TGF-b/BMP signaling and other molecular events: regulation of osteoblastogenesis and bone formation. Bone Res., 3, 15005. 18. Pan,A., Chang,L., Nguyen,A. et al. (2013) A review of hedgehog signaling in cranial bone development. Front. Physiol., 4, 61. 19. Rudnicki,M.A. and Williams,B.O. (2015) Wnt signaling in bone and muscle. Bone, 80, 6066. 20. Zanotti,S. and Canalis,E. (2013) Notch signaling in skeletal health and disease. Eur. J. Endocrinol., 168, 95103.

Database, Vol. 2016, Article ID baw127 21. Krakow,D., Robertson,S.P., King,L.M. et al. (2004) Mutations in the gene encoding filamin B disrupt vertebral segmentation, joint formation and skeletogenesis. Nat. Genet., 36, 405410. 22. Van,B.H., Celli,J., van,R.J. et al. (2005) MYCN haploinsufficiency is associated with reduced brain size and intestinal atresias in Feingold syndrome. Nat. Genet., 37, 465467. 23. Forlino,A. and Marini,J.C. (2015) Osteogenesis imperfecta. Lancet, 15, 16571671. 24. Alameddine,H.S. (2012) Matrix metalloproteinases in skeletal muscles: friends or foes? Neurobiol. Dis., 48, 508518. 25. Zhao,F., Pan,X., Zhao,K. et al. (2013) Two novel mutations of fibrillin-1 gene correlate with different phenotypes of Marfan syndrome in Chinese families. Mol. Vis., 19, 751758. 26. Speir,M.L., Zweig,A.S., Rosenbloom,K.R. et al. (2016) The UCSC Genome Browser database: 2016 update. Nucleic Acids Res., 44, 717725. 27. Wang,K., Li,M., and Hakonarson,H. (2010) ANNOVAR: functional annotation of genetic variants from high-throughput sequencing data. Nucleic Acids Res., 38, e164. 28. Sherry,S.T., Ward,M.H., Kholodov,M. et al. (2001) dbSNP: the NCBI database of genetic variation. Nucleic Acids Res., 29, 308311. 29. 1000 Genomes Project Consortium (2012) An integrated map of genetic variation from 1,092 human genomes. Nature, 491, 5665. 30. Horton,W.A., Hall,J.G., and Hecht,J.T. (2007) Achondroplasia. Lancet, 370, 162172. 31. Lindhurst,M.J., Sapp,J.C., Teer,J.K. et al. (2011) A mosaic activating mutation in AKT1 associated with the Proteus syndrome. N. Engl. J. Med., 365, 611619. 32. Cho,T.J., Lee,K.E., Lee,S.K. et al. (2012) A single recurrent mutation in the 5'-UTR of IFITM5 causes osteogenesis imperfecta type V. Am. J. Hum. Genet., 91, 343348. 33. Yamamoto,G.L., Baratela,W.A., Almeida,T.F. et al. (2014) Mutations in PCYT1A cause spondylometaphyseal dysplasia with cone-rod dystrophy. Am. J. Hum. Genet., 94, 113119. 34. Schmidts,M., Vodopiutz,J., Christou,S.S. et al. (2013) Mutations in the gene encoding IFT dynein complex component WDR34 cause Jeune asphyxiating thoracic dystrophy. Am. J. Hum. Genet., 93, 932944. 35. Martin,C.A., Ahmad,I., Klingseisen,A. et al. (2014) Mutations in PLK4, encoding a master regulator of centriole biogenesis, cause microcephaly, growth failure and retinopathy. Nat. Genet., 46, 12831292.

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Received July 27, 2016, accepted August 10, 2016, date of publication September 7, 2016, date of current version November 18, 2016.
Digital Object Identifier 10.1109/ACCESS.2016.2605759

Interactive Reference Region Based
Multi-Objective Evolutionary Algorithm
Through Decomposition
RUOCHEN LIU, (Member, IEEE), RUINAN WANG, WEN FENG, JUNJUN HUANG,
AND LICHENG JIAO, (Senior Member, IEEE)
Key Laboratory of Intelligent Perception and Image Understanding Key of Ministry of Education, International Research Center for Intelligent Perception and
Computation, Xidian University, Xi’an 710071, China

Corresponding author: R. Liu (ruochenliu@xidian.edu.cn)
This work was supported in part by the National Natural Science Foundation of China under Grant 61373111, Grant 61272279,
Grant 61672405, and Grant 61203303, in part by the Fundamental Research Funds for the Central Universities under
Grant K50511020014, Grant K5051302084, Grant K50510020011, Grant JB150227 and JBG160229,
and in part by the Provincial Natural Science Foundation of Shaanxi of China under Grant 2014JM8321.

ABSTRACT Many evolutionary multi-objective optimization (EMOs) methodologies have been proposed
and shown a great potential in approximating the entire Pareto front. While in real world, what decision
makers (DMs) want is one or several solutions to satisfy their requirements. It has become a hot problem
that dynamically using preference information provided by DMs during the optimization process guides
the search of EMO algorithms. An interactive reference region-based evolutionary algorithm through
decomposition is proposed, denoted as RR-MOEA/D in this paper, which focuses the search on the desire of
DMs to save computational resources. MOEA/D, as a well-known multi-objective optimization algorithm,
is used as a basic framework here. In MOEA/D, by dealing with the sub-problems in the preference region
and ignoring uninterested ones, the solutions obtained can converge to the regions which the DM prefers
on the Pareto front and the computational complexity can be saved to a great extent. At each interaction, a
humanized and simple interactive condition is adopted so that the reference region can be changed in a very
intuitive way if the DM is unsatisfied the results in the interactive process. A rapid interaction is designed
and a set of rough solutions can be obtained quickly whenever the preference information is changed. The
proposed algorithm is tested on several benchmark problems and the experimental results show that the
proposed algorithm can take full use of preference information and successfully converge to the reference
region due to its reasonable and simple interaction mechanism.
INDEX TERMS Interactive preference, multi-objective optimization problem, reference region selection
mechanism.

I. INTRODUCTION

Evolutionary multi-objective optimization algorithms (EMOs)
have been widely applied in many engineering areas. Multiple objectives with conflicts make it impossible to get a
single optimal solution which can optimize all the objectives
at the same time. As a result, the optimal solutions obtained
by EMOs are trade-offs, known as Pareto optimal solutions,
which are characterized by that some improvement in one
objective must lead to deterioration in at least one other
objective.
Without incorporating preference information, an approximation to the entire Pareto front (PF) is obtained by most

VOLUME 4, 2016

EMOs. However, in many practical applications, what a
decision maker (DM) requires may be a single solution or
a set of solutions in a preferred region instead of entire
PF. It is very time-consuming and unnecessary to obtain
the whole PF which DMs are not interested in. Meanwhile,
many preference information based EMOs have been proposed to select the most desirable solutions on the Paretooptimal front, and those can be mainly divided into three categories: prior, interactive and posterior. The apriori approaches
require the DM provides his/her preference information in
advance. Often this preference information is used as a variant of selection mechanism to substitute or compose the

2169-3536 
 2016 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

7331

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

tradition one. A disadvantage of these apriori approaches lies
in that it is very difficult and confused for a DM to express
his/her preference without related knowledge of available
solutions or structure of the search space. On the other
hand, for those posterior approaches, the DMs can choose
those preferred solutions on the basis of the convergence
and spread of solutions. Disadvantages and criticisms for the
posterior methods include high computational costs and time
consumption, and it will become more difficult when dealing with many-objective problems (more than three objectives). The interactive, as one of the preference methods,
is a new trend. It can be described as a DM dynamically
guides the searching process by providing their reference
information in an interactive way until he/she is satisfied with
the outcome. The interactive methods allow DMs to guide
searching and modify their preferences at various stages
during the optimization process. Moreover, ignoring those
regions with little interest and applying reference information as guidance can effectively reduce the computational
costs.
Multi-objective evolutionary algorithm based on decomposition (MOEA/D) [1] is selected as a basic MOEA framework in this paper. MOEA/D decomposes a multi-objective
problem (MOP) into multiple single-objective subproblems
in which each subproblem is a different aggregation of all
the objectives. From this viewpoint, the aggregation can
be described as different preference of objectives, in other
words, different subproblems are generated under different preference condition. Therefore, it is a convenient way
to find the regions where the DM is interested by solving the corresponding subproblems according to the DM’s
preferences.
In this paper, we propose an interactive reference region
based multi-objective evolutionary algorithm through decomposition (denoted as RR-MOEA/D), which focus the search
on the desire of DMs to save computational resources. Once
the preference is given by a DM at interactive stage, the proposed algorithm reduces the computational cost by ignoring
the regions the DM is not interested in and only optimizing the
subproblems the DM prefers. Whereas during optimization
process, weight vectors of subproblems preferred is adapted
dynamically to avoid the solutions we obtain flying away
from the preferred regions. At each interaction, our algorithm
provides a set of current solutions to the DM and the reference
region can be changed in a very intuitive way if the DM
is unsatisfied. Therefore, a rapid interaction is designed in
this paper. A set of rough solutions can be obtained quickly
whenever the preference information change.
The major advantages of the proposed RR-MOEA/D algorithm are enumerated as follows:
1) Reduce the amount of calculation and save the computing resources by focusing the search on the preferred regions;
2) It needs not to calculate any achievement scalarizing
functions (ASF) [2] used in other preference mechanisms;
3) A simple and intuitive way to set the reference information the DM prefers;
7332

4) A fast way to get rough solutions in the reference regions
can avoid wasting time in searching Pareto-optimal solutions
in regions the DM is not preferred;
5) Giving uncertain and inconsistent human decisions as
the DM does in real life are allowed and changing the preference information in the interaction stage is permitted.
The remainder of this paper is structured as follows.
Section 2 introduces the related background. The proposed
algorithm is presented in Section 3 in details. Section 4
presents experimental results and validates the performance
of the proposed algorithm. Section 5 concludes the paper.
II. RELATED BACKGROUND
A. MULTI-OBJECTIVE OPTIMIZATION

Generally, a minimized multi-objective
problem (MOP) can be defined as follows:

optimization

min F(x) = (f1 (x), f2 (x), . . . , fm (x))T
s.t. x ∈ 

(1)

where x = (x1 , x2 , . . . , xn ) is the decision vector, n is the
dimensionality of the decision space, and  is the feasible
region in the decision space. F(x) is a set of objective function
consisting of m objective functions.
Very often, since the objectives in Eq. (1) conflict each
other, it is impossible to get a single optimal solution which
optimizes all the objectives at the same time. As a result, the
optimal solutions we obtained are trade-offs, known as Pareto
optimal solutions. Some important related terms are stated as
follows:
Definition 1 (Pareto-Dominance): Let xa , xb ∈ , xa is
said to Pareto dominate xb , denoted as xa  xb if and only if:
(
∀i = 1, 2, . . . m, fi (xa ) ≤ fi (xb )
(2)
∃j = 1, 2, . . . m, fj (xa ) < fj (xb )
Definition 2 (Pareto-Optimal): A point is Pareto optimal to
Eq. (1) if:
¬∃x ∈  : x  x∗

(3)

The Pareto-optimal set (PS) consists of all the Paretooptimal solutions and the set of all the Pareto optimal objective vectors is called Pareto front (PF) [3].
B. MULTI-OBJECTIVE OPTIMIZATION BASED
ON DECOMPOSITION

Decomposition is a basic strategy in traditional multiobjective optimization. Relying on a decomposition strategy such as the weighted-sum method [3], the Tchebycheff
approach [3] or the boundary intersection approach [4], [5],
a multi-objective problem can be converted into a number of single-objective problems. In 2007, Zhang et al. [1]
proposed a decomposition algorithm based evolutionary
multi-objective algorithm using the Tchebycheff as the
decomposition strategy. Since we use the Tchebycheff
approach in our study, a brief introduction of the Tchebycheff
approach is given in the following.
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

The Tchebycheff method plays an important role not only
in converting a multi-objective problem into a number of
single-objective problems [3] but also in locally approximating the underlying preference utility function [6]. Each subproblem is obtained by using the following equation [1], [3]:
	

 
minimize gte (x λ, z∗ ) = max λi fi (x) − z∗ 
1≤i≤m

subject to x ∈ 

(4)

Where, for i ∈ {1, 2, . . . , m}, z∗i is the ideal point, i.e.,
z∗i = min {fi (x) |x ∈  }. For any weight vector λ, the optimal
solutions of Eq. (4) corresponds to a Pareto-optimal solution
of Eq. (1). All the optimal solutions obtained can approximate
to the entire PF with proper weight vectors.
As shown in Eq. (4), each subproblem can be translated
as a different aggregation of all the objectives and different
aggregation could be constructed as the different preference
for objectives. By setting proper weights, solutions desired by
DMs can be obtained by solving those related subproblems
in the preference regions. Therefore, associating preference
method with decomposition is a very natural thought. As the
most common decomposition based multi-objective evolutionary algorithms, MOEA/D [1] has shown great potential.
Here is a brief introduction of MOEA/D and its variants.
MOEA/D uses the Tchebycheff method as a decomposition method and converts a multi-objective problem into a set
of subproblems and optimizes all the subproblems simultaneously in a single run. Different subproblems in MOEA/D have
different weight vectors. For each weight vector, a neighborhood is defined as a set of its several closest weight vectors.
Offspring of each subproblem is generated by using some
genetic operators and its parents are randomly selected from
its neighborhood. The solution of each subproblem is the best
solution found so far in the neighborhood. That is to say, each
subproblem optimized by its neighboring subproblems only
obtain its optimal solution in evolving process, which makes
that MOEA/D has such advantages as low computational
complexity and fast convergence rate. MOEA/D-DE [7] is
an improved version of MOEA/D, which has three improvements: 1) using a differential evolution (DE) [8] operator
instead of SBX operator [9] in updating process; 2) setting a
probability that parent solutions are selected from the neighborhood; 3) setting a upper limit number of solutions in
neighborhood replaced by each child solution. All these three
improvements aim at maintaining the population diversity.
Since our algorithm works only in reference regions and
has a small population size, keeping diversity is an important task. So, the proposed algorithm is mainly based on
MOEA/D-DE in fact. As another new version of MOEA/D,
MOEA/D-M2M [10] converts a multi-objective problem into
several simple multi-objective problems. For each subproblem, any Pareto based EMOs can be used. MOEA/DD [11]
suggests a hybrid paradigm which combines decomposition
and dominance together. Most recently, an inverse model
based MOEA (IM-MOEA) [12] is proposed, which uses a
set of reference vectors to partition the objective space into a
VOLUME 4, 2016

number of subspaces and then the inverse models which maps
the objective vectors into the decision vectors are built in
each subspace to sample new solutions. More recently, a reference vector guide evolution algorithm for many-objective
optimization (REVA) [13] is proposed, in which reference
vectors is used to decompose the original multi-objective
optimization problem into a number of single-objective subproblems. Additionally, a scalarization approach, termed the
angle penalized distance, is adopted to balance convergence
and diversity of the solutions in the high-dimensional objective space. NSGA-III [14] combines a decomposition strategy
in MOEA/D [1] with non-dominated sorting approach from
NSGA-II [15], and has been able to successfully find a wellconverged and well-diversified set of solutions for manyobjective problems.
C. PREFERENCE INFORMATION BASED EMO

The goal of evolutionary search is to help that the DM
selects the solution which best matches his/her preferences
within a representative set of Pareto optimal solutions. Various preference-based EMOs have been proposed in the last
decade. Two surveys were provided in this topic [16], [17].
EMOs are categorized apriori, interactive, or aposteriori algorithms according to the treatment of preference. An apriori
approach requires the preference information provided by
the DM before the optimization stage and the evolutionary
search is guided by the preference information. On the other
hand, a posteriori approach firstly needs to approximate to
the whole Pareto front, and then the DM selects their preferred solutions. With the character of the user involving in
the optimization process, the interactive approaches offer a
chance for the user to know about the structure of the search
space and express his /her authentic preference correctly,
and computational costs is reduced largely by progressively
focusing on the most relevant areas and ignoring regions of
little interest. We will give a simple review about the three
approaches in the following section.
Some typical apriori methods are listed here:
Wierzbicki [18] proposed a reference point based optimization of classical multi-criterion decision making algorithm
which combines three approaches in a single multi-objective
optimization framework and enables the DMs to refine the
problem definition and to reduce the size of the objective
space iteratively. Jaszkiewicz and Slowinski [19] proposed
a light beam search procedure which connects the reference point with tools of multi-attribute decision analysis.
This algorithm tries to overcome the inconvenience when
the DM chooses the candidates for the best compromise.
Deb et al. [20] hybridized the reference direction strategy
with NSGA-II, in which, a reference direction can be determined by setting a starting point and a reference point,
and reference solutions along the reference direction on PF
can be obtained. Deb and Kumar [21] combined a reference
point based reference information with NSGA-II in order
to find a set of solutions instead of a single optimal solution and used a modified crowding distance to control the
7333

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

density of solutions near reference points. Thiele et al. [22]
proposed a preference based evolutionary approach which
directly uses reference point information in fitness evaluation.
Recently, Molina et al. [23] proposed another definition of
Pareto dominance called g-dominance, in which, solutions
satisfy all aspiration levels and solutions fulfilling none of the
aspiration levels are preferred over those solutions satisfying
some aspiration levels. All these algorithms mentioned above
belong to the apriori method.
For the posteriori approach, in fact, many famous EMOs
like MOEA/D [1], NSGA-II [15], SPEA-II [24], NNIA [25],
and so on belong to it. After those algorithms obtain a
set of compromised solutions which are closest to the true
Pareto front and the DMs pick out the solutions they are
interested in.
An interactive process aims to integrate the preference
information provided by DMs into algorithm in an interactive
way during the optimizing process. That is to say, some tradeoff factors expressing the desires of the DMs are added into
the optimization process to guide the latter search. According
to [26], Tanino et al. [27] suggested the earliest attempt to
incorporate the preference information expressed iteratively
by the DM to evaluate solutions. Fonseca and Fleming [28]
allowed the user to specify aspiration levels in form of a
reference point, and used it to modify the EMO’s rank scheme
in order to focus the search. Kita et al. [29] interleaved generations of a Pareto ranking based evolutionary algorithm
with the rank of the solutions provided by a DM. Todd and
Sen [30] also tried to learn the user’s utility function, instead
of only considering linear weightings of objectives, they
used the preference information provided by the DM to train
an artificial neural network, which is then used to evaluate
solutions in the evolutionary algorithm. Parmee et al. [31]
described the concept of an interactive evolutionary design
system which is related to multi-objective information gathering and subsequent design space redefinition. Instead of
using linear programming to derive a weighting of the objectives most compatible with the pairwise comparisons as in
[32], Phelps and Kö ksalan [33] used two evolutionary algorithms, one is to find the solutions, and another is to determine the most compatible ranking. Kamalian et al. [34] suggested using a posteriori evolutionary multi-objective optimization followed by an interactive evolutionary algorithm.
Reference [35] allowed the user to modify the Pareto ranking computed automatically by changing the rank of some
of the solutions. Thiele et al. [36] also used DM’s preferences interactively expressed in the form of reference points.
Avigad and Moshaiov [37] presented a new interactive concept based multi-objective evolutionary algorithm to handle
Pareto-directed IC-MOPs. Said et al. [38] described an interactive run of r-NSGA-II algorithm in which is changed for
a certain number of generations freely determined by the
DM, and if the running result the DMs are dissatisfied, they
can change the reference point. Deb et al. [39] presented a
progressively interactive EMO approach which applied the
constructed value function to search more preferred solutions.
7334

And an interactive evolutionary algorithm can contribute
to designing elegant object-oriented software which was
discussed by Simons and Parmee [40]. Sun et al. [41] proposed a new surrogate-assisted interactive genetic algorithm
to exploit the uncertainty of subjective fitness evaluations
both in training the surrogates and in managing surrogates.
Ruiz et al. [42] developed an interactive WASF-GA which
is based on a preference-based evolutionary multiobjective
optimization algorithm called WASF-GA [43], it required
the DM to provide preference information at each iteration
simply as a reference point and then the desired number of
solutions is generated to represent the region of interest of the
Pareto optimal front associated to the reference point given.
Among the interactive algorithms, those algorithms based
on decomposition strategy are very few. We just found one
references on this topic. iMOEA/D [44] is an interactive
version of the MOEA/D algorithm [1], in which the preferred
solutions are presented to the decision maker at intermediate generations, then, the searching process is guided to
the neighbor regions of the preferred solution. It is obvious
that both iMOEA/D and the proposed algorithm are based
on MOEA/D although there are some little differences like
initiation and updating of population. The biggest differences
between the two algorithms lie in the way of determining preferred region and interactive method. In iMOEA/D,
a generic polynomial utility function [45] is used to represent
the human DM. During the stage of interactive, P current
solutions are randomly presented to DM for choosing their
favorite one; and the selected solution becomes the center
of preferred weight region in the following optimization process. The detailed descriptions of about the way of determining preferred region and interactive method in the proposed
algorithm is presented in Section 3.
III. THE PROPOSED APPROACH

In this section, the proposed reference region based multiobjective evolutionary algorithm through decomposition, is
presented in details. If the final aim is to choose and implement a solution, then the goal of applying a multi-objective
optimization method is to find a single, most preferred, final
solution. However, in some cases, it may be preferable to find
a set of solutions instead of one. This may be particularly true
in case of robustness considerations when some aspects of
uncertainty, imprecision or inconsistency in data or model are
to be taken into account [26]. For most of preference based
multi-objectives evolution algorithms, the preference information is treated as a variant of special selection mechanism
to substitute or compose the traditional selection process [46],
and the whole population converges close to the reference
point or reference direction [44]. MOEA/D-M2M [10] converts a multi-objective problem into several simple multiobjective problems based on PF or PS shapes and searches
and optimizes each simple multi-objective problem respectively. Inspired by all these, we consider that a reference
region can be treated as a simple multi-objective problem
and thus the search is focused on solving this problem only.
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

As a result, a reference region based preference mechanism is
designed in which the population evolves only in this region
where the DM desires. Provided the reference information, it
is possible to find the preference regions and then initialize
the individuals in the preference regions and search in the
region until approximating properly to the Pareto front. At the
initial stage, MOEA/D [1] is used to generate the initial population. When the preference information is given, namely
one or several reference regions are determined and those new
population are formed in the preference regions corresponding to some subproblems. Whereas in the evolution process,
the subproblems are updated by using MOEA/D-DE until
the DM is satisfied. Here it should be emphasized that each
individual solution belongs to different corresponding subproblem and each subproblem corresponds to certain weight.
Thus collective and dynamic adapting the weight vectors in
the course of evolution can ensure not only the distribution
range but also the convergence in the preference region. At
the interactive stage, a humanized and simple interactive
condition is adopted in this paper. A set of rough solutions are
presented to DM when the interactive condition is met. If the
DM is satisfied with the present solutions, the process would
be terminated; otherwise, the decision maker could change
his/her prefer information, a similar process is started until
the DM obtain the satisfied solutions. Therefore, the DM can
control the preference information in a very intuitive and fast
way.
A. PREFERENCE REGION

The first challenge in the proposed approach is how to determine the preference region. As we know, interaction involves
the DM in the process of optimization for dynamically offering preference information. And the DM dynamically guides
the algorithm to search in the preference region in the interaction processing. Therefore, an effective method should be
found for determining the preference region when the interaction happens.
As a basic algorithm framework in this paper, MOEA/D [1]
is characterized by 1) each subproblem is a independent
single-objective optimization problem and the solutions of
all the subproblems form the entire population; 2) all subproblems in MOEA/D spread uniformly in objective space
and each subproblem and its neighbor subproblems form a
preferred region. That is to say, finding the optimal subproblem which is nearest to the reference direction or the
reference point and these individuals in its neighbor can form
a preference region. In this paper, those reference directions
are selected as the preferred information which are marked
by owning more intuitive and better linked with weight vectors. As a result, the preferred regions can be determined
by two factors in the proposed algorithm: one is the searching direction which is determined by an aspiration and a
reservation point provided by DMs. another is radius which
determines the range of reference region. So, once a DM
offers a searching direction, we can find the optimal weight
vector close to the searching direction and obtain the neighbor
VOLUME 4, 2016

FIGURE 1. The illustration of determining the preference region.

of the optimal subproblem by computing the Euclidean distances between the optimal weight vector and other weight
vectors. By setting the radius and reasonably initializing the
weight vectors in this range, we can get subproblems in the
preference region. Fig. 1 shows this process visually. Here,
we can see that the vector determined by aspiration point and
reservation point is just what we define as the reference direction. The radius determines the scope of reference region, and
then we can find the optimal vector and other subproblems in
this reference region.
Finding an optimal weight vector close to the searching
direction is not trivial. In this paper, we employ the basic
MOEA/D [1] to generate the initial population. To determine
optimal weight vector, we connect each individual to the
aspiration point so as to form a vector and compute the Cosine
value between this vector and the searching direction:
cos(a, r) =

a•r
|a| × |r|

(5)

where vector a is obtained by connecting individual x with the
aspiration point and vector r is the searching direction. The
bigger the Cosine values, the closer to the searching direction
the individual is. We can select the subproblem’s weight
vector with the biggest Cosine value as the optimal weight
vector and re-initialize all the subproblems in the reference
region. Algorithm 1 shows the process of the initializing the
weights and finding initial population in the reference region
in details.
The human decision is characterized by uncertainty and
inconsistency [45]. In the proposed algorithm, the reference
information and the size of population are given by the decision maker. It is a common thing that the parameters the
DM inputs might not well match with each other. Therefore, we should take account of various circumstances. If
the number of subproblems (population size) is too small to
cover entire preferred region, the subproblem in the region
most close to the reference direction will become a priority
(Algorithm 1: line 8-9). In this paper we set the parameter
penalty to 1.5 to control the range of small population (it
can also be changed in some specific application). Otherwise,
7335

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

Algorithm 1 REFREGION
Interation Parameters:
num_v: the number of reference directions
PF: initial population
N: the size of population close to each reference direction
start: the vector of num_v aspiration points
fend: the vector of num_v reservation points
radius: determine the size of reference region
Output:
D0 : initial population in reference region
W0 : initial weight vectors in reference region
1. ref _dir ← fend − start
2. for i = 1, 2, . . . num_v do
3. best_weight i ← max(cos _value(PF, ref _dir))
4. range_weight i ← max(euc_dis(all_weight,
best_weight) < radius)
5. L = length(range_weight i )
6. for j = 1, 2, ....L
7. calculate the interval distance dj between the jth
and the (j + 1)th weight
8.
if L > penalty∗ N (:, i)
9.
Wi ← init_weight(min_euc_dis(range_weighti ,
N (:, i)))
10. else if L < penalty∗ N (:, i) & & L > N (:, i)
11.
Wi = range_weighti
12. while L > N (:, i)
13. find out the weight w with minimum interval
distance d
14.
Wi ← delete w from Wi
15.
L– –
16. end while
17. else
18.
Wi = range_weighti
19. while L < N (:, i)
20. find out the weight w with maximum interval
distance d
21. Wi ← insert a middle weight between w and the
following weight of w in Wi
22. L++
23. end while
24. end if
25. end for
26. Di ← max(cos_value(PF, ref _dir(:, i)), N (:, i))
27. D0 ← (D1 , . . . Dnum_v ), W0 ← (W1 , . . . Wnum_v )
28. end for

the subproblems could be evenly distributed in the preference
region to satisfy requirements (Algorithm 1: line 10-23). Consequently, the algorithm can keep diversity and robustness
as well.
B. Interactive Condition

As one of the preference methods, the interactive ones allow a
DM to dynamically guide the searching process until he/she is
satisfied with the resulting outcome. Whereas a key problem
7336

is when the current solutions are presented to the DM, in
many interactive algorithms, the process of interacting with a
DM will happen periodically [45], [46]. A long time interval
between two interactions may lead to a waste of time in
searching the region the DM does not favor. On the contrary,
it is tiresome to bother the DM excessively and it is also
a tiring work to find several preferred points from a large
number of random points in the beginning. Determining when
the algorithm interacts with the DM is important. We use an
indicator to determine whether the rough result needs to be
presented to the DM which is defined as follows:
δ=

N
i
1 X fti − ft−1
i
N
ft−1

(6)

i=1

Where N is the population size in the reference region, and
fti denotes the objective value of individual i at the tth generation. In this case, the function f is one with the minimum value of the individual i. Further, δ means the average
improvement degree of function value that the current individuals are compared with their parent individuals. The precision of the solutions presented to the DM can be controlled
by setting the value of δ. In other words, the parameter δ is
determined by the DM, when the condition of derta ≤ δ is
satisfied, the interaction happens.
Let us make some detailed illustration about a few aspects
of the Algorithm 1.
Step1: The reference direction(s) must be given by the DM,
in which aspiration points are start and reservation points are
fend.
Step3 to Step4: The two steps are designed to calculate
the best weight corresponding to the reference direction and
estimate the range weight around the preference direction
simultaneously.
Step5 to Step 26 used to determine the initial population
and the initial weight according to the relationship between
the preference region weight.
C. RR-MOEA/D

The overall procedure of the proposed interactive preference region based multi-objective evolutionary algorithm through decomposition (RR-MOEA/D) is described
in Algorithm 2.
Here, we give a more detailed explanation about several
key steps.
Initialization: In order to get uniform distribution of the
initial solutions and to make sure any region the DM prefers
has enough corresponding individuals, the initial population
is produced by limited iterations of MOEA/D instead of
random initialization. In this way, we can easily obtain the
individual and its corresponding weights in the preference
region, and shift quickly into new initial population when
DM’s searching regions are changed.
Updating Strategy: The population in the reference region
is updated by using MOEA/D until the termination criteria
is satisfied. It is worth noticing that the weights need to
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

Algorithm 2 RR-MOEA/D
Step1. Initialization: run pre-determined generations
MOEA/D to generate initial population;
Step2. Parameter input: ask the DM to input his/her
preference information;
Step3. Determining reference region: determine the reference region using the preference information
according to Algorithm 1;
Step4. Updating: update the population in the preference
region by MOEA/D-DE until the stopping
criterion is meet;
Step5. Interaction: If the DM is not satisfied with reference information, go to Step2 to reset the reference
information; If the DM wants to get better results in this
reference region, go to Step4; otherwise, output the results.

be updated every generation. Updating weights needs two
indicators: update direction and update step. The update
direction is determined by the subproblem farthest from the
reference direction to the subproblem nearest from the reference direction. The update step is defined as a unit length
(Algorithm 3: line 3). It is evitable that the population may
flies away from the preferred region by dynamically adjusting
the weights. The details of updating weights are stated in
Algorithm 3.
Interaction: At this stage, the current population is presented to the DM. If the DM is unsatisfied with the result,
the preference information can be reset. A new population
is produced according to the new desire of the DM and
a mixed population combining the current population with
the initial population. The DM can make tentative decisions
before he/she finds his/her desired regions. When the reference region is determined, the DM can reset the precision of
the solutions as the stopping criteria to get the final output
results.
Algorithm 3 Updating Weights
Interation Parameters:
num_v: the number of reference directions
PF: initial population
N: the size of population close to each reference direction
start: the vector of num_v aspiration points
fend: the vector of num_v reservation points
radius: determine the size of reference region
Output:
Wt : weights after updating
1. ref _dir ← fend − start
2. for i = 1, 2, . . . , num_v do
3.
stepi ← 2 × radiusi /N i
4.
best i ← max(cos(Dt , ref _dir(:, i)))
5.
worst i ← min(cos(Dt , ref _dir(:,i)))
6.
update_directioni ← best i − worst i
7.
Wt ← update_weight(update_directioni , stepi , Wt )
8. end for
VOLUME 4, 2016

In the algorithm, the size of the neighborhood is decided by
the size of population in the reference region and the range of
reference region. If a small population or a quite small range
which the DM is interested in, all the subproblems are treated
as a neighborhood. In this case, the algorithm is simplified
due to avoiding calculating the Euclidean distances between
all of the weight vectors.
Given the preference direction(s), RR-MOEA/D can control its searching regions in which the DM is interested.
By optimizing the subproblems in those regions, those solutions can be gotten which are converged to the preferred
regions.
IV. EXPERIMENTS AND RESULTS

In order to validate the performance of the proposed algorithm, we apply it to solving two 2-objective ZDT problems
and two 3-objective DTLZ problems respectively. All experiments were carried out on a personal computer with AMD
A8-5550M APU with Radeon (tm) HD Graphics (2.10GHz)
and 4 GB of RAM, Running Windows7.
A. BENCHMARK FUNCTIONS AND
EXPERIMENTAL SETTING

In our experiments, we use ZDT2and ZDT4 [47] and threeobjective benchmark problems, DTLZ1 and DTLZ2 [21], to
test our algorithm, which are presented in Table 1.
The parameters in RR-MOEA/D are set as follows: the
initial population size of MOEA/D-DE is set to 100 and 300
for the two objective and three objective problems respectively and MOEA/D-DE is executed 50 generations before
inputting the preference information. The algorithm will stop
after 250 generations. The size of population in the preference region N and the parameter of controlling interactive
condition δ depend on the decision maker. Here, in order to
illustrate experimental result, the size of radius is set as 0.005
and N is set to 30 and 60 for the two objective and three
objective problems and the penalty is set as 1.5. The impact
of the parameters provided by the DM on the performance of
the algorithm is analyzed in Section 4.3.
B. EXPERIMENTAL RESULT AND ANALYSIS
1) RESULTS OF TWO-OBJECTIVE TEST PROBLEMS

ZDT2 is a two-objective problem with 30variables and its
PF is non-convex as shown in Fig. 2. Four cases based on
different preference information are analyzed in order to
verify the performance of RR-MOEA/D.
Case 1: The reference direction is defined by two points
(0,0) and (1,1), we suppose the DM prefers the middle region
of the PF in this case.
Case 2: The reference direction is between (0,0) and (1,0),
which means one objective gets more preferences.
Case 3: The same as Case 2 and the aspiration point and
reservation point is (0,0) and (0,1), respectively.
Case 4: To show the performance of algorithm when the
DM sets more than one reference directions, we consider two
7337

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

TABLE 1. Test functions used in the study.

reference directions: D1 is determined by (0, 0) and (1, 2);
and D2 is determined by (0, 0) and (2, 1).
Fig. 2 shows that RR-MOEA/D has a good performance
for all these four cases. For different kind of reference region
required by the DM, the algorithm can obtain a set of reasonable and uniformly distributed solutions in the preferred
region. Also, the algorithm can deal with more than one
reference region simultaneously, and all preferred solutions
can be obtained in each reference region.
Another two objective problem is ZDT4, which is hard due
to its many local optimal solutions and its convex PF with
10 variables. Similar with ZDT2 problem, we also adopt the
four cases above to test ZDT4 problem (see in Fig. 3).
It is obvious in Fig. 3 that the proposed algorithm can deal
with the convex problem as well and get the ideal solutions in
any region the DM prefers. Uniform distribution and convergence in the preference region on ZDT4, a hard problem of
7338

ZDT test problems, show a good performance of the proposed
algorithm.
2) RESULTS OF THREE-OBJECTIVE TEST PROBLEMS

DTLZ1 is a 10-variable, three-objective
problem. Its efficient
P
frontier is known and is given by i∈[1,m] fi = 0.5, as shown
in Fig. 4. Fig. 4(a) shows the obtained solutions with one
reference direction when the reference direction is given by
two points (0, 0, 0) and (0.5, 0.5, 0.5). To show the performance with more than one preference region, Fig. 4(b)
shows the obtained solutions with two reference direction
regions. Two reference directions are: {(0,0,0), (0.5,0.5,0.3)},
{(0,0,0), (0.1,0.1,0.5)}.
DTLZ2 has a convex
PF with 10 variables and its efficient
P
frontier satisfies i∈[1,m] fi2 = 1. Similar with DTLZ1 problem, we use one and two reference directions to show the performance. The first reference direction is from point (0,0,0)
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 2. Preferred solutions obtained by RR-MOEA/D on ZDT2: (a) case 1; (b) case 2; (c) case 3; (d) case 4.

FIGURE 3. Preferred solutions obtained by RR-MOEA/D on ZDT4 : (a) case 1; (b) case 2; (c) case 3; (d) case 4.

to point (0.7, 0.8, 0.5). The two reference directions in
Fig. 5(b) are {(0,0,0), (1,1,0.8)}, {(0,0,0), (0.4,0.4,0.1)},
respectively.
VOLUME 4, 2016

The final results of three-objective problem are shown in
Fig. 4 and Fig. 5. Obviously our algorithm also can well
converge to the true PF within the referred region the decision
7339

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 4. Preferred solutions obtained by RR-MOEA/D on DTLZ1 (a) one reference region; (b) two reference regions.

FIGURE 5. Preferred solutions obtained by RR-MOEA/D on DTLZ2 (a) one reference region; (b) two reference regions.

maker favors. It needs to point out here that the shape of
obtained solutions is determined by the way of initializing
the weights in MOEA/D.
C. DISCUSSION
1) THE DISCUSSION OF INTERACTION PERFORMANCE

As an interactive method, the interaction performance of
the algorithm is vitally importance. A simple interaction is
presented in the proposed algorithm. It is not an easy task

FIGURE 6. Initial solutions obtained in the interactive stage of ZDT2
((0, 0) →(1,1)).
7340

that an exact preference region is required at the beginning,
so the algorithm allows the decision maker to pick up a
reference direction as a trial. When the interaction condition
is met, these current solutions which are rough solutions in
the reference region are presented to the DM. If the DM is
interested in the reference region, our algorithm will go on
with finding the solutions approximate to PF in the reference
region. Otherwise, searching direction can be changed by the
DM. Here is an example to illustrate that we have plenty
ways to change the searching directions. Supposed we set
an initial searching direction from (0, 0) to (1, 1) and the
size of preference region radius = 0.005 for ZDT2 problem,
a set of rough solutions in the reference region are presented
to the DM (see in Fig. 6) when the interaction condition is
satisfied (Eq. 6), in which, δ = 0.001. If the DM is unsatisfied
with the reference region he/she set earlier, they change their
preference information as follows:
Case 1: The DM changes the initial searching region to
a near preference region(e.g. from (0,0) → (1,1) to (0,0) →
(1,1.1)).
Case 2: The reference region is changed far from the
initial reference region by the DM(e.g. from (0,0) →(1,1) to
(0,0) → (0.3,0.8)).
Case 3: A new preference region is added (e.g. from
(0,0) → (1,1) to (0,0) →(0.5,1) and (0,0) → (1,1)).
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 7. Solutions obtained of ZDT2 after different interactions: (a) case 1; (b) case 2; (c) case 3; (d) case 4.
TABLE 2. Mean generations and its standard variance with different δ.

Case 4: The DM changes an initial searching region to
two different searching regions (e.g. from (0,0) → (1,1) to
(0,0) → (0.5,1) and (0,0) → (1,0.5)).

VOLUME 4, 2016

The final results of these four cases are shown in
Fig. 7. If the DM is not satisfied with the searching
region he/she set before, our algorithm also can fast capture

7341

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 8. Solutions obtained on ZDT2 with the radius=0.5: (a) N = 20; (b) N = 50; (c) N = 100.

FIGURE 9. Solutions obtained on ZDT2 with the radius=0.05: (a) N = 20; (b) N = 50; (c) N = 100.

the new preference region and well converge to the PF
within the new preferred region to meet various user
requirements.
7342

A fast way to obtain a number of rough solutions in the
reference region to interact with the DM is another distinguishing characteristic. As mentioned earlier, δ is provided
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 10. Solutions obtained on ZDT2 with the radius=0.005: (a) N = 20; (b) N = 50; (c) N = 100.

FIGURE 11. Solutions obtained on ZDT2 problem with the radius=0.005: (a) penalty=1.0; (b) penalty=1.5;
(c) Penalty=2.0; (d) penalty=3.0.

by the decision maker to control the accuracy of the solution.
According to the interactive condition used in our study,
rapidly getting most solutions in DM’s preferred region is presented to the DM. In Table 2, the mean generations needed are
VOLUME 4, 2016

shown when the interaction happens for ZDT test problems
based on 10 independent runs (δ is set as 0.1, 0.01, 0.001). It
is easy to see that it just needs no more than 20 generations for
the proposed algorithm to find a rough set of solution when
7343

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

δ = 0.001, even when δ = 0.1, the proposed algorithm can
find some rough solutions in no more than 100 generations.
It is obvious that RR-MOEAD is a fast algorithm to meet
the interactive condition and this means the algorithm can
quickly get the rough solutions in the region preferred by the
DM.
2) THE DISCUSSION OF PARAMETERS
(RADIUS, N AND PENALTY)

At interactive stage, RR-MOEA/D allows the DM to input
different preference information to satisfy different preference requirements. In order to analyze the impact of the input
parameters like the population size N , the size of preferred
region i.e. the radius and the penalty on the performance of
the algorithm, we set the population size as 20, 50 and 100
respectively when the radius size is fixed as 0.5, 0.05, and
0.005 for 2-objective ZDT2 problem and perform a series of
experiments. The aspiration point and the reservation point
are set to (0, 0) and (1, 1).
Further, in order to determine the effect of the parameter
penalty on the experiment’s results. We set the penalty as
1.0, 1.5, 2.0 and 3.0 respectively when the value of radius
is fixed as 0.005 and the population is 30 for ZDT2, the
aspiration point and the reservation point are set to (0,0)
and (1,1).
From Fig. 8 Fig. 10, we can see that the larger the radius
is, it is better to set the radius as a large value. When DMs
provide a small value for the parameter radius, it is not
necessary to set a large value for the parameter N . in this
paper, we set radius = 0.005 and N is set to 30 and 60 for
the two objective and three objective problems.
Fig. 11 shows the solutions obtained with different value of
the penalty. We can conclude that if the value of the penalty is
set around 1.5, the entire reference region would be well converged to near the reference direction. Meanwhile, the penalty
being fixed as a greater value, the converged front will deviate
the reference direction. So, in the experimental study, we set
it as1.5. From here, we can draw a conclusion that the more
detailed information the DM provide, the more accurate solutions our algorithm can find. For example, if the smaller value
of radius and penalty are given, the front obtained would be
well converged to the reference direction. No matter what
reference region the DM needs, our algorithm can get a wellspaced front well converged to the region most preferred.
V. CONCLUSIONS

In this paper, an interactive reference region based evolutionary algorithm through decomposition named RR-MOEA/D
is proposed. At the beginning of the proposed algorithm,
MOEA/D is used to generate initial solutions before the
preference desired by the decision maker are given. We can
initialize the suitable subproblems in the reference regions
according to the DM’s requirements. By dealing with the
subproblems in the reference region, the solutions obtained
can converge to the regions of the Pareto front which the DM
prefers.
7344

During the stage of interaction, a simple interactive condition is adopted in this paper. The proposed algorithm can get
a set of rough solutions in one or several reference regions
quickly and avoid wasting time in searching the region which
the DM is not interested in. Solutions obtained are presented
to DM when the interactive condition is met. If the DM is
unsatisfied with these solutions, the new preference information can be reset. Therefore, the decision maker can control
the preference information in a very intuitive way. In addition, RR-MOEA/D allows the DM to reset different kinds of
preference information and it can quickly obtain a set rough
solution according to new preference information.
It should be emphasized that all of subproblems in the
preference region are determined by weight vectors in the
evolving stage. It is evitable that the population converges
to the region on PF where the user is undesired by dynamically adjusting the corresponding weights. If the number of
sub problems (population size) is so small to cover entire
preference region, the subproblems in the region most close
to the reference direction will become a priority. In other
words, RR-MOEA/D allows the DM to give uncertain and
inconsistent human decision as he or she does in real life.
Both MOPs with two and three objectives are adopted
to test the performance of RR-MOEA/D. According to the
results of experimental study, the proposed algorithm can successfully converge to different regions the DM most prefers.
It is obvious that our algorithm can realize a fast and simple
interactive process and copes with different preference information including uncertain and inconsistent human decision.
In our future work, we will try to find a better way to construct
the preference regions. As we known, there are still many
aspects of interaction strategy to worth further exploration.
Dealing with interaction information better and more humanized is our another work.
REFERENCES
[1] Q. Zhang and H. Li, ‘‘MOEA/D: A multiobjective evolutionary algorithm
based on decomposition,’’ IEEE Trans. Evol. Comput., vol. 11, no. 6,
pp. 712–731, Dec. 2007.
[2] K. Sindhya, A. B. Ruiz, and K. Miettinen, ‘‘A preference based interactive evolutionary algorithm for multi-objective optimization: PIE,’’ in
Evolutionary Multi-Criterion Optimization (Lecture Notes in Computer
Science), vol. 6576. Berlin, Germany: Springer-Verlag, 2011, pp. 212–225.
[3] K. Miettinen, Nonlinear Multiobjective Optimization. Norwell, MA, USA:
Kluwer, 1999.
[4] I. Das and J. E. Dennis, ‘‘Normal-boundary intersection: A new method
for generating the Pareto surface in nonlinear multicriteria optimization
problems,’’ SIAM J. Optim., vol. 8, no. 3, pp. 631–657, 1998.
[5] A. Messac, A. Ismail-Yahaya, and C. A. Mattson, ‘‘The normalized normal
constraint method for generating the Pareto frontier,’’ Struct. Multidisciplinary Optim., vol. 25, no. 2, pp. 86–98, Jul. 2003.
[6] R. F. Dell and M. H. Karwan, ‘‘An interactive MCDM weight space
reduction method utilizing a Tchebycheff utility function,’’ Naval Res.
Logistics, vol. 37, no. 2, pp. 263–277, Apr. 1990.
[7] H. Li and Q. Zhang, ‘‘Multiobjective optimization problems with complicated Pareto sets, MOEA/D and NSGA-II,’’ IEEE Trans. Evol. Comput.,
vol. 13, no. 2, pp. 284–302, Apr. 2009.
[8] R. Storn and K. Price, ‘‘Differential evolution—A simple and efficient
heuristic for global optimization over continuous spaces,’’ J. Glob. Optim.,
vol. 11, no. 4, pp. 341–359, 1997.
[9] K. Deb and R. B. Agrawal, ‘‘Simulated binary crossover for continuous
search space,’’ Complex Syst., vol. 9, no. 3, pp. 115–148, 1995.
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

[10] H.-L. Liu, F. Gu, and Q. Zhang, ‘‘Decomposition of a multiobjective optimization problem into a number of simple multiobjective subproblems,’’
IEEE Trans. Evol. Comput., vol. 18, no. 3, pp. 450–455, Jun. 2013.
[11] K. Li, K. Deb, Q. Zhang, and S. Kwong, ‘‘An evolutionary many-objective
optimization algorithm based on dominance and decomposition,’’ IEEE
Trans. Evol. Comput., vol. 19, no. 5, pp. 694–716, Oct. 2015.
[12] R. Cheng, Y. Jin, K. Narukawa, and B. Sendhoff, ‘‘A multiobjective
evolutionary algorithm using Gaussian process-based inverse modeling,’’
IEEE Trans. Evol. Comput., vol. 19, no. 6, pp. 838–856, Dec. 2015.
[13] R. Cheng, Y. Jin, M. Olhofer, and B. Sendhoff, ‘‘A reference vector guided
evolutionary algorithm for many-objective optimization,’’ IEEE Trans.
Evol. Comput., vol. 20, no. 5, pp. 773–791, 2016.
[14] K. Deb and H. Jain, ‘‘An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part I:
Solving problems with box constraints,’’ IEEE Trans. Evol. Comput.,
vol. 18, no. 4, pp. 577–601, Aug. 2014.
[15] K. Deb, S. Agarwal, A. Pratap, and T. Meyarivan, ‘‘A fast and elitist
multiobjective genetic algorithm: NSGA-II,’’ IEEE Trans. Evol. Comput.,
vol. 6, no. 2, pp. 182–197, Apr. 2002.
[16] C. A. C. Coello, ‘‘Handling preferences in evolutionary multiobjective
optimization: A survey,’’ in Proc. Congr. Evol. Comput., Jul. 2000,
pp. 30–37.
[17] L. Rachmawati and D. Srinivasan, ‘‘Preference incorporation in multiobjective evolutionary algorithms: A survey,’’ in Proc. IEEE Congr. Evol.
Comput. (CEC), Jul. 2006, pp. 3385–3391.
[18] A. P. Wierzbicki, ‘‘The use of reference objectives in multiobjective optimization,’’ in Multiple Criteria Decision Making Theory and Application.
Heidelberg, Germany: Springer, 1980, pp. 468–486.
[19] A. Jaszkiewicz and R. Slowiński, ‘‘The ‘light beam search’ approach—An
overview of methodology applications,’’ Eur. Jour. Oper. Res., vol. 113,
no. 2, pp. 300–314, Mar. 1999.
[20] K. Deb and J. Sundar, ‘‘Reference point based multi-objective optimization
using evolutionary algorithms,’’ Int. J. Comput. Intell. Res., vol. 2, no. 3,
pp. 273–286, 2006.
[21] K. Deb and A. Kumar, ‘‘Interactive evolutionary multi-objective optimization and decision-making using reference direction method,’’ in Proc.
9th Annu. Conf. Genet. Evol. Comput. (GECCO), 2007, pp. 781–788.
[22] L. Thiele, K. Miettinen, P. J. Korhonen, and J. Molina, ‘‘A preference-based
evolutionary algorithm for multi-objective optimization,’’ Evol. Comput.,
vol. 17, no. 3, pp. 411–436, 2009.
[23] J. Molina, L. V. Santana, A. G. Hernández-Diaz, C. A. C. Coello, and
R. Caballero, ‘‘g-dominance: Reference point based dominance for multiobjective metaheuristics,’’ Eur. J. Oper. Res., vol. 197, no. 2, pp. 685–692,
Sep. 2009.
[24] E. Zitzler, M. Laumanns, and L. Thiele, ‘‘SPEA2: Improving
the strength Pareto evolutionary algorithm,’’ in Evolutionary
Methods for Design, Optimization and Control With Applications
to Industrial Problems K. C. G. Koglou, D. T. Tsahalis, J. Périaux,
and T. Fogarty, Eds. Heidelberg, Germany: Springer, 2001,
pp. 95–100.
[25] M. G. Gong, L. Jiao, H. Du, and L. Bo, ‘‘Multiobjective immune algorithm
with nondominated neighbor-based selection,’’ Evol. Comput., vol. 16,
no. 2, pp. 225–255, 2008.
[26] J. Branke, K. Deb, and K. Miettinen, Multiobjective Optimization: Interactive and Evolutionary Approaches, vol. LNCS 5252. Berlin, Germany:
Springer-Verlag, 2008.
[27] T. Tanino, M. Tanaka, and C. Hojo, ‘‘An interactive multicriteria decision
making method by using a genetic algorithm,’’ Manage. Sci., vol. 22 no. 6,
pp. 652–663, 1976.
[28] C. M. Fonseca and P. J. Fleming, ‘‘Multiobjective optimization and multiple constraint handling with evolutionary algorithms. I. A unified formulation,’’ IEEE Trans. Syst., Man, Cybern. A, Syst., Humans, vol. 28, no. 1,
pp. 26–37, Jan. 1998.
[29] M. Shibuya, H. Kita, and S. Kobayashi, ‘‘Integration of multi-objective
and interactive genetic algorithms and its application to animation design,’’
in Proc. IEEE Int. Conf. Syst., Man, Cybern. IEEE SMC, Oct. 1999,
pp. 646–651.
[30] D. S. Todd and P. Sen, ‘‘Directed multiple objective search of design spaces
using Genetic Algorithms and neural networks,’’ in Proc. Genetic Evol.
Comput. Conf., 1999, pp. 1738–1743.
[31] I. C. Parmee, D. Cvetković, A. Watson, and C. Bonham, ‘‘Multiobjective
satisfaction within an interactive evolutionary design environment,’’ Evol.
Comput., vol. 8, no. 2, pp. 197–222, Jun. 2000.
VOLUME 4, 2016

[32] H. J. C. Barbosa and A. M. S. Barreto, ‘‘An interactive genetic algorithm with co-evolution of weights for multiobjective problems,’’ in Proc.
Genetic Evol. Comput. Conf., 2001, pp. 203–210.
[33] S. Phelps and M. Köksalan, ‘‘An interactive evolutionary metaheuristic for
multiobjective combinatorial optimization,’’ Manage. Sci., vol. 49, no. 12,
pp. 1726–1738, 2003.
[34] R. Kamalian, H. Takagi, and A. M. Agogino, ‘‘Optimized design of MEMS
by evolutionary multi-objective optimization with interactive evolutionary computation,’’ in Proc. GECCO, Berlin, Germany, vol. 3103. 2004,
pp. 1030–1041.
[35] R. Kamalian, Y. Zhang, H. Takagi, and A. M. Agogino, ‘Evolutionary synthesis of micromachines using supervisory multiobjective interactive evolutionary computation,’’ in Proc. ICMLC, LNAI3930, 2006,
pp. 428–437.
[36] L. Thiele, K. Miettinen, P. J. Korhonen, and J. Molina, ‘‘A preference-based
interactive evolutionary algorithm for multiobjective optimization,’’ Dept.
Inf. Technol. Electr. Eng., Aalto Univ. School Bus., Helsinki, Finland,
Tech. Rep., 2007.
[37] G. Avigad and A. Moshaiov, ‘‘Interactive evolutionary multiobjective
search and optimization of set-based concepts,’’ IEEE Trans. Syst., Man,
Cybern. B, Cybern., vol. 39, no. 4, pp. 1013–1027, Aug. 2009.
[38] L. B. Said, S. Bechikh, and K. Ghedira, ‘‘The r-dominance: A new dominance relation for interactive evolutionary multicriteria decision making,’’
IEEE Trans. Evol. Comput., vol. 14, no. 5, pp. 801–818, Oct. 2010.
[39] K. Deb, A. Sinha, P. J. Korhonen, and J. Wallenius, ‘‘An interactive
evolutionary multiobjective optimization method based on progressively
approximated value functions,’’ IEEE Trans. Evol. Comput., vol. 14, no. 5,
pp. 723–739, Oct. 2010.
[40] C. L. Simons and I. C. Parmee, ‘‘Elegant object-oriented software design
via interactive, evolutionary computation,’’ IEEE Trans. Syst., Man,
Cybern. C, Appl. Rev., vol. 42, no. 6, pp. 1797–1805, Nov. 2012.
[41] X. Sun, D. Gong, Y. Jin, and S. Chen, ‘‘ A new surrogate-assisted interactive genetic algorithm with weighted semisupervised learning,’’ IEEE
Trans. Cybern., vol. 43, no. 2, pp. 685–698, Apr. 2013.
[42] Evolutionary Multi-Criterion Optimization (Lecture Notes in Computer
Science), vol. 9019. Switzerland, Springer International Publishing, 2015,
pp. 249–263.
[43] A. B. Ruiz, R. Saborido, and M. Luque, ‘‘A preference-based evolutionary
algorithm for multiobjective optimization: The weighting achievement
scalarizing function genetic algorithm,’’ J. Global Optim., vol. 62, no. 1,
pp. 101–129, May 2015.
[44] M. G. Gong, F. Liu, W. Zhang, L. Jiao, and Q. Zhang, ‘‘Interactive
MOEA/D for multi-objective decision making,’’ in Proc. 13th Annu. Conf.
Genetic Evol. Comput., 2011, pp. 721–728.
[45] R. Battiti and A. Passerini, ‘‘Brain–computer evolutionary multiobjective
optimization: A genetic algorithm adapting to the decision maker,’’ IEEE
Trans. Evol. Comput., vol. 14, no. 5, pp. 671–687, Oct. 2010.
[46] C. Fonseca and P. J. Fleming, ‘‘Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization,’’ In Proc.
5th Int. Conf. Genet. Algorithms, 1993, pp. 416–423.
[47] E. Zitzler, L. Thiele, and K. Deb, ‘‘Comparison of multiobjective evolutionary algorithms: Empirical results,’’ Evol. Comput., vol. 8, no. 2,
pp. 173–195, Jun. 2000.

RUOCHEN LIU (M’07) received the Ph.D. degree
from Xidian University, Xian, China, in 2005. She
is currently an Associate Professor with the Intelligent Information Processing Innovative Research
Team of the Ministry of Education of China, Xidian University. Her research interests are broadly
in the area of computational intelligence. Her areas
of special interest include artificial immune systems, evolutionary computation, data mining, and
optimization.
7345

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

7346

RUINAN WANG received the B.S. degree from
the Hebei University of Science and Technology, Hebei, China. He is currently pursuing the
M.S. degree from Xidian University. His current
research focuses on multi-objective optimization
and data mining.

JUNJUN HUANG received the B.S. degree from
the Huaiyin Institute of Technology, Huai’an,
China. He is currently pursuing the M.S. degree
with Xidian University. His current research
focuses on multi-objective optimization and image
processing.

WEN FENG received the B.S. degree from Xidian
University, Xi’an, China, where she is currently
pursuing the M.S. degree. Her current research
focuses on multi-objective optimization.

LICHENG JIAO (SM’89) received the Ph.D.
degree from Xian Jiaotong University, Xian,
China, in 1990. He is currently a Professor and the
Dean of the Electronic Engineering School, Xidian
University, China. His current research focuses on
intelligent information processing.

VOLUME 4, 2016

Database, 2016, 18 doi: 10.1093/database/baw127 Database Tool

Database Tool

Skeleton Genetics: a comprehensive database for genes and mutations related to genetic skeletal disorders
Chong Chen1,, Yi Jiang2,, Chenyang Xu1, Xinting Liu2, Lin Hu3, Yanbao Xiang1, Qingshuang Chen1, Denghui Chen2, Huanzheng Li1, Xueqin Xu1 and Shaohua Tang1,3,*
Department of Genetics of Dingli Clinical Medical School, Wenzhou Central Hospital, Wenzhou 325000, China, 2Institute of Genomic Medicine, Wenzhou Medical University, Wenzhou 325035, China, 3 School of Laboratory Medicine and Life Science, Wenzhou Medical University, Wenzhou 325035, China
*Corresponding author: Tel/Fax: 86 577 88070197; Email: tsh006@126.com


Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

1

These authors contributed equally to this work.

Citation details: Chen,C., Jiang,J., Xu,C. et al. SkeletonGenetics: a comprehensive database for genes and mutations related to genetic skeletal disorders. Database (2016) Vol. 2016: article ID baw127; doi:10.1093/database/baw127
Received 2 May 2016; Revised 2 August 2016; Accepted 12 August 2016

Abstract
Genetic skeletal disorders (GSD) involving the skeletal system arises through disturbances in the complex processes of skeletal development, growth and homeostasis and remain a diagnostic challenge because of their clinical heterogeneity and genetic variety. Over the past decades, tremendous effort platforms have been made to explore the complex heterogeneity, and massive new genes and mutations have been identified in different GSD, but the information supplied by literature is still limited and it is hard to meet the further needs of scientists and clinicians. In this study, combined with Nosology and Classification of genetic skeletal disorders, we developed the first comprehensive and annotated genetic skeletal disorders database, named `SkeletonGenetics', which contains information about all GSD-related knowledge including 8225 mutations in 357 genes, with detailed information associated with 481 clinical diseases (2260 clinical phenotype) classified in 42 groups defined by molecular, biochemical and/or radiographic criteria from 1698 publications. Further annotations were performed to each entry including Gene Ontology, pathways analysis, proteinprotein interaction, mutation annotations, diseasedisease clustering and genedisease networking. Furthermore, using concise search methods, intuitive graphical displays, convenient browsing functions and constantly updatable features, `SkeletonGenetics' could serve as a central and integrative database for unveiling the genetic and pathways pre-dispositions of GSD. Database URL: http://101.200.211.232/skeletongenetics/

C The Author(s) 2016. Published by Oxford University Press. V

Page 1 of 8

This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. (page number not for citation purposes)

Page 2 of 8

Database, Vol. 2016, Article ID baw127

Introduction
The skeleton provides the structural framework in humans for muscle attachments, assists movement, protects organs, and maintains the homeostasis of the vascular systems (1). Perturbation of development in bone, cartilage and joints could result in human skeletal dysplasias, which affect approximately 1 in 5000 live births and is one of the common causes of neonatal birth defect in departments of obstetrics (2). Genetic skeletal disorders (GSD), which are a group of disorders involving gene mutations or genetic susceptibility to bone disease, account for most of the human skeletal dysplasias, involving a variety of pathogenic factors and diseases (3). In the Nosology and Classification of Genetic Skeletal Disorders 2015 revision (hereinafter referred to as NCGSD-2015), 436 conditions were included and placed in 42 groups, which were associated with mutations in one or more of 336 unique genes (4). Till now, especially with the impact the next-generation sequencing technology has made on the study of genetic skeletal disorders, a large amount of mutations and novel GSD-related genes have been identified. Genetic skeletal disorders face similar problems to other complex diseases with exceptional genetic heterogeneity and clinical variety (5). Notably, the same skeletal dysplasia gene can lead to substantially different clinical phenotypes or a specific skeletal phenotype can be caused by mutations in different genes (6). Therefore, understanding genotypephenotype correlations and functional diversity remains one of the major challenges for GSD. The NCGSD-2015 provides an overview of the recognized diagnostic entities of GSD by clinical, anatomical site and molecular pathogenesis for clinicians, scientists and the radiology community to diagnose individual cases and describe novel disorders (5). However, the increasing availability of next-generation sequencing technology and other new sequencing platforms will likely result in a rapid identification of novel GSD-related genes and mutations, and novel phenotypes associated with mutations in genes linked to other phenotypes has increased dramatically to date (7, 8). Therefore, the catalog of GSD-related genotype-phenotype has become so large as to surpass the scope of a `Nosology', so the Nosology must be transformed into an automated annotation and query database. Thus, it is crucial to integrate the existing data and present an organized comprehensive mutation repository to construct an integrative, informative and updatable resource for GSD-related genetic pre-dispositions which could greatly facilitate the counseling, diagnosis and therapy for pediatrics and genetics. In this study, we reviewed three dependent databases and public resources related to genetic skeletal disorders and made the first annotated database about GSD, named `SkeletonGenetics', which provides full-scale gene and

mutation information and extensive annotations for GSD. Initially, we retrospectively extracted the basic information for each mutation (disease conditions or syndrome, gene symbol, number of OMIM, hereditary mode, coding sequence change, transcript, ethnicity, age, PubMed ID, etc.) from open publications. Additionally, extensive annotations were performed, which include gene information, Gene Ontology, pathways analysis, protein-protein interaction, mutations clustering and gene-disease network. As a result, 42 groups of GSD, 481 diseases or syndromes, 357 genes, 5884 single nucleotide variations (SNVs), 516 insertions, 1427 deletions and 2260 different phenotypes were included. Taken together, `SkeletonGenetics' is constructed to be a well-organized, internal-standard and comprehensive resource for scientists and clinicians looking for the clinical correlates of mutations, genes and pathways involved in skeletal biology.

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Methods
The standard of data entry group of GSD
The Nosology Group of the International Skeletal Dysplasia Society formulated the table of NCGSD-2015 in September, 2015. The criteria used for genetic skeletal disorders were unchanged from NCGSD-2010 revision. They were (1) significant skeletal involvement, including skeletal dysplasias, skeletal dysostoses, metabolic bone disorders and skeletal malformation and/or reduction; (2) publication and/or listing in OMIM; (3) genetic basis proven by pedigree or very likely based on homogeneity of phenotype in unrelated families; (4) nosologic autonomy confirmed by molecular or linkage analysis and/or by the presence of distinctive diagnostic features and of observation in multiple individuals or families (4). In accordance with the standards of NCGSD-2015, 481 different conditions were placed in 42 groups, associated with one or more of 357 different genes. The grouping results and criteria are presented in Supplementary Table S1.

Data collection and content
Based on the standards of NCGSD-2015, we performed comprehensive searches for GSD-related publications and databases to obtain a complete list of mutation information associated with GSD. Firstly, we retrospectively queried the PubMed database (http://www.ncbi.nih.gov/ pubmed) for genes and diseases retrieved in Entrez with terms `gene or disease symbol [Title/Abstract] AND mutation [Title/Abstract] OR variant [Title/Abstract]. Secondly, other databases were taken as supplementary sources, including Leiden Open Variation Database 3.0 (LOVD3.0,

Database, Vol. 2016, Article ID baw127

Page 3 of 8

http://www.lovd.nl/3.0/home) which contained fanconi anemia relevant genes and the mutations predicted to be benign or which did not segregate with phenotype were excluded (9), Clinvar (http://www.ncbi.nlm.nih.gov/clin var/), which contained pathogenic or likely pathogenic clinical mutation information. Human Phenotype Ontology (HPO, http://human-phenotype-ontology. github.io/) (10) and Online Mendelian Inheritance in Man (OMIM) (11) were used to describe phenotypic information. Patient clinical data have been obtained in accordance with the tenets of the Declaration of Helsinki. Finally, >2000 publications were queried from open resource beginning in 1990. After manually screening the abstracts and full-text of these publications, we excluded those studying diseases other than GSD and eventually retained 1698 publications. We extracted the basic information for each mutation, including disease name, gene symbol, inheritance mode, CDS change, PubMed ID, ethnicity, age, gender and functional study through reading the full-text articles and double-checked these manually. The in-house perl program was used to obtain the correct genomic coordinate for each entry. As of the beginning of 2016, the `SkeletonGenetics' database contains 481 disease conditions, associated with mutations in one or more of 357 different genes, 8225 variations (5884 SNVs, 1427 deletions, 516 insertions and 398 Block substitution), 2260 clinical phenotypes and their detailed information included in 42 groups of GSD from 1698 publications ranging from common, recurrent mutations to `private' mutations found in single families or individuals. The results are presented in Table 1.

Table 1 Data content and statistics of genetic data in SkeletonGenetics
Data type Mutation SNVs Deletions Insertions Block substitution Genes Disease group Disease or syndrome Phenotypes GOs Pathways KEGG pathways Wiki pathways MicroRNA Target PPI Publications Data count 8225 5884 1427 516 398 357 42 481 2260 2535 138 64 74 66 40 1698

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

syndrome, Cornelia de Lange syndrome, which all belong to the Top 5 of the disease list, all have >367 gene mutation information. At the same time, we classified the number of mutations by group: the limb hypoplasia-reduction defects group; the overgrowth syndromes with skeletal involvement group; the lysosomal storage diseases with skeletal involvement (dysostosis multiplex group); the disorganized development of skeletal components group; and the dysostoses with predominant craniofacial involvement group all have >375 mutation information. Finally, according to the information collected, we statistically analyzed the bias by chromosomes, mutation types, mutation effect, gender, ages of onset and inheritance mode.

Information statistics
Specific skeletal phenotype can be caused by mutations in different genes or the same gene can lead to substantially different clinical phenotypes. This leads to the bias of genotype and phenotype of GSD, therefore statistics on the number of mutations for gene, disease and the group become very important. The gene Top 5 with the highest number of mutations in `SkeletonGenetics' is `FBN1', associated with four skeletal diseases (Marfan syndrome, Weill-Marchesani syndrome 2, Geleophysic dysplasia type 2, Acromicric dysplasia) with 639 mutations; followed by `NF1' associated with Neurofibromatosis type 1 with 415 mutations; `NIPBL' associated with Cornelia de Lange syndrome 1 with 406 mutations; `NSD1' associated with Sotos syndrome 1 with 362 mutations; `GNPTAB' associated with two diseases (Mucolipidosis II, alpha/beta type; Mucolipidosis III (Pseudo-Hurler polydystrophy), alpha/ beta type) with 231 mutations. Correspondingly, Marfan syndrom, Fanconi anemia, Neurofibromatosis, Sotos

Functional and enrichment analysis
To further interpret the function and heterogeneity of GSD, `SkeletonGenetics' performs a series of functional analyses, which included enrichment analysis, mutation annotation, mutation spectrum and gene-disease network construction (Figure 1). For the aspect of enrichment analysis, we provided Gene Ontology (GO), KEGG pathways, Wikipathways, MicroRNA Target and ProteinProtein interaction. Meanwhile, we used WebGestalt software to store the gene list in `SkeletonGenetics' (12, 13). Users can click on the `Analysis' page to see whether the gene of interest is involved in any GO terms, function pathways, microRNA target or PPIs. Gene ontology annotation terms involved the biological process, the cellular component and molecular function of three biological processes, the Top5 most statistically

Page 4 of 8

Database, Vol. 2016, Article ID baw127

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Figure 1. Flowchart of the procedure for `SkeletonGenetics'. `SkeletonGenetics' mainly consists of three parts: (i) data extraction based on literature search and GSD-related databases, (ii) annotation of all mutations and genes using ANNOVAR and (iii) enrichment analysis by WebGestalt and genedisease network analysis graphically.

significant (P-values) of biological process terms in `SkeletonGenetics', namely, skeletal system development (P-value: 3.29E-63, number of genes, 87) (14); appendage development (P-value: 6.25E-46, number of genes, 50) (15); limb development (P-value: 6.25E-46, number of genes, 50) (16); appendage morphogenesis (P-value: 5.11E-45, number of genes, 48) (15); and limb morphogenesis (P-value: 5.11E-45, number of genes, 48) (16). The above entries of the most statistically significant GOs terms are all related to skeletal system or component formation, morphological differentiation and location control. Meanwhile, we provided functions pathways, including KEGG pathways and Wikipathways of two annotation methods. A complex series of signaling pathways including TGF-beta signaling pathway (P-value: 1.07E-13, number of genes, 14) (17); Hedgehog signaling pathway (P-value: 5.85E-09, number of genes, 9) (18); WNT signaling pathway and pluripotency (P-value: 1.72E-06, number of genes, 10) (19); Notch signaling pathway (P-value: 4.79E07, number of genes, 7) (20) and metabolic processing (P-value: 1.48E-14, number of genes, 41) are essential for proper skeletogenesis (21), mainly distributed in the cell, extracellular matrix, and transcriptional regulation related to bone, cartilage, and joint formation. Researchers recommend use of a morphogen rheostat model to conceptualize the differential signaling inputs which lead to divergent skeletal phenotypes within a temporal and spatial context

(1). In these terms, we have established some function pathways of the relationship between different gene mutations and groups of bone diseases. The modification of microRNA by degraded target mRNA maintains cellular homeostasis and regulates cell fate transitions during differentiation. These processes are important to ensure proper organogenesis and growth of skeleton. `hsa_CTTTGCA, MIR-527', which ranks first with the P-value of 3.93E-07 among all MicroRNA Target enriched, including the MYCN gene, in which miRNA cluster heterozygous mutations cause Feingold syndrome, a disorder that involves limb malformations, microcephaly, learning disability/mental retardation, hand and foot abnormalities and may include hypoplastic thumbs, clinodactyly of the second and fifth fingers, syndactyly (characteristically between the second and the fourth and fifth toes) and shortened or absent middle phalanges, cardiac and renal malformations, vertebral anomalies and deafness (22). Defining the targets of this miRNAs gene will give a deeper understanding of the pathophysiology and complex genetics of GSD. Finally, PPIs enriched was introduced, `Hsapiens_Module_19', which was the most statistically significant with the P-value of 1.83E-17 among all PPIs, was mainly involved in the collagen group (COL1A1, COL1A2, COL2A1, COL9A1, COL9A2, COL9A3, COL10A1, COL11A1 and COL11A2) (23), the matrix

Database, Vol. 2016, Article ID baw127

Page 5 of 8

metalloproteinase group (MMP2, MMP9 and MMP13) (24) and the Fibrillin-TGFbeta receptor group (causing overgrowth syndromes, including FBN1, FBN2, TGFb1 and TGFb2) (25). The above functional and enrichment analysis results, including classical genetic and epigenetic modifications, is consistent with previous findings that skeletal system development, appendage development, limb development, appendage morphogenesis and limb morphogenesis are related to genetic skeletal disorders.

Mutation annotation
`SkeletonGenetics' performed the detailed annotation information of all mutations to facilitate the users to assess the regarding interest mutations. Firstly, the coordinates of variations, such as SNV, (FBN1, NM_000138 and c.5198G > A) or InDels, (COL2A1, NM_033150 and c.4234_4245del) were converted to the corresponding coordinates on the human reference genome GRC37/hg19 (chr15:48755305-48755305) and (chr12:48367202-4836 7213) by UCSC Genome Browser (26) and the in-house perl program was used to convert coordinates from CDS to genome. Secondly, the general annotation of mutations, such as the effects on protein coding (frameshift, nonframeshift, non-synonymous, splicing, stopgain, stoploss, etc.), amino acid change and the location of the mutation (exonic, intronic, intergenic, region, etc.) were performed by ANNOVAR (27). Additionally, more detailed clinical information was provided about each entry, including PubMed ID, ethnicity, gender (male or female), age-ofonset (death, newborn, days, weeks, months, years) and hereditary mode. Another 27 databases or data sets were linked and annotated, such as seven quick links (NCBI, HGNC, MGI, OMIM, Ensembl, Vega and GeneCards), 15 functional prediction software, dbSNP (28) and 1000 Genomes Project (29), ESP, CG69, ExAC. Phenotype information extracted from the HPO databases and provided the OMIM ID, MGI ID, phenotype or syndrome name (such as WeillMarchesani syndrome 2, dominant), phenotype description (related to search module phenotype button), cosmic70, clinvar_20150330 information, etc.

Therefore, in order to facilitate the search for mutation information and statistics on the bias of the mutation position, we used scalable vector graphics (SVG) to visualize the mutation distribution in each GSD-gene for related syndromes, each simulated fonts including gene position information, gene name (number of exons, transcript ID), and encoded information, with different colors to represent different mutation effects or types, which presented a gene level overview of the summarized mutations. For example, the syndrome of achondrogenesis type 2 (ACG2; LangerSaldino), can be expressed as chr12:48 367 189-48 398 104, Gene: COL2A1 (54 coding exons, NM_001844 or NM_033150) and mutations in exonic or intronic, the variations were more than once distinguished by different colors and fonts from those first identified. Besides, SVG was used to construct a graphical gene-disease network to provide the potential relations of GSD and skeletal-related genes for understanding the complex heterogeneity of GSD. Information about the genedisease network includes the number of GSD genes, mutation information and different disease phenotypes. For example, the COL2A1 related to nine common genetic skeletal disorders, including achondrogenesis type 2 (ACG2; Langer-Saldino), platyspondylic dysplasia, Torrance type, hypochondrogenesis, spondyloepiphyseal dysplasia congenital (SEDC), spondyloepimetaphyseal dysplasia (SEMD) strudwick type, kniest dysplasia, spondyloperipheral dysplasia, mild SED with premature onset arthrosis, SED with metatarsal shortening (formerly Czech dysplasia), stickler syndrome type 1, are depicted by a simple ball (red ball represents gene and blue ball represents disease) and a straight line to construct the genedisease network. Users click on the corresponding graphics and can quickly link to detailed information on mutations and phenotypes.

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Data search and browse
`SkeletonGenetics' provides a quick and concise search box on the home page for searching by five symbols. Firstly, there are three gene symbol search modules, `gene symbol', `gene ID' and `gene transcript'; secondly, mutation position and phenotype information were incorporated to allows users to search by (i) specifying options like gene name (capital letters), gene ID, transcript information (ii) investigating the specified phenotype of the typical skeletal changes related to the position of the forearm of the upper limb, to the description of the variability to the extent of skeletal changes (iii) search mutation position data of more than one gene or mutation, or when it is known that a mutation or gene is located on a particular area or chromosome, a position symbol list will be needed to achieve this fuzzy search. To facilitate users browsing the data, two

Mutation spectrum and genedisease network
The location of the gene mutation is biased, some of which are located in the 50 -UTR region, 30 -UTR region or the mutation-rich region, and some are distributed in single mutation sites, such as 1138G > A mutation in FGFR3 of achondroplasia (30) and 49G > A mutation in AKT1 of proteus syndrome (31), which accounted for 98% and 100% of the total number of mutations, respectively.

Page 6 of 8

Database, Vol. 2016, Article ID baw127

different approaches are provided: (i) browse by disorders (ii) browse by chromosome (Figure 2). The `browse by disorders' option provides 42 groups and 481 conditions of genetic skeletal disorders for users to conveniently retrieve the information about mutations of interest. The genes and mutations related to this group or disease conditions can be easily retrieved by selecting from the list. Additionally, users can browse all the variants that are mapped on the entry chromosome or chromosomal bands in a graphical way in `browse by chromosome', which is linked to the mutations information page.

provide a user-friendly web interface for searching and browsing. Meanwhile, the database was organized in two different table output formats by ANNOVAR software (27) and GOs annotation, KEGG pathways, Wikipathways, MicroRNA Target and ProteinProtein interaction were stored in separate tables. All data can be freely downloaded from the website (Download page). The web client has been successfully tested with Internet Explorer 10, Chrome 48.0, Safari 7.1 and Firefox 2/3 and is implemented independently of the operating system.

Results and discussion
Database organization and web interface
In `SkeletonGenetics', all the data were stored and managed in a MySQL relational database and run on an Apache HTTP server by PHP and JavaScript program to The assignment of genetic skeletal disorders into specific groups has been practiced since the previous versions of the `Nosology' by biochemical, molecular information available, or the group of disorders with similar

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Figure 2. A screenshot of the search, browse and annotation module in `SkeletonGenetics'. Search box at home page for searching by five symbols. `gene symbol', `gene ID' and `gene transcript', `mutations position' and phenotype information. `Browse by chromosome' is used to retrieve all GSDrelated genes mapped on chromosomes, `Browse by disease' is used to retrieve all GSD-related genes'. Annotation module including functional and enrichment analysis, mutation annotation, mutation spectrum and genedisease network.

Database, Vol. 2016, Article ID baw127

Page 7 of 8

phenotypic features defined by common radiographic features or anatomical site. Meanwhile, researchers criticize the previous versions, focusing on their `hybrid' nature, which does not stick to a single systematic approach. Firstly, disorders should be classified on phenotypic similarities. Secondly, they should be reclassified based on the pathway or gene related to the functional abnormality (4). Based on the above principles, and as more and more resources are published on the network resource, we developed a comprehensive database for genes and mutations related to genetic skeletal disorders, `SkeletonGenetics' integrated data types associated with GSD through indepth mining of 1698 publications and extensive functional analysis, which covered a broad range of data including lists of disease grouping and disease names, genes, mutations and mutations spectrum, GO terms, pathways, microRNA target, proteinprotein interaction and genedisease network. Meanwhile, combined with concise search methods, intuitive graphical displays, convenient browsing functions and constantly updatable features, the `SkeletonGenetics' database could serve as a reclassified reference tool and valuable resource for unveiling the genetic and pathway basis of GSD. With the development of the high-throughput sequencing, massive genetic skeletal disorder related genes and mutations have been identified in the past decade, but there is still about 3040% of GSD with undiscovered disease genes (4, 6) because of the restriction of patients and complexity of the gene interaction network. In this study, functional analysis of the known GSD genes in publications mostly involved in specific GO items, such as skeletal system development, appendage development, limb development, appendage morphogenesis and limb morphogenesis or pathways, such as classical FGFs, TGF-beta, Hedgehog, WNT, Notch signaling pathways. Meanwhile, many of the new identified genes interact with known GSD genes (32 35) or key GSD genes could induce the disease state. In SkeletonGenetics, if researchers have found a specific GSD gene, they may link to the `Analysis' page to see whether the new gene is involved in any skeletal-related GO terms, pathways or PPIs. This database would be important and useful for revealing novel GSD-related genes and pathways. Therefore, researchers could focus on the other unknown genes, which were involved in the same skeletal development and homeostasis functions module combined with biochemical, molecular information or the group of disorders with similar phenotypic features. The increasing availability of massive parallel sequencing and other new sequencing technologies will likely result in a rapid and cost-effective identification of many GSDcausing genes and mutations, or novel phenotypes associated with mutations in genes already linked to other

phenotypes. Automatic mining methods will be used in `SkeletonGenetics' for updating GSD-related data, (1) collect the latest disorders, genes or mutation from PubMed or open databases related to GSD; (2) perform more extensive functional and enrichment analysis based on the updated data sets; (3) improve the mutation spectrum, genedisease network and other database functionalities. In SkeletonGenetics, researchers may use the `Submission' page to upload de novo mutations for GSD or new genetic skeletal disorders to keep the database up-to-date and comprehensive. We believe that `SkeletonGenetics' will hopefully have paved the way by setting standards for the recognition and definition of skeletal phenotypes and understanding of the complex heterogeneity of GSD and hope that the continued efforts to improve `SkeletonGenetics' will ultimately help improve diagnosis and treatment of genetic skeletal disorders.

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Supplementary data
Supplementary data are available at Database Online.

Acknowledgements
We thank Drs Lili Zhou, Xiangnan Chen, Jiaojiao Lv, Ping Wang, Yunying Chen, Ke Wu, Zhaotang Luan, Manli Jia, at the Department of Genetics of Dingli Clinical Medical School, Wenzhou Central Hospital and School of Laboratory Medicine and Life Science, Wenzhou Medical University, for the data collecting and testing.

Funding
Natural Science Foundation of Zhejiang province (LY13H200002); Science and technology project of Wenzhou (Y20150092); The Ministry of Health Project (WKJ2011-2-017). This study was supported by the Natural Science Foundation of Zhejiang province in Hangzhou, China (); Science and technology project of Wenzhou in Zhejiang, China(); The Ministry of Health Project in (). Conflict of interest. None declared.

References
1. Baldridge,D., Shchelochkov,O., Kelley,B. et al. (2010) Signaling pathways in human skeletal dysplasias. Annu. Rev. Genomics. Hum. Genet., 11, 189217. 2. Krakow,D. and Rimoin,D.L. (2010) The skeletal dysplasias. Genet. Med., 12, 327341. 3. Mortier,G.R. (2001) The diagnosis of skeletal dysplasias: a multidisciplinary approach. Eur. J. Radiol., 40, 161167. 4. Warman,M.L., Cormier,D.V., Hall,C. et al. (2015) Nosology and classification of genetic skeletal disorders: 2015 revision. Am. J. Med. Genet., 167A, 28692892. 5. Rousseau,F., Bonaventure,J., Legeai,M.L. et al. (1996) Clinical and genetic heterogeneity of hypochondroplasia. J. Med. Genet., 33, 749752.

Page 8 of 8 6. Geister,K.A. and Camper,S.A. (2015) Advances in Skeletal Dysplasia Genetics. Annu. Rev. Genomics Hum. Genet., 16, 199227. 7. Bernier,F.P., Caluseriu,O., Ng,S. et al. (2012) Haploinsufficiency of SF3B4, a component of the pre-mRNA spliceosomal complex, causes Nager syndrome. Am. J. Hum. Genet., 90, 925933. 8. Twigg,S.R. and Wilkie,A.O. (2015) A geneticpathophysiological framework for craniosynostosis. Am. J. Hum. Genet., 97, 359377. 9. Fokkema,I.F., Taschner,P.E., Schaafsma,G.C. et al. (2011) LOVD v.2.0: the next generation in gene variant databases. Hum. Mutat., 32, 557563. 10. Ko  hler,S., Doelken,S.C., Mungall,C.J. et al. (2014) The Human Phenotype Ontology project: linking molecular biology and disease through phenotype data. Nucleic Acids Res., 42, 966974. 11. Hamosh,A., Scott,A.F., Amberger,J.S. et al. (2005) Online Mendelian Inheritance in Man (OMIM), a knowledgebase of human genes and genetic disorders. Nucleic Acids Res., 33, 514517. 12. Wang,J., Duncan,D., Shi,Z. et al. (2013) WEB-based GEne SeT AnaLysis Toolkit (WebGestalt): update 2013. Nucleic Acids Res., 41, 7783. 13. Zhang,B., Kirov,S., and Snoddy,J. (2005) WebGestalt: an integrated system for exploring gene sets in various biological contexts. Nucleic Acids Res., 33, 741748. 14. Pacak,C.A. and Cowan,D.B. (2014) Growth of bone marrow and skeletal muscle side population stem cells in suspension culture. Methods Mol. Biol., 1210, 5161. 15. Hadzhiev,Y., Lele,Z., Schindler,S. et al. (2007) Hedgehog signaling patterns the outgrowth of unpaired skeletal appendages in zebrafish. BMC Dev. Biol., 7, 75. 16. Rodriguez,L.J., Tomas,A.R., Johnson,A. et al. (2013) Recent advances in the study of limb development: the emergence and function of the apical ectodermal ridge. J. Stem Cells, 8, 7998. 17. Rahman,M.S., Akhtar,N., Jamil,H.M. et al. (2015) TGF-b/BMP signaling and other molecular events: regulation of osteoblastogenesis and bone formation. Bone Res., 3, 15005. 18. Pan,A., Chang,L., Nguyen,A. et al. (2013) A review of hedgehog signaling in cranial bone development. Front. Physiol., 4, 61. 19. Rudnicki,M.A. and Williams,B.O. (2015) Wnt signaling in bone and muscle. Bone, 80, 6066. 20. Zanotti,S. and Canalis,E. (2013) Notch signaling in skeletal health and disease. Eur. J. Endocrinol., 168, 95103.

Database, Vol. 2016, Article ID baw127 21. Krakow,D., Robertson,S.P., King,L.M. et al. (2004) Mutations in the gene encoding filamin B disrupt vertebral segmentation, joint formation and skeletogenesis. Nat. Genet., 36, 405410. 22. Van,B.H., Celli,J., van,R.J. et al. (2005) MYCN haploinsufficiency is associated with reduced brain size and intestinal atresias in Feingold syndrome. Nat. Genet., 37, 465467. 23. Forlino,A. and Marini,J.C. (2015) Osteogenesis imperfecta. Lancet, 15, 16571671. 24. Alameddine,H.S. (2012) Matrix metalloproteinases in skeletal muscles: friends or foes? Neurobiol. Dis., 48, 508518. 25. Zhao,F., Pan,X., Zhao,K. et al. (2013) Two novel mutations of fibrillin-1 gene correlate with different phenotypes of Marfan syndrome in Chinese families. Mol. Vis., 19, 751758. 26. Speir,M.L., Zweig,A.S., Rosenbloom,K.R. et al. (2016) The UCSC Genome Browser database: 2016 update. Nucleic Acids Res., 44, 717725. 27. Wang,K., Li,M., and Hakonarson,H. (2010) ANNOVAR: functional annotation of genetic variants from high-throughput sequencing data. Nucleic Acids Res., 38, e164. 28. Sherry,S.T., Ward,M.H., Kholodov,M. et al. (2001) dbSNP: the NCBI database of genetic variation. Nucleic Acids Res., 29, 308311. 29. 1000 Genomes Project Consortium (2012) An integrated map of genetic variation from 1,092 human genomes. Nature, 491, 5665. 30. Horton,W.A., Hall,J.G., and Hecht,J.T. (2007) Achondroplasia. Lancet, 370, 162172. 31. Lindhurst,M.J., Sapp,J.C., Teer,J.K. et al. (2011) A mosaic activating mutation in AKT1 associated with the Proteus syndrome. N. Engl. J. Med., 365, 611619. 32. Cho,T.J., Lee,K.E., Lee,S.K. et al. (2012) A single recurrent mutation in the 5'-UTR of IFITM5 causes osteogenesis imperfecta type V. Am. J. Hum. Genet., 91, 343348. 33. Yamamoto,G.L., Baratela,W.A., Almeida,T.F. et al. (2014) Mutations in PCYT1A cause spondylometaphyseal dysplasia with cone-rod dystrophy. Am. J. Hum. Genet., 94, 113119. 34. Schmidts,M., Vodopiutz,J., Christou,S.S. et al. (2013) Mutations in the gene encoding IFT dynein complex component WDR34 cause Jeune asphyxiating thoracic dystrophy. Am. J. Hum. Genet., 93, 932944. 35. Martin,C.A., Ahmad,I., Klingseisen,A. et al. (2014) Mutations in PLK4, encoding a master regulator of centriole biogenesis, cause microcephaly, growth failure and retinopathy. Nat. Genet., 46, 12831292.

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/298797075

Medicalimageclassificationusingspatial adjacenthistogrambasedonadaptivelocal binarypatterns
ArticleinComputersinBiologyandMedicineMarch2016
DOI:10.1016/j.compbiomed.2016.03.010

CITATION

READS

1
6authors,including: HuilingChen WenzhouUniversity
67PUBLICATIONS765CITATIONS
SEEPROFILE

92

AllcontentfollowingthispagewasuploadedbyHuilingChenon22April2016.

Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Computers in Biology and Medicine 72 (2016) 185 200

Contents lists available at ScienceDirect

Computers in Biology and Medicine
journal homepage: www.elsevier.com/locate/cbm

Medical image classification using spatial adjacent histogram based on adaptive local binary patterns
Dong Liu a,b, Shengsheng Wang a,b,n, Dezhi Huang a,b, Gang Deng c, Fantao Zeng a,b, Huiling Chen d
a

Jilin University, College of Computer Science and Technology, Changchun 130012, China Jilin University, Key Laboratory of Symbolic Computation and Knowledge Engineering of the Ministry of Education, Changchun 130012, China c Renmin Hospital of Wuhan University, Department of Neurosurgery, Wuhan 430060, China d College of Physics and Electronic Information Engineering, Wenzhou University, Wenzhou 325035, China
b

art ic l e i nf o
Article history: Received 2 September 2015 Received in revised form 16 March 2016 Accepted 16 March 2016 keywords: Local binary patterns Image classification Feature extraction Medical images Microscope images

a b s t r a c t
Medical image recognition is an important task in both computer vision and computational biology. In the field of medical image classification, representing an image based on local binary patterns (LBP) descriptor has become popular. However, most existing LBP-based methods encode the binary patterns in a fixed neighborhood radius and ignore the spatial relationships among local patterns. The ignoring of the spatial relationships in the LBP will cause a poor performance in the process of capturing discriminative features for complex samples, such as medical images obtained by microscope. To address this problem, in this paper we propose a novel method to improve local binary patterns by assigning an adaptive neighborhood radius for each pixel. Based on these adaptive local binary patterns, we further propose a spatial adjacent histogram strategy to encode the micro-structures for image representation. An extensive set of evaluations are performed on four medical datasets which show that the proposed method significantly improves standard LBP and compares favorably with several other prevailing approaches. & 2016 Elsevier Ltd. All rights reserved.

1. Introduction Medical images have played an important role in the diagnostic workup of patients. Automated classification of medical images is a desirable tool to assign the interpretation of images, and then would help the expert in diagnosis of diseases [13]. Compared with general image recognition, medical image recognition is more challenging because of the higher ambiguity and complexity; most of the medical image contents are quite similar, but also different in their emphasis. In terms of the features used for medical image recognition, it can be mainly classified into three groups: shape, color, and texture features. For example, in [4], shape features such as moment invariants and Fourier descriptor are employed to classify medical X-ray images. A color vector field is considered in [5] for improving the performance of endoscopic image classification. The local binary patterns, first proposed by [6], are widely considered as a state-of-the-art image feature descriptor among
n Corresponding author at: Jilin University, College of Computer Science and Technology, Changchun 130012, China. E-mail address: wss@jlu.edu.cn (S. Wang).

texture descriptors, since it can more effectively describe texture information. It has been successfully applied to many applications, such as face recognition, texture classification, scene recognition, human detection and others. LBP has several attractive advantages: it has proven to be a powerful discriminator with low computational cost, it is robust against changes in image intensity, and it can be easily implemented. Due to these merits, it makes a good choice for extracting fine features for medical images. However, the standard LBP still suffers from several drawbacks, including limited semantic description of local patterns, sensitive to non-uniform patterns and affine transformation, and missing of efficient spatial encoding among patterns. To overcome these shortcomings, numerous works [911] focused on improving LBP in recent years, in terms of rotation-invariant, multi-scale, the utilization of non-uniform patterns, and so on. There are two types of LBP patterns: uniform and non-uniform patterns. Some works, such as [7], only considered uniform patterns for extracting LBP features since non-uniform patterns involve noise and high dimensionality. And the work [8] proposed a hierarchical multiscale LBP to further utilize the information of non-uniform patterns. They also certify that, the percentage of non-uniform patterns increases as the neighborhood radius increases. To reduce the LBP dimensionality, center-symmetric local binary patterns

http://dx.doi.org/10.1016/j.compbiomed.2016.03.010 0010-4825/& 2016 Elsevier Ltd. All rights reserved.

186

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200

(CSLBP) [15] is studied and applied to image recognition. Since LBP is sensitive to noise in near uniform regions, Local Ternary Pattern [14] with three value coding scheme was proposed to address this problem. The rotation invariant [6] descriptor can be obtained through the circular neighborhood definition, but in some cases the anisotropic structural information is lost. To utilize these anisotropic structural information, a novel elliptical binary pattern (EBP) [16] has been proposed for face recognition, in which elliptical neighborhood definitions are studied. Completed LBP [13] utilizes both the sign and magnitude information in the difference between the central pixel and the neighborhood pixels. In the work [18], LBP is combined with Gabor filters to achieve a better classification performance. The study [19] extracted the most frequent patterns in LBP histogram and formed a novel descriptor, which achieved better performance with this technique. Mesh-LBP [21] is novel method which computed the mesh-local binary pattern on a triangular-mesh manifold. In [22], a scale- and rotation-invariant LBP is proposed, in which the rotation-invariant is combined with a scale-adaptive texton for texture classification. SOALBP [23] constructed a novel scale- and orientation invariant LBP feature combined in a multi-resolution representation, which has been proven superior in texture classification. The basic idea behind LBP is that it describes an image by local patterns. The existing methods have been proven to improve the LBP to some extent by reconfiguring or utilizing the patterns. However, most of existing works encode the binary patterns in a fixed neighborhood radius. This fixed neighborhood radius strategy is irrelevant to local image content and disregards microstructure information of the multi-scale patterns. Intuitively, the micro-structures, i.e. the spatial relationships among local patterns generated by adaptive radius, provide crucial feedback in disambiguating texture information especially for complex medical images, i.e. microscope images that involve with pathological changes. This subsequently leads to improved recognition performance. To this end, our target is to design a novel LBP histogram representation for medical images to (1) compute the local binary patterns in an adaptive neighborhood radius, and (2) encode micro-structures among the multi-scale patterns. In the first stage, with the help of gradient operators, we obtain a gradient map from each original image, and the adaptive LBP neighborhood radius could be then determined for each pixel by utilizing the gradient information. As a result, our adaptive strategy will assign a relatively small radius to pixels that are located in local regions with dramatic gray variation, while assigning a relatively large radius to pixels that are located in local regions with slight gray variation. This adaptive technique will provide the image with rich micro-structure textures, which is discriminative in image representation. Then in the next stage, we propose a spatial adjacent histogram based on adaptive LBP radius to describe these discriminative micro-structure features. Finally, the adaptive LBP

radius and spatial adjacent histogram strategies produce a much more powerful LBP variant, which performs well in four benchmark medical datasets and compares favorably to other methods. In this context, our contribution is threefold. 1) Using the adaptive strategy we proposed, the neighborhood radius of LBP is determined based on local image content, therefore more adaptive and useful features can be obtained. 2) We propose to use spatial adjacent histogram to encode the micro-structures produced by adaptive strategy, which results in convincing improvement on the standard LBP histogram. 3) Our approach also considers three LBP coding schemes, i.e. set the threshold T in three different ways when computing the LBP value. And we further evaluate the three LBP coding schemes in order to find which one performs more competitive in medical image classification. The remainder of this paper is organized as follows. In Section 2, we introduce the proposed algorithm, i.e. spatial adjacent histogram based on adaptive local binary patterns for image classification. Section 3 presents an extensive set of experimental evaluations on four medical image datasets, and finally, in Section 4, we draw the conclusions.

2. Spatial adjacent histogram based on adaptive local binary patterns for image classification In this section, we propose a novel idea using spatial adjacent histogram based on adaptive local binary patterns for medical image classification. We first present a concise review of standard local binary patterns (LBP) in subsection 2.1. Next, we explain how to determine the adaptive radius for each pixel in subsection 2.2, then in subsection 2.3, three coding schemes are introduced to compute the LBP value for each pixel. In subsection 2.4, we propose a spatial adjacent histogram technique based on adaptive LBP to represent the whole image. Finally, the proposed image classification framework using our spatial adjacent histogram based on adaptive local binary patterns is presented in subsection 2.5. 2.1. Brief review of LBP Given an image I, the LBP is a gray-scale texture operator that characterizes the local spatial patterns of the image texture, which is calculated at each pixel by evaluating the binary differences between it and its neighbors: ( P 1 X 1; x Z 0 LBP P ; R  sg i  T 2i ; sx  1 0; x o 0
i0

where P is the number of pixels in the neighborhood, R is the

Fig. 1. Three circularly symmetric neighborhood sets for different (P, R).

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200

187

radius of the neighborhood and T is a threshold. The original LBP set T as the gray value of the central pixel and gi is the gray value of its neighborhood. Fig. 1 shows examples of different configurations of (P, R). A pattern is considered uniform if the number of transitions in the sequence between 0 and 1 is less than or equal to two. For instance, LBP pattern 00100000 is uniform (2 times transition) and 00101000 is non-uniform (4 times transition). 2.2. Determining the adaptive radius R In order to extract micro-structures of different scales, the key idea behind this paper is to adaptively obtain the LBP radius of each pixel by analyzing the differences based on calculated gradients. Given an image I and we use f(x,y) as the gray value of pixel (x, y), then the Sobel [20] gradient magnitude g(x, y) can be obtained by Eq. (2) and Eq. (3). 8 g   f x  1; y  1  2f x; y  1  f x  1; y  1 > > > x > <   f x  1; y  1  2f x; y  1  f x  1; y  1 2 g y   f x  1; y  1  2f x  1; y  f x  1; y  1 > > > > :   f x  1; y  1  2f x  1; y  f x  1; y  1 g x; y  qffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi gx 2  gy 2 3

left to right. In each adaptive radius map, four colors denote different LBP radii for each pixel, i.e. red denotes R  4, green denotes R  3, blue denotes R  2 and black denotes R  1. The first and second original images in Fig. 3 are taken from the two different categories (microtubules and endosome) of 2D HeLa dataset [28], respectively, in which the pixels outside the objects are not considered since it represents the black background. It is observed that Fig. 3(a) and Fig. 3(b) have much of different spatial distribution of adaptive radius, and the micro-structures produced by neighbor adaptive radius are much discriminative for two images. Another intuitive instance can be found in Fig. 3(c), in which the original image is a scene image. Therefore, one could conclude that the micro-structures of neighbor adaptive radius are much related to image contents. Furthermore, we could discover that our adaptive strategy assigned a relatively small radius to pixels that are located in local regions with dramatic gray variation, while assigning a relatively large radius to pixels that are located in local regions with slight gray variation. In the rest of this paper, we will show how to utilize these discriminative features coming from micro-structures. 2.3. LBP coding schemes based on adaptive radius In this subsection, we formulate the process of LBP coding based on adaptive radius R. Given a center pixel gc, the neighborhood radius R can be computed by Eq. (5). Inspired by state-ofthe-art LBP coding strategy, three coding schemes are selected based on setting different thresholds to T of Eq. (1) adaptively, which are named T1, T2 and T3 for short, respectively. And LBPT1, LBPT2 and LBPT3 denote standard LBPs obtained by the three coding schemes, respectively. We start with an original image I, let gc be a center pixel and gi denotes its neighborhood pixel. P denotes the total number of the neighbors, and R denotes the adaptive radius of gc obtained during the process in subsection 2.2. (1) The first coding scheme T1 Set threshold T as the gray value of the central pixel, i.e. T1  gc. thus, the LBPT1 can be formulated as LBPT 1 
P 1 X i0

Then, given a center pixel gc and its neighborhood radius k, the average gradients Gk x; y in (2k  1)  (2k  1) image blocks is defined as follows:
x k P y k P

Gk x; y 

i  xk j  yk 2

g i; j ; k A f1; 2; :::; Rmax g 4

2k  1

where Rmax is the maximal search radius. Finally, the gray variation is defined as the intensity difference among the average gradients in different sizes of image blocks. Thus, the neighborhood radius Rgc of the central pixel gc can be obtained according to the maximum value:   Rgc  arg max Gk  1  Gk ; k A f1; 2; :::; Rmax g 5
k

An illustration of the proposed adaptive radius computing process for a central point in a 7  7 blocks is provided in Fig. 2. The input central point and its neighborhood of 7  7 blocks, which are described as gradient magnitude, appears on the left part of Fig. 2. Then the three scales of blocks and the average gradients are defined by Eq. (4). It is due to the fact that the most dramatic change lies between G2 and G3, so the adaptive radius for the central pixel is set as 2, which are presented in the right part of Fig. 2. This means the outermost neighborhood pixels are not considered based on our adaptive radius strategy, because they adopt irrelevant gray value and may be harmful for the robustness. Three toy examples of adaptive radius map are illustrated in Fig. 3. In each example, the original image, adaptive radius map and the percent of pixels with different radius are arranged from

sg i  T 1 2i

6

(2) The second coding scheme T2 Let mi  | gi  gc |, set T2 as the mean value of mi, i.e. T2 
P 1 X i0

mi =P 

P 1 X i0

j g i  g c j =P

7

Fig. 2. An example of proposed adaptive radius computing process for determining the radius.

188

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200

Fig. 3. Toy examples of adaptive radius map, i.e. different radius of each pixel: red denotes R  4, green denotes R  3, blue denotes R  2 and black denotes R  1. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

Then, the LBPT2 are presented as follows: LBPT 2 
P 1 X i0

Then we can define the third LBPT3 as follows 8 LBPT 3 
P 1 X i0

smi  T 2 2i 

P 1 X i0

  sg i  g c   T 2 2i

sg i  T 3 2i

10

(3) The third coding scheme T3 Set T3 as the mean gray value of the (2R  1)  (2R  1) image blocks, i.e. T3 
N X i1

g i =N ;

N  2R  1  2R  1

9

Fig. 4 illustrates the coding process using different thresholds for the 3  3 sample blocks. Note that the first and second coding schemes are equivalent to the standard LBP [6] and LDBP [12], respectively. For visualization, LBP transformed the images by replacing each pixel of the image with its LBP value, these are also presented. Fig. 5 shows examples of LBP transformed images with three coding schemes. From the examples, we can see that all schemes retain the global structure of the image, but display much diversity in capturing the local structures. In the next experiments, we will evaluate performances of the three coding schemes based

Fig. 4. Coding process of the three thresholds. (Here R  1, P  8).

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200

189

Fig. 5. Example image and its LBP transformed images with three coding schemes.

on our proposed algorithm to explore which one is more competitive in medical image classification. 2.4. Spatial adjacent histograms based on adaptive LBP From the Fig. 3, one can see that adaptive radius map provides the image with rich micro-structure textures. These microstructures reflect discriminative features and can be well represented by the approaches of histogram analysis. In this subsection, we explain how to construct the spatial adjacent histogram for an image when considering both micro-structures and adaptive radius patterns. As mentioned in subsection 2.2, each pixel of an image has different neighborhood radius during our adaptive strategy, however, different configurations of (P, R) will result in LBPTi of each pixel mapping to different dimension. In this paper, we set the number of involved adaptive neighbors as 8, i.e. P  8, in a similar fashion to [22]. Besides, our spatial adjacent histogram encoding strategy will result in the final feature vector with 2  Rmax  2p dimensions. This means that increasing the values for P and Rmax will make the feature dimension significantly large, that will further increase the time complexity. Considering the trade-off between description and performance, we set the maximal search radius Rmax as 4. Moreover, increasing Rmax means that more image pixels are involved to interpolate and form neighborhoods with fixed number P  8. This may bring more noise to the interpolated neighborhoods. So we think it is appropriate to set Rmax as 4.

Then, four histograms with 256 bins under four radii can be extracted for each image: X hij Rk  isequallbpj  i; i  1; :::; D; k  1; :::; 4 11
j A Rk

where Rk indicates the set of pixels with radius R  k, and D denotes the dimension of LBP histogram (here D is equal to 256). The function lbp(j) returns the LBP value of pixels j, and the function isequal(lbp(j)  i) returns 1 if lbp(j)  i, else returns 0. Then for image representation, an easy option is that we directly concatenate (for short, we name this histogram construction strategy as DC strategy) the four histograms under the different radii, then the concatenated histogram DCLBP is as follows. DCLBP T i  H T i R1   H T i R2   H T i R3   H T i R4 ; i A f1; 2; 3g 12

where (  )indicates the operation of concatenation, superscript Ti denotes the three coding schemes we have selected. A brief example is shown in Fig. 6(a). Based on the preliminary experimental results, we found that the pixels which have been assigned R  1 are sparse but discriminative. We propose to use R  1 to calculate the spatial relation between them and pixels with larger radii for two reasons: (1) As shown in Fig. 3, the pixels with R  1 are mainly located in the regions with dramatic changes in gray value. The pixels with dramatic changes are also involved with ample microstructures of different scales, i.e. spatial correlation of neighbor pixels with different LBP radii. We do not choose pixels

Fig. 6. The two strategies of combining sub-histograms of pixels with different radii to represent the image. (a) the directly concatenate strategy; (b) the spatial adjacent histograms strategy.

190

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200

Table 1 The percentage of non-uniform patterns in medical datasets. R  1, P  8 (%) 2D-Hela [28] Hep-2 [29] PAP [30] 27.84 10.96 13.82 R  2, P  8 (%) 38.24 18.78 18.58 R  3, P  8 (%) 44.21 21.86 22.06 R  4, P  8 (%) 45.13 26.89 24.82

is not discarding the non-uniform local patterns, since some non-uniform patterns also can provide much useful features. And how to dig out the useful non-uniforms among all nonuniforms is another topic. Therefore, instead of using original histograms of pixels with R  1, we propose to use spatial adjacent histogram strategy to embed the micro-structures information to hij R1 . For each pixel j A R1 of an image, we calculate not only the number of occurrences of each LBP (j) in the image, but also the number hij R1 ; Rk , that is the number of pixels with radius Rk which appears next to pixel j. X X isequalLBP j  i; k  1; 2; 3; 4 13 hij R1 ; Rk  
j A R1 ;w A Rk w A N j

with larger radii for constructing the spatial adjacent histogram, such as R  4, since they are mainly located in regions with relative smooth change in gray value. Intuitively, the spatial correlation between pixels with dramatic changes and its neighbor pixels provide crucial feedback in disambiguating texture features. (2) Using R  1 is helpful to incorporate uniform idea. It has been validated that uniform patterns show superiority in texture classification and some non-uniform patterns involve noise [7]. Uniform patterns are also considered as the major parts of all patterns. Moreover, [8] certify that, the percentage of nonuniform patterns increases as the neighborhood radius increases. Table 1 shows the percentage of non-uniform patterns in three medical datasets and presents the same conclusion with [8]. An appropriate way of incorporating uniform idea and avoiding noise caused by some nonuniform patterns, in our case, is to choose pixels with R  1 (that involve the most uniform patterns) as baseline to construct the spatial adjacent histogram. Note that our goal

Where N(j) is the four-neighbor system of j. Then the spatial adjacent histogram (SAH) for pixels set with radius R  1 can be defined as: SAHR1   fhij R1 ; hij R1 ; R1 ; hij R1 ; R2 ; hij R1 ; R3 ; hij R1 ; R4 g; i  1; :::; 256 14

It is easy to find that the dimension of h(i)|(R1,Rk) is 256, resulting in that the dimension of SAH(R1) is 256  256*4  1280. Thus, the dimension of the histogram in Formula (14) is quintuple larger than that in Formula (11). A toy example of explaining how to count SAH(R1) is present in Fig. 7, we design three image patches with 5  5 size, i.e. 25 pixels. To simplify the visualization, note that, we assume that the pixels

Fig. 7. Histograms for representing the pixels set with radius R  1.

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200

191

with radius R  1 only be assigned to four types of LBP values, i.e. A to D (0 o A o B o C o D o 255). Furthermore, each pixel has an adaptive radius which is marked by different colors and textures, i.e. yellow pixel with no texture denotes R  1, green pixel with vertical texture denotes R  2, blue pixel with crossed texture denotes R  3, and red pixel with horizontal texture denotes R  4. Then the corresponding histogram and feature vectors are shown in Fig. 7. The four black bins with gradual change in each histogram denote the number of occurrences of A, B, C, and D, respectively. Note that the numbers of occurrences of A, B, C, and D are the same in the three image patches, i.e. for all image patches: h (A)|R1  3, h(B)|R1  2, h(C)|R1  2 and h(D)|R1  2. Following each black bin with gradual change, there are four bins with different textures which denote the number of four-neighbor pixels with four different radii, i.e. h(A)|(R1,R1), h(A)|(R1,R2), h(A)|(R1,R3), and h (A)|(R1,R4), respectively. More specifically, looking at the image patches in Fig. 7(a), the first black bin with gradual change denotes the number of occurrences of A. Since there exist three yellow pixels with no texture located in the four-neighbors of all type A, then h(A)|(R1,R1)  3, drawn as the first yellow bin with no texture. Note that if two fourneighbor pixels are marked as the same type, then we count it only one time to update the histogram. For example, two pixels marked with A in Fig. 7(a) are mutual four-neighbors, but we count it only one time to update h(A)|(R1,R1). There is only one

green pixel with vertical texture located in the four-neighbors of all type A, so h(A)|(R1,R2)  1, drawn as the first green bin with vertical texture. There is also only one blue pixel with crossed texture located in the four-neighbors of all type A, so h(A)|(R1,R3)  1, drawn as the first blue bin with crossed texture. And there are four red pixels with horizontal texture located in the fourneighbors of all type A, so h(A)|(R1,R4)  4. It is similar for counting the other non-black bins, i.e. h(B)|(R1,R1), h(B)|(R1,R2), h(B)|(R1, R3), h(B)|(R1,R4), h(C)|(R1,R1), h(C)|(R1,R2), h(C)|(R1,R3), h(C)|(R1,R4), h (D)|(R1,R1), h(D)|(R1,R2), h(D)|(R1,R3) and h(D)|(R1,R4). Comparing the three image patches in Fig. 7, using the original LBP histogram model, the pixels set with radius R  1 in three image patches are represented as the same feature histogram (3,2,2,2) as shown by the black bins in each histogram. But using the histograms obtained by our spatial adjacent histogram (SAH) strategy, these image patches can be effectively distinguished from each other. This indicates that the micro-structure features have been effectively utilized in SAH strategy, that had been ignored in DC strategy. As explained above, for an image, we can obtain four histograms, i.e. one histogram with 1280 dimensions to represent the pixels set with radius R  1 (see Eq. (14) ), and three histograms with 256 dimensions to represent the pixels set with radii R  2, 3, 4, respectively (see Eq. (11)). Then, the final LBP feature vectors

Fig. 8. Example images and its spatial adjacent histograms (the third coding scheme T3 is employed here).

192

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200

with our spatial adjacent histograms strategy can be computed as: SAHLBP  SAH R1   H R2   H R3  H R4 ; i A f1; 2; 3g 15 where (  ) indicates the operation of concatenation, superscript Ti denotes the three coding schemes above. And the dimension of SAHLBP is 1280  (256*3)  2048. Fig. 6 shows the comparison of the directly concatenate strategy and the spatial adjacent histograms strategy to represent the images. In order to present an intuitive example, Fig. 8 shows four images and its SAHLBP histogram features. The first two images come from the `golgpp' category of 2D HeLa dataset, while the last two images come from the `nucleolus' category of 2D HeLa dataset. Though it may be a challenge to distinguish the two categories with human vision, but the SAHLBP histogram features can provide much discriminative information to distinguish the two different types of patterns. Therefore, our SAHLBP method can be expected to perform well in medical image classification, we will give a complete evaluation in next experiments. 2.5. The proposed image classification framework As illustrated in Fig. 9, the framework for medical image classification consists of four steps, i.e. determining the adaptive LBP radius for each pixel, computing the spatial adjacent histogram based on adaptive LBP radius, representing the image based on the spatial adjacent histograms, and training a SVM classifier for recognizing new samples. Furthermore, the first three steps can be considered as feature extraction for medical image, which is the key idea of our proposal.
Ti Ti Ti Ti Ti

use. Experiments are performed on the following four reference datasets. 3.1.1. The 2D HeLa dataset This dataset [28] is composed of 862 single-cell images (16 bit gray scale of size 512 by 382 pixels) from ten classes, i.e. Actin, Nucleus, Endsome, ER, Golgi Giantin, Golgi GPP130, Lysosome, Microtubules, Mitochondria, and Nucleolus. The description of the dataset in terms of samples per class is presented in Table 2, and some sample images from the ten categories are shown in Fig. 10. In our experiments, the LBP based histogram bin with the highest occurrence is discarded since it represents the black background. 3.1.2. Hep-2 cell dataset Hep-2 cell dataset [29] consists of 28 slide images, and each image contains several cells that are segmented by specialist. The total number of cells is 1455, including training set with 721 cells and testing set with 734 cells. And each cell has been assigned to one of the six categories: centromere, homogeneous, nucleolar, coarse speckled, fine speckled, and cytoplasmatic. A summary of the Hep-2 cell dataset in terms of number of training set, testing set and classes is reported in Table 3. Some sample images are also presented in Fig. 11. 3.1.3. PAP dataset The PAP smear dataset [30] contains 917 samples collected at the Herlev University Hospital using a digital camera and microscope. As shown in Table 4, the samples belong to seven different classes. And two skilled cyto-technicians further classified each sample into two super classes (normal and abnormal). The sample images that came from the two super classes are shown in Fig. 12. 3.1.4. The brain tumor dataset The brain tumor dataset used throughout this experiment consists of 285 images of size 256*256, which was collected at the department of neurosurgery in Renmin Hospital of Wuhan University, P.R China. All samples are derived from real pathological images and 19 patients who suffer different types of brain tumor

3. Experimental results 3.1. Experimental datasets The medical datasets for the evaluation of the proposed method are selected based on their variety and broadness of their

Fig. 9. Overview of the proposed image classification framework.

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200 Table 2 A summary of the 2D HeLa dataset: classes and number of samples per class. Class Actin Nucleus Endsome ER Golgi Giantin Golgi GPP130 Lysosome Microtubules Mitochondria Nucleolus Total Number 98 87 91 86 87 85 84 91 73 80 862 Centromere Homogeneous Nucleolar Coarse speckled Fine speckled Cytoplasmatic Total 208 150 102 109 94 58 721 149 180 139 101 114 51 734 357 330 241 210 208 109 1455

193

Table 3 A summary of the Hep-2 cell dataset: classes and number of samples per class. Class Training set Testing set Number of each class

are involved. In pathological analysis, the high resolution microscopy (Olympus BX51) was used to perform on the tumor lesion in order to get a histopathological diagnosis. Then five categories of microscopic images could be obtained according to the five types of brain tumor, i.e. astrocytoma, tuberculum sellae meningioma, olfactory groove meningioma, acoustic neuroma and pituitary tumor. Table 5 reports the number in each class and Fig. 13 depicts the microscopic images from each category. Note that the original images in Hep-2 cell, PAP and brain tumor datasets are color images. In this study, the LBP variants and SIFT descriptors are extracted from the grayscale version of the corresponding images. The grayscale values are transformed by a weighted sum of R (red), G (green), B (blue) channels, i.e. Gray  0.2990 * R  0.5870 * G  0.1140 * B. 3.2. Experimental setup and implementation We implement the experiments in an iterated random splitting-scheme (consistent among all methods) for training and testing. Each dataset is randomly divided into 80% for training and 20% for testing. We use the training-split to optimize the SVM parameters and train model in a 5-fold cross validation. And then the trained model is employed for the test-split. The random splitting is repeated 30 times. For performance quantification, the

average mean classification accuracy over 30 times iterations with its standard deviation is reported. In order to use this evaluation protocol consistently in all datasets, we merge the training and testing set for Hep-2 cell dataset, as shown in Table 3. Since different classes may have different numbers of samples, we use stratified cross validation instead of standard cross validation to handle this imbalance of image numbers among classes. In stratified cross validation, the distribution of samples among classes in each subset is basically consistent with that in original dataset. For the classifier, we employ the SVM with linear kernel in the next experiment. For the implementation of the SVM classifier, the public LIBSVM library [31] is employed. 3.3. Evaluation of our methods In this section, we report our experimental results using the methods and dataset with performance indicators described in previous sections. Evaluation of the proposed model is accomplished through two different tasks: (1) evaluate the performance of three different coding schemes for medical image datasets; (2) verify that the spatial adjacent histograms is effective to increase the classification accuracy based on our adaptive radius strategy. For the first task, we report on the classification accuracy of standard LBP with three scales (R  1, P  8), (R  2, P  8), and (R  2, P  16) and our method that adopted three coding schemes respectively, i.e. LBPT1(R  1, P  8), LBPT2(R  1, P  8), LBPT3(R  1, P  8),

Fig. 10. Ten type of 2D-Hela images.

194

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200

Fig. 11. Six type of Hep-2 cell image: homogeneous, centromere, nucleolar, fine speckled, coarse speckled and cytoplasmatic. Table 4 A summary of the PAP smear dataset: classes and number of samples per class. Class Superficial squamous epithelial Intermediate squamous epithelial Columnar epithelial Mild squamous non-keratinizing dysplasia Moderate squamous non-keratinizing dysplasia Severe squamous non-keratinizing dysplasia Squamous cell carcinoma in situ intermediate Pap dataset Super classes Total 74 70 98 182 146 197 150 Normal Normal Normal Abnormal Abnormal Abnormal Abnormal 242

(2)

675

LBPT1(R  2, P  8), LBPT2(R  2, P  8), LBPT3(R  2, P  8), LBPT1(R  2, P  16), LBPT2(R  2, P  16), LBPT3(R  2, P  16), SAHLBPT1, SAHLBPT2 and SAHLBPT3, as shown in Table 6. In each table, the best results in each dataset are bolded. For the second task, we compare the methods generated by spatial adjacent histograms (SAH) strategy and directly concatenate (DC) strategy under the three coding schemes, respectively, i.e. DCLBPT1 and SAHLBPT1, DCLBPT2 and SAHLBPT2, DCLBPT3 and SAHLBPT3. The results are also reported in Table 6. Examining the Table 6, we can make the following conclusions: (1) Our SAHLBP models consistently outperform standard LBP with (R  1, P  8) and (R  2, P  8) on the four medical datasets, no matter which coding schemes that are employed. Our SAHLBP models also outperform standard LBP with R  2, P  16 in all cases except that perform on PAP dataset with T2, i.e. only SAHLBPT2 is inferior to LBPT2(R  2, P  16) on PAP dataset. Moreover, the dimension of our method (i.e. 2048 dimensions) is much smaller than that of LBPT(R  2, P  16) (i.e. 65536 dimensions). This is probably because standard LBPs cannot capture spatial relationships among local textures, while our SAHLBP models describe micro-structures obtained by

(3)

(4)

(5)

adaptive radius and perform successfully well in capturing the valuable structural information, which is beneficial to medical image classification. The models with the third coding scheme T3 achieved higher accuracy, on the average, than that with the first and second coding schemes. More specifically, for the standard LBP with R  1and P  8, the first coding scheme T1 gained the best performance in the 2D-Hela and Tumor datasets, while T3 gained the best performance in the PAP smear and Hep-2 cell datasets. For the standard LBP with R  2 and P  16, T1 gained the best performance in the 2D-Hela dataset and T3 proves superior in the remaining datasets. For SAHLBPs, the T3 performed better than the other coding schemes in the PAP smear, 2D-Hela and Tumor datasets, while the T1 achieved the best performance in the Hep-2 cell datasets. Particularly interesting are the results of the methods with the second coding scheme T2, performs slightly worse than the other two schemes in most of the cases. However, the coding scheme T2 has been proven in [12] that it describes the global structure successfully and has a good performance in natural scene image classification. From this observation, we can appreciate that the micro-structure features also remain very relevant to coding scheme. LBPTi with (R  2, P  16) performs better than DCLBPTi. We conjecture that it is because the LBPTi with (R  2, P  16) captures more texture information with very large feature dimension (i.e. 65536 dimensions), while the feature dimension of DCLBPT is 1024. However, the DCLBPTi achieves higher accuracies than LBPTi with both (R  1, P  8) and (R  2, P  8) in most cases. The comparison between the DCLBPTi and LBPTi suggests that the adaptive radius is not very suitable to combine with directly concatenate strategy. We mainly compare the DCLBPTi with SAHLBPTi to show the superiority of adaptive radius combined with spatial adjacent histogram. Our spatial adjacent histogram (SAH) based on adaptive radius is better than directly concatenate strategy for all datasets, no

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200

195

Fig. 12. Sample images from the pap smear dataset. Table 5 A summary of the brain tumor dataset: classes and number of samples per class. Class Astrocytoma Tuberculum sellae meningioma Olfactory groove meningioma Acoustic neuroma Pituitary tumor Total Number 58 54 54 56 63 285

matter which coding schemes T adopted, i.e. SAHLBPTi is always better than DCLBPTi. Therefore, one can conclude that SAH is highly suitable for representing the micro-structures which is discriminative in medical image classification. To provide more robust results, we further perform the Wilcoxon ranksum test in combination with the iterated splitting scheme to compare the methods. More specifically, for each method in Table 6, we can obtain 30 classification results during 30 times splitting iterations for each dataset. Then for a pair of compared methods, we perform Wilcoxon ranksum test with their 30 classification results over one specific dataset.

Fig. 13. Microscope images for different types of brain tumor lesion. (a) astrocytoma; (b) tuberculum sellae meningioma; (c) olfactory groove meningioma; (d) acoustic neuroma; (e) pituitary tumor.

196

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200 Table 7 Classification accuracy of compared methods over four datasets. Methods 2D-Hela 85.24 7 2.4 83.23 7 1.9 84.31 7 2.1 86.07 7 2.5 82.59 7 1.8 85.39 7 2.2 87.32 7 2.2 86.23 7 1.9 86.76 7 2.1 84.96 7 1.4 85.34 7 1.4 87.84 7 2.2 89.68 7 2.0 87.29 7 1.6 90.06 7 1.5 Hep-2 75.65 7 1.1 72.17 7 1.2 79.94 7 1.6 82.14 7 1.4 76.23 7 1.5 80.23 7 1.1 87.67 7 1.4 85.23 7 1.0 87.92 7 1.2 81.31 7 0.8 76.39 7 1.0 82.22 7 1.1 91.89 7 0.8 88.95 7 0.8 91.86 7 0.9 Tumor 75.41 7 1.8 74.45 7 1.7 75.22 7 1.8 75.04 7 2.1 73.62 7 2.1 73.78 7 2.1 79.17 7 1.6 78.82 7 1.9 79.25 7 2.0 77.69 7 1.9 75.22 7 1.8 78.13 7 1.5 82.91 7 1.6 81.24 7 2.1 84.48 7 1.8 LBP-r ELBP LBPsu2 LBPnr LBP-HF LDBP CLBP CoALBP SAHLBPT3 PAP 75.5 7 1.2 80.02 7 1.5 82.42 7 1.3 83.14 7 1.8 84.13 7 1.5 81.26 7 1.7 87.21 7 1.4 87.02 7 1.9 88.03 7 1.7 2D-Hela 81.21 7 1.9 85.11 7 1.7 84.82 7 1.5 85.76 7 2.1 85.14 7 2.0 82.5 7 1.9 88.91 7 1.1 87.72 7 2.1 90.06 7 1.5 Hep-2 75.23 7 0.9 80.17 7 1.1 83.23 7 0.6 82.76 7 1.3 87.23 7 1.0 79.58 7 0.7 87.91 7 0.9 92.76 7 1.1 91.86 7 0.9 Tumor 73.71 7 1.4 75.82 7 1.5 76.72 7 0.6 78.29 7 2.0 77.75 7 1.7 75.11 7 2.0 81.12 7 1.8 81.33 7 1.9 84.48 7 1.8

Table 6 Classification accuracy of each method over four datasets. Methods Datasets PAP LBPT1(R  1, P  8) LBPT2(R  1, P  8) LBPT3(R  1, P  8) LBPT1(R  2, P  8) LBPT2(R  2, P  8) LBPT3(R  2, P  8) LBPT1(R  2, P  16) LBPT2(R  2, P  16) LBPT3(R  2, P  16) DCLBPT1 DCLBPT2 DCLBPT3 SAHLBPT1 SAHLBPT2 SAHLBPT3 81.12 7 2.3 80.92 7 1.7 81.87 7 1.5 80.34 7 2.5 79.76 7 2.5 82.78 7 2.3 85.47 7 2.2 84.56 7 2.0 85.56 7 1.9 83.96 7 1.9 82.14 7 1.9 83.29 7 1.9 86.69 7 2.1 84.03 7 2.1 88.03 7 1.7

More specifically, for 2D-HeLa dataset, we have compared the following approaches using the Wilcoxon ranksum test: 1. SAHLBPTi versus LBPTi (i  1, 2, 3) with different scales, SAHLBPTi wins against LBPTi with all scales (R  1, P  8), (R  2, P  8), and (R  2, P  16), i.e. we reject the null hypothesis at the level of significance 0.05, and accept that SAHLBPTi and LBPTi have significant different performance. 2. SAHLBPTi versus DCLBPTi (i  1, 2, 3), SAHLBPTi wins against DCLBPTi. 3. SAHLBPT3 versus SAHLBPTi (i  1, 2), SAHLBPT3 wins against SAHLBPT2, however, SAHLBPT3 and SAHLBPT1 have no significant different performance in 2D-HeLa dataset. We also do the same Wilcoxon ranksum test for the other three datasets, and most of the conclusions remain valid. Very few different conclusions are marked as the following: (1) For PAP and brain tumor datasets, SAHLBPT3 wins against both SAHLBPT1 and SAHLBPT2. (2) For PAP dataset, SAHLBPT2 and LBPT2(with R  2, P  16) have no significant different performance. The p-values for all tests are presented in Appendix A. Finally, we analyze the confusion matrices for SAHLBPT3 which display the best results selected from the 30 times iterations for each dataset. The confusion matrices for the three multiclass datasets, i.e. 2D-HeLa, Hep-2 cell, and brain tumor datasets, are presented in Appendix A. From the results, we can confirm that our method is very useful for some particular patterns, such as actin, dna, golgia, golgpp, nucleolus patterns in 2D-Hela dataset and nucleolar pattern in Hep-2 cell dataset. From the analysis of the above experimental results, it is clear the advantage of our spatial adjacent histogram strategy employed the third coding scheme T3, i.e. the SAHLBPT3. Hence, to achieve a tradeoff between description and performance, we have chosen the SAHLBPT3 in the remainder of the experiments. 3.4. Comparison of the LBP-based methods The tests in this subsection are aimed at comparing our method with the recent LBP variants reported in the literature. In particular, the following methods are evaluated which are often used for medical image classification: 1) LBP-r [6], the standard LBP with rotation invariant; 2) EBP [24], the elliptical binary pattern variant; 3) LBPsu2[22], scale-adaptive and subuniform-based rotation invariant LBP;

4) LBPnr[22], scale-adaptive LBP without rotation invariant, that employs the LBPsu2 approach by using non-rotation invariant instead of rotation invariant. 5) LBP-HF[25], local binary pattern histogram Fourier features; 6) LDBP[12], the local difference magnitude binary pattern variant; 7) CLBP[26], in which three types of features are combined to form the completed LBP features; 8) CoALBP [27], in which the co-occurrence among adjacent LBPs has been considered. The effectiveness of CoALBP for medical images can be confirmed since it won the first prize in the 2012 HEP-2 cells classification contest. In Table 7, we report the classification accuracies using the datasets and the compared methods described above. In each table, the best result in each datasets is bolded. From Table 7, we can make the following findings. First, SAHLBPT3 is one of the best variants proposed in this work. It achieves, on average, a better result than the compared methods. Also, SAHLBPT3 achieves the highest accuracy in PAP, 2DHela, and Brain tumor datasets. We conjecture that it is because our method enhanced its performance by exploiting the spatial information of the adaptive radius with different scales, which produced more discriminative features for medical images. Second, CoALBP works better than SAHLBPT3 in Hep-2 cell dataset, and SAHLBPT3 outperforms the CoALBP for the other three datasets. However, our SAHLBPT3 has one advantage over the CoALBP: its feature dimension is much smaller. The experimental feature sizes of SAHLBPT3 and CoALBP are 2048 and 3072, respectively. Finally, SAHLBPT3 works much better than the methods with rotation invariant, such as LBP-r and LBPsu2, even though the rotation invariant features are not involved in our methods. However, the rotation invariant contains additional discriminative information. How to utilize this rotation invariant feature based on our adaptive strategy will be explored in our future research. From Table 7, it can be observed that CLBP, CoALBP, and SAHLBPT3 achieve similar classification accuracies. To provide another basis for comparison of the three methods, we have also reported the macro-averaged F1-scores as shown in Table 8. Note that the reported classification accuracy in Table 7 is defined as the number of samples correctly classified divided by the total number of samples in the test set. It is equivalent to the micro-averaged F1 score in the single-label classification task. Comparing the results
Table 8 Macro-averaged F1 scores of the three compared methods over four datasets. Methods CLBP CoALBP SAHLBPT3 PAP 82.87 7 2.1 81.45 7 1.9 84.23 7 1.9 2D-Hela 87.12 7 1.9 86.23 7 1.7 88.76 7 1.5 Hep-2 86.41 7 1.5 91.89 7 1.7 91.29 7 1.5 Tumor 78.65 7 2.1 79.23 7 2.0 83.81 7 1.9

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200 Table 9 Statistical significance of differences in classification accuracies for different methods. Datasets PAP 2D-Hela Hep-2 Tumor Compared methods SAHLBPT3 SAHLBPT3 SAHLBPT3 SAHLBPT3 SAHLBPT3 SAHLBPT3 SAHLBPT3 SAHLBPT3 vs. vs. vs. vs. vs. vs. vs. vs. CLBP CoALBP CLBP CoALBP CLBP CoALBP CLBP CoALBP p-values 0.1435 0.0122 0.0028 o 0.0001 o 0.0001 0.1723 o 0.0001 o 0.0001 Corrected  0.00625 0.00625 0.00625 0.00625 0.00625 0.00625 0.00625 0.00625 Result NS NS * * * NS * *

197

the computational time for feature extraction (for a single image), training and testing on 2D-Hela dataset, as shown in Table 10. The results show that SAHLBPT3 is slower than standard LBP (with both R  1, P  8 and R  2, P  16) and CoALBP, which is caused by the increased time demand time for searching the adaptive radius and computing spatial adjacent histogram. It is worth stating that, in training and testing stage, our method is faster than LBP (with R  2, P  16) and CoALBP. This is because the feature dimension of our method (i.e. 2048 dimensions) is shorter than LBP(R  2, P  16) (i.e. 65536 dimensions) and CoALBP (i.e. 3072 dimensions). Based on consideration of both time and performance, we infer that SAHLBPT3 is suitable for medical image classification.

Fig. 14. Classification accuracies of our method on four database for different SVM kernels.

of Table 7 and Table 8, we can find out that the micro- and macroaveraged results do not differ significantly and the previous findings based on accuracy would not change considering the macroaveraged F1-score. In order to further perform statistical analysis for comparing our SAHLBPT3 with the other two representative methods, i.e. CLBP and CoALBP, we employ Wilcoxon ranksum test and the Bonferroni method [36] for multiple pair-wise comparisons. In this case, the level of significance has been corrected as   0.05/ 8  0.00625 according to the Bonferroni method. The significant pvalues and corrected  are reported in Table 9, where * denotes that the two methods have significant different performance and NS denotes that the two methods have no significant different performance. In summary, we can conclude that our method is indeed a superior algorithm with a clear motivation, convincing improvements, and an easy implementation. The advantage of our method with respect to other variants makes it a good choice for medical image classification. 3.5. Runtime performance analysis To study the computational demand of the proposed method, we analyze the required time of our model and some representative methods. The MATLAB R2014a are employed, running on an Intel i5-2400 processor at 3.1 GHz with 4 GB RAM. We report
Table 10 Computation time for feature extraction (single image), training and testing on 2DHela dataset. Methods LBP(R  1, P  8) LBP(R  2, P  16) CoALBP SAHLBPT3 Feature extraction 0. 103 s 0.197 s 0.562 s 0.823 s Training 1.423 s 814.27 s 12.37 s 6.553 s Testing 0.056 s 44.305 s 0.798 s 0.493 s

Furthermore, The computational time for feature extraction mainly depends on the descriptor and the image size, while the training and testing are mainly dependant on the classifier, feature dimension and the scale of the dataset. Our method has took 0.823 s in the feature extraction stage per image with 512  382 size and 6.553 s in the training stage for 862 samples with 3072 feature dimensions. We think that our computational demand is an adequate trade-off. Viewed from this perspective, we believe our method can be extended to large-scale datasets. Due to the limitations of the scale of medical image datasets, we will evaluate our method on more general datasets with larger scale in our future research. 3.6. Influence of different SVM kernels Since the choice of the SVM kernel may have an impact on the classification rates, we also conducted an experiment by using several SVM kernels. Though abundantly new kernels have been proposed, the most frequently used kernel functions [32,33] in image classification are linear, polynomial, Radial Basis Function (RBF), and histogram intersection (HI) kernels [34], formulated as in Eq.(16)Eq.(19), respectively. 1 2 3 Linear : K xi ; xj   xi T xj Polynomial : K xi ; xj    xi T xj  r d ;  4 0 Radial basis functionRBF  18
n X i1

16 17

: K xi ; xj   exp   xi  xj 2 ;  4 0 4 Histogram intersectionHI  : K xi ; xj   minxi ; xj 

19

Here,  , r, and d are kernel parameters. In our test, the default parameter values are used for different kernels. Fig. 14 shows the average classification accuracies we achieved with different kernels on the four datasets. Note that we

198

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200 Table 13 Statistical significance of differences in classification accuracies for different local descriptors in BoVW model. datasets PAP Compared methods SAHLBPT3 vs. SIFT SAHLBPT3 vs. LBP SIFT  SAHLBPT3 vs. SAHLBPT3 vs. SIFT SAHLBPT3 vs. LBP SIFT  SAHLBPT3 vs. SAHLBPT3 vs. SIFT SAHLBPT3 vs. LBP SIFT  SAHLBPT3 vs. SAHLBPT3 vs. SIFT SAHLBPT3 vs. LBP SIFT  SAHLBPT3 vs. p-values 0.0031 o 0.0001 0.0053 0.0079 o 0.0001 0.0370 o 0.0001 o 0.0001 0.0208 o 0.0001 o 0.0001 0.0013 Corrected  0.00417 0.00417 0.00417 0.00417 0.00417 0. 00417 0.00417 0.00417 0.00417 0.00417 0.00417 0.00417 Result * * NS NS * NS * * NS * * *

Table 11 Classification accuracy of compared descriptors in dense BoVW framework over four datasets. PAP BoVW framework SIFT LBP SAHLBPT3 SIFT  SAHLBPT3 SAHLBPT3 2D-Hela Hep-2 Tumor

84.03 7 2.3 83.79 7 2.5 80.88 7 1.5 78.23 7 1.7 81.43 7 2.1 81.47 7 2.1 77.52 7 1.9 75.21 7 2.1 86.21 7 2.0 84.89 7 2.2 85.06 7 2.0 81.45 7 1.7 87.63 7 2.1 86.21 7 2.5 83.47 7 1.8 83.34 7 1.9 88.03 7 1.7 90.06 7 1.5 91.86 7 0.9 84.48 7 1.8

SAHLBPT3

2D-Hela

Proposed framework

SAHLBPT3

Hep-2

SAHLBPT3

Table 12 Macro-averaged F1 score of compared descriptors in dense BoVW framework over four datasets. PAP BoVW framework SIFT LBP SAHLBPT3 SIFT  SAHLBPT3 2D-Hela Hep-2 Tumor

Tumor

SAHLBPT3

77.97 7 2.5 82.29 7 2.4 79.76 7 1.8 77.07 7 1.9 73.17 7 2.5 80.13 7 2.1 76.19 7 1.7 74.01 7 2.1 81.58 7 2.4 83.75 7 2.0 84.05 7 1.9 80.13 7 1.7 83.27 7 2.4 85.35 7 2.3 82.31 7 2.1 82.41 7 1.9

have not optimized SVM kernel parameters in Fig. 14. Particularly noteworthy, in our case is that, the parameter optimization performed for SVM with RBF kernel is time-consuming, while the parameter optimization performed for SVM with linear kernel has a little influence on classification performance. This means the classification result of our method has little change if we set different cost factor for SVM with linear kernel. This may be due to the fact that the feature vectors of our method are rather sparse with high dimensionality. Based on the average results on four datasets, linear SVM kernels should be preferred in this case, since computation time in this case varies linearly with the size of the training data. 3.7. Evaluating the performance of the SAHLBPT in BoVW framework In this section, for the sake of completeness, we further test the performance of our texture descriptor within the bag of visual words (BoVW) framework. The BoVW model is one of the most popular algorithms for image representation by using visual words formed by vector-quantizing local features with a clustering method (such as K-means). The standard LBP and our SAHLBPT3 method can also be used as local texture descriptors for BoVW. In the experiment, we use dense sampling strategy for the BoVW model. More specifically, the local features are extracted from 16  16 image patches on a regular grid spaced at 8 pixels for all images. Since the SIFT descriptor is probably the most widely used descriptor for describing the local patches in BoVW model, we mainly compare our method with dense SIFT descriptor. For dense SIFT descriptor, the two parameters patch size and grid spacing are set as 16 and 8, respectively, to allow for dense sampling. Moreover, the spatial pyramid matching (SPM) [37] settling is employed for SIFT descriptor. The SPM is calculated at three levels. At the first level, the original image is considered as a sub-region. At the second and the third level, the original image is divided into 2  2 and 4  4 sub-regions, respectively. Then all BoVW histograms of 21 sub-regions are concatenated to form the SPM representation of the image. Additionally, single feature may fail to capture the rich information within local image patches, as such, it is reasonable to extract multiple features for compensation. Therefore, we also use multiple features that consist of SIFT and SAHLBPT3 to describe local patches and then apply it to construct the visual vocabulary (named SIFT  SAHLBPT3 for short).

The classification accuracies and the macro-averaged F1 scores for different local descriptors used in BoVW model are reported in Table 11 and Table 12, respectively. Here the visual vocabulary size is 200 for all methods. To perform statistical analysis, we also compare the SAHLBPT3 with SIFT, LBP and SIFT  SAHLBPT3 within BoVW framework tested by Wilcoxon ranksum test, the test protocol is the same as that in subsection 3.4. In this test, the level of significance has been corrected as   0.05/12  0.00417 according to the Bonferroni method. The p-values and corrected  are reported in Table 13, where * denotes that the two methods have significant different performance and NS denotes that the two methods have no significant different performance. From Table 11, Table 12 and Table 13, we can make the following conclusions. (1) From Table 11 and Table 12, it can be observed that the SAHLBPT3 consistently outperforms both SIFT and LBP on the four datasets when they are applied to BoVW framework. However, the LBP is slightly weaker than SIFT as local descriptors in BoVW model. Looking at the significance of differences in Table 13, we can find out that the improvements of SAHLBPT3 over LBP are significant on the four datasets. Moreover, SAHLBPT3 and SIFT have significant different performance in PAP, Hep-2 cell and Tumor datasets. (2) The BoVW model using the multiple features, i.e. SIFT  SAHLBPT3, excels that using a single feature on PAP, 2D-Hela, and Tumor datasets based on the accuracy and the macroaveraged F1 score. However, the SAHLBPT3 works better than SIFT  SAHLBPT3 on Hep-2 cell dataset, which means that SIFT and SAHLBPT3 descriptors do not provide very useful compensation for Hep-2 cell dataset. Looking at Table 13, SIFT  SAHLBPT3 and SAHLBPT3 have significant different performance in Tumor datasets, but they have no significant different performance in PAP, 2D-Hela and Hep-2 cell datasets. This means that SIFT and our SAHLBPT3 are complementary features for Tumor datasets in BoVW model. (3) To discuss, we return to the classification performance of the SAHLBPT3 in our framework and also re-record the results in Table 7. We can easily find that SAHLBPT3 in our framework achieves better classification results than that in the BoVW framework. This is because our SAHLBPT3 aims to describe the micro-structures of the whole image and these microstructures tend to play the largest role with a globally encoding mode. Encoding the micro-structures in local image patches would slightly decrease the performance of SAHLBPT3. Moreover, the run-time for clustering in BoVW framework is time-consuming. Based on both the classification result and

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200

199

Fig. 15. Comparison of classification accuracies on 2D-Hela datasets using different vocabulary sizes for BoVW model. Fig. 16. Confusion matrices of the proposed method for 2D-HeLa dataset. C1:actin; C2:dna; C3:endosome; C4: er; C5:golgia; C6:golgpp; C7:lysosome; C8:mircrotubules; C9:mitochondiria; C10: nucleolus. Tumor o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 0.0015 o 0.0001 o 0.0001 o 0.0001 o 0.0001 0.0109 o 0.0001

Table 14 P-values of statistical tests for compared methods on four datasets. Compared methods SAHLBPT1 SAHLBPT2 SAHLBPT3 SAHLBPT1 SAHLBPT2 SAHLBPT3 SAHLBPT1 SAHLBPT2 SAHLBPT3 SAHLBPT1 SAHLBPT2 SAHLBPT3 SAHLBPT3 SAHLBPT3 vs. vs. vs. vs. vs. vs. vs. vs. vs. vs. vs. vs. vs. vs. LBPT1(R  1,P  8) LBPT2(R  1,P  8) LBPT3(R  1,P  8) LBPT1(R  2,P  8) LBPT2(R  2,P  8) LBPT3(R  2,P  8) LBPT1(R  2,P  16) LBPT2(R  2,P  16) LBPT3(R  2,P  16) DCLBPT1 DCLBPT2 DCLBPT3 SAHLBPT1 SAHLBPT2 PAP o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 0.0213 0.1072 0.0019 o 0.0001 o 0.0001 o 0.0001 0.0181 o 0.0001 2D-Hela o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 0.0021 0.0033 o 0.0001 o 0.0001 0.0018 o 0.0001 0.0823 o 0.0001 Hep-2 o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 o 0.0001 0.1823 0.0013

time complexity, the SAHLBPT3 involved in our framework should be more preferred in medical image classification task. Finally, we then report the classification accuracies of the methods using different vocabulary sizes. In Fig. 15, the classification accuracies have a similar trend as the visual vocabulary size increases, i.e. the performance saturates when the vocabulary size larger than 200. Moreover, SAHLBPT3 consistently outperforms both SIFT and LBP using different vocabulary sizes.

Fig. 17. Confusion matrices of the proposed method for Hep-2 cell dataset. C1: centromere; C2:coarse speckled; C3:cytoplasmatic; C4: fine speckled; C5:homogeneous; C6:nucleolar.

Despite the well achieved accuracy, there are still some open problems in our framework. 4. Conclusion and future work In this paper, we proposed a novel framework based on our adaptive local binary patterns and spatial adjacent histogram for medical image classification. We first employed gradient operator to determine the adaptive neighborhood radius of each pixel. Then three coding schemes based on adaptive radius were introduced for processing the LBP histograms. To capture the discriminative micro-structures features produced by the adaptive radius, we proposed to using spatial adjacent histogram strategy for image representation. Finally, a SVM classifier is learned for medical image classification task. We also evaluated four SVM kernels into our algorithm to enhance its classification power. Based on the experiments on PAP smear, 2D-Hela, Hep-2 cell and Brain tumor datasets, we can draw a conclusion that our proposed method achieved better performance compared with other LBP-based approaches. (1) The dimension of our model is much larger than that of the standard LBP, and this will result in the computational time increasing linearly as training data grow in number. The scale of medical image datasets is generally not large, however, our method will cost more time when encountering general datasets with large scale. The feature selection techniques could be employed to solve this problem. (2) In our proposal, we focus on improving LBP in terms of encoding the micro-structures features, and we did not design patterns with rotation invariant. However, the rotation invariants are expected to further improve the performance of our model. (3) We mainly employ SVM with linear kernel and we also report the performance of different kernels with default parameter values. However, a number of optimization algorithms could be employed to further improve the classification performance, such as swarm intelligence optimization algorithm.

200

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185 200 [4] H. Pourghassem, H. Ghassemian, Content-based medical image classification using a new hierarchical merging scheme, Comput. Med. Imaging Graph. 32 (2008) 651661. [5] M. Hfnera, M. Liedlgruber, A. Uhl, A. Vcsei, F. Wrba, Color treatment in endoscopic image classification using multi-scale local color vector patterns, Medical Image Analysis, 16, pp. 7586. [6] T. Ojala, M. Pietikinen, T. Menp, Multiresolution gray-scale and rotation invariant texture classification with local binary pattern, IEEE Trans. Pattern Anal. Mach. Intell. 24 (7) (2002) 971987. [7] H. Zhou, R. Wang, C. Wang, A novel extended local binary pattern operator for texture analysis, Inf. Sci. 22 (2008) 43144325. [8] Z.H. Guo, L. Zhang, D. Zhang, X.Q. Mou, Hierarchical multiscale lbp for face and palmprint recognition, in: Proceedings of the 17th IEEE International Coference Image Processing (ICIP 2010), 2010, pp. 45214524. [9] L. Nanni, A. Lumini, S. Brahnam, Survey on LBP based texture descriptors for image classification, Expert Syst. Appl. 39 (2012) 36343641. [10] J.F. Ren, X.D. Jiang, J.S. Yuan, Noise-resistant local binary pattern with an embedded error-correction mechanism, IEEE Trans. Image Process. 22 (10) (2013) 40494060. [11] Jun Shang, et al., Robust image region descriptor using local derivative ordinal binary pattern, J. Electron. Imaging 24 (3) (2015) 033009. [12] X.L. Meng, Z.Z. Wang, L.Z. Wu, Building global image features for scene regognition, Pattern Recognit. 5 (2012) 373380. [13] Z. Guo, L. Zhang, D. Zhang, A completed modeling of local binary pattern operator for texture classification, IEEE Trans. Image Process. 19 (2010) (16751663). [14] X. Tan., B. Triggs., Enhanced local texture feature sets for face recognition under difficult lighting conditions, IEEE Trans. Image Process. 19 (6) (2010) 16351650. [15] C. Zhu, C.E. Bichot, L. Chen, Multi-scale color local binary patterns for visual object classes recognition, in: Proceedings of the International Conference on Pattern Recognition (ICPR), 2010, pp. 30653068. [16] S. Liao, A.C.S. Chung, In: Face Recognition by Using Elongated Local Binary Patterns with Average Maximum Distance Gradient Magnitude, Asian Conference on Computer Vision, 2007. [18] W. Zhang, S. Shan, W. Gao, X. Chen, H. Zhang, Local Gabor binary pattern Histogramse quence (LGBPHS): a novel non-statistical model for face representation and recognition, Int. Conf. Comput. Vision. 1 (2005) 786791. [19] S. Liao, M.W.K. Law, A.C.S. Chung, Dominant local binary patterns for texture classification, IEEE Trans. Image Process. 18 (2009) 11071118. [20] N. Senthilkumaran, R. Rajesh., Edge detection techniques for image segmentationa survey of soft computing approaches, Int. J. Recent Trends Eng. 1 (2) (2009). [21] N. Werghi, S. Berretti, A.D. Bimbo, The mesh-LBP: a framework for extracting local binary patterns from discrete manifolds, IEEE Tans. Image Process. 24 (1) (2015). [22] Z. Li, G. Liu, Y. Yang, et al., Scale-and rotation-invariant local binary pattern using scale-adaptive texton and subuniform-based circular shift, Image Process. IEEE Trans. 21 (4) (2012) 21302140. [23] S. Hegenbart, A. Uhl, A scale-and orientation-adaptive extension of Local Binary Patterns for texture classification, Pattern Recognit. 48 (8) (2015) 26332644. [24] S. Liao, A.C.S. Chung, Face recognition by using elongated local binary patterns with average maximum distance gradient magnitude, Asian Conf. Comput. Vision (2007) 627629. [25] T. Ahonen, J. Matas, C. He, M. Pietikainen, Rotation invariant image description with local binary pattern histogram Fourier features, Image Anal. (2009) 6170. [26] Z. Guo, L. Zhang, D. Zhang, A completed modeling of local binary pattern operator for texture classification, IEEE Trans. Image Process. 19 (2010) 16571663. [27] Ryusuke Nosaka, Yasuhiro Ohkawa, Kazuhiro Fukui., Feature Extraction Based on Co-occurrence of Adjacent Local Binary Patterns. Advances in Image and Video Technology, Springer Berlin Heidelberg (2012), p. 8291. [28] A. Chebira, Y. Barbotin, C. Jackson, T. Merryman, G. Srinivasa, R.F. Murphy, J. Kovacevi, A multiresolution approach to automated classification of protein subcellular location images, BMC Bioinform. 8 (1) (2007) 210. [29] P. Foggia, G. Percannella, P. Soda, M. Vento, Benchmarking HEp-2 cells classification methods, Med. Imaging IEEE Trans. 32 (10) (2013) 18781889. [30] J. Jantzen, J. Norup, G. Dounias, B. Bjerregaard, Pap-smear benchmark data for pattern classification, Nat. Inspired Smart Inf. Syst. (2005) 19. [31] C.C. Chang, C.J..Lin, LIBSVM: A Library for Support Vector Machines, 2001. Software available from: http://www.csie.ntu.edu.tw/ $ cjlin/libsvm. [32] H. Byun, S.W. Lee, Applications of Support Vector Machines for Pattern Recognition: A Survey[m]/Pattern Recognition with Support Vector Machines, Springer Berlin Heidelberg (2002), p. 213236. [33] G.B. Huang, D.H. Wang, Y. Lan, Extreme learning machines: a survey, Int. J. Mach. Learn. Cybern. 2 (2) (2011) 107122. [34] S. Maji, A.C. Berg, J. Malik, Classification using intersection kernel support vector machines is efficient[C]/Computer Vision and Pattern Recognition, 2008, CVPR 2008, IEEE Conference on IEEE, 2008: pp. 18. [36] J.M. Bland, D.G. Altman, Multiple significance tests: the Bonferroni method, BMJ 310 (6973) (1995) 170. [37] S. Lazebnik, C. Schmid, J. Ponce, Beyond bags of features: spatial pyramid matching for recognizing natural scene categories, Comput. Vision. Pattern Recognit. (CVPR) (2006).

Fig.18. Confusion matrices of the proposed method for brain tumor dataset. C1: astrocytoma; C2: tuberculum sellae meningioma; C3: olfactory groove meningioma; C4: acoustic neuroma; C5: pituitary tumor.

We will explore these problems in the future towards a more powerful medical image classification model.

Conflict of interest statement None declared.

Acknowledgments This work was supported by the National Natural Science Foundation of China (61472161, 61133011, 61402195, 61502198, 61303132, 61202308), Science and Technology Development Project of Jilin Province (20140101201JC), the Science and Technology Plan Project of Wenzhou of China (G20140048) and the Program of China Scholarships Council (No. 201406170116).

Appendix A. Supporting information The p-values of statistical tests for compared methods mentioned in Section 3.3 are presented in Table 14. The confusion matrices for 2D-HeLa, Hep-2 cell, and brain tumor datasets, are presented in Fig. 16, Fig. 17, and Fig. 18, respectively. For each confusion matrix, the average classification accuracies for individual classes are listed along the diagonal, and the entry in the ith row and jth column is the percentage of images from the class i that are misidentified as class j.

References
[1] K. Doi, Computer-aided diagnosis in medical imaging: historical review, current status and future potential, Comput. Med. Imaging Graph. 31 (4) (2007) 198211. [2] F. Lalys, L. Riffaud, X. Morandi, P. Jannin, Automatic Phases Recognition in Pituitary Surgeries by Microscope Images Classification. In Information Processing in Computer-assisted Interventions, Springer Berlin Heidelberg (2010), p. 3444. [3] T.F. Cootes., C.J. Taylor., Statistical models of appearance for medical image analysis and computer vision. Medical imaging 2001, Int. Soc. Opt. Photon. (2001) 236248.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/291421471

Enhancingtheflowexperienceofconsumersin Chinathroughinterpersonalinteractionin socialcommerce
ArticleinComputersinHumanBehaviorMay2016
DOI:10.1016/j.chb.2016.01.012

CITATIONS

READS

3
4authors,including: HefuLiu UniversityofScienceandTechnologyofChina
36PUBLICATIONS345CITATIONS
SEEPROFILE

320

QianHuang UniversityofScienceandTechnologyofChina
19PUBLICATIONS194CITATIONS
SEEPROFILE

SummerXiayuChen UniversityofScienceandTechnologyofChina
7PUBLICATIONS7CITATIONS
SEEPROFILE

AllcontentfollowingthispagewasuploadedbySummerXiayuChenon25January2016.

Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Computers in Human Behavior 58 (2016) 306e314

Contents lists available at ScienceDirect

Computers in Human Behavior
journal homepage: www.elsevier.com/locate/comphumbeh

Enhancing the flow experience of consumers in China through interpersonal interaction in social commerce
Hefu Liu, Haili Chu, Qian Huang*, Xiayu Chen
School of Management, University of Science and Technology of China, Jinzhai Road 96, Hefei, Anhui, China

a r t i c l e i n f o
Article history: Received 22 March 2015 Received in revised form 21 December 2015 Accepted 10 January 2016 Available online xxx Keywords: Social commerce Interpersonal interaction Flow experience Purchase intention

a b s t r a c t
Although research on flow experience has recently received much attention, few studies have been published on the perceived interpersonal interaction factors of consumers and their influence in social commerce. In addition, few studies have focused on the impact of interpersonal interaction factors on flow experience. Drawing on the stimulus-organism-response framework, this study examines the impact of interpersonal interaction factors (perceived expertise, similarity, and familiarity) on the formation of flow experience and its subsequent effects on purchase intention in the context of social commerce. We investigate whether the impact of the three interpersonal interaction factors on flow experience differs between young and old users. We conduct a survey and collect 349 responses from users of a social shopping site in China. Our results indicate that interpersonal interaction factors positively relate to flow experience and subsequently influence purchase intention. We also find differences between young and old users in this area.  2016 Published by Elsevier Ltd.

1. Introduction Social commerce is an emerging business trend that is growing rapidly in China. According to the 2015 McKinsey report, consumers in China spend 78 min per day on social commerce. Approximately 50% of customers in China make their purchase decisions according to recommendations from relatives and friends. In recent years, the virtual experiences of customers in the social commerce context have gained importance. Providing consumers with unforgettable experiences has emerged as an important issue in driving customer participation and developing favorable consumer behavior responses in social commerce (Huang & Benyoucef, 2014; Zhang, Lu, Gupta, & Zhao, 2014). When considering the provision of online consumption experiences, scholars have highlighted the importance of flow (Chang, 2013; Faiola, Newlon, Pfaff, & Smyslova, 2013). Flow is a state of concentration in which people are so involved that nothing else matters (Gao & Bai, 2014). Specifically, flow refers to a temporarily unaware experience in which an individual engages in a social shopping activity in a social shopping website with total

* Corresponding author. E-mail addresses: liuhf@ustc.edu.cn (H. Liu), Chl0816@ustc.edu.cn (H. Chu), huangq@ustc.edu.cn (Q. Huang), cxy1023@mail.ustc.edu.cn (X. Chen). http://dx.doi.org/10.1016/j.chb.2016.01.012 0747-5632/ 2016 Published by Elsevier Ltd.

concentration, control, and enjoyment (Gao & Bai, 2014). In emphasizing the importance of flow and the formation of compelling experiences, Hoffman and Novak (1996) went as far as declaring that "creating a commercially compelling website depends on facilitating a state of flow for consumers [and that] ... an important objective for marketers is to provide these opportunities" (Hoffman & Novak, 1996, p. 66). Zhang et al. (2014) argued that enhancing the flow experience is essential for the survival of social commerce. Despite the understanding of the contribution of flow to the creation of compelling experiences, investigating the drivers of customer flow experiences is important for the success of social shopping sites, however, little effort has been devoted to studying the factors contributing to flow experience in social commerce. In order to fill this gap, the present study is trying to explore the formation drivers of flow in social commerce. With the growing competition, online vendors rely on web atmospherics to create an environment that can produce positive emotional and cognitive states of online shoppers (Gao & Bai, 2014; Zhang et al., 2014). Chang (2013) suggested that social interaction among members in social networking sites would yield a state of flow. In the social commerce context, interpersonal interactional factors have received much attention (Hsiao, Lin, Wang, Lu, & Yu, 2010; Liao, Chu, Huang, & Shen, 2010; Lu, Zhao, & Wang, 2010). Social commerce involves using social media to support social interaction, and its unique characteristics provide opportunities for

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

307

consumers to make better buying decisions (Ng, 2013). Carlson and O'Cass (2011) posited that future studies should explore the effects of consumer-based variables on the formation of flow experience. However, to our knowledge, little is known about the interpersonal interaction factors that promote the flow experience for customers in social commerce. Drawing from the above literature review, we infer that investigating the impact of interpersonal interaction factors on the creation of flow experience should be a promising research area in social commerce. On the basis of Liao et al. (2010) study, three interpersonal interaction factors are investigated, namely, perceived expertise of group members, similarity of group members, and familiarity of group members. As the context of our research is similar to the virtual community context, we focus on these three interpersonal interaction factors. Therefore, this study draws on the stimuluseorganismeresponse (SOR) model to investigate the impact of the three interpersonal attraction factors on the flow experience and the relationship between flow experience and purchase intention. This research makes important contributions to the extant literature. First, we extend the extant literature by testing and validating a model by incorporating interpersonal drivers of flow experience in social commerce. Second, our data analysis reveals significant differences between young and old users. Third, the present study advances the understanding that interpersonal interaction factors remain important in the context of social commerce. Fourth, our research provides empirical evidence to the deduction that interpersonal interaction factors positively affect purchase intention through flow experience in social commerce. 2. Theoretical background and research hypotheses 2.1. SOR framework The SOR model is extensively used in studies that measure the impact of perceived website features on consumer responses (Gao & Bai, 2014; Zhang et al., 2014). According to the SOR model, environmental stimuli (S) influence consumer internal states (O) and correspondingly affect consumers' overall responses (R). Donovan and Rossiter (1982) proposed a model that is adapted to the retail context. The model treats atmospheric cues as stimuli, two major emotional states as organism, and shopping behaviors within the store as response. We can learn from the application of the SOR model in the retailing context that environmental stimuli influence consumer internal states, which in turn drive their behavioral intention toward the store. Fiore and Kim (2007) developed an integrated SOR model for the brick-and-mortar context. In the framework, the stimuli include ambient, design, and social cues. In the online shopping environment, some researchers use actual stimuli (Animesh, Pinsonneault, Yang, & Oh, 2011; Kim & Lennon, 2010; Wang, Hernandez, & Minor, 2010), and others use customer assessments of the stimuli to denote the stimulus segment of the model (Koo & Ju, 2010; Manganari, Siomkos, Rigopoulou, & Vrechopoulos, 2011; Nath, 2009). Research on the social factors of online shopping environment is growing in the context of virtual community and social shopping sites (Hsiao et al., 2010; Lu et al., 2010). The present study adopts the SOR model using interpersonal interaction factors as the environmental stimulus (S). The organism pertains to emotional and cognitive states and includes experiences (Jiang, Chan, Tan, & Chua, 2010). In the current study, adapting from the research of Gao and Bai (2014), the organism is the customer's cognitive judgment of the online consumers' experience, which is presented in the form of flow experience. The responses refer to website patronage intention

(Jeong, Fiore, Niehm, & Lorenz, 2009), purchase intention (Hsu, Chang, & Chen, 2011), and intention to use and buy (Huang, 2013). In our study, we follow Hsu et al. (2011) and treat consumer purchase intention as consumer behavioral outcomes. The application of the SOR paradigm as a holistic theory is appropriate for this study for two reasons. First, the SOR paradigm was extensively used in previous research on online customer behavior (Chang, Chih, Liou, & Hwang, 2014; Hsieh, Hsieh, Chiu, & Yang, 2014; Parboteeah, Valacich, & Wells, 2009). For example, using the SOR paradigm, Zhang et al. (2014) examined the effects of three technological stimuli (perceived interactivity, personalization, and sociability) on consumers' virtual experiences and subsequent social commerce intention. Parboteeah et al. (2009) applied the SOR paradigm to explore the impact of task-relevant cues and mood-relevant cues on perceived usefulness and perceived enjoyment, and then online purchase intention. The findings of these studies support the use of the SOR paradigm in accounting for consumer internal reactions and behavioral outcomes to the stimuli. Second, the SOR paradigm provides a strict and structured manner to examine the impact of interpersonal interaction factors as environmental stimuli on consumer online experiences (e.g., flow) and their subsequent intention to purchase from social commerce sites. 2.2. Social influence factors as environmental stimuli (S) Social commerce involves the application of social media to support social interaction, communication, and user-generated content for assisting consumers in online buying. One of its unique characteristics is that it provides an opportunity for consumers to make better buying decisions and improve their future shopping experience (Ng, 2013). Therefore, social commerce sites need to facilitate member interaction. Frequent member interaction will enhance the interpersonal attraction of websites (Liao et al., 2010). As our research focuses on interpersonal interaction factors, flow experience, and online buying behavior, we consider three interpersonal interaction factors proposed by Liao et al. (2010), namely, perceived similarity, expertise, and familiarity. Perceived similarity refers to the commonness shared by customers in taste, preference, and liking toward products. Perceived expertise refers to other consumers' ability to recommend products based on their knowledge and experience. Perceived familiarity relates to the frequency of interactions and relationships with other shoppers in social shopping sites (Liao et al., 2010). 2.3. Flow experience as customer internal states (O) The SOR paradigm suggests that the impact of environmental stimuli on consumer behavior is mediated by virtual experiences (Animesh et al., 2011; Zhang et al., 2014). Generally, studies show that the effects of web atmospherics can be studied from two major perspectives. The first perspective includes cognitive reactions from the stimulus such as perceived usefulness (Parboteeah et al., 2009). The second perspective refers to affective reactions from the stimulus such as perceived enjoyment (Floh & Madlberger, 2013). According to Gao and Bai (2014), focusing on the cognitive aspects is significant. Furthermore, there is a lack of investigations focus only on the cognitive responses of consumers, such as flow experience, especially in social commerce. According to Ding, Hu, Verma, and Wardell (2010), flow experience has been treated as a basis to facilitate the creation of a compelling experience. Flow is a psychological state in which people become completely involved within a stimulus, and it can be described as the whole experience that individuals feel when they are fully absorbed (Gao & Bai, 2014). Some scholars highlight the

308

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

importance of flow in computer-mediated environments and suggest that the success of online vendors depends on their ability to create opportunities for customers to experience flow (Gao & Bai, 2014; Hoffman & Novak, 1996; Hsu, Chang, & Chen, 2012). In computer-mediated environments, the interactivity among members creates a sense of immersion and induces a state of flow for users (Mollen & Wilson, 2010; Teng, Huang, Jeng, Chou, & Hu, 2012). In the application of the SOR model, many scholars identify the relationship between flow experience and purchase intention. For example, Animesh et al. (2011) explored the impact of technological and spatial factors on purchase intention through the mediating effect of flow, telepresence, and social presence. Gao and Bai (2014) investigated the influence of website atmospheric cues on purchase intention and satisfaction by applying flow as a mediator. As flow is a broad concept in different contexts, many studies argue that flow is a multidimensional concept with different components. For example, Wang, Baker, Wagner, and Wakefield (2007) reported that flow consists of control, interest, attention, and curiosity. According to Gao and Bai (2014), we define flow as a temporarily unaware experience in which an individual engages in an social shopping activity in a social shopping website with total concentration, control, and enjoyment.

of members in social shopping sites and their knowledge of other group members (Liao et al., 2010). Familiarity can reduce uncertainty (Hinds, Carley, Krackhardt, & Wholey, 2000), increase cognitive trust (Komiak & Benbasat, 2006), and promote individual social interactions. In addition, social interactions among consumers in social commerce sites help in the purchase of a product, and they lead consumers to generating a sense of self-worth or selfefficacy (Zhang et al., 2014). Consumers will then find social commerce enjoyable and involving (Pagani & Mirabello, 2011). Consumer participation in this involvement process may help lead them to be fully immersed in their activities, which could induce a sense of flow (Animesh et al., 2011). Therefore, this study argues that consumers who perceive familiarity with members in social commerce sites may experience a state of flow. H3. Perceived familiarity is positively related to flow.

2.5. Purchase intention as response (R) In social commerce, social media tools are used to support social interactions and user contributions to promote activities in the process of selling and buying products (Wang & Zhang, 2012). According to Liao et al. (2010), customers are exposed to various interpersonal attraction factors and influences, such as perceived expertise, perceived familiarity, perceived similarity, informational influence, and normative influence, which will motivate their subsequent behavior. Previous studies have demonstrated that consumer purchase intention could reflect consumer behavioral outcomes (Gao & Bai, 2014; Huang, 2013; Jiang et al., 2010). Therefore, we use purchase intention, specifically purchase intention in social commerce, as the response in the model. Our research is consistent with studies that use the SOR model and treat purchase intention as the response (Jiang et al., 2010; Kim & Lennon, 2013; Wu, Lee, Fu, & Wang, 2013). 2.6. Flow experience(O) and purchase intention(R) Flow experience is a compelling experience that affects consumer behavior in online shopping (Koufaris, 2002) and social network games (Shin & Shin, 2011). Previous studies argued that flow experience leads to specific behavioral outcomes (Gao & Bai, 2014; Hsu et al., 2012; Zhang et al., 2014). Hoffman and Novak (1996) suggested that individuals who experience flow states would have higher satisfaction and loyalty than those who do not. Koufaris (2002) posited that consumers who experience flow when visiting an online store are likely to make unplanned purchases. In social commerce, consumers who have experienced flow are likely to participate in social commerce activities (Zhang et al., 2014), which affect customer purchase intention. Lee and Chen (2010) and Gao and Bai (2014) noted that flow experience affects consumer behavioral intention, such as the likelihood to purchase from the website. We argue that participants who have compelling experiences will become fully involved in their interactions in social shopping sites and will be more likely to buy products for their virtual existence (Animesh et al., 2011). On the basis of Animesh et al. (2011), we infer that consumers who enjoy their virtual existence are likely to spend more time and money in the purchase of products in social shopping sites. Building on past research, this study indicates that customers who experience a state of flow in social shopping sites are likely to purchase from the social commerce sites. H4. Flow experience is positively related to purchase intention in social commerce.

2.4. Interpersonal stimulus(S) and flow experience(O) Expertise is defined as a person's amount of knowledge about a field (Liao et al., 2010). The expertise of the source in a group is important for information acceptance (Petty, Cacioppo, & Goldman, 1981), and people agree more with an expert's view when suffering from social influence (Kelman, 1961). In the domain of a social shopping site, members with higher expertise contribute useful advice (Constant, Sproull, & Kiesler, 1996). The useful and relevant message provided by group members will reduce information asymmetry and cost, leading to enjoyable experiences (Kim & Li, 2009). Moreover, consumers are likely to have higher levels of interaction if useful and relevant information is provided (MacKenzie & Lutz, 1989; Zhou, 2013). The involvement in the interaction will make customers lose their self-awareness and experience a flow state (Gao & Bai, 2014). Therefore, we argue that perceived expertise of group members in the social shopping environment will induce flow experience. H1. Perceived expertise is positively related to flow. In the present study, similarity refers to the self-perceived similarity of psychological traits (e.g., preferences and tastes) of members of social shopping sites. According to the similarityeattraction theory, individuals are attracted by those who are similar to them (Al-Natour, Benbasat, & Cenfetelli, 2005). Al-Natour et al. (2005) pointed out that consumers' perceived similarity of other members helps them enjoy interactions. Empirical studies demonstrate that social interactions are correlated with the experience of flow (Animesh et al., 2011; Chang, 2013). Therefore, social interaction enhances the level of enjoyment and makes virtual experiences more enjoyable and engaging, thus leading to a flow state (Animesh et al., 2011). Therefore, we can infer that if consumers feel that the interaction is interesting, they will prefer to focus on the interaction and consequently enter a state of flow (Gao & Bai, 2014). The current study suggests that consumers who perceive similarity with members in social commerce sites are likely to experience a state of flow. H2. Perceived similarity is positively related to flow. In social shopping, familiarity refers to the previous interaction

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

309

2.7. Mediating effect of flow experience The present study tests for the mediating effect of flow experience. The SOR model provides a theoretical foundation for the mediating effect of flow experience. Studies that apply the SOR framework demonstrate that consumer internal state (organism) can mediate between stimuli and consumer response behavior (Gao & Bai, 2014; Ha & Lennon, 2010; Yoon, 2012). For example, Manganari et al. (2011) investigated the impact of perceived online store features (ease of use of the layout) on consumer responses (satisfaction and trust) and posited that consumer internal states (pleasure and attitude) mediate such impact. Ha and Lennon (2010) examined the mediating role of affective states between website design and consumer response behavior. Based on the SOR model and previous studies on the mediating effects of internal cognitive states, the current study suggests that flow plays a mediating role between the interpersonal interaction factors of the website and consumer responses. Fig. 1 shows the conceptual model of the current study. H5. Flow mediates the relationship between interpersonal interaction factors (perceived similarity, expertise, and familiarity) and purchase intention.

IS Ph.D. students who are fluent in English were involved in the translation process. The initial Chinese questionnaire was piloted among some of our peers and online friends. Forty useful responses were returned before being accepted as the final version. Several control variables were included in our model to ensure that empirical results were not due to covariance with other variables. Previous literature suggests that consumers' gender, level of education, and income may affect the intention to purchase on the Internet (Fang et al., 2014; Pavlou & Fygenson, 2006). On the basis of Lee, Qu, and Kim (2007), we included gender, education, and income of shoppers as control variables in our study. 3.2. Survey design We conducted a survey to test our research model. We chose the survey method because this quantitative research method predicts behavior and examines the relations between variables and constructs (Newsted, Huff, & Munro, 1998). Besides, the survey method has been widely employed in investigating behaviors in social commerce (Huang & Benyoucef, 2014; Zhang et al., 2014). To collect our survey data, an online survey was used for the present study. Our target population comprised online users of a particular website. Using an online survey can maintain the consistency between the research and data collection contexts. Moreover, an online survey has many advantages, such as wide reach. In addition, in our research, the model is integrated and includes a lot of social variables that are difficult to measure by other methods, such as case studies or experiments. Thus we believe that survey is an appropriate method for the current study (Cheung & Lee, 2009). 3.3. Data collection We chose one of the largest social shopping website as the research context. At the same time, this website claimed to be a fashion shopping website aiming to help users to make better purchase decisions. On this website, registered users can establish their own profiles, build relationships with other online consumers, and contact with other through using communication tools. Accordingly, we think this website is a suitable research context for studying social commerce topic. We collected data through an online survey, and our target samples were the registered users of this website. Only those users who have purchase experience in the website were included in our survey. An online survey questionnaire was created on an online survey website in China (www. wenjuanxing.com). Data were collected from one channel. The

3. Methods 3.1. Measurement development In this study, the items used in the survey were adapted from existing research to fit the context of social commerce. We followed the generally accepted suggestion on wording questions when developing and finalizing the questionnaire (Fang et al., 2014). Perceived similarity was measured using four items adapted from Liao et al. (2010). Items for perceived expertise and familiarity were also adapted from Liao et al. (2010). Items for flow were adapted from Zhang et al. (2014). Items for purchase intention were adapted from Pavlou and Fygenson (2006). Appendix A lists the measure items and their related sources. Seven-point Likert scales ranging from "strongly disagree" to "strongly agree" were used to measure all items in the survey. To ensure content validity, we conducted an expert review to refine the instruments. All construct items were originally developed in English. As our research was conducted in China, all instrument items were translated to Chinese following the translation committee approach (Van de Vijver & Leung, 1997). Four native Chinese

Fig. 1. Conceptual model of this study.

310

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

online questionnaire was distributed through email to potential users with the help of an online survey website. To encourage participation in our study, respondents were told that they would be rewarded with 300 points that could be used to exchange for money if they completed the survey. Finally, 349 valid responses were obtained for the final data analysis. Table 1 shows the demographic profiles of the respondents. 3.4. Data analysis To test our measurement and structural model, we choose the structural equation modeling using partial least squares (PLS) estimation. The PLS is a powerful technique, which combines the principal components analysis (CFA) and regresstion, to estimate the measurement and structural model simultaneously (Hair, Anderson, Tatham, & Black, 1998). In the current study, we used the software of Smart-PLS 2.0 to conduct the PLS estimation. According to Barnes (2011), the Smart-PLS 2.0 is better equipped to deal with formative measures and moderating relationships. Tamjidyamcholo, Gholipour, Baba, and Yamchello (2013) posited that Smart PLS is not only able to formulate a formative model for latent constructs but also demands fewer requirements to verify a model. Thus, we used Smart PLS 2.0 software to examine both the CFA and the structural model in our study. 4. Results 4.1. Common method bias When all data were perceptual and collected from a single source at the same point of time, the issue of common method bias might be a threat to the validity of the research (Podsakoff, MacKenzie, Lee, & Podsakoff, 2003). In our study, common method bias was examined using Harman's single factor test. The analysis finally showed that all the items can be categorized into five factors, and the first factor explains only 14.85% of the variance. These results suggested that common method bias was not a serious concern in the present study. 4.2. Measurement model The measurement model were examined based on the CFA (Hair

et al., 1998). Specifically, we assessed the measurement model by testing the content, convergent, and discriminant validities. By reviewing the relevant literature and pilot testing the instrument, we assessed the content validity. During this process, we dropped some items because of their item-to-total correlations. Convergent validity was assessed by testing the value of the factor loadings, Cronbach's alpha, composite reliability, and average variance extracted (AVE). The results of the confirmatory factor analysis show that all item loadings are above 0.7. The threshold levels for Cronbach's alpha, composite reliability, and average variance are 0.7, 0.7, and 0.5, respectively (Flynn, Sakakibara, Schroeder, Bates, & Flynn, 1990; Fornell & Larcker, 1981; Hair et al., 1998; Nunally & Bernstein, 1978). As shown in Table 2, the Cronbach's alpha and composite reliability values are above 0.8, and the AVE of all constructs is above 0.7. Therefore, the results indicate good convergent validity. Discriminant validity determines whether the measures of a construct are distinct from other constructs. To assess discriminant validity, we adopted two approaches (Gefen & Straub, 2005). First, according to Fornell and Larcker (1981), we assessed the discriminant validity by comparing the relationship between the correlations among constructs and the square root of the AVE of constructs. As shown in Table 3, the square roots of the AVE are higher than the correlations among constructs, thus indicating good discriminant validity. Second, we examined the items in the item loadings and cross-loadings to construct the correlations. As shown in Table 4, all the item loadings of the corresponding constructs are higher than the cross-loading values of the other latent variables, thus suggesting sufficient discriminant validity. To ensure that multicollinearity was not an issue, we examined the variance inflation factors (VIFs) and tolerance values of the independent values. When VIFs are lower than 10 or when tolerance values are higher than 0.1, multicollinearity may not be an issue (Mason & Perreault, 1991). The results indicate that the VIF values range from 1.638 to 2.322. Therefore, multicollinearity is not an important issue in this study. 4.3. Structural model After demonstrating the validity of the measurement model, we tested the hypothesized relationships using Smart PLS. Fig. 2 shows the results of the Smart PLS analysis on the full dataset. The results indicate that perceived expertise (b  0.231, p < 0.001), perceived similarity (b  0.427, p < 0.001), and perceived familiarity (b  0.220, p < 0.01) have positive effects on flow. Therefore, H1, H2, and H3 are supported. Flow has a significant effect on purchase intention (b  0.557, p < 0.001), thus supporting H4. The model illustrates that 54.0% of the variance exists in flow, and 41.8% of the variance is related to purchase intention. Only one control variable (income) has a significant effect on flow. Further, we tested the structural model for the old and young users separately. Specifically, Heinonen and Strandvik (2007) suggested that age is a key differentiator of responses to digital media between younger and older consumers. The results of the study of Barutu (2007) demonstrate that younger consumers tend to have more positive attitudes than older customers about mobile entertainment. In the study of Persaud and Azhar (2012), 18- to 25-yearold customers comprised most Internet consumers (Table 1). In terms of age, 49% of the respondents were below 24, and 51% were 25 years old or older. To explore the impact of age on our research model, the respondents in our study were divided into two groups: (1) those less than 25 years old called young users and (2) those over 25 years old called old users. Fig. 3 shows that perceived familiarity for old users (b  0.342, p < 0.001) has a positive effect on flow experience, and perceived

Table 1 Demographics of respondents. Demographics Gender Male Female Age range Below 25 25e29 30e39 Above 40 Educational level High school or below Junior college University Master or above Personal income in RMB (monthly) Below 1000 1000e1999 2000e3999 4000e5999 6000e7999 Above 8000 Frequency 90 259 171 138 32 8 6 25 294 24 106 40 56 72 41 34 Percentage (%) 25.8% 74.2% 49.0% 39.5% 9.2% 2.3% 1.7% 7.2% 84.2% 6.9% 30.4% 11.5% 16.0% 20.6% 11.8% 9.7%

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314 Table 2 Results of the confirmatory factor analysis. Constructs Perceived Similarity (PS) Items PS1 PS2 PS3 PS4 PE1 PE2 PE3 PE4 PF1 PF2 PF3 PF4 FL1 FL2 FL3 FL4 PUI1 PUI2 PUI3 PUI4 Loading 0.877 0.857 0.861 0.849 0.911 0.889 0.914 0.865 0.882 0.920 0.919 0.895 0.811 0.872 0.807 0.821 0.916 0.927 0.914 0.917 Cronbach' s alpha 0.884 Composite reliability 0.920

311

Average Variance extracted 0.741

Perceived Expertise (PE)

0.917

0.942

0.801

Perceived Familiarity (PF)

0.926

0.947

0.817

Flow Experience (FL)

0.847

0.897

0.686

Purchase Intention (PUI)

0.938

0.956

0.844

Note: All factor loading are significant at the p < 0.001 level.

Table 3 Correlations among constructs. Constructs 1. 2. 3. 4. 5. Perceived Expertise Perceived Similarity Perceived Familiarity Flow Experience Purchase Intention AVE 0.801 0.741 0.817 0.686 0.844 Cronbatch alpha 0.917 0.884 0.926 0.847 0.938 1 0.895 0.576 0.445 0.575 0.539 2 0.861 0.516 0.673 0.561 3 4 5

0.904 0.543 0.513

0.828 0.617

0.919

Note: Diagonal elements are the square root of the average variance extracted of each construct; Pearson correlations are shown below the diagonal.

Table 4 Item loadings and cross loadings. Constructs Perceived Similarity (PS) Items PS1 PS2 PS3 PS4 PE1 PE2 PE3 PE4 PF1 PF2 PF3 PF4 FL1 FL2 FL3 FL4 PUI1 PUI2 PUI3 PUI4 PS 0.877 0.857 0.861 0.849 0.529 0.477 0.520 0.530 0.487 0.436 0.445 0.488 0.568 0.592 0.524 0.543 0.487 0.521 0.540 0.512 PE 0.492 0.477 0.513 0.502 0.911 0.889 0.914 0.865 0.453 0.333 0.368 0.438 0.524 0.506 0.410 0.456 0.503 0.497 0.490 0.491 PF 0.492 0.458 0.396 0.428 0.359 0.391 0.404 0.437 0.882 0.920 0.919 0.895 0.379 0.527 0.433 0.454 0.461 0.474 0.478 0.473 FL 0.595 0.574 0.565 0.584 0.532 0.483 0.488 0.548 0.525 0.447 0.450 0.526 0.811 0.872 0.807 0.821 0.538 0.580 0.549 0.597 PUI 0.482 0.463 0.481 0.506 0.495 0.470 0.453 0.505 0.479 0.432 0.423 0.509 0.542 0.562 0.476 0.455 0.916 0.927 0.914 0.917

Perceived Expertise (PE)

Perceived Familiarity (PF)

behavior (purchase intention). We used the bootstrapping approach (Preacher & Hayes, 2008; Shrout & Bolger, 2002) to test the mediating effect. Table 5 shows that the indirect effect of flow experience on the relationship between perceived expertise and purchase intention is significant with a 95% bootstrap confidence interval, excluding zero (CI.95  0.0471, 0.1397). This finding suggests that flow experience mediates the effect of perceived expertise on purchase intention. The indirect effect of perceived similarity on purchase intention is also significant (CI.95  0.0667, 0.1803). Moreover, the mediating effect of perceived familiarity is valid (CI.95  0.0239, 0.0880). 5. Discussion and conclusion This study explores the role of flow experience in influencing customers' purchase intention in social shopping sites based on the SOR framework from the perspective of interpersonal interaction. According to Liao et al. (2010), we classify interpersonal attraction factors into perceived expertise, similarity, and familiarity. Then, we investigate the differences in the influence of three interpersonal interaction factors on flow experience and the impact of flow experience on purchase intention according to age. The findings of our research support all the hypotheses, thus confirming that flow experience is a useful predictor of purchase intention in social commerce. Our findings present that all three interpersonal attraction factors significantly affect flow experience in social commerce. Perceived similarity of group members seems to be more important than perceived expertise and perceived familiarity of group members in the context of social commerce. We also find that flow experience has a positive impact on purchase intention, which

Flow experience (FL)

Purchase Intention (PUI)

familiarity for young users has no effect on flow experience, t  3.97. The results indicate that the impact of flow experience on purchase intention is stronger for old users (b  0.64, p < 0.001) than for young users (b  0.45, p < 0.001), t  4.19. 4.4. Mediation analyses H5 posits that consumer flow experience mediates the effect of the interpersonal interaction factors on consumer's response

312

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

Fig. 2. Results of the research model tests.

Fig. 3. Results of the research model tests of young users (old users).

Table 5 Bootstrapping results. 95% Bootstrap confidence intervals for indirect effect Flow experience Effect Perceived expertise Perceived similarity Perceived familiarity 0.090 0.120 0.053 SE 0.023 0.286 0.017 CIs (0.047,0.140) (0.067,0.180) (0.024,0.088)

corresponds with the study of Gao and Bai (2014). Therefore, consumers entering a flow state in a social shopping site will likely purchase from that website. Furthermore, Fig. 3 shows the differences between young and old users. For young social shopping consumers, the impact of similarity perceived by consumers tends to be more influential than perceived expertise on the formation of flow state. A possible explanation is that young consumers prefer to interact with members who have similar tastes with them rather than experts. Our results also indicate that perceived familiarity has no effect on flow experience for young consumers. A potential explanation is that the motivation of young consumers who participate in social commerce is more likely to obtain purchase advice rather than build social bonds with members in social commerce. This group is

involved in social shopping sites to obtain more useful purchase advice. For old social shopping consumers, perceived familiarity and similarity of other members on a social shopping site are more important drivers than perceived expertise in inducing flow experience. Considering perceived familiarity of group members, old users tend to experience stronger flow state than young users. A possible explanation is that older consumers pay more attention to building social bonds than younger consumers. When respondents experience flow state, old users are more likely to purchase from social shopping sites than young users. A potential illustration is that flow experience is more important for old users than young users when making purchase decisions. 6. Limitations and future directions Our study has several limitations. First, this study was conducted with data collected from a social shopping site in China. The results of this study might be different had the model been retested in a different context or in a different cultural environment. In the future, scholars should further test and validate our findings in different contexts and cultural environments. Second, because of the features of the focal social shopping sites in the study, the participants of this study were mainly females. Therefore, future research should study male-oriented social shopping sites and

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

313

provide more insights into the differences between male and female shoppers' shopping behavior. Third, our study was cross sectional. As social shopping sites are dynamic in their development, future research can use a longitudinal design to identify the roles and effects of interpersonal interaction factors perceived by consumers in social shopping sites. 7. Research implications 7.1. Theoretical implications Our study makes important contributions to the existing literature. First, this study extends the extant literature by testing and validating a model that incorporates interpersonal drivers of flow experience in social commerce. Flow experience is proved to be an important predictor of social commerce participation (Zhang et al., 2014). Moreover, interpersonal interaction factors play a useful role in the context of virtual community, which is similar to the social commerce. Only a few studies have examined the flow experience in social commerce, especially from the perspective of interpersonal attraction. Therefore, our study enriches the literature on the drivers of flow experience. The results indicate that interpersonal interaction enhances flow experience. To the best of our knowledge, this study is among the first to empirically test the effects of three kinds of interpersonal interaction factors (perceived expertise, similarity,
Survey Questionnaire Items Constructs Items Measures

social shopping sites. First, our study helps managers to understand the formation of flow experience thoroughly in social commerce. As interpersonal attraction factors significantly affect customer flow experience, practitioners should focus on enhancing the interpersonal attraction of social shopping sites and pay more attention to the factors of perceived expertise, similarity, and familiarity. Second, our study provides a new insight into the different users of social shopping sites. For users in social shopping sites, especially for young users, the role of similarity seems to be more important than the role of familiarity. Therefore, different types of groups should be constructed in social shopping sites to help young members find people who have similar likes and tastes. As for the role of familiarity, old users seem to focus on this factor. Therefore, for those groups in which more old users participate, frequent member interactions should be encouraged. Acknowledgements The work described in this paper was supported by the grants from "the National Natural Science Foundation of China" (NSFC: 71571169, 71571177, 71201150, and 71332001); and supported by "the Fundamental Research Funds for the Central Universities" (WK2040000009). Appendix A

Sources (Liao et al., 2010)

Perceived expertise PE1 PE2 PE3 PE4 Perceived similarity PS1 PS2 PS3 PS4 Perceived PF1 familiarity PF2 PF3 PF4 Flow experience FL1 FL2 FL3 FL4 Purchase intention PUI1 PUI2 PUI3 PUI4

Some members on this website are very knowledgeable about fashion and beauty products. Some members on this website are experts in fashion and beauty products. Some members on this website are highly experienced in fashion and beauty products. Compared with other sites, this website contains much information and knowledge about fashion and beauty products. As regards the styles in fashion and beauty products, I am similar to some members on this website. As regards the tastes in fashion and beauty products, I am similar to some members on this website. As regards my likes and dislikes about fashion and beauty products, I am similar to some members on this website. As regards preferences in fashion and beauty products, I am similar to some members on this website. Members of this website are as familiar to me as good friends are. I have frequent interactions with other members of this website by writing or replying to articles. The members on this website are familiar to me. I keep close contact with this website members. It is fun to interact on this website. The interaction on this website is interesting. When shopping on this website, I feel the excitement of exploring. I am absorbed when shopping on this website. Whenever I need to shop, I intend to purchase products on this website. Whenever I need to shop, I plan to purchase products on this website. I predict that I will purchase products on this website. It is highly likely that I will purchase products on this website.

(Liao et al., 2010)

(Liao et al., 2010)

(Zhang et al., 2014)

(Pavlou & Fygenson, 2006)

and familiarity) on the formation of flow in social commerce. Second, this study advances the understanding that interpersonal interaction factors are important in social commerce. Liao et al. (2010) highlighted the importance of interpersonal interaction factors in affecting the loyalty of members in a virtual community. The present study extends the findings of Liao et al. (2010) by demonstrating that interpersonal interaction factors influence consumer purchase intention in social commerce. Third, we divided our respondents into two groups based on age and found significant differences between old and young users. As only a few previous studies on flow experience compared the differences based on age, our study enriches the literature on flow and purchase intention. 7.2. Managerial implications This research contributes to improving the management of

References
Al-Natour, S., Benbasat, I., & Cenfetelli, R. T. (2005). The role of similarity in ecommerce interactions: the case of online shopping assistants. In SIGHCI 2005 proceedings, 4. Animesh, A., Pinsonneault, A., Yang, S.-B., & Oh, W. (2011). An odyssey into virtual worlds: exploring the impacts of technological and spatial environments on intention to purchase virtual products. MIS Quarterly-Management Information Systems, 35(3), 789e810. Barnes, S. J. (2011). Understanding use continuance in virtual worlds: empirical test of a research model. Information & Management, 48(8), 313e319. Barutu, S. (2007). Attitudes towards mobile marketing tools: a study of Turkish consumers. Journal of Targeting, Measurement and Analysis for Marketing, 16(1), 26e38. Carlson, J., & O'Cass, A. (2011). Creating commercially compelling website-service encounters: an examination of the effect of website-service interface performance components on flow experiences. Electronic Markets, 21(4), 237e253. Chang, C.-C. (2013). Examining users' intention to continue using social network games: a flow experience perspective. Telematics and Informatics, 30(4), 311e321. Chang, S.-H., Chih, W.-H., Liou, D.-K., & Hwang, L.-R. (2014). The influence of web aesthetics on customers' PAD. Computers in Human Behavior, 36, 168e178.

314

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314 Koufaris, M. (2002). Applying the technology acceptance model and flow theory to online consumer behavior. Information Systems Research, 13(2), 205e223. Lee, S. M., & Chen, L. (2010). The impact of flow on online consumer behavior. Journal of Computer Information Systems, 50(4), 1. Lee, H. Y., Qu, H., & Kim, Y. S. (2007). A study of the impact of personal innovativeness on online travel shopping behaviordA case study of Korean travelers. Tourism Management, 28(3), 886e897. Liao, H.-C., Chu, C.-H., Huang, C.-Y., & Shen, Y.-C. (2010). Virtual community loyalty: an interpersonal-interaction perspective. International Journal of Electronic Commerce, 15(1), 49e74. Lu, Y., Zhao, L., & Wang, B. (2010). From virtual community members to C2C ecommerce buyers: trust in virtual communities and its effect on consumers' purchase intention. Electronic Commerce Research and Applications, 9(4), 346e360. MacKenzie, S. B., & Lutz, R. J. (1989). An empirical examination of the structural antecedents of attitude toward the ad in an advertising pretesting context. The Journal of Marketing, 53, 48e65. Manganari, E. E., Siomkos, G. J., Rigopoulou, I. D., & Vrechopoulos, A. P. (2011). Virtual store layout effects on consumer behaviour: applying an environmental psychology approach in the online travel industry. Internet Research, 21(3), 326e346. Mason, C. H., & Perreault, W. D., Jr. (1991). Collinearity, power, and interpretation of multiple regression analysis. Journal of Marketing Research, 28(3), 268e280. Mollen, A., & Wilson, H. (2010). Engagement, telepresence and interactivity in online consumer experience: reconciling scholastic and managerial perspectives. Journal of Business Research, 63(9), 919e925. Nath, C. K. (2009). Behaviour of customers in retail store environment-an empirical study. Vilakshan: The XIMB Journal of Management, 6(2). Newsted, P. R., Huff, S. L., & Munro, M. C. (1998). Survey instruments in information systems. MIS Quarterly, 22(4), 553e554. Ng, C. S.-P. (2013). Intention to purchase on social commerce websites across cultures: a cross-regional study. Information & Management, 50(8), 609e620. Nunally, J. C., & Bernstein, I. H. (1978). Psychometric theory. New York: McGraw-Hill. Pagani, M., & Mirabello, A. (2011). The influence of personal and social-interactive engagement in social TV web sites. International Journal of Electronic Commerce, 16(2), 41e68. Parboteeah, D. V., Valacich, J. S., & Wells, J. D. (2009). The influence of website characteristics on a Consumer's urge to buy impulsively. Information Systems Research, 20(1), 60e78. Pavlou, P. A., & Fygenson, M. (2006). Understanding and predicting electronic commerce adoption: an extension of the theory of planned behavior. MIS Quarterly, 30(1), 115e143. Persaud, A., & Azhar, I. (2012). Innovative mobile marketing via smartphones. Marketing Intelligence & Planning, 30(4), 418e443. Petty, R. E., Cacioppo, J. T., & Goldman, R. (1981). Personal involvement as a determinant of argument-based persuasion. Journal of Personality and Social Psychology, 41(5), 847. Podsakoff, P. M., MacKenzie, S. B., Lee, J.-Y., & Podsakoff, N. P. (2003). Common method biases in behavioral research: a critical review of the literature and recommended remedies. Journal of Applied Psychology, 88(5), 879. Preacher, K. J., & Hayes, A. F. (2008). Asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models. Behavior Research Methods, 40(3), 879e891. Shin, D.-H., & Shin, Y.-J. (2011). Why do people play social network games? Computers in Human Behavior, 27(2), 852e861. Shrout, P. E., & Bolger, N. (2002). Mediation in experimental and nonexperimental studies: new procedures and recommendations. Psychological Methods, 7(4), 422. Tamjidyamcholo, A., Gholipour, R., Baba, M. S. B., & Yamchello, H. T. (2013). Information security professional perceptions of knowledge-sharing intention in virtual communities under social cognitive theory. Research and innovation in information systems (ICRIIS). In 2013 international conference on, IEEE2013 (pp. 416e421). Teng, C.-I., Huang, L.-S., Jeng, S.-P., Chou, Y.-J., & Hu, H.-H. (2012). Who may be loyal? Personality, flow experience and customer e-loyalty. International Journal of Electronic Customer Relationship Management, 6(1), 20e47. Van de Vijver, F. J., & Leung, K. (1997). Methods and data analysis for cross-cultural research. Wang, L. C., Baker, J., Wagner, J. A., & Wakefield, K. (2007). Can a retail web site be social? Journal of Marketing, 71(3), 143e157. Wang, Y. J., Hernandez, M. D., & Minor, M. S. (2010). Web aesthetics effects on perceived online service quality and satisfaction in an e-tail environment: the moderating role of purchase task. Journal of Business Research, 63(9), 935e942. Wang, C., & Zhang, P. (2012). The evolution of social commerce: the people, management, technology, and information dimensions. Communications of the Association for Information Systems, 31(5), 1e23. Wu, W.-Y., Lee, C.-L., Fu, C.-S., & Wang, H.-C. (2013). How can online store layout design and atmosphere influence consumer shopping intention on a website? International Journal of Retail & Distribution Management, 42(1), 4e24. Yoon, E. (2012). Effects of website environmental cues on consumers' response and outcome behaviors. In Open access theses and dissertations from the college of education and human science, (Paper 4). Zhang, H., Lu, Y., Gupta, S., & Zhao, L. (2014). What motivates customers to participate in social commerce? the impact of technological environments and virtual customer experiences. Information & Management, 51(8), 1017e1030. Zhou, T. (2013). An empirical examination of continuance intention of mobile payment services. Decision Support Systems, 54(2), 1085e1091.

Cheung, C. M., & Lee, M. K. (2009). Understanding the sustainability of a virtual community: model development and empirical test. Journal of Information Science, 35(3), 279e298. Constant, D., Sproull, L., & Kiesler, S. (1996). The kindness of strangers: the usefulness of electronic weak ties for technical advice. Organization Science, 7(2), 119e135. Ding, D. X., Hu, P. J.-H., Verma, R., & Wardell, D. G. (2010). The impact of service system design and flow experience on customer satisfaction in online financial services. Journal of Service Research, 13(1), 96e110. Donovan, R. J., & Rossiter, J. R. (1982). Store atmosphere: an environmental psychology approach. Journal of Retailing, 58(1), 34e57. Faiola, A., Newlon, C., Pfaff, M., & Smyslova, O. (2013). Correlating the effects of flow and telepresence in virtual worlds: enhancing our understanding of user behavior in game-based learning. Computers in Human Behavior, 29(3), 1113e1121. Fang, Y., Qureshi, I., Sun, H., McCole, P., Ramsey, E., & Lim, K. H. (2014). Trust, satisfaction, and online repurchase intention: the moderating role of perceived effectiveness of E-Commerce institutional mechanisms. MIS Quarterly, 38(2), 407e427. Fiore, A. M., & Kim, J. (2007). An integrative framework capturing experiential and utilitarian shopping experience. International Journal of Retail & Distribution Management, 35(6), 421e442. Floh, A., & Madlberger, M. (2013). The role of atmospheric cues in online impulsebuying behavior. Electronic Commerce Research and Applications, 12(6), 425e439. Flynn, B. B., Sakakibara, S., Schroeder, R. G., Bates, K. A., & Flynn, E. J. (1990). Empirical research methods in operations management. Journal of Operations Management, 9(2), 250e284. Fornell, C., & Larcker, D. F. (1981). Evaluating structural equation models with unobservable variables and measurement error. Journal of Marketing Research, 18(1), 39e50. Gao, L., & Bai, X. (2014). Online consumer behaviour and its relationship to website atmospheric induced flow: Insights into online travel agencies in China. Journal of Retailing and Consumer Services, 21(4), 653e665. Gefen, D., & Straub, D. (2005). A practical guide to factorial validity using PLSGraph: tutorial and annotated example. Communications of the Association for Information Systems, 16(1), 91e109. Hair, J., Anderson, R. E., Tatham, R. L., & Black, W. (1998). Multivariate data analysis New Jersey. USA: Englewood Cliffs. Ha, Y., & Lennon, S. J. (2010). Online visual merchandising (VMD) cues and consumer pleasure and arousal: purchasing versus browsing situation. Psychology & Marketing, 27(2), 141e165. Heinonen, K., & Strandvik, T. (2007). Consumer responsiveness to mobile marketing. International Journal of Mobile Communications, 5(6), 603e617. Hinds, P. J., Carley, K. M., Krackhardt, D., & Wholey, D. (2000). Choosing work group members: balancing similarity, competence, and familiarity. Organizational behavior and human decision processes, 81(2), 226e251. Hoffman, D. L., & Novak, T. P. (1996). Marketing in hypermedia computer-mediated environments: conceptual foundations. The Journal of Marketing, 60(3), 50e68. Hsiao, K.-L., Lin, J. C.-C., Wang, X.-Y., Lu, H.-P., & Yu, H. (2010). Antecedents and consequences of trust in online product recommendations: an empirical study in social shopping. Online Information Review, 34(6), 935e953. Hsieh, J.-K., Hsieh, Y.-C., Chiu, H.-C., & Yang, Y.-R. (2014). Customer response to web site atmospherics: task-relevant cues, situational involvement and PAD. Journal of Interactive Marketing, 28(3), 225e236. Hsu, C.-L., Chang, K.-C., & Chen, M.-C. (2011). The impact of website quality on customer satisfaction and purchase intention: perceived playfulness and perceived flow as mediators. Information Systems and e-Business Management, 10(4), 549e570. Hsu, C. L., Chang, K. C., & Chen, M. C. (2012). Flow experience and internet shopping behavior: Investigating the moderating effect of consumer characteristics. Systems Research and Behavioral Science, 29(3), 317e332. Huang, E. (2013). Interactivity and identification influences on virtual shopping. International Journal of Electronic Commerce Studies, 4(2), 305e312. Huang, Z., & Benyoucef, M. (2014). User preferences of social features on social commerce websites: an empirical study. Technological Forecasting and Social Change, 95, 57e72. Jeong, S. W., Fiore, A. M., Niehm, L. S., & Lorenz, F. O. (2009). The role of experiential value in online shopping: the impacts of product presentation on consumer responses towards an apparel web site. Internet Research, 19(1), 105e124. Jiang, Z., Chan, J., Tan, B. C., & Chua, W. S. (2010). Effects of interactivity on website involvement and purchase intention. Journal of the Association for Information Systems, 11(1), 34e59. Kelman, H. C. (1961). Processes of opinion change. Public Opinion Quarterly, 25(1), 57e78. Kim, H., & Lennon, S. J. (2010). E-atmosphere, emotional, cognitive, and behavioral responses. Journal of Fashion Marketing and Management, 14(3), 412e428. Kim, J., & Lennon, S. J. (2013). Effects of reputation and website quality on online consumers' emotion, perceived risk and purchase intention: based on the stimulus-organism-response model. Journal of Research in Interactive Marketing, 7(1), 33e56. Kim, Y. G., & Li, G. (2009). Customer satisfaction with and loyalty towards online travel products: a transaction cost economics perspective. Tourism Economics, 15(4), 825e846. Komiak, S. Y., & Benbasat, I. (2006). The effects of personalization and familiarity on trust and adoption of recommendation agents. MIS Quarterly, 30(4), 941e960. Koo, D.-M., & Ju, S.-H. (2010). The interactional effects of atmospherics and perceptual curiosity on emotions and online shopping intention. Computers in Human Behavior, 26(3), 377e388.

View publication stats

Database, 2016, 1–8
doi: 10.1093/database/baw127
Database Tool

Database Tool

Skeleton Genetics: a comprehensive database
for genes and mutations related to genetic
skeletal disorders
Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Chong Chen1,†, Yi Jiang2,†, Chenyang Xu1, Xinting Liu2, Lin Hu3,
Yanbao Xiang1, Qingshuang Chen1, Denghui Chen2, Huanzheng Li1,
Xueqin Xu1 and Shaohua Tang1,3,*
1

Department of Genetics of Dingli Clinical Medical School, Wenzhou Central Hospital, Wenzhou
325000, China, 2Institute of Genomic Medicine, Wenzhou Medical University, Wenzhou 325035, China,
3
School of Laboratory Medicine and Life Science, Wenzhou Medical University, Wenzhou 325035, China
*Corresponding author: Tel/Fax: 86 577 88070197; Email: tsh006@126.com
†

These authors contributed equally to this work.

Citation details: Chen,C., Jiang,J., Xu,C. et al. SkeletonGenetics: a comprehensive database for genes and mutations
related to genetic skeletal disorders. Database (2016) Vol. 2016: article ID baw127; doi:10.1093/database/baw127
Received 2 May 2016; Revised 2 August 2016; Accepted 12 August 2016

Abstract
Genetic skeletal disorders (GSD) involving the skeletal system arises through disturbances
in the complex processes of skeletal development, growth and homeostasis and remain a
diagnostic challenge because of their clinical heterogeneity and genetic variety. Over the
past decades, tremendous effort platforms have been made to explore the complex heterogeneity, and massive new genes and mutations have been identified in different GSD,
but the information supplied by literature is still limited and it is hard to meet the further
needs of scientists and clinicians. In this study, combined with Nosology and Classification
of genetic skeletal disorders, we developed the first comprehensive and annotated genetic
skeletal disorders database, named ‘SkeletonGenetics’, which contains information about
all GSD-related knowledge including 8225 mutations in 357 genes, with detailed information associated with 481 clinical diseases (2260 clinical phenotype) classified in 42 groups
defined by molecular, biochemical and/or radiographic criteria from 1698 publications.
Further annotations were performed to each entry including Gene Ontology, pathways
analysis, protein–protein interaction, mutation annotations, disease–disease clustering
and gene–disease networking. Furthermore, using concise search methods, intuitive
graphical displays, convenient browsing functions and constantly updatable features,
‘SkeletonGenetics’ could serve as a central and integrative database for unveiling the genetic and pathways pre-dispositions of GSD.
Database URL: http://101.200.211.232/skeletongenetics/

C The Author(s) 2016. Published by Oxford University Press.
V

Page 1 of 8

This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.
(page number not for citation purposes)

Page 2 of 8

Introduction

mutation information and extensive annotations for GSD.
Initially, we retrospectively extracted the basic information
for each mutation (disease conditions or syndrome, gene
symbol, number of OMIM, hereditary mode, coding sequence change, transcript, ethnicity, age, PubMed ID, etc.)
from open publications. Additionally, extensive annotations were performed, which include gene information,
Gene Ontology, pathways analysis, protein-protein interaction, mutations clustering and gene-disease network. As
a result, 42 groups of GSD, 481 diseases or syndromes,
357 genes, 5884 single nucleotide variations (SNVs), 516
insertions, 1427 deletions and 2260 different phenotypes
were included. Taken together, ‘SkeletonGenetics’ is constructed to be a well-organized, internal-standard and
comprehensive resource for scientists and clinicians looking for the clinical correlates of mutations, genes and pathways involved in skeletal biology.

Methods
The standard of data entry group of GSD
The Nosology Group of the International Skeletal
Dysplasia Society formulated the table of NCGSD-2015 in
September, 2015. The criteria used for genetic skeletal disorders were unchanged from NCGSD-2010 revision. They
were (1) significant skeletal involvement, including skeletal
dysplasias, skeletal dysostoses, metabolic bone disorders
and skeletal malformation and/or reduction; (2) publication and/or listing in OMIM; (3) genetic basis proven by
pedigree or very likely based on homogeneity of phenotype
in unrelated families; (4) nosologic autonomy confirmed
by molecular or linkage analysis and/or by the presence of
distinctive diagnostic features and of observation in multiple individuals or families (4). In accordance with the
standards of NCGSD-2015, 481 different conditions were
placed in 42 groups, associated with one or more of 357
different genes. The grouping results and criteria are presented in Supplementary Table S1.

Data collection and content
Based on the standards of NCGSD-2015, we performed
comprehensive searches for GSD-related publications and
databases to obtain a complete list of mutation information associated with GSD. Firstly, we retrospectively
queried the PubMed database (http://www.ncbi.nih.gov/
pubmed) for genes and diseases retrieved in Entrez with
terms ‘gene or disease symbol [Title/Abstract] AND mutation [Title/Abstract] OR variant [Title/Abstract]. Secondly,
other databases were taken as supplementary sources,
including Leiden Open Variation Database 3.0 (LOVD3.0,

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

The skeleton provides the structural framework in humans
for muscle attachments, assists movement, protects organs,
and maintains the homeostasis of the vascular systems (1).
Perturbation of development in bone, cartilage and joints
could result in human skeletal dysplasias, which affect approximately 1 in 5000 live births and is one of the common
causes of neonatal birth defect in departments of obstetrics
(2). Genetic skeletal disorders (GSD), which are a group of
disorders involving gene mutations or genetic susceptibility
to bone disease, account for most of the human skeletal
dysplasias, involving a variety of pathogenic factors and
diseases (3). In the Nosology and Classification of Genetic
Skeletal Disorders 2015 revision (hereinafter referred to as
NCGSD-2015), 436 conditions were included and placed
in 42 groups, which were associated with mutations in one
or more of 336 unique genes (4). Till now, especially with
the impact the next-generation sequencing technology has
made on the study of genetic skeletal disorders, a large
amount of mutations and novel GSD-related genes have
been identified. Genetic skeletal disorders face similar problems to other complex diseases with exceptional genetic
heterogeneity and clinical variety (5). Notably, the same
skeletal dysplasia gene can lead to substantially different
clinical phenotypes or a specific skeletal phenotype can be
caused by mutations in different genes (6). Therefore,
understanding genotype–phenotype correlations and functional diversity remains one of the major challenges for
GSD.
The NCGSD-2015 provides an overview of the recognized diagnostic entities of GSD by clinical, anatomical site
and molecular pathogenesis for clinicians, scientists and the
radiology community to diagnose individual cases and
describe novel disorders (5). However, the increasing availability of next-generation sequencing technology and other
new sequencing platforms will likely result in a rapid identification of novel GSD-related genes and mutations, and
novel phenotypes associated with mutations in genes linked
to other phenotypes has increased dramatically to date (7,
8). Therefore, the catalog of GSD-related genotype-phenotype has become so large as to surpass the scope of a
‘Nosology’, so the Nosology must be transformed into an
automated annotation and query database. Thus, it is crucial
to integrate the existing data and present an organized comprehensive mutation repository to construct an integrative,
informative and updatable resource for GSD-related genetic
pre-dispositions which could greatly facilitate the counseling, diagnosis and therapy for pediatrics and genetics.
In this study, we reviewed three dependent databases
and public resources related to genetic skeletal disorders
and made the first annotated database about GSD, named
‘SkeletonGenetics’, which provides full-scale gene and

Database, Vol. 2016, Article ID baw127

Database, Vol. 2016, Article ID baw127

Table 1 Data content and statistics of genetic data in
SkeletonGenetics
Data type
Mutation
SNVs
Deletions
Insertions
Block substitution
Genes
Disease group
Disease or syndrome
Phenotypes
GOs
Pathways
KEGG pathways
Wiki pathways
MicroRNA Target
PPI
Publications

Data count
8225
5884
1427
516
398
357
42
481
2260
2535
138
64
74
66
40
1698

syndrome, Cornelia de Lange syndrome, which all belong
to the Top 5 of the disease list, all have >367 gene mutation information. At the same time, we classified the number of mutations by group: the limb hypoplasia-reduction
defects group; the overgrowth syndromes with skeletal involvement group; the lysosomal storage diseases with skeletal involvement (dysostosis multiplex group); the
disorganized development of skeletal components group;
and the dysostoses with predominant craniofacial involvement group all have >375 mutation information. Finally,
according to the information collected, we statistically analyzed the bias by chromosomes, mutation types, mutation
effect, gender, ages of onset and inheritance mode.

Information statistics
Specific skeletal phenotype can be caused by mutations in
different genes or the same gene can lead to substantially
different clinical phenotypes. This leads to the bias of genotype and phenotype of GSD, therefore statistics on the
number of mutations for gene, disease and the group become very important. The gene Top 5 with the highest
number of mutations in ‘SkeletonGenetics’ is ‘FBN1’, associated with four skeletal diseases (Marfan syndrome,
Weill-Marchesani syndrome 2, Geleophysic dysplasia type
2, Acromicric dysplasia) with 639 mutations; followed by
‘NF1’ associated with Neurofibromatosis type 1 with 415
mutations; ‘NIPBL’ associated with Cornelia de Lange syndrome 1 with 406 mutations; ‘NSD1’ associated with
Sotos syndrome 1 with 362 mutations; ‘GNPTAB’ associated with two diseases (Mucolipidosis II, alpha/beta type;
Mucolipidosis III (Pseudo-Hurler polydystrophy), alpha/
beta type) with 231 mutations. Correspondingly, Marfan
syndrom, Fanconi anemia, Neurofibromatosis, Sotos

Functional and enrichment analysis
To further interpret the function and heterogeneity of
GSD, ‘SkeletonGenetics’ performs a series of functional
analyses, which included enrichment analysis, mutation
annotation, mutation spectrum and gene-disease network
construction (Figure 1).
For the aspect of enrichment analysis, we provided
Gene Ontology (GO), KEGG pathways, Wikipathways,
MicroRNA Target and Protein–Protein interaction.
Meanwhile, we used WebGestalt software to store the
gene list in ‘SkeletonGenetics’ (12, 13). Users can click on
the ‘Analysis’ page to see whether the gene of interest is
involved in any GO terms, function pathways, microRNA
target or PPIs.
Gene ontology annotation terms involved the biological
process, the cellular component and molecular function of
three biological processes, the Top5 most statistically

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

http://www.lovd.nl/3.0/home) which contained fanconi anemia relevant genes and the mutations predicted to be benign or which did not segregate with phenotype were
excluded (9), Clinvar (http://www.ncbi.nlm.nih.gov/clin
var/), which contained pathogenic or likely pathogenic
clinical mutation information. Human Phenotype
Ontology
(HPO,
http://human-phenotype-ontology.
github.io/) (10) and Online Mendelian Inheritance in Man
(OMIM) (11) were used to describe phenotypic information. Patient clinical data have been obtained in accordance
with the tenets of the Declaration of Helsinki.
Finally, >2000 publications were queried from open resource beginning in 1990. After manually screening the abstracts and full-text of these publications, we excluded
those studying diseases other than GSD and eventually retained 1698 publications. We extracted the basic information for each mutation, including disease name, gene
symbol, inheritance mode, CDS change, PubMed ID, ethnicity, age, gender and functional study through reading
the full-text articles and double-checked these manually.
The in-house perl program was used to obtain the correct
genomic coordinate for each entry. As of the beginning of
2016, the ‘SkeletonGenetics’ database contains 481 disease
conditions, associated with mutations in one or more of
357 different genes, 8225 variations (5884 SNVs, 1427 deletions, 516 insertions and 398 Block substitution), 2260
clinical phenotypes and their detailed information included
in 42 groups of GSD from 1698 publications ranging from
common, recurrent mutations to ‘private’ mutations found
in single families or individuals. The results are presented
in Table 1.

Page 3 of 8

Page 4 of 8

Database, Vol. 2016, Article ID baw127

significant (P-values) of biological process terms in
‘SkeletonGenetics’, namely, skeletal system development
(P-value: 3.29E-63, number of genes, 87) (14); appendage
development (P-value: 6.25E-46, number of genes, 50)
(15); limb development (P-value: 6.25E-46, number of
genes, 50) (16); appendage morphogenesis (P-value:
5.11E-45, number of genes, 48) (15); and limb morphogenesis (P-value: 5.11E-45, number of genes, 48) (16). The
above entries of the most statistically significant GOs terms
are all related to skeletal system or component formation,
morphological differentiation and location control.
Meanwhile, we provided functions pathways, including
KEGG pathways and Wikipathways of two annotation
methods. A complex series of signaling pathways including
TGF-beta signaling pathway (P-value: 1.07E-13, number
of genes, 14) (17); Hedgehog signaling pathway (P-value:
5.85E-09, number of genes, 9) (18); WNT signaling pathway and pluripotency (P-value: 1.72E-06, number of
genes, 10) (19); Notch signaling pathway (P-value: 4.79E07, number of genes, 7) (20) and metabolic processing
(P-value: 1.48E-14, number of genes, 41) are essential for
proper skeletogenesis (21), mainly distributed in the cell,
extracellular matrix, and transcriptional regulation related
to bone, cartilage, and joint formation. Researchers recommend use of a morphogen rheostat model to conceptualize
the differential signaling inputs which lead to divergent
skeletal phenotypes within a temporal and spatial context

(1). In these terms, we have established some function
pathways of the relationship between different gene mutations and groups of bone diseases.
The modification of microRNA by degraded target
mRNA maintains cellular homeostasis and regulates cell
fate transitions during differentiation. These processes are
important to ensure proper organogenesis and growth of
skeleton. ‘hsa_CTTTGCA, MIR-527’, which ranks first
with the P-value of 3.93E-07 among all MicroRNA Target
enriched, including the MYCN gene, in which miRNA
cluster heterozygous mutations cause Feingold syndrome,
a disorder that involves limb malformations, microcephaly,
learning disability/mental retardation, hand and foot
abnormalities and may include hypoplastic thumbs, clinodactyly of the second and fifth fingers, syndactyly (characteristically between the second and the fourth and fifth
toes) and shortened or absent middle phalanges, cardiac
and renal malformations, vertebral anomalies and deafness
(22). Defining the targets of this miRNAs gene will give a
deeper understanding of the pathophysiology and complex
genetics of GSD.
Finally, PPIs enriched was introduced, ‘Hsapiens_Module_19’, which was the most statistically significant with the P-value of 1.83E-17 among all PPIs, was
mainly involved in the collagen group (COL1A1,
COL1A2, COL2A1, COL9A1, COL9A2, COL9A3,
COL10A1, COL11A1 and COL11A2) (23), the matrix

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

Figure 1. Flowchart of the procedure for ‘SkeletonGenetics’. ‘SkeletonGenetics’ mainly consists of three parts: (i) data extraction based on literature
search and GSD-related databases, (ii) annotation of all mutations and genes using ANNOVAR and (iii) enrichment analysis by WebGestalt and
gene–disease network analysis graphically.

Database, Vol. 2016, Article ID baw127

metalloproteinase group (MMP2, MMP9 and MMP13)
(24) and the Fibrillin-TGFbeta receptor group (causing
overgrowth syndromes, including FBN1, FBN2, TGFb1
and TGFb2) (25).
The above functional and enrichment analysis results,
including classical genetic and epigenetic modifications, is
consistent with previous findings that skeletal system development, appendage development, limb development,
appendage morphogenesis and limb morphogenesis are
related to genetic skeletal disorders.

Mutation annotation

Mutation spectrum and gene–disease network
The location of the gene mutation is biased, some of which
are located in the 50 -UTR region, 30 -UTR region or the
mutation-rich region, and some are distributed in single
mutation sites, such as 1138G > A mutation in FGFR3 of
achondroplasia (30) and 49G > A mutation in AKT1 of
proteus syndrome (31), which accounted for 98% and
100% of the total number of mutations, respectively.

Therefore, in order to facilitate the search for mutation information and statistics on the bias of the mutation position, we used scalable vector graphics (SVG) to visualize
the mutation distribution in each GSD-gene for related syndromes, each simulated fonts including gene position information, gene name (number of exons, transcript ID), and
encoded information, with different colors to represent different mutation effects or types, which presented a gene
level overview of the summarized mutations. For example,
the syndrome of achondrogenesis type 2 (ACG2; LangerSaldino), can be expressed as chr12:48 367 189-48 398 104,
Gene: COL2A1 (54 coding exons, NM_001844 or
NM_033150) and mutations in exonic or intronic, the variations were more than once distinguished by different colors
and fonts from those first identified. Besides, SVG was used
to construct a graphical gene-disease network to provide the
potential relations of GSD and skeletal-related genes for
understanding the complex heterogeneity of GSD.
Information about the gene–disease network includes the
number of GSD genes, mutation information and different
disease phenotypes. For example, the COL2A1 related to
nine common genetic skeletal disorders, including achondrogenesis type 2 (ACG2; Langer-Saldino), platyspondylic
dysplasia, Torrance type, hypochondrogenesis, spondyloepiphyseal dysplasia congenital (SEDC), spondyloepimetaphyseal dysplasia (SEMD) strudwick type, kniest dysplasia,
spondyloperipheral dysplasia, mild SED with premature
onset arthrosis, SED with metatarsal shortening (formerly
Czech dysplasia), stickler syndrome type 1, are depicted by
a simple ball (red ball represents gene and blue ball represents disease) and a straight line to construct the gene–disease network. Users click on the corresponding graphics and
can quickly link to detailed information on mutations and
phenotypes.

Data search and browse
‘SkeletonGenetics’ provides a quick and concise search box
on the home page for searching by five symbols. Firstly,
there are three gene symbol search modules, ‘gene symbol’,
‘gene ID’ and ‘gene transcript’; secondly, mutation position
and phenotype information were incorporated to allows
users to search by (i) specifying options like gene name
(capital letters), gene ID, transcript information (ii) investigating the specified phenotype of the typical skeletal
changes related to the position of the forearm of the upper
limb, to the description of the variability to the extent of
skeletal changes (iii) search mutation position data of more
than one gene or mutation, or when it is known that a mutation or gene is located on a particular area or chromosome, a position symbol list will be needed to achieve this
fuzzy search. To facilitate users browsing the data, two

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

‘SkeletonGenetics’ performed the detailed annotation information of all mutations to facilitate the users to assess
the regarding interest mutations. Firstly, the coordinates of
variations, such as SNV, (FBN1, NM_000138 and
c.5198G > A) or InDels, (COL2A1, NM_033150 and
c.4234_4245del) were converted to the corresponding
coordinates on the human reference genome GRC37/hg19
(chr15:48755305-48755305) and (chr12:48367202-4836
7213) by UCSC Genome Browser (26) and the in-house
perl program was used to convert coordinates from CDS to
genome. Secondly, the general annotation of mutations,
such as the effects on protein coding (frameshift, nonframeshift, non-synonymous, splicing, stopgain, stoploss,
etc.), amino acid change and the location of the mutation
(exonic, intronic, intergenic, region, etc.) were performed
by ANNOVAR (27). Additionally, more detailed clinical
information was provided about each entry, including
PubMed ID, ethnicity, gender (male or female), age-ofonset (death, newborn, days, weeks, months, years) and
hereditary mode. Another 27 databases or data sets were
linked and annotated, such as seven quick links (NCBI,
HGNC, MGI, OMIM, Ensembl, Vega and GeneCards),
15 functional prediction software, dbSNP (28) and 1000
Genomes Project (29), ESP, CG69, ExAC. Phenotype
information extracted from the HPO databases and provided the OMIM ID, MGI ID, phenotype or syndrome
name (such as Weill–Marchesani syndrome 2, dominant),
phenotype description (related to search module phenotype
button), cosmic70, clinvar_20150330 information, etc.

Page 5 of 8

Page 6 of 8

different approaches are provided: (i) browse by disorders
(ii) browse by chromosome (Figure 2). The ‘browse by disorders’ option provides 42 groups and 481 conditions of
genetic skeletal disorders for users to conveniently retrieve
the information about mutations of interest. The genes and
mutations related to this group or disease conditions can
be easily retrieved by selecting from the list. Additionally,
users can browse all the variants that are mapped on the
entry chromosome or chromosomal bands in a graphical
way in ‘browse by chromosome’, which is linked to the
mutations information page.

Database, Vol. 2016, Article ID baw127

provide a user-friendly web interface for searching and
browsing. Meanwhile, the database was organized in two
different table output formats by ANNOVAR software (27)
and GOs annotation, KEGG pathways, Wikipathways,
MicroRNA Target and Protein–Protein interaction were
stored in separate tables. All data can be freely downloaded
from the website (Download page). The web client has been
successfully tested with Internet Explorer 10, Chrome 48.0,
Safari 7.1 and Firefox 2/3 and is implemented independently
of the operating system.

Results and discussion
Database organization and web interface

The assignment of genetic skeletal disorders into specific
groups has been practiced since the previous versions of
the ‘Nosology’ by biochemical, molecular information
available, or the group of disorders with similar

Figure 2. A screenshot of the search, browse and annotation module in ‘SkeletonGenetics’. Search box at home page for searching by five symbols.
‘gene symbol’, ‘gene ID’ and ‘gene transcript’, ‘mutations position’ and phenotype information. ‘Browse by chromosome’ is used to retrieve all GSDrelated genes mapped on chromosomes, ‘Browse by disease’ is used to retrieve all GSD-related genes’. Annotation module including functional and
enrichment analysis, mutation annotation, mutation spectrum and gene–disease network.

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

In ‘SkeletonGenetics’, all the data were stored and managed in a MySQL relational database and run on an
Apache HTTP server by PHP and JavaScript program to

Database, Vol. 2016, Article ID baw127

phenotypes. Automatic mining methods will be used in
‘SkeletonGenetics’ for updating GSD-related data, (1) collect the latest disorders, genes or mutation from PubMed
or open databases related to GSD; (2) perform more extensive functional and enrichment analysis based on the
updated data sets; (3) improve the mutation spectrum,
gene–disease network and other database functionalities.
In SkeletonGenetics, researchers may use the ‘Submission’
page to upload de novo mutations for GSD or new genetic
skeletal disorders to keep the database up-to-date and
comprehensive. We believe that ‘SkeletonGenetics’ will
hopefully have paved the way by setting standards for the
recognition and definition of skeletal phenotypes and
understanding of the complex heterogeneity of GSD and
hope that the continued efforts to improve
‘SkeletonGenetics’ will ultimately help improve diagnosis
and treatment of genetic skeletal disorders.

Supplementary data
Supplementary data are available at Database Online.

Acknowledgements
We thank Drs Lili Zhou, Xiangnan Chen, Jiaojiao Lv, Ping Wang,
Yunying Chen, Ke Wu, Zhaotang Luan, Manli Jia, at the
Department of Genetics of Dingli Clinical Medical School,
Wenzhou Central Hospital and School of Laboratory Medicine and
Life Science, Wenzhou Medical University, for the data collecting
and testing.

Funding
Natural Science Foundation of Zhejiang province (LY13H200002);
Science and technology project of Wenzhou (Y20150092); The
Ministry of Health Project (WKJ2011-2-017). This study was supported by the Natural Science Foundation of Zhejiang province in
Hangzhou, China (); Science and technology project of Wenzhou in
Zhejiang, China(); The Ministry of Health Project in ().
Conflict of interest. None declared.

References
1. Baldridge,D., Shchelochkov,O., Kelley,B. et al. (2010) Signaling
pathways in human skeletal dysplasias. Annu. Rev. Genomics.
Hum. Genet., 11, 189–217.
2. Krakow,D. and Rimoin,D.L. (2010) The skeletal dysplasias.
Genet. Med., 12, 327–341.
3. Mortier,G.R. (2001) The diagnosis of skeletal dysplasias: a
multidisciplinary approach. Eur. J. Radiol., 40, 161–167.
4. Warman,M.L., Cormier,D.V., Hall,C. et al. (2015) Nosology
and classification of genetic skeletal disorders: 2015 revision.
Am. J. Med. Genet., 167A, 2869–2892.
5. Rousseau,F., Bonaventure,J., Legeai,M.L. et al. (1996) Clinical
and genetic heterogeneity of hypochondroplasia. J. Med. Genet.,
33, 749–752.

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

phenotypic features defined by common radiographic features or anatomical site. Meanwhile, researchers criticize
the previous versions, focusing on their ‘hybrid’ nature,
which does not stick to a single systematic approach.
Firstly, disorders should be classified on phenotypic similarities. Secondly, they should be reclassified based on the
pathway or gene related to the functional abnormality (4).
Based on the above principles, and as more and more resources are published on the network resource, we developed a comprehensive database for genes and mutations
related to genetic skeletal disorders, ‘SkeletonGenetics’
integrated data types associated with GSD through indepth mining of 1698 publications and extensive functional analysis, which covered a broad range of data
including lists of disease grouping and disease names,
genes, mutations and mutations spectrum, GO terms, pathways, microRNA target, protein–protein interaction and
gene–disease network. Meanwhile, combined with concise
search methods, intuitive graphical displays, convenient
browsing functions and constantly updatable features, the
‘SkeletonGenetics’ database could serve as a reclassified
reference tool and valuable resource for unveiling the genetic and pathway basis of GSD.
With the development of the high-throughput sequencing, massive genetic skeletal disorder related genes and mutations have been identified in the past decade, but there is
still about 30–40% of GSD with undiscovered disease
genes (4, 6) because of the restriction of patients and complexity of the gene interaction network. In this study, functional analysis of the known GSD genes in publications
mostly involved in specific GO items, such as skeletal system development, appendage development, limb development, appendage morphogenesis and limb morphogenesis
or pathways, such as classical FGFs, TGF-beta, Hedgehog,
WNT, Notch signaling pathways. Meanwhile, many of the
new identified genes interact with known GSD genes (32–
35) or key GSD genes could induce the disease state. In
SkeletonGenetics, if researchers have found a specific GSD
gene, they may link to the ‘Analysis’ page to see whether
the new gene is involved in any skeletal-related GO terms,
pathways or PPIs. This database would be important and
useful for revealing novel GSD-related genes and pathways. Therefore, researchers could focus on the other unknown genes, which were involved in the same skeletal
development and homeostasis functions module combined
with biochemical, molecular information or the group of
disorders with similar phenotypic features.
The increasing availability of massive parallel sequencing and other new sequencing technologies will likely result
in a rapid and cost-effective identification of many GSDcausing genes and mutations, or novel phenotypes associated with mutations in genes already linked to other

Page 7 of 8

Page 8 of 8

21. Krakow,D., Robertson,S.P., King,L.M. et al. (2004) Mutations
in the gene encoding filamin B disrupt vertebral segmentation,
joint formation and skeletogenesis. Nat. Genet., 36, 405–410.
22. Van,B.H., Celli,J., van,R.J. et al. (2005) MYCN haploinsufficiency is associated with reduced brain size and intestinal atresias
in Feingold syndrome. Nat. Genet., 37, 465–467.
23. Forlino,A. and Marini,J.C. (2015) Osteogenesis imperfecta.
Lancet, 15, 1657–1671.
24. Alameddine,H.S. (2012) Matrix metalloproteinases in skeletal
muscles: friends or foes? Neurobiol. Dis., 48, 508–518.
25. Zhao,F., Pan,X., Zhao,K. et al. (2013) Two novel mutations of
fibrillin-1 gene correlate with different phenotypes of Marfan
syndrome in Chinese families. Mol. Vis., 19, 751–758.
26. Speir,M.L., Zweig,A.S., Rosenbloom,K.R. et al. (2016) The
UCSC Genome Browser database: 2016 update. Nucleic Acids
Res., 44, 717–725.
27. Wang,K., Li,M., and Hakonarson,H. (2010) ANNOVAR: functional annotation of genetic variants from high-throughput
sequencing data. Nucleic Acids Res., 38, e164.
28. Sherry,S.T., Ward,M.H., Kholodov,M. et al. (2001) dbSNP: the
NCBI database of genetic variation. Nucleic Acids Res., 29,
308–311.
29. 1000 Genomes Project Consortium (2012) An integrated map of
genetic variation from 1,092 human genomes. Nature, 491,
56–65.
30. Horton,W.A., Hall,J.G., and Hecht,J.T. (2007) Achondroplasia.
Lancet, 370, 162–172.
31. Lindhurst,M.J., Sapp,J.C., Teer,J.K. et al. (2011) A mosaic activating mutation in AKT1 associated with the Proteus syndrome.
N. Engl. J. Med., 365, 611–619.
32. Cho,T.J., Lee,K.E., Lee,S.K. et al. (2012) A single recurrent mutation in the 5’-UTR of IFITM5 causes osteogenesis imperfecta
type V. Am. J. Hum. Genet., 91, 343–348.
33. Yamamoto,G.L., Baratela,W.A., Almeida,T.F. et al. (2014)
Mutations in PCYT1A cause spondylometaphyseal dysplasia
with cone-rod dystrophy. Am. J. Hum. Genet., 94, 113–119.
34. Schmidts,M., Vodopiutz,J., Christou,S.S. et al. (2013)
Mutations in the gene encoding IFT dynein complex component
WDR34 cause Jeune asphyxiating thoracic dystrophy. Am. J.
Hum. Genet., 93, 932–944.
35. Martin,C.A., Ahmad,I., Klingseisen,A. et al. (2014) Mutations
in PLK4, encoding a master regulator of centriole biogenesis,
cause microcephaly, growth failure and retinopathy. Nat.
Genet., 46, 1283–1292.

Downloaded from http://database.oxfordjournals.org/ by guest on December 23, 2016

6. Geister,K.A. and Camper,S.A. (2015) Advances in Skeletal
Dysplasia Genetics. Annu. Rev. Genomics Hum. Genet., 16,
199–227.
7. Bernier,F.P.,
Caluseriu,O.,
Ng,S.
et
al.
(2012)
Haploinsufficiency of SF3B4, a component of the pre-mRNA
spliceosomal complex, causes Nager syndrome. Am. J. Hum.
Genet., 90, 925–933.
8. Twigg,S.R.
and
Wilkie,A.O.
(2015)
A
geneticpathophysiological framework for craniosynostosis. Am. J.
Hum. Genet., 97, 359–377.
9. Fokkema,I.F., Taschner,P.E., Schaafsma,G.C. et al. (2011)
LOVD v.2.0: the next generation in gene variant databases.
Hum. Mutat., 32, 557–563.
10. Köhler,S., Doelken,S.C., Mungall,C.J. et al. (2014) The Human
Phenotype Ontology project: linking molecular biology and disease through phenotype data. Nucleic Acids Res., 42, 966–974.
11. Hamosh,A., Scott,A.F., Amberger,J.S. et al. (2005) Online
Mendelian Inheritance in Man (OMIM), a knowledgebase of
human genes and genetic disorders. Nucleic Acids Res., 33,
514–517.
12. Wang,J., Duncan,D., Shi,Z. et al. (2013) WEB-based GEne SeT
AnaLysis Toolkit (WebGestalt): update 2013. Nucleic Acids
Res., 41, 77–83.
13. Zhang,B., Kirov,S., and Snoddy,J. (2005) WebGestalt: an integrated system for exploring gene sets in various biological contexts. Nucleic Acids Res., 33, 741–748.
14. Pacak,C.A. and Cowan,D.B. (2014) Growth of bone marrow
and skeletal muscle side population stem cells in suspension culture. Methods Mol. Biol., 1210, 51–61.
15. Hadzhiev,Y., Lele,Z., Schindler,S. et al. (2007) Hedgehog signaling patterns the outgrowth of unpaired skeletal appendages in
zebrafish. BMC Dev. Biol., 7, 75.
16. Rodriguez,L.J., Tomas,A.R., Johnson,A. et al. (2013) Recent advances in the study of limb development: the emergence and
function of the apical ectodermal ridge. J. Stem Cells, 8, 79–98.
17. Rahman,M.S., Akhtar,N., Jamil,H.M. et al. (2015) TGF-b/BMP
signaling and other molecular events: regulation of osteoblastogenesis and bone formation. Bone Res., 3, 15005.
18. Pan,A., Chang,L., Nguyen,A. et al. (2013) A review of hedgehog
signaling in cranial bone development. Front. Physiol., 4, 61.
19. Rudnicki,M.A. and Williams,B.O. (2015) Wnt signaling in bone
and muscle. Bone, 80, 60–66.
20. Zanotti,S. and Canalis,E. (2013) Notch signaling in skeletal
health and disease. Eur. J. Endocrinol., 168, 95–103.

Database, Vol. 2016, Article ID baw127

SPECIAL SECTION ON RECENT ADVANCES IN SOCIALLY-AWARE MOBILE NETWORKING
Received August 31, 2016, accepted September 11, 2016, date of publication September 21, 2016,
date of current version October 31, 2016.
Digital Object Identifier 10.1109/ACCESS.2016.2611863

Social-Aware Data Collection Scheme Through
Opportunistic Communication in Vehicular
Mobile Networks
ZHIPENG TANG1 , ANFENG LIU1 , AND CHANGQIN HUANG2

1 School
2 School

of Information Science and Engineering, Central South University, Changsha 410083, China
of Information Technology in Education, South China Normal University, Guangzhou 510631, China

Corresponding author: A. F. Liu (afengliu@mail.csu.edu.cn)
This work was supported in part by the National Natural Science Foundation of China under Grant 61379110, Grant 61073104,
Grant 61370229, and Grant 61370178, in part by the National Basic Research Program of China (973 Program) under
Grant 2014CB046305, in part by the National Key Technology R&D Program of China under Grant 2014BAH28F02,
in part by the S &T Projects of Guangdong Province under Grant 2014B010103004, Grant 2014B010117007,
Grant 2015A030401087, Grant 2015B010110002, and Grant 2016B010109008, and in part by GDUPS (2015).

ABSTRACT To enable the intelligent management of Smart City and improve overall social welfare, it is
desirable for the status of infrastructures detected and reported by intelligent devices embedded in them to
be forwarded to the data centers. Using ‘‘SCmules’’ such as taxis, to opportunistically communicate with
intelligent devices and collect data from the sparse networks formed by them in the process of moving
is an economical and effective way to achieve this goal. In this paper, the social welfare data collection
paradigm SWDCP-SCmules data collection framework is proposed to collect data generated by intelligent
devices and forward them to data centers, in which ‘‘SCmules’’ are data transmitters picking up data from
nearby intelligent devices and then store-carry-forwarding them to nearby data centers via short-range
wireless connections in the process of moving. Because of the storage limitations, ‘‘SCmules’’ need to weigh
the value of data and select some less valuable data to discard when necessary. To quantify the value of data
and find a well-performed selection strategy, the concept of priority is introduced to the SWDCP-SCmules
scheme, and then, the simulated annealing for priority assignment SA-PA algorithm is proposed to guide
the priority assignment. The SA-PA algorithm is a universal algorithm that can improve the performance of
SWDCP-SCmules scheme by finding better priority assignment with respect to various optimization targets,
such as maximizing collection rate or minimizing redundancy rate, in which priority assignment problem is
converted into an optimization problem and simulated annealing is used to optimize the priority assignment.
From the perspective of machine learning, the process of optimization is equal to automatically learn socialaware patterns from past GPS trajectory data. Experiments based on real GPS trajectory data of taxis in
Beijing are conducted to show the effectiveness and efficiency of SWDCP-SCmules scheme and
SA-PA algorithm.
INDEX TERMS Vehicular mobile networks, social welfare, data collection, opportunistic communication,
oblivious data mules, simulated annealing, machine learning.

I. INTRODUCTION

Internet of Things (IoT) is experiencing extremely rapid
development. There are several phenomena which can prove
this trends: Firstly, the amount of data generated by IoT has
been growing exponentially since past few years [1]–[9].
A Cisco’s report reveals that the overall data traffic of
IoT in 2014 has grown to 69 percent and is approximately 30 times the size of the entire global Internet
6480

in 2000 [3], [10]. Secondly, the number of connected devices
have already exceeded the total number of people living on
Earth since 2011. Connected devices have reached 9 billion
and are expected to reach 24 billion by 2020 [11], [12].
Thirdly, in 2012, application systems based on IoT has contributed $4.8 trillion to revenue of international corporations
and the number is still soaring since then, according to
another report from Cisco [10], [13].

2169-3536 
 2016 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

Due to the rapid development of IoT, the intelligent management of city becomes possible, which enables us to
achieve the dream of Smart City.
In the process of constructing Smart City, sensors and
actuators will definitely find their place because of their low
costs and energy consumption [14]–[17]. Intelligent devices,
which are equipped with various sensors and actuators, can
be embedded into infrastructures of Smart City to enhance
their productivity and functionality, such as detecting and
reporting the status of infrastructures [14]. For example:
garbage cans with intelligent devices can monitor their trash
level, if they are filled up with trashes, the intelligent devices
will detect it and send request for cleaning; Street lights
with intelligent devices can detect their status, such as the
power supply and the condition of bulbs, and send request
for maintaining. More applications can be found in the fields
of electricity facility, communication system, contamination
monitoring, sanitary engineering, etc [18].
Although the form of applications is distinct, they all share
a common abstract paradigm: detecting status of infrastructures and forwarding it to related municipal departments,
which will then take measures to maintain the infrastructures. This paradigm realizes the intelligent management
of city infrastructures, which will make a more comfortable city environment and improve social welfare significantly [17], [18]. In this paper, this paradigm is called Social
Welfare Data Collection Paradigm (SWDCP Paradigm).
To implement SWDCP paradigm, how to establish network
connections between the large amount of intelligent devices
distributed in the whole city and municipal departments is a
core problem [17].
In fact, the way to establish such kind of network connections really depends on the requirements of communication:
For real time communications, there are two feasible
approaches: the first one is to equip each intelligent device
with a SIM card [17], the second one is to build permanent
base stations to relay data/messages generated by intelligent
devices located in their coverage area [19]. However, they
both have their own shortcomings. For the first approach, the
high costs and energy consumption sharply reduce the advantage of this approach [20], [21]. Besides that, this approach
potentially increases the number of devices occupying radio
frequency (RF): concretely speaking, RF is a kind of valuable
limited communication resource. Devices occupying RF have
already reached 9 billion and are expected to reach 24 billion
by 2020 [22]. The first approach will significantly increase
such devices and thereby make the problem of the scarcity of
RF much more serious [1], [3], [8]. For the second approach,
as new infrastructures are built and old infrastructures are
removed in the process of city construction, the distribution
of intelligent devices in the city also constantly change, which
makes building permanent base stations covering all intelligent devices impossible [17].
However, for most applications in SWDCP paradigm,
real time communication is not necessary, That is to say,
the data/messages generated by devices are not necessary
VOLUME 4, 2016

to be report to municipal departments immediately, some
delay is acceptable. For example, when the bulb of a street
light doesn’t work, the intelligent device embedded in this
street light doesn’t need to report this status to maintenance
department immediately since one street light’s malfunction
doesn’t influence the overall illumination of the city, several hours’ data transmission delay is acceptable. Similarly,
when a garbage can is filled with trashes, the intelligent
device embedded in this garbage can doesn’t need to report
this status to sanitary department immediately since there
are other available garbage cans nearby, several days’ data
transmission delay is acceptable [17]. Based on this feature,
Bonola et al. think this type of network connections
shares a lot of common features with Delay Tolerant
Network (DTN) [17], [23].
Bonola et al. further proposed a data collection scheme
for this kind of network connection based on Oblivious Data
Mules [17]. There are three entities in their scheme: oblivious
data mules, intelligent devices and data centers. Oblivious
data mules (hereinafter referred to as mules), which are
inspired by the work of Shah et al. [19], are mobile IoT nodes
moving in Smart City and can opportunistically communicate
with intelligent devices via short-range wireless communication to pick up data/messages and buffer them in their
own storage. When mules move to the neighborhood of data
centers, the buffered data/messages will be forwarded to data
centers via short-range wireless communication. Intelligent
devices are incorporated into the infrastructures of Smart
City, they are capable of detecting the status of infrastructures and sending the data/messages to mules via short-range
wireless communication. Data centers are special computing and processing nodes distributed in fixed locations of
Smart City, they can connect with cloud tier via high-speed
network to forward data/messages, their major responsibility
is receiving data/messages buffered by nearby mules and
forwarding them to cloud tier, which is accessible to corresponding municipal departments. Once municipal departments receive these data/messages, they will take measures
to maintain related infrastructures. Most data centers are
deployed in downtown area of Smart City to boost collection
efficiency.
Here is a concrete instance: many objects can be regarded
as mules in Smart City, such as taxis, buses and pedestrians
holding portable communication devices [24], [25]. Taxis are
the most ideal among them since they have several great
features: they move independently according to customers’
demands and can cover an extensive area of Smart City with
somewhat random paths (with respect to buses that move
only on main streets) and work 24/7. Taxis equipped with
transceivers, which are capable of communicating via shortrange wireless networks and exchanging data with neighboring intelligent devices and data centers [19], can be regarded
as mobile IoT nodes, i.e. mules. The transceivers are incorporated into the chips of taxis and can automatically incidentally
pick up data/messages sent by nearby intelligent devices
when taxis are moving in Smart City to send passengers to
6481

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

their destinations. The data/messages incidentally collected
in the process of motion are buffered and then dumped to
data centers when taxis are passing near them. Data centers
then simply filter out duplicated data/messages and forward
the remaining information to cloud tier via the high-speed
network connection. The relative municipal departments can
access these data/messages from cloud tier and then take
measures to maintain corresponding infrastructures. Note that
the major task of taxis is serving passengers, the process of
data/messages collection is incidental, which is automatically
manipulated by transceivers, rather than drivers, that’s why
we call data mules oblivious.
The scheme proposed by Bonola et al. is cost-saving
and energy-efficient [17]. However, this model can still be
refined because it assumes the storage of mules is infinite,
that is to say, mules can store all data/messages picked up
along the course of moving and don’t need to worry about
the situation that their storage is full. Conversely, in reality, infinite storage is of impossibility and mules can only
store limited data/messages. We call this refined model of
mules as Storage-constrained oblivious data mules (hereinafter referred to as SCmules). Due to the storage limitation,
SCmules have to discard some less important data/messages
stored in buffer to make some free space in the situation that
their storage is full and new more valuable data/messages are
picked up. This situation occurs frequently because of the
large number of intelligent devices installed in city. The selection strategy of how to choose more valuable data/messages
to store and less valuable data/messages to discard plays a
vital role in improving data/messages collection efficiency
and overall social welfare of Smart City [26]. To prove the
vitality of the selection strategy, we continue to use the aforementioned example to illustrate it: If the selection strategy
is simply discarding new picked-up data/messages when the
storage is full, a bad phenomenon will occur: data centers will
receive a lot of duplications of data/messages from the downtown areas of a city, i.e. hotspot areas, but few data/messages
from other areas, especially the remote areas, are collected.
This is because the hotspot areas are always the transportation
hubs of the city. The frequency that taxis pass by there is
much higher than the frequency of other areas. Therefore, the
probability that data/messages from hotspot areas are picked
up by taxis is much higher than the probability of other areas,
i.e. data/messages from hotspot areas have higher probability
of occupying the storage and then being forwarded to data
centers. Conversely, when taxis pass through remote area, it is
likely to have no more storage to pick up new data/messages
from there. So this example illustrates a bad selection strategy
will lead to low data/messages collection efficiency and poor
social welfare.
Now that the selection strategy is of vitality, how can we
find an effective and efficient selection strategy? To answer
this question, two tasks have to be finished: the first one is
to introduce a universal way to formally describe selection
strategy, the second one is to find a well-performed selection
strategy.
6482

A natural way to formally describe selection strategy is to
introduce the concept of priority: We assign each intelligent
device with a unique priority. Every registered SCmule owns
a priority table, which records the priorities of all intelligent devices. The priority table is used to guide how to
weigh the importance of data/messages. In detail, when the
storage of a SCmule is not full, this SCmule will greedily
buffer data/messages as much as possible; but when the
storage is full, this SCmule will use the priority table to
weigh the priority of new picked-up data/messages with the
priorities of those stored in buffer. If the new picked-up
one’s priority is less than the priorities of all data/messages
buffered, it will be simply discarded; otherwise, it will replace
the buffered data/message with the least priority. The main
principle of this process is to maximize the sum of priorities of data/messages buffered, which is referred as Greedy
Selection Principle.
Obviously, the priority model is a natural and effective way
to formally describe the selection strategy. In the example of
illustrating the vitality of selection strategy, the poor selection
strategy we used can be regarded as assigning each intelligent
devices with equal priorities.
By combing SCmules, the model of priority and greedy
selection principle, we propose a new concrete implementation of SWDCP paradigm for DTN, which is called
Social Welfare Data Collection Paradigm based on StorageConstrained Oblivious Data Mules (SWDCP-SCmules
scheme).
To implement SWDCP-SCmules, we still have to address
the second task which is how to assign priority to make
an effective and efficient selection strategy. We refer to this
problem as Priority Assignment Problem.
However, the priority assignment problem is really a challenge since there are three potential difficulties:
(1) The meaning of effectiveness and efficiency is ambiguous, different scenarios have different interpretations and correspond to different priority assignment
methods. So it is troublesome to design new algorithms for every concrete interpretation of effectiveness
and efficiency. For example, as will be described in
section 3.2, in most cases, the definition of effectiveness and efficiency corresponds to maximizing collection rate and minimizing redundancy rate, but there are
always some new demands, such as reducing collection delay of some important infrastructures, emerging
with the advent of new applications. The algorithm
designed for original demands fails to optimize these
new demands and require a complete redesign. It is
too troublesome to design new algorithm for every new
demand.
(2) Even if we have an accurate definition of effectiveness
and efficiency, i.e. optimization target, it is still too hard
to design an appropriate algorithm that can optimize
the performance of priority assignment according to
this definition, not to mention a universal algorithm
that can solve all unpredictable optimization targets.
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

For example, if the optimization target is to maximize
collection rate and minimize redundancy rate, finding an effective pattern that can support the design
of algorithm needs vast amounts of manual observation and thinking. In fact, it is impossible for human
intelligence to perceive all the potential patterns lying
in essence. Even if we have found an important pattern, e.g. decreasing the priorities of intelligent devices
near data centers and increasing the priorities of those
far from data centers can increase collection rate and
decrease redundancy rate, you will still probably ignore
the fact that this pattern doesn’t work when the intelligent devices locate in the end point of one-way streets
(In section 5.1, we will give a detailed possible explanation of how this pattern works and why it fails in that
case).
(3) The most fatal problem is that patterns are not static.
As time goes on, new patterns emerge, old patterns
disappear and some patterns change periodically. For
example, in hot summer, district near natatoriums will
become hotspot areas and share the common features
of downtown areas, but in other seasons, these districts
will be ordinary areas.
According to the analysis above, we can infer that it is
difficult to solve priority assignment problem using traditional approach, which is finding effective patterns and then
using these patterns to design algorithms. However, we can
consider to use the idea from machine learning, that is, to
use learning algorithm to automatically learn social-aware
patterns or other information from data and then utilize them
to address this challenging problem.
The basis of why learning algorithm works is that there
are a lot of social-aware patterns, which are patterns that can
reflect the social preferences of citizens in Smart City, lying
in the GPS trajectory data of SCmules. Concretely speaking,
the GPS trajectory data of SCmules can reflect the overall
social preferences of SCmules (i.e. the social preferences of
citizens in Smart City). To prove this statement, we briefly
illustrate an instance in this section (In section 5.1, we will
explore the meaning of social-awareness more deeply): the
GPS trajectory data of taxis, one of the typical examples
of SCmules, can implicitly show the social preferences of
citizens in the Smart City, such as the hotspot areas of Smart
City in certain time interval, the preferred destinations of
passengers in certain time interval, the preferred routes that
drivers take in certain time interval and many other social
labels. Therefore, we can use learning algorithm to automatically mine for these potential social-aware patterns lying in
the data without any manual teaching and use them to make
a better priority assignment to improve the performance of
SWDCP-SCmules scheme.
In this paper, we try to convert the priority assignment
problem to an optimization problem based on the idea from
machine learning and then use simulated annealing metaheuristic to design a universal algorithm that can automatically find a priority assignment of good quality with respect
VOLUME 4, 2016

to various optimization targets based on the social-aware
patterns lying in the GPS trajectory data of SCmules in the
past time. Concretely speaking, first, we need to specify the
meaning of effectiveness and efficiency, that is to say, we
need to use a performance measure to quantify the meaning
of effectiveness and efficiency. The performance measure
we used is the optimization target function J of priority
E then, due to the future shares similar socialassignment P;
aware patterns with the past, we can use the past GPS traE that
jectory data of SCmules to train a priority assignment P
can optimize the optimization target function J and predict
E will also achieve good performance in the future.
that P
Because of the complexity of the computation of J with
E based on the past GPS trajectory data, we apply
respect to P
simulated annealing metaheuristic to search for appropriate
E in search space that can optimize J. We refer to this method
P
of solving priority assignment problem as Simulated Annealing for Priority Assignment Algorithm (SA-PA algorithm).
Based on real GPS trajectory dataset of taxis during the
period of Feb. 2 to Feb. 8, 2008 in Beijing, the effectiveness
of SA-PA algorithm can be proved in section 6.
The main contributions of this paper can be described as
three points:
(1) We propose a Social Welfare Data Collection
Paradigm based on Storage-Constrained Oblivious
Data Mules (SWDCP-SCmules scheme) in which
storage-constrained oblivious data mules can incidentally pick up data/messages generated by intelligent devices embedded in infrastructures of Smart
City in the process of motion and store-carry-forward
them to data centers, which will then notify related
department to maintain corresponding infrastructures.
Unlike the previous research, in SWDCP-SCmules
scheme, SCmules are storage-constrained, which
is much more real than the previous model.
SWDCP-SCmules scheme enables the intelligent management of city to be possible and leads to the improvement of overall social welfare.
(2) Because of the storage limitation, SCmules have to
weigh the importance of data/messages and discard
some less important data/messages stored in the situation that their storage is full and new valuable
data/messages are picked up. Therefore, the selection
strategy is extremely important in SWDCP-SCmules.
In this paper, the concept of priority is introduced
to model the selection strategy to the priority assignment. Every intelligent device is assigned with a
unique priority, which forms the priority table. Based
on priority table, SCmules use greedy selection principle, which is maximizing the sum of priority of
data/messages buffered, to guide the storing of more
important data/messages and the discarding of less
important data/messages.
(3) To find an optimized priority assignment, Simulated
Annealing
for
Priority
Assignment
6483

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

Algorithm (SA-PA algorithm) is proposed to learn
social-aware patterns from GPS trajectory data of
SCmules. The SA-PA algorithm converts the priority
assignment problem to an optimization problem based
on the idea from machine learning and makes a simulated annealing-based universal algorithm that can
automatically find priority assignments of good quality
with respect to various optimization targets based on
the GPS trajectory data of SCmules in the past time.
Below is the organization of the rest of this paper:
In section 2, related works are reviewed. In section 3, the
system model and problem statement of SWDCP-SCmules
scheme and SA-PA algorithm are described. In section 4,
SWDCP-SCmules scheme is proposed in detail. In section 5,
SA-PA algorithm is described in detail. In section 6,
experimental results are given to show the performance
of SWDCP-SCmules scheme and SA-PA algorithm.
In section 7, the conclusion of this paper is described.
II. RELATED WORK
A. DATA COLLECTION FRAMEWORK OF IoT

We first observe the structure of SWDCP-SCmules scheme
from the perspective of data collection tasks. As illustrated in
Figure 1, IoT data collection framework can be divided into
2 tiers: the data collection tier (DCT) and the cloud tier (CT).
The main function of DCT, which is the base tier of IoT data
collection framework, is sensing and collecting data [27]. The
main function of CT, which is the core tier of IoT, is refining
data collected by DCT and convert them to the elements of
service [10].

FIGURE 1. The data collection framework of IoT.

For example, the network constructed in SWDCPSCmules scheme can be seen as a kind of vehicular mobile
networks, a concrete form of the IoT data collection framework, in a narrow sense. In DCT, sensors are incorporated
into infrastructures of the city [28] to sense the status
6484

of them. Vehicles opportunistically communicate with nearby
sensors to pick up data/messages generated by the nearby
sensors and then store-carry-forward to nearby data centers
when passing the nearby data centers [17]. Data centers then
send the received data/messages from DCT to CT via highspeed network connections to notify related departments to
maintain corresponding infrastructures.
In this paper, we focus on the DCT part of the IoT data
collection framework. According to the difference of applications, DCT can be categorized as different networks, such
as wireless sensor networks [5], [6], [14], [15], [26], [28],
participatory sensing networks [18], [26], [29], crowd sensing
networks [29], vehicular mobile networks [17] and delay
tolerant networks [23]. The researches on wireless sensor
networks (WSNs) start comparatively earlier [30]. In WSNs,
a large amount of wireless sensor nodes are deployed in
geographic position to construct a fine-grained network connections. WSNs can be used to monitor certain objects or
indexes in geographic areas, such as local temperatures [31].
Unlike WSNs, the main participant objects in participatory
sensing networks and crowd sensing networks are people
holding mobile smart phones which are equipped with sensing devices. As people move around city, those mobile smart
phones automatically sense and collect data using sensing
devices and communicate via cell networks [18], [26], [29].
The two main features of participatory sensing networks
and crowd sensing networks are that the number of sensing
devices is large and these sensing devices can move around to
sense and collect data. Vehicular mobile networks are another
form of networks in DCT. The basis of its network connection
is the motion of vehicles and their opportunistic communication [17]. In delay tolerant networks (DTNs), all sensors
are moving around and opportunistically communicate with
each other to exchange data [23]. The major feature of DTNS
is that transmission delay is not an important measure of
network performance.
In this paper, the networks constructed by the
SWDCP-SCmules scheme can be regarded as a synthesis of
these five networks mentioned above [17]. In the synthesized
networks, the geographic locations of intelligent devices,
which are responsible for sensing and collecting data, are
fixed, which is similar to sensors in WSNs. But SCmules,
which are responsible for collecting and transferring data,
are moving around the city to pick up data and store-carryforward to data centers, which is same as vehicular mobile
networks. The data collection process of the synthesized
networks relies on the extensive participation of SCmules,
which is similar to participatory sensing networks and crowd
sensing networks. The transmission delay of data collection
is not that important in the synthesized networks, which is
similar to DTNs.
In the rest of this subsection, we will introduce these five
networks in detail, i.e. wireless sensor networks, participatory sensing networks and crowd sensing networks, vehicular
mobile networks, delay tolerant networks and their relations
to the SWDCP-SCmules scheme respectively.
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

1) WIRELESS SENSOR NETWORKS

In early days, WSNs are put forward to monitor certain
objects or indexes in local areas by deploying a large
amount of wireless sensor nodes to form fine-grained
networks [2], [5], [6], [9], [16], [21], [31]. This is somewhat
similar to SWDCP-SCmules scheme’s intelligent devices
embedded in infrastructures. But a major difference between
them is the deployment of intelligent devices is planless,
sporadic and geographically sparse, which means they will
not form self-organized networks like WSNs. Therefore, the
data collection process relies on the motion and store-carryforward mechanism of SCmules, rather than the fine-grained
networks formed by wireless sensors.
There are two major elements in WSNs: sensors and sinks.
Sensors are ordinary wireless sensor nodes which is responsible for sensing data. Besides sensors, there are some special
wireless sensor nodes called sinks deployed in WSNs, which
can be regarded as the destination of data collection.
Routing algorithm is the algorithm that routes data from
sensors to sinks [2], [3], [31]. In WSNs, the key problem
of data collection is to find an efficient routing algorithm,
that is to say, to find a routing path from sensors to sinks
with high quality of service (QoS). Some typical measures of
QoS are energy consumption, network lifetime, transmission
delay and packet loss rate. Usually, there exist some tradeoffs
between different measures of QoS, for example, reducing
transmission delay and packet loss rate will lead to higher
energy consumption and shorter network lifetime [2], [3], [9],
[26], [31]. Unlike WSNs, routing problem is not a problem
in SWDCP-SCmules scheme, because the intelligent devices
cannot form fine-grained networks and data transmission
relies on the opportunistic communications among SCmules,
intelligent devices and data centers.
Another relevant networks is wireless sensor actor
networks (WSANs). WSANs consist of a large amount of
inexpensive sensor nodes and a small amount of expensive
powerful mobile actor nodes (e.g. robots) [32]. After receiving messages (e.g. fire alarm) sent by a certain sensor node,
actor nodes (e.g. robots) will move to the position of this
sensor node and deal with the emergency reported in the
messages (e.g. putting out the fire). In WSANs, actor nodes
move constantly, therefore how to effectively and reliably
route the messages generated by sensor nodes to moving actor
nodes becomes the central problem in WSANs.
In some forms of WSNs, mule nodes are adopted to relay
data/messages [24], [25]. In most cases, there are some
hotspot areas, in which energy consumption is much higher
than other areas, in WSNs. Mule nodes relay data between
sinks and hotspot areas to reduce energy consumption in
hotspot areas, thereby increasing network lifetime.
2) PARTICIPATORY SENSING NETWORKS &
CROWD SENSING NETWORKS

With the rapid development of portable devices, participatory sensing networks and crowd sensing networks
have great potential in collecting and sharing sensing
VOLUME 4, 2016

information through smartphones and other portable
devices [18], [26], [29].
The main feature of participatory sensing networks and
crowd sensing networks is that a large amount of sensing
devices participate in monitoring and sensing the same type
of data to accomplish the same task. For example, when
the task is monitoring PM 2.5 of a city, it is too costly
to establish a lot of PM 2.5 monitor stations distributed in
the whole city. In addition, due to the limited number of
PM 2.5 monitors, the accuracy of results is also limited. But
when using participatory sensing networks and crowd sensing
networks, citizens’ mobile phones can sense and report their
location’s PM 2.5 spontaneously, which significantly reduce
the costs and increase the accuracy of results [18], [29].
In participatory sensing networks and crowd sensing networks, portable devices can directly connect to Internet to
share data, therefore the focus of participatory sensing networks and crowd sensing networks is not how to forward the
data collected by portable devices to centralized station, but
how to motivate users to participate in the process of collecting sensing data. A practical adaptive incentive strategy to
motivate the enthusiasm of users is to offer rewards to data
reporters [18]. In this strategy, data are linked to rewards
(e.g. money). If we want to collect more data of a certain
part (e.g. the PM 2.5 of a certain area), we can increase the
rewards to reporters who report this part of data. Conversely,
if we want to collect less data of another certain part, we
can decrease the rewards to reporters who reports this part
of data. The adaptive adjustment of rewards will finally level
off to market equilibrium, which will increase the overall
efficiency.
In the SWDCP-SCmules scheme, we don’t adopt this
adaptive incentive strategy. The strategy we adopt is that
fixed rewards are given to users who are willing to install
transceivers and participate in the collection of sensing data,
i.e. SCmules. In this strategy, the existence of transceivers is
nearly transparent to users. Transceivers automatically sense
and collect data according to unified arrangement from data
collector. Users don’t need to worry about the running of
transceivers, to get rewards, what they need to do is to carry
the transceivers when they are moving around the city. This
unified strategy can increase the overall efficiency and avoid
Prisoner’s dilemma.
There is a problem in the participatory sensing networks
and crowd sensing networks. That is, there are some hotspot
areas in the networks where more portable devices locate.
Data from hotspot areas is much easier to be collected than
data from other areas. If all portable devices report the same
amount of data, the amount of data we collected from hotspot
areas will be excessive, but the amount of data we collected
from other areas will be not enough [26]. SWDCP-SCmules
scheme also have similar problem. Tham et al. proposed
a data collection scheme based on Quality of Contribution
which can balance the collection of sensing data in its covered area [29]. Besides this, Quality of Information is also
proposed to address this problem [33].
6485

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

3) VEHICULAR MOBILE NETWORKS

In vehicular mobile networks, vehicles spontaneously
connect with each other via wireless networks to exchange
data [34]. A wide range of applications can be realized by
vehicular mobile networks, such as traffic jam reporting [35].
At present, the research of vehicular mobile networks focus
on the communication of vehicles to vehicles (V2V) and
vehicles to devices (V2D).
SWDCP-SCmules scheme can be implemented by vehicular mobile networks. Taxis are typical SCmules that can be
used in this scheme. However, unlike traditional vehicular
mobile networks, the main communication objects of this
scheme are intelligent devices and data centers.
4) DELAY TOLERANT NETWORKS

SWDCP-SCmules scheme can be regarded as a special case
of DTNs from the perspective of transmission delay requirements [17]. In traditional DTN, both the data generator and
receiver are mobile, when two mobile nodes encounter with
each other, they can exchange and relay their data to achieve
the goal of data diffusion [23]. The major feature of DTNs is
that data’s transmission delay and the aim of data diffusion
has no strict requirement. In most cases, the delay of data
transmission is not an important measure of network performance and the aim of data transmission is just diffusing data
as large as possible.
However, in the SWDCP-SCmules scheme, the positions
of data generators (i.e. intelligent devices) are fixed, the
positions of data transmitters (i.e. SCmules) are mobile and
the positions of data receivers (i.e. data centers) are fixed.
The aim of data transmission is to obliviously transmit data
from data generators to data receivers by data transmitters.
To achieve this aim, we don’t put any management strategy
on data transmitters, the process of data collection is totally
incidental and oblivious.
B. MACHINE LEARNING

Tom Mitchell gives a received definition of machine learning in 1998: A computer program is said to learn from
experience E with respect to some task T and some performance measure P, if its performance on T , as measured
by P, improves with experience E [36]. As will be illustrated
in section 5.3, SA-PA algorithm can be categorized into
machine learning program according to this definition.
One typical work flow of machine learning program is
to find an appropriate hypothesis from hypothesis set and
use learning algorithm to learn the parameters of hypothesis
based on training set and generate the final hypothesis [37].
The SA-PA algorithm conforms to this work flow according
to the illustration of section 5.3.
C. SIMULATED ANNEALING METAHEURISTIC

Optimization problems are the problems of finding the best
solution from all feasible solutions [38], the priority assignment problem is a typical optimization problem.
6486

Metaheuristics are one of the practical techniques to solve
optimization problems. Typical metaheuristics include simulated annealing, tabu search, ant colony optimization and so
on. Although the quality of solution provided by metaheuristics have no guarantee, for most computationally complicated
optimization problems, metaheuristics can lead to better solution than other algorithm design method when given limited
time [38]. So metaheuristics are perfect candidates for solving
the priority assignment problem due to the computationally
complication.
Simulated annealing is a metaheuristic proposed by
Suman and Kumar [39]. It can be regarded as an improvement of local search algorithm. The aim of local search
algorithm, such as hill climbing method [39], is to find local
optimums by moving from solution to solution in search
space using local changes. Simulated annealing introduces
acceptance probability into local search algorithm to allow
inferior moves to be accepted. A move will be accepted in
the following acceptance probability X:
(
1
1 − e T0 , 1 < 0
X=
1,
1≥0
where 1 is the performance of the move, or more simply
speaking, the difference between the height after taking the
move and the height before taking the move, T0 is the temperature parameter, which controls the probability of taking
an inferior move [39].
To apply simulated annealing metaheuristic to solve optimization problems, three elements need to be specified: configuration, evaluation function and neighborhood function.
Configuration is the representation of feasible solution, all
configurations form the search space. Evaluation function is
the measure of optimization target. Neighborhood function
defines the way moving from solution to solution.
III. THE SYSTEM MODEL AND PROBLEM STATEMENT
A. THE SYSTEM MODEL

In SWDCP-SCmules scheme, intelligent devices are
embedded in the infrastructures of city. Assume there
are m intelligent devices in total, which consist of set
S = {S1 , S 2 , . . . , Sm }. These intelligent devices are selfpowered, which is possible nowadays due to the development of energy harvesting technology [40]. The intelligent
devices have two main functions: detecting the status of
infrastructures and sending data/messages via short-range
wireless communications (due to the limitation of energy,
the communication range is limited). For example, intelligent
devices deployed in garbage cans can detect the level of
trashes periodically and, when SCmules pass by them, they
can forward the data/messages of the trash level to SCmules
via wireless communications.
Every intelligent device is assigned with a unique priority,
which is a measure of the importance of the data/messages
generated by this intelligent device. For intelligent device Si ,
its corresponding priority is Pi . Suppose the set of priority is
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

P = {P1 , P2 , . . . , Pm } and its corresponding m-dimensional
E = [P1 , P2 , . . . , Pm ]. There is a one-to-one
vector is P
E We will not explicitly
correspondence between P and P.
E in following context since both P and
distinguish P and P
E can be used to express priority assignments, i.e. priority
P
table. Obviously, from the perspective of linear algebra, all
possible priority assignments form a subset of m-dimensional
space.
Besides intelligent devices, there are k data centers distributed in fixed locations of Smart City, especially the downtown areas. The i-th data center is hi , all data centers form
the set H = {h1 , h2 , . . . , hk }. As described in section 1,
data centers are special computing and processing nodes
which can connect with cloud tier via high-speed networks
to forward data/messages. Their major responsibilities are
receiving data/messages buffered by nearby SCmules and
then forwarding them to cloud tier, which is accessible to corresponding municipal departments. Loosely speaking, data
centers can be viewed as the destination of transmission:
data/messages generated by intelligent devices are eventually
forwarded to data centers via SCmules. Data centers may
receive a lot of duplicated information since several SCmules
may buffer the same data/messages when passing by the same
intelligent devices in the same time period. This phenomenon
is unwanted because it wastes energy and occupies precious
limited storage of SCmules.
There are n total SCmules which consist of set V =
{V1 , V 2 , . . . , V n }. SCmules, which can be regarded as
mobile IoT nodes, move independently in Smart City. They
are equipped with short-range transceivers. In the process
of motion, when SCmules pass near intelligent devices,
transceivers can automatically detect the active intelligent
devices within their communication range, pick up the
data/messages which these active intelligent devices are sending and then buffer them into their storage. Similarly, when
SCmules pass near data centers, the transceiver can detect the
existence of these data centers within their communication
range, dump all the data/messages buffered in their storage
to data centers and then clear their storage. To simplify the
model, we assume the data transmission processes between
intelligent devices and SCmules and the data transmission
process between data centers and SCmules are instant. This
is feasible in most cases due to the rapid development of
new radio technologies such as Ultra-Wideband (UWB) [41],
even if the worst situation occurs, such as packet loss or
unfinished packet transmission, SCmules can simply discard
these damaged packets since the oblivious and incidental data
collection feature of SWDCP-SCmules scheme.
The storages of SCmules are limited. Suppose SCmule
Vi can store Qi data/messages at most and all Qi form set
Q = {Q1 , Q2 , . . . , Qm }. When the storage is full and new
valuable data/messages are picked up. SCmules will use
greedy selection principle, which is described in section 1 and
section 4.4, to maximize the sum of priorities of
data/messages buffered.
VOLUME 4, 2016

We refer to the quintuple C = (S, P, H, V, Q) as a configuration, which is a concrete deployment of SWDCP-SCmules
scheme in Smart City.
B. PROBLEM STATEMENTS

Simply speaking, the biggest problem of SWDCP-SCmules
scheme is how to make it effective and efficient. For a certain
configuration C = (S, P, H, V, Q) in Smart City, the only
element that can be manually changed to improve performance is priority assignment P. After all, intelligent devices,
data centers and transceivers are deployed in advance and
cannot be changed arbitrarily. That is to say, the priority
assignment P is the only factor that can be easily altered
to improve the performance of SWDCP-SCmules scheme as
illustrated in section 1.
However, the meaning of effectiveness and efficiency is
ambiguous since different scenarios have their own different
concrete meaning. To specify the meaning of effectiveness
and efficiency, we introduce a universal optimization target
function J to quantify them. J can be thought as a quantified performance measure of SWDCP-SCmules scheme.
The larger J is, the more effective and efficient SWDCPSCmules scheme is. We convert the problem of making
SWDCP-SCmules scheme effective and efficient to an optimization problem of J with respect to C (In fact, C is dominated by P as other elements cannot be controlled easily).
For example, in most cases, the interpretation of effectiveness and efficiency corresponds to high collection rate and
low redundancy rate. Collection rate, redundancy rate and
their synthesized rate are formally defined as follow:
1) COLLECTION RATE CC
T

Collection rate is the number of distinct data/messages
(excluding duplicated data/messages) collected by data centers in a given time interval. It is obvious that the higher
collection rate is, the less data/messages loss in the process
of data transmission of SCmules. Therefore, we should maximize collection rate.
For a given time interval T = [tx , ty ) where tx and ty
are timestamps, assume the total number of data/messages
generated by all intelligent devices is GC
T , the total number
C
of distinct data/messages collected by data centers is kT , we
C
can define the collection rate CT as
C

CC
T

=

kT

GC
T

.

2) REDUNDANCY RATE RC
T

Redundancy rate is the ratio of the number of duplicated
data/messages collected by data centers to the number of
data/messages collected by data centers in a given time interval. It is obvious that the more duplicated data/messages
data centers collect, the lower storage utilization efficiency
SCmules are and the more energy the whole system wastes.
Therefore, we should minimize redundancy rate.
6487

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

For a given time interval T = [tx , ty ) where tx and ty
are timestamps, the total number of data/messages collected by data centers is KC
T , the total number of duplicated
C
data/messages collected by data centers is KC
T − kT , we can
define the redundancy rate RC
T as

TABLE 1. Main notations.

KC
T − kT

C

RC
T

=

KC
T

In the following context, we will also use effective collecC
tion rate RT as a substitution of RC
T , which is defined as
C

C
RT

=

1 − RC
T

=1−

KC
T − kT
KC
T

C

=

kT

KC
T

C

Obviously, the higher RC
T is, the lower RT is. Conversely,
C

the lower RC
T is, the higher RT is.
By the way, it is worth to mention that the way we define
redundancy rate is similar to the definition of repurchase rate
in the field of management, which proves the definition of
redundancy rate is reasonable.
Based on the definitions of collection rate and redundancy rate, we can define three distinct optimization target
functions:
(1) Collection rate optimization target function JC
JCTC = CC
T
The optimization target of JC is to maximize the overall
collection rate.
(2) Effective collection rate optimization target
function JR
JRC
T

=

1 − RC
T

=

C
RT

The optimization target of JR is to minimize redundancy rate or maximize effective collection rate.
(3) Synthesized optimization target function JS
We can synthesize the two optimization targets, i.e.
maximizing collection rate CC
T and minimizing redundancy
rate RC
,
to
form
a
synthesized
rate and make a concrete
T
synthesized optimization target function JS :


C
JSTC = λ1 CC
T + λ 2 1 − RT
where λ1 + λ2 = 1, λ1 ≥ 0 and λ2 ≥ 0. Maximizing JS
is equivalent to maximizing C and minimizing R in the same
time. The higher JS is, the higher C is and the lower R is.
JC , JR and JS will be used in the following context
as examples of optimization target functions to prove the
effectiveness and efficiency of SWDCP-SCmules scheme
and SA-PA algorithm.
In summary, we introduce a universal optimization target
function J to measure the performance of a certain configuration of SWDCP-SCmules scheme. In different applications,
the meaning of effectiveness and efficiency is different, we
can use different J to quantify them. Optimizing J with
respect to configuration C (In fact, C is dominated by P as
6488

other elements cannot be controlled easily) can acquire an
optimized configuration that significantly improve the performance of SWDCP-SCmules scheme. Therefore, the formal
problem we have to tackle with is
max J
C

We propose the SA-PA algorithm to solve this problem in
section 5.
IV. SWDCP-SCMULES SCHEME
A. OVERVIEW

To realize the intelligent management of city and improve the
overall social welfare, we can deploy intelligent devices to
infrastructures of city and make them monitor and report the
status of these infrastructures to corresponding departments,
of which responsibility is to maintain the infrastructures’
normal function. As illustrated in section 1, we refer to this
abstract paradigm as Social welfare data collection paradigm
(SWDCP paradigm). Many applications can be realized by
this paradigm, such as the intelligent monitoring of the conditions of street lights.
To give a concrete, feasible and economical scheme
to implement SWDCP paradigm, many related researches
have been done as introduced in section 1. In this
paper, we propose a Social Welfare Data Collection
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

Scheme based on Storage-Constrained Oblivious Data Mules
(SWDCP-SCmules scheme), which can be regarded as a
refined model proposed by Bonola et al. [17], to collect
data/messages distributed in the sparse network formed by
intelligent devices. The main feature of this scheme is that
oblivious data mules are storage-constrained, which is much
more realistic than previous researches. In the following context, we refer to this kind of oblivious data mules as SCmules.
Below we describe this scheme on the whole:
Intelligent devices are embedded into the infrastructures
of Smart City to detect their status. Once the status meets a
certain condition, the communication function of intelligent
devices will be activated and tries to report data/messages
to nearby SCmules via short-range wireless communication.
SCmules are mobile IoT nodes moving in Smart City, they
are equipped with transceivers which enable them to opportunistically communicate with nearby nodes. When SCmules
pass near active intelligent devices which are trying to report
data/messages, they will detect the existence of them and pick
up data/messages sent by them obliviously and incidentally.
As the storage is limited, when receiving new data/messages,
SCmules will determine whether the new data/messages will
be buffered or not according to the remaining storage space
they have and the priority of the intelligent devices generated these data/messages. The selection principle is to greedily maximize the sum of priorities of data/messages stored.
We refer to this principle as greedy selection principle.
Besides intelligent devices and SCmules, there are many data
centers deployed in fixed locations of Smart City, especially
downtown areas. Data centers can be viewed as the destination of data collection, they are special computing and processing nodes that can connect with cloud tier via high-speed
network to forward data/messages. When SCmules pass near
them, they will dump all data/messages buffered in their storage to them and then clear their buffer. Data centers received
these data/messages will forward them to cloud tier, which
is accessible to municipal departments. Once corresponding
municipal departments see these data/messages, they will
take measures to maintain the related infrastructures.
To illustrate the scheme in a concrete way, below
is an example of the application of monitoring the
health status of allée trees using SWDCP-SCmules scheme
(see Figure 2): Intelligent devices are deployed on allée trees
to monitor their health status, once these devices detect that
these allée trees are in unhealthy condition, such as water
shortage, pest threaten, the communication function of these
intelligent devices will be activated immediately and try to
report the messages via short-range wireless communication.
Taxis equipped with transceivers, which are typical types of
SCmules, move in Smart City to send passengers to destinations, when taxis pass near these active intelligent devices,
which are trying to report messages, transceivers will detect
the existence of them, pick up the messages obliviously
and incidentally and then buffer them according to greedy
selection principle. In the process of sending passengers to
destinations, taxis may also pass near some data centers,
VOLUME 4, 2016

FIGURE 2. A concrete example of SWDCP-SCmules scheme.

which are distributed in fixed locations of Smart City. When
a certain data center is nearby, transceivers will detect the
existence of it and dump all data/messages buffered in storage
to it. After receiving these messages, data centers will simply
process them, such as filter out duplicated messages and then
forward them to cloud tier via high-speed network connections, gardening department will receive this message from
cloud tier and then send staffs to water the allée trees.
In rest of this section, we will give a detailed introduction
of how intelligent devices, data centers and SCmules work in
section 4.2, 4.3 and 4.4, respectively.
B. INTELLIGENT DEVICES

Various intelligent devices are embedded in the infrastructures of Smart City to enhance their functionality and productivity. Most of them are self-powered. They have two
main functions: detecting the status of the infrastructures and
communicating via short-range wireless networks.
To illustrate the running process of intelligent devices,
we divide their running process into three states: detecting
state, latency state and transmission state. In detecting state,
the detecting function of intelligent devices is activated to
detect the status of infrastructures. If a certain condition
is satisfied (e.g. the infrastructures turn into bad condition
or the current time is the arranged time of reporting), the
latency state is activated. In latency state, the communication function of intelligent devices is active and waits for
the appearance of SCmules within communication range to
transmit data/messages to them. This state may consume
unnecessary energy since intelligent devices have no idea
of when SCmules will pass near them, many researches
have been done to solve this problem in ad hoc networks,
which can also be used to address the problem in this situation [42], [43]. Once a certain SCmule appears in the communication range of intelligent devices, the intelligent devices
will turn into transmission state imediately, it will then communicate directly with the SCmule to send data/messages
to it. Figure 3 illustrates the running process of intelligent
devices.
6489

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

FIGURE 3. The state diagram of intelligent devices.

Algorithm 1 The Running Process of Intelligent Devices
1: While true
2:
switch into detecting state
3:
detect the infrastructure’s status
4:
If infrastructure’s status satisfies a certain condition
5:
switch into latency state
6:
While true
7:
If SCmule Vi enters the communication range
8:
switch into transmission state
9:
report the infrastructure’s status to Vi
10:
End
11:
End
12: End
13: End

Algorithm 1 is the pseudo-code of the running process of
intelligent devices.
C. DATA CENTERS

Data centers are special computing and processing nodes
distributed in fixed locations of Smart City. They are connected with cloud tier using high-speed networks to forward
data/messages and can detect the existence of SCmules within
their communication range and receive data/messages from
them via short-range wireless communication. They have sufficient stable power supply because they are installed in fixed
locations, so their communication range is usually larger than
intelligent devices.
The running process of data centers can be divided into
three states (see Figure 4): latency state, receiving state
and forwarding state. Unlike intelligent devices, data centers are always in latency state to detect the appearance of
SCmules within their communication range. Once SCmules
pass near them, they will switch into receiving state in which
they communicate with SCmules’ transceivers to receive
all data/messages buffered in their storage. After receiving
data/messages from SCmules, data centers will send ACK
as a feedback to SCmules to tell them the data/messages
are received and they can safely clear their buffer and
then switch into forwarding state in which they process
6490

FIGURE 4. The state diagram of data centers.

Algorithm 2 The Running Process of Data Centers
1: While true
2:
switch into latency state
3:
detect SCmules which enter communication range
4:
If SCmule Vi enters communication range
5:
wait for communication request from Vi
6:
switch into receiving state
7:
receive data/messages dumped by Vi
8:
switch into forwarding state
9:
process received data/messages
10:
send received data /messages to cloud tier
11:
End
12: End

the received data/messages, such as filtering out duplicated
information, and then forward them to cloud tier. Municipal
departments can access the data/messages stored in cloud
tier. Once they check these data/messages in cloud tier, they
will take measures to maintain related infrastructures. Usually, data centers are deployed in the downtown areas of
Smart City, i.e. hotspot areas, where SCmules travel more
frequently than other areas, to boost the efficiency of data
collection.
Algorithm 2 is the pseudo-code of the running process of
data centers.
D. SCmules

SCmules are mobile IoT nodes moving in Smart City,
they are equipped with transceivers which enable them to
opportunistically communicate with nearby nodes (including intelligent devices and data centers). Typical SCmules
include taxis, private vehicles, buses and people holding
smart mobile phone. When SCmules pass near intelligent
devices which are in latency state, they will detect the existence of them and pick up data/messages they are reporting obliviously and incidentally. As the storage is limited,
SCmules have to face with the problem whether to buffer
the new picked-up data/messages or not. More specifically, in
the situation that their storage is full and new more valuable
data/messages are picked up, they have to discard some less
important data/messages buffered in storage to make room for
them. SCmules use greedy selection principle, which means
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

greedily maximize the sum of priorities of data/messages
buffered, to guide the selection. That is to say, when the
storage is not full, SCmules will store data/messages as much
as possible, but when the storage is full, SCmules will compare the priority of the intelligent devices generating these
new picked-up data/messages with the priorities of intelligent
devices generating the data/messages buffered in SCmules’
storage according to priority table and then make decision: if
the priority of these new picked-up data/messages is less than
any priorities of the data/messages buffered, the new pickedup data/messages will be discarded directly, otherwise, the
data/messages in storage with the least priority will be discarded to make room for the new picked-up data/messages
and then the new more valuable picked-up data/messages will
be buffered in the storage. When SCmules pass near data centers, SCmules will send request for dumping data/messages
to data centers, if data centers approve of the request, they
will dump all data/messages buffered in their storage to data
centers and then clear the storage.
The running process of SCmules, or more accurately the
transceivers deployed in SCmules can be divided into three
states: latency state, buffering state and dumping state (see
Figure 5). In latency state, SCmules are detecting the existence of intelligent devices or data centers within their communication range. When detecting that intelligent devices are
nearby, SCmules rapidly switch into buffering state to pick up
new data/messages and then use greedy selection principle to
determine whether to buffer them or not. When detecting that
data centers are nearby, SCmules will switch into dumping
state and then dump all the data/messages stored to data
centers.

FIGURE 5. The state diagram of SCmules.

Algorithm 3 is the pseudo-code of the running process of
SCmules, or more accurately, the transceivers deployed in
SCmules.
V. SA-PA ALGORITHM
A. OVERVIEW

Section 4 gives an almost comprehensive introduction to
SWDCP-SCmules scheme, but sidestep the problem of priority assignment. As illustrated in section 1, the priority assignment problem is key to SWDCP-SCmules scheme since it
almost determines the final performance of this scheme.
In other words, as illustrated in section 3.2, the performance of SWDCP-SCmules scheme is quantified as an optimization target function J, the priority assignment almost
VOLUME 4, 2016

Algorithm 3 The Running Process of SCmules’ Transceivers
1: While true
2:
switch into latency state
3:
detect the existence of intelligent devices or data
centers
4:
If intelligent device Si in latency state enters
communication range
5:
switch into buffering state
6:
pick up data/message d which is sent by Si
7:
If SCmule’s storage is full
8:
select data/message e with the least
priority ℘ from SCmule’s storage
9:
If Pi < ℘
10:
discard data/message d
11:
Else
12:
discard data/message e from
storage
13:
store d into SCmule’s storage
14:
End
15:
Else
16:
store d into SCmule’s storage
17:
End
18:
Else If data center hi enters communication range
19:
switch into dumping state
20:
send dumping request to hi
21:
If hi approve of the request
22:
dump all data/messages stored to hi
23:
End
24:
End
25: End

determines the final value of J. However, the traditional
approach, which is finding effective patterns and then using
these patterns to design algorithms, to solve the priority
assignment problem is unfeasible because of the ambiguity
of the meaning of effectiveness and efficiency, the difficulty
of finding patterns and the variability of patterns as described
in section 1 [44].
To solve the priority assignment problem, we try to convert
the priority assignment problem to an optimization problem
based on the idea from machine learning and use simulated
annealing metaheuristic to design a universal algorithm that
can automatically find a well-performed priority assignment
with respect to various optimization targets, which are modeled by optimization target function J, based on the socialaware patterns lying in the GPS trajectory data of SCmules
in the past time.
Concretely speaking, first, we quantify the performance
of the SWDCP-SCmules scheme as a universal optimization
E then, we use the
target function J of priority assignment P;
past GPS trajectory data of SCmules to train a priority assignE that can optimize J with respect to P
E and predict that
ment P
it will also achieve good performance in the future because
the future shares similar patterns with the past. Because of
the complexity of the computation of J, we use simulated
6491

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

E that can optimize J, in
annealing metaheuristic to search for P
search space. We refer to this solution to priority assignment
problem as Simulated Annealing for Priority Assignment
Algorithm (SA-PA algorithm).
Why a priority assignment perform well in the past can
also perform well in the future? Because the running process
of SWDCP-SCmules scheme in the future is similar to the
running process in the past under the same configuration C.
In other words, they have similar social-aware patterns.
Usually, similar social-aware patterns will lead to similar results if the configuration C is fixed. We use a
manually observed social-aware pattern to illustrate this
statement:
As illustrated in section 1, the frequency that SCmules pass
by downtown areas, i.e. hotspot areas, is much higher than
the frequency of other areas, especially remote areas. If we
deploy data centers in hotspot areas, then the data/messages
generated by intelligent devices which are near data centers are more likely to be collected than those generated by
intelligent devices located in other areas. This lead to a bad
phenomenon that data centers may receive many duplicated
data/messages generated by the intelligent devices located
near them, but receive a few data/messages from intelligent
devices located in other areas. Therefore, we should decrease
the priorities of intelligent devices near data centers and
increase the priorities of intelligent devices far from data centers: Decreasing the priorities of intelligent devices near data
centers will not interfere with the data/messages collection
of those intelligent devices, on the contrary, it can decrease
the duplicated data/messages collected by data centers (i.e.
redundancy rate) and thereby decreasing unnecessary energy
consumption. Increasing the priorities of intelligent devices
far from data centers will decrease data/messages loss and
thereby increasing collection rate. Due to the pattern that the
locations of hotspot areas in the future are usually the same
as the locations of hotspot areas in the past, assigning priorities according to the priority assignment method mentioned
above can achieve good results both in the past and in the
future.
However, we can’t just rely on the patterns simply acquired
by our observation to design algorithm for priority assignment problem. As described in section 1, the pattern mentioned above is just one of the huge number of patterns
lying in essence. In fact, most of patterns are difficult for
human intelligence to observe and even understand, not to
mention that different definitions of effectiveness and efficiency (i.e. optimization target function J) need the support
of different patterns, which means a former observed pattern
may become useless in new applications. Besides that, some
patterns may change occasionally or periodically. Based on
the above inference, we could conclude that if we simply use
the patterns acquired by our observation to design algorithm
solving the priority assignment problem, which is the traditional approach for algorithm design, there must be some
unforeseen problems lying in the algorithm that are not taken
into consideration.
6492

We follow the above example to illustrate two unforeseen
problems lying in priority assignment method proposed in
that example:
The performance of the priority assignment method in that
example is based on the assumption that all data centers are
located in hotspot areas of Smart City. This strong assumption
is not that realistic. In fact, it is impossible to deploy data
centers for all hotspot areas since there may be temporary
hotspot areas in a certain time period, e.g. the districts near
natatoriums in hot summer (see section 1). For hotspot areas
without the deployment of data centers, the priority assignment method proposed in that example is inefficient because
intelligent devices in those hotspot areas are assigned with
high priorities since they are far from data centers, but their
data/messages may be repeatedly collected by SCmules since
they are in hotspot areas. Therefore, these hotspot areas will
contribute a lot of redundant data/messages to data centers
and thus increase redundancy rate significantly.
Even if the assumption is satisfied, i.e. all hotspot areas are
deployed with data centers, some data/messages generated by
intelligent devices located near data centers may still have
difficulties to be collected. For example, if the data center is
deployed near the starting point of the one-way street but the
intelligent device is deployed in the end point. Taxis, a typical
kind of SCmules, moving along the one-way street will first
pass by the data center and dump buffered data/messages
to it and then pass by the intelligent devices and pick up
data/messages from it. If the one-way street lead taxis from
a downtown area to a remote area, although this intelligent
device locates in a hotspot area and near a data center, we still
can’t guarantee that the data/messages from this intelligent
device can be collected rapidly just like those generated by
other ordinary intelligent devices located near data centers.
However, according to the priority assignment method proposed in that example, this intelligent device will be assigned
with low priority, its data/messages are easily replaced by
data/messages generated by intelligent devices located in
remote areas. This will lead to the loss of data/messages
generated by this intelligent device (see Figure 6).

FIGURE 6. The one-way street in counterexample.

Unlike traditional algorithm design approach, machine
learning can automatic find social-aware patterns, which
are patterns that can reflect the social preferences of citizens in Smart City, by training hypothesis using training
data. Using the idea from machine learning, we propose the
SA-PA algorithm which is a universal algorithm for priority
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

assignment problem. No matter what optimization target J is,
how difficult the observation of potential social-aware patterns are and how social-aware patterns lying in the GPS trajectory data change constantly, SA-PA algorithm can always
automatically learn social-aware patterns from training data
(i.e. the GPS trajectory data of SCmules) without manual
teaching and then use these social-aware patterns to guide
priority assignment.
An important attribute of SA-PA algorithm is its socialawareness since it can automatically learn citizens’ social
behaviors, which can be formally expressed by social-aware
patterns, from GPS trajectory data of SCmules in Smart City.
As illustrated in section 1, the basis of learning is that there
are a lot of social-aware information lying in the data. This is
obviously true because a lot of instances can be provided. For
example, if SCmules are taxis, the preferred places citizens
in Smart City usually go to, i.e. the hotspot areas, can be
analyzed from the frequencies that taxis travel to a certain
place with; if SCmules are private vehicles, the places the
owners of the private vehicles prefer to go to can be acquired
from the trajectories of these private vehicles. The key to
the success of SA-PA algorithm is that it can automatically
discover these social-aware information from data, express
them in the form of hypothesis (the meaning of hypothesis
is hard to explain and sometimes even beyond the understanding of human beings) and then use them to improve the
priority assignment. This attributes to the magic of machine
learning.
In section 5.2, we formalize the priority assignment problem. In section 5.3, we introduce SA-PA algorithm in detail.
B. FORMAL PROBLEM DESCRIPTION

Informally speaking, the problem solved by SA-PA algorithm
can be described as follow:
Given the following four elements
• the information of the intelligent devices (e.g. positions,
communication range);
• the information of data centers (e.g. positions, communication range);
• the list of all registered SCmules and their configurations
(e.g. storage size, communication range of transceivers);
• GPS trajectory data of SCmules in past time interval
[t1 , t2 ) where t1 and t2 are timestamps and t1 < t2 .
We want to find a priority assignment that can optimize the
optimization target (e.g. maximize collection rate, minimize
redundancy rate) in the future time interval [t2 , t3 ) where
t2 and t3 are timestamps and t2 < t3 .
To introduce SA-PA algorithm, we need formalize the
problem description above: As defined in section 3.1,
the intelligent devices and data centers can be expressed
as S and H respectively. All registered SCmules and their
storage size can be expressed as V and Q respectively.
To formalize the GPS trajectory of SCmule Vi in a given time
interval, we define trajectory function D as follow:
D (Vi , tx ) = (x, y)
VOLUME 4, 2016

The output of D is the geographic coordinate (x, y) of SCmule
Vi in timestamp tx . Therefore, the meaning of knowing the
GPS trajectories of SCmule Vi in time interval [t1 , t2 ) can be
formalized as knowing the value of D (Vi , tx ) where Vi ∈ V
and tx ∈ [t1 , t2 ).
Next, we formalize the notation of priority assignment.
We follow the way we express priority assignment in
section 3.1. That is, all priorities of intelligent devices form
set P = {P1 , P2 , . . . , Pm } where m is the number of
intelligent devices and Pi is the priority assigned to intelE =
ligent device Si . The corresponding vector form is P
[P1 , P2 , . . . , Pm ]. Obviously, there is a one-to-one-to-one
E It is
correspondence among priority assignment, P and P.
worth to mention all possible priority assignments form a
E is used to express priority
subset of m-dimensional space if P
assignments.
In the informal problem description, the meaning of optimization target is not specified, which means any concrete
target, such as maximize collection rate or minimize redundancy rate, can be embedded into the problem and then be
solved by SA-PA algorithm. As illustrated in section 3.2, we
use a universal optimization target function J to model the
unspecified optimization target. Based on J, if P is known,
the configuration C = (S, P, H, V, Q) is determined and we
can directly compute the value of optimization target function
in time interval [t2 , t3 ), which is JC
.
[t2 ,t3 )
To illustrate the meaning of J, three concrete examples of J
are introduced in section 3.2, i.e. JC , JR and JS , which will
be used in section 6 to conduct experiments.
To simplify expressions, we introduce the simplified notation of optimization target function. Before doing that, we
first introduce the simplified function notations of collection
rate and redundancy rate:
(S,P,H,V,Q)

C (P, T) = CT

(S,P,H,V,Q)

R (P, T) = RT

where P is priority assignment and T is time interval.
S, H, V and Q are known elements of C.
The simplified notation of optimization target function is
(S,P,H,V,Q)

J (P, T) = JT

where P is priority assignment and T is time interval.
S, H, V and Q are known elements of C.
As a result, the simplified notation of JC , JR and JS are
listed below:
JC (P, T) = C (P, T)
JR (P, T) = 1 − R (P, T)

JS (P, T) = λ1 C (P, T) + λ2 (1 − R (P, T))
(λ1 + λ2 = 1, λ1 ≥ 0 and λ2 ≥ 0)

In summary, the formal description of the problem solved
by SA-PA algorithm can be stated as follow:
Given S, H, V, Q and the value of D (Vi , tx ) where
Vi ∈ V and tx ∈ [t1 , t2 ), find a Pbest which can maximize
J (Pbest , [t2 , t3 )).
6493

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

C. FORMAL DESCRIPTION OF SA-PA ALGORITHM

To address the problem in section 5.2, we can use the
GPS trajectory data in past time interval [t1 , t2 ) of all
SCmules in V as training set and find a priority assignment
which can achieve good performance in training set by automatically learning the social-aware patterns lying in training
set. Due to the future shares similar social-aware patterns with
the past, we claim this priority assignment will also achieve
good performance in future time interval [t2 , t3 ). This is the
sketch of SA-PA algorithm.
Below details the sketch of SA-PA algorithm:
Assume P is given, according to the formal problem
description given in last subsection, all elements of configuration C = (S, P, H, V, Q) are known. Besides these,
we also know the GPS trajectory data of SCmules in time
interval [t1 , t2 ), i.e. the value of D (Vi , tx ) where Vi ∈ V and
tx ∈ [t1 , t2 ). Based on the given information, we can estimate
the value of J (P, [t1 , t2 )), which is a measure of performance
as described in section 5.1, by simulating the running process
of SWDCP-SCmules scheme in time interval [t1 , t2 ).
However, we don’t know which P will make training set
performs well, i.e. maximize J (P, [t1 , t2 )). To find such P
that can optimize J, we convert the original problem to a
optimization problem:
Given S, H, V, Q and the value of D (Vi , tx ) where
Vi ∈ V and tx ∈ [t1 , t2 ), find a priority assignment ℘ which
can maximize J (℘, [t1 , t2 )). We say ℘ is a good approximation of Pbest , which can maximize J (Pbest , [t2 , t3 )).
Note that J cannot be simply expressed by closed formula.
To compute J, we need to simulate the running process of
Smart City based on the past GPS trajectory data. That’s
why we cannot use iterative optimization algorithms such
as gradient descent and Newton method [45]. Metaheuristics
are perfect solution to optimizing this kind of computational
complicated function as described in section 2.3. Therefore,
we use simulated annealing metaheuristic to solve this optimization problem.
To apply simulated metaheuristic to this optimization
problem, we need to define configuration, evaluation function and neighborhood function as described in section 2.3.
For this optimization problem, we can naturally define
configuration as ℘, i.e. a priority assignment and define
evaluation function as J (℘, [t1 , t2 )). The definition of neighborhood function is not as direct as configuration and
evaluation function. We propose a neighborhood function based on swapping the priorities of two intelligent
devices:



< ℘1 , . . . , ℘i , . . . , ℘j , . . . , ℘m , i, j


= ℘1 , . . . , ℘ j , . . . , ℘ i , . . . , ℘ m
where 1 ≤ i < j ≤ m.
In section 5.1, we have given an intuitive explanation of
why SA-PA algorithm takes effect. In section 6.3, we conduct
experiments to prove its effectiveness.
Algorithm 4 is the pseudo-code of SA-PA algorithm.
6494

Algorithm 4 SA-PA Algorithm
Input: T0 , α,maxMv, S, H, V, Q and
D (Vi , tx ) (Vi ∈ V, tx ∈ [t1 , t2 ))
Output: ℘best
1: nbMv := 0
2: ℘current :=generateRandomPriorityAssignment(S)
3: ℘best := ℘current
4: While nbMv < maxMv
5:
pick random number i and j (1 ≤ i < j ≤ m)
6:
℘new := <(℘current , i, j)
7:
1 := J (℘new , [t1 , t2 )) − J (℘current , [t1 , t2 ))
8:
If 1 ≥ 0 or e1/T0 ≥ rand(0, 1)
9:
℘current := ℘new
10:
nbMv := nbMv + 1
11:
If J (℘current , [t1 , t2 )) ≥
J (℘best , [t1 , t2 ))
12:
℘best := ℘current
13:
End
14:
End
15: End
16: Return ℘best

Parenthetically, the conditional statements of line 8
is equivalent to the acceptance probability illustrated
in section 2.3.
Notice SA-PA algorithm uses idea from machine learning. It conforms to the received definition of machine
learning proposed by Tom Mitchell, which is described in
section 2.2 [36]. In SA-PA algorithm, E is the GPS trajectory
data in time interval [t1 , t2 ), P is the optimization target function J and T is to find a well-performed priority assignment.
From the perspective of hypothesis, a configuration C is a
hypothesis and P is the parameters of the hypothesis, we
use training set, i.e. the GPS trajectory data in time interval
[t1 , t2 ), to train the hypothesis to find a well-performed P as
the parameters of the hypothesis.
VI. EXPERIMENTAL RESULTS
A. OVERVIEW OF DATASET

To prove the effectiveness of the SWDCP-SCmules scheme
and SA-PA algorithm, we use T-Drive trajectory dataset,
which is provided by MSRA [46], [47], to do experiments.
The T-Drive trajectory dataset contains GPS trajectories of
10357 taxis during the period of Feb. 2 to Feb. 8, 2008, in
Beijing. The number of GPS waypoints in this dataset reaches
up to 15 million and the total distance of the trajectories
reaches up to 9 million kilometers. Figure 7 illustrates the
dataset visually, different colors reflect different density distribution of the GPS waypoints.
Figure 8, which is extracted from the instruments of the
dataset, shows the distribution of time interval between two
consecutive waypoints. It indicates, in most cases, GPS
devices equipped in taxis sample geographical data every one
second or five seconds. This means these discrete waypoints
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

FIGURE 7. Visualization of T-Drive trajectory dataset.

FIGURE 10. Dataset after filtering out invalid waypoints.

B. EXPERIMENTAL METHODOLOGY

FIGURE 8. The distribution of sampling time interval [46], [47].

have enough information to observe the continuous trajectories of taxis.
Besides that, we find there exist some invalid waypoints
in dataset, they deviate from the geographical position of the
last valid waypoints in very short time interval. For example,
in Figure 9, invalid waypoints are marked by red colors.
These invalid waypoints usually occur when taxis are in
the areas where GPS service quality is poor, e.g. tunnels.
To guarantee the accuracy of experimental results, we need to
filter out these invalid waypoints. Below is the simple filtering
algorithm we propose:

FIGURE 9. Invalid waypoints in dataset.

For a given waypoint, we check the distance between the
last valid waypoint and this waypoint divides the time interval
between them, if it is larger than the maximum speed limit
in Beijing, which is 80 km/h in most places, we mark this
waypoint as invalid waypoint and filter it out. Figure 10
illustrates the effectiveness of this filtering method.
VOLUME 4, 2016

As SA-PA algorithm need repeatedly simulate the running
process of training set with respect to different priority assignment ℘ to compute J (℘, [t1 , t2 )), which is a extremely timeconsuming process, we should simplify the running process
to accelerate the algorithm:
We separate the map of Beijing by compact square grid
cells, the area of each grid cell is about 40 × 40m2 , which is
the typical coverage range of low power wireless technologies
like 802.15.4 and Bluetooth in free space. We assume the
communication range of every intelligent device, data center
or the transceiver is the area covered by the grid cell it locates
in. Although this approximation will introduce some computational error, it can significantly accelerate the simulation.
Since we have separated the map of Beijing by compact
grid cells, we can also compress the T-Drive trajectory dataset
to filter out unnecessary information based on the grid cells.
Because we no longer concern about the exact geographical
positions of taxis if we use grid cells as the substitution
of original maps, Instead, what we only need to concern
is which grid cells these taxis locate in. Therefore, we can
compress the dataset and only retain the trajectory of grid
cells rather than trajectory of exact longitudes and latitudes.
By compressing the dataset, the amount of data needed to
be processed significantly reduces, and thereby accelerate the
simulation.
We then discuss the geographical deployment of intelligent
devices and data centers since they determine two important
elements of configuration C = (S, P, H, V, Q), i.e. S and H,
respectively.
For data centers H, we follow the suggestion in section 4.3,
that is, data centers are usually deployed in downtown areas
of Smart City, i.e. hotspot areas. To find hotspot areas, we
count the frequency of being travelled by taxis of each grid
cell, select the grid cells with high frequency as hotspot areas
and then deploy data centers to these grid cells.
By contrast, the deployment of intelligent devices S is
comparatively free, we randomly select grid cells and deploy
intelligent devices to these grid cells. These grid cells need
to be travelled by taxis because taxis can usually cover most
of places of the city except the non-constructive lands, such
as rivers and forests. Non-constructive land usually have no
infrastructures deployed. If a grid cell has no taxis passing by,
it is likely to be non-constructive land. In addition, random
6495

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

selection can reduce bias since infrastructures of all areas
have probability to be selected to deploy intelligent devices.
For the other three elements V, Q and P of C. V is determined by dataset. Q can be controlled by ourselves. Both
V and Q are easy to be handled in experiments.
P need to be trained by SA-PA algorithm with respect
to various optimization target function J. Three concrete
optimization target functions, which are introduced in
section 3.2, are used in the experiments: JC , JR and JS .
Based on the research paradigm of machine learning, we
separate T-Drive trajectory dataset into two parts: training
set and test set. Training set is used to train the hypothesis,
i.e. optimize J with respect to P to get a well-performed C
(or more accurately, ℘), using SA-PA algorithm. Test set is
used to report the performance of the trained hypothesis, i.e.
the performance of C. Because the T-Drive trajectory dataset
provided by MSRA is a kind of spatial-temporal data, we
separate it according to time: The trajectory data in period
of Feb. 2 to Feb. 6 is training set and the trajectory data in
period of Feb. 7 to Feb. 8 is test set.

FIGURE 11. The optimization process of JC .

C. PERFORMANCE ANALYSIS
1) PERFORMANCE WITH RESPECT TO JC

As illustrated in section 6.2, three concrete optimization target functions are used to do experiments: JC , JR and JS .
In this subsection, JC is used as optimization target function
to illustrate the performance of SWDCP-SCmules scheme
and SA-PA algorithm.
We first try to prove the effectiveness of SA-PA algorithm experimentally. To prove its effectiveness, we fix elements S, H, V and Q of configuration C, and observe if
the value of JC C
increases with the increase of JC C
[t2 ,t3 )

where [t1 , t2 ) is the time interval of training set and [t2 , t3 )
is the time interval of test set. If JC C
and JC C
have
[t1 ,t2 )

[t2 ,t3 )

a positive correlation, it proves the social-aware patterns in
the past are similar to those in the future and these similarities lead to similar performance, which proves the basis of
SA-PA algorithm.
In Figure 11, the blue line roughly tilts up with the increase
of horizontal axis, which means that the priority assignment
℘ that can optimize JC (P, [t1 , t2 )) will also roughly optimize
JC (P, [t2 , t3 )), i.e. JC (P, [t1 , t2 )) and JC (P, [t2 , t3 )) have a
positive correlation.
Next we closely compare the degree of improvement provided by SA-PA algorithm from the change of JC C
with
[t2 ,t3 )
respect to the increase of storage limit in test set.
In Figure 14, the black line illustrates the increase of
JC (℘random , [t2 , t3 )) with respect to the increase of storage
limit where ℘random is a randomly generated priority assignment. assignment. The red line illustrates the increase of
JC (℘1000 , [t2 , t3 )) with respect to the increase of storage
limit where ℘1000 is the priority assignment generated by
a running of SA-PA algorithm with nbMV = 1000 based
on ℘random . From Figure 14, we can see that no matter which
6496

FIGURE 12. The change of JC with respect to storage limit.

[t1 ,t2 )

priority assignment is used, the collection rate of SWDCPSCmules scheme increases with the increase of storage limit,
which conforms to intuition. Besides that, we can also see
that ℘1000 outperforms ℘random since the red line is above
the black line, which means the collection rate of red line is
higher than the collection rate of black line, line, especially
when the storage limit is not large. This proves the effectiveness of SA-PA algorithm. However, when storage limit is
large, the effectiveness of SA-PA algorithm is not significant,
this is because the storage is large enough to get rid of the
limitation of storage.
Then we closely compare the degree of improvement provided by SA-PA algorithm from the change of JC C
with
[t2 ,t3 )
respect to the increase of time in test set.
In Figure 13, the black line illustrates the increase of
JC (℘random , [t2 , t3 )) with respect to the increase of time
where ℘random is a randomly generated priority assignment.
The red line illustrates the increase of JC (℘200 , [t2 , t3 )) with
respect to the the increase of time where ℘200 is the priority
assignment generated by a running of SA-PA algorithm with
nbMV = 200 based on ℘random . From Figure 13, we can
see that no matter which priority assignment is used, the
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

FIGURE 13. The change of JC with respect to time.

FIGURE 14. The change of JC with respect to the number of data centers.

of SWDCP-SCmules scheme increases with the increase of
time, which conforms to intuition. Besides that, we can also
see we can also see that ℘200 outperforms ℘random since the
red line is above the black line, which means the collection
rate of ℘200 is larger than the collection rate of ℘random in
every time. This proves the algorithm.
Finally, we closely compare the degree of improvement
provided by SA-PA algorithm from the change of JC C
[t2 ,t3 )
with respect to the increase of data centers deployed in Smart
City in test set.
In Figure 12, the black line illustrates the increase of
JC (℘random , [t2 , t3 )) with respect to the increase of the number of data centers where ℘random is a randomly generated
priority assignment. The red line illustrates the increase
of JC (℘500 , [t2 , t3 )) with respect to the increase of the
number of data centers where ℘500 is the priority assignment generated by a running of SA-PA algorithm with
nbMV = 500 based on ℘random . From Figure 12, we can
see that no matter which priority assignment is used, the
collection rate of SWDCP-SCmules scheme increases with
the increase of the number of data centers, which conforms to
intuition. Besides that, we can also see that ℘500 outperforms
℘random since the red line is above the black line, which means
means the collection rate of ℘500 is larger than the collection
rate of ℘random no matter how many data deployed. This
proves the effectiveness of SA-PA algorithm.

time interval of test set. If J

2) PERFORMANCE WITH RESPECT TO JR

As illustrated in section 6.2, three concrete optimization target functions are used to do experiments: JC , JR and JS . In
this subsection, JR is used as optimization target function to
illustrate the performance of SWDCP-Scmules scheme and
SA-PA algorithm.
We first try to prove the effectiveness of SA-PA algorithm
experimentally. To prove its effectiveness, we fix elements
S, H, V and Q of configuration C, and observe if the value
of J C
increases with the increase of J C
where
R t ,t
R t ,t
[ 2 3)
[ 1 2)
[t1 , t2 ) is the time interval of training set and [t2 , t3 ) is the
VOLUME 4, 2016

C
R t

and J

C
R t

have a
[ )
[ 2 ,t3 )
positive correlation, it proves the social-aware patterns in the
past are similar to those in the future and these similarities
lead to similar performance, which proves the basis of SAPA algorithm.
In Figure 15, the blue line roughly tilts up with the increase
of horizontal axis, which means that the priority assignment
℘ that can optimize JR (P, [t1 , t2 )) will also roughly optimize
JR (P, [t2 , t3 )), i.e. JR (P, [t1 , t2 )) and JR (P, [t2 , t3 )) have a
positive correlation.
1 ,t2

FIGURE 15. The optimization process of JR .

Next we closely compare the degree of improvement provided by SA-PA algorithm from the change of J C
with
R t ,t
[ 2 3)
respect to the increase of storage limit in test set.
In Figure 16, the black line illustrates the change of
JR (℘random , [t2 , t3 )) with respect to the increase of storage
limit where ℘random is a randomly generated priority assignment. The red line illustrates the change of JR (℘1000 , [t2 , t3 ))
with respect to the increase of storage limit where ℘1000
is the priority assignment generated by a running of
SA-PA algorithm with nbMV = 1000 based on ℘random .
6497

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

FIGURE 16. The change of JR with respect to storage limit.

From Figure 16, we can see that no matter priority assignment
is used, the effective collection rate of SWDCP-SCmules
scheme decreases with the increase of storage limit, which is
because the more storage SCmules have, the larger probability they will carry redundant data/messages. Besides that, we
can also see that ℘1000 outperforms ℘random since the red line
is above the black line, which means the effective collection
rate of red line is higher than the effective collection rate of
black line, especially when the storage limit is not large. This
proves the effectiveness of SA-PA algorithm. However, when
storage limit is large, the effectiveness of optimization is not
significant, this is because the storage is large enough to get
rid of the limitation of storage.
Then we closely compare the degree of improvement provided by SA-PA algorithm from the change of J C
with
R t ,t
[ 2 3)
respect to the increase of time in test set.

FIGURE 17. The change of JR with respect to time.

In Figure 17, the black line illustrates the change of
JR (℘random , [t2 , t3 )) with respect to the increase of time
where ℘random is a randomly generated priority assignment.
The red line illustrates the change of JR (℘200 , [t2 , t3 )) with
6498

FIGURE 18. The change of JR with respect to the number of data centers.

respect to the increase of time where ℘200 is the priority
assignment generated by a running of SA-PA algorithm with
nbMV = 200 based on ℘random . From Figure 17, we can
see that no matter which priority assignment is used, the
effective collection rate of SWDCP-SCmules scheme roughly
decreases with the increase of time. Besides that, we can also
see that ℘200 outperforms ℘random since the red line is above
the black line, which means the effective collection rate of
℘200 is larger than the effective collection rate of ℘random in
every time. This proves the effectiveness of SA-PA algorithm.
Finally, we closely compare the degree of improvement
provided by SA-PA algorithm from the change of J C
R t ,t
[ 2 3)
with respect to the increase of data centers deployed in Smart
City in test set.
In Figure 18, the black line illustrates the increase of
JR (℘random , [t2 , t3 )) with respect to the increase of the
number of data centers where ℘random is a randomly priority assignment. The red line illustrates the increase of
JR (℘500 , [t2 , t3 )) with respect to the increase of the number of data centers where ℘500 is the priority assignment
generated by a running of SA-PA algorithm with nbMV =
500 based on ℘random . From Figure 18, we can see that the
effective collection rate fluctuates with the increase of the
number of data centers, this is because the effective collection
rate really depends on the selection of positions to deploy data
centers. Besides that, we can also see that ℘500 outperforms
℘random since the red line is above the black line, which means
the effective collection rate of ℘500 is larger than the effective
collection rate of ℘random no matter how many number of
data centers are deployed. This proves the effectiveness of
SA-PA algorithm.
3) PERFORMANCE WITH RESPECT TO JS

As illustrated in section 6.2, three concrete optimization target functions are used to do experiments: JC , JR and JS .
In this subsection, JS is used as optimization target function
to illustrate the performance of SWDCP-SCmules scheme
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

FIGURE 19. The optimization process of JS .

and SA-PA algorithm. The concrete JS we use in this subsection is


C
JSTC = 0.8CC
T + 0.2 1 − RT
where λ1 = 0.8, λ2 = 0.2 and T is a time interval.
We first try to prove the effectiveness of SA-PA algorithm
experimentally. To prove its effectiveness, we fix elements
S, H, V and Q of configuration C, and observe if the value
of JS C
increases with the increase of JS C
where
[t2 ,t3 )
[t1 ,t2 )
[t1 , t2 ) is the time interval of training set and [t2 , t3 ) is
the time interval of test set. If JS C
and JS C
have
[t1 ,t2 )
[t2 ,t3 )
a positive correlation, it proves the social-aware patterns in
the past are similar to those in the future and these similarities lead to similar performance, which proves the basis of
SA-PA algorithm.
In Figure 19, the black line roughly tilts up with the
increase of horizontal axis, which means that the priority assignment ℘ that can optimize JS (P, [t1 , t2 )) will
also roughly optimize JS (P, [t2 , t3 )), i.e. JS (P, [t1 , t2 )) and
JS (P, [t2 , t3 )) have a positive correlation. Besides the black
line, red line and blue line illustrates the changes of collection
rate and effective collection rate in the process of optimization, respectively.
Next we closely compare the degree of improvement provided by SA-PA algorithm from the change of JS C
with
[t2 ,t3 )
respect to the increase of storage limit in test set.
In Figure 20, the black line illustrates the increase of
JS (℘random , [t2 , t3 )) with respect to the increase of storage
limit where ℘random is a randomly generated priority assignment. assignment. The red line illustrates the increase of
JS (℘1000 , [t2 , t3 )) with respect to the increase of storage limit
where ℘1000 is the priority assignment generated by a running
of SA-PA algorithm with nbMV = 1000 based on ℘random .
From Figure 20, we can see that no matter which priority
assignment is used, the synthesized rate of SWDCP-SCmules
scheme roughly increases with the increase of storage limit,
which is because the weight we set to collection rate is much
VOLUME 4, 2016

FIGURE 20. The change of JS with respect to storage limit.

larger than the weight we set to redundancy rate, i.e. λ1 > λ2 ,
in synthesized rate. Besides that, we can also see that ℘1000
outperforms ℘random since the red line is above the black
line, which means the synthesized rate of red line is rate of
black line, especially when the storage limit is not large. This
proves the effectiveness of SA-PA algorithm. However, when
storage limit is large, the effectiveness of optimization is not
significant, this is because the storage is large enough to get
rid of the limitation of storage.
Then we closely compare the degree of improvement provided by SA-PA algorithm from the change of JS C
with
[t2 ,t3 )
respect to the increase of time in test set.
In Figure 21, the black line illustrates the increase of
JS (℘random , [t2 , t3 )) with respect to the increase of time
where ℘random is a randomly generated priority assignment.
The red line illustrates the increase of JS (℘200 , [t2 , t3 )) with
respect to the the increase of time where ℘200 is the priority
assignment generated by a running of SA-PA algorithm with
nbMV = 200 based on ℘random . From Figure 21, we can
see that no matter which priority assignment is used, the rate
of SWDCP-SCmules roughly increase with the increase of
time, which is because the weight we set to collection rate
is much larger than the weight we set to redundancy rate,
i.e. λ1 > λ2 , in synthesized rate. Besides synthesized rate.
Besides that, we can also see that ℘200 outperforms ℘random
since the red line is above the black line, which means the
synthesized rate of ℘200 is larger than the synthesized rate
of ℘random in every time. This proves the effectiveness of
SA-PA algorithm.
Finally we closely compare the degree of improvement
provided by SA-PA algorithm from the change of JS C
[t2 ,t3 )
with respect to the increase of the number of data centers in
test set.
In Figure 22, the black line illustrates the increase of
JS (℘random , [t2 , t3 )) with respect to the increase of the number of data centers where ℘random is a randomly generated
priority assignment. The red line illustrates the increase of
6499

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

TABLE 2. Main notations.

FIGURE 21. The change of JS with respect to time.

We use SCmules with a priority assignment optimized
by SA-PA algorithm with respect to the optimization target function JS , where the size of storage of SCmules
is 20, to compare with mules, which has no storage, J C
constraint. Table 2 gives the value of JS C
[t2 ,t3 ) C[t2 ,t3 )
and J C
using SCmules and mules. From this table,
R t ,t
[ 2 3)
we can draw the conclusion that, although the performance, especially the collection rate, can be influenced by
whether the oblivious data mules are storage-constrained
or not, the bad influence can be significantly mitigated by
using a proper priority assignment found by SA-PA algorithm. In addition, the proper priority assignment found
by SA-PA algorithm can reduce the redundancy rate and
therefore reducing energetic waste and increasing network
lifetime.
VII. CONCLUSIONS

FIGURE 22. The change of JS with respect to the number of data centers.

JS (℘500 , [t2 , t3 )) with respect to the increase of the number
of data centers where ℘500 is the priority assignment generated by a running of SA-PA algorithm with nbMV = 500
based on ℘random . From Figure 22, we can see that no matter
which priority assignment is used, the synthesized rate of
SWDCP-SCmules roughly increase with the increase of the
number of data centers, which is because the weight we set
to collection rate is much larger than the weight we set to
redundancy rate, i.e. λ1 > λ2 , in synthesized rate. Besides
that, we can also see that ℘500 outperforms ℘random since the
red line is above the black line, which means the synthesized
rate of ℘500 is larger than the synthesized rate of ℘random no
matter how many data centers are deployed. This proves the
effectiveness of SA-PA algorithm.
4) SCmules VERSUS MULES

In this subsection, we analyze the influence of storageconstraint, in other words, we compare the performance
of SCmules introduced in this paper with the performance of mules (i.e. oblivious data mules) proposed
by Bonola et al. [17].
6500

In this paper, we propose a Social Welfare Data Collection paradigm based on Storage-Constrained Oblivious Data
Mules (SWDCP-SCmules scheme). In this scheme, intelligent devices are embedded to infrastructures of Smart City
to detect and report their status, data centers are deployed
in hotspot areas of Smart City to collect data from intelligent devices and SCmules are mobile IoT nodes moving in Smart City to obliviously pick up data reported
by intelligent devices and store-carry-forward to data centers. The SWDCP-SCmules scheme enables the intelligent management of cities and boost the overall social
welfare.
However, the storage size of SCmules are limited, to
cope with the storage limitations, the concept of priority is introduced to model the selection strategy used
in the situation that SCmules lack of spare storage and
converts selection strategy to priority assignment. The
Simulated Annealing for Priority Assignment Algorithm
(SA-PA algorithm) is proposed to guide the priority assignment of intelligent devices. The SA-PA algorithm is a universal machine learning algorithm which can automatically
find well-performed priority assignment with respect to various optimization targets by learning social-aware patterns
from the past GPS trajectory data of SCmules and significantly improve the performance of SWDCP-SCmules
scheme.
In general, the SWDCP-SCmules combined with SA-PA
algorithm can propel the construction of Smart City and
thereby lead to significant improvement of overall social
welfare.
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

REFERENCES
[1] Z. Zhao, M. Peng, Z. Ding, W. Wang, and H. V. Poor, ‘‘Cluster content
caching: An energy-efficient approach to improve quality of service in
cloud radio access networks,’’ IEEE J. Sel. Areas Commun., vol. 34, no. 5,
pp. 1207–1221, May 2016.
[2] S. He, S. He, D.-H. Shin, J. Zhang, J. Chen, and Y. Sun, ‘‘Fullview area coverage in camera sensor networks: Dimension reduction
and near-optimal solutions,’’ IEEE Trans. Veh. Technol., vol. 65, no. 9,
pp. 7448–7461, Sep. 2016, doi: 10.1109/TVT.2015.2498281.
[3] M. Peng, H. Xiang, Y. Cheng, S. Yan, and H. V. Poor, ‘‘Inter-tier interference suppression in heterogeneous cloud radio access networks,’’ IEEE
Access, vol. 3, pp. 2441–2455, 2015.
[4] H. Li, X. Lin, H. Yang, X. Liang, R. Lu, and X. Shen, ‘‘EPPDR: An efficient privacy-preserving demand response scheme with adaptive key evolution in smart grid,’’ IEEE Trans. Parallel Distrib. Syst., vol. 25, no. 8,
pp. 2053–2064, Aug. 2014.
[5] S. He, J. Chen, X. Li, X. Shen, and Y. Sun, ‘‘Mobility and intruder
prior information improving the barrier coverage of sparse sensor networks,’’ IEEE Trans. Mobile Comput., vol. 13, no. 6, pp. 1268–1282,
Jun. 2015.
[6] X. Liu, ‘‘A deployment strategy for multiple types of requirements
in wireless sensor networks,’’ IEEE Trans. Cybern., vol. 45, no. 10,
pp. 2364–2376, Oct. 2015.
[7] Q. Yang, S. He, J. Li, J. Chen, and Y. Sun, ‘‘Energy-efficient probabilistic
area coverage in wireless sensor,’’ IEEE Trans. Veh. Technol., vol. 61, no. 1,
pp. 367–377, Jan. 2015.
[8] Q. Hu, M. Peng, Z. Mao, X. Xie, and H. V. Poor, ‘‘Training design for
channel estimation in uplink cloud radio access networks,’’ IEEE Trans.
Signal Process., vol. 64, no. 13, pp. 3324–3337, Jul. 2016.
[9] H. Li, Y. Yang, T. H. Luan, X. Liang, L. Zhou, and X. S. Shen, ‘‘Enabling
fine-grained multi-keyword search supporting classified sub-dictionaries
over encrypted cloud data,’’ IEEE Trans. Dependable Secure Comput.,
vol. 13, no. 3, pp. 312–325, May/Jun. 2016.
[10] S. Sarkar, S. Chatterjee, and S. Misra, ‘‘Assessment of the suitability of
fog computing in the context of Internet of things,’’ IEEE Trans. Cloud
Comput., to be published, doi: 10.1109/TCC.2015.2485206.2015.
[11] M. Aazam and E.-N. Huh, ‘‘Fog computing micro datacenter based
dynamic resource estimation and pricing model for IoT,’’ in Proc. IEEE
29th Int. Conf. Adv. Inf. Netw. Appl., Mar. 2015, pp. 687–694.
[12] L. Piras. (Mar. 2014). A Brief History of the Internet of Things
[Infographic]. [Online]. Available: http://www.psfk.com/2014/03/
internet-ofthings-infographic.html
[13] Internet of Things Market Forecast Cisco. [Online]. Available:
http://postscapes.com/internet-of-things-market-size
[14] X. Liu, ‘‘A novel transmission range adjustment strategy for energy hole
avoiding in wireless sensor networks,’’ J. Netw. Comput. Appl., vol. 67,
pp. 43–52, May 2016.
[15] Y. Hu and A. Liu, ‘‘Improving the quality of mobile target detection
through portion of node with full duty cycle in WSNs,’’ Comput. Syst. Sci.
Eng., vol. 31, no. 1, pp. 5–17, 2016.
[16] Y. Liu et al., ‘‘FFSC: An energy efficiency communications approach
for delay minimizing in Internet of Things,’’ IEEE Access, vol. 4,
pp. 3775–3793, 2016.
[17] M. Bonola, L. Bracciale, P. Loreti, P. Amici, A. Rabuffi, and G. Bianchi,
‘‘Opportunistic communication in smart city: Experimental insight with
small-scale taxi fleets as data carriers,’’ Ad Hoc Netw., vol. 43, pp. 43–55,
Jun. 2016.
[18] N. Luong, D. Hoang, P. Wang, D. Niyato, D. Kim, and Z. Han, ‘‘Data
collection and wireless communication in Internet of Things (IoT) using
economic analysis and pricing models: A survey,’’ IEEE Commun. Surveys
Tut., to be published, doi: 10.1109/COMST.2016.2582841,2016.
[19] R. C. Shah, S. Roy, S. Jain, W. Brunette, ‘‘Data MULEs: Modeling a
three-tier architecture for sparse sensor networks,’’ Ad Hoc Netw., vol. 1,
nos. 2–3, pp. 215–233, Sep. 2003.
[20] R. Xie, A. Liu, and J. Gao, ‘‘A residual energy aware schedule scheme
for wsns employing adjustable awake/sleep duty cycle,’’ Wireless Pers.
Commun., vol. 2016, pp. 1–29, Jun. 2016, doi: 10.1007/s11277-016-34280.2016.
[21] H. Li, D. Liu, Y. Dai, and T. H. Luan, ‘‘Engineering searchable encryption of mobile cloud networks: When QoE meets QoP,’’ IEEE Wireless
Commun., vol. 22, no. 4, pp. 74–80, Aug. 2015.
[22] J. Gubbi, R. Buyya, S. Marusic, and M. Palaniswami, ‘‘Internet of
Things (IoT): A vision, architectural elements, and future directions,’’
Tech. Rep. CLOUDS-TR-2012-2, Jul. 2012.
VOLUME 4, 2016

[23] C. C. Sobin, V. Raychoudhury, G. Marfia, and A. Singla, ‘‘A survey
of routing and data dissemination in delay tolerant networks,’’ J. Netw.
Comput. Appl., vol. 67, pp. 128–146, May 2016.
[24] M. Raj, N. Li, D. Liu, M. Wright, S. K. Das , ‘‘Using data mules to preserve
source location privacy in wireless sensor networks,’’ Pervasive Mobile
Comput., vol. 11, pp. 244–260, Apr. 2014.
[25] Y.-C. Tseng, F.-J. Wu, W.-T. Lai, ‘‘Opportunistic data collection for disconnected wireless sensor networks by mobile mules,’’ Ad Hoc Netw., vol. 11,
no. 3, pp. 1150–1164, May 2013.
[26] C.-K. Tham and T. Luo, ‘‘Fairness and social welfare in service allocation
schemes for participatory sensing,’’ Comput. Netw., vol. 73, pp. 58–71,
Nov. 2014.
[27] A. Liu, Y. Hu, and Z. Chen, ‘‘An energy-efficient mobile target detection
scheme with adjustable duty cycles in wireless sensor networks,’’ Int. J. Ad
Hoc Ubiquitous Comput., vol. 22, no. 4, pp. 203–225, 2016.
[28] Y. Liu, M. Dong, K. Ota, and A. Liu, ‘‘ActiveTrust: Secure and trustable
routing in wireless sensor networks,’’ IEEE Trans. Inf. Forensics Secur.,
vol. 11, no. 9, pp. 2013–2027, Sep. 2016.
[29] C.-K. Tham and T. Luo, ‘‘Quality of contributed service and market equilibrium for participatory sensing,’’ IEEE Trans. Mobile Comput., vol. 14,
no. 4, pp. 829–842, Apr. 2015.
[30] A. Liu, X. Liu, and Y. Liu, ‘‘A comprehensive analysis for fair probability
marking based traceback approach in WSNs,’’ Secur. Commun. Netw.,
vol. 9, no. 14, pp. 2448–2475, Sep. 2016.
[31] M. Dong, K. Ota, and A. Liu, ‘‘RMER: Reliable and energy-efficient data
collection for large-scale wireless sensor networks,’’ IEEE Internet Things
J., vol. 3, no. 4, pp. 511–519, Aug. 2016.
[32] J. Long, C. Gao, S. He, X. Liu, and A. Liu, ‘‘Bridging the gap among actor–
sensor–actor communication through load balancing multi-path routing,’’
EURASIP J. Wireless Commun. Netw., vol. 2015, p. 256, Dec. 2015, doi:
10.1186/s13638-015-0484-1.
[33] Z. Song, C. H. Liu, J. Wu, J. Ma, and W. Wang, ‘‘Qoi-aware
multitask-oriented dynamic participant selection with budget constraints,’’ IEEE Trans. Veh. Technol., vol. 63, no. 9, pp. 4618–4632,
Nov. 2014.
[34] S. Yousefi, M. S. Mousavi, and M. Fathy, ‘‘Vehicular ad hoc networks (VANETs): Challenges and perspectives,’’ in Proc. 6th Int. Conf. ITS
Telecommun., Jul. 2006, pp. 761–766, doi: 10.1109/ITST.2006.289012.
[35] Y. Saeed, SA. Lodhi, K. Ahmed, ‘‘Obstacle management in VANET using
game theory and fuzzy logic control,’’ Int. J. Commun., vol. 4, no. 1,
pp. 9–13, Jan. 2013.
[36] M. T. Mitchell, Machine Learning. vol. 45. Burr Ridge, IL, USA:
McGraw Hill, 1997, p. 37.
[37] C. Bishop, Pattern Recognition and Machine Learning (Information Science and Statistics), 2nd ed. 2007.
[38] I. Boussaïd, J. Lepagnot, and P. Siarry, ‘‘A survey on optimization metaheuristics,’’ Inf. Sci., vol. 237, pp. 82–117, Jul. 2013.
[39] B. Suman and P. Kumar, ‘‘A survey of simulated annealing as a tool for
single and multiobjective optimization,’’ J. Oper. Res. Soc., vol. 57, no. 10,
pp. 1143–1160, Oct. 2006.
[40] T. Tanaka, T. Suzuki, and K. Kurihara, ‘‘Energy harvesting technology for
maintenance-free sensors,’’ Fujitsu Sci. Technol. J., vol. 50, pp. 93–100,
Jan. 2014.
[41] D. G. Leeper, ‘‘A long-term view of short-range wireless,’’ Computer,
vol. 34, no. 6, pp. 39–44, Jun. 2001.
[42] J. M. Rabaey, M. J. Ammer, J. L. da Silva, D. Patel, and S. Roundy, ‘‘Picoradio supports ad hoc ultra-low power wireless networking,’’ Computer,
vol. 33, no. 7, pp. 42–48, Jul. 2000.
[43] W. Ye, J. Heidemann, and D. Estrin, ‘‘An energy-efficient MAC protocol
for wireless sensor networks,’’ in Proc. INFOCOM, vol. 3. Jun. 2002,
pp. 1567–1576.
[44] C. Wang, D. Mu, F. Zhao, J. W. Sutherland, ‘‘A parallel simulated annealing method for the vehicle routing problem with simultaneous pickup–
delivery and time windows,’’ Comput. Ind. Eng., vol. 83, pp. 111–122,
May 2015.
[45] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge, U.K.:
Cambridge Univ. Press, 2004.
[46] S. He, J. Chen, F. Jiang, D. K. Y. Yau, G. Xing, and Y. Sun, ‘‘Energy provisioning in wireless rechargeable sensor networks,’’ IEEE Trans. Mobile
Comput., vol. 12, no. 10, pp. 1931–1942, Oct. 2013.
[47] J. Yuan, Y. Zheng, X. Xie, and G. Sun, ‘‘Driving with knowledge
from the physical world,’’ in Proc. 17th ACM SIGKDD Int. Conf.
Knowl. Discovery Data Mining KDD, New York, NY, USA, 2011,
pp. 316–324.
6501

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

ZHIPENG TANG is currently pursuing the degree
with the School of Information Science and Engineering, Central South University, China. His
research interests include services-based network,
crowd sensing networks, and wireless sensor
networks.

ANFENG LIU received the M.Sc. and Ph.D.
degrees in computer science from Central South
University, China, in 2002 and 2005, respectively. He is currently a Professor with the School
of Information Science and Engineering, Central
South University, China. He is also a member
(E200012141M) of China Computer Federation.
His major research interest is wireless sensor
network.

6502

CHANGQIN HUANG received the Ph.D. degree
in computer science and technology from Zhejiang
University, Hangzhou, in 2005. He is a currently
a Professor with the School of Information Technology in Education, South China Normal University, China. He is also a Guangdong Specially
Appointed Professor (Pearl River Scholar), and a
Senior Member (E200014100S) of China Computer Federation. He has authored over 80 research
papers in international journals and conferences.
His research interests include service computing, cloud computing, semantic
web, and education informationization.

VOLUME 4, 2016

SPECIAL SECTION ON GREEN COMMUNICATIONS AND NETWORKING FOR 5G WIRELESS

Received May 26, 2016, accepted June 12, 2016, date of publication June 23, 2016, date of current version August 4, 2016.
Digital Object Identifier 10.1109/ACCESS.2016.2584088

Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications
LUTAO LIU, (Member, IEEE), AND HUAN LIU
College of Information and Telecommunication Engineering, Harbin Engineering University, Harbin 150001, China

Corresponding author: L. Liu (liulutao@msn.com) This work was supported in part by the Fundamental Research Funds for the Central Universities under Grant HEUCF1608 and in part by the National Natural Science Foundation of China under Grant 61201410 and Grant 61571149.

ABSTRACT In a multipath communication scenario, it is often relevant to estimate the directions and relative delays of each multipath signal. We present an effect algorithm for the simultaneous estimation of these parameters by re-iterative super-resolution, beamforming, and MUSIC-like searching techniques. The algorithm first separates and estimates direction of arrival (DOA) of the multipath signals. Linear constrained minimum power beamforming is used to obtain the transmitted time function of the desired signal in certain incident angle. Then, time difference of arrival (TDOA) for the incident signals are mapped into phase shifts in the frequency domain. Using the DFT of the desired signal and the received data, we can separate the phase shifts in the frequency domain due to time delay by MUSIC-like searching. At the same time, the pairing of the estimated DOA and TDOA is automatically determined. Computer simulations illustrating the performance of the proposed algorithm with the CramerRao bound are included. INDEX TERMS Wireless communication, array signal processing, joint TDOA-DOA estimation, multipath signal.

I. INTRODUCTION

With the increasing demand for the mobile communication system, innovative approaches are needed to improve the performance for overcoming errors caused by mobile channel. Long Term Evolution (LTE) is considered as one of the promising systems that is suitable to these requirements [1], [2]. As the signal from the User Equipment (UE) to base station undergoes multipath rays of the direct signal, the multipath signals with different DOAs and time delays combined with direct signal will degrade and influence the estimation of the desired signal. Channel estimation of the base station involves the DOAs and time delays parameter estimation for multipath and direct sources transmitted by the UE. So joint DOA and TDOA estimation by the base station is of interest for advanced handover schemes, emergency localization, and potentially many user services [3]. Many space-time processing approaches have been investigated to estimate the DOA and TDOA with the smart array for obtaining the desired signals. In [4] and [5], the approaches require to ML searches with great computation load or need good accurate initial value for the accurate estimation. In [6][8], the algorithms are developed to transform the data in frequency domain and maps delays into phase shifts, and the joint estimation problem is changed to one that can
VOLUME 4, 2016

be solved using 2-D ESPRIT techniques. Joint DOA and TDOA estimation by multi-invariance MUSIC are proposed in [9] and [10]. Those algorithms require the known modulation pulse shape of one symbol to recovery the time delays. And the delay less than the symbol duration can be obtained by the algorithms. In mobile communication, time delays between different paths are most likely to be longer than the symbol rate. The algorithms [11], [12] do not need the known modulation pulse shape instead of the known transmitted signal. The algorithms require the preamble to recovery the time delays between each arrival. In [13], a method is proposed, which associates DOA from the MUSIC algorithm and TDOA from the correlator separatively. It considered multipath signals as independents sources, assumed the delays between the components are larger than the duration of autocorrelation of emitted signals. In this work, a practical method to estimate TDOA associated with DOA information in the presence of multipath without any preamble sequence is proposed. The algorithm is based on an efficient high-resolution scheme that transforms the multi-dimensional estimation involved in two sets of simple one-dimensional (1-D) estimation. To do so, we propose to use the RISR algorithm to estimate DOAs of the multipath signals without forward-backward averaging and
3815

2169-3536 
 2016 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

spatial smoothing [14]. And the LCMP beamforming [15] in conjunction with the received data by the array is used to estimate the time function of the desired signal in certain incident angle. Then time delays of the incident signals are mapped into phase shifts in frequency domain by Discrete Fourier Transformation (DFT). Using the sequence of the desired signal and the received data in frequency domain, we can estimate the time delays of the signals by MUSIC-like peaks searching. The new proposed numerical method is able to associate correctly the DOA from the RISR algorithm and TDOA from the MUSIC-like algorithm. Our major contributions are summary as follows: (1) Use the RISR, beamforming and MUSIC-like techniques in conjunction to realize the DOA and TODA estimation without preamble knowledge (2) The algorithms is suitable of arbitrary array structures (3) Time delay within a fraction of the sampling time can be estimated accurately in frequency domain. To our knowledge, no other existing algorithm is able to jointly estimate of TDOA and DOA of the multipath signals without preamble. The structure of the paper is as follows. In Section II, we begin our discussion by formulating the problem and describing the data model. Section III contains a detailed derivation of the basic steps of the algorithm, including the techniques to obtain the DOAs of multiple sources and to estimate the time function of them. Identifiability of the DOAs and TDOAs using the proposed technique is addressed in Section IV. Section V illustrates the performance using computer simulations. Notation: Vectors (matrices) are denoted by boldface lower (upper) case letters, all vectors are column vectors, superscripts ()T , () , ()H and () denote transpose, conjugate, complex conjugate transpose and Pseudo-inverse respectively, E [] denotes statistical expectation, I is the identity matrix, |  | represents the modulus of a complex number, and  is the Euclidean norm of a vector. The symbol denotes the Hadamard product between two matrices of appropriate size and the symbol  is used to denote convolution operator. product.
II. SYSTEM MODEL

FIGURE 1. Schematic diagram of an antenna array with one impinging signal.

described as
K

y(t ) =
k =1

a(k )sk (t ) + v(t )

(2)

where y(t ) = [y1 (t ), y2 (t ),    , yM (t )]T is the M dimensional snapshot data vector of the array, v(t ) = [v1 (t ), v2 (t ),    , vM (t )]T is the M dimensional noise data vector. The vector-valued function a(k ) is the array response vector (steering vector) for an array of M elements to the k th source signal from the direction k , expressed as follows: a(k ) = [1, e-jk 1 ,    , e-jki ,    , e-jk (M -1) ]T (3)

where the phase shift of the ith element for each narrowband arrival signal can be defined as ki = 2 fc di sin(k ) c. fc is the carrier frequency of the incident signals, c is the speed of light. It is assumed that the signals and noise are stationary, zero mean uncorrelated random processes. Further, the noise vector is Additive Gaussian White Noise (AGWN) with variance  2 .
III. REVIEW OF SOME TECHNIQUES

The proposed algorithm is partly based on the RISR algorithm and beamforming techniques, so it is necessary to make a brief review of them.
A. RISR DOA ESTIMATION ALGORITHM

Consider a linear array composed of M omnidirectional sensors in a base station to receive K (K < M ) narrow-band plane wave signals from directions 1 , 2 ,    K , in which one of the signals is the direct signal and the other K - 1 signals are reflection components. The distance between the first reference element and the i-th element is di in the array with at least two elements spaced at half of the wavelength of radiation sources or closer, as shown in Fig. 1. Signal sd (t ) is defined as the direct signal with fading coefficient d = 1 and time delay d = 0, then the other reflection signals received can be expressed as sk (t ) = k sd (t - k ). (1)

In general, array antenna received signal vector y(t ) formed by the superposition of the multipath signals and noise can be
3816

Many traditional DOA estimation methods are based on the covariance information of spatial samples, and the estimation performance will be significantly decreased when some sources are related in time domain. The essence of Re-Iterative Super-Resolution (RISR) algorithm is the recursive implementation of the minimum mean-square error (MMSE) [16]. When the determined spatial covariance information and the approximate value of the array calibration tolerance are given, RISR determines an MMSE filter bank according to the estimate of the spatial power distribution. Then the MMSE filter bank is applied to the received data for the updated estimation of spatial power distribution which is used to redefine the MMSE filter banks. The two processes above constantly alternates. Finally, the number of signal sources (the prior knowledge of the number of signal source is not needed), their respective DOAs, and their respective magnitudes can be determined automatically.
VOLUME 4, 2016

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

One of the advantages for the algorithm is that it can distinguish the coherent signals received by the array, which often happen in the mobile communication. Given K narrowband signals from the far field which simultaneously incident on the array. After A/D conversion, The l th time sample, which contains superposition of signal and noise can be expressed in vector notation, same as (2)
K

is meaningless in terms of the RISR algorithm because the RISR formulation just works on each snapshot independently or combines power estimates via non-coherent integration. In order to perform the temporally uncorrelated assumptions, the spatial energy distribution matrix can be written as P (l ) = E {s(l )sH (l )} IM (10)

y(l ) =
k =1

xk (l ) + v(l )

(4)

where xk (l ) = a(k )sk (l ). For simply addressing DOA estimation, let k = 2 fc sin k c. Then the steering vector a(k ) in  -space is transformed to  -space as a(k ). In order to determine the DOA of each signal in  -space at the l th sample, we approximate received data y(l ) in (4) with a parameterized version as ~ (l ) = As(l ) + v(l ). y(l )  =y (5)

where I M is an M  M identity matrix. The diagonal elements of P (l ) comprise the power distribution in space. Substituting the noise covariance matrix and spatial power distribution matrix of (10) into (9) yield W (l ) = (AP (l )AH + Rv )-1 AP (l ). (11)

Given the filter bank W (l ), the MMSE estimation of s(l ) can be obtained as ^ (l ) = W H (l )y (l ) s (12)

According to the array configure, the M  N dimensional array manifold matrix A is constructed and defined as A = [a(0), a( ),     1 1 jd1  1 e  =. . . . . . 1 ejdM -1  , a((N - 1) )]  1  ejd1  (N -1) . .  .  ejdM -1 
(N -1)

     (6)

Since the prior knowledge of P (l ) is unknown, the recursive process of the previous estimation is needed. Based on the matched filter bank strategy, the pre-estimate of s(l ) can be determined as ^0 (l ) = AH y(l ). s (13)

So the initial estimation P (l ) subsequently is computed as ^ 0 (l ) = [s ^0 (l )s ^H P 0 (l )] IM . (14)

which consists of N spatial steering vectors corresponding to angles defined over 2 with the angular increment  = 2/N . The angle range of incident signals is separated to N grids. With the parameter N increasing, the quantization effect of the angle becomes better, while the capability to solve the problem of noise, source correlation effects and array modeling errors becomes stronger. The idea of the RISR algorithm is to estimate the Minimum Mean-Square Error through the M  N adaptive filter bank, with the received signal model defined in (5) and the assumed knowledge of spatial complex vector s(l ). The MMSE cost function can be written as J (W ) = s(l ) - W H (l )y(l )
2

^ i (l ) The relation between new MMSE filter bank estimation W ^ i-1 (l ) is and previous estimation P ^ i (l ) = (AP ^ i-1 (l )AH + Rv )-1 AP ^ i-1 (l ), W (15)

then a new MMSE estimation of s(l ) can be expressed as follow ^H ^i (l ) = W s i (l )y (l ) . (16)

.

(7)

The ith spatial power distribution estimation has the same expression form like (14) ^ i (l ) = [s ^i (l )s ^H P i (l )] IM . (17)

Minimizing (7) and the well-known MMSE filter can be obtained as W (l ) = (E {y(l )yH (l )})-1 E {y(l )sH (l )}. (8)

Substituting y(l ) of (5) into (8), with the added assumption that the signal and noise are uncorrelated and statistically independent, then results is found to be W (l ) = (AE {s(l )sH (l )}AH + E {v(l )vH (l )})-1 AE {s(l )sH (l )} (9) where noise covariance matrix Rv = , can be written in the form as  2 I . As a measure of statistical similarity over time, the temporal correlation of the signal sources
VOLUME 4, 2016

Recursive procedure for spatial power distribution estimation 2 ^i (l ) - s ^i-1 (l ) <  , where the  is may be stopped until s a pre-specified threshold or after some pre-determined number of recursions. At the end of the recursion, the diagonal elements of the diagonal matrix P i (l ) is ``RISR spectrum'' and the spatial magnitude distribution consistent with the  -space division for the radiation sources is given. After that, we can calculate the DOAs of the signals recieved by k = arc sin k c . 2 fc (18)
3817

E {v(l )vH (l )}

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

B. LCMP BEAMFORMING

Weighted vector w of LCMP (linear constrained minimum power) beamforming algorithm [15] can be obtained by minimizing the output power, under Mc linear constraint conditions as wH C = gH (19)

A. MUSIC-LIKE ALGORITHM FOR TIME DELAY ESTIMATION

MUSIC is explored into high-resolution time delay estimation though the conventional MUSIC algorithm [17] was proposed to estimate the DOAs of the sources. As the data model description in (1) and (2), the wireless multipath channel containing K path components can be modeled by
K

where w is an M -dimensional vector, gH is an Mc -dimensional vector and C is an M  Mc matrix, the column vectors of which are linearly independent. Assumed that the first column of C corresponds to the steering vector of the signal of interest, and the first element of g is unit, the signal will pass through the filter undistorted. Furthermore the data covariance matrix Ry , which keeps the output power minimum in the constraint condition of (19) is known as [15] Pout = wH Ry w minimize the following function J = wH Ry w + [wH C - gH ] + H [C H w - g], we can obtain  = -g [C
H H H 1 -1 R- y C]

h(t ) =
k =1

a(k )k  (t - k )

(25)

where  (t ) is the dirac pulse. Therefore, the signal model can be rewritten as y(t ) = s(t )  h(t ) + v(t )
K

(26) (27)

=
k =1

a(k )k s(t - k ) + v(t ).

(20)

The DFT expression of y(t ) in (26) can be written as Y (f ) = S (f )H (f ) + V (f )
K

(21)

= S (f )
k =1

a(k )k e-j2 f k + V (f ) (28)

= [A  ] [S (f )b ] + V (f ) (22)

and
H H -1 -1 H -1 wH LCMP = g [C Ry C ] C Ry .

(23)

In our case, we will use LCMP beamforming to estimate the time function of one desired signal with other signals suppressed. According to (19), the distortionless constraint with adding constraints, can be written as: wH a(k ) = 1 wH a(i ) = 0, i  [1, . . . , K ] and i = k (24)

where S (f ), H (f ) and V (f ) are the Fourier transform of s(t ), h(t ) and v(t ), respectively. A = [a(1 ), a(2 ),    , a(K )] ,  = diag{[1 , 2 ,    , K ]} and b = [ej2 f 1 ,    , ej2 f K ]T . The diag{} is the diagonal operator, which form a vector creates a diagonal matrix, whose elements are the elements of the vector. As stated in [18], we transform the data received into frequency domain and construct an M  L matrix Y L = [Y (f1 ), Y (f2 ),    , Y (fL )] = [A  ] B diag{S(f )} + V L = A[, ] BT [f , ] + V L (29) where  L represents the number of different frequency points in frequency domain  A[, ] = A  is an M  K matrix  S(f ) = [S (f1 ), S (f2 ),    , S (fL )] is the L -point DFT of the signal function  V L is an M  L matrix, which presents the noise in frequency domain.  B is a K  L matrix, which can be written as  - j2  f   1 1 e e-j2 f2 1    e-j2 fL 1  e-j2 f1 2 e-j2 f2 2    e-j2 fL 2    B =   . . . .. . . .   . . . . - j 2  f  - j 2  f  - j 2  f  K k L k 1 2 e e  e (30)


IV. PROPOSED ALGORITHM

Our goal is to jointly estimate DOA and TDOA parameters without knowledge of the emitter signal under multipath propagation conditions. Firstly, the RISR algorithm presented in III-A will be applied to separate the coherent signals and determine DOAs of them. Furthermore, the LCMP beamforming is established according to the DOA information obtained. The optimal beamformer can maximize the ratio of interest output signal to the sum power of interference (other signals) and noise. It is also considered as a spatial filter, which makes the useful signal pass and make the output power of the noise and interference as small as possible. Next, we will find the delays of different impinging signals in the array. MUSIC-like searching method is used to estimate the delays through the orthogonality principle between the signal subspace and the noise subspace. There are different peaks in the searching which are related to the delays of arrival, associated each delay with its proper DOA.
3818

B[f , ] = [B diag{S(f )}]T is a L  K matrix, the ith column of which can be expressed as bi = [S (f1 )e-j2 f1 i ,    , S (fL )e-j2 fL i ]T . (31)

VOLUME 4, 2016

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

As the analysis above, the transpose of Y L can be expressed as
T T YT L = B[f , ] A[, ] + V L 

  = [b1 , b2 ,    , bK ]  

K aT (K )

1 aT (1 ) 2 aT (2 ) . . .

    + VT L.  (32)

Comparing it with (2), Y T L in (32) behaves like the received signal at an array whose manifold matrix is B[f , ] and the equivalent signal matrix is represented by AT [, ] . The columns of B[f , ] behave like the steering vectors in an array with L virtual sensors. The correlation matrix of Y T L can be written as
T RYY = E [Y T L YL H

]
H

T H H T T = B[f , ] E [AT [, ] (A[, ] ) ]B[f , ] + E [V L V L

] (33)

= B[f , ] P [, ] BH [f , ] + RVV

conventional time delay estimation algorithms take advantage of the cross-relation between the transmitted signal and the received signal while Rayleigh restriction limits the resolution of these algorithms. The MUSIC-like time delay estimation algorithm has more high resolution for delay estimation, which can distinguish the multipath signal components of the time intervals below the Rayleigh limit. The specific steps are as follows: 1) Calculate the angles of incidence signals by RISR algorithm, and pay attention to select the appropriate number of iterations. According to the observation, RISR can always reach steady-state less than 15 iterations, regardless of the number of signal source or the spatial structure of the array. 2) Apply Beamforming technique to one of the interest directions obtained in step 1), by phasing the array to steer the main lobe in the specific direction k so that all the received signals except the specific direction can be eliminated. In narrowband beamforming, a complex weight is applied to the signal at each sensor and summed to form the beamformer output sk (t ) = wH (k )y(t ). (37)

T H where P [, ] = E [AT [, ] (A[, ] ) ] is the covariance matrix with rank K . Assumed L > K , the rank of matrix RYY will be K without noise effect. We can apply EVD(eigenvalue decomposition) to RYY

RYY = U S

H SUS

+ UN

H N UN

(34)

where S stands for a K  K diagonal matrix whose diagonal elements are the largest K eigenvalues, corresponding to U S = [u1    uK ], called signal subspace. And N stands for a diagonal matrix whose diagonal elements are composed of the smallest L - K eigenvalues, corresponding to U N = [uK +1    uL ], called noise subspace. By utilizing the orthogonality principle between the signal subspace and the noise subspace, we can get the time delay estimation of the sources in frequency domain by MUSIC-like searching as F ( ) = 1 bH U N U H Nb (35)

We make w = wLCMP which is calculated as (23), the sk (t ) in the direction k will be well recovered . 3) MUSIC-like algorithm is applied to the time delay estimation. As described in IV-A. The arbitrary signal sp (t ), p  [1,    , K ], obtained from the previous step of the beamforming can be assumed as a reference signal(maybe not the the direct signal). Then we transform the reference signal to frequency domain, we can obtain Sp (f ) = S (f )e-j2 f p . And construct the steering vector of time delay  b = [Sp (f1 )e-j2 f1  ,    , Sp (fL )e-j2 fL  ]T (38)

where b = [S (f1 )e-j2 f1  ,    , S (fL )e-j2 fL  ]T has same form as (31). The estimate of multipath time delay  is  ^ = arg max F


(36)

Substituting of (38) into (35), use the spectral searching with respect to  in the specified range of time delay, we can get K peaks, which correspond to the relative delay to the signal Sp (f ). Since sp (t ) = p sd (t - p ) according to (1), so MUSIC-like algorithm will produce spectral peaks on k =  + p in the specific search range, which is relative time delay to p . The real time delay of K path signals would be  = [1 - p ,    , K - p ]. (39) In fact, we always believe that the arrival time d of the direct signal on the antenna is the shortest. Assumed that d = 0, d - p is the minimum element in  . Thus, we have d = arg min[k - p ].
k

Searching the time delay variable  with a constant increment in the delay range by the MUSIC-like method, we finally obtain TDOA of the signals.
B. REALIZATION OF JOINT DOA AND TDOA ESTIMATION BY THE PROPOSED ALGORITHM

The algorithm proposed in this paper has many advantages in DOA estimation, RISR algorithm is applicable to arbitrary array as long as the array manifold is known. And compared with other convectional algorithms [17], [19], RISR can estimate the direction of arrival accurately in the case that the number of multipath sources is unknown. In addition, many
VOLUME 4, 2016

(40)

Finally, the real delay time of each arrival signal to the direct signal can be obtained by the calculation as follow  - (d - p ). (41)
3819

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

If we want to link DOA with the right TDOA, signals in other arrival directions provided by the beamformer are considered as reference signals to estimate relative time delays by the MUSIC-like method, respectively. According to the relations of the positions of spectral peaks in multiple searching, the DOA and TDOA can be paired correctly.
V. SIMULATION RESULTS

In this section, we conduct several simulations to demonstrate the performance of the proposed algorithm. The incident narrow-band multipath signals with carrier frequency fc = 300MHz and sampling rate fs =1GHz under the AWGN background are received by a 10-element uniform linear array (ULA), in which the distance between adjacent elements is half-wavelength. And the number of snapshots is 100. (a) Suppose number of multipaths are K = 3 and incident directions of three related signals are  = [-30 , 15 , 60 ] corresponding to time delays  = [3.2, 0, 6.7]ns. And 1 1 the fading coefficient are  = [0.4ej 3  , 1, 0.7ej 5  ]. We consider that the direct signal comes from the direction of  = 15 . Under the condition of SNR=20dB, RISR algorithm is applied to estimate DOA with phase angle  quantization N = 720, see (6). After iterating 10 times, the pseudo-spectrum for DOA estimation is shown in Fig.2. Three spectral peaks of the spatial energy distribution by RISR are very sharp, corresponding to three estimation values [-29.82 , 14.97 , 60.39 ].

FIGURE 2. DOA estimation in 20dB with RISR pseudospectrum.

FIGURE 3. Pseudo-spectral of time delay estimation in SNR=20dB. ^ = -29.82 as the reference signal function. (b) Signal (a) Signal from  1 ^ = 14.97 as the reference signal. (c) Signal from  ^ = 60.39 as from  1 1 the reference signal.

After getting incident angles of the signals, we need to estimate the time function of signals using the beamforming before realizing the high-resolution time delay estimation of each arrival signal. We firstly choose the signal ^p = -29.82 as the reference signal, from the direction  and use LCMP beamforming in this direction to calculating the desired signal function s ^p (t ) as sp (t ) = p sd (t - tp ). From the pseudo-spectrum of MUSIC-like search in Fig.3(a), we can see three peaks corresponding to the time delay
3820

^ = [-3.2, 0, 3.5]ns, which is the relative delays to the sig ^p = -29.82 . The spectrum peaks of the time nal s ^p (t ) from  delays should be positioned in  = [1 - p , 2 - p , 3 - p ]. ^ is 1 - p = -3.2ns, we can Since the minimum value in  know that real time delays of the signals are [0, 3.2, 6.7]ns ^ = -29.82 with time delay and the reference signal from  3.2ns is not the direct signal. Consider the incident signal
VOLUME 4, 2016

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

^p = 14.97 as the reference signal, the relative delays from  ^ = [0, 3.2, 6.7]ns in Fig.3(b). The signal which to it are  ^ = 14.97 is the direct signal, since the relative comes from  time delay in the three peaks are all positive in this spectrum. From the calculation above, we can also know that the signal of  = 60.39 has the time delay  = 6.7ns. Thus, the TDOA and DOA of each signal is paired. Fig.3(c) shows that delay time of the signals from three directions relative to the third incident angle of  = 60.34 , which is not necessary for pairing the DOA and TDOA in general. In the Fig.4, we show Root Mean Square Error (RMSE) of DOA and time delay estimation for the signal with the Cramer-Rao bound (CRB) [6] versus SNR by 500 Monte Carlo trials.

FIGURE 4. RMSE of DOA and TDOA for the three signal versus SNR.

(b) In order to further illustrate the DOA estimation performance of the proposed algorithm, it is compared with MUSIC [17], spatial smoothing MUSIC [20] and Topelize algorithm [21] in the separation ability for coherent signals. It is also assumed that the number of sources is an prior knowledge and the ULA of 10-element is divided into 7 overlapped sub-arrays (each sub-array have 4 elements) for smoothing MUSIC. In order to show clearly the performance of these algorithms, we reset the incident DOAs of the multipath signals. In Fig.5(a), the incident angle is  = [10 , 13 , 60 ] with the time delay [0, 3.2, 6.7]ns. We can see that MUSIC algorithm can't separate the coherent signals at all; Toeplize algorithm can barely distinguish between two adjacent signals. By comparison, the spatial smoothing and proposed algorithm can clearly distinguish all the signals. With the angle of incidence between the two coherent signals closing, Toeplize algorithm failed in Fig.5(b) and smoothing MUSIC in Fig.5(c). It can be seen that the proposed algorithm can form a substantially deeper null between the two closely spaced sources. Fig.6 shows the performance of the probability of separation with regard to the incident angle difference of two sources in SNR=20dB for the four methods . As we expected, the MUSIC algorithm for separating adjacent spatial sources is invalid when the sources with a strong temporal correlation, other algorithms are able to
VOLUME 4, 2016

FIGURE 5. DOA separation for correlated sources in SNR=20dB. (a)  = 10 , 13 , 60 with  = 0ns, 3.2ns, 6.7ns. (b)  = 10 , 12 , 60 with  = 0ns, 3.2ns, 6.7ns. (c)  = 10 , 11 , 60 with  = 0ns, 3.2ns, 6.7ns.

separate the adjacent sources. And proposed method exhibits the remarkable DOA separation ability among the methods. (c) We compare RMSE of the delay estimates of the proposed method, the Minimum Variance (MV) [22] and the
3821

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

direction of each relevant signal source can be distinguished and the number of the signal sources can be determined clearly. MUSIC-like searching process is used to obtain time delays of the sources by the phase shifts in frequency domain, so time delay within a fraction of the sampling time can be estimated. Monte Carlo simulations show the proposed algorithm has good performance for joint DOA and TOA estimation.
REFERENCES
[1] A. Ghosh, R. Ratasuk, B. Mondal, N. Mangalvedhe, and T. Thomas, ``LTE-advanced: Next-generation wireless broadband technology [invited paper],'' IEEE Wireless Commun., vol. 17, no. 3, pp. 1022, Jun. 2010. [2] N. Abu-Ali, A. M. Taha, M. Salah, and H. Hassanein, ``Uplink scheduling in LTE and LTE-advanced: Tutorial, survey and evaluation framework,'' IEEE Commun. Surveys Tuts., vol. 16, no. 3, pp. 12391265, Dec. 2014. [3] T. S. Rappaport, J. Reed, and B. D. Woerner, ``Position location using wireless communications on highways of the future,'' IEEE Commun. Mag., vol. 34, no. 10, pp. 3341, Oct. 1996. [4] M. Wax and A. Leshem, ``Joint estimation of time delays and directions of arrival of multiple reflections of a known signal,'' in Proc. IEEE ICASSP, Atlanta, GA, USA, May 1996, pp. 26222625. [5] A. L. Swindlehurst, ``Time delay and spatial signature estimation using known asynchronous signals,'' IEEE Trans. Signal Process., vol. 46, no. 2, pp. 449462, Feb. 1998. [6] A.-J. van der Veen, M. C. Vanderveen, and A. Paulraj, ``Joint angle and delay estimation using shift-invariance techniques,'' IEEE Trans. Signal Process., vol. 46, no. 2, pp. 405418, Feb. 1998. [7] S. Al-Jazzar and J. Caffery, ``ESPRIT-based joint AOA/delay estimation for CDMA systems,'' in Proc. IEEE Wireless Commun. Netw. Conf. (WCNC), Atlanta, GA, USA, May 2004, pp. 22442249. [8] J. Picheral and U. Spagnolini, ``Angle and delay estimation of space-time channels for TD-CDMA systems,'' IEEE Trans. Wireless Commun., vol. 3, no. 3, pp. 758769, May 2004. [9] X. Zhang, G. Feng, and D. Xu, ``Blind direction of angle and time delay estimation algorithm for uniform linear array employing multi-invariance music,'' Prog. Electromagn. Res. Lett., vol. 13, no. 2, pp. 1120, Feb. 2010. [10] Y. Wang, J. Chen, and W. Fang, ``TST-MUSIC for joint doa-delay estimation,'' IEEE Trans. Signal Process., vol. 49, no. 4, pp. 721729, Apr. 2001. [11] P. Singh and P. Sircar, ``Time delays and angles of arrival estimation using known signals,'' Signal Image Video Process., vol. 6, no. 2, pp. 171178, 2012. [12] M. Wax and A. Leshem, ``Joint estimation of time delays and directions of arrival of multipath reflections of a known signal,'' IEEE Trans. Signal Process., vol. 45, no. 10, pp. 24772484, Oct. 1997. [13] D. Grenier, B. Elahian, and A. Blanchard-Lapierre, ``Joint delay and direction of arrivals estimation in mobile communications,'' Signal, Image Video Process., vol. 10, no. 1, pp. 4554, Jan. 2016. [14] T.-J. Shan, M. Wax, and T. Kailath, ``On spatial smoothing for directionof-arrival estimation of coherent signals,'' IEEE Trans. Acoust., Speech, Signal Process., vol. 33, no. 4, pp. 806811, Apr. 1985. [15] H. L. Van Trees, Optimum Array Processing: Part IV of Detection, Estimation, and Modulation Theory. New York, NY, USA: Wiley, 2002. [16] S. D. Blunt and K. Gerlach, ``Adaptive pulse compression via MMSE estimation,'' IEEE Trans. Aerosp. Electron. Syst., vol. 42, no. 2, pp. 572584, Apr. 2006. [17] R. O. Schmidt, ``Multiple emitter location and signal parameter estimation,'' IEEE Trans. Antennas Propag., vol. 34, no. 3, pp. 276280, Mar. 1986. [18] J. S. Wang and Z. X. Shen, ``An improved music TOA estimator for RFID positioning,'' in Proc. IEEE Radar Conf., Edinburgh, U.K., Oct. 2002, pp. 478482. [19] R. Roy, ``ESPRIT-estimation of signal parameters via rotational invariance techniques,'' Ph.D. dissertation, Dept. Elect. Eng., Stanford Univ., Stanford, CA, USA, 1987. [20] S. U. Pillai and B. H. Kwon, ``Forward/backward spatial smoothing techniques for coherent signal identification,'' IEEE Trans. Acoust., Speech, Signal Process., vol. 37, no. 1, pp. 815, Jan. 1989.
VOLUME 4, 2016

FIGURE 6. Probability of separation for correlated signals versus

.

FIGURE 7. RMSE of time delay versus SNR.

GP [11] (which utilizes the geometrical properties of the signal in the frequency domain alternately for the time delay variables) algorithms. We reset the incident angles and the corresponding time delays of multipaths same to (a). In Fig.7, it shows RMSE of time delay estimation of the signal from  = 60 as an example with respect to the SNR. The RMSE value is calculated for 500 Monte Carlo simulations. The RMSE of the time delay estimated by the proposed algorithm is less than by utilizing the MV algorithm or GP algorithm. It is proved that the estimation of the proposed method is more effective.
VI. CONCLUSION

The paper proposes a novel algorithm which combines RISR technique along with the LCMP beamforming techniques and MUSIC-like searching algorithm to jointly estimate the DOAs and TDOAs under the multipath propagation conditions. The algorithm transforms the multi-dimensional estimation involved into two simple one-dimensional estimation. The DOA estimation process has super high-resolution based on an iterative scheme, which can be used to separate the relevant signals to any known array manifold without spatial smoothing. According to the obtained RISR spectrum, the
3822

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

[21] K. Takao, N. Kikuma, and T. Yano, ``Toeplitzization of correlation matrix in multipath environment,'' in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., Tokyo, Japan, Apr. 1986, pp. 18731876. [22] J. Vidal, M. Najar, and R. Jativa, ``High resolution time-of-arrival detection for wireless positioning systems,'' in Proc. IEEE 56th Veh. Technol. Conf. (VTC-Fall), vol. 4. Sep. 2002, pp. 22832287.

HUAN LIU was born in Harbin, China, in 1993. She received the B.A. degree in telecommunication engineering from the Harbin University of Science and Technology, China. She is currently pursuing the M.Sc. degree with the College of Information and Telecommunication, Harbin Engineering University, China.

LUTAO LIU was born in Harbin, China, in 1977. He received the B.A. degree in electrical engineering from the Southeast University of China in 2000, the M.Sc. degree in telecommunication engineering from the Harbin Institute of Technology, China, in 2003, the M.Sc. degree in microelectronics from the Delft University of Technology, The Netherlands, in 2005, and the Ph.D. degree in telecommunication engineering from the Harbin Engineering University, China, in 2011. In 2013, he was a Visiting Scholar with the Signal Processing and Communication Laboratory, Stevens Institute of Technology, USA. He is currently an Associate Professor with the College of Information and Telecommunication, Harbin Engineering University, China. His research interests are in the general area of signal processing for telecommunication.

VOLUME 4, 2016

3823

See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/294421534

Protein	Inference:	A	Protein	Quantification
Perspective
Article		in		Computational	biology	and	chemistry	·	February	2016
DOI:	10.1016/j.compbiolchem.2016.02.006

CITATIONS

READS

2

85

6	authors,	including:
Zengyou	He

Ting	Huang

Dalian	University	of	Technology

Dalian	University	of	Technology

87	PUBLICATIONS			1,834	CITATIONS			

9	PUBLICATIONS			64	CITATIONS			

SEE	PROFILE

SEE	PROFILE

Peijun	Zhu

Ben	Teng

3	PUBLICATIONS			2	CITATIONS			

Dalian	University	of	Technology

SEE	PROFILE

6	PUBLICATIONS			14	CITATIONS			
SEE	PROFILE

Some	of	the	authors	of	this	publication	are	also	working	on	these	related	projects:

network	inference,	dense	subgraph	mining	View	project

All	content	following	this	page	was	uploaded	by	Zengyou	He	on	15	February	2016.
The	user	has	requested	enhancement	of	the	downloaded	file.	All	in-text	references	underlined	in	blue	are	added	to	the	original	document
and	are	linked	to	publications	on	ResearchGate,	letting	you	access	and	read	them	immediately.

Computational Biology and Chemistry 00 (2015) 1–14

COMPUT
BIOL
CHEM

Protein Inference: A Protein Quantiﬁcation Perspective
Zengyou Hea,b,∗, Ting Huangc , Xiaoqing Liua , Peijun Zhua , Ben Tenga , Shengchun Dengd
a School of Software, Dalian University of Technology, Dalian, China.
Laboratory for Ubiquitous Network and Service Software of Liaoning, Dalian, China.
c College of Computer and Information Science, Northeastern University, USA.
d School of Computer Science and Engineering, Harbin Institute of Technology, China.

b Key

Abstract
In mass spectrometry-based shotgun proteomics, protein quantiﬁcation and protein identiﬁcation are two major
computational problems. To quantify the protein abundance, a list of proteins must be ﬁrstly inferred from the raw
data. Then the relative or absolute protein abundance is estimated with quantiﬁcation methods, such as spectral
counting. Until now, most researchers have been dealing with these two processes separately. In fact, the protein
inference problem can be regarded as a special protein quantiﬁcation problem in the sense that truly present proteins
are those proteins whose abundance values are not zero. Some recent published papers have conceptually discussed
this possibility. However, there is still a lack of rigorous experimental studies to test this hypothesis.
In this paper, we investigate the feasibility of using protein quantiﬁcation methods to solve the protein inference
problem. Protein inference methods aim to determine whether each candidate protein is present in the sample
or not. Protein quantiﬁcation methods estimate the abundance value of each inferred protein. Naturally, the
abundance value of an absent protein should be zero. Thus, we argue that the protein inference problem can be
viewed as a special protein quantiﬁcation problem in which one protein is considered to be present if its abundance
is not zero. Based on this idea, our paper tries to use three simple protein quantiﬁcation methods to solve the
protein inference problem eﬀectively. The experimental results on six data sets show that these three methods
are competitive with previous protein inference algorithms. This demonstrates that it is plausible to model the
protein inference problem as a special protein quantiﬁcation task, which opens the door of devising more eﬀective
protein inference algorithms from a quantiﬁcation perspective. The source codes of our methods are available at:
http://code.google.com/p/protein-inference/.
c 2016 Published by Elsevier Ltd.
°
Keywords: Shotgun proteomics, Protein inference, Protein quantiﬁcation, Spectral counting, Linear programming.

1. Introduction
Mass spectrometry (MS)-based shotgun proteomics is currently the most widely used method for the
identiﬁcation and quantiﬁcation of proteins (Nesvizhskii et al., 2007). As shown in Figure 1, it ﬁrst digests
proteins in the sample into a mixture of peptides by enzymes such as trypsin. The resulting peptide mixtures
are scanned by tandem mass spectrometry (MS/MS) to generate a set of MS/MS spectra. Then the peptide
identiﬁcation algorithm reports a set of peptide-spectrum matches (PSMs) by searching the MS/MS spectra
∗ Corresponding

author. Tel.: +86 411 62274405. E-mail address: zyhe@dlut.edu.cn (Z. He)

1

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

2

#4/5-6164001
0-78,*4

!" #$

)*+,-./01 213

)-7,.5-1
6.9,:*-

%&#'(
;

!"#$%

3
<)##=( )21%#!-,
!-.#2#-/#

&'(&'
)#*%!+#,
!+#-%!.!/0%!1-

340-%!.!/0%!15&'(&',$*#/%206,
/14-%$7

Figure 1: Protein identiﬁcation and quantiﬁcation using mass spectrometry in shotgun proteomics. There
are three major computational problems: peptide identiﬁcation, protein inference and protein quantiﬁcation.
against a protein database. From these peptide identiﬁcations, we infer the existence of proteins with protein
inference algorithms and calculate the relative or absolute abundances of proteins with protein quantiﬁcation
approaches.
Until recently, people tackle the identiﬁcation and quantiﬁcation of proteins as two individual and subsequent tasks: ﬁrst select a subset of proteins that are truly present and then determine the quantities of
these proteins. For both problems, many elegant approaches have been developed in the past decades. The
readers can refer to two recent reviews Huang et al. (2012) and Nikolov et al. (2012) for details.
The starting point of this paper is fact that protein inference can be regarded as a special case of protein
quantiﬁcation. In protein inference, the objective is to generate a binary presence indicator value (1 or
0) for each candidate protein. In this regard, “protein existence inference” is probably more accurate for
describing the original protein inference task. In protein quantiﬁcation or “protein abundance inference”,
the goal is to determine the abundance of each protein. Clearly, if one protein is not present, its abundance
value should be 0. Hence, the protein inference problem can be investigated from the perspective of protein
quantiﬁcation: present proteins are those proteins whose abundance values are not zero. In other words, we
can adopt available protein quantiﬁcation methods directly to solve the protein inference problem. This new
angle may enable a better understanding of the protein inference problem and help in devising improved or
hybrid protein inference methods by borrowing the power from protein quantiﬁcation.
The possibility of exploiting protein quantiﬁcation methods to solve the protein inference problem has
been conceptually discussed in several papers (Dost et al., 2012; Li and Radivojac, 2012). Dost et al. (2012)
used a simple example to show that it is feasible to obtain more accurate protein identiﬁcations with protein
quantiﬁcation methods than traditional parsimonious approaches. Li and Radivojac (2012) also pointed out
that the protein inference problem can be regarded as a special protein quantiﬁcation problem. However,
they argued that existing protein quantiﬁcation methods have not yet reached the accuracy needed for the
wide dynamic range of quantities observed in cellular proteomics. As a result, solving the more general and
diﬃcult quantiﬁcation problem may not provide a more accurate solution for the protein inference problem.
Although people have realized the potential of solving the protein inference problem from a quantiﬁcation perspective, there are still no rigorous and extensive experimental studies to test this hypothesis.
To fulﬁll this void, we empirically demonstrate the feasibility of solving the protein inference problem with
existing protein quantiﬁcation methods in the context of label-free proteomics. In the label-free quantitative proteomics studies, quantiﬁcation methods which are based on peak ion intensities (from MS data)
(Neilson et al., 2011) and spectral counting (from MS/MS data) (Lundgren et al., 2010; Choi et al., 2008)
have been widely used.
2

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

3

Spectral counting measures the abundance of each protein based on the number of MS/MS spectra
that match its constituent peptides. Given the peptide identiﬁcation result, we can directly obtain spectral
counting information since we just need to count the number of MS/MS spectra. In this paper, we use
spectral counting as the quantiﬁcation approach for solving the protein inference problem.
We ﬁrst try two simple spectral counting methods in the literature. In both methods, the protein
abundance is calculated as the sum of peptide abundance values. Their diﬀerence lies in how to handle the
shared peptide. If the abundance of one shared peptide is b and it has k parent proteins, then b is used as its
abundance value in the ﬁrst method while b/k is used as its abundance value in the second method. These two
methods assume that all the candidate proteins are present in the sample. As a result, the abundance value
of each candidate protein will not be zero. However, this assumption contradicts the objective of protein
inference: distinguishing present proteins (abundance6=0) from absent proteins (abundance=0). Thus, we
extend the second linear programming model in (Dost et al., 2012) to distribute the abundance values of
shared peptides automatically in order to shrink the abundance values of absent proteins to zero.
To our knowledge, our paper is the ﬁrst rigorous study with extensive experiments to demonstrate
the feasibility of using protein quantiﬁcation methods for solving the protein inference problem. Such an
attempt connects two important computational problems that have long been investigated separately. The
experimental results show that we can obtain better performance in most data sets even when the most
simple version of spectral counting is utilized. Hence, the advance in protein quantiﬁcation studies will
promote the development of more eﬀective protein inference algorithms.
In Section 2, we describe the details of three methods. Section 3 shows the experimental results on six
data sets. Section 4 presents some discussions and Section 5 concludes the paper.
2. Methods
As shown in the left side of Figure 2, the input of the protein inference problem can be represented as
a tripartite graph G = (X ∪ Y ∪ Z, E1 ∪ E2 ), where X, Y and Z are the set of l MS/MS experimental
spectra, m identiﬁed peptides and n candidate proteins, respectively. For all xi ∈ X, yj ∈ Y , there is an
edge (xi , yj ) ∈ E1 if and only if the spectrum xi matches the peptide yi in the peptide identiﬁcation results.
Similarly, (yj , zk ) ∈ E2 means that the peptide yj is one part of the protein zk . Each MS/MS spectrum
corresponds to one and only one identiﬁed peptide whereas some peptides may have more than one matching
spectrum, such as the peptides y2 and y3 in Figure 2. The relationship between peptides and proteins is
more complex: one candidate protein may have several identiﬁed peptides and each peptide can be shared by
multiple proteins. How to correctly distribute these shared peptides is one of the most challenging problem
in protein inference.
We ﬁrst formulate the protein inference problem as a special protein quantiﬁcation problem. The objective of protein inference is to determine whether each candidate protein is present in the sample. The aim of
protein quantiﬁcation is to estimate the abundance value of each identiﬁed protein. Clearly, if one protein
is not present in the sample, its abundance value should be 0. In this paper, the protein inference problem
is re-visited from the perspective of protein quantiﬁcation through seeking those proteins whose abundance
values are not zero.
To obtain the protein abundance, we start with calculating the peptide abundance. Let bj denote the
abundance value of the peptide yj , which can calculated as the sum of PSM probabilities (or scores):
∑
bj =
ai ,
(1)
(xi ,yj )∈E1

where ai is the probability that the spectrum xi matches the peptide yj . Notice that ai can be also viewed
as the weight of edge (xi , yj ) ∈ E1 , which can be obtained from peptide identiﬁcation algorithms such
as Mascot (Perkins et al., 1999) or post-processing tools such as PeptideProphet (Keller et al., 2002). In
the traditional spectral counting methods, the peptide abundance is simply the number of MS/MS spectra
identiﬁed for each peptide. Here, we generalize this spectral counting method to account for the quality of
PSMs. More precisely, the contribution of each spectrum to the peptide abundance becomes a quantitative
3

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14
:4-;,*<
6!

)-4,.5-0
%!

)*+,-./0

4

'!(
$

!

%$
#

6$
$

6#

'$(

%$

6"

%$

$

%$

#

#

6&
%#
67
68

"

'#(

%"

%$

&

69

%&

123$

$

123$
#

Figure 2: Three approaches for solving the shared peptide problem. y1 and y3 are unique peptides while y2 ,
y4 and y5 are shared peptides. The abundance of peptide yj is represented by bj . We use the peptide y2 as
an example to explain how these three approaches work.
value between 0 and 1 rather than a ﬁxed value of 1. Such an extension is extremely important for protein
inference since it may help us to distinguish between the proteins with the same number of PSMs.
To calculate the protein abundance, we need to distribute the abundance of each peptide to its parent
proteins. The main diﬃculty is how to deal with the degenerate peptide that is shared by more than one
protein since such a peptide can be generated by any subset of its parent proteins (Yang et al., 2013).
There are several approaches for solving the shared peptide problem in protein quantiﬁcation (Zhang et al.,
2010), as shown in the right side of Figure 2. The ﬁrst approach is to simply discard the shared peptides and
only use the unique peptides to calculate the protein abundance. But this approach has one disadvantage:
it causes the loss of information, especially for the proteins whose identiﬁed peptides are all shared peptides.
In Figure 2, if we delete the shared peptide y2 , then the proteins z2 and z3 do not have any identiﬁed
peptides and they would be considered absent in the sample. In fact, at least one of these two proteins
must be present if we assume the existence of peptide y2 . Alternatively, we can use both unique and shared
peptides to estimate the protein abundance. In the second approach, the abundance of the shared peptide
is utilized to calculate the abundance of all its parent proteins. In other words, each peptide is counted
multiple times so that the abundance values of some proteins may be over-estimated. We call this method
“multiple counting” in this paper. For example, the peptide y2 in Figure 2 is counted twice in the second
approach, which means that we artiﬁcially increase the abundance of peptide y2 from b2 to 2 ∗ b2 . The third
approach divides the abundance of the shared peptide into diﬀerent parts and then distributes each part to
one of its parent proteins. This approach ensures that each peptide is “counted” only once. One typical
representative in this category is the “equal division” method, which partitions the peptide abundance into
k equal parts (k is the number of proteins that share this peptide).
Since both multiple counting and equal division are the most popular and simple quantiﬁcation approaches based on spectral counting, we ﬁrst try these two methods and test their performance for the
protein inference task. Note that these two methods have an implicit assumption that the abundance value
of each candidate protein is not zero. However, this assumption does not hold in the context of protein inference since the abundance values of some absent proteins should be zero. Thus, a new linear programming
model is proposed, which can automatically distribute the peptide abundance so as to shrink the abundance
values of some proteins to zero.

4

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

5

2.1. Multiple Counting
In this method, the shared peptides are used in the same way as the unique peptides and receive no
special treatment. The abundance of a protein is simply the sum of abundance values from all its identiﬁed
peptides:
∑
ck =
bj ,
(2)
(yj ,zk )∈E2

where ck is the abundance of protein zk . If the peptide yj has qj parent proteins, then it is counted qj times
and its actual abundance value used in the calculation is qj · bj .
2.2. Equal Division
Diﬀerent from the above method that counts shared peptides multiple times, the equal division method
counts each peptide only once. It equally distributes the abundance of each shared peptide to its parent
proteins:
∑
bj
ck =
,
(3)
qj
(yj ,zk )∈E2

where qj is the number of candidate proteins sharing the peptide yj . If the peptide yj is a unique peptide,
then qj = 1.
2.3. Linear Programming Model
Shared peptides play an important role in both protein inference and protein quantiﬁcation. Dost et al.
(2012) presented a linear programming (LP) model which used shared peptides to estimate the relative
protein abundance. Kim (2012) modiﬁed this LP model to qualify the absolute protein abundance. On the
basis of these attempts, we further extend the LP model and apply it to infer the identities of proteins.
For each identiﬁed peptide yj , the peptide abundance can be computed as:
∑
∑
bj =
detjk · ck =
djk ,
(4)
{k|(yj ,zk )∈E2 }

{k|(yj ,zk )∈E2 }

where detjk ∈ (0, 1) is the detectability of the peptide yj and it represents the probability that the peptide
yj can be identiﬁed in a standard experiment if its parent protein zk is present (Tang et al., 2006). In
order to simplify the model, we introduce a new variable djk to replace the product between the peptide
detectability detjk and the protein abundance ck . Then, djk is interpreted as the abundance that the protein
zk contributes to the peptide yj . The variable djk can serve as the bridge between the peptide abundance and
the protein abundance. On one hand, we can use djk to explain the known peptide abundance. On the other
hand, we can calculate the unknown protein abundance through djk . Therefore, the protein quantiﬁcation
problem is equivalent to ﬁnding an optimal matrix D = (djk ).
According to the above analysis, we propose a new LP model to solve the protein quantiﬁcation problem:
min

tk ,

(5)

djk = 0,

(6)

∀j, k : djk ≤ tk ,

(7)

D

∑

∀j : bj −

n
∑
k=1

{k|(yj ,zk )∈E2 }

∀j, k : djk

{
=0
∼
≥0

if (yj , zk ) ∈
/ E2
else

.

(8)

Constraint (6) forces the predicted peptide abundance to be equal to the observed abundance value.
Constraint (7) is to ﬁnd the maximum value in each column vector dk (the kth column of the matrix D).
5

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

6

Then, minimizing the objective function (the sum of maximum peptide abundance value from each protein)
will shrink the abundance values of some proteins to 0.
After obtaining the matrix D, it is still a non-trivial task to recover the
∑ protein abundance value ck
since the peptide detectability value detjk is unknown. If we assume that {j|(yj ,zk )∈E2 } detjk = 1, then
the protein abundance ck can be calculated as:
∑
∑
ck =
detjk · ck =
djk .
(9)
{j|(yj ,zk )∈E2 }

{j|(yj ,zk )∈E2 }

Notice that the above assumption on the sum of peptide detectability values is generally not true.
Therefore, the calculated value according to Equation (9) is only an estimated value of the true protein
abundance.
Previously, we have introduced a linear programming method, ProteinLP (Huang and He, 2012), to solve
the protein inference problem. The LP model presented in this paper is essentially diﬀerent from ProteinLP
at least in the following ways:
• Our paper is based on the idea that the protein inference problem can be solved as a special protein
quantiﬁcation problem. Here we want to show the possibility of using protein quantiﬁcation methods
to address the protein inference problem. Thus, the LP method in this paper is actually a special
protein quantiﬁcation method, which mainly deals with peptide/protein abundance values. While
ProteinLP focuses on calculating the protein existence probability based on the peptide identiﬁcation
probability values.
• These two methods have diﬀerent assumptions. ProteinLP assumes that one peptide will be absent
if all its parent proteins are not present in the sample. The LP model in this paper is based on the
assumption that the abundance value of a peptide is equal to the sum of the abundance values from
all its parent proteins.
• The variables in these two LP models are diﬀerent. The variable of ProteinLP is a mathematical
transformation of the joint probability that both a protein and its constituent peptide are present in
the sample. The variable in this paper is the abundance that one parent protein contributes to its
constituent peptide.
• The outputs of these two methods are diﬀerent. The output of ProteinLP is the probability that one
protein is present while that of our method is the protein abundance.
• The new LP model does not need any parameters while ProteinLP has to specify a threshold parameter
manually. In order to ﬁnd the proper parameter automatically, ProteinLP still needs to run an
additional parameter selection procedure.
2.4. Converting Scores into Probabilities
After knowing the protein abundance, it is beneﬁcial to convert the abundance into well-calibrated
probability. The main reason is that the probability estimation allows us to select the appropriate threshold
for reporting a set of conﬁdent proteins. In fact, the problem of converting ranking scores into estimated
probabilities has been widely investigated in diﬀerent domains (e.g., Gao and Tan (2006)). In this paper,
we use the method proposed in (Gao and Tan, 2006) to fulﬁll this task.
We ﬁrst estimate the probability pk that the protein zk is present in the sample given its abundance ck :
P r(zk = 1|ck )
P r(ck |zk = 1)P r(zk = 1)
P r(ck |zk = 1)P r(zk = 1) + P r(ck |zk = 0)P r(zk = 0)
1
=
,
1 + exp(−fk )
=

6

(10)

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

where
fk = log

P r(ck |zk = 1)P r(zk = 1)
.
P r(ck |zk = 0)P r(zk = 0)

7

(11)

fk can be considered as a discriminant function which has a Gaussian distribution with equal covariance
matrices (Bishop, 1995). Then, Equation (10) becomes
pk =

1
.
1 + exp(Ack + B)

(12)

Now, we need to estimate the parameters, A and B. Let rk be a binary variable whose value is 1 if the
protein zk is present in the sample and 0 otherwise. Then, R = (r1 , r2 , · · · , rn ) is the presence indicator
vector of n candidate proteins. If we assume that the existence of each protein is independent of other
proteins, the probability of observing R given C is:
P r(R|C) =

n
∏

prkk (1 − pk )1−rk ,

(13)

k=1

where C = {c1 , c2 , · · · , cn }. The optimal parameter values should maximize P r(R|C), i.e., minimize the
following negative log likelihood function:
LL(R|C) =

n
∑

[(1 − rk )(−Ack − B) + log(1 + exp(Ack + B))].

(14)

k=1

Equation (14) is based on the assumption that we have already known the indicator vector R. However,
we do not know such information in the protein inference process. Thus, we consider rk s as hidden variables
and employ the EM algorithm to simultaneously estimate A, B and R.
The EM algorithm utilizes an iterative procedure to estimate the parameter value θ = {A, B}. The
procedure includes two steps: set rks+1 = E(rks |C, θs ) (E-step) and compute θs+1 = arg minθ LL(Rs+1 |C)
(M-step), where s is the iteration index. During the E-step, the unknown vector R is replaced by its expected
value Rs+1 under the current estimated parameter value θs . Since θs is ﬁxed, LL(R|C) is minimized by
setting rk = 0 if Ack + B > 0 or rk = 1 if Ack + B ≤ 0. During the M-step, a new parameter estimation θs+1
is computed by minimizing LL(R|C) given the vector Rs+1 calculated by the ﬁrst step. Since Rs = [rks ] is
ﬁxed, minimizing LL(R|C) with respect to A and B is a two-parameter optimization problem, which can
be solved using the model-trust algorithm described in (Platt, 2000).
In the above score transformation procedure, all proteins share the same set of model parameters. In fact,
the estimated abundance values from diﬀerent proteins are generally not comparable since longer proteins
may tend to have more matched mass spectra than shorter proteins even they have the same quantities.
Therefore, a new model that takes into account more factors such as the length and ionization properties of
proteins should be developed in the future.
3. Experimental Results
To test the performance of quantiﬁcation-based protein inference methods, we have compared our methods with ProteinProphet (Nesvizhskii et al., 2003) and ProteinLP (Huang and He, 2012) on the six datasets.
3.1. Data sets
We choose six publicly available data sets to validate the performance of our methods. The names and
URLs of these data sets are given in Table 1. These six data sets are divided into two categories: three data
sets with reference sets and the other three data sets without reference sets. The ﬁrst three data sets, 18
mixtures (Klimek et al., 2008), Sigma49 (Tabb et al., 2007) and yeast (Ramakrishnan et al., 2009a), have
their corresponding reference sets that contain the ground-truth proteins. The another three data sets, DME
(Brunner et al., 2007), HumanMD (Ramakrishnan et al., 2009b) and HumanEKC (Ramakrishnan et al.,
7

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

8

2009a), do not have such reference sets. For the data sets without reference sets, a target-decoy strategy
is used instead to assess the performance. This strategy searches MS/MS spectra against a hybrid protein
database which is composed of target protein sequences from the original database and the same number
of decoy sequences (Teng et al., 2014). Thus, an identiﬁed protein is considered as a true positive if it is
present in the protein reference set or comes from the target protein database.
Mixture of 18 Puriﬁed Proteins (18 mixtures) and Sigma49 data set. These two data sets
are both generated from the sample of synthetic proteins mixtures. The protein database used for the 18
mixtures data set consists of 1,819 protein sequences, which includes 18 ground-truth proteins and some
contaminant proteins. The database for the Sigma49 data set contains 15,682 Swiss-Prot human protein
sequences.
Yeast data set. Its reference set is available at http://www.marcottelab.org/MSdata/gold/yeast.html.
The protein database includes 6,714 protein sequences.
D. melanogaster data set (DME). The DME data set is produced from the embryonal Kc 167 cell
line of D. melanogaster. We use Flybase (release 5.2) as the protein database, which contains 20,726 entries.
HumanMD data set and HumanEKC data set. The HumanMD data set is generated from
medulloblastoma Daoy cell line and the HumanEKC data set is produced from human embryonic kidney
T293 cell line. We use Ensembl (version 49.36k) as the protein database, which has 22,997 entries.
Table 1:

The data sets used in the experiment and their URLs.

Data Set

The URL of Raw Data

Mixture of 18 Puriﬁed Proteins (Klimek et al., 2008)

http://regis-web.systemsbiology.net/PublicDatasets/

Sigma49 Data Set (Tabb et al., 2007)

https://proteomecommons.org/dataset.jsp?i=71610

Yeast Data Set (Ramakrishnan et al., 2009a)

http://www.marcottelab.org/users/MSdata/Data_02/

D. melanogaster Data Set (Brunner et al., 2007)

http://www.peptideatlas.org/repository/ (PAe001349)

HumanMD Data Set (Ramakrishnan et al., 2009b)

http://www.marcottelab.org/MSdata/Data_05/

HumanEKC Data Set (Ramakrishnan et al., 2009a)

http://www.marcottelab.org/MSdata/Data_07/

3.2. Peptide Identiﬁcation
We use X!Tandem (v2010.10.01.1) (Craig and Beavis, 2004) for peptide identiﬁcation with default search
parameters. For the data sets with the reference sets, the MS/MS spectra are only searched against the target
protein databases. For the data sets without the reference sets, the spectra are searched against both target
and decoy protein databases. The peptide identiﬁcation results are post-processed with PeptideProphet
(Trans-Proteomic Pipeline v4.5) to obtain the presence probability for each peptide.
3.3. Protein Inference
We choose ProteinProphet and ProteinLP as the competing methods. ProteinProphet is the most
popular method for protein inference so far. ProteinLP is one representative method that is also based on
linear programming. We run ProteinProphet with its default parameter setting and run ProteinLP with
parameter ² = 0. Since some distinct proteins may have the same set of identiﬁed peptides, we cannot
distinguish these proteins from each other without further evidence. Therefore, all the protein inference
methods in the experiments will put these indistinguishable proteins into the same group. Each group of
indistinguishable proteins is treated as a single protein during the protein inference procedure. When we
evaluate the performance of diﬀerent methods, we count all proteins in each group and use the presence
probability of each group as the identiﬁcation probability for proteins in that group.
3.4. Results
We use the curve that shows the number of true positives as a function of the q-value to assess the
performance of diﬀerent methods. Given a certain probability threshold t, the q-value is the minimal
false discovery rate (FDR) that is reported for a protein: qt = mint0 ≤t F DRt0 . The FDR is estimated
8

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

9

Sigma49

18 mixtures

45

20

Number of true positives

Number of true positives

40
15

10
PQ−1
PQ−2
5

PQ−3
PLP

35
30
25
20
PQ−1
15

PQ−2

10

PQ−3
PLP

5

PP
0

PP

0
0

0.1

0.2

0.3

0.4

0.5

0

0.05

0.1

0.15

q−value

0.2
q−value

0.3

0.35

800

1000

700

800
600
PQ−1
400

PQ−2

200

PQ−3
PLP

600
500
400
PQ−1

300

PQ−2
200

PQ−3
PLP

100

PP

PP

0
0

0.02

0.04

0.06

0.08

0.1

0
0

0.02

0.04

q−value

0.06

0.08

0.1

q−value

HumanMD

HumanEKC

350

700

300

600

250
200
150

PQ−1

100

PQ−2

50

PQ−3
PLP

Number of true positives

Number of true positives

0.4

DME

1200

Number of true positives

Number of true positives

Yeast

0.25

500
400
300

PQ−1

200

PQ−2

100

PQ−3
PLP

PP

PP

0

0
0

0.02

0.04

0.06

0.08

0.1

q−value

0

0.005

0.01

0.015
q−value

0.02

0.025

0.03

Figure 3: The comparison of identiﬁcation performance among ProteinLP (PLP), ProteinProphet (PP) and
our own three methods: multiple counting (PQ-1), equal division (PQ-2) and linear programming (PQ-3).
If some proteins have the same probability in the ordered protein list, we skip these proteins with the same
probability and calculate the q-value at the ﬁrst encountered protein with a diﬀerent probability.
as F DRt0 = Ft0 /(Ft0 + Tt0 ), where there are Tt0 true positives (TPs) and Ft0 false positives (FPs) with
0
probabilities ≥ t .
Figure 3 displays the number of TPs reported by the ﬁve methods at diﬀerent q-values. It shows that
our methods are competitive with available protein inference algorithms. Throughout the six data sets,
our three methods can always achieve zero FPs among the highest ranking proteins while the other two
algorithms do not have such a property. This fact indicates that our methods have more strong distinction
power than existing methods. More speciﬁcally, we have the following important observations.
9

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

10

First, the multiple counting method is the best performer on the HumanMD and HumanEKC data sets.
For the HumanMD data set, it reports the largest number of TPs when the q-value is equal to 0. For
the HumanEKC data set, it just identiﬁes 17 fewer true positives than ProteinProphet at q-value=0. Even
though the multiple counting method does not keep such excellent performance on the 18 mixtures, Sigma49
and DME data sets, it never performs the worst.
Second, equal division is the best performer (or tied with other algorithms) on the 18 mixtures, Sigma49
and yeast data sets. Similarly, when the q-value is equal to 0, it identiﬁes the more TPs than other methods
on the 18 mixtures, Sigma49 and yeast data sets. For the HumanMD data set, equal division does not have
the worst performance. For the HumanEKC data set, the curve of equal division is almost tied with the
curve of our LP model, ProteinProphet and ProteinLP and the gaps among these four methods are very
small.
Third, our LP model exhibits the most stable identiﬁcation performance among these ﬁve methods.
More precisely, it does not perform the worst across all six data sets. ProteinLP also has such a property,
but its performance is worse than three algorithms on the 18 mixtures and Sigma 49 data sets. In contrast,
there is only one time that the performance of our LP model is worse than three algorithms (on the DME
data set). The other three methods perform the worst on at least one data set. The number of data sets is
1, 2, 3 for multiple counting, equal division and ProteinProphet, respectively.
In the calculation of protein abundance, we generalize the number of MS/MS spectra to the sum of PSM
probabilities. We wish such an extension may help us to distinguish between proteins with the same number
of PSMs and further improve the identiﬁcation performance. Figure 4 describes the performance gain when
the generalized spectral counting is used instead of the traditional spectral counting. The experimental
results of these three methods on the six datasets agree with our expectation: using the sum of PSM probabilities actually performs better than using the number of PSMs. Overall, there are 18 comparison results
since we run our three methods on the six data sets. In these comparisons, the generalized spectral counting
method performs obviously better than traditional spectral counting in 13 comparisons and performs as well
as traditional spectral counting method in the remaining 5 comparisons.
The LP model in this paper is expected to be able to shrink the abundance values of some proteins to
zero. Table 2 shows the eﬀect of shrinkage on the six data sets. We record the number of total candidate
proteins, the number of the proteins whose abundance values are zero and their rate. For the ﬁrst two data
sets generated from simple protein mixtures, there are around 4% proteins with abundance=0 while the
proportion becomes 7% ∼ 8% for the remaining four data sets generated from real samples.
Table 2:

The eﬀect of shrinkage. The percentage of proteins with abundance=0 is deﬁned as the quotient between the

number of proteins with abundance=0 and the number of total candidate proteins.
Number of total candidate proteins
Number of proteins with abundance=0
Percentage of proteins with abundance=0

18 mixtures

Sigma49

Yeast

DME

HumanMD

HumanEKC

49

105

1285

907

414

669

2

4

91

66

34

50

4.1%

3.8%

7.1%

7.3%

8.2%

7.5%

After obtaining the protein abundance, we use an EM algorithm to convert the abundance score into
a well-calibrated probability. Alternatively, we can just normalize the protein abundance by dividing the
maximum of all calculated protein abundance values. The second strategy also gives us a protein score
between 0 and 1 and keeps the holistic distribution of the original protein abundance unchanged. Figure 5
shows the reason why we adopt the more complex probability estimation approach. In this ﬁgure, the
distributions of new scores generated from these two transformation methods are depicted. It is clearly
visible that the probability estimation method is capable of generating a score distribution that is more
close to the uniform distribution than the simple normalization method. This means that the probability
estimation method allows for distinction between diﬀerent proteins on a ﬁne level.

10

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

18 mixtures

18 mixtures

18 mixtures

20

20

20

15

15

15

10

10

10

5

PQ−1

5

PQ−2

NPQ−1

5

0
0.2

40

40

30

30

20

20

0

0.2

0.4

0

Yeast

Sigma49

0.2

PQ−2

10

NPQ−2

NPQ−1

0

0.4

PQ−1

10

NPQ−3

0
0

Sigma49

Sigma49

PQ−3

NPQ−2

11

0

0

0.4

0

Yeast

0.2

0.4

0

0.2

0.4

DME

Yeast

400
1000

40

600

1000

PQ−1

300

NPQ−1

30
400
200

20
PQ−3

10

500

100

NPQ−3

PQ−2

NPQ−1

0
0.2

0.4

DME

0.02

0.04

200

0
0.02

0

0.04

HumanMD

0.02

0.04

HumanMD

300

300

200

200

200

100

0

0.04

0.02

0.04

600

400

400

400

0.02

0.04

PQ−3
NPQ−3
0

0

0.02

0.04

0

0.02

0.04

PQ−3

PQ−2

NPQ−1
0.01

NPQ−2

200

200

0

NPQ−1

HumanEKC

HumanEKC

PQ−1

100
PQ−2

0
0

600

200

100
PQ−1

0
0

HumanEKC

0.005

0.02

300

600

0

0

HumanMD

NPQ−3

400

100

0

0.04

PQ−3

NPQ−2

200

0.02

DME
600

PQ−2

0

0
0

200

NPQ−3

0
0

300

PQ−3

NPQ−2

0
0

500

PQ−1

NPQ−3

NPQ−2
0

0
0

0.005

0.01

0

0.005

0.01

Figure 4: The comparison of identiﬁcation performance between the generalized spectral counting methods
(PQ-1, PQ-2, PQ-3) and the traditional spectral counting methods (NPQ-1, NPQ-2, NPQ-3). The y-axis
is the number of true positives and x-axis is the corresponding q-value (the minimum FDR to report these
proteins). The abbreviations for diﬀerent methods are the same as those in Figure 3.

11

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

18 mixtures

Sigma49

1

Yeast

1

0.8

1

NS

NS

PE

PE

0.8

NS

Score

Score

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0
10

20

30

40

0

50

0

20

Index

40

60
Index

80

100

0

120

0

0.6
Score

Score

Score
0.4

0.4

0.4

0.2

0.2

0.2

800

1200

PE

0.8

0.6

600

1000

NS

PE

0.8

0.6

400
Index

800

NS

PE

0

600
Index

HumanEKC

NS
0.8

400

1

1

200

200

HumanMD

DME
1

0

PE

0.8

0.6

Score

0.6

0

12

0

0
0

100

200
Index

300

400

0

200

400
Index

600

800

Figure 5: The comparison of the score distribution between normalized score (NS) and probability estimation
(PE) when the protein abundance value is generated with the LP model. The scores of all the identiﬁed
proteins are sorted by descending order.

12

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

13

4. Discussions
There have been already more than 20 protein inference algorithms in the literature, whose details are
summarized in several reviews (Huang et al., 2012; Li and Radivojac, 2012; Claassen, 2012). Here we only
discuss two inference methods that are most closely related with our work.
Based on the observation that peptides belonging to the same protein will show a good correlation with
respect to their quantiﬁcation patterns, Lukasse and America (2014) used the correlation of these patterns
to validate peptide to protein matches. BagReg (Zhao et al., 2015) is a learning-based method for protein
inference, which built a classiﬁcation model based on several features such as the number of matched spectra
for each protein. Overall, both methods utilized the quantiﬁcation information in their algorithms rather
than modeled the protein inference problem as a protein quantiﬁcation problem.
The correct assignment of shared peptides to their parent proteins is one of most challenging problems
in protein inference. However, it is generally very diﬃcult to fulﬁll this task since the information included
in the peptide-protein bipartite graph is insuﬃcient for distinguishing correct peptide-protein matches from
incorrect ones. Yang et al. (2013) mathematically investigated the ambiguity that will be induced by the
uncertainty on the assignment of shared peptides. They derived a lower bound and an upper bound on
the protein existence probability. Roughly speaking, all statistical protein inference methods will deliver a
probability value between the lower bound and the upper bound. This partially explains why no methods
can always perform the best in our experiments since all these methods cannot completely resolve the shared
peptide assignment problem. In other words, all existing methods have already reached their theoretical
limitation in protein inference if no supplementary data are provided for facilitating the inference. Therefore,
it is unlikely that one can further improve the identiﬁcation performance by only digging more on the
mathematical formulation of the protein inference problem based on standard input data.
In fact, many researchers have already realized the aforementioned problem and begun to seek solutions by including supplementary information in the protein inference process. That is, in addition to
the standard input data, supplementary data and information such as the single-stage MS data (He et al.,
2010, 2011), peptide detectabilities (Li et al., 2009b; Huang et al., 2013) and protein-protein interactions
(Ramakrishnan et al., 2009a; Li et al., 2009a) are utilized in the protein inference model as well. The use
of extra information from other data sources may overcome the limitation of currently available protein
inference algorithms.
5. Conclusions
Protein inference problem can be regarded as a special protein quantiﬁcation problem. In this paper,
we investigate the feasibility of solving the protein inference problem with existing protein quantiﬁcation
methods in the context of label-free proteomics. The experimental results show that such a new angle
enables us to obtain better identiﬁcation performance even with some simple quantiﬁcation approaches.
We have tested three protein quantiﬁcation methods for solving the protein inference problem. These
three methods can achieve good performance but none of them are consistently the best method on all the
data sets. Thus, it is still necessary to develop better algorithms. In the future work, we plan to try more
quantiﬁcation methods to check if we can further improve the identiﬁcation performance.
Acknowledgements
This work was partially supported by the Natural Science Foundation of China under Grant No. 61572094
and the Fundamental Research Funds for the Central Universities of China (DUT14QY07).
References
Bishop, C. M., 1995. Neural Networks for Pattern Recognition. Oxford University Press, USA.

13

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

14

Brunner, E., Ahrens, C. H., Mohanty, S., Baetschmann, H., Loevenich, S., Potthast, F., Deutsch, E. W., Panse, C., de Lichtenberg, U., Rinner, O., Lee, H., Pedrioli, P. G. A., Malmstrom, J., Koehler, K., Schrimpf, S., Krijgsveld, J., Kregenow,
F., Heck, A. J. R., Hafen, E., Schlapbach, R., Aebersold, R., 2007. A high-quality catalog of the drosophila melanogaster
proteome. Nature Biotechnology 25 (5), 576–583.
Choi, H., Fermin, D., Nesvizhskii, A. I., 2008. Signiﬁcance analysis of spectral count data in label-free shotgun proteomics.
Molecular & Cellular Proteomics 7 (12), 2373–2385.
Claassen, M., 2012. Inference and validation of protein identiﬁcations. Molecular & Cellular Proteomics 11 (11), 1097–1104.
Craig, R., Beavis, R. C., 2004. Tandem: matching proteins with tandem mass spectra. Bioinformatics 20 (9), 1466–1467.
Dost, B., Bandeira, N., Li, X., Shen, Z., Briggs, S., Bafna, V., 2012. Accurate mass spectrometry based protein quantiﬁcation
via shared peptides. Journal of Computational Biology 19 (4), 337–348.
Gao, J., Tan, P.-N., 2006. Converting output scores from outlier detection algorithms into probability estimates. In: IEEE
International Conference on Data Mining. Hong Kong, China, pp. 212–221.
He, Z., Yang, C., Yang et al, C., 2010. Optimization-based peptide mass ﬁngerprinting for protein mixture identiﬁcation.
Journal of Computational Biology 17 (3), 221–235.
He, Z., Yang, C., Yu, W., 2011. A partial set covering model for protein mixture identiﬁcation using mass spectrometry data.
IEEE/ACM Transactions on Computational Biology and Bioinformatics 8 (2), 368–380.
Huang, T., Gong, H., Yang, C., He, Z., 2013. Proteinlasso: A lasso regression approach to protein inference problem in shotgun
proteomics. Computational Biology and Chemistry 43, 46–54.
Huang, T., He, Z., 2012. A linear programming model for protein inference problem in shotgun proteomics. Bioinformatics
28 (22), 2956–2962.
Huang, T., Wang, J., Yu, W., He, Z., 2012. Protein inference: A review. Brieﬁngs in Bioinformatics 13 (5), 586–614.
Keller, A., Nesvizhskii, A. I., Kolker, E., Aebersold, R., 2002. Empirical statistical model to estimate the accuracy of peptide
identiﬁcations made by MS/MS and database search. Analytical Chemistry 74 (20), 5383–5392.
Kim, D. H., 2012. Deconvolution of PPI networks: Approximation algorithms and optimization techniques. Ph.D. thesis, McGill
University.
Klimek, J., Eddes, J. S., Hohmann, L., 2008. The Standard Protein Mix Database: A diverse data set to assist in the production
of improved peptide and protein identiﬁcation software tools. Journal of Proteome Research 7 (1), 96–103.
Li, J., Zimmerman, L. J., Park, B.-H., Tabb, D. L., Liebler, D. C., Zhang, B., 2009a. Network-assisted protein identiﬁcation
and data interpretation in shotgun proteomics. Molecular Systems Biology 5, 303.
Li, Y. F., Arnold, R. J., Li, Y., Radivojac, P., Sheng, Q., Tang, H., 2009b. A Bayesian approach to protein inference problem
in shotgun proteomics. Journal of Computational Biology 16 (8), 1–11.
Li, Y. F., Radivojac, P., 2012. Computational approaches to protein inference in shotgun proteomics. BMC Bioinformatics
13 (Suppl 16), S4.
Lukasse, P. N., America, A. H., 2014. Protein inference using peptide quantiﬁcation patterns. Journal of proteome research
13 (7), 3191–3199.
Lundgren, D. H., Hwang, S.-I., Wu, L., Han, D. K., 2010. Role of spectral counting in quantitative proteomics. Expert Review
of Proteomics 7 (1), 39–53.
Neilson, K. A., Ali, N. A., Muralidharan, S., Mirzaei, M., Mariani, M., Assadourian, G., Lee, A., van Sluyter, S. C., Haynes,
P. A., 2011. Less label, more free: Approaches in label-free quantitative mass spectrometry. Proteomics 11 (4), 535–553.
Nesvizhskii, A. I., Keller, A., Kolker, E., Aebersold, R., 2003. A statistical model for identifying proteins by tandem mass
spectrometry. Analytical Chemistry 75 (17), 4646–4658.
Nesvizhskii, A. I., Vitek, O., Aebersold, R., 2007. Analysis and validation of proteomic data generated by tandem mass
spectrometry. Nature Methods 4 (10), 787–797.
Nikolov, M., Schmidt, C., Urlaub, H., 2012. Quantitative mass spectrometry-based proteomics: an overview. Methods in
Molecular Biology 893, 85–100.
Perkins, D. N., J.Pappin, D., M.Creasy, D., Cottrell, J. S., 1999. Probability-based protein identiﬁcation by searching sequence
databases using mass spectrometry data. Electrophoresis 20 (18), 3551–3567.
Platt, J. C., 2000. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In:
Advances in Large Margin Classiﬁers. MIT Press, pp. 61–74.
Ramakrishnan, S. R., Vogel, C., Kwon, T., Penalva, L. O., Marcotte, E. M., Miranker, D. P., 2009a. Mining gene functional
networks to improve mass-spectrometry based protein identiﬁcation. Bioinformatics 25 (22), 2955–2961.
Ramakrishnan, S. R., Vogel, C., Prince, J. T., Wang, R., Li, Z., Penalva, L. O., Myers, M., Marcotte, E. M., Miranker, D. P.,
2009b. Integrating shotgun proteomics and mRNA expression data to improve protein identiﬁcation. Bioinformatics 25 (11),
1397–1403.
Tabb, D. L., Fernando, C. G., Chambers, M. C., 2007. Myrimatch: highly accurate tandem mass spectral peptide identiﬁcation
by multivariate hypergeometric analysis. Journal of Proteome Research 6 (2), 654–661.
Tang, H., Arnold, R. J., Alves et al, P., 2006. A computational approach toward label-free protein quantiﬁcation using predicted
peptide detectability. Bioinformatics 22 (14), 481–488.
Teng, B., Huang, T., He, Z., 2014. Decoy-free protein-level false discovery rate estimation. Bioinformatics 30 (5), 675–681.
Yang, C., He, Z., Yu, W., 2013. A combinatorial perspective of the protein inference problem. IEEE/ACM Transactions on
Computational Biology and Bioinformatics 10 (6), 1542–1547.
Zhang, Y., Wen, Z., Washburn, M. P., Florens, L., 2010. Reﬁnements to label free proteome quantitation: How to deal with
peptides shared by multiple proteins. Molecular & Cellular Proteomics 82 (6), 2272–2281.
Zhao, C., Liu, D., Teng, B., He, Z., 2015. BagReg: Protein inference through machine learning. Computational biology and
chemistry 57, 12–20.

14

View publication stats

Received July 14, 2016, accepted August 1, 2016, date of publication August 17, 2016, date of current version September 28, 2016.
Digital Object Identifier 10.1109/ACCESS.2016.2601167

Robots That Think Fast and Slow: An Example
of Throwing the Ball Into the Basket
TZUU-HSENG S. LI, (Member, IEEE), PING-HUAN KUO, YA-FANG HO, CHIN-YIN LIU,
TING-CHIEH YU, YAN-TING YE, CHIEN-YU CHANG, GUAN-YU CHEN, CHIH-WEI CHIEN,
WEI-CHUNG CHEN, LI-FAN WU, AND NIEN-CHU FANG
National Cheng Kung University, Tainan 701, Taiwan

Corresponding author: T.-H. S. Li (thsli@mail.ncku.edu.tw)
This work was supported in part by the Ministry of Science and Technology, Taiwan, under
Grant MOST 103-2221-E-006-252 and Grant MOST 104-2221-E-006-228-MY2, and in part by the Ministry of Education, Taiwan, within
the Aim for the Top University Project through National Cheng Kung University, Tainan, Taiwan.

ABSTRACT Can a robot think like a human being? Scientists in recent years have been trying to achieve
this dream, and we are also committed to this same goal. In this paper, we use an example of throwing the
ball into the basket to make the robots process with human-like thinking behavior. Such thinking behavior
adopted in this paper is divided into two modes: fast and slow. The fast mode belongs to the intuitional
reaction, and the slow mode represents the complicated cogitation in human brain. This fascinating human
thinking concept is inspired by the book, Thinking, Fast and Slow, which explains the process of the human
brain. In addition, the psychology theories proposed in this book are also adopted to realize the thinking
algorithms, and our experiments verify that the thinking mode of human beings is reasonable and effective
in robots.
INDEX TERMS Anchoring effect, fast and slow systems, FIRA, humanoid robot, learning algorithm,
peak-end rule, psychology.

I. INTRODUCTION

An interesting concept of human thinking has been proposed
in Thinking, Fast and Slow [1], and it explains the process
of the human brain. This concept divides human thinking
into two modes, fast and slow. The fast mode, System 1,
belongs to the intuitional reaction, and the slow mode,
System 2, represents the complicated cogitation in human
brain. However, can a robot think like a human being? Here
we show an example of throwing the ball into the basket to
make the robots implement the human-like thinking behavior.
In this study, we built two humanoid robots,
David Junior and David II [2], to accomplish the experiments. Robots are asked to place on the basketball
field to learn the best shooting motion by the fast-slow
system. The establishing and correcting procedure of System 1 and System 2 are fully presented in the experiments.
In addition, some psychology theories are also adopted
to realize the thinking algorithms [3]–[5]. Our results
demonstrate that though the human thinking behavior is
certainly effective in a robot, peak-end rule [3],[4] and
anchoring effect [5], which are seemingly defective, are
helpful in the procedures. In recent years, many of

5052

intelligent methods are derived from biological effects similar to animals’ distinctive behaviors [6], such as genetic
algorithms [7]–[11], particle swarm optimization [12]–[16],
artificial bee colony optimization [17]–[19], ant colony optimization [20]–[24], or other developed robots with animal
behaviors [25]–[27]. Nevertheless, human thinking is the
most intelligent and complicated type of thinking. The powerful deep learning method [28]–[32] is also inspired by
the process human brain. So it is worth adopting human
psychological theories to develop a novel machine learning
method to solve other complex engineering problems, and
make the machines more intelligent in the future.
Due to the explosion of knowledge, scientists can barely
do research manually today [33]. Even though the processing
speed of computers nowadays is much higher than that of the
human brain, the functions of computers, such as recognition
and navigation, still cannot compare with those of human
beings [34]. And scientists continue to enhance the intelligence of robots to improve our living environment [35]. They
are committing themselves to promoting creativity [36], interaction ability [37], natural language speaking ability [38],
and collective intelligence [39] in the robot, and so on.

2169-3536 
 2016 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

VOLUME 4, 2016

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

Furthermore, brains are the fountain of wisdom of this
world. In order to forage, avoid predators, mate and protect
offspring, the brains of our ancestors became more intelligent
over hundreds of millions of years of evolution [34]. Nevertheless, we still do not understand our brains well enough.
The potential of the human brain is very powerful, but we
have no efficient way to explore it. Therefore, building a
brain-based robot to understand the whole process of the
human brain is important and urgently needed [40]. In the
case of the robot, its artificial brain ought to understand itself
to be able to handle all kinds of missions. In order to be able
to coexist with robots in the future, cognitive algorithms and
the personality of robots can also be achieved through experiments [41]. Furthermore, the machine learning methods of
robots have caused a revolution in education. Psychology,
neuroscience, and machine learning are considered among
the principles of human learning [42]. All in all, the brainbased robot provides a pipeline for exploring the process of
the human brain, and this learning concept can be employed
in other applications. We believe that when the psychological
learning theories of human beings and robots are consummated, the interactive and cooperation mode between humans
and robots will be entirely changed in the future.
The major contributions of this paper are 1) using an example of throwing the ball into the basket to make the robots
process with human-like thinking behavior; 2) adopting the
proposed cognitive learning algorithm to construct the experience curve of the shooting postures for basketball games;
3) unprecedentedly utilizing the concepts of peak-end rule
and anchoring effect to enhance the learning process and final
results; and 4) presenting the feasibility and practicality of the
proposed novel learning algorithm.
This paper is organized as follows. In Section II, the
background knowledge in Thinking, Fast and Slow are introduced. The architectures of the basketball learning system are
described in Section III. The learning methods for the basketball competitionare depicted in Section IV. In Section V,
the experimental results are presented to verify the feasibility
and practicality of the proposed method. The discussions of
the proposed method are addressed in Section VI. Finally,
Section VII concludes the paper.

II. BACKGROUND KNOWLEDGE
A. THE TWO SYSTEMS

From the point of view of psychology, thinking behavior can
be divided into two modes, fast and slow, and psychologists
have been intensely interested in this behavior for several
decades. These two thinking modes are named ‘‘System 1’’
and ‘‘System 2,’’ and they play different roles in the human
brain. These two modes in our brain also have different
personalities and functions; they usually take turns dominating our thinking behavior. System 1 belongs to automated
operation mode, in which the reaction time in this thinking is
very fast and does not involve spending a lot of effort during
the operation. And because it is autonomous, we cannot
VOLUME 4, 2016

control what it does at all. System 2 involves laborious and
time-consuming work; its operation is logical and complicated. We have to spend a lot of extra effort when System 2
is in operation. Therefore, System 2 can also be regarded as
the aspect of rational thinking in the human brain.
Generally speaking, System 1 has the characteristics of
intuitive thinking, and System 2 has the characteristics of
logical thinking. At first glance, System 2 seems to be
much more reliable than System 1. But human beings almost
entirely use System 1 for thinking in daily life. For instance,
we do not have to pay much attention to thinking when we
walk over to the table and drink a cup of coffee. Moreover,
we do not have to think hard about running or riding a bicycle
while doing these actions. In other words, System 1 is the
industrious worker in the brain. Precisely since its operation
does not take much effort, it can run almost all day and
does not get tired at all. On the other hand, System 2 does
not appear frequently. And it usually does not interfere with
determinations made by System 1 or accept decisions easily.
Therefore, System 2 is called ‘‘The Lazy Controller [1].’’
All in all, System 1 and System 2 have different roles and
functions in the brain.
TABLE 1. The operating occasions of System 1 and System 2.

The operating occasions of System 1 and System 2 are
illustrated in Table 1, where System 1 is suitable for handling
simple problems and some reflex actions which need to be
performed quickly, while System 2 is usually used for solving
complex problems. Furthermore, when a decision made by
System 1 is wrong, or the situation is too complex to solve,
System 2 will take over to resolve the problem. Based on the
above, the thinking behavior of human beings is processed
by these two cores. However, they cannot work alone in one
human being because both of them are not perfect at all.
We can obtain the most efficient way to make a right decision
only through the proper division of labor on the part of System
1 and System 2. Therefore, it is important to select a suitable
thinking mode at the appropriate time in our daily lives.
B. PEAK-END RULE

Kahneman’s colonoscopy experiments give interesting
results [1]. As shown in Fig. 1, why does patient A, who has
a shorter pain time, feel more miserable than patient B? The
experimental results tell us a surprising fact: The length of
the process has no effect on the rating. No matter how long
the pain time is, the overall rating only depends on the most
painful moment of the experience and its end. That is to say,
the average intensity of feeling will not be influenced by the
length of the process [3], [4]. Such a phenomenon always
follows the ‘‘peak-end rule.’’
5053

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

FIGURE 1. Kahneman’s colonoscopy experiments [1].

C. ANCHORING EFFECT

The anchoring effect is already widely used in commercial
fields. The promotional advertising of Campbell’s soup in
Sioux City is a good example [1]. The sales with a ‘‘limit of
12 per person’’ are two times larger than the sales with
‘‘no limit per person.’’ No matter whether ‘‘12 cans’’ is the
result of a precise calculation or random generation, this
will become an anchor and produce an anchoring effect subconsciously in consumers. Furthermore, Kahneman’s experiment on ‘‘wheel of fortune [1]’’ has a very amusing result.
Why is the percentage of African nations in the United
Nations related to a wheel of fortune? Participants’ judgments
were influenced by an obviously uninformed number5. These
seemingly absurd results show that the anchoring effect does
absolutely and constantly happen today.
III. ARCHITECTURES OF THE BASKETBALL
LEARNING SYSTEM

Computer analogy involves developing computational models to understand human or animal cognition. With human
beings as an example, the aim of this discussion is to describe
the similarities that might exist between algorithms and the
way people think. This approach can be applied to imitating
the operation of the human brain. In this section, the architectures of the basketball learning system will be described
as follows.
A. ROBOT ARCHITECTURES

Since David Junior is a humanoid robot, he possesses a
human-like appearance. David Junior has 26 Degrees of Freedom (DOF). He weighs 9.6 kg and is 95 cm tall. The material
of mechanism consists of Al-Mg alloy, Acrylonitrile Butadiene Styrene (ABS), and Polyoxymethylene Resin (POM).
Aluminum-magnesium is widely applied to David Junior,
especially in the skeleton, because of the high stiffness
and light weight. However, there are many series of
Al-Mg alloy. The parts, including the hip and ankle, are made
of A7075 alloy since these parts require the highest stiffness
and strength. Other parts such as the arm and body are made
of A6061 alloy. In order to reduce the weight of the robot,
the parts that sustain light loading are made of ABS such as
the hands, head, and so on. All the gears of David Junior
are made of POM rather than steel because the strength of
5054

FIGURE 2. The system architecture of David Junior and David II.

POM is enough for the load of David Junior. David Junior has
26 DOF in total, and most of the joints are actuated by
one motor. However, some joints of the robot such as the
knee sustain high loading, and the power of a single motor
is insufficient for these parts. Therefore, the high loading
parts are actuated by two motors, which are connected by a
synchronization cable or actuated by one motor with a gear
to enhance the torque.
To meet all the requirements of our own robot, the best way
is to design the peripheral circuit board for all the required
parts. The system architecture and the integrated circuit board
are shown in Fig. 2. The architecture of David Junior and
David II includes two main parts: One is the Decision-Making
Center and the other is the Motion Control Center. Besides,
we put an emphasis on the autonomy of the robots, so it is
important that the robots have a brain to decide ‘‘what to do.’’
In the robots, this part is called the Decision-Making Center
and consists of a webcam as the robot’s eye for vision and
a laptop associated with image processing, logical strategies,
and some other computations. If the robot knows what to do,
the commands will be transferred to the next indispensable
unit, which tells the robot ‘‘how to do’’ something. We call
this unit the Motion Control Center.
B. LEARNING ENVIRONMENT

As with athletes, the aim of their training is to help their
actions become intuitive motions. In other words, these practices help them to construct the experience database for
System 1. In the same way, the goal of the issue addressed
here is to build the experience curve to give David Junior
the ability to shoot the ball into the basket from any angle.
System 1 is also an intuitive thinking process. Its thinking
movement is not only very fast but also effortless. In addition,
it can be thought of as the accumulation of the experiences of
human beings. When System 1 is dealing with a problem, it
VOLUME 4, 2016

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

FIGURE 3. Learning environment and control systems of David Junior.

wants to use a look-up table approach to get answers from past
experiences. In this issue, two polynomials represent the rotational speed and angle experimental functions, respectively.
Therefore, the rotational speed and angle decisions made by
System 1 can be obtained easily by inquiring about these two
experimental functions.
David Junior is a humanoid robot, as shown in Fig. 3(b).
He is equipped with a 9-axis inertial measurement unit
including web camera, accelerator, gyroscope, and electronic
compass. Fig. 3(a) shows the learning environment and the
relative position of the basket and two IP cameras. One
camera is mounted on the left side of the basket and is 300 cm
from the basket. The camera is held in place by an aluminum
structure. The other camera is installed in the ceiling and is
positioned directly above the basket, which is 170 cm distant
from the top camera. As for the basket, it is 30 cm in diameter
and is fixed on the plastic structure. Its image data can be
transmitted to David Junior to recognize the shooting error.
Like human beings, David Junior can make decisions using
System 1 or System 2 after these images are captured by the
camera on the robot’s head. If he makes a decision, whether
with System 1 or System 2, his brain will send a message
to the motion control center to perform the corresponding
motions. The server motor control methods of David Junior’s
shooting motion are illustrated in Fig. 3(c). We can use the
speed and position control to modify the trajectory of the ball.
In Fig. 3(d), the upper and lower partitions show the detected
ball path in red, the recognized rim framed in purple, and the
purple circles representing when the ball has gone through
the hoop and been captured by the IP cameras. The red circles
denote the calculated falling position of the ball. After that,
David Junior can recognize the shooting error so as to adjust
his shooting motion for another try.
IV. LEARNING METHODS FOR PLAYING BASKETBALL

The system flowcharts of the proposed learning algorithm
are shown in Fig. 4(a). First, the procedure of the image
processing will be executed. The main goal in this step is
VOLUME 4, 2016

FIGURE 4. The system flowcharts of the proposed learning algorithms.

to get information on the target as long as the object has
been recognized. According to the values of the distance and
direction between the basket and David Junior, he will step
forward, step back, turn left, or turn right autonomously to
meet the requirements of the distance and the direction. For
example, the robot will keep turning left or right until the
angle between the basket and himself is smaller than the
threshold. Before deciding the shooting motions, the robot
will execute the position adjustment depending on the information on the basket. After that, David Junior will utilize
System 1 to decide on the rotational speed and angle he
should perform.
David Junior will shoot the ball according to the rotational speed and angle which has just been obtained from
System 1 or System 2. After the ball is shot, David Junior will
check whether the accuracy meets the requirement or not. The
falling location of the ball is used to calculate the shooting
error, and the error will be the distance between the center of
the basket and the center of the ball. If the ball falls into the
region within the quarter of the basket radius, this is called
a good shot. If the accuracy situation meets the requirement,
the experience curves can be updated. Otherwise, the robot
will go to a step in System 2 to have another try at the same
position.
As long as the accuracy and the times do not meet the
requirement, System 2 will operate. System 2 is logical and is
continuously monitoring the human brain. When a decision
made by System 1 is wrong, System 2 will take over the
thinking behavior and support more detailed and specific
processing to solve the problem. However, if the accuracy
meets the requirement, the experience curves of the rotational
speed and angle will both be updated. The final step is to
check whether the experience curves of the power and the
direction are completed or not. If the experience curves are
5055

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

completely trained, the learning ends. Otherwise, the distance
between the basket and David Junior will change and the
learning algorithm will restart from the beginning.
A. THE MODELS OF SYSTEM 1

The models of System 1 can be defined as the following
equations.
Ssys1 = cs0 + cs1 l + cs2 l 2 + cs3 l 3 + · · · + csn l n
2

3

Asys1 = ca0 + ca1 l + ca2 l + ca3 l + · · · + can l

(1)
n

(2)

where Ssys1 and Asys1 are the output values of the shooting
rotational speed and angle decided on by System 1, and
l denotes the location where the robot stands. cs0 to csn , and
ca0 to can are the coefficients of the fitted experience curves.
The number of the coefficients is n + 1. Because the robot
trains the shooting motions at six positions, in this study, n is
set to be 5. By the experimental points which have existed,
the experience curves can predict the appropriate rotational
speed and angle for shooting.
The flowchart of System 1 is shown in Fig. 4(b). First,
System 1 will check whether the experience curves have been
established or not. As a result, if the experience curves have
been established, David Junior will acquire the next rotational
speed and angle by inquiring about the two experience curves,
respectively. On the other hand, if there are no established
experience curves, random rotational speed and angle will
be performed for shooting the ball. It is worth noting that
random rotational speed and angle are all in the limited ranges
constrained by the specification of the motors.
B. THE METHODS OF SYSTEM 2

The flowchart of System 2 is depicted in Fig. 4(c). The aim
of System 2 is to find the best shooting motion for the robots.
According to past experiences, the robots can use several
methods to estimate the optimal motion. However, the methods of System 2 can be changed. In this paper, three methods
of System 2 (short-term memory, long-term memory, and
peak-end rule) are adopted for the experiments.
The short-term memory method is constructed by simple
artificial intelligence. The robot can only remember the last
experience to adjust the motion of the next shot. The adjustment of the next rotational speed and angle scales depend on
the specification of the server motors, and the correction rule
is described in equations (3) and (4).
 

S(t)−1S3 ×sgn(ey ), if ey > R




S(t)−1S2 ×sgn(ey ), if R ≥ ey  >
 (R/2)

S(t +1) =
ey  > (R/4)
S(t)−1S
×sgn(e
),
if
(R/2)
≥

1
y




S(t),
if ey  ≤ (R/4)
(3)

|e
|
A(t)−1A
×sgn(e
),
if
>
R

3
x
x


A(t)−1A2 ×sgn(ex ), if R ≥ |ex | > (R/2)
A(t +1) =
A(t)−1A
if (R/2) ≥ |ex | > (R/4)

1 ×sgn(ex ),


A(t),
if |ex | ≤ (R/4)
(4)
5056

FIGURE 5. The system flowcharts of the anchoring effect adjustments.

where R represents the radius of the basket, ex and ey are the
errors from the center of the basket to the falling location of
the ball, and S(t + 1) and A(t + 1) are the next output values
of the rotational speed and angle at time t. M S1 , M S2 , M S3 ,
M A1 , M A2 , and M A3 are the adjusting parameters for the
shooting rotational speed and angle.
In aspect of the methods of System 2, both the long-term
memory type and the peak-end rule type can remember all
the past experiences. However, the estimating method of the
long-term memory type is obtained by a regression line, and
this regression line is calculated from all the shooting experiences. Although these three methods all give acceptable
results, the performances of the long-term memory type and
peak-end rule type are better than those of the short-term
memory type. On the other hand, the peak-end rule type just
uses the best and the last experiences to correct the shooting
motion. The calculated amount of the peak-end rule type is
lower than that of the long-term memory type.
C. ANCHORING EFFECT

The anchoring effect has been widely used in the commerce
field. However, this is the first paper which has adopted the
concept in robots. The adjusting process of the anchoring
effect (global anchor) is illustrated in Fig. 5(a). If the voltage
of the batteries is insufficient, the robot will start the adjusting
process of the global anchor. After the robot shoots the ball
using System 1, the shooting error will be calculated. If the
error is not acceptable for the robot, the anchoring effect of
System 2 will be enabled to modify the effect of System 1
by a global anchor. Because the robot cannot ensure the best
position of the global anchor, System 2 will continuously
adjust the global anchor to obtain the most appropriate value.
If the adjustment is completed, then the global anchor will not
be modified until the status of the robot is restored.
VOLUME 4, 2016

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

The adjusting procedure of the anchoring effect (local
anchor) is depicted in Fig. 5(b). Unlike the global anchor,
many local anchors can be added into the experience curves.
In addition, because of the damage to the robot, the robot
cannot throw precisely from some specific positions using
the old System 1. Therefore, if the robot cannot perform
a good shooting motion, a local anchor will be added into
System 1. Then, the anchoring effect of System 2 will start to
adjust the position of the local anchor. After several adjustments, the robot will be able to shoot precisely, and the local
anchor will also be fixed. If all the distances in which the
robot stands are adjusted, the adjustment is completed.
The process of the anchoring effect of System 1 is denoted
in Fig. 5(c). The robot will perform the shooting motion by
the rotational speed and angle from the experience curves.
On the other hand, the process of the anchoring effect of
System 2 is shown in Fig. 5(d). According to past shooting
experiences, a temporary anchor can be added or modified
to update the experience curves. Then the updated System 1
is used to obtain the next shooting motion. Besides, without a complicated retraining process, all the anchors can be
removed to perform the original System 1 if the robot reverts
to the initial condition.
V. EXPERIMENTAL RESULTS

In this section, three parts of the experiments are presented.
Part A is the basketball learning process. A comparison of the
three types of System 2 is shown in Part B. Furthermore, the
tuning processes of anchoring effect are revealed in Part C.

ability to change the ideas produced by System 1, and it
will take over to solve these problems. No matter the desired
shooting motion obtained by System 1 or System 2, the optimal shooting rotational speed and angle will be remembered
by an ‘‘anchor,’’ as shown in the blue circles in Fig. 6(a).
After the memory anchors are remembered, David Junior will
construct a new experience curve and walk forward to another
position for the next learning step. Finally, the learning process will be completed when the robot establishes all the
experience curves at 135 cm. After the learning process, we
place the ball randomly at 12 different positions. Then David
Junior gets the ball and utilizes the established System 1 to
shoot the ball into the basket, as shown in Fig. 6(b).
The learning environment of David II is presented
in Fig. 7. In Fig. 7(c), the trained experience curves are
different from those of David Junior because the robot model
and the shooting motion of David Junior and David II are
different. Furthermore, the effects of the changes in the rotational speed and angle are dependent. In other words, the
increased rotational speed may change the shooting direction,
and the changes in the shooting angle may also influence the
shooting power. Therefore, the experience curves may not be
linear at all. The cognition of human beings may be different,
which is also true for robots. And for different robot models,
various motions, or various tasks, System 1 can be established
in different ways to achieve specific assignments.

A. BASKETBALL LEARNING PROCESS

The primary purpose of the issue is to make the robot learn
the shooting rotational speed and angle by himself. Here we
adopt two robots, David Junior and David II, to accomplish
this task.

FIGURE 7. The learning environment and the results of David II.

B. PEAK-END RULE
FIGURE 6. The learning process and final results.

The construction process of System 1 is illustrated in
Fig. 6(a). The upper and lower figures belong to the
shooting rotational speed and angle, respectively. Initially,
David Junior stands at the position where the distance
between the basket and the robot is 165 cm. In general, David
Junior acts like a human being, and his decisions are instinctively made by System 1 at first. However, the decisions
made by System 1 may not always be reasonable [43]. If
a decision is wrong and unacceptable, System 2 has some
VOLUME 4, 2016

The detailed thinking methods of System 2 can change.
For example, the street parking skills of everyone are different, and they also consider the specific subjects they want for
the driving references. Even though the thinking methods of
System 2 are different, they can still achieve the same goal.
Here we utilize three types of System 2. For the short-term
memory type, the robot can only remember the last shot, and
he only uses the last experience to adjust the shooting motion.
For the long-term memory type, the robot can remember all
the shots, and he can calculate the best shooting motion by
the regression line of all the shooting experiences. For the
5057

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

FIGURE 8. Peak-end rule based System 2.

FIGURE 10. The adjusting process of anchoring effect.

1) GLOBAL ANCHORS

FIGURE 9. The experimental results of three types of System 2.

peak-end rule type, the robot can remember all the shots, but
only the best and the latest shot are considered for finding
the optimal shooting motion, as shown in Fig. 8(a) and (b).
Fig. 9 shows the experimental results of the three types of
System 2. In this experiment, David Junior stands at three
different positions and uses the three types of System 2 to
estimate the optimal shooting motion. For these three cases,
David Junior can obtain acceptable results from all types
of System 2. Obviously, the performances of the long-term
memory and the peak-end rule types are about the same,
but better than the short-term memory type. However, the
calculated amount of peak-end rule type is lower than the
long-term memory type. Consequently, the peak-end rule
type of System 2 is suggested to be applied for this issue. Surprisingly, the concept of peak-end rule, which is seemingly
defective in human beings, is helpful in the process.
C. ANCHORING EFFECT

After the learning process, the shooting motion may change
due to a variety of factors, such as the charge of the batteries,
the aging of the mechanism, the flatness of the ground, and
so on. These factors will change the shooting motion and
cause unexpected results. Besides the experience curves representing the angle and rotational speed with which the robot
should perform, the optimal angles and rotational speeds in
the impression are also assembled by the anchors of the robot.
When the robot’s old cognition already cannot fit in with the
actual environment, it is time to adjust the cognition of the
robot. Based on the adjustability of the anchors in the human
brain, we establish a learning procedure for David Junior by
altering the anchors to overcome these problems. Following
are two types of external anchors the robot may adopt.
5058

When we walk to the brand counters, we naturally feel that
the commodity prices are much more expensive here than
in other areas. This means that in order to meet the reality
of the situation, our consumption anchors improve comprehensively. Similarly, if the robot has low battery voltage, as
shown in Fig. 10(a), the overall shooting rotational speed
will be reduced. Therefore, in order to make the robot meet
our expectations, adding a global anchor to the robot is an
efficient way. All the values will be shifted higher or lower
by the global anchor to correct the shooting motion. According to the shooting errors, David Junior can also adjust the
value of the global anchor. The adjusting process is shown
in Fig. 10(b).
2) LOCAL ANCHORS

When we walk in an environment which is surrounded with
brands, sometimes we wonder why certain commodities are
still cheap there. Therefore, at this time, some anchors of
these certain commodities will be reduced automatically.
Similarly, if the robot is damaged by an accident, as shown in
Fig. 10(c), the robot may not perform well in some specific
cases. Therefore, placing local anchors on the robot is a good
choice for solving this problem. As shown in Fig. 10(d),
the experience curves will be revised precisely by the local
anchors (the orange circles). The adjusting process of the
local anchors is shown in Fig. 10(d).
As basketball players do, if their physical condition is not
very good or the competition environment is not familiar to
them, the players will try to adjust their playing motions to
keep up their skill level. However, if the players come back to
the accustomed environment in good physical condition, they
can simply remove the adjustments and not have to relearn all
of their basketball playing skills to play the game. This is the
same with robots; if all the situations of the robot are restored,
the added anchors can be removed immediately. The robot’s
System 1 will also revert to the initial situation. Besides, in
order to verify the feasibility of the proposed learning concept
VOLUME 4, 2016

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

in other robot models and motions, David II participates in
experiments. David II is also a humanoid robot, but he is
higher than David Junior. In addition, the throwing motion
of David II is different from that of David Junior. No matter
what robot we use, or what throwing motion it performs, the
throwing task scan still be achieved by the proposed learning
concept. Moreover, the models of System 1 and the adjusting
method of System 2 can be modified for different cases. For
the reasons above, the proposed learning concept is practical
and can also be utilized for many applications in the future.
VI. DISCUSSIONS

Previously, the conventional method to achieve this task was
dealing with human-in-the-loop testing, and the tuning time
was about 90 minutes. The proposed method provides an
autonomous learning procedure for a robot, and the learning time is within 20 minutes or even faster. The operating efficiency is much improved, and the robot can shoot
precisely from any distance and direction after the learning
process. In the experiment, the robot can shoot precisely at
12 random positions. Besides, the competition results also
validate the robustness of the proposed method. If the charge
of the batteries or the flatness of the ground is changed,
the anchoring effect method can be immediately adopted
for the correction of the robot’s experience curves. In System 2, the concept of peak-end rule used in the algorithm can
also reduce the computational cost. Since the first international robot competition was held at KAIST, Daejeon, Korea
in 1996, the FIRA RoboWorld Cup [44] has become an
important indicator of international robot contests in the
world. Our laboratory, aiRobots, has been actively involved
in this contest for a decade. Moreover, we have won the
championship in the basketball event for six consecutive
years and the all-round championship for last three years.
Obviously, such an algorithm which mimics human thinking
is feasible and practical. The experiments also verify that the
thinking mode of a human brain is reasonable, and it can also
be applied to make the robot think more like a human being.
The proposed learning algorithm is similar to the internal
and external control loops in a complicated control system.
Nevertheless, in the field of system control, some theories
and control methods also include the analogous concepts of
fast and slow, such as implicit and explicit control [45], slowfast systems in singular perturbation theory [46], and cascade
control [47]. However, these concepts must involve a system
model and so are hard to apply in this work. The autonomic
nervous systems and higher cognitive functions in animals are
one kind of neurological cognition. In this paper, the proposed
algorithm emphasizes the learning of cognitive behaviors to
establish a human-like learning system in a humanoid robot.
In order to reduce the effect of the luminance variations,
HSV [48] color space is used in the searching, and matching process of this work. However, if the complexity of the
searched object is getting higher, the Binary Robust Invariant
Scalable Keypoints (BRISK) [49] can be applied in the future
work. Besides, the model of System 1 can also be changed.
VOLUME 4, 2016

In order to improve the accuracy of the System 1 in the future
work, the established experiences curves can be represented
by Extreme Learning Machines (ELMs) [50], which tries to
make human like machines with minimum training.
For scientists, one of the main goals of robotic development
is for robots to be made to more closely resemble human
beings. The proposed learning method is very close to that of
human thinking, and these thinking behaviors have already
been proved by psychological theories. Moreover, the celebrated phenomena in psychology called peak-end rule and
anchoring effect are also utilized in this cognitive learning
algorithm in an unprecedented way to improve the learning
process and the final results. For a basketball player, the
training process for the shooting motion must be executed
from every orientation. However, David Junior can be trained
in just one direction, and he performs the precise shot in all the
orientations through the proposed method. In the experiments
and competitions, there are many good validations showing
that the proposed novel learning method is feasible and practical, and it indeed enhances the intelligence of the humanoid
robot in the basketball training event.
D. Kahneman began surveying the thinking systems of
human beings in 1979, and he was awarded the Nobel
Memorial Prize in Economics in 2002. In 2011, he wrote
a book, Thinking, Fast and Slow, to systematically address
these research achievements. The current paper continues to
use these appellations, which are unprecedented, to design
a novel learning method for humanoid robots. In this paper,
the theoretical evidence and empirical proofs of the thinking
systems have already been proved by psychologists in many
research studies [3]–[5]. The evidence is quite strong and
exact, and it has matured in psychology in the past few years.
Moreover, these psychological concepts used in basketball
learning events have also been validated in experiments and
robot competitions. As a result, this paper adopts these concepts in reasonable ways. From the standpoint of engineering,
at least the proposed learning algorithm is feasible for realization in a humanoid robot.
Up to now, the literature on basketball learning applications
is still quite scarce. Classical methods have been unable to
solve shooting motion learning from a practical standpoint.
In terms of learning, any method can be utilized for the fixed
shooting position, but these methods are hard to integrate
into System 1 and System 2 since the proposed learning
method is not only a process of finding the best parameter
or solution, but also an entire thinking framework in robot
intelligent systems. In the aspect of the basketball learning
event, the construction procedure of the experience curves
requires two reflecting cores. One is the instinct response
(System 1) for testing; the other one is the rational thinking
(System 2) for supervising and correcting. If and only if
the experience curves are comprehensively established can
the robot shoot precisely from various distances. However,
the general classical methods do not consist of two such
computing cores. Due to this issue, classical methods are hard
to apply in this case. A specific problem may be solved by
5059

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

several methods. However, this is the first paper which has
adopted the concepts of cognitive psychology from Thinking,
Fast and Slow to develop a learning method for a humanoid
robot.
The learning algorithm proposed in this paper is inspired
by the concept of human thinking systems, and this learning
method can be further used in many applications. For example, when a small child is learning to catch an object, he or she
may perform imprecisely at the beginning. This is because
the System 1 of the child is not well established in this state.
Therefore, System 2 will take over the decision process to
‘‘think’’ how to accomplish this task. In the process of dealing
with the correction by System 2, System 1 is also modified
according to the experiences of the child. After several tries,
the System 1 of the child will be well established, and she or
he can finally catch the object intuitively without any misses.
However, this thinking method not only works in human
beings, but also in robots. The ability to catch an object is
an important issue in robotics, and the proposed method can
also be used for this function in future work. This is the first
paper to adopt the concepts of cognitive psychology, which
may provide a different and efficient way to solve learning
problems in the future.
The proposed concept applied in robots is novel, and
the basketball event is chosen for the validations. However,
besides the applications of basketball events and catching an
object, there are still many functions which can adopt this
learning method. As long as the behavior can be executed by
a fitting model (even just a lookup table), the fitting model
can be treated as System 1. Then complicated calculations
(System 2) can be established to monitor and modify the
decisions of System 1. As a result, the learning effect is
already achieved, and this learning method can easily be
utilized in many applications. Moreover, the model of System 1 and the calculating method of System 2 are changeable,
depending on the experimental environments or different
cases. For other applications, the proposed algorithm may
provide a good solution for robotic learning issues in the
future.

the future, and the preliminary step of the investigation of a
robot’s cognitive psychology is demonstrated in this paper.
APPENDIX

In the aspect of controlling the falling position of the ball,
inverse kinematics cannot provide the entire solution for
adjusting the shooting motion because this motion must
involve the speed and angle control of the server motors.
Nevertheless, inverse kinematics still plays an important role
in this study. As shown in the supplementary video, there
are many hand motions David Junior must perform, such as
catching the ball, raising the ball for shooting, and so on.
Except for the final state of the shooting motion, all the hand
motions of David Junior are controlled by inverse kinematics.

FIGURE 11. The coordinate frames and joint definition of David Junior’s
hand.

TABLE 2. D-H parameters of David Junior’s hand.

VII. CONCLUSIONS

A novel psychology learning algorithm has been proposed in
this paper. This algorithm is derived from Thinking, Fast and
Slow by Daniel Kahneman, the Nobel Memorial Prize winner
in 2002 for Economic Science. Like a human being, the developed humanoid robots, can apply the thinking modes, System 1 and System 2, to accomplish the learning process for
basketball games. The concepts of peak-end rule and anchoring effect are unprecedentedly used to enhance the learning
process and final results. What is noteworthy is that these
two phenomena are seemingly defective in human beings, but
are very helpful in the learning procedures. Furthermore, the
performance and practicality of the proposed cognitive learning algorithm has had good validations at FIRA RoboWorld
Cup 2015. By the construction of the cognitive learning algorithm, this concept can be applied to many applications in
5060

The coordinate frames and the joint definition of
David Junior’s hand are illustrated in Fig. 11. According
to these definitions, the Denavit-Hartenberg (D-H) parameters of David Junior’s hand can be established, as shown in
Table 2. Therefore, the transformation matrices can also be
obtained as follows.


cos θi −cos αi sin θi
sin αi sin θi
ai cos θi
sin θi
cos αi cos θi −sin αi cos θi ai sin θi

Hii−1 = 
 0
sin αi
cos αi
di 
0
0
0
1
VOLUME 4, 2016

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket


cos θ1 0
sin θ1
0
 sin θ1 0 − cos θ1 0 


 0
1
0
0
0
0
0
1

◦
cos (θ2 − 90 ) 0
sin (θ2 − 90◦ )
 sin (θ2 − 90◦ ) 0 − cos (θ2 − 90◦ )


0
1
0
0
0
0


sin θ2
0 − cos θ2 0
 − cos θ2 0 − sin θ2 0 



0
1
0
0
0
0
0
1


cos θ3 0
sin θ3
0
 sin θ3 0 − cos θ3 0 


 0
1
0
l3 
0
0
0
1


0 −1 0 0
1
0
0 0


0
0
1 0
0
0
0 1


cos θ4 − sin θ4 0 l4 cos θ4
 sin θ4
cos θ4
0 l4 sin θ4 


 0

0
1
0
0
0
0
1


cos θ5 − sin θ5 0 l5 cos θ5
 sin θ5
cos θ5
0 l5 sin θ5 


 0

0
1
0
0
0
0
1


−1 0 0 0
 0
0 1 0


 0
1 0 0
0
0 0 1


H10 =

H21 =

=

H320 =

0

H33 =

H43 =

H54 =

H65 =


0
0

0
1

Then, we can define the matrix P to represent the transformation matrix H50 . According to the position (Px , Py , Pz )
and the orientation matrix we gave, all the rotational angles of
each server motor can be obtained, as shown in the following
equation.
0

H50 = H10 H21 H320 H33 H43 H54 H65

r11 r12 r13
 r21 r22 r23
=P=
 r31 r32 r33
0
0
0


Px
Py 

Pz 
1

Fig. 12 shows the geometric relationships among the links.
According to the relationships, l05 can be obtained by the
following equations. Then, we can use the cosine theorem to
calculate θ4 , and determine the plus or minus sign of θ4 .


∗ ∗ ∗ Px + l5 × r11

−1  ∗ ∗ ∗ P + l × r 
y
5
21 
H40 = P H54 H65
=
 ∗ ∗ ∗ Pz + l5 × r31 
0 0 0
1
q
l05 = (Px +l5 × r11 )2 +(Px +l5 × r21 )2 +(Px +l5 × r31 )2
VOLUME 4, 2016

FIGURE 12. The geometric relationships among the links.

θ4 = ± cos−1

2 − l2 − l2
l05
4
3
2l3 l4

!

In order to obtain the angles of α and β, l0P must first be
calculated. As depicted in Fig. 12(a), we can also figure out
the relationship of θ5 , α and β. Then, the plus or minus sign
of θ5 is determined by θ4 .
q
l0P = P2x + P2y + P2z
!
2
2
2
−1 l05 + l4 − l3
α = cos
2l05 l4
!
2
2
2
−1 l05 + l5 − l0P
β = cos
2l05 l5

θ5 = sgn(θ4 ) × 180◦ − α − β
Considering the following equation, θ3 can be obtained by
the left hand side and right hand side of H30 .

−1
0
H10 H21 H320 H33 = P H43 H54 H65


∗
∗
∗
∗
∗
∗
∗
∗

LHS = 
 ∗ cos (θ2 ) × cos (θ3 ) − cos (θ2 ) × sin (θ3 ) ∗ 
0
0
0
1


∗
∗
∗ ∗
∗
∗
∗ ∗

RHS = 
 ∗ r33 ×cos (θ4 +θ5 ) − r31 ×sin (θ4 +θ5 ) r32 ∗ 
0
0
0 1
−r32
sin (θ3 ) =
cos (θ2 )
r33 × cos (θ4 + θ5 ) − r31 × sin (θ4 + θ5 )
cos (θ3 ) =
cos (θ2 )


−r32
−1
θ3 = tan
r33 × cos (θ4 + θ5 ) − r31 × sin (θ4 + θ5 )
Furthermore, the left hand side and the right hand side of
matrix H20 can be calculated as the following equations.

−1
0
H10 H21 = P H320 H33 H43 H54 H65


∗
sin (θ1 )
∗
∗

∗
− cos (θ1 )
∗
∗

LHS = 
 − cos (θ2 )
∗
− sin (θ2 ) ∗ 
0
0
0
1
5061

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket



∗
∗
RHS = 
c
0

a
b
∗
0

∗
∗
d
0


∗
∗

∗
1

where
a = r11 × sin (θ4 + θ5 ) sin (θ3 )
−r13 × cos (θ4 + θ5 ) sin (θ3 ) − r12 × cos (θ3 )
b = r21 × sin (θ4 + θ5 ) sin (θ3 )
−r23 × cos (θ4 + θ5 ) sin (θ3 ) − r22 × cos (θ3 )
c = r32 × sin (θ3 ) − r33 × cos (θ4 + θ5 ) cos (θ3 )
+r31 × sin (θ4 + θ5 ) cos (θ3 )
d = −r31 × cos (θ4 + θ5 ) − r33 × sin (θ4 + θ5 )
Finally, θ1 and θ2 can be obtained by the following equations.
sin (θ1 ) = a,

− cos (θ1 ) = b


−a
θ1 = tan−1
b
− cos (θ2 ) = c, − sin (θ2 ) = d
 
−1 d
θ2 = tan
c
REFERENCES
[1] D. Kahneman, Thinking, Fast and Slow. New York, NY, USA:
Farrar, Straus, 2011.
[2] T.-H. S. Li, P.-H. Kuo, Y.-F. Ho, M.-C. Kao, and L.-H. Tai, ‘‘A biped gait
learning algorithm for humanoid robots based on environmental impact
assessed artificial bee colony,’’ IEEE Access, vol. 3, pp. 13–26, 2015.
[3] E. Diener, D. Wirtz, and S. Oishi, ‘‘End effects of rated life quality: The
James dean effect,’’ Psychol. Sci., vol. 12, no. 2, pp. 124–128, 2001.
[4] S. Kemp, C. D. B. Burt, and L. Furneaux, ‘‘A test of the peak-end rule
with extended autobiographical events,’’ Memory Cognit., vol. 36, no. 1,
pp. 132–138, 2008.
[5] A. Tversky and D. Kahneman, ‘‘Judgment under uncertainty: Heuristics
and biases,’’ Science, vol. 185, no. 4157, pp. 1124–1131, 1974.
[6] J. O. Kephart, ‘‘Learning from nature,’’ Science, vol. 331, no. 6018,
pp. 682–683, 2011.
[7] J. Holland, Genetic Algorithms. Scientific American, vol. 267, no. 1,
pp. 66–72, 1992.
[8] F. Wang, J. Li, S. Liu, X. Zhao, D. Zhang, and Y. Tian, ‘‘An improved adaptive genetic algorithm for image segmentation and vision alignment used
in microelectronic bonding,’’ IEEE/ASME Trans. Mechatronics, vol. 19,
no. 3, pp. 916–923, Jun. 2014.
[9] P.-H. Kuo, T.-H. S. Li, Y.-F. Ho, and C.-J. Lin, ‘‘Development of an
automatic emotional music accompaniment system by fuzzy logic and
adaptive partition evolutionary genetic algorithm,’’ IEEE Access, vol. 3,
pp. 815–824, 2015.
[10] T.-K. Liu, Y.-P. Chen, and J.-H. Chou, ‘‘Developing a multiobjective optimization scheduling system for a screw manufacturer: A refined genetic
algorithm approach,’’ IEEE Access, vol. 2, pp. 356–364, 2014.
[11] T. K. Liu, Y. P. Chen, and J. H. Chou, ‘‘Solving distributed and flexible jobshop scheduling problems for a real-world fastener manufacturer,’’ IEEE
Access, vol. 2, pp. 1598–1606, 2014.
[12] J. Kennedy and R. Eberhart, ‘‘Particle swarm optimization,’’ in Proc. IEEE
Int. Conf. Neural Netw., 1995, pp. 1942–1948.
[13] Y. Ding, L. Cheng, W. Pedrycz, and K. Hao, ‘‘Global nonlinear kernel
prediction for large data set with a particle swarm-optimized interval
support vector regression,’’ IEEE Trans. Neural Netw. Learn. Syst., vol. 26,
no. 10, pp. 2521–2534, Oct. 2015.
[14] Z. Ren, A. Zhang, C. Wen, and Z. Feng, ‘‘A scatter learning particle swarm
optimization algorithm for multimodal problems,’’ IEEE Trans. Cybern.,
vol. 44, no. 7, pp. 1127–1140, Jul. 2014.
5062

[15] P.-Y. Chou, J.-T. Tsai, and J.-H. Chou, ‘‘Modeling and optimizing tensile
strength and yield point on a steel bar using an artificial neural network
with taguchi particle swarm optimizer,’’ IEEE Access, vol. 4, pp. 585–593,
2016.
[16] Y. Gao, W. Du, and G. Yan, ‘‘Selectively-informed particle swarm optimization,’’ Sci. Rep., vol. 5, Mar. 2015, Art. no. 9295.
[17] D. Karaboga and B. Basturk, ‘‘A powerful and efficient algorithm for
numerical function optimization: Artificial bee colony (ABC) algorithm,’’
J. Global Optim., vol. 39, no. 3, pp. 459–471, Apr. 2007.
[18] W.-F. Gao, S.-Y. Liu, and L.-L. Huang, ‘‘A novel artificial bee colony
algorithm based on modified search equation and orthogonal learning,’’
IEEE Trans. Cybern., vol. 43, no. 3, pp. 1011–1024, Jun. 2013.
[19] T.-J. Hsieh and W.-C. Yeh, ‘‘Knowledge discovery employing grid scheme
least squares support vector machines based on orthogonal design bee
colony algorithm,’’ IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 41,
no. 5, pp. 1198–1212, Oct. 2011.
[20] M. Dorigo, V. Maniezzo, and A. Colorni, ‘‘Ant system: Optimization by
a colony of cooperating agents,’’ IEEE Trans. Syst., Man, Cybern. B,
Cybern., vol. 26, no. 1, pp. 29–41, Feb. 1996.
[21] M. Shen, W.-N. Chen, J. Zhang, H. S.-H. Chung, and O. Kaynak, ‘‘Optimal
selection of parameters for nonuniform embedding of chaotic time series
using ant colony optimization,’’ IEEE Trans. Cybern., vol. 43, no. 2,
pp. 790–802, Apr. 2013.
[22] Y. Zhou, X. Lai, Y. Li, and W. Dong, ‘‘Ant colony optimization with
combining Gaussian eliminations for matrix multiplication,’’ IEEE Trans.
Cybern., vol. 43, no. 1, pp. 347–357, Feb. 2013.
[23] L.-N. Xing, P. Rohlfshagen, Y.-W. Chen, and X. Yao, ‘‘A hybrid ant colony
optimization algorithm for the extended capacitated arc routing problem,’’
IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 41, no. 4, pp. 1110–1123,
Aug. 2011.
[24] X.-M. Hu, J. Zhang, H. S.-H. Chung, Y. Li, and O. Liu, ‘‘SamACO:
Variable sampling ant colony optimization algorithm for continuous optimization,’’ IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 40, no. 6,
pp. 1555–1566, Dec. 2010.
[25] T. Libby et al., ‘‘Tail-assisted pitch control in lizards, robots and
dinosaurs,’’ Nature, vol. 481, no. 7380, pp. 181–184, 2012.
[26] A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret, ‘‘Robots that can adapt
like animals,’’ Nature, vol. 521, no. 7553, pp. 503–507, 2015.
[27] J. Werfel, K. Petersen, and R. Nagpa, ‘‘Designing collective behavior in
a termite-inspired robot construction team,’’ Science, vol. 343, no. 6172,
pp. 754–758, 2014.
[28] Y. LeCun, Y. Bengio, and G. Hinton, ‘‘Deep learning,’’ Nature, vol. 521,
no. 7553, pp. 436–444, 2015.
[29] Y. Park and M. Kellis, ‘‘Deep learning for regulatory genomics,’’ Nature
Biotechnol., vol. 33, no. 8, pp. 825–826, 2015.
[30] W. Hou, X. Gao, D. Tao, and X. Li, ‘‘Blind image quality assessment via
deep learning,’’ IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 6,
pp. 1275–1286, Jun. 2015.
[31] H. Goh, N. Thome, M. Cord, and J.-H. Lim, ‘‘Learning deep hierarchical
visual feature coding,’’ IEEE Trans. Neural Netw. Learn. Syst., vol. 25,
no. 12, pp. 2212–2225, Dec. 2014.
[32] Y. Yuan, L. Mou, and X. Lu, ‘‘Scene recognition by manifold regularized deep learning architecture,’’ IEEE Trans. Neural Netw. Learn. Syst.,
vol. 26, no. 10, pp. 2222–2233, Oct. 2015.
[33] J. Evans and A. Rzhetsky, ‘‘Machine science,’’ Science, vol. 329, no. 5990,
pp. 399–400, 2010.
[34] H. Moravec, Rise of the Robots, Scientific American Reports, vol. 18, no. 1,
pp. 12–19, 2008.
[35] D. J. Cook, ‘‘How smart is your home?’’ Science, vol. 335, no. 6076,
pp. 1579–1581, 2012.
[36] H. J. Briegel and G. De Las Cuevas, ‘‘Projective simulation for artificial
intelligence,’’ Sci. Rep., vol. 2, May 2012, Art. no. 400.
[37] J. Halloy et al., ‘‘Social integration of robots into groups of cockroaches to control self-organized choices,’’ Science, vol. 318, no. 5853,
pp. 1155–1158, 2007.
[38] Y. Wilks, ‘‘Is there progress on talking sensibly to machines?’’ Science,
vol. 318, no. 5852, pp. 927–928, 2007.
[39] M. Rubenstein, A. Cornejo, and R. Nagpal, ‘‘Programmable self-assembly
in a thousand-robot swarm,’’ Science, vol. 345, no. 6198, pp. 795–799,
2014.
[40] G. M. Edelman, ‘‘Learning in and from brain-based devices,’’ Science,
vol. 318, no. 5853, pp. 1103–1105, 2007.
[41] C. Adami, ‘‘What do robots dream of?’’ Science, vol. 314, no. 5802,
pp. 1093–1094, 2006.
VOLUME 4, 2016

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

[42] A. N. Meltzoff, P. K. Kuhl, J. Movellan, and T. J. Sejnowski, ‘‘Foundations
for a new science of learning,’’ Science, vol. 325, no. 5938, pp. 284–288,
2009.
[43] A. Tversky and D. Kahneman, ‘‘The framing of decisions and the psychology of choice,’’ Science, vol. 211, no. 4481, pp. 453–458, 1981.
[44] Website of FIRA, accessed on Jul. 1, 2016. [Online].
http://www.fira.net/main
[45] P. Mazzoni and N. S. Wexler, ‘‘Parallel explicit and implicit control of
reaching,’’ PLoS ONE, vol. 4, no. 10, p. e7557, 2009.
[46] T.-H. S. Li and K.-J. Lin, ‘‘Composite fuzzy control of nonlinear singularly
perturbed systems,’’ IEEE Trans. Fuzzy Syst., vol. 15, no. 2, pp. 176–187,
Apr. 2007.
[47] J. Zhang, S. Zhou, M. Ren, and H. Yue, ‘‘Adaptive neural network cascade
control system with entropy-based design,’’ IET Control Theory Appl.,
vol. 10, no. 10, pp. 1151–1160, 2016.
[48] I. Yoon, S. Kim, D. Kim, M. H. Hayes, and J. Paik, ‘‘Adaptive defogging
with color correction in the HSV color space for consumer surveillance
system,’’ IEEE Trans. Consum. Electron., vol. 58, no. 1, pp. 111–116,
Feb. 2012.
[49] S. Leutenegger, M. Chli, and R. Y. Siegwart, ‘‘BRISK: Binary robust
invariant scalable keypoints,’’ in Proc. IEEE Int. Conf. Comput. Vis.,
Nov. 2011, pp. 2548–2555.
[50] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, ‘‘Extreme learning machine:
Theory and applications,’’ Neurocomputing, vol. 70, nos. 1–3,
pp. 489–501, 2006.

TZUU-HSENG S. LI (S’85–M’90) received the
B.S. degree in electrical engineering from the
Tatung Institute of Technology, Taipei, Taiwan, in
1981, and the M.S. and Ph.D. degrees in electrical engineering from the National Cheng Kung
University (NCKU), Tainan, Taiwan, in 1985 and
1989, respectively. Since 1985, he has been with
the Department of Electrical Engineering, NCKU,
where he is currently a Distinguished Professor.
From 1996 to 2009, he was a Researcher with the
Engineering and Technology Promotion Center, National Science Council,
Tainan. From 1999 to 2002, he was the Director of the Electrical Laboratories, NCKU. From 2009 to 2012, he was the Dean of the College of Electrical
Engineering and Computer Science, National United University, Miaoli City,
Taiwan. He has been the Vice President of the Federation of International
Robot-Soccer Association since 2009. He has been the Director of the Center
for Intelligent Robotics and Automation, NCKU, since 2014. His current
research interests include artificial and/or biological intelligence and applications, fuzzy system and control, home service robots, humanoid robots,
mechatronics, 4WIS4WID vehicles, and singular perturbation methodology.
Dr. Li was a recipient of the Outstanding Automatic Control Award in 2006
from the Chinese Automatic Control Society (CACS) in Taiwan. He was a
Technical Editor of the IEEE/ASME TRANSACTIONS on MECHATRONICS and an
Associate Editor of the Asia Journal of Control. He is currently an Associate
Editor of the International Journal of Electrical Engineering, the International
Journal of Fuzzy Systems, and the IEEE TRANSACTIONS on CYBERNETICS. He
was elected as the President of the CACS from 2008 to 2011 and the Robotics
Society of Taiwan from 2012 to 2015. In 2008, he was elevated to CACS
Fellow.

PING-HUAN KUO was born in Pingtung, Taiwan,
in 1986. He received the B.S., M.S., and Ph.D.
degrees from the Department of Electrical Engineering from National Cheng Kung University,
Tainan, Taiwan, in 2008, 2010, and 2015, respectively. His major research interests include fuzzy
control, intelligent algorithms, humanoid robot,
image processing, and robotic application.

VOLUME 4, 2016

YA-FANG HO was born in Kaohsiung, Taiwan,
in 1988. She received the B.S. and M.S. degrees
with the Department of Electrical Engineering,
National Cheng Kung University, Tainan, Taiwan, in 2010 and 2011. She is currently pursuing
the Ph.D. degree with the Department of Electrical Engineering, National Cheng-Kung University.
Her major research interests include fuzzy control,
intelligent system, humanoid robot, image processing, robotic application, and FIRA/RoboCup
game.

CHIN-YIN LIU received the B.S. degree from the
Department of Social Work from National Taiwan
University, Taipei, Taiwan, in 2003. She received
the M.S. degree with the Department of Psychology from National Chung Cheng University, ChiaYi, Taiwan, in 2005. She is currently pursuing
the Ph.D. degree with the Department of Electrical Engineering, National Cheng-Kung University, Tainan, Taiwan. Her major research interests
include intelligent control system, robot learning,
home service robot, robot cooperation, and human–robot interaction.

TING-CHIEH YU received the B.S. and M.S.
degrees with the Department of Electrical Engineering, National Cheng-Kung University, Tainan,
Taiwan, in 2012 and 2014. Her major research
interests include fuzzy control, intelligent system,
humanoid robot, image processing, robotic application, and FIRA/RoboCup game.

YAN-TING YE received the B.S. and M.S. degrees
with the Department of Electrical Engineering
from National Cheng-Kung University, Tainan,
Taiwan, in 2013 and 2015, respectively. His major
research interests include fuzzy control, intelligent system, humanoid robot, image processing,
robotic application, and FIRA/RoboCup game.

CHIEN-YU CHANG received the B.S. degree
from the Department of Electronic Engineering, Chung Yuan Christian University, Taoyuan,
Taiwan, in 2009, and the M.S degrees with
the Department of Electrical Engineering from
National Cheng-Kung University, Tainan, Taiwan,
in 2016. His major research interests include
fuzzy control, intelligent system, humanoid
robot, image processing, robotic application, and
FIRA/RoboCup game.

5063

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

GUAN-YU CHEN received the B.S. and M.S.
degrees from the Department of Electrical Engineering, National Cheng-Kung University, Tainan,
Taiwan, in 2013 and 2015, respectively. His major
research interests include fuzzy control, intelligent system, humanoid robot, image processing,
robotic application, and FIRA/RoboCup game.

WEI-CHUNG CHEN received the B.S. and M.S.
degrees from the Department of Electrical Engineering, National Cheng-Kung University, Tainan,
Taiwan, in 2014 and 2016, respectively. His major
research interests include fuzzy control, intelligent system, humanoid robot, image processing,
robotic application, and FIRA/RoboCup game.

LI-FAN WU received the B.S. degree from the
Department of Mechanical Engineering, National
Cheng-Kung University, Tainan, Taiwan, in 2015.
He is currently pursuing the M.S. degree with the
Department of Electrical Engineering, National
Cheng-Kung University. His major research interests include fuzzy control, intelligent system,
humanoid robot, image processing, robotic application, and FIRA/RoboCup game.

CHIH-WEI CHIEN received the B.S. and M.S.
degrees from the Department of Electrical Engineering, National Cheng Kung University, Tainan,
Taiwan, in 2014 and 2016, respectively. His major
research interests include fuzzy control, intelligent system, humanoid robot, image processing,
robotic application, and FIRA/RoboCup game.

5064

NIEN-CHU FANG received the B.S. degree
from the Department of Electrical Engineering, National Cheng-Kung University, Tainan,
Taiwan, in 2015. He is currently pursuing the M.S.
degree with the Department of Electrical Engineering, National Cheng-Kung University. His major
research interests include fuzzy control, intelligent system, humanoid robot, image processing,
robotic application, and FIRA/RoboCup game.

VOLUME 4, 2016

SPECIAL SECTION ON GREEN COMMUNICATIONS AND NETWORKING FOR 5G WIRELESS

Received May 26, 2016, accepted June 12, 2016, date of publication June 23, 2016, date of current version August 4, 2016.
Digital Object Identifier 10.1109/ACCESS.2016.2584088

Joint Estimation of DOA and TDOA of Multiple
Reflections in Mobile Communications
LUTAO LIU, (Member, IEEE), AND HUAN LIU
College of Information and Telecommunication Engineering, Harbin Engineering University, Harbin 150001, China

Corresponding author: L. Liu (liulutao@msn.com)
This work was supported in part by the Fundamental Research Funds for the Central Universities under Grant HEUCF1608 and in part by
the National Natural Science Foundation of China under Grant 61201410 and Grant 61571149.

ABSTRACT In a multipath communication scenario, it is often relevant to estimate the directions and
relative delays of each multipath signal. We present an effect algorithm for the simultaneous estimation of
these parameters by re-iterative super-resolution, beamforming, and MUSIC-like searching techniques. The
algorithm first separates and estimates direction of arrival (DOA) of the multipath signals. Linear constrained
minimum power beamforming is used to obtain the transmitted time function of the desired signal in certain
incident angle. Then, time difference of arrival (TDOA) for the incident signals are mapped into phase
shifts in the frequency domain. Using the DFT of the desired signal and the received data, we can separate
the phase shifts in the frequency domain due to time delay by MUSIC-like searching. At the same time,
the pairing of the estimated DOA and TDOA is automatically determined. Computer simulations illustrating
the performance of the proposed algorithm with the Cramer–Rao bound are included.
INDEX TERMS Wireless communication, array signal processing, joint TDOA-DOA estimation, multipath
signal.

I. INTRODUCTION

With the increasing demand for the mobile communication system, innovative approaches are needed to improve
the performance for overcoming errors caused by mobile
channel. Long Term Evolution (LTE) is considered as one
of the promising systems that is suitable to these requirements [1], [2]. As the signal from the User Equipment (UE)
to base station undergoes multipath rays of the direct signal,
the multipath signals with different DOAs and time delays
combined with direct signal will degrade and influence the
estimation of the desired signal. Channel estimation of the
base station involves the DOAs and time delays parameter
estimation for multipath and direct sources transmitted by
the UE. So joint DOA and TDOA estimation by the base station is of interest for advanced handover schemes, emergency
localization, and potentially many user services [3].
Many space-time processing approaches have been investigated to estimate the DOA and TDOA with the smart array for
obtaining the desired signals. In [4] and [5], the approaches
require to ML searches with great computation load or
need good accurate initial value for the accurate estimation.
In [6]–[8], the algorithms are developed to transform the
data in frequency domain and maps delays into phase shifts,
and the joint estimation problem is changed to one that can
VOLUME 4, 2016

be solved using 2-D ESPRIT techniques. Joint DOA and
TDOA estimation by multi-invariance MUSIC are proposed
in [9] and [10]. Those algorithms require the known modulation pulse shape of one symbol to recovery the time delays.
And the delay less than the symbol duration can be obtained
by the algorithms. In mobile communication, time delays
between different paths are most likely to be longer than the
symbol rate. The algorithms [11], [12] do not need the known
modulation pulse shape instead of the known transmitted
signal. The algorithms require the preamble to recovery the
time delays between each arrival. In [13], a method is proposed, which associates DOA from the MUSIC algorithm and
TDOA from the correlator separatively. It considered multipath signals as independents sources, assumed the delays
between the components are larger than the duration of autocorrelation of emitted signals.
In this work, a practical method to estimate TDOA associated with DOA information in the presence of multipath
without any preamble sequence is proposed. The algorithm
is based on an efficient high-resolution scheme that transforms the multi-dimensional estimation involved in two sets
of simple one-dimensional (1-D) estimation. To do so, we
propose to use the RISR algorithm to estimate DOAs of the
multipath signals without forward-backward averaging and

2169-3536 
 2016 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

3815

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

spatial smoothing [14]. And the LCMP beamforming [15]
in conjunction with the received data by the array is used
to estimate the time function of the desired signal in certain
incident angle. Then time delays of the incident signals are
mapped into phase shifts in frequency domain by Discrete
Fourier Transformation (DFT). Using the sequence of the
desired signal and the received data in frequency domain, we
can estimate the time delays of the signals by MUSIC-like
peaks searching. The new proposed numerical method is able
to associate correctly the DOA from the RISR algorithm and
TDOA from the MUSIC-like algorithm.
Our major contributions are summary as follows: (1) Use
the RISR, beamforming and MUSIC-like techniques in conjunction to realize the DOA and TODA estimation without
preamble knowledge (2) The algorithms is suitable of arbitrary array structures (3) Time delay within a fraction of
the sampling time can be estimated accurately in frequency
domain. To our knowledge, no other existing algorithm is able
to jointly estimate of TDOA and DOA of the multipath signals
without preamble.
The structure of the paper is as follows. In Section II,
we begin our discussion by formulating the problem and
describing the data model. Section III contains a detailed
derivation of the basic steps of the algorithm, including the
techniques to obtain the DOAs of multiple sources and to
estimate the time function of them. Identifiability of the
DOAs and TDOAs using the proposed technique is addressed
in Section IV. Section V illustrates the performance using
computer simulations.
Notation: Vectors (matrices) are denoted by boldface lower
(upper) case letters, all vectors are column vectors, superscripts (·)T , (·)∗ , (·)H and (·)† denote transpose, conjugate,
complex conjugate transpose and Pseudo-inverse respectively, E[·] denotes statistical expectation, I is the identity
matrix, | · | represents the modulus of a complex number, and
k · k is the Euclidean norm of a vector. The symbol  denotes
the Hadamard product between two matrices of appropriate
size and the symbol ◦ is used to denote convolution operator.
product.
II. SYSTEM MODEL

Consider a linear array composed of M omnidirectional sensors in a base station to receive K (K < M ) narrow-band
plane wave signals from directions θ1 , θ2 , · · · θK , in which
one of the signals is the direct signal and the other K − 1
signals are reflection components. The distance between the
first reference element and the i-th element is di in the array
with at least two elements spaced at half of the wavelength of
radiation sources or closer, as shown in Fig. 1. Signal sd (t) is
defined as the direct signal with fading coefficient βd = 1 and
time delay τd = 0, then the other reflection signals received
can be expressed as
sk (t) = βk sd (t − τk ).

(1)

In general, array antenna received signal vector y(t) formed
by the superposition of the multipath signals and noise can be
3816

FIGURE 1. Schematic diagram of an antenna array with one impinging
signal.

described as
y(t) =

K
X

a(θk )sk (t) + v(t)

(2)

k=1

where y(t)
=
[y1 (t), y2 (t), · · · , yM (t)]T is the M
dimensional snapshot data vector of the array, v(t) =
[v1 (t), v2 (t), · · · , vM (t)]T is the M dimensional noise data
vector. The vector-valued function a(θk ) is the array response
vector (steering vector) for an array of M elements to the
kth source signal from the direction θk , expressed as follows:
a(θk ) = [1, e−jφk1 , · · · , e−jφki , · · · , e−jφk(M −1) ]T

(3)

where the phase shift of the ith element for each narrowband

arrival signal can be defined as φki = 2π fc di sin(θk ) c. fc is
the carrier frequency of the incident signals, c is the speed
of light. It is assumed that the signals and noise are stationary, zero mean uncorrelated random processes. Further, the
noise vector is Additive Gaussian White Noise (AGWN) with
variance σ 2 .
III. REVIEW OF SOME TECHNIQUES

The proposed algorithm is partly based on the RISR algorithm and beamforming techniques, so it is necessary to make
a brief review of them.
A. RISR DOA ESTIMATION ALGORITHM

Many traditional DOA estimation methods are based on
the covariance information of spatial samples, and the estimation performance will be significantly decreased when
some sources are related in time domain. The essence of
Re-Iterative Super-Resolution (RISR) algorithm is the
recursive implementation of the minimum mean-square
error (MMSE) [16]. When the determined spatial covariance information and the approximate value of the array
calibration tolerance are given, RISR determines an MMSE
filter bank according to the estimate of the spatial power
distribution. Then the MMSE filter bank is applied to the
received data for the updated estimation of spatial power
distribution which is used to redefine the MMSE filter banks.
The two processes above constantly alternates. Finally, the
number of signal sources (the prior knowledge of the number
of signal source is not needed), their respective DOAs, and
their respective magnitudes can be determined automatically.
VOLUME 4, 2016

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

One of the advantages for the algorithm is that it can distinguish the coherent signals received by the array, which often
happen in the mobile communication.
Given K narrowband signals from the far field which
simultaneously incident on the array. After A/D conversion,
The lth time sample, which contains superposition of signal
and noise can be expressed in vector notation, same as (2)
y(l) =

K
X

xk (l) + v(l)

(4)

k=1

where xk (l) = a(θk )sk (l). For 
simply addressing DOA estimation, let ψk = 2π fc sin θk c. Then the steering vector
a(θk ) in θ -space is transformed to ψ-space as a(ψk ). In order
to determine the DOA of each signal in ψ-space at the
lth sample, we approximate received data y(l) in (4) with a
parameterized version as
1

y(l) ∼
= ỹ(l) = As(l) + v(l).

(5)

According to the array configure, the M × N dimensional
array manifold matrix A is constructed and defined as
A = [a(0), a(ψ1 ), · · ·

1
1
jd1 ψ1
1
e

=.
..
 ..
.
1

ejdM −1 ψ1

, a((N − 1)ψ1 )]
···
1
···
ejd1 ψ1 (N −1)
..
···
.
···

ejdM −1 ψ1 (N −1)







(6)

P(l) = E{s(l)sH (l)}  I M

Minimizing (7) and the well-known MMSE filter can be
obtained as
(8)

Substituting y(l) of (5) into (8), with the added assumption
that the signal and noise are uncorrelated and statistically
independent, then results is found to be
W (l) = (AE{s(l)sH (l)}AH + E{v(l)vH (l)})−1 AE{s(l)sH (l)}

where I M is an M ×M identity matrix. The diagonal elements
of P(l) comprise the power distribution in space. Substituting
the noise covariance matrix and spatial power distribution
matrix of (10) into (9) yield
W (l) = (AP(l)AH + Rv )−1 AP(l).

(11)

Given the filter bank W (l), the MMSE estimation of s(l) can
be obtained as
ŝ (l) = W H (l)y (l)

(12)

Since the prior knowledge of P(l) is unknown, the recursive
process of the previous estimation is needed. Based on the
matched filter bank strategy, the pre-estimate of s(l) can be
determined as
ŝ0 (l) = AH y(l).

(9)

P̂ 0 (l) = [ŝ0 (l)ŝH
0 (l)]  I M .

(13)

where noise covariance matrix Rv =
, can be
written in the form as σ 2 I. As a measure of statistical similarity over time, the temporal correlation of the signal sources

(14)

The relation between new MMSE filter bank estimation Ŵ i (l)
and previous estimation P̂ i−1 (l) is
Ŵ i (l) = (AP̂ i−1 (l)AH + Rv )−1 AP̂ i−1 (l),

(15)

then a new MMSE estimation of s(l) can be expressed as
follow
H

ŝi (l) = Ŵ i (l)y (l) .

(16)

The ith spatial power distribution estimation has the same
expression form like (14)
P̂ i (l) = [ŝi (l)ŝH
i (l)]  I M .

(17)

Recursive procedure for
distribution estimation
 spatial power 
2
may be stopped until ŝi (l) − ŝi−1 (l) < ε, where the ε is
a pre-specified threshold or after some pre-determined number of recursions. At the end of√the recursion, the diagonal
elements of the diagonal matrix P i (l) is ‘‘RISR spectrum’’
and the spatial magnitude distribution consistent with the
ψ-space division for the radiation sources is given. After that,
we can calculate the DOAs of the signals recieved by

E{v(l)vH (l)}

VOLUME 4, 2016

(10)

So the initial estimation P(l) subsequently is computed as

which consists of N spatial steering vectors corresponding to angles defined over 2π with the angular increment
ψ1 = 2π/N . The angle range of incident signals is separated
to N grids. With the parameter N increasing, the quantization
effect of the angle becomes better, while the capability to
solve the problem of noise, source correlation effects and
array modeling errors becomes stronger.
The idea of the RISR algorithm is to estimate the Minimum
Mean-Square Error through the M × N adaptive filter bank,
with the received signal model defined in (5) and the assumed
knowledge of spatial complex vector s(l). The MMSE cost
function can be written as

2


J (W ) = s(l) − W H (l)y(l) .
(7)

W (l) = (E{y(l)yH (l)})−1 E{y(l)sH (l)}.

is meaningless in terms of the RISR algorithm because the
RISR formulation just works on each snapshot independently
or combines power estimates via non-coherent integration.
In order to perform the temporally uncorrelated assumptions,
the spatial energy distribution matrix can be written as

θk = arc sin

ψk c
.
2π fc

(18)
3817

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

B. LCMP BEAMFORMING

Weighted vector w of LCMP (linear constrained minimum
power) beamforming algorithm [15] can be obtained by
minimizing the output power, under Mc linear constraint
conditions as
wH C = gH

(19)

where w is an M -dimensional vector, gH is an
Mc -dimensional vector and C is an M × Mc matrix, the
column vectors of which are linearly independent. Assumed
that the first column of C corresponds to the steering vector
of the signal of interest, and the first element of g is unit,
the signal will pass through the filter undistorted. Furthermore the data covariance matrix Ry , which keeps the output
power minimum in the constraint condition of (19) is known
as [15]

A. MUSIC-LIKE ALGORITHM FOR TIME
DELAY ESTIMATION

MUSIC is explored into high-resolution time delay estimation though the conventional MUSIC algorithm [17] was proposed to estimate the DOAs of the sources. As the data model
description in (1) and (2), the wireless multipath channel
containing K path components can be modeled by
h(t) =

K
X

a(θk )βk δ(t − τk )

(25)

k=1

where δ(t) is the dirac pulse. Therefore, the signal model can
be rewritten as
y(t) = s(t) ◦ h(t) + v(t)
K
X
=
a(θk )βk s(t − τk ) + v(t).

(26)
(27)

k=1

Pout = wH Ry w

(20)

The DFT expression of y(t) in (26) can be written as
Y (f ) = S(f )H(f ) + V (f )
K
X
a(θk )βk e−j2πf τk + V (f )
= S(f )

minimize the following function
1

J = wH Ry w + [wH C − gH ]λ + λH [C H w − g],

(21)

k=1

= [Aθ β] [S(f )bτ ] + V (f )

we can obtain
λ = −g [C
H

H

H

−1
R−1
y C]

(22)

and
H
H −1 −1 H −1
wH
LCMP = g [C Ry C] C Ry .

(23)

In our case, we will use LCMP beamforming to estimate
the time function of one desired signal with other signals
suppressed. According to (19), the distortionless constraint
with adding constraints, can be written as:
(
wH a(θk ) = 1
(24)
wH a(θi ) = 0, i ∈ [1, . . . , K ] and i 6 = k
IV. PROPOSED ALGORITHM

Our goal is to jointly estimate DOA and TDOA parameters
without knowledge of the emitter signal under multipath
propagation conditions. Firstly, the RISR algorithm presented
in III-A will be applied to separate the coherent signals and
determine DOAs of them. Furthermore, the LCMP beamforming is established according to the DOA information
obtained. The optimal beamformer can maximize the ratio
of interest output signal to the sum power of interference
(other signals) and noise. It is also considered as a spatial
filter, which makes the useful signal pass and make the output
power of the noise and interference as small as possible. Next,
we will find the delays of different impinging signals in the
array. MUSIC-like searching method is used to estimate the
delays through the orthogonality principle between the signal
subspace and the noise subspace. There are different peaks
in the searching which are related to the delays of arrival,
associated each delay with its proper DOA.
3818

(28)

where S(f ), H(f ) and V (f ) are the Fourier transform of s(t),
h(t) and v(t), respectively. Aθ = [a(θ1 ), a(θ2 ), · · · , a(θK )] ,
β = diag{[β1 , β2 , · · · , βK ]} and bτ = [ej2πf τ1 , · · · , ej2πf τK ]T .
The diag{·} is the diagonal operator, which form a vector
creates a diagonal matrix, whose elements are the elements of
the vector. As stated in [18], we transform the data received
into frequency domain and construct an M × L matrix
Y L = [Y (f1 ), Y (f2 ), · · · , Y (fL )]


= [Aθ β] Bτ diag{S(f )} + V L
= A[β,θ ] BT[f ,τ ] + V L

(29)

where
• L represents the number of different frequency points in
frequency domain
• A[β,θ ] = Aθ β is an M × K matrix
• S(f ) = [S(f1 ), S(f2 ), · · · , S(fL )] is the L-point DFT of
the signal function
• V L is an M × L matrix, which presents the noise in
frequency domain.
• Bτ is a K × L matrix, which can be written as
 −j2πf τ

1 1
e
e−j2πf2 τ1 · · · e−j2πfL τ1
 e−j2πf1 τ2
e−j2πf2 τ2 · · · e−j2πfL τ2 


Bτ = 

..
..
..
..


.
.
.
.
−j2π
f
τ
−j2πf
τ
−j2πf
τ
K
k
L
k
1
2
e
e
··· e
(30)
•

B[f ,τ ] = [Bτ diag{S(f )}]T is a L × K matrix, the ith
column of which can be expressed as
bi = [S(f1 )e−j2πf1 τi , · · · , S(fL )e−j2π fL τi ]T .

(31)

VOLUME 4, 2016

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

As the analysis above, the transpose of Y L can be expressed as
Y TL = B[f ,τ ] AT[β,θ ] + V TL



= [b1 , b2 , · · · , bK ] 


β1 aT (θ1 )
β2 aT (θ2 )
..
.

βK aT (θK )




 + V TL .


(32)

Comparing it with (2), Y TL in (32) behaves like the received
signal at an array whose manifold matrix is B[f ,τ ] and
the equivalent signal matrix is represented by AT[β,θ ] . The
columns of B[f ,τ ] behave like the steering vectors in an array
with L virtual sensors. The correlation matrix of Y TL can be
written as
 H
RYY = E[Y TL Y TL ]
 H
T
T
= B[f ,τ ] E[AT[β,θ ] (AT[β,θ ] )H ]BH
]
[f ,τ ] + E[V L V L
= B[f ,τ ] P [β,θ ] BH
[f ,τ ] + RVV

(33)

where P [β,θ] = E[AT[β,θ] (AT[β,θ ] )H ] is the covariance matrix
with rank K . Assumed L > K , the rank of matrix RYY will
be K without noise effect. We can apply EVD(eigenvalue
decomposition) to RYY
H
RYY = U S 6 S U H
S + U N 6N U N

(34)

where 6 S stands for a K ×K diagonal matrix whose diagonal
elements are the largest K eigenvalues, corresponding to
U S = [u1 · · · uK ], called signal subspace. And 6 N stands
for a diagonal matrix whose diagonal elements are composed
of the smallest L − K eigenvalues, corresponding to U N =
[uK +1 · · · uL ], called noise subspace. By utilizing the orthogonality principle between the signal subspace and the noise
subspace, we can get the time delay estimation of the sources
in frequency domain by MUSIC-like searching as
F(τ ) =

1
bH U N U H
Nb

(35)

where b = [S(f1 )e−j2πf1 τ , · · · , S(fL )e−j2πfL τ ]T has same
form as (31). The estimate of multipath time delay τ is
τ̂ = arg max F
τ

(36)

Searching the time delay variable τ with a constant increment
in the delay range by the MUSIC-like method, we finally
obtain TDOA of the signals.
B. REALIZATION OF JOINT DOA AND TDOA ESTIMATION
BY THE PROPOSED ALGORITHM

The algorithm proposed in this paper has many advantages
in DOA estimation, RISR algorithm is applicable to arbitrary
array as long as the array manifold is known. And compared
with other convectional algorithms [17], [19], RISR can estimate the direction of arrival accurately in the case that the
number of multipath sources is unknown. In addition, many
VOLUME 4, 2016

conventional time delay estimation algorithms take advantage
of the cross-relation between the transmitted signal and the
received signal while Rayleigh restriction limits the resolution of these algorithms. The MUSIC-like time delay estimation algorithm has more high resolution for delay estimation,
which can distinguish the multipath signal components of the
time intervals below the Rayleigh limit.
The specific steps are as follows:
1) Calculate the angles of incidence signals by RISR algorithm, and pay attention to select the appropriate number of iterations. According to the observation, RISR
can always reach steady-state less than 15 iterations,
regardless of the number of signal source or the spatial
structure of the array.
2) Apply Beamforming technique to one of the interest
directions obtained in step 1), by phasing the array to
steer the main lobe in the specific direction θk so that
all the received signals except the specific direction
can be eliminated. In narrowband beamforming, a complex weight is applied to the signal at each sensor and
summed to form the beamformer output
sk (t) = wH (θk )y(t).

(37)

We make w = wLCMP which is calculated as (23), the
sk (t) in the direction θk will be well recovered .
3) MUSIC-like algorithm is applied to the time delay
estimation. As described in IV-A. The arbitrary signal
sp (t), p ∈ [1, · · · , K ], obtained from the previous step
of the beamforming can be assumed as a reference signal(maybe not the the direct signal). Then we transform
the reference signal to frequency domain, we can obtain
Sp (f ) = S(f )e−j2πf τp . And construct the steering vector
of time delay τ
b = [Sp (f1 )e−j2πf1 τ , · · · , Sp (fL )e−j2πfL τ ]T

(38)

Substituting of (38) into (35), use the spectral searching
with respect to τ in the specified range of time delay, we
can get K peaks, which correspond to the relative delay
to the signal Sp (f ). Since sp (t) = βp sd (t −τp ) according
to (1), so MUSIC-like algorithm will produce spectral
peaks on τk = τ + τp in the specific search range,
which is relative time delay to τp . The real time delay
of K path signals would be
τ = [τ1 − τp , · · · , τK − τp ].

(39)

In fact, we always believe that the arrival time τd of the
direct signal on the antenna is the shortest. Assumed
that τd = 0, τd − τp is the minimum element in τ .
Thus, we have
d = arg min[τk − τp ].
k

(40)

Finally, the real delay time of each arrival signal to
the direct signal can be obtained by the calculation as
follow
τ − (τd − τp ).

(41)
3819

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

If we want to link DOA with the right TDOA, signals
in other arrival directions provided by the beamformer
are considered as reference signals to estimate relative
time delays by the MUSIC-like method, respectively.
According to the relations of the positions of spectral
peaks in multiple searching, the DOA and TDOA can
be paired correctly.
V. SIMULATION RESULTS

In this section, we conduct several simulations to demonstrate the performance of the proposed algorithm. The incident narrow-band multipath signals with carrier frequency
fc = 300MHz and sampling rate fs =1GHz under the
AWGN background are received by a 10-element uniform
linear array (ULA), in which the distance between adjacent
elements is half-wavelength. And the number of snapshots
is 100.
(a) Suppose number of multipaths are K = 3 and incident
directions of three related signals are θ = [−30◦ , 15◦ , 60◦ ]
corresponding to time delays τ = [3.2, 0, 6.7]ns. And
1
1
the fading coefficient are β = [0.4ej 3 π , 1, 0.7ej 5 π ].
We consider that the direct signal comes from the direction
of θ = 15◦ . Under the condition of SNR=20dB, RISR
algorithm is applied to estimate DOA with phase angle ψ
quantization N = 720, see (6). After iterating 10 times,
the pseudo-spectrum for DOA estimation is shown in Fig.2.
Three spectral peaks of the spatial energy distribution by
RISR are very sharp, corresponding to three estimation values
[−29.82◦ , 14.97◦ , 60.39◦ ].

FIGURE 2. DOA estimation in 20dB with RISR pseudospectrum.

After getting incident angles of the signals, we need to
estimate the time function of signals using the beamforming before realizing the high-resolution time delay estimation of each arrival signal. We firstly choose the signal
from the direction θ̂p = −29.82◦ as the reference signal,
and use LCMP beamforming in this direction to calculating
the desired signal function ŝp (t) as sp (t) = βp sd (t − tp ).
From the pseudo-spectrum of MUSIC-like search in Fig.3(a),
we can see three peaks corresponding to the time delay
3820

FIGURE 3. Pseudo-spectral of time delay estimation in SNR=20dB.
(a) Signal from θ̂1 = −29.82◦ as the reference signal function. (b) Signal
from θ̂1 = 14.97◦ as the reference signal. (c) Signal from θ̂1 = 60.39◦ as
the reference signal.

τ̂ = [−3.2, 0, 3.5]ns, which is the relative delays to the signal ŝp (t) from θ̂p = −29.82◦ . The spectrum peaks of the time
delays should be positioned in τ = [τ1 − τp , τ2 − τp , τ3 − τp ].
Since the minimum value in τ̂ is τ1 − τp = −3.2ns, we can
know that real time delays of the signals are [0, 3.2, 6.7]ns
and the reference signal from θ̂ = −29.82◦ with time delay
3.2ns is not the direct signal. Consider the incident signal
VOLUME 4, 2016

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

from θ̂p = 14.97◦ as the reference signal, the relative delays
to it are τ̂ = [0, 3.2, 6.7]ns in Fig.3(b). The signal which
comes from θ̂ = 14.97◦ is the direct signal, since the relative
time delay in the three peaks are all positive in this spectrum.
From the calculation above, we can also know that the signal
of θ = 60.39◦ has the time delay τ = 6.7ns. Thus, the
TDOA and DOA of each signal is paired. Fig.3(c) shows
that delay time of the signals from three directions relative
to the third incident angle of θ = 60.34◦ , which is not
necessary for pairing the DOA and TDOA in general. In the
Fig.4, we show Root Mean Square Error (RMSE) of DOA
and time delay estimation for the signal with the Cramer-Rao
bound (CRB) [6] versus SNR by 500 Monte Carlo trials.

FIGURE 4. RMSE of DOA and TDOA for the three signal versus SNR.

(b) In order to further illustrate the DOA estimation performance of the proposed algorithm, it is compared with
MUSIC [17], spatial smoothing MUSIC [20] and Topelize
algorithm [21] in the separation ability for coherent signals. It is also assumed that the number of sources is an
prior knowledge and the ULA of 10-element is divided into
7 overlapped sub-arrays (each sub-array have 4 elements)
for smoothing MUSIC. In order to show clearly the performance of these algorithms, we reset the incident DOAs
of the multipath signals. In Fig.5(a), the incident angle is
θ = [10◦ , 13◦ , 60◦ ] with the time delay [0, 3.2, 6.7]ns.
We can see that MUSIC algorithm can’t separate the coherent signals at all; Toeplize algorithm can barely distinguish
between two adjacent signals. By comparison, the spatial
smoothing and proposed algorithm can clearly distinguish
all the signals. With the angle of incidence between the two
coherent signals closing, Toeplize algorithm failed in Fig.5(b)
and smoothing MUSIC in Fig.5(c). It can be seen that the proposed algorithm can form a substantially deeper null between
the two closely spaced sources. Fig.6 shows the performance
of the probability of separation with regard to the incident
angle difference of two sources in SNR=20dB for the four
methods . As we expected, the MUSIC algorithm for separating adjacent spatial sources is invalid when the sources with
a strong temporal correlation, other algorithms are able to
VOLUME 4, 2016

FIGURE 5. DOA separation for correlated sources in SNR=20dB.
(a) θ = 10◦ , 13◦ , 60◦ with τ = 0ns, 3.2ns, 6.7ns. (b) θ = 10◦ , 12◦ , 60◦ with
τ = 0ns, 3.2ns, 6.7ns. (c) θ = 10◦ , 11◦ , 60◦ with τ = 0ns, 3.2ns, 6.7ns.

separate the adjacent sources. And proposed method exhibits
the remarkable DOA separation ability among the methods.
(c) We compare RMSE of the delay estimates of the proposed method, the Minimum Variance (MV) [22] and the
3821

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

direction of each relevant signal source can be distinguished
and the number of the signal sources can be determined
clearly. MUSIC-like searching process is used to obtain time
delays of the sources by the phase shifts in frequency domain,
so time delay within a fraction of the sampling time can
be estimated. Monte Carlo simulations show the proposed
algorithm has good performance for joint DOA and TOA
estimation.
REFERENCES

FIGURE 6. Probability of separation for correlated signals versus 1θ .

FIGURE 7. RMSE of time delay versus SNR.

GP [11] (which utilizes the geometrical properties of the
signal in the frequency domain alternately for the time delay
variables) algorithms. We reset the incident angles and the
corresponding time delays of multipaths same to (a). In Fig.7,
it shows RMSE of time delay estimation of the signal from
θ = 60◦ as an example with respect to the SNR. The RMSE
value is calculated for 500 Monte Carlo simulations. The
RMSE of the time delay estimated by the proposed algorithm
is less than by utilizing the MV algorithm or GP algorithm.
It is proved that the estimation of the proposed method is more
effective.
VI. CONCLUSION

The paper proposes a novel algorithm which combines RISR
technique along with the LCMP beamforming techniques
and MUSIC-like searching algorithm to jointly estimate the
DOAs and TDOAs under the multipath propagation conditions. The algorithm transforms the multi-dimensional estimation involved into two simple one-dimensional estimation.
The DOA estimation process has super high-resolution based
on an iterative scheme, which can be used to separate the
relevant signals to any known array manifold without spatial
smoothing. According to the obtained RISR spectrum, the
3822

[1] A. Ghosh, R. Ratasuk, B. Mondal, N. Mangalvedhe, and T. Thomas,
‘‘LTE-advanced: Next-generation wireless broadband technology
[invited paper],’’ IEEE Wireless Commun., vol. 17, no. 3, pp. 10–22,
Jun. 2010.
[2] N. Abu-Ali, A. M. Taha, M. Salah, and H. Hassanein, ‘‘Uplink scheduling
in LTE and LTE-advanced: Tutorial, survey and evaluation framework,’’
IEEE Commun. Surveys Tuts., vol. 16, no. 3, pp. 1239–1265, Dec. 2014.
[3] T. S. Rappaport, J. Reed, and B. D. Woerner, ‘‘Position location using
wireless communications on highways of the future,’’ IEEE Commun.
Mag., vol. 34, no. 10, pp. 33–41, Oct. 1996.
[4] M. Wax and A. Leshem, ‘‘Joint estimation of time delays and directions of
arrival of multiple reflections of a known signal,’’ in Proc. IEEE ICASSP,
Atlanta, GA, USA, May 1996, pp. 2622–2625.
[5] A. L. Swindlehurst, ‘‘Time delay and spatial signature estimation using
known asynchronous signals,’’ IEEE Trans. Signal Process., vol. 46, no. 2,
pp. 449–462, Feb. 1998.
[6] A.-J. van der Veen, M. C. Vanderveen, and A. Paulraj, ‘‘Joint angle and
delay estimation using shift-invariance techniques,’’ IEEE Trans. Signal
Process., vol. 46, no. 2, pp. 405–418, Feb. 1998.
[7] S. Al-Jazzar and J. Caffery, ‘‘ESPRIT-based joint AOA/delay estimation for CDMA systems,’’ in Proc. IEEE Wireless Commun. Netw.
Conf. (WCNC), Atlanta, GA, USA, May 2004, pp. 2244–2249.
[8] J. Picheral and U. Spagnolini, ‘‘Angle and delay estimation of space-time
channels for TD-CDMA systems,’’ IEEE Trans. Wireless Commun., vol. 3,
no. 3, pp. 758–769, May 2004.
[9] X. Zhang, G. Feng, and D. Xu, ‘‘Blind direction of angle and time delay
estimation algorithm for uniform linear array employing multi-invariance
music,’’ Prog. Electromagn. Res. Lett., vol. 13, no. 2, pp. 11–20, Feb. 2010.
[10] Y. Wang, J. Chen, and W. Fang, ‘‘TST-MUSIC for joint doa-delay estimation,’’ IEEE Trans. Signal Process., vol. 49, no. 4, pp. 721–729, Apr. 2001.
[11] P. Singh and P. Sircar, ‘‘Time delays and angles of arrival estimation using
known signals,’’ Signal Image Video Process., vol. 6, no. 2, pp. 171–178,
2012.
[12] M. Wax and A. Leshem, ‘‘Joint estimation of time delays and directions
of arrival of multipath reflections of a known signal,’’ IEEE Trans. Signal
Process., vol. 45, no. 10, pp. 2477–2484, Oct. 1997.
[13] D. Grenier, B. Elahian, and A. Blanchard-Lapierre, ‘‘Joint delay and
direction of arrivals estimation in mobile communications,’’ Signal, Image
Video Process., vol. 10, no. 1, pp. 45–54, Jan. 2016.
[14] T.-J. Shan, M. Wax, and T. Kailath, ‘‘On spatial smoothing for directionof-arrival estimation of coherent signals,’’ IEEE Trans. Acoust., Speech,
Signal Process., vol. 33, no. 4, pp. 806–811, Apr. 1985.
[15] H. L. Van Trees, Optimum Array Processing: Part IV of Detection, Estimation, and Modulation Theory. New York, NY, USA: Wiley, 2002.
[16] S. D. Blunt and K. Gerlach, ‘‘Adaptive pulse compression via MMSE
estimation,’’ IEEE Trans. Aerosp. Electron. Syst., vol. 42, no. 2,
pp. 572–584, Apr. 2006.
[17] R. O. Schmidt, ‘‘Multiple emitter location and signal parameter estimation,’’ IEEE Trans. Antennas Propag., vol. 34, no. 3, pp. 276–280,
Mar. 1986.
[18] J. S. Wang and Z. X. Shen, ‘‘An improved music TOA estimator for RFID
positioning,’’ in Proc. IEEE Radar Conf., Edinburgh, U.K., Oct. 2002,
pp. 478–482.
[19] R. Roy, ‘‘ESPRIT-estimation of signal parameters via rotational invariance techniques,’’ Ph.D. dissertation, Dept. Elect. Eng., Stanford Univ.,
Stanford, CA, USA, 1987.
[20] S. U. Pillai and B. H. Kwon, ‘‘Forward/backward spatial smoothing techniques for coherent signal identification,’’ IEEE Trans. Acoust., Speech,
Signal Process., vol. 37, no. 1, pp. 8–15, Jan. 1989.
VOLUME 4, 2016

L. Liu, H. Liu: Joint Estimation of DOA and TDOA of Multiple Reflections in Mobile Communications

[21] K. Takao, N. Kikuma, and T. Yano, ‘‘Toeplitzization of correlation matrix
in multipath environment,’’ in Proc. IEEE Int. Conf. Acoust., Speech,
Signal Process., Tokyo, Japan, Apr. 1986, pp. 1873–1876.
[22] J. Vidal, M. Najar, and R. Jativa, ‘‘High resolution time-of-arrival detection for wireless positioning systems,’’ in Proc. IEEE 56th Veh. Technol.
Conf. (VTC-Fall), vol. 4. Sep. 2002, pp. 2283–2287.

HUAN LIU was born in Harbin, China, in 1993.
She received the B.A. degree in telecommunication engineering from the Harbin University
of Science and Technology, China. She is currently pursuing the M.Sc. degree with the College
of Information and Telecommunication, Harbin
Engineering University, China.

LUTAO LIU was born in Harbin, China, in 1977.
He received the B.A. degree in electrical engineering from the Southeast University of China
in 2000, the M.Sc. degree in telecommunication engineering from the Harbin Institute of
Technology, China, in 2003, the M.Sc. degree
in microelectronics from the Delft University of
Technology, The Netherlands, in 2005, and the
Ph.D. degree in telecommunication engineering
from the Harbin Engineering University, China,
in 2011. In 2013, he was a Visiting Scholar with the Signal Processing
and Communication Laboratory, Stevens Institute of Technology, USA.
He is currently an Associate Professor with the College of Information
and Telecommunication, Harbin Engineering University, China. His research
interests are in the general area of signal processing for telecommunication.

VOLUME 4, 2016

3823

Received July 14, 2016, accepted August 1, 2016, date of publication August 17, 2016, date of current version September 28, 2016.
Digital Object Identifier 10.1109/ACCESS.2016.2601167

Robots That Think Fast and Slow: An Example
of Throwing the Ball Into the Basket
TZUU-HSENG S. LI, (Member, IEEE), PING-HUAN KUO, YA-FANG HO, CHIN-YIN LIU,
TING-CHIEH YU, YAN-TING YE, CHIEN-YU CHANG, GUAN-YU CHEN, CHIH-WEI CHIEN,
WEI-CHUNG CHEN, LI-FAN WU, AND NIEN-CHU FANG
National Cheng Kung University, Tainan 701, Taiwan

Corresponding author: T.-H. S. Li (thsli@mail.ncku.edu.tw)
This work was supported in part by the Ministry of Science and Technology, Taiwan, under
Grant MOST 103-2221-E-006-252 and Grant MOST 104-2221-E-006-228-MY2, and in part by the Ministry of Education, Taiwan, within
the Aim for the Top University Project through National Cheng Kung University, Tainan, Taiwan.

ABSTRACT Can a robot think like a human being? Scientists in recent years have been trying to achieve
this dream, and we are also committed to this same goal. In this paper, we use an example of throwing the
ball into the basket to make the robots process with human-like thinking behavior. Such thinking behavior
adopted in this paper is divided into two modes: fast and slow. The fast mode belongs to the intuitional
reaction, and the slow mode represents the complicated cogitation in human brain. This fascinating human
thinking concept is inspired by the book, Thinking, Fast and Slow, which explains the process of the human
brain. In addition, the psychology theories proposed in this book are also adopted to realize the thinking
algorithms, and our experiments verify that the thinking mode of human beings is reasonable and effective
in robots.
INDEX TERMS Anchoring effect, fast and slow systems, FIRA, humanoid robot, learning algorithm,
peak-end rule, psychology.

I. INTRODUCTION

An interesting concept of human thinking has been proposed
in Thinking, Fast and Slow [1], and it explains the process
of the human brain. This concept divides human thinking
into two modes, fast and slow. The fast mode, System 1,
belongs to the intuitional reaction, and the slow mode,
System 2, represents the complicated cogitation in human
brain. However, can a robot think like a human being? Here
we show an example of throwing the ball into the basket to
make the robots implement the human-like thinking behavior.
In this study, we built two humanoid robots,
David Junior and David II [2], to accomplish the experiments. Robots are asked to place on the basketball
field to learn the best shooting motion by the fast-slow
system. The establishing and correcting procedure of System 1 and System 2 are fully presented in the experiments.
In addition, some psychology theories are also adopted
to realize the thinking algorithms [3]–[5]. Our results
demonstrate that though the human thinking behavior is
certainly effective in a robot, peak-end rule [3],[4] and
anchoring effect [5], which are seemingly defective, are
helpful in the procedures. In recent years, many of

5052

intelligent methods are derived from biological effects similar to animals’ distinctive behaviors [6], such as genetic
algorithms [7]–[11], particle swarm optimization [12]–[16],
artificial bee colony optimization [17]–[19], ant colony optimization [20]–[24], or other developed robots with animal
behaviors [25]–[27]. Nevertheless, human thinking is the
most intelligent and complicated type of thinking. The powerful deep learning method [28]–[32] is also inspired by
the process human brain. So it is worth adopting human
psychological theories to develop a novel machine learning
method to solve other complex engineering problems, and
make the machines more intelligent in the future.
Due to the explosion of knowledge, scientists can barely
do research manually today [33]. Even though the processing
speed of computers nowadays is much higher than that of the
human brain, the functions of computers, such as recognition
and navigation, still cannot compare with those of human
beings [34]. And scientists continue to enhance the intelligence of robots to improve our living environment [35]. They
are committing themselves to promoting creativity [36], interaction ability [37], natural language speaking ability [38],
and collective intelligence [39] in the robot, and so on.

2169-3536 
 2016 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

VOLUME 4, 2016

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

Furthermore, brains are the fountain of wisdom of this
world. In order to forage, avoid predators, mate and protect
offspring, the brains of our ancestors became more intelligent
over hundreds of millions of years of evolution [34]. Nevertheless, we still do not understand our brains well enough.
The potential of the human brain is very powerful, but we
have no efficient way to explore it. Therefore, building a
brain-based robot to understand the whole process of the
human brain is important and urgently needed [40]. In the
case of the robot, its artificial brain ought to understand itself
to be able to handle all kinds of missions. In order to be able
to coexist with robots in the future, cognitive algorithms and
the personality of robots can also be achieved through experiments [41]. Furthermore, the machine learning methods of
robots have caused a revolution in education. Psychology,
neuroscience, and machine learning are considered among
the principles of human learning [42]. All in all, the brainbased robot provides a pipeline for exploring the process of
the human brain, and this learning concept can be employed
in other applications. We believe that when the psychological
learning theories of human beings and robots are consummated, the interactive and cooperation mode between humans
and robots will be entirely changed in the future.
The major contributions of this paper are 1) using an example of throwing the ball into the basket to make the robots
process with human-like thinking behavior; 2) adopting the
proposed cognitive learning algorithm to construct the experience curve of the shooting postures for basketball games;
3) unprecedentedly utilizing the concepts of peak-end rule
and anchoring effect to enhance the learning process and final
results; and 4) presenting the feasibility and practicality of the
proposed novel learning algorithm.
This paper is organized as follows. In Section II, the
background knowledge in Thinking, Fast and Slow are introduced. The architectures of the basketball learning system are
described in Section III. The learning methods for the basketball competitionare depicted in Section IV. In Section V,
the experimental results are presented to verify the feasibility
and practicality of the proposed method. The discussions of
the proposed method are addressed in Section VI. Finally,
Section VII concludes the paper.

II. BACKGROUND KNOWLEDGE
A. THE TWO SYSTEMS

From the point of view of psychology, thinking behavior can
be divided into two modes, fast and slow, and psychologists
have been intensely interested in this behavior for several
decades. These two thinking modes are named ‘‘System 1’’
and ‘‘System 2,’’ and they play different roles in the human
brain. These two modes in our brain also have different
personalities and functions; they usually take turns dominating our thinking behavior. System 1 belongs to automated
operation mode, in which the reaction time in this thinking is
very fast and does not involve spending a lot of effort during
the operation. And because it is autonomous, we cannot
VOLUME 4, 2016

control what it does at all. System 2 involves laborious and
time-consuming work; its operation is logical and complicated. We have to spend a lot of extra effort when System 2
is in operation. Therefore, System 2 can also be regarded as
the aspect of rational thinking in the human brain.
Generally speaking, System 1 has the characteristics of
intuitive thinking, and System 2 has the characteristics of
logical thinking. At first glance, System 2 seems to be
much more reliable than System 1. But human beings almost
entirely use System 1 for thinking in daily life. For instance,
we do not have to pay much attention to thinking when we
walk over to the table and drink a cup of coffee. Moreover,
we do not have to think hard about running or riding a bicycle
while doing these actions. In other words, System 1 is the
industrious worker in the brain. Precisely since its operation
does not take much effort, it can run almost all day and
does not get tired at all. On the other hand, System 2 does
not appear frequently. And it usually does not interfere with
determinations made by System 1 or accept decisions easily.
Therefore, System 2 is called ‘‘The Lazy Controller [1].’’
All in all, System 1 and System 2 have different roles and
functions in the brain.
TABLE 1. The operating occasions of System 1 and System 2.

The operating occasions of System 1 and System 2 are
illustrated in Table 1, where System 1 is suitable for handling
simple problems and some reflex actions which need to be
performed quickly, while System 2 is usually used for solving
complex problems. Furthermore, when a decision made by
System 1 is wrong, or the situation is too complex to solve,
System 2 will take over to resolve the problem. Based on the
above, the thinking behavior of human beings is processed
by these two cores. However, they cannot work alone in one
human being because both of them are not perfect at all.
We can obtain the most efficient way to make a right decision
only through the proper division of labor on the part of System
1 and System 2. Therefore, it is important to select a suitable
thinking mode at the appropriate time in our daily lives.
B. PEAK-END RULE

Kahneman’s colonoscopy experiments give interesting
results [1]. As shown in Fig. 1, why does patient A, who has
a shorter pain time, feel more miserable than patient B? The
experimental results tell us a surprising fact: The length of
the process has no effect on the rating. No matter how long
the pain time is, the overall rating only depends on the most
painful moment of the experience and its end. That is to say,
the average intensity of feeling will not be influenced by the
length of the process [3], [4]. Such a phenomenon always
follows the ‘‘peak-end rule.’’
5053

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

FIGURE 1. Kahneman’s colonoscopy experiments [1].

C. ANCHORING EFFECT

The anchoring effect is already widely used in commercial
fields. The promotional advertising of Campbell’s soup in
Sioux City is a good example [1]. The sales with a ‘‘limit of
12 per person’’ are two times larger than the sales with
‘‘no limit per person.’’ No matter whether ‘‘12 cans’’ is the
result of a precise calculation or random generation, this
will become an anchor and produce an anchoring effect subconsciously in consumers. Furthermore, Kahneman’s experiment on ‘‘wheel of fortune [1]’’ has a very amusing result.
Why is the percentage of African nations in the United
Nations related to a wheel of fortune? Participants’ judgments
were influenced by an obviously uninformed number5. These
seemingly absurd results show that the anchoring effect does
absolutely and constantly happen today.
III. ARCHITECTURES OF THE BASKETBALL
LEARNING SYSTEM

Computer analogy involves developing computational models to understand human or animal cognition. With human
beings as an example, the aim of this discussion is to describe
the similarities that might exist between algorithms and the
way people think. This approach can be applied to imitating
the operation of the human brain. In this section, the architectures of the basketball learning system will be described
as follows.
A. ROBOT ARCHITECTURES

Since David Junior is a humanoid robot, he possesses a
human-like appearance. David Junior has 26 Degrees of Freedom (DOF). He weighs 9.6 kg and is 95 cm tall. The material
of mechanism consists of Al-Mg alloy, Acrylonitrile Butadiene Styrene (ABS), and Polyoxymethylene Resin (POM).
Aluminum-magnesium is widely applied to David Junior,
especially in the skeleton, because of the high stiffness
and light weight. However, there are many series of
Al-Mg alloy. The parts, including the hip and ankle, are made
of A7075 alloy since these parts require the highest stiffness
and strength. Other parts such as the arm and body are made
of A6061 alloy. In order to reduce the weight of the robot,
the parts that sustain light loading are made of ABS such as
the hands, head, and so on. All the gears of David Junior
are made of POM rather than steel because the strength of
5054

FIGURE 2. The system architecture of David Junior and David II.

POM is enough for the load of David Junior. David Junior has
26 DOF in total, and most of the joints are actuated by
one motor. However, some joints of the robot such as the
knee sustain high loading, and the power of a single motor
is insufficient for these parts. Therefore, the high loading
parts are actuated by two motors, which are connected by a
synchronization cable or actuated by one motor with a gear
to enhance the torque.
To meet all the requirements of our own robot, the best way
is to design the peripheral circuit board for all the required
parts. The system architecture and the integrated circuit board
are shown in Fig. 2. The architecture of David Junior and
David II includes two main parts: One is the Decision-Making
Center and the other is the Motion Control Center. Besides,
we put an emphasis on the autonomy of the robots, so it is
important that the robots have a brain to decide ‘‘what to do.’’
In the robots, this part is called the Decision-Making Center
and consists of a webcam as the robot’s eye for vision and
a laptop associated with image processing, logical strategies,
and some other computations. If the robot knows what to do,
the commands will be transferred to the next indispensable
unit, which tells the robot ‘‘how to do’’ something. We call
this unit the Motion Control Center.
B. LEARNING ENVIRONMENT

As with athletes, the aim of their training is to help their
actions become intuitive motions. In other words, these practices help them to construct the experience database for
System 1. In the same way, the goal of the issue addressed
here is to build the experience curve to give David Junior
the ability to shoot the ball into the basket from any angle.
System 1 is also an intuitive thinking process. Its thinking
movement is not only very fast but also effortless. In addition,
it can be thought of as the accumulation of the experiences of
human beings. When System 1 is dealing with a problem, it
VOLUME 4, 2016

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

FIGURE 3. Learning environment and control systems of David Junior.

wants to use a look-up table approach to get answers from past
experiences. In this issue, two polynomials represent the rotational speed and angle experimental functions, respectively.
Therefore, the rotational speed and angle decisions made by
System 1 can be obtained easily by inquiring about these two
experimental functions.
David Junior is a humanoid robot, as shown in Fig. 3(b).
He is equipped with a 9-axis inertial measurement unit
including web camera, accelerator, gyroscope, and electronic
compass. Fig. 3(a) shows the learning environment and the
relative position of the basket and two IP cameras. One
camera is mounted on the left side of the basket and is 300 cm
from the basket. The camera is held in place by an aluminum
structure. The other camera is installed in the ceiling and is
positioned directly above the basket, which is 170 cm distant
from the top camera. As for the basket, it is 30 cm in diameter
and is fixed on the plastic structure. Its image data can be
transmitted to David Junior to recognize the shooting error.
Like human beings, David Junior can make decisions using
System 1 or System 2 after these images are captured by the
camera on the robot’s head. If he makes a decision, whether
with System 1 or System 2, his brain will send a message
to the motion control center to perform the corresponding
motions. The server motor control methods of David Junior’s
shooting motion are illustrated in Fig. 3(c). We can use the
speed and position control to modify the trajectory of the ball.
In Fig. 3(d), the upper and lower partitions show the detected
ball path in red, the recognized rim framed in purple, and the
purple circles representing when the ball has gone through
the hoop and been captured by the IP cameras. The red circles
denote the calculated falling position of the ball. After that,
David Junior can recognize the shooting error so as to adjust
his shooting motion for another try.
IV. LEARNING METHODS FOR PLAYING BASKETBALL

The system flowcharts of the proposed learning algorithm
are shown in Fig. 4(a). First, the procedure of the image
processing will be executed. The main goal in this step is
VOLUME 4, 2016

FIGURE 4. The system flowcharts of the proposed learning algorithms.

to get information on the target as long as the object has
been recognized. According to the values of the distance and
direction between the basket and David Junior, he will step
forward, step back, turn left, or turn right autonomously to
meet the requirements of the distance and the direction. For
example, the robot will keep turning left or right until the
angle between the basket and himself is smaller than the
threshold. Before deciding the shooting motions, the robot
will execute the position adjustment depending on the information on the basket. After that, David Junior will utilize
System 1 to decide on the rotational speed and angle he
should perform.
David Junior will shoot the ball according to the rotational speed and angle which has just been obtained from
System 1 or System 2. After the ball is shot, David Junior will
check whether the accuracy meets the requirement or not. The
falling location of the ball is used to calculate the shooting
error, and the error will be the distance between the center of
the basket and the center of the ball. If the ball falls into the
region within the quarter of the basket radius, this is called
a good shot. If the accuracy situation meets the requirement,
the experience curves can be updated. Otherwise, the robot
will go to a step in System 2 to have another try at the same
position.
As long as the accuracy and the times do not meet the
requirement, System 2 will operate. System 2 is logical and is
continuously monitoring the human brain. When a decision
made by System 1 is wrong, System 2 will take over the
thinking behavior and support more detailed and specific
processing to solve the problem. However, if the accuracy
meets the requirement, the experience curves of the rotational
speed and angle will both be updated. The final step is to
check whether the experience curves of the power and the
direction are completed or not. If the experience curves are
5055

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

completely trained, the learning ends. Otherwise, the distance
between the basket and David Junior will change and the
learning algorithm will restart from the beginning.
A. THE MODELS OF SYSTEM 1

The models of System 1 can be defined as the following
equations.
Ssys1 = cs0 + cs1 l + cs2 l 2 + cs3 l 3 + · · · + csn l n
2

3

Asys1 = ca0 + ca1 l + ca2 l + ca3 l + · · · + can l

(1)
n

(2)

where Ssys1 and Asys1 are the output values of the shooting
rotational speed and angle decided on by System 1, and
l denotes the location where the robot stands. cs0 to csn , and
ca0 to can are the coefficients of the fitted experience curves.
The number of the coefficients is n + 1. Because the robot
trains the shooting motions at six positions, in this study, n is
set to be 5. By the experimental points which have existed,
the experience curves can predict the appropriate rotational
speed and angle for shooting.
The flowchart of System 1 is shown in Fig. 4(b). First,
System 1 will check whether the experience curves have been
established or not. As a result, if the experience curves have
been established, David Junior will acquire the next rotational
speed and angle by inquiring about the two experience curves,
respectively. On the other hand, if there are no established
experience curves, random rotational speed and angle will
be performed for shooting the ball. It is worth noting that
random rotational speed and angle are all in the limited ranges
constrained by the specification of the motors.
B. THE METHODS OF SYSTEM 2

The flowchart of System 2 is depicted in Fig. 4(c). The aim
of System 2 is to find the best shooting motion for the robots.
According to past experiences, the robots can use several
methods to estimate the optimal motion. However, the methods of System 2 can be changed. In this paper, three methods
of System 2 (short-term memory, long-term memory, and
peak-end rule) are adopted for the experiments.
The short-term memory method is constructed by simple
artificial intelligence. The robot can only remember the last
experience to adjust the motion of the next shot. The adjustment of the next rotational speed and angle scales depend on
the specification of the server motors, and the correction rule
is described in equations (3) and (4).
 

S(t)−1S3 ×sgn(ey ), if ey > R




S(t)−1S2 ×sgn(ey ), if R ≥ ey  >
 (R/2)

S(t +1) =
ey  > (R/4)
S(t)−1S
×sgn(e
),
if
(R/2)
≥

1
y




S(t),
if ey  ≤ (R/4)
(3)

|e
|
A(t)−1A
×sgn(e
),
if
>
R

3
x
x


A(t)−1A2 ×sgn(ex ), if R ≥ |ex | > (R/2)
A(t +1) =
A(t)−1A
if (R/2) ≥ |ex | > (R/4)

1 ×sgn(ex ),


A(t),
if |ex | ≤ (R/4)
(4)
5056

FIGURE 5. The system flowcharts of the anchoring effect adjustments.

where R represents the radius of the basket, ex and ey are the
errors from the center of the basket to the falling location of
the ball, and S(t + 1) and A(t + 1) are the next output values
of the rotational speed and angle at time t. M S1 , M S2 , M S3 ,
M A1 , M A2 , and M A3 are the adjusting parameters for the
shooting rotational speed and angle.
In aspect of the methods of System 2, both the long-term
memory type and the peak-end rule type can remember all
the past experiences. However, the estimating method of the
long-term memory type is obtained by a regression line, and
this regression line is calculated from all the shooting experiences. Although these three methods all give acceptable
results, the performances of the long-term memory type and
peak-end rule type are better than those of the short-term
memory type. On the other hand, the peak-end rule type just
uses the best and the last experiences to correct the shooting
motion. The calculated amount of the peak-end rule type is
lower than that of the long-term memory type.
C. ANCHORING EFFECT

The anchoring effect has been widely used in the commerce
field. However, this is the first paper which has adopted the
concept in robots. The adjusting process of the anchoring
effect (global anchor) is illustrated in Fig. 5(a). If the voltage
of the batteries is insufficient, the robot will start the adjusting
process of the global anchor. After the robot shoots the ball
using System 1, the shooting error will be calculated. If the
error is not acceptable for the robot, the anchoring effect of
System 2 will be enabled to modify the effect of System 1
by a global anchor. Because the robot cannot ensure the best
position of the global anchor, System 2 will continuously
adjust the global anchor to obtain the most appropriate value.
If the adjustment is completed, then the global anchor will not
be modified until the status of the robot is restored.
VOLUME 4, 2016

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

The adjusting procedure of the anchoring effect (local
anchor) is depicted in Fig. 5(b). Unlike the global anchor,
many local anchors can be added into the experience curves.
In addition, because of the damage to the robot, the robot
cannot throw precisely from some specific positions using
the old System 1. Therefore, if the robot cannot perform
a good shooting motion, a local anchor will be added into
System 1. Then, the anchoring effect of System 2 will start to
adjust the position of the local anchor. After several adjustments, the robot will be able to shoot precisely, and the local
anchor will also be fixed. If all the distances in which the
robot stands are adjusted, the adjustment is completed.
The process of the anchoring effect of System 1 is denoted
in Fig. 5(c). The robot will perform the shooting motion by
the rotational speed and angle from the experience curves.
On the other hand, the process of the anchoring effect of
System 2 is shown in Fig. 5(d). According to past shooting
experiences, a temporary anchor can be added or modified
to update the experience curves. Then the updated System 1
is used to obtain the next shooting motion. Besides, without a complicated retraining process, all the anchors can be
removed to perform the original System 1 if the robot reverts
to the initial condition.
V. EXPERIMENTAL RESULTS

In this section, three parts of the experiments are presented.
Part A is the basketball learning process. A comparison of the
three types of System 2 is shown in Part B. Furthermore, the
tuning processes of anchoring effect are revealed in Part C.

ability to change the ideas produced by System 1, and it
will take over to solve these problems. No matter the desired
shooting motion obtained by System 1 or System 2, the optimal shooting rotational speed and angle will be remembered
by an ‘‘anchor,’’ as shown in the blue circles in Fig. 6(a).
After the memory anchors are remembered, David Junior will
construct a new experience curve and walk forward to another
position for the next learning step. Finally, the learning process will be completed when the robot establishes all the
experience curves at 135 cm. After the learning process, we
place the ball randomly at 12 different positions. Then David
Junior gets the ball and utilizes the established System 1 to
shoot the ball into the basket, as shown in Fig. 6(b).
The learning environment of David II is presented
in Fig. 7. In Fig. 7(c), the trained experience curves are
different from those of David Junior because the robot model
and the shooting motion of David Junior and David II are
different. Furthermore, the effects of the changes in the rotational speed and angle are dependent. In other words, the
increased rotational speed may change the shooting direction,
and the changes in the shooting angle may also influence the
shooting power. Therefore, the experience curves may not be
linear at all. The cognition of human beings may be different,
which is also true for robots. And for different robot models,
various motions, or various tasks, System 1 can be established
in different ways to achieve specific assignments.

A. BASKETBALL LEARNING PROCESS

The primary purpose of the issue is to make the robot learn
the shooting rotational speed and angle by himself. Here we
adopt two robots, David Junior and David II, to accomplish
this task.

FIGURE 7. The learning environment and the results of David II.

B. PEAK-END RULE
FIGURE 6. The learning process and final results.

The construction process of System 1 is illustrated in
Fig. 6(a). The upper and lower figures belong to the
shooting rotational speed and angle, respectively. Initially,
David Junior stands at the position where the distance
between the basket and the robot is 165 cm. In general, David
Junior acts like a human being, and his decisions are instinctively made by System 1 at first. However, the decisions
made by System 1 may not always be reasonable [43]. If
a decision is wrong and unacceptable, System 2 has some
VOLUME 4, 2016

The detailed thinking methods of System 2 can change.
For example, the street parking skills of everyone are different, and they also consider the specific subjects they want for
the driving references. Even though the thinking methods of
System 2 are different, they can still achieve the same goal.
Here we utilize three types of System 2. For the short-term
memory type, the robot can only remember the last shot, and
he only uses the last experience to adjust the shooting motion.
For the long-term memory type, the robot can remember all
the shots, and he can calculate the best shooting motion by
the regression line of all the shooting experiences. For the
5057

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

FIGURE 8. Peak-end rule based System 2.

FIGURE 10. The adjusting process of anchoring effect.

1) GLOBAL ANCHORS

FIGURE 9. The experimental results of three types of System 2.

peak-end rule type, the robot can remember all the shots, but
only the best and the latest shot are considered for finding
the optimal shooting motion, as shown in Fig. 8(a) and (b).
Fig. 9 shows the experimental results of the three types of
System 2. In this experiment, David Junior stands at three
different positions and uses the three types of System 2 to
estimate the optimal shooting motion. For these three cases,
David Junior can obtain acceptable results from all types
of System 2. Obviously, the performances of the long-term
memory and the peak-end rule types are about the same,
but better than the short-term memory type. However, the
calculated amount of peak-end rule type is lower than the
long-term memory type. Consequently, the peak-end rule
type of System 2 is suggested to be applied for this issue. Surprisingly, the concept of peak-end rule, which is seemingly
defective in human beings, is helpful in the process.
C. ANCHORING EFFECT

After the learning process, the shooting motion may change
due to a variety of factors, such as the charge of the batteries,
the aging of the mechanism, the flatness of the ground, and
so on. These factors will change the shooting motion and
cause unexpected results. Besides the experience curves representing the angle and rotational speed with which the robot
should perform, the optimal angles and rotational speeds in
the impression are also assembled by the anchors of the robot.
When the robot’s old cognition already cannot fit in with the
actual environment, it is time to adjust the cognition of the
robot. Based on the adjustability of the anchors in the human
brain, we establish a learning procedure for David Junior by
altering the anchors to overcome these problems. Following
are two types of external anchors the robot may adopt.
5058

When we walk to the brand counters, we naturally feel that
the commodity prices are much more expensive here than
in other areas. This means that in order to meet the reality
of the situation, our consumption anchors improve comprehensively. Similarly, if the robot has low battery voltage, as
shown in Fig. 10(a), the overall shooting rotational speed
will be reduced. Therefore, in order to make the robot meet
our expectations, adding a global anchor to the robot is an
efficient way. All the values will be shifted higher or lower
by the global anchor to correct the shooting motion. According to the shooting errors, David Junior can also adjust the
value of the global anchor. The adjusting process is shown
in Fig. 10(b).
2) LOCAL ANCHORS

When we walk in an environment which is surrounded with
brands, sometimes we wonder why certain commodities are
still cheap there. Therefore, at this time, some anchors of
these certain commodities will be reduced automatically.
Similarly, if the robot is damaged by an accident, as shown in
Fig. 10(c), the robot may not perform well in some specific
cases. Therefore, placing local anchors on the robot is a good
choice for solving this problem. As shown in Fig. 10(d),
the experience curves will be revised precisely by the local
anchors (the orange circles). The adjusting process of the
local anchors is shown in Fig. 10(d).
As basketball players do, if their physical condition is not
very good or the competition environment is not familiar to
them, the players will try to adjust their playing motions to
keep up their skill level. However, if the players come back to
the accustomed environment in good physical condition, they
can simply remove the adjustments and not have to relearn all
of their basketball playing skills to play the game. This is the
same with robots; if all the situations of the robot are restored,
the added anchors can be removed immediately. The robot’s
System 1 will also revert to the initial situation. Besides, in
order to verify the feasibility of the proposed learning concept
VOLUME 4, 2016

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

in other robot models and motions, David II participates in
experiments. David II is also a humanoid robot, but he is
higher than David Junior. In addition, the throwing motion
of David II is different from that of David Junior. No matter
what robot we use, or what throwing motion it performs, the
throwing task scan still be achieved by the proposed learning
concept. Moreover, the models of System 1 and the adjusting
method of System 2 can be modified for different cases. For
the reasons above, the proposed learning concept is practical
and can also be utilized for many applications in the future.
VI. DISCUSSIONS

Previously, the conventional method to achieve this task was
dealing with human-in-the-loop testing, and the tuning time
was about 90 minutes. The proposed method provides an
autonomous learning procedure for a robot, and the learning time is within 20 minutes or even faster. The operating efficiency is much improved, and the robot can shoot
precisely from any distance and direction after the learning
process. In the experiment, the robot can shoot precisely at
12 random positions. Besides, the competition results also
validate the robustness of the proposed method. If the charge
of the batteries or the flatness of the ground is changed,
the anchoring effect method can be immediately adopted
for the correction of the robot’s experience curves. In System 2, the concept of peak-end rule used in the algorithm can
also reduce the computational cost. Since the first international robot competition was held at KAIST, Daejeon, Korea
in 1996, the FIRA RoboWorld Cup [44] has become an
important indicator of international robot contests in the
world. Our laboratory, aiRobots, has been actively involved
in this contest for a decade. Moreover, we have won the
championship in the basketball event for six consecutive
years and the all-round championship for last three years.
Obviously, such an algorithm which mimics human thinking
is feasible and practical. The experiments also verify that the
thinking mode of a human brain is reasonable, and it can also
be applied to make the robot think more like a human being.
The proposed learning algorithm is similar to the internal
and external control loops in a complicated control system.
Nevertheless, in the field of system control, some theories
and control methods also include the analogous concepts of
fast and slow, such as implicit and explicit control [45], slowfast systems in singular perturbation theory [46], and cascade
control [47]. However, these concepts must involve a system
model and so are hard to apply in this work. The autonomic
nervous systems and higher cognitive functions in animals are
one kind of neurological cognition. In this paper, the proposed
algorithm emphasizes the learning of cognitive behaviors to
establish a human-like learning system in a humanoid robot.
In order to reduce the effect of the luminance variations,
HSV [48] color space is used in the searching, and matching process of this work. However, if the complexity of the
searched object is getting higher, the Binary Robust Invariant
Scalable Keypoints (BRISK) [49] can be applied in the future
work. Besides, the model of System 1 can also be changed.
VOLUME 4, 2016

In order to improve the accuracy of the System 1 in the future
work, the established experiences curves can be represented
by Extreme Learning Machines (ELMs) [50], which tries to
make human like machines with minimum training.
For scientists, one of the main goals of robotic development
is for robots to be made to more closely resemble human
beings. The proposed learning method is very close to that of
human thinking, and these thinking behaviors have already
been proved by psychological theories. Moreover, the celebrated phenomena in psychology called peak-end rule and
anchoring effect are also utilized in this cognitive learning
algorithm in an unprecedented way to improve the learning
process and the final results. For a basketball player, the
training process for the shooting motion must be executed
from every orientation. However, David Junior can be trained
in just one direction, and he performs the precise shot in all the
orientations through the proposed method. In the experiments
and competitions, there are many good validations showing
that the proposed novel learning method is feasible and practical, and it indeed enhances the intelligence of the humanoid
robot in the basketball training event.
D. Kahneman began surveying the thinking systems of
human beings in 1979, and he was awarded the Nobel
Memorial Prize in Economics in 2002. In 2011, he wrote
a book, Thinking, Fast and Slow, to systematically address
these research achievements. The current paper continues to
use these appellations, which are unprecedented, to design
a novel learning method for humanoid robots. In this paper,
the theoretical evidence and empirical proofs of the thinking
systems have already been proved by psychologists in many
research studies [3]–[5]. The evidence is quite strong and
exact, and it has matured in psychology in the past few years.
Moreover, these psychological concepts used in basketball
learning events have also been validated in experiments and
robot competitions. As a result, this paper adopts these concepts in reasonable ways. From the standpoint of engineering,
at least the proposed learning algorithm is feasible for realization in a humanoid robot.
Up to now, the literature on basketball learning applications
is still quite scarce. Classical methods have been unable to
solve shooting motion learning from a practical standpoint.
In terms of learning, any method can be utilized for the fixed
shooting position, but these methods are hard to integrate
into System 1 and System 2 since the proposed learning
method is not only a process of finding the best parameter
or solution, but also an entire thinking framework in robot
intelligent systems. In the aspect of the basketball learning
event, the construction procedure of the experience curves
requires two reflecting cores. One is the instinct response
(System 1) for testing; the other one is the rational thinking
(System 2) for supervising and correcting. If and only if
the experience curves are comprehensively established can
the robot shoot precisely from various distances. However,
the general classical methods do not consist of two such
computing cores. Due to this issue, classical methods are hard
to apply in this case. A specific problem may be solved by
5059

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

several methods. However, this is the first paper which has
adopted the concepts of cognitive psychology from Thinking,
Fast and Slow to develop a learning method for a humanoid
robot.
The learning algorithm proposed in this paper is inspired
by the concept of human thinking systems, and this learning
method can be further used in many applications. For example, when a small child is learning to catch an object, he or she
may perform imprecisely at the beginning. This is because
the System 1 of the child is not well established in this state.
Therefore, System 2 will take over the decision process to
‘‘think’’ how to accomplish this task. In the process of dealing
with the correction by System 2, System 1 is also modified
according to the experiences of the child. After several tries,
the System 1 of the child will be well established, and she or
he can finally catch the object intuitively without any misses.
However, this thinking method not only works in human
beings, but also in robots. The ability to catch an object is
an important issue in robotics, and the proposed method can
also be used for this function in future work. This is the first
paper to adopt the concepts of cognitive psychology, which
may provide a different and efficient way to solve learning
problems in the future.
The proposed concept applied in robots is novel, and
the basketball event is chosen for the validations. However,
besides the applications of basketball events and catching an
object, there are still many functions which can adopt this
learning method. As long as the behavior can be executed by
a fitting model (even just a lookup table), the fitting model
can be treated as System 1. Then complicated calculations
(System 2) can be established to monitor and modify the
decisions of System 1. As a result, the learning effect is
already achieved, and this learning method can easily be
utilized in many applications. Moreover, the model of System 1 and the calculating method of System 2 are changeable,
depending on the experimental environments or different
cases. For other applications, the proposed algorithm may
provide a good solution for robotic learning issues in the
future.

the future, and the preliminary step of the investigation of a
robot’s cognitive psychology is demonstrated in this paper.
APPENDIX

In the aspect of controlling the falling position of the ball,
inverse kinematics cannot provide the entire solution for
adjusting the shooting motion because this motion must
involve the speed and angle control of the server motors.
Nevertheless, inverse kinematics still plays an important role
in this study. As shown in the supplementary video, there
are many hand motions David Junior must perform, such as
catching the ball, raising the ball for shooting, and so on.
Except for the final state of the shooting motion, all the hand
motions of David Junior are controlled by inverse kinematics.

FIGURE 11. The coordinate frames and joint definition of David Junior’s
hand.

TABLE 2. D-H parameters of David Junior’s hand.

VII. CONCLUSIONS

A novel psychology learning algorithm has been proposed in
this paper. This algorithm is derived from Thinking, Fast and
Slow by Daniel Kahneman, the Nobel Memorial Prize winner
in 2002 for Economic Science. Like a human being, the developed humanoid robots, can apply the thinking modes, System 1 and System 2, to accomplish the learning process for
basketball games. The concepts of peak-end rule and anchoring effect are unprecedentedly used to enhance the learning
process and final results. What is noteworthy is that these
two phenomena are seemingly defective in human beings, but
are very helpful in the learning procedures. Furthermore, the
performance and practicality of the proposed cognitive learning algorithm has had good validations at FIRA RoboWorld
Cup 2015. By the construction of the cognitive learning algorithm, this concept can be applied to many applications in
5060

The coordinate frames and the joint definition of
David Junior’s hand are illustrated in Fig. 11. According
to these definitions, the Denavit-Hartenberg (D-H) parameters of David Junior’s hand can be established, as shown in
Table 2. Therefore, the transformation matrices can also be
obtained as follows.


cos θi −cos αi sin θi
sin αi sin θi
ai cos θi
sin θi
cos αi cos θi −sin αi cos θi ai sin θi

Hii−1 = 
 0
sin αi
cos αi
di 
0
0
0
1
VOLUME 4, 2016

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket


cos θ1 0
sin θ1
0
 sin θ1 0 − cos θ1 0 


 0
1
0
0
0
0
0
1

◦
cos (θ2 − 90 ) 0
sin (θ2 − 90◦ )
 sin (θ2 − 90◦ ) 0 − cos (θ2 − 90◦ )


0
1
0
0
0
0


sin θ2
0 − cos θ2 0
 − cos θ2 0 − sin θ2 0 



0
1
0
0
0
0
0
1


cos θ3 0
sin θ3
0
 sin θ3 0 − cos θ3 0 


 0
1
0
l3 
0
0
0
1


0 −1 0 0
1
0
0 0


0
0
1 0
0
0
0 1


cos θ4 − sin θ4 0 l4 cos θ4
 sin θ4
cos θ4
0 l4 sin θ4 


 0

0
1
0
0
0
0
1


cos θ5 − sin θ5 0 l5 cos θ5
 sin θ5
cos θ5
0 l5 sin θ5 


 0

0
1
0
0
0
0
1


−1 0 0 0
 0
0 1 0


 0
1 0 0
0
0 0 1


H10 =

H21 =

=

H320 =

0

H33 =

H43 =

H54 =

H65 =


0
0

0
1

Then, we can define the matrix P to represent the transformation matrix H50 . According to the position (Px , Py , Pz )
and the orientation matrix we gave, all the rotational angles of
each server motor can be obtained, as shown in the following
equation.
0

H50 = H10 H21 H320 H33 H43 H54 H65

r11 r12 r13
 r21 r22 r23
=P=
 r31 r32 r33
0
0
0


Px
Py 

Pz 
1

Fig. 12 shows the geometric relationships among the links.
According to the relationships, l05 can be obtained by the
following equations. Then, we can use the cosine theorem to
calculate θ4 , and determine the plus or minus sign of θ4 .


∗ ∗ ∗ Px + l5 × r11

−1  ∗ ∗ ∗ P + l × r 
y
5
21 
H40 = P H54 H65
=
 ∗ ∗ ∗ Pz + l5 × r31 
0 0 0
1
q
l05 = (Px +l5 × r11 )2 +(Px +l5 × r21 )2 +(Px +l5 × r31 )2
VOLUME 4, 2016

FIGURE 12. The geometric relationships among the links.

θ4 = ± cos−1

2 − l2 − l2
l05
4
3
2l3 l4

!

In order to obtain the angles of α and β, l0P must first be
calculated. As depicted in Fig. 12(a), we can also figure out
the relationship of θ5 , α and β. Then, the plus or minus sign
of θ5 is determined by θ4 .
q
l0P = P2x + P2y + P2z
!
2
2
2
−1 l05 + l4 − l3
α = cos
2l05 l4
!
2
2
2
−1 l05 + l5 − l0P
β = cos
2l05 l5

θ5 = sgn(θ4 ) × 180◦ − α − β
Considering the following equation, θ3 can be obtained by
the left hand side and right hand side of H30 .

−1
0
H10 H21 H320 H33 = P H43 H54 H65


∗
∗
∗
∗
∗
∗
∗
∗

LHS = 
 ∗ cos (θ2 ) × cos (θ3 ) − cos (θ2 ) × sin (θ3 ) ∗ 
0
0
0
1


∗
∗
∗ ∗
∗
∗
∗ ∗

RHS = 
 ∗ r33 ×cos (θ4 +θ5 ) − r31 ×sin (θ4 +θ5 ) r32 ∗ 
0
0
0 1
−r32
sin (θ3 ) =
cos (θ2 )
r33 × cos (θ4 + θ5 ) − r31 × sin (θ4 + θ5 )
cos (θ3 ) =
cos (θ2 )


−r32
−1
θ3 = tan
r33 × cos (θ4 + θ5 ) − r31 × sin (θ4 + θ5 )
Furthermore, the left hand side and the right hand side of
matrix H20 can be calculated as the following equations.

−1
0
H10 H21 = P H320 H33 H43 H54 H65


∗
sin (θ1 )
∗
∗

∗
− cos (θ1 )
∗
∗

LHS = 
 − cos (θ2 )
∗
− sin (θ2 ) ∗ 
0
0
0
1
5061

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket



∗
∗
RHS = 
c
0

a
b
∗
0

∗
∗
d
0


∗
∗

∗
1

where
a = r11 × sin (θ4 + θ5 ) sin (θ3 )
−r13 × cos (θ4 + θ5 ) sin (θ3 ) − r12 × cos (θ3 )
b = r21 × sin (θ4 + θ5 ) sin (θ3 )
−r23 × cos (θ4 + θ5 ) sin (θ3 ) − r22 × cos (θ3 )
c = r32 × sin (θ3 ) − r33 × cos (θ4 + θ5 ) cos (θ3 )
+r31 × sin (θ4 + θ5 ) cos (θ3 )
d = −r31 × cos (θ4 + θ5 ) − r33 × sin (θ4 + θ5 )
Finally, θ1 and θ2 can be obtained by the following equations.
sin (θ1 ) = a,

− cos (θ1 ) = b


−a
θ1 = tan−1
b
− cos (θ2 ) = c, − sin (θ2 ) = d
 
−1 d
θ2 = tan
c
REFERENCES
[1] D. Kahneman, Thinking, Fast and Slow. New York, NY, USA:
Farrar, Straus, 2011.
[2] T.-H. S. Li, P.-H. Kuo, Y.-F. Ho, M.-C. Kao, and L.-H. Tai, ‘‘A biped gait
learning algorithm for humanoid robots based on environmental impact
assessed artificial bee colony,’’ IEEE Access, vol. 3, pp. 13–26, 2015.
[3] E. Diener, D. Wirtz, and S. Oishi, ‘‘End effects of rated life quality: The
James dean effect,’’ Psychol. Sci., vol. 12, no. 2, pp. 124–128, 2001.
[4] S. Kemp, C. D. B. Burt, and L. Furneaux, ‘‘A test of the peak-end rule
with extended autobiographical events,’’ Memory Cognit., vol. 36, no. 1,
pp. 132–138, 2008.
[5] A. Tversky and D. Kahneman, ‘‘Judgment under uncertainty: Heuristics
and biases,’’ Science, vol. 185, no. 4157, pp. 1124–1131, 1974.
[6] J. O. Kephart, ‘‘Learning from nature,’’ Science, vol. 331, no. 6018,
pp. 682–683, 2011.
[7] J. Holland, Genetic Algorithms. Scientific American, vol. 267, no. 1,
pp. 66–72, 1992.
[8] F. Wang, J. Li, S. Liu, X. Zhao, D. Zhang, and Y. Tian, ‘‘An improved adaptive genetic algorithm for image segmentation and vision alignment used
in microelectronic bonding,’’ IEEE/ASME Trans. Mechatronics, vol. 19,
no. 3, pp. 916–923, Jun. 2014.
[9] P.-H. Kuo, T.-H. S. Li, Y.-F. Ho, and C.-J. Lin, ‘‘Development of an
automatic emotional music accompaniment system by fuzzy logic and
adaptive partition evolutionary genetic algorithm,’’ IEEE Access, vol. 3,
pp. 815–824, 2015.
[10] T.-K. Liu, Y.-P. Chen, and J.-H. Chou, ‘‘Developing a multiobjective optimization scheduling system for a screw manufacturer: A refined genetic
algorithm approach,’’ IEEE Access, vol. 2, pp. 356–364, 2014.
[11] T. K. Liu, Y. P. Chen, and J. H. Chou, ‘‘Solving distributed and flexible jobshop scheduling problems for a real-world fastener manufacturer,’’ IEEE
Access, vol. 2, pp. 1598–1606, 2014.
[12] J. Kennedy and R. Eberhart, ‘‘Particle swarm optimization,’’ in Proc. IEEE
Int. Conf. Neural Netw., 1995, pp. 1942–1948.
[13] Y. Ding, L. Cheng, W. Pedrycz, and K. Hao, ‘‘Global nonlinear kernel
prediction for large data set with a particle swarm-optimized interval
support vector regression,’’ IEEE Trans. Neural Netw. Learn. Syst., vol. 26,
no. 10, pp. 2521–2534, Oct. 2015.
[14] Z. Ren, A. Zhang, C. Wen, and Z. Feng, ‘‘A scatter learning particle swarm
optimization algorithm for multimodal problems,’’ IEEE Trans. Cybern.,
vol. 44, no. 7, pp. 1127–1140, Jul. 2014.
5062

[15] P.-Y. Chou, J.-T. Tsai, and J.-H. Chou, ‘‘Modeling and optimizing tensile
strength and yield point on a steel bar using an artificial neural network
with taguchi particle swarm optimizer,’’ IEEE Access, vol. 4, pp. 585–593,
2016.
[16] Y. Gao, W. Du, and G. Yan, ‘‘Selectively-informed particle swarm optimization,’’ Sci. Rep., vol. 5, Mar. 2015, Art. no. 9295.
[17] D. Karaboga and B. Basturk, ‘‘A powerful and efficient algorithm for
numerical function optimization: Artificial bee colony (ABC) algorithm,’’
J. Global Optim., vol. 39, no. 3, pp. 459–471, Apr. 2007.
[18] W.-F. Gao, S.-Y. Liu, and L.-L. Huang, ‘‘A novel artificial bee colony
algorithm based on modified search equation and orthogonal learning,’’
IEEE Trans. Cybern., vol. 43, no. 3, pp. 1011–1024, Jun. 2013.
[19] T.-J. Hsieh and W.-C. Yeh, ‘‘Knowledge discovery employing grid scheme
least squares support vector machines based on orthogonal design bee
colony algorithm,’’ IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 41,
no. 5, pp. 1198–1212, Oct. 2011.
[20] M. Dorigo, V. Maniezzo, and A. Colorni, ‘‘Ant system: Optimization by
a colony of cooperating agents,’’ IEEE Trans. Syst., Man, Cybern. B,
Cybern., vol. 26, no. 1, pp. 29–41, Feb. 1996.
[21] M. Shen, W.-N. Chen, J. Zhang, H. S.-H. Chung, and O. Kaynak, ‘‘Optimal
selection of parameters for nonuniform embedding of chaotic time series
using ant colony optimization,’’ IEEE Trans. Cybern., vol. 43, no. 2,
pp. 790–802, Apr. 2013.
[22] Y. Zhou, X. Lai, Y. Li, and W. Dong, ‘‘Ant colony optimization with
combining Gaussian eliminations for matrix multiplication,’’ IEEE Trans.
Cybern., vol. 43, no. 1, pp. 347–357, Feb. 2013.
[23] L.-N. Xing, P. Rohlfshagen, Y.-W. Chen, and X. Yao, ‘‘A hybrid ant colony
optimization algorithm for the extended capacitated arc routing problem,’’
IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 41, no. 4, pp. 1110–1123,
Aug. 2011.
[24] X.-M. Hu, J. Zhang, H. S.-H. Chung, Y. Li, and O. Liu, ‘‘SamACO:
Variable sampling ant colony optimization algorithm for continuous optimization,’’ IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 40, no. 6,
pp. 1555–1566, Dec. 2010.
[25] T. Libby et al., ‘‘Tail-assisted pitch control in lizards, robots and
dinosaurs,’’ Nature, vol. 481, no. 7380, pp. 181–184, 2012.
[26] A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret, ‘‘Robots that can adapt
like animals,’’ Nature, vol. 521, no. 7553, pp. 503–507, 2015.
[27] J. Werfel, K. Petersen, and R. Nagpa, ‘‘Designing collective behavior in
a termite-inspired robot construction team,’’ Science, vol. 343, no. 6172,
pp. 754–758, 2014.
[28] Y. LeCun, Y. Bengio, and G. Hinton, ‘‘Deep learning,’’ Nature, vol. 521,
no. 7553, pp. 436–444, 2015.
[29] Y. Park and M. Kellis, ‘‘Deep learning for regulatory genomics,’’ Nature
Biotechnol., vol. 33, no. 8, pp. 825–826, 2015.
[30] W. Hou, X. Gao, D. Tao, and X. Li, ‘‘Blind image quality assessment via
deep learning,’’ IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 6,
pp. 1275–1286, Jun. 2015.
[31] H. Goh, N. Thome, M. Cord, and J.-H. Lim, ‘‘Learning deep hierarchical
visual feature coding,’’ IEEE Trans. Neural Netw. Learn. Syst., vol. 25,
no. 12, pp. 2212–2225, Dec. 2014.
[32] Y. Yuan, L. Mou, and X. Lu, ‘‘Scene recognition by manifold regularized deep learning architecture,’’ IEEE Trans. Neural Netw. Learn. Syst.,
vol. 26, no. 10, pp. 2222–2233, Oct. 2015.
[33] J. Evans and A. Rzhetsky, ‘‘Machine science,’’ Science, vol. 329, no. 5990,
pp. 399–400, 2010.
[34] H. Moravec, Rise of the Robots, Scientific American Reports, vol. 18, no. 1,
pp. 12–19, 2008.
[35] D. J. Cook, ‘‘How smart is your home?’’ Science, vol. 335, no. 6076,
pp. 1579–1581, 2012.
[36] H. J. Briegel and G. De Las Cuevas, ‘‘Projective simulation for artificial
intelligence,’’ Sci. Rep., vol. 2, May 2012, Art. no. 400.
[37] J. Halloy et al., ‘‘Social integration of robots into groups of cockroaches to control self-organized choices,’’ Science, vol. 318, no. 5853,
pp. 1155–1158, 2007.
[38] Y. Wilks, ‘‘Is there progress on talking sensibly to machines?’’ Science,
vol. 318, no. 5852, pp. 927–928, 2007.
[39] M. Rubenstein, A. Cornejo, and R. Nagpal, ‘‘Programmable self-assembly
in a thousand-robot swarm,’’ Science, vol. 345, no. 6198, pp. 795–799,
2014.
[40] G. M. Edelman, ‘‘Learning in and from brain-based devices,’’ Science,
vol. 318, no. 5853, pp. 1103–1105, 2007.
[41] C. Adami, ‘‘What do robots dream of?’’ Science, vol. 314, no. 5802,
pp. 1093–1094, 2006.
VOLUME 4, 2016

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

[42] A. N. Meltzoff, P. K. Kuhl, J. Movellan, and T. J. Sejnowski, ‘‘Foundations
for a new science of learning,’’ Science, vol. 325, no. 5938, pp. 284–288,
2009.
[43] A. Tversky and D. Kahneman, ‘‘The framing of decisions and the psychology of choice,’’ Science, vol. 211, no. 4481, pp. 453–458, 1981.
[44] Website of FIRA, accessed on Jul. 1, 2016. [Online].
http://www.fira.net/main
[45] P. Mazzoni and N. S. Wexler, ‘‘Parallel explicit and implicit control of
reaching,’’ PLoS ONE, vol. 4, no. 10, p. e7557, 2009.
[46] T.-H. S. Li and K.-J. Lin, ‘‘Composite fuzzy control of nonlinear singularly
perturbed systems,’’ IEEE Trans. Fuzzy Syst., vol. 15, no. 2, pp. 176–187,
Apr. 2007.
[47] J. Zhang, S. Zhou, M. Ren, and H. Yue, ‘‘Adaptive neural network cascade
control system with entropy-based design,’’ IET Control Theory Appl.,
vol. 10, no. 10, pp. 1151–1160, 2016.
[48] I. Yoon, S. Kim, D. Kim, M. H. Hayes, and J. Paik, ‘‘Adaptive defogging
with color correction in the HSV color space for consumer surveillance
system,’’ IEEE Trans. Consum. Electron., vol. 58, no. 1, pp. 111–116,
Feb. 2012.
[49] S. Leutenegger, M. Chli, and R. Y. Siegwart, ‘‘BRISK: Binary robust
invariant scalable keypoints,’’ in Proc. IEEE Int. Conf. Comput. Vis.,
Nov. 2011, pp. 2548–2555.
[50] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, ‘‘Extreme learning machine:
Theory and applications,’’ Neurocomputing, vol. 70, nos. 1–3,
pp. 489–501, 2006.

TZUU-HSENG S. LI (S’85–M’90) received the
B.S. degree in electrical engineering from the
Tatung Institute of Technology, Taipei, Taiwan, in
1981, and the M.S. and Ph.D. degrees in electrical engineering from the National Cheng Kung
University (NCKU), Tainan, Taiwan, in 1985 and
1989, respectively. Since 1985, he has been with
the Department of Electrical Engineering, NCKU,
where he is currently a Distinguished Professor.
From 1996 to 2009, he was a Researcher with the
Engineering and Technology Promotion Center, National Science Council,
Tainan. From 1999 to 2002, he was the Director of the Electrical Laboratories, NCKU. From 2009 to 2012, he was the Dean of the College of Electrical
Engineering and Computer Science, National United University, Miaoli City,
Taiwan. He has been the Vice President of the Federation of International
Robot-Soccer Association since 2009. He has been the Director of the Center
for Intelligent Robotics and Automation, NCKU, since 2014. His current
research interests include artificial and/or biological intelligence and applications, fuzzy system and control, home service robots, humanoid robots,
mechatronics, 4WIS4WID vehicles, and singular perturbation methodology.
Dr. Li was a recipient of the Outstanding Automatic Control Award in 2006
from the Chinese Automatic Control Society (CACS) in Taiwan. He was a
Technical Editor of the IEEE/ASME TRANSACTIONS on MECHATRONICS and an
Associate Editor of the Asia Journal of Control. He is currently an Associate
Editor of the International Journal of Electrical Engineering, the International
Journal of Fuzzy Systems, and the IEEE TRANSACTIONS on CYBERNETICS. He
was elected as the President of the CACS from 2008 to 2011 and the Robotics
Society of Taiwan from 2012 to 2015. In 2008, he was elevated to CACS
Fellow.

PING-HUAN KUO was born in Pingtung, Taiwan,
in 1986. He received the B.S., M.S., and Ph.D.
degrees from the Department of Electrical Engineering from National Cheng Kung University,
Tainan, Taiwan, in 2008, 2010, and 2015, respectively. His major research interests include fuzzy
control, intelligent algorithms, humanoid robot,
image processing, and robotic application.

VOLUME 4, 2016

YA-FANG HO was born in Kaohsiung, Taiwan,
in 1988. She received the B.S. and M.S. degrees
with the Department of Electrical Engineering,
National Cheng Kung University, Tainan, Taiwan, in 2010 and 2011. She is currently pursuing
the Ph.D. degree with the Department of Electrical Engineering, National Cheng-Kung University.
Her major research interests include fuzzy control,
intelligent system, humanoid robot, image processing, robotic application, and FIRA/RoboCup
game.

CHIN-YIN LIU received the B.S. degree from the
Department of Social Work from National Taiwan
University, Taipei, Taiwan, in 2003. She received
the M.S. degree with the Department of Psychology from National Chung Cheng University, ChiaYi, Taiwan, in 2005. She is currently pursuing
the Ph.D. degree with the Department of Electrical Engineering, National Cheng-Kung University, Tainan, Taiwan. Her major research interests
include intelligent control system, robot learning,
home service robot, robot cooperation, and human–robot interaction.

TING-CHIEH YU received the B.S. and M.S.
degrees with the Department of Electrical Engineering, National Cheng-Kung University, Tainan,
Taiwan, in 2012 and 2014. Her major research
interests include fuzzy control, intelligent system,
humanoid robot, image processing, robotic application, and FIRA/RoboCup game.

YAN-TING YE received the B.S. and M.S. degrees
with the Department of Electrical Engineering
from National Cheng-Kung University, Tainan,
Taiwan, in 2013 and 2015, respectively. His major
research interests include fuzzy control, intelligent system, humanoid robot, image processing,
robotic application, and FIRA/RoboCup game.

CHIEN-YU CHANG received the B.S. degree
from the Department of Electronic Engineering, Chung Yuan Christian University, Taoyuan,
Taiwan, in 2009, and the M.S degrees with
the Department of Electrical Engineering from
National Cheng-Kung University, Tainan, Taiwan,
in 2016. His major research interests include
fuzzy control, intelligent system, humanoid
robot, image processing, robotic application, and
FIRA/RoboCup game.

5063

T.-H. S. Li et al.: Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket

GUAN-YU CHEN received the B.S. and M.S.
degrees from the Department of Electrical Engineering, National Cheng-Kung University, Tainan,
Taiwan, in 2013 and 2015, respectively. His major
research interests include fuzzy control, intelligent system, humanoid robot, image processing,
robotic application, and FIRA/RoboCup game.

WEI-CHUNG CHEN received the B.S. and M.S.
degrees from the Department of Electrical Engineering, National Cheng-Kung University, Tainan,
Taiwan, in 2014 and 2016, respectively. His major
research interests include fuzzy control, intelligent system, humanoid robot, image processing,
robotic application, and FIRA/RoboCup game.

LI-FAN WU received the B.S. degree from the
Department of Mechanical Engineering, National
Cheng-Kung University, Tainan, Taiwan, in 2015.
He is currently pursuing the M.S. degree with the
Department of Electrical Engineering, National
Cheng-Kung University. His major research interests include fuzzy control, intelligent system,
humanoid robot, image processing, robotic application, and FIRA/RoboCup game.

CHIH-WEI CHIEN received the B.S. and M.S.
degrees from the Department of Electrical Engineering, National Cheng Kung University, Tainan,
Taiwan, in 2014 and 2016, respectively. His major
research interests include fuzzy control, intelligent system, humanoid robot, image processing,
robotic application, and FIRA/RoboCup game.

5064

NIEN-CHU FANG received the B.S. degree
from the Department of Electrical Engineering, National Cheng-Kung University, Tainan,
Taiwan, in 2015. He is currently pursuing the M.S.
degree with the Department of Electrical Engineering, National Cheng-Kung University. His major
research interests include fuzzy control, intelligent system, humanoid robot, image processing,
robotic application, and FIRA/RoboCup game.

VOLUME 4, 2016

See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/305828246

A	review	of	key	techniques	of	vision-based
control	for	harvesting	robot
Article		in		Computers	and	Electronics	in	Agriculture	·	September	2016
DOI:	10.1016/j.compag.2016.06.022

CITATIONS

READS

0

126

4	authors,	including:
Yuanshen	Zhao

Liang	Gong

Shanghai	Jiao	Tong	University

Shanghai	Jiao	Tong	University

12	PUBLICATIONS			10	CITATIONS			

35	PUBLICATIONS			84	CITATIONS			

SEE	PROFILE

SEE	PROFILE

Yixiang	Huang
Shanghai	Jiao	Tong	University
27	PUBLICATIONS			163	CITATIONS			
SEE	PROFILE

Some	of	the	authors	of	this	publication	are	also	working	on	these	related	projects:

A	study	on	key	techniques	of	multi-arm	harvesting	robot	View	project

All	content	following	this	page	was	uploaded	by	Yuanshen	Zhao	on	06	August	2016.
The	user	has	requested	enhancement	of	the	downloaded	file.	All	in-text	references	underlined	in	blue	are	added	to	the	original	document
and	are	linked	to	publications	on	ResearchGate,	letting	you	access	and	read	them	immediately.

Computers and Electronics in Agriculture 127 (2016) 311–323

Contents lists available at ScienceDirect

Computers and Electronics in Agriculture
journal homepage: www.elsevier.com/locate/compag

Review

A review of key techniques of vision-based control for harvesting robot
Yuanshen Zhao, Liang Gong, Yixiang Huang, Chengliang Liu ⇑
School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China

a r t i c l e

i n f o

Article history:
Received 11 December 2015
Received in revised form 16 June 2016
Accepted 20 June 2016

Keywords:
Harvesting robot
Vision-based control
Vision information acquisition
Fruit recognition
Eye-hand coordination

a b s t r a c t
Although there is a rapid development of agricultural robotic technologies, a lack of access to robust fruit
recognition and precision picking capabilities has limited the commercial application of harvesting
robots. On the other hand, recent advances in key techniques in vision-based control have improved this
situation. These techniques include vision information acquisition strategies, fruit recognition algorithms, and eye-hand coordination methods. In a fruit or vegetable harvesting robot, vision control is
employed to solve two major problems in detecting objects in tree canopies and picking objects using
visual information. This paper presents a review on these key vision control techniques and their potential applications in fruit or vegetable harvesting robots. The challenges and feature trends of applying
these vision control techniques in harvesting robots are also described and discussed in the review.
Ó 2016 Elsevier B.V. All rights reserved.

Contents
1.
2.

3.

4.

5.

6.
7.

8.

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Vision-based control for harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.
The concept of vision-based control for harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.
The role of vision-based control in harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Vision schemes for harvesting robot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.
Monocular camera scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.
Binocular stereovision scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.
Laser active visual scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.
Thermal imaging scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5.
Spectral imaging scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Recognition approaches for harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.
Single feature analysis approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.
Multiple features fusion approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3.
Pattern recognition approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Eye-hand coordination in harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.1.
Open-loop visual control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2.
Visual servo control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Examples of fruit harvesting robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Challenges and future trends. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.1.
Enhancing the vision-based control of harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2.
Human–machine collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3.
Multi-arms cooperating for robotic harvesting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.4.
Making the environment more suitable for robotic harvesting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Conclusions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

⇑ Corresponding author.
E-mail address: chlliu@sjtu.edu.cn (C. Liu).
http://dx.doi.org/10.1016/j.compag.2016.06.022
0168-1699/Ó 2016 Elsevier B.V. All rights reserved.

312
312
312
312
312
313
313
314
314
315
316
316
316
317
318
318
318
319
319
319
320
320
320
321
321
321

312

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

1. Introduction

2. Vision-based control for harvesting robot

With the development of modern agriculture, the application of
robotic and intelligent machines in agriculture has followed several trends in technology advancements. Firstly, increasing cost
and decreasing supply of skilled labor force are becoming a huge
challenge to the agriculture industry. Traditional farming is highly
labor intensive and contains many menial and tedious tasks, it is
one of the last industries to use robots (Sistler, 1987). Secondly,
food safety is an important issue that requires the use of reliable
robotic machines to reduce the risk of contaminations (Edan
et al., 2009). Thirdly, sustainable agriculture, which provides
enough food while not harming the environment, also needs
robotic systems to improve productivity at low cost (Grift et al.,
2008). It is evident that wide application of robots can offer a significant benefit to agriculture.
The emergence of agricultural robots is accompanied with
other industries such as manufacturing and mining that have
embraced the robotic revolution. Agricultural robots are perceptive and intelligent machines which are programmed to perform
a variety of agricultural tasks such as transplanting, cultivating,
spraying, trimming and harvesting (Edan et al., 2009).
Considering the economic benefit, high harvesting costs have
led the harvesting robot as a research focus (Hayashi et al.,
2005). Harvesting robots are designed to sense the complex
agricultural environment by various sensors and use that
information, together with a goal, to perform the harvesting
actions (Edan and Gaines, 1994). Although harvesting robot holds
ample promise for the future, currently the overall performance
of harvesting robot is often insufficient to compete with manual
operation (Grift et al., 2008). The bottleneck to promote the application of harvesting robot lies on the performance of vision-based
control.
The use of visual information for the control of robotic manipulator is called vision-based control, which began with the work of
Shirai and Inoue (1973). The progress of vision control in that era
was hindered largely by various technological issues, in particular,
extracting information form vision sensors (Corke and Hager,
1998). Since 1990, there has been a marked rise in the interest in
this field of vision control, largely fueled by the increasing computing power of personal computers. After that, vision-based control
for harvesting robot had ushered in the era of rapid development.
Although numerous research results have been reported on development of vision control technology for robotic harvesting, the low
successful rate of fruit recognition and inefficiency of eye-hand
coordination are the main factors to limit the performance of harvesting robot. Thus, a review of this research field is necessary to
promote further developments of vision-based control technology
for harvesting robot.
This article provides a review of the past and current research
and development of vision-based control for harvesting robot. It
is aimed to introduce an up-date account of useful methods found
in literature to provide solutions to the two key issues: (a) the
recognition of target fruit; (b) eye-hand coordination control. The
remaining of this paper is organized as follows. In Section 2, a general background to vision-based control is introduced. Representative vision schemes for harvesting robots are presented in
Section 3. In Section 4, approaches adopted for fruit target recognitions are discussed. A review of eye-hand coordination techniques
is given in Section 5. Section 6 presents some examples of fruit or
vegetable harvesting robots. Challenges and future trends for harvesting robots are discussed in Section 7. A conclusion is drawn in
Section 8.

2.1. The concept of vision-based control for harvesting robot
Vision-based control for harvesting robot is a framework by
which the robot accomplishes the fruit picking task under the
guidance of visual information. This framework is constructed with
two objectives; fruit recognition and eye-hand coordination
(Hashimoto, 2003). Automatic fruit recognition for harvesting
robot means identifying and locating the fruit in a natural complex
scene. These two tasks are the foundation of picking operation.
Eye-hand coordination for harvesting robot is concerned with the
interaction between the robot visual perception of the workspace and it actuators (Goncalves and Torres, 2010).

2.2. The role of vision-based control in harvesting robot
The idea of robotic harvesting was firstly proposed by Schertz
and Brown (1968) in 1960s for citrus harvesting. Compared with
the traditional mechanical harvesting approaches using shaker or
air blast, robotic harvesting is a precision harvesting approach.
Typical fruit or vegetable harvesting robots are built with manipulators, end-effectors, vision systems, and motion systems (Edan
et al., 2009). Among these, vision-based control plays an important
role of autonomous harvesting. On the contrary to industrial
robots which are simple, repetitive, well-defined and known a priori, harvesting robots need to work in an unstructured, uncertain,
and varying environment. Vision-based control for harvesting
robots is designed to solve the follow difficult problems. Firstly,
the manipulated objects of harvesting robots are natural objects
which have a high degree of variety in fruit size, shape, color, texture and hardness as a result of environmental and genetic differences. Secondly, the workspace is complex and loosely structured
with large variations in illumination and degree of object occlusion. Thirdly, the random location of target fruits requires picking
to operate in a three-dimensional continuously changing track.
Thus, vision-based control is an attractive approach to meet these
challenges.

3. Vision schemes for harvesting robot
Fruit or vegetable detection for harvesting robot is conducted
by various visual sensors. According to the principle of imaging,
the visual sensors used to recognize objects are classified into
two-dimension (2D) visual imaging sensors and three-dimension
(3D) visual imaging sensors. The 2D images acquired can indicate
morphological features of the target fruit such as color, shape
and texture. Three-dimensional visual image sensors provide 3D
coordinate maps of the entire scene which can give the shape
and spatial location of the fruit object. The vision scheme also
has relationship with the recognition process. As shown in Fig. 1,
for identifying different kinds of objects, it is needed to select available visual sensors cooperating with a certain recognition
algorithm.
The review of recognition algorithms such as color based analysis, edge detection, K-means clustering, and Bayes classification is
given in Section 4. The follow sub-sections contain a critical review
of visual sensors used in the past for fruit detection in harvesting
robot. The applications, principles, advantages and limitations of
various vision schemes for harvesting robots are summarized in
Table 1.

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

Vision schemes

·Monocular
·Binocular stereoscope
·Laser active visual
·Spectral imaging
·Thermal imaging
ĊĊ

Algorithms

313

Objects

·Color based analysis
·Shape based analysis
·Texture based analysis
·Neural network approach
·Multi-feature fusion
·Fuzzy clustering means
·Support vector machine
ĂĂ

·Tomato
·Apple
·Cucumber
·Pepper
·Strawberry
·Citrus
·Eggplant
·ĂĂ

Fig. 1. Various vision schemes and recognition algorithms for different kinds of fruits (Li et al., 2011a,b).

Table 1
The classification of applications, principles, advantages and limitations of various vision schemes for harvesting robots.
Vision
scheme

Applications and principles

Advantages

Limitations

Monocular

Identifying target fruits by color, shape, and
texture feature
Identifying fruits using color shape, and texture
features; positing target fruits through the
principle of triangulation
Identifying fruits using 3D shape feature;
positing target fruits through 3D reconstruction
approach
Identifying target fruits using the differences of
infrared radiation between target fruit and
background
Identifying target fruits using features extracted
from invisible wavelengths

Monocular vision system is the simplest
and lowest cost
The binocular stereo is the most common
approach to obtain the 3D position of
detected fruit
It is an alternative to obtain the 3D position
in the condition of light changing and
background clustering
It is available to detect target fruit in
varying illumination condition, especially at
night
It can detect the green color or overlapped
fruits

Only provides 2D information, light change influences
the imaging results
Sensor calibration is required, image matching is very
time consuming; errors in 3D measurement is
unavoidable
Vision system is required for geo-referencing, complex
and large image data are needed. The imaging
processing is also a challenge
Sensor calibration and atmospheric correction are
required, high computation consumption for image
processing
Imaging processing is very time consuming, the sensor
cost is high

Binocular
stereo
Laser
active
visual
Thermal
imaging
Spectral
imaging

3.1. Monocular camera scheme

3.2. Binocular stereovision scheme

Monocular scheme is a machine vision system consisting of a
single camera, which was used in some earliest studies for detecting fruit (Jimenez et al., 2000a,b). The cameras with Charged Coupled Device (CCD) sensors or Complementary Metal Oxide
Semiconductor (CMOS) sensors are widely used in monocular
schemes. In the MAGALI project, a B/W camera was applied to
detect the fruits based on geometric feature (d’Esnon et al.,
1987). In later years, the B/W camera was replaced with a color
camera to enhance the color contrast between red apples and
green leaves. Slaughter and Harrell (1987) also used a digital color
camera with a filter of 675 nm wavelength to amplify the contrast
between oranges and background. The author reported a detection
accuracy of 75%. Zhao et al. (2005) used a color camera to identify
apples based on color and texture features and reported an accuracy of 90%. Baeten et al. (2008) developed a monocular vision system combining a camera and a high frequency light source to
detect apples in outdoor environment. The author recognized that
high frequency light could reduce the illumination influence. There
are other vision systems for harvesting robot with several cameras
forming a redundancy monocular system. Edan et al. (2000) developed a multiple monocular cameras system which was constructed
with two B/W CCD cameras to detect and locate melons in the
field. The two B/W cameras mounted on the platform and griper
could acquire far scene images and near scene images. The author
reported that the use of multiple monocular cameras could
improve the recognition accuracy. The major disadvantage of
monocular scheme is that images captured by the visual sensor
are sensitive to illumination conditions (see Table 2).

The Binocular stereovision scheme is designed with two cameras separated in a certain distance with an angle between them,
and they capture the same scene in two images. The threedimensional map of fruit object can be obtained through triangulation (Sun et al., 2011). Buemi et al. (1995, 1996) used a color
stereoscopic vision system consisting of two micro cameras in a
tomato harvesting robot named Agrobot. The stereovision system
mounted on the robot head could be used to navigate and identify
the ripe tomato. Shinsuke and Koichi (2005) installed a parallel
stereovision system of two cameras in a sweet pepper picking
robot. The stereovision system controlled by a camera positioning
system could move to a desired location to capture images of sweet
peppers. Yang et al. (2007) developed an improved stereovision
system based on the Color Layer Growing (CLG) algorithm to
reconstruct the 3D model of fruit object. Fruit objects with stereo
and self-occlusion in a strong sunlight condition could be detected
and located by the improved stereovision system. Xiang et al.
(2014) also studied a clustered tomato recognition method based
on depth map acquired by a binocular stereo camera. The recognition accuracy of clustered tomatoes was 87.9% at an acquisition
distance of 300–500 mm. Li et al. (2011a,b) developed a stereovision system, with optical filters, which was an attempt to capture
different waveband images. The recognition test results indicated
that the polarizer filtered data is slightly better than neutral density filtered data, and much better than the original image data.
Si et al. (2015) also used a stereo camera to detect and locate
mature apples in tree canopies. As shown in Fig. 2, the stereo camera was mounted on the slide bar in parallel with a distance of

314

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

Table 2
The comparison of different single feature analysis approaches.
Features

Fruits

Application situations

Accuracy

Limitations

Reference

Color

Tomato
Strawberry
Apple
Peach
Pineapple
Bitter melon

Artificial light condition
Uncontrolled environment
Uncontrolled environment
Uncontrolled environment
Field
Single objects

96.36%
>95%
94%
90%
85%
100%

It is not available to the green color fruit such as cucumber

Arefi et al. (2011)
Wei et al. (2014)
Kelman and Linker (2014)
Xie et al. (2011)
Chaivivatrakul and Dailey (2014)

High computation cost, occlusion is a big challenge

Fig. 2. Fruit harvesting robot with a binocular stereoscope. (1) Binocular stereoscope, (2) Manipulator, (3) End-effector, (4) Fruit object, (5) mobile platform (Si
et al., 2015).

200 mm between the centers of the two camera lens. The author
reported that over 89.5% of apples were successfully recognized
and the errors were less than 20 mm when the measuring distance
was between 400 mm and 1500 mm. The disadvantage of stereovision scheme is its complexity and long computation time due to
stereo matching (Hannan and Burks, 2004).
3.3. Laser active visual scheme
Although there are several techniques to obtain depth information, but considering some desirable features of the sensed image,
the laser active visual is a better choice. The 3D shape of fruit
object is measured by scanning the laser beams and the fruit can

Laser beam
(Infrared and red)

Oscillator
17.5kHz

3.4. Thermal imaging scheme
Thermal imaging is also called infrared thermograph which is
the visualization of infrared radiation (Li et al., 2014). Because of
the physical structure and characteristic, leaves accumulate less
heat and radiate faster than fruits, the temperature distributions
of the plant canopy with fruit can be applied for fruit detection
(Vadivambal and Jayas, 2001). Xu and Ying (2004) used a thermal
camera to identify the citrus in a tree canopy. From the analysis
result shown in Fig. 4, the temperature distribution along line AB

Timing signal

Lock in
amplifier

Filter

Lock in
amplifier

Filter

Lock in
amplifier

Filter

Preamplifier
PSD

Fruit object

Cold filter

Oscillator
35kHz

Red
laser diode

Infrared
laser diode

be distinguished from other obstacles according to different
spectral-reflections. The laser active visual scheme for fruit detection is illustrated in Fig. 3 (Gotou et al., 2003). Both laser beams
scan the fruit object simultaneously and the locations of fruit
objects and obstacles are recognized through image processing.
An infrared laser range-finder was installed in an orange harvesting robot named Agribot working in non-structured environments,
Jimenez et al. (2000a,b). The output of the infrared laser range finder includes 3D position, radius, and surface reflectivity of fruit
object. Tankgaki et al. (2008) designed a machine vision system
equipped with red and infrared laser scanning devices to detect
cherry on the tree, which could prevent the influence of the sunlight. Yin et al. (2009a,b) used a laser active visual sensor to measure the turned angles of robot arm and distance between target
tomato and end-effector. Zhang et al. (2015) developed a novel
apple stem recognition system using the 3D reconstruction technique combined with near-infrared and linear-array structured
lighting. The author reported that 97.5% overall recognition accuracy for the 100 samples was obtained by the proposed system
and method. Even though the accuracy of laser active visual system
is promising, the complexity of the system often limits its practical
application.

Lens

DC voltage
Preamplifier

Fig. 3. A scheme of the laser active visual system for fruit recognition (Gotou et al., 2003).

Computer

Texture

The contour loss is less than 1/2

A/D converter

Shape

315

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

19.7
19.1
18.6
18.0
17.5

A

B

16.9
16.3
15.8
15.2
80

160

240

320

400

480

560

640

Fig. 4. The typical test results of thermal image for citrus recognition.

Fig. 5. The model of fruit object detection using hyperspectral image.

across the citrus area, leaves and others showed that the temperature difference between citrus and other objects was more than
1 °C. Bulanon et al. (2008) also used a thermal infrared camera to
detect the citrus in day and night. The fruits were successfully segmented in the thermal images using image processing techniques
according to the largest temperature difference. Based on prior
research, Bulanon et al. (2009) proposed an improved fruit detection approach combined with a thermal image and a visible image.
Results showed that the performance of the image fusion approach
was better than using the thermal image alone. Even though thermal imaging has advantages on detecting fruits even when fruit
and background color are similar, the accuracy of recognition using
thermal imaging is affected by the shadow of the tree canopy
(Stajnko et al., 2004).
3.5. Spectral imaging scheme
The spectral camera is developed to integrate both spectroscopic and imaging techniques into one system to obtain a set of

monochromatic images at a continuum of wavelengths (Zhang
et al., 2014). With recent development of spectral imaging, the
spectral cameras have been used to recognize fruits. The model
of fruit object detection using multispectral image is shown in
Fig. 5 (Manolakis et al., 2003). A monochromatic near-infrared
camera, equipped with three different optical band pass filters
(1064, 1150 and 1572 nm), was used to identify in-field green
citrus by Kane and Lee (2007). The author reported an average correct pixel identification of 84.5%. Safren et al. (2007) used a hyperspectral camera to detect green apples. The hyperspectral imaging
was capable of giving a wealth of information both in the visible
and the near-infrared (NIR) regions and thus offered the potential
to detect the green apples. Okamoto and Lee (2009) also proposed
a green citrus recognition method using a hyperspectral camera of
369–1042 nm to solve the detection problem arising from similar
color between fruits and natural scenes. The test results reflected
that 80–89% of the citrus in the foreground of the validation set
were identified correctly. By comparing the spectral reflectance
difference of cucumber plant (fruit, leaf and flower) from visible

316

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

to infrared (350–1200 nm), sensitive bands of fruit information
were obtained by statistical variance analysis (Yuan et al., 2011).
4. Recognition approaches for harvesting robot
The recognition algorithm is a key factor affecting the performance of a vision recognition system. Numerous literatures
reported that various recognition algorithms have been employed
for robotic harvesting of fruits. Those recognition approaches can
be classified into single feature analysis approaches, multiple features fusion analysis approaches and pattern recognition
approaches.
4.1. Single feature analysis approaches
Color is one of the most prominent features used to distinguish
the mature fruit from the complex natural background. In the studies of color-based segmentation for fruit recognition, the image
pixels are categorized into two groups according to the threshold
which decides whether a pixel belongs to the fruit object or to
the background. However, the accuracy of segmentation using
color feature is sensitive to varing illumination conditions. For alleviating the influence of varying illumination, several color spaces
such as HIS, L⁄a⁄b⁄, and LCD are used to extracting color features
(Huang and He, 2012; Yin et al., 2009a,b). Arefi et al. (2011) developed a ripe tomato recognition algorithm using a combination of
RGB, HSI, and YIQ color spaces and fruit morphological features.
The authors argued that the total accuracy was 96.36% when the
proposed approach was adopted in a greenhouse with artificial
illumination. Mao et al. (2009) employed a Drg-Drb color indexing
to segment apples from background and an accuracy of 90% was
achieved. Wei et al. (2014) also proposed a fruit recognition
method based on an improved OSTU threshold algorithm using a
new color feature in the OHTA color space. The OHTA color space
was transform from the RGB color model through linear conversion
and the extraction accuracy was more than 95%. When the fruits
and leaves have similar colors, color-based segmentation methods
are not available for recognizing fruits (Reis et al., 2012).
Fruit recognition algorithms based on extracting geometric features are universal for detecting spherical fruit such as tomatoes,
apples, and citrus (Liu et al., 2007; Xie et al., 2010). Because of
independent color features, the shape-based analysis approach is
not affected by varying illuminations. Whittaker et al. (1987) proposed a modified Circular Hough Transform algorithm for locating
mature tomatoes which were partially hidden from obstacles. The
authors recommended that this shape-based analysis algorithm
could be valid for situations in which the perimeter of the fruit is
partially obscured by leaves or by overlapped tomatoes. Xie et al.
(2007, 2011) also put forward a concave spots searching algorithm

based on Hough Transform to improve the accuracy of strawberry
recognition. The authors argued that the proposed strawberry
recognition method is effective both for single fruit and complex
situation when the strawberry contour loss is less than 1/2.
Kelman and Linker (2014) also proposed a localization algorithm
of mature apples in trees using convexity. Together with the
removing 99.8% of the edges initially identified by Canny detector,
94% of the visible apples were correctly detected.
The images captured in natural outdoor conditions have some
texture differences which can be used to facilitate separation of
fruits from their background. Thus, texture feature plays an important role in fruit recognition especially when the fruits are clustered or occluded (Zhao et al., 2005; Kurtulmus et al., 2011a,b;
Rakun et al., 2011). To use color, texture, and shape information
by histogram based separation, circular Gabor texture features
and eigen-fruit approaches were implemented in the fruit recognition algorithm by Kurtulmus et al. (2011a,b). Notice that the application of texture features are always combined with color features
and/or geometric features.
4.2. Multiple features fusion approaches
In order to increase recognition reliability in uncontrolled environments caused by uneven illumination conditions, partly
occluded surfaces and similar background features, some researchers apply multiple features (color, geometry, and texture) fusion
algorithms to recognize fruits. Hannan et al. (2009) also developed
a machine vision algorithm consisted of color-based segmentation
and perimeter-based detection. Yin et al. (2009a,b) proposed a ripe
tomato recognition method which is combined with the tomato’s
shape features and the color features. With the color features
extracted from the L⁄a⁄b⁄ color space and the shape feature
acquired by a laser ranging sensor, the recognition and localization
system for tomato harvesting robot could be used to handle the situations of tomato overlapping and sheltering. Zhao et al. (2005)
proposed a texture based edge detection algorithm combined with
color properties analysis to recognize on-tree apples. The authors
presented that 90% of apples were correctly detected using the
recognition approach. Colors, intensity, edge and orientations as
the features of the target were considered by Patel et al. (2011)
to develop an improved multiple features based algorithm for fruit
detection. The authors reported that the detection efficiency was
achieved up to 90% using the optimal weights of different features.
Lu et al. (2014) also developed a novel method based on fusing the
segmentation results of chromatic aberration map (CAM) and
luminance map (LM) to recognize the citrus in a tree canopy.
Rakun et al. (2011) comprehensively considered three distinct features; color, texture and 3D shape of the fruit object for overcoming low recognition reliability in uncontrolled environments. They

Table 3
The major types of multi-modal images recognition algorithms.
Features

Fruits

Vision scheme

Accuracy

Reference

Color + Geometry

Tomato
Orange
Apple
Citrus
Peach
Apple
Apple
Citrus
Tomato
Apple
Apple
Citrus

Camera and Laser ranging sensor
Two cameras
Camera
Camera
Camera
Camera and Laser ranging sensor
Camera
Camera
Camera
Camera and ToF camera
Camera and thermal camera
Camera and thermal camera

NR
>90
90%
75.3%
90%
>90%
NR
86.81%
93%
>83.67%
74%
74.37%

Yin et al. (2009a,b)
Hannan et al. (2009)
Zhao et al. (2005)
Kurtulmus et al. (2011a,b)
Patel et al. (2011)
Bulanon and Kataoka (2010)
Rakun et al. (2011)
Lu et al. (2014)
Zhao et al. (2016a,b)
Feng et al. (2014)
Wachs et al. (2009, 2010)
Bulanon et al. (2009), Bulanon and Kataoka (2010)

Color + Texture
Color + Texture + Geometry
Color + 3D shape
Color + Texture + 3D shape
CAM + LM
I-component + a⁄-component
Color + Amplitude image
Color + Thermal image

CAM = Chromatic Aberration Map; LM = Luminance Map.

317

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323
Table 4
The main kinds of multi-modal images recognition algorithms.
Pattern recognition types

Classification algorithms

Fruits

Correct/error rate

Reference

Statistical pattern recognition

Linear decision classifier
Bayesian classifier
X-means clustering
PCA
Adaboost

Peach
Apple
Orange
Tomato
Citrus
Kiwifruit

>89%/NR
80%/3%
>75%/6%
88%/NR
75.3%/NR
>92.1%/7%

Sites and Delwiche (1988)
Bulanon et al. (2004)
Slaughter and Harrell (1989)
Yamamoto et al. (2014)
Kurtulmus et al. (2011a,b)
Zhan et al. (2013)

Fuzzy pattern recognition

FCM

Tomato

NR/16.55%

Wang et al. (2015)

Soft computing methods

Feed-forward neural network
Fuzzy neural network
SVM

Tomato
Apple
Pepper

95.45%/NR
>92.31%/NR
>74.2%/NR

Arefi and Motlagh (2013)
Ma et al. (2013)
Song et al. (2014)

applied color segmentation to multiple scene images to separate
potential regions from background and verify them first with texture analysis and then by reconstructing in the 3D space.
The recognition methods based on multi-sensor fusion technology have been used to fruit recognition (Bulanon et al., 2009;
Bulanon and Kataoka, 2010; Wachs et al., 2009, 2010; Feng et al.,
2014). For overcoming the challenge of recognizing green fruit in
the tree canopy, Bulanon et al. (2009) and Wachs et al. (2010) used
infrared images and visible images fusion to improve fruit detection. The infrared image captured by thermal infrared camera
and visible image captured by a color camera required image registration prior to image fusion. Maximization of the mutual information was employed by Wachs et al. (2009) to find the optimal
registration parameters for image fusion. The authors argued that
the recognition accuracy of fusion approach (74%) was increased
compared to the conventional approach of detection using either
color (66%) or IR (52%) alone. The amplitude image acquired by
ToF camera and H component image extracted from HSI color
space were selected as source images for fusion by Feng et al.
(2014). The fusion algorithm which is aimed at enhancing the fruit
object area distribution in the fused image could produce more
accurate and robust fruit recognition. A summary of the major
types of multi-modal based algorithms is given in Table 3.
4.3. Pattern recognition approaches
Pattern recognition approaches have long been investigated for
application in fruit recognition (see Table 4). Early in the 1970s,
Parrish and Goksel (1977) had suggested that pattern recognition
approaches could be used for fruit recognition. Two linear classification techniques including a non-parametric linear classifier and
linear decision function classifier were evaluated by Sites and
Delwiche (1988). The outcome indicated that both classification
algorithms produced similar results, and the non-parametric linear
classifier was easier to implement. Bulanon et al. (2004) also developed an apple detection method using the linear decision classifier
and the trichromatic coefficients as patterns. About 80% of fruit
pixels were correctly classified under all lighting conditions with
less than 3% error rate. Slaughter and Harrell (1989) used a
Bayesian classifier to discriminate oranges from the natural background. The classification model using chrominance and intensity
information could correctly classify over 75% of the fruit pixels.
In order to solve the overlapping problem in plantlets recognition,
Pastrana and Rath (2013) developed a novel pattern recognition
approach using an active shape model (ASM). Yamamoto et al.
(2014) applied the X-means clustering algorithm on the basis of
K-means clustering to determine the optimal number of clusters
and to detect individual fruit in a multi-fruit blob. Due to their similarities, fruit detection tasks can be conducted with the similar
method for face recognition and detection. Kurtulmus et al.
(2011a,b) used an ‘eigenfruit’ approach based on principle compo-

nent analysis (PCA) to detect green citrus under natural illumination. Zhan et al. (2013) used an Adaboost algorithm to recognize
the kiwifruit in field and achieved and ideal effect for the segmentation between kiwifruit and trunk, soil and branches. The Adaboost algorithm could combine the strengths of two weak
classifiers and mitigate their shortcomings. Zhao et al. (2016a,b)
also developed an algorithm combining AdaBoost classifier and
color analysis for the automatic detection of ripe tomatoes in
greenhouse. It argued that over 96% of ripe tomatoes were correctly detected.
These statistical pattern recognition approaches are developed
according to the posterior probability of the samples. Thus, more
and more attention are being paid on intelligent pattern recognition methods such as artificial neural networks (ANN), support
vector machine (SVM), and fuzzy pattern recognition. ANN and
SVM are supervised learning algorithms that have the ability to
learn from the data through an iterative training process and
improve its performance after each iteration. An olive recognition
method using neural networks was presented by Gatica et al.
(2013). The process of fruit recognition comprised of two stages:
the first stage focused on deciding whether or not the candidate
identified in the image corresponds to an olive fruit, the second
stage focused on olives overlapping within the tree canopy. Arefi
and Motlagh (2013) developed an experts system based on wavelet
transform and ANN for ripe tomato detection. Totally 90 wavelet
features were extracted from each tomato, and a feed-forward
neural network was used to distinguish the ripe tomato from its
background. An accuracy of 95.45% was obtained from the proposed recognition algorithm. In order to overcome the fuzziness
and uncertain factor existing in the color image boundary pixels,
a model combining quantum genetic algorithm and fuzzy neural
networks was built by Ma et al. (2013). An improved fuzzy neural
network could avoid redundant iteration and the tendency to fall
into the local minimum of traditional BP neural networks. Ji et al.
(2012) introduced a new classification algorithm based on support
vector machine to improve the apple recognition accuracy and efficiency. The new classifier had balanced the recognition success
rate and the time used in recognition. A statistical classifier, an
ANN and a SVM classifier were built and used for detecting peach
fruit by Kurtulmus et al. (2011a,b, 2014). Authors reported that
84.6%, 77.9% and 71.2% of the actual fruits were successfully
detected, using the three classifiers for the same validation set.
For improving the tomatoes identifying accurately, Song et al.
(2014) also used a bag-of-words (BoW) model to locate peppers
on the plant. The BoW model represented each image by a frequency distribution of its visual vocabularies, which was classified
to a fruit class. Wang et al. (2015) presented a Fuzzy Clustering
Means (FCM) algorithm to recognize the clustered tomatoes. The
superiority of this algorithm was verified according to a comparison with K-means and Otsu threshold. In order to accelerate the
computation of the traditional FCM, Xiong et al. (2013) presented

318

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

Table 5
The comparison of two types of eye-hand coordination control.
Eye-hand
coordination

Principle

Advantages

Limitations

Open-loop
visual
control

Hierarchical
controlling
based on
precision 3D
measuring

Control law is
simpler;
controllability and
region of stability
are better

Visual servo
control

Dynamic
interacting
between the
robot and
environment

Calibration is not
required; real-time
tracking is achieved
and it is object
friendly

Performance
depends on the
accuracy of
measurement,
assembly and
calibration
Local minima of
potential and
unexpected camera
trajectory

an improved FCM algorithm based on the fusion of the bicubic
interpolation algorithm and the FCM algorithm. The improved
recognition algorithm was available for litchi recognition which
enabled the recognition system to operate in real-time.
5. Eye-hand coordination in harvesting robot
Vision-based robot control has been investigated for more than
30 years. This technology promises substantial advantages when
working with the targets whose positions are unknown, or with
manipulators which may be inaccurate. Visual information can
be used for controlling the robotic manipulator or guiding its
motion. The two types of vision-based control applications in robot
control loop are called eye-hand coordination and visual navigation. Visual navigation has been widely applied (Khadraoui et al.,
1998), while the eye-hand coordination is a bottleneck to improve
the performance of harvesting robot. Thus, further developments
in vision-based control for harvesting robot are necessary. In this
section, an overview of the eye-hand coordination control in harvesting robot is given.
Traditionally, eye-hand coordination control systems were
based on an open loop control framework which employs the
‘‘looking then moving” mode of operation. The control precision
depends directly on the accuracy of the vision system, and the calibration of manipulator and assembly (Yau and Wang, 1996). An
alternative to increase the accuracy of these subsystems is to use
a visual feedback control loop (Corke and Hager, 1998). This particular vision-based robotic control mode is also called visual servo.
The visual servo is a framework to implement ‘‘looking and moving” as a dynamical system. The task of visual servo for harvesting
robot is to control the pose of the robot’s end-effector using image
features which are extracted from the image captured by the
camera-in-hand or fixed camera (Hashimoto, 2003). The comparison of these two types of eye-hand coordination control is given
in Table 5.

acquired by a stereo camera. The end-effector was first sent to a
location based on the X, Z position of the fruit and the Y (depth
information) of the cluster center. If the fruit could not be reached,
the end-effector advanced forward 50 mm (in Y direction) in the
next movement. Thus, the author argued that the harvesting success rate was affected by the accuracy of the calculated depth.
Inoue et al. (1996) also developed an open-loop visual control
scheme based on precise position detection for robotic harvesting.
The minimum distance gathering path to effectively touching the
target fruit could be calculated from this control model. In order
to improve the precision of position measurement, multiple visual
sensors are employed in the vision system. Hayashi et al. (2010)
developed a machine vision unit equipped with three aligned cameras to enhance the recognition rate. The center camera was used
to calculate the inclination of the peduncle, whereas the two cameras mounted on both sides of the center camera form a stereovision system to determine the 3D position of the fruit. Han et al.
(2012) also developed the strawberry harvesting robot based on
open-loop visual control. The 3D position of target strawberry
was acquired by a color stereoscope camera and a laser device.
The performance of the vision-based control scheme in field tests
showed that the execution time for successful harvest of a single
strawberry was less than 7 s.
In some cases, the fruit position in the tree canopy would be
influenced by wind or manipulator movement. When this situation
occurs, the efficiency of open-loop visual control for robotic harvesting is very low. Shen et al. (2011) have researched on the
increasing harvest efficiency of the fruit harvest robot in oscillatory
conditions. The oscillation frequency was obtained by curve fitting
and applying fast Fourier transform to video samples. With the calculated movement duration of the end-effector, it can eliminate
the time waiting for the oscillation to decay. Font et al. (2014) also
investigated a vision control strategy by combining open-loop
visual control and visual closed-loop control. A stereovision camera mounted on a robot arm could acquire the initial fruit location.
With the open-loop visual control, the grasper could move quickly
to the front of the target fruit. The final picking operation was conducted by iteratively adjusting the vertical and horizontal positions
of the gripper through closed-loop visual control. Aiming at solving
the positioning problem, Zou et al. (2012) developed a binocular
stereo vision system and position principle of the picking manipulator in virtual environment (VE). The stereo vision data was
mapped to the manipulator and was guided by accurate positioning in VE. The simulation results in VE could be applied to control
harvesting robot operation and to correct the positioning errors in
real-time.
5.2. Visual servo control
Compared to open-loop visual control, the input of visual servo
is continuous and contains dynamic image information. Therefore,
frame rates of the video must match the closed-loop bandwidths of

5.1. Open-loop visual control
The open-loop visual control mode is built for accurate positioning of the fruit object in 3D workspace. Therefore, vision systems of open loop control may consist of stereo vision or laser
range sensor that can measure the spatial distance between the
target fruit and the end-effector. Following a precision distance
measurement, the trajectory of the manipulator can be planned
through calculating the kinematics of the robot. Hence, the manipulator kinematic model and calibration of vision system have to be
very precise.
In the study of vision-based control for robotic harvesting of
cherry tomatoes (Kondo et al., 1996), a open-loop visual control
scheme was implemented based on the 3D position detection

Target

+

Error
Control law

Angle
Velocity
Acceleration

Motion
Manipulator

Video

Pose

Pose
estimation

Visual
processing
Visual feedback

Fig. 6. The structure of visual servo (Pan et al., 2000).

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

319

Camera in Hand

Tomato plant
Robot controller

Robot

Fixed Camera
Control signal

Field-of-view of CiH

Image signal

Industry Computer

Global view from Fixed camera

Fig. 7. Schematic diagram of a cooperative vision system for harvesting robot.

vision controllers. As shown in Fig. 6, pose estimation is the core
issue of visual feedback control. Input of the visual servo control
are the time-varying position errors between the target fruit and
end-effector. The advantages of visual servo are that its performance does not relay on the precise kinematic model of the robot
and the calibration of the vision system.
Mehta and Burks (2014) developed a visual servo system for
robotic citrus harvesting. As shown in Fig. 7, a cooperative vision
system consisting of a fixed camera and a camera-in-hand (CiH)
was incorporated such that the fixed camera and provided a global
view of a tree canopy. The CiH, due to its proximity, provided high
resolution fruit image. In contrast to the open-loop visual control
model the visual servo control strategy used perspective camera
geometry to obtain absolute range estimates. The estimated range
information can be used to generate a global map of fruit locations,
and a rotation controller was developed to orient the robot endeffector towards the target fruit such that the fruit could enter
the field of view of the CiH. The performance of the proposed visual
servo controller was demonstrated using a 7 DOF robotic manipulator in an artificial environment. Van Henten et al. (2003a,b, 2010)
introduced a novel eye-hand coordination approach based on the
A⁄-search algorithm which could assured collision–free motions
when the robot harvested cucumbers in a greenhouse. Eye-hand
coordination based on the A⁄-search algorithm had assured an
optimal motion path was obtained such that the cucumber picking
time can be reduced. Zhao et al. (2011) developed an apple harvesting robot consisting of a manipulator, end-effector and
image-based vision servo control system. In the visual control system, the image-based vision servo (IBVS) control method was
employed for localization and picking motion for the target fruit.
The IBVS was often used to control the manipulator according to
image features. The key issue of this method is how to calculate
the Jacobian matrix which describes the relationship between
camera coordinate and robot coordinate (Harrell et al., 1985).
Robot joint motion could be controlled based on feedback from

the position of a target fruit in an image. Vision servo was accomplished by controlling the velocities of each joint according to the
vertical and horizontal offsets of a fruit’s image position from the
image center (Harrell et al., 1990). Moreover, the closed-loop bandwidths of vision controllers could be varied from 1.0 to 1.1 Hz.
6. Examples of fruit harvesting robots
Robotics harvesting is not a new phenomenon but with the history of over 30 years. Currently, harvesting robots have not been
advanced to the commercialization stage because of their low efficiencies, low intelligence, and high costs. On the other hand, different designs of fruit harvesting robots have emerged in recent years.
Examples of major fruit harvesting robots are shown in Table 6. For
convenience, the examples of harvesting robot are categorized
according the types of target fruit or vegetable. The vision schemes
and eye-hand coordination models of the harvesting robots are
described in the table. The performances of different vision-based
control approaches applied in various harvesting robot are also
shown in the table.
7. Challenges and future trends
7.1. Enhancing the vision-based control of harvesting robot
With the development of robotic technology and sensor technology, enhancing the performance of fruit harvesting robot can
be a positive trend in meeting the challenges. Several suggestions
are given to improve the ability of the harvesting robot to deal with
the complex working environment (Bac et al., 2014).
Firstly, improvements in sensing are required. The sensors currently used have certain shortcomings in the application to fruit
recognizing (Gongal et al., 2015). Additionally, the combination
of multi-sensor may satisfy the performance required for fruit

320

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

Table 6
A comparison of the development of harvesting robots in the main countries.
Products

Robots

Vision scheme

Eye-hand coordination

Success
rate

Speed

Reference

Fruits

Apple harvesting robot

A camera-in-hand and positioning sensor

Open-loop visual
control
Open-loop visual
control
Visual servo control
Visual servo control
Visual servo control
Visual servo control
Visual servo control
Visual servo control
Open-loop visual
control
Open-loop visual
control
Visual servo control
Open-loop visual
control

77%

15s

Zhao et al. (2011)

>90%

7.1s

NR
50%
>85%
<41.3%
NR
100%
66.7%

<8s
36s
<22s
11.5s
7s
12s
NR

Bulanon and Kataoka
(2010)
Mehta et al. (2014)
Harrell et al. (1990)
Edan et al. (2000)
Hayashi et al. (2010)
Han et al. (2012)
Sakai et al. (2007)
Umeda et al. (1999)

NR

NR

Scarfe et al. (2009)

NR
66.7%

NR
14s

Kondo (1991)
Tankgaki et al. (2008)

Open-loop visual
control
Visual servo control
Visual servo control
Open-loop visual
control
Visual servo control
Visual servo control
Visual servo control

70%

3s-5s

Kondo et al. (1996)

88.6%
80%
74.4%

37.2s
45s
65.2s

NR
>80%
62.5%

<7s
6.7s
64.1s

Open-loop visual
control
Open-loop visual
control
Open-loop visual
control

79%

NR

Ji et al. (2014)
Van Henten et al. (2002)
Van Henten et al. (2003a,
b)
Foglia and Reina (2006)
Reed et al. (2001)
Hayashi et al. (2001,
2002)
Hemming et al. (2014)

NR

NR

Sakai et al. (2013)

82.22%

NR

Kohan et al. (2011)

A camera-in-hand and a laser range sensor
Citrus harvesting robot

Watermelon harvesting
robot

A fixed camera and a camera-in-hand
A fixed camera
A far-vision CCD and a near-vision CCD
A stereovision system and a central camera
A stereo camera, a camera and a laser sensor
A stereo vision sensor and a camera in hand
A stereo vision sensor

Kiwifruit harvesting robot

Eight cameras (four stereo vision systems)

Grape harvesting robot
Cherry harvesting robot

A camera-in-hand
A red and infrared laser active sensor

Tomato harvesting robot

A binocular stereo vision sensor

Cucumber harvesting robot

A stereo vision sensor
A fixed camera and a camera-in-hand
A near-infrared camera and a camera

Radicchio harvesting robot
Mushroom Harvesting robot
Eggplant harvesting robot

A camera-in-hand
A monochrome camera in hand
A camera-in-hand

Sweet-pepper harvesting
robot
Asparagus harvesting robot

Two ToF cameras, a stereo camera and a
camera
Two infrared laser sensors

Rosa harvesting robot

A stereo vision camera

Melon harvesting robot
Strawberry harvesting robot

Vegetables

detection and localization (Fernandez et al., 2013). Though given
the large number of articles that described a number of fruit recognition algorithms, the development of advanced image processing
algorithms is also a challenge for precision fruit recognition.
Recently, more and more attentions have been attracted by new
visual sensors such as ToF camera, light-field camera, and chlorophyll fluorescence camera. The core of the application considerations of these sensors is how to take advantage of the data
acquired.
On the other hand, vision control precision and efficiency for
fruit harvesting robot need to improve. There are many recent
works regarding eye-hand coordination for outdoor operation
robot (Mariottini et al., 2006). The control law of calibration-free
eye-hand coordination was described byHager et al. (1994). Base
on the auto disturbance rejection control (ADRC) strategy, Su proposed an advanced calibration-free eye-hand coordination which
has a strong adaptability and robustness (Su et al., 2004). Another
new visual servo control approach based on adaptive neural network has been applied for dynamic positioning of underwater
vehicles (Gao et al., 2015).
7.2. Human–machine collaboration
To date, the commercial application of fruit harvesting robot is
still unavailable because of lack of high efficiency and economic
justification (Edan, 1999). One of the new approaches to improve
the applicability of robotic harvesting is to combine human workers and robots synergistically. This approach for robotic harvesting
is to separate the fruit recognition stage from the harvest stage by
marking the target fruit a priori. In the Agribot project, a robot was
designed and built for a new aided-harvesting strategy, involving a
harmonic human–machine task distribution (Ceres et al., 1998). Ji

et al. (2014) introduced an assistant-mark approach to recognize
and locate the picking-point of the harvesting robot. Bechar and
Oren has defined and implemented the collaboration of a human
operator and robot applied to target fruit detection (Bechar and
Edan, 2003; Oren et al., 2012). Experimental results indicated that
the target recognition system based on human-robot collaboration
could increase the target fruit detection rate to 94% and reduce the
time needed by 20%.
7.3. Multi-arms cooperating for robotic harvesting
Another approach towards the goal of efficient robotic harvesting is the multi-arm robotic harvester. The idea is that a number of
manipulators are mounted on the mobile robot platform, and each
of the robotic arms is assigned a specific fruit to harvest. Zion from
Israel has designed a multi-arm melons harvesting robot which
enabled the maximum number of melons to be harvested (Zion
et al., 2014). According to the idea of multi-robot cooperation for
fruit harvesting, Noguchi et al. (2004) also proposed a master–
slave robot system for field operations. In this multiple robot system, a high level of autonomy on the robots was achieved to allow
them to cope with unexpected events and obstacles.
7.4. Making the environment more suitable for robotic harvesting
There are major technical challenges in automation due to the
uncontrolled environment in combination with the fact that the
harvest objects and materials are highly inconsistent in shape
and size. The harvesting robot working in the complex natural
environment requires a higher degree of skill and a wider range
of operating. In order to improve the efficiency of robotic harvesting in the future, collaboration among engineers and agronomists

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

is needed. Many engineers, working with agronomists, in the world
now have developed advanced robots (Burks et al., 2005; He et al.,
2013). Their solution for the problem of low harvest efficiency is
making environment robot more user friendly. Trees or plants
can be pruned to obtain suitable plant geometry for robotic harvesting and, hence, the picking cycle time can be reduced (Edan
et al., 1990).
8. Conclusions
In this paper, a broad overview of the development of vision
control technology applied in fruit harvesting robot is given. Vision
control for fruit harvesting robot includes two key elements, fruit
recognition and eye-hand coordination. For recognizing the target
fruit in the canopy, different types of visual sensors and image
analysis algorithms are equipped in the fruit harvesting robots.
Two-dimensional imaging data of the target fruit have been successfully acquired from monocular camera, hyperspectral imaging,
and thermal imaging. Three-dimensional surface reconstruction of
the target fruit requires data acquisition from binocular stereoscope or structured light sensor. There is a large difference in the
image processing algorithm between 2D imaging schemes and
3D imaging schemes. The scheme of fruit recognition for robotic
harvest may depend on the species of harvesting fruit. Moreover,
the performance of the fruit recognition system is also influenced
by many factors such as variable light, occlusions and many others.
Therefore, the reliability of recognition methods must consider the
environment in which the robot is working, and the proper selection of sensors.
In addition to the techniques reviewed, the development of eyehand coordination control for fruit harvesting robot is also
described in this paper. We have classified eye-hand coordination
into two main categories according to whether the visual control is
open-loop or close-loop. There is a large difference in the control
principle between open-loop visual control mode and visual servo.
The input image of open-loop visual control is a static image. However, the input of visual servo is the video image where frame rates
of the video must match the closed-loop bandwidths of vision controllers. The performance of the open-loop visual control relies on
the precision calibration of the camera and manipulators. The fruit
harvesting robot adopting visual servo can arrive at certain control
precision without calibration. With the advancement of current
imaging technologies and the development of new control algorithms, more information will be available to aid recognizing the
target fruit and speeding up fruit picking.
Acknowledgments
Financial support from the National High-Tech R&D Program of
China (863 Program No. 2013AA102307) and the National Key
Technology R&D Programs (2014BAD08B01 and 2015BAF12B02)
are gratefully acknowledged.
References
Arefi, A., Motlagh, A.M., Mollazade, K., Teimourlou, R.F., 2011. Recognition and
localization of ripen tomato based on machine vision. Aust. J. Crop Sci. 5 (10),
1144–1149.
Arefi, A., Motlagh, A.M., 2013. Development of an expert system based on wavelet
transform and artificial neural networks for the ripe tomato harvesting robot.
Aust. J. Crop Sci. 7 (5), 699–705.
Baeten, J., Donne, K., Boedrij, S., Bechers, W., Claesen, E., 2008. Autonomous fruit
picking machine: a robotic apple harvester. 6th International Conference on
Field and Service Robotics, vol. 42, pp. 531–539.
Bac, C.W., Heten, E.J.V., Hemming, J., 2014. Harvesting robots for high-value crops:
state-of-the-art review and challenges ahead. J. Field Robot 31 (6), 888–991.
Bechar, A., Edan, Y., 2003. Human-robot collaboration for improved target
recognition of agricultural robots. Ind. Robot 30 (5), 432–436.

321

Buemi, F., Massa, M., Sandini, G., 1995. Agrobot: a robotic system for greenhouse
operations. Robotics Agric. Food Ind. 4, 172–184.
Buemi, F., Massa, M., Sandini, G., Costi, G., 1996. The Agrobot project. Adv. Space Res.
18, 185–189.
Bulanon, D.M., Burks, T.F., Alchanatis, V., 2008. Study on temporal variation in citrus
canopy using thermal imaging for citrus fruit detection. Biosyst. Eng. 101 (2),
161–171.
Bulanon, D.M., Burks, T.F., Alchanatis, V., 2009. Image fusion of visible and thermal
images for fruit detection. Biosyst. Eng. 103, 12–22.
Bulanon, D.M., Kataoka, T., Okamoto, H., Hata, S., 2004. Development of a real-time
machine vision system for apple harvesting robot. SICE Ann. Conf.,
595–598
Bulanon, D.M., Kataoka, T., 2010. A fruit detection system and an end effector for
robotic harvesting of Fuji apples. Agric. Eng. Int.: CIGR J. 2010 (7), 1–14.
Burks, T., Villegas, F., Hannan, M., 2005. Engineering and horticultural aspects of
robotic harvesting: opportunities and constraints. Horttechnology 15 (1), 79–
87.
Ceres, R., Pons, J.L., Jimenez, A.R., Martin, J.M., Calderon, L., 1998. Design and
implementation of an aided fruit-harvesting robot. Ind. Robot 25 (5), 337–346.
Chaivivatrakul, S., Dailey, M.N., 2014. Texture-based fruit detection. Precis. Agric. 15
(6), 662–683.
Corke, P.I., Hager, G.D., 1998. Vision-based robot control. Control Problems Robotics
Automat., 177–192
d’Esnon, G.A., Rabatel, G., Pellenc, R., Joumeau, A., Aldon, M.J., 1987. MAGALI: a selfpropelled robot to pick apples. In: Proceedings of American Society of
Agricultural Engineers, Baltimore, Maryland.
Edan, Y., Flash, T., Shmulevich, I., 1990. An algorithm defining the motions of a citrus
picking robot. J. Agr. Eng. Res. 46, 259–273.
Edan, Y., Gaines, E., 1994. Systems engineering of agricultural robot design. IEEE
Trans. Syst. Man, Cybern. 24 (8), 1259–1265.
Edan, Y., 1999. Food and agriculture robotic. Handbook of Industrial Robotic. Second
edition. Chapter 60, 1143–1155.
Edan, Y., Rogozin, D., Flash, T., 2000. Robotic melon harvesting. IEEE Trans. Rob.
Autom. 16 (6), 831–834.
Edan, Y., Han, S.F., Kondo, N., 2009. Automation in agriculture. Springer Handbook of
Automation, Part G, 1095–1128.
Feng, J., Zeng, L.H., Liu, G., 2014. Fruit recognition algorithm based on multi-source
images fusion. Trans. CSAM 45 (2), 73–80.
Fernandez, R., Salinas, C., Montes, H., Sarria, J., Armada, M., 2013. Validation of a
multisensory system for fruit harvesting robots in lab conditions. First Iberian
Robotics Conf. Adv. Intell. Syst. Comput. 252, 495–504.
Foglia, M.M., Reina, G., 2006. Agricultural robot for radicchio harvesting. J. Field
Robot 23, 363–377.
Font, D., Palleja, T., Tresanchez, M., Rucan, D., Moreno, J., Martinez, D., Teixido, M.,
Palacin, J.T., 2014. A proposal for automatic fruit harvesting by combining a low
cost stereovision camera and a robotic arm. Sensors 14, 11557–11579.
Gao, J., Proctor, A., Bradley, C., 2015. Adaptive neural network visual servo control
for dynamic positioning of underwater vehicles. Neurocomputing 167, 604–
613.
Gatica, G., Best, S., Ceroni, J., Lefranc, G., 2013. Olive fruits recognition using neural
networks. Procedia Comput. Sci. 17, 412–419.
Goncalves, P.J.S., Torres, M.B., 2010. Learning approaches to visual control of robotic
manipulators. In: The Second International Conference on Advanced Cognitive
Technologies and Applications, pp. 103–108, Lisbon, Portugal, 21-26 November.
Gongal, A., Amatya, S., Karkee, M., 2015. Sensors and systems for fruit detection and
localization: a review. Comput. Electr. Agr. 116, 8–19.
Gotou, K., Fujiura, T., Nishiura, Y., 2003. 3-D vision system of tomato production
robot. Int. Conf. Adv. Intell. Mech., 1210–1215
Grift, T., Zhang, Q., Kondo, N., Ting, K.C., 2008. A review of automation and robotics
for the bio-industry. J. Biomech. Eng. 1 (1), 37–54.
Hager, G.H., Chang, W.C., Morse, A.S., 1994. Robot feedback control based on stereo
vision: towards calibration-free hand-eye coordination. IEEE Int. Conf. Robotic
Automat., 2850–2856
Han, K.S., Kim, S.C., Lee, Y.B., Kim, S.C., Im, D.H., Choi, H.K., Hwang, H., 2012.
Strawberry harvesting robot for bench-type cultivation. Biosyst. Eng. 37 (1), 65–
74.
Hannan, M.W., Burks, T.F., 2004. Current developments in automated citrus
harvesting. In ASAE/CSAE Annual International Meeting. St. Joseph, MI. pp.
043827.
Hannan, M.W., Burks, T.F., Bulanon, D.M., 2009. A machine vision algorithm
combining adaptive segmentation and shape analysis for orange fruit detection.
Agric. Eng. Int.: CIGR J. 6, 1–17.
Harrell, R.C., Adsit, P.D., Slaughter, D.C., 1985. Real-time vision-servoing of a robotic
tree fruit harvester. Trans. ASAE 85–3550, 1–15.
Harrell, R.C., Adsit, P.D., Munilla, R.D., 1990. Robotic picking of citrus. Robotica 8,
269–278.
Hashimoto, K., 2003. A review on vision-based control of robot manipulators. Adv.
Robotics 17 (10), 969–991.
Hayashi, S., Ganno, K., Ishii, Y., 2001. Development of a harvesting end-effector for
eggplants. SHITA 13 (2), 97–103.
Hayashi, S., Ganno, K., Ishii, Y., Tanaka, I., 2002. Robotic harvesting system for
Eggplants. JARQ 36 (3), 163–168.
Hayashi, S., Ota, T., Kubota, K., Ganno, K., Kondo, N., 2005. Robotic harvesting
technology for fruit vegetables in protected horticultural production. In:
Information and Technology for Sustainable Fruit and Vegetable Production,
pp. 227–236, 12-16 September.

322

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

Hayashi, S., Shigematsu, K., Yamamoto, S., Kobayashi, K., Kohno, Y., Kamata, J.,
Kurita, M., 2010. Evaluation of a strawberry-harvesting robot in a field test.
Biosyst. Eng. 105, 160–171.
He, L., Zhang, Q., Charvet, H.J., 2013. A kont-tying end-effector for robotic hop
twining. Biosyst. Eng. 114, 334–350.
Hemming, J., Bac, C.W., Tuijl, B.A.J.V., 2014. A robot for harvesting sweet-pepper in
greenhouses. In: 2014 International Conference of Agricultural Engineering, pp.
1–8, Zurich, Switzerland.
Huang, L.W., He, D.J., 2012. Ripe Fuji apple detection model analysis in natural tree
canopy. TELKOMNIKA 10 (7), 1771–1778.
Inoue, S., Ojika, T., Harayama, M., Kobayashi, T., Imai, T., 1996. Cooperated operation
of plural hand-robots for automatic harvest system. Math. Comput. Simulat. 41,
357–365.
Ji, C., Zhang, J., Yuan, T., Li, W., 2014. Research on key technology of truss tomato
harvesting robot in greenhouse. AMM 442, 480–486.
Ji, W., Zhao, D., Cheng, F.Y., 2012. Automatic recognition vision system guided for
apple harvesting robot. Comput. Electr. Agric. 38, 1186–1195.
Jimenez, A.R., Ceres, R., Pons, J.L., 2000a. A machine vision system based on a laser
rang-finder applied to robotic fruit harvesting. Mach. Vision Appl. 11, 321–329.
Jimenez, A.R., Ceres, R., Pons, J.L., 2000b. A survey of computer vision methods for
locating fruit on trees. Trans. ASAE 43 (6), 1911–1920.
Kane, K.E., Lee, W.S., 2007. Multispectral imaging for in-field green citrus
identification. In: ASABE Annual International Meeting, St. Joseph, MI. Paper
Number: 073025.
Kelman, E., Linker, R., 2014. Vision-based localization of mature apples in tree
images using convexity. Biosyst. Eng. 2014 (118), 174–185.
Khadraoui, D., Debain, C., Rouveure, R., Martinet, P., Bonton, P., Gallice, J., 1998.
Vision-based control in driving assistance of agricultural vehicles. Int. J. Rob.
Res. 17 (10), 1040–1054.
Kohan, A., Borghaee, A.M., Yazdi, M., Minaei, S., Sheykhdavudi, M.J., 2011. Robotic
harvesting of rosa damascene using stereoscopic machine vision. WASJ 12 (2),
231–237.
Kondo, N., 1991. Study on grape harvesting robot. Mathematical and Control
Applications in Agriculture and Horticulture, a volume in IFAC Workshop Series,
pp. 243–246.
Kondo, N., Nishitsuji, Y., Ling, P.P., 1996. Visual feedback guided robotic cherry
tomato harvesting. Trans. ASAE 39 (6), 2331–2338.
Kurtulmus, F., Lee, W.S., Vardar, A., 2011a. Green citrus detection using ‘eigenfruit’,
color and circular Gabor texture features under natural outdoor conditions.
Comput. Electr. Agr. 78, 140–149.
Kurtulmus, F., Lee, W.S., Vardar, A., 2011b. An advanced green citrus detection
algorithm using color images and neural networks. J. Agric. Mach. Sci. 7 (2),
145–151.
Kurtulmus, F., Lee, W.S., Vardar, A., 2014. Immature peach detection in colour
images acquired in natural illumination conditions using statistic classifiers and
neural network. Precis. Agric. 15, 57–79.
Li, L., Zhang, Q., Huang, D.F., 2014. A review of imaging techniques for plant
phenotyping. Sensors 14, 20078–20111.
Li, P.L., Lee, S.H., Hsu, H.Y., 2011a. Study on citrus fruit image data separability by
segmentation methods. Procedia Eng. 23, 408–416.
Li, P.L., Lee, S.H., Hsu, H.Y., 2011b. Review on fruit harvesting method for potential
use of automatic fruit harvesting systems. Procedia Eng. 23, 351–366.
Liu, J.Z., Li, P.P., Li, Z.G., 2007. A multi-sensory end-effector for spherical fruit
harvesting robot. Int. Conf. Automat. Logist., 258–262
Lu, J., Sang, N., Hu, Y., 2014. Detecting citrus fruits with highlight on tree based on
fusion of multi-map. Optics 125, 1903–1907.
Ma, X.D., Liu, G., Zhou, W., 2013. Apple recognition based fuzzy neural network and
quantum genetic algorithm. Trans. CSAM 44 (12), 227–232.
Manolakis, D., Marden, D., Shaw, G.A., 2003. Hyperspectral image processing for
automatic target detection applications. Lincoln Lab. J. 14 (1), 79–116.
Mao, W.H., Ji, B.P., Zhan, J.C., Zhang, X.C., Hu, X.A., 2009. Apple location method for
the apple harvesting robot. In: 2nd International Congress on Image and Signal
Processing, pp. 17–19.
Mariottini, G.L., Prattichizzo, D., Oriolo, G., 2006. Image-based visual servoing for
nonholonomic mobile robots with central catadioptric camera. Int. Conf.
Robotics Automat., 538–544
Mehta, S.S., Burks, T.F., 2014. Vision-based control of robotic manipulator for citrus
harvesting. Comput. Electr. Agric. 102, 146–158.
Mehta, S.S., MacKunis, W., Burks, T.F., 2014. Nonlinear robust visual servo control
for robotic citrus harvesting. The 19th world congress of the International
Federation of Automatic. Control, 8110–8115.
Noguchi, N., Will, J., Reid, J., Zhang, Q., 2004. Development of a master-slave robot
system for farm operations. Comput. Electr. Agr. 44, 1–19.
Okamoto, H., Lee, W.S., 2009. Green citrus detection using hyperspectral imaging.
Comput. Electr. Agr. 66, 201–208.
Oren, Y., Bechar, A., Edan, Y., 2012. Performance analysis of a human-robot
collaborative target recognition system. Robotica 30, 813–826.
Pan, Q.L., Su, J.B., Xi, Y.G., 2000. Uncalibrated 3D robotic visual tracking based on
stereo vision. ROBOT 22 (4), 293–299.
Parrish, E.A., Goksel, J.A.K., 1977. Pictorial pattern recognition applied to fruit
harvesting. Trans. ASAE 20 (5), 822–827.
Pastrana, J.C., Rath, T., 2013. Novel image processing approach for solving the
overlapping problem in agriculture. Biosyst. Eng. 115, 106–115.
Patel, H.N., Jain, R.K., Joshi, M.V., 2011. Fruit Detection using improved Multiple
Features based Algorithm. IJCA 13 (2), 1–5.

Rakun, J., Stajnko, D., Zazula, D., 2011. Detecting fruits in natural scenes by using
spatial-frequency based texture analysis and multiview geometry. Comput.
Electr. Agric. 76 (1), 80–88.
Reed, J.N., Miles, S.J., Butler, J., 2001. Automatic mushroom harvester development.
J. Agric. Eng. Res. 78 (1), 15–23.
Reis, M.J.C.S., Morais, R., Peres, E., Pereira, C., Contente, O., Soares, S., Valente, A.,
Baptista, J., Ferreira, P.J.S.G., Cruz, J.B., 2012. Automatic detection of bunches of
grapes in natural environment from color images. J. Appl. Logic. 10, 285–290.
Safren, Q., Alchanatis, V., Ostrovsky, V., Levi, O., 2007. Detection of green apples in
hyperspectral images of apple-tree foliage using machine vision. Trans. ASABE
50 (6), 2303–2313.
Sakai, H., Shiigi, T., Kondo, N., Ogawa, Y., 2013. Accurate Position Detecting during
asparagus spear harvesting using a laser sensor. EAEF 6 (3), 105–110.
Sakai, S., Osuka, K., Maekwaw, T., Umeda, M., 2007. Robust control systems of a
heavy material handling agricultural robot: a case study for initial cost problem.
IEEE Trans. Control Syst. Tchnol. 15 (6), 1038–1048.
Scarfe, A.J., Flemmer, R.C., Bakker, H.H., Flemmer, C.L., 2009. Development of an
autonomous kiwifruit picking robot. In: The 4th International Conference on
Autonomous Robots and Agents, pp. 380–384, Wellington, New Zealand.
Schertz, C.E., Brown, G.K., 1968. Basic considerations in mechanizing citrus harvest.
Trans. ASAE, 343–346.
Shen, H.L., Zhao, D., Ji, W., 2011. Research on the strategy of advantage of advancing
harvest efficiency of fruit harvest robot in the oscillation conditions. Third Int.
Conf. Intell. Human-Mach. Syst. Cybernet. 215–218.
Shinsuke, K., Koichi, O., 2005. Recognition and cutting system of sweet pepper
picking robot in greenhouse horticulture. In: Proceedings of the IEEE
International Conference on Mechatronics & Automation, pp. 1807–1812,
Nigara Falls, Ontario, Canada, 29 July-1 August.
Shirai, Y., Inoue, H., 1973. Guiding a robot by visual feedback in assembling tasks.
Patt. Recogn. 5, 99–108.
Si, Y.S., Liu, G., Feng, J., 2015. Location of apples in trees using stereoscopic vision.
Comput. Electr. Agric. 112 (3), 68–74.
Sistler, F.E., 1987. Robotics and intelligent machines in agriculture. IEEE J. Robotic
Autom. 3 (1), 3–6.
Sites, P.W., Delwiche, M.J., 1988. Computer vision to locate fruit on a tree. Trans.
ASAE 31 (1), 257–263.
Slaughter, D.C., Harrell, R.C., 1987. Color vision in robotic fruit harvesting. Trans.
ASAE 30 (4), 1144–1148.
Slaughter, D.C., Harrell, R.C., 1989. Discriminating fruit for robotic harvest using
color in natural outdoor scenes. Trans. ASAE 32 (2), 757–763.
Song, Y., Glasbey, C.A., Horgan, G.W., 2014. Automatic fruit recognition and
counting from multiple images. Biosyst. Eng. 18, 203–215.
Stajnko, D., Lakota, M., Hocevar, M., 2004. Estimation of number and diameter of
apple fruits in an orchard during the growing season by thermal imaging.
Comput. Electron. Agric. 42 (1), 31–42.
Su, J.B., Qiu, W.B., Ma, H.Y., 2004. Calibration-free robotic eye-hand coordination
based on an auto disturbance-rejection controller. IEEE Trans. Robot. 20 (5),
889–907.
Sun, J., Lu, B., Mao, H.P., 2011. Fruits recognition in complex background using
binocular stereovision. J. Jiangsu Univ. Nat. Sci. Ed. 32 (4), 423–427.
Tankgaki, K., Fujiura, T., Akase, A., 2008. Cherry-harvesting robot. Comput. Electr.
Agric. 63, 65–72.
Umeda, M., Kubota, S., Iida, M., 1999. Development of ‘‘STORK”, a watermelonharvesting robot. Artif. Life Robot. 3, 143–147.
Vadivambal, R., Jayas, D.S., 2001. Applications of thermal imaging in Agriculture and
food industry-a review. Food Bioprocess Tech. 4, 186–199.
Van Henten, E.J., Hemming, J., Tuijl, B.A.J.V., 2002. An autonomous robot for
harvesting cucumbers in greenhouses. Auton. Robot. 13 (3), 241–258.
Van Henten, E.J., Hemming, J., Tuijl, B.A.J.V., 2003a. Collision-free motion planning
for a cucumber picking robot. Biosyst. Eng. 86 (2), 135–144.
Van Henten, E.J., Tuijl, B.A.J.V., Hemming, J., 2003b. Field test of an autonomous
cucumber picking robot. Biosyst. Eng. 86 (3), 305–313.
Van Henten, E.J., Schenk, E.J.L., Willigenburg, G.V., 2010. Collision-free inverse
kinematics of the redundant seven-link manipulator used in a cucumber
picking robot. Biosyst. Eng. 106, 112–124.
Wachs, J.P., Stern, H.I., Burks, T., 2009. Apple detection in natural tree canopies from
multimodal image. In: 6th European Conference on Precision Agriculture, pp.
293–302.
Wachs, J.P., Stern, H.I., Burks, T., 2010. Low and high-level visual feature-based
apple detection from multi-modal images. Precis. Agric. 11, 717–735.
Wang, F.C., Xu, Y., Song, H.B., 2015. Study on identification of tomatoes based on
fuzzy clustering algorithm. J. Agric. Mech. Res. 10, 24–28.
Wei, X.Q., Kun, J., Jin, H.L., 2014. Automatic method of fruit object extraction under
complex agricultural background for vision system of fruit picking robot. Optics
125 (12), 5684–5689.
Whittaker, A.D., Miles, G.E., Mitchell, O.R., 1987. Fruit location in a partially
occluded image. Trans. ASAE 30 (3), 591–596.
Xiang, R., Jiang, H.Y., Ying, Y.B., 2014. Recognition of clustered tomatoes based on
binocular stereo vision. Comput. Electr. Agric. 106 (8), 75–90.
Xie, Z.Y., Zhang, T.Z., Zhao, J.Y., 2007. Ripened strawberry recognition based on
Hough transform. Trans. CSAM 38 (3), 106–109.
Xie, Z.H., Ji, C.Y., Guo, X.Q., 2010. An object detection method for quasi-circular
fruits based on improved Hough transform. Trans. CSAE 26 (7), 157–162.
Xie, Z.H., Ji, C.Y., Guo, X.Q., 2011. Detection and location algorithm for overlapped
fruits based on concave spots searching. Trans. CSAE 42 (12), 191–196.

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323
Xiong, J.T., Zou, X.J., Wang, H.J., 2013. Recognition of ripe litchi in different
illumination conditions based on Retinex image enhancement. Trans. CSAE 29
(12), 170–178.
Xu, H.R., Ying, Y.B., 2004. Detection citrus in a tree canopy using infrared thermal
imaging. SPIE 5271, 321–327.
Yamamoto, K., Guo, W., Yoshioka, Y., Ninomiya, S., 2014. On plant detection of intact
tomato fruits using image analysis and machine learning methods. Sensors 14,
12191–12206.
Yang, L., Dickinson, J., Wu, Q.M.J., 2007. A fruit recognition method for automatic
harvesting. In: 14th International Conference on Mechatronics and Machine
Vision in Practice, pp. 152–157.
Yau, W.Y., Wang, H., 1996. Robust hand-eye coordination. Adv. Robotic 11 (1),
57–73.
Yin, H.P., Chai, Y., Yang, S.X., 2009a. Ripe tomato recognition and localization for a
tomato harvesting robotic system. Int. Conf. Soft Comput. Patter Recogn.
557–562.
Yin, H.P., Chai, Y., Yang, S.X., 2009b. Ripe tomato extraction for a harvesting robotic
system. In: Proceedings of IEEE International Conference on System, Man, and
Cybernetics, pp. 2984–2989.
Yuan, T., Ji, C., Chen, Y., Li, W., Zhang, J.X., 2011. Greenhouse cucumber recognition
based on spectral imaging technology. Trans. CSAM S1, 172–176.
Zhan, W.T., He, D.J., Shi, S.L., 2013. Recognition of kiwifruit in field based on
adaboost algorithm. Trans. CSAE 29 (23), 140–146.

View publication stats

323

Zhang, B.H., Huang, W.Q., Li, J.B., 2014. Principles, developments and applications of
computer vision for external quality inspection of fruits and vegetables: a
review. Food Res. Int. 62, 326–343.
Zhang, B.H., Huang, W.Q., Wang, C.P., 2015. Computer vision recognition of stem
and calyx in apples using near-infrared linear-array structured light and 3D
reconstruction. Biosyst. Eng. 139 (12), 25–34.
Zhao, D.A., Lv, J.D., Ji, W., 2011. Design and control of an apple harvesting robot.
Biosyst. Eng. 110 (2), 112–122.
Zhao, J., Tow, J., Katupitiya, J., 2005. On-tree fruit recognition using texture
properties and color data. In: IEEE/RSJ International Conference on Intelligent
Robots and Systems, pp. 263–268. Aug.
Zhao, Y.S., Gong, L., Huang, Y.X., Liu, C.L., 2016a. Robust tomato recognition for
robotic harvesting using feature images fusion. Sensors 16 (2), 173–185.
Zhao, Y.S., Gong, L., Zhou, B., Huang, Y.X., Liu, C.L., 2016b. Detecting tomatoes in
greenhouse scenes by combining AdaBoost classifier and colour analysis.
Biosyst. Eng. 148 (8), 127–137.
Zion, B., Mann, M., Levin, D., Shilo, A., Rubinstein, D., Shmulevich, I., 2014.
Harvested-order planning for a multiarm robotic harvester. Comput. Electr.
Agric. 103, 75–81.
Zou, X.J., Zou, H.X., Lu, J., 2012. Virtual manipulator-based binocular stereo vision
positioning system and errors modelling. Mach. Vision Appl. 23, 43–63.

Most existing discriminant manifold learning methods aim to maximize the margin among nearby data, which is determined in the high-dimensional original space. As such, they do not necessarily best maximize the margin between different classes in the low-dimensional space, which is a critically important property for image classification. To handle this problem, we propose an adaptive maximum margin analysis (AMMA) for feature extraction. AMMA aims to seek a projection matrix that best maximize the margin, which is calculated in the low- dimensional space. It uses sparse representation to adaptively construct the intrinsic and penalty graphs. Finally, an iterative algorithm is developed to solve the projection matrix. Extensive experimental results on several image databases illustrate the effectiveness of the proposed approach.
Multiplicative noise and blur corruptions usually happen in coherent imaging systems, such as the synthetic aperture radar. Total variation regularized multiplicative noise and blur removal models have been widely studied in the literature, which can preserve sharp edges of the recovered images. However, the images recovered from the total variation based models usually suffer from staircase effects. To overcome this deficiency, we propose a total generalized variation regularized convex optimization model. The resulting objective function involves the total generalized variation regularization term, the MAP based data fitting term and a quadratic penalty term which is based on the statistical property of the noise. Indeed, the MAP estimated data fitting term in the multiplicative noise and blur removal model is nonconvex. Under a mild condition, the quadratic penalty term makes the objective function convex. A primal-dual algorithm is developed to solve the minimization problem. Numerical experiments show that the proposed method outperforms some state-of-the-art methods.
Hypernetwork, as a useful representation of natural and social systems has received increasing interests from researchers. Community is crucial to understand the structural and functional properties of the hypernetworks. Here, we propose a new method to uncover the communities of hypernetworks. We construct a Density-Ordered Tree (DOT) to represent original data by combining density and distance, and the community detection in hypernetwork is converted to a DOT partition problem. Then, an anomaly detection strategy using box-plot rule is applied to partition DOT and judge whether there is a significant community structure in the hypernetwork. Moreover, visual inspection as a complementary approach of box-plot rule can effectively improve the effectiveness of community detection. Finally, the method is compared with existing methods in both synthetic and real-world networks.
Analytic two-dark soliton solutions for a variable–coefficient nonlinear Schrödinger equation are obtained via modified Hirota method. Parallel solitons are observed and soliton control such as the soliton compression is realized with different group velocity dispersion profiles. Besides, soliton interactions are investigated with the interaction distance being adjusted. In addition, soliton repulsive structures as well as attractive ones are obtained with exponential dispersion profile. Results in our research may be useful for the soliton control in inhomogeneous optical fibers, which will be a benefit to the realistic optical communication systems.
Artificial bee colony algorithm (ABC) is a relatively new optimization algorithm. However, ABC does well in exploration but badly in exploitation. One possible way to improve the exploitation ability of the algorithm is to combine ABC with other operations. Differential evolution (DE) can be considered as a good choice for this purpose. Based on this consideration, we propose a new algorithm, i.e. DGABC, which combines DE with gbest-guided ABC (GABC) by an evaluation strategy with an attempt to utilize more prior information of the previous search experience to speed up the convergence. In addition, to improve the global convergence, when producing the initial population, a chaotic opposition-based population initialization method is employed. The comparison results on a set of 27 benchmark functions demonstrate that the proposed method has better performance than the other algorithms.
Coordinated path planning for multiple unmanned aerial vehicles (multi-UAVs) is a highly significant problem encountered in their coordinated control. In the interests of completing mission securely and efficiently, the advanced multi-UAVs control technology requires a universal smoothing method as well as a precise coordination strategy. In this paper, we propose a novel multi-UAVs coordinated path planning method based on the k-degree smoothing, a more complex environment consists of multiple threat sources of which is constructed. By employing the Improved Ant Colony Optimization algorithm, a k-degree smoothing method is also presented aiming at obtaining a more flyable path. Additionally, the multi-UAVs coordination algorithm is induced by k-degree smoothing, allowing the UAVs to arrive at the destination simultaneously or in an acceptable time interval. Finally, simulations of the comparison between the Improved Ant Colony Optimization and classic algorithm, the detailed smoothing method, and the coordination are respectively conducted to validate that the proposed approach is feasible and effective in multi-UAVs coordinated path planning problems.
More recently, Chang and Lee proposed a secure single sign-on mechanism to allow mobile users to use the unitary token to access service providers. In this paper, we fist demonstrate that, not as claimed, their scheme is not efficient and thus not suitable for mobile devices in distributed computer networks. Then we employ elliptic curve cryptosystems and proxy signature to design a new scheme. Both the security and performance analysis shows that our scheme is better suited for resource constrained devices.
As a special class of non-coding RNAs (ncRNAs), microRNAs (miRNAs) perform important roles in numerous biological and pathological processes. The realization of miRNA functions depends largely on how miRNAs regulate specific target genes. It is therefore critical to identify, analyze, and cross-reference miRNA-target interactions to better explore and delineate miRNA functions. Semantic technologies can help in this regard. We previously developed a miRNA domain-specific application ontology, Ontology for MIcroRNA Target (OMIT), whose goal was to serve as a foundation for semantic annotation, data integration, and semantic search in the miRNA field. In this paper we describe our continuing effort to develop the OMIT, and demonstrate its use within a semantic search system, OmniSearch, designed to facilitate knowledge capture of miRNA-target interaction data. Important changes in the current version OMIT are summarized as: (1) following a modularized ontology design (with 2559 terms imported from the NCRO ontology); (2) encoding all 1884 human miRNAs (vs. 300 in previous versions); and (3) setting up a GitHub project site along with an issue tracker for more effective community collaboration on the ontology development. The OMIT ontology is free and open to all users, accessible at: http://purl.obolibrary.org/obo/omit.owl. The OmniSearch system is also free and open to all users, accessible at: http://omnisearch.soc.southalabama.edu/index.php/Software.
Protein solvent accessibility prediction is a pivotal intermediate step towards modeling protein tertiary structures directly from one-dimensional sequences. It also plays an important part in identifying protein folds and domains. Although some methods have been presented to the protein solvent accessibility prediction in recent years, the performance is far from satisfactory. In this work, we propose PredRSA, a computational method that can accurately predict relative solvent accessible surface area (RSA) of residues by exploring various local and global sequence features which have been observed to be associated with solvent accessibility. Based on these features, a novel and efficient approach, Gradient Boosted Regression Trees (GBRT), is first adopted to predict RSA.
Protein solvent accessibility prediction is a pivotal intermediate step towards modeling protein tertiary structures directly from one-dimensional sequences. It also plays an important part in identifying protein folds and domains. Although some methods have been presented to the protein solvent accessibility prediction in recent years, the performance is far from satisfactory. In this work, we propose PredRSA, a computational method that can accurately predict relative solvent accessible surface area (RSA) of residues by exploring various local and global sequence features which have been observed to be associated with solvent accessibility. Based on these features, a novel and efficient approach, Gradient Boosted Regression Trees (GBRT), is first adopted to predict RSA.
All biological processes are inherently dynamic. Biological systems evolve transiently or sustainably according to sequential time points after perturbation by environment insults, drugs and chemicals. Investigating the temporal behavior of molecular events has been an important subject to understand the underlying mechanisms governing the biological system in response to, such as, drug treatment. The intrinsic complexity of time series data requires appropriate computational algorithms for data interpretation. In this study, we propose, for the first time, the application of dynamic topic models (DTM) for analyzing time-series gene expression data.
Anguilla japonica (Japanese eel) is currently one of the most important research subjects in eastern Asia aquaculture. Enigmatic life cycle of the organism makes study of artificial reproduction extremely limited. Henceforth genomic and transcriptomic resources of eels are urgently needed to help solving the problems surrounding this organism across multiple fields. We hereby provide a reconstructed transcriptome from deep sequencing of juvenile (glass eels) whole body samples. The provided expressed sequence tags were used to annotate the currently available draft genome sequence. Homologous information derived from the annotation result was applied to improve the group of scaffolds into available linkage groups.
This paper presents a minimum void length scale control method for structural topology optimization. Void length scale control has been actively investigated for decades, which intends to ensure the topology design manufacturable given the machining tool access. However, only a single lower bound has been applied in existing methods, which does not fit the multi-stage rough-to-finish machining. To fix this issue, the proposed minimum void length scale control method employs double lower bounds which corresponds to the rough and finish machining operations, respectively. This method has been implemented under the level set framework. For technical details, the rough machining lower bound is satisfied by developing a signed distance-related constraint, which ensures enough space for the rough machining tool movement and thus, guarantees the machining efficiency. The finish machining lower bound is addressed through the curvature flow control, which ensures the small features manufacturable and also a good finish dimension and surface. Through a few numerical case studies, it is proven that the minimum void length scale can be effectively controlled without sacrificing much of the structural performance.
This work proposes a novel Adaptive Instruction Codec Architecture (AICA) for network-on-chip (NoC) that improves channel utilization to transfer packets and flits, in order to solve issues of power consumption and throughput. The proposed architecture allows multiple packets to be stuffed into a single packet, and thus can transfer more packets than other network interface (NI) in one time unit. Reducing the number of packets for transmission allows the channel to be reused to transfer additional messages, thus improving channel throughput. This architecture reduces the number of packets transmitted, thus indirectly alleviating the deadlock problem. Many repeating and similar instructions are frequently transferred in NoC. The proposed AICA reduces transmission redundancy, and supports process elements (PE) with 16-bit or 64-bit core CPU. Experimental results show that the proposed architecture and algorithms delivers improvement of up to 48.1% on power consumption, and 46.3% on throughput.
Representing rotational symmetry vector as a set of vectors is not suitable for design due to lacking of a consistent ordering for measurement. In this paper we introduce a spectral method to find rotation invariant harmonic functions for symmetry vector field design. This method is developed for 3D vector fields, but it is applicable in 2D. Given the finite symmetry group G of a symmetry vector field v(x) on a 3D domain Ω, we formulate the harmonic function h(s) as a stationary point of group G. Using the real spherical harmonic (SH) bases, we showed the coefficients of the harmonic functions are an eigenvector of the SH rotation matrices corresponding to group G. Instead of solving eigen problems to obtain the eigenvector, we developed a forward constructive method based on orthogonal group theory. The harmonic function found by our method is not only invariant under G, but also expressive and can distinguish different rotations with respect to G. At last, we demonstrate some vector field design results with tetrahedron-symmetry, cube-symmetry and dodecahedron-symmetry groups.
See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/294421534

Protein	Inference:	A	Protein	Quantification
Perspective
Article		in		Computational	biology	and	chemistry	·	February	2016
DOI:	10.1016/j.compbiolchem.2016.02.006

CITATIONS

READS

2

85

6	authors,	including:
Zengyou	He

Ting	Huang

Dalian	University	of	Technology

Dalian	University	of	Technology

87	PUBLICATIONS			1,834	CITATIONS			

9	PUBLICATIONS			64	CITATIONS			

SEE	PROFILE

SEE	PROFILE

Peijun	Zhu

Ben	Teng

3	PUBLICATIONS			2	CITATIONS			

Dalian	University	of	Technology

SEE	PROFILE

6	PUBLICATIONS			14	CITATIONS			
SEE	PROFILE

Some	of	the	authors	of	this	publication	are	also	working	on	these	related	projects:

network	inference,	dense	subgraph	mining	View	project

All	content	following	this	page	was	uploaded	by	Zengyou	He	on	15	February	2016.
The	user	has	requested	enhancement	of	the	downloaded	file.	All	in-text	references	underlined	in	blue	are	added	to	the	original	document
and	are	linked	to	publications	on	ResearchGate,	letting	you	access	and	read	them	immediately.

Computational Biology and Chemistry 00 (2015) 1–14

COMPUT
BIOL
CHEM

Protein Inference: A Protein Quantiﬁcation Perspective
Zengyou Hea,b,∗, Ting Huangc , Xiaoqing Liua , Peijun Zhua , Ben Tenga , Shengchun Dengd
a School of Software, Dalian University of Technology, Dalian, China.
Laboratory for Ubiquitous Network and Service Software of Liaoning, Dalian, China.
c College of Computer and Information Science, Northeastern University, USA.
d School of Computer Science and Engineering, Harbin Institute of Technology, China.

b Key

Abstract
In mass spectrometry-based shotgun proteomics, protein quantiﬁcation and protein identiﬁcation are two major
computational problems. To quantify the protein abundance, a list of proteins must be ﬁrstly inferred from the raw
data. Then the relative or absolute protein abundance is estimated with quantiﬁcation methods, such as spectral
counting. Until now, most researchers have been dealing with these two processes separately. In fact, the protein
inference problem can be regarded as a special protein quantiﬁcation problem in the sense that truly present proteins
are those proteins whose abundance values are not zero. Some recent published papers have conceptually discussed
this possibility. However, there is still a lack of rigorous experimental studies to test this hypothesis.
In this paper, we investigate the feasibility of using protein quantiﬁcation methods to solve the protein inference
problem. Protein inference methods aim to determine whether each candidate protein is present in the sample
or not. Protein quantiﬁcation methods estimate the abundance value of each inferred protein. Naturally, the
abundance value of an absent protein should be zero. Thus, we argue that the protein inference problem can be
viewed as a special protein quantiﬁcation problem in which one protein is considered to be present if its abundance
is not zero. Based on this idea, our paper tries to use three simple protein quantiﬁcation methods to solve the
protein inference problem eﬀectively. The experimental results on six data sets show that these three methods
are competitive with previous protein inference algorithms. This demonstrates that it is plausible to model the
protein inference problem as a special protein quantiﬁcation task, which opens the door of devising more eﬀective
protein inference algorithms from a quantiﬁcation perspective. The source codes of our methods are available at:
http://code.google.com/p/protein-inference/.
c 2016 Published by Elsevier Ltd.
°
Keywords: Shotgun proteomics, Protein inference, Protein quantiﬁcation, Spectral counting, Linear programming.

1. Introduction
Mass spectrometry (MS)-based shotgun proteomics is currently the most widely used method for the
identiﬁcation and quantiﬁcation of proteins (Nesvizhskii et al., 2007). As shown in Figure 1, it ﬁrst digests
proteins in the sample into a mixture of peptides by enzymes such as trypsin. The resulting peptide mixtures
are scanned by tandem mass spectrometry (MS/MS) to generate a set of MS/MS spectra. Then the peptide
identiﬁcation algorithm reports a set of peptide-spectrum matches (PSMs) by searching the MS/MS spectra
∗ Corresponding

author. Tel.: +86 411 62274405. E-mail address: zyhe@dlut.edu.cn (Z. He)

1

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

2

#4/5-6164001
0-78,*4

!" #$

)*+,-./01 213

)-7,.5-1
6.9,:*-

%&#'(
;

!"#$%

3
<)##=( )21%#!-,
!-.#2#-/#

&'(&'
)#*%!+#,
!+#-%!.!/0%!1-

340-%!.!/0%!15&'(&',$*#/%206,
/14-%$7

Figure 1: Protein identiﬁcation and quantiﬁcation using mass spectrometry in shotgun proteomics. There
are three major computational problems: peptide identiﬁcation, protein inference and protein quantiﬁcation.
against a protein database. From these peptide identiﬁcations, we infer the existence of proteins with protein
inference algorithms and calculate the relative or absolute abundances of proteins with protein quantiﬁcation
approaches.
Until recently, people tackle the identiﬁcation and quantiﬁcation of proteins as two individual and subsequent tasks: ﬁrst select a subset of proteins that are truly present and then determine the quantities of
these proteins. For both problems, many elegant approaches have been developed in the past decades. The
readers can refer to two recent reviews Huang et al. (2012) and Nikolov et al. (2012) for details.
The starting point of this paper is fact that protein inference can be regarded as a special case of protein
quantiﬁcation. In protein inference, the objective is to generate a binary presence indicator value (1 or
0) for each candidate protein. In this regard, “protein existence inference” is probably more accurate for
describing the original protein inference task. In protein quantiﬁcation or “protein abundance inference”,
the goal is to determine the abundance of each protein. Clearly, if one protein is not present, its abundance
value should be 0. Hence, the protein inference problem can be investigated from the perspective of protein
quantiﬁcation: present proteins are those proteins whose abundance values are not zero. In other words, we
can adopt available protein quantiﬁcation methods directly to solve the protein inference problem. This new
angle may enable a better understanding of the protein inference problem and help in devising improved or
hybrid protein inference methods by borrowing the power from protein quantiﬁcation.
The possibility of exploiting protein quantiﬁcation methods to solve the protein inference problem has
been conceptually discussed in several papers (Dost et al., 2012; Li and Radivojac, 2012). Dost et al. (2012)
used a simple example to show that it is feasible to obtain more accurate protein identiﬁcations with protein
quantiﬁcation methods than traditional parsimonious approaches. Li and Radivojac (2012) also pointed out
that the protein inference problem can be regarded as a special protein quantiﬁcation problem. However,
they argued that existing protein quantiﬁcation methods have not yet reached the accuracy needed for the
wide dynamic range of quantities observed in cellular proteomics. As a result, solving the more general and
diﬃcult quantiﬁcation problem may not provide a more accurate solution for the protein inference problem.
Although people have realized the potential of solving the protein inference problem from a quantiﬁcation perspective, there are still no rigorous and extensive experimental studies to test this hypothesis.
To fulﬁll this void, we empirically demonstrate the feasibility of solving the protein inference problem with
existing protein quantiﬁcation methods in the context of label-free proteomics. In the label-free quantitative proteomics studies, quantiﬁcation methods which are based on peak ion intensities (from MS data)
(Neilson et al., 2011) and spectral counting (from MS/MS data) (Lundgren et al., 2010; Choi et al., 2008)
have been widely used.
2

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

3

Spectral counting measures the abundance of each protein based on the number of MS/MS spectra
that match its constituent peptides. Given the peptide identiﬁcation result, we can directly obtain spectral
counting information since we just need to count the number of MS/MS spectra. In this paper, we use
spectral counting as the quantiﬁcation approach for solving the protein inference problem.
We ﬁrst try two simple spectral counting methods in the literature. In both methods, the protein
abundance is calculated as the sum of peptide abundance values. Their diﬀerence lies in how to handle the
shared peptide. If the abundance of one shared peptide is b and it has k parent proteins, then b is used as its
abundance value in the ﬁrst method while b/k is used as its abundance value in the second method. These two
methods assume that all the candidate proteins are present in the sample. As a result, the abundance value
of each candidate protein will not be zero. However, this assumption contradicts the objective of protein
inference: distinguishing present proteins (abundance6=0) from absent proteins (abundance=0). Thus, we
extend the second linear programming model in (Dost et al., 2012) to distribute the abundance values of
shared peptides automatically in order to shrink the abundance values of absent proteins to zero.
To our knowledge, our paper is the ﬁrst rigorous study with extensive experiments to demonstrate
the feasibility of using protein quantiﬁcation methods for solving the protein inference problem. Such an
attempt connects two important computational problems that have long been investigated separately. The
experimental results show that we can obtain better performance in most data sets even when the most
simple version of spectral counting is utilized. Hence, the advance in protein quantiﬁcation studies will
promote the development of more eﬀective protein inference algorithms.
In Section 2, we describe the details of three methods. Section 3 shows the experimental results on six
data sets. Section 4 presents some discussions and Section 5 concludes the paper.
2. Methods
As shown in the left side of Figure 2, the input of the protein inference problem can be represented as
a tripartite graph G = (X ∪ Y ∪ Z, E1 ∪ E2 ), where X, Y and Z are the set of l MS/MS experimental
spectra, m identiﬁed peptides and n candidate proteins, respectively. For all xi ∈ X, yj ∈ Y , there is an
edge (xi , yj ) ∈ E1 if and only if the spectrum xi matches the peptide yi in the peptide identiﬁcation results.
Similarly, (yj , zk ) ∈ E2 means that the peptide yj is one part of the protein zk . Each MS/MS spectrum
corresponds to one and only one identiﬁed peptide whereas some peptides may have more than one matching
spectrum, such as the peptides y2 and y3 in Figure 2. The relationship between peptides and proteins is
more complex: one candidate protein may have several identiﬁed peptides and each peptide can be shared by
multiple proteins. How to correctly distribute these shared peptides is one of the most challenging problem
in protein inference.
We ﬁrst formulate the protein inference problem as a special protein quantiﬁcation problem. The objective of protein inference is to determine whether each candidate protein is present in the sample. The aim of
protein quantiﬁcation is to estimate the abundance value of each identiﬁed protein. Clearly, if one protein
is not present in the sample, its abundance value should be 0. In this paper, the protein inference problem
is re-visited from the perspective of protein quantiﬁcation through seeking those proteins whose abundance
values are not zero.
To obtain the protein abundance, we start with calculating the peptide abundance. Let bj denote the
abundance value of the peptide yj , which can calculated as the sum of PSM probabilities (or scores):
∑
bj =
ai ,
(1)
(xi ,yj )∈E1

where ai is the probability that the spectrum xi matches the peptide yj . Notice that ai can be also viewed
as the weight of edge (xi , yj ) ∈ E1 , which can be obtained from peptide identiﬁcation algorithms such
as Mascot (Perkins et al., 1999) or post-processing tools such as PeptideProphet (Keller et al., 2002). In
the traditional spectral counting methods, the peptide abundance is simply the number of MS/MS spectra
identiﬁed for each peptide. Here, we generalize this spectral counting method to account for the quality of
PSMs. More precisely, the contribution of each spectrum to the peptide abundance becomes a quantitative
3

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14
:4-;,*<
6!

)-4,.5-0
%!

)*+,-./0

4

'!(
$

!

%$
#

6$
$

6#

'$(

%$

6"

%$

$

%$

#

#

6&
%#
67
68

"

'#(

%"

%$

&

69

%&

123$

$

123$
#

Figure 2: Three approaches for solving the shared peptide problem. y1 and y3 are unique peptides while y2 ,
y4 and y5 are shared peptides. The abundance of peptide yj is represented by bj . We use the peptide y2 as
an example to explain how these three approaches work.
value between 0 and 1 rather than a ﬁxed value of 1. Such an extension is extremely important for protein
inference since it may help us to distinguish between the proteins with the same number of PSMs.
To calculate the protein abundance, we need to distribute the abundance of each peptide to its parent
proteins. The main diﬃculty is how to deal with the degenerate peptide that is shared by more than one
protein since such a peptide can be generated by any subset of its parent proteins (Yang et al., 2013).
There are several approaches for solving the shared peptide problem in protein quantiﬁcation (Zhang et al.,
2010), as shown in the right side of Figure 2. The ﬁrst approach is to simply discard the shared peptides and
only use the unique peptides to calculate the protein abundance. But this approach has one disadvantage:
it causes the loss of information, especially for the proteins whose identiﬁed peptides are all shared peptides.
In Figure 2, if we delete the shared peptide y2 , then the proteins z2 and z3 do not have any identiﬁed
peptides and they would be considered absent in the sample. In fact, at least one of these two proteins
must be present if we assume the existence of peptide y2 . Alternatively, we can use both unique and shared
peptides to estimate the protein abundance. In the second approach, the abundance of the shared peptide
is utilized to calculate the abundance of all its parent proteins. In other words, each peptide is counted
multiple times so that the abundance values of some proteins may be over-estimated. We call this method
“multiple counting” in this paper. For example, the peptide y2 in Figure 2 is counted twice in the second
approach, which means that we artiﬁcially increase the abundance of peptide y2 from b2 to 2 ∗ b2 . The third
approach divides the abundance of the shared peptide into diﬀerent parts and then distributes each part to
one of its parent proteins. This approach ensures that each peptide is “counted” only once. One typical
representative in this category is the “equal division” method, which partitions the peptide abundance into
k equal parts (k is the number of proteins that share this peptide).
Since both multiple counting and equal division are the most popular and simple quantiﬁcation approaches based on spectral counting, we ﬁrst try these two methods and test their performance for the
protein inference task. Note that these two methods have an implicit assumption that the abundance value
of each candidate protein is not zero. However, this assumption does not hold in the context of protein inference since the abundance values of some absent proteins should be zero. Thus, a new linear programming
model is proposed, which can automatically distribute the peptide abundance so as to shrink the abundance
values of some proteins to zero.

4

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

5

2.1. Multiple Counting
In this method, the shared peptides are used in the same way as the unique peptides and receive no
special treatment. The abundance of a protein is simply the sum of abundance values from all its identiﬁed
peptides:
∑
ck =
bj ,
(2)
(yj ,zk )∈E2

where ck is the abundance of protein zk . If the peptide yj has qj parent proteins, then it is counted qj times
and its actual abundance value used in the calculation is qj · bj .
2.2. Equal Division
Diﬀerent from the above method that counts shared peptides multiple times, the equal division method
counts each peptide only once. It equally distributes the abundance of each shared peptide to its parent
proteins:
∑
bj
ck =
,
(3)
qj
(yj ,zk )∈E2

where qj is the number of candidate proteins sharing the peptide yj . If the peptide yj is a unique peptide,
then qj = 1.
2.3. Linear Programming Model
Shared peptides play an important role in both protein inference and protein quantiﬁcation. Dost et al.
(2012) presented a linear programming (LP) model which used shared peptides to estimate the relative
protein abundance. Kim (2012) modiﬁed this LP model to qualify the absolute protein abundance. On the
basis of these attempts, we further extend the LP model and apply it to infer the identities of proteins.
For each identiﬁed peptide yj , the peptide abundance can be computed as:
∑
∑
bj =
detjk · ck =
djk ,
(4)
{k|(yj ,zk )∈E2 }

{k|(yj ,zk )∈E2 }

where detjk ∈ (0, 1) is the detectability of the peptide yj and it represents the probability that the peptide
yj can be identiﬁed in a standard experiment if its parent protein zk is present (Tang et al., 2006). In
order to simplify the model, we introduce a new variable djk to replace the product between the peptide
detectability detjk and the protein abundance ck . Then, djk is interpreted as the abundance that the protein
zk contributes to the peptide yj . The variable djk can serve as the bridge between the peptide abundance and
the protein abundance. On one hand, we can use djk to explain the known peptide abundance. On the other
hand, we can calculate the unknown protein abundance through djk . Therefore, the protein quantiﬁcation
problem is equivalent to ﬁnding an optimal matrix D = (djk ).
According to the above analysis, we propose a new LP model to solve the protein quantiﬁcation problem:
min

tk ,

(5)

djk = 0,

(6)

∀j, k : djk ≤ tk ,

(7)

D

∑

∀j : bj −

n
∑
k=1

{k|(yj ,zk )∈E2 }

∀j, k : djk

{
=0
∼
≥0

if (yj , zk ) ∈
/ E2
else

.

(8)

Constraint (6) forces the predicted peptide abundance to be equal to the observed abundance value.
Constraint (7) is to ﬁnd the maximum value in each column vector dk (the kth column of the matrix D).
5

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

6

Then, minimizing the objective function (the sum of maximum peptide abundance value from each protein)
will shrink the abundance values of some proteins to 0.
After obtaining the matrix D, it is still a non-trivial task to recover the
∑ protein abundance value ck
since the peptide detectability value detjk is unknown. If we assume that {j|(yj ,zk )∈E2 } detjk = 1, then
the protein abundance ck can be calculated as:
∑
∑
ck =
detjk · ck =
djk .
(9)
{j|(yj ,zk )∈E2 }

{j|(yj ,zk )∈E2 }

Notice that the above assumption on the sum of peptide detectability values is generally not true.
Therefore, the calculated value according to Equation (9) is only an estimated value of the true protein
abundance.
Previously, we have introduced a linear programming method, ProteinLP (Huang and He, 2012), to solve
the protein inference problem. The LP model presented in this paper is essentially diﬀerent from ProteinLP
at least in the following ways:
• Our paper is based on the idea that the protein inference problem can be solved as a special protein
quantiﬁcation problem. Here we want to show the possibility of using protein quantiﬁcation methods
to address the protein inference problem. Thus, the LP method in this paper is actually a special
protein quantiﬁcation method, which mainly deals with peptide/protein abundance values. While
ProteinLP focuses on calculating the protein existence probability based on the peptide identiﬁcation
probability values.
• These two methods have diﬀerent assumptions. ProteinLP assumes that one peptide will be absent
if all its parent proteins are not present in the sample. The LP model in this paper is based on the
assumption that the abundance value of a peptide is equal to the sum of the abundance values from
all its parent proteins.
• The variables in these two LP models are diﬀerent. The variable of ProteinLP is a mathematical
transformation of the joint probability that both a protein and its constituent peptide are present in
the sample. The variable in this paper is the abundance that one parent protein contributes to its
constituent peptide.
• The outputs of these two methods are diﬀerent. The output of ProteinLP is the probability that one
protein is present while that of our method is the protein abundance.
• The new LP model does not need any parameters while ProteinLP has to specify a threshold parameter
manually. In order to ﬁnd the proper parameter automatically, ProteinLP still needs to run an
additional parameter selection procedure.
2.4. Converting Scores into Probabilities
After knowing the protein abundance, it is beneﬁcial to convert the abundance into well-calibrated
probability. The main reason is that the probability estimation allows us to select the appropriate threshold
for reporting a set of conﬁdent proteins. In fact, the problem of converting ranking scores into estimated
probabilities has been widely investigated in diﬀerent domains (e.g., Gao and Tan (2006)). In this paper,
we use the method proposed in (Gao and Tan, 2006) to fulﬁll this task.
We ﬁrst estimate the probability pk that the protein zk is present in the sample given its abundance ck :
P r(zk = 1|ck )
P r(ck |zk = 1)P r(zk = 1)
P r(ck |zk = 1)P r(zk = 1) + P r(ck |zk = 0)P r(zk = 0)
1
=
,
1 + exp(−fk )
=

6

(10)

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

where
fk = log

P r(ck |zk = 1)P r(zk = 1)
.
P r(ck |zk = 0)P r(zk = 0)

7

(11)

fk can be considered as a discriminant function which has a Gaussian distribution with equal covariance
matrices (Bishop, 1995). Then, Equation (10) becomes
pk =

1
.
1 + exp(Ack + B)

(12)

Now, we need to estimate the parameters, A and B. Let rk be a binary variable whose value is 1 if the
protein zk is present in the sample and 0 otherwise. Then, R = (r1 , r2 , · · · , rn ) is the presence indicator
vector of n candidate proteins. If we assume that the existence of each protein is independent of other
proteins, the probability of observing R given C is:
P r(R|C) =

n
∏

prkk (1 − pk )1−rk ,

(13)

k=1

where C = {c1 , c2 , · · · , cn }. The optimal parameter values should maximize P r(R|C), i.e., minimize the
following negative log likelihood function:
LL(R|C) =

n
∑

[(1 − rk )(−Ack − B) + log(1 + exp(Ack + B))].

(14)

k=1

Equation (14) is based on the assumption that we have already known the indicator vector R. However,
we do not know such information in the protein inference process. Thus, we consider rk s as hidden variables
and employ the EM algorithm to simultaneously estimate A, B and R.
The EM algorithm utilizes an iterative procedure to estimate the parameter value θ = {A, B}. The
procedure includes two steps: set rks+1 = E(rks |C, θs ) (E-step) and compute θs+1 = arg minθ LL(Rs+1 |C)
(M-step), where s is the iteration index. During the E-step, the unknown vector R is replaced by its expected
value Rs+1 under the current estimated parameter value θs . Since θs is ﬁxed, LL(R|C) is minimized by
setting rk = 0 if Ack + B > 0 or rk = 1 if Ack + B ≤ 0. During the M-step, a new parameter estimation θs+1
is computed by minimizing LL(R|C) given the vector Rs+1 calculated by the ﬁrst step. Since Rs = [rks ] is
ﬁxed, minimizing LL(R|C) with respect to A and B is a two-parameter optimization problem, which can
be solved using the model-trust algorithm described in (Platt, 2000).
In the above score transformation procedure, all proteins share the same set of model parameters. In fact,
the estimated abundance values from diﬀerent proteins are generally not comparable since longer proteins
may tend to have more matched mass spectra than shorter proteins even they have the same quantities.
Therefore, a new model that takes into account more factors such as the length and ionization properties of
proteins should be developed in the future.
3. Experimental Results
To test the performance of quantiﬁcation-based protein inference methods, we have compared our methods with ProteinProphet (Nesvizhskii et al., 2003) and ProteinLP (Huang and He, 2012) on the six datasets.
3.1. Data sets
We choose six publicly available data sets to validate the performance of our methods. The names and
URLs of these data sets are given in Table 1. These six data sets are divided into two categories: three data
sets with reference sets and the other three data sets without reference sets. The ﬁrst three data sets, 18
mixtures (Klimek et al., 2008), Sigma49 (Tabb et al., 2007) and yeast (Ramakrishnan et al., 2009a), have
their corresponding reference sets that contain the ground-truth proteins. The another three data sets, DME
(Brunner et al., 2007), HumanMD (Ramakrishnan et al., 2009b) and HumanEKC (Ramakrishnan et al.,
7

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

8

2009a), do not have such reference sets. For the data sets without reference sets, a target-decoy strategy
is used instead to assess the performance. This strategy searches MS/MS spectra against a hybrid protein
database which is composed of target protein sequences from the original database and the same number
of decoy sequences (Teng et al., 2014). Thus, an identiﬁed protein is considered as a true positive if it is
present in the protein reference set or comes from the target protein database.
Mixture of 18 Puriﬁed Proteins (18 mixtures) and Sigma49 data set. These two data sets
are both generated from the sample of synthetic proteins mixtures. The protein database used for the 18
mixtures data set consists of 1,819 protein sequences, which includes 18 ground-truth proteins and some
contaminant proteins. The database for the Sigma49 data set contains 15,682 Swiss-Prot human protein
sequences.
Yeast data set. Its reference set is available at http://www.marcottelab.org/MSdata/gold/yeast.html.
The protein database includes 6,714 protein sequences.
D. melanogaster data set (DME). The DME data set is produced from the embryonal Kc 167 cell
line of D. melanogaster. We use Flybase (release 5.2) as the protein database, which contains 20,726 entries.
HumanMD data set and HumanEKC data set. The HumanMD data set is generated from
medulloblastoma Daoy cell line and the HumanEKC data set is produced from human embryonic kidney
T293 cell line. We use Ensembl (version 49.36k) as the protein database, which has 22,997 entries.
Table 1:

The data sets used in the experiment and their URLs.

Data Set

The URL of Raw Data

Mixture of 18 Puriﬁed Proteins (Klimek et al., 2008)

http://regis-web.systemsbiology.net/PublicDatasets/

Sigma49 Data Set (Tabb et al., 2007)

https://proteomecommons.org/dataset.jsp?i=71610

Yeast Data Set (Ramakrishnan et al., 2009a)

http://www.marcottelab.org/users/MSdata/Data_02/

D. melanogaster Data Set (Brunner et al., 2007)

http://www.peptideatlas.org/repository/ (PAe001349)

HumanMD Data Set (Ramakrishnan et al., 2009b)

http://www.marcottelab.org/MSdata/Data_05/

HumanEKC Data Set (Ramakrishnan et al., 2009a)

http://www.marcottelab.org/MSdata/Data_07/

3.2. Peptide Identiﬁcation
We use X!Tandem (v2010.10.01.1) (Craig and Beavis, 2004) for peptide identiﬁcation with default search
parameters. For the data sets with the reference sets, the MS/MS spectra are only searched against the target
protein databases. For the data sets without the reference sets, the spectra are searched against both target
and decoy protein databases. The peptide identiﬁcation results are post-processed with PeptideProphet
(Trans-Proteomic Pipeline v4.5) to obtain the presence probability for each peptide.
3.3. Protein Inference
We choose ProteinProphet and ProteinLP as the competing methods. ProteinProphet is the most
popular method for protein inference so far. ProteinLP is one representative method that is also based on
linear programming. We run ProteinProphet with its default parameter setting and run ProteinLP with
parameter ² = 0. Since some distinct proteins may have the same set of identiﬁed peptides, we cannot
distinguish these proteins from each other without further evidence. Therefore, all the protein inference
methods in the experiments will put these indistinguishable proteins into the same group. Each group of
indistinguishable proteins is treated as a single protein during the protein inference procedure. When we
evaluate the performance of diﬀerent methods, we count all proteins in each group and use the presence
probability of each group as the identiﬁcation probability for proteins in that group.
3.4. Results
We use the curve that shows the number of true positives as a function of the q-value to assess the
performance of diﬀerent methods. Given a certain probability threshold t, the q-value is the minimal
false discovery rate (FDR) that is reported for a protein: qt = mint0 ≤t F DRt0 . The FDR is estimated
8

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

9

Sigma49

18 mixtures

45

20

Number of true positives

Number of true positives

40
15

10
PQ−1
PQ−2
5

PQ−3
PLP

35
30
25
20
PQ−1
15

PQ−2

10

PQ−3
PLP

5

PP
0

PP

0
0

0.1

0.2

0.3

0.4

0.5

0

0.05

0.1

0.15

q−value

0.2
q−value

0.3

0.35

800

1000

700

800
600
PQ−1
400

PQ−2

200

PQ−3
PLP

600
500
400
PQ−1

300

PQ−2
200

PQ−3
PLP

100

PP

PP

0
0

0.02

0.04

0.06

0.08

0.1

0
0

0.02

0.04

q−value

0.06

0.08

0.1

q−value

HumanMD

HumanEKC

350

700

300

600

250
200
150

PQ−1

100

PQ−2

50

PQ−3
PLP

Number of true positives

Number of true positives

0.4

DME

1200

Number of true positives

Number of true positives

Yeast

0.25

500
400
300

PQ−1

200

PQ−2

100

PQ−3
PLP

PP

PP

0

0
0

0.02

0.04

0.06

0.08

0.1

q−value

0

0.005

0.01

0.015
q−value

0.02

0.025

0.03

Figure 3: The comparison of identiﬁcation performance among ProteinLP (PLP), ProteinProphet (PP) and
our own three methods: multiple counting (PQ-1), equal division (PQ-2) and linear programming (PQ-3).
If some proteins have the same probability in the ordered protein list, we skip these proteins with the same
probability and calculate the q-value at the ﬁrst encountered protein with a diﬀerent probability.
as F DRt0 = Ft0 /(Ft0 + Tt0 ), where there are Tt0 true positives (TPs) and Ft0 false positives (FPs) with
0
probabilities ≥ t .
Figure 3 displays the number of TPs reported by the ﬁve methods at diﬀerent q-values. It shows that
our methods are competitive with available protein inference algorithms. Throughout the six data sets,
our three methods can always achieve zero FPs among the highest ranking proteins while the other two
algorithms do not have such a property. This fact indicates that our methods have more strong distinction
power than existing methods. More speciﬁcally, we have the following important observations.
9

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

10

First, the multiple counting method is the best performer on the HumanMD and HumanEKC data sets.
For the HumanMD data set, it reports the largest number of TPs when the q-value is equal to 0. For
the HumanEKC data set, it just identiﬁes 17 fewer true positives than ProteinProphet at q-value=0. Even
though the multiple counting method does not keep such excellent performance on the 18 mixtures, Sigma49
and DME data sets, it never performs the worst.
Second, equal division is the best performer (or tied with other algorithms) on the 18 mixtures, Sigma49
and yeast data sets. Similarly, when the q-value is equal to 0, it identiﬁes the more TPs than other methods
on the 18 mixtures, Sigma49 and yeast data sets. For the HumanMD data set, equal division does not have
the worst performance. For the HumanEKC data set, the curve of equal division is almost tied with the
curve of our LP model, ProteinProphet and ProteinLP and the gaps among these four methods are very
small.
Third, our LP model exhibits the most stable identiﬁcation performance among these ﬁve methods.
More precisely, it does not perform the worst across all six data sets. ProteinLP also has such a property,
but its performance is worse than three algorithms on the 18 mixtures and Sigma 49 data sets. In contrast,
there is only one time that the performance of our LP model is worse than three algorithms (on the DME
data set). The other three methods perform the worst on at least one data set. The number of data sets is
1, 2, 3 for multiple counting, equal division and ProteinProphet, respectively.
In the calculation of protein abundance, we generalize the number of MS/MS spectra to the sum of PSM
probabilities. We wish such an extension may help us to distinguish between proteins with the same number
of PSMs and further improve the identiﬁcation performance. Figure 4 describes the performance gain when
the generalized spectral counting is used instead of the traditional spectral counting. The experimental
results of these three methods on the six datasets agree with our expectation: using the sum of PSM probabilities actually performs better than using the number of PSMs. Overall, there are 18 comparison results
since we run our three methods on the six data sets. In these comparisons, the generalized spectral counting
method performs obviously better than traditional spectral counting in 13 comparisons and performs as well
as traditional spectral counting method in the remaining 5 comparisons.
The LP model in this paper is expected to be able to shrink the abundance values of some proteins to
zero. Table 2 shows the eﬀect of shrinkage on the six data sets. We record the number of total candidate
proteins, the number of the proteins whose abundance values are zero and their rate. For the ﬁrst two data
sets generated from simple protein mixtures, there are around 4% proteins with abundance=0 while the
proportion becomes 7% ∼ 8% for the remaining four data sets generated from real samples.
Table 2:

The eﬀect of shrinkage. The percentage of proteins with abundance=0 is deﬁned as the quotient between the

number of proteins with abundance=0 and the number of total candidate proteins.
Number of total candidate proteins
Number of proteins with abundance=0
Percentage of proteins with abundance=0

18 mixtures

Sigma49

Yeast

DME

HumanMD

HumanEKC

49

105

1285

907

414

669

2

4

91

66

34

50

4.1%

3.8%

7.1%

7.3%

8.2%

7.5%

After obtaining the protein abundance, we use an EM algorithm to convert the abundance score into
a well-calibrated probability. Alternatively, we can just normalize the protein abundance by dividing the
maximum of all calculated protein abundance values. The second strategy also gives us a protein score
between 0 and 1 and keeps the holistic distribution of the original protein abundance unchanged. Figure 5
shows the reason why we adopt the more complex probability estimation approach. In this ﬁgure, the
distributions of new scores generated from these two transformation methods are depicted. It is clearly
visible that the probability estimation method is capable of generating a score distribution that is more
close to the uniform distribution than the simple normalization method. This means that the probability
estimation method allows for distinction between diﬀerent proteins on a ﬁne level.

10

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

18 mixtures

18 mixtures

18 mixtures

20

20

20

15

15

15

10

10

10

5

PQ−1

5

PQ−2

NPQ−1

5

0
0.2

40

40

30

30

20

20

0

0.2

0.4

0

Yeast

Sigma49

0.2

PQ−2

10

NPQ−2

NPQ−1

0

0.4

PQ−1

10

NPQ−3

0
0

Sigma49

Sigma49

PQ−3

NPQ−2

11

0

0

0.4

0

Yeast

0.2

0.4

0

0.2

0.4

DME

Yeast

400
1000

40

600

1000

PQ−1

300

NPQ−1

30
400
200

20
PQ−3

10

500

100

NPQ−3

PQ−2

NPQ−1

0
0.2

0.4

DME

0.02

0.04

200

0
0.02

0

0.04

HumanMD

0.02

0.04

HumanMD

300

300

200

200

200

100

0

0.04

0.02

0.04

600

400

400

400

0.02

0.04

PQ−3
NPQ−3
0

0

0.02

0.04

0

0.02

0.04

PQ−3

PQ−2

NPQ−1
0.01

NPQ−2

200

200

0

NPQ−1

HumanEKC

HumanEKC

PQ−1

100
PQ−2

0
0

600

200

100
PQ−1

0
0

HumanEKC

0.005

0.02

300

600

0

0

HumanMD

NPQ−3

400

100

0

0.04

PQ−3

NPQ−2

200

0.02

DME
600

PQ−2

0

0
0

200

NPQ−3

0
0

300

PQ−3

NPQ−2

0
0

500

PQ−1

NPQ−3

NPQ−2
0

0
0

0.005

0.01

0

0.005

0.01

Figure 4: The comparison of identiﬁcation performance between the generalized spectral counting methods
(PQ-1, PQ-2, PQ-3) and the traditional spectral counting methods (NPQ-1, NPQ-2, NPQ-3). The y-axis
is the number of true positives and x-axis is the corresponding q-value (the minimum FDR to report these
proteins). The abbreviations for diﬀerent methods are the same as those in Figure 3.

11

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

18 mixtures

Sigma49

1

Yeast

1

0.8

1

NS

NS

PE

PE

0.8

NS

Score

Score

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0
10

20

30

40

0

50

0

20

Index

40

60
Index

80

100

0

120

0

0.6
Score

Score

Score
0.4

0.4

0.4

0.2

0.2

0.2

800

1200

PE

0.8

0.6

600

1000

NS

PE

0.8

0.6

400
Index

800

NS

PE

0

600
Index

HumanEKC

NS
0.8

400

1

1

200

200

HumanMD

DME
1

0

PE

0.8

0.6

Score

0.6

0

12

0

0
0

100

200
Index

300

400

0

200

400
Index

600

800

Figure 5: The comparison of the score distribution between normalized score (NS) and probability estimation
(PE) when the protein abundance value is generated with the LP model. The scores of all the identiﬁed
proteins are sorted by descending order.

12

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

13

4. Discussions
There have been already more than 20 protein inference algorithms in the literature, whose details are
summarized in several reviews (Huang et al., 2012; Li and Radivojac, 2012; Claassen, 2012). Here we only
discuss two inference methods that are most closely related with our work.
Based on the observation that peptides belonging to the same protein will show a good correlation with
respect to their quantiﬁcation patterns, Lukasse and America (2014) used the correlation of these patterns
to validate peptide to protein matches. BagReg (Zhao et al., 2015) is a learning-based method for protein
inference, which built a classiﬁcation model based on several features such as the number of matched spectra
for each protein. Overall, both methods utilized the quantiﬁcation information in their algorithms rather
than modeled the protein inference problem as a protein quantiﬁcation problem.
The correct assignment of shared peptides to their parent proteins is one of most challenging problems
in protein inference. However, it is generally very diﬃcult to fulﬁll this task since the information included
in the peptide-protein bipartite graph is insuﬃcient for distinguishing correct peptide-protein matches from
incorrect ones. Yang et al. (2013) mathematically investigated the ambiguity that will be induced by the
uncertainty on the assignment of shared peptides. They derived a lower bound and an upper bound on
the protein existence probability. Roughly speaking, all statistical protein inference methods will deliver a
probability value between the lower bound and the upper bound. This partially explains why no methods
can always perform the best in our experiments since all these methods cannot completely resolve the shared
peptide assignment problem. In other words, all existing methods have already reached their theoretical
limitation in protein inference if no supplementary data are provided for facilitating the inference. Therefore,
it is unlikely that one can further improve the identiﬁcation performance by only digging more on the
mathematical formulation of the protein inference problem based on standard input data.
In fact, many researchers have already realized the aforementioned problem and begun to seek solutions by including supplementary information in the protein inference process. That is, in addition to
the standard input data, supplementary data and information such as the single-stage MS data (He et al.,
2010, 2011), peptide detectabilities (Li et al., 2009b; Huang et al., 2013) and protein-protein interactions
(Ramakrishnan et al., 2009a; Li et al., 2009a) are utilized in the protein inference model as well. The use
of extra information from other data sources may overcome the limitation of currently available protein
inference algorithms.
5. Conclusions
Protein inference problem can be regarded as a special protein quantiﬁcation problem. In this paper,
we investigate the feasibility of solving the protein inference problem with existing protein quantiﬁcation
methods in the context of label-free proteomics. The experimental results show that such a new angle
enables us to obtain better identiﬁcation performance even with some simple quantiﬁcation approaches.
We have tested three protein quantiﬁcation methods for solving the protein inference problem. These
three methods can achieve good performance but none of them are consistently the best method on all the
data sets. Thus, it is still necessary to develop better algorithms. In the future work, we plan to try more
quantiﬁcation methods to check if we can further improve the identiﬁcation performance.
Acknowledgements
This work was partially supported by the Natural Science Foundation of China under Grant No. 61572094
and the Fundamental Research Funds for the Central Universities of China (DUT14QY07).
References
Bishop, C. M., 1995. Neural Networks for Pattern Recognition. Oxford University Press, USA.

13

Z.He et al. / Computational Biology and Chemistry 00 (2015) 1–14

14

Brunner, E., Ahrens, C. H., Mohanty, S., Baetschmann, H., Loevenich, S., Potthast, F., Deutsch, E. W., Panse, C., de Lichtenberg, U., Rinner, O., Lee, H., Pedrioli, P. G. A., Malmstrom, J., Koehler, K., Schrimpf, S., Krijgsveld, J., Kregenow,
F., Heck, A. J. R., Hafen, E., Schlapbach, R., Aebersold, R., 2007. A high-quality catalog of the drosophila melanogaster
proteome. Nature Biotechnology 25 (5), 576–583.
Choi, H., Fermin, D., Nesvizhskii, A. I., 2008. Signiﬁcance analysis of spectral count data in label-free shotgun proteomics.
Molecular & Cellular Proteomics 7 (12), 2373–2385.
Claassen, M., 2012. Inference and validation of protein identiﬁcations. Molecular & Cellular Proteomics 11 (11), 1097–1104.
Craig, R., Beavis, R. C., 2004. Tandem: matching proteins with tandem mass spectra. Bioinformatics 20 (9), 1466–1467.
Dost, B., Bandeira, N., Li, X., Shen, Z., Briggs, S., Bafna, V., 2012. Accurate mass spectrometry based protein quantiﬁcation
via shared peptides. Journal of Computational Biology 19 (4), 337–348.
Gao, J., Tan, P.-N., 2006. Converting output scores from outlier detection algorithms into probability estimates. In: IEEE
International Conference on Data Mining. Hong Kong, China, pp. 212–221.
He, Z., Yang, C., Yang et al, C., 2010. Optimization-based peptide mass ﬁngerprinting for protein mixture identiﬁcation.
Journal of Computational Biology 17 (3), 221–235.
He, Z., Yang, C., Yu, W., 2011. A partial set covering model for protein mixture identiﬁcation using mass spectrometry data.
IEEE/ACM Transactions on Computational Biology and Bioinformatics 8 (2), 368–380.
Huang, T., Gong, H., Yang, C., He, Z., 2013. Proteinlasso: A lasso regression approach to protein inference problem in shotgun
proteomics. Computational Biology and Chemistry 43, 46–54.
Huang, T., He, Z., 2012. A linear programming model for protein inference problem in shotgun proteomics. Bioinformatics
28 (22), 2956–2962.
Huang, T., Wang, J., Yu, W., He, Z., 2012. Protein inference: A review. Brieﬁngs in Bioinformatics 13 (5), 586–614.
Keller, A., Nesvizhskii, A. I., Kolker, E., Aebersold, R., 2002. Empirical statistical model to estimate the accuracy of peptide
identiﬁcations made by MS/MS and database search. Analytical Chemistry 74 (20), 5383–5392.
Kim, D. H., 2012. Deconvolution of PPI networks: Approximation algorithms and optimization techniques. Ph.D. thesis, McGill
University.
Klimek, J., Eddes, J. S., Hohmann, L., 2008. The Standard Protein Mix Database: A diverse data set to assist in the production
of improved peptide and protein identiﬁcation software tools. Journal of Proteome Research 7 (1), 96–103.
Li, J., Zimmerman, L. J., Park, B.-H., Tabb, D. L., Liebler, D. C., Zhang, B., 2009a. Network-assisted protein identiﬁcation
and data interpretation in shotgun proteomics. Molecular Systems Biology 5, 303.
Li, Y. F., Arnold, R. J., Li, Y., Radivojac, P., Sheng, Q., Tang, H., 2009b. A Bayesian approach to protein inference problem
in shotgun proteomics. Journal of Computational Biology 16 (8), 1–11.
Li, Y. F., Radivojac, P., 2012. Computational approaches to protein inference in shotgun proteomics. BMC Bioinformatics
13 (Suppl 16), S4.
Lukasse, P. N., America, A. H., 2014. Protein inference using peptide quantiﬁcation patterns. Journal of proteome research
13 (7), 3191–3199.
Lundgren, D. H., Hwang, S.-I., Wu, L., Han, D. K., 2010. Role of spectral counting in quantitative proteomics. Expert Review
of Proteomics 7 (1), 39–53.
Neilson, K. A., Ali, N. A., Muralidharan, S., Mirzaei, M., Mariani, M., Assadourian, G., Lee, A., van Sluyter, S. C., Haynes,
P. A., 2011. Less label, more free: Approaches in label-free quantitative mass spectrometry. Proteomics 11 (4), 535–553.
Nesvizhskii, A. I., Keller, A., Kolker, E., Aebersold, R., 2003. A statistical model for identifying proteins by tandem mass
spectrometry. Analytical Chemistry 75 (17), 4646–4658.
Nesvizhskii, A. I., Vitek, O., Aebersold, R., 2007. Analysis and validation of proteomic data generated by tandem mass
spectrometry. Nature Methods 4 (10), 787–797.
Nikolov, M., Schmidt, C., Urlaub, H., 2012. Quantitative mass spectrometry-based proteomics: an overview. Methods in
Molecular Biology 893, 85–100.
Perkins, D. N., J.Pappin, D., M.Creasy, D., Cottrell, J. S., 1999. Probability-based protein identiﬁcation by searching sequence
databases using mass spectrometry data. Electrophoresis 20 (18), 3551–3567.
Platt, J. C., 2000. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In:
Advances in Large Margin Classiﬁers. MIT Press, pp. 61–74.
Ramakrishnan, S. R., Vogel, C., Kwon, T., Penalva, L. O., Marcotte, E. M., Miranker, D. P., 2009a. Mining gene functional
networks to improve mass-spectrometry based protein identiﬁcation. Bioinformatics 25 (22), 2955–2961.
Ramakrishnan, S. R., Vogel, C., Prince, J. T., Wang, R., Li, Z., Penalva, L. O., Myers, M., Marcotte, E. M., Miranker, D. P.,
2009b. Integrating shotgun proteomics and mRNA expression data to improve protein identiﬁcation. Bioinformatics 25 (11),
1397–1403.
Tabb, D. L., Fernando, C. G., Chambers, M. C., 2007. Myrimatch: highly accurate tandem mass spectral peptide identiﬁcation
by multivariate hypergeometric analysis. Journal of Proteome Research 6 (2), 654–661.
Tang, H., Arnold, R. J., Alves et al, P., 2006. A computational approach toward label-free protein quantiﬁcation using predicted
peptide detectability. Bioinformatics 22 (14), 481–488.
Teng, B., Huang, T., He, Z., 2014. Decoy-free protein-level false discovery rate estimation. Bioinformatics 30 (5), 675–681.
Yang, C., He, Z., Yu, W., 2013. A combinatorial perspective of the protein inference problem. IEEE/ACM Transactions on
Computational Biology and Bioinformatics 10 (6), 1542–1547.
Zhang, Y., Wen, Z., Washburn, M. P., Florens, L., 2010. Reﬁnements to label free proteome quantitation: How to deal with
peptides shared by multiple proteins. Molecular & Cellular Proteomics 82 (6), 2272–2281.
Zhao, C., Liu, D., Teng, B., He, Z., 2015. BagReg: Protein inference through machine learning. Computational biology and
chemistry 57, 12–20.

14

View publication stats

Received July 27, 2016, accepted August 10, 2016, date of publication September 7, 2016, date of current version November 18, 2016.
Digital Object Identifier 10.1109/ACCESS.2016.2605759

Interactive Reference Region Based
Multi-Objective Evolutionary Algorithm
Through Decomposition
RUOCHEN LIU, (Member, IEEE), RUINAN WANG, WEN FENG, JUNJUN HUANG,
AND LICHENG JIAO, (Senior Member, IEEE)
Key Laboratory of Intelligent Perception and Image Understanding Key of Ministry of Education, International Research Center for Intelligent Perception and
Computation, Xidian University, Xi’an 710071, China

Corresponding author: R. Liu (ruochenliu@xidian.edu.cn)
This work was supported in part by the National Natural Science Foundation of China under Grant 61373111, Grant 61272279,
Grant 61672405, and Grant 61203303, in part by the Fundamental Research Funds for the Central Universities under
Grant K50511020014, Grant K5051302084, Grant K50510020011, Grant JB150227 and JBG160229,
and in part by the Provincial Natural Science Foundation of Shaanxi of China under Grant 2014JM8321.

ABSTRACT Many evolutionary multi-objective optimization (EMOs) methodologies have been proposed
and shown a great potential in approximating the entire Pareto front. While in real world, what decision
makers (DMs) want is one or several solutions to satisfy their requirements. It has become a hot problem
that dynamically using preference information provided by DMs during the optimization process guides
the search of EMO algorithms. An interactive reference region-based evolutionary algorithm through
decomposition is proposed, denoted as RR-MOEA/D in this paper, which focuses the search on the desire of
DMs to save computational resources. MOEA/D, as a well-known multi-objective optimization algorithm,
is used as a basic framework here. In MOEA/D, by dealing with the sub-problems in the preference region
and ignoring uninterested ones, the solutions obtained can converge to the regions which the DM prefers
on the Pareto front and the computational complexity can be saved to a great extent. At each interaction, a
humanized and simple interactive condition is adopted so that the reference region can be changed in a very
intuitive way if the DM is unsatisfied the results in the interactive process. A rapid interaction is designed
and a set of rough solutions can be obtained quickly whenever the preference information is changed. The
proposed algorithm is tested on several benchmark problems and the experimental results show that the
proposed algorithm can take full use of preference information and successfully converge to the reference
region due to its reasonable and simple interaction mechanism.
INDEX TERMS Interactive preference, multi-objective optimization problem, reference region selection
mechanism.

I. INTRODUCTION

Evolutionary multi-objective optimization algorithms (EMOs)
have been widely applied in many engineering areas. Multiple objectives with conflicts make it impossible to get a
single optimal solution which can optimize all the objectives
at the same time. As a result, the optimal solutions obtained
by EMOs are trade-offs, known as Pareto optimal solutions,
which are characterized by that some improvement in one
objective must lead to deterioration in at least one other
objective.
Without incorporating preference information, an approximation to the entire Pareto front (PF) is obtained by most

VOLUME 4, 2016

EMOs. However, in many practical applications, what a
decision maker (DM) requires may be a single solution or
a set of solutions in a preferred region instead of entire
PF. It is very time-consuming and unnecessary to obtain
the whole PF which DMs are not interested in. Meanwhile,
many preference information based EMOs have been proposed to select the most desirable solutions on the Paretooptimal front, and those can be mainly divided into three categories: prior, interactive and posterior. The apriori approaches
require the DM provides his/her preference information in
advance. Often this preference information is used as a variant of selection mechanism to substitute or compose the

2169-3536 
 2016 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

7331

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

tradition one. A disadvantage of these apriori approaches lies
in that it is very difficult and confused for a DM to express
his/her preference without related knowledge of available
solutions or structure of the search space. On the other
hand, for those posterior approaches, the DMs can choose
those preferred solutions on the basis of the convergence
and spread of solutions. Disadvantages and criticisms for the
posterior methods include high computational costs and time
consumption, and it will become more difficult when dealing with many-objective problems (more than three objectives). The interactive, as one of the preference methods,
is a new trend. It can be described as a DM dynamically
guides the searching process by providing their reference
information in an interactive way until he/she is satisfied with
the outcome. The interactive methods allow DMs to guide
searching and modify their preferences at various stages
during the optimization process. Moreover, ignoring those
regions with little interest and applying reference information as guidance can effectively reduce the computational
costs.
Multi-objective evolutionary algorithm based on decomposition (MOEA/D) [1] is selected as a basic MOEA framework in this paper. MOEA/D decomposes a multi-objective
problem (MOP) into multiple single-objective subproblems
in which each subproblem is a different aggregation of all
the objectives. From this viewpoint, the aggregation can
be described as different preference of objectives, in other
words, different subproblems are generated under different preference condition. Therefore, it is a convenient way
to find the regions where the DM is interested by solving the corresponding subproblems according to the DM’s
preferences.
In this paper, we propose an interactive reference region
based multi-objective evolutionary algorithm through decomposition (denoted as RR-MOEA/D), which focus the search
on the desire of DMs to save computational resources. Once
the preference is given by a DM at interactive stage, the proposed algorithm reduces the computational cost by ignoring
the regions the DM is not interested in and only optimizing the
subproblems the DM prefers. Whereas during optimization
process, weight vectors of subproblems preferred is adapted
dynamically to avoid the solutions we obtain flying away
from the preferred regions. At each interaction, our algorithm
provides a set of current solutions to the DM and the reference
region can be changed in a very intuitive way if the DM
is unsatisfied. Therefore, a rapid interaction is designed in
this paper. A set of rough solutions can be obtained quickly
whenever the preference information change.
The major advantages of the proposed RR-MOEA/D algorithm are enumerated as follows:
1) Reduce the amount of calculation and save the computing resources by focusing the search on the preferred regions;
2) It needs not to calculate any achievement scalarizing
functions (ASF) [2] used in other preference mechanisms;
3) A simple and intuitive way to set the reference information the DM prefers;
7332

4) A fast way to get rough solutions in the reference regions
can avoid wasting time in searching Pareto-optimal solutions
in regions the DM is not preferred;
5) Giving uncertain and inconsistent human decisions as
the DM does in real life are allowed and changing the preference information in the interaction stage is permitted.
The remainder of this paper is structured as follows.
Section 2 introduces the related background. The proposed
algorithm is presented in Section 3 in details. Section 4
presents experimental results and validates the performance
of the proposed algorithm. Section 5 concludes the paper.
II. RELATED BACKGROUND
A. MULTI-OBJECTIVE OPTIMIZATION

Generally, a minimized multi-objective
problem (MOP) can be defined as follows:

optimization

min F(x) = (f1 (x), f2 (x), . . . , fm (x))T
s.t. x ∈ 

(1)

where x = (x1 , x2 , . . . , xn ) is the decision vector, n is the
dimensionality of the decision space, and  is the feasible
region in the decision space. F(x) is a set of objective function
consisting of m objective functions.
Very often, since the objectives in Eq. (1) conflict each
other, it is impossible to get a single optimal solution which
optimizes all the objectives at the same time. As a result, the
optimal solutions we obtained are trade-offs, known as Pareto
optimal solutions. Some important related terms are stated as
follows:
Definition 1 (Pareto-Dominance): Let xa , xb ∈ , xa is
said to Pareto dominate xb , denoted as xa  xb if and only if:
(
∀i = 1, 2, . . . m, fi (xa ) ≤ fi (xb )
(2)
∃j = 1, 2, . . . m, fj (xa ) < fj (xb )
Definition 2 (Pareto-Optimal): A point is Pareto optimal to
Eq. (1) if:
¬∃x ∈  : x  x∗

(3)

The Pareto-optimal set (PS) consists of all the Paretooptimal solutions and the set of all the Pareto optimal objective vectors is called Pareto front (PF) [3].
B. MULTI-OBJECTIVE OPTIMIZATION BASED
ON DECOMPOSITION

Decomposition is a basic strategy in traditional multiobjective optimization. Relying on a decomposition strategy such as the weighted-sum method [3], the Tchebycheff
approach [3] or the boundary intersection approach [4], [5],
a multi-objective problem can be converted into a number of single-objective problems. In 2007, Zhang et al. [1]
proposed a decomposition algorithm based evolutionary
multi-objective algorithm using the Tchebycheff as the
decomposition strategy. Since we use the Tchebycheff
approach in our study, a brief introduction of the Tchebycheff
approach is given in the following.
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

The Tchebycheff method plays an important role not only
in converting a multi-objective problem into a number of
single-objective problems [3] but also in locally approximating the underlying preference utility function [6]. Each subproblem is obtained by using the following equation [1], [3]:
	

 
minimize gte (x λ, z∗ ) = max λi fi (x) − z∗ 
1≤i≤m

subject to x ∈ 

(4)

Where, for i ∈ {1, 2, . . . , m}, z∗i is the ideal point, i.e.,
z∗i = min {fi (x) |x ∈  }. For any weight vector λ, the optimal
solutions of Eq. (4) corresponds to a Pareto-optimal solution
of Eq. (1). All the optimal solutions obtained can approximate
to the entire PF with proper weight vectors.
As shown in Eq. (4), each subproblem can be translated
as a different aggregation of all the objectives and different
aggregation could be constructed as the different preference
for objectives. By setting proper weights, solutions desired by
DMs can be obtained by solving those related subproblems
in the preference regions. Therefore, associating preference
method with decomposition is a very natural thought. As the
most common decomposition based multi-objective evolutionary algorithms, MOEA/D [1] has shown great potential.
Here is a brief introduction of MOEA/D and its variants.
MOEA/D uses the Tchebycheff method as a decomposition method and converts a multi-objective problem into a set
of subproblems and optimizes all the subproblems simultaneously in a single run. Different subproblems in MOEA/D have
different weight vectors. For each weight vector, a neighborhood is defined as a set of its several closest weight vectors.
Offspring of each subproblem is generated by using some
genetic operators and its parents are randomly selected from
its neighborhood. The solution of each subproblem is the best
solution found so far in the neighborhood. That is to say, each
subproblem optimized by its neighboring subproblems only
obtain its optimal solution in evolving process, which makes
that MOEA/D has such advantages as low computational
complexity and fast convergence rate. MOEA/D-DE [7] is
an improved version of MOEA/D, which has three improvements: 1) using a differential evolution (DE) [8] operator
instead of SBX operator [9] in updating process; 2) setting a
probability that parent solutions are selected from the neighborhood; 3) setting a upper limit number of solutions in
neighborhood replaced by each child solution. All these three
improvements aim at maintaining the population diversity.
Since our algorithm works only in reference regions and
has a small population size, keeping diversity is an important task. So, the proposed algorithm is mainly based on
MOEA/D-DE in fact. As another new version of MOEA/D,
MOEA/D-M2M [10] converts a multi-objective problem into
several simple multi-objective problems. For each subproblem, any Pareto based EMOs can be used. MOEA/DD [11]
suggests a hybrid paradigm which combines decomposition
and dominance together. Most recently, an inverse model
based MOEA (IM-MOEA) [12] is proposed, which uses a
set of reference vectors to partition the objective space into a
VOLUME 4, 2016

number of subspaces and then the inverse models which maps
the objective vectors into the decision vectors are built in
each subspace to sample new solutions. More recently, a reference vector guide evolution algorithm for many-objective
optimization (REVA) [13] is proposed, in which reference
vectors is used to decompose the original multi-objective
optimization problem into a number of single-objective subproblems. Additionally, a scalarization approach, termed the
angle penalized distance, is adopted to balance convergence
and diversity of the solutions in the high-dimensional objective space. NSGA-III [14] combines a decomposition strategy
in MOEA/D [1] with non-dominated sorting approach from
NSGA-II [15], and has been able to successfully find a wellconverged and well-diversified set of solutions for manyobjective problems.
C. PREFERENCE INFORMATION BASED EMO

The goal of evolutionary search is to help that the DM
selects the solution which best matches his/her preferences
within a representative set of Pareto optimal solutions. Various preference-based EMOs have been proposed in the last
decade. Two surveys were provided in this topic [16], [17].
EMOs are categorized apriori, interactive, or aposteriori algorithms according to the treatment of preference. An apriori
approach requires the preference information provided by
the DM before the optimization stage and the evolutionary
search is guided by the preference information. On the other
hand, a posteriori approach firstly needs to approximate to
the whole Pareto front, and then the DM selects their preferred solutions. With the character of the user involving in
the optimization process, the interactive approaches offer a
chance for the user to know about the structure of the search
space and express his /her authentic preference correctly,
and computational costs is reduced largely by progressively
focusing on the most relevant areas and ignoring regions of
little interest. We will give a simple review about the three
approaches in the following section.
Some typical apriori methods are listed here:
Wierzbicki [18] proposed a reference point based optimization of classical multi-criterion decision making algorithm
which combines three approaches in a single multi-objective
optimization framework and enables the DMs to refine the
problem definition and to reduce the size of the objective
space iteratively. Jaszkiewicz and Slowinski [19] proposed
a light beam search procedure which connects the reference point with tools of multi-attribute decision analysis.
This algorithm tries to overcome the inconvenience when
the DM chooses the candidates for the best compromise.
Deb et al. [20] hybridized the reference direction strategy
with NSGA-II, in which, a reference direction can be determined by setting a starting point and a reference point,
and reference solutions along the reference direction on PF
can be obtained. Deb and Kumar [21] combined a reference
point based reference information with NSGA-II in order
to find a set of solutions instead of a single optimal solution and used a modified crowding distance to control the
7333

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

density of solutions near reference points. Thiele et al. [22]
proposed a preference based evolutionary approach which
directly uses reference point information in fitness evaluation.
Recently, Molina et al. [23] proposed another definition of
Pareto dominance called g-dominance, in which, solutions
satisfy all aspiration levels and solutions fulfilling none of the
aspiration levels are preferred over those solutions satisfying
some aspiration levels. All these algorithms mentioned above
belong to the apriori method.
For the posteriori approach, in fact, many famous EMOs
like MOEA/D [1], NSGA-II [15], SPEA-II [24], NNIA [25],
and so on belong to it. After those algorithms obtain a
set of compromised solutions which are closest to the true
Pareto front and the DMs pick out the solutions they are
interested in.
An interactive process aims to integrate the preference
information provided by DMs into algorithm in an interactive
way during the optimizing process. That is to say, some tradeoff factors expressing the desires of the DMs are added into
the optimization process to guide the latter search. According
to [26], Tanino et al. [27] suggested the earliest attempt to
incorporate the preference information expressed iteratively
by the DM to evaluate solutions. Fonseca and Fleming [28]
allowed the user to specify aspiration levels in form of a
reference point, and used it to modify the EMO’s rank scheme
in order to focus the search. Kita et al. [29] interleaved generations of a Pareto ranking based evolutionary algorithm
with the rank of the solutions provided by a DM. Todd and
Sen [30] also tried to learn the user’s utility function, instead
of only considering linear weightings of objectives, they
used the preference information provided by the DM to train
an artificial neural network, which is then used to evaluate
solutions in the evolutionary algorithm. Parmee et al. [31]
described the concept of an interactive evolutionary design
system which is related to multi-objective information gathering and subsequent design space redefinition. Instead of
using linear programming to derive a weighting of the objectives most compatible with the pairwise comparisons as in
[32], Phelps and Kö ksalan [33] used two evolutionary algorithms, one is to find the solutions, and another is to determine the most compatible ranking. Kamalian et al. [34] suggested using a posteriori evolutionary multi-objective optimization followed by an interactive evolutionary algorithm.
Reference [35] allowed the user to modify the Pareto ranking computed automatically by changing the rank of some
of the solutions. Thiele et al. [36] also used DM’s preferences interactively expressed in the form of reference points.
Avigad and Moshaiov [37] presented a new interactive concept based multi-objective evolutionary algorithm to handle
Pareto-directed IC-MOPs. Said et al. [38] described an interactive run of r-NSGA-II algorithm in which is changed for
a certain number of generations freely determined by the
DM, and if the running result the DMs are dissatisfied, they
can change the reference point. Deb et al. [39] presented a
progressively interactive EMO approach which applied the
constructed value function to search more preferred solutions.
7334

And an interactive evolutionary algorithm can contribute
to designing elegant object-oriented software which was
discussed by Simons and Parmee [40]. Sun et al. [41] proposed a new surrogate-assisted interactive genetic algorithm
to exploit the uncertainty of subjective fitness evaluations
both in training the surrogates and in managing surrogates.
Ruiz et al. [42] developed an interactive WASF-GA which
is based on a preference-based evolutionary multiobjective
optimization algorithm called WASF-GA [43], it required
the DM to provide preference information at each iteration
simply as a reference point and then the desired number of
solutions is generated to represent the region of interest of the
Pareto optimal front associated to the reference point given.
Among the interactive algorithms, those algorithms based
on decomposition strategy are very few. We just found one
references on this topic. iMOEA/D [44] is an interactive
version of the MOEA/D algorithm [1], in which the preferred
solutions are presented to the decision maker at intermediate generations, then, the searching process is guided to
the neighbor regions of the preferred solution. It is obvious
that both iMOEA/D and the proposed algorithm are based
on MOEA/D although there are some little differences like
initiation and updating of population. The biggest differences
between the two algorithms lie in the way of determining preferred region and interactive method. In iMOEA/D,
a generic polynomial utility function [45] is used to represent
the human DM. During the stage of interactive, P current
solutions are randomly presented to DM for choosing their
favorite one; and the selected solution becomes the center
of preferred weight region in the following optimization process. The detailed descriptions of about the way of determining preferred region and interactive method in the proposed
algorithm is presented in Section 3.
III. THE PROPOSED APPROACH

In this section, the proposed reference region based multiobjective evolutionary algorithm through decomposition, is
presented in details. If the final aim is to choose and implement a solution, then the goal of applying a multi-objective
optimization method is to find a single, most preferred, final
solution. However, in some cases, it may be preferable to find
a set of solutions instead of one. This may be particularly true
in case of robustness considerations when some aspects of
uncertainty, imprecision or inconsistency in data or model are
to be taken into account [26]. For most of preference based
multi-objectives evolution algorithms, the preference information is treated as a variant of special selection mechanism
to substitute or compose the traditional selection process [46],
and the whole population converges close to the reference
point or reference direction [44]. MOEA/D-M2M [10] converts a multi-objective problem into several simple multiobjective problems based on PF or PS shapes and searches
and optimizes each simple multi-objective problem respectively. Inspired by all these, we consider that a reference
region can be treated as a simple multi-objective problem
and thus the search is focused on solving this problem only.
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

As a result, a reference region based preference mechanism is
designed in which the population evolves only in this region
where the DM desires. Provided the reference information, it
is possible to find the preference regions and then initialize
the individuals in the preference regions and search in the
region until approximating properly to the Pareto front. At the
initial stage, MOEA/D [1] is used to generate the initial population. When the preference information is given, namely
one or several reference regions are determined and those new
population are formed in the preference regions corresponding to some subproblems. Whereas in the evolution process,
the subproblems are updated by using MOEA/D-DE until
the DM is satisfied. Here it should be emphasized that each
individual solution belongs to different corresponding subproblem and each subproblem corresponds to certain weight.
Thus collective and dynamic adapting the weight vectors in
the course of evolution can ensure not only the distribution
range but also the convergence in the preference region. At
the interactive stage, a humanized and simple interactive
condition is adopted in this paper. A set of rough solutions are
presented to DM when the interactive condition is met. If the
DM is satisfied with the present solutions, the process would
be terminated; otherwise, the decision maker could change
his/her prefer information, a similar process is started until
the DM obtain the satisfied solutions. Therefore, the DM can
control the preference information in a very intuitive and fast
way.
A. PREFERENCE REGION

The first challenge in the proposed approach is how to determine the preference region. As we know, interaction involves
the DM in the process of optimization for dynamically offering preference information. And the DM dynamically guides
the algorithm to search in the preference region in the interaction processing. Therefore, an effective method should be
found for determining the preference region when the interaction happens.
As a basic algorithm framework in this paper, MOEA/D [1]
is characterized by 1) each subproblem is a independent
single-objective optimization problem and the solutions of
all the subproblems form the entire population; 2) all subproblems in MOEA/D spread uniformly in objective space
and each subproblem and its neighbor subproblems form a
preferred region. That is to say, finding the optimal subproblem which is nearest to the reference direction or the
reference point and these individuals in its neighbor can form
a preference region. In this paper, those reference directions
are selected as the preferred information which are marked
by owning more intuitive and better linked with weight vectors. As a result, the preferred regions can be determined
by two factors in the proposed algorithm: one is the searching direction which is determined by an aspiration and a
reservation point provided by DMs. another is radius which
determines the range of reference region. So, once a DM
offers a searching direction, we can find the optimal weight
vector close to the searching direction and obtain the neighbor
VOLUME 4, 2016

FIGURE 1. The illustration of determining the preference region.

of the optimal subproblem by computing the Euclidean distances between the optimal weight vector and other weight
vectors. By setting the radius and reasonably initializing the
weight vectors in this range, we can get subproblems in the
preference region. Fig. 1 shows this process visually. Here,
we can see that the vector determined by aspiration point and
reservation point is just what we define as the reference direction. The radius determines the scope of reference region, and
then we can find the optimal vector and other subproblems in
this reference region.
Finding an optimal weight vector close to the searching
direction is not trivial. In this paper, we employ the basic
MOEA/D [1] to generate the initial population. To determine
optimal weight vector, we connect each individual to the
aspiration point so as to form a vector and compute the Cosine
value between this vector and the searching direction:
cos(a, r) =

a•r
|a| × |r|

(5)

where vector a is obtained by connecting individual x with the
aspiration point and vector r is the searching direction. The
bigger the Cosine values, the closer to the searching direction
the individual is. We can select the subproblem’s weight
vector with the biggest Cosine value as the optimal weight
vector and re-initialize all the subproblems in the reference
region. Algorithm 1 shows the process of the initializing the
weights and finding initial population in the reference region
in details.
The human decision is characterized by uncertainty and
inconsistency [45]. In the proposed algorithm, the reference
information and the size of population are given by the decision maker. It is a common thing that the parameters the
DM inputs might not well match with each other. Therefore, we should take account of various circumstances. If
the number of subproblems (population size) is too small to
cover entire preferred region, the subproblem in the region
most close to the reference direction will become a priority
(Algorithm 1: line 8-9). In this paper we set the parameter
penalty to 1.5 to control the range of small population (it
can also be changed in some specific application). Otherwise,
7335

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

Algorithm 1 REFREGION
Interation Parameters:
num_v: the number of reference directions
PF: initial population
N: the size of population close to each reference direction
start: the vector of num_v aspiration points
fend: the vector of num_v reservation points
radius: determine the size of reference region
Output:
D0 : initial population in reference region
W0 : initial weight vectors in reference region
1. ref _dir ← fend − start
2. for i = 1, 2, . . . num_v do
3. best_weight i ← max(cos _value(PF, ref _dir))
4. range_weight i ← max(euc_dis(all_weight,
best_weight) < radius)
5. L = length(range_weight i )
6. for j = 1, 2, ....L
7. calculate the interval distance dj between the jth
and the (j + 1)th weight
8.
if L > penalty∗ N (:, i)
9.
Wi ← init_weight(min_euc_dis(range_weighti ,
N (:, i)))
10. else if L < penalty∗ N (:, i) & & L > N (:, i)
11.
Wi = range_weighti
12. while L > N (:, i)
13. find out the weight w with minimum interval
distance d
14.
Wi ← delete w from Wi
15.
L– –
16. end while
17. else
18.
Wi = range_weighti
19. while L < N (:, i)
20. find out the weight w with maximum interval
distance d
21. Wi ← insert a middle weight between w and the
following weight of w in Wi
22. L++
23. end while
24. end if
25. end for
26. Di ← max(cos_value(PF, ref _dir(:, i)), N (:, i))
27. D0 ← (D1 , . . . Dnum_v ), W0 ← (W1 , . . . Wnum_v )
28. end for

the subproblems could be evenly distributed in the preference
region to satisfy requirements (Algorithm 1: line 10-23). Consequently, the algorithm can keep diversity and robustness
as well.
B. Interactive Condition

As one of the preference methods, the interactive ones allow a
DM to dynamically guide the searching process until he/she is
satisfied with the resulting outcome. Whereas a key problem
7336

is when the current solutions are presented to the DM, in
many interactive algorithms, the process of interacting with a
DM will happen periodically [45], [46]. A long time interval
between two interactions may lead to a waste of time in
searching the region the DM does not favor. On the contrary,
it is tiresome to bother the DM excessively and it is also
a tiring work to find several preferred points from a large
number of random points in the beginning. Determining when
the algorithm interacts with the DM is important. We use an
indicator to determine whether the rough result needs to be
presented to the DM which is defined as follows:
δ=

N
i
1 X fti − ft−1
i
N
ft−1

(6)

i=1

Where N is the population size in the reference region, and
fti denotes the objective value of individual i at the tth generation. In this case, the function f is one with the minimum value of the individual i. Further, δ means the average
improvement degree of function value that the current individuals are compared with their parent individuals. The precision of the solutions presented to the DM can be controlled
by setting the value of δ. In other words, the parameter δ is
determined by the DM, when the condition of derta ≤ δ is
satisfied, the interaction happens.
Let us make some detailed illustration about a few aspects
of the Algorithm 1.
Step1: The reference direction(s) must be given by the DM,
in which aspiration points are start and reservation points are
fend.
Step3 to Step4: The two steps are designed to calculate
the best weight corresponding to the reference direction and
estimate the range weight around the preference direction
simultaneously.
Step5 to Step 26 used to determine the initial population
and the initial weight according to the relationship between
the preference region weight.
C. RR-MOEA/D

The overall procedure of the proposed interactive preference region based multi-objective evolutionary algorithm through decomposition (RR-MOEA/D) is described
in Algorithm 2.
Here, we give a more detailed explanation about several
key steps.
Initialization: In order to get uniform distribution of the
initial solutions and to make sure any region the DM prefers
has enough corresponding individuals, the initial population
is produced by limited iterations of MOEA/D instead of
random initialization. In this way, we can easily obtain the
individual and its corresponding weights in the preference
region, and shift quickly into new initial population when
DM’s searching regions are changed.
Updating Strategy: The population in the reference region
is updated by using MOEA/D until the termination criteria
is satisfied. It is worth noticing that the weights need to
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

Algorithm 2 RR-MOEA/D
Step1. Initialization: run pre-determined generations
MOEA/D to generate initial population;
Step2. Parameter input: ask the DM to input his/her
preference information;
Step3. Determining reference region: determine the reference region using the preference information
according to Algorithm 1;
Step4. Updating: update the population in the preference
region by MOEA/D-DE until the stopping
criterion is meet;
Step5. Interaction: If the DM is not satisfied with reference information, go to Step2 to reset the reference
information; If the DM wants to get better results in this
reference region, go to Step4; otherwise, output the results.

be updated every generation. Updating weights needs two
indicators: update direction and update step. The update
direction is determined by the subproblem farthest from the
reference direction to the subproblem nearest from the reference direction. The update step is defined as a unit length
(Algorithm 3: line 3). It is evitable that the population may
flies away from the preferred region by dynamically adjusting
the weights. The details of updating weights are stated in
Algorithm 3.
Interaction: At this stage, the current population is presented to the DM. If the DM is unsatisfied with the result,
the preference information can be reset. A new population
is produced according to the new desire of the DM and
a mixed population combining the current population with
the initial population. The DM can make tentative decisions
before he/she finds his/her desired regions. When the reference region is determined, the DM can reset the precision of
the solutions as the stopping criteria to get the final output
results.
Algorithm 3 Updating Weights
Interation Parameters:
num_v: the number of reference directions
PF: initial population
N: the size of population close to each reference direction
start: the vector of num_v aspiration points
fend: the vector of num_v reservation points
radius: determine the size of reference region
Output:
Wt : weights after updating
1. ref _dir ← fend − start
2. for i = 1, 2, . . . , num_v do
3.
stepi ← 2 × radiusi /N i
4.
best i ← max(cos(Dt , ref _dir(:, i)))
5.
worst i ← min(cos(Dt , ref _dir(:,i)))
6.
update_directioni ← best i − worst i
7.
Wt ← update_weight(update_directioni , stepi , Wt )
8. end for
VOLUME 4, 2016

In the algorithm, the size of the neighborhood is decided by
the size of population in the reference region and the range of
reference region. If a small population or a quite small range
which the DM is interested in, all the subproblems are treated
as a neighborhood. In this case, the algorithm is simplified
due to avoiding calculating the Euclidean distances between
all of the weight vectors.
Given the preference direction(s), RR-MOEA/D can control its searching regions in which the DM is interested.
By optimizing the subproblems in those regions, those solutions can be gotten which are converged to the preferred
regions.
IV. EXPERIMENTS AND RESULTS

In order to validate the performance of the proposed algorithm, we apply it to solving two 2-objective ZDT problems
and two 3-objective DTLZ problems respectively. All experiments were carried out on a personal computer with AMD
A8-5550M APU with Radeon (tm) HD Graphics (2.10GHz)
and 4 GB of RAM, Running Windows7.
A. BENCHMARK FUNCTIONS AND
EXPERIMENTAL SETTING

In our experiments, we use ZDT2and ZDT4 [47] and threeobjective benchmark problems, DTLZ1 and DTLZ2 [21], to
test our algorithm, which are presented in Table 1.
The parameters in RR-MOEA/D are set as follows: the
initial population size of MOEA/D-DE is set to 100 and 300
for the two objective and three objective problems respectively and MOEA/D-DE is executed 50 generations before
inputting the preference information. The algorithm will stop
after 250 generations. The size of population in the preference region N and the parameter of controlling interactive
condition δ depend on the decision maker. Here, in order to
illustrate experimental result, the size of radius is set as 0.005
and N is set to 30 and 60 for the two objective and three
objective problems and the penalty is set as 1.5. The impact
of the parameters provided by the DM on the performance of
the algorithm is analyzed in Section 4.3.
B. EXPERIMENTAL RESULT AND ANALYSIS
1) RESULTS OF TWO-OBJECTIVE TEST PROBLEMS

ZDT2 is a two-objective problem with 30variables and its
PF is non-convex as shown in Fig. 2. Four cases based on
different preference information are analyzed in order to
verify the performance of RR-MOEA/D.
Case 1: The reference direction is defined by two points
(0,0) and (1,1), we suppose the DM prefers the middle region
of the PF in this case.
Case 2: The reference direction is between (0,0) and (1,0),
which means one objective gets more preferences.
Case 3: The same as Case 2 and the aspiration point and
reservation point is (0,0) and (0,1), respectively.
Case 4: To show the performance of algorithm when the
DM sets more than one reference directions, we consider two
7337

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

TABLE 1. Test functions used in the study.

reference directions: D1 is determined by (0, 0) and (1, 2);
and D2 is determined by (0, 0) and (2, 1).
Fig. 2 shows that RR-MOEA/D has a good performance
for all these four cases. For different kind of reference region
required by the DM, the algorithm can obtain a set of reasonable and uniformly distributed solutions in the preferred
region. Also, the algorithm can deal with more than one
reference region simultaneously, and all preferred solutions
can be obtained in each reference region.
Another two objective problem is ZDT4, which is hard due
to its many local optimal solutions and its convex PF with
10 variables. Similar with ZDT2 problem, we also adopt the
four cases above to test ZDT4 problem (see in Fig. 3).
It is obvious in Fig. 3 that the proposed algorithm can deal
with the convex problem as well and get the ideal solutions in
any region the DM prefers. Uniform distribution and convergence in the preference region on ZDT4, a hard problem of
7338

ZDT test problems, show a good performance of the proposed
algorithm.
2) RESULTS OF THREE-OBJECTIVE TEST PROBLEMS

DTLZ1 is a 10-variable, three-objective
problem. Its efficient
P
frontier is known and is given by i∈[1,m] fi = 0.5, as shown
in Fig. 4. Fig. 4(a) shows the obtained solutions with one
reference direction when the reference direction is given by
two points (0, 0, 0) and (0.5, 0.5, 0.5). To show the performance with more than one preference region, Fig. 4(b)
shows the obtained solutions with two reference direction
regions. Two reference directions are: {(0,0,0), (0.5,0.5,0.3)},
{(0,0,0), (0.1,0.1,0.5)}.
DTLZ2 has a convex
PF with 10 variables and its efficient
P
frontier satisfies i∈[1,m] fi2 = 1. Similar with DTLZ1 problem, we use one and two reference directions to show the performance. The first reference direction is from point (0,0,0)
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 2. Preferred solutions obtained by RR-MOEA/D on ZDT2: (a) case 1; (b) case 2; (c) case 3; (d) case 4.

FIGURE 3. Preferred solutions obtained by RR-MOEA/D on ZDT4 : (a) case 1; (b) case 2; (c) case 3; (d) case 4.

to point (0.7, 0.8, 0.5). The two reference directions in
Fig. 5(b) are {(0,0,0), (1,1,0.8)}, {(0,0,0), (0.4,0.4,0.1)},
respectively.
VOLUME 4, 2016

The final results of three-objective problem are shown in
Fig. 4 and Fig. 5. Obviously our algorithm also can well
converge to the true PF within the referred region the decision
7339

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 4. Preferred solutions obtained by RR-MOEA/D on DTLZ1 (a) one reference region; (b) two reference regions.

FIGURE 5. Preferred solutions obtained by RR-MOEA/D on DTLZ2 (a) one reference region; (b) two reference regions.

maker favors. It needs to point out here that the shape of
obtained solutions is determined by the way of initializing
the weights in MOEA/D.
C. DISCUSSION
1) THE DISCUSSION OF INTERACTION PERFORMANCE

As an interactive method, the interaction performance of
the algorithm is vitally importance. A simple interaction is
presented in the proposed algorithm. It is not an easy task

FIGURE 6. Initial solutions obtained in the interactive stage of ZDT2
((0, 0) →(1,1)).
7340

that an exact preference region is required at the beginning,
so the algorithm allows the decision maker to pick up a
reference direction as a trial. When the interaction condition
is met, these current solutions which are rough solutions in
the reference region are presented to the DM. If the DM is
interested in the reference region, our algorithm will go on
with finding the solutions approximate to PF in the reference
region. Otherwise, searching direction can be changed by the
DM. Here is an example to illustrate that we have plenty
ways to change the searching directions. Supposed we set
an initial searching direction from (0, 0) to (1, 1) and the
size of preference region radius = 0.005 for ZDT2 problem,
a set of rough solutions in the reference region are presented
to the DM (see in Fig. 6) when the interaction condition is
satisfied (Eq. 6), in which, δ = 0.001. If the DM is unsatisfied
with the reference region he/she set earlier, they change their
preference information as follows:
Case 1: The DM changes the initial searching region to
a near preference region(e.g. from (0,0) → (1,1) to (0,0) →
(1,1.1)).
Case 2: The reference region is changed far from the
initial reference region by the DM(e.g. from (0,0) →(1,1) to
(0,0) → (0.3,0.8)).
Case 3: A new preference region is added (e.g. from
(0,0) → (1,1) to (0,0) →(0.5,1) and (0,0) → (1,1)).
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 7. Solutions obtained of ZDT2 after different interactions: (a) case 1; (b) case 2; (c) case 3; (d) case 4.
TABLE 2. Mean generations and its standard variance with different δ.

Case 4: The DM changes an initial searching region to
two different searching regions (e.g. from (0,0) → (1,1) to
(0,0) → (0.5,1) and (0,0) → (1,0.5)).

VOLUME 4, 2016

The final results of these four cases are shown in
Fig. 7. If the DM is not satisfied with the searching
region he/she set before, our algorithm also can fast capture

7341

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 8. Solutions obtained on ZDT2 with the radius=0.5: (a) N = 20; (b) N = 50; (c) N = 100.

FIGURE 9. Solutions obtained on ZDT2 with the radius=0.05: (a) N = 20; (b) N = 50; (c) N = 100.

the new preference region and well converge to the PF
within the new preferred region to meet various user
requirements.
7342

A fast way to obtain a number of rough solutions in the
reference region to interact with the DM is another distinguishing characteristic. As mentioned earlier, δ is provided
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

FIGURE 10. Solutions obtained on ZDT2 with the radius=0.005: (a) N = 20; (b) N = 50; (c) N = 100.

FIGURE 11. Solutions obtained on ZDT2 problem with the radius=0.005: (a) penalty=1.0; (b) penalty=1.5;
(c) Penalty=2.0; (d) penalty=3.0.

by the decision maker to control the accuracy of the solution.
According to the interactive condition used in our study,
rapidly getting most solutions in DM’s preferred region is presented to the DM. In Table 2, the mean generations needed are
VOLUME 4, 2016

shown when the interaction happens for ZDT test problems
based on 10 independent runs (δ is set as 0.1, 0.01, 0.001). It
is easy to see that it just needs no more than 20 generations for
the proposed algorithm to find a rough set of solution when
7343

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

δ = 0.001, even when δ = 0.1, the proposed algorithm can
find some rough solutions in no more than 100 generations.
It is obvious that RR-MOEAD is a fast algorithm to meet
the interactive condition and this means the algorithm can
quickly get the rough solutions in the region preferred by the
DM.
2) THE DISCUSSION OF PARAMETERS
(RADIUS, N AND PENALTY)

At interactive stage, RR-MOEA/D allows the DM to input
different preference information to satisfy different preference requirements. In order to analyze the impact of the input
parameters like the population size N , the size of preferred
region i.e. the radius and the penalty on the performance of
the algorithm, we set the population size as 20, 50 and 100
respectively when the radius size is fixed as 0.5, 0.05, and
0.005 for 2-objective ZDT2 problem and perform a series of
experiments. The aspiration point and the reservation point
are set to (0, 0) and (1, 1).
Further, in order to determine the effect of the parameter
penalty on the experiment’s results. We set the penalty as
1.0, 1.5, 2.0 and 3.0 respectively when the value of radius
is fixed as 0.005 and the population is 30 for ZDT2, the
aspiration point and the reservation point are set to (0,0)
and (1,1).
From Fig. 8 Fig. 10, we can see that the larger the radius
is, it is better to set the radius as a large value. When DMs
provide a small value for the parameter radius, it is not
necessary to set a large value for the parameter N . in this
paper, we set radius = 0.005 and N is set to 30 and 60 for
the two objective and three objective problems.
Fig. 11 shows the solutions obtained with different value of
the penalty. We can conclude that if the value of the penalty is
set around 1.5, the entire reference region would be well converged to near the reference direction. Meanwhile, the penalty
being fixed as a greater value, the converged front will deviate
the reference direction. So, in the experimental study, we set
it as1.5. From here, we can draw a conclusion that the more
detailed information the DM provide, the more accurate solutions our algorithm can find. For example, if the smaller value
of radius and penalty are given, the front obtained would be
well converged to the reference direction. No matter what
reference region the DM needs, our algorithm can get a wellspaced front well converged to the region most preferred.
V. CONCLUSIONS

In this paper, an interactive reference region based evolutionary algorithm through decomposition named RR-MOEA/D
is proposed. At the beginning of the proposed algorithm,
MOEA/D is used to generate initial solutions before the
preference desired by the decision maker are given. We can
initialize the suitable subproblems in the reference regions
according to the DM’s requirements. By dealing with the
subproblems in the reference region, the solutions obtained
can converge to the regions of the Pareto front which the DM
prefers.
7344

During the stage of interaction, a simple interactive condition is adopted in this paper. The proposed algorithm can get
a set of rough solutions in one or several reference regions
quickly and avoid wasting time in searching the region which
the DM is not interested in. Solutions obtained are presented
to DM when the interactive condition is met. If the DM is
unsatisfied with these solutions, the new preference information can be reset. Therefore, the decision maker can control
the preference information in a very intuitive way. In addition, RR-MOEA/D allows the DM to reset different kinds of
preference information and it can quickly obtain a set rough
solution according to new preference information.
It should be emphasized that all of subproblems in the
preference region are determined by weight vectors in the
evolving stage. It is evitable that the population converges
to the region on PF where the user is undesired by dynamically adjusting the corresponding weights. If the number of
sub problems (population size) is so small to cover entire
preference region, the subproblems in the region most close
to the reference direction will become a priority. In other
words, RR-MOEA/D allows the DM to give uncertain and
inconsistent human decision as he or she does in real life.
Both MOPs with two and three objectives are adopted
to test the performance of RR-MOEA/D. According to the
results of experimental study, the proposed algorithm can successfully converge to different regions the DM most prefers.
It is obvious that our algorithm can realize a fast and simple
interactive process and copes with different preference information including uncertain and inconsistent human decision.
In our future work, we will try to find a better way to construct
the preference regions. As we known, there are still many
aspects of interaction strategy to worth further exploration.
Dealing with interaction information better and more humanized is our another work.
REFERENCES
[1] Q. Zhang and H. Li, ‘‘MOEA/D: A multiobjective evolutionary algorithm
based on decomposition,’’ IEEE Trans. Evol. Comput., vol. 11, no. 6,
pp. 712–731, Dec. 2007.
[2] K. Sindhya, A. B. Ruiz, and K. Miettinen, ‘‘A preference based interactive evolutionary algorithm for multi-objective optimization: PIE,’’ in
Evolutionary Multi-Criterion Optimization (Lecture Notes in Computer
Science), vol. 6576. Berlin, Germany: Springer-Verlag, 2011, pp. 212–225.
[3] K. Miettinen, Nonlinear Multiobjective Optimization. Norwell, MA, USA:
Kluwer, 1999.
[4] I. Das and J. E. Dennis, ‘‘Normal-boundary intersection: A new method
for generating the Pareto surface in nonlinear multicriteria optimization
problems,’’ SIAM J. Optim., vol. 8, no. 3, pp. 631–657, 1998.
[5] A. Messac, A. Ismail-Yahaya, and C. A. Mattson, ‘‘The normalized normal
constraint method for generating the Pareto frontier,’’ Struct. Multidisciplinary Optim., vol. 25, no. 2, pp. 86–98, Jul. 2003.
[6] R. F. Dell and M. H. Karwan, ‘‘An interactive MCDM weight space
reduction method utilizing a Tchebycheff utility function,’’ Naval Res.
Logistics, vol. 37, no. 2, pp. 263–277, Apr. 1990.
[7] H. Li and Q. Zhang, ‘‘Multiobjective optimization problems with complicated Pareto sets, MOEA/D and NSGA-II,’’ IEEE Trans. Evol. Comput.,
vol. 13, no. 2, pp. 284–302, Apr. 2009.
[8] R. Storn and K. Price, ‘‘Differential evolution—A simple and efficient
heuristic for global optimization over continuous spaces,’’ J. Glob. Optim.,
vol. 11, no. 4, pp. 341–359, 1997.
[9] K. Deb and R. B. Agrawal, ‘‘Simulated binary crossover for continuous
search space,’’ Complex Syst., vol. 9, no. 3, pp. 115–148, 1995.
VOLUME 4, 2016

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

[10] H.-L. Liu, F. Gu, and Q. Zhang, ‘‘Decomposition of a multiobjective optimization problem into a number of simple multiobjective subproblems,’’
IEEE Trans. Evol. Comput., vol. 18, no. 3, pp. 450–455, Jun. 2013.
[11] K. Li, K. Deb, Q. Zhang, and S. Kwong, ‘‘An evolutionary many-objective
optimization algorithm based on dominance and decomposition,’’ IEEE
Trans. Evol. Comput., vol. 19, no. 5, pp. 694–716, Oct. 2015.
[12] R. Cheng, Y. Jin, K. Narukawa, and B. Sendhoff, ‘‘A multiobjective
evolutionary algorithm using Gaussian process-based inverse modeling,’’
IEEE Trans. Evol. Comput., vol. 19, no. 6, pp. 838–856, Dec. 2015.
[13] R. Cheng, Y. Jin, M. Olhofer, and B. Sendhoff, ‘‘A reference vector guided
evolutionary algorithm for many-objective optimization,’’ IEEE Trans.
Evol. Comput., vol. 20, no. 5, pp. 773–791, 2016.
[14] K. Deb and H. Jain, ‘‘An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part I:
Solving problems with box constraints,’’ IEEE Trans. Evol. Comput.,
vol. 18, no. 4, pp. 577–601, Aug. 2014.
[15] K. Deb, S. Agarwal, A. Pratap, and T. Meyarivan, ‘‘A fast and elitist
multiobjective genetic algorithm: NSGA-II,’’ IEEE Trans. Evol. Comput.,
vol. 6, no. 2, pp. 182–197, Apr. 2002.
[16] C. A. C. Coello, ‘‘Handling preferences in evolutionary multiobjective
optimization: A survey,’’ in Proc. Congr. Evol. Comput., Jul. 2000,
pp. 30–37.
[17] L. Rachmawati and D. Srinivasan, ‘‘Preference incorporation in multiobjective evolutionary algorithms: A survey,’’ in Proc. IEEE Congr. Evol.
Comput. (CEC), Jul. 2006, pp. 3385–3391.
[18] A. P. Wierzbicki, ‘‘The use of reference objectives in multiobjective optimization,’’ in Multiple Criteria Decision Making Theory and Application.
Heidelberg, Germany: Springer, 1980, pp. 468–486.
[19] A. Jaszkiewicz and R. Slowiński, ‘‘The ‘light beam search’ approach—An
overview of methodology applications,’’ Eur. Jour. Oper. Res., vol. 113,
no. 2, pp. 300–314, Mar. 1999.
[20] K. Deb and J. Sundar, ‘‘Reference point based multi-objective optimization
using evolutionary algorithms,’’ Int. J. Comput. Intell. Res., vol. 2, no. 3,
pp. 273–286, 2006.
[21] K. Deb and A. Kumar, ‘‘Interactive evolutionary multi-objective optimization and decision-making using reference direction method,’’ in Proc.
9th Annu. Conf. Genet. Evol. Comput. (GECCO), 2007, pp. 781–788.
[22] L. Thiele, K. Miettinen, P. J. Korhonen, and J. Molina, ‘‘A preference-based
evolutionary algorithm for multi-objective optimization,’’ Evol. Comput.,
vol. 17, no. 3, pp. 411–436, 2009.
[23] J. Molina, L. V. Santana, A. G. Hernández-Diaz, C. A. C. Coello, and
R. Caballero, ‘‘g-dominance: Reference point based dominance for multiobjective metaheuristics,’’ Eur. J. Oper. Res., vol. 197, no. 2, pp. 685–692,
Sep. 2009.
[24] E. Zitzler, M. Laumanns, and L. Thiele, ‘‘SPEA2: Improving
the strength Pareto evolutionary algorithm,’’ in Evolutionary
Methods for Design, Optimization and Control With Applications
to Industrial Problems K. C. G. Koglou, D. T. Tsahalis, J. Périaux,
and T. Fogarty, Eds. Heidelberg, Germany: Springer, 2001,
pp. 95–100.
[25] M. G. Gong, L. Jiao, H. Du, and L. Bo, ‘‘Multiobjective immune algorithm
with nondominated neighbor-based selection,’’ Evol. Comput., vol. 16,
no. 2, pp. 225–255, 2008.
[26] J. Branke, K. Deb, and K. Miettinen, Multiobjective Optimization: Interactive and Evolutionary Approaches, vol. LNCS 5252. Berlin, Germany:
Springer-Verlag, 2008.
[27] T. Tanino, M. Tanaka, and C. Hojo, ‘‘An interactive multicriteria decision
making method by using a genetic algorithm,’’ Manage. Sci., vol. 22 no. 6,
pp. 652–663, 1976.
[28] C. M. Fonseca and P. J. Fleming, ‘‘Multiobjective optimization and multiple constraint handling with evolutionary algorithms. I. A unified formulation,’’ IEEE Trans. Syst., Man, Cybern. A, Syst., Humans, vol. 28, no. 1,
pp. 26–37, Jan. 1998.
[29] M. Shibuya, H. Kita, and S. Kobayashi, ‘‘Integration of multi-objective
and interactive genetic algorithms and its application to animation design,’’
in Proc. IEEE Int. Conf. Syst., Man, Cybern. IEEE SMC, Oct. 1999,
pp. 646–651.
[30] D. S. Todd and P. Sen, ‘‘Directed multiple objective search of design spaces
using Genetic Algorithms and neural networks,’’ in Proc. Genetic Evol.
Comput. Conf., 1999, pp. 1738–1743.
[31] I. C. Parmee, D. Cvetković, A. Watson, and C. Bonham, ‘‘Multiobjective
satisfaction within an interactive evolutionary design environment,’’ Evol.
Comput., vol. 8, no. 2, pp. 197–222, Jun. 2000.
VOLUME 4, 2016

[32] H. J. C. Barbosa and A. M. S. Barreto, ‘‘An interactive genetic algorithm with co-evolution of weights for multiobjective problems,’’ in Proc.
Genetic Evol. Comput. Conf., 2001, pp. 203–210.
[33] S. Phelps and M. Köksalan, ‘‘An interactive evolutionary metaheuristic for
multiobjective combinatorial optimization,’’ Manage. Sci., vol. 49, no. 12,
pp. 1726–1738, 2003.
[34] R. Kamalian, H. Takagi, and A. M. Agogino, ‘‘Optimized design of MEMS
by evolutionary multi-objective optimization with interactive evolutionary computation,’’ in Proc. GECCO, Berlin, Germany, vol. 3103. 2004,
pp. 1030–1041.
[35] R. Kamalian, Y. Zhang, H. Takagi, and A. M. Agogino, ‘Evolutionary synthesis of micromachines using supervisory multiobjective interactive evolutionary computation,’’ in Proc. ICMLC, LNAI3930, 2006,
pp. 428–437.
[36] L. Thiele, K. Miettinen, P. J. Korhonen, and J. Molina, ‘‘A preference-based
interactive evolutionary algorithm for multiobjective optimization,’’ Dept.
Inf. Technol. Electr. Eng., Aalto Univ. School Bus., Helsinki, Finland,
Tech. Rep., 2007.
[37] G. Avigad and A. Moshaiov, ‘‘Interactive evolutionary multiobjective
search and optimization of set-based concepts,’’ IEEE Trans. Syst., Man,
Cybern. B, Cybern., vol. 39, no. 4, pp. 1013–1027, Aug. 2009.
[38] L. B. Said, S. Bechikh, and K. Ghedira, ‘‘The r-dominance: A new dominance relation for interactive evolutionary multicriteria decision making,’’
IEEE Trans. Evol. Comput., vol. 14, no. 5, pp. 801–818, Oct. 2010.
[39] K. Deb, A. Sinha, P. J. Korhonen, and J. Wallenius, ‘‘An interactive
evolutionary multiobjective optimization method based on progressively
approximated value functions,’’ IEEE Trans. Evol. Comput., vol. 14, no. 5,
pp. 723–739, Oct. 2010.
[40] C. L. Simons and I. C. Parmee, ‘‘Elegant object-oriented software design
via interactive, evolutionary computation,’’ IEEE Trans. Syst., Man,
Cybern. C, Appl. Rev., vol. 42, no. 6, pp. 1797–1805, Nov. 2012.
[41] X. Sun, D. Gong, Y. Jin, and S. Chen, ‘‘ A new surrogate-assisted interactive genetic algorithm with weighted semisupervised learning,’’ IEEE
Trans. Cybern., vol. 43, no. 2, pp. 685–698, Apr. 2013.
[42] Evolutionary Multi-Criterion Optimization (Lecture Notes in Computer
Science), vol. 9019. Switzerland, Springer International Publishing, 2015,
pp. 249–263.
[43] A. B. Ruiz, R. Saborido, and M. Luque, ‘‘A preference-based evolutionary
algorithm for multiobjective optimization: The weighting achievement
scalarizing function genetic algorithm,’’ J. Global Optim., vol. 62, no. 1,
pp. 101–129, May 2015.
[44] M. G. Gong, F. Liu, W. Zhang, L. Jiao, and Q. Zhang, ‘‘Interactive
MOEA/D for multi-objective decision making,’’ in Proc. 13th Annu. Conf.
Genetic Evol. Comput., 2011, pp. 721–728.
[45] R. Battiti and A. Passerini, ‘‘Brain–computer evolutionary multiobjective
optimization: A genetic algorithm adapting to the decision maker,’’ IEEE
Trans. Evol. Comput., vol. 14, no. 5, pp. 671–687, Oct. 2010.
[46] C. Fonseca and P. J. Fleming, ‘‘Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization,’’ In Proc.
5th Int. Conf. Genet. Algorithms, 1993, pp. 416–423.
[47] E. Zitzler, L. Thiele, and K. Deb, ‘‘Comparison of multiobjective evolutionary algorithms: Empirical results,’’ Evol. Comput., vol. 8, no. 2,
pp. 173–195, Jun. 2000.

RUOCHEN LIU (M’07) received the Ph.D. degree
from Xidian University, Xian, China, in 2005. She
is currently an Associate Professor with the Intelligent Information Processing Innovative Research
Team of the Ministry of Education of China, Xidian University. Her research interests are broadly
in the area of computational intelligence. Her areas
of special interest include artificial immune systems, evolutionary computation, data mining, and
optimization.
7345

R. Liu et al.: Interactive Reference Region-Based Multi-Objective Evolutionary Algorithm

7346

RUINAN WANG received the B.S. degree from
the Hebei University of Science and Technology, Hebei, China. He is currently pursuing the
M.S. degree from Xidian University. His current
research focuses on multi-objective optimization
and data mining.

JUNJUN HUANG received the B.S. degree from
the Huaiyin Institute of Technology, Huai’an,
China. He is currently pursuing the M.S. degree
with Xidian University. His current research
focuses on multi-objective optimization and image
processing.

WEN FENG received the B.S. degree from Xidian
University, Xi’an, China, where she is currently
pursuing the M.S. degree. Her current research
focuses on multi-objective optimization.

LICHENG JIAO (SM’89) received the Ph.D.
degree from Xian Jiaotong University, Xian,
China, in 1990. He is currently a Professor and the
Dean of the Electronic Engineering School, Xidian
University, China. His current research focuses on
intelligent information processing.

VOLUME 4, 2016

Applied Mathematics Letters 61 (2016) 2634

Contents lists available at ScienceDirect

Applied Mathematics Letters
www.elsevier.com/locate/aml

A note on MongeAmp` ere KellerSegel equation
Hui Huang a,b, , Jian-Guo Liu b
a b

Department of Mathematical Sciences, Tsinghua University, Beijing, 100084, People's Republic of China Departments of Physics and Mathematics, Duke University, Durham, NC 27708, USA

article

info

abstract
This note studies the MongeAmp` ere KellerSegel equation in a periodic domain Td (d  2), a fully nonlinear modification of the KellerSegel equation where the MongeAmp` ere equation det(I + 2 v ) = u + 1 substitutes for the usual Poisson equation v = u. The existence of global weak solutions  is obtained for this modified  equation. Moreover, we prove the regularity in L 0, T ; L  W 1,1+ (Td ) for some  > 0.  2016 Elsevier Ltd. All rights reserved.

Article history: Received 8 April 2016 Received in revised form 5 May 2016 Accepted 5 May 2016 Available online 13 May 2016 Keywords: Chemotaxis Polar factorization Convex potential Brenier map Global existence

1. Introduction KellerSegel (KS) model was firstly presented in 1970 to describe the chemotaxis of cellular slime molds [1]. The original model was considered in 2-dimension,  2  t u = u +   (uv ), x  R , t > 0, (1) v = u(t, x),  u(0, x) = u (x). 0 In the context of biological aggregation, u(t, x) represents the bacteria density, and v (t, x) represents the chemical substance concentration. In this note, we study the MongeAmp` ere KellerSegel (MAKS) model in a periodic domain Td = Rd /Zd (d  2):  d  t u = u +   (uv ), x  T , t > 0, (2) det(I + 2 v ) = u + 1,  u(0, x) = u (x). 0
 Corresponding author at: Department of Mathematical Sciences, Tsinghua University, Beijing, 100084, People's Republic of China. E-mail addresses: huanghui12@mails.tsinghua.edu.cn (H. Huang), jliu@phy.duke.edu (J.-G. Liu).

http://dx.doi.org/10.1016/j.aml.2016.05.003 0893-9659/ 2016 Elsevier Ltd. All rights reserved.

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 2634

27

where I is the identity matrix. In the absence of u term in (2), this model was introduced by Brenier [2, (5.34), (5.36)] as a fully nonlinear version of popular models in chemotaxis theory, such as the celebrated KellerSegel model or similar models in astrophysics. We will prove the global existence of weak solutions to MAKS model (2) in a weak sense, which is made precise in Section 2. MongeAmp` ere KellerSegel system (2) is an approximation of the original KS system (1) in the following re-scaling. Let us recast the equation (2) by introducing the new unknowns:     1 t x t x u (t, x) = u , ; v  (t, x) = v , .      Then we have  u(t, x) = u (t, x);  v (t, x) = v  (t, x).

Moreover, these new unknowns should be governed by the following MAKS system  t u = u +   (u v  ), det(I +  2 v  ) = 1 + u . We formally linearize the determinant det(I +  2 v  ) around the identity matrix and obtain 1 + u = det(I +  2 v  ) = 1 +  v  + O( 2 ).

(3)

(4)

Then the MongeAmp` ere equation turns into the Poisson equation v  = u + O( ), from which, when we set O( ) = 0, we recognize the MAKS system (3) as the original KS system showed in (1). The density u in the original KS system (1) is driven by the gradient of Newtonian potential v = N  u, where N is the fundamental solution of Laplacian equation, and potential v has the superposition principle relation with u. Moreover, it has an important property: if 0  u  L (Td ), then v is log-Lipschitz continuous. However, for MAKS model (2), the Newtonian potential is replaced by a convex potential V [u] discovered by Brenier [3]. The advantage is that v = V [u] - x is globally convex and has uniform L bound if 0  u  L1 (Td ). But the convex potential will lose the superposition principle relation with the density. There are many mathematical models involved substituting the fully nonlinear MongeAmp` ere equation for the Poisson equation. For example, the semigeotrophic equations in meteorology have a long history. After suitable changes of variables, they can be reformulated as a coupled MongeAmp` ere/transport problem [4], which appear as a variant of the two-dimensional incompressible Euler equations in vorticity form, where the Poisson equation that relates to the stream function and the vorticity field is replaced by the MongeAmp` ere equation [47]. Moreover, in [8], Brenier and Loeper studied the VlasovMongeAmp` ere system, a fully non-linear version of the VlasovPoisson system. Similarly, Brenier [9], by substituting the MongeAmp` ere equation for the linear Poisson equation to model gravitation, he introduced a modified Zeldovich approximate model related to the early universe reconstruction problem. 2. The polar decomposition theorem The polar factorization of maps has been discovered by Brenier [3]. It was later extended to the general case of Riemannian manifolds by McCann in [10]. - - - Let us consider a mapping X : Rd  Rd such that for all  p  Zd , X ( +  p) = X + p . We use the d push-forward of Lebesgue measure of R by X , and it is denoted by u = X dx. Then u is a probability measure on Td and we have the following theorem: Theorem 1 (Theorem 1.2 [6]). Let X : Rd  Rd be described as above with u = X dx.

28

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 2634

 [u] such that V  [u] - x2 /2 is Zd -periodic (and 1. Up to a constant, there exists a unique convex function V d  thus V [u] - x is Z -periodic), and   C 0 (Td ),   [u](x))dx = (V
Td Td

 (x)du(x). (5)

 [u]. If u is Lebesgue integrable, then V [u] is a convex function 2. Let V [u] be the Legendre transform of V 2 d satisfying that V [u] - x /2 is Z -periodic (and thus V [u] - x is Zd -periodic), unique up to a constant, and   C (T ),
Td 0 d

 (V [u](x))du(x) = 

 (x)dx.
Td

(6)

Moreover we have the bound V [u] - xL (Td ) 

d/2.

Link with the MongeAmp` ere equation. We can interpret (5) as a weak version of the Monge Amp` ere equation  ) det 2 V  = 1, u(V and (6) can be seen as a weak version of another MongeAmp` ere equation det 2 V = u. (8) (7)

Moreover, we will also use the following result originally from [3]. The first one establishes the continuity of the polar decomposition. Theorem 2 (Theorem 2.6 [6]). Let un be a sequence of Lebesgue integrable positive measures on Td , such that   =V  [un ], Vn = V [un ] be as defined in Theorem 1. If for any   C 0 (Td ) for all n, Td dun  C and let V  n n can be chosen in such a way that V n converges such that dun converges to du, then the sequence V d 1, 1 d d  to V [u] uniformly on T and strongly in W (T ), and Vn converges to V [u] uniformly on T and strongly in W 1,1 (Td ). Theorem 1 allows us to recast MAKS equation (2) as t u = u +   (u(V [u + 1] - x)), u(0, x) = u0 (x). where V [u + 1] is as defined in Theorem 1. For simplicity, we denote V [u + 1] as V [u]. Remark 1. If u is continuous and satisfies 0  u  C1 , it has been proved in [11] that V [u](x) is log-Lipschitz continuous. The log-Lipschitz continuity usually ensures the uniqueness and stability in the Wasserstein distance. Moreover, according to [8, Theorem 4.4], if u  C  (Td ),   (0, 1), then V [u] is a classical solution of det 2 V [u] = u + 1. (10) x  Td , t > 0, (9a) (9b)

3. Existence of global weak solutions To begin this section, we give the following definition of the weak solution to the MAKS equation (9).

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 2634

29

Definition 1. Let initial data 0  u0  L1 (Td ). Then (u, V ) is a global weak solution to (9) if it satisfies for any T > 0:       1. u  L 0, T ; L1 (Td )  L2 0, T ; H 1 (Td ) and t u  L2 0, T ; H -1 (Td ) .    2.    Cc [0, T )  Td ,  T  T  ut dxdt = (u + u(V - x)  )dxdt -
0 Td 0 Td Td

u0 (0, x)dx,

where V is defined as in Theorem 1. The main result of this note is as follows: Theorem 3. Let initial data 0  u0  L2 (Td ). Then the MAKS system (2) admits a global non-negative weak   solution (u, V ) in t  [0, T ] for any T > 0. And the conservation of mass holds: Td u(t, x)dx = Td u0 (x)dx. Proof. We build a sequence of approximate solutions (u , V )>0 by regularization and let  goes to zero. To do the limiting process, the non-linear term will be treated with the help of Theorem 2. Step 1: Construction of a sequence of approximate solutions. We consider a mollifier  (x)    (Rd ) such that  (x)  0, Td  (x)dx = 1 and  (x) = -d  (x/). And we can define the mollification Cc  as   u0 := Td  (x - y )u0 (y )dy . Then we study solutions to the following approximate problem t u = u +   (u (V (x) - x)) , u,0 (x) =   u0 (x), V (x) =   V [u ]. x  Td , t > 0, (11a) (11b) (11c)

Since V given by (11c) is bounded in H k (Td ) for any k and  > 0, the estimate for Eq. (11a) for any fixed  > 0 is basically same as that for the heat equation. Hence, the solvability of the regularized problem (11) can be obtained by using the technique in Majda and Bertozzi [12, Section 3.2.2], where it proved the global existence of the solution to a regularization of the Euler and NavierStokes equation by using the Picard theorem and continuation property of ODEs on a Banach space. We omit the detail here. Step 2: Weak convergence of u and u . Multiplying Eq. (11a) by 2u and integrating over Td , we obtain  d 2 2 2 u 2 + 2 u 2 = - u (V - x)  u dx  u 2 + C u 2 (12) 2. dt d T  where we have used V - x  d/2. Hence for any T > 0, the following estimates hold u L (0,T ;L2 (Td ))  CT , u L2 (0,T ;L2 (Td ))  CT . (13)

According to the above estimates, there is a subsequence (still denote u ), such that as   0, the following weak convergence results hold      u -  u in L 0, T ; L2 (Td ) , u  u in L2 0, T ; L2 (Td ) . (14) Step 3: Strong convergence of V (t, ) a.e. t. We claim that for any p  [1, ), V (t, )  V (t, ) in Lp (Td ), a.e. t  [0, T ]. (15)

30

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 2634

Indeed, such strong convergence of V follows from Theorem 2 provided that we have for a.e. t  [0, T ],   (x)u (t, x)dx  (x)u(t, x)dx, (16)
Td Td

for any   C 0 (Td ). To verify (16), we need to prove that there is a subsequence (still denote u ) u  u in L2 (Td ) a.e. t  [0, T ], as   0. Indeed, it is easy to check that t u L2 (0,T ;H -1 (Td ))  CT , which leads to   u  u in L2 0, T ; L2 (Td ) , as   0, (17)

(18)

by using AubinLions lemma as H 1 (Td )  L2 (Td )  H -1 (Td ). Then (17) follows from (18), which completes the proof of (15). Step 4: Existence of a global weak solution. Next, we will show that (u, V [u]) is a weak solution to    (9). The weak formulation for u is that for any test function   Cc [0, T )  Td ,  T  T  u t dxdt = (u  + u (V - x)  )dxdt - u,0 (0, x)dx. (19)
0 Td 0 Td

Recall that (14), (15), (18) and V - x  d/2. Then by using the dominant convergence theorem, one concludes that by passing limit   0 in (19)  T  T  ut dxdt = (u + u(V - x)  )dxdt - u0 (0, x)dx. (20)
0 Td 0 Td Td



Td

We finished the proof of the existence of global weak solutions. Step 5: Positivity preserving. By using Lemma 7.6 in [13], if we define the negative part of the function u as u- := min{u, 0}, then one can easily prove that  d 2 2 u- 2 + 2  u  = - u- (V - x)  u- dx  u- 2 + C u- 2 (21) - 2 2 2. dt d T Applying Gronwall's inequality to d 2 u- 2 2  C u- 2 ; dt one has u- 2 2  0, which leads to u(t, x)  0. Step 6: Conservation of mass. Integrating (9a) over Td and using the fact that u, V - x are periodic, one has    d udx = udx +   (u(V - x))dx = 0. (23) dt Td Td Td Thus, we conclude that  u(t, x)dx =
Td Td

u0- 2 2 = 0,

(22)

 u0 (x)dx. (24)

4. Regularity in L (0, T ; L  W 1,1+ (Td )) Theorem 4. Let initial data 0  u0  L (Td ) and u0  L1+ (Td ) for some  > 0. Suppose (u, V ) be a weak solution to MAKS equation (9), then for any T > 0 and t  [0, T ],   u(t, x)  L 0, T ; L  W 1,1+ (Td ) . (25)

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 2634

31

Proof. First we will prove that u(, t)  C (T, d, A0 ), (26)

with A0 = max{1, u0 L1 (Td ) , u0 L (Td ) }. Multiplying (9a) with pup-1 , p  2 and integrating over Td , we have    p 2 p p 4(p - 1)  d  p p 2 (V - x)u dx = -2(p - 1) (V - x)u 2 u 2 dx up + u  = -(p - 1) dt p 2 Td Td  p    p p 2 2(p - 1)   2   (27)  C u  u 2   u 2  + C up p. p 2 2 2 Then the L bound can be obtained directly by the standard Moser iteration after getting (27). For example, one can check the paper by Alikakos [14], formula (3.20) and the computation afterwards. For completeness, we put these detail computation in Appendix. From Theorem 3, we know that u is positivity preserving and the conservation of mass hold: u  0; u(t, x)1 = u0 1 , for t  [0, T ]. (28)

By the construction of V in Theorem 1, one concludes that  2  1  det( V )  u + 1, V convex,  V - x2 /2 periodic. Recall the result in [15, P.16], for some  > 0 we have 2 V 1+  C (T, d, u ).

(29)

(30)

 |x+k|2 1 - 4t The heat semigroup operator et defined by et u := H (t, x)  u, where H (t, x) = (4t kZd e )d/2 is the periodic heat kernel. It follows immediately from Young's inequality for the convolution that et up  Ct- 2 ( q - p ) uq ,
1 1 d

et up  Ct- 2 - 2 ( q - p ) uq ,
1 1 1

d

(31)

for any 1  q  p  +, u  Lp (Td ) and all t > 0. Here C is constant dependent of p, q . By the fundamental solution representation of the heat equation, the solution to the MAKS equation can be represented as  t u = et u0 + e(t-s) (  (u(V - x)))ds, (32)
0

for any T > 0, 0 < t < T . By choosing q = p = 1 +  in (31), a simple computation leads to  t u1+  C u0 1+ + (t - s)-1/2   (u(V - x))1+ ds.
0

(33)

 From Theorem 1, we have that V - x  d/2 and moreover 2 V 1+  C (T, d, u ), then we conclude      (u(V - x))1+  d/2u1+ + C T, d, u , |Td | . (34) Hence we have  u1+  C1 + C2
0 t

(t - s)-1/2 u1+ ds.

(35)

32

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 2634

Applying a generalized Gronwall's inequality with weak singularities [16, Lemma 7.1.1], we have u1+  C (T, d, u0 1+ , u0  ) , which concludes the proof. Acknowledgments The authors would like to thank Yann Brenier for suggesting this problem to us. The work of Jian-Guo Liu is partially supported by KI-Net NSF RNMS grant No. 1107291 and NSF grant DMS 1514826. Hui Huang is partially supported by National Natural Science Foundation of China (Grant number: 41390452, 11271118). Appendix. The proof of L estimate in Theorem 4 (36)

Proof. Using Gronwall's inequality in (27), one concludes that
Ct p u(, t)p p  e u0 p  C (T, d, A0 ).

(A.1)

Define pk := 2k + 2 with k  0. For k = 0, p0 = 3, from (A.1) we have
0 u(, t)p p0  C (T, d, A0 ).

(A.2)

For k  1, take pk upk -1 as a test function in (9a), one has  2 pk 4(pk - 1)  d   k 2  - (p - 1) up (V - x)up = -  u  k pk k dx dt pk 2 Td  2   p   pk pk     k   -2Cpk u 2  + pk u 2  u 2  .
2 2 2

(A.3)

Now, we focus on estimating the term u 2  p     p  pk k    2   k u  u 2   u 2  2d
2 2

pk

d-2

2 u 2 2  p 1-    1+  p 1- pk pk k  k   2     2  , u  u 2   Sd u 2  u 
r 2 2 r
2d d-2

pk

(A.4)

with

pk 2 r

= pk-1 ,  =

1 1 r-2 d-2 1 - r 2d

, where we have used the Sobolev inequality u

 Sd u2 . The Young's

inequality tells that  2 pk d   q q2 pk k 2  + C ( )p 2 S up  - C  u  p pk k k d upk-1 , dt 2 where  = Cpk , q2 =
2 1-

(A.5)

 d + 2.

On the other hand,  p 2  21  p 2(1-1 ) pk k  k   2   2 2 1  k 2  up = u  S  u ,    u  pk d
2 2 r

(A.6)

where r is the same as before and 1 =

1 1 r-2 d- 2 1 - r 2d

. Similar to (A.5), we have (A.7)

 2 pk   2 1 2 pk 2 (1-1 ) k 2  + C ( )S up up , pk   u d k -1
2

where 2 =

1 1- 1 .

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 2634

33

Hence from (A.5) and (A.7), we deduce d q2 q2 2 1 2 pk pk k k up up pk - 1 . pk  -upk + C ( )pk Sd upk-1 + C ( )Sd dt Define
q2 C1 (pk ) := C ( )Sd ; 2 1 2 C2 (pk ) := C ( )Sd .

(A.8)

It is easy to know that C1 (pk ) and C2 (pk ) is uniformly bounded for any k  1. So, we let C (d) > 1 be a common upper bound of C1 (pk ) and C2 (pk ), we obtain the following inequality d q2 pk pk k up pk  -upk + C (d)pk upk-1 . dt Solving the inequality (A.9), we get
q2 pk t d+2 k(d+2) t k  k (et up 2 sup up pk )  C (d)pk upk-1 e  2C (d)4 pk-1 e , t0

(A.9)

(A.10)

where the last inequality used 1 < q2  d + 2.
p k -1 k Notice that u0 p , so we have pk  u0 1 u0  pk k max{u0 p pk , 1 }  A ,

(A.11)

where constant A > 1 is independent of k but depends on u0 1 and u0  . Let ak := 2C (d)4d+2 2k(d+2) > 1 and integrate (A.10), then one has   pk -t p k -t pk pk k up  a sup  u  (1 - e ) +  u  e  a max sup  u  , A . (A.12) k 0 k pk pk - 1 pk pk - 1
t0 t0

Taking the power

1 pk

to above inequality, then upk 
1/p ak k

 max sup upk-1 , A .
t0



(A.13)

After some iterative steps, we have upk 
1/p 1/pk-1 ak k ak-1 1/p    a1 1

 max sup up0 , A
t0



 (2C (d)4

d+2 1- 21 k

)

(2

k d+2 2- 2k1 -1 - 2k

)

 max sup up0 , A .
t0



(A.14)

 0 Recall up estimate is obtained by passing to the limit k   in (A.14). p0  C (T, d, A0 ), then the L

References
[1] E.F. Keller, L.A. Segel, Initiation of slime mold aggregation viewed as an instability, J. Theoret. Biol. 26 (3) (1970) 399415. http://dx.doi.org/10.1016/0022-5193(70)90092-5. [2] Y. Brenier, Hilbertian approaches to some non-linear conservation laws, Contemp. Math. 526 (2010) 1934. [3] Y. Brenier, Polar factorization and monotone rearrangement of vector-valued functions, Comm. Pure Appl. Math. 44 (4) (1991) 375417. http://dx.doi.org/10.1002/cpa.3160440402. [4] J. Benamou, Y. Brenier, Weak existence for the semigeostrophic equations formulated as a coupled MongeAmp` ere transport problem, SIAM J. Appl. Math. 58 (5) (1998) 14501461. http://dx.doi.org/10.1137/S0036139995294111. [5] M. Cullen, M. Feldman, Lagrangian solutions of semigeostrophic equations in physical space, SIAM J. Math. Anal. 37 (5) (2006) 13711395. http://dx.doi.org/10.1137/040615444. [6] G. Loeper, A fully nonlinear version of the incompressible Euler equations: the semigeostrophic system, SIAM J. Math. Anal. 38 (3) (2006) 795823. http://dx.doi.org/10.1137/050629070.

34

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 2634

[7] M. Lopes Filho, H.N. Lopes, Existence of a weak solution for the semigeostrophic equation with integrable initial data, in: Proceedings of the Royal Society of Edinburgh-A-Mathematics, Vol. 132, Cambridge Univ Press, 2002, pp. 329340. [8] Y. Brenier, G. Loeper, A geometric approximation to the Euler equations: the VlasovMongeAmp` ere system, Geom. Funct. Anal. GAFA 14 (6) (2004) 11821218. http://dx.doi.org/10.1007/s00039-004-0488-1. [9] Y. Brenier, A modified least action principle allowing mass concentrations for the early universe reconstruction problem, Confluentes Math. 3 (03) (2011) 361385. http://dx.doi.org/10.1142/S1793744211000400. [10] R.J. McCann, Polar factorization of maps on riemannian manifolds, Geom. Funct. Anal. GAFA 11 (3) (2001) 589608. http://dx.doi.org/10.1007/PL00001679. [11] H.-Y. Jian, X.-J. Wang, Continuity estimates for the MongeAmpere equation, SIAM J. Math. Anal. 39 (2) (2007) 608626. http://dx.doi.org/10.1137/060669036. [12] A.J. Majda, A.L. Bertozzi, Vorticity and Incompressible Flow, Vol. 27, Cambridge University Press, 2002, http://dx.doi.org/10.1017/CBO9780511613203. [13] D. Gilbarg, N.S. Trudinger, Elliptic Partial Differential Equations of Second Order, springer, 2015, http://dx.doi.org/10.1007/978-3-642-96379-7. [14] N.D. Alikakos, lp bounds of solutions of reactiondiffusion equations, Comm. Partial Differential Equations 4 (8) (1979) 827868. http://dx.doi.org/10.1080/03605307908820113. [15] A. Figalli, Global Existence for the Semigeostrophic Equations via Sobolev Estimates for MongeAmpere, in: CIME Lecture Notes, Springer, to appear. [16] D. Henry, Geometric Theory of Semilinear Parabolic Equations, Vol. 840, Springer-Verlag, New York, 1981, http://dx.doi.org/10.1007/BFb0089647.

Received May 30, 2016, accepted June 13, 2016, date of publication August 8, 2016, date of current version November 18, 2016.
Digital Object Identifier 10.1109/ACCESS.2016.2590500

A Survey on Cloud Gaming: Future
of Computer Games
WEI CAI1 , (Member, IEEE), RYAN SHEA2 , (Member, IEEE),
CHUN-YING HUANG3 , (Member, IEEE), KUAN-TA CHEN4 , (Senior Member, IEEE),
JIANGCHUAN LIU2 , (Senior Member, IEEE), VICTOR C. M. LEUNG1 , (Fellow, IEEE),
AND CHENG-HSIN HSU5 , (Senior Member, IEEE)

1 Department

of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC V6T 1Z4, Canada
of Computing Science, Simon Fraser University, Burnaby, BC V5A 1S6, Canada
of Computer Science, National Chiao Tung University, Hsinchu 300, Taiwan
4 Institute of Information Science, Academia Sinica, Taipei 115, Taiwan
5 Department of Computer Science, National Tsing Hua University, Hsinchu 300, Taiwan
2 School

3 Department

Corresponding author: C.-H. Hsu (chsu@cs.nthu.edu.tw)
This work was supported in part by the University of British Columbia through the Four Year Doctoral Fellowship, in part by the Natural
Sciences and Engineering Research Council of Canada under Grant STPGP 447524, in part by the National Natural Science Foundation of
China under Grant 61271182, in part by the Ministry of Science and Technology of Taiwan under Grant 102-2221-E-007-062-MY3,
Grant 103-2221-E-009-230-MY2, Grant 104-2221-E-009-200-MY3, and Grant 103-2221-E-001-023-MY2, in part by an NSERC E.W.R.
Steacie Memorial Fellowship, in part by an NSERC Strategic Project Grant, and in part by a Discovery Grant.

ABSTRACT Cloud gaming is a new way to deliver high-quality gaming experience to gamers anywhere and
anytime. In cloud gaming, sophisticated game software runs on powerful servers in data centers, rendered
game scenes are streamed to gamers over the Internet in real time, and the gamers use light-weight software
executed on heterogeneous devices to interact with the games. Due to the proliferation of high-speed
networks and cloud computing, cloud gaming has attracted tremendous attentions in both the academia and
industry since late 2000’s. In this paper, we survey the latest cloud gaming research from different aspects,
spanning over cloud gaming platforms, optimization techniques, and commercial cloud gaming services.
The readers will gain the overview of cloud gaming research and get familiar with the recent developments
in this area.
INDEX TERMS Clouds, distributed computing, video coding, quality of service, computer graphics.

I. INTRODUCTION

Cloud gaming refers to a new way to deliver computer
games to users, where computationally complex games are
executed on powerful cloud servers, the rendered game
scenes are streamed over the Internet to gamers with thin
clients on heterogeneous devices, and the control events from
input devices are sent back to cloud servers for interactions.
Figure 1 presents how cloud gaming services work. In the
cloud, a cloud gaming platform is hosted on cloud servers
in one or multiple data centers. The cloud gaming platform
runs computer game programs, which can be roughly divided
into two major components: (i) game logic that is responsible
to convert gamer commands into in-game interactions, and
(ii) scene renderer that generates game scenes in real-time.
The gamer commands come from the command interpreter,
and the game scenes are captured by video capturer into
videos, which are then compressed by video encoder. The
command interpreter, video capturer, and video encoder are
VOLUME 4, 2016

all implemented as parts of the cloud gaming platform.
As shown in this figure, the cloud gaming platform sends the
video frames to, and receives user inputs from thin clients
used by gamers for playing games. It is a thin client, because
only two low-complexity components are required: (i) command receiver, which connects to the game controllers, such
as gampads, joysticks, keyboards, and mouses, and (ii) video
decoder, which can be realized using massively produced
(inexpensive) decoder chips. The communications between
the cloud game platform and thin clients are over the besteffort Internet, which in turn makes supporting real-time
computer games quite challenging.
In late 2000’s, we started to see cloud gaming services
offered by startups, such as OnLive [67], Gaikai [27],
G-cluster [26], and Ubitus [93]. We also witnessed that Gaikai
was acquired by SONY, which is a major game console developer [86]. This was followed by the competition between
Sony’s PlayStation Now (PS Now) [68] and Nvidia’s Grid

2169-3536 
 2016 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

7605

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

FIGURE 1. Typical cloud gaming services.

Game Streaming Service [65], which further heats up the
cloud gaming market. In fact, a 2014 report from Strategy
Analytics [75] indicates that the number of cloud gaming
users increases from 30 millions in 2014 to 150 millions
in 2015. The same report also predicts that other leading
game console manufactures will soon join the cloud gaming
market.
The tremendous popularity of cloud gaming may be
attributed to several potential advantages to gamers, game
developers, and service providers. For gamers, cloud gaming
enables them to: (i) have access to their games anywhere and
anytime, (ii) purchase or rent games on-demand, (iii) avoid
regularly upgrading their hardware, and (iv) enjoy unique features such as migrating across client computers during game
sessions, observing ongoing tournaments, and sharing game
replays with friends. For game developers, cloud gaming
allows them to: (i) concentrate on a single platform, which in
turn reduces the porting and testing costs, (ii) bypass retailers
for higher profit margins, (iii) reach out to more gamers, and
(iv) avoid piracy as the game software is never downloaded
to client computers. For service providers, cloud gaming:
(i) leads to new business models, (ii) creates more demands on
already-deployed cloud resources, and (iii) demonstrates the
potential of other/new remote execution applications, since
cloud gaming imposes the strictest constraints on various
computing and networking resources.
Despite the great opportunities of cloud gaming,
several crucial challenges must be addressed by the research
community before it reaches its full potentials to attract more
gamers, game developers, and service providers. We summarize the most important aspect as follows. First, cloud gaming
platforms and testbeds must be built up for comprehensive
7606

performance evaluations. The evaluations include measurements on Quality of Service (QoS) metrics, such as energy
consumption and network metrics, and Quality of Experience (QoE) metrics, such as gamer perceived experience.
Building platforms and testbeds, designing the test scenarios,
and carrying out the evaluations, require significant efforts,
while analyzing the complex interplay between QoS and QoE
metrics is even more difficult.
Second, the resulting platforms and evaluation procedures
allow the research community to optimize various components, such as cloud servers and communication channels.
More specifically, optimization techniques for: (i) better
resource allocation and distributed architecture are possible
at cloud servers, and (ii) optimal content coding and adaptive
transmissions are possible in communication channels.
Third, computer games are of various game genres [19].
These genres can be categorized on the basis of two elements:
viewpoint and theme. Viewpoint is how a gamer observe
the game scene. It determines the variability of rendered
video on the screen. Most commonly seen viewpoints include
first-person, second-person, third-person, and omnipresent.
First-person games adopt graphical perspectives rendered
from the viewpoint of the in-game characters, such as in
Counter-Strike. Second-person games are rendered from the
back of the in-game characters, so that gamers can see the
characters on the screen, like in Grand Theft Auto. Thirdperson games fix the gamers’ views on 3D scenes, projected
onto 2D spaces. Modern third-person games usually adopts
the sky view, also known as God view. Classic third-person
games include Diablo, Command & Conquer, FreeStyle,
and etc. Last, omnipresent enables gamers to fully control
views on the region of interest (RoI) from different angels
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

and distances. Many recent war games, e.g., Age of
Empires 3, Stronghold 2, and Warcraft III, fall into this category. Game theme determines how gamers interact with game
content. Common themes include shooting, fighting, sports,
turn-based role-playing (RPG), action role-playing (ARPG),
turn-based strategy, real-time strategy (RTS), and management simulation. Although the viewpoint may be restricted by
game theme, but generally a game genre can be describe by
a pair of viewpoint and theme, such as first-person shooting,
third-person ARPG, omnipresent RTS, and etc. Among them,
fast-paced first-person shooting games impose the highest
scene complexity, which are the most challenging games
for cloud gaming service providers. In contrast, third-person
turn-based RPG games are least sensitive to delays and thus
more suitable for cloud gaming.
Cloud gaming is an exciting research area and the existing
literature aims to address several aforementioned challenges.
Nonetheless, to the best of our knowledge, there is no comprehensive survey on cloud gaming research. The lack of
a central survey of existing literature may delay or even
prevent researchers, who are interested in cloud gaming or
other remote execution applications, from joining the community. A thorough understanding and exploration of existing
academic and industrial research and development can help
lead to the building of future cloud gaming platforms. One
such advance might come from future games being designed
specifically with cloud gaming functionalities and supports in
mind. How we accomplish this is still an open question, for
example game developers could create cloud gaming aware
contexts or even whole new programing paradigms. With this
in mind, we carefully connect existing research on solving
current challenges together, and come up with a classification
system described below.
A. SCOPE AND CLASSIFICATIONS

In the current article, we survey the cloud gaming literature. We first collect representative cloud gaming papers, and
group them into several classifications. We emphasize that
only a selective set of papers are surveyed, in order to give
the readers better understanding on the landscape of the cloud
gaming research. Upon selecting the representative papers,
we propose a classification system as summarized in Figure 2.
More details on the classification system follow.
1) Cloud Gaming Overview (Section II): We survey
the overview, introductory, and positioning papers on
either general cloud gaming, or specialized topics,
such as mobile cloud gaming and Game-as-a-Service
(GaaS).
2) Cloud Gaming Platforms (Section III): We
consider papers that construct basic cloud gaming
platforms, which support different performance evaluation methodologies. These studies can be further
categorized into three groups: system integration, QoS
evaluations, and QoE evaluations.
a) System Integration (Section III-A): The fundamental step of cloud gaming research, like many
VOLUME 4, 2016

other systems areas, is to put up basic platforms,
based on existing tools. We summarize such
system integration efforts, which serve as cornerstones of related research.
b) Quality of Service Evaluations (Section III-B):
We survey the studies on objective metric evaluations, which algorithmically quantify the system
performance, i.e., without subject assessments.
Existing papers focus on two types of objective metrics, Energy Consumption and Network
Metrics. The energy consumption is critical to
mobile cloud gaming clients, in order to prolong the precious battery life. There are several
network metrics affecting the gamer experience,
and interaction latency is a representative network metric. The interaction latency refers to the
time difference between a gamer input and the
corresponding game scene update on the client
computer. Because gamers are highly sensitive to
interaction latency [19], its measurement methodologies draw a lot of attentions in the literature.
c) Quality
of
Experience
Evaluations
(Section III-C): We discuss the papers on subjective metric evaluations, which are based on user
studies, where subject gamers give opinion scores
to their cloud gaming experience. Conducting
user studies is inherently expensive and tedious,
and thus most QoE studies attempt to analyze the
relationship between the QoS and QoE metrics.
The resulting models may in turn be used to
optimize cloud gaming platforms.
3) Optimizing Cloud Gaming Platforms (Section IV):
We consider papers that optimize cloud gaming platforms from specific aspects; usually each work focuses
on optimizing one or a few components. Such studies
can be further categorized into two groups: cloud server
infrastructure and communications.
a) Cloud Server Infrastructure (Section IV-A):
The existing studies on optimizing cloud server
infrastructure are surveyed. Several papers study
the Resource Allocation problem of server and
network resources among multiple data centers,
server nodes, and game clients to optimize the
overall cloud gaming experience, where diverse
criteria are considered. Other papers optimize
the Distributed Architectures of cloud gaming
platforms, e.g., using Peer-to-Peer (P2P) overlays
or multi-tier clouds for better performance and
scalability.
b) Communications (Section IV-B): We survey
the existing work on optimizing the efficiency of
content streaming over the dynamic and heterogeneous communication channels. These studies
are further classified into two groups. First,
several papers consider the problem of Data
Compression, e.g., layered coding and graphics
7607

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

FIGURE 2. Our proposed classification system of cloud gaming papers.

compression are proposed, which may outperform the conventional 2D image compression in
certain environments. Second, there are papers
on Adaptive Transmission, which cope with the
network dynamics by continuously changing various parameters, such as encoding bitrate, frame
rate, and image resolution. The same adaptive
transmissions may also be used to absorb the
negative impacts due to insufficient resources on
cloud servers and game clients.
4) Commercial Cloud Gaming Services (Section V):
We survey the representative commercial cloud gaming services, and classify them along different aspects.
We also discuss the advantages and disadvantage of
different cloud gaming services.
In the rest of this article, we survey the four classes of
papers in Sections II–V. They are followed by Section VI,
which concludes the survey.
II. CLOUD GAMING OVERVIEW PAPERS

As a promising cloud service provisioning paradigm, cloud
gaming has attracted interests from prominent research teams
all over the world. These teams have shared their thoughts
7608

and ideas on cloud gaming from their viewpoints in several
high-level overview papers. In this section, we survey and
summarize the representative papers along this direction. Our
concise summary puts readers into the context of cloud gaming research, while interested readers may find new research
directions in the surveyed overview papers.
Ross [74] is the first literature that introduces the cloud
gaming model to the academia in 2009, nine years after the
G-cluster’s demonstration of cloud gaming technology at E3.
The author describes gaming as cloud computing’s killer app
and depicts the blueprint of novel gaming delivery paradigm,
proposed by Advanced Micro Devices (AMD), which renders
games’ scene videos, compresses them, and transmit them to
the gamers through the Internet. This approach enables online
gamers to offload their graphic rendering tasks to the cloud,
thus, eliminates the computational workload on gamers’ local
platforms. This is the most popular definition of cloud gaming
adopted by most of the research work in this area. However,
a recent publication [59] provides a more general definition,
by envisioning the cloud gaming system as a novel computer
architecture that leverages cloud resources to improve gaming
performance, such as rendering, response time, precision and
fairness. The authors distribute system workload to multiple
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

cloud servers and game clients to enable this vision. For a
further step, Cai et al. [4] explore the essence of cloud games
as inter-dependent components, thus, define cloud gaming as
utilizing cloud resources to host gaming components, thereby
reducing workload at gamers’ local platforms and increasing the overall system performance. According to different
integration approach of the cloud, the authors identify and
discuss the research directions of three cloud gaming architectures, which are Remote Rendering, Local Rendering, and
Cognitive Resource Allocation.
After the official launch of OnLive in March 2010,
the business model for cloud gaming becomes a hot topic
in research society. Riungu-kalliosaari et al. [73] conduct
interviews in small and medium size gaming companies to
qualitatively study the adoption dynamics of cloud computing. With grounded theory method, the authors observe that
the concept of cloud gaming are relatively well-known in the
industry, while gaming organizations still hesitate in adopting
cloud computing services and technologies due to the lack of
clear business models and success stories. To this end, Ojala
and Tyrvainen [66] start their investigations on developing
business models for cloud gaming services. As a case study
for Software as a Service (SaaS), the authors select G-cluster,
one of famous cloud gaming companies, and study its business model over five years from 2005 to 2010. They conclude
that, over time, the business model in cloud gaming becomes
simpler and has fewer actors, which increases the revenue
per gamer. In addition, they also expect the cloud gaming
solution will make illegal copying practically impossible.
Another work [61] considers the convergence of mobile cloud
in gaming industry from a business model proposition. The
authors discuss the first sketch of a possible business model of
Kusanagi project, a proposed end-to-end infrastructure, from
domains of service, technology, organization, and financial,
while compare these domains of three cloud examples, i.e.,
G-cluster, Gaikai, and OnLive.
During the decade of development, there have been cloud
gaming systems and services in the market. A number of
positioning papers consider these systems and envision the
opportunities, challenges, and directions in this area. The
literature, e.g., [4], [11], [17], [23], [59], [85], [100], covers
both the commercial and academic platforms, while their
concerns of open issues are greatly overlapped in the topics of
response time minimization, graphical video encoding, network aware adaption, QoE optimization, and cloud resource
management.
Besides of these common focuses, each research team
has particular interests and directions. Dey [23] concentrate
on developing device aware scalable applications, which
involve open issues of extending cloud to wireless networks.
Soliman et al. [85] briefly discuss related legal issues, including patents, ownership concerns, guaranteed service levels,
and pricing schemes. In contrast, piracy and hacking may no
long be an issue, since the executable game program will
not be delivered to the gamers. Wu [100] explores cloud
gaming architecture from the aspect of cloud computing’s
VOLUME 4, 2016

three layers, i.e., IaaS, SaaS and PaaS. The author identifies security as a potential challenge in cloud gaming, especially data protection and location. Cai et al. [4] investigate
the features of different game genres and identify their
impact on cloud gaming system design. In addition, they
provide a vision on GaaS provisioning for mobile devices.
Mishra et al. [59] explain how to enhance the quality of
online gaming by integrating techniques from cloud gaming
research communities. Featured topics include the interplay
between QoS and QoE metrics, game models, and cloud
expansion. Chen et al. [11] point out some unique research
directions in cloud gaming, such as game integration, visualization, user interface, server selection, and resource scheduling. Chuan et al. [17] study cloud gaming from a green media
perspective. They discuss the major cloud gaming subsystems with green designs, which include a cloud data centre,
graphics rendering modules, video compression techniques,
and network delivery methods.
In addition to these high-level studies, more cloud gaming
papers focus on individual research problems. We divide
them into several classifications and survey them in the
following sections.
III. CLOUD GAMING PLATFORMS

This section presents the work related to cloud gaming platforms in three steps: (i) integrated cloud gaming platforms
for complete prototype systems, (ii) measurement studies on
QoS metrics, and (iii) measurement studies on QoE metrics.
A. SYSTEM INTEGRATION

Providing an easy-to-use platform for (cloud) game
developers is very challenging. This is because of the complex, distributed, and heterogeneous nature of the cloud
gaming platforms. In fact, there is a clear tradeoff between
development complexity and optimization room. Platforms
opt for very low (or even no) additional development complexity may suffer from limited room for optimization, which
are referred to as transparent platforms that run unmodified games. In contrast, other platforms opt for more optimized performance at the expense of requiring additional
development complexity, such as code augmentation and
recompilation, which are called non-transparent platforms.
These two classes of cloud gaming platforms have advantages
and disadvantages, and we describe representative studies in
individual classes below.
The transparent platforms ease the burden of deploying
new games on cloud gaming platforms, at the expense of
potentially suboptimal performance. Depasquale et al. [22]
present a cloud gaming platform based on the RemoteFX
extension of Windows remote desktop protocol. Modern Windows servers leverage GPUs and Hyper-V virtual
machines to enable various remote applications, including
cloud games. Their experiments reveal that RemoteFX allows
Windows servers to better adapt to network dynamics, but
still suffers from high frame loss rate and inferior responsiveness. Kim et al. [44] propose another cloud gaming platform,
7609

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

which consists of a distributed service platform, a distributed
rendering system, and an encoding/streaming system. Their
platform supports isolated audio/video capturing, multiple
clients, and browser-based clients. Real experiments with
40 subjects have been done, showing high responsiveness.
Both Depasquale et al. [22] and Kim et al. [44] are proprietary
platforms, and are less suitable for cloud gaming research.
GamingAnywhere [38], [40] is the first open source transparent cloud gaming platform. Its design principles can be
summarized as extensive, portable, configurable, and open.
The GamingAnywhere server supports Windows and Linux,
and the GamingAnywhere client runs on Windows, Linux,
Mac OS, and Android. It is shown that GamingAnywhere
outperforms several commercial/proprietary cloud gaming
platforms, and has been used and enhanced in several cloud
gaming studies in the literature. For example, Hong et al. [35]
develop adaptation algorithms for multiple gamers, to maximize the gamer experience. In addition to: (i) a user study
to map cloud gaming parameters to gamer experience and
(ii) optimization algorithms for resource allocation, they also
enhance GamingAnywhere [38], [40] to support on-the-fly
adaption of frame rate and bitrate.
The non-transparent platforms require augmenting and
recompiling existing games to leverage unique features
for better gaming experience, which may potentially be
time-consuming, expensive, and error-prone. For example,
current games can be ported to Google’s Native Client technology [62], [63] or to Mozilla’s asm.js language [1], [24].
Several other studies focus on integrating new techniques
with cloud gaming platforms for better gaming experience.
Nan et al. [64] propose a joint video and graphics streaming
system for higher coding efficiency as well. Moreover, they
present a rate adaptation algorithm to further minimize the
bandwidth consumption. Lee et al. [48], [49] present a system
to improve the responsiveness of mobile cloud gaming by
compensating network delay. In particular, their system prerenders potential future frames based on some prediction
algorithm and delivers the rendered frames to mobile clients
when the network conditions are good. These frames are
then used to compensate late video frames due to unstable networks. They integrate the proposed system with two
open source games, and conduct a user study of 23 subjects. The subjects report good gaming experience under
nontrivial network delay, as high as 250 ms. Cai et al. [3]
build a prototype platform for decomposed cloud gaming,
and rigorously address several system issues, which were not
thoroughly investigated in their earlier work [4]. Their main
contribution is the very first cognitive cloud gaming platform
that automatically adapts to distributive workload in run-time,
in order to optimally utilize distributed resources (on different entities, like cloud servers, in-network computing nodes,
and gamers’ local platforms) for the best gamer experience.
On the resulting platform, several games are developed and
empirically evaluated, demonstrating the potentials of cognitive cloud gaming platforms. Several enhancements on such
a platform are still possible, such as implementing more
7610

sophisticated games, supporting more gamers, and providing
more completed SDK (Software Development Kit) to cloud
game developers.
B. QUALITY OF SERVICE EVALUATIONS

Performing QoS measurements is crucial for quantifying the
performance of the cloud gaming platforms. Moreover, doing
so in real-time allows us to effectively troubleshoot and even
to dynamically optimize the cloud gaming platforms. The
QoS related cloud gaming papers are roughly categorized into
two classes: (i) energy consumption and (ii) network metrics.
They are surveyed in the following.
1) ENERGY CONSUMPTION

Games have been known to push consumer computing platforms to their maximum capacity. In traditional systems such
as desktop computers, it is often expected and accepted that
Game software will push a system to its limits. However,
mobile environments are in a strikingly different scenario
as they have limited power reserves. A fully utilized mobile
device may have a greatly reduced running time, thus it is
important to reduce the complexity of these game software
for mobile devices. Luckily, cloud gaming systems provide
a potential way forward by offloading complicated processing tasks such as 3D rendering and physics calculations
to powerful cloud servers. However, cares must be taken
because the decoding of video, especially high definition
video is far from a trivial task. We will cover some pioneering
work [29], [39], [91] that has been done on this important
subject.
Hans et al. [29] systematically test the energy
performance of their in-house cloud gaming server
MCGS.KOM on real world tablets. They find that when
WLAN was used as the access network, cloud game software
could save between 12% and 38% of energy use, depending
on the types of games and tablets. Explorations on important
energy saving coding parameters for H.264/AVC are reported
in Taher et al. [91]. Further, Huang et al. [39] explore the
energy consumption of the cloud gaming video decoders. The
researchers found that frame rate has the largest impact on the
decoders energy consumption, with bit rate and resolution
also being major contributors. Moreover, Shea et al. [79]
explore the performance and energy implications of combing
cloud gaming systems with live broadcasting systems such as
Twitch.
2) NETWORK METRICS

Like many other distributed multimedia applications, user
experience highly depends on network conditions. Therefore,
evaluating different network metrics in cloud gaming is crucial, and we present detailed survey below.
Claypool [18] measures the contents variety of different game genres in details. 28 games from 4 perspectives,
including First-Person Linear, Third-Person Linear, ThirdPerson Isometric, and Omnipresent, are selected to analyze
their scene complexity and motion, indicated by average
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

Intra-coded Block Size (IBS) and Percentage of
Forward/backward or Intra-coded Macroblocks (PFIM),
respectively. Measurements conducted by the author suggest
that Microsoft’s remote desktop achieves better bitrate than
NoMachine’s NX client, while NX client has higher frame
rate. A following work [21] investigates OnLive’s network
characteristics, such as the data size and frequency being sent
and the overall downlink and uplink bitrates. The authors
reveal that the high downlink bitrates of OnLive games are
very similar to those of live videos, nevertheless, OnLive’s
uplink bitrates are much more moderate, which are comparable to traditional game uplink traffic. They also indicate
that the game traffic features are similar for three types
of game genres, including First-Person, Third-Person, and
Omnipresent, while the total bitrates can vary by as much
as 50%. Another important finding is that OnLive does not
demonstrate its ability in adapting bitrate and frame rates to
network latency.
Chen et al. [10] analyze a cloud gaming system’s response
delays and segment it into three components, including network delay, processing delay, and playout delay. With this
decomposition, the authors propose a methodology to measure the latency components and apply the methodology
on OnLive and StreamMyGame, two of the popular cloud
gaming platforms. The authors identify that OnLive system
outperforming StreamMyGame in terms of latency, due to the
different resource provisioning strategy based on game genres. A following work [9] by the same group extend the model
by adding game delay, which represents the latency introduced by the game program to process commands and render
the next video frame of the game scene. They also study
how system design and selective parameters affect responsiveness, including scene complexity, updated region sizes,
screen resolutions, and computation power. Their observation
in network traffics are inline with previous work conducted
by Claypool et al. [21]. Lower network quality, including
the higher packet loss rate and insufficient bandwidth, will
impose negative impacts on both of OnLive and StreamMyGame, resulting lower frame rates and worse graphic
quality. Moreover, by quantifying the streaming quality, the
authors further reveal that OnLive implements an algorithm
to adapt its frame rate to the network delay, while StreamMyGame doesn’t.
Manzano et al. [55] collect and compare network traffic
traces of OnLive and Gaikai, including packet inter-arrival
times, packet size, and packet inter-departure time, to observe
the difference between cloud gaming and traditional online
gaming from the perspectives of network load and traffic
characteristics. The authors reveal that the package size distributions between the two platforms are similar, while the
packet inter-arrival times are distinct. Afterwards, Manzano
et al. [56] claim to be the first research work on specific
network protocols used by cloud gaming platforms. They
focus on conducting a reverse engineering study on OnLive,
based on extensive traffic traces of several games. The authors
further propose a per-flow traffic model for OnLive,
VOLUME 4, 2016

which can be used for network dimensioning, planning optimization, and other studies.
Shea et al. [81] measure the interaction delay and image
quality of OnLive system, under diverse games, computers,
and network configurations. The authors conclude that cloud
procedure introduces 100 to 120 ms latency to the overall
system, which requires further developments in both video
encoders and streaming software. Meanwhile, the impacts of
compression mechanism on video quality are quite noticeable, especially under the circumstances with lower available
bandwidth. They later present an experimental study [80]
on the performance of existing commercial games and raytracing applications with graphical processing units (GPUs).
According to their analysis, gaming applications in virtualized environments demonstrate poorer performance than the
instances executing in non-virtualized bare-metal baseline.
Detailed hardware profiling further reveals that the passthrough access introduces memory bottleneck, especially for
those games with real-time interactions. Another work [36],
however, observes more advanced virtualization technologies
such as mediated pass-through maintain high performance
in virtualized environments. In the authors’ measurement
work, rendering with virtualized GPUs may achieves better
performance than direct pass-through ones. In addition, if the
system adopts software video coding, the CPU may became
the bottleneck, while hypervisor will no longer be the constraint of the system performance. Based on these analysis,
the authors conclude that current virtualization techniques are
already good enough for cloud gaming.
Suznjevic et al. [89] measure 18 games on GamingAnywhere [38] to analyze the correlation between the characteristics of the games played and their network traffic. The
authors observe the highest values for motion, action game
and shooter games, while the majority of strategy games are
relatively low. In contrast, for spatial metrics the situation is
reversed. They also conclude that the bandwidth usage for
most games are within the range of 3 and 4 Mbit/s, except the
strategy games that consume less network resources. Another
notable finding is that, gamers’ action rate will introduce a
slight packet rate increase, but will not affect the generated
network traffic volume.
Lampe et al. [46] conduct experimental evaluations of userperceived latency in cloud games and locally executed video
games. Their results, produced by a semi-automatic measurement tool called GALAMETO.KOM, indicate that cloud
gaming introduces additional latency to game programs,
which is approximately 85% to 800% higher than local executions. This work also features the significant impact of roundtrip time. The measurement results confirm the hypothesis
that the geographical placement of cloud data centres is an
important element in determining response delay, specifically
when the cloud gaming services are accessed through cellular
networks.
Xue et al. [102] conduct a passive and active measurement study for CloudUnion, a Chinese cloud gaming system. The authors characterize the platform from the aspects
7611

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

of architecture, traffic pattern, user behaviour, frame rate
and gaming latency. Observations include: (i) CloudUnion
adopts a geo-distributed infrastructure; (ii) CloudUnion suffers from a queuing problem with different locations from
time to time; (iii) the User Datagram Protocol (UDP) outperforms the Transmission Control Protocol (TCP) in terms
of response delay while sacrificing the video quality; and
(iv) CloudUnion adopts conservative video rate recommendation strategy. By comparing CloudUnion and GamingAnywhere [38], the authors observe four common problems. First,
the uplink and downlink data rates are asymmetric. Second,
low-motion games perceive a periodical jitter at the interval
of 10 seconds. Third, audio and video streams are suffering
from synchronization problem. Fourth, packet loss in network
transmission degrades gaming experiences significantly.
C. QUALITY OF EXPERIENCE EVALUATIONS

Measuring and modeling cloud gaming QoE are no easy tasks
because QoE metrics are subjective. In particular, enough
subjects need to be recruited, and time-consuming, tedious,
and expensive user studies need to be carried out. After
that, practical models to relate the QoS and QoE metrics
need to be proposed, trained, and evaluated. Only when the
resulting models are validated with large datasets, they can be
employed in actual cloud gaming platforms. Cloud gaming
QoE has been studied in the literature and can be categorized
into two classes: (i) general cloud gaming QoE evaluations,
and (ii) mobile cloud gaming QoE evaluations, which are
tailored for mobile cloud games, where mobile devices are
resource constrained and vulnerable to inferior wireless network conditions. We survey the related work in these two
classes below.
Chang et al. [8] present a measurement and modeling
methodology on cloud gaming QoE using three popular
remote desktop systems. Their experiment results reveal that
the QoE (in gamer performance) is a function of frame rate
and graphics quality, and the actual functions are derived
using regression. They also show that different remote desktop systems lead to quite diverse QoE levels under the same
network conditions. Jarschel et al. [42] present a testbed
for a user study on cloud gaming services. Mean Opinion
Score (MOS) values are used as the QoE metrics, and the
resulting MOS values are found to depend on QoS parameters, such as network delay and packet loss, and context, such
as game genres and gamer skills. Their survey also indicates
that very few gamers are willing to commit themselves in a
monthly fee plan for cloud gaming. Hence, better business
models are critical to long-term success of cloud gaming.
Möller et al. [60] also conduct a subjective test in the labs,
and consider 7 different MOS values: input sensitivity, video
quality, audio quality, overall quality, complexity, pleasantness, and perceived value. They observe complex interplays
among QoE metrics, QoS metrics, testbed setup, and software
implementation. For example, the rate control algorithm
implemented in cloud gaming client is found to interfere
with the bandwidth throttled by a traffic shaper. Several open
7612

issues are raised after analyzing the results of the user study,
partially due to the limited number of participants.
Slivar et al. [84] carry out a user study of in-home cloud gaming, i.e., the cloud gaming servers and clients are connected
over a LAN. Several insights are revealed, e.g., switching
from a standard game client to in-home cloud gaming client
leads to QoE degradation, measured in MOS values. Moreover, more skilled gamers are less satisfied with in-home
cloud gaming. Hossain et al. [37] adopt gamer emotion as
a QoE metric and study how several screen effects affect
gamer emotion. Sample screen effects include adjusting:
(i) redness, (ii) blueness, (iii) greenness, (iv) brightness, and
(v) contrast; and the goal of applying these screen effects
is to mitigate negative gamer emotion. They then perform
QoE optimization after deriving an empirical model between
screen effects and gamer emotion.
Some other QoE studies focus on the response delay, which
is probably the most crucial performance metric in cloud
gaming, where servers may be geographically far away from
clients. Lee et al. [50] find that response delay imposes
different levels of implications on QoE with different game
genres. They also develop a model to capture this implication
as a function of gamer inputs and game scene dynamics.
Quax et al. [71] make similar conclusions after conducting
extensive experiments, e.g., gamers playing action games
are more sensitive to high responsive delay. Claypool and
Finkel [20] perform user studies to understand the objective
and subjective effects of network latency on cloud gaming.
They find that both MOS values and gamer performance
degrade linearly with network latency. Moreover, cloud gaming is very sensitive to network latency, similar to the traditional first-person avatar games. Raaen [72] designs a user
study to quantify the smallest response delay that can be
detected by gamers. It is observed that some gamers can
perceive < 40 ms response delay, and half of the gamers
cannot tolerate ≥ 100 ms response delay.
Huang et al. [41] perform extensive cloud gaming
experiments using both mobile and desktop clients. Their
work reveals several interesting insights. For example,
gamers’ satisfaction on mobile clients are more related to
graphics quality, while the case on desktop clients is more
correlated to control quality. Furthermore, graphics and
smoothness quality are significantly affected by the bitrate,
frame rate, and network latency, while the control quality
is determined only by the client types (mobile or desktop).
Wang and Dey [94], [97] build a mobile cloud gaming testbed
in their lab for subjective tests. They propose a Game Mean
Opinion Score (GMOS) model, which is a function of game
genre, streaming configuration, measured Peak Signal-toNoise Ratio (PSNR), network latency, and packet loss. The
derivations of model parameters are done via offline regression, and the resulting models can be used for optimizing
mobile cloud gaming experience. Along this line,
Liu et al. [54] propose a Cloud Mobile Rendering–Mean
Opinion Score (CMR-MOS) model, which is a variation of GMOS. CMR-MOS has been used in selecting
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

detail levels of remote rendering applications, like cloud
games.
IV. OPTIMIZING CLOUD GAMING PLATFORMS

This section surveys optimization studies on cloud gaming
platforms, which are further divided into two classes: (i) cloud
server infrastructure and (ii) communications.
A. CLOUD SERVER INFRASTRUCTURE

To cope with the staggering demands from the massive
number of cloud gaming users, carefully-designed cloud
server infrastructures are required for high-quality, robust,
and sustainable cloud gaming services. Cloud server infrastructures can be optimized by: (i) intelligently allocating
resources among servers or (ii) creating innovative distributed
structures. We detail these two types of work in the following.
1) RESOURCE ALLOCATION

The amount of resources allocated to high performance
multimedia applications such as cloud gaming continues to
grow in both public and private data centers. The high demand
and utilization patterns of these platforms make the smart
allocation of these resources paramount to the efficiency of
both public and private clouds. From Virtual Machine (VM)
placement to shared GPUs, researchers from many areas have
been exploring how to efficiently use the cloud to host cloud
gaming platforms. We now explore the important work done
in this area to facilitate efficient deployment of cloud gaming
platforms.
Critical work has been done on both VM placement and
cloud scheduling to facilitate better quality of cloud gaming
services. For example, Wang et al. [98] show that, with proper
scheduling of cloud instances, cloud gaming servers could be
made wireless networking aware. Simulations of their proposed scheduler show the potential of increased performance
and decreased costs for cloud gaming platforms. Researchers
also explore making resource provisioning cloud gaming
aware. For example, a novel QoE aware VM placement strategy for cloud gaming is developed [33]. Further, research has
been done to increase the efficiency of resource provisioning for massively multi-player online games (MMOG) [57].
The researchers develop greedy heuristics to allocate the
minimum number of computing nodes required to meet the
MMOG service needs. Researchers also study the popularity
of games on the cloud gaming service OnLive and propose
methods to improve performance of these systems based
on game popularity [25]. Later, a resource allocation strategy [51] based on the expected ending time of each play
session is proposed. The strategy can reduce the cost of
operation to cloud gaming providers by reducing the number
of purchased nodes required to meet their clients needs. They
note that classical placement algorithms such as First Fit and
Best Fit, are not effective for cloud gaming. After extensive experiments, the authors show an algorithm leveraging
on neural-network-based predictions, which could improve
VM deployment, and potentially decreases operating costs.
VOLUME 4, 2016

Although many cloud computing workloads do not require
a dedicated GPU, cloud gaming servers require access to
a rendering device to provide 3D graphics. As such VM
and workload placements have been researched to ensure
cloud gaming servers have access to adequate GPU resources.
Kim et al. [45] propose a novel architecture to support
multiple-view cloud gaming servers, which share a single
GPU. This architecture provides multi-focal points inside a
shared cloud game, allowing multiple gamers to potentially
share a game world, which is rendered on a single GPU.
Zhao et al. [104] perform an analysis of the performance of
combined CPU/GPU servers for game cloud deployments.
They try offloading different aspects of game processing to
these cloud servers, while maintaining some local processing
at the client side. They conclude that keeping some processing at the client side may lead to an increase in QoS of cloud
gaming platforms.
Pioneering research has also been done on GPU sharing
and resource isolation for cloud gaming servers [70], [103].
These works show that with proper scheduling and allocation of resources we can maximize GPUs utilization, while
maintaining high performance for the gamers sharing a single
GPU. Shea and Liu [80] show that direct GPU assignment to
a virtualized gaming instance can lead to frame rate degradation of over 50% in some gaming applications. They find that
the GPU device pass-through severely diminishes the data
transfer rate between the main memory and the GPU. Their
follow-up work using more advanced platforms [78] reveals
that although the memory transfer degradation still exists,
it no longer affects the frame rate of current generation games.
Hong et al. [34] perform a parallel work, where they discover
that the frame rate issue presents in virtualized clouds may be
mitigated by using mediated pass-through, instead of direct
assignment.
In addition, work has been done to augment existing clouds
and games to improve cloud gaming efficiency. It has been
shown that using game engine information can greatly reduce
the resources needed to calculate the motion estimation (ME)
needed for conventional compression algorithms such as
H.264/AVC [76]. Research into these technique shows that
we can accelerate the motion estimation phase by over 14%
if we use in-game information for encoding. Others have proposed using reusable modules for cloud gaming servers [30].
They refer to these reusable modules as substrates and test
the latency between the different components. All these data
compression studies affect resource allocation; we provide a
comprehensive survey on data compression for cloud gaming
in Section IV-B.1.
2) DISTRIBUTED ARCHITECTURES

Due to the vast geographic distribution of the cloud gaming
clients the design of distributed architectures is of critical
importance to the deployment of cloud gaming systems. The
design of these systems must be carefully optimized to ensure
that a cloud gaming system can sufficiently cover its target audience. Further, to maintain the extremely low delay
7613

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

tolerance required for high QoE even the placement of different server components must be optimized for the lowest possible latency. These innovative distributed architectures have
been investigated in the literature, and we detail them below.
Sselbeck et al. [90] discover that running a cloud gaming
based massively multi-player online game (MMOG) may
suffer from increased latency. These issues are aggravated
in a cloud gaming context because MMOG are already
extremely latency sensitive applications. The increased
latency introduced by a cloud gaming may vastly decrease
the playability of these games. To deal with this increased
latency, they propose a P2P based solution. Similarly,
Prabu and Purushotham [69] propose a P2P system based on
Windows Azure to support online games.
Research has also been done on issues created by the
geographical distance between the end user of cloud gaming
and a cloud gaming data center. Choy et al. [13] show that
the current geographical deployments of public data centers leave a large fraction of the USA with an unacceptable
RTT for low latency applications such as cloud gaming.
To help mitigate this issue, they propose deploying edge
servers near some users for cloud gaming; a follow up work
further explores this architecture and shows that hybrid edgecloud architectures could indeed expand the reach of cloud
gaming data centers [14]. Similarly, Siekkinen et al. [83]
propose a distributed cloud gaming architecture with servers
deployed near local gamers when necessary. The researchers
prototype the system and show that if being deployed widely
enough, for example at the ISP level, cloud gaming could
reach an even larger audience. Tian et al. [92] perform an
extensive investigation into issues of deploying cloud gaming
architecture with distributed data centers. They focus on a
scenario where adaptive streaming technology is available to
the cloud provider. The authors give an optimization algorithm, which can improve gamer QoE as well as reducing
operating costs of the cloud gaming provider. The algorithm
is evaluated using trace driven simulations, and the results
show a potential cost savings of 25% to the cloud gaming
provider.
B. COMMUNICATIONS

Due to the distributed nature of cloud gaming services, the
efficiency and robustness of the communication channels
between cloud gaming servers and clients are crucial and have
been studied. These studies can be classified into two groups:
(i) the data compression algorithms to reduce the network
traffic amount and (ii) the transmission adaptation algorithms
to cope with network dynamics. We survey the work in these
two groups in the following.
1) DATA COMPRESSION

After game scenes are computed on cloud servers, they
have to be captured in proper representations and compressed before being streamed over networks. This can
be done in one of the three data compression schemes:
(i) video compression, which encodes 2D rendered videos and
potentially auxiliary videos (such as depth videos) for client
7614

side post-rendering operations, (ii) graphics compression,
which encodes 3D structures and 2D textures, and (iii) hybrid
compression, which combines both video and graphics compression. Upon cloud gaming servers produce compressed
data streams, the servers send the streams to client computers
over communication channels. We survey each of the three
schemes below.
Video compression is the most widely-used data
compression schemes for cloud gaming probably because
2D video codecs are quite mature. These proposals strive
to improve the coding efficiency in cloud gaming, and
can be further classified into groups depending on whether
in-game graphics contexts, such as camera locations and orientations, are leveraged for higher coding efficiency. We first
survey the proposals that do not leverage graphics contexts.
Cai et al. [6] propose to cooperatively encode cloud gaming
videos of different gamers in the same game session, in
order to leverage inter-gamer redundancy. This is based on
an observation that game scenes of close-by gamers have
non-trivial overlapping areas, and thus adding inter-gamer
predictive video frames may improve the coding efficiency.
The high-level idea is similar to multiview video codecs, such
as H.264/MVC, and the video packets shared by multiple
gamers are exchanged over an auxiliary short-range ad-hoc
network in a P2P fashion. Cai et al. [5] improve upon the
earlier work [6] by addressing three more research problems:
(i) uncertainty due to mobility, (ii) diversity of network
conditions, and (iii) model of QoE. These problems are solved
by a suite of optimization algorithms proposed in their work.
Sun and Wu [88] solve the video rate control problem in cloud
gaming in two steps. First, they adopt the concept of RoI,
and define heterogeneous importance weights for different
regions of game scenes. Next, they propose a macroblocklevel rate control scheme to optimize the RoI-weighted video
quality. Cheung et al. [12] propose to concatenate the graphic
renderer with a customized video coder on servers in cellular
networks and multicast the coded video stream to a gamer
and multiple observers. Their key innovation is to leverage
the depth information used in 3D rendering process to locate
the RoI and then allocate more bits to that region. The
resulting video coder is customized for cloud gaming, yet produces standard compliant video streams for mobile devices.
Liu et al. [53] also leverage rendering information to improve
video encoding in cloud gaming for better perceived video
quality and shorter encoding time. In particular, they first
analyze the rendering information to identify RoI and allocate more bits on more important regions, which leads to
better perceived video quality. In addition, they use this
information to accelerate the encoding process, especially
the time used in motion estimation and macroblock mode
selection. Experiments reveal that their proposed video coder
saves 42% of encoding time and achieves perceived video
quality similar to the unmodified video coder. Similarly,
Semsarzadeh et al. [76] study the feasibility of using
rendering information to accelerate the computationallyintensive motion estimation and demonstrate that it is
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

possible to save 14.32% of the motion estimation time and
8.86% of the total encoding time. The same authors [77]
then concertize and enhance their proposed method, in which
they present the general method, well-designed programming
interface, and detailed motion estimation optimization. Both
subjective and objective tests show that their method suffers from very little quality drop compared to the unmodified video coder. It is reported that they achieve 24% and
39% speedups on the whole encoding process and motion
estimation, respectively.
Next, we survey the proposals that utilize graphics
contexts [82], [101]. Shi et al. [82] propose a video compression scheme for cloud gaming, which consists of two
unique techniques: (i) 3D warping-assisted coding and
(ii) dynamic auxiliary frames. 3D warping is a light-weight
2D post-rendering process, which takes one or multiple
reference view (with image and depth videos) to generate
a virtual view at a different camera location/orientation.
Using 3D warping allows video coders to skip some video
frames, which are then wrapped at client computers. Dynamic
auxiliary frames refer to those video frames rendered with
intelligently-chosen camera location/orientations that are not
part of the game plays. They show that the auxiliary frames
help to improve 3D warping performance. Xu et al. [101] also
propose two techniques to improve the coding efficiency in
cloud gaming. First, the camera rotation is rectified to produce video frames that are more motion estimation friendly.
On client computers, the rectified videos are compensated
with some camera parameters using a light-weight 2D process. Second, a new interpolation algorithm is designed to
preserve sharp edges, which are common in-game scenes.
Last, we notice that the video compression schemes are
mostly orthogonal to the underneath video coding standards,
and can be readily integrated with the recent (or future) video
codecs for further performance improvement.
Graphics compression is proposed for better scalability,
because 3D rendering is done at individual client computers.
Compressing graphics data, however, is quite challenging
and may consume excessive network bandwidth [52], [58].
Lin et al. [52] design a cloud gaming platform based on
graphics compression. Their platform has three graphics
compression tools: (i) intra-frame compression, (ii) interframe compression, and (iii) caching. These tools are applied
to graphics commands, 3D structures, and 2D textures.
Meilnder et al. [58] also develop a similar platform for mobile
devices, where the graphics are sent from cloud servers to
proxy clients, which then render game scenes for mobile
devices. They also propose three graphics compression tools:
(i) caching, (ii) lossy compression, and (iii) multi-layer compression. Generally speaking, tuning cloud gaming platforms
based on graphics compression for heterogeneous client
computers is non-trivial, because mobile (or even some
stationary) computers may not have enough computational
power to locally render game scenes.
Hybrid compression [15], [16] attempts to fully utilize
the available computational power on client computers
VOLUME 4, 2016

to maximize the coding efficiency. For example,
Chuah and Cheung [15] propose to apply graphics compression on simplified 3D structures and 2D textures, and send
them to client computers. The simplified scenes are then
rendered on client computers, which is called the base layer.
Both the full-quality video and the base-layer video are rendered on cloud servers, and the residue video is compressed
using video compression and sent to client computers. This
is called the enhancement layer. Since the base layer is compressed as graphics and the enhancement layer is compressed
as videos, the proposed approach is a hybrid scheme. Based
on the layered coding proposal, Chuah et al. [16] further
propose a complexity-scalable base-layer rendering pipeline
suitable for heterogeneous mobile receivers. In particular,
they employ scalable Blinn-Phong lighting for rendering
the base-layer, which achieves maximum bandwidth saving
under the computing constraints of mobile receivers. Their
experiments demonstrate that their hybrid compression solution, customized for cloud gaming, outperforms single-layer
general-purpose video codecs.
2) ADAPTIVE TRANSMISSION

Even though data compression techniques have been applied
to reduce the network transmission rate, the fluctuating network provisioning still results in unstable service quality to
the gamers in cloud gaming system. These unpredictable
factors include bandwidth, round-trip time, jitter, and etc.
Under this circumstance, adaptive transmission is introduced
to further optimize gamers’ QoE. The foundation of these
studies is based on a common sense: gamers would prefer to
scarify video quality to gain smoother playing experience in
insufficient network QoS supplement.
Jarvinen et al. [43] explore the approach to adapt the
gaming video transmission to available bandwidth. This is
accomplished by integrating a video adaptation module into
the system, which estimates the network status from network monitor in real-time and dynamically manipulates the
encoding parameters, such as frame rate and quantization, to
produce specific adaptive bit rate video stream. The authors
utilize RTT jitter value to detect the network congestion,
in order to decide if the bit rate adaptation should be triggered.
To evaluate this proposal, a following work [47] conducts
experiments on a normal television with an IPTV set-topbox. The authors simulate the network scenarios in homes
and hotels to verify that the proposed adaptation performed
notably better.
Adaptive transmission has also been studied in mobile scenarios. Wang and Dey [95] first decompose the cloud gaming
system’s response time into sub-components: server delay,
network uplink/downlink delay, and client delay. Among
the optimization techniques applied, rate-selection algorithm
provides a dynamic solution that determine the time and the
way to switch the bit rate according to the network delay.
As a further step, Wang and Dey [96] study the potential
of rendering adaptation. They identify the rendering parameters that affect a particular game, including realistic effect
7615

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

(e.g., colour depth, multi-sample, texture-filter, and lighting mode), texture detail, view distance and enabling grass.
Afterwards, they analyze these parameters’ characteristics
of communications and computation costs and propose their
rendering adaptation scheme, which is consisted of optimal
adaptive rendering settings and level-selection algorithm.
With the experiments conducted on commercial wireless
networks, the authors demonstrate that acceptable mobile
gaming user experience can be ensured by their rendering
adaption technique. Thus, they claim that their proposal is
able to facilitate cloud gaming over mobile networks.
Other aspects of transmission adaptation have also been
investigated in the literature. He et al. [31] consider the adaptive transmission from the perspective of multi-player. The
authors calculate the packet urgency based on buffer status
estimation and propose a scheduling algorithm. In addition,
they also suggest an adaptive video segment request scheme,
which estimates media access control (MAC) queue as an
additional information to determine the request time interval
for each gamer, on the purpose of improving the playback
experience. Bujari et al. [2] provides a VoAP algorithm to
address the flow coexistence issue in wireless cloud gaming service delivery. This research problem is introduced by
the concurrent transmissions of TCP-based and UDP-based
streams in home scenario, where the downlink requirement of
gaming video exacerbate the operation of above mentioned
transport protocols. The authors’ solution is to dynamically
modify the advertised window, in such way the system
can limit the growth of the TCP flow’s sending rate.
Wu et al. [99] present a novel transmission scheduling framework dubbed AdaPtive HFR vIdeo Streaming (APHIS) to
address the issue in the cloud gaming video delivery through
wireless networks. The authors first propose an online video
frame selection algorithm to minimize the total distortion
based on network status, input video data, and delay constraint. Afterwards, they introduce an unequal forward error
correction (FEC) coding scheme to provide differentiated
protection for Intra (I) and Predicted (P) frames with lowlatency cost. The proposed APHIS framework is able to
appropriately filter video frames and adjust data protection
levels to optimize the quality of HFR video streaming.
Hemmati et al. [32] propose an object selection algorithm to
provide an adaptive scene rendering solution. The basic idea
is to exclude less important objects from the final output, thus
to reduce less processing time for the server to render and
encode the frames. In such a way, the cloud gaming system is
able to achieve a lower bit rate to stream the resulting video.
The proposed algorithm evaluates the importance of objects
from the game scene based on the analysis of gamers’ activities and do the selection work. Experiments demonstrate that
this approach reduces streaming bit rate by up to 8.8%.
V. COMMERCIAL CLOUD GAMING SERVICES

In addition to the technical problems discussed in prior
sections, commercialization and business models of cloud
gaming services are critical to their success. We survey the
7616

commercialization efforts starting from a short history on
cloud gaming services. G-cluster [26] starts building cloud
gaming services since early 2000’s. In particular, G-cluster
publicly demonstrated live game streaming1 over WiFi to a
PDA in 2001, and a commercial game-on-demand service
in 2004. G-cluster’s service is tightly coupled with several
third-party companies, including game developers, network
operators, and game portals. This can be partially attributed
to the less mature Internet connectivity and data centers,
which force G-cluster to rely on network QoS supports from
network operators. Ojala and Tyrvainen [66] presents the
evolution of G-cluster’s business model, and observe that the
number of G-cluster’s third-party companies is reduced over
years. The number of households having access to G-cluster’s
IPTV-based cloud gaming service increased from 15,000 to
3,000,000 between 2005 and 2010.
In late 2000’s, emerging cloud computing companies start
offering Over-The-Top (OTT) cloud gaming services, represented by OnLive [67], Gaikai [27], and GameNow [28].
OTT refers to delivering multimedia content over the Internet above arbitrary network operators to end users, which
trades QoS supports for ubiquitous access to cloud games.
OnLive [67] was made public in 2009, and was a well-known
cloud gaming service, probably because of its investors
including Warner Bros, AT&T, Ubisoft, and Atrari. OnLive
provided subscription based service, and hosted its servers
in several States within the US, to control the latency due
to geographical distances. OnLive ran into financial difficulty in 2012, and ceased operations in 2015 after selling
their patents to Sony [87]. Gaikai [27] offered cloud gaming
service using a different business model. Gaikai adopt cloud
gaming to allow gamers to try new games without purchasing
and installing software on their own machines. At the end
of each gameplay, gamers are given options to buy the game
if they like it. That is, Gaikai is more like an advertisement
service for game developers to boost their sales. Gaikai was
acquired by Sony [86] in 2012, which leads to a new cloud
gaming service from Sony, called PS Now [68] launched
in 2014. PS Now allows gamers to play PlayStation games
as cloud games, and adopts two charging models: per-game
and monthly subscription.
The aforementioned cloud gaming services can be classified in groups from two aspects. We discuss the advantages
and disadvantages of different groups in the following. First,
cloud gaming services are either: (i) integrated with underlaying networks or (ii) provided as OTT services. Tighter integration provides better QoS guarantees which potentially lead
to better user experience, while OTT reduces the expenses on
cloud gaming services at a possible risk of unstable and worse
user experience. Second, cloud gaming services adopt one of
the three charging models: (i) subscription, (ii) per-game, and
(iii) free to gamers. More specifically, cloud gaming users
pay for services in the first two charging models, while thirdparty companies, which can be game developers or network
1 At that time, the term cloud was not yet popular.
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

operators, pay for services in the third charging model. In the
future, there may be innovative ways to offer cloud gaming
services to general publics in a commercially-viable manner.

hence, we believe that we are on the edge of a new era of
a whole new cloud gaming ecosystem, which will eventually
leads to the next generation cloud gaming services.

VI. CONCLUSION AND OUTLOOK

REFERENCES

In this article, we grouped the existing cloud gaming
research into four classifications: (i) overview, (ii) platform,
(iii) optimization, and (iv) commercialization. In Section II
(overview), we included papers that introducing general and
specialized (such as mobile) cloud gaming. In Section III
(platform), we presented the basic cloud gaming platforms
that support quantitative performance measurements. More
specifically, we considered: (i) QoS evaluations, such as
energy consumption and network metrics, and (ii) QoE evaluations, such as gamer experience. In Section IV (optimization), we presented the two major optimization directions:
(i) cloud server infrastructure, such as resource allocation
and distributed architecture, and (ii) communications, such
as data compression and adaptive transmission. In Section V
(commercialization), we gave a brief history of cloud gaming
services, followed by the design decisions made by representative commercial cloud gaming services.
Cloud gaming is not a panacea and incurs non-trivial costs
to service providers. Minimizing the cost on cloud and networking resources while achieving high gamer experience
requires careful optimization like the approaches explored
in this survey. Without these optimizations, service provider
cannot consolidate enough cloud gaming users to each physical machine. This in turn leads to much lower profits, and
may drive the service provider out of business. Some early
industrial pioneers such as OnLive [67] have unfortunately
exited the market. More recent cloud gaming services such
as PS Now [68] and GameNow [28] are better optimized
and will be more competitive in the current gaming industry.
As commercial cloud gaming services become financially
sustainable, the new cloud gaming ecosystem will continue
to expand, leading to more investments and technologies to
improve these services. Much of the innovation needed to
push cloud gaming to the next level may reside in creating
new programing paradigms to support the unique needs of
these complex systems. Most current cloud gaming platforms
work as a ‘‘black box’’ simply wrapping a traditionally programed game in a support system to enable cloud gaming.
Although, the original black box model of cloud gaming has
led to many practical real world implementations a more
integrated approach may be necessary. It is likely that using
in-game contexts or whole new programing paradigms may
solve some of cloud gaming shortcomings [7]. Future cloud
gaming aware programing paradigms will help facilitate both
better user experience and resource utilization. This will
allow more innovative, yet demanding ideas to be implemented, which in turn results in critical momentum towards
building the next generation cloud gaming services.
In summary, the advances of technologies turn playable
cloud gaming services into reality; more optimization techniques gradually make cloud gaming services profitable;
VOLUME 4, 2016

[1] (Mar. 2013). asm.js. [Online]. Available: http://asmjs.org/
[2] A. Bujari, M. Massaro, and C. Palazzi, ‘‘Vegas over access point: Making
room for thin client game systems in a wireless home,’’ IEEE Trans.
Circuits Syst. Video Technol., vol. 25, no. 12, pp. 2002–2012, Dec. 2015.
[3] W. Cai, H. Chan, X. Wang, and V. Leung, ‘‘Cognitive resource optimization for the decomposed cloud gaming platform,’’ IEEE Trans. Circuits
Syst. Video Technol., vol. 25, no. 12, pp. 2038–2051, Dec. 2015.
[4] W. Cai, M. Chen, and V. C. M. Leung, ‘‘Toward gaming as a service,’’
IEEE Internet Comput., vol. 18, no. 3, pp. 12–18, May/Jun. 2014.
[5] W. Cai, Z. Hong, X. Wang, H. Chan, and V. Leung, ‘‘Quality-ofexperience optimization for a cloud gaming system with ad hoc cloudlet
assistance,’’ IEEE Trans. Circuits Syst. Video Technol., vol. 25, no. 12,
pp. 2092–2104, Dec. 2015.
[6] W. Cai, V. Leung, and L. Hu, ‘‘A cloudlet-assisted multiplayer cloud gaming system,’’ Mobile Netw. Appl., vol. 19, no. 2, pp. 144–152, Nov. 2013.
[7] W. Cai et al., ‘‘The future of cloud gaming,’’ Proc. IEEE, vol. 104, no. 4,
pp. 687–691, Apr. 2016.
[8] Y.-C. Chang, P.-H. Tseng, K.-T. Chen, and C.-L. Lei, ‘‘Understanding the
performance of thin-client gaming,’’ in Proc. IEEE Int. Workshop Tech.
Committee Commun. Quality Rel. (CQR), Naples, FL, USA, May 2011,
pp. 1–6.
[9] K.-T. Chen, Y.-C. Chang, H.-J. Hsu, D.-Y. Chen, C.-Y. Huang, and
C.-H. Hsu, ‘‘On the quality of service of cloud gaming systems,’’ IEEE
Trans. Multimedia, vol. 16, no. 2, pp. 480–495, Feb. 2014.
[10] K. Chen, Y. Chang, P. Tseng, C. Huang, and C. Lei, ‘‘Measuring the latency of cloud gaming systems,’’ in Proc. ACM Int. Conf.
Multimedia (MM), Scottsdale, AZ, USA, Nov. 2011, pp. 1269–1272.
[11] K.-T. Chen, C.-Y. Huang, and C.-H. Hsu, ‘‘Cloud gaming onward:
Research opportunities and outlook,’’ in Proc. IEEE Conf. Multimedia
Expo Workshops (ICMEW), Chengdu, China, Jul. 2014, pp. 1–4.
[12] G. Cheung, T. Sakamoto, and W. Tan, ‘‘Graphics-to-video encoding for
3G mobile game viewer multicast using depth values,’’ in Proc. IEEE Int.
Conf. Image Process. (ICIP), Singapore, Oct. 2004, pp. 2805–2808.
[13] S. Choy, B. Wong, G. Simon, and C. Rosenberg, ‘‘The brewing
storm in cloud gaming: A measurement study on cloud to end-user
latency,’’ in Proc. 11th IEEE Annu. Workshop Netw. Syst. Support
Games (NetGames), Venice, Italy, Nov. 2012, pp. 1–6.
[14] S. Choy, B. Wong, G. Simon, and C. Rosenberg, ‘‘A hybrid edge-cloud
architecture for reducing on-demand gaming latency,’’ Multimedia Syst.,
vol. 20, no. 5, pp. 503–519, Oct. 2014.
[15] S.-P. Chuah and N.-M. Cheung, ‘‘Layered coding for mobile cloud
gaming,’’ in Proc. Int. Workshop Massively Multiuser Virtual
Environ. (MMVE), Singapore, Mar. 2014, pp. 1–6.
[16] S.-P. Chuah, N.-M. Cheung, and C. Yuen, ‘‘Layered coding for mobile
cloud gaming using scalable Blinn–Phong lighting,’’ IEEE Trans. Image
Process., vol. 25, no. 7, pp. 3112–3125, Jul. 2016.
[17] S.-P. Chuah, C. Yuen, and N.-M. Cheung, ‘‘Cloud gaming: A green
solution to massive multiplayer online games,’’ IEEE Wireless Commun.,
vol. 21, no. 4, pp. 78–87, Aug. 2014.
[18] M. Claypool, ‘‘Motion and scene complexity for streaming video games,’’
in Proc. Int. Conf. Found. Digit. Games (FDG), Orlando, FL, USA,
Apr. 2009, pp. 34–41.
[19] M. Claypool and K. Claypool, ‘‘Latency and player actions in online
games,’’ Commun. ACM, vol. 49, no. 11, pp. 40–45, Nov. 2006.
[20] M. Claypool and D. Finkel, ‘‘The effects of latency on player performance
in cloud-based games,’’ in Proc. 13th Annu. Workshop Netw. Syst. Support
Games (NetGames), Nagoya, Japan, Dec. 2014, pp. 1–6.
[21] M. Claypool, D. Finkel, A. Grant, and M. Solano, ‘‘Thin to win?
Network performance analysis of the OnLive thin client game system,’’ in Proc. 11th IEEE Annu. Workshop Netw. Syst. Support
Games (NetGames), Venice, Italy, Nov. 2012, pp. 1–6.
[22] E. Depasquale et al., ‘‘An analytical method of assessment of RemoteFX
as a cloud gaming platform,’’ in Proc. IEEE Conf. Medit. Electrotech.
Conf. (MELECON), Beirut, Lebanon, Apr. 2014, pp. 127–133.
[23] S. Dey, ‘‘Cloud mobile media: Opportunities, challenges, and directions,’’
in Proc. IEEE Conf. Comput., Netw. Commun. (ICNC), Maui, HI, USA,
Feb. 2012, pp. 929–933.
7617

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

[24] (Aug. 2013). Emscripten. [Online]. Available: http://emscripten.org
[25] D. Finkel, M. Claypool, S. Jaffe, T. Nguyen, and B. Stephen, ‘‘Assignment of games to servers in the OnLive cloud game system,’’ in Proc.
Annu. Workshop Netw. Syst. Support Games (NetGames), Nagoya, Japan,
Dec. 2014, Art. no. 4.
[26] (Jan. 2015). G-Cluster. [Online]. Available: http://www.gcluster.com/eng
[27] (Jan. 2015). GaiKai. [Online]. Available: http://www.gaikai.com/
[28] (Jan. 2015). GameNow. [Online]. Available: http://www.ugamenow.com
[29] R. Hans, U. Lampe, D. Burgstahler, M. Hellwig, and R. Steinmetz,
‘‘Where did my battery go? Quantifying the energy consumption of cloud
gaming,’’ in Proc. IEEE Int. Conf. Mobile Services (MS), Anchorage, AK,
USA, Jun. 2014, pp. 63–67.
[30] M. Hassam, N. Kara, F. Belqasmi, and R. Glitho, ‘‘Virtualized infrastructure for video game applications in cloud environments,’’ in Proc.
ACM Symp. Mobility Manage. Wireless Access (MobiWac), Montreal,
QC, Canada, Sep. 2014, pp. 109–114.
[31] L. He, G. Liu, and C. Yuchen, ‘‘Buffer status and content aware scheduling scheme for cloud gaming based on video streaming,’’ in Proc. IEEE
Conf. Multimedia Expo Workshops (ICMEW), Chengdu, China, Jul. 2014,
pp. 1–6.
[32] M. Hemmati, A. Javadtalab, A. Shirehjini, S. Shirmohammadi, and
T. Arici, ‘‘Game as video: Bit rate reduction through adaptive object
encoding,’’ in Proc. ACM Workshop Netw. Oper. Syst. Support Digit.
Audio Video (NOSSDAV), Oslo, Norway, Feb. 2013, pp. 7–12.
[33] H.-J. Hong, D.-Y. Chen, C.-Y. Huang, K.-T. Chen, and C.-H. Hsu, ‘‘QoEaware virtual machine placement for cloud games,’’ in Proc. Annu.
Workshop Netw. Syst. Support Games (NetGames), Denver, CO, USA,
Dec. 2013, pp. 1–2.
[34] H.-J. Hong, D.-Y. Chen, C.-Y. Huang, K.-T. Chen, and C.-H. Hsu, ‘‘Placing virtual machines to optimize cloud gaming experience,’’ IEEE Trans.
Cloud Comput., vol. 3, no. 1, pp. 42–53, Jan. 2015.
[35] H.-J. Hong, C.-F. Hsu, T.-H. Tsai, C.-Y. Huang, K.-T. Chen, and
C.-H. Hsu, ‘‘Enabling adaptive cloud gaming in an open-source cloud
gaming platform,’’ IEEE Trans. Circuits Syst. Video Technol., vol. 25,
no. 12, pp. 2078–2091, Dec. 2015.
[36] H.-J. Hong, T.-Y. Fan-Chiang, C.-R. Lee, K.-T. Chen, C.-Y. Huang, and
C.-H. Hsu, ‘‘GPU consolidation for cloud games: Are we there yet?’’ in
Proc. 13th Annu. Workshop Netw. Syst. Support Games, Nagoya, Japan,
Dec. 2014, pp. 1–6.
[37] M. S. Hossain, G. Muhammad, B. Song, M. Hassan, A. Alelaiwi, and
A. Alamri, ‘‘Audio–visual emotion-aware cloud gaming framework,’’
IEEE Trans. Circuits Syst. Video Technol., vol. 25, no. 12, pp. 2105–2118,
Dec. 2015.
[38] C.-Y. Huang, K.-T. Chen, D.-Y. Chen, H.-J. Hsu, and C.-H. Hsu, ‘‘GamingAnywhere: The first open source cloud gaming system,’’ ACM Trans.
Multimedia Comput., Commun., Appl., vol. 10, no. 1s, pp. 10:1–10:25,
Jan. 2014.
[39] C.-Y. Huang, P.-H. Chen, Y.-L. Huang, K.-T. Chen, and C.-H. Hsu,
‘‘Measuring the client performance and energy consumption in mobile
cloud gaming,’’ in Proc. 13th Annu. Workshop Netw. Syst. Support
Games (NetGames), Nagoya, Japan, Dec. 2014, pp. 1–3.
[40] C.-Y. Huang, C.-H. Hsu, Y.-C. Chang, and K.-T. Chen, ‘‘GamingAnywhere: An open cloud gaming system,’’ in Proc. ACM Multimedia Syst.
Conf. (MMSys), Oslo, Norway, Feb. 2013, pp. 36–47.
[41] C.-Y. Huang, C.-H. Hsu, D.-Y. Chen, and K.-T. Chen, ‘‘Quantifying user
satisfaction in mobile cloud games,’’ in Proc. Workshop Mobile Video
Del. (MoVid), Singapore, Mar. 2013, pp. 4:1–4:6.
[42] M. Jarschel, D. Schlosser, S. Scheuring, and T. Hoßfeld, ‘‘Gaming in the
clouds: QoE and the users’ perspective,’’ Math. Comput. Model., vol. 57,
nos. 11–12, pp. 2883–2894, Jun. 2013.
[43] S. Jarvinen, J. P. Laulajainen, T. Sutinen, and S. Sallinen, ‘‘QoS-aware
real-time video encoding: How to improve the user experience of a
gaming-on-demand service,’’ in Proc. IEEE Consum. Commun. Netw.
Conf. (CCNC), Las Vegas, NV, USA, Jan. 2006, pp. 994–997.
[44] K. I. Kim, S. Y. Bae, D. C. Lee, C. S. Cho, H. J. Lee, and K. C. Lee,
‘‘Cloud-based gaming service platform supporting multiple devices,’’
ETRI J., vol. 35, no. 6, pp. 960–968, Dec. 2013.
[45] S. S. Kim, K. I. Kim, and J. Won, ‘‘Multi-view rendering approach
for cloud-based gaming services,’’ in Proc. Int. Conf. Adv. Future
Internet (AFIN), French Riviera, France, Aug. 2011, pp. 102–107.
[46] U. Lampe, Q. Wu, S. Dargutev, R. Hans, A. Miede, and R. Steinmetz,
‘‘Assessing latency in cloud gaming,’’ in Proc. Int. Conf. Cloud Comput.
Services Sci. (CLOSER), Barcelona, Spain, Sep. 2014, pp. 52–68.
7618

[47] J. Laulajainen, T. Sutinen, and S. Járvinen, ‘‘Experiments with QoSaware gaming-on-demand service,’’ in Proc. Int. Conf. Adv. Inf. Netw.
Appl. (AINA), Vienna, Austria, Apr. 2006, pp. 805–810.
[48] K. Lee et al., ‘‘Outatime: Using speculation to enable low-latency continuous interaction for cloud gaming,’’ in Proc. Annu. Int. Conf. Mobile
Syst., Appl., Services (MobiSys), Florence, Italy, May 2015, pp. 151–165.
[49] K. Lee, D. Chu, E. Cuervo, A. Wolman, and J. Flinn, ‘‘Demo: DeLorean:
Using speculation to enable low-latency continuous interaction for
mobile cloud gaming,’’ in Proc. Annu. Int. Conf. Mobile Syst., Appl.,
Services (MobiSys), Florence, Italy, May 2015, p. 347.
[50] Y.-T. Lee, K.-T. Chen, H.-I. Su, and C.-L. Lei, ‘‘Are all games equally
cloud-gaming-friendly? An electromyographic approach,’’ in Proc. 11th
Annu. Workshop Netw. Syst. Support Games (NetGames), Venice, Italy,
Nov. 2012, pp. 1–6.
[51] Y. Li, X. Tang, and W. Cai, ‘‘Play request dispatching for efficient
virtual machine usage in cloud gaming,’’ IEEE Trans. Circuits Syst. Video
Technol., vol. 25, no. 12, pp. 2052–2063, Dec. 2015.
[52] L. Lin et al., ‘‘LiveRender: A cloud gaming system based on compressed
graphics streaming,’’ in Proc. ACM Int. Conf. Multimedia (MM), Orlando,
FL, USA, Nov. 2014, pp. 347–356.
[53] Y. Liu, S. Dey, and Y. Lu, ‘‘Enhancing video encoding for cloud gaming
using rendering information,’’ IEEE Trans. Circuits Syst. Video Technol.,
vol. 25, no. 12, pp. 1960–1974, Dec. 2015.
[54] Y. Liu, S. Wang, and S. Dey, ‘‘Modeling, characterizing, and enhancing
user experience in cloud mobile rendering,’’ in Proc. Int. Conf. Comput.,
Netw. Commun. (ICNC), Maui, HI, USA, Jan. 2012, pp. 739–745.
[55] M. Manzano, J. A. Hernandez, M. Uruena, and E. Calle, ‘‘An empirical
study of cloud gaming,’’ in Proc. 11th IEEE Annu. Workshop Netw. Syst.
Support Games (NetGames), Venice, Italy, Nov. 2012, pp. 1–2.
[56] M. Manzano, M. Urueña, M. Sužnjević, E. Calle, J. Hernández, and
M. Matijasevic, ‘‘Dissecting the protocol and network traffic of the
OnLive cloud gaming platform,’’ Multimedia Syst., vol. 20, no. 5,
pp. 451–470, Mar. 2014.
[57] M. Marzolla, S. Ferretti, and G. D’Angelo, ‘‘Dynamic resource provisioning for cloud-based gaming infrastructures,’’ ACM Comput. Entertainment, vol. 10, no. 3, pp. 4:1–4:20, Dec. 2012.
[58] D. Meilander, F. Glinka, S. Gorlatch, L. Lin, W. Zhang, and X. Liao,
‘‘Bringing mobile online games to clouds,’’ in Proc. IEEE Conf. Comput.
Commun. Workshops (INFOCOMW), Toronto, ON, Canada, Apr. 2014,
pp. 340–345.
[59] D. Mishra, M. El Zarki, A. Erbad, C.-H. Hsu, and N. Venkatasubramanian, ‘‘Clouds+Games: A multifaceted approach,’’ IEEE Internet
Comput., vol. 18, no. 3, pp. 20–27, May 2014.
[60] S. Möller, D. Pommer, J. Beyer, and J. Rake-revelant, ‘‘Factors influencing gaming QoE: Lessons learned from the evaluation of cloud gaming
services,’’ in Proc. Int. Workshop Perceptual Quality Syst. (PQS), Vienna,
Austria, Sep. 2013, pp. 1–5.
[61] C. Moreno, N. Tizon, and M. Preda, ‘‘Mobile cloud convergence in
GaaS: A business model proposition,’’ in Proc. Hawaii Int. Conf. Syst.
Sci. (HICSS), Maui, HI, USA, Jan. 2012, pp. 1344–1352.
[62] (Mar. 2010). Welcome to Native Client. [Online]. Available:
https://developer.chrome.com/native-client
[63] (Mar. 2010). Google’s Native Client Goes ARM and Beyond. [Online].
Available: http://www.h-online.com/open/news/item/Google-s-NativeClient-goes-ARM-and-beyond-957478.html
[64] X. Nan et al., ‘‘A novel cloud gaming framework using joint video and
graphics streaming,’’ in Proc. IEEE Int. Conf. Multimedia Expo (ICME),
Chengdu, China, Jul. 2014, pp. 1–6.
[65] (May 2016). NVidia Grid. [Online]. Available: http://www.nvidia.com/
object/cloud-gaming.html
[66] A. Ojala and P. Tyrvainen, ‘‘Developing cloud business models: A case
study on cloud gaming,’’ IEEE Softw., vol. 28, no. 4, pp. 42–47, Jul. 2011.
[67] (Jan. 2015). OnLive. [Online]. Available: http://www.onlive.com/
[68] (Jan. 2015). PlayStation Now. [Online]. Available: http://
www.playstation.com/en-us/explore/playstationnow/
[69] S. Prabu and S. Purushotham, ‘‘Cloud gaming with P2P network
using XAML and Windows Azure,’’ in Proc. Conf. Recent Trends
Comput., Commun. Inf. Technol. (ObCom), Vellore, India, Dec. 2011,
pp. 165–172.
[70] Z. Qi, J. Yao, C. Zhang, M. Yu, Z. Yang, and H. Guan, ‘‘VGRIS:
Virtualized GPU resource isolation and scheduling in cloud gaming,’’ ACM Trans. Archit. Code Optim., vol. 11, no. 2, pp. 203–214,
Jul. 2014.
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

[71] P. Quax, A. Beznosyk, W. Vanmontfort, R. Marx, and W. Lamotte,
‘‘An evaluation of the impact of game genre on user experience in cloud
gaming,’’ in Proc. IEEE Int. Games Innov. Conf. (IGIC), Vancouver, BC,
Canada, Sep. 2013, pp. 216–221.
[72] K. Raaen, R. Eg, and C. Griwodz, ‘‘Can gamers detect cloud delay?’’ in
Proc. Annu. Workshop Netw. Syst. Support Games (NetGames), Nagoya,
Japan, Dec. 2014, pp. 1–3.
[73] L. Riungu-kalliosaari, J. Kasurinen, and K. Smolander, ‘‘Cloud services
and cloud gaming in game development,’’ in Proc. IADIS Game Entertainment Technol. (GET), Prague, Czech Republic, Jul. 2013, pp. 1–10.
[74] P. E. Ross, ‘‘Cloud computing’s killer app: Gaming,’’ IEEE Spectr.,
vol. 46, no. 3, p. 14, Mar. 2009.
[75] (Nov. 2014). Cloud Gaming to Reach Inflection Point in 2015. [Online].
Available: http://tinyurl.com/p3z9hs2
[76] M. Semsarzadeh, M. Hemmati, A. Javadtalab, A. Yassine, and
S. Shirmohammadi, ‘‘A video encoding speed-up architecture for
cloud gaming,’’ in Proc. IEEE Int. Conf. Multimedia Expo Workshops,
Chengdu, China, Jul. 2014, pp. 1–6.
[77] M. Semsarzadeh, A. Yassine, and S. Shirmohammadi, ‘‘Video encoding
acceleration in cloud gaming,’’ IEEE Trans. Circuits Syst. Video Technol.,
vol. 25, no. 12, pp. 1975–1987, Dec. 2015.
[78] R. Shea, D. Fu, and J. Liu, ‘‘Cloud gaming: Understanding the support
from advanced virtualization and hardware,’’ IEEE Trans. Circuits Syst.
Video Technol., vol. 25, no. 12, pp. 2026–2037, Dec. 2015.
[79] R. Shea, D. Fu, and J. Liu, ‘‘Towards bridging online game playing and
live broadcasting: Design and optimization,’’ in Proc. ACM Workshop
Netw. Oper. Syst. Support Digit. Audio Video (NOSSDAV), Portland, OR,
USA, Mar. 2015, pp. 61–66.
[80] R. Shea and J. Liu, ‘‘On GPU pass-through performance for cloud gaming: Experiments and analysis,’’ in Proc. Annu. Workshop Netw. Syst.
Support for Games (NetGames’13), p. 6:1–6:6, Denver, CO, Dec. 2013.
[81] R. Shea, J. Liu, E. C.-H. Ngai, and Y. Cui, ‘‘Cloud gaming: Architecture
and performance,’’ IEEE Netw., vol. 27, no. 4, pp. 16–21, Jul./Aug. 2013.
[82] S. Shi, C.-H. Hsu, K. Nahrstedt, and R. Campbell, ‘‘Using graphics
rendering contexts to enhance the real-time video coding for mobile cloud
gaming,’’ in Proc. ACM Multimedia (MM), Nov. 2011, pp. 103–112.
[83] T. Kämäräinen, M. Siekkinen, Y. Xiao, and A. Ylä-Jääski, ‘‘Towards
pervasive and mobile gaming with distributed cloud infrastructure,’’ in
Proc. Annu. Workshop Netw. Syst. Support Games (NetGames), Nagoya,
Japan, Dec. 2014, pp. 1–6.
[84] I. Slivar, M. Suznjevic, L. Skorin-Kapov, and M. Matijasevic, ‘‘Empirical
QoE study of in-home streaming of online games,’’ in Proc. Annu. Workshop Netw. Syst. Support Games (NetGames), Nagoya, Japan, Dec. 2014,
pp. 1–6.
[85] O. Soliman, A. Rezgui, H. Soliman, and N. Manea, ‘‘Mobile cloud
gaming: Issues and challenges,’’ in Proc. Int. Conf. Mobile Web Inf.
Syst. (MobiWIS), Paphos, Cyprus, Aug. 2013, pp. 121–128.
[86] (Jul. 2012). Cloud Gaming Adoption is Accelerating . . . and Fast!
[Online]. Available: http://www.nttcom.tv/2012/07/09/cloud-gamingadoption-is-acceleratingand-fast/
[87] (Apr. 2015). Sony buys OnLive Streaming Game Service, Which Will Shut
Down Later This Month. [Online]. Available: http://goo.gl/6Xe9Dx
[88] K. Sun and D. Wu, ‘‘Video rate control strategies for cloud gaming,’’
J. Vis. Commun. Image Represent., vol. 30, pp. 234–241, Jul. 2015.
[89] M. Suznjevic, J. Beyer, L. Skorin-Kapov, S. Moller, and N. Sorsa,
‘‘Towards understanding the relationship between game type and network
traffic for cloud gaming,’’ in Proc. IEEE Int. Conf. Multimedia Expo
Workshops (ICMEW), Chengdu, China, Jul. 2014, pp. 1–6.
[90] R. Süselbeck, G. Schiele, and C. Becker, ‘‘Peer-to-peer support for lowlatency massively multiplayer online games in the cloud,’’ in Proc. 8th
Annu. Workshop Netw. Syst. Support Games (NetGames), Paris, France,
Nov. 2009, pp. 1–2.
[91] M. R. Hosseinzadeh Taher, H. Ahmadi, and M. R. Hashemi,
‘‘Power-aware analysis of H.264/AVC encoding parameters
for cloud gaming,’’ in Proc. IEEE Int. Conf. Multimedia Expo
Workshops (ICMEW), Chengdu, China, Jul. 2014, pp. 1–6.
[92] H. Tian, D. Wu, J. He, Y. Xu, and M. Chen, ‘‘On achieving cost-effective
adaptive cloud gaming in geo-distributed data centers,’’ IEEE Trans.
Circuits Syst. Video Technol., vol. 25, no. 12, pp. 2064–2077, Dec. 2015.
[93] (Jan. 2015). Ubitus. [Online]. Available: http://www.ubitus.net
[94] S. Wang and S. Dey, ‘‘Modeling and characterizing user experience in
a cloud server based mobile gaming approach,’’ in Proc. IEEE Global
Telecommun. Conf. (GLOBECOM), Honolulu, HI, USA, Nov. 2009,
pp. 1–7.
VOLUME 4, 2016

[95] S. Wang and S. Dey, ‘‘Addressing response time and video quality in
remote server based Internet mobile gaming,’’ in Proc. IEEE Wireless
Commun. Netw. Conf., Sydney, NSW, Australia, Apr. 2010, pp. 1–6.
[96] S. Wang and S. Dey, ‘‘Rendering adaptation to address communication
and computation constraints in cloud mobile gaming,’’ in Proc. IEEE
Global Telecommun. Conf. (GLOBECOM), Miami, FL, USA, Dec. 2010,
pp. 1–6.
[97] S. Wang and S. Dey, ‘‘Cloud mobile gaming: Modeling and measuring user experience in mobile wireless networks,’’ ACM Trans. Mobile
Comput. Commun. Rev., vol. 16, no. 1, pp. 10–21, Jan. 2012.
[98] S. Wang, Y. Liu, and S. Dey, ‘‘Wireless network aware cloud scheduler for
scalable cloud mobile gaming,’’ in Proc. IEEE Int. Conf. Commun. (ICC),
Ottawa, ON, Canada, Jun. 2012, pp. 2081–2086.
[99] J. Wu, C. Yuen, N.-M. Cheung, J. Chen, and C. W. Chen, ‘‘Enabling
adaptive high-frame-rate video streaming in mobile cloud gaming applications,’’ IEEE Trans. Circuits Syst. Video Technol., vol. 25, no. 12,
pp. 1988–2001, Dec. 2015.
[100] Z. Wu, ‘‘Gaming in the cloud: One of the future entertainment,’’ in Proc.
Interact. Multimedia Conf., Southampton, U.K., Jan. 2014, pp. 1–6.
[101] L. Xu, X. Guo, Y. Lu, S. Li, O. Au, and L. Fang, ‘‘A low latency cloud
gaming system using edge preserved image homography,’’ in Proc. IEEE
Int. Conf. Multimedia Expo (ICME), Chengdu, China, Jul. 2014, pp. 1–6.
[102] Z. Xue, D. Wu, J. He, X. Hei, and Y. Liu, ‘‘Playing high-end video games
in the cloud: A measurement study,’’ IEEE Trans. Circuits Syst. Video
Technol., vol. 25, no. 12, pp. 2013–2025, Dec. 2015.
[103] C. Zhang, Z. Qi, J. Yao, M. Yu, and H. Guan, ‘‘vGASA: Adaptive scheduling algorithm of virtualized GPU resource in cloud gaming,’’ IEEE Trans.
Parallel Distrib. Syst., vol. 25, no. 11, pp. 3036–3045, Nov. 2014.
[104] Z. Zhao, K. Hwang, and J. Villeta, ‘‘Game cloud design with
virtualized CPU/GPU servers and initial performance results,’’ in Proc.
Workshop Sci. Cloud Comput. Date (ScienceCloud), Delft,
The Netherlands, Jun. 2012, pp. 23–30.

WEI CAI [S’12–M’16] received the B.Eng. degree
in software engineering from Xiamen University
in 2008, the M.S. degree in electrical engineering
and computer science from Seoul National University in 2011, and the Ph.D. degree in electrical
and computer engineering from The University of
British Columbia (UBC), Vancouver, BC, Canada,
in 2016. He is currently a Post-Doctoral Research
Fellow with the UBC. He has completed visiting
researches with Academia Sinica, The Hong Kong
Polytechnic University, and National Institute of Informatics, Japan. His
researches focus on gaming as a service, mobile cloud computing, online
gaming, software engineering, and interactive multimedia. He received
awards of the 2015 Chinese Government Award for the Outstanding SelfFinanced Students Abroad, UBC Doctoral Four-Year-Fellowship, Brain
Korea 21 Scholarship, and Excellent Student Scholarship from the Bank
of China. He is also a co-recipient of the best paper awards from the
CloudCom2014, the SmartComp2014, and the CloudComp2013.

RYAN SHEA [S’08–M’16] received the
Ph.D. degree in computer science from Simon
Fraser University, Burnaby, BC, Canada, in 2016.
He is currently a University Research Associate
with the Big Data Systems. His research interests
include computer and network virtualization, performance issues in cloud computing, and energy
and performance issues with the Big Data Systems. He has received the Natural Sciences and
Engineering Research Council of Canada, Alexander Graham Bell Canada Graduate Scholarship in 2013. He was a recipient
of the best student paper award from the IEEE/ACM 21st International
Workshop on Quality of Service for his paper Understanding the Impact
of Denial of Service Attacks on Virtual Machines in 2012. His recent
publications include a point of view article in the Proceedings of the IEEE
entitled The Future of Cloud Gaming.
7619

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

CHUN-YING HUANG [S’03–M’08] received the
Ph.D. degree in electrical engineering department from National Taiwan University in 2007.
He leads the Security and Systems Laboratory
with National Chiao Tung University. From 2008
to 2013, he was an Assistant Professor with the
Department of Computer Science and Engineering, National Taiwan Ocean University, where he
was an Associate Professor from 2013 to 2016.
He has been an Associate Professor with the
Department of Computer Science, National Chiao Tung University, since
2016. His research interests include system security, multimedia networking,
and mobile computing. He is a member of ACM.
KUAN-TA CHEN [S’04–M’06–SM’15] received
the B.S. and M.S. degrees in computer science
from National Tsing-Hua University, in 1998 and
2000, respectively, and the Ph.D. degree in electrical engineering from National Taiwan University
in 2006. He is a Research Fellow with the Institute
of Information Science, and the Research Center for Information Technology Innovation (joint
appointment) of Academia Sinica. His research
interests include quality of experience, multimedia
systems, and social computing. He received the best paper award in the
IWSEC 2008 and the K. T. Li Distinguished Young Scholar Award from
ACM Taipei/Taiwan Chapter in 2009. He also received the Outstanding
Young Electrical Engineer Award from The Chinese Institute of Electrical
Engineering in 2010, the Young Scholar’s Creativity Award from Foundation
for the Advancement of Outstanding Scholarship in 2013, and the IEEE
ComSoc MMTC Best Journal Paper Award in 2014. He was an Associate
Editor of IEEE TRANSACTIONS ON MULTIMEDIA from 2011 to 2014 and has
been an Associate Editor of ACM Transactions on Multimedia Computing,
Communications, and Applications since 2015. He is a Senior Member
of ACM.
JIANGCHUAN LIU [S’01–M’03–SM’08] received
the B.Eng. (cum laude) degree in computer science
from Tsinghua University, Beijing, China, in 1999,
and the Ph.D. degree in computer science from The
Hong Kong University of Science and Technology
in 2003.
He was an Assistant Professor with The Chinese
University of Hong Kong from 2003 to 2004.
He is a University Professor with the School
of Computing Science, Simon Fraser University,
British Columbia, Canada, and an NSERC E.W.R. Steacie Memorial Fellow.
He is an EMC-Endowed Visiting Chair Professor of Tsinghua University,
Beijing, from 2013 to 2016. His research interests include multimedia
systems and networks, cloud computing, social networking, online gaming,
big data computing, wireless sensor networks, and peer-to-peer networks.
Dr. Liu is a co-recipient of the inaugural Test of Time Paper
Award of the IEEE INFOCOM (2015), the ACM SIGMM TOMCCAP
Nicolas D. Georganas best paper award (2013), the ACM Multimedia
best paper award (2012), the IEEE Globecom best paper award (2011),
and the IEEE Communications Society best paper award on Multimedia
Communications (2009). His students received the Best Student Paper Award
of the IEEE/ACM IWQoS in 2008 and 2012. He has served on the editorial
boards of the IEEE TRANSACTIONS ON BIG DATA, the IEEE TRANSACTIONS ON
MULTIMEDIA, the IEEE COMMUNICATIONS SURVEYS AND TUTORIALS, the IEEE
ACCESS, the IEEE INTERNET OF THINGS JOURNAL, Computer Communications,
and Wiley Wireless Communications and Mobile Computing. He is the
Steering Committee Chair of the IEEE/ACM IWQoS (2015–2017) and
TPC Co-Chair of the IEEE IC2E’2017, and the IEEE/ACM IWQoS’2014.
He serves as an Area Chair of the IEEE INFOCOM, ACM Multimedia, and
the IEEE ICME. According to Google Scholar, the citations of his papers are
over 10,000, and as an h-index of 46.

7620

VICTOR C. M. LEUNG [S’75–M’89–SM’97–
F’03] is a Professor of Electrical and Computer
Engineering and holder of the TELUS Mobility
Research Chair with The University of British
Columbia (UBC). His research is in the areas of
wireless networks and mobile systems. He has
co-authored over 900 technical papers in archival
journals and refereed conference proceedings, several of which had received best paper awards.
He is a fellow of the Royal Society of Canada,
the Canadian Academy of Engineering, and the Engineering Institute
of Canada. He is serving or has served on the editorial boards of the
IEEE ACCESS, the IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS –
SERIES ON GREEN COMMUNICATIONS AND NETWORKING, the IEEE WIRELESS
COMMUNICATIONS LETTERS, the IEEE TRANSACTIONS ON COMPUTERS, the IEEE
TRANSACTIONS ON WIRELESS COMMUNICATIONS, and the IEEE TRANSACTIONS
ON VEHICULAR TECHNOLOGY, and several other journals. He has provided
leadership to the technical program committees and organizing committees
of numerous international conferences. He was the recipient of the 1977
APEBC Gold Medal, NSERC Postgraduate Scholarships from 1977 to 1981,
2012 UBC Killam Research Prize, and the IEEE Vancouver Section Centennial Award.

CHENG-HSIN HSU [S’09–M’10–SM’16] received
M.S./B.S. degrees from National Chung-Cheng
University, the M.Eng. degree from the University
of Maryland, and the Ph.D. degree from Simon
Fraser University. He was with the Deutsche
Telekom Laboratory, Motorola Inc., and Lucent
Technologies. He has been an Associate Professor with the Department of Computer Science,
National Tsing Hua University, since 2014, where
he was an Assistant Professor from 2011 to 2014.
He visited the University of California at Irvine, Qatar Computing Research
Institute, and University of Illinois Urbana–Champaign, in 2013, 2014, and
2015, respectively. His research interests are in multimedia networking,
mobile computing, and computer networks. He has been served as an
Associate Editor of ACM Transactions on Multimedia Computing, Communications, and Applications since 2014, and the IEEE MMTC E-LETTER
from 2012 to 2014. He and his colleagues received the best paper award
from the IEEE RTAS’12, the TAOS best paper award from the IEEE
GLOBECOM’12, best paper award from the IEEE Innovation’08, and the
Best Demo Award from the ACM Multimedia’08.

VOLUME 4, 2016

Received May 30, 2016, accepted June 13, 2016, date of publication August 8, 2016, date of current version November 18, 2016.
Digital Object Identifier 10.1109/ACCESS.2016.2590500

A Survey on Cloud Gaming: Future
of Computer Games
WEI CAI1 , (Member, IEEE), RYAN SHEA2 , (Member, IEEE),
CHUN-YING HUANG3 , (Member, IEEE), KUAN-TA CHEN4 , (Senior Member, IEEE),
JIANGCHUAN LIU2 , (Senior Member, IEEE), VICTOR C. M. LEUNG1 , (Fellow, IEEE),
AND CHENG-HSIN HSU5 , (Senior Member, IEEE)

1 Department

of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC V6T 1Z4, Canada
of Computing Science, Simon Fraser University, Burnaby, BC V5A 1S6, Canada
of Computer Science, National Chiao Tung University, Hsinchu 300, Taiwan
4 Institute of Information Science, Academia Sinica, Taipei 115, Taiwan
5 Department of Computer Science, National Tsing Hua University, Hsinchu 300, Taiwan
2 School

3 Department

Corresponding author: C.-H. Hsu (chsu@cs.nthu.edu.tw)
This work was supported in part by the University of British Columbia through the Four Year Doctoral Fellowship, in part by the Natural
Sciences and Engineering Research Council of Canada under Grant STPGP 447524, in part by the National Natural Science Foundation of
China under Grant 61271182, in part by the Ministry of Science and Technology of Taiwan under Grant 102-2221-E-007-062-MY3,
Grant 103-2221-E-009-230-MY2, Grant 104-2221-E-009-200-MY3, and Grant 103-2221-E-001-023-MY2, in part by an NSERC E.W.R.
Steacie Memorial Fellowship, in part by an NSERC Strategic Project Grant, and in part by a Discovery Grant.

ABSTRACT Cloud gaming is a new way to deliver high-quality gaming experience to gamers anywhere and
anytime. In cloud gaming, sophisticated game software runs on powerful servers in data centers, rendered
game scenes are streamed to gamers over the Internet in real time, and the gamers use light-weight software
executed on heterogeneous devices to interact with the games. Due to the proliferation of high-speed
networks and cloud computing, cloud gaming has attracted tremendous attentions in both the academia and
industry since late 2000’s. In this paper, we survey the latest cloud gaming research from different aspects,
spanning over cloud gaming platforms, optimization techniques, and commercial cloud gaming services.
The readers will gain the overview of cloud gaming research and get familiar with the recent developments
in this area.
INDEX TERMS Clouds, distributed computing, video coding, quality of service, computer graphics.

I. INTRODUCTION

Cloud gaming refers to a new way to deliver computer
games to users, where computationally complex games are
executed on powerful cloud servers, the rendered game
scenes are streamed over the Internet to gamers with thin
clients on heterogeneous devices, and the control events from
input devices are sent back to cloud servers for interactions.
Figure 1 presents how cloud gaming services work. In the
cloud, a cloud gaming platform is hosted on cloud servers
in one or multiple data centers. The cloud gaming platform
runs computer game programs, which can be roughly divided
into two major components: (i) game logic that is responsible
to convert gamer commands into in-game interactions, and
(ii) scene renderer that generates game scenes in real-time.
The gamer commands come from the command interpreter,
and the game scenes are captured by video capturer into
videos, which are then compressed by video encoder. The
command interpreter, video capturer, and video encoder are
VOLUME 4, 2016

all implemented as parts of the cloud gaming platform.
As shown in this figure, the cloud gaming platform sends the
video frames to, and receives user inputs from thin clients
used by gamers for playing games. It is a thin client, because
only two low-complexity components are required: (i) command receiver, which connects to the game controllers, such
as gampads, joysticks, keyboards, and mouses, and (ii) video
decoder, which can be realized using massively produced
(inexpensive) decoder chips. The communications between
the cloud game platform and thin clients are over the besteffort Internet, which in turn makes supporting real-time
computer games quite challenging.
In late 2000’s, we started to see cloud gaming services
offered by startups, such as OnLive [67], Gaikai [27],
G-cluster [26], and Ubitus [93]. We also witnessed that Gaikai
was acquired by SONY, which is a major game console developer [86]. This was followed by the competition between
Sony’s PlayStation Now (PS Now) [68] and Nvidia’s Grid

2169-3536 
 2016 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

7605

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

FIGURE 1. Typical cloud gaming services.

Game Streaming Service [65], which further heats up the
cloud gaming market. In fact, a 2014 report from Strategy
Analytics [75] indicates that the number of cloud gaming
users increases from 30 millions in 2014 to 150 millions
in 2015. The same report also predicts that other leading
game console manufactures will soon join the cloud gaming
market.
The tremendous popularity of cloud gaming may be
attributed to several potential advantages to gamers, game
developers, and service providers. For gamers, cloud gaming
enables them to: (i) have access to their games anywhere and
anytime, (ii) purchase or rent games on-demand, (iii) avoid
regularly upgrading their hardware, and (iv) enjoy unique features such as migrating across client computers during game
sessions, observing ongoing tournaments, and sharing game
replays with friends. For game developers, cloud gaming
allows them to: (i) concentrate on a single platform, which in
turn reduces the porting and testing costs, (ii) bypass retailers
for higher profit margins, (iii) reach out to more gamers, and
(iv) avoid piracy as the game software is never downloaded
to client computers. For service providers, cloud gaming:
(i) leads to new business models, (ii) creates more demands on
already-deployed cloud resources, and (iii) demonstrates the
potential of other/new remote execution applications, since
cloud gaming imposes the strictest constraints on various
computing and networking resources.
Despite the great opportunities of cloud gaming,
several crucial challenges must be addressed by the research
community before it reaches its full potentials to attract more
gamers, game developers, and service providers. We summarize the most important aspect as follows. First, cloud gaming
platforms and testbeds must be built up for comprehensive
7606

performance evaluations. The evaluations include measurements on Quality of Service (QoS) metrics, such as energy
consumption and network metrics, and Quality of Experience (QoE) metrics, such as gamer perceived experience.
Building platforms and testbeds, designing the test scenarios,
and carrying out the evaluations, require significant efforts,
while analyzing the complex interplay between QoS and QoE
metrics is even more difficult.
Second, the resulting platforms and evaluation procedures
allow the research community to optimize various components, such as cloud servers and communication channels.
More specifically, optimization techniques for: (i) better
resource allocation and distributed architecture are possible
at cloud servers, and (ii) optimal content coding and adaptive
transmissions are possible in communication channels.
Third, computer games are of various game genres [19].
These genres can be categorized on the basis of two elements:
viewpoint and theme. Viewpoint is how a gamer observe
the game scene. It determines the variability of rendered
video on the screen. Most commonly seen viewpoints include
first-person, second-person, third-person, and omnipresent.
First-person games adopt graphical perspectives rendered
from the viewpoint of the in-game characters, such as in
Counter-Strike. Second-person games are rendered from the
back of the in-game characters, so that gamers can see the
characters on the screen, like in Grand Theft Auto. Thirdperson games fix the gamers’ views on 3D scenes, projected
onto 2D spaces. Modern third-person games usually adopts
the sky view, also known as God view. Classic third-person
games include Diablo, Command & Conquer, FreeStyle,
and etc. Last, omnipresent enables gamers to fully control
views on the region of interest (RoI) from different angels
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

and distances. Many recent war games, e.g., Age of
Empires 3, Stronghold 2, and Warcraft III, fall into this category. Game theme determines how gamers interact with game
content. Common themes include shooting, fighting, sports,
turn-based role-playing (RPG), action role-playing (ARPG),
turn-based strategy, real-time strategy (RTS), and management simulation. Although the viewpoint may be restricted by
game theme, but generally a game genre can be describe by
a pair of viewpoint and theme, such as first-person shooting,
third-person ARPG, omnipresent RTS, and etc. Among them,
fast-paced first-person shooting games impose the highest
scene complexity, which are the most challenging games
for cloud gaming service providers. In contrast, third-person
turn-based RPG games are least sensitive to delays and thus
more suitable for cloud gaming.
Cloud gaming is an exciting research area and the existing
literature aims to address several aforementioned challenges.
Nonetheless, to the best of our knowledge, there is no comprehensive survey on cloud gaming research. The lack of
a central survey of existing literature may delay or even
prevent researchers, who are interested in cloud gaming or
other remote execution applications, from joining the community. A thorough understanding and exploration of existing
academic and industrial research and development can help
lead to the building of future cloud gaming platforms. One
such advance might come from future games being designed
specifically with cloud gaming functionalities and supports in
mind. How we accomplish this is still an open question, for
example game developers could create cloud gaming aware
contexts or even whole new programing paradigms. With this
in mind, we carefully connect existing research on solving
current challenges together, and come up with a classification
system described below.
A. SCOPE AND CLASSIFICATIONS

In the current article, we survey the cloud gaming literature. We first collect representative cloud gaming papers, and
group them into several classifications. We emphasize that
only a selective set of papers are surveyed, in order to give
the readers better understanding on the landscape of the cloud
gaming research. Upon selecting the representative papers,
we propose a classification system as summarized in Figure 2.
More details on the classification system follow.
1) Cloud Gaming Overview (Section II): We survey
the overview, introductory, and positioning papers on
either general cloud gaming, or specialized topics,
such as mobile cloud gaming and Game-as-a-Service
(GaaS).
2) Cloud Gaming Platforms (Section III): We
consider papers that construct basic cloud gaming
platforms, which support different performance evaluation methodologies. These studies can be further
categorized into three groups: system integration, QoS
evaluations, and QoE evaluations.
a) System Integration (Section III-A): The fundamental step of cloud gaming research, like many
VOLUME 4, 2016

other systems areas, is to put up basic platforms,
based on existing tools. We summarize such
system integration efforts, which serve as cornerstones of related research.
b) Quality of Service Evaluations (Section III-B):
We survey the studies on objective metric evaluations, which algorithmically quantify the system
performance, i.e., without subject assessments.
Existing papers focus on two types of objective metrics, Energy Consumption and Network
Metrics. The energy consumption is critical to
mobile cloud gaming clients, in order to prolong the precious battery life. There are several
network metrics affecting the gamer experience,
and interaction latency is a representative network metric. The interaction latency refers to the
time difference between a gamer input and the
corresponding game scene update on the client
computer. Because gamers are highly sensitive to
interaction latency [19], its measurement methodologies draw a lot of attentions in the literature.
c) Quality
of
Experience
Evaluations
(Section III-C): We discuss the papers on subjective metric evaluations, which are based on user
studies, where subject gamers give opinion scores
to their cloud gaming experience. Conducting
user studies is inherently expensive and tedious,
and thus most QoE studies attempt to analyze the
relationship between the QoS and QoE metrics.
The resulting models may in turn be used to
optimize cloud gaming platforms.
3) Optimizing Cloud Gaming Platforms (Section IV):
We consider papers that optimize cloud gaming platforms from specific aspects; usually each work focuses
on optimizing one or a few components. Such studies
can be further categorized into two groups: cloud server
infrastructure and communications.
a) Cloud Server Infrastructure (Section IV-A):
The existing studies on optimizing cloud server
infrastructure are surveyed. Several papers study
the Resource Allocation problem of server and
network resources among multiple data centers,
server nodes, and game clients to optimize the
overall cloud gaming experience, where diverse
criteria are considered. Other papers optimize
the Distributed Architectures of cloud gaming
platforms, e.g., using Peer-to-Peer (P2P) overlays
or multi-tier clouds for better performance and
scalability.
b) Communications (Section IV-B): We survey
the existing work on optimizing the efficiency of
content streaming over the dynamic and heterogeneous communication channels. These studies
are further classified into two groups. First,
several papers consider the problem of Data
Compression, e.g., layered coding and graphics
7607

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

FIGURE 2. Our proposed classification system of cloud gaming papers.

compression are proposed, which may outperform the conventional 2D image compression in
certain environments. Second, there are papers
on Adaptive Transmission, which cope with the
network dynamics by continuously changing various parameters, such as encoding bitrate, frame
rate, and image resolution. The same adaptive
transmissions may also be used to absorb the
negative impacts due to insufficient resources on
cloud servers and game clients.
4) Commercial Cloud Gaming Services (Section V):
We survey the representative commercial cloud gaming services, and classify them along different aspects.
We also discuss the advantages and disadvantage of
different cloud gaming services.
In the rest of this article, we survey the four classes of
papers in Sections II–V. They are followed by Section VI,
which concludes the survey.
II. CLOUD GAMING OVERVIEW PAPERS

As a promising cloud service provisioning paradigm, cloud
gaming has attracted interests from prominent research teams
all over the world. These teams have shared their thoughts
7608

and ideas on cloud gaming from their viewpoints in several
high-level overview papers. In this section, we survey and
summarize the representative papers along this direction. Our
concise summary puts readers into the context of cloud gaming research, while interested readers may find new research
directions in the surveyed overview papers.
Ross [74] is the first literature that introduces the cloud
gaming model to the academia in 2009, nine years after the
G-cluster’s demonstration of cloud gaming technology at E3.
The author describes gaming as cloud computing’s killer app
and depicts the blueprint of novel gaming delivery paradigm,
proposed by Advanced Micro Devices (AMD), which renders
games’ scene videos, compresses them, and transmit them to
the gamers through the Internet. This approach enables online
gamers to offload their graphic rendering tasks to the cloud,
thus, eliminates the computational workload on gamers’ local
platforms. This is the most popular definition of cloud gaming
adopted by most of the research work in this area. However,
a recent publication [59] provides a more general definition,
by envisioning the cloud gaming system as a novel computer
architecture that leverages cloud resources to improve gaming
performance, such as rendering, response time, precision and
fairness. The authors distribute system workload to multiple
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

cloud servers and game clients to enable this vision. For a
further step, Cai et al. [4] explore the essence of cloud games
as inter-dependent components, thus, define cloud gaming as
utilizing cloud resources to host gaming components, thereby
reducing workload at gamers’ local platforms and increasing the overall system performance. According to different
integration approach of the cloud, the authors identify and
discuss the research directions of three cloud gaming architectures, which are Remote Rendering, Local Rendering, and
Cognitive Resource Allocation.
After the official launch of OnLive in March 2010,
the business model for cloud gaming becomes a hot topic
in research society. Riungu-kalliosaari et al. [73] conduct
interviews in small and medium size gaming companies to
qualitatively study the adoption dynamics of cloud computing. With grounded theory method, the authors observe that
the concept of cloud gaming are relatively well-known in the
industry, while gaming organizations still hesitate in adopting
cloud computing services and technologies due to the lack of
clear business models and success stories. To this end, Ojala
and Tyrvainen [66] start their investigations on developing
business models for cloud gaming services. As a case study
for Software as a Service (SaaS), the authors select G-cluster,
one of famous cloud gaming companies, and study its business model over five years from 2005 to 2010. They conclude
that, over time, the business model in cloud gaming becomes
simpler and has fewer actors, which increases the revenue
per gamer. In addition, they also expect the cloud gaming
solution will make illegal copying practically impossible.
Another work [61] considers the convergence of mobile cloud
in gaming industry from a business model proposition. The
authors discuss the first sketch of a possible business model of
Kusanagi project, a proposed end-to-end infrastructure, from
domains of service, technology, organization, and financial,
while compare these domains of three cloud examples, i.e.,
G-cluster, Gaikai, and OnLive.
During the decade of development, there have been cloud
gaming systems and services in the market. A number of
positioning papers consider these systems and envision the
opportunities, challenges, and directions in this area. The
literature, e.g., [4], [11], [17], [23], [59], [85], [100], covers
both the commercial and academic platforms, while their
concerns of open issues are greatly overlapped in the topics of
response time minimization, graphical video encoding, network aware adaption, QoE optimization, and cloud resource
management.
Besides of these common focuses, each research team
has particular interests and directions. Dey [23] concentrate
on developing device aware scalable applications, which
involve open issues of extending cloud to wireless networks.
Soliman et al. [85] briefly discuss related legal issues, including patents, ownership concerns, guaranteed service levels,
and pricing schemes. In contrast, piracy and hacking may no
long be an issue, since the executable game program will
not be delivered to the gamers. Wu [100] explores cloud
gaming architecture from the aspect of cloud computing’s
VOLUME 4, 2016

three layers, i.e., IaaS, SaaS and PaaS. The author identifies security as a potential challenge in cloud gaming, especially data protection and location. Cai et al. [4] investigate
the features of different game genres and identify their
impact on cloud gaming system design. In addition, they
provide a vision on GaaS provisioning for mobile devices.
Mishra et al. [59] explain how to enhance the quality of
online gaming by integrating techniques from cloud gaming
research communities. Featured topics include the interplay
between QoS and QoE metrics, game models, and cloud
expansion. Chen et al. [11] point out some unique research
directions in cloud gaming, such as game integration, visualization, user interface, server selection, and resource scheduling. Chuan et al. [17] study cloud gaming from a green media
perspective. They discuss the major cloud gaming subsystems with green designs, which include a cloud data centre,
graphics rendering modules, video compression techniques,
and network delivery methods.
In addition to these high-level studies, more cloud gaming
papers focus on individual research problems. We divide
them into several classifications and survey them in the
following sections.
III. CLOUD GAMING PLATFORMS

This section presents the work related to cloud gaming platforms in three steps: (i) integrated cloud gaming platforms
for complete prototype systems, (ii) measurement studies on
QoS metrics, and (iii) measurement studies on QoE metrics.
A. SYSTEM INTEGRATION

Providing an easy-to-use platform for (cloud) game
developers is very challenging. This is because of the complex, distributed, and heterogeneous nature of the cloud
gaming platforms. In fact, there is a clear tradeoff between
development complexity and optimization room. Platforms
opt for very low (or even no) additional development complexity may suffer from limited room for optimization, which
are referred to as transparent platforms that run unmodified games. In contrast, other platforms opt for more optimized performance at the expense of requiring additional
development complexity, such as code augmentation and
recompilation, which are called non-transparent platforms.
These two classes of cloud gaming platforms have advantages
and disadvantages, and we describe representative studies in
individual classes below.
The transparent platforms ease the burden of deploying
new games on cloud gaming platforms, at the expense of
potentially suboptimal performance. Depasquale et al. [22]
present a cloud gaming platform based on the RemoteFX
extension of Windows remote desktop protocol. Modern Windows servers leverage GPUs and Hyper-V virtual
machines to enable various remote applications, including
cloud games. Their experiments reveal that RemoteFX allows
Windows servers to better adapt to network dynamics, but
still suffers from high frame loss rate and inferior responsiveness. Kim et al. [44] propose another cloud gaming platform,
7609

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

which consists of a distributed service platform, a distributed
rendering system, and an encoding/streaming system. Their
platform supports isolated audio/video capturing, multiple
clients, and browser-based clients. Real experiments with
40 subjects have been done, showing high responsiveness.
Both Depasquale et al. [22] and Kim et al. [44] are proprietary
platforms, and are less suitable for cloud gaming research.
GamingAnywhere [38], [40] is the first open source transparent cloud gaming platform. Its design principles can be
summarized as extensive, portable, configurable, and open.
The GamingAnywhere server supports Windows and Linux,
and the GamingAnywhere client runs on Windows, Linux,
Mac OS, and Android. It is shown that GamingAnywhere
outperforms several commercial/proprietary cloud gaming
platforms, and has been used and enhanced in several cloud
gaming studies in the literature. For example, Hong et al. [35]
develop adaptation algorithms for multiple gamers, to maximize the gamer experience. In addition to: (i) a user study
to map cloud gaming parameters to gamer experience and
(ii) optimization algorithms for resource allocation, they also
enhance GamingAnywhere [38], [40] to support on-the-fly
adaption of frame rate and bitrate.
The non-transparent platforms require augmenting and
recompiling existing games to leverage unique features
for better gaming experience, which may potentially be
time-consuming, expensive, and error-prone. For example,
current games can be ported to Google’s Native Client technology [62], [63] or to Mozilla’s asm.js language [1], [24].
Several other studies focus on integrating new techniques
with cloud gaming platforms for better gaming experience.
Nan et al. [64] propose a joint video and graphics streaming
system for higher coding efficiency as well. Moreover, they
present a rate adaptation algorithm to further minimize the
bandwidth consumption. Lee et al. [48], [49] present a system
to improve the responsiveness of mobile cloud gaming by
compensating network delay. In particular, their system prerenders potential future frames based on some prediction
algorithm and delivers the rendered frames to mobile clients
when the network conditions are good. These frames are
then used to compensate late video frames due to unstable networks. They integrate the proposed system with two
open source games, and conduct a user study of 23 subjects. The subjects report good gaming experience under
nontrivial network delay, as high as 250 ms. Cai et al. [3]
build a prototype platform for decomposed cloud gaming,
and rigorously address several system issues, which were not
thoroughly investigated in their earlier work [4]. Their main
contribution is the very first cognitive cloud gaming platform
that automatically adapts to distributive workload in run-time,
in order to optimally utilize distributed resources (on different entities, like cloud servers, in-network computing nodes,
and gamers’ local platforms) for the best gamer experience.
On the resulting platform, several games are developed and
empirically evaluated, demonstrating the potentials of cognitive cloud gaming platforms. Several enhancements on such
a platform are still possible, such as implementing more
7610

sophisticated games, supporting more gamers, and providing
more completed SDK (Software Development Kit) to cloud
game developers.
B. QUALITY OF SERVICE EVALUATIONS

Performing QoS measurements is crucial for quantifying the
performance of the cloud gaming platforms. Moreover, doing
so in real-time allows us to effectively troubleshoot and even
to dynamically optimize the cloud gaming platforms. The
QoS related cloud gaming papers are roughly categorized into
two classes: (i) energy consumption and (ii) network metrics.
They are surveyed in the following.
1) ENERGY CONSUMPTION

Games have been known to push consumer computing platforms to their maximum capacity. In traditional systems such
as desktop computers, it is often expected and accepted that
Game software will push a system to its limits. However,
mobile environments are in a strikingly different scenario
as they have limited power reserves. A fully utilized mobile
device may have a greatly reduced running time, thus it is
important to reduce the complexity of these game software
for mobile devices. Luckily, cloud gaming systems provide
a potential way forward by offloading complicated processing tasks such as 3D rendering and physics calculations
to powerful cloud servers. However, cares must be taken
because the decoding of video, especially high definition
video is far from a trivial task. We will cover some pioneering
work [29], [39], [91] that has been done on this important
subject.
Hans et al. [29] systematically test the energy
performance of their in-house cloud gaming server
MCGS.KOM on real world tablets. They find that when
WLAN was used as the access network, cloud game software
could save between 12% and 38% of energy use, depending
on the types of games and tablets. Explorations on important
energy saving coding parameters for H.264/AVC are reported
in Taher et al. [91]. Further, Huang et al. [39] explore the
energy consumption of the cloud gaming video decoders. The
researchers found that frame rate has the largest impact on the
decoders energy consumption, with bit rate and resolution
also being major contributors. Moreover, Shea et al. [79]
explore the performance and energy implications of combing
cloud gaming systems with live broadcasting systems such as
Twitch.
2) NETWORK METRICS

Like many other distributed multimedia applications, user
experience highly depends on network conditions. Therefore,
evaluating different network metrics in cloud gaming is crucial, and we present detailed survey below.
Claypool [18] measures the contents variety of different game genres in details. 28 games from 4 perspectives,
including First-Person Linear, Third-Person Linear, ThirdPerson Isometric, and Omnipresent, are selected to analyze
their scene complexity and motion, indicated by average
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

Intra-coded Block Size (IBS) and Percentage of
Forward/backward or Intra-coded Macroblocks (PFIM),
respectively. Measurements conducted by the author suggest
that Microsoft’s remote desktop achieves better bitrate than
NoMachine’s NX client, while NX client has higher frame
rate. A following work [21] investigates OnLive’s network
characteristics, such as the data size and frequency being sent
and the overall downlink and uplink bitrates. The authors
reveal that the high downlink bitrates of OnLive games are
very similar to those of live videos, nevertheless, OnLive’s
uplink bitrates are much more moderate, which are comparable to traditional game uplink traffic. They also indicate
that the game traffic features are similar for three types
of game genres, including First-Person, Third-Person, and
Omnipresent, while the total bitrates can vary by as much
as 50%. Another important finding is that OnLive does not
demonstrate its ability in adapting bitrate and frame rates to
network latency.
Chen et al. [10] analyze a cloud gaming system’s response
delays and segment it into three components, including network delay, processing delay, and playout delay. With this
decomposition, the authors propose a methodology to measure the latency components and apply the methodology
on OnLive and StreamMyGame, two of the popular cloud
gaming platforms. The authors identify that OnLive system
outperforming StreamMyGame in terms of latency, due to the
different resource provisioning strategy based on game genres. A following work [9] by the same group extend the model
by adding game delay, which represents the latency introduced by the game program to process commands and render
the next video frame of the game scene. They also study
how system design and selective parameters affect responsiveness, including scene complexity, updated region sizes,
screen resolutions, and computation power. Their observation
in network traffics are inline with previous work conducted
by Claypool et al. [21]. Lower network quality, including
the higher packet loss rate and insufficient bandwidth, will
impose negative impacts on both of OnLive and StreamMyGame, resulting lower frame rates and worse graphic
quality. Moreover, by quantifying the streaming quality, the
authors further reveal that OnLive implements an algorithm
to adapt its frame rate to the network delay, while StreamMyGame doesn’t.
Manzano et al. [55] collect and compare network traffic
traces of OnLive and Gaikai, including packet inter-arrival
times, packet size, and packet inter-departure time, to observe
the difference between cloud gaming and traditional online
gaming from the perspectives of network load and traffic
characteristics. The authors reveal that the package size distributions between the two platforms are similar, while the
packet inter-arrival times are distinct. Afterwards, Manzano
et al. [56] claim to be the first research work on specific
network protocols used by cloud gaming platforms. They
focus on conducting a reverse engineering study on OnLive,
based on extensive traffic traces of several games. The authors
further propose a per-flow traffic model for OnLive,
VOLUME 4, 2016

which can be used for network dimensioning, planning optimization, and other studies.
Shea et al. [81] measure the interaction delay and image
quality of OnLive system, under diverse games, computers,
and network configurations. The authors conclude that cloud
procedure introduces 100 to 120 ms latency to the overall
system, which requires further developments in both video
encoders and streaming software. Meanwhile, the impacts of
compression mechanism on video quality are quite noticeable, especially under the circumstances with lower available
bandwidth. They later present an experimental study [80]
on the performance of existing commercial games and raytracing applications with graphical processing units (GPUs).
According to their analysis, gaming applications in virtualized environments demonstrate poorer performance than the
instances executing in non-virtualized bare-metal baseline.
Detailed hardware profiling further reveals that the passthrough access introduces memory bottleneck, especially for
those games with real-time interactions. Another work [36],
however, observes more advanced virtualization technologies
such as mediated pass-through maintain high performance
in virtualized environments. In the authors’ measurement
work, rendering with virtualized GPUs may achieves better
performance than direct pass-through ones. In addition, if the
system adopts software video coding, the CPU may became
the bottleneck, while hypervisor will no longer be the constraint of the system performance. Based on these analysis,
the authors conclude that current virtualization techniques are
already good enough for cloud gaming.
Suznjevic et al. [89] measure 18 games on GamingAnywhere [38] to analyze the correlation between the characteristics of the games played and their network traffic. The
authors observe the highest values for motion, action game
and shooter games, while the majority of strategy games are
relatively low. In contrast, for spatial metrics the situation is
reversed. They also conclude that the bandwidth usage for
most games are within the range of 3 and 4 Mbit/s, except the
strategy games that consume less network resources. Another
notable finding is that, gamers’ action rate will introduce a
slight packet rate increase, but will not affect the generated
network traffic volume.
Lampe et al. [46] conduct experimental evaluations of userperceived latency in cloud games and locally executed video
games. Their results, produced by a semi-automatic measurement tool called GALAMETO.KOM, indicate that cloud
gaming introduces additional latency to game programs,
which is approximately 85% to 800% higher than local executions. This work also features the significant impact of roundtrip time. The measurement results confirm the hypothesis
that the geographical placement of cloud data centres is an
important element in determining response delay, specifically
when the cloud gaming services are accessed through cellular
networks.
Xue et al. [102] conduct a passive and active measurement study for CloudUnion, a Chinese cloud gaming system. The authors characterize the platform from the aspects
7611

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

of architecture, traffic pattern, user behaviour, frame rate
and gaming latency. Observations include: (i) CloudUnion
adopts a geo-distributed infrastructure; (ii) CloudUnion suffers from a queuing problem with different locations from
time to time; (iii) the User Datagram Protocol (UDP) outperforms the Transmission Control Protocol (TCP) in terms
of response delay while sacrificing the video quality; and
(iv) CloudUnion adopts conservative video rate recommendation strategy. By comparing CloudUnion and GamingAnywhere [38], the authors observe four common problems. First,
the uplink and downlink data rates are asymmetric. Second,
low-motion games perceive a periodical jitter at the interval
of 10 seconds. Third, audio and video streams are suffering
from synchronization problem. Fourth, packet loss in network
transmission degrades gaming experiences significantly.
C. QUALITY OF EXPERIENCE EVALUATIONS

Measuring and modeling cloud gaming QoE are no easy tasks
because QoE metrics are subjective. In particular, enough
subjects need to be recruited, and time-consuming, tedious,
and expensive user studies need to be carried out. After
that, practical models to relate the QoS and QoE metrics
need to be proposed, trained, and evaluated. Only when the
resulting models are validated with large datasets, they can be
employed in actual cloud gaming platforms. Cloud gaming
QoE has been studied in the literature and can be categorized
into two classes: (i) general cloud gaming QoE evaluations,
and (ii) mobile cloud gaming QoE evaluations, which are
tailored for mobile cloud games, where mobile devices are
resource constrained and vulnerable to inferior wireless network conditions. We survey the related work in these two
classes below.
Chang et al. [8] present a measurement and modeling
methodology on cloud gaming QoE using three popular
remote desktop systems. Their experiment results reveal that
the QoE (in gamer performance) is a function of frame rate
and graphics quality, and the actual functions are derived
using regression. They also show that different remote desktop systems lead to quite diverse QoE levels under the same
network conditions. Jarschel et al. [42] present a testbed
for a user study on cloud gaming services. Mean Opinion
Score (MOS) values are used as the QoE metrics, and the
resulting MOS values are found to depend on QoS parameters, such as network delay and packet loss, and context, such
as game genres and gamer skills. Their survey also indicates
that very few gamers are willing to commit themselves in a
monthly fee plan for cloud gaming. Hence, better business
models are critical to long-term success of cloud gaming.
Möller et al. [60] also conduct a subjective test in the labs,
and consider 7 different MOS values: input sensitivity, video
quality, audio quality, overall quality, complexity, pleasantness, and perceived value. They observe complex interplays
among QoE metrics, QoS metrics, testbed setup, and software
implementation. For example, the rate control algorithm
implemented in cloud gaming client is found to interfere
with the bandwidth throttled by a traffic shaper. Several open
7612

issues are raised after analyzing the results of the user study,
partially due to the limited number of participants.
Slivar et al. [84] carry out a user study of in-home cloud gaming, i.e., the cloud gaming servers and clients are connected
over a LAN. Several insights are revealed, e.g., switching
from a standard game client to in-home cloud gaming client
leads to QoE degradation, measured in MOS values. Moreover, more skilled gamers are less satisfied with in-home
cloud gaming. Hossain et al. [37] adopt gamer emotion as
a QoE metric and study how several screen effects affect
gamer emotion. Sample screen effects include adjusting:
(i) redness, (ii) blueness, (iii) greenness, (iv) brightness, and
(v) contrast; and the goal of applying these screen effects
is to mitigate negative gamer emotion. They then perform
QoE optimization after deriving an empirical model between
screen effects and gamer emotion.
Some other QoE studies focus on the response delay, which
is probably the most crucial performance metric in cloud
gaming, where servers may be geographically far away from
clients. Lee et al. [50] find that response delay imposes
different levels of implications on QoE with different game
genres. They also develop a model to capture this implication
as a function of gamer inputs and game scene dynamics.
Quax et al. [71] make similar conclusions after conducting
extensive experiments, e.g., gamers playing action games
are more sensitive to high responsive delay. Claypool and
Finkel [20] perform user studies to understand the objective
and subjective effects of network latency on cloud gaming.
They find that both MOS values and gamer performance
degrade linearly with network latency. Moreover, cloud gaming is very sensitive to network latency, similar to the traditional first-person avatar games. Raaen [72] designs a user
study to quantify the smallest response delay that can be
detected by gamers. It is observed that some gamers can
perceive < 40 ms response delay, and half of the gamers
cannot tolerate ≥ 100 ms response delay.
Huang et al. [41] perform extensive cloud gaming
experiments using both mobile and desktop clients. Their
work reveals several interesting insights. For example,
gamers’ satisfaction on mobile clients are more related to
graphics quality, while the case on desktop clients is more
correlated to control quality. Furthermore, graphics and
smoothness quality are significantly affected by the bitrate,
frame rate, and network latency, while the control quality
is determined only by the client types (mobile or desktop).
Wang and Dey [94], [97] build a mobile cloud gaming testbed
in their lab for subjective tests. They propose a Game Mean
Opinion Score (GMOS) model, which is a function of game
genre, streaming configuration, measured Peak Signal-toNoise Ratio (PSNR), network latency, and packet loss. The
derivations of model parameters are done via offline regression, and the resulting models can be used for optimizing
mobile cloud gaming experience. Along this line,
Liu et al. [54] propose a Cloud Mobile Rendering–Mean
Opinion Score (CMR-MOS) model, which is a variation of GMOS. CMR-MOS has been used in selecting
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

detail levels of remote rendering applications, like cloud
games.
IV. OPTIMIZING CLOUD GAMING PLATFORMS

This section surveys optimization studies on cloud gaming
platforms, which are further divided into two classes: (i) cloud
server infrastructure and (ii) communications.
A. CLOUD SERVER INFRASTRUCTURE

To cope with the staggering demands from the massive
number of cloud gaming users, carefully-designed cloud
server infrastructures are required for high-quality, robust,
and sustainable cloud gaming services. Cloud server infrastructures can be optimized by: (i) intelligently allocating
resources among servers or (ii) creating innovative distributed
structures. We detail these two types of work in the following.
1) RESOURCE ALLOCATION

The amount of resources allocated to high performance
multimedia applications such as cloud gaming continues to
grow in both public and private data centers. The high demand
and utilization patterns of these platforms make the smart
allocation of these resources paramount to the efficiency of
both public and private clouds. From Virtual Machine (VM)
placement to shared GPUs, researchers from many areas have
been exploring how to efficiently use the cloud to host cloud
gaming platforms. We now explore the important work done
in this area to facilitate efficient deployment of cloud gaming
platforms.
Critical work has been done on both VM placement and
cloud scheduling to facilitate better quality of cloud gaming
services. For example, Wang et al. [98] show that, with proper
scheduling of cloud instances, cloud gaming servers could be
made wireless networking aware. Simulations of their proposed scheduler show the potential of increased performance
and decreased costs for cloud gaming platforms. Researchers
also explore making resource provisioning cloud gaming
aware. For example, a novel QoE aware VM placement strategy for cloud gaming is developed [33]. Further, research has
been done to increase the efficiency of resource provisioning for massively multi-player online games (MMOG) [57].
The researchers develop greedy heuristics to allocate the
minimum number of computing nodes required to meet the
MMOG service needs. Researchers also study the popularity
of games on the cloud gaming service OnLive and propose
methods to improve performance of these systems based
on game popularity [25]. Later, a resource allocation strategy [51] based on the expected ending time of each play
session is proposed. The strategy can reduce the cost of
operation to cloud gaming providers by reducing the number
of purchased nodes required to meet their clients needs. They
note that classical placement algorithms such as First Fit and
Best Fit, are not effective for cloud gaming. After extensive experiments, the authors show an algorithm leveraging
on neural-network-based predictions, which could improve
VM deployment, and potentially decreases operating costs.
VOLUME 4, 2016

Although many cloud computing workloads do not require
a dedicated GPU, cloud gaming servers require access to
a rendering device to provide 3D graphics. As such VM
and workload placements have been researched to ensure
cloud gaming servers have access to adequate GPU resources.
Kim et al. [45] propose a novel architecture to support
multiple-view cloud gaming servers, which share a single
GPU. This architecture provides multi-focal points inside a
shared cloud game, allowing multiple gamers to potentially
share a game world, which is rendered on a single GPU.
Zhao et al. [104] perform an analysis of the performance of
combined CPU/GPU servers for game cloud deployments.
They try offloading different aspects of game processing to
these cloud servers, while maintaining some local processing
at the client side. They conclude that keeping some processing at the client side may lead to an increase in QoS of cloud
gaming platforms.
Pioneering research has also been done on GPU sharing
and resource isolation for cloud gaming servers [70], [103].
These works show that with proper scheduling and allocation of resources we can maximize GPUs utilization, while
maintaining high performance for the gamers sharing a single
GPU. Shea and Liu [80] show that direct GPU assignment to
a virtualized gaming instance can lead to frame rate degradation of over 50% in some gaming applications. They find that
the GPU device pass-through severely diminishes the data
transfer rate between the main memory and the GPU. Their
follow-up work using more advanced platforms [78] reveals
that although the memory transfer degradation still exists,
it no longer affects the frame rate of current generation games.
Hong et al. [34] perform a parallel work, where they discover
that the frame rate issue presents in virtualized clouds may be
mitigated by using mediated pass-through, instead of direct
assignment.
In addition, work has been done to augment existing clouds
and games to improve cloud gaming efficiency. It has been
shown that using game engine information can greatly reduce
the resources needed to calculate the motion estimation (ME)
needed for conventional compression algorithms such as
H.264/AVC [76]. Research into these technique shows that
we can accelerate the motion estimation phase by over 14%
if we use in-game information for encoding. Others have proposed using reusable modules for cloud gaming servers [30].
They refer to these reusable modules as substrates and test
the latency between the different components. All these data
compression studies affect resource allocation; we provide a
comprehensive survey on data compression for cloud gaming
in Section IV-B.1.
2) DISTRIBUTED ARCHITECTURES

Due to the vast geographic distribution of the cloud gaming
clients the design of distributed architectures is of critical
importance to the deployment of cloud gaming systems. The
design of these systems must be carefully optimized to ensure
that a cloud gaming system can sufficiently cover its target audience. Further, to maintain the extremely low delay
7613

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

tolerance required for high QoE even the placement of different server components must be optimized for the lowest possible latency. These innovative distributed architectures have
been investigated in the literature, and we detail them below.
Sselbeck et al. [90] discover that running a cloud gaming
based massively multi-player online game (MMOG) may
suffer from increased latency. These issues are aggravated
in a cloud gaming context because MMOG are already
extremely latency sensitive applications. The increased
latency introduced by a cloud gaming may vastly decrease
the playability of these games. To deal with this increased
latency, they propose a P2P based solution. Similarly,
Prabu and Purushotham [69] propose a P2P system based on
Windows Azure to support online games.
Research has also been done on issues created by the
geographical distance between the end user of cloud gaming
and a cloud gaming data center. Choy et al. [13] show that
the current geographical deployments of public data centers leave a large fraction of the USA with an unacceptable
RTT for low latency applications such as cloud gaming.
To help mitigate this issue, they propose deploying edge
servers near some users for cloud gaming; a follow up work
further explores this architecture and shows that hybrid edgecloud architectures could indeed expand the reach of cloud
gaming data centers [14]. Similarly, Siekkinen et al. [83]
propose a distributed cloud gaming architecture with servers
deployed near local gamers when necessary. The researchers
prototype the system and show that if being deployed widely
enough, for example at the ISP level, cloud gaming could
reach an even larger audience. Tian et al. [92] perform an
extensive investigation into issues of deploying cloud gaming
architecture with distributed data centers. They focus on a
scenario where adaptive streaming technology is available to
the cloud provider. The authors give an optimization algorithm, which can improve gamer QoE as well as reducing
operating costs of the cloud gaming provider. The algorithm
is evaluated using trace driven simulations, and the results
show a potential cost savings of 25% to the cloud gaming
provider.
B. COMMUNICATIONS

Due to the distributed nature of cloud gaming services, the
efficiency and robustness of the communication channels
between cloud gaming servers and clients are crucial and have
been studied. These studies can be classified into two groups:
(i) the data compression algorithms to reduce the network
traffic amount and (ii) the transmission adaptation algorithms
to cope with network dynamics. We survey the work in these
two groups in the following.
1) DATA COMPRESSION

After game scenes are computed on cloud servers, they
have to be captured in proper representations and compressed before being streamed over networks. This can
be done in one of the three data compression schemes:
(i) video compression, which encodes 2D rendered videos and
potentially auxiliary videos (such as depth videos) for client
7614

side post-rendering operations, (ii) graphics compression,
which encodes 3D structures and 2D textures, and (iii) hybrid
compression, which combines both video and graphics compression. Upon cloud gaming servers produce compressed
data streams, the servers send the streams to client computers
over communication channels. We survey each of the three
schemes below.
Video compression is the most widely-used data
compression schemes for cloud gaming probably because
2D video codecs are quite mature. These proposals strive
to improve the coding efficiency in cloud gaming, and
can be further classified into groups depending on whether
in-game graphics contexts, such as camera locations and orientations, are leveraged for higher coding efficiency. We first
survey the proposals that do not leverage graphics contexts.
Cai et al. [6] propose to cooperatively encode cloud gaming
videos of different gamers in the same game session, in
order to leverage inter-gamer redundancy. This is based on
an observation that game scenes of close-by gamers have
non-trivial overlapping areas, and thus adding inter-gamer
predictive video frames may improve the coding efficiency.
The high-level idea is similar to multiview video codecs, such
as H.264/MVC, and the video packets shared by multiple
gamers are exchanged over an auxiliary short-range ad-hoc
network in a P2P fashion. Cai et al. [5] improve upon the
earlier work [6] by addressing three more research problems:
(i) uncertainty due to mobility, (ii) diversity of network
conditions, and (iii) model of QoE. These problems are solved
by a suite of optimization algorithms proposed in their work.
Sun and Wu [88] solve the video rate control problem in cloud
gaming in two steps. First, they adopt the concept of RoI,
and define heterogeneous importance weights for different
regions of game scenes. Next, they propose a macroblocklevel rate control scheme to optimize the RoI-weighted video
quality. Cheung et al. [12] propose to concatenate the graphic
renderer with a customized video coder on servers in cellular
networks and multicast the coded video stream to a gamer
and multiple observers. Their key innovation is to leverage
the depth information used in 3D rendering process to locate
the RoI and then allocate more bits to that region. The
resulting video coder is customized for cloud gaming, yet produces standard compliant video streams for mobile devices.
Liu et al. [53] also leverage rendering information to improve
video encoding in cloud gaming for better perceived video
quality and shorter encoding time. In particular, they first
analyze the rendering information to identify RoI and allocate more bits on more important regions, which leads to
better perceived video quality. In addition, they use this
information to accelerate the encoding process, especially
the time used in motion estimation and macroblock mode
selection. Experiments reveal that their proposed video coder
saves 42% of encoding time and achieves perceived video
quality similar to the unmodified video coder. Similarly,
Semsarzadeh et al. [76] study the feasibility of using
rendering information to accelerate the computationallyintensive motion estimation and demonstrate that it is
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

possible to save 14.32% of the motion estimation time and
8.86% of the total encoding time. The same authors [77]
then concertize and enhance their proposed method, in which
they present the general method, well-designed programming
interface, and detailed motion estimation optimization. Both
subjective and objective tests show that their method suffers from very little quality drop compared to the unmodified video coder. It is reported that they achieve 24% and
39% speedups on the whole encoding process and motion
estimation, respectively.
Next, we survey the proposals that utilize graphics
contexts [82], [101]. Shi et al. [82] propose a video compression scheme for cloud gaming, which consists of two
unique techniques: (i) 3D warping-assisted coding and
(ii) dynamic auxiliary frames. 3D warping is a light-weight
2D post-rendering process, which takes one or multiple
reference view (with image and depth videos) to generate
a virtual view at a different camera location/orientation.
Using 3D warping allows video coders to skip some video
frames, which are then wrapped at client computers. Dynamic
auxiliary frames refer to those video frames rendered with
intelligently-chosen camera location/orientations that are not
part of the game plays. They show that the auxiliary frames
help to improve 3D warping performance. Xu et al. [101] also
propose two techniques to improve the coding efficiency in
cloud gaming. First, the camera rotation is rectified to produce video frames that are more motion estimation friendly.
On client computers, the rectified videos are compensated
with some camera parameters using a light-weight 2D process. Second, a new interpolation algorithm is designed to
preserve sharp edges, which are common in-game scenes.
Last, we notice that the video compression schemes are
mostly orthogonal to the underneath video coding standards,
and can be readily integrated with the recent (or future) video
codecs for further performance improvement.
Graphics compression is proposed for better scalability,
because 3D rendering is done at individual client computers.
Compressing graphics data, however, is quite challenging
and may consume excessive network bandwidth [52], [58].
Lin et al. [52] design a cloud gaming platform based on
graphics compression. Their platform has three graphics
compression tools: (i) intra-frame compression, (ii) interframe compression, and (iii) caching. These tools are applied
to graphics commands, 3D structures, and 2D textures.
Meilnder et al. [58] also develop a similar platform for mobile
devices, where the graphics are sent from cloud servers to
proxy clients, which then render game scenes for mobile
devices. They also propose three graphics compression tools:
(i) caching, (ii) lossy compression, and (iii) multi-layer compression. Generally speaking, tuning cloud gaming platforms
based on graphics compression for heterogeneous client
computers is non-trivial, because mobile (or even some
stationary) computers may not have enough computational
power to locally render game scenes.
Hybrid compression [15], [16] attempts to fully utilize
the available computational power on client computers
VOLUME 4, 2016

to maximize the coding efficiency. For example,
Chuah and Cheung [15] propose to apply graphics compression on simplified 3D structures and 2D textures, and send
them to client computers. The simplified scenes are then
rendered on client computers, which is called the base layer.
Both the full-quality video and the base-layer video are rendered on cloud servers, and the residue video is compressed
using video compression and sent to client computers. This
is called the enhancement layer. Since the base layer is compressed as graphics and the enhancement layer is compressed
as videos, the proposed approach is a hybrid scheme. Based
on the layered coding proposal, Chuah et al. [16] further
propose a complexity-scalable base-layer rendering pipeline
suitable for heterogeneous mobile receivers. In particular,
they employ scalable Blinn-Phong lighting for rendering
the base-layer, which achieves maximum bandwidth saving
under the computing constraints of mobile receivers. Their
experiments demonstrate that their hybrid compression solution, customized for cloud gaming, outperforms single-layer
general-purpose video codecs.
2) ADAPTIVE TRANSMISSION

Even though data compression techniques have been applied
to reduce the network transmission rate, the fluctuating network provisioning still results in unstable service quality to
the gamers in cloud gaming system. These unpredictable
factors include bandwidth, round-trip time, jitter, and etc.
Under this circumstance, adaptive transmission is introduced
to further optimize gamers’ QoE. The foundation of these
studies is based on a common sense: gamers would prefer to
scarify video quality to gain smoother playing experience in
insufficient network QoS supplement.
Jarvinen et al. [43] explore the approach to adapt the
gaming video transmission to available bandwidth. This is
accomplished by integrating a video adaptation module into
the system, which estimates the network status from network monitor in real-time and dynamically manipulates the
encoding parameters, such as frame rate and quantization, to
produce specific adaptive bit rate video stream. The authors
utilize RTT jitter value to detect the network congestion,
in order to decide if the bit rate adaptation should be triggered.
To evaluate this proposal, a following work [47] conducts
experiments on a normal television with an IPTV set-topbox. The authors simulate the network scenarios in homes
and hotels to verify that the proposed adaptation performed
notably better.
Adaptive transmission has also been studied in mobile scenarios. Wang and Dey [95] first decompose the cloud gaming
system’s response time into sub-components: server delay,
network uplink/downlink delay, and client delay. Among
the optimization techniques applied, rate-selection algorithm
provides a dynamic solution that determine the time and the
way to switch the bit rate according to the network delay.
As a further step, Wang and Dey [96] study the potential
of rendering adaptation. They identify the rendering parameters that affect a particular game, including realistic effect
7615

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

(e.g., colour depth, multi-sample, texture-filter, and lighting mode), texture detail, view distance and enabling grass.
Afterwards, they analyze these parameters’ characteristics
of communications and computation costs and propose their
rendering adaptation scheme, which is consisted of optimal
adaptive rendering settings and level-selection algorithm.
With the experiments conducted on commercial wireless
networks, the authors demonstrate that acceptable mobile
gaming user experience can be ensured by their rendering
adaption technique. Thus, they claim that their proposal is
able to facilitate cloud gaming over mobile networks.
Other aspects of transmission adaptation have also been
investigated in the literature. He et al. [31] consider the adaptive transmission from the perspective of multi-player. The
authors calculate the packet urgency based on buffer status
estimation and propose a scheduling algorithm. In addition,
they also suggest an adaptive video segment request scheme,
which estimates media access control (MAC) queue as an
additional information to determine the request time interval
for each gamer, on the purpose of improving the playback
experience. Bujari et al. [2] provides a VoAP algorithm to
address the flow coexistence issue in wireless cloud gaming service delivery. This research problem is introduced by
the concurrent transmissions of TCP-based and UDP-based
streams in home scenario, where the downlink requirement of
gaming video exacerbate the operation of above mentioned
transport protocols. The authors’ solution is to dynamically
modify the advertised window, in such way the system
can limit the growth of the TCP flow’s sending rate.
Wu et al. [99] present a novel transmission scheduling framework dubbed AdaPtive HFR vIdeo Streaming (APHIS) to
address the issue in the cloud gaming video delivery through
wireless networks. The authors first propose an online video
frame selection algorithm to minimize the total distortion
based on network status, input video data, and delay constraint. Afterwards, they introduce an unequal forward error
correction (FEC) coding scheme to provide differentiated
protection for Intra (I) and Predicted (P) frames with lowlatency cost. The proposed APHIS framework is able to
appropriately filter video frames and adjust data protection
levels to optimize the quality of HFR video streaming.
Hemmati et al. [32] propose an object selection algorithm to
provide an adaptive scene rendering solution. The basic idea
is to exclude less important objects from the final output, thus
to reduce less processing time for the server to render and
encode the frames. In such a way, the cloud gaming system is
able to achieve a lower bit rate to stream the resulting video.
The proposed algorithm evaluates the importance of objects
from the game scene based on the analysis of gamers’ activities and do the selection work. Experiments demonstrate that
this approach reduces streaming bit rate by up to 8.8%.
V. COMMERCIAL CLOUD GAMING SERVICES

In addition to the technical problems discussed in prior
sections, commercialization and business models of cloud
gaming services are critical to their success. We survey the
7616

commercialization efforts starting from a short history on
cloud gaming services. G-cluster [26] starts building cloud
gaming services since early 2000’s. In particular, G-cluster
publicly demonstrated live game streaming1 over WiFi to a
PDA in 2001, and a commercial game-on-demand service
in 2004. G-cluster’s service is tightly coupled with several
third-party companies, including game developers, network
operators, and game portals. This can be partially attributed
to the less mature Internet connectivity and data centers,
which force G-cluster to rely on network QoS supports from
network operators. Ojala and Tyrvainen [66] presents the
evolution of G-cluster’s business model, and observe that the
number of G-cluster’s third-party companies is reduced over
years. The number of households having access to G-cluster’s
IPTV-based cloud gaming service increased from 15,000 to
3,000,000 between 2005 and 2010.
In late 2000’s, emerging cloud computing companies start
offering Over-The-Top (OTT) cloud gaming services, represented by OnLive [67], Gaikai [27], and GameNow [28].
OTT refers to delivering multimedia content over the Internet above arbitrary network operators to end users, which
trades QoS supports for ubiquitous access to cloud games.
OnLive [67] was made public in 2009, and was a well-known
cloud gaming service, probably because of its investors
including Warner Bros, AT&T, Ubisoft, and Atrari. OnLive
provided subscription based service, and hosted its servers
in several States within the US, to control the latency due
to geographical distances. OnLive ran into financial difficulty in 2012, and ceased operations in 2015 after selling
their patents to Sony [87]. Gaikai [27] offered cloud gaming
service using a different business model. Gaikai adopt cloud
gaming to allow gamers to try new games without purchasing
and installing software on their own machines. At the end
of each gameplay, gamers are given options to buy the game
if they like it. That is, Gaikai is more like an advertisement
service for game developers to boost their sales. Gaikai was
acquired by Sony [86] in 2012, which leads to a new cloud
gaming service from Sony, called PS Now [68] launched
in 2014. PS Now allows gamers to play PlayStation games
as cloud games, and adopts two charging models: per-game
and monthly subscription.
The aforementioned cloud gaming services can be classified in groups from two aspects. We discuss the advantages
and disadvantages of different groups in the following. First,
cloud gaming services are either: (i) integrated with underlaying networks or (ii) provided as OTT services. Tighter integration provides better QoS guarantees which potentially lead
to better user experience, while OTT reduces the expenses on
cloud gaming services at a possible risk of unstable and worse
user experience. Second, cloud gaming services adopt one of
the three charging models: (i) subscription, (ii) per-game, and
(iii) free to gamers. More specifically, cloud gaming users
pay for services in the first two charging models, while thirdparty companies, which can be game developers or network
1 At that time, the term cloud was not yet popular.
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

operators, pay for services in the third charging model. In the
future, there may be innovative ways to offer cloud gaming
services to general publics in a commercially-viable manner.

hence, we believe that we are on the edge of a new era of
a whole new cloud gaming ecosystem, which will eventually
leads to the next generation cloud gaming services.

VI. CONCLUSION AND OUTLOOK

REFERENCES

In this article, we grouped the existing cloud gaming
research into four classifications: (i) overview, (ii) platform,
(iii) optimization, and (iv) commercialization. In Section II
(overview), we included papers that introducing general and
specialized (such as mobile) cloud gaming. In Section III
(platform), we presented the basic cloud gaming platforms
that support quantitative performance measurements. More
specifically, we considered: (i) QoS evaluations, such as
energy consumption and network metrics, and (ii) QoE evaluations, such as gamer experience. In Section IV (optimization), we presented the two major optimization directions:
(i) cloud server infrastructure, such as resource allocation
and distributed architecture, and (ii) communications, such
as data compression and adaptive transmission. In Section V
(commercialization), we gave a brief history of cloud gaming
services, followed by the design decisions made by representative commercial cloud gaming services.
Cloud gaming is not a panacea and incurs non-trivial costs
to service providers. Minimizing the cost on cloud and networking resources while achieving high gamer experience
requires careful optimization like the approaches explored
in this survey. Without these optimizations, service provider
cannot consolidate enough cloud gaming users to each physical machine. This in turn leads to much lower profits, and
may drive the service provider out of business. Some early
industrial pioneers such as OnLive [67] have unfortunately
exited the market. More recent cloud gaming services such
as PS Now [68] and GameNow [28] are better optimized
and will be more competitive in the current gaming industry.
As commercial cloud gaming services become financially
sustainable, the new cloud gaming ecosystem will continue
to expand, leading to more investments and technologies to
improve these services. Much of the innovation needed to
push cloud gaming to the next level may reside in creating
new programing paradigms to support the unique needs of
these complex systems. Most current cloud gaming platforms
work as a ‘‘black box’’ simply wrapping a traditionally programed game in a support system to enable cloud gaming.
Although, the original black box model of cloud gaming has
led to many practical real world implementations a more
integrated approach may be necessary. It is likely that using
in-game contexts or whole new programing paradigms may
solve some of cloud gaming shortcomings [7]. Future cloud
gaming aware programing paradigms will help facilitate both
better user experience and resource utilization. This will
allow more innovative, yet demanding ideas to be implemented, which in turn results in critical momentum towards
building the next generation cloud gaming services.
In summary, the advances of technologies turn playable
cloud gaming services into reality; more optimization techniques gradually make cloud gaming services profitable;
VOLUME 4, 2016

[1] (Mar. 2013). asm.js. [Online]. Available: http://asmjs.org/
[2] A. Bujari, M. Massaro, and C. Palazzi, ‘‘Vegas over access point: Making
room for thin client game systems in a wireless home,’’ IEEE Trans.
Circuits Syst. Video Technol., vol. 25, no. 12, pp. 2002–2012, Dec. 2015.
[3] W. Cai, H. Chan, X. Wang, and V. Leung, ‘‘Cognitive resource optimization for the decomposed cloud gaming platform,’’ IEEE Trans. Circuits
Syst. Video Technol., vol. 25, no. 12, pp. 2038–2051, Dec. 2015.
[4] W. Cai, M. Chen, and V. C. M. Leung, ‘‘Toward gaming as a service,’’
IEEE Internet Comput., vol. 18, no. 3, pp. 12–18, May/Jun. 2014.
[5] W. Cai, Z. Hong, X. Wang, H. Chan, and V. Leung, ‘‘Quality-ofexperience optimization for a cloud gaming system with ad hoc cloudlet
assistance,’’ IEEE Trans. Circuits Syst. Video Technol., vol. 25, no. 12,
pp. 2092–2104, Dec. 2015.
[6] W. Cai, V. Leung, and L. Hu, ‘‘A cloudlet-assisted multiplayer cloud gaming system,’’ Mobile Netw. Appl., vol. 19, no. 2, pp. 144–152, Nov. 2013.
[7] W. Cai et al., ‘‘The future of cloud gaming,’’ Proc. IEEE, vol. 104, no. 4,
pp. 687–691, Apr. 2016.
[8] Y.-C. Chang, P.-H. Tseng, K.-T. Chen, and C.-L. Lei, ‘‘Understanding the
performance of thin-client gaming,’’ in Proc. IEEE Int. Workshop Tech.
Committee Commun. Quality Rel. (CQR), Naples, FL, USA, May 2011,
pp. 1–6.
[9] K.-T. Chen, Y.-C. Chang, H.-J. Hsu, D.-Y. Chen, C.-Y. Huang, and
C.-H. Hsu, ‘‘On the quality of service of cloud gaming systems,’’ IEEE
Trans. Multimedia, vol. 16, no. 2, pp. 480–495, Feb. 2014.
[10] K. Chen, Y. Chang, P. Tseng, C. Huang, and C. Lei, ‘‘Measuring the latency of cloud gaming systems,’’ in Proc. ACM Int. Conf.
Multimedia (MM), Scottsdale, AZ, USA, Nov. 2011, pp. 1269–1272.
[11] K.-T. Chen, C.-Y. Huang, and C.-H. Hsu, ‘‘Cloud gaming onward:
Research opportunities and outlook,’’ in Proc. IEEE Conf. Multimedia
Expo Workshops (ICMEW), Chengdu, China, Jul. 2014, pp. 1–4.
[12] G. Cheung, T. Sakamoto, and W. Tan, ‘‘Graphics-to-video encoding for
3G mobile game viewer multicast using depth values,’’ in Proc. IEEE Int.
Conf. Image Process. (ICIP), Singapore, Oct. 2004, pp. 2805–2808.
[13] S. Choy, B. Wong, G. Simon, and C. Rosenberg, ‘‘The brewing
storm in cloud gaming: A measurement study on cloud to end-user
latency,’’ in Proc. 11th IEEE Annu. Workshop Netw. Syst. Support
Games (NetGames), Venice, Italy, Nov. 2012, pp. 1–6.
[14] S. Choy, B. Wong, G. Simon, and C. Rosenberg, ‘‘A hybrid edge-cloud
architecture for reducing on-demand gaming latency,’’ Multimedia Syst.,
vol. 20, no. 5, pp. 503–519, Oct. 2014.
[15] S.-P. Chuah and N.-M. Cheung, ‘‘Layered coding for mobile cloud
gaming,’’ in Proc. Int. Workshop Massively Multiuser Virtual
Environ. (MMVE), Singapore, Mar. 2014, pp. 1–6.
[16] S.-P. Chuah, N.-M. Cheung, and C. Yuen, ‘‘Layered coding for mobile
cloud gaming using scalable Blinn–Phong lighting,’’ IEEE Trans. Image
Process., vol. 25, no. 7, pp. 3112–3125, Jul. 2016.
[17] S.-P. Chuah, C. Yuen, and N.-M. Cheung, ‘‘Cloud gaming: A green
solution to massive multiplayer online games,’’ IEEE Wireless Commun.,
vol. 21, no. 4, pp. 78–87, Aug. 2014.
[18] M. Claypool, ‘‘Motion and scene complexity for streaming video games,’’
in Proc. Int. Conf. Found. Digit. Games (FDG), Orlando, FL, USA,
Apr. 2009, pp. 34–41.
[19] M. Claypool and K. Claypool, ‘‘Latency and player actions in online
games,’’ Commun. ACM, vol. 49, no. 11, pp. 40–45, Nov. 2006.
[20] M. Claypool and D. Finkel, ‘‘The effects of latency on player performance
in cloud-based games,’’ in Proc. 13th Annu. Workshop Netw. Syst. Support
Games (NetGames), Nagoya, Japan, Dec. 2014, pp. 1–6.
[21] M. Claypool, D. Finkel, A. Grant, and M. Solano, ‘‘Thin to win?
Network performance analysis of the OnLive thin client game system,’’ in Proc. 11th IEEE Annu. Workshop Netw. Syst. Support
Games (NetGames), Venice, Italy, Nov. 2012, pp. 1–6.
[22] E. Depasquale et al., ‘‘An analytical method of assessment of RemoteFX
as a cloud gaming platform,’’ in Proc. IEEE Conf. Medit. Electrotech.
Conf. (MELECON), Beirut, Lebanon, Apr. 2014, pp. 127–133.
[23] S. Dey, ‘‘Cloud mobile media: Opportunities, challenges, and directions,’’
in Proc. IEEE Conf. Comput., Netw. Commun. (ICNC), Maui, HI, USA,
Feb. 2012, pp. 929–933.
7617

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

[24] (Aug. 2013). Emscripten. [Online]. Available: http://emscripten.org
[25] D. Finkel, M. Claypool, S. Jaffe, T. Nguyen, and B. Stephen, ‘‘Assignment of games to servers in the OnLive cloud game system,’’ in Proc.
Annu. Workshop Netw. Syst. Support Games (NetGames), Nagoya, Japan,
Dec. 2014, Art. no. 4.
[26] (Jan. 2015). G-Cluster. [Online]. Available: http://www.gcluster.com/eng
[27] (Jan. 2015). GaiKai. [Online]. Available: http://www.gaikai.com/
[28] (Jan. 2015). GameNow. [Online]. Available: http://www.ugamenow.com
[29] R. Hans, U. Lampe, D. Burgstahler, M. Hellwig, and R. Steinmetz,
‘‘Where did my battery go? Quantifying the energy consumption of cloud
gaming,’’ in Proc. IEEE Int. Conf. Mobile Services (MS), Anchorage, AK,
USA, Jun. 2014, pp. 63–67.
[30] M. Hassam, N. Kara, F. Belqasmi, and R. Glitho, ‘‘Virtualized infrastructure for video game applications in cloud environments,’’ in Proc.
ACM Symp. Mobility Manage. Wireless Access (MobiWac), Montreal,
QC, Canada, Sep. 2014, pp. 109–114.
[31] L. He, G. Liu, and C. Yuchen, ‘‘Buffer status and content aware scheduling scheme for cloud gaming based on video streaming,’’ in Proc. IEEE
Conf. Multimedia Expo Workshops (ICMEW), Chengdu, China, Jul. 2014,
pp. 1–6.
[32] M. Hemmati, A. Javadtalab, A. Shirehjini, S. Shirmohammadi, and
T. Arici, ‘‘Game as video: Bit rate reduction through adaptive object
encoding,’’ in Proc. ACM Workshop Netw. Oper. Syst. Support Digit.
Audio Video (NOSSDAV), Oslo, Norway, Feb. 2013, pp. 7–12.
[33] H.-J. Hong, D.-Y. Chen, C.-Y. Huang, K.-T. Chen, and C.-H. Hsu, ‘‘QoEaware virtual machine placement for cloud games,’’ in Proc. Annu.
Workshop Netw. Syst. Support Games (NetGames), Denver, CO, USA,
Dec. 2013, pp. 1–2.
[34] H.-J. Hong, D.-Y. Chen, C.-Y. Huang, K.-T. Chen, and C.-H. Hsu, ‘‘Placing virtual machines to optimize cloud gaming experience,’’ IEEE Trans.
Cloud Comput., vol. 3, no. 1, pp. 42–53, Jan. 2015.
[35] H.-J. Hong, C.-F. Hsu, T.-H. Tsai, C.-Y. Huang, K.-T. Chen, and
C.-H. Hsu, ‘‘Enabling adaptive cloud gaming in an open-source cloud
gaming platform,’’ IEEE Trans. Circuits Syst. Video Technol., vol. 25,
no. 12, pp. 2078–2091, Dec. 2015.
[36] H.-J. Hong, T.-Y. Fan-Chiang, C.-R. Lee, K.-T. Chen, C.-Y. Huang, and
C.-H. Hsu, ‘‘GPU consolidation for cloud games: Are we there yet?’’ in
Proc. 13th Annu. Workshop Netw. Syst. Support Games, Nagoya, Japan,
Dec. 2014, pp. 1–6.
[37] M. S. Hossain, G. Muhammad, B. Song, M. Hassan, A. Alelaiwi, and
A. Alamri, ‘‘Audio–visual emotion-aware cloud gaming framework,’’
IEEE Trans. Circuits Syst. Video Technol., vol. 25, no. 12, pp. 2105–2118,
Dec. 2015.
[38] C.-Y. Huang, K.-T. Chen, D.-Y. Chen, H.-J. Hsu, and C.-H. Hsu, ‘‘GamingAnywhere: The first open source cloud gaming system,’’ ACM Trans.
Multimedia Comput., Commun., Appl., vol. 10, no. 1s, pp. 10:1–10:25,
Jan. 2014.
[39] C.-Y. Huang, P.-H. Chen, Y.-L. Huang, K.-T. Chen, and C.-H. Hsu,
‘‘Measuring the client performance and energy consumption in mobile
cloud gaming,’’ in Proc. 13th Annu. Workshop Netw. Syst. Support
Games (NetGames), Nagoya, Japan, Dec. 2014, pp. 1–3.
[40] C.-Y. Huang, C.-H. Hsu, Y.-C. Chang, and K.-T. Chen, ‘‘GamingAnywhere: An open cloud gaming system,’’ in Proc. ACM Multimedia Syst.
Conf. (MMSys), Oslo, Norway, Feb. 2013, pp. 36–47.
[41] C.-Y. Huang, C.-H. Hsu, D.-Y. Chen, and K.-T. Chen, ‘‘Quantifying user
satisfaction in mobile cloud games,’’ in Proc. Workshop Mobile Video
Del. (MoVid), Singapore, Mar. 2013, pp. 4:1–4:6.
[42] M. Jarschel, D. Schlosser, S. Scheuring, and T. Hoßfeld, ‘‘Gaming in the
clouds: QoE and the users’ perspective,’’ Math. Comput. Model., vol. 57,
nos. 11–12, pp. 2883–2894, Jun. 2013.
[43] S. Jarvinen, J. P. Laulajainen, T. Sutinen, and S. Sallinen, ‘‘QoS-aware
real-time video encoding: How to improve the user experience of a
gaming-on-demand service,’’ in Proc. IEEE Consum. Commun. Netw.
Conf. (CCNC), Las Vegas, NV, USA, Jan. 2006, pp. 994–997.
[44] K. I. Kim, S. Y. Bae, D. C. Lee, C. S. Cho, H. J. Lee, and K. C. Lee,
‘‘Cloud-based gaming service platform supporting multiple devices,’’
ETRI J., vol. 35, no. 6, pp. 960–968, Dec. 2013.
[45] S. S. Kim, K. I. Kim, and J. Won, ‘‘Multi-view rendering approach
for cloud-based gaming services,’’ in Proc. Int. Conf. Adv. Future
Internet (AFIN), French Riviera, France, Aug. 2011, pp. 102–107.
[46] U. Lampe, Q. Wu, S. Dargutev, R. Hans, A. Miede, and R. Steinmetz,
‘‘Assessing latency in cloud gaming,’’ in Proc. Int. Conf. Cloud Comput.
Services Sci. (CLOSER), Barcelona, Spain, Sep. 2014, pp. 52–68.
7618

[47] J. Laulajainen, T. Sutinen, and S. Járvinen, ‘‘Experiments with QoSaware gaming-on-demand service,’’ in Proc. Int. Conf. Adv. Inf. Netw.
Appl. (AINA), Vienna, Austria, Apr. 2006, pp. 805–810.
[48] K. Lee et al., ‘‘Outatime: Using speculation to enable low-latency continuous interaction for cloud gaming,’’ in Proc. Annu. Int. Conf. Mobile
Syst., Appl., Services (MobiSys), Florence, Italy, May 2015, pp. 151–165.
[49] K. Lee, D. Chu, E. Cuervo, A. Wolman, and J. Flinn, ‘‘Demo: DeLorean:
Using speculation to enable low-latency continuous interaction for
mobile cloud gaming,’’ in Proc. Annu. Int. Conf. Mobile Syst., Appl.,
Services (MobiSys), Florence, Italy, May 2015, p. 347.
[50] Y.-T. Lee, K.-T. Chen, H.-I. Su, and C.-L. Lei, ‘‘Are all games equally
cloud-gaming-friendly? An electromyographic approach,’’ in Proc. 11th
Annu. Workshop Netw. Syst. Support Games (NetGames), Venice, Italy,
Nov. 2012, pp. 1–6.
[51] Y. Li, X. Tang, and W. Cai, ‘‘Play request dispatching for efficient
virtual machine usage in cloud gaming,’’ IEEE Trans. Circuits Syst. Video
Technol., vol. 25, no. 12, pp. 2052–2063, Dec. 2015.
[52] L. Lin et al., ‘‘LiveRender: A cloud gaming system based on compressed
graphics streaming,’’ in Proc. ACM Int. Conf. Multimedia (MM), Orlando,
FL, USA, Nov. 2014, pp. 347–356.
[53] Y. Liu, S. Dey, and Y. Lu, ‘‘Enhancing video encoding for cloud gaming
using rendering information,’’ IEEE Trans. Circuits Syst. Video Technol.,
vol. 25, no. 12, pp. 1960–1974, Dec. 2015.
[54] Y. Liu, S. Wang, and S. Dey, ‘‘Modeling, characterizing, and enhancing
user experience in cloud mobile rendering,’’ in Proc. Int. Conf. Comput.,
Netw. Commun. (ICNC), Maui, HI, USA, Jan. 2012, pp. 739–745.
[55] M. Manzano, J. A. Hernandez, M. Uruena, and E. Calle, ‘‘An empirical
study of cloud gaming,’’ in Proc. 11th IEEE Annu. Workshop Netw. Syst.
Support Games (NetGames), Venice, Italy, Nov. 2012, pp. 1–2.
[56] M. Manzano, M. Urueña, M. Sužnjević, E. Calle, J. Hernández, and
M. Matijasevic, ‘‘Dissecting the protocol and network traffic of the
OnLive cloud gaming platform,’’ Multimedia Syst., vol. 20, no. 5,
pp. 451–470, Mar. 2014.
[57] M. Marzolla, S. Ferretti, and G. D’Angelo, ‘‘Dynamic resource provisioning for cloud-based gaming infrastructures,’’ ACM Comput. Entertainment, vol. 10, no. 3, pp. 4:1–4:20, Dec. 2012.
[58] D. Meilander, F. Glinka, S. Gorlatch, L. Lin, W. Zhang, and X. Liao,
‘‘Bringing mobile online games to clouds,’’ in Proc. IEEE Conf. Comput.
Commun. Workshops (INFOCOMW), Toronto, ON, Canada, Apr. 2014,
pp. 340–345.
[59] D. Mishra, M. El Zarki, A. Erbad, C.-H. Hsu, and N. Venkatasubramanian, ‘‘Clouds+Games: A multifaceted approach,’’ IEEE Internet
Comput., vol. 18, no. 3, pp. 20–27, May 2014.
[60] S. Möller, D. Pommer, J. Beyer, and J. Rake-revelant, ‘‘Factors influencing gaming QoE: Lessons learned from the evaluation of cloud gaming
services,’’ in Proc. Int. Workshop Perceptual Quality Syst. (PQS), Vienna,
Austria, Sep. 2013, pp. 1–5.
[61] C. Moreno, N. Tizon, and M. Preda, ‘‘Mobile cloud convergence in
GaaS: A business model proposition,’’ in Proc. Hawaii Int. Conf. Syst.
Sci. (HICSS), Maui, HI, USA, Jan. 2012, pp. 1344–1352.
[62] (Mar. 2010). Welcome to Native Client. [Online]. Available:
https://developer.chrome.com/native-client
[63] (Mar. 2010). Google’s Native Client Goes ARM and Beyond. [Online].
Available: http://www.h-online.com/open/news/item/Google-s-NativeClient-goes-ARM-and-beyond-957478.html
[64] X. Nan et al., ‘‘A novel cloud gaming framework using joint video and
graphics streaming,’’ in Proc. IEEE Int. Conf. Multimedia Expo (ICME),
Chengdu, China, Jul. 2014, pp. 1–6.
[65] (May 2016). NVidia Grid. [Online]. Available: http://www.nvidia.com/
object/cloud-gaming.html
[66] A. Ojala and P. Tyrvainen, ‘‘Developing cloud business models: A case
study on cloud gaming,’’ IEEE Softw., vol. 28, no. 4, pp. 42–47, Jul. 2011.
[67] (Jan. 2015). OnLive. [Online]. Available: http://www.onlive.com/
[68] (Jan. 2015). PlayStation Now. [Online]. Available: http://
www.playstation.com/en-us/explore/playstationnow/
[69] S. Prabu and S. Purushotham, ‘‘Cloud gaming with P2P network
using XAML and Windows Azure,’’ in Proc. Conf. Recent Trends
Comput., Commun. Inf. Technol. (ObCom), Vellore, India, Dec. 2011,
pp. 165–172.
[70] Z. Qi, J. Yao, C. Zhang, M. Yu, Z. Yang, and H. Guan, ‘‘VGRIS:
Virtualized GPU resource isolation and scheduling in cloud gaming,’’ ACM Trans. Archit. Code Optim., vol. 11, no. 2, pp. 203–214,
Jul. 2014.
VOLUME 4, 2016

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

[71] P. Quax, A. Beznosyk, W. Vanmontfort, R. Marx, and W. Lamotte,
‘‘An evaluation of the impact of game genre on user experience in cloud
gaming,’’ in Proc. IEEE Int. Games Innov. Conf. (IGIC), Vancouver, BC,
Canada, Sep. 2013, pp. 216–221.
[72] K. Raaen, R. Eg, and C. Griwodz, ‘‘Can gamers detect cloud delay?’’ in
Proc. Annu. Workshop Netw. Syst. Support Games (NetGames), Nagoya,
Japan, Dec. 2014, pp. 1–3.
[73] L. Riungu-kalliosaari, J. Kasurinen, and K. Smolander, ‘‘Cloud services
and cloud gaming in game development,’’ in Proc. IADIS Game Entertainment Technol. (GET), Prague, Czech Republic, Jul. 2013, pp. 1–10.
[74] P. E. Ross, ‘‘Cloud computing’s killer app: Gaming,’’ IEEE Spectr.,
vol. 46, no. 3, p. 14, Mar. 2009.
[75] (Nov. 2014). Cloud Gaming to Reach Inflection Point in 2015. [Online].
Available: http://tinyurl.com/p3z9hs2
[76] M. Semsarzadeh, M. Hemmati, A. Javadtalab, A. Yassine, and
S. Shirmohammadi, ‘‘A video encoding speed-up architecture for
cloud gaming,’’ in Proc. IEEE Int. Conf. Multimedia Expo Workshops,
Chengdu, China, Jul. 2014, pp. 1–6.
[77] M. Semsarzadeh, A. Yassine, and S. Shirmohammadi, ‘‘Video encoding
acceleration in cloud gaming,’’ IEEE Trans. Circuits Syst. Video Technol.,
vol. 25, no. 12, pp. 1975–1987, Dec. 2015.
[78] R. Shea, D. Fu, and J. Liu, ‘‘Cloud gaming: Understanding the support
from advanced virtualization and hardware,’’ IEEE Trans. Circuits Syst.
Video Technol., vol. 25, no. 12, pp. 2026–2037, Dec. 2015.
[79] R. Shea, D. Fu, and J. Liu, ‘‘Towards bridging online game playing and
live broadcasting: Design and optimization,’’ in Proc. ACM Workshop
Netw. Oper. Syst. Support Digit. Audio Video (NOSSDAV), Portland, OR,
USA, Mar. 2015, pp. 61–66.
[80] R. Shea and J. Liu, ‘‘On GPU pass-through performance for cloud gaming: Experiments and analysis,’’ in Proc. Annu. Workshop Netw. Syst.
Support for Games (NetGames’13), p. 6:1–6:6, Denver, CO, Dec. 2013.
[81] R. Shea, J. Liu, E. C.-H. Ngai, and Y. Cui, ‘‘Cloud gaming: Architecture
and performance,’’ IEEE Netw., vol. 27, no. 4, pp. 16–21, Jul./Aug. 2013.
[82] S. Shi, C.-H. Hsu, K. Nahrstedt, and R. Campbell, ‘‘Using graphics
rendering contexts to enhance the real-time video coding for mobile cloud
gaming,’’ in Proc. ACM Multimedia (MM), Nov. 2011, pp. 103–112.
[83] T. Kämäräinen, M. Siekkinen, Y. Xiao, and A. Ylä-Jääski, ‘‘Towards
pervasive and mobile gaming with distributed cloud infrastructure,’’ in
Proc. Annu. Workshop Netw. Syst. Support Games (NetGames), Nagoya,
Japan, Dec. 2014, pp. 1–6.
[84] I. Slivar, M. Suznjevic, L. Skorin-Kapov, and M. Matijasevic, ‘‘Empirical
QoE study of in-home streaming of online games,’’ in Proc. Annu. Workshop Netw. Syst. Support Games (NetGames), Nagoya, Japan, Dec. 2014,
pp. 1–6.
[85] O. Soliman, A. Rezgui, H. Soliman, and N. Manea, ‘‘Mobile cloud
gaming: Issues and challenges,’’ in Proc. Int. Conf. Mobile Web Inf.
Syst. (MobiWIS), Paphos, Cyprus, Aug. 2013, pp. 121–128.
[86] (Jul. 2012). Cloud Gaming Adoption is Accelerating . . . and Fast!
[Online]. Available: http://www.nttcom.tv/2012/07/09/cloud-gamingadoption-is-acceleratingand-fast/
[87] (Apr. 2015). Sony buys OnLive Streaming Game Service, Which Will Shut
Down Later This Month. [Online]. Available: http://goo.gl/6Xe9Dx
[88] K. Sun and D. Wu, ‘‘Video rate control strategies for cloud gaming,’’
J. Vis. Commun. Image Represent., vol. 30, pp. 234–241, Jul. 2015.
[89] M. Suznjevic, J. Beyer, L. Skorin-Kapov, S. Moller, and N. Sorsa,
‘‘Towards understanding the relationship between game type and network
traffic for cloud gaming,’’ in Proc. IEEE Int. Conf. Multimedia Expo
Workshops (ICMEW), Chengdu, China, Jul. 2014, pp. 1–6.
[90] R. Süselbeck, G. Schiele, and C. Becker, ‘‘Peer-to-peer support for lowlatency massively multiplayer online games in the cloud,’’ in Proc. 8th
Annu. Workshop Netw. Syst. Support Games (NetGames), Paris, France,
Nov. 2009, pp. 1–2.
[91] M. R. Hosseinzadeh Taher, H. Ahmadi, and M. R. Hashemi,
‘‘Power-aware analysis of H.264/AVC encoding parameters
for cloud gaming,’’ in Proc. IEEE Int. Conf. Multimedia Expo
Workshops (ICMEW), Chengdu, China, Jul. 2014, pp. 1–6.
[92] H. Tian, D. Wu, J. He, Y. Xu, and M. Chen, ‘‘On achieving cost-effective
adaptive cloud gaming in geo-distributed data centers,’’ IEEE Trans.
Circuits Syst. Video Technol., vol. 25, no. 12, pp. 2064–2077, Dec. 2015.
[93] (Jan. 2015). Ubitus. [Online]. Available: http://www.ubitus.net
[94] S. Wang and S. Dey, ‘‘Modeling and characterizing user experience in
a cloud server based mobile gaming approach,’’ in Proc. IEEE Global
Telecommun. Conf. (GLOBECOM), Honolulu, HI, USA, Nov. 2009,
pp. 1–7.
VOLUME 4, 2016

[95] S. Wang and S. Dey, ‘‘Addressing response time and video quality in
remote server based Internet mobile gaming,’’ in Proc. IEEE Wireless
Commun. Netw. Conf., Sydney, NSW, Australia, Apr. 2010, pp. 1–6.
[96] S. Wang and S. Dey, ‘‘Rendering adaptation to address communication
and computation constraints in cloud mobile gaming,’’ in Proc. IEEE
Global Telecommun. Conf. (GLOBECOM), Miami, FL, USA, Dec. 2010,
pp. 1–6.
[97] S. Wang and S. Dey, ‘‘Cloud mobile gaming: Modeling and measuring user experience in mobile wireless networks,’’ ACM Trans. Mobile
Comput. Commun. Rev., vol. 16, no. 1, pp. 10–21, Jan. 2012.
[98] S. Wang, Y. Liu, and S. Dey, ‘‘Wireless network aware cloud scheduler for
scalable cloud mobile gaming,’’ in Proc. IEEE Int. Conf. Commun. (ICC),
Ottawa, ON, Canada, Jun. 2012, pp. 2081–2086.
[99] J. Wu, C. Yuen, N.-M. Cheung, J. Chen, and C. W. Chen, ‘‘Enabling
adaptive high-frame-rate video streaming in mobile cloud gaming applications,’’ IEEE Trans. Circuits Syst. Video Technol., vol. 25, no. 12,
pp. 1988–2001, Dec. 2015.
[100] Z. Wu, ‘‘Gaming in the cloud: One of the future entertainment,’’ in Proc.
Interact. Multimedia Conf., Southampton, U.K., Jan. 2014, pp. 1–6.
[101] L. Xu, X. Guo, Y. Lu, S. Li, O. Au, and L. Fang, ‘‘A low latency cloud
gaming system using edge preserved image homography,’’ in Proc. IEEE
Int. Conf. Multimedia Expo (ICME), Chengdu, China, Jul. 2014, pp. 1–6.
[102] Z. Xue, D. Wu, J. He, X. Hei, and Y. Liu, ‘‘Playing high-end video games
in the cloud: A measurement study,’’ IEEE Trans. Circuits Syst. Video
Technol., vol. 25, no. 12, pp. 2013–2025, Dec. 2015.
[103] C. Zhang, Z. Qi, J. Yao, M. Yu, and H. Guan, ‘‘vGASA: Adaptive scheduling algorithm of virtualized GPU resource in cloud gaming,’’ IEEE Trans.
Parallel Distrib. Syst., vol. 25, no. 11, pp. 3036–3045, Nov. 2014.
[104] Z. Zhao, K. Hwang, and J. Villeta, ‘‘Game cloud design with
virtualized CPU/GPU servers and initial performance results,’’ in Proc.
Workshop Sci. Cloud Comput. Date (ScienceCloud), Delft,
The Netherlands, Jun. 2012, pp. 23–30.

WEI CAI [S’12–M’16] received the B.Eng. degree
in software engineering from Xiamen University
in 2008, the M.S. degree in electrical engineering
and computer science from Seoul National University in 2011, and the Ph.D. degree in electrical
and computer engineering from The University of
British Columbia (UBC), Vancouver, BC, Canada,
in 2016. He is currently a Post-Doctoral Research
Fellow with the UBC. He has completed visiting
researches with Academia Sinica, The Hong Kong
Polytechnic University, and National Institute of Informatics, Japan. His
researches focus on gaming as a service, mobile cloud computing, online
gaming, software engineering, and interactive multimedia. He received
awards of the 2015 Chinese Government Award for the Outstanding SelfFinanced Students Abroad, UBC Doctoral Four-Year-Fellowship, Brain
Korea 21 Scholarship, and Excellent Student Scholarship from the Bank
of China. He is also a co-recipient of the best paper awards from the
CloudCom2014, the SmartComp2014, and the CloudComp2013.

RYAN SHEA [S’08–M’16] received the
Ph.D. degree in computer science from Simon
Fraser University, Burnaby, BC, Canada, in 2016.
He is currently a University Research Associate
with the Big Data Systems. His research interests
include computer and network virtualization, performance issues in cloud computing, and energy
and performance issues with the Big Data Systems. He has received the Natural Sciences and
Engineering Research Council of Canada, Alexander Graham Bell Canada Graduate Scholarship in 2013. He was a recipient
of the best student paper award from the IEEE/ACM 21st International
Workshop on Quality of Service for his paper Understanding the Impact
of Denial of Service Attacks on Virtual Machines in 2012. His recent
publications include a point of view article in the Proceedings of the IEEE
entitled The Future of Cloud Gaming.
7619

W. Cai et al.: Survey on Cloud Gaming: Future of Computer Games

CHUN-YING HUANG [S’03–M’08] received the
Ph.D. degree in electrical engineering department from National Taiwan University in 2007.
He leads the Security and Systems Laboratory
with National Chiao Tung University. From 2008
to 2013, he was an Assistant Professor with the
Department of Computer Science and Engineering, National Taiwan Ocean University, where he
was an Associate Professor from 2013 to 2016.
He has been an Associate Professor with the
Department of Computer Science, National Chiao Tung University, since
2016. His research interests include system security, multimedia networking,
and mobile computing. He is a member of ACM.
KUAN-TA CHEN [S’04–M’06–SM’15] received
the B.S. and M.S. degrees in computer science
from National Tsing-Hua University, in 1998 and
2000, respectively, and the Ph.D. degree in electrical engineering from National Taiwan University
in 2006. He is a Research Fellow with the Institute
of Information Science, and the Research Center for Information Technology Innovation (joint
appointment) of Academia Sinica. His research
interests include quality of experience, multimedia
systems, and social computing. He received the best paper award in the
IWSEC 2008 and the K. T. Li Distinguished Young Scholar Award from
ACM Taipei/Taiwan Chapter in 2009. He also received the Outstanding
Young Electrical Engineer Award from The Chinese Institute of Electrical
Engineering in 2010, the Young Scholar’s Creativity Award from Foundation
for the Advancement of Outstanding Scholarship in 2013, and the IEEE
ComSoc MMTC Best Journal Paper Award in 2014. He was an Associate
Editor of IEEE TRANSACTIONS ON MULTIMEDIA from 2011 to 2014 and has
been an Associate Editor of ACM Transactions on Multimedia Computing,
Communications, and Applications since 2015. He is a Senior Member
of ACM.
JIANGCHUAN LIU [S’01–M’03–SM’08] received
the B.Eng. (cum laude) degree in computer science
from Tsinghua University, Beijing, China, in 1999,
and the Ph.D. degree in computer science from The
Hong Kong University of Science and Technology
in 2003.
He was an Assistant Professor with The Chinese
University of Hong Kong from 2003 to 2004.
He is a University Professor with the School
of Computing Science, Simon Fraser University,
British Columbia, Canada, and an NSERC E.W.R. Steacie Memorial Fellow.
He is an EMC-Endowed Visiting Chair Professor of Tsinghua University,
Beijing, from 2013 to 2016. His research interests include multimedia
systems and networks, cloud computing, social networking, online gaming,
big data computing, wireless sensor networks, and peer-to-peer networks.
Dr. Liu is a co-recipient of the inaugural Test of Time Paper
Award of the IEEE INFOCOM (2015), the ACM SIGMM TOMCCAP
Nicolas D. Georganas best paper award (2013), the ACM Multimedia
best paper award (2012), the IEEE Globecom best paper award (2011),
and the IEEE Communications Society best paper award on Multimedia
Communications (2009). His students received the Best Student Paper Award
of the IEEE/ACM IWQoS in 2008 and 2012. He has served on the editorial
boards of the IEEE TRANSACTIONS ON BIG DATA, the IEEE TRANSACTIONS ON
MULTIMEDIA, the IEEE COMMUNICATIONS SURVEYS AND TUTORIALS, the IEEE
ACCESS, the IEEE INTERNET OF THINGS JOURNAL, Computer Communications,
and Wiley Wireless Communications and Mobile Computing. He is the
Steering Committee Chair of the IEEE/ACM IWQoS (2015–2017) and
TPC Co-Chair of the IEEE IC2E’2017, and the IEEE/ACM IWQoS’2014.
He serves as an Area Chair of the IEEE INFOCOM, ACM Multimedia, and
the IEEE ICME. According to Google Scholar, the citations of his papers are
over 10,000, and as an h-index of 46.

7620

VICTOR C. M. LEUNG [S’75–M’89–SM’97–
F’03] is a Professor of Electrical and Computer
Engineering and holder of the TELUS Mobility
Research Chair with The University of British
Columbia (UBC). His research is in the areas of
wireless networks and mobile systems. He has
co-authored over 900 technical papers in archival
journals and refereed conference proceedings, several of which had received best paper awards.
He is a fellow of the Royal Society of Canada,
the Canadian Academy of Engineering, and the Engineering Institute
of Canada. He is serving or has served on the editorial boards of the
IEEE ACCESS, the IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS –
SERIES ON GREEN COMMUNICATIONS AND NETWORKING, the IEEE WIRELESS
COMMUNICATIONS LETTERS, the IEEE TRANSACTIONS ON COMPUTERS, the IEEE
TRANSACTIONS ON WIRELESS COMMUNICATIONS, and the IEEE TRANSACTIONS
ON VEHICULAR TECHNOLOGY, and several other journals. He has provided
leadership to the technical program committees and organizing committees
of numerous international conferences. He was the recipient of the 1977
APEBC Gold Medal, NSERC Postgraduate Scholarships from 1977 to 1981,
2012 UBC Killam Research Prize, and the IEEE Vancouver Section Centennial Award.

CHENG-HSIN HSU [S’09–M’10–SM’16] received
M.S./B.S. degrees from National Chung-Cheng
University, the M.Eng. degree from the University
of Maryland, and the Ph.D. degree from Simon
Fraser University. He was with the Deutsche
Telekom Laboratory, Motorola Inc., and Lucent
Technologies. He has been an Associate Professor with the Department of Computer Science,
National Tsing Hua University, since 2014, where
he was an Assistant Professor from 2011 to 2014.
He visited the University of California at Irvine, Qatar Computing Research
Institute, and University of Illinois Urbana–Champaign, in 2013, 2014, and
2015, respectively. His research interests are in multimedia networking,
mobile computing, and computer networks. He has been served as an
Associate Editor of ACM Transactions on Multimedia Computing, Communications, and Applications since 2014, and the IEEE MMTC E-LETTER
from 2012 to 2014. He and his colleagues received the best paper award
from the IEEE RTAS’12, the TAOS best paper award from the IEEE
GLOBECOM’12, best paper award from the IEEE Innovation’08, and the
Best Demo Award from the ACM Multimedia’08.

VOLUME 4, 2016

SPECIAL SECTION ON RECENT ADVANCES IN SOCIALLY-AWARE MOBILE NETWORKING
Received August 31, 2016, accepted September 11, 2016, date of publication September 21, 2016,
date of current version October 31, 2016.
Digital Object Identifier 10.1109/ACCESS.2016.2611863

Social-Aware Data Collection Scheme Through
Opportunistic Communication in Vehicular
Mobile Networks
ZHIPENG TANG1 , ANFENG LIU1 , AND CHANGQIN HUANG2

1 School
2 School

of Information Science and Engineering, Central South University, Changsha 410083, China
of Information Technology in Education, South China Normal University, Guangzhou 510631, China

Corresponding author: A. F. Liu (afengliu@mail.csu.edu.cn)
This work was supported in part by the National Natural Science Foundation of China under Grant 61379110, Grant 61073104,
Grant 61370229, and Grant 61370178, in part by the National Basic Research Program of China (973 Program) under
Grant 2014CB046305, in part by the National Key Technology R&D Program of China under Grant 2014BAH28F02,
in part by the S &T Projects of Guangdong Province under Grant 2014B010103004, Grant 2014B010117007,
Grant 2015A030401087, Grant 2015B010110002, and Grant 2016B010109008, and in part by GDUPS (2015).

ABSTRACT To enable the intelligent management of Smart City and improve overall social welfare, it is
desirable for the status of infrastructures detected and reported by intelligent devices embedded in them to
be forwarded to the data centers. Using ‘‘SCmules’’ such as taxis, to opportunistically communicate with
intelligent devices and collect data from the sparse networks formed by them in the process of moving
is an economical and effective way to achieve this goal. In this paper, the social welfare data collection
paradigm SWDCP-SCmules data collection framework is proposed to collect data generated by intelligent
devices and forward them to data centers, in which ‘‘SCmules’’ are data transmitters picking up data from
nearby intelligent devices and then store-carry-forwarding them to nearby data centers via short-range
wireless connections in the process of moving. Because of the storage limitations, ‘‘SCmules’’ need to weigh
the value of data and select some less valuable data to discard when necessary. To quantify the value of data
and find a well-performed selection strategy, the concept of priority is introduced to the SWDCP-SCmules
scheme, and then, the simulated annealing for priority assignment SA-PA algorithm is proposed to guide
the priority assignment. The SA-PA algorithm is a universal algorithm that can improve the performance of
SWDCP-SCmules scheme by finding better priority assignment with respect to various optimization targets,
such as maximizing collection rate or minimizing redundancy rate, in which priority assignment problem is
converted into an optimization problem and simulated annealing is used to optimize the priority assignment.
From the perspective of machine learning, the process of optimization is equal to automatically learn socialaware patterns from past GPS trajectory data. Experiments based on real GPS trajectory data of taxis in
Beijing are conducted to show the effectiveness and efficiency of SWDCP-SCmules scheme and
SA-PA algorithm.
INDEX TERMS Vehicular mobile networks, social welfare, data collection, opportunistic communication,
oblivious data mules, simulated annealing, machine learning.

I. INTRODUCTION

Internet of Things (IoT) is experiencing extremely rapid
development. There are several phenomena which can prove
this trends: Firstly, the amount of data generated by IoT has
been growing exponentially since past few years [1]–[9].
A Cisco’s report reveals that the overall data traffic of
IoT in 2014 has grown to 69 percent and is approximately 30 times the size of the entire global Internet
6480

in 2000 [3], [10]. Secondly, the number of connected devices
have already exceeded the total number of people living on
Earth since 2011. Connected devices have reached 9 billion
and are expected to reach 24 billion by 2020 [11], [12].
Thirdly, in 2012, application systems based on IoT has contributed $4.8 trillion to revenue of international corporations
and the number is still soaring since then, according to
another report from Cisco [10], [13].

2169-3536 
 2016 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

Due to the rapid development of IoT, the intelligent management of city becomes possible, which enables us to
achieve the dream of Smart City.
In the process of constructing Smart City, sensors and
actuators will definitely find their place because of their low
costs and energy consumption [14]–[17]. Intelligent devices,
which are equipped with various sensors and actuators, can
be embedded into infrastructures of Smart City to enhance
their productivity and functionality, such as detecting and
reporting the status of infrastructures [14]. For example:
garbage cans with intelligent devices can monitor their trash
level, if they are filled up with trashes, the intelligent devices
will detect it and send request for cleaning; Street lights
with intelligent devices can detect their status, such as the
power supply and the condition of bulbs, and send request
for maintaining. More applications can be found in the fields
of electricity facility, communication system, contamination
monitoring, sanitary engineering, etc [18].
Although the form of applications is distinct, they all share
a common abstract paradigm: detecting status of infrastructures and forwarding it to related municipal departments,
which will then take measures to maintain the infrastructures. This paradigm realizes the intelligent management
of city infrastructures, which will make a more comfortable city environment and improve social welfare significantly [17], [18]. In this paper, this paradigm is called Social
Welfare Data Collection Paradigm (SWDCP Paradigm).
To implement SWDCP paradigm, how to establish network
connections between the large amount of intelligent devices
distributed in the whole city and municipal departments is a
core problem [17].
In fact, the way to establish such kind of network connections really depends on the requirements of communication:
For real time communications, there are two feasible
approaches: the first one is to equip each intelligent device
with a SIM card [17], the second one is to build permanent
base stations to relay data/messages generated by intelligent
devices located in their coverage area [19]. However, they
both have their own shortcomings. For the first approach, the
high costs and energy consumption sharply reduce the advantage of this approach [20], [21]. Besides that, this approach
potentially increases the number of devices occupying radio
frequency (RF): concretely speaking, RF is a kind of valuable
limited communication resource. Devices occupying RF have
already reached 9 billion and are expected to reach 24 billion
by 2020 [22]. The first approach will significantly increase
such devices and thereby make the problem of the scarcity of
RF much more serious [1], [3], [8]. For the second approach,
as new infrastructures are built and old infrastructures are
removed in the process of city construction, the distribution
of intelligent devices in the city also constantly change, which
makes building permanent base stations covering all intelligent devices impossible [17].
However, for most applications in SWDCP paradigm,
real time communication is not necessary, That is to say,
the data/messages generated by devices are not necessary
VOLUME 4, 2016

to be report to municipal departments immediately, some
delay is acceptable. For example, when the bulb of a street
light doesn’t work, the intelligent device embedded in this
street light doesn’t need to report this status to maintenance
department immediately since one street light’s malfunction
doesn’t influence the overall illumination of the city, several hours’ data transmission delay is acceptable. Similarly,
when a garbage can is filled with trashes, the intelligent
device embedded in this garbage can doesn’t need to report
this status to sanitary department immediately since there
are other available garbage cans nearby, several days’ data
transmission delay is acceptable [17]. Based on this feature,
Bonola et al. think this type of network connections
shares a lot of common features with Delay Tolerant
Network (DTN) [17], [23].
Bonola et al. further proposed a data collection scheme
for this kind of network connection based on Oblivious Data
Mules [17]. There are three entities in their scheme: oblivious
data mules, intelligent devices and data centers. Oblivious
data mules (hereinafter referred to as mules), which are
inspired by the work of Shah et al. [19], are mobile IoT nodes
moving in Smart City and can opportunistically communicate
with intelligent devices via short-range wireless communication to pick up data/messages and buffer them in their
own storage. When mules move to the neighborhood of data
centers, the buffered data/messages will be forwarded to data
centers via short-range wireless communication. Intelligent
devices are incorporated into the infrastructures of Smart
City, they are capable of detecting the status of infrastructures and sending the data/messages to mules via short-range
wireless communication. Data centers are special computing and processing nodes distributed in fixed locations of
Smart City, they can connect with cloud tier via high-speed
network to forward data/messages, their major responsibility
is receiving data/messages buffered by nearby mules and
forwarding them to cloud tier, which is accessible to corresponding municipal departments. Once municipal departments receive these data/messages, they will take measures
to maintain related infrastructures. Most data centers are
deployed in downtown area of Smart City to boost collection
efficiency.
Here is a concrete instance: many objects can be regarded
as mules in Smart City, such as taxis, buses and pedestrians
holding portable communication devices [24], [25]. Taxis are
the most ideal among them since they have several great
features: they move independently according to customers’
demands and can cover an extensive area of Smart City with
somewhat random paths (with respect to buses that move
only on main streets) and work 24/7. Taxis equipped with
transceivers, which are capable of communicating via shortrange wireless networks and exchanging data with neighboring intelligent devices and data centers [19], can be regarded
as mobile IoT nodes, i.e. mules. The transceivers are incorporated into the chips of taxis and can automatically incidentally
pick up data/messages sent by nearby intelligent devices
when taxis are moving in Smart City to send passengers to
6481

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

their destinations. The data/messages incidentally collected
in the process of motion are buffered and then dumped to
data centers when taxis are passing near them. Data centers
then simply filter out duplicated data/messages and forward
the remaining information to cloud tier via the high-speed
network connection. The relative municipal departments can
access these data/messages from cloud tier and then take
measures to maintain corresponding infrastructures. Note that
the major task of taxis is serving passengers, the process of
data/messages collection is incidental, which is automatically
manipulated by transceivers, rather than drivers, that’s why
we call data mules oblivious.
The scheme proposed by Bonola et al. is cost-saving
and energy-efficient [17]. However, this model can still be
refined because it assumes the storage of mules is infinite,
that is to say, mules can store all data/messages picked up
along the course of moving and don’t need to worry about
the situation that their storage is full. Conversely, in reality, infinite storage is of impossibility and mules can only
store limited data/messages. We call this refined model of
mules as Storage-constrained oblivious data mules (hereinafter referred to as SCmules). Due to the storage limitation,
SCmules have to discard some less important data/messages
stored in buffer to make some free space in the situation that
their storage is full and new more valuable data/messages are
picked up. This situation occurs frequently because of the
large number of intelligent devices installed in city. The selection strategy of how to choose more valuable data/messages
to store and less valuable data/messages to discard plays a
vital role in improving data/messages collection efficiency
and overall social welfare of Smart City [26]. To prove the
vitality of the selection strategy, we continue to use the aforementioned example to illustrate it: If the selection strategy
is simply discarding new picked-up data/messages when the
storage is full, a bad phenomenon will occur: data centers will
receive a lot of duplications of data/messages from the downtown areas of a city, i.e. hotspot areas, but few data/messages
from other areas, especially the remote areas, are collected.
This is because the hotspot areas are always the transportation
hubs of the city. The frequency that taxis pass by there is
much higher than the frequency of other areas. Therefore, the
probability that data/messages from hotspot areas are picked
up by taxis is much higher than the probability of other areas,
i.e. data/messages from hotspot areas have higher probability
of occupying the storage and then being forwarded to data
centers. Conversely, when taxis pass through remote area, it is
likely to have no more storage to pick up new data/messages
from there. So this example illustrates a bad selection strategy
will lead to low data/messages collection efficiency and poor
social welfare.
Now that the selection strategy is of vitality, how can we
find an effective and efficient selection strategy? To answer
this question, two tasks have to be finished: the first one is
to introduce a universal way to formally describe selection
strategy, the second one is to find a well-performed selection
strategy.
6482

A natural way to formally describe selection strategy is to
introduce the concept of priority: We assign each intelligent
device with a unique priority. Every registered SCmule owns
a priority table, which records the priorities of all intelligent devices. The priority table is used to guide how to
weigh the importance of data/messages. In detail, when the
storage of a SCmule is not full, this SCmule will greedily
buffer data/messages as much as possible; but when the
storage is full, this SCmule will use the priority table to
weigh the priority of new picked-up data/messages with the
priorities of those stored in buffer. If the new picked-up
one’s priority is less than the priorities of all data/messages
buffered, it will be simply discarded; otherwise, it will replace
the buffered data/message with the least priority. The main
principle of this process is to maximize the sum of priorities of data/messages buffered, which is referred as Greedy
Selection Principle.
Obviously, the priority model is a natural and effective way
to formally describe the selection strategy. In the example of
illustrating the vitality of selection strategy, the poor selection
strategy we used can be regarded as assigning each intelligent
devices with equal priorities.
By combing SCmules, the model of priority and greedy
selection principle, we propose a new concrete implementation of SWDCP paradigm for DTN, which is called
Social Welfare Data Collection Paradigm based on StorageConstrained Oblivious Data Mules (SWDCP-SCmules
scheme).
To implement SWDCP-SCmules, we still have to address
the second task which is how to assign priority to make
an effective and efficient selection strategy. We refer to this
problem as Priority Assignment Problem.
However, the priority assignment problem is really a challenge since there are three potential difficulties:
(1) The meaning of effectiveness and efficiency is ambiguous, different scenarios have different interpretations and correspond to different priority assignment
methods. So it is troublesome to design new algorithms for every concrete interpretation of effectiveness
and efficiency. For example, as will be described in
section 3.2, in most cases, the definition of effectiveness and efficiency corresponds to maximizing collection rate and minimizing redundancy rate, but there are
always some new demands, such as reducing collection delay of some important infrastructures, emerging
with the advent of new applications. The algorithm
designed for original demands fails to optimize these
new demands and require a complete redesign. It is
too troublesome to design new algorithm for every new
demand.
(2) Even if we have an accurate definition of effectiveness
and efficiency, i.e. optimization target, it is still too hard
to design an appropriate algorithm that can optimize
the performance of priority assignment according to
this definition, not to mention a universal algorithm
that can solve all unpredictable optimization targets.
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

For example, if the optimization target is to maximize
collection rate and minimize redundancy rate, finding an effective pattern that can support the design
of algorithm needs vast amounts of manual observation and thinking. In fact, it is impossible for human
intelligence to perceive all the potential patterns lying
in essence. Even if we have found an important pattern, e.g. decreasing the priorities of intelligent devices
near data centers and increasing the priorities of those
far from data centers can increase collection rate and
decrease redundancy rate, you will still probably ignore
the fact that this pattern doesn’t work when the intelligent devices locate in the end point of one-way streets
(In section 5.1, we will give a detailed possible explanation of how this pattern works and why it fails in that
case).
(3) The most fatal problem is that patterns are not static.
As time goes on, new patterns emerge, old patterns
disappear and some patterns change periodically. For
example, in hot summer, district near natatoriums will
become hotspot areas and share the common features
of downtown areas, but in other seasons, these districts
will be ordinary areas.
According to the analysis above, we can infer that it is
difficult to solve priority assignment problem using traditional approach, which is finding effective patterns and then
using these patterns to design algorithms. However, we can
consider to use the idea from machine learning, that is, to
use learning algorithm to automatically learn social-aware
patterns or other information from data and then utilize them
to address this challenging problem.
The basis of why learning algorithm works is that there
are a lot of social-aware patterns, which are patterns that can
reflect the social preferences of citizens in Smart City, lying
in the GPS trajectory data of SCmules. Concretely speaking,
the GPS trajectory data of SCmules can reflect the overall
social preferences of SCmules (i.e. the social preferences of
citizens in Smart City). To prove this statement, we briefly
illustrate an instance in this section (In section 5.1, we will
explore the meaning of social-awareness more deeply): the
GPS trajectory data of taxis, one of the typical examples
of SCmules, can implicitly show the social preferences of
citizens in the Smart City, such as the hotspot areas of Smart
City in certain time interval, the preferred destinations of
passengers in certain time interval, the preferred routes that
drivers take in certain time interval and many other social
labels. Therefore, we can use learning algorithm to automatically mine for these potential social-aware patterns lying in
the data without any manual teaching and use them to make
a better priority assignment to improve the performance of
SWDCP-SCmules scheme.
In this paper, we try to convert the priority assignment
problem to an optimization problem based on the idea from
machine learning and then use simulated annealing metaheuristic to design a universal algorithm that can automatically find a priority assignment of good quality with respect
VOLUME 4, 2016

to various optimization targets based on the social-aware
patterns lying in the GPS trajectory data of SCmules in the
past time. Concretely speaking, first, we need to specify the
meaning of effectiveness and efficiency, that is to say, we
need to use a performance measure to quantify the meaning
of effectiveness and efficiency. The performance measure
we used is the optimization target function J of priority
E then, due to the future shares similar socialassignment P;
aware patterns with the past, we can use the past GPS traE that
jectory data of SCmules to train a priority assignment P
can optimize the optimization target function J and predict
E will also achieve good performance in the future.
that P
Because of the complexity of the computation of J with
E based on the past GPS trajectory data, we apply
respect to P
simulated annealing metaheuristic to search for appropriate
E in search space that can optimize J. We refer to this method
P
of solving priority assignment problem as Simulated Annealing for Priority Assignment Algorithm (SA-PA algorithm).
Based on real GPS trajectory dataset of taxis during the
period of Feb. 2 to Feb. 8, 2008 in Beijing, the effectiveness
of SA-PA algorithm can be proved in section 6.
The main contributions of this paper can be described as
three points:
(1) We propose a Social Welfare Data Collection
Paradigm based on Storage-Constrained Oblivious
Data Mules (SWDCP-SCmules scheme) in which
storage-constrained oblivious data mules can incidentally pick up data/messages generated by intelligent devices embedded in infrastructures of Smart
City in the process of motion and store-carry-forward
them to data centers, which will then notify related
department to maintain corresponding infrastructures.
Unlike the previous research, in SWDCP-SCmules
scheme, SCmules are storage-constrained, which
is much more real than the previous model.
SWDCP-SCmules scheme enables the intelligent management of city to be possible and leads to the improvement of overall social welfare.
(2) Because of the storage limitation, SCmules have to
weigh the importance of data/messages and discard
some less important data/messages stored in the situation that their storage is full and new valuable
data/messages are picked up. Therefore, the selection
strategy is extremely important in SWDCP-SCmules.
In this paper, the concept of priority is introduced
to model the selection strategy to the priority assignment. Every intelligent device is assigned with a
unique priority, which forms the priority table. Based
on priority table, SCmules use greedy selection principle, which is maximizing the sum of priority of
data/messages buffered, to guide the storing of more
important data/messages and the discarding of less
important data/messages.
(3) To find an optimized priority assignment, Simulated
Annealing
for
Priority
Assignment
6483

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

Algorithm (SA-PA algorithm) is proposed to learn
social-aware patterns from GPS trajectory data of
SCmules. The SA-PA algorithm converts the priority
assignment problem to an optimization problem based
on the idea from machine learning and makes a simulated annealing-based universal algorithm that can
automatically find priority assignments of good quality
with respect to various optimization targets based on
the GPS trajectory data of SCmules in the past time.
Below is the organization of the rest of this paper:
In section 2, related works are reviewed. In section 3, the
system model and problem statement of SWDCP-SCmules
scheme and SA-PA algorithm are described. In section 4,
SWDCP-SCmules scheme is proposed in detail. In section 5,
SA-PA algorithm is described in detail. In section 6,
experimental results are given to show the performance
of SWDCP-SCmules scheme and SA-PA algorithm.
In section 7, the conclusion of this paper is described.
II. RELATED WORK
A. DATA COLLECTION FRAMEWORK OF IoT

We first observe the structure of SWDCP-SCmules scheme
from the perspective of data collection tasks. As illustrated in
Figure 1, IoT data collection framework can be divided into
2 tiers: the data collection tier (DCT) and the cloud tier (CT).
The main function of DCT, which is the base tier of IoT data
collection framework, is sensing and collecting data [27]. The
main function of CT, which is the core tier of IoT, is refining
data collected by DCT and convert them to the elements of
service [10].

FIGURE 1. The data collection framework of IoT.

For example, the network constructed in SWDCPSCmules scheme can be seen as a kind of vehicular mobile
networks, a concrete form of the IoT data collection framework, in a narrow sense. In DCT, sensors are incorporated
into infrastructures of the city [28] to sense the status
6484

of them. Vehicles opportunistically communicate with nearby
sensors to pick up data/messages generated by the nearby
sensors and then store-carry-forward to nearby data centers
when passing the nearby data centers [17]. Data centers then
send the received data/messages from DCT to CT via highspeed network connections to notify related departments to
maintain corresponding infrastructures.
In this paper, we focus on the DCT part of the IoT data
collection framework. According to the difference of applications, DCT can be categorized as different networks, such
as wireless sensor networks [5], [6], [14], [15], [26], [28],
participatory sensing networks [18], [26], [29], crowd sensing
networks [29], vehicular mobile networks [17] and delay
tolerant networks [23]. The researches on wireless sensor
networks (WSNs) start comparatively earlier [30]. In WSNs,
a large amount of wireless sensor nodes are deployed in
geographic position to construct a fine-grained network connections. WSNs can be used to monitor certain objects or
indexes in geographic areas, such as local temperatures [31].
Unlike WSNs, the main participant objects in participatory
sensing networks and crowd sensing networks are people
holding mobile smart phones which are equipped with sensing devices. As people move around city, those mobile smart
phones automatically sense and collect data using sensing
devices and communicate via cell networks [18], [26], [29].
The two main features of participatory sensing networks
and crowd sensing networks are that the number of sensing
devices is large and these sensing devices can move around to
sense and collect data. Vehicular mobile networks are another
form of networks in DCT. The basis of its network connection
is the motion of vehicles and their opportunistic communication [17]. In delay tolerant networks (DTNs), all sensors
are moving around and opportunistically communicate with
each other to exchange data [23]. The major feature of DTNS
is that transmission delay is not an important measure of
network performance.
In this paper, the networks constructed by the
SWDCP-SCmules scheme can be regarded as a synthesis of
these five networks mentioned above [17]. In the synthesized
networks, the geographic locations of intelligent devices,
which are responsible for sensing and collecting data, are
fixed, which is similar to sensors in WSNs. But SCmules,
which are responsible for collecting and transferring data,
are moving around the city to pick up data and store-carryforward to data centers, which is same as vehicular mobile
networks. The data collection process of the synthesized
networks relies on the extensive participation of SCmules,
which is similar to participatory sensing networks and crowd
sensing networks. The transmission delay of data collection
is not that important in the synthesized networks, which is
similar to DTNs.
In the rest of this subsection, we will introduce these five
networks in detail, i.e. wireless sensor networks, participatory sensing networks and crowd sensing networks, vehicular
mobile networks, delay tolerant networks and their relations
to the SWDCP-SCmules scheme respectively.
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

1) WIRELESS SENSOR NETWORKS

In early days, WSNs are put forward to monitor certain
objects or indexes in local areas by deploying a large
amount of wireless sensor nodes to form fine-grained
networks [2], [5], [6], [9], [16], [21], [31]. This is somewhat
similar to SWDCP-SCmules scheme’s intelligent devices
embedded in infrastructures. But a major difference between
them is the deployment of intelligent devices is planless,
sporadic and geographically sparse, which means they will
not form self-organized networks like WSNs. Therefore, the
data collection process relies on the motion and store-carryforward mechanism of SCmules, rather than the fine-grained
networks formed by wireless sensors.
There are two major elements in WSNs: sensors and sinks.
Sensors are ordinary wireless sensor nodes which is responsible for sensing data. Besides sensors, there are some special
wireless sensor nodes called sinks deployed in WSNs, which
can be regarded as the destination of data collection.
Routing algorithm is the algorithm that routes data from
sensors to sinks [2], [3], [31]. In WSNs, the key problem
of data collection is to find an efficient routing algorithm,
that is to say, to find a routing path from sensors to sinks
with high quality of service (QoS). Some typical measures of
QoS are energy consumption, network lifetime, transmission
delay and packet loss rate. Usually, there exist some tradeoffs
between different measures of QoS, for example, reducing
transmission delay and packet loss rate will lead to higher
energy consumption and shorter network lifetime [2], [3], [9],
[26], [31]. Unlike WSNs, routing problem is not a problem
in SWDCP-SCmules scheme, because the intelligent devices
cannot form fine-grained networks and data transmission
relies on the opportunistic communications among SCmules,
intelligent devices and data centers.
Another relevant networks is wireless sensor actor
networks (WSANs). WSANs consist of a large amount of
inexpensive sensor nodes and a small amount of expensive
powerful mobile actor nodes (e.g. robots) [32]. After receiving messages (e.g. fire alarm) sent by a certain sensor node,
actor nodes (e.g. robots) will move to the position of this
sensor node and deal with the emergency reported in the
messages (e.g. putting out the fire). In WSANs, actor nodes
move constantly, therefore how to effectively and reliably
route the messages generated by sensor nodes to moving actor
nodes becomes the central problem in WSANs.
In some forms of WSNs, mule nodes are adopted to relay
data/messages [24], [25]. In most cases, there are some
hotspot areas, in which energy consumption is much higher
than other areas, in WSNs. Mule nodes relay data between
sinks and hotspot areas to reduce energy consumption in
hotspot areas, thereby increasing network lifetime.
2) PARTICIPATORY SENSING NETWORKS &
CROWD SENSING NETWORKS

With the rapid development of portable devices, participatory sensing networks and crowd sensing networks
have great potential in collecting and sharing sensing
VOLUME 4, 2016

information through smartphones and other portable
devices [18], [26], [29].
The main feature of participatory sensing networks and
crowd sensing networks is that a large amount of sensing
devices participate in monitoring and sensing the same type
of data to accomplish the same task. For example, when
the task is monitoring PM 2.5 of a city, it is too costly
to establish a lot of PM 2.5 monitor stations distributed in
the whole city. In addition, due to the limited number of
PM 2.5 monitors, the accuracy of results is also limited. But
when using participatory sensing networks and crowd sensing
networks, citizens’ mobile phones can sense and report their
location’s PM 2.5 spontaneously, which significantly reduce
the costs and increase the accuracy of results [18], [29].
In participatory sensing networks and crowd sensing networks, portable devices can directly connect to Internet to
share data, therefore the focus of participatory sensing networks and crowd sensing networks is not how to forward the
data collected by portable devices to centralized station, but
how to motivate users to participate in the process of collecting sensing data. A practical adaptive incentive strategy to
motivate the enthusiasm of users is to offer rewards to data
reporters [18]. In this strategy, data are linked to rewards
(e.g. money). If we want to collect more data of a certain
part (e.g. the PM 2.5 of a certain area), we can increase the
rewards to reporters who report this part of data. Conversely,
if we want to collect less data of another certain part, we
can decrease the rewards to reporters who reports this part
of data. The adaptive adjustment of rewards will finally level
off to market equilibrium, which will increase the overall
efficiency.
In the SWDCP-SCmules scheme, we don’t adopt this
adaptive incentive strategy. The strategy we adopt is that
fixed rewards are given to users who are willing to install
transceivers and participate in the collection of sensing data,
i.e. SCmules. In this strategy, the existence of transceivers is
nearly transparent to users. Transceivers automatically sense
and collect data according to unified arrangement from data
collector. Users don’t need to worry about the running of
transceivers, to get rewards, what they need to do is to carry
the transceivers when they are moving around the city. This
unified strategy can increase the overall efficiency and avoid
Prisoner’s dilemma.
There is a problem in the participatory sensing networks
and crowd sensing networks. That is, there are some hotspot
areas in the networks where more portable devices locate.
Data from hotspot areas is much easier to be collected than
data from other areas. If all portable devices report the same
amount of data, the amount of data we collected from hotspot
areas will be excessive, but the amount of data we collected
from other areas will be not enough [26]. SWDCP-SCmules
scheme also have similar problem. Tham et al. proposed
a data collection scheme based on Quality of Contribution
which can balance the collection of sensing data in its covered area [29]. Besides this, Quality of Information is also
proposed to address this problem [33].
6485

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

3) VEHICULAR MOBILE NETWORKS

In vehicular mobile networks, vehicles spontaneously
connect with each other via wireless networks to exchange
data [34]. A wide range of applications can be realized by
vehicular mobile networks, such as traffic jam reporting [35].
At present, the research of vehicular mobile networks focus
on the communication of vehicles to vehicles (V2V) and
vehicles to devices (V2D).
SWDCP-SCmules scheme can be implemented by vehicular mobile networks. Taxis are typical SCmules that can be
used in this scheme. However, unlike traditional vehicular
mobile networks, the main communication objects of this
scheme are intelligent devices and data centers.
4) DELAY TOLERANT NETWORKS

SWDCP-SCmules scheme can be regarded as a special case
of DTNs from the perspective of transmission delay requirements [17]. In traditional DTN, both the data generator and
receiver are mobile, when two mobile nodes encounter with
each other, they can exchange and relay their data to achieve
the goal of data diffusion [23]. The major feature of DTNs is
that data’s transmission delay and the aim of data diffusion
has no strict requirement. In most cases, the delay of data
transmission is not an important measure of network performance and the aim of data transmission is just diffusing data
as large as possible.
However, in the SWDCP-SCmules scheme, the positions
of data generators (i.e. intelligent devices) are fixed, the
positions of data transmitters (i.e. SCmules) are mobile and
the positions of data receivers (i.e. data centers) are fixed.
The aim of data transmission is to obliviously transmit data
from data generators to data receivers by data transmitters.
To achieve this aim, we don’t put any management strategy
on data transmitters, the process of data collection is totally
incidental and oblivious.
B. MACHINE LEARNING

Tom Mitchell gives a received definition of machine learning in 1998: A computer program is said to learn from
experience E with respect to some task T and some performance measure P, if its performance on T , as measured
by P, improves with experience E [36]. As will be illustrated
in section 5.3, SA-PA algorithm can be categorized into
machine learning program according to this definition.
One typical work flow of machine learning program is
to find an appropriate hypothesis from hypothesis set and
use learning algorithm to learn the parameters of hypothesis
based on training set and generate the final hypothesis [37].
The SA-PA algorithm conforms to this work flow according
to the illustration of section 5.3.
C. SIMULATED ANNEALING METAHEURISTIC

Optimization problems are the problems of finding the best
solution from all feasible solutions [38], the priority assignment problem is a typical optimization problem.
6486

Metaheuristics are one of the practical techniques to solve
optimization problems. Typical metaheuristics include simulated annealing, tabu search, ant colony optimization and so
on. Although the quality of solution provided by metaheuristics have no guarantee, for most computationally complicated
optimization problems, metaheuristics can lead to better solution than other algorithm design method when given limited
time [38]. So metaheuristics are perfect candidates for solving
the priority assignment problem due to the computationally
complication.
Simulated annealing is a metaheuristic proposed by
Suman and Kumar [39]. It can be regarded as an improvement of local search algorithm. The aim of local search
algorithm, such as hill climbing method [39], is to find local
optimums by moving from solution to solution in search
space using local changes. Simulated annealing introduces
acceptance probability into local search algorithm to allow
inferior moves to be accepted. A move will be accepted in
the following acceptance probability X:
(
1
1 − e T0 , 1 < 0
X=
1,
1≥0
where 1 is the performance of the move, or more simply
speaking, the difference between the height after taking the
move and the height before taking the move, T0 is the temperature parameter, which controls the probability of taking
an inferior move [39].
To apply simulated annealing metaheuristic to solve optimization problems, three elements need to be specified: configuration, evaluation function and neighborhood function.
Configuration is the representation of feasible solution, all
configurations form the search space. Evaluation function is
the measure of optimization target. Neighborhood function
defines the way moving from solution to solution.
III. THE SYSTEM MODEL AND PROBLEM STATEMENT
A. THE SYSTEM MODEL

In SWDCP-SCmules scheme, intelligent devices are
embedded in the infrastructures of city. Assume there
are m intelligent devices in total, which consist of set
S = {S1 , S 2 , . . . , Sm }. These intelligent devices are selfpowered, which is possible nowadays due to the development of energy harvesting technology [40]. The intelligent
devices have two main functions: detecting the status of
infrastructures and sending data/messages via short-range
wireless communications (due to the limitation of energy,
the communication range is limited). For example, intelligent
devices deployed in garbage cans can detect the level of
trashes periodically and, when SCmules pass by them, they
can forward the data/messages of the trash level to SCmules
via wireless communications.
Every intelligent device is assigned with a unique priority,
which is a measure of the importance of the data/messages
generated by this intelligent device. For intelligent device Si ,
its corresponding priority is Pi . Suppose the set of priority is
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

P = {P1 , P2 , . . . , Pm } and its corresponding m-dimensional
E = [P1 , P2 , . . . , Pm ]. There is a one-to-one
vector is P
E We will not explicitly
correspondence between P and P.
E in following context since both P and
distinguish P and P
E can be used to express priority assignments, i.e. priority
P
table. Obviously, from the perspective of linear algebra, all
possible priority assignments form a subset of m-dimensional
space.
Besides intelligent devices, there are k data centers distributed in fixed locations of Smart City, especially the downtown areas. The i-th data center is hi , all data centers form
the set H = {h1 , h2 , . . . , hk }. As described in section 1,
data centers are special computing and processing nodes
which can connect with cloud tier via high-speed networks
to forward data/messages. Their major responsibilities are
receiving data/messages buffered by nearby SCmules and
then forwarding them to cloud tier, which is accessible to corresponding municipal departments. Loosely speaking, data
centers can be viewed as the destination of transmission:
data/messages generated by intelligent devices are eventually
forwarded to data centers via SCmules. Data centers may
receive a lot of duplicated information since several SCmules
may buffer the same data/messages when passing by the same
intelligent devices in the same time period. This phenomenon
is unwanted because it wastes energy and occupies precious
limited storage of SCmules.
There are n total SCmules which consist of set V =
{V1 , V 2 , . . . , V n }. SCmules, which can be regarded as
mobile IoT nodes, move independently in Smart City. They
are equipped with short-range transceivers. In the process
of motion, when SCmules pass near intelligent devices,
transceivers can automatically detect the active intelligent
devices within their communication range, pick up the
data/messages which these active intelligent devices are sending and then buffer them into their storage. Similarly, when
SCmules pass near data centers, the transceiver can detect the
existence of these data centers within their communication
range, dump all the data/messages buffered in their storage
to data centers and then clear their storage. To simplify the
model, we assume the data transmission processes between
intelligent devices and SCmules and the data transmission
process between data centers and SCmules are instant. This
is feasible in most cases due to the rapid development of
new radio technologies such as Ultra-Wideband (UWB) [41],
even if the worst situation occurs, such as packet loss or
unfinished packet transmission, SCmules can simply discard
these damaged packets since the oblivious and incidental data
collection feature of SWDCP-SCmules scheme.
The storages of SCmules are limited. Suppose SCmule
Vi can store Qi data/messages at most and all Qi form set
Q = {Q1 , Q2 , . . . , Qm }. When the storage is full and new
valuable data/messages are picked up. SCmules will use
greedy selection principle, which is described in section 1 and
section 4.4, to maximize the sum of priorities of
data/messages buffered.
VOLUME 4, 2016

We refer to the quintuple C = (S, P, H, V, Q) as a configuration, which is a concrete deployment of SWDCP-SCmules
scheme in Smart City.
B. PROBLEM STATEMENTS

Simply speaking, the biggest problem of SWDCP-SCmules
scheme is how to make it effective and efficient. For a certain
configuration C = (S, P, H, V, Q) in Smart City, the only
element that can be manually changed to improve performance is priority assignment P. After all, intelligent devices,
data centers and transceivers are deployed in advance and
cannot be changed arbitrarily. That is to say, the priority
assignment P is the only factor that can be easily altered
to improve the performance of SWDCP-SCmules scheme as
illustrated in section 1.
However, the meaning of effectiveness and efficiency is
ambiguous since different scenarios have their own different
concrete meaning. To specify the meaning of effectiveness
and efficiency, we introduce a universal optimization target
function J to quantify them. J can be thought as a quantified performance measure of SWDCP-SCmules scheme.
The larger J is, the more effective and efficient SWDCPSCmules scheme is. We convert the problem of making
SWDCP-SCmules scheme effective and efficient to an optimization problem of J with respect to C (In fact, C is dominated by P as other elements cannot be controlled easily).
For example, in most cases, the interpretation of effectiveness and efficiency corresponds to high collection rate and
low redundancy rate. Collection rate, redundancy rate and
their synthesized rate are formally defined as follow:
1) COLLECTION RATE CC
T

Collection rate is the number of distinct data/messages
(excluding duplicated data/messages) collected by data centers in a given time interval. It is obvious that the higher
collection rate is, the less data/messages loss in the process
of data transmission of SCmules. Therefore, we should maximize collection rate.
For a given time interval T = [tx , ty ) where tx and ty
are timestamps, assume the total number of data/messages
generated by all intelligent devices is GC
T , the total number
C
of distinct data/messages collected by data centers is kT , we
C
can define the collection rate CT as
C

CC
T

=

kT

GC
T

.

2) REDUNDANCY RATE RC
T

Redundancy rate is the ratio of the number of duplicated
data/messages collected by data centers to the number of
data/messages collected by data centers in a given time interval. It is obvious that the more duplicated data/messages
data centers collect, the lower storage utilization efficiency
SCmules are and the more energy the whole system wastes.
Therefore, we should minimize redundancy rate.
6487

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

For a given time interval T = [tx , ty ) where tx and ty
are timestamps, the total number of data/messages collected by data centers is KC
T , the total number of duplicated
C
data/messages collected by data centers is KC
T − kT , we can
define the redundancy rate RC
T as

TABLE 1. Main notations.

KC
T − kT

C

RC
T

=

KC
T

In the following context, we will also use effective collecC
tion rate RT as a substitution of RC
T , which is defined as
C

C
RT

=

1 − RC
T

=1−

KC
T − kT
KC
T

C

=

kT

KC
T

C

Obviously, the higher RC
T is, the lower RT is. Conversely,
C

the lower RC
T is, the higher RT is.
By the way, it is worth to mention that the way we define
redundancy rate is similar to the definition of repurchase rate
in the field of management, which proves the definition of
redundancy rate is reasonable.
Based on the definitions of collection rate and redundancy rate, we can define three distinct optimization target
functions:
(1) Collection rate optimization target function JC
JCTC = CC
T
The optimization target of JC is to maximize the overall
collection rate.
(2) Effective collection rate optimization target
function JR
JRC
T

=

1 − RC
T

=

C
RT

The optimization target of JR is to minimize redundancy rate or maximize effective collection rate.
(3) Synthesized optimization target function JS
We can synthesize the two optimization targets, i.e.
maximizing collection rate CC
T and minimizing redundancy
rate RC
,
to
form
a
synthesized
rate and make a concrete
T
synthesized optimization target function JS :


C
JSTC = λ1 CC
T + λ 2 1 − RT
where λ1 + λ2 = 1, λ1 ≥ 0 and λ2 ≥ 0. Maximizing JS
is equivalent to maximizing C and minimizing R in the same
time. The higher JS is, the higher C is and the lower R is.
JC , JR and JS will be used in the following context
as examples of optimization target functions to prove the
effectiveness and efficiency of SWDCP-SCmules scheme
and SA-PA algorithm.
In summary, we introduce a universal optimization target
function J to measure the performance of a certain configuration of SWDCP-SCmules scheme. In different applications,
the meaning of effectiveness and efficiency is different, we
can use different J to quantify them. Optimizing J with
respect to configuration C (In fact, C is dominated by P as
6488

other elements cannot be controlled easily) can acquire an
optimized configuration that significantly improve the performance of SWDCP-SCmules scheme. Therefore, the formal
problem we have to tackle with is
max J
C

We propose the SA-PA algorithm to solve this problem in
section 5.
IV. SWDCP-SCMULES SCHEME
A. OVERVIEW

To realize the intelligent management of city and improve the
overall social welfare, we can deploy intelligent devices to
infrastructures of city and make them monitor and report the
status of these infrastructures to corresponding departments,
of which responsibility is to maintain the infrastructures’
normal function. As illustrated in section 1, we refer to this
abstract paradigm as Social welfare data collection paradigm
(SWDCP paradigm). Many applications can be realized by
this paradigm, such as the intelligent monitoring of the conditions of street lights.
To give a concrete, feasible and economical scheme
to implement SWDCP paradigm, many related researches
have been done as introduced in section 1. In this
paper, we propose a Social Welfare Data Collection
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

Scheme based on Storage-Constrained Oblivious Data Mules
(SWDCP-SCmules scheme), which can be regarded as a
refined model proposed by Bonola et al. [17], to collect
data/messages distributed in the sparse network formed by
intelligent devices. The main feature of this scheme is that
oblivious data mules are storage-constrained, which is much
more realistic than previous researches. In the following context, we refer to this kind of oblivious data mules as SCmules.
Below we describe this scheme on the whole:
Intelligent devices are embedded into the infrastructures
of Smart City to detect their status. Once the status meets a
certain condition, the communication function of intelligent
devices will be activated and tries to report data/messages
to nearby SCmules via short-range wireless communication.
SCmules are mobile IoT nodes moving in Smart City, they
are equipped with transceivers which enable them to opportunistically communicate with nearby nodes. When SCmules
pass near active intelligent devices which are trying to report
data/messages, they will detect the existence of them and pick
up data/messages sent by them obliviously and incidentally.
As the storage is limited, when receiving new data/messages,
SCmules will determine whether the new data/messages will
be buffered or not according to the remaining storage space
they have and the priority of the intelligent devices generated these data/messages. The selection principle is to greedily maximize the sum of priorities of data/messages stored.
We refer to this principle as greedy selection principle.
Besides intelligent devices and SCmules, there are many data
centers deployed in fixed locations of Smart City, especially
downtown areas. Data centers can be viewed as the destination of data collection, they are special computing and processing nodes that can connect with cloud tier via high-speed
network to forward data/messages. When SCmules pass near
them, they will dump all data/messages buffered in their storage to them and then clear their buffer. Data centers received
these data/messages will forward them to cloud tier, which
is accessible to municipal departments. Once corresponding
municipal departments see these data/messages, they will
take measures to maintain the related infrastructures.
To illustrate the scheme in a concrete way, below
is an example of the application of monitoring the
health status of allée trees using SWDCP-SCmules scheme
(see Figure 2): Intelligent devices are deployed on allée trees
to monitor their health status, once these devices detect that
these allée trees are in unhealthy condition, such as water
shortage, pest threaten, the communication function of these
intelligent devices will be activated immediately and try to
report the messages via short-range wireless communication.
Taxis equipped with transceivers, which are typical types of
SCmules, move in Smart City to send passengers to destinations, when taxis pass near these active intelligent devices,
which are trying to report messages, transceivers will detect
the existence of them, pick up the messages obliviously
and incidentally and then buffer them according to greedy
selection principle. In the process of sending passengers to
destinations, taxis may also pass near some data centers,
VOLUME 4, 2016

FIGURE 2. A concrete example of SWDCP-SCmules scheme.

which are distributed in fixed locations of Smart City. When
a certain data center is nearby, transceivers will detect the
existence of it and dump all data/messages buffered in storage
to it. After receiving these messages, data centers will simply
process them, such as filter out duplicated messages and then
forward them to cloud tier via high-speed network connections, gardening department will receive this message from
cloud tier and then send staffs to water the allée trees.
In rest of this section, we will give a detailed introduction
of how intelligent devices, data centers and SCmules work in
section 4.2, 4.3 and 4.4, respectively.
B. INTELLIGENT DEVICES

Various intelligent devices are embedded in the infrastructures of Smart City to enhance their functionality and productivity. Most of them are self-powered. They have two
main functions: detecting the status of the infrastructures and
communicating via short-range wireless networks.
To illustrate the running process of intelligent devices,
we divide their running process into three states: detecting
state, latency state and transmission state. In detecting state,
the detecting function of intelligent devices is activated to
detect the status of infrastructures. If a certain condition
is satisfied (e.g. the infrastructures turn into bad condition
or the current time is the arranged time of reporting), the
latency state is activated. In latency state, the communication function of intelligent devices is active and waits for
the appearance of SCmules within communication range to
transmit data/messages to them. This state may consume
unnecessary energy since intelligent devices have no idea
of when SCmules will pass near them, many researches
have been done to solve this problem in ad hoc networks,
which can also be used to address the problem in this situation [42], [43]. Once a certain SCmule appears in the communication range of intelligent devices, the intelligent devices
will turn into transmission state imediately, it will then communicate directly with the SCmule to send data/messages
to it. Figure 3 illustrates the running process of intelligent
devices.
6489

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

FIGURE 3. The state diagram of intelligent devices.

Algorithm 1 The Running Process of Intelligent Devices
1: While true
2:
switch into detecting state
3:
detect the infrastructure’s status
4:
If infrastructure’s status satisfies a certain condition
5:
switch into latency state
6:
While true
7:
If SCmule Vi enters the communication range
8:
switch into transmission state
9:
report the infrastructure’s status to Vi
10:
End
11:
End
12: End
13: End

Algorithm 1 is the pseudo-code of the running process of
intelligent devices.
C. DATA CENTERS

Data centers are special computing and processing nodes
distributed in fixed locations of Smart City. They are connected with cloud tier using high-speed networks to forward
data/messages and can detect the existence of SCmules within
their communication range and receive data/messages from
them via short-range wireless communication. They have sufficient stable power supply because they are installed in fixed
locations, so their communication range is usually larger than
intelligent devices.
The running process of data centers can be divided into
three states (see Figure 4): latency state, receiving state
and forwarding state. Unlike intelligent devices, data centers are always in latency state to detect the appearance of
SCmules within their communication range. Once SCmules
pass near them, they will switch into receiving state in which
they communicate with SCmules’ transceivers to receive
all data/messages buffered in their storage. After receiving
data/messages from SCmules, data centers will send ACK
as a feedback to SCmules to tell them the data/messages
are received and they can safely clear their buffer and
then switch into forwarding state in which they process
6490

FIGURE 4. The state diagram of data centers.

Algorithm 2 The Running Process of Data Centers
1: While true
2:
switch into latency state
3:
detect SCmules which enter communication range
4:
If SCmule Vi enters communication range
5:
wait for communication request from Vi
6:
switch into receiving state
7:
receive data/messages dumped by Vi
8:
switch into forwarding state
9:
process received data/messages
10:
send received data /messages to cloud tier
11:
End
12: End

the received data/messages, such as filtering out duplicated
information, and then forward them to cloud tier. Municipal
departments can access the data/messages stored in cloud
tier. Once they check these data/messages in cloud tier, they
will take measures to maintain related infrastructures. Usually, data centers are deployed in the downtown areas of
Smart City, i.e. hotspot areas, where SCmules travel more
frequently than other areas, to boost the efficiency of data
collection.
Algorithm 2 is the pseudo-code of the running process of
data centers.
D. SCmules

SCmules are mobile IoT nodes moving in Smart City,
they are equipped with transceivers which enable them to
opportunistically communicate with nearby nodes (including intelligent devices and data centers). Typical SCmules
include taxis, private vehicles, buses and people holding
smart mobile phone. When SCmules pass near intelligent
devices which are in latency state, they will detect the existence of them and pick up data/messages they are reporting obliviously and incidentally. As the storage is limited,
SCmules have to face with the problem whether to buffer
the new picked-up data/messages or not. More specifically, in
the situation that their storage is full and new more valuable
data/messages are picked up, they have to discard some less
important data/messages buffered in storage to make room for
them. SCmules use greedy selection principle, which means
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

greedily maximize the sum of priorities of data/messages
buffered, to guide the selection. That is to say, when the
storage is not full, SCmules will store data/messages as much
as possible, but when the storage is full, SCmules will compare the priority of the intelligent devices generating these
new picked-up data/messages with the priorities of intelligent
devices generating the data/messages buffered in SCmules’
storage according to priority table and then make decision: if
the priority of these new picked-up data/messages is less than
any priorities of the data/messages buffered, the new pickedup data/messages will be discarded directly, otherwise, the
data/messages in storage with the least priority will be discarded to make room for the new picked-up data/messages
and then the new more valuable picked-up data/messages will
be buffered in the storage. When SCmules pass near data centers, SCmules will send request for dumping data/messages
to data centers, if data centers approve of the request, they
will dump all data/messages buffered in their storage to data
centers and then clear the storage.
The running process of SCmules, or more accurately the
transceivers deployed in SCmules can be divided into three
states: latency state, buffering state and dumping state (see
Figure 5). In latency state, SCmules are detecting the existence of intelligent devices or data centers within their communication range. When detecting that intelligent devices are
nearby, SCmules rapidly switch into buffering state to pick up
new data/messages and then use greedy selection principle to
determine whether to buffer them or not. When detecting that
data centers are nearby, SCmules will switch into dumping
state and then dump all the data/messages stored to data
centers.

FIGURE 5. The state diagram of SCmules.

Algorithm 3 is the pseudo-code of the running process of
SCmules, or more accurately, the transceivers deployed in
SCmules.
V. SA-PA ALGORITHM
A. OVERVIEW

Section 4 gives an almost comprehensive introduction to
SWDCP-SCmules scheme, but sidestep the problem of priority assignment. As illustrated in section 1, the priority assignment problem is key to SWDCP-SCmules scheme since it
almost determines the final performance of this scheme.
In other words, as illustrated in section 3.2, the performance of SWDCP-SCmules scheme is quantified as an optimization target function J, the priority assignment almost
VOLUME 4, 2016

Algorithm 3 The Running Process of SCmules’ Transceivers
1: While true
2:
switch into latency state
3:
detect the existence of intelligent devices or data
centers
4:
If intelligent device Si in latency state enters
communication range
5:
switch into buffering state
6:
pick up data/message d which is sent by Si
7:
If SCmule’s storage is full
8:
select data/message e with the least
priority ℘ from SCmule’s storage
9:
If Pi < ℘
10:
discard data/message d
11:
Else
12:
discard data/message e from
storage
13:
store d into SCmule’s storage
14:
End
15:
Else
16:
store d into SCmule’s storage
17:
End
18:
Else If data center hi enters communication range
19:
switch into dumping state
20:
send dumping request to hi
21:
If hi approve of the request
22:
dump all data/messages stored to hi
23:
End
24:
End
25: End

determines the final value of J. However, the traditional
approach, which is finding effective patterns and then using
these patterns to design algorithms, to solve the priority
assignment problem is unfeasible because of the ambiguity
of the meaning of effectiveness and efficiency, the difficulty
of finding patterns and the variability of patterns as described
in section 1 [44].
To solve the priority assignment problem, we try to convert
the priority assignment problem to an optimization problem
based on the idea from machine learning and use simulated
annealing metaheuristic to design a universal algorithm that
can automatically find a well-performed priority assignment
with respect to various optimization targets, which are modeled by optimization target function J, based on the socialaware patterns lying in the GPS trajectory data of SCmules
in the past time.
Concretely speaking, first, we quantify the performance
of the SWDCP-SCmules scheme as a universal optimization
E then, we use the
target function J of priority assignment P;
past GPS trajectory data of SCmules to train a priority assignE that can optimize J with respect to P
E and predict that
ment P
it will also achieve good performance in the future because
the future shares similar patterns with the past. Because of
the complexity of the computation of J, we use simulated
6491

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

E that can optimize J, in
annealing metaheuristic to search for P
search space. We refer to this solution to priority assignment
problem as Simulated Annealing for Priority Assignment
Algorithm (SA-PA algorithm).
Why a priority assignment perform well in the past can
also perform well in the future? Because the running process
of SWDCP-SCmules scheme in the future is similar to the
running process in the past under the same configuration C.
In other words, they have similar social-aware patterns.
Usually, similar social-aware patterns will lead to similar results if the configuration C is fixed. We use a
manually observed social-aware pattern to illustrate this
statement:
As illustrated in section 1, the frequency that SCmules pass
by downtown areas, i.e. hotspot areas, is much higher than
the frequency of other areas, especially remote areas. If we
deploy data centers in hotspot areas, then the data/messages
generated by intelligent devices which are near data centers are more likely to be collected than those generated by
intelligent devices located in other areas. This lead to a bad
phenomenon that data centers may receive many duplicated
data/messages generated by the intelligent devices located
near them, but receive a few data/messages from intelligent
devices located in other areas. Therefore, we should decrease
the priorities of intelligent devices near data centers and
increase the priorities of intelligent devices far from data centers: Decreasing the priorities of intelligent devices near data
centers will not interfere with the data/messages collection
of those intelligent devices, on the contrary, it can decrease
the duplicated data/messages collected by data centers (i.e.
redundancy rate) and thereby decreasing unnecessary energy
consumption. Increasing the priorities of intelligent devices
far from data centers will decrease data/messages loss and
thereby increasing collection rate. Due to the pattern that the
locations of hotspot areas in the future are usually the same
as the locations of hotspot areas in the past, assigning priorities according to the priority assignment method mentioned
above can achieve good results both in the past and in the
future.
However, we can’t just rely on the patterns simply acquired
by our observation to design algorithm for priority assignment problem. As described in section 1, the pattern mentioned above is just one of the huge number of patterns
lying in essence. In fact, most of patterns are difficult for
human intelligence to observe and even understand, not to
mention that different definitions of effectiveness and efficiency (i.e. optimization target function J) need the support
of different patterns, which means a former observed pattern
may become useless in new applications. Besides that, some
patterns may change occasionally or periodically. Based on
the above inference, we could conclude that if we simply use
the patterns acquired by our observation to design algorithm
solving the priority assignment problem, which is the traditional approach for algorithm design, there must be some
unforeseen problems lying in the algorithm that are not taken
into consideration.
6492

We follow the above example to illustrate two unforeseen
problems lying in priority assignment method proposed in
that example:
The performance of the priority assignment method in that
example is based on the assumption that all data centers are
located in hotspot areas of Smart City. This strong assumption
is not that realistic. In fact, it is impossible to deploy data
centers for all hotspot areas since there may be temporary
hotspot areas in a certain time period, e.g. the districts near
natatoriums in hot summer (see section 1). For hotspot areas
without the deployment of data centers, the priority assignment method proposed in that example is inefficient because
intelligent devices in those hotspot areas are assigned with
high priorities since they are far from data centers, but their
data/messages may be repeatedly collected by SCmules since
they are in hotspot areas. Therefore, these hotspot areas will
contribute a lot of redundant data/messages to data centers
and thus increase redundancy rate significantly.
Even if the assumption is satisfied, i.e. all hotspot areas are
deployed with data centers, some data/messages generated by
intelligent devices located near data centers may still have
difficulties to be collected. For example, if the data center is
deployed near the starting point of the one-way street but the
intelligent device is deployed in the end point. Taxis, a typical
kind of SCmules, moving along the one-way street will first
pass by the data center and dump buffered data/messages
to it and then pass by the intelligent devices and pick up
data/messages from it. If the one-way street lead taxis from
a downtown area to a remote area, although this intelligent
device locates in a hotspot area and near a data center, we still
can’t guarantee that the data/messages from this intelligent
device can be collected rapidly just like those generated by
other ordinary intelligent devices located near data centers.
However, according to the priority assignment method proposed in that example, this intelligent device will be assigned
with low priority, its data/messages are easily replaced by
data/messages generated by intelligent devices located in
remote areas. This will lead to the loss of data/messages
generated by this intelligent device (see Figure 6).

FIGURE 6. The one-way street in counterexample.

Unlike traditional algorithm design approach, machine
learning can automatic find social-aware patterns, which
are patterns that can reflect the social preferences of citizens in Smart City, by training hypothesis using training
data. Using the idea from machine learning, we propose the
SA-PA algorithm which is a universal algorithm for priority
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

assignment problem. No matter what optimization target J is,
how difficult the observation of potential social-aware patterns are and how social-aware patterns lying in the GPS trajectory data change constantly, SA-PA algorithm can always
automatically learn social-aware patterns from training data
(i.e. the GPS trajectory data of SCmules) without manual
teaching and then use these social-aware patterns to guide
priority assignment.
An important attribute of SA-PA algorithm is its socialawareness since it can automatically learn citizens’ social
behaviors, which can be formally expressed by social-aware
patterns, from GPS trajectory data of SCmules in Smart City.
As illustrated in section 1, the basis of learning is that there
are a lot of social-aware information lying in the data. This is
obviously true because a lot of instances can be provided. For
example, if SCmules are taxis, the preferred places citizens
in Smart City usually go to, i.e. the hotspot areas, can be
analyzed from the frequencies that taxis travel to a certain
place with; if SCmules are private vehicles, the places the
owners of the private vehicles prefer to go to can be acquired
from the trajectories of these private vehicles. The key to
the success of SA-PA algorithm is that it can automatically
discover these social-aware information from data, express
them in the form of hypothesis (the meaning of hypothesis
is hard to explain and sometimes even beyond the understanding of human beings) and then use them to improve the
priority assignment. This attributes to the magic of machine
learning.
In section 5.2, we formalize the priority assignment problem. In section 5.3, we introduce SA-PA algorithm in detail.
B. FORMAL PROBLEM DESCRIPTION

Informally speaking, the problem solved by SA-PA algorithm
can be described as follow:
Given the following four elements
• the information of the intelligent devices (e.g. positions,
communication range);
• the information of data centers (e.g. positions, communication range);
• the list of all registered SCmules and their configurations
(e.g. storage size, communication range of transceivers);
• GPS trajectory data of SCmules in past time interval
[t1 , t2 ) where t1 and t2 are timestamps and t1 < t2 .
We want to find a priority assignment that can optimize the
optimization target (e.g. maximize collection rate, minimize
redundancy rate) in the future time interval [t2 , t3 ) where
t2 and t3 are timestamps and t2 < t3 .
To introduce SA-PA algorithm, we need formalize the
problem description above: As defined in section 3.1,
the intelligent devices and data centers can be expressed
as S and H respectively. All registered SCmules and their
storage size can be expressed as V and Q respectively.
To formalize the GPS trajectory of SCmule Vi in a given time
interval, we define trajectory function D as follow:
D (Vi , tx ) = (x, y)
VOLUME 4, 2016

The output of D is the geographic coordinate (x, y) of SCmule
Vi in timestamp tx . Therefore, the meaning of knowing the
GPS trajectories of SCmule Vi in time interval [t1 , t2 ) can be
formalized as knowing the value of D (Vi , tx ) where Vi ∈ V
and tx ∈ [t1 , t2 ).
Next, we formalize the notation of priority assignment.
We follow the way we express priority assignment in
section 3.1. That is, all priorities of intelligent devices form
set P = {P1 , P2 , . . . , Pm } where m is the number of
intelligent devices and Pi is the priority assigned to intelE =
ligent device Si . The corresponding vector form is P
[P1 , P2 , . . . , Pm ]. Obviously, there is a one-to-one-to-one
E It is
correspondence among priority assignment, P and P.
worth to mention all possible priority assignments form a
E is used to express priority
subset of m-dimensional space if P
assignments.
In the informal problem description, the meaning of optimization target is not specified, which means any concrete
target, such as maximize collection rate or minimize redundancy rate, can be embedded into the problem and then be
solved by SA-PA algorithm. As illustrated in section 3.2, we
use a universal optimization target function J to model the
unspecified optimization target. Based on J, if P is known,
the configuration C = (S, P, H, V, Q) is determined and we
can directly compute the value of optimization target function
in time interval [t2 , t3 ), which is JC
.
[t2 ,t3 )
To illustrate the meaning of J, three concrete examples of J
are introduced in section 3.2, i.e. JC , JR and JS , which will
be used in section 6 to conduct experiments.
To simplify expressions, we introduce the simplified notation of optimization target function. Before doing that, we
first introduce the simplified function notations of collection
rate and redundancy rate:
(S,P,H,V,Q)

C (P, T) = CT

(S,P,H,V,Q)

R (P, T) = RT

where P is priority assignment and T is time interval.
S, H, V and Q are known elements of C.
The simplified notation of optimization target function is
(S,P,H,V,Q)

J (P, T) = JT

where P is priority assignment and T is time interval.
S, H, V and Q are known elements of C.
As a result, the simplified notation of JC , JR and JS are
listed below:
JC (P, T) = C (P, T)
JR (P, T) = 1 − R (P, T)

JS (P, T) = λ1 C (P, T) + λ2 (1 − R (P, T))
(λ1 + λ2 = 1, λ1 ≥ 0 and λ2 ≥ 0)

In summary, the formal description of the problem solved
by SA-PA algorithm can be stated as follow:
Given S, H, V, Q and the value of D (Vi , tx ) where
Vi ∈ V and tx ∈ [t1 , t2 ), find a Pbest which can maximize
J (Pbest , [t2 , t3 )).
6493

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

C. FORMAL DESCRIPTION OF SA-PA ALGORITHM

To address the problem in section 5.2, we can use the
GPS trajectory data in past time interval [t1 , t2 ) of all
SCmules in V as training set and find a priority assignment
which can achieve good performance in training set by automatically learning the social-aware patterns lying in training
set. Due to the future shares similar social-aware patterns with
the past, we claim this priority assignment will also achieve
good performance in future time interval [t2 , t3 ). This is the
sketch of SA-PA algorithm.
Below details the sketch of SA-PA algorithm:
Assume P is given, according to the formal problem
description given in last subsection, all elements of configuration C = (S, P, H, V, Q) are known. Besides these,
we also know the GPS trajectory data of SCmules in time
interval [t1 , t2 ), i.e. the value of D (Vi , tx ) where Vi ∈ V and
tx ∈ [t1 , t2 ). Based on the given information, we can estimate
the value of J (P, [t1 , t2 )), which is a measure of performance
as described in section 5.1, by simulating the running process
of SWDCP-SCmules scheme in time interval [t1 , t2 ).
However, we don’t know which P will make training set
performs well, i.e. maximize J (P, [t1 , t2 )). To find such P
that can optimize J, we convert the original problem to a
optimization problem:
Given S, H, V, Q and the value of D (Vi , tx ) where
Vi ∈ V and tx ∈ [t1 , t2 ), find a priority assignment ℘ which
can maximize J (℘, [t1 , t2 )). We say ℘ is a good approximation of Pbest , which can maximize J (Pbest , [t2 , t3 )).
Note that J cannot be simply expressed by closed formula.
To compute J, we need to simulate the running process of
Smart City based on the past GPS trajectory data. That’s
why we cannot use iterative optimization algorithms such
as gradient descent and Newton method [45]. Metaheuristics
are perfect solution to optimizing this kind of computational
complicated function as described in section 2.3. Therefore,
we use simulated annealing metaheuristic to solve this optimization problem.
To apply simulated metaheuristic to this optimization
problem, we need to define configuration, evaluation function and neighborhood function as described in section 2.3.
For this optimization problem, we can naturally define
configuration as ℘, i.e. a priority assignment and define
evaluation function as J (℘, [t1 , t2 )). The definition of neighborhood function is not as direct as configuration and
evaluation function. We propose a neighborhood function based on swapping the priorities of two intelligent
devices:



< ℘1 , . . . , ℘i , . . . , ℘j , . . . , ℘m , i, j


= ℘1 , . . . , ℘ j , . . . , ℘ i , . . . , ℘ m
where 1 ≤ i < j ≤ m.
In section 5.1, we have given an intuitive explanation of
why SA-PA algorithm takes effect. In section 6.3, we conduct
experiments to prove its effectiveness.
Algorithm 4 is the pseudo-code of SA-PA algorithm.
6494

Algorithm 4 SA-PA Algorithm
Input: T0 , α,maxMv, S, H, V, Q and
D (Vi , tx ) (Vi ∈ V, tx ∈ [t1 , t2 ))
Output: ℘best
1: nbMv := 0
2: ℘current :=generateRandomPriorityAssignment(S)
3: ℘best := ℘current
4: While nbMv < maxMv
5:
pick random number i and j (1 ≤ i < j ≤ m)
6:
℘new := <(℘current , i, j)
7:
1 := J (℘new , [t1 , t2 )) − J (℘current , [t1 , t2 ))
8:
If 1 ≥ 0 or e1/T0 ≥ rand(0, 1)
9:
℘current := ℘new
10:
nbMv := nbMv + 1
11:
If J (℘current , [t1 , t2 )) ≥
J (℘best , [t1 , t2 ))
12:
℘best := ℘current
13:
End
14:
End
15: End
16: Return ℘best

Parenthetically, the conditional statements of line 8
is equivalent to the acceptance probability illustrated
in section 2.3.
Notice SA-PA algorithm uses idea from machine learning. It conforms to the received definition of machine
learning proposed by Tom Mitchell, which is described in
section 2.2 [36]. In SA-PA algorithm, E is the GPS trajectory
data in time interval [t1 , t2 ), P is the optimization target function J and T is to find a well-performed priority assignment.
From the perspective of hypothesis, a configuration C is a
hypothesis and P is the parameters of the hypothesis, we
use training set, i.e. the GPS trajectory data in time interval
[t1 , t2 ), to train the hypothesis to find a well-performed P as
the parameters of the hypothesis.
VI. EXPERIMENTAL RESULTS
A. OVERVIEW OF DATASET

To prove the effectiveness of the SWDCP-SCmules scheme
and SA-PA algorithm, we use T-Drive trajectory dataset,
which is provided by MSRA [46], [47], to do experiments.
The T-Drive trajectory dataset contains GPS trajectories of
10357 taxis during the period of Feb. 2 to Feb. 8, 2008, in
Beijing. The number of GPS waypoints in this dataset reaches
up to 15 million and the total distance of the trajectories
reaches up to 9 million kilometers. Figure 7 illustrates the
dataset visually, different colors reflect different density distribution of the GPS waypoints.
Figure 8, which is extracted from the instruments of the
dataset, shows the distribution of time interval between two
consecutive waypoints. It indicates, in most cases, GPS
devices equipped in taxis sample geographical data every one
second or five seconds. This means these discrete waypoints
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

FIGURE 7. Visualization of T-Drive trajectory dataset.

FIGURE 10. Dataset after filtering out invalid waypoints.

B. EXPERIMENTAL METHODOLOGY

FIGURE 8. The distribution of sampling time interval [46], [47].

have enough information to observe the continuous trajectories of taxis.
Besides that, we find there exist some invalid waypoints
in dataset, they deviate from the geographical position of the
last valid waypoints in very short time interval. For example,
in Figure 9, invalid waypoints are marked by red colors.
These invalid waypoints usually occur when taxis are in
the areas where GPS service quality is poor, e.g. tunnels.
To guarantee the accuracy of experimental results, we need to
filter out these invalid waypoints. Below is the simple filtering
algorithm we propose:

FIGURE 9. Invalid waypoints in dataset.

For a given waypoint, we check the distance between the
last valid waypoint and this waypoint divides the time interval
between them, if it is larger than the maximum speed limit
in Beijing, which is 80 km/h in most places, we mark this
waypoint as invalid waypoint and filter it out. Figure 10
illustrates the effectiveness of this filtering method.
VOLUME 4, 2016

As SA-PA algorithm need repeatedly simulate the running
process of training set with respect to different priority assignment ℘ to compute J (℘, [t1 , t2 )), which is a extremely timeconsuming process, we should simplify the running process
to accelerate the algorithm:
We separate the map of Beijing by compact square grid
cells, the area of each grid cell is about 40 × 40m2 , which is
the typical coverage range of low power wireless technologies
like 802.15.4 and Bluetooth in free space. We assume the
communication range of every intelligent device, data center
or the transceiver is the area covered by the grid cell it locates
in. Although this approximation will introduce some computational error, it can significantly accelerate the simulation.
Since we have separated the map of Beijing by compact
grid cells, we can also compress the T-Drive trajectory dataset
to filter out unnecessary information based on the grid cells.
Because we no longer concern about the exact geographical
positions of taxis if we use grid cells as the substitution
of original maps, Instead, what we only need to concern
is which grid cells these taxis locate in. Therefore, we can
compress the dataset and only retain the trajectory of grid
cells rather than trajectory of exact longitudes and latitudes.
By compressing the dataset, the amount of data needed to
be processed significantly reduces, and thereby accelerate the
simulation.
We then discuss the geographical deployment of intelligent
devices and data centers since they determine two important
elements of configuration C = (S, P, H, V, Q), i.e. S and H,
respectively.
For data centers H, we follow the suggestion in section 4.3,
that is, data centers are usually deployed in downtown areas
of Smart City, i.e. hotspot areas. To find hotspot areas, we
count the frequency of being travelled by taxis of each grid
cell, select the grid cells with high frequency as hotspot areas
and then deploy data centers to these grid cells.
By contrast, the deployment of intelligent devices S is
comparatively free, we randomly select grid cells and deploy
intelligent devices to these grid cells. These grid cells need
to be travelled by taxis because taxis can usually cover most
of places of the city except the non-constructive lands, such
as rivers and forests. Non-constructive land usually have no
infrastructures deployed. If a grid cell has no taxis passing by,
it is likely to be non-constructive land. In addition, random
6495

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

selection can reduce bias since infrastructures of all areas
have probability to be selected to deploy intelligent devices.
For the other three elements V, Q and P of C. V is determined by dataset. Q can be controlled by ourselves. Both
V and Q are easy to be handled in experiments.
P need to be trained by SA-PA algorithm with respect
to various optimization target function J. Three concrete
optimization target functions, which are introduced in
section 3.2, are used in the experiments: JC , JR and JS .
Based on the research paradigm of machine learning, we
separate T-Drive trajectory dataset into two parts: training
set and test set. Training set is used to train the hypothesis,
i.e. optimize J with respect to P to get a well-performed C
(or more accurately, ℘), using SA-PA algorithm. Test set is
used to report the performance of the trained hypothesis, i.e.
the performance of C. Because the T-Drive trajectory dataset
provided by MSRA is a kind of spatial-temporal data, we
separate it according to time: The trajectory data in period
of Feb. 2 to Feb. 6 is training set and the trajectory data in
period of Feb. 7 to Feb. 8 is test set.

FIGURE 11. The optimization process of JC .

C. PERFORMANCE ANALYSIS
1) PERFORMANCE WITH RESPECT TO JC

As illustrated in section 6.2, three concrete optimization target functions are used to do experiments: JC , JR and JS .
In this subsection, JC is used as optimization target function
to illustrate the performance of SWDCP-SCmules scheme
and SA-PA algorithm.
We first try to prove the effectiveness of SA-PA algorithm experimentally. To prove its effectiveness, we fix elements S, H, V and Q of configuration C, and observe if
the value of JC C
increases with the increase of JC C
[t2 ,t3 )

where [t1 , t2 ) is the time interval of training set and [t2 , t3 )
is the time interval of test set. If JC C
and JC C
have
[t1 ,t2 )

[t2 ,t3 )

a positive correlation, it proves the social-aware patterns in
the past are similar to those in the future and these similarities lead to similar performance, which proves the basis of
SA-PA algorithm.
In Figure 11, the blue line roughly tilts up with the increase
of horizontal axis, which means that the priority assignment
℘ that can optimize JC (P, [t1 , t2 )) will also roughly optimize
JC (P, [t2 , t3 )), i.e. JC (P, [t1 , t2 )) and JC (P, [t2 , t3 )) have a
positive correlation.
Next we closely compare the degree of improvement provided by SA-PA algorithm from the change of JC C
with
[t2 ,t3 )
respect to the increase of storage limit in test set.
In Figure 14, the black line illustrates the increase of
JC (℘random , [t2 , t3 )) with respect to the increase of storage
limit where ℘random is a randomly generated priority assignment. assignment. The red line illustrates the increase of
JC (℘1000 , [t2 , t3 )) with respect to the increase of storage
limit where ℘1000 is the priority assignment generated by
a running of SA-PA algorithm with nbMV = 1000 based
on ℘random . From Figure 14, we can see that no matter which
6496

FIGURE 12. The change of JC with respect to storage limit.

[t1 ,t2 )

priority assignment is used, the collection rate of SWDCPSCmules scheme increases with the increase of storage limit,
which conforms to intuition. Besides that, we can also see
that ℘1000 outperforms ℘random since the red line is above
the black line, which means the collection rate of red line is
higher than the collection rate of black line, line, especially
when the storage limit is not large. This proves the effectiveness of SA-PA algorithm. However, when storage limit is
large, the effectiveness of SA-PA algorithm is not significant,
this is because the storage is large enough to get rid of the
limitation of storage.
Then we closely compare the degree of improvement provided by SA-PA algorithm from the change of JC C
with
[t2 ,t3 )
respect to the increase of time in test set.
In Figure 13, the black line illustrates the increase of
JC (℘random , [t2 , t3 )) with respect to the increase of time
where ℘random is a randomly generated priority assignment.
The red line illustrates the increase of JC (℘200 , [t2 , t3 )) with
respect to the the increase of time where ℘200 is the priority
assignment generated by a running of SA-PA algorithm with
nbMV = 200 based on ℘random . From Figure 13, we can
see that no matter which priority assignment is used, the
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

FIGURE 13. The change of JC with respect to time.

FIGURE 14. The change of JC with respect to the number of data centers.

of SWDCP-SCmules scheme increases with the increase of
time, which conforms to intuition. Besides that, we can also
see we can also see that ℘200 outperforms ℘random since the
red line is above the black line, which means the collection
rate of ℘200 is larger than the collection rate of ℘random in
every time. This proves the algorithm.
Finally, we closely compare the degree of improvement
provided by SA-PA algorithm from the change of JC C
[t2 ,t3 )
with respect to the increase of data centers deployed in Smart
City in test set.
In Figure 12, the black line illustrates the increase of
JC (℘random , [t2 , t3 )) with respect to the increase of the number of data centers where ℘random is a randomly generated
priority assignment. The red line illustrates the increase
of JC (℘500 , [t2 , t3 )) with respect to the increase of the
number of data centers where ℘500 is the priority assignment generated by a running of SA-PA algorithm with
nbMV = 500 based on ℘random . From Figure 12, we can
see that no matter which priority assignment is used, the
collection rate of SWDCP-SCmules scheme increases with
the increase of the number of data centers, which conforms to
intuition. Besides that, we can also see that ℘500 outperforms
℘random since the red line is above the black line, which means
means the collection rate of ℘500 is larger than the collection
rate of ℘random no matter how many data deployed. This
proves the effectiveness of SA-PA algorithm.

time interval of test set. If J

2) PERFORMANCE WITH RESPECT TO JR

As illustrated in section 6.2, three concrete optimization target functions are used to do experiments: JC , JR and JS . In
this subsection, JR is used as optimization target function to
illustrate the performance of SWDCP-Scmules scheme and
SA-PA algorithm.
We first try to prove the effectiveness of SA-PA algorithm
experimentally. To prove its effectiveness, we fix elements
S, H, V and Q of configuration C, and observe if the value
of J C
increases with the increase of J C
where
R t ,t
R t ,t
[ 2 3)
[ 1 2)
[t1 , t2 ) is the time interval of training set and [t2 , t3 ) is the
VOLUME 4, 2016

C
R t

and J

C
R t

have a
[ )
[ 2 ,t3 )
positive correlation, it proves the social-aware patterns in the
past are similar to those in the future and these similarities
lead to similar performance, which proves the basis of SAPA algorithm.
In Figure 15, the blue line roughly tilts up with the increase
of horizontal axis, which means that the priority assignment
℘ that can optimize JR (P, [t1 , t2 )) will also roughly optimize
JR (P, [t2 , t3 )), i.e. JR (P, [t1 , t2 )) and JR (P, [t2 , t3 )) have a
positive correlation.
1 ,t2

FIGURE 15. The optimization process of JR .

Next we closely compare the degree of improvement provided by SA-PA algorithm from the change of J C
with
R t ,t
[ 2 3)
respect to the increase of storage limit in test set.
In Figure 16, the black line illustrates the change of
JR (℘random , [t2 , t3 )) with respect to the increase of storage
limit where ℘random is a randomly generated priority assignment. The red line illustrates the change of JR (℘1000 , [t2 , t3 ))
with respect to the increase of storage limit where ℘1000
is the priority assignment generated by a running of
SA-PA algorithm with nbMV = 1000 based on ℘random .
6497

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

FIGURE 16. The change of JR with respect to storage limit.

From Figure 16, we can see that no matter priority assignment
is used, the effective collection rate of SWDCP-SCmules
scheme decreases with the increase of storage limit, which is
because the more storage SCmules have, the larger probability they will carry redundant data/messages. Besides that, we
can also see that ℘1000 outperforms ℘random since the red line
is above the black line, which means the effective collection
rate of red line is higher than the effective collection rate of
black line, especially when the storage limit is not large. This
proves the effectiveness of SA-PA algorithm. However, when
storage limit is large, the effectiveness of optimization is not
significant, this is because the storage is large enough to get
rid of the limitation of storage.
Then we closely compare the degree of improvement provided by SA-PA algorithm from the change of J C
with
R t ,t
[ 2 3)
respect to the increase of time in test set.

FIGURE 17. The change of JR with respect to time.

In Figure 17, the black line illustrates the change of
JR (℘random , [t2 , t3 )) with respect to the increase of time
where ℘random is a randomly generated priority assignment.
The red line illustrates the change of JR (℘200 , [t2 , t3 )) with
6498

FIGURE 18. The change of JR with respect to the number of data centers.

respect to the increase of time where ℘200 is the priority
assignment generated by a running of SA-PA algorithm with
nbMV = 200 based on ℘random . From Figure 17, we can
see that no matter which priority assignment is used, the
effective collection rate of SWDCP-SCmules scheme roughly
decreases with the increase of time. Besides that, we can also
see that ℘200 outperforms ℘random since the red line is above
the black line, which means the effective collection rate of
℘200 is larger than the effective collection rate of ℘random in
every time. This proves the effectiveness of SA-PA algorithm.
Finally, we closely compare the degree of improvement
provided by SA-PA algorithm from the change of J C
R t ,t
[ 2 3)
with respect to the increase of data centers deployed in Smart
City in test set.
In Figure 18, the black line illustrates the increase of
JR (℘random , [t2 , t3 )) with respect to the increase of the
number of data centers where ℘random is a randomly priority assignment. The red line illustrates the increase of
JR (℘500 , [t2 , t3 )) with respect to the increase of the number of data centers where ℘500 is the priority assignment
generated by a running of SA-PA algorithm with nbMV =
500 based on ℘random . From Figure 18, we can see that the
effective collection rate fluctuates with the increase of the
number of data centers, this is because the effective collection
rate really depends on the selection of positions to deploy data
centers. Besides that, we can also see that ℘500 outperforms
℘random since the red line is above the black line, which means
the effective collection rate of ℘500 is larger than the effective
collection rate of ℘random no matter how many number of
data centers are deployed. This proves the effectiveness of
SA-PA algorithm.
3) PERFORMANCE WITH RESPECT TO JS

As illustrated in section 6.2, three concrete optimization target functions are used to do experiments: JC , JR and JS .
In this subsection, JS is used as optimization target function
to illustrate the performance of SWDCP-SCmules scheme
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

FIGURE 19. The optimization process of JS .

and SA-PA algorithm. The concrete JS we use in this subsection is


C
JSTC = 0.8CC
T + 0.2 1 − RT
where λ1 = 0.8, λ2 = 0.2 and T is a time interval.
We first try to prove the effectiveness of SA-PA algorithm
experimentally. To prove its effectiveness, we fix elements
S, H, V and Q of configuration C, and observe if the value
of JS C
increases with the increase of JS C
where
[t2 ,t3 )
[t1 ,t2 )
[t1 , t2 ) is the time interval of training set and [t2 , t3 ) is
the time interval of test set. If JS C
and JS C
have
[t1 ,t2 )
[t2 ,t3 )
a positive correlation, it proves the social-aware patterns in
the past are similar to those in the future and these similarities lead to similar performance, which proves the basis of
SA-PA algorithm.
In Figure 19, the black line roughly tilts up with the
increase of horizontal axis, which means that the priority assignment ℘ that can optimize JS (P, [t1 , t2 )) will
also roughly optimize JS (P, [t2 , t3 )), i.e. JS (P, [t1 , t2 )) and
JS (P, [t2 , t3 )) have a positive correlation. Besides the black
line, red line and blue line illustrates the changes of collection
rate and effective collection rate in the process of optimization, respectively.
Next we closely compare the degree of improvement provided by SA-PA algorithm from the change of JS C
with
[t2 ,t3 )
respect to the increase of storage limit in test set.
In Figure 20, the black line illustrates the increase of
JS (℘random , [t2 , t3 )) with respect to the increase of storage
limit where ℘random is a randomly generated priority assignment. assignment. The red line illustrates the increase of
JS (℘1000 , [t2 , t3 )) with respect to the increase of storage limit
where ℘1000 is the priority assignment generated by a running
of SA-PA algorithm with nbMV = 1000 based on ℘random .
From Figure 20, we can see that no matter which priority
assignment is used, the synthesized rate of SWDCP-SCmules
scheme roughly increases with the increase of storage limit,
which is because the weight we set to collection rate is much
VOLUME 4, 2016

FIGURE 20. The change of JS with respect to storage limit.

larger than the weight we set to redundancy rate, i.e. λ1 > λ2 ,
in synthesized rate. Besides that, we can also see that ℘1000
outperforms ℘random since the red line is above the black
line, which means the synthesized rate of red line is rate of
black line, especially when the storage limit is not large. This
proves the effectiveness of SA-PA algorithm. However, when
storage limit is large, the effectiveness of optimization is not
significant, this is because the storage is large enough to get
rid of the limitation of storage.
Then we closely compare the degree of improvement provided by SA-PA algorithm from the change of JS C
with
[t2 ,t3 )
respect to the increase of time in test set.
In Figure 21, the black line illustrates the increase of
JS (℘random , [t2 , t3 )) with respect to the increase of time
where ℘random is a randomly generated priority assignment.
The red line illustrates the increase of JS (℘200 , [t2 , t3 )) with
respect to the the increase of time where ℘200 is the priority
assignment generated by a running of SA-PA algorithm with
nbMV = 200 based on ℘random . From Figure 21, we can
see that no matter which priority assignment is used, the rate
of SWDCP-SCmules roughly increase with the increase of
time, which is because the weight we set to collection rate
is much larger than the weight we set to redundancy rate,
i.e. λ1 > λ2 , in synthesized rate. Besides synthesized rate.
Besides that, we can also see that ℘200 outperforms ℘random
since the red line is above the black line, which means the
synthesized rate of ℘200 is larger than the synthesized rate
of ℘random in every time. This proves the effectiveness of
SA-PA algorithm.
Finally we closely compare the degree of improvement
provided by SA-PA algorithm from the change of JS C
[t2 ,t3 )
with respect to the increase of the number of data centers in
test set.
In Figure 22, the black line illustrates the increase of
JS (℘random , [t2 , t3 )) with respect to the increase of the number of data centers where ℘random is a randomly generated
priority assignment. The red line illustrates the increase of
6499

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

TABLE 2. Main notations.

FIGURE 21. The change of JS with respect to time.

We use SCmules with a priority assignment optimized
by SA-PA algorithm with respect to the optimization target function JS , where the size of storage of SCmules
is 20, to compare with mules, which has no storage, J C
constraint. Table 2 gives the value of JS C
[t2 ,t3 ) C[t2 ,t3 )
and J C
using SCmules and mules. From this table,
R t ,t
[ 2 3)
we can draw the conclusion that, although the performance, especially the collection rate, can be influenced by
whether the oblivious data mules are storage-constrained
or not, the bad influence can be significantly mitigated by
using a proper priority assignment found by SA-PA algorithm. In addition, the proper priority assignment found
by SA-PA algorithm can reduce the redundancy rate and
therefore reducing energetic waste and increasing network
lifetime.
VII. CONCLUSIONS

FIGURE 22. The change of JS with respect to the number of data centers.

JS (℘500 , [t2 , t3 )) with respect to the increase of the number
of data centers where ℘500 is the priority assignment generated by a running of SA-PA algorithm with nbMV = 500
based on ℘random . From Figure 22, we can see that no matter
which priority assignment is used, the synthesized rate of
SWDCP-SCmules roughly increase with the increase of the
number of data centers, which is because the weight we set
to collection rate is much larger than the weight we set to
redundancy rate, i.e. λ1 > λ2 , in synthesized rate. Besides
that, we can also see that ℘500 outperforms ℘random since the
red line is above the black line, which means the synthesized
rate of ℘500 is larger than the synthesized rate of ℘random no
matter how many data centers are deployed. This proves the
effectiveness of SA-PA algorithm.
4) SCmules VERSUS MULES

In this subsection, we analyze the influence of storageconstraint, in other words, we compare the performance
of SCmules introduced in this paper with the performance of mules (i.e. oblivious data mules) proposed
by Bonola et al. [17].
6500

In this paper, we propose a Social Welfare Data Collection paradigm based on Storage-Constrained Oblivious Data
Mules (SWDCP-SCmules scheme). In this scheme, intelligent devices are embedded to infrastructures of Smart City
to detect and report their status, data centers are deployed
in hotspot areas of Smart City to collect data from intelligent devices and SCmules are mobile IoT nodes moving in Smart City to obliviously pick up data reported
by intelligent devices and store-carry-forward to data centers. The SWDCP-SCmules scheme enables the intelligent management of cities and boost the overall social
welfare.
However, the storage size of SCmules are limited, to
cope with the storage limitations, the concept of priority is introduced to model the selection strategy used
in the situation that SCmules lack of spare storage and
converts selection strategy to priority assignment. The
Simulated Annealing for Priority Assignment Algorithm
(SA-PA algorithm) is proposed to guide the priority assignment of intelligent devices. The SA-PA algorithm is a universal machine learning algorithm which can automatically
find well-performed priority assignment with respect to various optimization targets by learning social-aware patterns
from the past GPS trajectory data of SCmules and significantly improve the performance of SWDCP-SCmules
scheme.
In general, the SWDCP-SCmules combined with SA-PA
algorithm can propel the construction of Smart City and
thereby lead to significant improvement of overall social
welfare.
VOLUME 4, 2016

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

REFERENCES
[1] Z. Zhao, M. Peng, Z. Ding, W. Wang, and H. V. Poor, ‘‘Cluster content
caching: An energy-efficient approach to improve quality of service in
cloud radio access networks,’’ IEEE J. Sel. Areas Commun., vol. 34, no. 5,
pp. 1207–1221, May 2016.
[2] S. He, S. He, D.-H. Shin, J. Zhang, J. Chen, and Y. Sun, ‘‘Fullview area coverage in camera sensor networks: Dimension reduction
and near-optimal solutions,’’ IEEE Trans. Veh. Technol., vol. 65, no. 9,
pp. 7448–7461, Sep. 2016, doi: 10.1109/TVT.2015.2498281.
[3] M. Peng, H. Xiang, Y. Cheng, S. Yan, and H. V. Poor, ‘‘Inter-tier interference suppression in heterogeneous cloud radio access networks,’’ IEEE
Access, vol. 3, pp. 2441–2455, 2015.
[4] H. Li, X. Lin, H. Yang, X. Liang, R. Lu, and X. Shen, ‘‘EPPDR: An efficient privacy-preserving demand response scheme with adaptive key evolution in smart grid,’’ IEEE Trans. Parallel Distrib. Syst., vol. 25, no. 8,
pp. 2053–2064, Aug. 2014.
[5] S. He, J. Chen, X. Li, X. Shen, and Y. Sun, ‘‘Mobility and intruder
prior information improving the barrier coverage of sparse sensor networks,’’ IEEE Trans. Mobile Comput., vol. 13, no. 6, pp. 1268–1282,
Jun. 2015.
[6] X. Liu, ‘‘A deployment strategy for multiple types of requirements
in wireless sensor networks,’’ IEEE Trans. Cybern., vol. 45, no. 10,
pp. 2364–2376, Oct. 2015.
[7] Q. Yang, S. He, J. Li, J. Chen, and Y. Sun, ‘‘Energy-efficient probabilistic
area coverage in wireless sensor,’’ IEEE Trans. Veh. Technol., vol. 61, no. 1,
pp. 367–377, Jan. 2015.
[8] Q. Hu, M. Peng, Z. Mao, X. Xie, and H. V. Poor, ‘‘Training design for
channel estimation in uplink cloud radio access networks,’’ IEEE Trans.
Signal Process., vol. 64, no. 13, pp. 3324–3337, Jul. 2016.
[9] H. Li, Y. Yang, T. H. Luan, X. Liang, L. Zhou, and X. S. Shen, ‘‘Enabling
fine-grained multi-keyword search supporting classified sub-dictionaries
over encrypted cloud data,’’ IEEE Trans. Dependable Secure Comput.,
vol. 13, no. 3, pp. 312–325, May/Jun. 2016.
[10] S. Sarkar, S. Chatterjee, and S. Misra, ‘‘Assessment of the suitability of
fog computing in the context of Internet of things,’’ IEEE Trans. Cloud
Comput., to be published, doi: 10.1109/TCC.2015.2485206.2015.
[11] M. Aazam and E.-N. Huh, ‘‘Fog computing micro datacenter based
dynamic resource estimation and pricing model for IoT,’’ in Proc. IEEE
29th Int. Conf. Adv. Inf. Netw. Appl., Mar. 2015, pp. 687–694.
[12] L. Piras. (Mar. 2014). A Brief History of the Internet of Things
[Infographic]. [Online]. Available: http://www.psfk.com/2014/03/
internet-ofthings-infographic.html
[13] Internet of Things Market Forecast Cisco. [Online]. Available:
http://postscapes.com/internet-of-things-market-size
[14] X. Liu, ‘‘A novel transmission range adjustment strategy for energy hole
avoiding in wireless sensor networks,’’ J. Netw. Comput. Appl., vol. 67,
pp. 43–52, May 2016.
[15] Y. Hu and A. Liu, ‘‘Improving the quality of mobile target detection
through portion of node with full duty cycle in WSNs,’’ Comput. Syst. Sci.
Eng., vol. 31, no. 1, pp. 5–17, 2016.
[16] Y. Liu et al., ‘‘FFSC: An energy efficiency communications approach
for delay minimizing in Internet of Things,’’ IEEE Access, vol. 4,
pp. 3775–3793, 2016.
[17] M. Bonola, L. Bracciale, P. Loreti, P. Amici, A. Rabuffi, and G. Bianchi,
‘‘Opportunistic communication in smart city: Experimental insight with
small-scale taxi fleets as data carriers,’’ Ad Hoc Netw., vol. 43, pp. 43–55,
Jun. 2016.
[18] N. Luong, D. Hoang, P. Wang, D. Niyato, D. Kim, and Z. Han, ‘‘Data
collection and wireless communication in Internet of Things (IoT) using
economic analysis and pricing models: A survey,’’ IEEE Commun. Surveys
Tut., to be published, doi: 10.1109/COMST.2016.2582841,2016.
[19] R. C. Shah, S. Roy, S. Jain, W. Brunette, ‘‘Data MULEs: Modeling a
three-tier architecture for sparse sensor networks,’’ Ad Hoc Netw., vol. 1,
nos. 2–3, pp. 215–233, Sep. 2003.
[20] R. Xie, A. Liu, and J. Gao, ‘‘A residual energy aware schedule scheme
for wsns employing adjustable awake/sleep duty cycle,’’ Wireless Pers.
Commun., vol. 2016, pp. 1–29, Jun. 2016, doi: 10.1007/s11277-016-34280.2016.
[21] H. Li, D. Liu, Y. Dai, and T. H. Luan, ‘‘Engineering searchable encryption of mobile cloud networks: When QoE meets QoP,’’ IEEE Wireless
Commun., vol. 22, no. 4, pp. 74–80, Aug. 2015.
[22] J. Gubbi, R. Buyya, S. Marusic, and M. Palaniswami, ‘‘Internet of
Things (IoT): A vision, architectural elements, and future directions,’’
Tech. Rep. CLOUDS-TR-2012-2, Jul. 2012.
VOLUME 4, 2016

[23] C. C. Sobin, V. Raychoudhury, G. Marfia, and A. Singla, ‘‘A survey
of routing and data dissemination in delay tolerant networks,’’ J. Netw.
Comput. Appl., vol. 67, pp. 128–146, May 2016.
[24] M. Raj, N. Li, D. Liu, M. Wright, S. K. Das , ‘‘Using data mules to preserve
source location privacy in wireless sensor networks,’’ Pervasive Mobile
Comput., vol. 11, pp. 244–260, Apr. 2014.
[25] Y.-C. Tseng, F.-J. Wu, W.-T. Lai, ‘‘Opportunistic data collection for disconnected wireless sensor networks by mobile mules,’’ Ad Hoc Netw., vol. 11,
no. 3, pp. 1150–1164, May 2013.
[26] C.-K. Tham and T. Luo, ‘‘Fairness and social welfare in service allocation
schemes for participatory sensing,’’ Comput. Netw., vol. 73, pp. 58–71,
Nov. 2014.
[27] A. Liu, Y. Hu, and Z. Chen, ‘‘An energy-efficient mobile target detection
scheme with adjustable duty cycles in wireless sensor networks,’’ Int. J. Ad
Hoc Ubiquitous Comput., vol. 22, no. 4, pp. 203–225, 2016.
[28] Y. Liu, M. Dong, K. Ota, and A. Liu, ‘‘ActiveTrust: Secure and trustable
routing in wireless sensor networks,’’ IEEE Trans. Inf. Forensics Secur.,
vol. 11, no. 9, pp. 2013–2027, Sep. 2016.
[29] C.-K. Tham and T. Luo, ‘‘Quality of contributed service and market equilibrium for participatory sensing,’’ IEEE Trans. Mobile Comput., vol. 14,
no. 4, pp. 829–842, Apr. 2015.
[30] A. Liu, X. Liu, and Y. Liu, ‘‘A comprehensive analysis for fair probability
marking based traceback approach in WSNs,’’ Secur. Commun. Netw.,
vol. 9, no. 14, pp. 2448–2475, Sep. 2016.
[31] M. Dong, K. Ota, and A. Liu, ‘‘RMER: Reliable and energy-efficient data
collection for large-scale wireless sensor networks,’’ IEEE Internet Things
J., vol. 3, no. 4, pp. 511–519, Aug. 2016.
[32] J. Long, C. Gao, S. He, X. Liu, and A. Liu, ‘‘Bridging the gap among actor–
sensor–actor communication through load balancing multi-path routing,’’
EURASIP J. Wireless Commun. Netw., vol. 2015, p. 256, Dec. 2015, doi:
10.1186/s13638-015-0484-1.
[33] Z. Song, C. H. Liu, J. Wu, J. Ma, and W. Wang, ‘‘Qoi-aware
multitask-oriented dynamic participant selection with budget constraints,’’ IEEE Trans. Veh. Technol., vol. 63, no. 9, pp. 4618–4632,
Nov. 2014.
[34] S. Yousefi, M. S. Mousavi, and M. Fathy, ‘‘Vehicular ad hoc networks (VANETs): Challenges and perspectives,’’ in Proc. 6th Int. Conf. ITS
Telecommun., Jul. 2006, pp. 761–766, doi: 10.1109/ITST.2006.289012.
[35] Y. Saeed, SA. Lodhi, K. Ahmed, ‘‘Obstacle management in VANET using
game theory and fuzzy logic control,’’ Int. J. Commun., vol. 4, no. 1,
pp. 9–13, Jan. 2013.
[36] M. T. Mitchell, Machine Learning. vol. 45. Burr Ridge, IL, USA:
McGraw Hill, 1997, p. 37.
[37] C. Bishop, Pattern Recognition and Machine Learning (Information Science and Statistics), 2nd ed. 2007.
[38] I. Boussaïd, J. Lepagnot, and P. Siarry, ‘‘A survey on optimization metaheuristics,’’ Inf. Sci., vol. 237, pp. 82–117, Jul. 2013.
[39] B. Suman and P. Kumar, ‘‘A survey of simulated annealing as a tool for
single and multiobjective optimization,’’ J. Oper. Res. Soc., vol. 57, no. 10,
pp. 1143–1160, Oct. 2006.
[40] T. Tanaka, T. Suzuki, and K. Kurihara, ‘‘Energy harvesting technology for
maintenance-free sensors,’’ Fujitsu Sci. Technol. J., vol. 50, pp. 93–100,
Jan. 2014.
[41] D. G. Leeper, ‘‘A long-term view of short-range wireless,’’ Computer,
vol. 34, no. 6, pp. 39–44, Jun. 2001.
[42] J. M. Rabaey, M. J. Ammer, J. L. da Silva, D. Patel, and S. Roundy, ‘‘Picoradio supports ad hoc ultra-low power wireless networking,’’ Computer,
vol. 33, no. 7, pp. 42–48, Jul. 2000.
[43] W. Ye, J. Heidemann, and D. Estrin, ‘‘An energy-efficient MAC protocol
for wireless sensor networks,’’ in Proc. INFOCOM, vol. 3. Jun. 2002,
pp. 1567–1576.
[44] C. Wang, D. Mu, F. Zhao, J. W. Sutherland, ‘‘A parallel simulated annealing method for the vehicle routing problem with simultaneous pickup–
delivery and time windows,’’ Comput. Ind. Eng., vol. 83, pp. 111–122,
May 2015.
[45] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge, U.K.:
Cambridge Univ. Press, 2004.
[46] S. He, J. Chen, F. Jiang, D. K. Y. Yau, G. Xing, and Y. Sun, ‘‘Energy provisioning in wireless rechargeable sensor networks,’’ IEEE Trans. Mobile
Comput., vol. 12, no. 10, pp. 1931–1942, Oct. 2013.
[47] J. Yuan, Y. Zheng, X. Xie, and G. Sun, ‘‘Driving with knowledge
from the physical world,’’ in Proc. 17th ACM SIGKDD Int. Conf.
Knowl. Discovery Data Mining KDD, New York, NY, USA, 2011,
pp. 316–324.
6501

Z. Tang et al.: Social-Aware Data Collection Scheme Through Opportunistic Communication

ZHIPENG TANG is currently pursuing the degree
with the School of Information Science and Engineering, Central South University, China. His
research interests include services-based network,
crowd sensing networks, and wireless sensor
networks.

ANFENG LIU received the M.Sc. and Ph.D.
degrees in computer science from Central South
University, China, in 2002 and 2005, respectively. He is currently a Professor with the School
of Information Science and Engineering, Central
South University, China. He is also a member
(E200012141M) of China Computer Federation.
His major research interest is wireless sensor
network.

6502

CHANGQIN HUANG received the Ph.D. degree
in computer science and technology from Zhejiang
University, Hangzhou, in 2005. He is a currently
a Professor with the School of Information Technology in Education, South China Normal University, China. He is also a Guangdong Specially
Appointed Professor (Pearl River Scholar), and a
Senior Member (E200014100S) of China Computer Federation. He has authored over 80 research
papers in international journals and conferences.
His research interests include service computing, cloud computing, semantic
web, and education informationization.

VOLUME 4, 2016

See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/291421471

Enhancing	the	flow	experience	of	consumers	in
China	through	interpersonal	interaction	in
social	commerce
Article		in		Computers	in	Human	Behavior	·	May	2016
DOI:	10.1016/j.chb.2016.01.012

CITATIONS

READS

3

320

4	authors,	including:
Hefu	Liu

Qian	Huang

University	of	Science	and	Technology	of	China

University	of	Science	and	Technology	of	China

36	PUBLICATIONS			345	CITATIONS			

19	PUBLICATIONS			194	CITATIONS			

SEE	PROFILE

SEE	PROFILE

Summer	Xiayu	Chen
University	of	Science	and	Technology	of	China
7	PUBLICATIONS			7	CITATIONS			
SEE	PROFILE

All	content	following	this	page	was	uploaded	by	Summer	Xiayu	Chen	on	25	January	2016.

The	user	has	requested	enhancement	of	the	downloaded	file.	All	in-text	references	underlined	in	blue	are	added	to	the	original	document
and	are	linked	to	publications	on	ResearchGate,	letting	you	access	and	read	them	immediately.

Computers in Human Behavior 58 (2016) 306e314

Contents lists available at ScienceDirect

Computers in Human Behavior
journal homepage: www.elsevier.com/locate/comphumbeh

Enhancing the ﬂow experience of consumers in China through
interpersonal interaction in social commerce
Hefu Liu, Haili Chu, Qian Huang*, Xiayu Chen
School of Management, University of Science and Technology of China, Jinzhai Road 96, Hefei, Anhui, China

a r t i c l e i n f o

a b s t r a c t

Article history:
Received 22 March 2015
Received in revised form
21 December 2015
Accepted 10 January 2016
Available online xxx

Although research on ﬂow experience has recently received much attention, few studies have been
published on the perceived interpersonal interaction factors of consumers and their inﬂuence in social
commerce. In addition, few studies have focused on the impact of interpersonal interaction factors on
ﬂow experience. Drawing on the stimulus-organism-response framework, this study examines the
impact of interpersonal interaction factors (perceived expertise, similarity, and familiarity) on the formation of ﬂow experience and its subsequent effects on purchase intention in the context of social
commerce. We investigate whether the impact of the three interpersonal interaction factors on ﬂow
experience differs between young and old users. We conduct a survey and collect 349 responses from
users of a social shopping site in China. Our results indicate that interpersonal interaction factors
positively relate to ﬂow experience and subsequently inﬂuence purchase intention. We also ﬁnd differences between young and old users in this area.
© 2016 Published by Elsevier Ltd.

Keywords:
Social commerce
Interpersonal interaction
Flow experience
Purchase intention

1. Introduction
Social commerce is an emerging business trend that is growing
rapidly in China. According to the 2015 McKinsey report, consumers in China spend 78 min per day on social commerce.
Approximately 50% of customers in China make their purchase
decisions according to recommendations from relatives and
friends. In recent years, the virtual experiences of customers in the
social commerce context have gained importance. Providing consumers with unforgettable experiences has emerged as an important issue in driving customer participation and developing
favorable consumer behavior responses in social commerce (Huang
& Benyoucef, 2014; Zhang, Lu, Gupta, & Zhao, 2014).
When considering the provision of online consumption experiences, scholars have highlighted the importance of ﬂow (Chang,
2013; Faiola, Newlon, Pfaff, & Smyslova, 2013). Flow is a state of
concentration in which people are so involved that nothing else
matters (Gao & Bai, 2014). Speciﬁcally, ﬂow refers to a temporarily
unaware experience in which an individual engages in a social
shopping activity in a social shopping website with total

* Corresponding author.
E-mail addresses: liuhf@ustc.edu.cn (H. Liu), Chl0816@ustc.edu.cn (H. Chu),
huangq@ustc.edu.cn (Q. Huang), cxy1023@mail.ustc.edu.cn (X. Chen).
http://dx.doi.org/10.1016/j.chb.2016.01.012
0747-5632/© 2016 Published by Elsevier Ltd.

concentration, control, and enjoyment (Gao & Bai, 2014). In
emphasizing the importance of ﬂow and the formation of
compelling experiences, Hoffman and Novak (1996) went as far as
declaring that “creating a commercially compelling website depends on facilitating a state of ﬂow for consumers [and that] … an
important objective for marketers is to provide these opportunities” (Hoffman & Novak, 1996, p. 66). Zhang et al. (2014) argued
that enhancing the ﬂow experience is essential for the survival of
social commerce. Despite the understanding of the contribution of
ﬂow to the creation of compelling experiences, investigating the
drivers of customer ﬂow experiences is important for the success of
social shopping sites, however, little effort has been devoted to
studying the factors contributing to ﬂow experience in social
commerce. In order to ﬁll this gap, the present study is trying to
explore the formation drivers of ﬂow in social commerce.
With the growing competition, online vendors rely on web atmospherics to create an environment that can produce positive
emotional and cognitive states of online shoppers (Gao & Bai, 2014;
Zhang et al., 2014). Chang (2013) suggested that social interaction
among members in social networking sites would yield a state of
ﬂow. In the social commerce context, interpersonal interactional
factors have received much attention (Hsiao, Lin, Wang, Lu, & Yu,
2010; Liao, Chu, Huang, & Shen, 2010; Lu, Zhao, & Wang, 2010).
Social commerce involves using social media to support social
interaction, and its unique characteristics provide opportunities for

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

consumers to make better buying decisions (Ng, 2013). Carlson and
O'Cass (2011) posited that future studies should explore the effects
of consumer-based variables on the formation of ﬂow experience.
However, to our knowledge, little is known about the interpersonal
interaction factors that promote the ﬂow experience for customers
in social commerce.
Drawing from the above literature review, we infer that investigating the impact of interpersonal interaction factors on the creation of ﬂow experience should be a promising research area in
social commerce. On the basis of Liao et al. (2010) study, three
interpersonal interaction factors are investigated, namely,
perceived expertise of group members, similarity of group members, and familiarity of group members. As the context of our
research is similar to the virtual community context, we focus on
these three interpersonal interaction factors. Therefore, this study
draws on the stimuluseorganismeresponse (SOR) model to
investigate the impact of the three interpersonal attraction factors
on the ﬂow experience and the relationship between ﬂow experience and purchase intention.
This research makes important contributions to the extant
literature. First, we extend the extant literature by testing and
validating a model by incorporating interpersonal drivers of ﬂow
experience in social commerce. Second, our data analysis reveals
signiﬁcant differences between young and old users. Third, the
present study advances the understanding that interpersonal
interaction factors remain important in the context of social commerce. Fourth, our research provides empirical evidence to the
deduction that interpersonal interaction factors positively affect
purchase intention through ﬂow experience in social commerce.
2. Theoretical background and research hypotheses
2.1. SOR framework
The SOR model is extensively used in studies that measure the
impact of perceived website features on consumer responses (Gao
& Bai, 2014; Zhang et al., 2014). According to the SOR model,
environmental stimuli (S) inﬂuence consumer internal states (O)
and correspondingly affect consumers' overall responses (R).
Donovan and Rossiter (1982) proposed a model that is adapted to
the retail context. The model treats atmospheric cues as stimuli,
two major emotional states as organism, and shopping behaviors
within the store as response. We can learn from the application of
the SOR model in the retailing context that environmental stimuli
inﬂuence consumer internal states, which in turn drive their
behavioral intention toward the store. Fiore and Kim (2007)
developed an integrated SOR model for the brick-and-mortar
context. In the framework, the stimuli include ambient, design,
and social cues.
In the online shopping environment, some researchers use
actual stimuli (Animesh, Pinsonneault, Yang, & Oh, 2011; Kim &
Lennon, 2010; Wang, Hernandez, & Minor, 2010), and others use
customer assessments of the stimuli to denote the stimulus
segment of the model (Koo & Ju, 2010; Manganari, Siomkos,
Rigopoulou, & Vrechopoulos, 2011; Nath, 2009). Research on the
social factors of online shopping environment is growing in the
context of virtual community and social shopping sites (Hsiao et al.,
2010; Lu et al., 2010). The present study adopts the SOR model using
interpersonal interaction factors as the environmental stimulus (S).
The organism pertains to emotional and cognitive states and includes experiences (Jiang, Chan, Tan, & Chua, 2010). In the current
study, adapting from the research of Gao and Bai (2014), the organism is the customer's cognitive judgment of the online consumers' experience, which is presented in the form of ﬂow
experience. The responses refer to website patronage intention

307

(Jeong, Fiore, Niehm, & Lorenz, 2009), purchase intention (Hsu,
Chang, & Chen, 2011), and intention to use and buy (Huang,
2013). In our study, we follow Hsu et al. (2011) and treat consumer purchase intention as consumer behavioral outcomes.
The application of the SOR paradigm as a holistic theory is
appropriate for this study for two reasons. First, the SOR paradigm
was extensively used in previous research on online customer
behavior (Chang, Chih, Liou, & Hwang, 2014; Hsieh, Hsieh, Chiu, &
Yang, 2014; Parboteeah, Valacich, & Wells, 2009). For example,
using the SOR paradigm, Zhang et al. (2014) examined the effects of
three technological stimuli (perceived interactivity, personalization, and sociability) on consumers' virtual experiences and subsequent social commerce intention. Parboteeah et al. (2009)
applied the SOR paradigm to explore the impact of task-relevant
cues and mood-relevant cues on perceived usefulness and
perceived enjoyment, and then online purchase intention. The
ﬁndings of these studies support the use of the SOR paradigm in
accounting for consumer internal reactions and behavioral outcomes to the stimuli. Second, the SOR paradigm provides a strict
and structured manner to examine the impact of interpersonal
interaction factors as environmental stimuli on consumer online
experiences (e.g., ﬂow) and their subsequent intention to purchase
from social commerce sites.
2.2. Social inﬂuence factors as environmental stimuli (S)
Social commerce involves the application of social media to
support social interaction, communication, and user-generated
content for assisting consumers in online buying. One of its
unique characteristics is that it provides an opportunity for consumers to make better buying decisions and improve their future
shopping experience (Ng, 2013). Therefore, social commerce sites
need to facilitate member interaction. Frequent member interaction will enhance the interpersonal attraction of websites (Liao
et al., 2010). As our research focuses on interpersonal interaction
factors, ﬂow experience, and online buying behavior, we consider
three interpersonal interaction factors proposed by Liao et al.
(2010), namely, perceived similarity, expertise, and familiarity.
Perceived similarity refers to the commonness shared by customers
in taste, preference, and liking toward products. Perceived expertise refers to other consumers' ability to recommend products
based on their knowledge and experience. Perceived familiarity
relates to the frequency of interactions and relationships with other
shoppers in social shopping sites (Liao et al., 2010).
2.3. Flow experience as customer internal states (O)
The SOR paradigm suggests that the impact of environmental
stimuli on consumer behavior is mediated by virtual experiences
(Animesh et al., 2011; Zhang et al., 2014). Generally, studies show
that the effects of web atmospherics can be studied from two major
perspectives. The ﬁrst perspective includes cognitive reactions
from the stimulus such as perceived usefulness (Parboteeah et al.,
2009). The second perspective refers to affective reactions from
the stimulus such as perceived enjoyment (Floh & Madlberger,
2013). According to Gao and Bai (2014), focusing on the cognitive
aspects is signiﬁcant. Furthermore, there is a lack of investigations
focus only on the cognitive responses of consumers, such as ﬂow
experience, especially in social commerce.
According to Ding, Hu, Verma, and Wardell (2010), ﬂow experience has been treated as a basis to facilitate the creation of a
compelling experience. Flow is a psychological state in which
people become completely involved within a stimulus, and it can be
described as the whole experience that individuals feel when they
are fully absorbed (Gao & Bai, 2014). Some scholars highlight the

308

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

importance of ﬂow in computer-mediated environments and suggest that the success of online vendors depends on their ability to
create opportunities for customers to experience ﬂow (Gao & Bai,
2014; Hoffman & Novak, 1996; Hsu, Chang, & Chen, 2012). In
computer-mediated environments, the interactivity among members creates a sense of immersion and induces a state of ﬂow for
users (Mollen & Wilson, 2010; Teng, Huang, Jeng, Chou, & Hu,
2012).
In the application of the SOR model, many scholars identify the
relationship between ﬂow experience and purchase intention. For
example, Animesh et al. (2011) explored the impact of technological and spatial factors on purchase intention through the mediating effect of ﬂow, telepresence, and social presence. Gao and Bai
(2014) investigated the inﬂuence of website atmospheric cues on
purchase intention and satisfaction by applying ﬂow as a mediator.
As ﬂow is a broad concept in different contexts, many studies argue
that ﬂow is a multidimensional concept with different components.
For example, Wang, Baker, Wagner, and Wakeﬁeld (2007) reported
that ﬂow consists of control, interest, attention, and curiosity. According to Gao and Bai (2014), we deﬁne ﬂow as a temporarily
unaware experience in which an individual engages in an social
shopping activity in a social shopping website with total concentration, control, and enjoyment.

2.4. Interpersonal stimulus(S) and ﬂow experience(O)
Expertise is deﬁned as a person's amount of knowledge about a
ﬁeld (Liao et al., 2010). The expertise of the source in a group is
important for information acceptance (Petty, Cacioppo, & Goldman,
1981), and people agree more with an expert's view when suffering
from social inﬂuence (Kelman, 1961). In the domain of a social
shopping site, members with higher expertise contribute useful
advice (Constant, Sproull, & Kiesler, 1996). The useful and relevant
message provided by group members will reduce information
asymmetry and cost, leading to enjoyable experiences (Kim & Li,
2009). Moreover, consumers are likely to have higher levels of
interaction if useful and relevant information is provided
(MacKenzie & Lutz, 1989; Zhou, 2013). The involvement in the
interaction will make customers lose their self-awareness and
experience a ﬂow state (Gao & Bai, 2014). Therefore, we argue that
perceived expertise of group members in the social shopping
environment will induce ﬂow experience.
H1. Perceived expertise is positively related to ﬂow.
In the present study, similarity refers to the self-perceived
similarity of psychological traits (e.g., preferences and tastes) of
members of social shopping sites. According to the similarityeattraction theory, individuals are attracted by those who are
similar to them (Al-Natour, Benbasat, & Cenfetelli, 2005). Al-Natour
et al. (2005) pointed out that consumers' perceived similarity of
other members helps them enjoy interactions. Empirical studies
demonstrate that social interactions are correlated with the experience of ﬂow (Animesh et al., 2011; Chang, 2013). Therefore, social
interaction enhances the level of enjoyment and makes virtual
experiences more enjoyable and engaging, thus leading to a ﬂow
state (Animesh et al., 2011). Therefore, we can infer that if consumers feel that the interaction is interesting, they will prefer to
focus on the interaction and consequently enter a state of ﬂow (Gao
& Bai, 2014). The current study suggests that consumers who
perceive similarity with members in social commerce sites are
likely to experience a state of ﬂow.
H2. Perceived similarity is positively related to ﬂow.
In social shopping, familiarity refers to the previous interaction

of members in social shopping sites and their knowledge of other
group members (Liao et al., 2010). Familiarity can reduce uncertainty (Hinds, Carley, Krackhardt, & Wholey, 2000), increase
cognitive trust (Komiak & Benbasat, 2006), and promote individual
social interactions. In addition, social interactions among consumers in social commerce sites help in the purchase of a product,
and they lead consumers to generating a sense of self-worth or selfefﬁcacy (Zhang et al., 2014). Consumers will then ﬁnd social commerce enjoyable and involving (Pagani & Mirabello, 2011). Consumer participation in this involvement process may help lead
them to be fully immersed in their activities, which could induce a
sense of ﬂow (Animesh et al., 2011). Therefore, this study argues
that consumers who perceive familiarity with members in social
commerce sites may experience a state of ﬂow.
H3. Perceived familiarity is positively related to ﬂow.

2.5. Purchase intention as response (R)
In social commerce, social media tools are used to support social
interactions and user contributions to promote activities in the
process of selling and buying products (Wang & Zhang, 2012). According to Liao et al. (2010), customers are exposed to various
interpersonal attraction factors and inﬂuences, such as perceived
expertise, perceived familiarity, perceived similarity, informational
inﬂuence, and normative inﬂuence, which will motivate their
subsequent behavior. Previous studies have demonstrated that
consumer purchase intention could reﬂect consumer behavioral
outcomes (Gao & Bai, 2014; Huang, 2013; Jiang et al., 2010).
Therefore, we use purchase intention, speciﬁcally purchase intention in social commerce, as the response in the model. Our research
is consistent with studies that use the SOR model and treat purchase intention as the response (Jiang et al., 2010; Kim & Lennon,
2013; Wu, Lee, Fu, & Wang, 2013).
2.6. Flow experience(O) and purchase intention(R)
Flow experience is a compelling experience that affects consumer behavior in online shopping (Koufaris, 2002) and social
network games (Shin & Shin, 2011). Previous studies argued that
ﬂow experience leads to speciﬁc behavioral outcomes (Gao & Bai,
2014; Hsu et al., 2012; Zhang et al., 2014). Hoffman and Novak
(1996) suggested that individuals who experience ﬂow states
would have higher satisfaction and loyalty than those who do not.
Koufaris (2002) posited that consumers who experience ﬂow when
visiting an online store are likely to make unplanned purchases. In
social commerce, consumers who have experienced ﬂow are likely
to participate in social commerce activities (Zhang et al., 2014),
which affect customer purchase intention. Lee and Chen (2010) and
Gao and Bai (2014) noted that ﬂow experience affects consumer
behavioral intention, such as the likelihood to purchase from the
website. We argue that participants who have compelling experiences will become fully involved in their interactions in social
shopping sites and will be more likely to buy products for their
virtual existence (Animesh et al., 2011). On the basis of Animesh
et al. (2011), we infer that consumers who enjoy their virtual existence are likely to spend more time and money in the purchase of
products in social shopping sites. Building on past research, this
study indicates that customers who experience a state of ﬂow in
social shopping sites are likely to purchase from the social commerce sites.
H4. Flow experience is positively related to purchase intention in
social commerce.

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

2.7. Mediating effect of ﬂow experience
The present study tests for the mediating effect of ﬂow experience. The SOR model provides a theoretical foundation for the
mediating effect of ﬂow experience. Studies that apply the SOR
framework demonstrate that consumer internal state (organism)
can mediate between stimuli and consumer response behavior
(Gao & Bai, 2014; Ha & Lennon, 2010; Yoon, 2012). For example,
Manganari et al. (2011) investigated the impact of perceived online
store features (ease of use of the layout) on consumer responses
(satisfaction and trust) and posited that consumer internal states
(pleasure and attitude) mediate such impact. Ha and Lennon (2010)
examined the mediating role of affective states between website
design and consumer response behavior. Based on the SOR model
and previous studies on the mediating effects of internal cognitive
states, the current study suggests that ﬂow plays a mediating role
between the interpersonal interaction factors of the website and
consumer responses. Fig. 1 shows the conceptual model of the
current study.
H5. Flow mediates the relationship between interpersonal interaction factors (perceived similarity, expertise, and familiarity) and
purchase intention.

3. Methods
3.1. Measurement development
In this study, the items used in the survey were adapted from
existing research to ﬁt the context of social commerce. We followed
the generally accepted suggestion on wording questions when
developing and ﬁnalizing the questionnaire (Fang et al., 2014).
Perceived similarity was measured using four items adapted from
Liao et al. (2010). Items for perceived expertise and familiarity were
also adapted from Liao et al. (2010). Items for ﬂow were adapted
from Zhang et al. (2014). Items for purchase intention were adapted
from Pavlou and Fygenson (2006). Appendix A lists the measure
items and their related sources.
Seven-point Likert scales ranging from “strongly disagree” to
“strongly agree” were used to measure all items in the survey. To
ensure content validity, we conducted an expert review to reﬁne
the instruments. All construct items were originally developed in
English. As our research was conducted in China, all instrument
items were translated to Chinese following the translation committee approach (Van de Vijver & Leung, 1997). Four native Chinese

309

IS Ph.D. students who are ﬂuent in English were involved in the
translation process. The initial Chinese questionnaire was piloted
among some of our peers and online friends. Forty useful responses
were returned before being accepted as the ﬁnal version. Several
control variables were included in our model to ensure that
empirical results were not due to covariance with other variables.
Previous literature suggests that consumers' gender, level of education, and income may affect the intention to purchase on the
Internet (Fang et al., 2014; Pavlou & Fygenson, 2006). On the basis
of Lee, Qu, and Kim (2007), we included gender, education, and
income of shoppers as control variables in our study.
3.2. Survey design
We conducted a survey to test our research model. We chose the
survey method because this quantitative research method predicts
behavior and examines the relations between variables and constructs (Newsted, Huff, & Munro, 1998). Besides, the survey method
has been widely employed in investigating behaviors in social
commerce (Huang & Benyoucef, 2014; Zhang et al., 2014). To collect
our survey data, an online survey was used for the present study.
Our target population comprised online users of a particular website. Using an online survey can maintain the consistency between
the research and data collection contexts. Moreover, an online
survey has many advantages, such as wide reach. In addition, in our
research, the model is integrated and includes a lot of social variables that are difﬁcult to measure by other methods, such as case
studies or experiments. Thus we believe that survey is an appropriate method for the current study (Cheung & Lee, 2009).
3.3. Data collection
We chose one of the largest social shopping website as the
research context. At the same time, this website claimed to be a
fashion shopping website aiming to help users to make better
purchase decisions. On this website, registered users can establish
their own proﬁles, build relationships with other online consumers,
and contact with other through using communication tools.
Accordingly, we think this website is a suitable research context for
studying social commerce topic. We collected data through an
online survey, and our target samples were the registered users of
this website. Only those users who have purchase experience in the
website were included in our survey. An online survey questionnaire was created on an online survey website in China (www.
wenjuanxing.com). Data were collected from one channel. The

Fig. 1. Conceptual model of this study.

310

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

online questionnaire was distributed through email to potential
users with the help of an online survey website. To encourage
participation in our study, respondents were told that they would
be rewarded with 300 points that could be used to exchange for
money if they completed the survey. Finally, 349 valid responses
were obtained for the ﬁnal data analysis. Table 1 shows the demographic proﬁles of the respondents.
3.4. Data analysis
To test our measurement and structural model, we choose the
structural equation modeling using partial least squares (PLS)
estimation. The PLS is a powerful technique, which combines the
principal components analysis (CFA) and regresstion, to estimate
the measurement and structural model simultaneously (Hair,
Anderson, Tatham, & Black, 1998). In the current study, we used
the software of Smart-PLS 2.0 to conduct the PLS estimation. According to Barnes (2011), the Smart-PLS 2.0 is better equipped to
deal with formative measures and moderating relationships.
Tamjidyamcholo, Gholipour, Baba, and Yamchello (2013) posited
that Smart PLS is not only able to formulate a formative model for
latent constructs but also demands fewer requirements to verify a
model. Thus, we used Smart PLS 2.0 software to examine both the
CFA and the structural model in our study.
4. Results
4.1. Common method bias
When all data were perceptual and collected from a single
source at the same point of time, the issue of common method bias
might be a threat to the validity of the research (Podsakoff,
MacKenzie, Lee, & Podsakoff, 2003). In our study, common
method bias was examined using Harman's single factor test. The
analysis ﬁnally showed that all the items can be categorized into
ﬁve factors, and the ﬁrst factor explains only 14.85% of the variance.
These results suggested that common method bias was not a
serious concern in the present study.
4.2. Measurement model
The measurement model were examined based on the CFA (Hair

Table 1
Demographics of respondents.
Demographics
Gender
Male
Female
Age range
Below 25
25e29
30e39
Above 40
Educational level
High school or below
Junior college
University
Master or above
Personal income in RMB (monthly)
Below 1000
1000e1999
2000e3999
4000e5999
6000e7999
Above 8000

Frequency

Percentage (%)

90
259

25.8%
74.2%

171
138
32
8

49.0%
39.5%
9.2%
2.3%

6
25
294
24

1.7%
7.2%
84.2%
6.9%

106
40
56
72
41
34

30.4%
11.5%
16.0%
20.6%
11.8%
9.7%

et al., 1998). Speciﬁcally, we assessed the measurement model by
testing the content, convergent, and discriminant validities. By
reviewing the relevant literature and pilot testing the instrument,
we assessed the content validity. During this process, we dropped
some items because of their item-to-total correlations. Convergent
validity was assessed by testing the value of the factor loadings,
Cronbach's alpha, composite reliability, and average variance
extracted (AVE). The results of the conﬁrmatory factor analysis
show that all item loadings are above 0.7. The threshold levels for
Cronbach's alpha, composite reliability, and average variance are
0.7, 0.7, and 0.5, respectively (Flynn, Sakakibara, Schroeder, Bates, &
Flynn, 1990; Fornell & Larcker, 1981; Hair et al., 1998; Nunally &
Bernstein, 1978). As shown in Table 2, the Cronbach's alpha and
composite reliability values are above 0.8, and the AVE of all constructs is above 0.7. Therefore, the results indicate good convergent
validity.
Discriminant validity determines whether the measures of a
construct are distinct from other constructs. To assess discriminant
validity, we adopted two approaches (Gefen & Straub, 2005). First,
according to Fornell and Larcker (1981), we assessed the discriminant validity by comparing the relationship between the correlations among constructs and the square root of the AVE of
constructs. As shown in Table 3, the square roots of the AVE are
higher than the correlations among constructs, thus indicating
good discriminant validity. Second, we examined the items in the
item loadings and cross-loadings to construct the correlations. As
shown in Table 4, all the item loadings of the corresponding constructs are higher than the cross-loading values of the other latent
variables, thus suggesting sufﬁcient discriminant validity.
To ensure that multicollinearity was not an issue, we examined
the variance inﬂation factors (VIFs) and tolerance values of the
independent values. When VIFs are lower than 10 or when tolerance values are higher than 0.1, multicollinearity may not be an
issue (Mason & Perreault, 1991). The results indicate that the VIF
values range from 1.638 to 2.322. Therefore, multicollinearity is not
an important issue in this study.
4.3. Structural model
After demonstrating the validity of the measurement model, we
tested the hypothesized relationships using Smart PLS. Fig. 2 shows
the results of the Smart PLS analysis on the full dataset. The results
indicate that perceived expertise (b ¼ 0.231, p < 0.001), perceived
similarity (b ¼ 0.427, p < 0.001), and perceived familiarity
(b ¼ 0.220, p < 0.01) have positive effects on ﬂow. Therefore, H1, H2,
and H3 are supported. Flow has a signiﬁcant effect on purchase
intention (b ¼ 0.557, p < 0.001), thus supporting H4. The model
illustrates that 54.0% of the variance exists in ﬂow, and 41.8% of the
variance is related to purchase intention. Only one control variable
(income) has a signiﬁcant effect on ﬂow.
Further, we tested the structural model for the old and young
users separately. Speciﬁcally, Heinonen and Strandvik (2007) suggested that age is a key differentiator of responses to digital media
between younger and older consumers. The results of the study of
Barutçu (2007) demonstrate that younger consumers tend to have
more positive attitudes than older customers about mobile entertainment. In the study of Persaud and Azhar (2012), 18- to 25-yearold customers comprised most Internet consumers (Table 1). In
terms of age, 49% of the respondents were below 24, and 51% were
25 years old or older. To explore the impact of age on our research
model, the respondents in our study were divided into two groups:
(1) those less than 25 years old called young users and (2) those
over 25 years old called old users.
Fig. 3 shows that perceived familiarity for old users (b ¼ 0.342,
p < 0.001) has a positive effect on ﬂow experience, and perceived

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

311

Table 2
Results of the conﬁrmatory factor analysis.
Constructs

Items

Loading

Cronbach’ s alpha

Composite reliability

Average Variance extracted

Perceived Similarity (PS)

PS1
PS2
PS3
PS4
PE1
PE2
PE3
PE4
PF1
PF2
PF3
PF4
FL1
FL2
FL3
FL4
PUI1
PUI2
PUI3
PUI4

0.877
0.857
0.861
0.849
0.911
0.889
0.914
0.865
0.882
0.920
0.919
0.895
0.811
0.872
0.807
0.821
0.916
0.927
0.914
0.917

0.884

0.920

0.741

0.917

0.942

0.801

0.926

0.947

0.817

0.847

0.897

0.686

0.938

0.956

0.844

Perceived Expertise (PE)

Perceived Familiarity (PF)

Flow Experience (FL)

Purchase Intention (PUI)

Note: All factor loading are signiﬁcant at the p < 0.001 level.

Table 3
Correlations among constructs.
Constructs

AVE

Cronbatch alpha

1

2

3

4

5

1.
2.
3.
4.
5.

0.801
0.741
0.817
0.686
0.844

0.917
0.884
0.926
0.847
0.938

0.895
0.576
0.445
0.575
0.539

0.861
0.516
0.673
0.561

0.904
0.543
0.513

0.828
0.617

0.919

Perceived Expertise
Perceived Similarity
Perceived Familiarity
Flow Experience
Purchase Intention

Note: Diagonal elements are the square root of the average variance extracted of each construct; Pearson correlations are shown below the diagonal.

Table 4
Item loadings and cross loadings.
Constructs

Items

PS

PE

PF

FL

PUI

Perceived Similarity (PS)

PS1
PS2
PS3
PS4
PE1
PE2
PE3
PE4
PF1
PF2
PF3
PF4
FL1
FL2
FL3
FL4
PUI1
PUI2
PUI3
PUI4

0.877
0.857
0.861
0.849
0.529
0.477
0.520
0.530
0.487
0.436
0.445
0.488
0.568
0.592
0.524
0.543
0.487
0.521
0.540
0.512

0.492
0.477
0.513
0.502
0.911
0.889
0.914
0.865
0.453
0.333
0.368
0.438
0.524
0.506
0.410
0.456
0.503
0.497
0.490
0.491

0.492
0.458
0.396
0.428
0.359
0.391
0.404
0.437
0.882
0.920
0.919
0.895
0.379
0.527
0.433
0.454
0.461
0.474
0.478
0.473

0.595
0.574
0.565
0.584
0.532
0.483
0.488
0.548
0.525
0.447
0.450
0.526
0.811
0.872
0.807
0.821
0.538
0.580
0.549
0.597

0.482
0.463
0.481
0.506
0.495
0.470
0.453
0.505
0.479
0.432
0.423
0.509
0.542
0.562
0.476
0.455
0.916
0.927
0.914
0.917

Perceived Expertise (PE)

Perceived Familiarity (PF)

Flow experience (FL)

Purchase Intention (PUI)

familiarity for young users has no effect on ﬂow experience,
t ¼ 3.97. The results indicate that the impact of ﬂow experience on
purchase intention is stronger for old users (b ¼ 0.64, p < 0.001)
than for young users (b ¼ 0.45, p < 0.001), t ¼ 4.19.
4.4. Mediation analyses
H5 posits that consumer ﬂow experience mediates the effect of
the interpersonal interaction factors on consumer's response

behavior (purchase intention). We used the bootstrapping
approach (Preacher & Hayes, 2008; Shrout & Bolger, 2002) to test
the mediating effect. Table 5 shows that the indirect effect of ﬂow
experience on the relationship between perceived expertise and
purchase intention is signiﬁcant with a 95% bootstrap conﬁdence
interval, excluding zero (CI.95 ¼ 0.0471, 0.1397). This ﬁnding suggests that ﬂow experience mediates the effect of perceived expertise on purchase intention. The indirect effect of perceived
similarity on purchase intention is also signiﬁcant (CI.95 ¼ 0.0667,
0.1803). Moreover, the mediating effect of perceived familiarity is
valid (CI.95 ¼ 0.0239, 0.0880).
5. Discussion and conclusion
This study explores the role of ﬂow experience in inﬂuencing
customers' purchase intention in social shopping sites based on the
SOR framework from the perspective of interpersonal interaction.
According to Liao et al. (2010), we classify interpersonal attraction
factors into perceived expertise, similarity, and familiarity. Then,
we investigate the differences in the inﬂuence of three interpersonal interaction factors on ﬂow experience and the impact of ﬂow
experience on purchase intention according to age. The ﬁndings of
our research support all the hypotheses, thus conﬁrming that ﬂow
experience is a useful predictor of purchase intention in social
commerce.
Our ﬁndings present that all three interpersonal attraction factors signiﬁcantly affect ﬂow experience in social commerce.
Perceived similarity of group members seems to be more important
than perceived expertise and perceived familiarity of group members in the context of social commerce. We also ﬁnd that ﬂow
experience has a positive impact on purchase intention, which

312

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

Fig. 2. Results of the research model tests.

Fig. 3. Results of the research model tests of young users (old users).

Table 5
Bootstrapping results.
95% Bootstrap conﬁdence intervals for indirect effect
Flow experience

Perceived expertise
Perceived similarity
Perceived familiarity

Effect

SE

CIs

0.090
0.120
0.053

0.023
0.286
0.017

(0.047,0.140)
(0.067,0.180)
(0.024,0.088)

corresponds with the study of Gao and Bai (2014). Therefore, consumers entering a ﬂow state in a social shopping site will likely
purchase from that website.
Furthermore, Fig. 3 shows the differences between young and
old users. For young social shopping consumers, the impact of
similarity perceived by consumers tends to be more inﬂuential than
perceived expertise on the formation of ﬂow state. A possible
explanation is that young consumers prefer to interact with
members who have similar tastes with them rather than experts.
Our results also indicate that perceived familiarity has no effect on
ﬂow experience for young consumers. A potential explanation is
that the motivation of young consumers who participate in social
commerce is more likely to obtain purchase advice rather than
build social bonds with members in social commerce. This group is

involved in social shopping sites to obtain more useful purchase
advice. For old social shopping consumers, perceived familiarity
and similarity of other members on a social shopping site are more
important drivers than perceived expertise in inducing ﬂow
experience. Considering perceived familiarity of group members,
old users tend to experience stronger ﬂow state than young users. A
possible explanation is that older consumers pay more attention to
building social bonds than younger consumers. When respondents
experience ﬂow state, old users are more likely to purchase from
social shopping sites than young users. A potential illustration is
that ﬂow experience is more important for old users than young
users when making purchase decisions.
6. Limitations and future directions
Our study has several limitations. First, this study was conducted with data collected from a social shopping site in China. The
results of this study might be different had the model been retested
in a different context or in a different cultural environment. In the
future, scholars should further test and validate our ﬁndings in
different contexts and cultural environments. Second, because of
the features of the focal social shopping sites in the study, the
participants of this study were mainly females. Therefore, future
research should study male-oriented social shopping sites and

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

provide more insights into the differences between male and female shoppers' shopping behavior. Third, our study was cross
sectional. As social shopping sites are dynamic in their development, future research can use a longitudinal design to identify the
roles and effects of interpersonal interaction factors perceived by
consumers in social shopping sites.
7. Research implications
7.1. Theoretical implications
Our study makes important contributions to the existing literature. First, this study extends the extant literature by testing and
validating a model that incorporates interpersonal drivers of ﬂow
experience in social commerce. Flow experience is proved to be an
important predictor of social commerce participation (Zhang et al.,
2014). Moreover, interpersonal interaction factors play a useful role
in the context of virtual community, which is similar to the social
commerce. Only a few studies have examined the ﬂow experience in
social commerce, especially from the perspective of interpersonal
attraction. Therefore, our study enriches the literature on the drivers
of ﬂow experience. The results indicate that interpersonal interaction enhances ﬂow experience. To the best of our knowledge, this
study is among the ﬁrst to empirically test the effects of three kinds
of interpersonal interaction factors (perceived expertise, similarity,

313

social shopping sites. First, our study helps managers to understand
the formation of ﬂow experience thoroughly in social commerce. As
interpersonal attraction factors signiﬁcantly affect customer ﬂow
experience, practitioners should focus on enhancing the interpersonal attraction of social shopping sites and pay more attention to
the factors of perceived expertise, similarity, and familiarity.
Second, our study provides a new insight into the different users
of social shopping sites. For users in social shopping sites, especially
for young users, the role of similarity seems to be more important
than the role of familiarity. Therefore, different types of groups
should be constructed in social shopping sites to help young
members ﬁnd people who have similar likes and tastes. As for the
role of familiarity, old users seem to focus on this factor. Therefore,
for those groups in which more old users participate, frequent
member interactions should be encouraged.
Acknowledgements
The work described in this paper was supported by the grants
from “the National Natural Science Foundation of China” (NSFC:
71571169, 71571177, 71201150, and 71332001); and supported by
“the Fundamental Research Funds for the Central Universities”
(WK2040000009).
Appendix A

Survey Questionnaire Items
Constructs

Items Measures

Perceived expertise PE1
PE2
PE3
PE4
Perceived similarity PS1
PS2
PS3
PS4
Perceived
PF1
familiarity
PF2
PF3
PF4
Flow experience
FL1
FL2
FL3
FL4
Purchase intention PUI1
PUI2
PUI3
PUI4

Sources

Some members on this website are very knowledgeable about fashion and beauty products.
Some members on this website are experts in fashion and beauty products.
Some members on this website are highly experienced in fashion and beauty products.
Compared with other sites, this website contains much information and knowledge about fashion and beauty
products.
As regards the styles in fashion and beauty products, I am similar to some members on this website.
As regards the tastes in fashion and beauty products, I am similar to some members on this website.
As regards my likes and dislikes about fashion and beauty products, I am similar to some members on this website.
As regards preferences in fashion and beauty products, I am similar to some members on this website.
Members of this website are as familiar to me as good friends are.
I have frequent interactions with other members of this website by writing or replying to articles.
The members on this website are familiar to me.
I keep close contact with this website members.
It is fun to interact on this website.
The interaction on this website is interesting.
When shopping on this website, I feel the excitement of exploring.
I am absorbed when shopping on this website.
Whenever I need to shop, I intend to purchase products on this website.
Whenever I need to shop, I plan to purchase products on this website.
I predict that I will purchase products on this website.
It is highly likely that I will purchase products on this website.

and familiarity) on the formation of ﬂow in social commerce.
Second, this study advances the understanding that interpersonal interaction factors are important in social commerce. Liao
et al. (2010) highlighted the importance of interpersonal interaction factors in affecting the loyalty of members in a virtual community. The present study extends the ﬁndings of Liao et al. (2010)
by demonstrating that interpersonal interaction factors inﬂuence
consumer purchase intention in social commerce.
Third, we divided our respondents into two groups based on age
and found signiﬁcant differences between old and young users. As
only a few previous studies on ﬂow experience compared the differences based on age, our study enriches the literature on ﬂow and
purchase intention.
7.2. Managerial implications
This research contributes to improving the management of

(Liao et al., 2010)

(Liao et al., 2010)

(Liao et al., 2010)

(Zhang et al., 2014)

(Pavlou & Fygenson,
2006)

References
Al-Natour, S., Benbasat, I., & Cenfetelli, R. T. (2005). The role of similarity in ecommerce interactions: the case of online shopping assistants. In SIGHCI 2005
proceedings, 4.
Animesh, A., Pinsonneault, A., Yang, S.-B., & Oh, W. (2011). An odyssey into virtual
worlds: exploring the impacts of technological and spatial environments on
intention to purchase virtual products. MIS Quarterly-Management Information
Systems, 35(3), 789e810.
Barnes, S. J. (2011). Understanding use continuance in virtual worlds: empirical test
of a research model. Information & Management, 48(8), 313e319.
Barutçu, S. (2007). Attitudes towards mobile marketing tools: a study of Turkish
consumers. Journal of Targeting, Measurement and Analysis for Marketing, 16(1),
26e38.
Carlson, J., & O'Cass, A. (2011). Creating commercially compelling website-service
encounters: an examination of the effect of website-service interface performance components on ﬂow experiences. Electronic Markets, 21(4), 237e253.
Chang, C.-C. (2013). Examining users' intention to continue using social network
games: a ﬂow experience perspective. Telematics and Informatics, 30(4),
311e321.
Chang, S.-H., Chih, W.-H., Liou, D.-K., & Hwang, L.-R. (2014). The inﬂuence of web
aesthetics on customers' PAD. Computers in Human Behavior, 36, 168e178.

314

H. Liu et al. / Computers in Human Behavior 58 (2016) 306e314

Cheung, C. M., & Lee, M. K. (2009). Understanding the sustainability of a virtual
community: model development and empirical test. Journal of Information
Science, 35(3), 279e298.
Constant, D., Sproull, L., & Kiesler, S. (1996). The kindness of strangers: the usefulness of
electronic weak ties for technical advice. Organization Science, 7(2), 119e135.
Ding, D. X., Hu, P. J.-H., Verma, R., & Wardell, D. G. (2010). The impact of service
system design and ﬂow experience on customer satisfaction in online ﬁnancial
services. Journal of Service Research, 13(1), 96e110.
Donovan, R. J., & Rossiter, J. R. (1982). Store atmosphere: an environmental psychology approach. Journal of Retailing, 58(1), 34e57.
Faiola, A., Newlon, C., Pfaff, M., & Smyslova, O. (2013). Correlating the effects of ﬂow and
telepresence in virtual worlds: enhancing our understanding of user behavior in
game-based learning. Computers in Human Behavior, 29(3), 1113e1121.
Fang, Y., Qureshi, I., Sun, H., McCole, P., Ramsey, E., & Lim, K. H. (2014). Trust, satisfaction,
and online repurchase intention: the moderating role of perceived effectiveness of
E-Commerce institutional mechanisms. MIS Quarterly, 38(2), 407e427.
Fiore, A. M., & Kim, J. (2007). An integrative framework capturing experiential and
utilitarian shopping experience. International Journal of Retail & Distribution
Management, 35(6), 421e442.
Floh, A., & Madlberger, M. (2013). The role of atmospheric cues in online impulsebuying behavior. Electronic Commerce Research and Applications, 12(6), 425e439.
Flynn, B. B., Sakakibara, S., Schroeder, R. G., Bates, K. A., & Flynn, E. J. (1990).
Empirical research methods in operations management. Journal of Operations
Management, 9(2), 250e284.
Fornell, C., & Larcker, D. F. (1981). Evaluating structural equation models with unobservable variables and measurement error. Journal of Marketing Research,
18(1), 39e50.
Gao, L., & Bai, X. (2014). Online consumer behaviour and its relationship to website
atmospheric induced ﬂow: Insights into online travel agencies in China. Journal
of Retailing and Consumer Services, 21(4), 653e665.
Gefen, D., & Straub, D. (2005). A practical guide to factorial validity using PLSGraph: tutorial and annotated example. Communications of the Association for
Information Systems, 16(1), 91e109.
Hair, J., Anderson, R. E., Tatham, R. L., & Black, W. (1998). Multivariate data analysis
New Jersey. USA: Englewood Cliffs.
Ha, Y., & Lennon, S. J. (2010). Online visual merchandising (VMD) cues and consumer pleasure and arousal: purchasing versus browsing situation. Psychology
& Marketing, 27(2), 141e165.
Heinonen, K., & Strandvik, T. (2007). Consumer responsiveness to mobile marketing. International Journal of Mobile Communications, 5(6), 603e617.
Hinds, P. J., Carley, K. M., Krackhardt, D., & Wholey, D. (2000). Choosing work group
members: balancing similarity, competence, and familiarity. Organizational
behavior and human decision processes, 81(2), 226e251.
Hoffman, D. L., & Novak, T. P. (1996). Marketing in hypermedia computer-mediated
environments: conceptual foundations. The Journal of Marketing, 60(3), 50e68.
Hsiao, K.-L., Lin, J. C.-C., Wang, X.-Y., Lu, H.-P., & Yu, H. (2010). Antecedents and
consequences of trust in online product recommendations: an empirical study
in social shopping. Online Information Review, 34(6), 935e953.
Hsieh, J.-K., Hsieh, Y.-C., Chiu, H.-C., & Yang, Y.-R. (2014). Customer response to web
site atmospherics: task-relevant cues, situational involvement and PAD. Journal
of Interactive Marketing, 28(3), 225e236.
Hsu, C.-L., Chang, K.-C., & Chen, M.-C. (2011). The impact of website quality on
customer satisfaction and purchase intention: perceived playfulness and
perceived ﬂow as mediators. Information Systems and e-Business Management,
10(4), 549e570.
Hsu, C. L., Chang, K. C., & Chen, M. C. (2012). Flow experience and internet shopping
behavior: Investigating the moderating effect of consumer characteristics.
Systems Research and Behavioral Science, 29(3), 317e332.
Huang, E. (2013). Interactivity and identiﬁcation inﬂuences on virtual shopping.
International Journal of Electronic Commerce Studies, 4(2), 305e312.
Huang, Z., & Benyoucef, M. (2014). User preferences of social features on social
commerce websites: an empirical study. Technological Forecasting and Social
Change, 95, 57e72.
Jeong, S. W., Fiore, A. M., Niehm, L. S., & Lorenz, F. O. (2009). The role of experiential
value in online shopping: the impacts of product presentation on consumer
responses towards an apparel web site. Internet Research, 19(1), 105e124.
Jiang, Z., Chan, J., Tan, B. C., & Chua, W. S. (2010). Effects of interactivity on website
involvement and purchase intention. Journal of the Association for Information
Systems, 11(1), 34e59.
Kelman, H. C. (1961). Processes of opinion change. Public Opinion Quarterly, 25(1),
57e78.
Kim, H., & Lennon, S. J. (2010). E-atmosphere, emotional, cognitive, and behavioral
responses. Journal of Fashion Marketing and Management, 14(3), 412e428.
Kim, J., & Lennon, S. J. (2013). Effects of reputation and website quality on online
consumers' emotion, perceived risk and purchase intention: based on the
stimulus-organism-response model. Journal of Research in Interactive Marketing,
7(1), 33e56.
Kim, Y. G., & Li, G. (2009). Customer satisfaction with and loyalty towards online
travel products: a transaction cost economics perspective. Tourism Economics,
15(4), 825e846.
Komiak, S. Y., & Benbasat, I. (2006). The effects of personalization and familiarity on
trust and adoption of recommendation agents. MIS Quarterly, 30(4), 941e960.
Koo, D.-M., & Ju, S.-H. (2010). The interactional effects of atmospherics and
perceptual curiosity on emotions and online shopping intention. Computers in
Human Behavior, 26(3), 377e388.

View publication stats

Koufaris, M. (2002). Applying the technology acceptance model and ﬂow theory to
online consumer behavior. Information Systems Research, 13(2), 205e223.
Lee, S. M., & Chen, L. (2010). The impact of ﬂow on online consumer behavior.
Journal of Computer Information Systems, 50(4), 1.
Lee, H. Y., Qu, H., & Kim, Y. S. (2007). A study of the impact of personal innovativeness on online travel shopping behaviordA case study of Korean travelers.
Tourism Management, 28(3), 886e897.
Liao, H.-C., Chu, C.-H., Huang, C.-Y., & Shen, Y.-C. (2010). Virtual community loyalty:
an interpersonal-interaction perspective. International Journal of Electronic
Commerce, 15(1), 49e74.
Lu, Y., Zhao, L., & Wang, B. (2010). From virtual community members to C2C ecommerce buyers: trust in virtual communities and its effect on consumers'
purchase intention. Electronic Commerce Research and Applications, 9(4),
346e360.
MacKenzie, S. B., & Lutz, R. J. (1989). An empirical examination of the structural
antecedents of attitude toward the ad in an advertising pretesting context. The
Journal of Marketing, 53, 48e65.
Manganari, E. E., Siomkos, G. J., Rigopoulou, I. D., & Vrechopoulos, A. P. (2011).
Virtual store layout effects on consumer behaviour: applying an environmental
psychology approach in the online travel industry. Internet Research, 21(3),
326e346.
Mason, C. H., & Perreault, W. D., Jr. (1991). Collinearity, power, and interpretation of
multiple regression analysis. Journal of Marketing Research, 28(3), 268e280.
Mollen, A., & Wilson, H. (2010). Engagement, telepresence and interactivity in online consumer experience: reconciling scholastic and managerial perspectives.
Journal of Business Research, 63(9), 919e925.
Nath, C. K. (2009). Behaviour of customers in retail store environment-an empirical
study. Vilakshan: The XIMB Journal of Management, 6(2).
Newsted, P. R., Huff, S. L., & Munro, M. C. (1998). Survey instruments in information
systems. MIS Quarterly, 22(4), 553e554.
Ng, C. S.-P. (2013). Intention to purchase on social commerce websites across cultures: a cross-regional study. Information & Management, 50(8), 609e620.
Nunally, J. C., & Bernstein, I. H. (1978). Psychometric theory. New York: McGraw-Hill.
Pagani, M., & Mirabello, A. (2011). The inﬂuence of personal and social-interactive
engagement in social TV web sites. International Journal of Electronic Commerce, 16(2), 41e68.
Parboteeah, D. V., Valacich, J. S., & Wells, J. D. (2009). The inﬂuence of website
characteristics on a Consumer's urge to buy impulsively. Information Systems
Research, 20(1), 60e78.
Pavlou, P. A., & Fygenson, M. (2006). Understanding and predicting electronic
commerce adoption: an extension of the theory of planned behavior. MIS
Quarterly, 30(1), 115e143.
Persaud, A., & Azhar, I. (2012). Innovative mobile marketing via smartphones.
Marketing Intelligence & Planning, 30(4), 418e443.
Petty, R. E., Cacioppo, J. T., & Goldman, R. (1981). Personal involvement as a
determinant of argument-based persuasion. Journal of Personality and Social
Psychology, 41(5), 847.
Podsakoff, P. M., MacKenzie, S. B., Lee, J.-Y., & Podsakoff, N. P. (2003). Common
method biases in behavioral research: a critical review of the literature and
recommended remedies. Journal of Applied Psychology, 88(5), 879.
Preacher, K. J., & Hayes, A. F. (2008). Asymptotic and resampling strategies for
assessing and comparing indirect effects in multiple mediator models. Behavior
Research Methods, 40(3), 879e891.
Shin, D.-H., & Shin, Y.-J. (2011). Why do people play social network games? Computers in Human Behavior, 27(2), 852e861.
Shrout, P. E., & Bolger, N. (2002). Mediation in experimental and nonexperimental
studies: new procedures and recommendations. Psychological Methods, 7(4), 422.
Tamjidyamcholo, A., Gholipour, R., Baba, M. S. B., & Yamchello, H. T. (2013). Information
security professional perceptions of knowledge-sharing intention in virtual communities under social cognitive theory. Research and innovation in information
systems (ICRIIS). In 2013 international conference on, IEEE2013 (pp. 416e421).
Teng, C.-I., Huang, L.-S., Jeng, S.-P., Chou, Y.-J., & Hu, H.-H. (2012). Who may be loyal?
Personality, ﬂow experience and customer e-loyalty. International Journal of
Electronic Customer Relationship Management, 6(1), 20e47.
Van de Vijver, F. J., & Leung, K. (1997). Methods and data analysis for cross-cultural
research.
Wang, L. C., Baker, J., Wagner, J. A., & Wakeﬁeld, K. (2007). Can a retail web site be
social? Journal of Marketing, 71(3), 143e157.
Wang, Y. J., Hernandez, M. D., & Minor, M. S. (2010). Web aesthetics effects on
perceived online service quality and satisfaction in an e-tail environment: the
moderating role of purchase task. Journal of Business Research, 63(9), 935e942.
Wang, C., & Zhang, P. (2012). The evolution of social commerce: the people, management, technology, and information dimensions. Communications of the Association for Information Systems, 31(5), 1e23.
Wu, W.-Y., Lee, C.-L., Fu, C.-S., & Wang, H.-C. (2013). How can online store layout
design and atmosphere inﬂuence consumer shopping intention on a website?
International Journal of Retail & Distribution Management, 42(1), 4e24.
Yoon, E. (2012). Effects of website environmental cues on consumers' response and
outcome behaviors. In Open access theses and dissertations from the college of
education and human science, (Paper 4).
Zhang, H., Lu, Y., Gupta, S., & Zhao, L. (2014). What motivates customers to participate in social commerce? the impact of technological environments and virtual
customer experiences. Information & Management, 51(8), 1017e1030.
Zhou, T. (2013). An empirical examination of continuance intention of mobile
payment services. Decision Support Systems, 54(2), 1085e1091.

See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/298797075

Medical	image	classification	using	spatial
adjacent	histogram	based	on	adaptive	local
binary	patterns
Article		in		Computers	in	Biology	and	Medicine	·	March	2016
DOI:	10.1016/j.compbiomed.2016.03.010

CITATION

READS

1

92

6	authors,	including:
Huiling	Chen
Wenzhou	University
67	PUBLICATIONS			765	CITATIONS			
SEE	PROFILE

All	content	following	this	page	was	uploaded	by	Huiling	Chen	on	22	April	2016.

The	user	has	requested	enhancement	of	the	downloaded	file.	All	in-text	references	underlined	in	blue	are	added	to	the	original	document
and	are	linked	to	publications	on	ResearchGate,	letting	you	access	and	read	them	immediately.

Computers in Biology and Medicine 72 (2016) 185–200

Contents lists available at ScienceDirect

Computers in Biology and Medicine
journal homepage: www.elsevier.com/locate/cbm

Medical image classiﬁcation using spatial adjacent histogram based on
adaptive local binary patterns
Dong Liu a,b, Shengsheng Wang a,b,n, Dezhi Huang a,b, Gang Deng c, Fantao Zeng a,b,
Huiling Chen d
a

Jilin University, College of Computer Science and Technology, Changchun 130012, China
Jilin University, Key Laboratory of Symbolic Computation and Knowledge Engineering of the Ministry of Education, Changchun 130012, China
c
Renmin Hospital of Wuhan University, Department of Neurosurgery, Wuhan 430060, China
d
College of Physics and Electronic Information Engineering, Wenzhou University, Wenzhou 325035, China
b

art ic l e i nf o

a b s t r a c t

Article history:
Received 2 September 2015
Received in revised form
16 March 2016
Accepted 16 March 2016

Medical image recognition is an important task in both computer vision and computational biology. In
the ﬁeld of medical image classiﬁcation, representing an image based on local binary patterns (LBP)
descriptor has become popular. However, most existing LBP-based methods encode the binary patterns
in a ﬁxed neighborhood radius and ignore the spatial relationships among local patterns. The ignoring of
the spatial relationships in the LBP will cause a poor performance in the process of capturing discriminative features for complex samples, such as medical images obtained by microscope. To address
this problem, in this paper we propose a novel method to improve local binary patterns by assigning an
adaptive neighborhood radius for each pixel. Based on these adaptive local binary patterns, we further
propose a spatial adjacent histogram strategy to encode the micro-structures for image representation.
An extensive set of evaluations are performed on four medical datasets which show that the proposed
method signiﬁcantly improves standard LBP and compares favorably with several other prevailing
approaches.
& 2016 Elsevier Ltd. All rights reserved.

keywords:
Local binary patterns
Image classiﬁcation
Feature extraction
Medical images
Microscope images

1. Introduction
Medical images have played an important role in the diagnostic
workup of patients. Automated classiﬁcation of medical images is
a desirable tool to assign the interpretation of images, and then
would help the expert in diagnosis of diseases [1–3]. Compared
with general image recognition, medical image recognition is
more challenging because of the higher ambiguity and complexity;
most of the medical image contents are quite similar, but also
different in their emphasis.
In terms of the features used for medical image recognition, it
can be mainly classiﬁed into three groups: shape, color, and texture features. For example, in [4], shape features such as moment
invariants and Fourier descriptor are employed to classify medical
X-ray images. A color vector ﬁeld is considered in [5] for improving
the performance of endoscopic image classiﬁcation.
The local binary patterns, ﬁrst proposed by [6], are widely
considered as a state-of-the-art image feature descriptor among
n
Corresponding author at: Jilin University, College of Computer Science and
Technology, Changchun 130012, China.
E-mail address: wss@jlu.edu.cn (S. Wang).

http://dx.doi.org/10.1016/j.compbiomed.2016.03.010
0010-4825/& 2016 Elsevier Ltd. All rights reserved.

texture descriptors, since it can more effectively describe texture
information. It has been successfully applied to many applications,
such as face recognition, texture classiﬁcation, scene recognition,
human detection and others. LBP has several attractive advantages: it has proven to be a powerful discriminator with low
computational cost, it is robust against changes in image intensity,
and it can be easily implemented. Due to these merits, it makes a
good choice for extracting ﬁne features for medical images.
However, the standard LBP still suffers from several drawbacks,
including limited semantic description of local patterns, sensitive
to non-uniform patterns and afﬁne transformation, and missing of
efﬁcient spatial encoding among patterns. To overcome these
shortcomings, numerous works [9–11] focused on improving LBP
in recent years, in terms of rotation-invariant, multi-scale, the
utilization of non-uniform patterns, and so on. There are two types
of LBP patterns: uniform and non-uniform patterns. Some works,
such as [7], only considered uniform patterns for extracting LBP
features since non-uniform patterns involve noise and high
dimensionality. And the work [8] proposed a hierarchical multiscale LBP to further utilize the information of non-uniform patterns. They also certify that, the percentage of non-uniform patterns increases as the neighborhood radius increases. To reduce
the LBP dimensionality, center-symmetric local binary patterns

186

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

(CSLBP) [15] is studied and applied to image recognition. Since LBP
is sensitive to noise in near uniform regions, Local Ternary Pattern
[14] with three value coding scheme was proposed to address this
problem. The rotation invariant [6] descriptor can be obtained
through the circular neighborhood deﬁnition, but in some cases
the anisotropic structural information is lost. To utilize these anisotropic structural information, a novel elliptical binary pattern
(EBP) [16] has been proposed for face recognition, in which elliptical neighborhood deﬁnitions are studied. Completed LBP [13]
utilizes both the sign and magnitude information in the difference
between the central pixel and the neighborhood pixels. In the
work [18], LBP is combined with Gabor ﬁlters to achieve a better
classiﬁcation performance. The study [19] extracted the most frequent patterns in LBP histogram and formed a novel descriptor,
which achieved better performance with this technique. Mesh-LBP
[21] is novel method which computed the mesh-local binary
pattern on a triangular-mesh manifold. In [22], a scale- and
rotation-invariant LBP is proposed, in which the rotation-invariant
is combined with a scale-adaptive texton for texture classiﬁcation.
SOALBP [23] constructed a novel scale- and orientation invariant
LBP feature combined in a multi-resolution representation, which
has been proven superior in texture classiﬁcation.
The basic idea behind LBP is that it describes an image by local
patterns. The existing methods have been proven to improve the
LBP to some extent by reconﬁguring or utilizing the patterns.
However, most of existing works encode the binary patterns in a
ﬁxed neighborhood radius. This ﬁxed neighborhood radius strategy is irrelevant to local image content and disregards microstructure information of the multi-scale patterns. Intuitively, the
micro-structures, i.e. the spatial relationships among local patterns
generated by adaptive radius, provide crucial feedback in disambiguating texture information especially for complex medical
images, i.e. microscope images that involve with pathological
changes. This subsequently leads to improved recognition performance. To this end, our target is to design a novel LBP histogram
representation for medical images to (1) compute the local binary
patterns in an adaptive neighborhood radius, and (2) encode
micro-structures among the multi-scale patterns. In the ﬁrst stage,
with the help of gradient operators, we obtain a gradient map
from each original image, and the adaptive LBP neighborhood
radius could be then determined for each pixel by utilizing the
gradient information. As a result, our adaptive strategy will assign
a relatively small radius to pixels that are located in local regions
with dramatic gray variation, while assigning a relatively large
radius to pixels that are located in local regions with slight gray
variation. This adaptive technique will provide the image with rich
micro-structure textures, which is discriminative in image representation. Then in the next stage, we propose a spatial adjacent
histogram based on adaptive LBP radius to describe these discriminative micro-structure features. Finally, the adaptive LBP

radius and spatial adjacent histogram strategies produce a much
more powerful LBP variant, which performs well in four benchmark medical datasets and compares favorably to other methods.
In this context, our contribution is threefold.
1) Using the adaptive strategy we proposed, the neighborhood
radius of LBP is determined based on local image content,
therefore more adaptive and useful features can be obtained.
2) We propose to use spatial adjacent histogram to encode the
micro-structures produced by adaptive strategy, which results
in convincing improvement on the standard LBP histogram.
3) Our approach also considers three LBP coding schemes, i.e. set
the threshold T in three different ways when computing the LBP
value. And we further evaluate the three LBP coding schemes in
order to ﬁnd which one performs more competitive in medical
image classiﬁcation.
The remainder of this paper is organized as follows. In Section
2, we introduce the proposed algorithm, i.e. spatial adjacent histogram based on adaptive local binary patterns for image classiﬁcation. Section 3 presents an extensive set of experimental evaluations on four medical image datasets, and ﬁnally, in Section 4,
we draw the conclusions.

2. Spatial adjacent histogram based on adaptive local binary
patterns for image classiﬁcation
In this section, we propose a novel idea using spatial adjacent
histogram based on adaptive local binary patterns for medical
image classiﬁcation. We ﬁrst present a concise review of standard
local binary patterns (LBP) in subsection 2.1. Next, we explain how
to determine the adaptive radius for each pixel in subsection 2.2,
then in subsection 2.3, three coding schemes are introduced to
compute the LBP value for each pixel. In subsection 2.4, we propose a spatial adjacent histogram technique based on adaptive LBP
to represent the whole image. Finally, the proposed image classiﬁcation framework using our spatial adjacent histogram based on
adaptive local binary patterns is presented in subsection 2.5.
2.1. Brief review of LBP
Given an image I, the LBP is a gray-scale texture operator that
characterizes the local spatial patterns of the image texture, which
is calculated at each pixel by evaluating the binary differences
between it and its neighbors:
(
PX
1
1; x Z 0
LBPðP; RÞ ¼
sðg i  TÞ2i ; sðxÞ ¼
ð1Þ
0; x o 0
i¼0

where P is the number of pixels in the neighborhood, R is the

Fig. 1. Three circularly symmetric neighborhood sets for different (P, R).

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

radius of the neighborhood and T is a threshold. The original LBP
set T as the gray value of the central pixel and gi is the gray value of
its neighborhood. Fig. 1 shows examples of different conﬁgurations of (P, R). A pattern is considered uniform if the number of
transitions in the sequence between 0 and 1 is less than or equal to
two. For instance, LBP pattern 00100000 is uniform (2 times
transition) and 00101000 is non-uniform (4 times transition).
2.2. Determining the adaptive radius R
In order to extract micro-structures of different scales, the key
idea behind this paper is to adaptively obtain the LBP radius of
each pixel by analyzing the differences based on calculated gradients. Given an image I and we use f(x,y) as the gray value of pixel
(x, y), then the Sobel [20] gradient magnitude g(x, y) can be
obtained by Eq. (2) and Eq. (3).
8
g ¼ ½ f ðx  1; y þ 1Þ þ2f ðx; y þ 1Þ þ f ðx þ 1; yþ 1Þ
>
>
> x
>
<  ½ f ðx  1; y  1Þ þ 2f ðx; y 1Þ þ f ðx þ 1; y  1Þ
ð2Þ
g y ¼ ½ f ðx þ1; y  1Þ þ 2f ðx þ 1; yÞ þ f ðx þ 1; y þ1Þ
>
>
>
>
:  ½ f ðx  1; y  1Þ þ 2f ðx  1; yÞ þ f ðx  1; y þ 1Þ
gðx; yÞ ¼

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
g x 2 þg y 2

ð3Þ

Then, given a center pixel gc and its neighborhood radius k, the
average gradients Gk ðx; yÞ in (2kþ 1)  (2kþ 1) image blocks is
deﬁned as follows:
xP
þk

Gk ðx; yÞ ¼

yP
þk

gði; jÞ

i ¼ xk j ¼ yk
2

ð2k þ 1Þ

; k A f1; 2; :::; Rmax g

ð4Þ

where Rmax is the maximal search radius.
Finally, the gray variation is deﬁned as the intensity difference
among the average gradients in different sizes of image blocks.
Thus, the neighborhood radius Rgc of the central pixel gc can be
obtained according to the maximum value:


Rgc ¼ arg max Gk þ 1  Gk ; k A f1; 2; :::; Rmax g
ð5Þ
k

An illustration of the proposed adaptive radius computing
process for a central point in a 7  7 blocks is provided in Fig. 2.
The input central point and its neighborhood of 7  7 blocks,
which are described as gradient magnitude, appears on the left
part of Fig. 2. Then the three scales of blocks and the average
gradients are deﬁned by Eq. (4). It is due to the fact that the most
dramatic change lies between G2 and G3, so the adaptive radius for
the central pixel is set as 2, which are presented in the right part of
Fig. 2. This means the outermost neighborhood pixels are not
considered based on our adaptive radius strategy, because they
adopt irrelevant gray value and may be harmful for the robustness.
Three toy examples of adaptive radius map are illustrated in
Fig. 3. In each example, the original image, adaptive radius map
and the percent of pixels with different radius are arranged from

187

left to right. In each adaptive radius map, four colors denote different LBP radii for each pixel, i.e. red denotes R ¼4, green denotes
R¼3, blue denotes R¼2 and black denotes R ¼1. The ﬁrst and
second original images in Fig. 3 are taken from the two different
categories (microtubules and endosome) of 2D HeLa dataset [28],
respectively, in which the pixels outside the objects are not considered since it represents the black background. It is observed
that Fig. 3(a) and Fig. 3(b) have much of different spatial distribution of adaptive radius, and the micro-structures produced by
neighbor adaptive radius are much discriminative for two images.
Another intuitive instance can be found in Fig. 3(c), in which the
original image is a scene image. Therefore, one could conclude that
the micro-structures of neighbor adaptive radius are much related
to image contents. Furthermore, we could discover that our
adaptive strategy assigned a relatively small radius to pixels that
are located in local regions with dramatic gray variation, while
assigning a relatively large radius to pixels that are located in local
regions with slight gray variation. In the rest of this paper, we will
show how to utilize these discriminative features coming from
micro-structures.
2.3. LBP coding schemes based on adaptive radius
In this subsection, we formulate the process of LBP coding
based on adaptive radius R. Given a center pixel gc, the neighborhood radius R can be computed by Eq. (5). Inspired by state-ofthe-art LBP coding strategy, three coding schemes are selected
based on setting different thresholds to T of Eq. (1) adaptively,
which are named T1, T2 and T3 for short, respectively. And LBPT1,
LBPT2 and LBPT3 denote standard LBPs obtained by the three
coding schemes, respectively.
We start with an original image I, let gc be a center pixel and gi
denotes its neighborhood pixel. P denotes the total number of the
neighbors, and R denotes the adaptive radius of gc obtained during
the process in subsection 2.2.
(1) The ﬁrst coding scheme T1
Set threshold T as the gray value of the central pixel, i.e. T1 ¼gc.
thus, the LBPT1 can be formulated as
LBPT 1 ¼

PX
1

sðg i  T 1 Þ2i

ð6Þ

i¼0

(2) The second coding scheme T2
Let mi ¼| gi  gc |, set T2 as the mean value of mi, i.e.
T2 ¼

PX
1
i¼0

mi =P ¼

PX
1

j g i  g c j =P

i¼0

Fig. 2. An example of proposed adaptive radius computing process for determining the radius.

ð7Þ

188

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

Fig. 3. Toy examples of adaptive radius map, i.e. different radius of each pixel: red denotes R ¼4, green denotes R ¼3, blue denotes R ¼2 and black denotes R¼ 1. (For
interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this article.)

Then we can deﬁne the third LBPT3 as follows

Then, the LBPT2 are presented as follows:
LBPT 2 ¼

PX
1
i¼0

sðmi  T 2 Þ2i ¼

PX
1



sðg i g c   T 2 Þ2i

ð8Þ

i¼0

Set T3 as the mean gray value of the (2Rþ1)  (2Rþ1) image
blocks, i.e.
N
X
i¼1

g i =N;

N ¼ ð2R þ 1Þ  ð2R þ 1Þ

PX
1

sðg i  T 3 Þ2i

ð10Þ

i¼0

(3) The third coding scheme T3

T3 ¼

LBPT 3 ¼

ð9Þ

Fig. 4 illustrates the coding process using different thresholds
for the 3  3 sample blocks. Note that the ﬁrst and second coding
schemes are equivalent to the standard LBP [6] and LDBP [12],
respectively. For visualization, LBP transformed the images by
replacing each pixel of the image with its LBP value, these are also
presented. Fig. 5 shows examples of LBP transformed images with
three coding schemes. From the examples, we can see that all
schemes retain the global structure of the image, but display much
diversity in capturing the local structures. In the next experiments,
we will evaluate performances of the three coding schemes based

Fig. 4. Coding process of the three thresholds. (Here R ¼1, P¼ 8).

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

189

Fig. 5. Example image and its LBP transformed images with three coding schemes.

on our proposed algorithm to explore which one is more competitive in medical image classiﬁcation.
2.4. Spatial adjacent histograms based on adaptive LBP
From the Fig. 3, one can see that adaptive radius map provides
the image with rich micro-structure textures. These microstructures reﬂect discriminative features and can be well represented by the approaches of histogram analysis. In this subsection,
we explain how to construct the spatial adjacent histogram for an
image when considering both micro-structures and adaptive
radius patterns.
As mentioned in subsection 2.2, each pixel of an image has
different neighborhood radius during our adaptive strategy, however, different conﬁgurations of (P, R) will result in LBPTi of each
pixel mapping to different dimension. In this paper, we set the
number of involved adaptive neighbors as 8, i.e. P ¼8, in a similar
fashion to [22]. Besides, our spatial adjacent histogram encoding
strategy will result in the ﬁnal feature vector with 2  Rmax  2p
dimensions. This means that increasing the values for P and Rmax
will make the feature dimension signiﬁcantly large, that will further increase the time complexity. Considering the trade-off
between description and performance, we set the maximal
search radius Rmax as 4. Moreover, increasing Rmax means that
more image pixels are involved to interpolate and form neighborhoods with ﬁxed number P¼ 8. This may bring more noise to
the interpolated neighborhoods. So we think it is appropriate to
set Rmax as 4.

Then, four histograms with 256 bins under four radii can be
extracted for each image:
X
hðiÞj Rk ¼
isequalðlbpðjÞ ¼ iÞ; i ¼ 1; :::; D; k ¼ 1; :::; 4
ð11Þ
j A Rk

where Rk indicates the set of pixels with radius R ¼k, and D
denotes the dimension of LBP histogram (here D is equal to 256).
The function lbp(j) returns the LBP value of pixels j, and the
function isequal(lbp(j)¼i) returns 1 if lbp(j)¼ i, else returns 0.
Then for image representation, an easy option is that we
directly concatenate (for short, we name this histogram construction strategy as DC strategy) the four histograms under the
different radii, then the concatenated histogram DCLBP is as follows.
DCLBP T i ¼ H T i ðR1 Þð þ ÞH T i ðR2 Þð þ ÞH T i ðR3 Þð þ ÞH T i ðR4 Þ; i A f1; 2; 3g

ð12Þ

where (þ )indicates the operation of concatenation, superscript Ti
denotes the three coding schemes we have selected. A brief
example is shown in Fig. 6(a).
Based on the preliminary experimental results, we found that
the pixels which have been assigned R ¼1 are sparse but discriminative. We propose to use R¼ 1 to calculate the spatial relation between them and pixels with larger radii for two reasons:
(1) As shown in Fig. 3, the pixels with R¼ 1 are mainly located in
the regions with dramatic changes in gray value. The pixels
with dramatic changes are also involved with ample microstructures of different scales, i.e. spatial correlation of neighbor pixels with different LBP radii. We do not choose pixels

Fig. 6. The two strategies of combining sub-histograms of pixels with different radii to represent the image. (a) the directly concatenate strategy; (b) the spatial adjacent
histograms strategy.

190

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

Table 1
The percentage of non-uniform patterns in medical datasets.

2D-Hela [28]
Hep-2 [29]
PAP [30]

R ¼ 1, P¼ 8 (%)

R¼ 2, P¼ 8 (%)

R¼ 3, P ¼8 (%)

R ¼4, P ¼8 (%)

27.84
10.96
13.82

38.24
18.78
18.58

44.21
21.86
22.06

45.13
26.89
24.82

with larger radii for constructing the spatial adjacent histogram, such as R¼ 4, since they are mainly located in regions
with relative smooth change in gray value. Intuitively, the
spatial correlation between pixels with dramatic changes and
its neighbor pixels provide crucial feedback in disambiguating
texture features.
(2) Using R ¼1 is helpful to incorporate uniform idea. It has been
validated that uniform patterns show superiority in texture
classiﬁcation and some non-uniform patterns involve noise
[7]. Uniform patterns are also considered as the major parts of
all patterns. Moreover, [8] certify that, the percentage of nonuniform patterns increases as the neighborhood radius
increases. Table 1 shows the percentage of non-uniform
patterns in three medical datasets and presents the same
conclusion with [8]. An appropriate way of incorporating
uniform idea and avoiding noise caused by some nonuniform patterns, in our case, is to choose pixels with R¼1
(that involve the most uniform patterns) as baseline to
construct the spatial adjacent histogram. Note that our goal

is not discarding the non-uniform local patterns, since some
non-uniform patterns also can provide much useful features.
And how to dig out the useful non-uniforms among all nonuniforms is another topic.
Therefore, instead of using original histograms of pixels with
R¼1, we propose to use spatial adjacent histogram strategy to
embed the micro-structures information to hðiÞj R1 . For each pixel
jA R1 of an image, we calculate not only the number of occurrences
of each LBP (j) in the image, but also the number hðiÞj ðR1 ; Rk Þ, that
is the number of pixels with radius Rk which appears next to pixel
j.
X
X
isequalðLBPðjÞ ¼ iÞ; k ¼ 1; 2; 3; 4
ð13Þ
hðiÞj ðR1 ; Rk Þ ¼
j A R1 ;w A Rk w A NðjÞ

Where N(j) is the four-neighbor system of j. Then the spatial
adjacent histogram (SAH) for pixels set with radius R¼ 1 can be
deﬁned as:
SAHðR1 Þ ¼ fhðiÞj R1 ; hðiÞj ðR1 ; R1 Þ; hðiÞj ðR1 ; R2 Þ; hðiÞj ðR1 ; R3 Þ;
hðiÞj ðR1 ; R4 Þg; i ¼ 1; :::; 256

ð14Þ

It is easy to ﬁnd that the dimension of h(i)|(R1,Rk) is 256,
resulting in that the dimension of SAH(R1) is 256 þ256*4 ¼1280.
Thus, the dimension of the histogram in Formula (14) is quintuple
larger than that in Formula (11).
A toy example of explaining how to count SAH(R1) is present in
Fig. 7, we design three image patches with 5  5 size, i.e. 25 pixels.
To simplify the visualization, note that, we assume that the pixels

Fig. 7. Histograms for representing the pixels set with radius R¼ 1.

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

with radius R¼1 only be assigned to four types of LBP values, i.e. A
to D (0 oA oB oC oD o255). Furthermore, each pixel has an
adaptive radius which is marked by different colors and textures,
i.e. yellow pixel with no texture denotes R ¼1, green pixel with
vertical texture denotes R¼ 2, blue pixel with crossed texture
denotes R¼3, and red pixel with horizontal texture denotes R¼4.
Then the corresponding histogram and feature vectors are shown
in Fig. 7. The four black bins with gradual change in each histogram denote the number of occurrences of A, B, C, and D, respectively. Note that the numbers of occurrences of A, B, C, and D are
the same in the three image patches, i.e. for all image patches: h
(A)|R1 ¼3, h(B)|R1 ¼2, h(C)|R1 ¼2 and h(D)|R1 ¼2. Following each
black bin with gradual change, there are four bins with different
textures which denote the number of four-neighbor pixels with
four different radii, i.e. h(A)|(R1,R1), h(A)|(R1,R2), h(A)|(R1,R3), and h
(A)|(R1,R4), respectively.
More speciﬁcally, looking at the image patches in Fig. 7(a), the
ﬁrst black bin with gradual change denotes the number of occurrences of A. Since there exist three yellow pixels with no texture
located in the four-neighbors of all type A, then h(A)|(R1,R1) ¼3,
drawn as the ﬁrst yellow bin with no texture. Note that if two fourneighbor pixels are marked as the same type, then we count it
only one time to update the histogram. For example, two pixels
marked with A in Fig. 7(a) are mutual four-neighbors, but we
count it only one time to update h(A)|(R1,R1). There is only one

191

green pixel with vertical texture located in the four-neighbors of
all type A, so h(A)|(R1,R2)¼1, drawn as the ﬁrst green bin with
vertical texture. There is also only one blue pixel with crossed
texture located in the four-neighbors of all type A, so h(A)|(R1,R3) ¼
1, drawn as the ﬁrst blue bin with crossed texture. And there are
four red pixels with horizontal texture located in the fourneighbors of all type A, so h(A)|(R1,R4) ¼4. It is similar for counting the other non-black bins, i.e. h(B)|(R1,R1), h(B)|(R1,R2), h(B)|(R1,
R3), h(B)|(R1,R4), h(C)|(R1,R1), h(C)|(R1,R2), h(C)|(R1,R3), h(C)|(R1,R4), h
(D)|(R1,R1), h(D)|(R1,R2), h(D)|(R1,R3) and h(D)|(R1,R4).
Comparing the three image patches in Fig. 7, using the original
LBP histogram model, the pixels set with radius R¼1 in three
image patches are represented as the same feature histogram
(3,2,2,2) as shown by the black bins in each histogram. But using
the histograms obtained by our spatial adjacent histogram (SAH)
strategy, these image patches can be effectively distinguished from
each other. This indicates that the micro-structure features have
been effectively utilized in SAH strategy, that had been ignored in
DC strategy.
As explained above, for an image, we can obtain four histograms, i.e. one histogram with 1280 dimensions to represent the
pixels set with radius R¼ 1 (see Eq. (14) ), and three histograms
with 256 dimensions to represent the pixels set with radii R¼ 2, 3,
4, respectively (see Eq. (11)). Then, the ﬁnal LBP feature vectors

Fig. 8. Example images and its spatial adjacent histograms (the third coding scheme T3 is employed here).

192

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

with our spatial adjacent histograms strategy can be computed as:
Ti

Ti

Ti

Ti

Ti

SAHLBP ¼ SAH ðR1 Þð þ ÞH ðR2 Þð þ ÞH ðR3 Þð þÞH ðR4 Þ; iA f1; 2; 3g
ð15Þ
where ( þ) indicates the operation of concatenation, superscript Ti
denotes the three coding schemes above. And the dimension of
SAHLBP is 1280 þ (256*3) ¼ 2048. Fig. 6 shows the comparison of
the directly concatenate strategy and the spatial adjacent histograms strategy to represent the images.
In order to present an intuitive example, Fig. 8 shows four
images and its SAHLBP histogram features. The ﬁrst two images
come from the ‘golgpp’ category of 2D HeLa dataset, while the last
two images come from the ‘nucleolus’ category of 2D HeLa dataset.
Though it may be a challenge to distinguish the two categories
with human vision, but the SAHLBP histogram features can provide
much discriminative information to distinguish the two different
types of patterns. Therefore, our SAHLBP method can be expected
to perform well in medical image classiﬁcation, we will give a
complete evaluation in next experiments.
2.5. The proposed image classiﬁcation framework
As illustrated in Fig. 9, the framework for medical image classiﬁcation consists of four steps, i.e. determining the adaptive LBP
radius for each pixel, computing the spatial adjacent histogram
based on adaptive LBP radius, representing the image based on the
spatial adjacent histograms, and training a SVM classiﬁer for
recognizing new samples. Furthermore, the ﬁrst three steps can be
considered as feature extraction for medical image, which is the
key idea of our proposal.

3. Experimental results
3.1. Experimental datasets
The medical datasets for the evaluation of the proposed
method are selected based on their variety and broadness of their

use. Experiments are performed on the following four reference
datasets.
3.1.1. The 2D HeLa dataset
This dataset [28] is composed of 862 single-cell images (16 bit
gray scale of size 512 by 382 pixels) from ten classes, i.e. Actin,
Nucleus, Endsome, ER, Golgi Giantin, Golgi GPP130, Lysosome,
Microtubules, Mitochondria, and Nucleolus. The description of the
dataset in terms of samples per class is presented in Table 2, and
some sample images from the ten categories are shown in Fig. 10.
In our experiments, the LBP based histogram bin with the highest
occurrence is discarded since it represents the black background.
3.1.2. Hep-2 cell dataset
Hep-2 cell dataset [29] consists of 28 slide images, and each
image contains several cells that are segmented by specialist. The
total number of cells is 1455, including training set with 721 cells
and testing set with 734 cells. And each cell has been assigned to
one of the six categories: centromere, homogeneous, nucleolar,
coarse speckled, ﬁne speckled, and cytoplasmatic. A summary of
the Hep-2 cell dataset in terms of number of training set, testing
set and classes is reported in Table 3. Some sample images are also
presented in Fig. 11.
3.1.3. PAP dataset
The PAP smear dataset [30] contains 917 samples collected at
the Herlev University Hospital using a digital camera and microscope. As shown in Table 4, the samples belong to seven different
classes. And two skilled cyto-technicians further classiﬁed each
sample into two super classes (normal and abnormal). The sample
images that came from the two super classes are shown in Fig. 12.
3.1.4. The brain tumor dataset
The brain tumor dataset used throughout this experiment
consists of 285 images of size 256*256, which was collected at the
department of neurosurgery in Renmin Hospital of Wuhan University, P.R China. All samples are derived from real pathological
images and 19 patients who suffer different types of brain tumor

Fig. 9. Overview of the proposed image classiﬁcation framework.

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

Table 2
A summary of the 2D HeLa dataset: classes
and number of samples per class.
Class
Actin
Nucleus
Endsome
ER
Golgi Giantin
Golgi GPP130
Lysosome
Microtubules
Mitochondria
Nucleolus
Total

193

Table 3
A summary of the Hep-2 cell dataset: classes and number of samples per class.
Class

Training set

Testing set

Number of each class

Centromere
Homogeneous
Nucleolar
Coarse speckled
Fine speckled
Cytoplasmatic

208
150
102
109
94
58

149
180
139
101
114
51

357
330
241
210
208
109

Total

721

734

1455

Number
98
87
91
86
87
85
84
91
73
80
862

are involved. In pathological analysis, the high resolution microscopy (Olympus BX51) was used to perform on the tumor lesion in
order to get a histopathological diagnosis. Then ﬁve categories of
microscopic images could be obtained according to the ﬁve types
of brain tumor, i.e. astrocytoma, tuberculum sellae meningioma,
olfactory groove meningioma, acoustic neuroma and pituitary
tumor. Table 5 reports the number in each class and Fig. 13 depicts
the microscopic images from each category.
Note that the original images in Hep-2 cell, PAP and brain
tumor datasets are color images. In this study, the LBP variants and
SIFT descriptors are extracted from the grayscale version of the
corresponding images. The grayscale values are transformed by a
weighted sum of R (red), G (green), B (blue) channels, i.e. Gray ¼
0.2990 * R þ0.5870 * G þ0.1140 * B.
3.2. Experimental setup and implementation
We implement the experiments in an iterated random
splitting-scheme (consistent among all methods) for training and
testing. Each dataset is randomly divided into 80% for training and
20% for testing. We use the training-split to optimize the SVM
parameters and train model in a 5-fold cross validation. And then
the trained model is employed for the test-split. The random
splitting is repeated 30 times. For performance quantiﬁcation, the

average mean classiﬁcation accuracy over 30 times iterations with
its standard deviation is reported. In order to use this evaluation
protocol consistently in all datasets, we merge the training and
testing set for Hep-2 cell dataset, as shown in Table 3.
Since different classes may have different numbers of samples,
we use stratiﬁed cross validation instead of standard cross validation to handle this imbalance of image numbers among classes.
In stratiﬁed cross validation, the distribution of samples among
classes in each subset is basically consistent with that in original
dataset.
For the classiﬁer, we employ the SVM with linear kernel in the
next experiment. For the implementation of the SVM classiﬁer, the
public LIBSVM library [31] is employed.
3.3. Evaluation of our methods
In this section, we report our experimental results using the
methods and dataset with performance indicators described in
previous sections. Evaluation of the proposed model is accomplished through two different tasks: (1) evaluate the performance
of three different coding schemes for medical image datasets;
(2) verify that the spatial adjacent histograms is effective to
increase the classiﬁcation accuracy based on our adaptive radius
strategy.
For the ﬁrst task, we report on the classiﬁcation accuracy of
standard LBP with three scales (R ¼1, P ¼8), (R ¼2, P ¼8), and
(R¼ 2, P ¼16) and our method that adopted three coding schemes
respectively, i.e. LBPT1(R ¼ 1, P ¼ 8), LBPT2(R ¼ 1, P ¼ 8), LBPT3(R ¼ 1, P ¼ 8),

Fig. 10. Ten type of 2D-Hela images.

194

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

Fig. 11. Six type of Hep-2 cell image: homogeneous, centromere, nucleolar, ﬁne speckled, coarse speckled and cytoplasmatic.
Table 4
A summary of the PAP smear dataset: classes and number of samples per class.
Class

Pap dataset Super classes Total

Superﬁcial squamous epithelial
Intermediate squamous epithelial
Columnar epithelial
Mild squamous non-keratinizing dysplasia
Moderate squamous non-keratinizing
dysplasia
Severe squamous non-keratinizing
dysplasia
Squamous cell carcinoma in situ
intermediate

74
70
98
182
146

Normal
Normal
Normal
Abnormal
Abnormal

197

Abnormal

150

Abnormal

(2)

242

675

LBPT1(R ¼ 2, P ¼ 8), LBPT2(R ¼ 2, P ¼ 8), LBPT3(R ¼ 2, P ¼ 8), LBPT1(R ¼ 2, P ¼ 16),
LBPT2(R ¼ 2, P ¼ 16), LBPT3(R ¼ 2, P ¼ 16), SAHLBPT1, SAHLBPT2 and
SAHLBPT3, as shown in Table 6. In each table, the best results in
each dataset are bolded.
For the second task, we compare the methods generated by
spatial adjacent histograms (SAH) strategy and directly concatenate (DC) strategy under the three coding schemes, respectively, i.e. DCLBPT1 and SAHLBPT1, DCLBPT2 and SAHLBPT2, DCLBPT3
and SAHLBPT3. The results are also reported in Table 6.
Examining the Table 6, we can make the following conclusions:
(1) Our SAHLBP models consistently outperform standard LBP
with (R¼ 1, P ¼8) and (R¼ 2, P¼ 8) on the four medical datasets, no matter which coding schemes that are employed. Our
SAHLBP models also outperform standard LBP with R ¼2, P ¼16
in all cases except that perform on PAP dataset with T2, i.e.
only SAHLBPT2 is inferior to LBPT2(R ¼ 2, P ¼ 16) on PAP dataset.
Moreover, the dimension of our method (i.e. 2048 dimensions) is much smaller than that of LBPT(R ¼ 2, P ¼ 16) (i.e. 65536
dimensions). This is probably because standard LBPs cannot
capture spatial relationships among local textures, while our
SAHLBP models describe micro-structures obtained by

(3)

(4)

(5)

adaptive radius and perform successfully well in capturing
the valuable structural information, which is beneﬁcial to
medical image classiﬁcation.
The models with the third coding scheme T3 achieved higher
accuracy, on the average, than that with the ﬁrst and second
coding schemes. More speciﬁcally, for the standard LBP with
R¼1and P ¼8, the ﬁrst coding scheme T1 gained the best
performance in the 2D-Hela and Tumor datasets, while T3
gained the best performance in the PAP smear and Hep-2 cell
datasets. For the standard LBP with R¼2 and P ¼16, T1 gained
the best performance in the 2D-Hela dataset and T3 proves
superior in the remaining datasets. For SAHLBPs, the T3
performed better than the other coding schemes in the PAP
smear, 2D-Hela and Tumor datasets, while the T1 achieved the
best performance in the Hep-2 cell datasets.
Particularly interesting are the results of the methods with the
second coding scheme T2, performs slightly worse than the
other two schemes in most of the cases. However, the coding
scheme T2 has been proven in [12] that it describes the global
structure successfully and has a good performance in natural
scene image classiﬁcation. From this observation, we can
appreciate that the micro-structure features also remain very
relevant to coding scheme.
LBPTi with (R ¼2, P ¼16) performs better than DCLBPTi. We
conjecture that it is because the LBPTi with (R ¼2, P ¼16)
captures more texture information with very large feature
dimension (i.e. 65536 dimensions), while the feature dimension of DCLBPT is 1024. However, the DCLBPTi achieves higher
accuracies than LBPTi with both (R¼1, P ¼8) and (R¼ 2, P ¼8)
in most cases. The comparison between the DCLBPTi and LBPTi
suggests that the adaptive radius is not very suitable to combine with directly concatenate strategy. We mainly compare
the DCLBPTi with SAHLBPTi to show the superiority of adaptive
radius combined with spatial adjacent histogram.
Our spatial adjacent histogram (SAH) based on adaptive radius
is better than directly concatenate strategy for all datasets, no

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

195

Fig. 12. Sample images from the pap smear dataset.
Table 5
A summary of the brain tumor dataset: classes and number of
samples per class.
Class
Astrocytoma
Tuberculum sellae meningioma
Olfactory groove meningioma
Acoustic neuroma
Pituitary tumor
Total

Number
58
54
54
56
63
285

matter which coding schemes T adopted, i.e. SAHLBPTi is
always better than DCLBPTi. Therefore, one can conclude that
SAH is highly suitable for representing the micro-structures
which is discriminative in medical image classiﬁcation.
To provide more robust results, we further perform the Wilcoxon ranksum test in combination with the iterated splitting
scheme to compare the methods. More speciﬁcally, for each
method in Table 6, we can obtain 30 classiﬁcation results during
30 times splitting iterations for each dataset. Then for a pair of
compared methods, we perform Wilcoxon ranksum test with their
30 classiﬁcation results over one speciﬁc dataset.

Fig. 13. Microscope images for different types of brain tumor lesion. (a) astrocytoma; (b) tuberculum sellae meningioma; (c) olfactory groove meningioma; (d) acoustic
neuroma; (e) pituitary tumor.

196

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

Table 6
Classiﬁcation accuracy of each method over four datasets.
Methods

LBPT1(R ¼ 1, P ¼ 8)
LBPT2(R ¼ 1, P ¼ 8)
LBPT3(R ¼ 1, P ¼ 8)
LBPT1(R ¼ 2, P ¼ 8)
LBPT2(R ¼ 2, P ¼ 8)
LBPT3(R ¼ 2, P ¼ 8)
LBPT1(R ¼ 2, P ¼ 16)
LBPT2(R ¼ 2, P ¼ 16)
LBPT3(R ¼ 2, P ¼ 16)
DCLBPT1
DCLBPT2
DCLBPT3
SAHLBPT1
SAHLBPT2
SAHLBPT3

Table 7
Classiﬁcation accuracy of compared methods over four datasets.

Datasets
PAP

2D-Hela

Hep-2

Tumor

81.127 2.3
80.92 7 1.7
81.877 1.5
80.34 7 2.5
79.767 2.5
82.78 7 2.3
85.477 2.2
84.56 7 2.0
85.56 7 1.9
83.96 7 1.9
82.147 1.9
83.29 7 1.9
86.69 7 2.1
84.03 7 2.1
88.03 7 1.7

85.247 2.4
83.23 7 1.9
84.317 2.1
86.077 2.5
82.59 7 1.8
85.39 7 2.2
87.327 2.2
86.23 7 1.9
86.767 2.1
84.96 7 1.4
85.34 7 1.4
87.847 2.2
89.687 2.0
87.29 7 1.6
90.06 7 1.5

75.65 7 1.1
72.177 1.2
79.947 1.6
82.147 1.4
76.23 7 1.5
80.23 7 1.1
87.677 1.4
85.23 7 1.0
87.927 1.2
81.317 0.8
76.39 7 1.0
82.227 1.1
91.897 0.8
88.95 7 0.8
91.86 7 0.9

75.417 1.8
74.45 7 1.7
75.22 7 1.8
75.047 2.1
73.62 7 2.1
73.78 7 2.1
79.177 1.6
78.82 7 1.9
79.25 7 2.0
77.69 7 1.9
75.22 7 1.8
78.137 1.5
82.917 1.6
81.247 2.1
84.487 1.8

More speciﬁcally, for 2D-HeLa dataset, we have compared the
following approaches using the Wilcoxon ranksum test:
1. SAHLBPTi versus LBPTi (i¼1, 2, 3) with different scales, SAHLBPTi
wins against LBPTi with all scales (R ¼1, P ¼8), (R¼ 2, P ¼8), and
(R¼2, P ¼16), i.e. we reject the null hypothesis at the level of
signiﬁcance 0.05, and accept that SAHLBPTi and LBPTi have signiﬁcant different performance.
2. SAHLBPTi versus DCLBPTi (i¼1, 2, 3), SAHLBPTi wins against
DCLBPTi.
3. SAHLBPT3 versus SAHLBPTi (i¼ 1, 2), SAHLBPT3 wins against
SAHLBPT2, however, SAHLBPT3 and SAHLBPT1 have no signiﬁcant
different performance in 2D-HeLa dataset.
We also do the same Wilcoxon ranksum test for the other three
datasets, and most of the conclusions remain valid. Very few different conclusions are marked as the following: (1) For PAP and
brain tumor datasets, SAHLBPT3 wins against both SAHLBPT1 and
SAHLBPT2. (2) For PAP dataset, SAHLBPT2 and LBPT2(with R ¼2,
P ¼16) have no signiﬁcant different performance. The p-values for
all tests are presented in Appendix A.
Finally, we analyze the confusion matrices for SAHLBPT3 which
display the best results selected from the 30 times iterations for
each dataset. The confusion matrices for the three multiclass
datasets, i.e. 2D-HeLa, Hep-2 cell, and brain tumor datasets, are
presented in Appendix A. From the results, we can conﬁrm that
our method is very useful for some particular patterns, such as
actin, dna, golgia, golgpp, nucleolus patterns in 2D-Hela dataset
and nucleolar pattern in Hep-2 cell dataset.
From the analysis of the above experimental results, it is clear
the advantage of our spatial adjacent histogram strategy employed
the third coding scheme T3, i.e. the SAHLBPT3. Hence, to achieve a
tradeoff between description and performance, we have chosen
the SAHLBPT3 in the remainder of the experiments.
3.4. Comparison of the LBP-based methods
The tests in this subsection are aimed at comparing our method
with the recent LBP variants reported in the literature. In particular, the following methods are evaluated which are often used
for medical image classiﬁcation:
1) LBP-r [6], the standard LBP with rotation invariant;
2) EBP [24], the elliptical binary pattern variant;
3) LBPsu2[22], scale-adaptive and subuniform-based rotation
invariant LBP;

Methods

PAP

2D-Hela

Hep-2

Tumor

LBP-r
ELBP
LBPsu2
LBPnr
LBP-HF
LDBP
CLBP
CoALBP

75.5 7 1.2
80.02 7 1.5
82.42 7 1.3
83.147 1.8
84.137 1.5
81.26 7 1.7
87.217 1.4
87.02 7 1.9

81.21 71.9
85.11 71.7
84.82 71.5
85.76 72.1
85.14 72.0
82.5 71.9
88.91 71.1
87.72 72.1

75.23 7 0.9
80.177 1.1
83.23 7 0.6
82.767 1.3
87.23 7 1.0
79.58 7 0.7
87.917 0.9
92.767 1.1

73.717 1.4
75.82 7 1.5
76.727 0.6
78.29 7 2.0
77.75 7 1.7
75.117 2.0
81.127 1.8
81.337 1.9

SAHLBPT3

88.037 1.7

90.06 71.5

91.86 7 0.9

84.487 1.8

4) LBPnr[22], scale-adaptive LBP without rotation invariant, that
employs the LBPsu2 approach by using non-rotation invariant
instead of rotation invariant.
5) LBP-HF[25], local binary pattern histogram Fourier features;
6) LDBP[12], the local difference magnitude binary pattern variant;
7) CLBP[26], in which three types of features are combined to form
the completed LBP features;
8) CoALBP [27], in which the co-occurrence among adjacent LBPs
has been considered. The effectiveness of CoALBP for medical
images can be conﬁrmed since it won the ﬁrst prize in the 2012
HEP-2 cells classiﬁcation contest.
In Table 7, we report the classiﬁcation accuracies using the
datasets and the compared methods described above. In each
table, the best result in each datasets is bolded.
From Table 7, we can make the following ﬁndings.
First, SAHLBPT3 is one of the best variants proposed in this
work. It achieves, on average, a better result than the compared
methods. Also, SAHLBPT3 achieves the highest accuracy in PAP, 2DHela, and Brain tumor datasets. We conjecture that it is because
our method enhanced its performance by exploiting the spatial
information of the adaptive radius with different scales, which
produced more discriminative features for medical images.
Second, CoALBP works better than SAHLBPT3 in Hep-2 cell
dataset, and SAHLBPT3 outperforms the CoALBP for the other three
datasets. However, our SAHLBPT3 has one advantage over the
CoALBP: its feature dimension is much smaller. The experimental
feature sizes of SAHLBPT3 and CoALBP are 2048 and 3072,
respectively.
Finally, SAHLBPT3 works much better than the methods with
rotation invariant, such as LBP-r and LBPsu2, even though the
rotation invariant features are not involved in our methods.
However, the rotation invariant contains additional discriminative
information. How to utilize this rotation invariant feature based on
our adaptive strategy will be explored in our future research.
From Table 7, it can be observed that CLBP, CoALBP, and
SAHLBPT3 achieve similar classiﬁcation accuracies. To provide
another basis for comparison of the three methods, we have also
reported the macro-averaged F1-scores as shown in Table 8. Note
that the reported classiﬁcation accuracy in Table 7 is deﬁned as the
number of samples correctly classiﬁed divided by the total number
of samples in the test set. It is equivalent to the micro-averaged F1
score in the single-label classiﬁcation task. Comparing the results
Table 8
Macro-averaged F1 scores of the three compared methods over four datasets.
Methods

PAP

2D-Hela

Hep-2

Tumor

CLBP
CoALBP
SAHLBPT3

82.87 72.1
81.45 71.9
84.23 71.9

87.127 1.9
86.23 7 1.7
88.767 1.5

86.417 1.5
91.897 1.7
91.29 7 1.5

78.65 7 2.1
79.23 7 2.0
83.817 1.9

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

Table 9
Statistical signiﬁcance of differences in classiﬁcation accuracies for different
methods.
Datasets

Compared methods

PAP

SAHLBPT3
SAHLBPT3
SAHLBPT3
SAHLBPT3
SAHLBPT3
SAHLBPT3
SAHLBPT3
SAHLBPT3

2D-Hela
Hep-2
Tumor

vs.
vs.
vs.
vs.
vs.
vs.
vs.
vs.

CLBP
CoALBP
CLBP
CoALBP
CLBP
CoALBP
CLBP
CoALBP

p-values

Corrected α

Result

0.1435
0.0122
0.0028
o 0.0001
o 0.0001
0.1723
o 0.0001
o 0.0001

0.00625
0.00625
0.00625
0.00625
0.00625
0.00625
0.00625
0.00625

NS
NS
*
*
*
NS
*
*

197

the computational time for feature extraction (for a single image),
training and testing on 2D-Hela dataset, as shown in Table 10. The
results show that SAHLBPT3 is slower than standard LBP (with both
R¼1, P ¼8 and R¼2, P ¼16) and CoALBP, which is caused by the
increased time demand time for searching the adaptive radius and
computing spatial adjacent histogram. It is worth stating that, in
training and testing stage, our method is faster than LBP (with
R¼2, P ¼16) and CoALBP. This is because the feature dimension of
our method (i.e. 2048 dimensions) is shorter than LBP(R ¼ 2, P ¼ 16)
(i.e. 65536 dimensions) and CoALBP (i.e. 3072 dimensions). Based
on consideration of both time and performance, we infer that
SAHLBPT3 is suitable for medical image classiﬁcation.

Fig. 14. Classiﬁcation accuracies of our method on four database for different SVM kernels.

of Table 7 and Table 8, we can ﬁnd out that the micro- and macroaveraged results do not differ signiﬁcantly and the previous ﬁndings based on accuracy would not change considering the macroaveraged F1-score.
In order to further perform statistical analysis for comparing
our SAHLBPT3 with the other two representative methods, i.e.
CLBP and CoALBP, we employ Wilcoxon ranksum test and the
Bonferroni method [36] for multiple pair-wise comparisons. In this
case, the level of signiﬁcance has been corrected as α ¼0.05/
8 ¼0.00625 according to the Bonferroni method. The signiﬁcant pvalues and corrected α are reported in Table 9, where * denotes
that the two methods have signiﬁcant different performance and
NS denotes that the two methods have no signiﬁcant different
performance.
In summary, we can conclude that our method is indeed a
superior algorithm with a clear motivation, convincing improvements, and an easy implementation. The advantage of our method
with respect to other variants makes it a good choice for medical
image classiﬁcation.
3.5. Runtime performance analysis
To study the computational demand of the proposed method,
we analyze the required time of our model and some representative methods. The MATLAB R2014a are employed, running on
an Intel i5-2400 processor at 3.1 GHz with 4 GB RAM. We report
Table 10
Computation time for feature extraction (single image), training and testing on 2DHela dataset.
Methods

Feature extraction

Training

Testing

LBP(R ¼ 1, P ¼ 8)
LBP(R ¼ 2, P ¼ 16)
CoALBP
SAHLBPT3

0. 103 s
0.197 s
0.562 s
0.823 s

1.423 s
814.27 s
12.37 s
6.553 s

0.056 s
44.305 s
0.798 s
0.493 s

Furthermore, The computational time for feature extraction
mainly depends on the descriptor and the image size, while the
training and testing are mainly dependant on the classiﬁer, feature
dimension and the scale of the dataset. Our method has took
0.823 s in the feature extraction stage per image with 512  382
size and 6.553 s in the training stage for 862 samples with 3072
feature dimensions. We think that our computational demand is
an adequate trade-off. Viewed from this perspective, we believe
our method can be extended to large-scale datasets. Due to the
limitations of the scale of medical image datasets, we will evaluate
our method on more general datasets with larger scale in our
future research.
3.6. Inﬂuence of different SVM kernels
Since the choice of the SVM kernel may have an impact on the
classiﬁcation rates, we also conducted an experiment by using
several SVM kernels. Though abundantly new kernels have been
proposed, the most frequently used kernel functions [32,33] in
image classiﬁcation are linear, polynomial, Radial Basis Function
(RBF), and histogram intersection (HI) kernels [34], formulated as
in Eq.(16)–Eq.(19), respectively.
ð1Þ

Linear : Kðxi ; xj Þ ¼ xi T xj

ð16Þ

ð2Þ

Polynomial : Kðxi ; xj Þ ¼ ðγ xi T xj þ rÞd ; γ 4 0

ð17Þ

ð3Þ

Radial basis functionðRBF Þ

: Kðxi ; xj Þ ¼ expð  γ ‖xi xj ‖2 Þ; γ 4 0
ð4Þ

Histogram intersectionðHI Þ : Kðxi ; xj Þ ¼

ð18Þ
n
X

minðxi ; xj Þ

ð19Þ

i¼1

Here, γ , r, and d are kernel parameters.
In our test, the default parameter values are used for different
kernels. Fig. 14 shows the average classiﬁcation accuracies we
achieved with different kernels on the four datasets. Note that we

198

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

Table 11
Classiﬁcation accuracy of compared descriptors in dense BoVW framework over
four datasets.
PAP
BoVW
framework

Proposed
framework

SIFT
LBP
SAHLBPT3
SIFT þ
SAHLBPT3
SAHLBPT3

2D-Hela

Hep-2

Tumor

84.03 72.3 83.79 7 2.5 80.88 71.5 78.23 7 1.7
81.43 72.1 81.477 2.1 77.52 71.9 75.217 2.1
86.21 72.0 84.89 7 2.2 85.06 72.0 81.45 7 1.7
87.63 72.1 86.217 2.5 83.47 71.8 83.34 7 1.9
88.03 71.7 90.06 7 1.5

datasets

Compared methods

p-values

Corrected α

Result

PAP

SAHLBPT3 vs. SIFT
SAHLBPT3 vs. LBP
SIFT þ SAHLBPT3 vs.
SAHLBPT3 vs. SIFT
SAHLBPT3 vs. LBP
SIFT þ SAHLBPT3 vs.
SAHLBPT3 vs. SIFT
SAHLBPT3 vs. LBP
SIFT þ SAHLBPT3 vs.
SAHLBPT3 vs. SIFT
SAHLBPT3 vs. LBP
SIFT þ SAHLBPT3 vs.

0.0031
o 0.0001
0.0053
0.0079
o 0.0001
0.0370
o 0.0001
o 0.0001
0.0208
o 0.0001
o 0.0001
0.0013

0.00417
0.00417
0.00417
0.00417
0.00417
0. 00417
0.00417
0.00417
0.00417
0.00417
0.00417
0.00417

*
*
NS
NS
*
NS
*
*
NS
*
*
*

2D-Hela

91.86 70.9 84.487 1.8
Hep-2

Table 12
Macro-averaged F1 score of compared descriptors in dense BoVW framework over
four datasets.
PAP
BoVW
framework

Table 13
Statistical signiﬁcance of differences in classiﬁcation accuracies for different local
descriptors in BoVW model.

SIFT
LBP
SAHLBPT3
SIFT þ
SAHLBPT3

2D-Hela

Hep-2

Tumor

SAHLBPT3

SAHLBPT3

SAHLBPT3

SAHLBPT3

Tumor

77.97 7 2.5 82.29 7 2.4 79.767 1.8 77.077 1.9
73.17 7 2.5 80.137 2.1 76.197 1.7 74.017 2.1
81.58 7 2.4 83.75 7 2.0 84.057 1.9 80.137 1.7
83.27 7 2.4 85.357 2.3 82.317 2.1 82.417 1.9

have not optimized SVM kernel parameters in Fig. 14. Particularly
noteworthy, in our case is that, the parameter optimization performed for SVM with RBF kernel is time-consuming, while the
parameter optimization performed for SVM with linear kernel has
a little inﬂuence on classiﬁcation performance. This means the
classiﬁcation result of our method has little change if we set different cost factor for SVM with linear kernel. This may be due to
the fact that the feature vectors of our method are rather sparse
with high dimensionality. Based on the average results on four
datasets, linear SVM kernels should be preferred in this case, since
computation time in this case varies linearly with the size of the
training data.
3.7. Evaluating the performance of the SAHLBPT in BoVW framework
In this section, for the sake of completeness, we further test the
performance of our texture descriptor within the bag of visual
words (BoVW) framework. The BoVW model is one of the most
popular algorithms for image representation by using visual words
formed by vector-quantizing local features with a clustering
method (such as K-means). The standard LBP and our SAHLBPT3
method can also be used as local texture descriptors for BoVW. In
the experiment, we use dense sampling strategy for the BoVW
model. More speciﬁcally, the local features are extracted from
16  16 image patches on a regular grid spaced at 8 pixels for all
images. Since the SIFT descriptor is probably the most widely used
descriptor for describing the local patches in BoVW model, we
mainly compare our method with dense SIFT descriptor. For dense
SIFT descriptor, the two parameters patch size and grid spacing are
set as 16 and 8, respectively, to allow for dense sampling. Moreover, the spatial pyramid matching (SPM) [37] settling is employed
for SIFT descriptor. The SPM is calculated at three levels. At the
ﬁrst level, the original image is considered as a sub-region. At the
second and the third level, the original image is divided into 2  2
and 4  4 sub-regions, respectively. Then all BoVW histograms of
21 sub-regions are concatenated to form the SPM representation
of the image. Additionally, single feature may fail to capture the
rich information within local image patches, as such, it is reasonable to extract multiple features for compensation. Therefore, we
also use multiple features that consist of SIFT and SAHLBPT3 to
describe local patches and then apply it to construct the visual
vocabulary (named SIFT þ SAHLBPT3 for short).

The classiﬁcation accuracies and the macro-averaged F1 scores
for different local descriptors used in BoVW model are reported in
Table 11 and Table 12, respectively. Here the visual vocabulary size
is 200 for all methods.
To perform statistical analysis, we also compare the SAHLBPT3
with SIFT, LBP and SIFTþSAHLBPT3 within BoVW framework tested by Wilcoxon ranksum test, the test protocol is the same as that
in subsection 3.4. In this test, the level of signiﬁcance has been
corrected as α ¼ 0.05/12 ¼0.00417 according to the Bonferroni
method. The p-values and corrected α are reported in Table 13,
where * denotes that the two methods have signiﬁcant different
performance and NS denotes that the two methods have no signiﬁcant different performance.
From Table 11, Table 12 and Table 13, we can make the following conclusions.
(1) From Table 11 and Table 12, it can be observed that the
SAHLBPT3 consistently outperforms both SIFT and LBP on the
four datasets when they are applied to BoVW framework.
However, the LBP is slightly weaker than SIFT as local
descriptors in BoVW model. Looking at the signiﬁcance of
differences in Table 13, we can ﬁnd out that the improvements
of SAHLBPT3 over LBP are signiﬁcant on the four datasets.
Moreover, SAHLBPT3 and SIFT have signiﬁcant different performance in PAP, Hep-2 cell and Tumor datasets.
(2) The BoVW model using the multiple features, i.e. SIFT þ
SAHLBPT3, excels that using a single feature on PAP, 2D-Hela,
and Tumor datasets based on the accuracy and the macroaveraged F1 score. However, the SAHLBPT3 works better than
SIFTþ SAHLBPT3 on Hep-2 cell dataset, which means that SIFT
and SAHLBPT3 descriptors do not provide very useful compensation for Hep-2 cell dataset. Looking at Table 13, SIFTþ
SAHLBPT3 and SAHLBPT3 have signiﬁcant different performance in Tumor datasets, but they have no signiﬁcant different performance in PAP, 2D-Hela and Hep-2 cell datasets. This
means that SIFT and our SAHLBPT3 are complementary features for Tumor datasets in BoVW model.
(3) To discuss, we return to the classiﬁcation performance of the
SAHLBPT3 in our framework and also re-record the results in
Table 7. We can easily ﬁnd that SAHLBPT3 in our framework
achieves better classiﬁcation results than that in the BoVW
framework. This is because our SAHLBPT3 aims to describe the
micro-structures of the whole image and these microstructures tend to play the largest role with a globally
encoding mode. Encoding the micro-structures in local image
patches would slightly decrease the performance of SAHLBPT3.
Moreover, the run-time for clustering in BoVW framework is
time-consuming. Based on both the classiﬁcation result and

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

199

Fig. 15. Comparison of classiﬁcation accuracies on 2D-Hela datasets using different
vocabulary sizes for BoVW model.
Fig. 16. Confusion matrices of the proposed method for 2D-HeLa dataset. C1:actin;
C2:dna; C3:endosome; C4: er; C5:golgia; C6:golgpp; C7:lysosome; C8:mircrotubules; C9:mitochondiria; C10: nucleolus.

Table 14
P-values of statistical tests for compared methods on four datasets.
Compared methods
SAHLBPT1
SAHLBPT2
SAHLBPT3
SAHLBPT1
SAHLBPT2
SAHLBPT3
SAHLBPT1
SAHLBPT2
SAHLBPT3
SAHLBPT1
SAHLBPT2
SAHLBPT3
SAHLBPT3
SAHLBPT3

vs.
vs.
vs.
vs.
vs.
vs.
vs.
vs.
vs.
vs.
vs.
vs.
vs.
vs.

LBPT1(R ¼ 1,P ¼ 8)
LBPT2(R ¼ 1,P ¼ 8)
LBPT3(R ¼ 1,P ¼ 8)
LBPT1(R ¼ 2,P ¼ 8)
LBPT2(R ¼ 2,P ¼ 8)
LBPT3(R ¼ 2,P ¼ 8)
LBPT1(R ¼ 2,P ¼ 16)
LBPT2(R ¼ 2,P ¼ 16)
LBPT3(R ¼ 2,P ¼ 16)
DCLBPT1
DCLBPT2
DCLBPT3
SAHLBPT1
SAHLBPT2

PAP

2D-Hela

Hep-2

Tumor

o 0.0001
o 0.0001
o 0.0001
o 0.0001
o 0.0001
o 0.0001
0.0213
0.1072
0.0019
o 0.0001
o 0.0001
o 0.0001
0.0181
o 0.0001

o 0.0001
o 0.0001
o 0.0001
o 0.0001
o 0.0001
o 0.0001
0.0021
0.0033
o 0.0001
o 0.0001
0.0018
o 0.0001
0.0823
o 0.0001

o 0.0001
o 0.0001
o 0.0001
o 0.0001
o 0.0001
o 0.0001
o 0.0001
o 0.0001
o 0.0001
o 0.0001
o 0.0001
o 0.0001
0.1823
0.0013

o0.0001
o0.0001
o0.0001
o0.0001
o0.0001
o0.0001
o0.0001
0.0015
o0.0001
o0.0001
o0.0001
o0.0001
0.0109
o0.0001

time complexity, the SAHLBPT3 involved in our framework
should be more preferred in medical image classiﬁcation task.
Finally, we then report the classiﬁcation accuracies of the
methods using different vocabulary sizes. In Fig. 15, the classiﬁcation accuracies have a similar trend as the visual vocabulary size
increases, i.e. the performance saturates when the vocabulary size
larger than 200. Moreover, SAHLBPT3 consistently outperforms
both SIFT and LBP using different vocabulary sizes.

Fig. 17. Confusion matrices of the proposed method for Hep-2 cell dataset. C1:
centromere; C2:coarse speckled; C3:cytoplasmatic; C4: ﬁne speckled; C5:homogeneous; C6:nucleolar.

Despite the well achieved accuracy, there are still some open
problems in our framework.
4. Conclusion and future work
In this paper, we proposed a novel framework based on our
adaptive local binary patterns and spatial adjacent histogram for
medical image classiﬁcation. We ﬁrst employed gradient operator
to determine the adaptive neighborhood radius of each pixel. Then
three coding schemes based on adaptive radius were introduced
for processing the LBP histograms. To capture the discriminative
micro-structures features produced by the adaptive radius, we
proposed to using spatial adjacent histogram strategy for image
representation. Finally, a SVM classiﬁer is learned for medical
image classiﬁcation task. We also evaluated four SVM kernels into
our algorithm to enhance its classiﬁcation power. Based on the
experiments on PAP smear, 2D-Hela, Hep-2 cell and Brain tumor
datasets, we can draw a conclusion that our proposed method
achieved better performance compared with other LBP-based
approaches.

(1) The dimension of our model is much larger than that of the
standard LBP, and this will result in the computational time
increasing linearly as training data grow in number. The scale
of medical image datasets is generally not large, however, our
method will cost more time when encountering general
datasets with large scale. The feature selection techniques
could be employed to solve this problem.
(2) In our proposal, we focus on improving LBP in terms of
encoding the micro-structures features, and we did not design
patterns with rotation invariant. However, the rotation invariants are expected to further improve the performance of
our model.
(3) We mainly employ SVM with linear kernel and we also report
the performance of different kernels with default parameter
values. However, a number of optimization algorithms could
be employed to further improve the classiﬁcation performance, such as swarm intelligence optimization algorithm.

200

D. Liu et al. / Computers in Biology and Medicine 72 (2016) 185–200

Fig.18. Confusion matrices of the proposed method for brain tumor dataset. C1:
astrocytoma; C2: tuberculum sellae meningioma; C3: olfactory groove meningioma; C4: acoustic neuroma; C5: pituitary tumor.

We will explore these problems in the future towards a more
powerful medical image classiﬁcation model.

Conﬂict of interest statement
None declared.

Acknowledgments
This work was supported by the National Natural Science
Foundation of China (61472161, 61133011, 61402195, 61502198,
61303132, 61202308), Science and Technology Development Project of Jilin Province (20140101201JC), the Science and Technology
Plan Project of Wenzhou of China (G20140048) and the Program of
China Scholarships Council (No. 201406170116).

Appendix A. Supporting information
The p-values of statistical tests for compared methods mentioned in Section 3.3 are presented in Table 14.
The confusion matrices for 2D-HeLa, Hep-2 cell, and brain
tumor datasets, are presented in Fig. 16, Fig. 17, and Fig. 18,
respectively. For each confusion matrix, the average classiﬁcation
accuracies for individual classes are listed along the diagonal, and
the entry in the ith row and jth column is the percentage of images
from the class i that are misidentiﬁed as class j.

References
[1] K. Doi, Computer-aided diagnosis in medical imaging: historical review, current status and future potential, Comput. Med. Imaging Graph. 31 (4) (2007)
198–211.
[2] F. Lalys, L. Riffaud, X. Morandi, P. Jannin, Automatic Phases Recognition in
Pituitary Surgeries by Microscope Images Classiﬁcation. In Information Processing in Computer-assisted Interventions, Springer Berlin Heidelberg (2010),
p. 34–44.
[3] T.F. Cootes., C.J. Taylor., Statistical models of appearance for medical image
analysis and computer vision. Medical imaging 2001, Int. Soc. Opt. Photon.
(2001) 236–248.

[4] H. Pourghassem, H. Ghassemian, Content-based medical image classiﬁcation
using a new hierarchical merging scheme, Comput. Med. Imaging Graph. 32
(2008) 651–661.
[5] M. Häfnera, M. Liedlgruber, A. Uhl, A. Vécsei, F. Wrba, Color treatment in
endoscopic image classiﬁcation using multi-scale local color vector patterns,
Medical Image Analysis, 16, pp. 75–86.
[6] T. Ojala, M. Pietikäinen, T. Mäenpää, Multiresolution gray-scale and rotation
invariant texture classiﬁcation with local binary pattern, IEEE Trans. Pattern
Anal. Mach. Intell. 24 (7) (2002) 971–987.
[7] H. Zhou, R. Wang, C. Wang, A novel extended local binary pattern operator for
texture analysis, Inf. Sci. 22 (2008) 4314–4325.
[8] Z.H. Guo, L. Zhang, D. Zhang, X.Q. Mou, Hierarchical multiscale lbp for face and
palmprint recognition, in: Proceedings of the 17th IEEE International Coference Image Processing (ICIP 2010), 2010, pp. 4521–4524.
[9] L. Nanni, A. Lumini, S. Brahnam, Survey on LBP based texture descriptors for
image classiﬁcation, Expert Syst. Appl. 39 (2012) 3634–3641.
[10] J.F. Ren, X.D. Jiang, J.S. Yuan, Noise-resistant local binary pattern with an
embedded error-correction mechanism, IEEE Trans. Image Process. 22 (10)
(2013) 4049–4060.
[11] Jun Shang, et al., Robust image region descriptor using local derivative ordinal
binary pattern, J. Electron. Imaging 24 (3) (2015) 033009.
[12] X.L. Meng, Z.Z. Wang, L.Z. Wu, Building global image features for scene
regognition, Pattern Recognit. 5 (2012) 373–380.
[13] Z. Guo, L. Zhang, D. Zhang, A completed modeling of local binary pattern
operator for texture classiﬁcation, IEEE Trans. Image Process. 19 (2010) (16751663).
[14] X. Tan., B. Triggs., Enhanced local texture feature sets for face recognition
under difﬁcult lighting conditions, IEEE Trans. Image Process. 19 (6) (2010)
1635–1650.
[15] C. Zhu, C.E. Bichot, L. Chen, Multi-scale color local binary patterns for visual
object classes recognition, in: Proceedings of the International Conference on
Pattern Recognition (ICPR), 2010, pp. 3065–3068.
[16] S. Liao, A.C.S. Chung, In: Face Recognition by Using Elongated Local Binary
Patterns with Average Maximum Distance Gradient Magnitude, Asian Conference on Computer Vision, 2007.
[18] W. Zhang, S. Shan, W. Gao, X. Chen, H. Zhang, Local Gabor binary pattern
Histogramse quence (LGBPHS): a novel non-statistical model for face representation and recognition, Int. Conf. Comput. Vision. 1 (2005) 786–791.
[19] S. Liao, M.W.K. Law, A.C.S. Chung, Dominant local binary patterns for texture
classiﬁcation, IEEE Trans. Image Process. 18 (2009) 1107–1118.
[20] N. Senthilkumaran, R. Rajesh., Edge detection techniques for image segmentation–a survey of soft computing approaches, Int. J. Recent Trends Eng. 1 (2)
(2009).
[21] N. Werghi, S. Berretti, A.D. Bimbo, The mesh-LBP: a framework for extracting
local binary patterns from discrete manifolds, IEEE Tans. Image Process. 24 (1)
(2015).
[22] Z. Li, G. Liu, Y. Yang, et al., Scale-and rotation-invariant local binary pattern
using scale-adaptive texton and subuniform-based circular shift, Image Process. IEEE Trans. 21 (4) (2012) 2130–2140.
[23] S. Hegenbart, A. Uhl, A scale-and orientation-adaptive extension of Local
Binary Patterns for texture classiﬁcation, Pattern Recognit. 48 (8) (2015)
2633–2644.
[24] S. Liao, A.C.S. Chung, Face recognition by using elongated local binary patterns
with average maximum distance gradient magnitude, Asian Conf. Comput.
Vision (2007) 627–629.
[25] T. Ahonen, J. Matas, C. He, M. Pietikainen, Rotation invariant image description
with local binary pattern histogram Fourier features, Image Anal. (2009)
61–70.
[26] Z. Guo, L. Zhang, D. Zhang, A completed modeling of local binary pattern
operator for texture classiﬁcation, IEEE Trans. Image Process. 19 (2010)
1657–1663.
[27] Ryusuke Nosaka, Yasuhiro Ohkawa, Kazuhiro Fukui., Feature Extraction Based
on Co-occurrence of Adjacent Local Binary Patterns. Advances in Image and
Video Technology, Springer Berlin Heidelberg (2012), p. 82–91.
[28] A. Chebira, Y. Barbotin, C. Jackson, T. Merryman, G. Srinivasa, R.F. Murphy,
J. Kovačević, A multiresolution approach to automated classiﬁcation of protein
subcellular location images, BMC Bioinform. 8 (1) (2007) 210.
[29] P. Foggia, G. Percannella, P. Soda, M. Vento, Benchmarking HEp-2 cells classiﬁcation methods, Med. Imaging IEEE Trans. 32 (10) (2013) 1878–1889.
[30] J. Jantzen, J. Norup, G. Dounias, B. Bjerregaard, Pap-smear benchmark data for
pattern classiﬁcation, Nat. Inspired Smart Inf. Syst. (2005) 1–9.
[31] C.C. Chang, C.J..Lin, LIBSVM: A Library for Support Vector Machines, 2001.
Software available from: 〈http://www.csie.ntu.edu.tw/  cjlin/libsvm〉.
[32] H. Byun, S.W. Lee, Applications of Support Vector Machines for Pattern
Recognition: A Survey[m]/Pattern Recognition with Support Vector Machines,
Springer Berlin Heidelberg (2002), p. 213–236.
[33] G.B. Huang, D.H. Wang, Y. Lan, Extreme learning machines: a survey, Int. J.
Mach. Learn. Cybern. 2 (2) (2011) 107–122.
[34] S. Maji, A.C. Berg, J. Malik, Classiﬁcation using intersection kernel support
vector machines is efﬁcient[C]/Computer Vision and Pattern Recognition,
2008, CVPR 2008, IEEE Conference on IEEE, 2008: pp. 1–8.
[36] J.M. Bland, D.G. Altman, Multiple signiﬁcance tests: the Bonferroni method,
BMJ 310 (6973) (1995) 170.
[37] S. Lazebnik, C. Schmid, J. Ponce, Beyond bags of features: spatial pyramid
matching for recognizing natural scene categories, Comput. Vision. Pattern
Recognit. (CVPR) (2006).

See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/305828246

A	review	of	key	techniques	of	vision-based
control	for	harvesting	robot
Article		in		Computers	and	Electronics	in	Agriculture	·	September	2016
DOI:	10.1016/j.compag.2016.06.022

CITATIONS

READS

0

126

4	authors,	including:
Yuanshen	Zhao

Liang	Gong

Shanghai	Jiao	Tong	University

Shanghai	Jiao	Tong	University

12	PUBLICATIONS			10	CITATIONS			

35	PUBLICATIONS			84	CITATIONS			

SEE	PROFILE

SEE	PROFILE

Yixiang	Huang
Shanghai	Jiao	Tong	University
27	PUBLICATIONS			163	CITATIONS			
SEE	PROFILE

Some	of	the	authors	of	this	publication	are	also	working	on	these	related	projects:

A	study	on	key	techniques	of	multi-arm	harvesting	robot	View	project

All	content	following	this	page	was	uploaded	by	Yuanshen	Zhao	on	06	August	2016.
The	user	has	requested	enhancement	of	the	downloaded	file.	All	in-text	references	underlined	in	blue	are	added	to	the	original	document
and	are	linked	to	publications	on	ResearchGate,	letting	you	access	and	read	them	immediately.

Computers and Electronics in Agriculture 127 (2016) 311–323

Contents lists available at ScienceDirect

Computers and Electronics in Agriculture
journal homepage: www.elsevier.com/locate/compag

Review

A review of key techniques of vision-based control for harvesting robot
Yuanshen Zhao, Liang Gong, Yixiang Huang, Chengliang Liu ⇑
School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China

a r t i c l e

i n f o

Article history:
Received 11 December 2015
Received in revised form 16 June 2016
Accepted 20 June 2016

Keywords:
Harvesting robot
Vision-based control
Vision information acquisition
Fruit recognition
Eye-hand coordination

a b s t r a c t
Although there is a rapid development of agricultural robotic technologies, a lack of access to robust fruit
recognition and precision picking capabilities has limited the commercial application of harvesting
robots. On the other hand, recent advances in key techniques in vision-based control have improved this
situation. These techniques include vision information acquisition strategies, fruit recognition algorithms, and eye-hand coordination methods. In a fruit or vegetable harvesting robot, vision control is
employed to solve two major problems in detecting objects in tree canopies and picking objects using
visual information. This paper presents a review on these key vision control techniques and their potential applications in fruit or vegetable harvesting robots. The challenges and feature trends of applying
these vision control techniques in harvesting robots are also described and discussed in the review.
Ó 2016 Elsevier B.V. All rights reserved.

Contents
1.
2.

3.

4.

5.

6.
7.

8.

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Vision-based control for harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.
The concept of vision-based control for harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.
The role of vision-based control in harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Vision schemes for harvesting robot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.
Monocular camera scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.
Binocular stereovision scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.
Laser active visual scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.
Thermal imaging scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5.
Spectral imaging scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Recognition approaches for harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.
Single feature analysis approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.
Multiple features fusion approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3.
Pattern recognition approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Eye-hand coordination in harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.1.
Open-loop visual control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2.
Visual servo control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Examples of fruit harvesting robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Challenges and future trends. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.1.
Enhancing the vision-based control of harvesting robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2.
Human–machine collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3.
Multi-arms cooperating for robotic harvesting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.4.
Making the environment more suitable for robotic harvesting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Conclusions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

⇑ Corresponding author.
E-mail address: chlliu@sjtu.edu.cn (C. Liu).
http://dx.doi.org/10.1016/j.compag.2016.06.022
0168-1699/Ó 2016 Elsevier B.V. All rights reserved.

312
312
312
312
312
313
313
314
314
315
316
316
316
317
318
318
318
319
319
319
320
320
320
321
321
321

312

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

1. Introduction

2. Vision-based control for harvesting robot

With the development of modern agriculture, the application of
robotic and intelligent machines in agriculture has followed several trends in technology advancements. Firstly, increasing cost
and decreasing supply of skilled labor force are becoming a huge
challenge to the agriculture industry. Traditional farming is highly
labor intensive and contains many menial and tedious tasks, it is
one of the last industries to use robots (Sistler, 1987). Secondly,
food safety is an important issue that requires the use of reliable
robotic machines to reduce the risk of contaminations (Edan
et al., 2009). Thirdly, sustainable agriculture, which provides
enough food while not harming the environment, also needs
robotic systems to improve productivity at low cost (Grift et al.,
2008). It is evident that wide application of robots can offer a significant benefit to agriculture.
The emergence of agricultural robots is accompanied with
other industries such as manufacturing and mining that have
embraced the robotic revolution. Agricultural robots are perceptive and intelligent machines which are programmed to perform
a variety of agricultural tasks such as transplanting, cultivating,
spraying, trimming and harvesting (Edan et al., 2009).
Considering the economic benefit, high harvesting costs have
led the harvesting robot as a research focus (Hayashi et al.,
2005). Harvesting robots are designed to sense the complex
agricultural environment by various sensors and use that
information, together with a goal, to perform the harvesting
actions (Edan and Gaines, 1994). Although harvesting robot holds
ample promise for the future, currently the overall performance
of harvesting robot is often insufficient to compete with manual
operation (Grift et al., 2008). The bottleneck to promote the application of harvesting robot lies on the performance of vision-based
control.
The use of visual information for the control of robotic manipulator is called vision-based control, which began with the work of
Shirai and Inoue (1973). The progress of vision control in that era
was hindered largely by various technological issues, in particular,
extracting information form vision sensors (Corke and Hager,
1998). Since 1990, there has been a marked rise in the interest in
this field of vision control, largely fueled by the increasing computing power of personal computers. After that, vision-based control
for harvesting robot had ushered in the era of rapid development.
Although numerous research results have been reported on development of vision control technology for robotic harvesting, the low
successful rate of fruit recognition and inefficiency of eye-hand
coordination are the main factors to limit the performance of harvesting robot. Thus, a review of this research field is necessary to
promote further developments of vision-based control technology
for harvesting robot.
This article provides a review of the past and current research
and development of vision-based control for harvesting robot. It
is aimed to introduce an up-date account of useful methods found
in literature to provide solutions to the two key issues: (a) the
recognition of target fruit; (b) eye-hand coordination control. The
remaining of this paper is organized as follows. In Section 2, a general background to vision-based control is introduced. Representative vision schemes for harvesting robots are presented in
Section 3. In Section 4, approaches adopted for fruit target recognitions are discussed. A review of eye-hand coordination techniques
is given in Section 5. Section 6 presents some examples of fruit or
vegetable harvesting robots. Challenges and future trends for harvesting robots are discussed in Section 7. A conclusion is drawn in
Section 8.

2.1. The concept of vision-based control for harvesting robot
Vision-based control for harvesting robot is a framework by
which the robot accomplishes the fruit picking task under the
guidance of visual information. This framework is constructed with
two objectives; fruit recognition and eye-hand coordination
(Hashimoto, 2003). Automatic fruit recognition for harvesting
robot means identifying and locating the fruit in a natural complex
scene. These two tasks are the foundation of picking operation.
Eye-hand coordination for harvesting robot is concerned with the
interaction between the robot visual perception of the workspace and it actuators (Goncalves and Torres, 2010).

2.2. The role of vision-based control in harvesting robot
The idea of robotic harvesting was firstly proposed by Schertz
and Brown (1968) in 1960s for citrus harvesting. Compared with
the traditional mechanical harvesting approaches using shaker or
air blast, robotic harvesting is a precision harvesting approach.
Typical fruit or vegetable harvesting robots are built with manipulators, end-effectors, vision systems, and motion systems (Edan
et al., 2009). Among these, vision-based control plays an important
role of autonomous harvesting. On the contrary to industrial
robots which are simple, repetitive, well-defined and known a priori, harvesting robots need to work in an unstructured, uncertain,
and varying environment. Vision-based control for harvesting
robots is designed to solve the follow difficult problems. Firstly,
the manipulated objects of harvesting robots are natural objects
which have a high degree of variety in fruit size, shape, color, texture and hardness as a result of environmental and genetic differences. Secondly, the workspace is complex and loosely structured
with large variations in illumination and degree of object occlusion. Thirdly, the random location of target fruits requires picking
to operate in a three-dimensional continuously changing track.
Thus, vision-based control is an attractive approach to meet these
challenges.

3. Vision schemes for harvesting robot
Fruit or vegetable detection for harvesting robot is conducted
by various visual sensors. According to the principle of imaging,
the visual sensors used to recognize objects are classified into
two-dimension (2D) visual imaging sensors and three-dimension
(3D) visual imaging sensors. The 2D images acquired can indicate
morphological features of the target fruit such as color, shape
and texture. Three-dimensional visual image sensors provide 3D
coordinate maps of the entire scene which can give the shape
and spatial location of the fruit object. The vision scheme also
has relationship with the recognition process. As shown in Fig. 1,
for identifying different kinds of objects, it is needed to select available visual sensors cooperating with a certain recognition
algorithm.
The review of recognition algorithms such as color based analysis, edge detection, K-means clustering, and Bayes classification is
given in Section 4. The follow sub-sections contain a critical review
of visual sensors used in the past for fruit detection in harvesting
robot. The applications, principles, advantages and limitations of
various vision schemes for harvesting robots are summarized in
Table 1.

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

Vision schemes

·Monocular
·Binocular stereoscope
·Laser active visual
·Spectral imaging
·Thermal imaging
ĊĊ

Algorithms

313

Objects

·Color based analysis
·Shape based analysis
·Texture based analysis
·Neural network approach
·Multi-feature fusion
·Fuzzy clustering means
·Support vector machine
ĂĂ

·Tomato
·Apple
·Cucumber
·Pepper
·Strawberry
·Citrus
·Eggplant
·ĂĂ

Fig. 1. Various vision schemes and recognition algorithms for different kinds of fruits (Li et al., 2011a,b).

Table 1
The classification of applications, principles, advantages and limitations of various vision schemes for harvesting robots.
Vision
scheme

Applications and principles

Advantages

Limitations

Monocular

Identifying target fruits by color, shape, and
texture feature
Identifying fruits using color shape, and texture
features; positing target fruits through the
principle of triangulation
Identifying fruits using 3D shape feature;
positing target fruits through 3D reconstruction
approach
Identifying target fruits using the differences of
infrared radiation between target fruit and
background
Identifying target fruits using features extracted
from invisible wavelengths

Monocular vision system is the simplest
and lowest cost
The binocular stereo is the most common
approach to obtain the 3D position of
detected fruit
It is an alternative to obtain the 3D position
in the condition of light changing and
background clustering
It is available to detect target fruit in
varying illumination condition, especially at
night
It can detect the green color or overlapped
fruits

Only provides 2D information, light change influences
the imaging results
Sensor calibration is required, image matching is very
time consuming; errors in 3D measurement is
unavoidable
Vision system is required for geo-referencing, complex
and large image data are needed. The imaging
processing is also a challenge
Sensor calibration and atmospheric correction are
required, high computation consumption for image
processing
Imaging processing is very time consuming, the sensor
cost is high

Binocular
stereo
Laser
active
visual
Thermal
imaging
Spectral
imaging

3.1. Monocular camera scheme

3.2. Binocular stereovision scheme

Monocular scheme is a machine vision system consisting of a
single camera, which was used in some earliest studies for detecting fruit (Jimenez et al., 2000a,b). The cameras with Charged Coupled Device (CCD) sensors or Complementary Metal Oxide
Semiconductor (CMOS) sensors are widely used in monocular
schemes. In the MAGALI project, a B/W camera was applied to
detect the fruits based on geometric feature (d’Esnon et al.,
1987). In later years, the B/W camera was replaced with a color
camera to enhance the color contrast between red apples and
green leaves. Slaughter and Harrell (1987) also used a digital color
camera with a filter of 675 nm wavelength to amplify the contrast
between oranges and background. The author reported a detection
accuracy of 75%. Zhao et al. (2005) used a color camera to identify
apples based on color and texture features and reported an accuracy of 90%. Baeten et al. (2008) developed a monocular vision system combining a camera and a high frequency light source to
detect apples in outdoor environment. The author recognized that
high frequency light could reduce the illumination influence. There
are other vision systems for harvesting robot with several cameras
forming a redundancy monocular system. Edan et al. (2000) developed a multiple monocular cameras system which was constructed
with two B/W CCD cameras to detect and locate melons in the
field. The two B/W cameras mounted on the platform and griper
could acquire far scene images and near scene images. The author
reported that the use of multiple monocular cameras could
improve the recognition accuracy. The major disadvantage of
monocular scheme is that images captured by the visual sensor
are sensitive to illumination conditions (see Table 2).

The Binocular stereovision scheme is designed with two cameras separated in a certain distance with an angle between them,
and they capture the same scene in two images. The threedimensional map of fruit object can be obtained through triangulation (Sun et al., 2011). Buemi et al. (1995, 1996) used a color
stereoscopic vision system consisting of two micro cameras in a
tomato harvesting robot named Agrobot. The stereovision system
mounted on the robot head could be used to navigate and identify
the ripe tomato. Shinsuke and Koichi (2005) installed a parallel
stereovision system of two cameras in a sweet pepper picking
robot. The stereovision system controlled by a camera positioning
system could move to a desired location to capture images of sweet
peppers. Yang et al. (2007) developed an improved stereovision
system based on the Color Layer Growing (CLG) algorithm to
reconstruct the 3D model of fruit object. Fruit objects with stereo
and self-occlusion in a strong sunlight condition could be detected
and located by the improved stereovision system. Xiang et al.
(2014) also studied a clustered tomato recognition method based
on depth map acquired by a binocular stereo camera. The recognition accuracy of clustered tomatoes was 87.9% at an acquisition
distance of 300–500 mm. Li et al. (2011a,b) developed a stereovision system, with optical filters, which was an attempt to capture
different waveband images. The recognition test results indicated
that the polarizer filtered data is slightly better than neutral density filtered data, and much better than the original image data.
Si et al. (2015) also used a stereo camera to detect and locate
mature apples in tree canopies. As shown in Fig. 2, the stereo camera was mounted on the slide bar in parallel with a distance of

314

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

Table 2
The comparison of different single feature analysis approaches.
Features

Fruits

Application situations

Accuracy

Limitations

Reference

Color

Tomato
Strawberry
Apple
Peach
Pineapple
Bitter melon

Artificial light condition
Uncontrolled environment
Uncontrolled environment
Uncontrolled environment
Field
Single objects

96.36%
>95%
94%
90%
85%
100%

It is not available to the green color fruit such as cucumber

Arefi et al. (2011)
Wei et al. (2014)
Kelman and Linker (2014)
Xie et al. (2011)
Chaivivatrakul and Dailey (2014)

High computation cost, occlusion is a big challenge

Fig. 2. Fruit harvesting robot with a binocular stereoscope. (1) Binocular stereoscope, (2) Manipulator, (3) End-effector, (4) Fruit object, (5) mobile platform (Si
et al., 2015).

200 mm between the centers of the two camera lens. The author
reported that over 89.5% of apples were successfully recognized
and the errors were less than 20 mm when the measuring distance
was between 400 mm and 1500 mm. The disadvantage of stereovision scheme is its complexity and long computation time due to
stereo matching (Hannan and Burks, 2004).
3.3. Laser active visual scheme
Although there are several techniques to obtain depth information, but considering some desirable features of the sensed image,
the laser active visual is a better choice. The 3D shape of fruit
object is measured by scanning the laser beams and the fruit can

Laser beam
(Infrared and red)

Oscillator
17.5kHz

3.4. Thermal imaging scheme
Thermal imaging is also called infrared thermograph which is
the visualization of infrared radiation (Li et al., 2014). Because of
the physical structure and characteristic, leaves accumulate less
heat and radiate faster than fruits, the temperature distributions
of the plant canopy with fruit can be applied for fruit detection
(Vadivambal and Jayas, 2001). Xu and Ying (2004) used a thermal
camera to identify the citrus in a tree canopy. From the analysis
result shown in Fig. 4, the temperature distribution along line AB

Timing signal

Lock in
amplifier

Filter

Lock in
amplifier

Filter

Lock in
amplifier

Filter

Preamplifier
PSD

Fruit object

Cold filter

Oscillator
35kHz

Red
laser diode

Infrared
laser diode

be distinguished from other obstacles according to different
spectral-reflections. The laser active visual scheme for fruit detection is illustrated in Fig. 3 (Gotou et al., 2003). Both laser beams
scan the fruit object simultaneously and the locations of fruit
objects and obstacles are recognized through image processing.
An infrared laser range-finder was installed in an orange harvesting robot named Agribot working in non-structured environments,
Jimenez et al. (2000a,b). The output of the infrared laser range finder includes 3D position, radius, and surface reflectivity of fruit
object. Tankgaki et al. (2008) designed a machine vision system
equipped with red and infrared laser scanning devices to detect
cherry on the tree, which could prevent the influence of the sunlight. Yin et al. (2009a,b) used a laser active visual sensor to measure the turned angles of robot arm and distance between target
tomato and end-effector. Zhang et al. (2015) developed a novel
apple stem recognition system using the 3D reconstruction technique combined with near-infrared and linear-array structured
lighting. The author reported that 97.5% overall recognition accuracy for the 100 samples was obtained by the proposed system
and method. Even though the accuracy of laser active visual system
is promising, the complexity of the system often limits its practical
application.

Lens

DC voltage
Preamplifier

Fig. 3. A scheme of the laser active visual system for fruit recognition (Gotou et al., 2003).

Computer

Texture

The contour loss is less than 1/2

A/D converter

Shape

315

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

19.7
19.1
18.6
18.0
17.5

A

B

16.9
16.3
15.8
15.2
80

160

240

320

400

480

560

640

Fig. 4. The typical test results of thermal image for citrus recognition.

Fig. 5. The model of fruit object detection using hyperspectral image.

across the citrus area, leaves and others showed that the temperature difference between citrus and other objects was more than
1 °C. Bulanon et al. (2008) also used a thermal infrared camera to
detect the citrus in day and night. The fruits were successfully segmented in the thermal images using image processing techniques
according to the largest temperature difference. Based on prior
research, Bulanon et al. (2009) proposed an improved fruit detection approach combined with a thermal image and a visible image.
Results showed that the performance of the image fusion approach
was better than using the thermal image alone. Even though thermal imaging has advantages on detecting fruits even when fruit
and background color are similar, the accuracy of recognition using
thermal imaging is affected by the shadow of the tree canopy
(Stajnko et al., 2004).
3.5. Spectral imaging scheme
The spectral camera is developed to integrate both spectroscopic and imaging techniques into one system to obtain a set of

monochromatic images at a continuum of wavelengths (Zhang
et al., 2014). With recent development of spectral imaging, the
spectral cameras have been used to recognize fruits. The model
of fruit object detection using multispectral image is shown in
Fig. 5 (Manolakis et al., 2003). A monochromatic near-infrared
camera, equipped with three different optical band pass filters
(1064, 1150 and 1572 nm), was used to identify in-field green
citrus by Kane and Lee (2007). The author reported an average correct pixel identification of 84.5%. Safren et al. (2007) used a hyperspectral camera to detect green apples. The hyperspectral imaging
was capable of giving a wealth of information both in the visible
and the near-infrared (NIR) regions and thus offered the potential
to detect the green apples. Okamoto and Lee (2009) also proposed
a green citrus recognition method using a hyperspectral camera of
369–1042 nm to solve the detection problem arising from similar
color between fruits and natural scenes. The test results reflected
that 80–89% of the citrus in the foreground of the validation set
were identified correctly. By comparing the spectral reflectance
difference of cucumber plant (fruit, leaf and flower) from visible

316

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

to infrared (350–1200 nm), sensitive bands of fruit information
were obtained by statistical variance analysis (Yuan et al., 2011).
4. Recognition approaches for harvesting robot
The recognition algorithm is a key factor affecting the performance of a vision recognition system. Numerous literatures
reported that various recognition algorithms have been employed
for robotic harvesting of fruits. Those recognition approaches can
be classified into single feature analysis approaches, multiple features fusion analysis approaches and pattern recognition
approaches.
4.1. Single feature analysis approaches
Color is one of the most prominent features used to distinguish
the mature fruit from the complex natural background. In the studies of color-based segmentation for fruit recognition, the image
pixels are categorized into two groups according to the threshold
which decides whether a pixel belongs to the fruit object or to
the background. However, the accuracy of segmentation using
color feature is sensitive to varing illumination conditions. For alleviating the influence of varying illumination, several color spaces
such as HIS, L⁄a⁄b⁄, and LCD are used to extracting color features
(Huang and He, 2012; Yin et al., 2009a,b). Arefi et al. (2011) developed a ripe tomato recognition algorithm using a combination of
RGB, HSI, and YIQ color spaces and fruit morphological features.
The authors argued that the total accuracy was 96.36% when the
proposed approach was adopted in a greenhouse with artificial
illumination. Mao et al. (2009) employed a Drg-Drb color indexing
to segment apples from background and an accuracy of 90% was
achieved. Wei et al. (2014) also proposed a fruit recognition
method based on an improved OSTU threshold algorithm using a
new color feature in the OHTA color space. The OHTA color space
was transform from the RGB color model through linear conversion
and the extraction accuracy was more than 95%. When the fruits
and leaves have similar colors, color-based segmentation methods
are not available for recognizing fruits (Reis et al., 2012).
Fruit recognition algorithms based on extracting geometric features are universal for detecting spherical fruit such as tomatoes,
apples, and citrus (Liu et al., 2007; Xie et al., 2010). Because of
independent color features, the shape-based analysis approach is
not affected by varying illuminations. Whittaker et al. (1987) proposed a modified Circular Hough Transform algorithm for locating
mature tomatoes which were partially hidden from obstacles. The
authors recommended that this shape-based analysis algorithm
could be valid for situations in which the perimeter of the fruit is
partially obscured by leaves or by overlapped tomatoes. Xie et al.
(2007, 2011) also put forward a concave spots searching algorithm

based on Hough Transform to improve the accuracy of strawberry
recognition. The authors argued that the proposed strawberry
recognition method is effective both for single fruit and complex
situation when the strawberry contour loss is less than 1/2.
Kelman and Linker (2014) also proposed a localization algorithm
of mature apples in trees using convexity. Together with the
removing 99.8% of the edges initially identified by Canny detector,
94% of the visible apples were correctly detected.
The images captured in natural outdoor conditions have some
texture differences which can be used to facilitate separation of
fruits from their background. Thus, texture feature plays an important role in fruit recognition especially when the fruits are clustered or occluded (Zhao et al., 2005; Kurtulmus et al., 2011a,b;
Rakun et al., 2011). To use color, texture, and shape information
by histogram based separation, circular Gabor texture features
and eigen-fruit approaches were implemented in the fruit recognition algorithm by Kurtulmus et al. (2011a,b). Notice that the application of texture features are always combined with color features
and/or geometric features.
4.2. Multiple features fusion approaches
In order to increase recognition reliability in uncontrolled environments caused by uneven illumination conditions, partly
occluded surfaces and similar background features, some researchers apply multiple features (color, geometry, and texture) fusion
algorithms to recognize fruits. Hannan et al. (2009) also developed
a machine vision algorithm consisted of color-based segmentation
and perimeter-based detection. Yin et al. (2009a,b) proposed a ripe
tomato recognition method which is combined with the tomato’s
shape features and the color features. With the color features
extracted from the L⁄a⁄b⁄ color space and the shape feature
acquired by a laser ranging sensor, the recognition and localization
system for tomato harvesting robot could be used to handle the situations of tomato overlapping and sheltering. Zhao et al. (2005)
proposed a texture based edge detection algorithm combined with
color properties analysis to recognize on-tree apples. The authors
presented that 90% of apples were correctly detected using the
recognition approach. Colors, intensity, edge and orientations as
the features of the target were considered by Patel et al. (2011)
to develop an improved multiple features based algorithm for fruit
detection. The authors reported that the detection efficiency was
achieved up to 90% using the optimal weights of different features.
Lu et al. (2014) also developed a novel method based on fusing the
segmentation results of chromatic aberration map (CAM) and
luminance map (LM) to recognize the citrus in a tree canopy.
Rakun et al. (2011) comprehensively considered three distinct features; color, texture and 3D shape of the fruit object for overcoming low recognition reliability in uncontrolled environments. They

Table 3
The major types of multi-modal images recognition algorithms.
Features

Fruits

Vision scheme

Accuracy

Reference

Color + Geometry

Tomato
Orange
Apple
Citrus
Peach
Apple
Apple
Citrus
Tomato
Apple
Apple
Citrus

Camera and Laser ranging sensor
Two cameras
Camera
Camera
Camera
Camera and Laser ranging sensor
Camera
Camera
Camera
Camera and ToF camera
Camera and thermal camera
Camera and thermal camera

NR
>90
90%
75.3%
90%
>90%
NR
86.81%
93%
>83.67%
74%
74.37%

Yin et al. (2009a,b)
Hannan et al. (2009)
Zhao et al. (2005)
Kurtulmus et al. (2011a,b)
Patel et al. (2011)
Bulanon and Kataoka (2010)
Rakun et al. (2011)
Lu et al. (2014)
Zhao et al. (2016a,b)
Feng et al. (2014)
Wachs et al. (2009, 2010)
Bulanon et al. (2009), Bulanon and Kataoka (2010)

Color + Texture
Color + Texture + Geometry
Color + 3D shape
Color + Texture + 3D shape
CAM + LM
I-component + a⁄-component
Color + Amplitude image
Color + Thermal image

CAM = Chromatic Aberration Map; LM = Luminance Map.

317

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323
Table 4
The main kinds of multi-modal images recognition algorithms.
Pattern recognition types

Classification algorithms

Fruits

Correct/error rate

Reference

Statistical pattern recognition

Linear decision classifier
Bayesian classifier
X-means clustering
PCA
Adaboost

Peach
Apple
Orange
Tomato
Citrus
Kiwifruit

>89%/NR
80%/3%
>75%/6%
88%/NR
75.3%/NR
>92.1%/7%

Sites and Delwiche (1988)
Bulanon et al. (2004)
Slaughter and Harrell (1989)
Yamamoto et al. (2014)
Kurtulmus et al. (2011a,b)
Zhan et al. (2013)

Fuzzy pattern recognition

FCM

Tomato

NR/16.55%

Wang et al. (2015)

Soft computing methods

Feed-forward neural network
Fuzzy neural network
SVM

Tomato
Apple
Pepper

95.45%/NR
>92.31%/NR
>74.2%/NR

Arefi and Motlagh (2013)
Ma et al. (2013)
Song et al. (2014)

applied color segmentation to multiple scene images to separate
potential regions from background and verify them first with texture analysis and then by reconstructing in the 3D space.
The recognition methods based on multi-sensor fusion technology have been used to fruit recognition (Bulanon et al., 2009;
Bulanon and Kataoka, 2010; Wachs et al., 2009, 2010; Feng et al.,
2014). For overcoming the challenge of recognizing green fruit in
the tree canopy, Bulanon et al. (2009) and Wachs et al. (2010) used
infrared images and visible images fusion to improve fruit detection. The infrared image captured by thermal infrared camera
and visible image captured by a color camera required image registration prior to image fusion. Maximization of the mutual information was employed by Wachs et al. (2009) to find the optimal
registration parameters for image fusion. The authors argued that
the recognition accuracy of fusion approach (74%) was increased
compared to the conventional approach of detection using either
color (66%) or IR (52%) alone. The amplitude image acquired by
ToF camera and H component image extracted from HSI color
space were selected as source images for fusion by Feng et al.
(2014). The fusion algorithm which is aimed at enhancing the fruit
object area distribution in the fused image could produce more
accurate and robust fruit recognition. A summary of the major
types of multi-modal based algorithms is given in Table 3.
4.3. Pattern recognition approaches
Pattern recognition approaches have long been investigated for
application in fruit recognition (see Table 4). Early in the 1970s,
Parrish and Goksel (1977) had suggested that pattern recognition
approaches could be used for fruit recognition. Two linear classification techniques including a non-parametric linear classifier and
linear decision function classifier were evaluated by Sites and
Delwiche (1988). The outcome indicated that both classification
algorithms produced similar results, and the non-parametric linear
classifier was easier to implement. Bulanon et al. (2004) also developed an apple detection method using the linear decision classifier
and the trichromatic coefficients as patterns. About 80% of fruit
pixels were correctly classified under all lighting conditions with
less than 3% error rate. Slaughter and Harrell (1989) used a
Bayesian classifier to discriminate oranges from the natural background. The classification model using chrominance and intensity
information could correctly classify over 75% of the fruit pixels.
In order to solve the overlapping problem in plantlets recognition,
Pastrana and Rath (2013) developed a novel pattern recognition
approach using an active shape model (ASM). Yamamoto et al.
(2014) applied the X-means clustering algorithm on the basis of
K-means clustering to determine the optimal number of clusters
and to detect individual fruit in a multi-fruit blob. Due to their similarities, fruit detection tasks can be conducted with the similar
method for face recognition and detection. Kurtulmus et al.
(2011a,b) used an ‘eigenfruit’ approach based on principle compo-

nent analysis (PCA) to detect green citrus under natural illumination. Zhan et al. (2013) used an Adaboost algorithm to recognize
the kiwifruit in field and achieved and ideal effect for the segmentation between kiwifruit and trunk, soil and branches. The Adaboost algorithm could combine the strengths of two weak
classifiers and mitigate their shortcomings. Zhao et al. (2016a,b)
also developed an algorithm combining AdaBoost classifier and
color analysis for the automatic detection of ripe tomatoes in
greenhouse. It argued that over 96% of ripe tomatoes were correctly detected.
These statistical pattern recognition approaches are developed
according to the posterior probability of the samples. Thus, more
and more attention are being paid on intelligent pattern recognition methods such as artificial neural networks (ANN), support
vector machine (SVM), and fuzzy pattern recognition. ANN and
SVM are supervised learning algorithms that have the ability to
learn from the data through an iterative training process and
improve its performance after each iteration. An olive recognition
method using neural networks was presented by Gatica et al.
(2013). The process of fruit recognition comprised of two stages:
the first stage focused on deciding whether or not the candidate
identified in the image corresponds to an olive fruit, the second
stage focused on olives overlapping within the tree canopy. Arefi
and Motlagh (2013) developed an experts system based on wavelet
transform and ANN for ripe tomato detection. Totally 90 wavelet
features were extracted from each tomato, and a feed-forward
neural network was used to distinguish the ripe tomato from its
background. An accuracy of 95.45% was obtained from the proposed recognition algorithm. In order to overcome the fuzziness
and uncertain factor existing in the color image boundary pixels,
a model combining quantum genetic algorithm and fuzzy neural
networks was built by Ma et al. (2013). An improved fuzzy neural
network could avoid redundant iteration and the tendency to fall
into the local minimum of traditional BP neural networks. Ji et al.
(2012) introduced a new classification algorithm based on support
vector machine to improve the apple recognition accuracy and efficiency. The new classifier had balanced the recognition success
rate and the time used in recognition. A statistical classifier, an
ANN and a SVM classifier were built and used for detecting peach
fruit by Kurtulmus et al. (2011a,b, 2014). Authors reported that
84.6%, 77.9% and 71.2% of the actual fruits were successfully
detected, using the three classifiers for the same validation set.
For improving the tomatoes identifying accurately, Song et al.
(2014) also used a bag-of-words (BoW) model to locate peppers
on the plant. The BoW model represented each image by a frequency distribution of its visual vocabularies, which was classified
to a fruit class. Wang et al. (2015) presented a Fuzzy Clustering
Means (FCM) algorithm to recognize the clustered tomatoes. The
superiority of this algorithm was verified according to a comparison with K-means and Otsu threshold. In order to accelerate the
computation of the traditional FCM, Xiong et al. (2013) presented

318

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

Table 5
The comparison of two types of eye-hand coordination control.
Eye-hand
coordination

Principle

Advantages

Limitations

Open-loop
visual
control

Hierarchical
controlling
based on
precision 3D
measuring

Control law is
simpler;
controllability and
region of stability
are better

Visual servo
control

Dynamic
interacting
between the
robot and
environment

Calibration is not
required; real-time
tracking is achieved
and it is object
friendly

Performance
depends on the
accuracy of
measurement,
assembly and
calibration
Local minima of
potential and
unexpected camera
trajectory

an improved FCM algorithm based on the fusion of the bicubic
interpolation algorithm and the FCM algorithm. The improved
recognition algorithm was available for litchi recognition which
enabled the recognition system to operate in real-time.
5. Eye-hand coordination in harvesting robot
Vision-based robot control has been investigated for more than
30 years. This technology promises substantial advantages when
working with the targets whose positions are unknown, or with
manipulators which may be inaccurate. Visual information can
be used for controlling the robotic manipulator or guiding its
motion. The two types of vision-based control applications in robot
control loop are called eye-hand coordination and visual navigation. Visual navigation has been widely applied (Khadraoui et al.,
1998), while the eye-hand coordination is a bottleneck to improve
the performance of harvesting robot. Thus, further developments
in vision-based control for harvesting robot are necessary. In this
section, an overview of the eye-hand coordination control in harvesting robot is given.
Traditionally, eye-hand coordination control systems were
based on an open loop control framework which employs the
‘‘looking then moving” mode of operation. The control precision
depends directly on the accuracy of the vision system, and the calibration of manipulator and assembly (Yau and Wang, 1996). An
alternative to increase the accuracy of these subsystems is to use
a visual feedback control loop (Corke and Hager, 1998). This particular vision-based robotic control mode is also called visual servo.
The visual servo is a framework to implement ‘‘looking and moving” as a dynamical system. The task of visual servo for harvesting
robot is to control the pose of the robot’s end-effector using image
features which are extracted from the image captured by the
camera-in-hand or fixed camera (Hashimoto, 2003). The comparison of these two types of eye-hand coordination control is given
in Table 5.

acquired by a stereo camera. The end-effector was first sent to a
location based on the X, Z position of the fruit and the Y (depth
information) of the cluster center. If the fruit could not be reached,
the end-effector advanced forward 50 mm (in Y direction) in the
next movement. Thus, the author argued that the harvesting success rate was affected by the accuracy of the calculated depth.
Inoue et al. (1996) also developed an open-loop visual control
scheme based on precise position detection for robotic harvesting.
The minimum distance gathering path to effectively touching the
target fruit could be calculated from this control model. In order
to improve the precision of position measurement, multiple visual
sensors are employed in the vision system. Hayashi et al. (2010)
developed a machine vision unit equipped with three aligned cameras to enhance the recognition rate. The center camera was used
to calculate the inclination of the peduncle, whereas the two cameras mounted on both sides of the center camera form a stereovision system to determine the 3D position of the fruit. Han et al.
(2012) also developed the strawberry harvesting robot based on
open-loop visual control. The 3D position of target strawberry
was acquired by a color stereoscope camera and a laser device.
The performance of the vision-based control scheme in field tests
showed that the execution time for successful harvest of a single
strawberry was less than 7 s.
In some cases, the fruit position in the tree canopy would be
influenced by wind or manipulator movement. When this situation
occurs, the efficiency of open-loop visual control for robotic harvesting is very low. Shen et al. (2011) have researched on the
increasing harvest efficiency of the fruit harvest robot in oscillatory
conditions. The oscillation frequency was obtained by curve fitting
and applying fast Fourier transform to video samples. With the calculated movement duration of the end-effector, it can eliminate
the time waiting for the oscillation to decay. Font et al. (2014) also
investigated a vision control strategy by combining open-loop
visual control and visual closed-loop control. A stereovision camera mounted on a robot arm could acquire the initial fruit location.
With the open-loop visual control, the grasper could move quickly
to the front of the target fruit. The final picking operation was conducted by iteratively adjusting the vertical and horizontal positions
of the gripper through closed-loop visual control. Aiming at solving
the positioning problem, Zou et al. (2012) developed a binocular
stereo vision system and position principle of the picking manipulator in virtual environment (VE). The stereo vision data was
mapped to the manipulator and was guided by accurate positioning in VE. The simulation results in VE could be applied to control
harvesting robot operation and to correct the positioning errors in
real-time.
5.2. Visual servo control
Compared to open-loop visual control, the input of visual servo
is continuous and contains dynamic image information. Therefore,
frame rates of the video must match the closed-loop bandwidths of

5.1. Open-loop visual control
The open-loop visual control mode is built for accurate positioning of the fruit object in 3D workspace. Therefore, vision systems of open loop control may consist of stereo vision or laser
range sensor that can measure the spatial distance between the
target fruit and the end-effector. Following a precision distance
measurement, the trajectory of the manipulator can be planned
through calculating the kinematics of the robot. Hence, the manipulator kinematic model and calibration of vision system have to be
very precise.
In the study of vision-based control for robotic harvesting of
cherry tomatoes (Kondo et al., 1996), a open-loop visual control
scheme was implemented based on the 3D position detection

Target

+

Error
Control law

Angle
Velocity
Acceleration

Motion
Manipulator

Video

Pose

Pose
estimation

Visual
processing
Visual feedback

Fig. 6. The structure of visual servo (Pan et al., 2000).

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

319

Camera in Hand

Tomato plant
Robot controller

Robot

Fixed Camera
Control signal

Field-of-view of CiH

Image signal

Industry Computer

Global view from Fixed camera

Fig. 7. Schematic diagram of a cooperative vision system for harvesting robot.

vision controllers. As shown in Fig. 6, pose estimation is the core
issue of visual feedback control. Input of the visual servo control
are the time-varying position errors between the target fruit and
end-effector. The advantages of visual servo are that its performance does not relay on the precise kinematic model of the robot
and the calibration of the vision system.
Mehta and Burks (2014) developed a visual servo system for
robotic citrus harvesting. As shown in Fig. 7, a cooperative vision
system consisting of a fixed camera and a camera-in-hand (CiH)
was incorporated such that the fixed camera and provided a global
view of a tree canopy. The CiH, due to its proximity, provided high
resolution fruit image. In contrast to the open-loop visual control
model the visual servo control strategy used perspective camera
geometry to obtain absolute range estimates. The estimated range
information can be used to generate a global map of fruit locations,
and a rotation controller was developed to orient the robot endeffector towards the target fruit such that the fruit could enter
the field of view of the CiH. The performance of the proposed visual
servo controller was demonstrated using a 7 DOF robotic manipulator in an artificial environment. Van Henten et al. (2003a,b, 2010)
introduced a novel eye-hand coordination approach based on the
A⁄-search algorithm which could assured collision–free motions
when the robot harvested cucumbers in a greenhouse. Eye-hand
coordination based on the A⁄-search algorithm had assured an
optimal motion path was obtained such that the cucumber picking
time can be reduced. Zhao et al. (2011) developed an apple harvesting robot consisting of a manipulator, end-effector and
image-based vision servo control system. In the visual control system, the image-based vision servo (IBVS) control method was
employed for localization and picking motion for the target fruit.
The IBVS was often used to control the manipulator according to
image features. The key issue of this method is how to calculate
the Jacobian matrix which describes the relationship between
camera coordinate and robot coordinate (Harrell et al., 1985).
Robot joint motion could be controlled based on feedback from

the position of a target fruit in an image. Vision servo was accomplished by controlling the velocities of each joint according to the
vertical and horizontal offsets of a fruit’s image position from the
image center (Harrell et al., 1990). Moreover, the closed-loop bandwidths of vision controllers could be varied from 1.0 to 1.1 Hz.
6. Examples of fruit harvesting robots
Robotics harvesting is not a new phenomenon but with the history of over 30 years. Currently, harvesting robots have not been
advanced to the commercialization stage because of their low efficiencies, low intelligence, and high costs. On the other hand, different designs of fruit harvesting robots have emerged in recent years.
Examples of major fruit harvesting robots are shown in Table 6. For
convenience, the examples of harvesting robot are categorized
according the types of target fruit or vegetable. The vision schemes
and eye-hand coordination models of the harvesting robots are
described in the table. The performances of different vision-based
control approaches applied in various harvesting robot are also
shown in the table.
7. Challenges and future trends
7.1. Enhancing the vision-based control of harvesting robot
With the development of robotic technology and sensor technology, enhancing the performance of fruit harvesting robot can
be a positive trend in meeting the challenges. Several suggestions
are given to improve the ability of the harvesting robot to deal with
the complex working environment (Bac et al., 2014).
Firstly, improvements in sensing are required. The sensors currently used have certain shortcomings in the application to fruit
recognizing (Gongal et al., 2015). Additionally, the combination
of multi-sensor may satisfy the performance required for fruit

320

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

Table 6
A comparison of the development of harvesting robots in the main countries.
Products

Robots

Vision scheme

Eye-hand coordination

Success
rate

Speed

Reference

Fruits

Apple harvesting robot

A camera-in-hand and positioning sensor

Open-loop visual
control
Open-loop visual
control
Visual servo control
Visual servo control
Visual servo control
Visual servo control
Visual servo control
Visual servo control
Open-loop visual
control
Open-loop visual
control
Visual servo control
Open-loop visual
control

77%

15s

Zhao et al. (2011)

>90%

7.1s

NR
50%
>85%
<41.3%
NR
100%
66.7%

<8s
36s
<22s
11.5s
7s
12s
NR

Bulanon and Kataoka
(2010)
Mehta et al. (2014)
Harrell et al. (1990)
Edan et al. (2000)
Hayashi et al. (2010)
Han et al. (2012)
Sakai et al. (2007)
Umeda et al. (1999)

NR

NR

Scarfe et al. (2009)

NR
66.7%

NR
14s

Kondo (1991)
Tankgaki et al. (2008)

Open-loop visual
control
Visual servo control
Visual servo control
Open-loop visual
control
Visual servo control
Visual servo control
Visual servo control

70%

3s-5s

Kondo et al. (1996)

88.6%
80%
74.4%

37.2s
45s
65.2s

NR
>80%
62.5%

<7s
6.7s
64.1s

Open-loop visual
control
Open-loop visual
control
Open-loop visual
control

79%

NR

Ji et al. (2014)
Van Henten et al. (2002)
Van Henten et al. (2003a,
b)
Foglia and Reina (2006)
Reed et al. (2001)
Hayashi et al. (2001,
2002)
Hemming et al. (2014)

NR

NR

Sakai et al. (2013)

82.22%

NR

Kohan et al. (2011)

A camera-in-hand and a laser range sensor
Citrus harvesting robot

Watermelon harvesting
robot

A fixed camera and a camera-in-hand
A fixed camera
A far-vision CCD and a near-vision CCD
A stereovision system and a central camera
A stereo camera, a camera and a laser sensor
A stereo vision sensor and a camera in hand
A stereo vision sensor

Kiwifruit harvesting robot

Eight cameras (four stereo vision systems)

Grape harvesting robot
Cherry harvesting robot

A camera-in-hand
A red and infrared laser active sensor

Tomato harvesting robot

A binocular stereo vision sensor

Cucumber harvesting robot

A stereo vision sensor
A fixed camera and a camera-in-hand
A near-infrared camera and a camera

Radicchio harvesting robot
Mushroom Harvesting robot
Eggplant harvesting robot

A camera-in-hand
A monochrome camera in hand
A camera-in-hand

Sweet-pepper harvesting
robot
Asparagus harvesting robot

Two ToF cameras, a stereo camera and a
camera
Two infrared laser sensors

Rosa harvesting robot

A stereo vision camera

Melon harvesting robot
Strawberry harvesting robot

Vegetables

detection and localization (Fernandez et al., 2013). Though given
the large number of articles that described a number of fruit recognition algorithms, the development of advanced image processing
algorithms is also a challenge for precision fruit recognition.
Recently, more and more attentions have been attracted by new
visual sensors such as ToF camera, light-field camera, and chlorophyll fluorescence camera. The core of the application considerations of these sensors is how to take advantage of the data
acquired.
On the other hand, vision control precision and efficiency for
fruit harvesting robot need to improve. There are many recent
works regarding eye-hand coordination for outdoor operation
robot (Mariottini et al., 2006). The control law of calibration-free
eye-hand coordination was described byHager et al. (1994). Base
on the auto disturbance rejection control (ADRC) strategy, Su proposed an advanced calibration-free eye-hand coordination which
has a strong adaptability and robustness (Su et al., 2004). Another
new visual servo control approach based on adaptive neural network has been applied for dynamic positioning of underwater
vehicles (Gao et al., 2015).
7.2. Human–machine collaboration
To date, the commercial application of fruit harvesting robot is
still unavailable because of lack of high efficiency and economic
justification (Edan, 1999). One of the new approaches to improve
the applicability of robotic harvesting is to combine human workers and robots synergistically. This approach for robotic harvesting
is to separate the fruit recognition stage from the harvest stage by
marking the target fruit a priori. In the Agribot project, a robot was
designed and built for a new aided-harvesting strategy, involving a
harmonic human–machine task distribution (Ceres et al., 1998). Ji

et al. (2014) introduced an assistant-mark approach to recognize
and locate the picking-point of the harvesting robot. Bechar and
Oren has defined and implemented the collaboration of a human
operator and robot applied to target fruit detection (Bechar and
Edan, 2003; Oren et al., 2012). Experimental results indicated that
the target recognition system based on human-robot collaboration
could increase the target fruit detection rate to 94% and reduce the
time needed by 20%.
7.3. Multi-arms cooperating for robotic harvesting
Another approach towards the goal of efficient robotic harvesting is the multi-arm robotic harvester. The idea is that a number of
manipulators are mounted on the mobile robot platform, and each
of the robotic arms is assigned a specific fruit to harvest. Zion from
Israel has designed a multi-arm melons harvesting robot which
enabled the maximum number of melons to be harvested (Zion
et al., 2014). According to the idea of multi-robot cooperation for
fruit harvesting, Noguchi et al. (2004) also proposed a master–
slave robot system for field operations. In this multiple robot system, a high level of autonomy on the robots was achieved to allow
them to cope with unexpected events and obstacles.
7.4. Making the environment more suitable for robotic harvesting
There are major technical challenges in automation due to the
uncontrolled environment in combination with the fact that the
harvest objects and materials are highly inconsistent in shape
and size. The harvesting robot working in the complex natural
environment requires a higher degree of skill and a wider range
of operating. In order to improve the efficiency of robotic harvesting in the future, collaboration among engineers and agronomists

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

is needed. Many engineers, working with agronomists, in the world
now have developed advanced robots (Burks et al., 2005; He et al.,
2013). Their solution for the problem of low harvest efficiency is
making environment robot more user friendly. Trees or plants
can be pruned to obtain suitable plant geometry for robotic harvesting and, hence, the picking cycle time can be reduced (Edan
et al., 1990).
8. Conclusions
In this paper, a broad overview of the development of vision
control technology applied in fruit harvesting robot is given. Vision
control for fruit harvesting robot includes two key elements, fruit
recognition and eye-hand coordination. For recognizing the target
fruit in the canopy, different types of visual sensors and image
analysis algorithms are equipped in the fruit harvesting robots.
Two-dimensional imaging data of the target fruit have been successfully acquired from monocular camera, hyperspectral imaging,
and thermal imaging. Three-dimensional surface reconstruction of
the target fruit requires data acquisition from binocular stereoscope or structured light sensor. There is a large difference in the
image processing algorithm between 2D imaging schemes and
3D imaging schemes. The scheme of fruit recognition for robotic
harvest may depend on the species of harvesting fruit. Moreover,
the performance of the fruit recognition system is also influenced
by many factors such as variable light, occlusions and many others.
Therefore, the reliability of recognition methods must consider the
environment in which the robot is working, and the proper selection of sensors.
In addition to the techniques reviewed, the development of eyehand coordination control for fruit harvesting robot is also
described in this paper. We have classified eye-hand coordination
into two main categories according to whether the visual control is
open-loop or close-loop. There is a large difference in the control
principle between open-loop visual control mode and visual servo.
The input image of open-loop visual control is a static image. However, the input of visual servo is the video image where frame rates
of the video must match the closed-loop bandwidths of vision controllers. The performance of the open-loop visual control relies on
the precision calibration of the camera and manipulators. The fruit
harvesting robot adopting visual servo can arrive at certain control
precision without calibration. With the advancement of current
imaging technologies and the development of new control algorithms, more information will be available to aid recognizing the
target fruit and speeding up fruit picking.
Acknowledgments
Financial support from the National High-Tech R&D Program of
China (863 Program No. 2013AA102307) and the National Key
Technology R&D Programs (2014BAD08B01 and 2015BAF12B02)
are gratefully acknowledged.
References
Arefi, A., Motlagh, A.M., Mollazade, K., Teimourlou, R.F., 2011. Recognition and
localization of ripen tomato based on machine vision. Aust. J. Crop Sci. 5 (10),
1144–1149.
Arefi, A., Motlagh, A.M., 2013. Development of an expert system based on wavelet
transform and artificial neural networks for the ripe tomato harvesting robot.
Aust. J. Crop Sci. 7 (5), 699–705.
Baeten, J., Donne, K., Boedrij, S., Bechers, W., Claesen, E., 2008. Autonomous fruit
picking machine: a robotic apple harvester. 6th International Conference on
Field and Service Robotics, vol. 42, pp. 531–539.
Bac, C.W., Heten, E.J.V., Hemming, J., 2014. Harvesting robots for high-value crops:
state-of-the-art review and challenges ahead. J. Field Robot 31 (6), 888–991.
Bechar, A., Edan, Y., 2003. Human-robot collaboration for improved target
recognition of agricultural robots. Ind. Robot 30 (5), 432–436.

321

Buemi, F., Massa, M., Sandini, G., 1995. Agrobot: a robotic system for greenhouse
operations. Robotics Agric. Food Ind. 4, 172–184.
Buemi, F., Massa, M., Sandini, G., Costi, G., 1996. The Agrobot project. Adv. Space Res.
18, 185–189.
Bulanon, D.M., Burks, T.F., Alchanatis, V., 2008. Study on temporal variation in citrus
canopy using thermal imaging for citrus fruit detection. Biosyst. Eng. 101 (2),
161–171.
Bulanon, D.M., Burks, T.F., Alchanatis, V., 2009. Image fusion of visible and thermal
images for fruit detection. Biosyst. Eng. 103, 12–22.
Bulanon, D.M., Kataoka, T., Okamoto, H., Hata, S., 2004. Development of a real-time
machine vision system for apple harvesting robot. SICE Ann. Conf.,
595–598
Bulanon, D.M., Kataoka, T., 2010. A fruit detection system and an end effector for
robotic harvesting of Fuji apples. Agric. Eng. Int.: CIGR J. 2010 (7), 1–14.
Burks, T., Villegas, F., Hannan, M., 2005. Engineering and horticultural aspects of
robotic harvesting: opportunities and constraints. Horttechnology 15 (1), 79–
87.
Ceres, R., Pons, J.L., Jimenez, A.R., Martin, J.M., Calderon, L., 1998. Design and
implementation of an aided fruit-harvesting robot. Ind. Robot 25 (5), 337–346.
Chaivivatrakul, S., Dailey, M.N., 2014. Texture-based fruit detection. Precis. Agric. 15
(6), 662–683.
Corke, P.I., Hager, G.D., 1998. Vision-based robot control. Control Problems Robotics
Automat., 177–192
d’Esnon, G.A., Rabatel, G., Pellenc, R., Joumeau, A., Aldon, M.J., 1987. MAGALI: a selfpropelled robot to pick apples. In: Proceedings of American Society of
Agricultural Engineers, Baltimore, Maryland.
Edan, Y., Flash, T., Shmulevich, I., 1990. An algorithm defining the motions of a citrus
picking robot. J. Agr. Eng. Res. 46, 259–273.
Edan, Y., Gaines, E., 1994. Systems engineering of agricultural robot design. IEEE
Trans. Syst. Man, Cybern. 24 (8), 1259–1265.
Edan, Y., 1999. Food and agriculture robotic. Handbook of Industrial Robotic. Second
edition. Chapter 60, 1143–1155.
Edan, Y., Rogozin, D., Flash, T., 2000. Robotic melon harvesting. IEEE Trans. Rob.
Autom. 16 (6), 831–834.
Edan, Y., Han, S.F., Kondo, N., 2009. Automation in agriculture. Springer Handbook of
Automation, Part G, 1095–1128.
Feng, J., Zeng, L.H., Liu, G., 2014. Fruit recognition algorithm based on multi-source
images fusion. Trans. CSAM 45 (2), 73–80.
Fernandez, R., Salinas, C., Montes, H., Sarria, J., Armada, M., 2013. Validation of a
multisensory system for fruit harvesting robots in lab conditions. First Iberian
Robotics Conf. Adv. Intell. Syst. Comput. 252, 495–504.
Foglia, M.M., Reina, G., 2006. Agricultural robot for radicchio harvesting. J. Field
Robot 23, 363–377.
Font, D., Palleja, T., Tresanchez, M., Rucan, D., Moreno, J., Martinez, D., Teixido, M.,
Palacin, J.T., 2014. A proposal for automatic fruit harvesting by combining a low
cost stereovision camera and a robotic arm. Sensors 14, 11557–11579.
Gao, J., Proctor, A., Bradley, C., 2015. Adaptive neural network visual servo control
for dynamic positioning of underwater vehicles. Neurocomputing 167, 604–
613.
Gatica, G., Best, S., Ceroni, J., Lefranc, G., 2013. Olive fruits recognition using neural
networks. Procedia Comput. Sci. 17, 412–419.
Goncalves, P.J.S., Torres, M.B., 2010. Learning approaches to visual control of robotic
manipulators. In: The Second International Conference on Advanced Cognitive
Technologies and Applications, pp. 103–108, Lisbon, Portugal, 21-26 November.
Gongal, A., Amatya, S., Karkee, M., 2015. Sensors and systems for fruit detection and
localization: a review. Comput. Electr. Agr. 116, 8–19.
Gotou, K., Fujiura, T., Nishiura, Y., 2003. 3-D vision system of tomato production
robot. Int. Conf. Adv. Intell. Mech., 1210–1215
Grift, T., Zhang, Q., Kondo, N., Ting, K.C., 2008. A review of automation and robotics
for the bio-industry. J. Biomech. Eng. 1 (1), 37–54.
Hager, G.H., Chang, W.C., Morse, A.S., 1994. Robot feedback control based on stereo
vision: towards calibration-free hand-eye coordination. IEEE Int. Conf. Robotic
Automat., 2850–2856
Han, K.S., Kim, S.C., Lee, Y.B., Kim, S.C., Im, D.H., Choi, H.K., Hwang, H., 2012.
Strawberry harvesting robot for bench-type cultivation. Biosyst. Eng. 37 (1), 65–
74.
Hannan, M.W., Burks, T.F., 2004. Current developments in automated citrus
harvesting. In ASAE/CSAE Annual International Meeting. St. Joseph, MI. pp.
043827.
Hannan, M.W., Burks, T.F., Bulanon, D.M., 2009. A machine vision algorithm
combining adaptive segmentation and shape analysis for orange fruit detection.
Agric. Eng. Int.: CIGR J. 6, 1–17.
Harrell, R.C., Adsit, P.D., Slaughter, D.C., 1985. Real-time vision-servoing of a robotic
tree fruit harvester. Trans. ASAE 85–3550, 1–15.
Harrell, R.C., Adsit, P.D., Munilla, R.D., 1990. Robotic picking of citrus. Robotica 8,
269–278.
Hashimoto, K., 2003. A review on vision-based control of robot manipulators. Adv.
Robotics 17 (10), 969–991.
Hayashi, S., Ganno, K., Ishii, Y., 2001. Development of a harvesting end-effector for
eggplants. SHITA 13 (2), 97–103.
Hayashi, S., Ganno, K., Ishii, Y., Tanaka, I., 2002. Robotic harvesting system for
Eggplants. JARQ 36 (3), 163–168.
Hayashi, S., Ota, T., Kubota, K., Ganno, K., Kondo, N., 2005. Robotic harvesting
technology for fruit vegetables in protected horticultural production. In:
Information and Technology for Sustainable Fruit and Vegetable Production,
pp. 227–236, 12-16 September.

322

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323

Hayashi, S., Shigematsu, K., Yamamoto, S., Kobayashi, K., Kohno, Y., Kamata, J.,
Kurita, M., 2010. Evaluation of a strawberry-harvesting robot in a field test.
Biosyst. Eng. 105, 160–171.
He, L., Zhang, Q., Charvet, H.J., 2013. A kont-tying end-effector for robotic hop
twining. Biosyst. Eng. 114, 334–350.
Hemming, J., Bac, C.W., Tuijl, B.A.J.V., 2014. A robot for harvesting sweet-pepper in
greenhouses. In: 2014 International Conference of Agricultural Engineering, pp.
1–8, Zurich, Switzerland.
Huang, L.W., He, D.J., 2012. Ripe Fuji apple detection model analysis in natural tree
canopy. TELKOMNIKA 10 (7), 1771–1778.
Inoue, S., Ojika, T., Harayama, M., Kobayashi, T., Imai, T., 1996. Cooperated operation
of plural hand-robots for automatic harvest system. Math. Comput. Simulat. 41,
357–365.
Ji, C., Zhang, J., Yuan, T., Li, W., 2014. Research on key technology of truss tomato
harvesting robot in greenhouse. AMM 442, 480–486.
Ji, W., Zhao, D., Cheng, F.Y., 2012. Automatic recognition vision system guided for
apple harvesting robot. Comput. Electr. Agric. 38, 1186–1195.
Jimenez, A.R., Ceres, R., Pons, J.L., 2000a. A machine vision system based on a laser
rang-finder applied to robotic fruit harvesting. Mach. Vision Appl. 11, 321–329.
Jimenez, A.R., Ceres, R., Pons, J.L., 2000b. A survey of computer vision methods for
locating fruit on trees. Trans. ASAE 43 (6), 1911–1920.
Kane, K.E., Lee, W.S., 2007. Multispectral imaging for in-field green citrus
identification. In: ASABE Annual International Meeting, St. Joseph, MI. Paper
Number: 073025.
Kelman, E., Linker, R., 2014. Vision-based localization of mature apples in tree
images using convexity. Biosyst. Eng. 2014 (118), 174–185.
Khadraoui, D., Debain, C., Rouveure, R., Martinet, P., Bonton, P., Gallice, J., 1998.
Vision-based control in driving assistance of agricultural vehicles. Int. J. Rob.
Res. 17 (10), 1040–1054.
Kohan, A., Borghaee, A.M., Yazdi, M., Minaei, S., Sheykhdavudi, M.J., 2011. Robotic
harvesting of rosa damascene using stereoscopic machine vision. WASJ 12 (2),
231–237.
Kondo, N., 1991. Study on grape harvesting robot. Mathematical and Control
Applications in Agriculture and Horticulture, a volume in IFAC Workshop Series,
pp. 243–246.
Kondo, N., Nishitsuji, Y., Ling, P.P., 1996. Visual feedback guided robotic cherry
tomato harvesting. Trans. ASAE 39 (6), 2331–2338.
Kurtulmus, F., Lee, W.S., Vardar, A., 2011a. Green citrus detection using ‘eigenfruit’,
color and circular Gabor texture features under natural outdoor conditions.
Comput. Electr. Agr. 78, 140–149.
Kurtulmus, F., Lee, W.S., Vardar, A., 2011b. An advanced green citrus detection
algorithm using color images and neural networks. J. Agric. Mach. Sci. 7 (2),
145–151.
Kurtulmus, F., Lee, W.S., Vardar, A., 2014. Immature peach detection in colour
images acquired in natural illumination conditions using statistic classifiers and
neural network. Precis. Agric. 15, 57–79.
Li, L., Zhang, Q., Huang, D.F., 2014. A review of imaging techniques for plant
phenotyping. Sensors 14, 20078–20111.
Li, P.L., Lee, S.H., Hsu, H.Y., 2011a. Study on citrus fruit image data separability by
segmentation methods. Procedia Eng. 23, 408–416.
Li, P.L., Lee, S.H., Hsu, H.Y., 2011b. Review on fruit harvesting method for potential
use of automatic fruit harvesting systems. Procedia Eng. 23, 351–366.
Liu, J.Z., Li, P.P., Li, Z.G., 2007. A multi-sensory end-effector for spherical fruit
harvesting robot. Int. Conf. Automat. Logist., 258–262
Lu, J., Sang, N., Hu, Y., 2014. Detecting citrus fruits with highlight on tree based on
fusion of multi-map. Optics 125, 1903–1907.
Ma, X.D., Liu, G., Zhou, W., 2013. Apple recognition based fuzzy neural network and
quantum genetic algorithm. Trans. CSAM 44 (12), 227–232.
Manolakis, D., Marden, D., Shaw, G.A., 2003. Hyperspectral image processing for
automatic target detection applications. Lincoln Lab. J. 14 (1), 79–116.
Mao, W.H., Ji, B.P., Zhan, J.C., Zhang, X.C., Hu, X.A., 2009. Apple location method for
the apple harvesting robot. In: 2nd International Congress on Image and Signal
Processing, pp. 17–19.
Mariottini, G.L., Prattichizzo, D., Oriolo, G., 2006. Image-based visual servoing for
nonholonomic mobile robots with central catadioptric camera. Int. Conf.
Robotics Automat., 538–544
Mehta, S.S., Burks, T.F., 2014. Vision-based control of robotic manipulator for citrus
harvesting. Comput. Electr. Agric. 102, 146–158.
Mehta, S.S., MacKunis, W., Burks, T.F., 2014. Nonlinear robust visual servo control
for robotic citrus harvesting. The 19th world congress of the International
Federation of Automatic. Control, 8110–8115.
Noguchi, N., Will, J., Reid, J., Zhang, Q., 2004. Development of a master-slave robot
system for farm operations. Comput. Electr. Agr. 44, 1–19.
Okamoto, H., Lee, W.S., 2009. Green citrus detection using hyperspectral imaging.
Comput. Electr. Agr. 66, 201–208.
Oren, Y., Bechar, A., Edan, Y., 2012. Performance analysis of a human-robot
collaborative target recognition system. Robotica 30, 813–826.
Pan, Q.L., Su, J.B., Xi, Y.G., 2000. Uncalibrated 3D robotic visual tracking based on
stereo vision. ROBOT 22 (4), 293–299.
Parrish, E.A., Goksel, J.A.K., 1977. Pictorial pattern recognition applied to fruit
harvesting. Trans. ASAE 20 (5), 822–827.
Pastrana, J.C., Rath, T., 2013. Novel image processing approach for solving the
overlapping problem in agriculture. Biosyst. Eng. 115, 106–115.
Patel, H.N., Jain, R.K., Joshi, M.V., 2011. Fruit Detection using improved Multiple
Features based Algorithm. IJCA 13 (2), 1–5.

Rakun, J., Stajnko, D., Zazula, D., 2011. Detecting fruits in natural scenes by using
spatial-frequency based texture analysis and multiview geometry. Comput.
Electr. Agric. 76 (1), 80–88.
Reed, J.N., Miles, S.J., Butler, J., 2001. Automatic mushroom harvester development.
J. Agric. Eng. Res. 78 (1), 15–23.
Reis, M.J.C.S., Morais, R., Peres, E., Pereira, C., Contente, O., Soares, S., Valente, A.,
Baptista, J., Ferreira, P.J.S.G., Cruz, J.B., 2012. Automatic detection of bunches of
grapes in natural environment from color images. J. Appl. Logic. 10, 285–290.
Safren, Q., Alchanatis, V., Ostrovsky, V., Levi, O., 2007. Detection of green apples in
hyperspectral images of apple-tree foliage using machine vision. Trans. ASABE
50 (6), 2303–2313.
Sakai, H., Shiigi, T., Kondo, N., Ogawa, Y., 2013. Accurate Position Detecting during
asparagus spear harvesting using a laser sensor. EAEF 6 (3), 105–110.
Sakai, S., Osuka, K., Maekwaw, T., Umeda, M., 2007. Robust control systems of a
heavy material handling agricultural robot: a case study for initial cost problem.
IEEE Trans. Control Syst. Tchnol. 15 (6), 1038–1048.
Scarfe, A.J., Flemmer, R.C., Bakker, H.H., Flemmer, C.L., 2009. Development of an
autonomous kiwifruit picking robot. In: The 4th International Conference on
Autonomous Robots and Agents, pp. 380–384, Wellington, New Zealand.
Schertz, C.E., Brown, G.K., 1968. Basic considerations in mechanizing citrus harvest.
Trans. ASAE, 343–346.
Shen, H.L., Zhao, D., Ji, W., 2011. Research on the strategy of advantage of advancing
harvest efficiency of fruit harvest robot in the oscillation conditions. Third Int.
Conf. Intell. Human-Mach. Syst. Cybernet. 215–218.
Shinsuke, K., Koichi, O., 2005. Recognition and cutting system of sweet pepper
picking robot in greenhouse horticulture. In: Proceedings of the IEEE
International Conference on Mechatronics & Automation, pp. 1807–1812,
Nigara Falls, Ontario, Canada, 29 July-1 August.
Shirai, Y., Inoue, H., 1973. Guiding a robot by visual feedback in assembling tasks.
Patt. Recogn. 5, 99–108.
Si, Y.S., Liu, G., Feng, J., 2015. Location of apples in trees using stereoscopic vision.
Comput. Electr. Agric. 112 (3), 68–74.
Sistler, F.E., 1987. Robotics and intelligent machines in agriculture. IEEE J. Robotic
Autom. 3 (1), 3–6.
Sites, P.W., Delwiche, M.J., 1988. Computer vision to locate fruit on a tree. Trans.
ASAE 31 (1), 257–263.
Slaughter, D.C., Harrell, R.C., 1987. Color vision in robotic fruit harvesting. Trans.
ASAE 30 (4), 1144–1148.
Slaughter, D.C., Harrell, R.C., 1989. Discriminating fruit for robotic harvest using
color in natural outdoor scenes. Trans. ASAE 32 (2), 757–763.
Song, Y., Glasbey, C.A., Horgan, G.W., 2014. Automatic fruit recognition and
counting from multiple images. Biosyst. Eng. 18, 203–215.
Stajnko, D., Lakota, M., Hocevar, M., 2004. Estimation of number and diameter of
apple fruits in an orchard during the growing season by thermal imaging.
Comput. Electron. Agric. 42 (1), 31–42.
Su, J.B., Qiu, W.B., Ma, H.Y., 2004. Calibration-free robotic eye-hand coordination
based on an auto disturbance-rejection controller. IEEE Trans. Robot. 20 (5),
889–907.
Sun, J., Lu, B., Mao, H.P., 2011. Fruits recognition in complex background using
binocular stereovision. J. Jiangsu Univ. Nat. Sci. Ed. 32 (4), 423–427.
Tankgaki, K., Fujiura, T., Akase, A., 2008. Cherry-harvesting robot. Comput. Electr.
Agric. 63, 65–72.
Umeda, M., Kubota, S., Iida, M., 1999. Development of ‘‘STORK”, a watermelonharvesting robot. Artif. Life Robot. 3, 143–147.
Vadivambal, R., Jayas, D.S., 2001. Applications of thermal imaging in Agriculture and
food industry-a review. Food Bioprocess Tech. 4, 186–199.
Van Henten, E.J., Hemming, J., Tuijl, B.A.J.V., 2002. An autonomous robot for
harvesting cucumbers in greenhouses. Auton. Robot. 13 (3), 241–258.
Van Henten, E.J., Hemming, J., Tuijl, B.A.J.V., 2003a. Collision-free motion planning
for a cucumber picking robot. Biosyst. Eng. 86 (2), 135–144.
Van Henten, E.J., Tuijl, B.A.J.V., Hemming, J., 2003b. Field test of an autonomous
cucumber picking robot. Biosyst. Eng. 86 (3), 305–313.
Van Henten, E.J., Schenk, E.J.L., Willigenburg, G.V., 2010. Collision-free inverse
kinematics of the redundant seven-link manipulator used in a cucumber
picking robot. Biosyst. Eng. 106, 112–124.
Wachs, J.P., Stern, H.I., Burks, T., 2009. Apple detection in natural tree canopies from
multimodal image. In: 6th European Conference on Precision Agriculture, pp.
293–302.
Wachs, J.P., Stern, H.I., Burks, T., 2010. Low and high-level visual feature-based
apple detection from multi-modal images. Precis. Agric. 11, 717–735.
Wang, F.C., Xu, Y., Song, H.B., 2015. Study on identification of tomatoes based on
fuzzy clustering algorithm. J. Agric. Mech. Res. 10, 24–28.
Wei, X.Q., Kun, J., Jin, H.L., 2014. Automatic method of fruit object extraction under
complex agricultural background for vision system of fruit picking robot. Optics
125 (12), 5684–5689.
Whittaker, A.D., Miles, G.E., Mitchell, O.R., 1987. Fruit location in a partially
occluded image. Trans. ASAE 30 (3), 591–596.
Xiang, R., Jiang, H.Y., Ying, Y.B., 2014. Recognition of clustered tomatoes based on
binocular stereo vision. Comput. Electr. Agric. 106 (8), 75–90.
Xie, Z.Y., Zhang, T.Z., Zhao, J.Y., 2007. Ripened strawberry recognition based on
Hough transform. Trans. CSAM 38 (3), 106–109.
Xie, Z.H., Ji, C.Y., Guo, X.Q., 2010. An object detection method for quasi-circular
fruits based on improved Hough transform. Trans. CSAE 26 (7), 157–162.
Xie, Z.H., Ji, C.Y., Guo, X.Q., 2011. Detection and location algorithm for overlapped
fruits based on concave spots searching. Trans. CSAE 42 (12), 191–196.

Y. Zhao et al. / Computers and Electronics in Agriculture 127 (2016) 311–323
Xiong, J.T., Zou, X.J., Wang, H.J., 2013. Recognition of ripe litchi in different
illumination conditions based on Retinex image enhancement. Trans. CSAE 29
(12), 170–178.
Xu, H.R., Ying, Y.B., 2004. Detection citrus in a tree canopy using infrared thermal
imaging. SPIE 5271, 321–327.
Yamamoto, K., Guo, W., Yoshioka, Y., Ninomiya, S., 2014. On plant detection of intact
tomato fruits using image analysis and machine learning methods. Sensors 14,
12191–12206.
Yang, L., Dickinson, J., Wu, Q.M.J., 2007. A fruit recognition method for automatic
harvesting. In: 14th International Conference on Mechatronics and Machine
Vision in Practice, pp. 152–157.
Yau, W.Y., Wang, H., 1996. Robust hand-eye coordination. Adv. Robotic 11 (1),
57–73.
Yin, H.P., Chai, Y., Yang, S.X., 2009a. Ripe tomato recognition and localization for a
tomato harvesting robotic system. Int. Conf. Soft Comput. Patter Recogn.
557–562.
Yin, H.P., Chai, Y., Yang, S.X., 2009b. Ripe tomato extraction for a harvesting robotic
system. In: Proceedings of IEEE International Conference on System, Man, and
Cybernetics, pp. 2984–2989.
Yuan, T., Ji, C., Chen, Y., Li, W., Zhang, J.X., 2011. Greenhouse cucumber recognition
based on spectral imaging technology. Trans. CSAM S1, 172–176.
Zhan, W.T., He, D.J., Shi, S.L., 2013. Recognition of kiwifruit in field based on
adaboost algorithm. Trans. CSAE 29 (23), 140–146.

View publication stats

323

Zhang, B.H., Huang, W.Q., Li, J.B., 2014. Principles, developments and applications of
computer vision for external quality inspection of fruits and vegetables: a
review. Food Res. Int. 62, 326–343.
Zhang, B.H., Huang, W.Q., Wang, C.P., 2015. Computer vision recognition of stem
and calyx in apples using near-infrared linear-array structured light and 3D
reconstruction. Biosyst. Eng. 139 (12), 25–34.
Zhao, D.A., Lv, J.D., Ji, W., 2011. Design and control of an apple harvesting robot.
Biosyst. Eng. 110 (2), 112–122.
Zhao, J., Tow, J., Katupitiya, J., 2005. On-tree fruit recognition using texture
properties and color data. In: IEEE/RSJ International Conference on Intelligent
Robots and Systems, pp. 263–268. Aug.
Zhao, Y.S., Gong, L., Huang, Y.X., Liu, C.L., 2016a. Robust tomato recognition for
robotic harvesting using feature images fusion. Sensors 16 (2), 173–185.
Zhao, Y.S., Gong, L., Zhou, B., Huang, Y.X., Liu, C.L., 2016b. Detecting tomatoes in
greenhouse scenes by combining AdaBoost classifier and colour analysis.
Biosyst. Eng. 148 (8), 127–137.
Zion, B., Mann, M., Levin, D., Shilo, A., Rubinstein, D., Shmulevich, I., 2014.
Harvested-order planning for a multiarm robotic harvester. Comput. Electr.
Agric. 103, 75–81.
Zou, X.J., Zou, H.X., Lu, J., 2012. Virtual manipulator-based binocular stereo vision
positioning system and errors modelling. Mach. Vision Appl. 23, 43–63.

Applied Mathematics Letters 61 (2016) 26–34

Contents lists available at ScienceDirect

Applied Mathematics Letters
www.elsevier.com/locate/aml

A note on Monge–Ampère Keller–Segel equation
Hui Huang a,b,∗ , Jian-Guo Liu b
a
b

Department of Mathematical Sciences, Tsinghua University, Beijing, 100084, People’s Republic of China
Departments of Physics and Mathematics, Duke University, Durham, NC 27708, USA

article

info

Article history:
Received 8 April 2016
Received in revised form 5 May 2016
Accepted 5 May 2016
Available online 13 May 2016
Keywords:
Chemotaxis
Polar factorization
Convex potential
Brenier map
Global existence

abstract
This note studies the Monge–Ampère Keller–Segel equation in a periodic domain
Td (d ≥ 2), a fully nonlinear modification of the Keller–Segel equation where the
Monge–Ampère equation det(I + ∇2 v) = u + 1 substitutes for the usual Poisson
equation ∆v = u. The existence of global weak solutions
 is obtained for this modified

equation. Moreover, we prove the regularity in L∞ 0, T ; L∞ ∩ W 1,1+γ (Td ) for
some γ > 0.
© 2016 Elsevier Ltd. All rights reserved.

1. Introduction
Keller–Segel (KS) model was firstly presented in 1970 to describe the chemotaxis of cellular slime molds [1].
The original model was considered in 2-dimension,

2

∂t u = △u + ∇ · (u∇v), x ∈ R , t > 0,
(1)
∆v = u(t, x),

u(0, x) = u (x).
0
In the context of biological aggregation, u(t, x) represents the bacteria density, and v(t, x) represents the
chemical substance concentration.
In this note, we study the Monge–Ampère Keller–Segel (MAKS) model in a periodic domain Td = Rd /Zd
(d ≥ 2):

d

∂t u = ∆u + ∇ · (u∇v), x ∈ T , t > 0,
(2)
det(I + ∇2 v) = u + 1,

u(0, x) = u (x).
0
∗ Corresponding author at: Department of Mathematical Sciences, Tsinghua University, Beijing, 100084, People’s Republic of
China.
E-mail addresses: huanghui12@mails.tsinghua.edu.cn (H. Huang), jliu@phy.duke.edu (J.-G. Liu).

http://dx.doi.org/10.1016/j.aml.2016.05.003
0893-9659/© 2016 Elsevier Ltd. All rights reserved.

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 26–34

27

where I is the identity matrix. In the absence of ∆u term in (2), this model was introduced by Brenier
[2, (5.34), (5.36)] as a fully nonlinear version of popular models in chemotaxis theory, such as the celebrated
Keller–Segel model or similar models in astrophysics. We will prove the global existence of weak solutions
to MAKS model (2) in a weak sense, which is made precise in Section 2.
Monge–Ampère Keller–Segel system (2) is an approximation of the original KS system (1) in the following
re-scaling. Let us recast the equation (2) by introducing the new unknowns:




1
t x
t x
uδ (t, x) = u
,√
;
v δ (t, x) = v
,√
.
δ
δ
δ
δ
δ
Then we have
√
u(t, x) = δuδ (δt, δx);

√
v(t, x) = v δ (δt, δx).

Moreover, these new unknowns should be governed by the following MAKS system

∂t uδ = ∆uδ + ∇ · (uδ ∇v δ ),
det(I + δ∇2 v δ ) = 1 + δuδ .

(3)

We formally linearize the determinant det(I + δ∇2 v δ ) around the identity matrix and obtain
1 + δuδ = det(I + δ∇2 v δ ) = 1 + δ∆v δ + O(δ 2 ).

(4)

Then the Monge–Ampère equation turns into the Poisson equation ∆v δ = uδ + O(δ), from which, when we
set O(δ) = 0, we recognize the MAKS system (3) as the original KS system showed in (1).
The density u in the original KS system (1) is driven by the gradient of Newtonian potential ∇v = ∇N ∗u,
where N is the fundamental solution of Laplacian equation, and potential v has the superposition principle
relation with u. Moreover, it has an important property: if 0 ≤ u ∈ L∞ (Td ), then ∇v is log-Lipschitz
continuous. However, for MAKS model (2), the Newtonian potential is replaced by a convex potential V [u]
discovered by Brenier [3]. The advantage is that ∇v = ∇V [u] − x is globally convex and has uniform L∞
bound if 0 ≤ u ∈ L1 (Td ). But the convex potential will lose the superposition principle relation with the
density.
There are many mathematical models involved substituting the fully nonlinear Monge–Ampère equation
for the Poisson equation. For example, the semigeotrophic equations in meteorology have a long history.
After suitable changes of variables, they can be reformulated as a coupled Monge–Ampère/transport
problem [4], which appear as a variant of the two-dimensional incompressible Euler equations in vorticity
form, where the Poisson equation that relates to the stream function and the vorticity field is replaced by
the Monge–Ampère equation [4–7]. Moreover, in [8], Brenier and Loeper studied the Vlasov–Monge–Ampère
system, a fully non-linear version of the Vlasov–Poisson system. Similarly, Brenier [9], by substituting the
Monge–Ampère equation for the linear Poisson equation to model gravitation, he introduced a modified
Zeldovich approximate model related to the early universe reconstruction problem.
2. The polar decomposition theorem
The polar factorization of maps has been discovered by Brenier [3]. It was later extended to the general
case of Riemannian manifolds by McCann in [10].
−
−
−
Let us consider a mapping X : Rd → Rd such that for all →
p ∈ Zd , X(· + →
p) = X +→
p . We use the
d
push-forward of Lebesgue measure of R by X, and it is denoted by u = X♯ dx. Then u is a probability
measure on Td and we have the following theorem:
Theorem 1 (Theorem 1.2 [6]). Let X : Rd → Rd be described as above with u = X♯ dx.

28

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 26–34

1. Up to a constant, there exists a unique convex function V [u] such that V [u] − x2 /2 is Zd -periodic (and
thus ∇V [u] − x is Zd -periodic), and


∀ϕ ∈ C 0 (Td ),


ϕ(∇V [u](x))dx =

Td

ϕ(x)du(x).

(5)

Td

2. Let V [u] be the Legendre transform of V [u]. If u is Lebesgue integrable, then V [u] is a convex function
satisfying that V [u] − x2 /2 is Zd -periodic (and thus ∇V [u] − x is Zd -periodic), unique up to a constant,
and
0



d



∀ϕ ∈ C (T ),

ϕ(∇V [u](x))du(x) =
Td

Moreover we have the bound ∥∇V [u] − x∥L∞ (Td ) ≤

ϕ(x)dx.

(6)

Td

√

d/2.

Link with the Monge–Ampère equation. We can interpret (5) as a weak version of the Monge–
Ampère equation
u(∇V ) det ∇2 V = 1,

(7)

and (6) can be seen as a weak version of another Monge–Ampère equation
det ∇2 V = u.

(8)

Moreover, we will also use the following result originally from [3]. The first one establishes the continuity
of the polar decomposition.
Theorem 2 (Theorem 2.6 [6]). Let un be a sequence of Lebesgue integrable positive measures on Td , such that

for all n, Td dun ≤ C and let Vn = V [un ], Vn = V [un ] be as defined in Theorem 1. If for any ϕ ∈ C 0 (Td )


such that ϕdun converges to ϕdu, then the sequence Vn can be chosen in such a way that Vn converges
to V [u] uniformly on Td and strongly in W 1,1 (Td ), and Vn converges to V [u] uniformly on Td and strongly
in W 1,1 (Td ).
Theorem 1 allows us to recast MAKS equation (2) as
∂t u = ∆u + ∇ · (u(∇V [u + 1] − x)),

x ∈ Td , t > 0,

u(0, x) = u0 (x).

(9a)
(9b)

where V [u + 1] is as defined in Theorem 1. For simplicity, we denote V [u + 1] as V [u].
Remark 1. If u is continuous and satisfies 0 ≤ u ≤ C1 , it has been proved in [11] that ∇V [u](x) is
log-Lipschitz continuous. The log-Lipschitz continuity usually ensures the uniqueness and stability in the
Wasserstein distance. Moreover, according to [8, Theorem 4.4], if u ∈ C α (Td ), α ∈ (0, 1), then V [u] is a
classical solution of
det ∇2 V [u] = u + 1.

(10)

3. Existence of global weak solutions
To begin this section, we give the following definition of the weak solution to the MAKS equation (9).

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 26–34

29

Definition 1. Let initial data 0 ≤ u0 ∈ L1 (Td ). Then (u, V ) is a global weak solution to (9) if it satisfies for
any T > 0:






1. u ∈ L∞ 0, T ; L1 (Td ) ∩ L2 0, T ; H 1 (Td ) and ∂t u ∈ L2 0, T ; H −1 (Td ) .


2. ∀ ϕ ∈ Cc∞ [0, T ) × Td ,
 T
 T

u∂t ϕdxdt =
(∇u∇ϕ + u(∇V − x) · ∇ϕ)dxdt −
0

Td

0

Td

u0 ϕ(0, x)dx,

Td

where V is defined as in Theorem 1.
The main result of this note is as follows:
Theorem 3. Let initial data 0 ≤ u0 ∈ L2 (Td ). Then the MAKS system (2) admits a global non-negative weak


solution (u, V ) in t ∈ [0, T ] for any T > 0. And the conservation of mass holds: Td u(t, x)dx = Td u0 (x)dx.
Proof. We build a sequence of approximate solutions (uε , Vε )ε>0 by regularization and let ε goes to zero. To
do the limiting process, the non-linear term will be treated with the help of Theorem 2.
Step 1: Construction of a sequence of approximate solutions. We consider a mollifier ψ(x) ∈

Cc∞ (Rd ) such that ψ(x) ≥ 0, Td ψ(x)dx = 1 and ψε (x) = ε−d ψ(x/ε). And we can define the mollification

as ψε ∗ u0 := Td ψε (x − y)u0 (y)dy. Then we study solutions to the following approximate problem
∂t uε = ∆uε + ∇ · (uε (∇Vε (x) − x)) ,

x ∈ Td , t > 0,

(11a)

uε,0 (x) = ψε ∗ u0 (x),

(11b)

Vε (x) = ψε ∗ V [uε ].

(11c)

Since Vε given by (11c) is bounded in H k (Td ) for any k and ε > 0, the estimate for Eq. (11a) for any fixed
ε > 0 is basically same as that for the heat equation. Hence, the solvability of the regularized problem (11)
can be obtained by using the technique in Majda and Bertozzi [12, Section 3.2.2], where it proved the global
existence of the solution to a regularization of the Euler and Navier–Stokes equation by using the Picard
theorem and continuation property of ODEs on a Banach space. We omit the detail here.
Step 2: Weak convergence of uε and ∇uε . Multiplying Eq. (11a) by 2uε and integrating over Td , we
obtain

d
2
2
2
∥uε ∥2 + 2 ∥∇uε ∥2 = −
uε (∇Vε − x) · ∇uε dx ≤ ∥∇uε ∥2 + C∥uε ∥22 .
(12)
dt
d
T
√
where we have used ∥∇Vε − x∥∞ ≤ d/2.
Hence for any T > 0, the following estimates hold
∥uε ∥L∞ (0,T ;L2 (Td )) ≤ CT ,

∥∇uε ∥L2 (0,T ;L2 (Td )) ≤ CT .

(13)

According to the above estimates, there is a subsequence (still denote uε ), such that as ε → 0, the following
weak convergence results hold




∗
uε −
⇀ u in L∞ 0, T ; L2 (Td ) ,
∇uε ⇀ ∇u in L2 0, T ; L2 (Td ) .
(14)
Step 3: Strong convergence of ∇Vε (t, ·) a.e. t. We claim that for any p ∈ [1, ∞),
∇Vε (t, ·) → ∇V (t, ·)

in Lp (Td ), a.e. t ∈ [0, T ].

(15)

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 26–34

30

Indeed, such strong convergence of ∇Vε follows from Theorem 2 provided that we have for a.e. t ∈ [0, T ],


ϕ(x)uε (t, x)dx →
ϕ(x)u(t, x)dx,
(16)
Td

Td

for any ϕ ∈ C 0 (Td ). To verify (16), we need to prove that there is a subsequence (still denote uε )
uε → u in L2 (Td ) a.e. t ∈ [0, T ], as ε → 0.

(17)

Indeed, it is easy to check that ∥∂t uε ∥L2 (0,T ;H −1 (Td )) ≤ CT , which leads to


uε → u in L2 0, T ; L2 (Td ) , as ε → 0,

(18)

by using Aubin–Lions lemma as H 1 (Td ) ↩→↩→ L2 (Td ) ↩→ H −1 (Td ). Then (17) follows from (18), which
completes the proof of (15).
Step 4: Existence of a global weak solution. Next, we will show that (u, V [u]) is a weak solution to


(9). The weak formulation for uε is that for any test function ϕ ∈ Cc∞ [0, T ) × Td ,
 T
 T

uε ∂t ϕdxdt =
(∇uε ∇ϕ + uε (∇Vε − x) · ∇ϕ)dxdt −
uε,0 ϕ(0, x)dx.
(19)
Td

0

Td

0

Td

√

Recall that (14), (15), (18) and ∥∇Vε − x∥∞ ≤ d/2. Then by using the dominant convergence theorem,
one concludes that by passing limit ε → 0 in (19)
 T
 T

u∂t ϕdxdt =
(∇u∇ϕ + u(∇V − x) · ∇ϕ)dxdt −
u0 ϕ(0, x)dx.
(20)
0

Td

Td

0

Td

We finished the proof of the existence of global weak solutions.
Step 5: Positivity preserving. By using Lemma 7.6 in [13], if we define the negative part of the function
u as u− := min{u, 0}, then one can easily prove that

d
2
2
∥u− ∥22 + 2 ∥∇u− ∥2 = −
u− (∇V − x) · ∇u− dx ≤ ∥∇u− ∥2 + C∥u− ∥22 .
(21)
dt
d
T
Applying Gronwall’s inequality to
d
∥u− ∥22 ≤ C∥u− ∥22 ;
dt

∥u0− ∥22 = 0,

(22)

one has ∥u− ∥22 ≡ 0, which leads to u(t, x) ≥ 0.
Step 6: Conservation of mass. Integrating (9a) over Td and using the fact that ∇u, ∇V − x are
periodic, one has



d
udx =
∆udx +
∇ · (u(∇V − x))dx = 0.
(23)
dt Td
Td
Td
Thus, we conclude that



u(t, x)dx =

Td

u0 (x)dx.



(24)

Td

4. Regularity in L∞ (0, T ; L∞ ∩ W 1,1+γ (Td ))
Theorem 4. Let initial data 0 ≤ u0 ∈ L∞ (Td ) and ∇u0 ∈ L1+γ (Td ) for some γ > 0. Suppose (u, V ) be a
weak solution to MAKS equation (9), then for any T > 0 and t ∈ [0, T ],


u(t, x) ∈ L∞ 0, T ; L∞ ∩ W 1,1+γ (Td ) .
(25)

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 26–34

31

Proof. First we will prove that
∥u(·, t)∥∞ ≤ C(T, d, A0 ),

(26)

with A0 = max{1, ∥u0 ∥L1 (Td ) , ∥u0 ∥L∞ (Td ) }. Multiplying (9a) with pup−1 , p ≥ 2 and integrating over Td , we
have



p 2
p
p
4(p − 1) 
d

p
p
2
(∇V − x)∇u dx = −2(p − 1)
(∇V − x)u 2 ∇u 2 dx
∥u∥p +
∇u  = −(p − 1)
dt
p
2
Td
Td
 p 


p
p 2
2(p − 1) 
 2 

(27)
≤ C u  ∇u 2  ≤
∇u 2  + C∥u∥pp .
p
2
2
2
Then the L∞ bound can be obtained directly by the standard Moser iteration after getting (27). For example,
one can check the paper by Alikakos [14], formula (3.20) and the computation afterwards. For completeness,
we put these detail computation in Appendix.
From Theorem 3, we know that u is positivity preserving and the conservation of mass hold:
u ≥ 0;

∥u(t, x)∥1 = ∥u0 ∥1 , for t ∈ [0, T ].

(28)

By the construction of V in Theorem 1, one concludes that

2

1 ≤ det(∇ V ) ≤ ∥u∥∞ + 1,
V convex,

V − x2 /2 periodic.

(29)

Recall the result in [15, P.16], for some γ > 0 we have
∥∇2 V ∥1+γ ≤ C(T, d, ∥u∥∞ ).

(30)


|x+k|2
1
− 4t
The heat semigroup operator et∆ defined by et∆ u := H(t, x) ∗ u, where H(t, x) = (4πt)
d/2
k∈Zd e
is the periodic heat kernel. It follows immediately from Young’s inequality for the convolution that
d

∥et∆ u∥p ≤ Ct− 2 ( q − p ) ∥u∥q ,
1

d

∥∇et∆ u∥p ≤ Ct− 2 − 2 ( q − p ) ∥u∥q ,

1

1

1

1

(31)

for any 1 ≤ q ≤ p ≤ +∞, u ∈ Lp (Td ) and all t > 0. Here C is constant dependent of p, q.
By the fundamental solution representation of the heat equation, the solution to the MAKS equation can
be represented as
 t
u = et∆ u0 +
e(t−s)∆ (∇ · (u(∇V − x)))ds,
(32)
0

for any T > 0, 0 < t < T .
By choosing q = p = 1 + γ in (31), a simple computation leads to
 t
∥∇u∥1+γ ≤ C∥∇u0 ∥1+γ +
(t − s)−1/2 ∥∇ · (u(∇V − x))∥1+γ ds.

(33)

0

√
From Theorem 1, we have that ∥∇V − x∥∞ ≤ d/2 and moreover ∥∇2 V ∥1+γ ≤ C(T, d, ∥u∥∞ ), then we
conclude
√


∥∇ · (u(∇V − x))∥1+γ ≤ d/2∥∇u∥1+γ + C T, d, ∥u∥∞ , |Td | .
(34)
Hence we have

∥∇u∥1+γ ≤ C1 + C2
0

t

(t − s)−1/2 ∥∇u∥1+γ ds.

(35)

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 26–34

32

Applying a generalized Gronwall’s inequality with weak singularities [16, Lemma 7.1.1], we have
∥∇u∥1+γ ≤ C (T, d, ∥∇u0 ∥1+γ , ∥u0 ∥∞ ) ,

(36)



which concludes the proof.
Acknowledgments

The authors would like to thank Yann Brenier for suggesting this problem to us. The work of Jian-Guo
Liu is partially supported by KI-Net NSF RNMS grant No. 1107291 and NSF grant DMS 1514826. Hui
Huang is partially supported by National Natural Science Foundation of China (Grant number: 41390452,
11271118).
Appendix. The proof of L∞ estimate in Theorem 4

Proof. Using Gronwall’s inequality in (27), one concludes that
∥u(·, t)∥pp ≤ eCt ∥u0 ∥pp ≤ C(T, d, A0 ).

(A.1)

Define pk := 2k + 2 with k ≥ 0. For k = 0, p0 = 3, from (A.1) we have
∥u(·, t)∥pp00 ≤ C(T, d, A0 ).

(A.2)

For k ≥ 1, take pk upk −1 as a test function in (9a), one has

2
pk
4(pk − 1) 
d


∥u∥ppkk = −
(∇V − x)∇upk dx
∇u 2  − (pk − 1)
dt
pk
2
Td

2

 p  
pk
pk



 k 
≤ −2Cpk ∇u 2  + pk u 2  ∇u 2  .
2

pk

Now, we focus on estimating the term ∥u 2
 p  

 p θ
pk
 2k  

 k
u  ∇u 2  ≤ u 2  2d
2

with

pk
2 r

2

1
1
r−2
d−2
1
−
r
2d

= pk−1 , θ =

d−2

2

(A.3)

2

pk

∥2 ∥∇u 2 ∥2
 p 1−θ 


1+θ  p 1−θ
pk
pk
 2k 




 2k 
,
u 
∇u 2  ≤ Sdθ ∇u 2 
u 
r

2

2

, where we have used the Sobolev inequality ∥u∥

2d
d−2

(A.4)

r

≤ Sd ∥∇u∥2 . The Young’s

inequality tells that

2
pk
d


∥u∥ppkk ≤ −Cpk ∇u 2  + C(σ)pqk2 Sdθq2 ∥u∥ppkk−1 ,
dt
2
where σ = Cpk , q2 =

2
1−θ

(A.5)

≤ d + 2.

On the other hand,
 p 2

2θ1  p 2(1−θ1 )
pk
 k

  k
∥u∥ppkk = u 2  ≤ Sd2θ1 ∇u 2  u 2 
,
2

where r is the same as before and θ1 =

1
1
r−2
d−2
1
−
r
2d

2

. Similar to (A.5), we have


2
pk


ℓ2 (1−θ1 )
∥u∥ppkk ≤ σ ∇u 2  + C(σ)Sd2θ1 ℓ2 ∥u∥ppkk−1
,
2

where ℓ2 =

1
1−θ1 .

(A.6)

r

(A.7)

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 26–34

33

Hence from (A.5) and (A.7), we deduce
d
∥u∥ppkk ≤ −∥u∥ppkk + C(σ)pqk2 Sdθq2 ∥u∥ppkk−1 + C(σ)Sd2θ1 ℓ2 ∥u∥ppkk−1 .
dt

(A.8)

Define
C1 (pk ) := C(σ)Sdθq2 ;

C2 (pk ) := C(σ)Sd2θ1 ℓ2 .

It is easy to know that C1 (pk ) and C2 (pk ) is uniformly bounded for any k ≥ 1. So, we let C(d) > 1 be a
common upper bound of C1 (pk ) and C2 (pk ), we obtain the following inequality
d
∥u∥ppkk ≤ −∥u∥ppkk + C(d)pqk2 ∥u∥ppkk−1 .
dt

(A.9)

Solving the inequality (A.9), we get
(et ∥u∥ppkk )′ ≤ C(d)pqk2 ∥u∥ppkk−1 et ≤ 2C(d)4d+2 2k(d+2) sup ∥u∥ppkk−1 et ,

(A.10)

t≥0

where the last inequality used 1 < q2 ≤ d + 2.
Notice that ∥u0 ∥ppkk ≤ ∥u0 ∥1 ∥u0 ∥p∞k −1 , so we have
max{∥u0 ∥ppkk , 1} ≤ Apk ,

(A.11)

where constant A > 1 is independent of k but depends on ∥u0 ∥1 and ∥u0 ∥∞ . Let ak := 2C(d)4d+2 2k(d+2) > 1
and integrate (A.10), then one has


∥u∥ppkk ≤ ak sup ∥u∥ppkk−1 (1 − e−t ) + ∥u0 ∥ppkk e−t ≤ ak max sup ∥u∥ppkk−1 , Apk .
(A.12)
t≥0

Taking the power

1
pk

t≥0

to above inequality, then
∥u∥pk ≤

1/p
ak k




max sup ∥u∥pk−1 , A .

(A.13)

t≥0

After some iterative steps, we have
∥u∥pk ≤

1/p 1/p
ak k ak−1k−1

≤ (2C(d)4

1/p
· · · a1 1

d+2 1− 21k

)




max sup ∥u∥p0 , A
t≥0

1
− kk
d+2 2− 2k−1
2

(2

)




max sup ∥u∥p0 , A .

(A.14)

t≥0

Recall ∥u∥pp00 ≤ C(T, d, A0 ), then the L∞ estimate is obtained by passing to the limit k → ∞ in (A.14).



References
[1] E.F. Keller, L.A. Segel, Initiation of slime mold aggregation viewed as an instability, J. Theoret. Biol. 26 (3) (1970)
399–415. http://dx.doi.org/10.1016/0022-5193(70)90092-5.
[2] Y. Brenier, Hilbertian approaches to some non-linear conservation laws, Contemp. Math. 526 (2010) 19–34.
[3] Y. Brenier, Polar factorization and monotone rearrangement of vector-valued functions, Comm. Pure Appl. Math. 44 (4)
(1991) 375–417. http://dx.doi.org/10.1002/cpa.3160440402.
[4] J. Benamou, Y. Brenier, Weak existence for the semigeostrophic equations formulated as a coupled Monge–Ampère
transport problem, SIAM J. Appl. Math. 58 (5) (1998) 1450–1461. http://dx.doi.org/10.1137/S0036139995294111.
[5] M. Cullen, M. Feldman, Lagrangian solutions of semigeostrophic equations in physical space, SIAM J. Math. Anal. 37 (5)
(2006) 1371–1395. http://dx.doi.org/10.1137/040615444.
[6] G. Loeper, A fully nonlinear version of the incompressible Euler equations: the semigeostrophic system, SIAM J. Math.
Anal. 38 (3) (2006) 795–823. http://dx.doi.org/10.1137/050629070.

34

H. Huang, J.-G. Liu / Applied Mathematics Letters 61 (2016) 26–34

[7] M. Lopes Filho, H.N. Lopes, Existence of a weak solution for the semigeostrophic equation with integrable initial data,
in: Proceedings of the Royal Society of Edinburgh-A-Mathematics, Vol. 132, Cambridge Univ Press, 2002, pp. 329–340.
[8] Y. Brenier, G. Loeper, A geometric approximation to the Euler equations: the Vlasov–Monge–Ampère system, Geom.
Funct. Anal. GAFA 14 (6) (2004) 1182–1218. http://dx.doi.org/10.1007/s00039-004-0488-1.
[9] Y. Brenier, A modified least action principle allowing mass concentrations for the early universe reconstruction problem,
Confluentes Math. 3 (03) (2011) 361–385. http://dx.doi.org/10.1142/S1793744211000400.
[10] R.J. McCann, Polar factorization of maps on riemannian manifolds, Geom. Funct. Anal. GAFA 11 (3) (2001) 589–608.
http://dx.doi.org/10.1007/PL00001679.
[11] H.-Y. Jian, X.-J. Wang, Continuity estimates for the Monge–Ampere equation, SIAM J. Math. Anal. 39 (2) (2007) 608–626.
http://dx.doi.org/10.1137/060669036.
[12] A.J. Majda, A.L. Bertozzi, Vorticity and Incompressible Flow, Vol. 27, Cambridge University Press, 2002,
http://dx.doi.org/10.1017/CBO9780511613203.
[13] D. Gilbarg, N.S. Trudinger, Elliptic Partial Differential Equations of Second Order, springer, 2015,
http://dx.doi.org/10.1007/978-3-642-96379-7.
[14] N.D. Alikakos, lp bounds of solutions of reaction–diffusion equations, Comm. Partial Differential Equations 4 (8) (1979)
827–868. http://dx.doi.org/10.1080/03605307908820113.
[15] A. Figalli, Global Existence for the Semigeostrophic Equations via Sobolev Estimates for Monge–Ampere, in: CIME Lecture
Notes, Springer, to appear.
[16] D. Henry, Geometric Theory of Semilinear Parabolic Equations, Vol. 840, Springer-Verlag, New York, 1981,
http://dx.doi.org/10.1007/BFb0089647.

