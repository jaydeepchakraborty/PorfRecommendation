25th IEEE International Symposium on
Robot and Human Interactive Communication (RO-MAN)
August 26-31, 2016. Columbia University, NY, USA

Projecting Robot Intentions
into Human Environments
Rasmus S. Andersen1 , Ole Madsen1 , Thomas B. Moeslund2 , and Heni Ben Amor3
Abstract— Trained human co-workers can often easily predict each other’s intentions based on prior
experience. When collaborating with a robot coworker, however, intentions are hard or impossible
to infer. This difficulty of mental introspection makes
human-robot collaboration challenging and can lead
to dangerous misunderstandings. In this paper, we
present a novel, object-aware projection technique
that allows robots to visualize task information and
intentions on physical objects in the environment.
The approach uses modern object tracking methods
in order to display information at specific spatial
locations taking into account the pose and shape of
surrounding objects. As a result, a human co-worker
can be informed in a timely manner about the safety
of the workspace, the site of next robot manipulation
tasks, and next subtasks to perform. A preliminary usability study compares the approach to collaboration
approaches based on monitors and printed text. The
study indicates that, on average, the user effectiveness
and satisfaction is higher with the projection based
approach.

(a)

I. INTRODUCTION

(b)

Traditional automated production leverages the speed,
precision, and reliability of industrial robots to produce
large quantities of identical or similar items. Such industrial robots are well suited for carrying out simple
tasks with a high degree of repeatability. For production
of smaller production series, however, an increasingly
higher degree of customization is required. This pushes
demand for flexible manufacturing lines including intelligent robots. Industrial robotics is therefore currently
experiencing a major trend moving from robots carrying
out simple, repetitive tasks while being separated from
human workers by security fences, towards collaborative
robots that carry out increasingly complex tasks [1]. Collaborative robots do not need to be physically separated
from human co-workers, but can instead work in a shared
workspace. Contrary to traditional industrial robots, it
is therefore possible for collaborative robots to directly
interact with human co-workers on joint physical tasks.
In this paper, we propose to facilitate interaction between industrial collaborative robots and human workers

Fig. 1. Interaction with tracked objects while information is being
projected onto the objects in real-time. In (a), a wireframe is being
projected onto a car door while it is being moved by a human. In
(b), a sign warns a user that a robot will interact and is interacting
with the particular object. In (c), the robot highlights a segment of
the object on which it will perform a manipulation.

by projecting information about the current task directly
into the workspace. This information can include the
state and intentions of the robot, instructions to the
human, and information about the current state of the
task. A novel object-aware projection mapping system
is developed and combined with existing object tracking
technology, which allows for precise 6-DOF object pose
detection from a standard RGB-camera. The tracker
enables the display of information on objects before,
during, and after they are manipulated by either the
human or the robot. Two examples are shown in Figure 1.
In (a), a car door is being tracked during a collaborative
assembly task1 . A wireframe model of the car door is projected onto the door itself, while it is manipulated by a
human. The projected door provides feedback about the
perception quality and accuracy of the robot. In (b), a
sign is projected onto an assembled box warning humans
that a robot intends to manipulate this particular object.

1 Rasmus S. Andersen and Ole Madsen are with the Department
of Mechanical and Manufacturing Engineering, Aalborg University,
Denmark {rsa,om}@m-tech.aau.dk
2 Thomas B. Moeslund is with the Department of Architecture, Design and Media Technology, Aalborg University, Denmark

tbm@create.aau.dk
3 Heni Ben Amor is with the Interactive Robotics Lab, Arizona
State University, Arizona, United States hbenamor@asu.edu

978-1-5090-3929-6/16/$31.00 ©2016 IEEE

(c)

1A

294

video is available at https://youtu.be/FT94aH98bew

Finally, in (c) the robot identifies the sub-component of
the car on which it will perform the next manipulation,
thereby clearly indicating to the human its intent. Both
the shape of the projection, as well as the color of the
projection are used as cognitive cues to communicate
information.
The main contribution of this work is an objectaware human-robot interaction approach intended for
collaborative robots which uses projections to provide
relevant information to human co-workers directly in
the common workspace. By continuously tracking taskrelated objects, we can use information about their shape
and pose in order to provide accurate visual cues at
precise spatial locations to a human collaboration partner. This allows for rapid and explicit communication
of the robot’s mental states. The approach is evaluated
against common interaction approaches in a comparative
usability study.
A secondary contribution of this paper is a robust implementation of simultaneous object detection, tracking
and projection mapping. In practice, it is challenging
to visually track an object while at the same time
changing its visual appearance through augmented imagery. This creates a closed-loop tracking-and-projection
system, which could result in drifting and unbounded
errors. We identify here a reduction of the problem and
propose a working solution which in practice provides
stable results.
The paper is organized as follows: In Section II, related
work within human-robot interaction is reviewed. In Section III, potential use cases for the intention projection
system are presented. In Section IV, our approach is
described, and finally in Section V, experiments are presented which evaluate both contributions of the paper.

floor. The surface becomes a replacement for the flat
display screen. Hence, the projection does not take into
account the 3D nature of the environment and does not
include information about the shape and pose of object.
Projections onto specific parts of an object are therefore
infeasible.
In a similar vein, Leutert et al. propose an interaction
system, SAR, which employs projections on flat surfaces
to facilitate interactions with an industrial robots [5].
They combine images from an external projector and a
flange-mounted projector to visualize various data, including instructions and dynamic movement data. They
propose to give instructions to the robot using an input
device that is tracked in 3D and then visualize the instructions in real time using projections. The movement
data includes floor projections of the intended future
pose of the robot. In addition to the above mentioned
limitations, their work does not include any evaluation
of the usability of the interaction system.
In a recent paper, Chadalavada et al. propose to use a
projector mounted on a mobile robot to visualize a small
fraction of its intended path a few meters ahead [6]. Test
persons were asked to walk towards and pass the robot
with and without projection enabled, and the average
user rating increased by 53% with projection, if the robot
moved in a straight line and by 65% if the robot took a
sudden turn. Even though the use case for this study is
different from what is suggested in the current paper,
it does suggest that direct visual feedback in a common
space in general can be advantageous.
Besides approaches that use an explicit approach for
communicating robot intentions, e.g. projection, recently
there has also been growing interest in implicit approaches. Dragan and colleagues [7]–[9] investigated how
robot motion can be planned so as to clearly communicate intentions to a human partner. They argue that
legible robot motion allows human interaction partners
to infer future goals of the robot. A similar approach is
also followed in [10]–[12]. In general, this line of research
treats robot motion as a communication channel through
which information can be conveyed. Yet, the amount and
type of information that can be transmitted through motion is limited. In contrast to that, the explicit projection
technique presented in our paper makes it possible to
project a much wider variety of information which are
useful for actual industrial human-robot collaboration
tasks.

II. RELATED WORK
The idea of using projection to understand the state
and intentions of a robot is not new in itself. One of
the first attempts was made by Ishii et al. in 2009, who
used a projection interface to give commands to a mobile
robot [2]. A user draws circles around objects using
an IMU pointing device. The drawings are visualized
by a projector and when a user accepts the task, the
mobile robot moves to the marked object and picks it
up. More recent work has explored how to visualize complex robot information in human environments. Several
research groups have designed workspaces with a number
of projectors mounted in the ceiling covering a large
floor area [3], [4]. For instance, the MAR-CPS system
presented in [3] combines a ceiling-to-floor projection
system with a motion capture system that is capable of
tracking flying drones as well as driving robots. This is
used for different research areas, including surveillance
coverage, path planning, and to identify robot states
and error messages in general. Common to these setups,
however, is that they display additional information by
projecting onto flat surfaces in the environment, e.g., the

III. USE CASES FOR INTENTION
PROJECTION
Understanding the internal state and intentions of a
robot is relevant to many situations involving modern,
collaborative robots. This includes situations where humans and robots have to enter each other’s workspace,
as well as situations where workspaces are shared between robots and humans for longer durations. Typical
tasks at a manufacturing plant involve machine tending,
295

Fig. 2. Overview of the system: A physical object (car door) is first detected using model-based tracking. Information about robot
intentions, e.g., next actions, is displayed by rendering a 2D texture on top of the object model. Finally, the rendered image is projected
into the real world.

used by a robot to communicate next steps or tasks to a
human partner. Projecting such information on top of the
required object reduces ambiguities and miscommunication. The human interaction partner is not required to
divide attention between an external information display
(screen) and the spatial location of the interaction. As
a result, attention, fluency and safety can be improved
during the collaborative task.

assembly, inspection, logistic tasks, and service tasks.
One workspace can be used for many tasks, and if
robots and humans have responsibility for different tasks,
coordination is necessary. In some cases, humans and
robots may even have to cooperate on the same tasks;
for instance an assembly task involving heavy parts.

IV. INTENTION PROJECTION FRAMEWORK
The intention projection system uses the environment
as a canvas in order to display critical information. To
this end, two steps are needed, namely (a) detection and
tracking of objects using a standard RGB-camera, and
(b) projection of important information into the scene.
Figure 2 gives an overview of this setup. The system
consists of several subcomponents; object tracking, pose
estimation, and rendering. Each of these is described in
the following subsections.

Fig. 3. Example domain: car doors are being transported on a
conveyor belt. Both human and robot are required to engage as
co-workers on the doors.

A use-case from a car factory is presented as an
example throughout the current paper. Car doors are
transported hanging from a conveyor belt as shown in
Figure 3. Human workers as well as robots must work on
the doors while they are being transported. The doors are
not fixed, hence their pose must be constantly tracked.
Figure 4(a)-(d) shows an interaction in a laboratory
setup. In (a), a human worker moves the car door while
the tracked pose is visualized as a wireframe projected
onto the door. The display of the wireframe provides
feedback about the robot vision system. Failure modes
of the vision system can be detected whenever the wireframe does not fit the underlying physical model.
Intention projection can also be used to project the
next manipulation location of a robot, or next grasping
locations. In Figure 4(b)-(d), for instance, the robot is
interacting with different parts of the door, and these
parts are marked by the projector in advance. This
makes it possible for the human to take the necessary
precautions by avoiding those areas. Color coding of the
projections can further be used in order to encode current
states of the robot, e.g., high stiffness vs. low stiffness
modes. In training scenarios, intention projection can be

A. Object Tracking
Objects can be tracked using different technologies;
two the most popular being passive vision and active
structured light depth sensing. For the current task,
passive vision has the advantage when compared to
structured light depth sensing that it is not dependent
of the object surface’ ability to reflect the emitted light.
Additionally, its performance is more robust to direct
and indirect sunlight. A problem for passive vision is
that projected light in the visible spectrum can disturb
the tracking. A way to handle this is to synchronize the
shutter systems of the projector and the camera so that
the projector is off whenever the camera captures an
image.
In the current system, passive vision based on offthe-shelf components is preferred instead of a highly
specialized and expensive synchronized projector-camera
setup. When much graphics is projected, the interference is instead reduced by projecting and capturing in
different ends of the visible light spectrum. The object
tracking is based on the Iterative Re-weighted Least
Squares (IRLS)-tracker by Choi and Christensen [13].
296

(a)

(b)

(c)

(d)

Fig. 4. Figure (a)-(d) is from a laboratory setup. In (a), the car door is manipulated with simultaneous real-time visualization of
the tracking. In (b)-(d), a robot’s intention to interact with particular parts of the car door is projected. This allows a human to take
precautions.

The tracker relies on a CAD model of the object to be
tracked. Sharp and dull edge features are extracted offline
from the model. Depending on a hypothesized pose, a set
of salient edges is created by combining all non-occluded
sharp edges with dull edges that are part of the object’s
silhouette. At run-time, edges are extracted frame-byframe using the Canny edge detector [14]. Points on the
model edges from frame t-1 are matched to detected
edges in frame t by searching along a line orthogonal
to the direction of the model edge. This results in an
error vector which is minimized iteratively by modifying
the pose of the model. An exhaustive description of the
IRLS-tracker is beyond the scope of this paper, and the
reader is instead referred to [13].
B. SURF Based Tracker Initialization

(a) Input frame

(b) Canny edges

(c) ROI and pose hypothesis
testing

(d) Final pose

Fig. 5.

The IRLS tracking algorithm requires initialization
with a pose of the object to track. Also, in the case
of tracking failure, it needs to be re-initialized. In previous works by Choi and Christensen, a SURF based
initialization is employed with convincing results [13],
[15]. A number of images of the object is captured while
the object is simultaneously tracked. In each of the
images, SURF features are estimated and mapped to
the object. When the SURF initialization is subsequently
used, SURF features are detected in the present image
and mapped to the stored features. In [13], where the
pose estimation is used in conjunction with the IRLS
tracker, RANSAC is employed to filter SURF matches.

Edge based initialization.

In the edge-based initialization, a large number of
edge maps is first generated offline from the object CAD
model for all legal, hypothetical poses. It is assumed
that objects are initially placed facing up in a certain
height; typically on a supporting surface. The search
space can therefore be reduced by only searching through
two spatial dimensions and one rotational dimension,
instead of the full 6-DOF space.
At runtime, edges are detected using the Canny edge
detector as seen in Figure 5(a) and (b). A distance
transform using the Euclidean norm is applied to the
edge map to facilitate robust matching of the model edge
maps. A threshold is subsequently applied to the distance
map to avoid extreme values far from any edges. The
resulting “softened” edge map is shown as a black/white
background image in Figure 5(c).
Recent movement in the scene is used to limit the
region-of-interest (RoI), as indicated in Figure 5(c) by
the yellow rectangle. The search area has been defined in
advance, and hypothetical object positions are displayed
in the figure as red and blue dots. To increase computation speed, model edge maps are disregarded if they

C. Projection-Robust Tracker Initialization
As will be shown through experiments in Section VA, the SURF based approach performs poorly when
graphics are projected onto a significant part of the
object. Even relatively dimmed light significantly obstructs the local features that such a method relies on.
Experimental results on this are presented in Section VA. For the system presented in this work, a templatebased approach has therefore been employed which relies
on edge features similar to the IRLS tracker. The method
is illustrated in Figure 5.
297

cannot fit inside the RoI. In the figure, red dots indicate
positions where no model edge maps were found to be
legal. The remaining model edge maps are matched to
the distance transform by computing a sum of all edge
locations. The best pose is shown in Figure 5(d).

Each method has been tested under three different conditions:
1) Full light: Normal indoor light conditions. This
condition has also been used to train the SURF
based method.
2) Low light: Lower indoor light.
3) Projected wireframe: Lower indoor light with
a wireframe being projected onto the object. A
wireframe is used because this fills most of the
surface of the object. It can therefore be considered
as a worst case projection.
The effect of the wireframe is minimized by projecting
only blue light and using only the red color channel for
pose estimation. The success rates and time consumptions have been estimated by performing at least 100
successful pose estimations in each scenario, and the
results are shown in Table I.

D. Rendering System
The task of the rendering system is to generate a
visual representation of intent and to project it into the
environment in a geometrically correct manner as shown
in Figure 1. The projector is calibrated intrinsically and
extrinsically with respect to the camera (refer to the
overview in Figure 2). The resulting transformations
are, in turn, used to set projection and the modelview
matrix, respectively. Other transformations are added to
the modelview matrix, including detected object poses
and relative positions in the environment. All objects
and visual entities are represented in a scenegraph for
rendering. The rendering process in effect pre-warps
the generated graphics, thereby producing an accurate
projection when the image is distorted by the shape of
the physical object. Our current system runs in realtime at frame rates of about 20–30Hz, including object
tracking and rendering.

TABLE I
Success rate and time consumption for pose estimation
methods
Method
SURF
SURF
SURF
Edge
Edge
Edge

V. EXPERIMENTAL RESULTS
The proposed projection system is evaluated both
technically (quantitative) and through a usability study
(qualitative). The technical evaluation estimates the robustness of the proposed pose estimation algorithm used
for initializing the tracker, while the usability study compares the usability of the proposed intention projection
to traditional interfaces.

Setting
Full light
Low light
Proj. wireframe
Full light
Low light
Proj. wireframe

Success rate
49.8%
17.6%
9.9%
(100%)
(100%)
(100%)

Avg duration
494.7 ms
487.2 ms
554.1 ms
15.3 ms
13.9 ms
16.9 ms

The success criterion for the SURF method is based on
the RANSAC algorithm; if too few features are matched,
estimation is considered a failure and the pose estimation
is restarted. The edge-based method does not employ
a similar self-validation method, and all estimations are
therefore used. Poor pose estimations of the edge-based
method will therefore reduce the mean accuracy of the
edge based method, while the SURF based method is
able to remove and disregard some poor matches.
It is clear from the success rate that the SURF based
approach is heavily affected by both changing light conditions and projections, even when different colors are
used for projection and pose estimation. At the same
time, the SURF based method is more than an order
of magnitude slower than the edge based method. It
should be noted, however, that both methods in this test
use single threaded CPU implementations and that their
computation time can be reduced using parallel GPU
implementations.
The accuracy of the successful pose estimations for
all scenarios should ideally be determined by comparing
the results against a ground truth pose. Ground truth
is unfortunately not available in our case. Instead, the
steady state pose from the IRLS tracker under optimal
light conditions is used. This will unavoidably affect the
estimated errors; possibly to the advantage of the edgebased method, since IRLS is itself also edge-based. The
SURF-based method is, however, based on surf features,
which are also found during steady state IRLS tracking.

A. Robustness of Initialization
Both methods for initializing the pose estimation mentioned in Section IV-A, one based on SURF features
and one based on edge features, have been tested and
compared. The evaluation was performed by repeatedly
estimating the pose of the car door in the setup shown
in Figure 6.

Fig. 6.
Setup for the pose estimation test. The camera and
projector is shown in the bottom of the left image.

The purpose of the test is to evaluate how the performance of the methods is affected by changing light
conditions as well as graphics projected onto the object.
298

The relative performance between the scenarios should
therefore only be slightly affected. Results are shown in
Figure 7.
Mean and added standard deviations
120
µ=255
σ=2993

µ=138
σ=4477

µ=4372
σ=35298

Absolute errors [cm/deg]

100
80

SURF: Full light
SURF: Low light
SURF: Proj. wireframe
Edge: Full light
Edge: Low light
Edge: Proj. wireframe

(a) Setup
60
40
20
0

x

y

z

roll
Dimensions

pitch

yaw

Fig. 7.
Comparison of the accuracy of the pose estimation
methods. The coordinate frame is oriented as shown in Figure 6.
The mean absolute error of each method when compared to the
steady state pose detected by the IRLS tracker is shown as wide
bars. The thinner lines are the standard deviation added to the
means.

(b) Movement instruction. The
destination area and distance is
updated continuously as the object is moved.

(c) Rotation instruction. The direction and remaining angle is
updated continuously as the object is rotated.

SURF-based methods perform poorly when a wireframe image is projected. This is the case even though
only the 9.9% of the poses that were evaluated as successful are included in these results. A manual sorting
through the data indicates that some estimates were actually successful; however other estimations were very far
off. The edge-based method performs, on the other hand,
almost identical under different conditions. The standard deviation is significantly lower for the edge-based
method, partly because the algorithm of this method is
deterministic, whereas the SURF based method is nondeterministic as a result of using RANSAC.

(d) Robot interaction warning

(e) Manipulation complete

Fig. 8. Setup and provided information for the usability study.
The task was to either rotate or move the white box based on
instructions provided by one of three methods. Figure (b)-(e) shows
the projected information. Sometimes a subtask is carried out by
the robot, and in this case the test person is notified by a warning
triangle as shown in (d).

B. User Study on Usability
as the object is moved and rotated. For humanmove instructions, the destination area is additionally marked with a white circle as seen in
Figure 8(b). Whenever a subtask is successfully
completed, check marks are shown on the object
as in Figure 8(e).
2) Monitor display: In the monitor display based
approach, the same information is given; the only
difference being that it is shown on a monitor
attached to the robot, as seen in Figure 8(a). The
destination areas are marked with numbers on the
table.
3) Text description: In the text based approach,
the entire task is presented as a numbered list
and given to the test person before the start of
the experiment.The destination areas are marked
with numbers on the table, and illustrations on the
text sheet show how the box should be oriented.
No validation is performed w.r.t. either position or
orientation with this method.

A usability study has been carried out to evaluate the
usability of intention projection compared to traditional
approaches. A total of 14 test persons with diverse backgrounds participated. Each test participant was asked to
complete three tasks in collaboration with a robot. Each
task consisted of 15 subtasks which included rotating an
object and moving it between three marked areas. The
following subtasks were used:
•
•
•
•

Move object to area X (human)
Rotate object Y◦ (human)
Move to area X (robot)
Rotate Y◦ (robot)

The test setup is shown in Figure 8(a). For each of the
tasks, a different interface for collaboration was used:
1) Projected information: With the projection approach, information is given for each subtask as
shown in Figure 8(b)-(d). Information is projected
directly onto all sides of the object facing the
projector. The information is updated in real-time
299

7

The usability of each approach is evaluated according
to the ISO standard 9241-11 (1998) [16], which defines
usability as a combination of three factors:
• Effectiveness: “The accuracy and completeness
with which users achieve specified goals.”

Likert score [1;7]

•

6

Efficiency: “The spent resources in relation to the
accuracy and completeness with which users achieve
specified goals.”

Text
Monitor
Projector

5
4
3
2
1

•

Satisfaction: “The freedom from discomfort and
positive attitudes towards the use of the product.”

Q1: Ease

errors
Errors
0
0
3

Time [min]

3) Satisfaction: The satisfaction of the participants
was measured through questionnaires based on Lewis’
After-Scenario Questionnaire (ASQ) [17]. For each collaboration approach, they were asked to indicate on a
1-7 Likert scale the extent to which they agreed with the
following statements:
Q1) Overall, I am satisfied with the ease of completing
the tasks in this scenario.
Q2) Overall, I am satisfied with the amount of time
it took to complete the tasks in this scenario.
Q3) Overall, I am satisfied with the support information (projections, display on monitor, text) when
completing the tasks.
The results are shown in Figure 9. The scores for question 2, satisfaction with the amount of time, are almost
identical for each of the approaches as could be expected
with the close to identical time consumptions. The results
for the remaining satisfaction related questions, the ease
and the support information, shows a clear tendency
that the text interface is considered worst whereas the
projector interface is considered best.
The average satisfaction scores across all three questions are listed in Table III. On a normalized scale, the
user satisfaction was on average 2.3 percentage points
better for projection when compared to monitor and
7.5 percentage points better when compared to text.
Relatively, the monitor and text based methods scored
respectively 16.3% and 52.3% worse than projection.

TABLE II
Effectiveness measured as the number of questions and

Questions
0
3
3

Q3: Info

Fig. 9. Results from the usability study concerning satisfaction
(Q1-Q3) and efficiency (time). The mean scores 95% confidence
intervals are shown. For Q1-Q3, 1 means strongly agree while 7
means strongly disagree.

Results for each of the evaluation criteria are presented
below.
1) Effectiveness: The effectiveness is measured objectively as the number of times in which the participant
required help and/or failed in completing a subtask. In
general, not much help was needed since all subtasks
were relatively simple. However, especially the text based
approach caused some problems. Three test persons
asked for assistance in determining if they had completed
one or more of their subtasks correctly due to the lack
of visual feedback. One subject mistakenly rotated the
object around one of its corners instead of its center
(as was instructed). A fourth test person was unable to
keep track of the current subtask and therefore skipped
a human subtask and was not aware of a robot subtask;
causing a near-collision with the robot. The collision
was only prevented through an emergency stop by the
experimenter. A total of six significant questions/errors
were thus counted for this approach.
In the monitor based approach, several test persons
had difficulties relating an arrow on the monitor to a
rotational direction of the objects. Most test persons
eventually figured out the relation, but three test persons
had to be assisted.
In the projection based approach, no significant questions or errors were present. One test person asked for
confirmation before each human subtask but did so for
all interfaces and was always correct. The effectiveness
of the three approaches is compared in Table II.

Method
Projector
Monitor
Text

Q2: Time

Total pr. person
0.00
0.21
0.43

TABLE III
Average satisfaction scores
Method
Projector
Monitor
Text

2) Efficiency: The efficiency is measured objectively
as the time spent to complete a task. The time consumption was almost identical for all collaboration approaches
as shown in the last column in Figure 9. One reason for
this is that the majority of the total time was taken up
by the robot’s movements and not by the actions of the
participants.

Likert [1;7]
1.86
2.00
2.31

Normalized [0;100]
14.33
16.67
21.83

The test participants were given the possibility to provide additional qualitative feedback on each approach.
Two issues were noted by several participants. For the
text based approach, they had an overview on the current
300

state of the overall task. It would be an advantage to
also have this for the other approaches. Also for the text
based approach, the participants knew what the robot
was supposed to do. For the other approaches, it was
only shown when the robot intended to interact and with
which item. A richer visualization of the robot’s current
intentions as well as likely future would be advantageous.

ACKNOWLEDGMENT
The authors would like to thank Prof. Henrik I.
Christensen and the Institute for Robotics & Intelligent
Machines at the Georgia Institute of Technology for
facilitating this joint work and for providing laboratory
space for development and initial tests.
References

VI. CONCLUSIONS
Collaborative robotics is a new and rapidly expanding
field with many challenges and opportunities. In this
paper, we propose to improve human-robot interaction
with collaborative robots by projecting task information
as well as the state and intentions of the robot into
human environments. The environments that serve as
canvas can include both workspaces that are shared
between robots and humans, as well as objects that are
tracked in real-time. The primary contribution of this
paper is a object-aware, projection approach for humanrobot interaction. As a secondary contribution, we identify a reduction of the computer vision problem which
allows us to propose a working solution for simultaneous
tracking and projection. In our experiments, we show
that the approach robustly tracks both simple objects,
e.g., cuboids, as well as highly complex objects, e.g., car
doors.
The usability of the projection-based interaction approach is evaluated in a comparative user study against
two common interaction approaches; one displaying information on a monitor and one with information as
printed text. Evaluation is performed according to ISO
9241-11 (1998) [16] as a combination of efficiency, effectiveness, and satisfaction. The efficiency, measured as the
time consumption for carrying out tasks, is similar for all
methods. The effectiveness, measured as the number of
questions and errors from the test participants, is good
for all approaches but slightly better for the projection
based approach. The user satisfaction is on average best
for the projection based approach. The test persons
were particularly satisfied with the ease and supporting
information of this approach. Usability is better for the
projection based approach than for the other approaches.
While not conclusive, these results does indicate a potential of projection based approaches in improving the
interaction quality in human-robot interaction.
As future research, we intend to conduct additional
experiments with a larger pool of participants in order to
further investigate this issue and draw more statistically
significant conclusions. A large scale comparative usability study with a more involved collaboration scenario
could shed further light on the strengths and weaknesses
of projection based interaction. Also, based on user
feedback, even richer information could be provided as
projections. This should include more precise information
on what the robot intends to do, as well as indications on
what the following actions of the robot are likely going
to be.

[1] EUROP, “Robotics visions to 2020 and beyond - the strategic
research agenda for robotics in europe (SRA),” EUROP, Tech.
Rep., 2009.
[2] K. Ishii, S. Zhao, M. Inami, T. Igarashi, and M. Imai, “Designing laser gesture interface for robot control,” in HumanComputer Interaction–INTERACT 2009. Springer, 2009, pp.
479–492.
[3] S. Omidshafiei, A.-A. Agha-Mohammadi, Y. F. Chen, N. K.
Ure, J. P. How, J. Vian, and R. Surati, “Mar-cps: Measurable
augmented reality for prototyping cyber-physical systems,”
2015.
[4] F. Ghiringhelli, J. Guzzi, G. Di Caro, V. Caglioti, L. M.
Gambardella, A. Giusti et al., “Interactive augmented reality for understanding and analyzing multi-robot systems,” in
Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ
International Conference on. IEEE, 2014, pp. 1195–1201.
[5] F. Leutert, C. Herrmann, and K. Schilling, “A spatial augmented reality system for intuitive display of robotic data,” in
Proceedings of the 8th ACM/IEEE international conference
on Human-robot interaction. IEEE Press, 2013, pp. 179–180.
[6] R. T. Chadalavada, H. Andreasson, R. Krug, and A. J.
Lilienthal, “That’s on my mind! robot to human intention
communication through on-board projection on shared floor
space,” 2015, presented at European Conference on Mobile
Robots (ECMR).
[7] A. Dragan and S. Srinivasa, “Generating legible motion,” in
Robotics: Science and Systems, June 2013.
[8] A. Dragan, S. Bauman, J. Forlizzi, and S. Srinivasa, “Effects
of robot motion on human-robot collaboration,” in HumanRobot Interaction, March 2015.
[9] A. Dragan, R. Holladay , and S. Srinivasa, “Deceptive robot
motion: Synthesis, analysis and experiments,” Autonomous
Robots, July 2015.
[10] L. Takayama, D. Dooley, and W. Ju, “Expressing thought:
Improving robot readability with animation principles,” in
Human-Robot Interaction (HRI), 2011 6th ACM/IEEE International Conference on, March 2011, pp. 69–76.
[11] J. Mainprice, E. A. Sisbot, T. Siméon, and R. Alami, “Planning safe and legible hand-over motions for human-robot
interaction,” IARP workshop on technical challenges for dependable robots in human environments, vol. 2, no. 6, p. 7,
2010.
[12] F. Stulp, J. Grizou, B. Busch, and M. Lopes, “Facilitating
Intention Prediction for Humans by Optimizing Robot Motions,” in International Conference on Intelligent Robots and
Systems (IROS), 2015.
[13] C. Choi, H. Christensen et al., “Real-time 3d model-based
tracking using edge and keypoint features for robotic manipulation,” in Robotics and Automation (ICRA), 2010 IEEE
International Conference on. IEEE, 2010, pp. 4048–4055.
[14] J. Canny, “A computational approach to edge detection,” Pattern Analysis and Machine Intelligence, IEEE Transactions
on, no. 6, pp. 679–698, 1986.
[15] C. Choi and H. I. Christensen, “Robust 3d visual tracking using particle filtering on the special euclidean group:
A combined approach of keypoint and edge features,” The
International Journal of Robotics Research, vol. 31, no. 4, pp.
498–519, 2012.
[16] W. ISO, “9241-11. ergonomic requirements for office work with
visual display terminals (vdts),” The international organization for standardization, 1998.
[17] J. R. Lewis, “Psychometric evaluation of an after-scenario
questionnaire for computer usability studies: the asq,” ACM
SIGCHI Bulletin, vol. 23, no. 1, pp. 78–81, 1991.

301

2014 14th IEEE-RAS International Conference on
Humanoid Robots (Humanoids)
November 18-20, 2014. Madrid, Spain

Transfer Entropy for Feature Extraction in Physical Human-Robot
Interaction: Detecting Perturbations from Low-Cost Sensors
Erik Berger1 , David Müller1 , David Vogt1 , Bernhard Jung1 , Heni Ben Amor2
Abstract— In physical human-robot interaction, robot behavior must be adjusted to forces applied by the human
interaction partner. For measuring such forces, special-purpose
sensors may be used, e.g. force-torque sensors, that are however
often heavy, expensive and prone to noise. In contrast, we
propose a machine learning approach for measuring external
perturbations of robot behavior that uses commonly available,
low-cost sensors only. During the training phase, behaviorspecific statistical models of sensor measurements, so-called
perturbation filters, are constructed using Principal Component
Analysis, Transfer Entropy and Dynamic Mode Decomposition.
During behavior execution, perturbation filters compare measured and predicted sensor values for estimating the amount
and direction of forces applied by the human interaction
partner. Such perturbation filters can therefore be regarded
as virtual force sensors that produce continuous estimates of
external forces.

I. I NTRODUCTION
Autonomous robots require accurate sensing capabilities in
order to act in an intelligent and meaningful way within their
environment. In particular human-robot interaction tasks
require sensors for measuring physical contact with a human
partner. Recorded measurements can be used by a robot to
ensure safety during interactions and to react to physical
perturbations. To this end, it is important that both the occurrence as well the magnitude of an external perturbation, e.g.,
a push, are reliably detected. Existing sensing technologies,
such as force-torque sensors, are often heavy, expensive, and
noise-prone. However, there are numerous affordable lowcost sensors available which, while not directly measuring
perturbation forces, can be used to generate estimates of
external perturbations.
In this paper, we present an approach for perturbation detection which is based on a combination of low-cost sensors
and machine learning techniques. During a training phase,
we extract a compact representation, called a perturbation
filter, which specifies the evolution of sensor readings during
regular execution of a motor skill. The extraction is guided
by information-theoretic measures such as Transfer Entropy,
that determine the relevance of a specific sensor w.r.t. the
executed robot behavior. In contrast to our previous work [1],
we will not use any higher level stability parameters, such as
the center-of-mass, center-of-pressure, or zero-moment-point
for learning. Instead, we will learn the perturbation filter from
low-level sensor data, solely. As a result, no knowledge about
the robot kinematics or dynamics is required.

Fig. 1: A NAO robot estimates the influence of external
perturbations applied by a human interaction partner to its
current behavior execution.

After a perturbation filter is learned, it is used to generate
a continuous estimate of the amount of external human
perturbations. During physical interaction between a robot
and a human, the estimated perturbations can be used to
compensate for the external forces or infer the intended
guidance of a human interaction partner. The presented
perturbation filter can be regarded as a virtual force sensor
that produces a continuous estimate of external forces.
II. R ELATED W ORK
In recent years, natural and intuitive approaches to HRI
have gained popularity. Various researchers have proposed
the so-called soft robotics paradigm: compliant robots that
“can cooperate in a safe manner with humans” [2]. An important robot control method for realizing such a compliance
is impedance control [3]. Impedance control can be used
to allow for touch based interaction and human guidance.
To this end, impedance controllers require accurate sensing
capabilities, in the form of force-torque sensors. However,
such sensors are typically heavy, expensive and suffer from
significant noise. Other sensors, such as torque sensors are

1 Institute of Computer Science, Technical University Bergakademie
Freiberg, Bernhard-von-Cotta-Str. 2, 09599 Freiberg, Germany
2 Institute for Robotics and Intelligent Machines, Georgia Institute of
Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA

978-1-4799-7174-9/14/$31.00 ©2014 IEEE

829

Offline

Data Acquisition
SensorData
Data
Sensor
Training
Data

Feature Extraction
PCA
+
Transfer
Entropy

Feature

Training
Space
Data

Data Model

Interpolation
Dynamic Mode
Decomposition

Online

Target

Live

DataData
Sensor

Live Interaction

Distance
Measurement
using DTW

Feature Space
Projection

Target Estimation

Perturbation

Perturbation Estimation

Fig. 2: An overview of the presented machine learning approach. Training data, together with a labeling target vector will be
processed using Principal Component Analysis, Transfer Entropy and Dynamic Mode Decomposition algorithms, providing
a training data model of vectors comprising the Feature Space. During live interaction, the recorded data is being projected
into this space and mapped to the nearest data model vector and its target vector using Dynamic Time Warping.

even more prone to issues related to noise and drift.
Still, the ability to sense physical influences is at the core of
recent advances made in the field of HRI. For example, Lee
et al. [4] use impedance control and force-torque sensors in
order to realize human-robot interaction during programming
by demonstration tasks. Wang et al. [5] present a robot
adapting its dancing steps based on the external forces
exerted by a human dance partner. Ben Amor et al. [6] use
touch information to teach new motor skills to a humanoid
robot. Touch information is only used to collect data for
subsequent learning of a robotic motor skill. Robot learning
approaches based on such kinesthetic teach-in have gained
considerable attention in the literature, with similar results
reported in [7] and [8]. A different approach aiming at
joint physical activities between humans and robots has been
reported in [9]. Ikemoto et al. use Gaussian mixture models
to adapt the timing of a humanoid robot to that of a human
partner in close-contact interaction scenarios. This approach
significantly improves physical interactions, but is limited to
learning timing information.
Stückler et al. [10] present a cooperative transportation
task where a robot follows the human guidance using arm
compliance. In doing so, the robot recognizes the desired
walking direction through visual observation of the object
being transported. A similar setting has been investigated
by Yokoyama et al. [11]. They use a HRP-2P humanoid
robot with a biped locomotion controller and an aural human
interface to carry a large panel together with a human. Forces
measured with sensors on the wrists are utilized to derive the
walking direction.
The main disadvantage of the above approaches is that
they require special aural and visual input devices or force

sensors, which are not present on many robot platforms. Additionally, none of the approaches using force-torque sensors
addresses the problem of uncertainty in the measurements.
As a result, all of these approaches assume high-quality
sensing capabilities and low-speed execution of the joint
motor task. We propose a new filtering algorithm that can
learn the natural variation in sensor values as a motor skill
is executed.
III. A PPROACH
The objective of the presented method is to estimate the
strength and direction of external perturbations caused by
a human interaction partner. To infer these estimates from
low-cost sensor readings, we condition behavior-specific
perturbation filters. An overview of the approach can be seen
in Figure 2. First, we record training data for a behavior
with different parameter configurations, e.g., varying step
lengths during walking. In this data acquisition phase, no
external perturbations from humans are applied. Thereafter,
the training data is used to create a Feature Space data model
during feature extraction. Linear combinations of different
sensors are weighted by their relevance to the observed
parameter and projected into the low-dimensional Feature
Space. In the following, the configuration parameter will
be referred to as the target vector. The relevance of a
specific sensor to the target vector is extracted using Transfer
Entropy [12] (TE). In this context, TE is used as a measure of
predictability and information flow between the target vector
and the conduct of sensors. Sensors that have a high TE
w.r.t. the robot’s behavior are deemed more influential and
relevant.
During behavior execution, an external perturbation is de830

tected by comparing the recorded training data to the current sensor data within the low-dimensional Feature Space.
Dynamic Time Warping (DTW) [13] is used as a distance
function in order to include the temporal pattern for the comparison. The estimation of a perturbation value is performed
by comparing the current sensor readings to the sensor
readings acquired during training. The perturbation value
is then inferred from the difference between the currently
configured behavior parameter, e.g., the currently employed
step length, and the estimated behavior parameter which
produced similar sensor readings during training.
In the following section, we will depict each step of our
approach in more detail. We will describe how to perform
feature space extraction and how to use the resulting embedding to estimate a continuous perturbation value.

carries. The eigenvector with highest eigenvalue is the direction with highest variance. Hence, in traditional PCA the
relevance of a feature is defined by the observed variance
along that dimension.
Instead of using the variance to infer the relevance of a
feature, we will focus on the relationship between the feature
values and the future state of the robot. Features that have a
strong statistical coherence with the future state of the robot
are more likely to be relevant. In other words, a feature is
deemed relevant, if its past activity is a good predictor of
the robot’s next state. From an information theoretic point
of view, this type of relationship can be estimated using
Transfer Entropy. [12]
We employ TE in order to measure the directed information
transfer between the target and each PC vector separately.
TE is a recently introduced information-theoretic measure,
which has been used extensively in diverse fields of science
[15][16][17].

A. Data Acquisition
The first step in our approach is to record training data
that reflects the evolution of sensor values during the regular execution of a motor skill. To this end, we perform
the investigated motor skill with varying parameter values,
e.g. varying step lengths during walking. For generalization
purposes, it is important to record the motor skill under large
a set of possible target parameter configurations. However,
since the parameter space may have a dynamic range, this can
lead to a time-consuming recording phase, which in consequence leads to wear and tear of the robot hardware. To avoid
a lengthy training session, Dynamic Mode Decomposition
can be used to learn a model of the sensor data using few
training samples. This process is not being detailed in this
publication, the interested reader is referred to our previous
work [14].
The training data is sampled equidistantly with 100Hz.
Please note, that we only record low-level sensor data.
Preprocessed variables, such as center-of-mass or the zero
moment point are not included in this process. In contrast
to our previous work, we will automatically identify and
combine relevant low-level sensor data.
To prevent a comparison between sensors of different units
(i.e. comparing angles with pressure values), a sensor group
is assigned to each sensor, enabling to deduce conclusions
from their individual relations.

T EJ→I =

X

p(it+1 , it , jt )log2

p(it+1 |it , jt )
p(it+1 |it )

(1)

TE quantifies the incorrectness of the assumption, that in the
absence of information flow from system J to system I, the
state of J has no influence on the transition probabilities on
system I [12].
To compute the TE, the conditional probability as well as
joint probability of co-occurrences in J and I is required.
To estimate these probabilities without resorting to density
estimation we quantize the sensor values and use a frequentist approach. The optimal parameters for the quantization are
empirically determined. We discretize each PC vector using
a quantile based transformation, which has the advantages of
stability and independence of input value transformations.
Variations of the robot’s target vector could possibly have a
time-delayed impact on sensory data. However, the standard
TE algorithm only allows to draw conclusions based on
transitions of a one-step delay between samples of J and I.
A more general approach can be implemented using Delayed
Transfer Entropy [18] which introduces multiple possible
time delays.
p(it+1 |it , jt+1−d )
p(it+1 |it )
(2)
We calculate the Transfer Entropy peak values T ET∗ →Pi =
argmax∀j T ET →P (dj ) between the target vector T and each
principal component Pi over a number of time delays dj
within a preset time delay window D, ∪∀j dj = D. See
Figure 3 for further details. These T E ∗ peak values are then
used to scale the previously acquired principal components,
such that components of higher T E ∗ values are stretched and
those of lower T E ∗ values are shrunk. The resulting scaled
PCA space is the so-called Feature Space. As mentioned
earlier, within the feature space, the relevance of variables
depends on their influence on the target vector.
The feature space projection of the acquired training data set
is now stored as the feature space data model.
T EJ→I (d) =

B. Feature Extraction
The next step in our approach is to extract relevant features
from the stream of sensor data. While it is often possible
to acquire a large number of different sensor values, we
are typically faced with significant redundancy and noise.
Additionally, it is often unclear which of these readings we
should pay attention to. Feature extraction can help to single
out important parts of a sensor stream.
For feature extraction, we first compute a low-dimensional
embedding of the sensor data by performing a PCA-like
procedure. We extract the principal components of the feature
space (called PC vectors) using an eigenvector decomposition of the sensor data matrix. In traditional PCA the
eigenvalues define how much information each eigenvector
831

X

p(it+1 , it , jt+1−d )log2

Target

TEJ I(d)
... d=20

PCA Space

...

...

PCn

0.8

...

0.6

...

...

...

0.1

...

0.3

2. PC

PC1

Feature Space

1. PC

→

d=1

4. PC

3. PC

Fig. 3: The Transfer Entropy measures the target vector’s
(red graph) influence on each PC vector for each time delay
(black arrow for d=1, orange arrow for d=20). The TE peak
value of each PC component (marked by red squares) is the
largest of each delayed TE value.

0

5

10

15

20

25

Time [s]

Fig. 4: The principle components (blue) and the principle
components scaled with TE (red) of a walking gait’s training
data. The third principle component has the highest TE value
and is stretched while the others are dampened. The scaled
principle components comprise the low dimensional Feature
Space.

C. Target Estimation
The next step is to use this data model to continuously
estimate current target behavior values and to detect and
qualify possible perturbations upon live human-robot interaction. To do so, a time window of raw sensor data
is projected into the previously specified low dimensional
Feature Space. To identify the most similar segment of the
projected training data, we use the subsequence dynamic time
warping technique (SDTW) [19].
Y ∗ (v) = argmin Γ(X, Y ).

robot should react to a perturbation depends on the specific
use-case and is left open for further research at this point.

(3)

IV. E XPERIMENTS

For this, the SDTW algorithm Γ is measuring the distance
between two temporal sequences X = (x1 , . . . , xN ) and
Y = (y1 , . . . , yM ) of length N ∈ N and M ∈ N. In our
specific case, the goal is to find a subset of the training data
Y ∗ with minimal distance to the projection of the currently
observed sequence X. As a result, the target behavior value
v of the corresponding training data subset can be used as
an estimation for the current one.
Since we captured the behavior for a discrete set of target
behavior values we can only make estimations for these. An
efficient way to expand our data model to cover continuous
behavior parameters can be achieved using interpolation
schemes. Therefore, we use a novel interpolation method
from fluid dynamics, the so called Dynamic Mode Decomposition (DMD) [20][21]. A detailed explanation of DMD
and how it can be used for the interpolation of robot sensor
data can be found in our previous publication [14].

In the following section, we evaluated our approach using
a NAO robot from Aldebaran Robotics. To do so, we
recorded a total of 52 seconds of training data from the
robot’s walking gait with step lengths between 3 cm and
−3 cm. Each sample contains readings of the angle and
pressure sensors. Next, we interpolated these samples with
a resolution of 0.01 cm utilizing Dynamic Mode Decomposition. Retaining 95 % of information we applied Principal
Component Analysis on the robots 24 angle sensors and its 8
foot pressure sensors separately resulting in a 4d-angle space
and a 6d-pressure space. Finally, each principal component
is scaled by its delayed Transfer Entropy whereas the target
value equals the gaits step-length. The resulting dimensions
of the low dimensional Feature Space are shown in Figure 4.
While the first, second and fourth dimensions are dampened,
the third dimension is stretched. This is due to the fact that
PCA extracts the most characteristic properties of a behavior
and not its dependency on the adjusted parameter.
Figure 5 shows the vector length of P C · T E ∗ , the resulting
Feature Space vectors compared to the simultaneous longitudinal center-of-mass which was used extensively in our
previous research [1] [14]. Obviously, they are very similar
even though our new approach has no knowledge about the
kinematic chain or the mass distribution of the robot.

D. Reaction
Accordingly, we can generate an estimate for the amount
of possible interfering external forces Ê by calculating the
difference between the configured behavior parameter P used
to control the robot and the estimated behavior parameter P̂
identified by the learned model for each sensor group.
Ê = P̂ − P

(4)

A. Estimation Quality
We utilize the learned low dimensional Feature Space data
model during runtime while the robot frequently reduces
its step-length. Figure 6 shows the resulting mean absolute
error (M AE) for the angle and pressure sensor groups. The
angles are especially influenced by spurious relationships,
which are strongly dampened by PCA, even without TE.

Thus, our approach can be used in scenarios where a robot
has to detect and react to external perturbations in order to
fulfill a specified task. Certain sensor groups will be suitable
to qualify certain perturbations, allowing conclusions about
perturbation characteristics. This and details about how the
832

Prediction
Configuration

|P C · T E ∗ |

Step Length [cm]

3

Longitudinal Center of Mass

2.5

2

1.5

1

0

13

26

39

0.5
0

52

Time [s]

¬PCA ¬TE

¬PCA TE

PCA ¬TE

PCA TE

for in human-robot interaction scenarios.
C. Perturbation Estimation
In this experiment, the human perturbs the robot during
execution of a walking gait. To verify the correctness and
universality of our approach, the perturbations are applied
to different parts of the robot. Figure 8 shows the resulting
parameter estimations for the angle and pressure groups as
well as the configured parameter value. Perturbation (a) is
recognized by both sensor groups, because the angles as well
as the force sensors are affected. If no external perturbation
is recognized during (b), the parameter estimations of both
sensor groups give a close approximation of the configured
parameter value. However, in (c) the perturbation does not
affect angles and in consequence can’t be measured by the
angle but by the pressure group. Finally, perturbation (d)
leads to a measurement of flat zeros for each pressure sensor,
which is not part of the training data set and, consequently,
can only be recognized by the angle group.
This confirms our assumption, that redundant sensor groups
can help to recognize and qualify a variety of perturbations.

2.5

M AE

15

Fig. 7: The configured behavior parameter (red) decreases
over time. The estimated behavior parameter (blue) recognized the robot’s reactions with a time delay, since the robot
needs to finish the current step before adapting to the new
parameter.

3

2
1.5
1
0.5
0

10

Time [s]

Fig. 5: The vector length of P C · T E ∗ compared to the
simultaneous longitudinal center-of-mass. In contrast to the
center-of-mass our approach does not need any knowledge
about the robots kinematics or mass distribution to generate
a comparable result.

3.5

5

Angle Group

Pressure Group

Fig. 6: The mean absolute error prediction results in cm.
Left: The angle group error with all permutations of PCA and
TE. Right: The pressure group error with all permutations of
PCA and TE. Obviously, using a combination of PCA and
TE increases the accuracy of the estimation.

Furthermore, as shown by the pressure group, PCA fails to
identify the relations between the sensors and the behavior
parameter. However, our Feature Space, combining PCA and
TE, increases the accuracy of the estimation for both sensor
groups allowing to better predict perturbations solely based
on raw low cost sensor data without further knowledge of
the robot’s anatomy.

V. C ONCLUSION
In this paper, we presented an approach for estimating
external perturbations during physical human-robot interaction tasks. Instead of using expensive force-torque sensors,
we leverage available information from low-cost sensors. To
this end, we introduced a machine learning approach that
can learn behavior-specific perturbation filters in software.
In turn, these filters can be used to generate a continuous estimate of the inflicted external perturbations. An informationtheoretic measure, in particular Transfer Entropy, is used to
guide the feature extraction process. Given a set of low-level
sensor data, our approach allows for the automatic identification of relevant sensor values by calculating the information
flow from sensors to future robot states. We have shown,
that this approach automatically leads to the emergence of

B. Parameter Estimation
In this experiment, we measure the robot’s hardware
delay using the 52 seconds of angle training data without
interpolation. For this, the step length is reduced from three
to one centimeters over a period of 15 seconds with a
sliding window of a 0.25 seconds. Figure 7 shows that
the robot needs about one second to react to parameter
reconfigurations. This indicates, that the robot has an average
hardware delay of 0.75 seconds which needs to be accounted
833

(a) Pulling

2.5

(b) No Perturbation

(c) Pushing

(d) Lifting

Step Length [cm]

2

1.5

1

0.5

0
0

Angle Estimation
5

Pressure Estimation
10

15

Configured Value
20

25

Time [s]

30

35

40

45

50

Fig. 8: The estimated perturbation value Ê for each of the external perturbations is the difference between the estimated
parameter P̂ and the configured parameter value P , as defined in Formula 4. (a) can be detected by both the angle and the
pressure sensors while the perturbations in (c) and (d) can only be detected by one of the sensor groups.

features that are remarkably similar to the center-of-mass,
without actually having to provide prior knowledge about the
robot kinematics or mass distributions. The automatic determination of these features is important, since manufacturersupplied mass distributions are effectively invalidated in tasks
where the robot is carrying weights. Further characterization
of causing effects and details about how to react to certain
perturbations should be investigated in future work. Using
spatial groupings of sensors on the robot could be used to
localize the external influences.

[9] S. Ikemoto, H. Ben Amor, T. Minato, B. Jung, and H. Ishiguro,
“Physical human-robot interaction: Mutual learning and adaptation,”
IEEE Robotics & Automation Magazine, vol. 19, no. 4, pp. 24–35,
2012.
[10] J. Stückler and S. Behnke, “Following human guidance to cooperatively carry a large object,” in Humanoids’11, 2011, pp. 218–223.
[11] K. Yokoyama, H. Handa, T. Isozumi, Y. Fukase, K. Kaneko, F. Kanehiro, Y. Kawai, F. Tomita, and H. Hirukawa, “Cooperative works by
a human and a humanoid robot,” in Proceedings. ICRA ’03. IEEE
International Conference on Robotics and Automation 2003., vol. 3,
2003, pp. 2985–2991 vol.3.
[12] T. Schreiber, “Measuring information transfer,” Physical Review Letters, vol. 85, no. 2, pp. 461–464, 2000.
[13] H. Sakoe, “Dynamic programming algorithm optimization for spoken
word recognition,” IEEE Transactions on Acoustics, Speech, and
Signal Processing, vol. 26, pp. 43–49, 1978.
[14] E. Berger, M. Sastuba, D. Vogt, B. Jung, and H. Ben Amor, “Dynamic
mode decomposition for perturbation estimation in human robot
interaction,” in RO-MAN, 2014 IEEE, 2014.
[15] S. Ito, M. E. Hansen, R. Heiland, A. Lumsdaine, A. M. Litke, and
J. M. Beggs, “Extending transfer entropy improves identification of
effective connectivity in a spiking cortical network model,” PloS one,
vol. 6, no. 11, p. e27431, 2011.
[16] M. P. Machado, C. W. Kulp, and D. Schlingman, “Composition and
analysis of music using mathematica,” Mathematica in Education and
Research, p. 1, 2007.
[17] S. K. Baek, W.-S. Jung, O. Kwon, and H.-T. Moon, “Transfer entropy
analysis of the stock market,” arXiv preprint physics/0509014, 2005.
[18] S. Ito, M. E. Hansen, R. Heiland, A. Lumsdaine, A. M. Litke, and
J. M. Beggs, “Extending Transfer Entropy Improves Identification of
Effective Connectivity in a Spiking Cortical Network Model,” PLoS
ONE, vol. 6, no. 11, pp. e27 431+, Nov. 2011.
[19] M. Müller, Information Retrieval for Music and Motion. Secaucus,
NJ, USA: Springer-Verlag New York, Inc., 2007.
[20] P. J. Schmid, “Dynamic mode decomposition of numerical and experimental data,” Journal of Fluid Mechanics, vol. 656, pp. 5–28, 8
2010.
[21] C. W. Rowley, I. Igor Mez, I. Shervin Bagher, R. Philipp Schlatte, and
D. S. Henningson, “Spectral analysis of nonlinear flows,” Journal of
Fluid Mechanics, vol. 641, no. -1, pp. 115–127, 2009.

R EFERENCES
[1] E. Berger, D. Vogt, N. Haji-Ghassemi, B. Jung, and H. Ben Amor,
“Inferring guidance information in cooperative human-robot tasks,” in
Humanoids’13, 2013.
[2] A. Albu-Schäffer, O. Eiberger, M. Fuchs, M. Grebenstein, S. Haddadin, C. Ott, A. Stemmer, T. Wimböck, S. Wolf, C. Borst, and
G. Hirzinger, “Anthropomorphic soft robotics from torque control to variable intrinsic compliance,” in Robotics Research, ser.
Springer Tracts in Advanced Robotics, C. Pradalier, R. Siegwart, and
G. Hirzinger, Eds. Springer Berlin Heidelberg, 2011, vol. 70, pp.
185–207.
[3] S. Haddadin, Towards Safe Robots - Approaching Asimov’s 1st Law.
Springer, 2014.
[4] D. Lee and C. Ott, “Incremental kinesthetic teaching of motion
primitives using the motion refinement tube,” Autonomous Robots,
vol. 31, no. 2-3, pp. 115–131, 2011.
[5] H. Wang and K. Kosuge, “Control of a robot dancer for enhancing
haptic human-robot interaction in waltz,” IEEE Trans. Haptics, vol. 5,
no. 3, pp. 264–273, Jan. 2012.
[6] H. Ben Amor, E. Berger, D. Vogt, and B. Jung, “Kinesthetic bootstrapping: Teaching motor skills to humanoid robots through physical
interaction,” in KI 2009: Advances in Artificial Intelligence. Springer
Berlin Heidelberg, 2009, pp. 492–499.
[7] S. Calinon, Robot programming by demonstration: A probabilistic
approach. EPFL Press, 2009.
[8] J. Kober and J. Peters, “Policy search for motor primitives in robotics,”
Machine Learning, vol. 84, no. 1, pp. 171–203, 2011.

834

Probabilistic Modeling of Human Movements for
Intention Inference
Zhikun Wang1,2 , Marc Peter Deisenroth2 , Heni Ben Amor2 , David Vogt3 , Bernhard Schölkopf1 , Jan Peters1,2
1
MPI for Intelligent Systems, Spemannstr. 38, 72076 Tübingen, Germany.
2
TU Darmstadt, Hochschulstr. 10, 64289 Darmstadt, Germany.
3
TU Freiberg, Bernhard-von-Cotta Str. 2, 09596 Freiberg, Germany.
{zhikun, bs}@tuebingen.mpg.de {marc, amor, peters}@ias.tu-darmstadt.de

Abstract—Inference of human intention may be an essential
step towards understanding human actions and is hence important for realizing efficient human-robot interaction. In this paper,
we propose the Intention-Driven Dynamics Model (IDDM), a
latent variable model for inferring unknown human intentions.
We train the model based on observed human movements/actions.
We introduce an efficient approximate inference algorithm to
infer the human’s intention from an ongoing movement. We
verify the feasibility of the IDDM in two scenarios, i.e., target inference in robot table tennis and action recognition for interactive
humanoid robots. In both tasks, the IDDM achieves substantial
improvements over state-of-the-art regression and classification.

I. I NTRODUCTION
Recent advances in sensors and algorithms allow for robots
with improved perception abilities. For example, robots can
now recognize human poses in real time using depth cameras
[22], which can enhance their ability to interact with humans.
However, effective perception alone may not be sufficient for
Human-Robot Interaction (HRI), since the robot’s reactions
should depend on understanding the human’s action. An important understanding problem is inferring the others’ intention
(also referred to as goal, target, desire, plan) [23], which
humans heavily rely on, for example, in sports, games and
social activities. Humans can learn and improve the ability of
prediction by training. For example, skilled tennis players are
usually trained to possess better anticipation than amateurs.
This observation raises the question of how a robot can also
learn to infer the underlying intention from human movements.
In order to infer the intention from an ongoing movment,
we first address its inverse problem, i.e., modeling how the
movement is governed by the intention. This idea is built
upon the hypothesis that a human movement usually follows
a goal-directed policy [4, 10]. The human movement considered here is represented by a time series of observations.
This makes discrete-time dynamics models a straightforward
choice for movement modeling and intention inference. In a
robotic scenario, we often rely on high-dimensional and noisy
sensor data. However, the intrinsic dimensionality is typically
much smaller. Therefore, we seek a low-dimensional latent
representation of the relevant information in the data, and
then model how the intention governs the dynamics in this
low-dimensional state space. Jointly considering both the lowdimensional representation and the latent dynamics leads to

(a) Robot table tennis.

(b) Interactive humanoid robot

Fig. 1: Two representative HRI scenarios where intention
inference plays an important role: (a) target inference in robot
table tennis games. (b) action recognition for human-robot
interaction.
smooth trajectories in latent space with respect to the intention.
In this paper, we exploit this smoothness property and propose the Intention-Driven Dynamics Model (IDDM), in which
the dynamic in the latent states is driven by the intention of the
human action/behavior. The IDDM can simultaneously find
a good low-dimensional representation of high-dimensional
and noisy observations and describe the dynamics in the lowdimensional latent space. Using the learned model, human
intention can be inferred from an ongoing movement. As
exact inference is not tractable in our model, we propose
an approximate inference algorithm and show that it can
efficiently infer the intention of a human partner.
To verify the feasibility of the proposed methods, we discuss
two representative scenarios where intention inference plays an
important role in human-robot interactions:
(1) Target inference in robot table tennis. We consider
human-robot table tennis games, as shown in Fig. 1a. The
robot’s hardware constraints impose strong limitations on its
flexibility. It requires sufficient time to execute a ball-hitting
plan: movement initiation to an appropriate preparation pose
is needed before the opponent returns the ball, to achieve the
required velocity for returning the ball [27]. The robot player
uses different preparation poses for forehand and backhand
hitting plans. Hence, it is necessary to choose between them
based on inference of the opponent’s target location.
(2) Action recognition for interactive humanoid robot. In
this setting, we use our technique to improve the interaction
capabilities of a NAO humanoid robot, as shown in Fig. 1b. In

measured from the d-dimensional state space X to the Ddimensional observation space Z, given by

g

zt = Wz̃t ,
x1

x2

x3

z1

z2

z3

(a) GPDM

···

x1

x2

x3

z1

z2

z3

···

(b) IDDM

Fig. 2: Graphical models of the Gaussian process dynamical
model (GPDM) and the proposed intention-driven dynamics
model (IDDM). The proposed model explicitly incorporates
the intention as an input to the transition function.
order to realize natural and compelling interactions, the robot
needs to correctly recognize the actions of its human partner.
This ability, in turn, allows it to act in a proactive manner. We
show that the IDDM has the potential to identify the intention
of action from movements in a simplified scenario.
The paper is organized as follows. We present the IDDM
and address the problem of training in Section II. In Section III, we study efficient approximate algorithms for intention inference. We verify the feasibility of the proposed
methods in the two scenarios in Section IV and V. Finally,
we provide a brief review of related work in Section VI, and
conclude in Section VII. 1
II. I NTENTION -D RIVEN DYNAMICS M ODEL
We propose the Intention-Driven Dynamics Model (IDDM),
which is an extension of the Gaussian Process Dynamical
Models (GPDM) [25]. The GPDM provides a nonparametric
approach to learning the transition function in the latent state
space and the measurement mapping from states to observations simultaneously. As shown in Fig. 2a, the transition
function in GPDM is only determined by the latent state.
However, in the cases considered in this paper, the underlying
intention, as an important drive of human movements, can
hardly be discovered directly from the observations or the
estimated latent states. Considering that the dynamics can be
substantially different when the actions are based on different
intentions, we propose the IDDM, the graphical model of
which is shown in Fig. 2b. The IDDM explicitly incorporates
the intention into the transition function in the latent state
space. The dynamics model is inspired by the hypothesis that
the human action is directed by the goal [4, 10]. For example,
in table tennis, the player swings the racket in order to return
the ball to the intended target.
The observations of a movement comprise a time series of
observations z1:T , [z1 , . . . , zT ]. For notation simplicity and
without loss of generality, we consider that all the observations
of movements have the same length T . In the proposed
generative model, we assume that the observation zi are
1 Supplementary technical details, demos and results can be found at
http://www.ias.tu-darmstadt.de/Research/ProbabilisticMovementModeling

z̃t = h(xt ) + nz,t ,

nz,t ∼ N (0, Σz,t ) , (1)

where W = diag(w1 , . . . , wD ), i.e., the i-th dimension of
observations zt is the scaled i-th dimension of the outputs
z̃t with a parameter wi . The scaling parameters W allow for
dealing with raw features that are measured in different units,
such as positions and velocities. We place a Gaussian process
(GP) prior distribution on every dimension of the unknown
function h [20], which can be marginalized out during learning
and inference. A GP is fully specified by a mean function
mz (·) and a positive semidefinite covariance (kernel) function
kz (·, ·). The predictive probability of the observations zt is
given by a Gaussian distribution zt ∼ N (mz (xt ), Σz (xt )) .
We consider first-order Markovian dynamics, see Fig. 2b,
which is modeled by a latent transition function f , given by
xt = f (xt−1 , g) + nx,t ,

nx,t ∼ N (0, Σx,t ) .

(2)

The state at time t is determined by the latent state at time
t − 1 as well as by the intention g. We place a GP prior
GP(mx (·), kx (·, ·)) on every dimension of f and marginalize
it out. Then, the predictive distribution of the latent state
xt conditioned on xt−1 and the intention g is a Gaussian
distribution given by xt ∼ N (mx (xt−1 , g), Σx (xt−1 , g)).
To summarize, in the proposed IDDM, one set of GPs models the transition function in the latent space conditioned on
the intention g. A second set of GPs models the measurement
mapping from the latent states x and the observations z.
A. Covariance Functions
For simplicity, we use GP prior mean functions that are zero
everywhere, i.e., mz (·) ≡ 0 and mx (·) ≡ 0. Hence, the model
is determined by the covariance functions kz (·, ·) and kx (·, ·),
which will be motivated in the following.
The underlying dynamics of human motion are usually
nonlinear. To account for nonlinearities, we use a flexible
Gaussian tensor-product covariance function with automatic
relevance determination for the dynamics, i.e.,
kx ([x, g], [x0 , g0 ]; α) = kx (x, x0 ; α)kx (g, g0 ; α)
(3)


−1
0 2
0 2
1
1
= α1 exp − 2 kx − x kΛx − 2 kg − g kΛg + α4 δxx0 δgg0 ,
where the shorthand notation kak2Λ , aT Λa, the diagonal matrices Λx = diag(α21 , . . . , α2d ) > 0 and Λg =
diag(α31 , . . . , α3|g| ) > 0 weight the corresponding input
dimensions, α is the set of all hyperparameters, and δ is
the Kronecker delta function. When the intention g is a
discrete variable, we set the hyperparameter α3 = ∞ such
that kx (g, g 0 ; α) ≡ δg,g0 .
The covariance function for the measurement mapping from
the state space to observation space is chosen depending on
the task. For example, the GPDM in [25] uses an isotropic
Gaussian covariance function parameterized by the hyperparameters β


kz (x, x0 ; β) = exp − β21 kx − x0 k2 + β2−1 δx,x0 , (4)

as intuitively the latent states that generate human poses lie
on a nonlinear manifold. Note that the hyperparameters β do
not contain the signal variance, which is parameterized by the
scaling factors W. When applying it to target prediction in
table tennis games, we use the linear kernel
kz (x, x0 ; β) = xT x0 + β1−1 δx,x0 ,

(5)

as the observations are already low-dimensional, but subject to
substantial noise. Empirical comparisons of different covariance functions for the measurement mapping, i.e., the isotropic
Gaussian in Eq. (4) and the linear covariance function in
Eq. (5), will be shown in Section IV.
B. Learning the IDDM
The proposed model can be learned from a training data set
D = {Z, g} of J movements and corresponding intentions.
Each movement Zj consists of a time series of observations
given by Zj = [zj1 , . . . , zjT ]T . We construct the overall
observation matrix Z by vertically concatenating observation
matrices Z1 , . . . , ZJ , and the overall intention matrix g from
g1 , . . . , gJ . In the robot table tennis example, one movement
corresponds to a stroke of the opponent, represented by a
time series of observed racket and ball configurations. We
assume the intention g can be obtained in the training data,
for example by post-processing the data. In the robot table
tennis example, the observed intention corresponds to the
target where the opponent returns the ball to (see Fig. 3 for
an illustration).
Similar to the work by [25], we find the maximum a
posteriori (MAP) estimate of the latent states X. Alternative
learning methods and an empirical comparison can be found
in [24, 5]. Given the model hyperparameters, the posterior
distribution of latent states X can be decomposed into the
probability of observations given states and the probability of
states given intention:
p(X|Z, g, α, β, W) ∝ p(Z|X, β, W)p(X|g, α),

(6)

both obtained by the GP marginal likelihood [20].
The GP marginal probability of the observations Z given
the latent states X is given by a Gaussian distribution
p(Z|X, β, W)
=√

|W|M
(2π)M D |Kz |D




2 T
exp − 12 tr K−1
,
z ZW Z

(7)

where M , JT is the length of observations Z, and Kz is
the covariance matrix of X computed by the kernel function
kz (·, ·).
Given the intention g, the sequence of latent states X has
a Gaussian probability
p(X|g, α) = p(X1 )p(X2:T |X1:T −1 , g, α)



−1
T
1
1)
exp
−
tr
K
X
X
,
= √ p(X
2:T
x
2:T
2
md
d
(2π)

|Kx |

(8)

where Xindices is constructed by vertically concatenated state
matrices x1indices , . . . , xJindices , m , J(T − 1) is the length of
X2:T , and Kx is the covariance matrix of X1:T −1 computed

Algorithm 1: The algorithm learns the model hyperparameters α, β, and W. They are obtained by maximizing the
marginal likelihood using the Monte Carlo EM algorithm.
Input : Data: D = {Z, g}
Input : Number of EM iterations: L
Output: Model hyperparameters: Θ = {α, β, W}
1 for l ← 1 to L do
2
for i ← 1 to I do
3
Initialize X by its MAP estimate ;
4
Draw sample X(i) from p(X|Z, g, Θ) using
HMC ;
PI
5
Maximize I1 i=1 p(Z, X(i) |g, Θ) w.r.t. Θ using
SCG;

by the kernel function kx (·, ·). We use a Gaussian prior
distribution on the initial states X1 .
Based on Eqs. (7)-(8), the MAP estimate of the states is
learned by maximizing the posterior in Eq. (6). We minimize
the negative log-posterior

−1
2 T
1
L(X) = D
− M log |W|
2 log |Kz | + 2 tr Kz ZW Z


T
−1
1
1
d
+ 2 log |Kx | + 2 tr Kx X2:T X2:T + 2 tr X1 XT1 + const
with respect to the states X, using the Scaled Conjugate
Gradient (SCG) method.
C. Learning Hyperparameters
A reliable approach to learning the hyperparameters
Θ = {α, β, W}
R is to maximize the marginal likelihood
p(Z|g, Θ) =
p(Z, X|g, Θ)dX, which can be achieved
approximately by using Expectation-Maximization (EM) algorithm. The EM algorithm computes the posterior distribution
of states q(X) = p(X|Z, g, Θ) in the Expectation (E) step,
and updates the hyperparameters by maximizing the expected
data likelihood Eq [p(Z, X|g, Θ)] in the Maximization (M)
step. However, the posterior distribution q(X) is difficult to
achieve in IDDM. Following [25], we draw samples of the
states X(1) , . . . , X(I) from the posterior distribution using
Hybrid Monte Carlo (HMC) [2], and hence the data likelihood is estimated viaPMonte Carlo integration given by
I
Eq [p(Z, X|g, Θ)] ≈ I1 i=1 p(Z, X(i) |g, Θ). In the M step,
we use SCG to update the hyperparameters. In practice, we
choose the number of samples I = 100 and the number of
EM iterations L = 10. Although this procedure, as described
in Algorithm 1, is time demanding, this is not a big issue in
practice as we learn the hyperparameters off-line.
The model also depends on the hyperparameter d: the
dimensionality of the latent state space. Choosing an appropriate d is important. If the dimensionality is too small the
latent states cannot recover the observations and, therefore,
leads to significant prediction errors. On the other hand, a
high-dimensional state space results in redundancy and can
cause a drop in performance and computational efficiency.

Nevertheless, model selection, for example based on crossvalidation, is helpful and should be conducted before learning
and applying the model.
To summarize, the model M = {X, Θ} can be learned
from a data set D. It is used to infer the unobserved intention
of a new ongoing movement.
III. A PPROXIMATE I NTENTION I NFERENCE
After learning the model M from the data set D, the
stationary intention g can be inferred from a sequence of
new observations z1:T at test time. In this section, we omit
the model M and the data set D for notation simplicity.
Exact inference is not tractable as the intention g is connected
to all the unobserved latent states x1:T in the graphical
model. Therefore, we use EM algorithm to find the maximumlikelihood estimate (MLE) of the intention g∗ , given by
Z
g∗ = arg maxg p(z1:T |g) = p(z1:T , x1:T |g)dx1:T , (9)
which is obtained by maximizing the marginal likelihood with
the latent states x1:T integrated out. To find the MLE, we apply
the EM algorithm.
In the E step, we calculate the posterior distribution of the
latent states p(x1:T |z1:T , g0 ) based on the current estimate of
the intention g0 . This problem corresponds to the filtering and
smoothing problems in nonlinear dynamical systems. For the
GPDM and the IDDM, approximate filtering and smoothing is
necessary, using particle filtering or Gaussian approximations
for example. In [13] Particle filters (GP-PF), extended Kalman
filters (GP-EKF), and unscented Kalman filters (GP-UKF) for
approximate filtering with GPs were proposed. Assumed Density Filter (GP-ADF) for efficient GP filtering and smoothing
based on moment matching were proposed in [8, 9]. We follow
the work in [8, 9] as the approximated posterior distribution
q(x1:T ) ≈ p(x1:T |z1:T , g0 ) provides credible error bars, i.e.,
it is robust to incoherent estimates. We describe our approach
to approximate filtering and smoothing in the intention-driven
dynamics model in Section III-A.
In the M step, we update the intention based on approximated posterior distribution q(x1:T ) ≈ p(x1:T |z1:T , g0 ), by
maximizing the expected data log-likelihood
arg maxQ(g|g0 ) = Eq [log p(x1:T , x1:T |g)]
g

= Eq [log p(z1:T |x1:T )] + Eq [log p(x1:T |g)], (10)
{z
} |
{z
}
|
Qz

Qx (g)

where Qz is independent of the intention g and hence can be
omitted, and decomposition of Qx (g) leads to
arg max Qx (g) = arg max
g

g

T
−1
X
t=1

Qt (g),

(11)

where
Qt (g) =

ZZ

q(xt , xt+1 ) log p(xt+1 |xt , g)dxt+1 dxt . (12)

This optimization problem can be solved approximately. We
present the M step in detail in Sec III-B.

A. Filtering and Smoothing in the IDDM
Given a prior distribution p(x1 ) and the current estimate
of g, we are interested in computing the posterior distribution
p(x1:T |z1:T , g0 ).
A Gaussian approximation of the joint distribution
p(x1:T |z1:T , g0 ) is computed explicitly by computing
the marginals p(xt |z1:T , g0 ) and the cross-covariances
p(xt−1 , xt |z1:T , g0 ). These steps yield a Gaussian approximation with a block-tri-diagonal covariance matrix. For the
computations, we employ forward-backward smoothing (GPRTSS) in GPDM, see [9].
The
computation
of
the
joint
distributions
p(xt−1 , xt |z1:t−1 , g0 ) and p(xt , zt |z1:t−1 , g0 ) suffices
for forward-backward smoothing in the IDDM, as the
Gaussian filter/smoothing updates can be expressed solely
in terms of means and (cross-)covariances of these joint
distributions [7].
We outline the computations required for the Gaussian
approximation of p(xt−1 , xt |z1:t−1 , g0 ) using moment matching; the Gaussian approximation of p(xt , zt |z1:t−1 , g0 ) follows analogously.
We approximate the joint distribution p(xt−1 , xt |z1:t−1 , g0 )
by the Gaussian
"
# 
!
µxt−1|t−1
Σxt−1|t−1 Σxt−1,t|t−1
.
(13)
N
,
Σxt|t−1
Σxt,t−1|t−1
µxt|t−1
We use the short-hand notation adb|c where a = µ denotes
the mean µ and a = Σ denotes the covariance, b denotes
the time step of interest, c denotes the time step up to which
we consider measurements, and d ∈ {x, z} denotes either the
latent space (x) or the observed space (z).
Without loss of generality, in Eq. (13), we assume
 that the
marginal distribution N xt−1 | µxt−1|t−1 , Σxt−1|t−1 is known
(corresponds to the filter distribution at time t−1). We compute
the remaining elements of Eq. (13) explicitly using momentmatching.
Using iterated expectations, the a-th dimension of the mean
of the marginal p(xt |z1:t−1 , g0 ) is


(µxt|t−1 )a = Ext−1 Efa [fa (xt−1 , g0 )|xt−1 ]|g0 , z1:t−1 (14)
Z
= max (xt−1 , g0 )p(xt−1 |z1:t−1 , g0 )dxt−1 ,
where we plugged in the posterior GP mean function, for the
inner expectation. Writing out the posterior mean function and
defining γ a := K−1
x ya , with yai , i = 1, . . . , M , being the
training targets of the GP with target dimension a, we obtain
(µxt|t−1 )a = q> γ a , where
Z
T
q = kx ([xt−1 , g0 ], X̄)p(xt−1 |z1:t−1 , g0 )dxt−1 .

(15)
(16)

Here, X̄ denotes the set of the M GP training inputs (xij , gj ),
i = 1, . . . , N, j = 1, . . . , J. The intention gj is assumed
stationary in a single trajectory x1j , . . . , xN j . For notational
convenience we assume that all training sequences are N time
steps long.

If kx is a Gaussian kernel, we can solve the integral in
Eq. (16) analytically and obtain the vector q with entries
qi , i = 1, . . . , M,
1

qi = σf2 |Σxt−1|t−1 Λx + I|− 2 exp − 12 ζ Ti Ω−1 ζ i ,
ζ i = xi − µxt−1|t−1 ,

Ω = Σxt−1|t−1 + Λ−1
x .

The computations of the (cross-)covariances Σxt−1,t|t−1 and
in Eq. (13) follow the same scheme—we have to solve
integrals of a Gaussian prior p(xt−1 |z1:t−1 , g0 ) times two
Gaussian kernels (instead of a single one for the mean, see
Eq. (16)). The computations are performed analytically but
are omitted here. We refer to [8, 9] for details.
With the Gaussian kernel as in Eq. (4), a Gaussian approximation to the second joint p(xt , zt |z1:t−1 , g0 ) can be
computed analogously. For the linear measurement kernel in
Eq. (5), we can also solve the integration corresponding to
Eq. (16) analytically and obtain
Σxt|t−1

q = β1 X̄µxt−1|t−1 .

(17)

Due to space restrictions, we omit further details, but the
required computations are straightforward.
Following [7], we obtain the updates for the latent state
posteriors (filter and smoothing distributions) as
z
−1
(zt − µzt|t−1 ) ,
µxt|t = µxt|t−1 + Σxz
t|t−1 (Σt|t−1 )

Σxt|t

=

µxt−1|T
Σxt|T

=
=

z
−1 zx
Σxt|t−1 − Σxz
Σt|t−1 ,
t|t−1 (Σt|t−1 )
x
x
x
µt−1|t−1 + Jt−1 (µt|T − µt|t−1 ) ,
Σxt−1|t−1 + Jt−1 (Σxt|T − Σxt|t−1 )JTt−1

(18)
(19)
(20)

,

(21)

where we have
Jt−1 = Σxt−1,t|t−1 (Σxt|t−1 )−1 .

(22)

These smoothing updates yield the marginals of
p(x1:T |z1:T , g0 ). The missing cross-covariances Σxt−1,t|T of
p(x1:T |z1:T , g0 ) that yield a block-tri-diagonal covariance
matrix are
Σxt−1,t|T = Jt−1 Σxt|T ,

(23)

where Jt−1 is given in Eq. (22). For more technical details, we
refer to [6] and the supplementary material (see Footnote 1).
B. Maximization
With these computation, we obtain a Gaussian approximation to the posterior q(x1:T ) ≈ p(x1:T |z1:T , g0 ) with a blocktri-diagonal covariance matrix. The result
"
# 
!
µxt|T
Σxt|T
Σxt,t+1|T
q(xt , xt+1 ) = N
,
(24)
µxt+1|T
Σxt+1,t|T Σxt+1|T
is used in the M step. Here, we first consider continuous
intention variables g.
The integration in Eq. (12) can be rewritten as
ZZ
Qt (g) =
q(xt , xt+1 ) log (p(xt+1 |xt , g)q(xt )) dxt+1 dxt
Z
− q(xt ) log q(xt )dxt .
(25)

Algorithm 2: The EM algorithm finds the maximumlikelihood estimate of continuous intention g.
Input : Observations x1:T
Output: MLE of the continuous intention g
1 Initialize g ;
2 repeat
3
g0 ← g ;
4
E step: approximate p(x1:T |z1:T , g0 ) with q(x1:T ) by
forward-backward smoothing (GP-RTSS) ;
5
M step: update g by optimizing (11) using SCG with
J steps, based on Eq. (26) ;
0
6 until kg − g k <  ;
We can approximate p(xt+1 |xt , g)q(xt ) with a joint Gaussian
distribution using the same technique as in the E step, and
obtain q̃(xt , xt+1 |g) = N (µ̃, Σ̃), as well as gradients ∂ µ̃/∂g
and ∂ Σ̃/∂g. As a result, the Eq. (12) can be approximated as
Qt (g) ≈ DKL (qt,t+1 ||q̃t,t+1 ) + H(qt,t+1 ) + H(q(xt )), (26)
with shorthand notation qt,t+1 , q(xt , xt+1 ) and q̃t,t+1 ,
q̃(xt , xt+1 |g). In Eq. (26), the entropy of a Gaussian distribution and the KL divergence between two Gaussians can be
computed analytically. Therefore, we can analytically compute
the approximate value of Qt (g) in Eq. (26) and the corresponding gradients with respect to the intention g. The M
step, the optimization problem in (10) and (11), is then solved
using SCG. The Algorithm 2 describes the inference algorithm
for continuous intentions g. We set the number of SCG steps
to J = 20 in experiments.
When the intention g is a discrete variable, for example the
type of action, a more efficient algorithm is to enumerate all
the possible value of g and choose the one with the maximum
approximated value (Jensen’s lower bound) of the log marginal
likelihood in Eq. (9), given by
log p(z1:T |g) ≈ H(q(x1:T )) + Eq [log p(z1:T , x1:T |g)] , (27)
where q(x1:T ) ≈ p(x1:T |z1:T , g) is the approximated posterior
distribution given the current value of intention g.
To summarize, we proposed an efficient approximate approach to intention inference from a new movement. The
method can deal with both continuous (Algorithm 2) and
discrete (Eq. (27)) intention variables.
IV. ROBOT TABLE T ENNIS
Playing table tennis is a challenging task for robots, as it
requires accurate prediction of the ball’s movement and very
fast response. Hence, robot table tennis has been used by many
groups as a benchmark task in robotics [16, 17]. Thus far, none
of the groups which have worked on robot table tennis ever got
to the levels of a young child despite having robots that could
see and move faster and more accurate than humans [17].
Likely explanations for this performance gap are (i) the human
ability to predict hitting points from opponent movements
and (ii) the robustness of human hitting movements [17]. In

mean absolute error in cm

45

Fig. 3: The robot’s hitting point is the intersection of the
coming ball’s trajectory and the virtual hitting plane 80 cm
behind the table.
this paper, we used a Barrett WAM robot arm to play table
tennis against human players. The robot’s hardware constraints
impose strong limitations on its flexibility.
The robot requires sufficient time to execute a ball-hitting
plan: to achieve the required velocity for returning the ball,
movement initiation to an appropriate preparation pose is
needed before the opponent hits the ball. The robot player uses
different preparation poses for forehand and backhand hitting
plans. Hence, it is necessary to choose between them based
on the modeling the opponent’s preference [26] and inference
of the opponent’s target location for the ball [27].
The robot perceives the ball and the opponent’s racket in
real-time, using seven Prosilica GE640C cameras [27]. These
cameras were synchronized and calibrated to the coordinate
system of the robot. The ball tracking system uses four
cameras to capture the ball on both courts of the table.
The racket tracking system provides the information of the
opponent’s racket, i.e., position and orientation. As a result,
the observation zt includes the ball’s position and velocity,
and the opponent’s racket position, velocity, and orientation
before the human plays the ball. We downsampled the observations at 12Hz. Here, the position and velocity of the
ball were processed online with an extended Kalman filter.
However, the same smoothing method cannot be applied to the
racket’s trajectory, as its dynamics are unknown. Therefore, the
obtained states of the racket were subject to substantial noise
and the model has to be robust to this noise.
The robot always chooses its hitting point on a virtual
hitting plane (80 cm behind the table), as shown in Fig. 3.
We define the human’s intended target g as the intersection of
the returned ball’s trajectory with the virtual hitting plane. As
the X-coordinate (see Fig. 3) is most important for choosing
either forehand or backhand hitting plans, the intention g considered here is the X-coordinate of the hitting point. Physical
limitations of the robot restrict the X-coordinate to the range
to ±1.2 m from the robot’s base (table is 1.52 m wide).
To evaluate the performance of target prediction, we collected a data set with recorded stroke movements from different human players. The true targets were obtained from the
ball tracking system. The data set was divided into a training
set with 100 plays and a test set with 126 plays. The standard
deviation of the target coordinate in the test set is 102.2 cm. A

IDDM
GPR

40

35

30

25

320

240
160
time in ms before opponent returns ball

80

Fig. 4: Mean absolute error of the ball’s target with standard
error of the mean. These errors are before the opponent has
hit the ball and only based on his movements.
straightforward approach to prediction is to learn a mapping
from the features zt to the target g. Hence, we compare our
model to Gaussian Process Regression (GPR) using a Gaussian
kernel with automatic relevance determination.
For every recorded play, we compared the performance
of the proposed IDDM intention inference and the GPR
prediction at 80 ms, 160 ms, 240 ms and 320 ms before the
opponent hits the ball. Note that this time-step was only used
such that the algorithms could be compared, and that the
algorithms were not aware of the hitting time of the opponent
in advance. As demonstrated in Fig. 4, the proposed model
outperformed the GPR. At 80 ms before the opponent hit the
ball, the proposed model resulted in the mean absolute error of
31.55 cm, which achieved a 13.3% improvement over the GPR
(p-value: 0.016), whose average error is 36.4 cm. One modelfree naive intention prediction would be to always predict the
center of the intentions in the training set. This naive prediction
model caused an error of 81.9 cm. Hence, both the GPR and
IDDM substantially outperformed naive goal prediction.
Note that the prediction was made before the opponent
hits the ball, and only used to choose between forehand and
backhand hitting plans. More fine-tuning can follow later when
the returned ball’s trajectory has been observed. Hence, a
certain amount of error is tolerable since the robot can apply
small changes to its hitting plan based on the ball’s trajectory.
However, it cannot switch between forehand and backhand
hitting plans because of torque/temporal constraints.
We performed model selection to determine the covariance
function kz , which can be either an isotropic Gaussian kernel,
see Eq. (4), or a linear covariance function, see Eq. (5). Furthermore, we performed model selection to find the dimension
d of the latent states. In the experiments, the model was
selected by cross-validation on the training set. The best model
under consideration was with a linear covariance function and
TABLE I: The mean absolute errors (in cm) of the goal
inference made 80 ms before the opponent hits the ball.
kernel
linear
Gaussian

d=3
41.52
38.49

d=4
31.55
34.16

d=5
35.36
34.44

d=6
37.04
37.28

four dimensional latent state space. Experiments on the test
set verified the model selection result, as shown in Table I.
Our results demonstrated that the IDDM can improve the
target prediction in robot table tennis and choose the correct
hitting plan. We have verified the model in a simulated
environment, but using data from real human movements
recorded from a human playing against another human. The
simulation showed that the robot could successfully return the
ball when given a prediction by the IDDM model. For a demo,
see Footnote 1.

Latent dimension 1
Time

C

V. ACTION R ECOGNITION FOR I NTERACTIVE ROBOTS
To realize safe and meaningful HRI, it is important that
robots can recognize the human’s action. The advent of robust,
marker-less motion capture techniques [22] has provided us
with the technology to record the full skeletal configuration of
the human during HRI. Yet, recognition of the human’s action
from this high-dimensional data set poses serious challenges.
In this paper, we show that the IDDM has the potential
to recognize the intention of action from movements in a
simplified scenario. Using a Kinect camera, we recorded the
32-dimensional skeletal configuration of a human during the
execution of a set of actions namely: crouching (C), jumping
(J), kick-high (KH), kick-low (KL), defense (D), punch-high
(PH), punch-low (PL), and turn-kick (TK). For each type of
action we collected a training set consisting of ten repetitions
and a test set of three repetitions. The system downsampled
the output of Kinect and processes three skeletal configurations
per second. The actions were performed slowly in the data set,
for example one jumping action took about 1.5 seconds, so that
the NAO robot has sufficient time to respond.
In this task, the intention g is a discrete variable and
corresponds to the type of action. Action recognition can be
regarded as a classification problem. We compared the proposed algorithm to Support Vector Machines (SVM) [21] and
multi-class Gaussian Process Classification (GPC) [20, 12].
The algorithms made a prediction after observing a new
skeletal configuration. The proposed IDDM used a sliding
window of length n = 5, i.e., it recognized actions based
on the recent n observations. We chose the IDDM with a
linear covariance function for kz and a two-dimensional state
space. The IDDM achieved the precision of 83.8%, which
outperformed SVM (77.5%) and GPC (79.4%) using the
same sliding windows. We observed that the SVM and GPC
confused between crouching and jumping, as they were similar
in the early and late stages. In contrast, the IDDM could
distinguish crouching (C) and jumping (J) from their different
dynamics, as shown in Fig. 5. The distinction between C and
J became significant while the human performed the actions:
The longer the movement was observed the more evidence
was used by the IDDM.
TABLE II: Trade-off between accuracy and time complexity,
by varying the size of sliding windows n.
n
Time(s)
Accuracy

4
0.27
79.0%

5
0.32
83.8%

6
0.39
86.6%

J

KH TK

C

J

KH TK

Latent dimension 2

C

J

KH TK

C

J

KH TK

Fig. 5: The trajectories of the 2D latent states for two consecutive Jumping actions are obtained by smoothing. The error
bars represent the corresponding standard deviations. The bar
charts correspond to the likelihood of Crouching, Jumping,
Kick-High and Turn-Kick at different stages of an action.
In order to apply the IDDM to real-time action recognition,
we need to trade-off between accuracy and time complexity,
by varying the size of sliding windows. As shown in Table II,
the proposed model could yield real-time action recognition
in 3Hz with a little sacrifice in accuracy. Note that this
was only a preliminary experiment to show the feasibility
of IDDM in action recognition. We will actively work on
more efficient algorithms and implementations. For future
experimental results and a demo, see Footnote 1.
VI. R ELATED W ORK
Inference of intention, or referred to in some contexts as
goal, target, desire, plan, etc., has been investigated under different settings. For example, an early work [18] used Hidden
Markov Models to model and predict human behavior where
different dynamics models are adopted to the corresponding
behaviors. Bayesian models were used for inferring goals
from behavior in [19], where a policy conditional on the
agent’s goal is learned to represent the behavior. Bayesian
models can also interpret the agent’s behavior and predict
its behavior in a similar environment with the learned model
[3]. Inverse Reinforcement Learning (IRL) [1] assumes a
rational agent that maximizes expected utility, and infers the
underlying utility function from its behavior. In a recent work
[10], a computational framework is proposed to model gaze
following, where GPs are used to model the policies with
actions directed by a goal.
Observation of human movement often consists of highdimensional features. Determining the low-dimensional latent
state space is an important issue for understanding observed
actions. Gaussian Process Latent Variable Model (GPLVM)
[15] finds the most likely latent variables by marginalizing
out the mapping function from latent to observed space. Its
extension, Gaussian Process Dynamical Models [25] can be
used to model the dynamics of human motion while simultaneously finding low-dimensional latent states. For example, it
can model and extrapolate the appearance of human walking.
Nonlinear dynamics models have successful applications
in robotics. For example, Dynamic Motion Primitives [11]

were used in imitation learning, parametrizing the dynamic
as differential equations to achieve robust dynamics and fast
learning. Dynamics models are also helpful in tracking, for
example, a small robotic blimp using two cameras [13], where
GP-Bayes filters were proposed for efficient filtering. In a
following work, the model is learned using GPLVM [14], so
that the latent states need not be provided for learning.
VII. D ISCUSSIONS
In this paper, we proposed the intention-driven dynamics
model (IDDM). Our contributions include: (1) suggesting the
IDDM, which simultaneously finds a good low-dimensional
representation of high-dimensional and noisy observations,
and models the dynamics that are driven by the intention; (2)
introducing an approximate algorithm to efficiently infer the
human’s intention from an ongoing movement; (3) verifying
the proposed model in two human-robot interaction scenarios,
i.e., target inference in robot table tennis and action recognition
for interactive robots. In these two scenarios, we show that
modeling the intention-driven dynamics can achieve better
prediction than algorithms without modeling dynamics.
There are many interesting directions that we are actively
exploring. The model suffers somewhat from its computational
complexity. The acceleration of the inference algorithm for its
real-time application in robot table tennis can be achieved by a
more efficient implementation, e.g. using parallel computing.
Moreover, the discovery of action types without supervision
can be interesting for HRI.
ACKNOWLEDGMENTS
Part of the research leading to these results has received
funding from the European Community’s Seventh Framework
Programme under grant agreements no. ICT-270327 (CompLACS) and no. ICT-248273 (GeRT). We thank Abdeslam
Boularias for valuable discussions on this work.
R EFERENCES
[1] P. Abbeel and A.Y. Ng. Apprenticeship learning via
inverse reinforcement learning. In ICML, 2004.
[2] C. Andrieu, N. De Freitas, A. Doucet, and M.I. Jordan.
An introduction to MCMC for machine learning. Machine learning, 50(1):5–43, 2003.
[3] C. Baker, J. Tenenbaum, and R. Saxe. Bayesian models
of human action understanding. In NIPS, 2006.
[4] C.L. Baker, R. Saxe, and J.B. Tenenbaum. Action
understanding as inverse planning. Cognition, 113(3),
2009.
[5] A.C. Damianou, M.K. Titsias, and N.D. Lawrence. Variational gaussian process dynamical systems. In NIPS,
2011.
[6] M.P. Deisenroth. Efficient Reinforcement Learning using
Gaussian Processes. KIT Scientific Publ., 2010.
[7] M.P. Deisenroth and H. Ohlsson. A general perspective
on Gaussian filtering and smoothing. In ACC, 2011.
[8] M.P. Deisenroth, M.F. Huber, and U.D. Hanebeck. Analytic moment-based Gaussian process filtering. In ICML,
2009.

[9] M.P. Deisenroth, R. Turner, M. Huber, U.D. Hanebeck,
and C.E. Rasmussen. Robust filtering and smoothing
with Gaussian processes. Trans. on Automatic Control,
2012.
[10] A.L. Friesen and R.P.N. Rao. Gaze following as goal
inference: A Bayesian model. In CogSci, 2011.
[11] A.J. Ijspeert, J. Nakanishi, and S. Schaal. Movement
imitation with nonlinear dynamical systems in humanoid
robots. In ICRA, 2002.
[12] M.E. Khan, S. Mohamed, B.M. Marlin, and K.P. Murphy.
A stick-breaking likelihood for categorical data analysis
with latent Gaussian models. In AISTATS, 2012.
[13] J. Ko and D. Fox. GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation models.
Autonomous Robots, 27(1), 2009.
[14] J. Ko and D. Fox. Learning GP-BayesFilters via Gaussian
process latent variable models. Autonomous Robots,
2009.
[15] N.D. Lawrence. Gaussian process latent variable models
for visualization of high dimensional data. In NIPS,
2004.
[16] M. Matsushima, T. Hashimoto, M. Takeuchi, and
F. Miyazaki. A learning approach to robotic table tennis.
IEEE Trans. on Robotics, 21(4), 2005.
[17] K. Mülling, J. Kober, and J. Peters. A biomimetic
approach to robot table tennis. Adaptive Behavior, 19
(5):359–376, 2011.
[18] A. Pentland and A. Liu. Modeling and prediction of
human behavior. Neural Computation, 11(1), 1999.
[19] R.P.N. Rao, A.P. Shon, and A.N. Meltzoff. A Bayesian
model of imitation in infants and robots. Imitation and
Social Learning in Robots, Humans, and Animals, 2004.
[20] C.E. Rasmussen and C.K.I. Williams. Gaussian Processes
for Machine Learning. MIT Press, 2006.
[21] B. Schölkopf and A.J. Smola. Learning with Kernels:
Support Vector Machines, Regularization, Optimization,
and Beyond. MIT Press, 2001. ISBN 0262194759.
[22] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Realtime human pose recognition in parts from single depth
images. In CVPR, 2011.
[23] M.A. Simon. Understanding Human Action: Social
Explanation and the Vision of Social Science. State
University of New York Press, 1982.
[24] R. Turner, M.P. Deisenroth, and C.E. Rasmussen. Statespace inference and learning with Gaussian processes. In
AISTATS, 2010.
[25] J.M. Wang, D.J. Fleet, and A. Hertzmann. Gaussian process dynamical models for human motion. IEEE Trans.
on Pattern Analysis and Machine Intelligence, 2008.
[26] Z. Wang, A. Boularias, K. Mülling, and J. Peters. Balancing safety and exploitability in opponent modeling.
In AAAI, 2011.
[27] Z. Wang, C.H. Lampert, K. Mülling, B. Schölkopf, and
J. Peters. Learning anticipation policies for robot table
tennis. In IROS, 2011.

2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems
October 7-12, 2012. Vilamoura, Algarve, Portugal

Maximally Informative Interaction Learning for Scene Exploration
Herke van Hoof1 , Oliver Kroemer1 , Heni Ben Amor1 and Jan Peters1,2
{hoof,kroemer,amor,peters}@ias.tu-darmstadt.de

Abstract— Creating robots that can act autonomously in
dynamic, unstructured environments is a major challenge. In
such environments, learning to recognize and manipulate novel
objects is an important capability. A truly autonomous robot
acquires knowledge through interaction with its environment
without using heuristics or prior information encoding human
domain insights. Static images often provide insufficient information for inferring the relevant properties of the objects
in a scene. Hence, a robot needs to explore these objects by
interacting with them. However, there may be many exploratory
actions possible, and a large portion of these actions may be
non-informative. To learn quickly and efficiently, a robot must
select actions that are expected to have the most informative
outcomes. In the proposed bottom-up approach, the robot
achieves this goal by quantifying the expected informativeness
of its own actions. We use this approach to segment a scene
into its constituent objects as a first step in learning the
properties and affordances of objects. Evaluations showed that
the proposed information-theoretic approach allows a robot to
efficiently infer the composite structure of its environment.

Fig. 1. Our robot learns the structure of its environment by observing
the effects of its actions. Based on its observations so far, the robot selects
the exploratory actions that are expected to yield maximally informative
outcomes, using information-theoretic principles.

I. I NTRODUCTION
Recognizing and manipulating objects is an essential capability for many robots. Today’s industrial robots operate in
structured environments with accurate object knowledge provided by human engineers. However, robots in unstructured
environments will frequently encounter new objects. Hence,
pre-defined object information does not always suffice. Instead, the robot should autonomously learn the properties of
objects in its environment using machine learning techniques.
Supervised machine learning techniques, however, often
require large amounts of manually annotated data. Furthermore, many techniques need a human expert to fine-tune
parameters and features to a specific situation. Such topdown methods, which rely on prior training and human
expertise, are usually not suitable for autonomous robots [1].
Alternatively, robots can autonomously collect new knowledge using interactive perception [2, 3]. This bottom-up
approach couples perception to physical interactions, such
as pushing, grasping, or lifting. Interactive perception allows
a robot to learn, for example, the appearance and shape
of objects [4–8], their haptic properties [9, 10], kinematic
structure [2], and how the state of those objects changes as
a result of manipulation [4, 10–12].
For such bottom-up approaches, selection of efficient
actions is a challenge. In real world domains, most robots
can execute a large variety of actions. Therefore, exploring a
scene by executing all possible actions is usually infeasible.
1 TU

Darmstadt, FB Informatik, FG IAS, Darmstadt, Germany
Planck Institute for Intelligent Systems, Tübingen, Germany

2 Max

978-1-4673-1736-8/12/S31.00 ©2012 IEEE

Instead, the robot should choose actions that are expected to
reveal the most information about the world [9, 13, 14].
In this paper, we propose using a bottom-up approach
that requires little prior knowledge to explore novel environments. In our approach, the robot uses its actions to
elicit the information required for the specific situations
that it encounters, as illustrated in Fig. 1. Rather than
finding explorative actions using heuristics, we propose an
autonomous system that selects maximally informative actions in a principled manner. The expected informativeness of
actions is quantified using the information-theoretic measure
of information gain.
We focus on segmenting the perceptual scene into separate
objects. Only after segmentation can the robot explore the
properties and affordances of individual objects [1, 15].
Methods for bottom-up object segmentation based solely
on static images are limited by inherent ambiguities in the
observations [3, 4]. For example, two differently-colored
adjacent regions may actually be distinct parts of the same
object, or belong to different objects. In our approach, we
resolve such segmentation ambiguities by testing whether the
regions are physically connected.
The knowledge gathered by our system is used to generate
a graph-based representation of the observed scene, wherein
the edges of the graph represent the probability of pairs
of segments belonging to the same object. Our experiments
show that the proposed approach efficiently discovers which
groups of segments correspond to objects.

5152

II. R ELATED WORK

C. Informed interaction

Segmentation is an important step in finding and learning
the properties and affordances of unknown objects in the
robot’s environment. Section II-A discusses how interaction
can be used to obtain such segmentations. Subsequently,
Section II-B discusses how to select maximally informative
actions. The application of informative action selection in
interactive settings is discussed in Section II-C.
A. Interactive scene segmentation
Fitzpatrick and Metta [4] did early work on using a robot’s
actions to segment visual scenes with unknown objects, by
detecting the motion resulting from sweeping the arm across
the workspace. Li and Kleeman [15] refined this method
by using short, accurate pushes in cluttered environments.
Kenney et al. [3] proposed to accumulate information over
time to increase the accuracy of the segmentation.
The methods in the previous paragraph relied on image
differencing to detect movement. Problems with this method,
e.g. handling textureless objects, were addressed by Beale
et al. [1], who started from a low-level over-segmentation
of the image. Instead of estimating for every pixel whether
it belongs to the object, object membership was estimated
per segment. If visual features on the objects can be reliable
tracked, these features can also be used to perform movement
detection in the context of interactive segmentation [8, 16].
In all of those methods, the robot used actions that were
either fixed, selected at random, or chosen by a heuristic.
For a large part, they considered scenes containing only one
object. However, if a robot has access to many possible
exploratory actions, trying them at random is not efficient.
Heuristics, on the other hand, rely on human insights and
are likely to fail in unforeseen situations.
Other segmentation methods focused on settings where the
robots held an object and used actions to segment it from the
background and learn its properties [6, 7, 12, 17, 18]. These
methods could further inspect objects after they have been
segmented and, hence, these approaches would be a good
complement to our work.
B. Selecting informative actions
Rather than using fixed actions or heuristics, the robot
should adapt to the current situation by evaluating the informativeness of different actions. For instance, in the work of
Denzler and Brown [13], the camera parameters that provide
the largest mutual information are selected. Schneider et
al. [9] proposed acquiring data with a tactile sensor at the
height expected to produce the largest reduction in entropy.
Hsiao et al. [19] minimized the expected costs of object
pose estimation by selecting an optimal n-length sequence of
actions. They found that usually a search depth of n = 1 is
already sufficient. These active perception approaches change
the perceptual parameters, but do not try to cause changes
in the environment. Furthermore, these methods are usually
applied after prior training.

In contrast to the methods in the previous section, interactive perception approaches attempt to cause changes in
the environment. By observing the effects of actions, object
knowledge can be obtained without prior supervised training. This interactive approach also benefits from maximally
informative actions. For instance, Krainin et al. [5] used a
next-best-view algorithm based on information gain to select
the best viewpoint and re-grasps for in-hand object modeling.
Another way to estimate the informativeness of an action is
by comparing the current situation to similar situations in the
past, and selecting the action that was the most successful
in uncovering new object properties in those situations. This
approach was used by Katz et al. [14] to estimate the value
of actions for exploring the kinematic structure of articulated
objects using Q-learning.
In summary, informed action selection has been shown to
outperform random actions in different active and interactive
perception scenarios [9, 13, 14]. However, the discussed
approaches for selecting informative actions did not address
the problem of learning about completely novel objects. Most
of these approaches required detailed models or training data
of the objects before being able to efficiently discriminate
between them [9, 13, 19]. Other approaches needed to
already have sufficient knowledge to pick up the object [5]
or required a human-crafted, domain-specific representation
to generalize past experiences to the current situation [14].
III. O BJECT SEGMENTATION USING MAXIMALLY
INFORMATIVE INTERACTION

The approaches for object learning and segmentation
reviewed in Section II depend largely on prior training
or human domain knowledge. In contrast, we propose to
enable robots to develop such knowledge autonomously by
interacting with novel objects. As a first step towards this
goal, a robot needs to efficiently decompose the scene it
perceives into its constituent objects.
The starting point of our segmentation approach is an oversegmentation of the visual scene into coherent segments. This
process is described in Section III-A. Subsequently, the robot
actively explores the scene to determine which segments
constitute coherent objects. Efficient exploration is accomplished by selecting actions that are expected to maximize
information gain. The action selection and execution methods
are described in Sections III-B to III-F. The robot observes
the resulting state of its environment as described in Section
III-G. An overview of this process is given in Fig. 2, and the
entire algorithm is summarized in Algorithm 1.
A. Finding object candidates using visual over-segmentation
As described in Section II-A, movement detection by
image differencing has several disadvantages. Therefore, our
approach uses visual over-segmentation to create segments
containing pixels close together in Euclidean and color space,
similar to the work of Beale et al. [1]. Subsequently, we
observe whether movement has occurred for each segment,
rather than for each pixel. Objects can consist of multiple

5153

fidence that they belong to the same object increases. On
the other hand, when only one of the segments moves, the
confidence that they belong to different objects increases.
The degree of confidence depends on the pushed vertex
v. The data gathered about the edge between vertices i
t
to time step
be denoted by Dij
=
andt j, up
 t, will
t
t
t
t
nij Nij mij Mij with nij the number of times both
t
vertices moved out of the Nij
times that one of those vertices
t
was pushed, and mij the number of times both vertices
t
moved out of the Mij
times at least one of them moved
but neither was pushed.
C. Finding informative actions for exploration

Fig. 2. Based on the results of previous actions, the action that is expected
to yield the maximum information gain is chosen. This action results in
some segments moving together, which increases the confidence that those
segments form an object.

regions, each of which is coherent in space and color. Hence,
we assume that a true segmentation of the objects in the scene
can be obtained by merging segments.
The point clouds to be segmented are obtained using an
RGBD camera mounted at the robot’s end effector, as shown
in Fig. 1. This set-up allows the robot to move the camera to
different positions. Calibration of the camera allows merging
of the point clouds and the removal of any points that do not
correspond to objects on the table in front of the robot.
To obtain an over-segmentation, points are clustered according to six-dimensional feature vectors containing the
color in CIELAB space [20] and the location in Euclidean
space using the k-means algorithm. This is very similar to
the SLIC algorithm [21], which operates in image space
rather than on three-dimensional point clouds. Examples of
segmentations are shown in Fig. 2. Segments are tracked
over time by initializing the cluster centers in the current
time step using the centers in the previous time step, similar
to [22, 23].
B. Graph-based scene representation
We represent the over-segmented visual scene as a graph
G = (V, E) with V being the set of vertices representing
the segments and E being the set of edges connecting every
pair of vertices that belong to the same object. We consider
every graph G ∈ G, with G denoting the set of all graphs
corresponding to partitionings of the set of all segments. For
every pair of vertices (i, j), we define gij = 1 if the vertices
belong to the same object ({i, j} ∈ E), and zero otherwise.
If a segment is pushed, all segments belonging to the same
object as the pushed segment should move as well. The
segments observed to move at time step t are represented
by ot , with otj = [ot ]j = 1 if vertex j was observed as
moving, and zero otherwise.
When two segments are moving synchronously, the con-

We want to find out which pairs of segments belong to the
same objects, using as few actions as possible. Therefore,
we want to find the vertex v to be pushed at time t that
maximizes the expected information gain, that is given by
the Kullback-Leibler divergence
X
P (i)
KL(PkQ) =
P (i) ln
Q(i)
i
between the conditional distribution over graphs after the
push p(G|Dt+1 ) and before the push p(G|Dt ).
In order to evaluate the expected information gain of every
push, the probability distribution of a scene structure given
observation data p(G|Dt ) needs to be calculated. According
to Bayes’ rule, and assuming independence of observations
given gij , the conditional probability p(G|Dt ) is proportional
to the joint probability
Y
t
p(G, Dt ) = p(G)
p(Dij
|gij ),
(1)
i;j>i

where the product is over all pairs of vertices (i, j) with
j > i. The probability p(oi = 1|oj = 1) depends on whether
i and j are part of the same object, i.e. gij = 1, and whether
j was pushed. As these probabilies are not known, we need
to estimate the parameter vectors

 

θ
p(oi = 1|j = v, oj = 1, gij = h)
θ h = h,0 =
, (2)
θh,1
p(oi = 1|j 6= v, oj = 1, gij = h)
for h ∈ {0, 1} together with the conditional distribution
over G. The parameters are re-estimated after every action,
according to the observed results. The estimate of the parameters Θ at time step t will be denoted by Θt .
Conditioning on the parameters, and assuming observations are independent over time, (1) can be written as
p(G|Dt , Θt ) ∝ p(G, Dt |Θt )
Y
t
= p(G)
p(Dij
|gij = h, θ th )
i;j>i

∝ p(G)

Y

Bin(nij |Nij , θh,0 )Bin(mij |Mij , θh,1 ),

(3)

i;j>i

where Bin(n|N, p) is the probability mass according to the
binomial distribution. Equation (3) is used to evaluate the
distribution over graphs G given the data available before
and after a potential action. These distributions are then used

5154

to calculate the expected gain in information due to pushing
vertex v


Eot+1 KL(p(G|Dt , ot+1, Θt )kp(G|Dt, Θt ))|Dt, v, Θt
= I(G; ot+1 |Dt , v, Θt ).
To evaluate this expectation over all possible observations
ot+1 , we need to compute
p(ot+1 |Dt , v, Θt )
P
= G∈G p(ot+1 |G, v, Θt )p(G|Dt , Θt ),

Algorithm 1 Maximally informative interaction algorithm.
1: t ← 0
2: loop
3:
estimate Θt and P (G|D)
4:
v ∗ ← arg max I(G; ot+1 |Dt , v, Θt )
v
5:
execute action v ∗
6:
t←t+1
7:
observe movement ot
8: end loop

(4)

where we assume that ot+1 is conditionally independent of
Dt given G, and G is independent of the selected action v.
Assuming that movement of vertex j is conditionally independent on the movement of other vertexes and graph edges
given pushed vertex v and gjv , the conditional distribution
p(ot+1 = 1|G, v, Θt )
Q
Q
= j p(ojt+1 = 1|gjv = k, v, Θt ) = j θk,0 .

(a) Scene observation

D. Estimating the model parameters
The unknown parameters Θ that represent the probability
of observing different events can be estimated together with
G using expectation-maximization, by considering G to be
a latent variable. Expectation-maximization is an iterative
technique for finding maximum likelihood solutions in the
presence of hidden variables.
Starting from an initial setting of the parameters Θ0 , the
algorithm iterates expectation and maximization steps. In
an expectation step, the conditional probability P (G|D, Θ)
is calculated, which is used to calculate a lower bound
Q(Θ|Θk ) on the log-likelihood function. Q(Θ|Θk ) expresses
the expected log-likelihood of parameters Θ conditioned on
the parameters in the previous iteration Θk , with Θk considered fixed. A maximization step then selects parameters
Θk+1 that maximize this expectation. As all variables in this
section refer to the variables at the current time step t, time
indexes are omitted for clarity.
Assuming observations are conditionally independent
given graph G, and assuming p(G) is independent of parameters Θ, the joint likelihood
Q
p(G, D|Θ) = i;j>i p(Dij |G, Θ)p(G).

G∈G


=

p(G|D, Θk ) log 

X
G∈G

p(Dij |G, Θ)p(G)

i;j>i

G∈G

=


Y

p(G|D, Θk )

X

(c) Effect observation

Fig. 3.
Before every action, the robot observes from different viewpoints (a). The observations so far are used to predict the action with the
highest information gain. The robot executes the selected action (b), thereby
changing the state of its environment. The resulting change is observed (c)
and the robot is ready to choose its next action.

values for the next iteration Θk+1 = arg max Q(Θ|Θk ).
Θ

Analytically solving ∇Θ Q(Θ|Θk ) = 0 yields
X
X
nij
p(gij = h|G)p(G|D, Θk )
i;j>i

θ k+1
h,0 = X

Nij

i;j>i

G∈G

X

p(gij = h|G)p(G|D, Θk )

.

(5)

G∈G

For the second element of the parameter vectors θ k+1
h,1 (h ∈
{0, 1}) the result is similar, but uses mij and Mij instead
of nij and Nij , respectively. The estimate of the prior P (G)
should be updated according to its parametrization.
E. Approximating the information gain
The number of partitions grows at a rate greater than exponentially in the number of vertices. Hence, the evaluation
of sums over all G becomes intractable for high numbers of
vertices. However, the expectation of the information gain
can be approximated by assuming that
Y
P (G) =
P (gij ).
i;j>i

Hence, the lower bound of the expected log-likelihood
X
Q(Θ|Θk ) =
p(G|D, Θk ) log(p(G, D|Θ))

X

(b) Action execution

log(p(Dij |G, Θ)p(G)).

i;j>i

In the maximization steps, the lower bound on the loglikelihood function is to be maximized to get the parameter

In this case, the expected information gain can be evaluated
in polynomial time.
Under this assumption, the Kullback-Leibler divergence
between the probability distributions over the graphs before
and after a push becomes a sum of the Kullback-Leibler
divergences of individual edges. The number of possible
observations that we have to take the expectation over
also grows faster than exponentially. However, under the
assumption of edge independence, we can sample from this
distribution in order to efficiently approximate the expectated
information gain numerically.

5155

The assumption of edge independence also allows efficient
estimation of parameters Θ together with latent variables gij
using expectation-maximization, as explained in Section IIID. In this case, the parameters are Θ = {θ 0 , θ 1 , π} with θ 0
and θ 1 as defined in (2) and the prior probabilities specified
by π h = P (gij = h) for h ∈ {0, 1}.
The sums in (5) calculate p(gij = h|D, Θk ). Still assuming the vertex connections gij are independent, an approximation can be calculated in closed form as
p̃(gij = h|Dij , Θk )
Y
∝ πh
Bin(nij |Nij , θ h,0 )Bin(mij |Mij , θ h,1 ).
i;j>i

Fig. 4. The matrices show, for every pair of segments, the system’s estimate
of the probability that they belong to the same object. Results after 5, 10,
and 15 actions are shown. The segments have been re-ordered according to
the object they belong to. Ideally, this matrix would be a block diagonal.

A. Experimental set-up

This approximation is inserted in (5) to estimate the parameters θ 0 and θ 1 . By solving ∇π Q(Θ|Θk ) = 0, we obtain
X
π 0 = (1 − π 1 ) =
p(gij = 1|D, Θk )/|E|,
i;j>i

with |E| being the number of edges. This approximate
inference algorithm has a time complexity cubic in the
number of vertices.
F. Action selection and execution
Once the expected information gain of pushing each action
is evaluated, the segment that is expected to yield the highest
one-step information gain when pushed is selected. A onestep look-ahead is commonly used [5, 9, 13], and Hsiao et
al. [19] found that, in their framework, one-step lookahead
is usually sufficient.
If the expected information gain for multiple actions is
equal, as it is at the beginning of learning, the segment
to be pushed is selected at random. The robot pushes the
center of the selected segment in a direction corresponding to a projection of the segment’s normal in the table
plane. Infeasible or dangerous actions, e.g., pushes outside
the workspace, are not executed. Instead, the next most
informative action is selected. Algorithm 1 summarizes our
approach to maximally informative interaction.
G. Observing the effect of the action
After an action has been executed, the resulting scene is
observed. Similar to the original observation, the new scene
is over-segmented using the methods described in Section
III-A. In order to track segments over different observations,
the segment centers are initialized using the previous segment
centers, as suggested by Heisele et al. [22, 23].
After clustering, the locations of the new segment centers
are compared to the locations of the corresponding old
segment centers. Any centers that had a displacement larger
than a predefined threshold are considered to have moved as
a result of the action.
IV. E XPERIMENTS
Our approach was evaluated on a robot platform together
with a system for uninformed action selection for comparison. This section will first describe the experimental set-up
and subsequently present our results and a discussion thereof.

The platform used for evaluation is a Mitsubishi PA-10
robot arm. Mounted on this arm are a force-torque sensor, an
RGBD camera and a rod for pushing the objects, as shown
in Fig. 1. In front of the robot is a table supporting the
objects to be segmented. The camera is calibrated such that
point clouds can be transformed to the robot’s coordinate
system. In this manner, point clouds corresponding to different viewpoints of the same scene can be merged. Observing
from different view points avoids changing the segmentation
too much when the object is pushed. Furthermore, parts of
the point clouds that do not correspond to the objects in the
robot’s workspace can be automatically removed.
The force-torque sensor enables the robot to detect collisions, so that it can operate safely without human supervision. Hence, experiments can be run in an entirely
autonomous manner.
This experimental set-up is shown in Fig. 3. It illustrates
how the robot observes its environment and subsequently
selects and executes the action that it expects to have the
highest information gain, based on its current internal representation of the environment. This maximally informative
interaction method is described in detail in Section III. We
compare this method to an uninformed method that chooses
vertices to be pushed at random with a uniform probability.
Initially, four objects are put closely together on the table
in front of the robot. They are represented using k = 30
segments. In this set-up, selecting the most informative action
takes less than a second, a small fraction of the time needed
to push and observe the result. Both rigid and non-rigid
objects were used in the experiment. After each action, the
robot outputs its estimate of the probability that each pair
of segments belongs to the same object p(gij |D1:t , Θt ) ∝
1:t
p(Dij
|gij , Θt ) as given in (3). These probabilities are calculated in the same manner for the informed algorithm. A
human annotation of the segments in the last frame is used
as a ground truth.
The average error between the ground truth of gij and
the estimated probability is used as performance measure.
This measure is calculated after every action. Segments to
which no points, or a large number of points from multiple
objects, have mistakenly been assigned are excluded from
this calculation. Similar initial configurations were used for
both methods.

5156

0.5
0.4

0.5
Maximally informative actions
Random actions

0.2

0.1

1

2

3

4

5

Maximally informative actions
Random actions

0.3

0.3

Average error

Average error

0.4

6 7 8 9 10 11 12 13 14 15 16
Number of actions

0.2

0.1
0.08
0.07
0.06
1

Fig. 5. Number of actions the robot takes in its environment plotted versus
the average error per edge for both action selection methods on a log scale.
Mean and standard error over 10 trials for each method are shown.

B. Experimental results
Each action selection systems was evaluated in ten complete trial runs on a real robot. Examples of the system’s
estimation of the probability that pairs of segments belong to
the same object during one such run is shown in Fig. 4. This
figure shows the uncertainty about which vertices belong
to the same object. This uncertainty reduces as the results
of an increasing number of actions are observed. After 15
actions, the interactive system has come close to a perfect
segmentation of the four objects. Fig. 5 shows the average
error of the estimated scene structure over all ten runs.
As the extend of evaluations that can be obtained on the
robot is limited, we also evaluated the methods on 100 simulated trial runs using twice as many objects and segments.
In simulation, all vertices that are part of the pushed objects
were considered to move together, but to simulate noise, each
element of the observation vector was flipped with a 10%
probability. Results of this simulation are shown in Fig. 6.
Similar to the real robot experiments, the system needs more
random actions than maximally informative actions to reduce
the error to the same value.
The data from these simulated runs was analyzed using
a paired-sample two-tailed t-test. After performing eight
random actions, the error is significantly different from the
error after six maximally informative actions, t(99) = −2.3,
p < 0.05, with informative actions resulting in lower errors.
This difference becomes larger and more significant as the
number of executed actions increases.
C. Discussion
Regardless of the action selection method, within 15
actions the system learns about the structure of its environment, with the error between the system’s prediction
and the true state of the world going down steadily. After
15 actions the error is still decreasing, which indicates a
better prediction could be made if the system was allowed
to perform more actions. The error seems sufficiently low for
the system to make useful predictions about interaction with
its environment, which shows that our interactive approach

2

3

4

5

6 7 8 9 10 11 12 13 14 15 16
Number of actions

Fig. 6. Number of actions plotted versus the average error per edge. Mean
and standard error over 100 simulated trials are shown for each method.

and the chosen graphical representation were effective in this
scenario.
By representing the environment using an oversegmentation of the raw sensory information, we could confine
possible exploratory behaviors to a relatively small discrete
set of actions. Because the set of actions was limited in this
case, exploring at random was not an unreasonable strategy.
Even though random action selection was a feasible strategy in the current set-up, the number of actions needed to
reduce the error to a certain value was still smaller when
maximally informative actions were chosen. For instance, 16
random actions are needed to reduce the error to the value
that is obtained by performing just 13 maximally informative
actions. However, compared to the variance among trial runs,
the advantage of selecting maximally informative actions is
limited. We expect this difference to be much larger in highdimensional or continuous action spaces.
One reason for the large inter-trial variance is that our vision system did not always find a suitable over-segmentation
and tracking of segments sometimes failed. Another source
of variance is the unmodelled interaction between objects.
For example, sometimes objects topple over or consistently
bump into each other, while in other trials such events did
not take place. However, modelling such interactions would
be possible only after obtaining a segmentation, or with the
use of additional prior information.
It is interesting to see that the random action selection
method seems to have a small initial advantage, both on
the real robot and in simulation. This advantage could be
due to the fact that after only a few actions the estimate of
the parameters is far from the actual values due to a lack
of data. Hence, the expected value of the information gain
of possible actions will not be reliable. If the information
gain cannot be predicted reliably, this lack of knowldedge
can cause the action selection system to choose sub-optimal
actions. A possible solution to this bootstrapping problem is
to initialize the system using a few random actions.
V. C ONCLUSION
In this paper, we have proposed a bottom-up approach
to object learning using interaction with the robot’s envi-

5157

ronment. Maximally informative interaction allows robots to
efficiently develop knowledge about their environment.
Our interactive approach was applied on a real robot
to learn a decomposition of the scene into objects. This
segmentation is an important first step in learning about
objects, as it allows subsequent exploration to focus on single
objects rather than the cluttered scene.
Our experiments show that it is possible to develop this
important knowledge completely autonomously. Except for
setting up the objects in the robot’s workspace, no human
supervision at all was needed while the robot explored its
environment. This makes it feasible for the robot to learn
over longer periods of time.
Useful representations could be learned through exploration using either random or maximally informative action
selection. However, these representations improved faster
when maximally informative actions were used.
We plan to use our approach to scene segmentation as
a stepping stone for learning object properties. Besides
intrinsic properties such as shape and mass, it is important for
a robot to learn which actions an object affords and how these
actions change the state of the objects in the environment.
Ideally, a robot would discover autonomously how to change
the state of its environment to a desired goal state.
ACKNOWLEDGMENT
The project receives funding from the European Community’s Seventh Framework Programme under grant agreement
no. ICT-248273 GeRT.
R EFERENCES
[1] D. Beale, P. Iravani, and P. Hall, “Probabilistic models
for robot-based object segmentation,” Robotics and
Autonomous Systems, vol. 59, no. 12, 2011.
[2] D. Katz and O. Brock, “Manipulating articulated objects with interactive perception,” in ICRA, 2008.
[3] J. Kenney, T. Buckley, and O. Brock, “Interactive
segmentation for manipulation in unstructured environments,” in ICRA, 2009, pp. 1377–1382.
[4] P. Fitzpatrick and G. Metta, “Grounding vision through
experimental manipulation,” Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences, vol. 361,
no. 1811, pp. 2165–2185, 2003.
[5] M. Krainin, B. Curless, and D. Fox, “Autonomous
generation of complete 3D object models using next
best view manipulation planning,” in ICRA, 2011.
[6] W. Li and L. Kleeman, “Interactive learning of visually
symmetric objects,” in IROS, 2009, pp. 4751–4756.
[7] A. Ude, D. Omrc̆en, and G. Cheng, “Making object
learning and recognition an active process,” International Journal of Humanoid Robotics, vol. 5, no. 2, pp.
267–286, 2008.
[8] D. Schiebener, A. Ude, J. Morimotot, T. Asfour, and
R. Dillmann, “Segmentation and learning of unknown
objects through physical interaction,” in International
Conference on Humanoid Robots, 2011, pp. 500 –506.

[9] A. Schneider, J. Sturm, C. Stachniss, M. Reisert,
H. Burkhardt, and W. Burgard, “Object identification
with tactile sensors using bag-of-features,” in IROS,
2009, pp. 243–248.
[10] J. Sinapov, T. Bergquist, C. Schenck, U. Ohiri, S. Griffith, and A. Stoytchev, “Interactive object recognition
using proprioceptive and auditory feedback,” The International Journal of Robotics Research, vol. 30, no. 10,
pp. 1250–1262, 2011.
[11] J. Modayil and B. Kuipers, “The initial development of
object knowledge by a learning robot,” Robotics and
Autonomous Systems, vol. 56, no. 11, 2008.
[12] D. Kraft, R. Detry, N. Pugealt, E. Başeski, F. Guerin,
J. Piater, and N. Krüger, “Development of object and
grasping knowledge by robot exploration,” Transactions
on Autonomous Mental Development, vol. 2, no. 4, pp.
368–383, 2010.
[13] J. Denzler and C. Brown, “Information theoretic sensor
data selection for active object recognition and state
estimation,” PAMI, pp. 145–157, 2002.
[14] D. Katz, Y. Pyuro, and O. Brock, “Learning to manipulate articulated objects in unstructured environments
using a grounded representation,” in Robotics: Science
and Systems IV, 2008, pp. 254–261.
[15] W. Li and L. Kleeman, “Autonomous segmentation
of near-symmetric objects through vision and robotic
nudging,” in ICRA, 2008, pp. 3604–3609.
[16] L. Chang, J. Smith, and D. Fox, “Interactive singulation
of objects from a pile,” in ICRA, 2012.
[17] L. Natale, F. Orabona, G. Metta, and G. Sandini, “Exploring the world through grasping: a developmental
approach,” in International Symposium on Computational Intelligence in Robotics and Automation, 2005.
[18] D. Kraft, N. Pugeault, E. Başeski, M. Popović,
D. Kragić, S. Kalkan, F. Wörgötter, and N. Krüger,
“Birth of the object: Detection of objectness and extraction of object shape through object action complexes,”
International Journal of Humanoid Robotics, vol. 5,
no. 2, pp. 247–265, 2008.
[19] K. Hsiao, L. P. Kaelbling, and T. Lozano-Pérez, “Robust grasping under object pose uncertainty,” Autonomous Robots, vol. 31, no. 2, pp. 253–268, 2011.
[20] Commission Internationale de l’Eclairage, “Colorimetry, second edition,” Tech. Rep. 15.2, 1986.
[21] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua,
and S. Süsstrunk, “Slic superpixels,” EPFL, Tech. Rep.
149300, 2010.
[22] B. Heisele, U. Kressel, and W. Ritter, “Tracking nonrigid, moving objects based on color cluster flow,”
in IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, 1997, pp. 257–260.
[23] B. Heisele and W. Ritter, “Segmentation of range and
intensity image sequences by clustering,” in Information Intelligence and Systems, International Conference
on, 1999, pp. 223–225.

5158

2013 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS)
November 3-7, 2013. Tokyo, Japan

Learning Responsive Robot Behavior by Imitation
Heni Ben Amor1 , David Vogt2 , Marco Ewerton1 , Erik Berger2 , Bernhard Jung2 , Jan Peters1

Abstract— In this paper we present a new approach for
learning responsive robot behavior by imitation of human interaction partners. Extending previous work on robot imitation
learning, that has so far mostly concentrated on learning from
demonstrations by a single actor, we simultaneously record
the movements of two humans engaged in on-going interaction
tasks and learn compact models of the interaction. Extracted
interaction models can thereafter be used by a robot to engage
in a similar interaction with a human partner. We present two
algorithms for deriving interaction models from motion capture
data as well as experimental results on a humanoid robot.

I. INTRODUCTION
While robots are becoming increasingly better at performing a wide range of motor skills, they are still limited in
their human interaction capabilities. To date, most robots are
not prepared to appropriately respond to the movements or
the behavior of a human partner. However, with application
domains of robots coming closer to our everyday life, there
is a need for adaptive algorithms that ensure responsive robot
behavior for human-robot interaction.
We present a new approach to robot learning that allows
anthropomorphic robots to learn a library of interaction skills
from demonstration. Traditional approaches to modelling
interactions assume a pre-specified symbolic representation
of the available actions. For example, they model interactions
in terms of commands such as wait, pick-up, and place.
Instead of such a top-down approach, we want to focus on
learning responsive behavior in a bottom-up fashion using a
trajectory based approach. The key idea behind our approach
is that the observation of human-human collaborations can
provide rich information specifying how and when to interact
in a particular situation. For example, by observing how
two human workmen collaborate on lifting a heavy box, a
robot could use machine learning algorithms to extract an
interaction model that specifies the states, movements, and
situational responses of the involved parties. In turn, such a
model can be used by the robot to assist in a similar lifting
task. Our approach is as an extension of imitation learning
[3] to multi-agent scenarios, in which the behavior and the
mutual interplay between two agents is imitated.
In this paper, we describe the general multi-agent imitation
learning setup for learning interaction models from motion
capture data. We also provide two first algorithms that enable
a robot to learn such interaction models between interacting
1 Heni Ben Amor, Marco Ewerton and Jan Peters are with the Technische Universitaet Darmstadt, Intelligent Autonomous Systems, Darmstadt,
Germany. {amor,ewerton,peters}@ias.tu-darmstadt.de
2 David Vogt, Erik Berger and Bernhard Jung are with the Technische
Universitaet Bergakademie Freiberg, Virtual Reality and Multimedia Group,
Freiberg, Germany. {david.vogt,bergere,jungb}@tu-freiberg.de

978-1-4673-6358-7/13/$31.00 ©2013 IEEE

Fig. 1.
A humanoid robot receives a book that is handed over by a
human interaction partner. The robot learned what to do in this situation
by observing a similar situation between two humans.

agents. The first algorithm PPCA-IM (Probabilistic Principal
Component Analysis-Interaction Model) frames the task as
a missing value estimation problem. The second algorithm
called PM-IM (Path Map-Interaction Model) uses a Hidden
Markov Model (HMM) [20] to represent the mutual dependencies of the interacting agents. A set of shared latent states
is used to map the behavior of one agent to the behavior of
the interaction partner. The principal difference between the
two algorithms presented in this paper is the representation
of the temporal dynamics of interaction. The PPCA-IM uses
an implicit representation of time via a temporal embedding
of the training data. In contrast, the PM-IM uses an explicit
representation of time via a discrete set of hidden nodes.
Through a series of experiments, we will show how the
two algorithms can be used to create a responsive robot that
learns to react to the movements and gestures of humans.
We will also provide a comparison of PPCA-IM and PM-IM
and discuss the advantages and drawbacks of each approach.
II. R ELATED W ORK
Finding simple and natural ways of specifying robot control programs is a focal point in robotics. Imitation learning,
also known as Programming by Demonstration, has been
proposed as a possible solution to this problem [22]. Based
on human-provided demonstrations of a specific skill, a robot
autonomously generates a control program that allows it to
generalize the skill to different situations. Most approaches
to imitation learning obtain a control policy which encodes
the behavior demonstrated by the user. The policy can
subsequently be used to generate a similar behavior that is

3257

Fig. 2. Overview of the interaction learning approach presented in this paper. The interaction behavior of two humans is observed, analyzed and imitated
in order in human-robot interaction scenarios. Left: The movements of two persons are recorded using motion capture technology. Middle: A compact
interaction model specifying the mutual influences and responses is learned. Right: The interaction model enables a robot to compute the best response to
the current behavior of a human interaction partner.

adapted to the current situation.
For example, the Dynamical Motor Primitive (DMP) [13]
approach uses dynamical systems to represent control policies. The DMP approach has been widely accepted in the
imitation learning community and has been used to learn
various motor skills such as locomotion [16], or drumming
[13]. Another way of encoding policies is to use statistical
modelling methods. For example, in the Mimesis Model [17]
a continuous hidden Markov model is used for encoding
the teacher’s demonstrations. A similar approach to motion
generation is presented by Calinon et al. [7] who used Gaussian Mixture Regression to learn gestures. The advantage
of statistical and probabilistic approaches, is the ability to
naturally model the spatial and temporal variability of human
motion.
The methods discussed so far are limited to single agent
imitation learning scenarios. Once the behavior is learned,
it is executed without taking into account the reaction of
an interaction partner. In recent years, various attempts have
been undertaken for using machine learning in human-robot
interaction scenarios. In [15], a recurrent neural network was
used to learn a simple interaction game between a human
and a robot. More recently, Wang et al. [24] presented an
extension of the Gaussian Process Dynamics Model that was
used to infer the intention of a human player during a tabletennis game. Through the analysis of the human player’s
movement, a robot player was able to determine the position
to which the ball will be returned. This predictive ability
allowed the robot to initiate its movements even before
the human hit the ball. In [14], Gaussian mixture models
were used to adapt the timing of a humanoid robot to that
of a human partner in close-contact interaction scenarios.
The parameters of the interaction model were updated using
binary evaluation information obtained from the human.
While the approach allowed for human-in-the-loop learning
and adaptation, it did not include any imitation of observed
interactions.
In a similar vein, the work in [17] showed how a robot
can be actively involved in learning how to interact with a

human partner. The robot performed a previously learned
motion pattern and observed the partner’s reaction to it.
Learning was realized by recognizing the observed reaction
and by encoding the action-reaction patterns in a HMM. The
HMM was then used to synthesize similar interactions. In
contrast, in our approach, learning of motion and interaction
is not split into two parts. Instead, we learn one integrated
interaction model which can directly synthesize an appropriate movement in response to an observed movement of
the human partner. Further, instead of modelling symbolic
action-reaction pairs, our approach is based on modelling
the joint dynamics during the execution of a movement.
In general, while the learning approaches discussed above
are placed within human-robot interaction settings, they only
learn from demonstrations by a single actor at a time. In
contrast, the work presented here focuses on imitation learning from simultaneously recorded movements by two human
interaction partners in order to learn integrated models of the
joint interaction.
III. L EARNING I NTERACTION M ODELS
The goal of learning an interaction model is to derive a
compact representation of how two agents behave and, in
particular, how they react to each other when they perform
a cooperative or competitive task together. The approach
followed in this paper derives such a representation from observations of human-human interactions. In Figure 2 we see
an overview of this approach. First, the movements of a pair
of persons performing a competitive (or cooperative) task
are recorded using motion capture technology. Subsequently,
an interaction model is learned from the recorded data. The
interaction model captures the reciprocal influences during
the execution of the task. In turn, the interaction model
enables us to predict the state (skeletal configuration) of one
human based on the observed states of the second human.
Finally, the learned model is used by a robot to engage in
a similar interaction with a human partner. In the example
depicted in Figure 2, the humanoid robot learns to perfom
defensive movements in response to a human performing
punching movements.

3258

An interaction model can be regarded as a mapping from
the current state of one agent to the state of a second agent.
In our particular application, we want to learn a mapping
from the state of the opponent agent (i.e. the human) to
the state of the controlled agent (i.e. the robot)1 . Input to
the learning algorithms are the two data sets A (controlled
agent) and B (opponent agent) consisting of joint angle
configurations of the two agents. Each point in A contains
information about the skeletal configuration of the controlled
agent at a particular time step, while B contains a joint
angle configurations for the opponent agent. Once a mapping
from B to A is learned, it can be used to compute the
most appropriate response of the controlled agent, given
the observed movements of the opponent agent. In this
section, we will present two algorithms that can learn such
a mapping.
A. Algorithm 1: PPCA-IM
The first algorithm that we present is the Probabilistic Principal Component Analysis - Interaction Model. The method
exploits the low-dimensional nature of human movement in
order to create a compact model of the interaction. It is well
known from human motor control, that motor tasks, e.g.,
grasping [21], walking [9], and also interactions between
humans [4] lie on low-dimensional manifolds. Such a manifold typically has a much smaller dimensionality than the
total number of joints involved in the motor task. Therefore,
instead of finding a mapping in the high-dimensional space
involving all joints, we can find a low-dimensional space in
which the relationship between the postures and movements
can be learned in a more efficient way. After learning is
finished, the joint values of the controlled agent are treated
as missing values that are estimated by maximizing the
likelihood in the low-dimensional latent space.
The first step to PPCA-IM is the temporal embedding
of the opponent agent’s data. Temporal embedding allows
us to disambiguate between similar movements by including information from prior time steps. In the second step,
namely dimensionality reduction, we then compute a lowdimensional projection of the data set and use this space to
estimate the most likely response for the controlled agent.
1) Temporal Embedding: One possible approach to learning an interaction model is to learn a mapping between
individual pairs of samples in A and B directly. However,
such an approach does not take the temporal offset between
action and reaction into account, and is therefore prone
to fail for many behaviors. For example, a stretched out
arm can either mean that the opponent wants to shake
hands or perform a Karate movement. To disambiguate the
behavior in such scenarios it is important to take the temporal
development of the movement into account. When using
PPCA-IM we perform a temporal embedding of the data in

B yielding a new data set B∗ . To each point in B we add
joint angles of the τ last time steps:


bt ,


b∗t =  ...
(1)
 ∀ bt ∈ B, t > τ.
bt−τ
This embedding is comparable to modelling the interaction
as a Markov chain of order τ , rather than a traditional firstorder Markov chain. A similar preprocessing of the data was
proposed in [2].
2) Dimensionality Reduction: The next step in PPCA-IM
is to create a shared latent space that models the interaction
dynamics of the two agents. As the name of the algorithm
suggests, we use Probabilistic Principal Component Analysis
(PPCA) to learn a shared low-dimensional representation of
the movements. To this end, we create a combined data set
Z which is a concatenation of A and B∗ :
z = [at , b∗t ] ∀ at , b∗t ∈ A, B∗

On the new data set Z we can then perform PPCA. PPCA
is an iterative version of the original PCA algorithm which
uses Expectation-Maximization (EM) [11] to determine the
optimal projection matrix C that maps the data set Z onto
a lower-dimensional principal component space. The PPCA
algorithm used here is based on [19]. An advantage of PPCA
over PCA is that it provides a probabilistic framework for
performing PCA on data sets that have missing values. In
our case, we treat the current joint angles of the controlled
agent as missing values that need to be estimated. The EMalgorithm can be used to estimate the missing values of our
data. This estimation is done by adding an additional entry
z to Z, which consists of an observed part zo and a hidden
part zh . The observed part contains (temporally embedded)
joint values zo of the opponent agent. The hidden part zh
will be estimated during the EM-algorithm and is initially
filled with zeros.
Before starting the EM algorithm, we initialize the projection matrix C and the variance σ 2 with random values
between zero and one. In the E-step, we first calculate new
estimates for the covariance matrix Σ and matrix Y of
projected points:
E-Step:

−1
Σ → I + σ −2 CCT
,
Y

→ σ −2 ΣCZ,

Based on these estimates, we can update in the M-step the
projection matrix C and the variance σ 2 :
M-Step:
C
σ2

1 For

the sake of clarity we will henceforth use the terms controlled agent
and opponent agent to refer to the agents involved in an interaction. Note
that this naming convention does not restrict the application to competitive
tasks only.

(2)

→ZYT (SΣ + YYT )−1 ,
"
#
S
X
1
S Tr{CT ΣC} +
||zi − CT yi ||2 + Dh σ 2 ,
→
SD
i=1

where S is the number of samples, D is the dimensionality
of the samples, and Dh is the total number of missing

3259

PC 1

200

0

-200

-200

0

200

τ=0

-200

0

PC 2

τ = 10

200

-200

0

200

τ = 20

Fig. 4. PPCA projections of a defense interaction for different values of τ .
During the interaction the opponent agent attacks and retracts back to the
rest pose while the controlled agent goes to defense stance and then retracts
to the rest pose. When τ = 0 the temporal context of a posture is not taken
into account.

Fig. 3. The projection of high-five motions into a low-dimensional space
using Probabilistic Principal Component Analysis. Each point in this space
corresponds to specific interaction situation and defines the postures of both
agents. Even if we observe the postures of one agent only, we can still infer
the most likely posture of the interaction partner using PPCA and missing
value estimation.

values in Z. The missing values zh of the matrix Z are
re-estimated before performing the next E-Step by first
calculating Zestim :
Zestim = CT Y,

(3)

and then replacing the missing values zh with the newly
estimated values from Zestim . The above EM-steps can be
iterated until the change in the error of the following objective function is below a given threshold (in our experiments
the threshold is 10−5 ):

B. Algorithm 2: PM-IM

2
Ψ(C, σ) = SD log σ 2 + σ −2 Dh σold
+
" S
#
X
 T
	
−2
T
2
σ
||zi − C yi || + Tr C ΣC ,
i=1
2
σold

pose. Figure 4 shows the projected movement for different
values of parameter τ .
When τ = 0 (Figure 4 left), the postures for going towards
the defensive stance (green) and the postures for retracting
back from the defensive stance (red) are mapped onto the
same points in the low-dimensional space. As a result a
robot cannot distinguish between the two different modes
of this particular movement and would produce the same
reaction in both situations. We can also see in Figure 4,
that with increased value for τ the points are more and
more disentangled. We can see that τ = 20 produces a
clear separation between the two modes, i.e. going to and
pulling back from the defense stance, of the movement. The
trajectory starts at the rest posture in (−250, 0)T , moves to
the defense stance at (250, 0)T , and then moves back to a position close to the rest posture. Note, that in this example we
used a two-dimensional projection for visualization purposes
only. To find a suitable value for the dimensionality of the
low-dimensional space, we can use intrinsic dimensionality
estimation methods [5]. A simpler approach, which was used
in this paper, is to use the number of principal components
that insures that 95% of the information in our training data
is retained after PPCA.

where
is the previous value for the variance. Once the
EM-algorithm is finished, we can use the missing values zh
as the new desired joint angles for the controlled agent.
Figure 3 shows the low-dimensional projection of a highfive interaction calculated with PPCA. Each point in the
low-dimensional space encodes the reaction of the controlled
agent with respect to the previous movements of the opponent agent. We can see that the interaction forms a smooth
trajectory in the low-dimensional space.
To better understand the role of temporal embedding in our
learning algorithm, we computed several PPCA projections
with different values for the parameter τ (from Equation
(1)). For this purpose, we recorded a defensive movement
in which a human starts in a rest pose, then moves both
arms in a defensive stance, and finally goes back to the rest

The second algorithm for learning interaction models is
called Path Map-Interaction Model (PM-IM). The algorithm
uses a HMM to represent the mutual dependency of the
interaction partners at different time steps.
A HMM is an efficient tool for modelling probability
distributions over time-series’. It assumes that a set of
observations was generated by a process with hidden
internal states. Given the Markov assumption, any state st
only depends on the predecessor state st−1 . Following the
notation in [20] and [6], a HMM can be defined as the tuple
θ = (S, π, Pj→i , pi (o|si )) where

3260

•

S = {s1 , ..., sN } is the set of states si of the HMM

•

π = {π1 , ..., πN } is a probability distribution specifying
the probability πi of starting in state i

•

Pj→i is the state transition matrix defining the probability p(sj |si )of transitioning from state si to sj

a1

a2

a3

S1

S2

S3

b1

b2

every situation.
An interesting feature of HMMs is the ability to use
several HMM models in parallel. For example, assume that
we learn two HMMs for different interaction tasks, e.g.,
punching and handing-over. Given a new observed movement
of the attacking agent, we can calculate the likelihood of this
movement with respect to each learned HMM and select the
model with highest likelihood according to:

...

θ∗ = arg max p(bt |θ).
θ

b3

Fig. 5.
The graphical model of a path map Hidden Markov Model.
Each hidden node (white) is connected to two observed nodes (colored)
corresponding to each of the interacting agents. Each observed node contains
the joint angle configuration of the respective agent which is depicted by
a small skeleton. The diagram shows a path map for an punch/defense
interaction.

(4)

Once the HMM with highest likelihood is selected, we can
calculate the emissions for the current situation and use the
resulting joint angle values for controlling the robot. The
above feature allows us to use the knowledge of several
HMMs in order to recognize an interaction scenario and also
to respond to the behavior of the human partner. The set of
interaction skills can, therefore, be gradually expanded.
IV. E XPERIMENTS

•

p(o|si ) is the emission probability distribution which
defines the probability of observing output o while
in state si . The emission probability distribution is
modelled using a Gaussian distribution.

As already mentioned, the nodes of an HMM can be divided
into observable nodes and hidden nodes. Typically, an HMM
is defined in such a way that each hidden node is connected
to one observable node only. In the following, however, we
will use an extension of HMM, sometimes also referred to
as a path map[6], which has a different graph structure.
A path map relates the time-series behavior of a cue
system to the behavior a target system. This is achieved
by connecting each hidden node to two observables nodes:
one observable for the cue system and one observable for
the target system. A path map for the task of interaction
modelling can be seen in Figure 5. The colored nodes
correspond to the observables of the controlled agent (blue)
and the opponent agent (red) respectively. Each observable
state holds the full joint angle configuration of the respective
agent in the current situation. The white nodes depict the
hidden states of the interaction task. Each hidden state
models a specific context or situation during the interaction.
In contrast to the standard HMM, a path map contains two
emission probability distributions pA (at |st ) and pB (bt |st );
one for each of the two agents. The training of the path
map, however, can be performed using the same approach
as for a standard HMM. First, a K-Means[18] clustering
algorithm is used to initialize the hidden states of the HMM.
Using the EM [11] algorithm, we can then estimate all
missing parameters of the HMM. A detailed description of
HMM training can be found in [20]. Once it is learned, the
path map in Figure 5 allows us to estimate the behavior
of one agent by observing the movements of the other. We
first calculate the most likely sequence of states given the
observed behavior of the opponent agent. Using the emission
probability distribution pB (bt |st ) of each state, we can then
generate an appropriate response for the controlled agent in

To evaluate the algorithms proposed in this paper, we
conducted a set of interaction experiments and analyzed the
results. In the following sections, we will report the results
achieved by applying the algorithms in simulation as well as
on a real robot performing human-robot interaction with a
human partner.
A. Interaction Data
Before training any specific model, we first collected
a set of training data representing different competitive
and cooperative interaction tasks. Specifically, we collected
motion capture data of two interacting humans. The data
set consisted of various interaction tasks acquired from the
CMU Motion Capture Library2 , as well as additional data
gathered using two Kinect cameras and two human subjects.
In order to be independent of a specific tracking device, we
transformed all motion capture data into the BioVision file
format and used the resulting joint angle information as input
to the learning algorithms. In total, we used 18 joints, with
each joint being parametrized by three joint angles. The final
data set consisted of four different interaction tasks:
• Boxing: One agent attacks with punches at different
heights and from different directions while the other
agent defends.
• Martial Arts: One agent attacks with punches and kicks
while the other defends.
• High Five: Both agents perform a high-five movement.
• Handing over: One agent hands a book over to the other
agent.
B. Runtimes
In interactive scenarios, a robot needs to quickly respond
to the behavior of the human partner. In the following we
will, therefore, analyze the computational demands of the
proposed algorithms and the runtimes for predicting the
appropriate response in the current situation. Figure 6 depicts

3261

2 http://mocap.cs.cmu.edu/

0.02

0.4

PPCA-IM 20
PPCA-IM 10
PPCA-IM 5

0.3

Human
PM-IM 20 cl.
PM-IM 40 cl.
PM-IM 60 cl.

0.2
0.01

Top
Center
Bottom

2.0

z-position [m]

T ime [sec]

0.03

0.1

1.8
1.6
1.4
1.2
1.0
0

2

3

4

Num. of Behaviors

5

0.0
1

20

40

60

80

100

120

T ime step
2

3

4

PM-IM

PPCA-IM

5

Num. of Behaviors

Fig. 6. The runtime of the PPCA-IM and the PM-IM algorithms. The values
indicate the measured time needed for predicting the optimal response of
the robot given the human’s action at a particular time step. With increasing
number of learned behaviors, the time needed to predict the optimal response
increases, too. Additionally, the plots also show how the size of the temporal
embedding window (20,10,5) affects the runtime of the PPCA. The right
plot shows how the number of states/clusters affects the runtime of the
PM-IM.

the runtimes of the PPCA-IM and the PM-IM algorithms.
For training we used an increasingly complex data set with
one to five different behaviors. Each behavior consisted of
approx. 120 data samples. The plots show how the number
of interactive behaviors affects the response time of the robot
when applying an interaction model after learning.
As can be seen in Figure 6, PPCA-IM has a significantly
faster response time. Especially, with increasing number of
clusters/states in the PM-IM, the response times quickly
deteriorate. With 60 hidden states, the PM-IM requires about
0.4 seconds to compute a prediction. A smaller number of
states can be used to speed up the algorithm. However, this
comes at the price of a significantly lower quality of the
learned model. In the above example, the PM-IM was only
able to produce accurate responses when 55 or more states
were used. The PPCA-IM was used with a 7 dimensional
latent space.
C. Generalization
Another important feature of interaction models is the
ability to generalize learned behaviors to new situations. To
analyze the generalization ability, we conducted a set of
experiments in which we trained interaction models for a
boxing/defending behavior. The models were trained with
high- and low-punches, and were later tested with several
other punches that aimed at a position inbetween the trained
punches. Figure 7 shows the z-position of the wrist of the
controlled agent while trying to defend several punches.
The gound truth data gathered from the human clearly
shows that the hand needs to be lifted to different levels, in
order to defend from the upcoming punch. The trajectories
generated with the PPCA-IM model have similar characteristics to the human trajectories. The blue trajectory in Figure 7
corresponds to a movement that was not seen in the training
data. Despite that it was not trained, the PPCA-IM was still
able to generalize the learned movement to this new situation.
Both the shape and height of the trajectory are close to the
ground truth of the human demonstrator. In contrast to that,
the PM-IM does not exhibit a similar generalization ability.
In the depicted case, the PM-IM repeatedly switches between

z-position [m]

0.0
1

2.0

2.0

1.8

1.8

1.6

1.6

1.4

1.4

1.2

1.2
1.0

1.0
0

20

40

60

80

T ime step

100

120

0

20

40

60

80

100

120

T ime step

Fig. 7. The wrist position of the controlled agent for different defenses. The
human raises his hand to different heights depending on the type of punch
he receives. The defense movement for a center punch was not learned. The
PPCA-IM algorithm is still able to generalize to this situation. The PM-IM
algorithm does not generalize well in this situation.

states for high-punches and low-punches leading, over time,
to oscillations with an increasing amplitude.
An interesting property of PPCA-IM is the fact that it
automatically produces continuous outputs in every time
step. The controlled agent reacts even to small changes in
the behavior of the opponent agent. By nature the PM-IM is
a discrete model and does not produce different outputs in
every time step. Still, a continuous output can be generated
by using interpolation (as done in the above examples) or by
incorporating velocity information into the model.
D. Robot Experiments
In order to validate our results on a real robot, we
conducted an experiment in which a NAO robot learned a
set of interaction skills that can be performed cooperatively
or competitively with a human partner.
However, in order to replay any of the synthesized movements with the used NAO robot, we first have to find a
mapping between the body parts of the human demonstrator
and the body parts of the robot. This problem is commonly
referred to as the correspondence problem [8] and is a
fundamental problem of imitation learning. In this paper,
we performed the mapping by using inverse kinematics (IK)
on the extremities of the robot. The human skeleton was
scaled to the size of the robot and IK was used to ensure
that the positions of the feet, hands, pelvis and head of
the robot matched the positions of the human extremities.
More specifically, we used the iTaSC [10] IK algorithm for
fitting the human skeleton to the robot. We have released the
software package for IK-based correspondence matching of
the NAO robot as an open-source tool for the general public3 .
To test our algorithms, we trained interaction models for
the martial arts data set. The robot learns to recognize and
3 The software can be downloaded as a Blender-extension from
https://bitbucket.org/JuvenileFlippancy/naoblender

3262

Log-likelihood

Robot

Human

Punch right high
Punch right low
Kick right low
Punch left high
Punch left low

200

-500

Time

Fig. 8. A martial arts scenario trained and executed with the PM-IM algorithm. Top: The captured movement of the human. Second row: The joint angle
configurations generated by the PM-IM after observing the human movement. Third row: The log-likelihoods for the different behaviors. Below: Pictures
of the interaction between the human and the robot. The human movement was recorded with a Kinect camera.

defend different types of attacks, e.g., punchrighthigh, punchleftlow, kickrightlow. A set of 12 behaviors was used for
training. Depending on the type of attack a different defense
behavior is executed. For learning the PM-IM we used 60
to 70 hidden states. The number of hidden states for each
behavior was estimated using cross-validation on the training
data. For PPCA-IM we used a sampling rate of 10Hz and
τ = 20.

Figure 8 shows the movements of the human and the
responses of the NAO robot. Note that the defense posture
for an attack with the right hand and attacks with the left
hand are different: robot lifts only one arm or both arms for
defense. Similarly, the defense stance for a low-kick requires
the robot to kneel down and block with one arm. The Figure
also shows the log-likelihoods for the different behaviors
that are generated by the HMMs. Interestingly the difference
in the log-likelihood is very high when the opponent agent
executes a kick-right-low movement. The robot can easily
disambiguate this case as it is only when of two trained
behaviors which use the leg. Both the PM-IM as well as
the PPCA-IM model can solve the above task and produce
appropriate responses for the NAO robot. The PPCA-IM
model was again trained with 7 latent dimensions. Apart
from martial arts examples we have also trained interaction
models for the other data sets. Figure 1 shows the behavior
of the robot after training a handing-over interaction task.

E. Discussion
The results suggest that both PPCA-IM and PM-IM can
be used encode and reproduce the joint dynamics of the
interaction partners in a shared task. However, the results
also show various advantages and shortcomings of these
algorithms. The PPCA-IM approach is particularly well
suited for modelling continuous reponses and correlations
in the movements of the interaction partners. It has limited
computational demands and generalizes to some extent to
new situations. This shows that dimensionality reduction can
be an effective measure for extracting the hidden structure
in interaction data. Without dimensionality reduction, the use
of the temporal embedding for motion capture data would be
computationally prohibitive and the learning would require
a significantly larger amount of data.
At the same time, PPCA-IM does not provide information
about the state or the development of the interaction. In
this regard, the HMM-based PM-IM algorithm provides a
richer set of tools for recognizing and estimating of the
current state of the interaction. Yet, this comes at the price
of significantly higher computational demands, as well as
limited generalization abilities. Consequently, it would be
interesting to combine the approaches presented in this paper
by using a HMM with PPCA-IM models as emissions. In
such a case, the HMM can be used to realize a spacetime linearization of the training data, while the PPCAIM can take on the role of modelling the correlations in
the movements of the interaction partners in a particular
temporal context. Recent advances in HMM training [23]

3263

also suggest a closer relationship between dimensionality
reduction and temporal models such as HMMs.
V. CONCLUSIONS
In this paper we presented a new approach for teaching
robots how to respond to the movements of a human partner.
Using motion capture technology, the movements of a pair of
persons are first recorded, and then processed using machine
learning algorithms. The result is a model of how each person
adapted its behavior to the movements of the respective other.
Once an interaction model is learned, it can be used by a
robot to engage in a similar task with a human counter part.
We have also provided two algorithms called PPCA-IM and
PM-IM, that are extensions to known methods, which can
be used for learning interaction models from motion capture
data. The algorithms allow a robot to learn when and how to
respond to the behavior of a human partner. All methods
were implemented on a NAO humanoid robot and were
evaluated in cooperative and competitive tasks. After learning
an interaction model, the NAO robot was able to generate
appropriate defense responses in a challenging martial arts
scenario. The discussion of the advantages and shortcomings
of each of the two algorithms suggests that a combination
of temporal models and dimensionality reduction can be an
interesting path for developing more sophisticated models of
interactions.
While the results in this paper are encouraging, there are
various aspects of imitation learning in multi-agent scenarios
that need further investigation. In particular, it is interesting
to investigate how learned models can be used to predict the
future behavior of an interaction partner given the actions
of the controlled agents. This can be helpful in avoiding
decisions that potentially lead to dangerous situations or
injuries. In this paper, we did not investigate the aspect of
force transfer between a human and a robot. Even small
forces that are exchanged between interaction partners can
have a significant impact on the execution and success of
a joint task. First research results on incorporating force
transfer in interaction models can be found in [1]. Another
aspect that needs further investigation is task space control.
For some interaction tasks it is important that constraints are
fulfilled within the task space. We are currently investigating
the use of Interaction Meshes [12] for this purpose. Finally,
it is also important to include tertiary objects, e.g., a jointly
lifted box, into the interaction model.
VI. ACKNOWLEDGMENT
The work presented in this paper is funded through the
CoDyCo project of the European Community’s Seventh
Framework Programme under the grant agreement n ICT600716 (CoDyCo).
R EFERENCES
[1] E. Berger, H. Ben Amor, N. Haji-Ghassemi, D. Vogt, and B. Jung.
Inferring guidance information in cooperative human-robot tasks.
In IEEE-RAS International Conference on Humanoid Robots (HUMANOIDS). IEEE, 2013 (submitted).

[2] F. Biessmann, F. C. Meinecke, A. Gretton, A. Rauch, G. Rainer,
N. Logothetis, and K.-R. Müller. Temporal kernel canonical correlation analysis and its application in multimodal neuronal data analysis.
Machine Learning, 79(1-2), 2009.
[3] A. Billard, S. Calinon, R. Dillmann, and S. Schaal. Survey: Robot
Programming by Demonstration. In Handbook of Robotics, volume
chapter 59. MIT Press, 2008.
[4] D.P. Black, M.A. Riley, and C.K. McCord. Synergies in intraand interpersonal interlimb rhythmic coordination. Motor Control,
11(4):348–73, 2007.
[5] C. Bouveyron, G. Celeux, and G. Stphane. Intrinsic dimension
estimation by maximum likelihood in isotropic probabilistic {PCA}.
Pattern Recognition Letters, 32(14):1706 – 1713, 2011.
[6] M. Brand and A. Hertzmann. Style machines. In Proceedings of
the 27th annual conference on Computer graphics and interactive
techniques, SIGGRAPH ’00, pages 183–192, New York, NY, USA,
2000. ACM Press/Addison-Wesley Publishing Co.
[7] S. Calinon, E.L. Sauser, A.G. Billard, and D.G. Caldwell. Evaluation
of a probabilistic approach to learn and reproduce gestures by imitation. In Proc. IEEE Intl Conf. on Robotics and Automation (ICRA),
pages 2381–2388, Anchorage, Alaska, USA, May 2010.
[8] K. Dautenhahn and C. L. Nehaniv. Imitation in Animals and Artifacts.
MIT Press, Campridge, 2002.
[9] A. d’Avella, P. Saltiel, and E. Bizzi. Combinations of muscle synergies
in the construction of a natural motor behavior. Nat Neurosci,
6(3):300–8, 2003.
[10] J. De Schutter, T. De Laet, J. Rutgeerts, W. Decré, R. Smits,
E. Aertbeliën, K. Claes, and H. Bruyninckx. Constraint-based task
specification and estimation for sensor-based robot systems in the
presence of geometric uncertainty. Int. J. Rob. Res., 26(5):433–455,
May 2007.
[11] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood
from Incomplete Data via the EM Algorithm. Journal of the Royal
Statistical Society. Series B (Methodological), 39(1):1–38, 1977.
[12] E. Ho, T. Komura, and C. Tai. Spatial relationship preserving character
motion adaptation. ACM Transactions on Graphics, 29(4):1–8, 2010.
[13] A. Ijspeert, J. Nakanishi, and S. Schaal. Learning attractor landscapes
for learning motor primitives. In Suzanna Becker, Sebastian Thrun,
and Klaus Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 1523–1530. MIT Press, 2002.
[14] S. Ikemoto, H Ben Amor, T. Minato, B. Jung, and H. Ishiguro.
Physical human-robot interaction: Mutual learning and adaptation.
IEEE Robotics and Automation Magazine, 19(4):24–35, Dec.
[15] M. Ito and J. Tani. On-line imitative interaction with a humanoid robot
using a dynamic neural network model of a mirror system. Adaptive
Behavior, 12(2):93–115, 2004.
[16] Z. Kolter, P. Abbeel, and A. Ng. Hierarchical Apprenticeship Learning
with Application to Quadruped Locomotion. In John C. Platt, Daphne
Koller, Yoram Singer, and Sam T. Roweis, editors, Advances in Neural
Information Processing Systems (NIPS). MIT Press, 2007.
[17] D. Lee, C. Ott, and Y. Nakamura. Mimetic communication model
with compliant physical contact in human-humanoid interaction. Int.
Journal of Robotics Research., 29(13):1684–1704, November 2010.
[18] J. B. MacQueen. Some methods for classification and analysis of
multivariate observations. In L. M. Le Cam and J. Neyman, editors,
Proc. of the fifth Berkeley Symposium on Mathematical Statistics and
Probability, volume 1, pages 281–297. University of California Press,
1967.
[19] J. Porta, J. Verbeek, and B. Krose. Active appearance-based robot
localization using stereo vision. Autonomous Robots, 18(1):59–80,
2005.
[20] L. Rabiner. A tutorial on HMM and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257–286, February 1989.
[21] M. Santello, M. Flanders, and J. F. Soechting. Postural Hand Synergies
for Tool Use. The Journal of Neuroscience, 18(23):10105–10115,
December 1998.
[22] S. Schaal. Is imitation learning the route to humanoid robots? Trends
in Cognitive Sciences, 3:233–242, 1999.
[23] L. Song, B. Boots, S. M. Siddiqi, G. J. Gordon, and A. J. Smola.
Hilbert space embeddings of hidden Markov models. In Proc. 27th
Intl. Conf. on Machine Learning (ICML), 2010.
[24] Z. Wang, M. Deisenroth, H. Ben Amor, D. Vogt, B. Schoelkopf, and
J Peters. Probabilistic modeling of human dynamics for intention
inference. In Proceedings of Robotics: Science and Systems (R:SS),
2012.

3264

2014 14th IEEE-RAS International Conference on
Humanoid Robots (Humanoids)
November 18-20, 2014. Madrid, Spain

Handover planning for every occasion
Ana Huamán Quispe

Heni Ben Amor

Abstract— In this paper we explore shared manipulation
tasks involving collaboration between two agents: a bimanual manipulator and a movable humanoid. We propose a
deterministic planner that outputs a handover pose for the
object manipulated, the grasps to be used for both agents,
and arm trajectories that allow the agents to reach, grasp,
interchange and place the object at its final goal pose. Most
existing approaches to shared manipulation assume that both
participating agents are (or can be) positioned face-to-face.
In this paper, we consider a more general set of scenarios in
which the relative pose between agents is not tightly constrained
to facing each other. To accomplish a successful interaction
between the agents, careful planning must be done in order to
select the best possible handover pose such that the movable
agent should not need to move its torso, only its arm, to interact
with the object. We present 3 simulation experiments with
manipulation tasks performed by a bimanual fixed manipulator
and a humanoid robot.

Reaching a cup from front Reaching a jar from above

Reaching a hammer from below
Fig. 1: Shared manipulation tasks with non face-to-face
interaction between agents

I. I NTRODUCTION
Useful home robots should be capable to perform simple
tasks that make human lives easier. In particular, we consider
collaborative manipulation tasks in which a robot acts as an
assistant, handing objects to a user. While inherently simple,
these types of tasks are more likely to happen in a household
in a regular basis, so we consider it a key robot capability.
Shared manipulation tasks can be studied from different
perspectives. From the side of human-robot interaction, there
is a vast amount of work that analyses the information
exchanged between robot and user during the handing over
of the manipulated object [2]. On the other hand, the manipulation planning community frames the problem mostly
from the point of view of the robot in order to select a robust
grasp on the object and then find a feasible arm trajectory that
allows the robot to perform its share of the task successfully.
In this paper, our goal is to enable a fixed robot to
effectively perform a handover task while keeping the movable receiver agent in mind. We acknowledge the movable
receiver agent by considering the receiver’s dexterity while
placing the object at its goal pose. Furthermore, we address
the fact that for different tasks, the movable receiver might
not be ideally located with respect to the fixed robot (i.e.
facing it), so it is the fixed robot’s task to select an arm
trajectory that places the manipulated object in a pose such
that the receiver can easily grasp it. Depending on the
specific situation, this could be naively simple or notoriously
difficult. As an illustration, consider the examples shown in
Figure 1. All of them involve a bimanual fixed manipulator
handing over an object to a movable humanoid whose relative
pose with respect to the fixed manipulator varies drastically

between examples (i.e. the humanoid does not face the fixed
manipulator in neither of these examples, a fact that is
commonly assumed for shared manipulation tasks). In this
paper, we intend to address situations as the ones depicted,
in which the movable receiver can vary its pose with respect
to the fixed robot. Among some questions that must be
answered to solve this problem are: A) Which arm should
the fixed robot use to hand the object? B) How should the
object be grasped such that the movable receiver can grasp
it easily? and C) Which handover pose is good enough as to
reduce the overall effort of the movable receiver?.
The rest of this paper is organized as follows: Section II
summarizes relevant literature on shared manipulation tasks.
Section III and IV explains our algorithm, the assumptions
considered and our current limitations. Section V presents
simulation results in 3 different arbitrary environments and
finally in Section VI we discuss our approach, its advantages
and shortcomings as well as future work.
II. P REVIOUS W ORK
In [5] Edsinger et al. argue that fluent handover between
a robot and a human allows for a range of cooperative
manipulation tasks. They present a set of perception and
control modules which can be used to realize a simple object
exchange between interaction partners. Kemp and colleagues
argue that the complimentary skills of humans and robots
can be exploited in such a scenario. More specifically, the
challenging task of robot grasp planning can be simplified by
having the human place the object in the robot’s end effector.
Mainprice and colleagues [8] investigated how the spatial
preferences of a human partner can be incorporated into
the robot decision making during handover tasks. They

Institute for Robotics and Intelligent Machines, Georgia Institute of
Technology, Atlanta, GA 30332, USA.ahuaman3@gatech.edu,

hbenamor@cc.gatech.edu
978-1-4799-7174-9/14/$31.00 ©2014 IEEE

Mike Stilman†

431

introduced a planning algorithm which can be parametrized
by specifying a mobility parameter. The mobility parameter
reflects the ability or willingness of the human partner to
move to a new position to obtain an object. In case of a high
mobility parameter, the robot can plan handover actions that
require some effort from the human partner. In contrast, a
low mobility parameter means that the robot should reduce
the human’s effort by moving to the exchange site location.
In [9], they also investigated how gaze direction and postural
comfort can be used to identify a transfer point
In [4] Cakmak and colleagues investigated human preferences in robot handover configurations. In particular, they
analyzed how humans responded to planned robot joint
configurations during handover tasks. In their study they
also allowed human subjects to provide positive and negative
feedback to the robot on which configurations are more
appropriate. This feedback was, then, used to improve the
naturalness and legibility of the robot handover motion. Their
experiments showed that planned handover motion can provide higher reachability of the object, while learned handover
behavior can improve the naturalness and appropriateness of
robot joint configurations.
Koene et al. [7] showed that temporal precision plays an
important role during object exchange. For fluent interaction,
a robot needs to synchronize the timing of the arm movement
with the arm movement of the human partner. While we
acknowledge that fluency and naturalness are important
characteristics for robot interactions with humans, this paper
focuses on planning interactions that allow two robot agents
to keep dexterous arm configurations during the handover,
placing less priority to the factors mentioned above (although
a high dexterity is usually correlated to smooth, humanlooking arm configurations).

Known scenario

Object (O)

Giver (Rg )

Receiver (Rr )

Fig. 2: Problem definition for butler scenario

Start pose Ts

Goal pose Tg

Fig. 3: Task definition for butler scenario
C. Algorithm Overview
At high level, our algorithm can be summarized as follows:
1) Decide which arm Rg will use to grasp the object.
2) Calculate a set of feasible grasps for Rr (Gr ) that
allows it to place O at Tg
3) Calculate a set of feasible grasps for Rg (Gg ) that
allows it to reach O at its start pose Ts
4) Calculate a set of feasible handover solutions H using
the information provided by the set of feasible grasps
Gg and Gr and the pose of the two robotic agents Rg
and Rr .
5) Select the handover solution from H that maximizes
the dexterity of the arms in the shared manipulation
task and plan the corresponding arm trajectories.
In section IV we will expand and explain each point above
in more detail. Any solution from H should have information
such as Figure 4: The handover pose Th and the grasps to
be executed by Rg and Rr .

III. P RELIMINARIES
In this section, we formally define the scenario and the
shared manipulation task to be addressed through this paper. We also provide an intuitive overview of our solution
approach, to be expanded in the following section.
A. Problem definition
Our shared manipulation task can be defined with the
following elements:
• A fully known 3D environment
• A target object O to be manipulated.
• Two agents: a fixed robot giver (Rg ) and a movable
robot receiver (Rr ).
In this and the following section, we will use as an
example our butler scenario, whose problem definition is
shown in Figure 2.

IV. A LGORITHM
In this section we will go in detail over the points briefly
addressed in III-C:
A. Arm selection for Rg

B. Task definition
The shared manipulation task consists in transporting the
target object O from its start pose Ts to a goal pose Tg .
Furthermore, we assume Ts to be only reachable by Rg and
Tg reachable by Rr , hence the object must be picked up by
Rg , handed off to Rr and finally placed in its final pose Tg
by Rr . An example of Ts and Tg for our butler test scenario
is shown in Figure 3.

Our planner must first determine if O is located within
the reach of Rg . For this, the reachability space [6] of Rg
(Rg ) is generated offline. We fill Rg by sampling a large
number of random joint configurations for each arm of Rg
and storing the average manipulability [11] of each sample
in the voxel corresponding to the Tool Center Point (TCP)
position of the end effectors (an entry of 0 means that the
432

Initial setup

Handover

Reach O at Ts
Grasp 0/28

Grasp 11/28

Grasp 17/28

Grasp 26/28

Place O at Tg

Fig. 4: Expected solution for butler scenario
arm cannot reach that location) . Since Rg is a (fixed) dual
manipulator, RG will provide us with a manipulability metric
for both left and right arms at each voxel.
The object O is considered within reach of Rg if the voxel
corresponding to Ts in RG has a non-zero entry for either
the left arm, right arm or both. If both entries are non-zero,
then the arm selected will be the one with the highest entry
value, since this suggests that the corresponding arm will
have more dexterity.
If only one entry is non-zero, then the corresponding arm
is selected. If both entries are zero then the task is considered
infeasible and no further action is required.

Fig. 5: Candidate grasps at Tg executed by Rr
of these feasible grasps for our butler scenario example can
be seen in Figure 6.

B. Generating Grasp candidates for Rr (Gr )
The next step consists on calculating a set of grasps and
arm configurations that allow Rr to place O at its goal
location Tg .
For this step we use a set of precomputed grasps for O.
After placing the object O at Tg , we test each of the grasps
in Gr and prune the candidates that either not kinematically
feasible or that present collision with the environment. This
procedure can be seen in Algorithm 1. For our butler
scenario, some of the non-pruned grasps (∈ Gr ) are depicted
in Figure 5.

Grasp 0/30

Grasp 3/30

Grasp 8/30

Grasp 18/30

Fig. 6: Candidate grasps at Ts executed by Rg
D. Calculation of feasible handover poses

Algorithm 1: PruneGrasps(R, O, G)
Input: Robot R, Object O and its corresponding grasp
set G
Output: A feasible subset of G (G ∗ ) and its arm
configurations (Q∗ )
1
2
3
4
5
6

There exists an infinite number of possible poses for the
object O to be handed from Rg to Rr . To perform an
exhaustive search in this space would demand significant
computation time.
Instead, we approach the problem by reducing the possible
handover poses space by considering 2 simple observations
from human grasping:
• When transporting an object, humans tend to reduce
orientation changes in the object being grasped, unless
necessary.
• Whenever possible, we keep our wrist in a resting pose
while manipulating an object.
Given the points above, we generate a set of handover
tuples (stored in H) using Algorithm 2. Each tuple contains
a handover pose for the object O, the grasps for Rr and Rg
with an arm configuration that allows Rr and Rg to execute
the grasp.

foreach g ∈ G do
q ← R.executeGrasp(g, O)
if checkCollision() is false then
G∗ ← G∗ ∪ g
Q∗ ← Q∗ ∪ q
return G ∗ , Q∗

C. Generating Grasp candidates for Rg (Gg )
Similar to the step above but now the object O is placed
in Ts and the generated grasps are stored in Gg . An example
433

The specific procedure to calculate a handover grasp pose
Th is shown in Algorithm 3. The translation component of
Th is obtained from the set of near neighbours of the middle
point between the shoulders of Rr and Rg . The voxel that can
be reached from both arms and that has the highest average
manipulability is chosen as the position for Th . Regarding
the rotation, we take advantage of the information provided
by the grasp set Gr : We calculate the pose by applying a
minimum rotation between the Rr ’s hand approach vector at
Tg and the direction between the shoulders of Rr and Rg .
A couple of example poses for our butler scenario can be
seen in Figure 7.
Grasp 4/29
Algorithm 2: Get set of candidate handover poses

Fig. 7: Candidate handover poses during transfer phase

foreach gr ∈ Gr do
Th , qr ← GetHandoverPose(Rg , Rr ,O,gr ,Tg )
if Th 6= NULL then
O.set(Th )
Rr .set(qr , gr )
Gg∗ ← PruneGrasps(Rg , Gg )
if Gr∗ 6= ∅ then
foreach gg ∈ Gg∗ do
H ∪ (Th ,gr , gg , qr , qg )

1
2
3
4
5
6
7
8
9

and generate arm trajectories between them in order to reach,
pick, handover and place the object. For this we use an
standard IKBiRRT [3] approach.
V. S IMULATION R ESULTS
We set 3 simulation environments, as shown in Figure 1.
All three scenarios involve 2 robots, which will be briefly
described in the following subsection:
A. Generalities
1) Crichton: Crichton is a bimanual fixed manipulator
consisting on 2 Schunk LWA4 arms with a Schunk Dexterous
Hand (SDH) attached to each last arm link. Each arm has
7 DOF, whereas the SDH possess 3 fingers and 8 DOF. A
pseudo-analytical IK solver for the LWA4 arm was used for
IK queries.
2) Hubo: Hubo is a humanoid robot with two 7-DOF
arms. Each arm has a hand at their last link. The left hand
has 3 fingers and 6 DOF whereas the right hand has 4 fingers
and 8 DOF. A inverse-Jacobian IK solver for the Hubo arms
was used for IK queries.
3) Simulation environment: For our simulation experiments we used DART [1]. The environment used, as well
as the object and robot models, are replicas of their real
hardware counterparts in order to make the transition to
hardware experiments easier.
4) Grasp generation: The grasp datasets for each object were generated offline and loaded online for all the
experiments shown. The numbers of grasps generated per
each object are of a few hundreds, as it can be seen in
Table I, hence, the selection of a handover pose and the
corresponding grasps to use per each robot is not trivial.
Since the grasp set generation is independent of the
planning algorithm presented here, any method to generate
grasps can be used. In particular, we used the common
approach of evenly sampling the object’s surface and set
the hand’s palm just above each sample, with the approach
direction parallel to the normal of the object at each of these
points. Other methods could be used to generate the set of
precomputed grasps, such as GraspIt! [10] or other similar
open source tools.
Finally, all experiments were ran in an Intel Core i7
machine (1.6GHz).

Algorithm 3: GetHandoverPose(Rg , Rr , O, gr , Tg )
Input: Robots, object, goal pose and grasp for Rr
Output: Handover pose Th and arm conf. for Rr (qr )
1

zshoulder ←getVec(Rr .shoulder(),
Rg .shoulder())

2

p ← Rr .shoulder() + 0.5 · zshoulder
Sr ← GetClosestVoxels(Rr , p)
Sg ← GetClosestVoxels(Rg , p)
S ← Sr ∩ Sg

/* Get translation for Th
3
4
5

/* Sort according to manipulability
6
7

9
10
11
12
13
14
15

*/

*/

S ∗ ← Sort(S)
Th .trans() ← S ∗ .front()
/* Get rotation for Th

8

Grasp 21/29

*/

O.setPose(Tg )
qr ← Rr .executeGrasp(gr ,O)
Thand ← Rr .getHandPose()
rot ← minRot(Thand .z(), zshoulder )
Th .rotation() ← rot × Thand × gr .Tho ()
O.setPose(Th )
qr ← Rr .executeGrasp(gr , O)
return Th ,qr

E. Selection of dexterous handover pose and arm trajectory
generation
After the step described before, our planner has a set H
of handover poses with the corresponding grasps for Rr and
Rg . The only remaining task is to select one of these tuples
434

TABLE I: Number of grasps per object
Object
Cup - Butler scenario
Hammer - Stool scenario
Food jar - Kneeling scenario

SDH Hand
316
445
448

Hubo left hand
368
590
448

TABLE II: Results for 100 randomized butler scenarios
Hubo right hand
368
644
448

B. Experiment 1: Butler scenario
Our first test problem is depicted in Figure 8: The target
object O (a green cup) is initially resting - upside down - on
a cart located to the left of the fixed manipulator (Crichton).
The goal is to place O upright on the middle of the tray
carried by Hubo’s right hand.
We performed 100 tests for randomized scenarios similar
to the one depicted in Figure 8. The variables randomized
were:
• Hubo’s translation along an axes parallel to the table’s
rim (total range of movement 1.6 m).
• Hubo’s rotation around its Z axis (pointing up) (total
range: 30 degrees with respect to orientation directly
facing Crichton).
• Tray’s height: The tray held by Hubo’s right hand was
randomly moved up and down in a total range of 6 cm.
Some statistics (such as planning time and number of
average handover poses generated) are shown in Table II
and 2 handover poses for different randomized examples are
shown in Figure 9

Ts : Inverted cup on cart

Parameter measured
Average Planning time
Successful plans
Average number of grasps for Rg
Average number of grasps for Rr
Average number of handover poses

Values
2.31 seconds
100/100
32.36
39.77
16.64

C. Experiment 2: Stool scenario
Our second test scenario consists on Hubo standing on top
of a 3-step stool while Crichton hands it a hammer from its
start pose on the table in order for Hubo to lift it in front of
it with its head pointing to the right (see Figure 10).

Ts : Hammer on table Tg : Hammer pointing up (+Y)
Fig. 10: Start and goal configurations for stool test case
In a similar way as the butler case, here we also performed
100 randomly varied runs. The varied parameters were the
location of Hubo (first, second or third step on the stool) and
the pose of the stool (some translation along an axis parallel
to the table longest side and a small rotation around the up
axis, no larger than 20 degrees). Results regarding number
of possible handover solutions are shown in Table III and
some solutions for the random cases are shown in Figure 11

Tg : Cup upright on tray

Fig. 8: Start and goal configurations for butler test case

Random test case 1 (first step)

Butler random case 1

Random test case 2 (third step)
Fig. 11: Sample solutions for random stool test cases
Notice that the planning time increases for this example,
this probably due to the fact that the size of the grasp sets
for the hammer object is bigger than for the cup, hence the
search space increases in size.

Butler random case 2
Fig. 9: Sample solutions for random butler test cases

435

TABLE III: Results for randomized stool scenarios
Parameter measured
Average Planning time
Successful plans
Average number of grasps for Rg
Average number of grasps for Rr
Average number of handover poses

TABLE IV: Results for 100 randomized kneeling scenarios

Values
4.62 seconds
100/100
56.87
143.28
89.49

Parameter measured
Average Planning time
Successful plans
Average number of grasps for Rg
Average number of grasps for Rr
Average number of handover poses

D. Experiment 3: Kneeling scenario
Our final test scenario is depicted in Figure 12: Hubo is
kneeling and needs to be handed an object from the table, in
this example O is a dog food jar whose goal pose is above
the dog’s plate and slightly tilted (rotated with respect to the
X axis going from table towards Hubo).

Ts : Dog jar on table

Values
3.1845 seconds
100/100
30.45
72.91
39.89

that maximizes the dexterity of Rr while performing the final
object placing.
Our approach reduces its computation time by considering
only a subset of all the possible handover poses. For this,
it takes advantage of the grasps calculated for Rr . By using
them and the direction between the shoulders of both robots,
our handover pose search space is greatly reduced. We
presented 3 simulated test cases in which our approach works
across diverse randomized situations.
While our method shows early indications of efficiency, it
should be noted that it is not complete; hence, the possibility
exists that our algorithm might not find a handover pose
even if one exists. As future work, we are transitioning our
simulation setup to our real hardware in order to validate our
preliminary simulation results with experimental tests.

Tg : Dog jar in pouring pose

Fig. 12: Start and goal configurations for kneeling test case

R EFERENCES
As in the cases mentioned before, the average results of
running our algorithm 100 times on this test case are shown
in Table IV. The randomized parameters for these cases were
the goal location of the dog food jar (position change of
no more than 3 cm and roll rotation changes of up to 20
degrees).

[1] Dynamic Animation and Robotics Toolkit. http://dartsim.
github.io/dart/. Accessed: 2014-07-21.
[2] H. Admoni, A. Dragan, S. Srinivasa, and B. Scassellati. Deliberate delays during robot-to-human handovers improve compliance with gaze
communication. In Proceedings of the 2014 ACM/IEEE international
conference on Human-robot interaction, pages 49–56. ACM, 2014.
[3] D. Berenson, S. Srinivasa, D. Ferguson, A. Collet, and J. Kuffner.
Manipulation planning with workspace goal regions. In Robotics and
Automation, 2009. ICRA’09. IEEE International Conference on, pages
618–624. IEEE, 2009.
[4] M. Cakmak, S. Srinivasa, M.K. Lee, J. Forlizzi, and S. Kiesler. Human
preferences for robot-human hand-over configurations. In Intelligent
Robots and Systems (IROS), 2011 IEEE/RSJ International Conference
on, pages 1986–1993. IEEE, 2011.
[5] A. Edsinger and C. Kemp. Human-robot interaction for cooperative
manipulation: Handing objects to one another. In Robot and Human
interactive Communication, 2007. RO-MAN 2007. The 16th IEEE
International Symposium on, pages 1167–1172. IEEE, 2007.
[6] D. Kee and W. Karwowski. Analytically derived three-dimensional
reach volumes based on multijoint movements. Human factors: the
journal of the human factors and ergonomics society, 44(4):530–544,
2002.
[7] A. Koene and M. Remazeilles. Relative importance of spatial and temporal precision for user satisfaction in human-robot object handover
interactions.
[8] J. Mainprice, M. Gharbi, T. Siméon, and R. Alami. Sharing effort in
planning human-robot handover tasks. In RO-MAN, 2012 IEEE, pages
764–770. IEEE, 2012.
[9] J. Mainprice, E. Sisbot, T. Siméon, and R. Alami. Planning safe
and legible hand-over motions for human-robot interaction. In IARP
Workshop on Technical Challenges for Dependable Robots in Human
Environments, volume 2, page 7, 2010.
[10] A. Miller and P.K. Allen. Graspit! a versatile simulator for robotic
grasping. Robotics & Automation Magazine, IEEE, 11(4):110–122,
2004.
[11] Y. Nakamura and H. Hanafusa. Inverse kinematic solutions with
singularity robustness for robot manipulator control. Journal of
dynamic systems, measurement, and control, 108(3):163–171, 1986.

Kneeling random test case 1

Kneeling random test case 2
Fig. 13: Sample solutions for random kneeling test cases
VI. D ISCUSSION AND C ONCLUSION
In this paper we have presented a planner to solve shared
manipulation tasks. Given an object that must be passed on
from one robot Rg to another Rr , our planner calculates
a set of possible handover solutions H and selects the one
436

Grasping for a Purpose: Using Task Goals for
Efficient Manipulation Planning

arXiv:1603.04338v1 [cs.RO] 14 Mar 2016

Ana Huamán Quispe

Heni Ben Amor

Henrik I. Christensen

M. Stilman†

Abstract—In this paper we propose an approach for efficient
grasp selection for manipulation tasks of unknown objects. Even
for simple tasks such as pick-and-place, a unique solution is rare
to occur. Rather, multiple candidate grasps must be considered
and (potentially) tested till a successful, kinematically feasible
path is found. To make this process efficient, the grasps should
be ordered such that those more likely to succeed are tested
first. We propose to use grasp manipulability as a metric to
prioritize grasps. We present results of simulation experiments
which demonstrate the usefulness of our metric. Additionally, we
present experiments with our physical robot performing simple
manipulation tasks with a small set of different household objects.

I. I NTRODUCTION
The ability to grasp objects in order to accomplish a task
is one of the hallmarks of human intelligence. Numerous
psychological studies show that humans grasp selection depends on the goal to be accomplished [14]. Decision making
during grasping is therefore not only based on stability during
manipulation, but also based on task requirements. If a specific
grasp does not facilitate the execution of the upcoming subtasks, it is omitted from the reasoning process.
In contrast to that, research on robot grasp synthesis has
been tilted towards optimizing stability metrics only. A prominent approach is to generate a set of physically stable grasps,
one of which is then selected by the high-level planner. If
a high-level task planner cannot achieve the goals of the
task, it has to back track and try a different grasp. Since no
information is flowing between high-level planning and lowerlevel grasp generation, a large number of grasps may have to
evaluated. If the required grasp is not within the optimized set
of candidates, the entire task will fail.
In this paper, we introduce a method for manipulation
planning which uses foresight to identify tasks constraints.
Constraints extracted from subsequent sub-tasks are used to
synthesize grasps that facilitate overall task completion. Our
goal is to derive a fast planning algorithm that can efficiently
generate manipulation sequences for previously unseen objects. These latter properties, hence, allow a robot to perform
manipulation tasks in new environments without resorting to
prior 3D models of the object or pre-calculated grasp sets.
This ability to generalize is realized by using a super-quadric
representation of objects. We show how super-quadrics can be
extracted from a single depth image and how they can be used
to generate a large set grasp candidates.
Institute for Robotics and Intelligent Machines, Georgia Institute of
Technology, Atlanta, GA 30332, USA. ahuaman3@gatech.edu,

hbenamor@cc.gatech.edu, hic@cc.gatech.edu

Fig. 1: Example of grasp selection based on goal constraints:
The goal location of the chips box is surrounded by nearby
objects, hence an overhead grasp must be selected for manipulation. If not for the obstacles present, a different grasp
(overlayed in red) from the side would be easier to execute.
Short planning times are realized by introducing a heuristics
that efficiently guides the search by incorporating arm kinematics. Inspired by the end-comfort effect [20] in humans,
grasps are preferred which lead to a comfortable arm configuration at the end of a task. We borrow ideas from this
work and propose to use a metric based on manipulability as
a measurement of end-comfort.
The contributions of this paper are threefold, namely (1)
a framework for online grasp planning that incorporates future task constraints into the grasp synthesis process, (2) an
efficient grasp generation approach based on super-quadrics
that works with previously unseen objects, (3) the end-comfort
heuristics for efficient search during manipulation planning.
The rest of this paper is organized as follows: Section
II present relevant work in the area of grasp synthesis and
grasp selection. Section IV presents our grasp generation
method using a primitive-based approach and in Section V
we introduce our manipulability-based strategies to prioritize
the generated grasps. Section VI shows the results of the
comparisons in simulation and the metrics we used to compare
their performance. Finally, we present the application of our
approach in our physical robot. We conclude this paper with
section VII, where we provide some discussion regarding
future work, and the advantages and shortcomings of our
approach.

Grasp Generation
at Goal Pose

Superquadric Fitting

Validation at
Start Pose

Prioritization

End
Yes

Found
Solution?

Plan Pick
and Place

Pop First
Grasp

No

Fig. 2: Manipulation planning pipeline: a partial point cloud of the object is first analyzed for symmetries and then turned
into a superquadric representation. Grasps at the goal and the start position are generated and then prioritized according to
end-comfort. Potential grasps are then analyzed within the plan and then executed.
II. R ELATED W ORK
In this section we review work concerning grasp synthesis
and grasp selection. For a more detailed review of previous
research in the area, we suggest the interested reader to consult
the excellent reviews from Bohg [3] and Sahbani [21].
Pioneering work on grasp selection was developed by
Cutkosky [5], who observed that humans select grasps in order
to satisfy 3 main types of constraints: Hand, object and taskbased constraints. As pointed out by Bohg et al. in [3], there
is little work on task-dependent grasping when compared to
work focused on the first two constraints. Hence, the main
goal for a planner is to find a grasp such that the robot can
approach the object and execute the said grasp, without further
regard of what the robot will do once the object is picked.
Grasp generation methods vary widely depending on the
assumptions considered. In the case of grasp planning for
known objects, Ciocarlie et al. [4] presented the concept
of eigengrasps, which was exploited to generate candidate
grasps searching in a low dimensional hand posture space
using their GraspIt! simulator. Diankov generated grasps by
sampling the surfaces of object meshes and using the normals
at the sample points to guide the approach direction of the
hand [6]. Approaches using primitive representations were
also proposed such that the grasp generation depends on
the particular primitive characterization: Miller et al. [15]
proposed to use a set of primitive shapes (cylinder, box, ball)
to decompose complex objects. Huebner and Kragic [12] used
bounding boxes, Przybylski et al. proposed the Medial Axis
representation [18], Goldfeder et al. [9] used superquadrics
due to their versatility to express different geometry types with
only 5 parameters.
In all the cases mentioned, the grasps are generated offline
and stored in a database for future use. These grasps are
usually ranked based on their force-closure properties, which
theoretically express the robustness and stability of a grasp.
One of the most popular metrics () was proposed by Ferrari
and Canny [7]. However, it has been noted by different
authors that analytical metrics do not guaranteee a stable
grasp when executed in a real robot. This can be explained

by the fact that these classical metrics consider assumptions
that don’t always hold true in real scenarios (dynamics,
perceptual and modelling inaccuracies, friction conditions). On
the other hand, studies that consider human heuristics to guide
grasp search have shown remarkable results, outperforming
classical approaches. In [1], Balasubramanian observed that
when humans kinestetically teach a robot how to grasp objects,
they strongly tend to align the robotic hand along one of
the object’s principal axis, which later results in more robust
grasps. The author termed skewness to the metric measuring
the axis deviation. In [19], Przybylski et al. combine the latter
metric with  and use it to rank grasps produced with GraspIt!.
Berenson et al. [2] proposed a score combining 3 measures: ,
object clearance and the robot relative position to the object.
In this work we are interested in manipulation of unknown
objects. Multiple approaches of this kind have flourished
during the last few years, particularly due to the advent of
affordable RGB-Depth sensors. Since the 3D information is
partial and noisy, classical approaches to grasp generation
cannot be directly used. Rather, most of the current work uses
heuristics to guide grasp generation based on local representation of the object geometry features (or global features if the
object shape is approximated). In [10], Hsiao et al. use the
bounding box of the object segmented pointcloud to calculate
grasp approach directions using a set of heuristics. We should
notice that for most of these approaches, their effectiveness
can only be verified empirically.
As we mentioned at the beginning of this section, the
metrics we discussed above do not consider the task to be
executed after the grasp is achieved. Some authors, however,
have investigated this issue at some level. In pioneering work,
[13] Li and Sastry proposed the concept of the task ellipsoid,
which maximizes the forces to be applied in the direction
of the task. More recently, Pandey et al. [16] proposed a
framework to select a grasp such that the object grasped can
be manipulated in a human-robot interaction scenario in which
the goal pose of the object is not entirely constrained.
Finally, although one of our main concerns is to select a
grasp that is suitable for the task to be executed, we also
consider important to use a grasp that allows for a simple, easy

Fig. 3: System setup and problem description.

arm execution. Interestingly, the problem of grasp planning is
usually considered isolated from arm planning. In some recent
work, Vahrenkamp et al. proposed Grasp-RRT [22] in order to
perform both grasp and arm planning combined. In a similar
vein, Roa et al. also proposed an approach that solve both
problems simultaneously [8]. Both approaches focus on reaching tasks. Our approach tackles pick-and-place tasks in which
reaching is only the first half of the solution (object placing
being the second). We make use of our proposed heuristics
to solve the complete pick-and-place problem in a manner as
efficient as possible by means of grasp prioritization.
III. P ROBLEM D EFINITION AND A SSUMPTIONS
Our problem description can be explained as follows: Given
a bimanual manipulator R and a simple object O, the manipulation task consists on transporting O from a given start pose
w
Ts to a final pose w Tg .
Figure 3 depicts the problem described. The following
constraints are considered:
• A 3D model of O is not available beforehand.
• A one-view pointcloud of the scenario is available from
the Kinect sensor mounted on top of the robot shoulders.
• Each limb of R consists of a 7-DOF arm (AL , AR ) and
a 3-fingered hand (HL , HR ). A semi-analytical IK solver
is available for AL and AR
In the following sections we will describe our basic approach for problems in which only the use of one arm is
required to solve the manipulation task described.
IV. G RASP G ENERATION FOR U NKNOWN O BJECTS
As our problem description stated, our approach must find
a plan such that O can be grasped at w Ts , transported and
finally repositioned at w Tg . Our approach consists on 4 main
steps, shown in Figure 2. The first 3 steps (object fitting, grasp
generation and grasp validation) will be explained in the rest
of this section.
A. Object Representation using Superquadrics
Requiring complete 3D models of objects before grasp
synthesis severely limits the application domains of robot
manipulation. Modern depth cameras partly solve the problem, since they allow the robot to estimate the surface of
an object. Yet, since the point clouds are acquired from a
specific perspective, they only hold partial shape information
about the visible frontal part. To fill any gaps and produce

a complete point cloud, multiple images can be acquired by
either iteratively moving the camera or the object. This process
is time-consuming and introduces new challenges such as the
precise matching of the individual point clouds of each view.
To solve this problem, we use a super-quadric representation
of objects and reason about symmetry in order to infer the
shape of any invisible part. Superquadrics are a family of
geometric shapes that can represent a wide range of diverse
objects using a limited set of parameters. Superquadrics can
be expressed with their implicit equation:
 2
  2
x 2  y  22 1  z  21
+
+
=1
a
b
c

(1)

In our approach, we generate a super-quadric representation
using a single depth image. Fitting of the parameters can
be performed online by minimizing the difference between
the model and the partial point cloud [11]. However, since
only one side of the object is visible, a standard approach to
fitting will result in erroneous approximations of the object. To
reproduce the entire shape from a partial point cloud, we added
an additional pre-processing step to the superquadric fitting
process. Instead of using the original point cloud as input, we
generate a mirrored version by finding an optimal symmetry
plane [11]. The goals of this step is to exploit symmetries to
infer invisible parts of an object.
The output of this process for a given object O consists on
a transformation w To in world coordinates and the parameters,
psq = { a, b, c, 1 , 2 }
defining its approximated geometry. A good number of
household objects can be easily described with generic shapes
such as boxes, cylinders and ovoids, for which we can further
bound the shape parameters considered:
1 ,2 ∈ [0.1, 1.9]
Figure 4 shows different geometric shapes corresponding to
superquadrics with different values for 1 and 2 .
The superquadric approach turns the pointcloud-based representation into a parametric representation, which can be
much more efficiently used during grasp synthesis. Calculations of principal-axes, normals and other features are much
faster and less susceptible to noise.
B. Generating Valid Candidates
Once the shape of O is approximated, we can proceed to
generate candidate grasps gi using a simulation of the robot,
and the object O, whose mesh is reconstructed by using the
superquadric parameters found in the previous section.
The candidates grasps must be kinematically feasible to
execute with O located at both start and goal conditions (w Ts
and w Tg ). Our approach accomplish this with Algorithm 1.
First, we set O at its goal pose w Tg and generate a set of
kinematically feasible grasps for it (G). Next, we set O at its
start pose w Ts . Finally, we test each of the grasps gi from G
in this scenario, discarding the grasps for which there exist

superquadrics equation. We use the method proposed by Pilu
and Fischer [17] to obtain an evenly-spaced set of points and
normals.
To define a grasp we calculate the transformation of the
hand H w.r.t. O(o Th ). We use the samples to generate this
transformation (lines 3 to 7 of Algorithm 2). After positioning
the TCP of the H at w Tp , we close the fingers. If there are
not collisions with the environment, we proceed to evaluate
if there exists at least an arm configuration that allows the
hand to execute the grasp. If so, then a corresponding grasp
is stored.

1.2

1.0

Fig. 4: Examples of superquadrics with different shapes. A
variety of shapes can be represented using a small number of
parameters.

Algorithm 2: GenerateGrasps(H, A, w To ,O,psq )
Input: H, A, w To , O, psq
Output: A feasible set of grasps G
/* psq = {1 ,2 ,a,b,c}

not a single IK solution. The surviving grasps in G are then
grasps that can be executed for the object O at both w Ts and
w
Tg .

1

S= sample_SQ(psq )

2

foreach (pi ,ni ) ∈ S do
/* p: TCP point in the hand H

3

o

Algorithm 1: get Valid Candidates
Input: H, A, w Ts , w Tg , O, psq
Output: Set of Candidate grasps G
1

3

5
6

wT

7

G ← generate_Grasps(H, A,w Tg ,O, psq )
set_Pose(O,w Ts )
/* Discard grasps invalid with O at

4
5
6

*/

g

wT
s

*/

foreach gi ∈ G do
if exist_IK_sol(gi , H, A) is false then
G.erase(gi )

9
10
11
12
13

*/

o

Tp .x = smallest_Axis(a,b,c)
o
Tp .y = o Th .z × o Th .x
w
Tp = w To ·o Tp
/* h: Origin of hand H

8

*/

Tp .z = -ni

/* x: Fingers closing direction

set_Pose(O,w Tg )
/* Generate grasps with O at

2

4

*/

Tp .trans = pi

/* z: Approach direction of H
o

*/

w

w

*/

p

Th = Tp · Th
setHand_Tcp(H, w Tp )
close_Hand(H)
if check_collision(H) is false then
if exist_IK_conf(H, A,w Th ) is true then
G ← Grasp(H, o Tp ·p Th )

7
8

return G

14
15

1) Grasp Generation at Goal Pose: The function
generate_Grasps, which we use to produce grasps exploiting the shape parameters of O is shown in Algorithm 2.
First, we uniformly sample the surface of O. This is easily
done by using the explicit equation defining the points in a
superquadric and their corresponding normals:
  

x
a cos1 η cos2 ω
π
π
y  =  b cos1 η sin2 ω  with 2 < η < 2
(2)
π<ω<π
z
c sin1 η


1
2−1
2−2
 
η cos
ω
 a cos
nx
1

2−
ny  =  cos2−1 η sin 2 ω 
(3)
b



nz
1
sin2−1 η
c
Sampling uniformly ω and η does not produce a uniform
sampling of surface points due to the high nonlinearity of the

return G

Algorithm 2 generates at most one grasp per each sampled
point. Optionally, we generated 2 additional possible grasps
per each point by rotating the hand an angle ±α around the
x axis of o Tp . We added this since we noticed that, when
executing the grasps on the physical robot, a slight inclination
usually made the grasp much easier to reach. In this paper we
used α = 30o . An example of the variated grasps generated
using α is shown in Figure 5.
2) Validation at Start Pose: Once a set of grasps feasible
to execute on O at w Tg is obtained, our algorithm discards
the grasps that cannot be executed with O at w Ts (lines 4 to
6).
V. G RASP P RIORITIZATION
Once a set of feasible grasps G is generated, paths for reaching and placing the object must be produced. A brute-force
approach would be to exhaustively try each grasp in a random

Fig. 5: Generating grasps varying the approach direction by
rotating the hand around the local x axis(pointing out the
page): Left: Original. Middle,right: After rotating by ±α
order until a solution is found. However, arm planning can be
a time-consuming process, particularly when using samplingbased methods. It is therefore desirable to first evaluate grasps
that are more likely to produce a solution. Since there is
likely more than one solution in G, it is preferable to choose
grasps such that the solution is quickly found. We propose
to use situated grasp manipulability as a metric to prioritize
the grasps and, hence, as a heuristic for guiding the search
process.
Manipulability(m) measures how dexterous the end-effector
of a robotic arm is at a given joint configuration q. Initially
proposed by Yoshikawa [23], m(q) is defined as:
q

(4)
m(q) = J(q)J T (q).
Manipulability is typically defined for a single joint configuration. In our scenario, we describe a situated grasp gi
for which multiple q might exist, due to redundancy. This
naturally leads to the definition of situated grasp manipulability(mg ). Given a target object O located at w To , and its
corresponding grasp gi , we define mg as the average manipulability of a uniform set of collision-free arm configurations
qi that allow executing gi :
mg =

N
1 X
m(qi )
N i=1

(5)

Please note that mg depends on both gi , w To and the
environment (for collisions) since only collision-free grasps
that reach the object are considered. Figure 6 shows an
example of a pick-and-place task wherein the green and red
markers indicate the w Ts and w Tg . In this case, mg at w Tg
is bigger than w Ts (where Ns = 76 and Ng = 108 are the
number of IK solutions for both situations). When the object
is at w Tg , the arm movement requires less effort.

Fig. 6: Examples of mg measured at w Ts and w Tg

From the shown example, it becomes evident that for a
pick-and-place manipulation problem, there are at least two
possible metrics to use per grasp gi : mg measured either at
w
Ts or w Tg . Choosing the first option means that we prioritize
grasps in which the pick phase is executed comfortably (w Ts ),
whereas by choosing the latter, we favor grasps in which the
arm configuration used at placing the object (w Tg ) is more
relaxed. In section VI, we present the results of experiments
comparing these two metrics and an additional control measure
to analyze their performance and choose which one is best
suited for our problem.
VI. E XPERIMENTS AND R ESULTS
In this section, we perform a set of experiments in simulation and on the real robot in order to evaluate the introduced
manipulation planning algorithm. The simulation experiments
are used to analyze the situated grasp manipulability using
a large number of trials. Experiments on the real robots are
performed to show the generation of manipulations based on
task goals and previously unknown objects. Generation of
superquadric object models was performed on the spot within
1 second.
A. Simulation Experiments
In this experiment, we consider three alternatives:
w
• Measure mg for grasp situated at Ts
w
• Measure mg for grasp situated at Tg
• Average of both measures above
We use 3 measures to compare the performance of the 3
evaluated metrics.
• First success: The main goal of the metrics evaluated is
to prioritize the grasps such that the first one tried is the
most likely to succeed. This metric indicates the number
of times that a solution is found by evaluating only the
grasp with the biggest value for the evaluated metric.
• Planning time: It measures the average planning time of
the successing grasps. The planning time is the total time
to plan a reach and transport path for the given grasp.
• Path length: It indicates the number of steps required
for the pick-and-place solution. The step length is a
normalized value in joint space, so this metric compare
the paths in configuration space.
The scenario we used is depicted in Figure 7. We fix the w Tg
to the middle of the table (red marker) and vary the start pose
w
Ts to 35 positions, each separated 0.1 m (green markers). We
devised 2 kind of experiments: In the first, w Ts and w Tg have
the same orientation, with only the position being changed (35
scenarios). In the second case, w Ts presents a rotation around
π
steps,
the Z axis w.r.t. w Tg in the interval [0, 2π] at each
4
so in total 35 × 8 = 280 scenarios are tested.
We used an standard IK-BiRRT to perform the arm planning. To account for its randomness, the results presented
in Table I and Table II are an average of 5 runs per each
experiment.
From the tables, we can observe that using mg evaluated at
w
Tg produces the best results in terms of success at the first

TABLE II: Evaluation with rotation change
Metric Type

Path Steps

Planning Time

Success

mg at

wT
s

100.7

4.70

255/278

mg at

wT
g

99.6

4.49

275/278

100.4

3.83

260/278

Avg. mg

Fig. 7: Setup for unimanual evaluation experiments
TABLE I: Evaluation with no rotation change
Metric Type
mg at

wT
s

mg at

wT
g

Path Steps

Avg. mg

Planning Time

Success

82.92

2.17

21.8/35

89.28

2.218

33/35

92.92

2.29

31.4/35

trial, whereas w Ts present the worst results. The average path
length for the general case of Table II is rather similar for
the 3 cases. Regarding planning times, the average mg gives
better results.
Given the results presented, we chose to use the mg at w Tg .
Its next best competitor (the avg. mg ) was not considered
since in order to calculate it, the mg at both w Ts and w Tg
must be calculated, which increases the computation time (for
the examples presented, the computation time of mg was 2
seconds). Given that the advantage of planning time is not
significant, we chose mg at w Tg .
B. Robot Experiments
Next, we perform a set of experiments on the real robot.
All performed experiments are pick-and-place tasks. However,
in some tasks we add environmental constraints at the goal
location which limit the range of applicable grasps. Figure 8
depcits two trials without any environmental constraint. The
robot has to pick an object at the starting location (green) and
move it successfully to the goal location (orange). The robot
has no prior knowledge of the object and needs to extract
shape information from a single depth image produced by a
depth sensor mounted in the head. As can be seen in the figure,
the grasp direction and the hand shape is adapted to suit the
object.
A different set of experiments can be seen in Figure 9.
Here, environmental constraints at the goal are introduced. In
the top row, the object has to be placed in a box. Accordingly,
the robot has to choose a grasp that allows it to place the
object in the box without colliding with it. Hence, the selected
grasps are mostly from above. The middle row show a different
scenario, in which the object has to be placed on a box which
is farther away. Choosing the wrong approach direction, e.g.
from above, would prevent the robot from successfully finishing the manipulation process, due to workspace limitations.
The bottom row shows normal runs without any environmental
constraints.

Fig. 8: Two examples of a pick-and-place without constraints.
The robot can identify suitable grasp for novel objects using
the superquadric representation.
VII. C ONCLUSION
In this paper, we introduced a new method for manipulation
planning with task constraints. Given a previously unknown
object and goals of the task, the method synthesizes online
a grasp that facilitates task completion. Planning and grasp
synthesis are effectively merged to efficiently produce manipulation sequences. Object acquisition, representation, grasp
synthesis and planning can be performed within a couple
of seconds, i.e., 2-5 seconds, for the presented examples.
We showed how superquadrics and a new heuristic, i.e., the
situated grasp manipulability can be used towards this end.
These properties, hence, allow a robot to perform successful,
goal-driven manipulation tasks in new environments without
resorting to prior 3D models of the object or pre-calculated
grasps.
While superquadrics can be efficiently calculated, they lack
accuracy when representing complex shapes and objects. In
this paper, we showed that many objects, in particular household objects can be represented by superquadrics. In our future
work, we want to explore extensions of this representation
that can model a larger set of objects. In particular, we want
to build upon our previous research on the identification of
rotational and linear extrusions [11] to represent more complex
shapes. In addition, we want to verify the introduced planning
approach on longer manipulation sequences.

Fig. 9: Final grasp configuration during manipulation with goal constraints (top and middle) and without goal constraints
(bottom).

R EFERENCES
[1] R. Balasubramanian, L. Xu, P. D Brook, J.R. Smith, and Y. Matsuoka.
Physical human interactive guidance: Identifying grasping principles
from human-planned grasps. In The Human Hand as an Inspiration
for Robot Hand Development. Springer, 2014.
[2] D. Berenson, R. Diankov, K. Nishiwaki, S. Kagami, and J. Kuffner.
Grasp planning in complex scenes. In 7th IEEE-RAS Int. Conf. on
Humanoid Robots, 2007.
[3] J. Bohg, A. Morales, T. Asfour, and D. Kragic. Data-driven grasp
synthesis: A survey. IEEE Transactions on Robotics, 2014.
[4] M. Ciocarlie, C. Goldfeder, and P. Allen. Dimensionality reduction for
hand-independent dexterous robotic grasping. In IEEE/RSJ Int. Conf.
on Intelligent Robots and Systems (IROS), pages 3270–3275, 2007.
[5] M.R. Cutkosky. On grasp choice, grasp models, and the design of
hands for manufacturing tasks. IEEE Transactions on Robotics and
Automation, 1989.
[6] R. Diankov and J. Kuffner. Openrave: A planning architecture for
autonomous robotics. Robotics Institute, Pittsburgh, PA, CMU-RI-TR08-34, 79, 2008.
[7] C. Ferrari and J. Canny. Planning optimal grasps. In IEEE Int. Conf.
on Robotics and Automation (ICRA), 1992.
[8] J. Fontanals, B.A. Dang-Vu, O. Porges, J. Rosell, and M. Roa. Integrated
grasp and motion planning using independent contact regions. 14th
IEEE-RAS Int. Conf. on Humanoid Robots (Humanoids), 2014.

[9] C. Goldfeder, P.K. Allen, C. Lackner, and R. Pelossof. Grasp planning
via decomposition trees. In IEEE Int. Conf. on Robotics and Automation
(ICRA), pages 4679–4684, 2007.
[10] K. Hsiao, S. Chitta, M. T. Ciocarlie, and E. G. Jones. Contact-reactive
grasping of objects with partial shape information. In IEEE/RSJ Int.
Conf. on Intelligent Robots and Systems (IROS), volume 99, page 124,
2010.
[11] A. Huamán Quispe, B. Milville, MA. Gutiérrez, C. Erdogan, M. Stilman, HI. Christensen, and H. Ben Amor. Exploiting symmetries and
extrusions for grasping household objects. IEEE Int. Conf. on Robotics
and Automation (ICRA)(to appear), 2015.
[12] K. Huebner and D. Kragic. Selection of robot pre-grasps using boxbased shape approximation. In IEEE/RSJ Int. Conf. on Intelligent Robots
and Systems (IROS), 2008.
[13] Z. Li and S.S. Sastry. Task-oriented optimal grasping by multifingered
robot hands. IEEE Journal of Robotics and Automation, 1988.
[14] J. Loucks and J. A. Sommerville. The role of motor experience in
understanding action function: The case of the precision grasp. Child
development, 83(3):801–809, 2012.
[15] A.T. Miller, S. Knoop, H.I. Christensen, and P.K. Allen. Automatic
grasp planning using shape primitives. In IEEE Int. Conf. on Robotics
and Automation,(ICRA), 2003.
[16] A.K. Pandey, J-P Saut, D. Sidobre, and R. Alami. Towards planning
human-robot interactive manipulation tasks: Task dependent and human

[17]
[18]
[19]
[20]
[21]
[22]
[23]

oriented autonomous selection of grasp and placement. In 4th IEEE
RAS & EMBS Int. Conf. on Biomedical Robotics and Biomechatronics
(BioRob), 2012.
M. Pilu and R.B. Fisher. Equal-distance sampling of superellipse
models. University of Edinburgh, Department of Artificial Intelligence,
1995.
M. Przybylski, T. Asfour, and R. Dillmann. Unions of balls for shape
approximation in robot grasping. In IEEE/RSJ Int. Conf. on Intelligent
Robots and Systems (IROS), 2010.
M. Przybylski, T. Asfour, R. Dillmann, R. Gilster, and H. Deubel.
Human-inspired selection of grasp hypotheses for execution on a humanoid robot. In 11th IEEE-RAS Int. Conf. on Humanoid Robots, 2011.
D. A. Rosenbaum, C. M. van Heugten, and G. E. Caldwell. From
cognition to biomechanics and back: The end-state comfort effect and
the middle-is-faster effect. Acta psychologica, 94(1):59–85, 1996.
A. Sahbani, S. El-Khoury, and P. Bidaud. An overview of 3d object grasp
synthesis algorithms. Robotics and Autonomous Systems, 60(3):326–336,
2012.
N. Vahrenkamp, T. Asfour, and R. Dillmann. Simultaneous grasp and
motion planning: Humanoid robot armar-iii. Robotics & Automation
Magazine, 2012.
T. Yoshikawa. Manipulability of robotic mechanisms. The International
Journal of Robotics Research, 4(2):3–9, 1985.

arXiv:1507.07882v1 [cs.CV] 27 Jul 2015

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

1

Occlusion-Aware Object Localization,
Segmentation and Pose Estimation
Samarth Brahmbhatt
http://www.cc.gatech.edu/~sbrahmbh

Heni Ben Amor

School of Interactive Computing
Georgia Institute of Technology
Atlanta, GA USA

http://henibenamor.weebly.com

Henrik Christensen
http://www.cc.gatech.edu/~hic

1

Abstract
We present a learning approach for localization and segmentation of objects in an
image in a manner that is robust to partial occlusion. Our algorithm produces a bounding
box around the full extent of the object and labels pixels in the interior that belong to the
object. Like existing segmentation aware detection approaches, we learn an appearance
model of the object and consider regions that do not fit this model as potential occlusions.
However, in addition to the established use of pairwise potentials for encouraging local
consistency, we use higher order potentials which capture information at the level of image segments. We also propose an efficient loss function that targets both localization and
segmentation performance. Our algorithm achieves 13.52% segmentation error and 0.81
area under the false-positive per image vs. recall curve on average over the challenging
CMU Kitchen Occlusion Dataset. This is a 42.44% decrease in segmentation error and
a 16.13% increase in localization performance compared to the state-of-the-art. Finally,
we show that the visibility labelling produced by our algorithm can make full 3D pose
estimation from a single image robust to occlusion.

Introduction and Related Work

In this paper we address the problem of localizing and segmenting partially occluded objects.
We do this by generating a bounding box around the full extent of the objects, while also segmenting the visible parts inside the box. This is different from semantic segmentation, which
typically does not provide information about the spatial position of labelled pixels inside the
object. While a lot of progress has been made in object detection [9, 13, 21], occlusion by
other objects still remains a challenge. A common theme is to model occlusion geometrically
or appearance-wise, thereby allowing it to contribute to the detection process. Wang et al.
[19] use a holistic Histogram of Oriented Gradients (HOG) template [6] to scan through the
image and use specially trained part templates for instances where some cells of the holistic template respond poorly. Girshick et al. [11] force the Deformable Part Model detector to
place a trained ‘occluder’ part in regions where the original parts respond weakly. The object
masks produced by both of these algorithms are only accurate up to the parts and hence not
usable for many applications e.g. edge-based 3D pose estimation. Xiang and Savarese [20]
c 2015. The copyright of this document resides with its authors.

It may be distributed unchanged freely in print or electronic forms.

2

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

approximate object structure in 3D using planar parts. A Conditional Random Field (CRF)
is then used to reason about visibility of the parts when the 3D planes are projected to the
image. However, such methods work well only for large objects that can be approximated
with planar parts.
Our approach is entitled Segmentation and Detection using Higher-Order Potentials (SDHOP). It is based on discriminatively learned HOG templates for objects and occlusion.
Whereas the object templates model the objects of interest, the occlusion templates provide
discriminative support and do not model a specific occluder. Segmentation is done by considering not only the response of patches to these templates, but also the segmentation of
neighbouring patches through a CRF with higher-order connections that encompass image
regions.
We will compare our approach to two existing approaches that have been designed to
handle partial occlusion. Hsiao and Hebert [14] approximate all occluders by boxes and generate occlusion hypotheses by finding locations of mismatch between image gradient and
object model gradient. These hypotheses are then validated by the visibility of other points
of the object and by an occlusion prior which assumes all objects rest on the same planar
surface. Our algorithm does not need such assumptions which reduce the segmentation accuracy. Gao et al. [10] learn discriminative appearance models of the object and occlusion
seen during training. Segmentation is achieved by defining a CRF to assign binary labels
to patches based on their response to these two filters. We build on their work but add several important modifications that lead to better localization and segmentation performance.
Firstly, we replace the edge-based pairwise terms with 4-connected pairwise terms that are
better able to propagate visibility relations. Secondly, we introduce the use of higher-order
potentials defined over groups of patches, allowing us to reason at the level of image segments which contain much more information than pairs of patches. We also introduce a
new loss function for structured learning that targets both localization and segmentation performance but is still decomposable over the energy terms. Lastly, we introduce a simple
procedure to convert the granular patch-level object mask produced by the algorithm to a
fine pixel-level mask that can be used to make 3D pose estimation of detected objects robust to partial occlusion. Our algorithm outperforms these approaches (Hsiao and Hebert
[14], Gao et al. [10]) at both object localization and segmentation on the CMU Kitchen
Occlusion dataset as shown in Section 3.
The rest of the paper is organized as follows. Section 2 describes our proposed approach.
We present evaluations on standard datasets and our own laboratory dataset in Section 3 and
summarize in Section 4.

2

Method

The training phase for SD-HOP requires a set of images with different occlusions of the
object(s) of interest. Each training sample is (1) over-segmented and (2) annotated with a
bounding box around the full extent of the object and a binary segmentation of the area
inside the box into object vs. non-object pixels. Given these training images and labels,
we train a structured Support Vector Machine (SVM) that produces the HOG templates and
CRF weights. Figure 1 shows an overview of our approach.
Object segmentation is done by assigning binary labels to all HOG cells within the
bounding box, 1 for visible and 0 for occluded. Instead of making independent decisions
for every cell, we allow neighbouring cells to influence each other. Neighbour influence can

3

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION
Training

Learning

Object 1

Object 2

Labels

Solve Constrained
QP
Feature Extraction

Feature Vector
Find Most
Violated Constraint

Segmentation Pyramid
Testing

Model

HOG Feature Pyramid

Inference

Figure 1: Overview of our approach. Top: During training, images are segmented and features are extracted from pyramids of segmentations and HOG features. An SVM model is
learned by max-margin learning. Bottom: After training, the model can be used to infer a
bounding box and visible segments of the object.
take two forms: (1) pairwise terms (Rother et al. [17]) that impose a cost for 4-connected
neighbours to have different labels and (2) higher-order potentials (Kohli et al. [16]) that impose a cost for cells to have a different label than the dominant label in their segment of the
image. These segments are produced separately by an unsupervised segmentation algorithm.

2.1

Notation

The label for an object in an image x is represented as y = (p, v, a), where p is the bounding
box, v is a vector of binary variables indicating the visibility of HOG cells within p and
a ∈ [1, A] indexes the discrete viewpoint. p = (px , py , pσ ) indicates the position of the top
left corner and the level in a scale-space pyramid. The width and height of the box are
fixed per viewpoint as wa and ha HOG cells respectively. Hence v has wa · ha elements.
All training images are also over-segmented to collect statistics for higher-order potentials.
Any unsupervised algorithm can be used for this, e.g. Felzenszwalb and Huttenlocher [8]
and Arbelaez et al. [2].

2.2

Feature Extraction

Given an image x and a labelling y, a sparse joint feature vector Ψ(x, y) is formed by stacking
A vectors. Each of these vectors has features for a different discretized viewpoint. All vectors
except for the one corresponding to viewpoint a are zeroed out. Below, we describe the
components of this vector.
1. 31-dimensional HOG features are extracted for all cells of 8x8 pixels in p as described
in Felzenszwalb et al. [9]. The feature vector is is constructed by stacking two groups
which are formed by zeroing out different parts, similarly to Vedaldi and Zisserman
[18]. The visible group φv (x, p) has the HOG features zeroed out for cells labelled 0
and the occlusion group φnv (x, p) has them zeroed out for cells labelled 1.
2. Complemented visibility labels, to learn a prior for a cell to be labelled 0: [1wh − v].

4

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

3. Count c(p) of cells in bounding box p lying outside the image boundaries, to learn a
cost for truncation by the image boundary, similarly to Vedaldi and Zisserman [18].
4. Number of 4-connected neighbouring cells in the bounding box that have different
labels, to learn a pairwise cost.
5. Each segment in the bounding box obtained from unsupervised segmentation defines
a clique of cells. To learn higher-order potentials, we need a vector θHOP that captures the distribution of 0/1 label agreement within cliques. A vector θc ∈ RK+1 is
constructed for each clique c as (θc )k = 1 if ∑i∈c vi = k. The sum of all θc within p
gives θHOP . In practice, since cliques do not have the same size we employ the normalization strategy described in Gould [12] and transform statistics of all cliques to a
standard clique size K (K = 4 in our experiments).
6. The constant 1, used to learn a bias term for different viewpoints.

2.3

Learning

Suppose w is a vector of weights for elements of the joint feature vector. We define wT Ψ(x, y)
as the ‘energy’ of the labelling y. The aim of learning is to find w such that the energy of the
correct label is minimum. Hence we define the label predicted by the algorithm as
f (x; w) = y∗ = argmin wT Ψ(x, y)

(1)

y

We use a labelled dataset (xi , yi )Ni=1 and learn w by solving the following constrained Quadratic
Program (QP)
N
1
min kwk2 +C ∑ ξi
(2)
w,ξ 2
i=1
s.t.

wT (Ψ(xi , ŷi ) − Ψ(xi , yi )) + ξi ≥ ∆(yi , ŷi ) ∀i, ŷ ∈ Yi
ξi ≥ 0 ∀i
D2 w ≥ 0

Intuitively this formulation requires that the score wT Ψ(xi , yi ) of any ground truth labelled
image xi must be smaller than the score wT Ψ(xi , ŷi ) of any other labelling ŷi by the distance
between the two labellings ∆(yi , ŷi ) minus the slack variable ξi , where kwk2 and ξi are
minimized. The regularization constant C adjusts the importance of minimizing the slack
variables. The above formulation has exponential constraints for each training image. For
tractability, training is performed by using the cutting plane training algorithm of Joachims
et al. [15] which maintains a working set Yi of most violated constraints (MVCs) for each
image. Gould [12] adapts this algorithm for training higher-order potentials. It uses D2 as a
second order curvature constraint on the K + 1 weights for the higher-order potentials, which
forces them to make a concave lower envelope. This encourages most cells in the image
segments to agree in visibility labelling. D2 is an appropriately 0-padded (to the left and
right) version of


−1 2
1 0 ...
 ..

..
.
..
 .
.
. ..
.
0

. . . −1

2

−1

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

5

The distance between two labels y and ŷ is called the loss function. It depends on the amount
of overlap between the two bounding boxes and the Hamming distance between the visibility
labellings

T
T 
area(p p̂)
area(p p̂)
S
S · H(v, v̂)
+
(3)
∆(y, ŷ) = 1 −
area(p p̂)
area(p p̂)
The mean Hamming distance H(v, v̂) between two labellings v and v̂ (potentially having different sizes as they might belong to different viewpoints) is calculated after projecting them
to the lowest level of the pyramid. By construction of the loss function, the difference in segmentation starts contributing to the loss only after the two bounding boxes start overlapping
each other. It also has the nice property of decomposing over the energy terms, as described
in Section 2.4.1.

2.4

Inference

To perform the inference as described in Eq. 1 we have to search through Y = A × P × V
where A is the set of viewpoints, P is the set of all pyramid locations and V is the exponential
set of all combinations of visibility variables. We enumerate over A and P and use an s − t
mincut to search over V at every location.
By construction, the feature vector w can be decomposed into weight vectors for the
different viewpoints i.e. w = [w1 , w2 , . . . , wA ]. In the following description, we will consider
one viewpoint and omit the superscript for brevity of notation. w can also be decomposed
as [wv , wnv , w pr , wtrunc ,W, wHOP , wbias ] into the six components described in Section 2.2. We
define the following terms that are used to construct the graph shown in Figure 2(b). φi (x, p)
are the vectorized HOG features extracted at cell i in bounding box p. Unary terms Fi (p) =
wv,i T φi (x, p) and Bi (p) = wnv,i T φi (x, p) are the responses at cell i for object and occlusion
filters respectively. Ri = w pr,i is the prior for cell i to be labelled 0. Constant term C(y) =
wtrunc · c(p) + wbias is the sum of image boundary truncation cost and bias. E is the set of
4-connected neighbouring cells in p and W is the pairwise weight. C(p) is the set of all
cliques in p and ψc (vc ) is the higher-order potential for clique c having nodes with visibility
labels vc . Combining these terms, the energy for a particular labelling is formulated as
wh

E(x, y) = wT Ψ(x, y) = ∑ Fi (p)vi + Bi (p)(1 − vi ) + Ri (1 − vi )
i=1

+

∑
(i, j)∈E

(4)
W |vi − v j | +

∑

ψc (vc ) +C(y)

c∈C (p)

ψc (vc ), the higher-order potential for clique c is defined as mink=1...K (sk ∑i∈c vi + bk ), following Gould [12]. Intuitively, it is the lower envelope of a set of lines whose slope is defined
as sk = M
K ((wHOP )k − (wHOP )k−1 ) and intercept as bk = (wHOP )k − sk k (recall that wHOP is a
K + 1 dimensional weight vector). M is the size of the clique. The normalization in sk makes
the potential invariant to the size of the clique (refer to Gould [12] for details). Figure 2(a)
shows a sample higher-order potential curve for a clique of K cells.
Given an image, a location, and a viewpoint we use s − t mincut on the graph construction shown in Figure 2(b) to find the labelling v that minimizes the energy in Eq. 4. Each
variable vi , i ∈ {1, 2, . . . , wh} defines a node and each clique has K − 1 auxiliary nodes in
the graph, z1 . . . zK−1 . For a detailed derivation of this graph structure please see Boykov
and Kolmogorov [3] and Gould [12]. After the maxflow algorithm finishes, the nodes vi still
connected to s are labelled 1 and others are labelled 0.

6

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

(a)

(b)

Figure 2: (a): Concave higher-order potentials encouraging cells in a clique to have the same
binary label, (b): Construction of graph to compute the energy minimizing binary labelling
of cells by s − t mincut.
Algorithm 1 Response-transfer between object detectors in overlapping regions
for all o ∈ [1, L] do {L is the number of objects, ◦ denotes the Hadamard product}
for all p ∈ P do
B(p) = C(p) ◦ 1[V(p) 6= 0] {Transfer equation for all cells in p}
end for
y∗ = argminy wT Ψ(x, y)
C(y∗ (p)) = F(y∗ (p)) ◦ y∗ (v) {Update equations for all cells in p}
V(y∗ (p)) = o · y∗ (v)
end for

2.4.1

Loss-augmented Inference

Loss-augmented inference is an important part of the cutting plane training algorithm (‘separation oracle’ in Joachims et al. [15]) and is used to find the most violated constraints. It
is defined as yMVC = argminŷ wT Ψ(x, ŷ) − ∆(y, ŷ), where y is the ground-truth labelling.
Our formulation of the loss function makes it solvable with the same complexity as normal
inference (Eq. 1) by decomposing the loss over the terms in Eq 4. The first term of Eq. 3 is
added to C(y), while the second term is distributed across Fi (p) and Bi (p) in Eq. 4.

2.5

Detection of Multiple Objects

Multiple objects of interest might overlap. Running the individual object detectors separately
leaves regions of ambiguity in overlapping areas if multiple detectors mark the same location
as visible. We find that running one iteration of α-expansion (see Boykov et al. [4]) in
overlapping areas resolves ambiguities coherently. The detectors are run sequentially. We
maintain a label map V that stores for each cell the label of the object that last marked it
visible, and a collected response map C that stores for each cell the object filter response
(Fi (p)) from the object that last marked it visible. While running the location search for
object o, we transfer object filter responses from C to the occlusion filter response map
(B(p)) for the current object as described in Algorithm 1. This is effectively one iteration

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

7

of α-expansion (see supplementary material for details). It causes decisions in overlapping
regions to be made between responses of well-defined object filters rather than between
responses of an object filter and a generic occlusion filter.
Such response-transfer requires the object models to be compatible with each other. We
achieve this by training the object models together as if they were different viewpoint components of the same object. The bias term in the feature vector makes the filter responses of
different components comparable.

2.6

3D Pose Estimation

The basic principle of many model based 3D pose estimation algorithms is to fit a given
3D model of the object to its corresponding edges in the image e.g. in Choi and Christensen [5], the 3D CAD model is projected into the image and correspondences between
the projected model edges and image edges are set up. The pose is estimated by solving
an Iterative Re-weighted Least Squares (IRLS) problem. However, partial occlusion causes
these approaches to fail by introducing new edges. We make the algorithm robust to partial
occlusion by first identifying visible pixels of the object using SD-HOP and discarding correspondences outside the visibility mask. We call our extension of the algorithm Occlusion
Reasoning-IRLS (OR-IRLS).

3

Evaluation

We implemented SD-HOP in Matlab, with MVC search and inference implemented in CUDA
since they are massively parallel problems. Inference on a 640x480 image with 11 scales
takes 3s for a single object with a single viewpoint on our 3.4 GHz CPU and NVIDIA GT730 GPU.

3.1

Localization and Segmentation

We evaluated our approach on the CMU Kitchen Occlusion Dataset from Hsiao and Hebert
[14]. This dataset was chosen because (1) it provides extensive labelled training data in the
form of images with bounding boxes and object masks, and (2) the dataset is challenging
and offers the opportunity to compare against an algorithm designed specifically to handle
occlusion. For the localization task we generated false positives per image (FPPI) vs. recall
curves, while for the segmentation task we measured the mean segmentation error against
ground truth as defined by the Pascal VOC segmentation challenge in Everingham et al. [7].
C = 25 (see eq. 2) was chosen by 5-fold cross-validation. While both results are presented
for the single pose part of the dataset, multiple poses are easily handled in our algorithm as
different components of the feature vector. Figure 3 shows FPPI vs. recall curves compared
with those reported by the rLINE2d+OCLP algorithm of Hsiao and Hebert [14] and those
generated from our implementation of Gao et al. [10]. Table 1 presents segmentation errors
compared with Gao et al. [10]. Hsiao and Hebert [14] do not report a segmentation of the
object.
Figure 3 shows that while both SD-HOP and Gao et al. [10] have similar recall at 1.0
FPPI, SD-HOP consistently preforms better in terms of area under the curve (AUC). Averaged over the 8 objects, SD-HOP achieves 16.13% more AUC than Gao et al. [10]. Table 1

8

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION
Colander

Pitcher

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.2

rLINE2D+OCLP
SD-HOP
Gao et al.

0
0.2

0.4

0.6

0.8

1

0.4

0.2

rLINE2D+OCLP
SD-HOP
Gao et al.

0
0

Recall

1

Recall

1

0.2

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

0

Cup

Scissors

0.8

0.8

0.6

0.6

0.6

0.2

0
0.2

0.4

0.6

0.8

1

FPPI

0.4

0.2

rLINE2D+OCLP
SD-HOP
Gao et al.

0
0

Recall

0.8

0.6

Recall

0.8

Recall

1

0.4

0.2

0.4

0.6

0.8

1

FPPI

0.8

1

0.4

0.2

rLINE2D+OCLP
SD-HOP
Gao et al.

0
0

0.6

Shaker

1

rLINE2D+OCLP
SD-HOP
Gao et al.

0.4
FPPI

1

0.2

0.2

FPPI

1

0.4

rLINE2D+OCLP
SD-HOP
Gao et al.

0

FPPI

Saucepan

0.4

0.2

rLINE2D+OCLP
SD-HOP
Gao et al.

0

FPPI

Recall

Thermos

1

Recall

Recall

Bakingpan
1

rLINE2D+OCLP
SD-HOP
Gao et al.

0
0

0.2

0.4

0.6

0.8

1

0

0.2

FPPI

0.4

0.6

0.8

1

FPPI

Figure 3: Object localization results on the CMU Kitchen Occlusion dataset
Table 1: Mean object segmentation error
Object
Gao et al. [10] SD-HOP

Table 2: Mean 3D pose estimation error
Pose parameter IRLS OR-IRLS

Bakingpan

0.2904

0.1516

Colander

0.2095

0.1249

X (cm)

1.6874

0.5774

Cup

0.2144

0.1430

Y (cm)

1.4953

0.6516

Pitcher

0.2499

0.1131

Z (cm)

8.228

2.1506

Saucepan

0.1956

0.1103

Roll (degrees)

1.1711

0.7152

Scissors

0.2391

0.1649

Pitch (degrees)

7.9100

2.3191

Shaker

0.2654

0.1453

Yaw (degrees)

5.7712

2.6055

Thermos

0.2271

0.1285

shows that SD-HOP consistently outperforms Gao et al. [10] in terms of segmentation error, achieving 42.44% less segmentation error averaged over the 8 objects. Figure 5 shows
examples of the algorithm’s output on various images from the CMU Kitchen Occlusion
dataset.

3.2

Ablation Study

We conducted an ablation study on the ‘pitcher’ object of the CMU Kitchen Occlusion
dataset to determine the individual effect of our contributions. Using the loss function
from Gao et al. [10] caused the segmentation error to increase from 0.1131 to 0.1547 and
area under curve (AUC) of FPPI vs. recall to drop from 0.7877 to 0.7071. To discern the
effect of 4-connected pairwise terms we removed the higher order terms from the model too.
Using the pairwise terms as described in Gao et al. [10] caused the segmentation error to
increase from 0.1547 to 0.2499 and AUC to decrease from 0.7071 to 0.6414.
Lastly, to quantify the effect of higher order potentials, we compared the full SD-HOP
model against one with higher order potentials removed. Removing higher order potentials

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

9

Figure 4: 3D pose estimation. Left to right: Pose estimation with IRLS, SD-HOP raw
segmentation mask, SD-HOP refined segmentation mask, Pose estimation with OR-IRLS.
Best viewed in colour.
caused the segmentation error to increase from 0.1131 to 0.1430 and AUC to drop from
0.7877 to 0.7544. We hypothesize that for small objects like the ones in the CMU Kitchen
Occlusion dataset, 4-connected pairwise terms are almost as informative as higher order
terms. To check this hypothesis we tested the effect of removing higher order potentials on
a close-up dataset of 41 images of a pasta-box occluded by various amounts through various
household objects. Removing the higher order potentials caused the segmentation error to
increase from 0.1308 to 0.1516 and area under curve AUC to drop from 0.9546 to 0.9008.
This indicates that higher order terms are more useful for objects with larger and hence more
informative segments.

3.3

3D Pose Estimation

We collected 3D pose estimation results produced by IRLS and OR-IRLS on a dataset which
has 17 images of a car-door in an indoor environment. The ground truth pose for the cardoor
was obtained by an ALVAR marker alv [1]. Table 2 shows the mean errors in the six pose
parameters. To discern the effect of errors inherent in the pose estimation process from the
effect of occlusion reasoning, the pose of the cardoor was constant throughout the dataset,
with various partial occlusions being introduced.
The granular HOG cell-level mask produced by SD-HOP caused some important silhouette edges to be missed for pose estimation. To solve this problem we utilized the unsupervised segmentation done earlier for defining higher order terms. If more than 80% of the
area within a segment was marked 1, we marked the whole segment with 1. Since segments
follow object boundaries, this produced much cleaner masks for pose estimation. Figure 4
shows the masks and pose estimation results for an example image from the dataset, with
more such examples presented in the supplementary material. Note that the segmentation
errors mentioned in Table 1 use the raw masks.

4

Conclusion

We presented an algorithm (SD-HOP) that localizes partially occluded objects robustly and
segments their visible regions accurately. In contrast to previous approaches that model occlusion, our algorithm uses higher order potentials to reason at the level of image segments
and employs a loss function that targets both localization and segmentation performance. We
demonstrated that our algorithm outperforms existing approaches on both tasks, when evaluated on a challenging dataset. Finally, we have shown that the segmentation output from

10

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

Figure 5: Object localization and segmentation results on the CMU Kitchen Occlusion
dataset. Left: Image, Center: Raw mask from SD-HOP, Right: Refined mask from SD-HOP
SD-HOP can be used to improve pose estimation performance in the presence of occlusion.
Avenues of future research include (1) training from weakly labelled data i.e. without segmentations, (2) a post-training algorithm to make object models comparable without having
to train them together, and (3) using the occlusion information to reason about interactions
between objects in scene understanding applications.
We would like to acknowledge Ana Huamán Quispe’s help with implementing this system on a bimanual robot. The system was used to enable the robot to pick up partially visible
objects lying on a table.

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

11

References
[1] ALVAR tracking library.
http://virtual.vtt.fi/virtual/proj2/
multimedia/alvar/index.html. Acessed: 2015-05-03.
[2] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 33(5):898–916, 2011. URL http://ieeexplore.ieee.
org/xpl/login.jsp?tp=&arnumber=5557884.
[3] Yuri Boykov and Vladimir Kolmogorov. An experimental comparison of mincut/max-flow algorithms for energy minimization in vision. Pattern Analysis and
Machine Intelligence, IEEE Transactions on, 26(9):1124–1137, 2004. URL http:
//ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1316848.
[4] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization
via graph cuts. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23
(11):1222–1239, 2001. URL http://ieeexplore.ieee.org/xpl/login.
jsp?tp=&arnumber=969114.
[5] Changhyun Choi and Henrik I Christensen. Robust 3D visual tracking using particle filtering on the special Euclidean group: A combined approach of keypoint
and edge features. The International Journal of Robotics Research, 31(4):498–519,
2012. URL http://ijr.sagepub.com/content/early/2012/03/01/
0278364912437213.
[6] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886–893. IEEE, 2005. URL http:
//ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1467360.
[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal
Visual Object Classes (VOC) Challenge. International Journal of Computer Vision, 88
(2):303–338, June 2010. URL http://link.springer.com/article/10.
1007%2Fs11263-009-0275-4.
[8] Pedro F Felzenszwalb and Daniel P Huttenlocher.
Efficient graph-based image segmentation.
International Journal of Computer Vision, 59(2):167–
181, 2004. URL http://link.springer.com/article/10.1023%2FB%
3AVISI.0000022288.19776.77.
[9] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. Pattern Analysis and
Machine Intelligence, IEEE Transactions on, 32(9):1627–1645, 2010. URL http:
//ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5255236.
[10] Tianshi Gao, Benjamin Packer, and Daphne Koller. A segmentation-aware object detection model with occlusion handling. In Computer Vision and Pattern Recognition
(CVPR), 2011 IEEE Conference on, pages 1361–1368. IEEE, 2011. URL http:
//ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995623.

12

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

[11] Ross B Girshick, Pedro F Felzenszwalb, and David A Mcallester. Object detection with
grammar models. In Advances in Neural Information Processing Systems, pages 442–
450, 2011. URL http://citeseerx.ist.psu.edu/viewdoc/summary?
doi=10.1.1.231.2429.
[12] Stephen Gould. Max-margin learning for lower linear envelope potentials in binary
markov random fields. In Proceedings of the 28th International Conference on Machine
Learning (ICML-11), pages 193–200, 2011. URL http://ieeexplore.ieee.
org/xpl/articleDetails.jsp?arnumber=6945904.
[13] Stefan Hinterstoisser, Cedric Cagniart, Slobodan Ilic, Peter Sturm, Nassir Navab, Pascal Fua, and Vincent Lepetit. Gradient response maps for real-time detection of textureless objects. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34
(5):876–888, 2012. URL http://ieeexplore.ieee.org/xpls/abs_all.
jsp?arnumber=6042881.
[14] Edward Hsiao and Martial Hebert. Occlusion reasoning for object detection under arbitrary viewpoint. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3146–3153. IEEE, 2012. URL http://ieeexplore.ieee.
org/xpl/login.jsp?tp=&arnumber=6248048.
[15] Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu. Cutting-plane training
of structural SVMs. Machine Learning, 77(1):27–59, 2009. URL http://link.
springer.com/article/10.1007%2Fs10994-009-5108-8.
[16] Pushmeet Kohli, Philip HS Torr, et al. Robust higher order potentials for enforcing label consistency. International Journal of Computer Vision, 82(3):302–
324, 2009. URL http://ieeexplore.ieee.org/xpl/login.jsp?tp=
&arnumber=4587417.
[17] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. Grabcut: Interactive
foreground extraction using iterated graph cuts. In ACM Transactions on Graphics
(TOG), volume 23, pages 309–314. ACM, 2004. URL http://dl.acm.org/
citation.cfm?id=1015720.
[18] Andrea Vedaldi and Andrew Zisserman. Structured output regression for detection
with partial truncation. In Advances in neural information processing systems, pages
1928–1936, 2009.
[19] Xiaoyu Wang, Tony X Han, and Shuicheng Yan. An HOG-LBP human detector with
partial occlusion handling. In Computer Vision, 2009 IEEE 12th International Conference on, pages 32–39. IEEE, 2009. URL http://ieeexplore.ieee.org/
xpl/login.jsp?tp=&arnumber=5459207.
[20] Yu Xiang and Silvio Savarese. Object Detection by 3D Aspectlets and Occlusion Reasoning. In Computer Vision Workshops (ICCVW), 2013 IEEE International Conference
on, pages 530–537. IEEE, 2013. URL http://ieeexplore.ieee.org/xpl/
articleDetails.jsp?reload=true&arnumber=6755942.
[21] Menglong Zhu, Konstantinos G Derpanis, Yinfei Yang, Samarth Brahmbhatt, Mabel Zhang, Cody Phillips, Matthieu Lecce, and Kostas Daniilidis. Single image 3D object detection and pose estimation for grasping.
In Robotics and

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

13

Automation (ICRA), 2014 IEEE International Conference on, pages 3936–3943.
IEEE, 2014. URL http://www.cis.upenn.edu/~menglong/papers/
icra2014_object_grasping.pdf.

2016 IEEE-RAS 16th International Conference on
Humanoid Robots (Humanoids)
Cancun, Mexico, Nov 15-17, 2016

Learning Human-Robot Interactions from Human-Human
Demonstrations
(with Applications in Lego Rocket Assembly)
David Vogt1 , Simon Stepputtis2 , Richard Weinhold2 , Bernhard Jung1 , Heni Ben Amor3
Abstract— This video demonstrates a novel imitation learning
approach for learning human-robot interactions from humanhuman demonstrations. During training, the movements of two
human interaction partners are recorded via motion capture.
From this, an interaction model is learned that inherently
captures important spatial relationships as well as temporal
synchrony of body movements between the two interacting
partners. The interaction model is based on interaction meshes
that were ﬁrst adopted by the computer graphics community
for the ofﬂine animation of interacting virtual characters. We
developed a variant of interaction meshes that is suitable for
real-time human-robot interaction scenarios. During humanrobot collaboration, the learned interaction model allows for
adequate spatio-temporal adaptation of the robots behavior
to the movements of the human cooperation partner. Thus,
the presented approach is well suited for collaborative tasks
requiring continuous body movement coordination of a human
and a robot. The feasibility of the approach is demonstrated
with the example of a cooperative Lego rocket assembly task.

Fig. 1. From motion captured human-human demonstrations an interaction
model of a cooperative task is learned. Using the learned model in a humanrobot collaboration, the robot’s behavior is continuously synchronized, both
spatially and temporally, with the actions of the human.

spatio-temporal generalization can be achieved through the
combination of motion recognition in low-dimensional posture spaces and a variant of IMs for real-time robot response
generation.

I. I NTRODUCTION
In this video, we propose to extract the interaction dynamics during a collaborative task using learning by demonstration. Our objective is to develop learning methods, that allow
robots to gradually increase their repertoire of interaction
skills without additional effort for a human programmer. For
this, example human-human demonstrations of the collaborative task are used to infer the robot’s role in each situation.
The recorded demonstrations are analyzed and an interaction
model is extracted. The model is composed of an Hidden
Markov Model (HMM) and a set of Interaction Meshes (IM).
The HMM allows a robot to identify different contexts. Once
a speciﬁc context is identiﬁed, a corresponding IM is selected
and used to generate the robot responses. An IM holds
information about the spatial relationship between the pose
of the robot and the pose of the user. Using an optimization
scheme, the IM is deformed to best ﬁt the current situation.
The approach builds upon previous results [1], [2], [3] and
extends them to complex physical interaction scenarios, e.g.,
collaborative assembly (see Fig. 1). In particular, complex

II. L EARNING AN I NTERACTION M ODEL
The interaction model serves to identify a robot’s response to the movement of the human user in cooperative
tasks. Structurally, the interaction model consists of a large
database of IMs as well as several data structures for identifying the best matching IM during an ongoing human-robot
interaction. Each IM represents, for one time-step, a pair of
postures of the human-human demonstration. They capture
the spatial relationships of the interaction and can be adapted
at runtime to the speciﬁcs of the human-robot interaction
such as spatial distances between body parts. We developed
a correlation-based variant of IMs that include only the most
relevant joints of the two human demonstrators in order to
make the adaptation real-time capable.
The selection of a suitable IM during runtime is a
multi-stage process involving several data structures that
are learned from the interaction demonstrations. A lowdimensional global posture space, in conjunction with a
Gaussian mixture model (GMM) and an HMM are used
to identify the human-human demonstration that ﬁts the
current interaction best. A low-dimensional local posture
space, deﬁned for each human-human demonstration, and
further segmented into smaller parts of the interaction, in
combination with dynamic time warping (DTW) serves to
identify the best matching IM (also adapting the robot’s
response to the timing of the human user).

1 David Vogt and Bernhard Jung are with Faculty of Mathematics and
Informatics, Technical University Bergakademie Freiberg, 09599, Freiberg,
Germany surname.name@informatik.tu-freiberg.de
2 Simon
Stepputtis
and
Richard
Weinhold
are
with
Faculty
of
Mathematics
and
Informatics,
Technical
University
Bergakademie
Freiberg,
09599,
Freiberg,
Germany

stepputt/weinhol3@student.tu-freiberg.de

3 Heni Ben Amor is with the School of Computing, Informatics, Decision
Systems Engineering, Arizona State University, 699 S Mill Ave, Tempe, AZ
85281 hbenamor@asu.edu

978-1-5090-4718-5/16/$31.00 ©2016 IEEE

142

Robot Gripper Height
Higher
24,4s

21,6s

Demonstration Height
22,3s

21,3s

27,9s

20,3s

27,3s

Lower
20,6s

20,8s

19,8s

Height in m

0.6

Subta
sk 4

k3
Subta
s

0

Subta
sk 2

0.2

Subta
sk 1

0.4

20,8s
16,6s
Validation Data
Interaction Model
After Adaptation

Execution 1 89,6s

Execution 2 96,1s

Execution 3 78,0s

Fig. 2. An important feature of our approach is the generalization of learned behaviors to new situations. To test the generalization capabilities of our
system we recorded several repetitions of the rocket assembly task. The ﬁgure shows 3 variations with differing hand-over heights. On the left, input user
motions signiﬁcantly higher than the original recording are shown. In the middle hand-over heights similar to the training data and on the right lower
hand-over heights are illustrated. The ﬁgure depicts how a frame from the interaction model (red trajectory) is adapted to the new situation (blue trajectory).
The green trajectory shows gripper heights after our adaptation.

III. S PATIOTEMPORAL G ENERALIZATION

with humans in a collaborative Lego rocket assembly tasks.
Whereas current imitation learning methods almost exclusively focus on a single agent, our method is based on
parallel behavior demonstrations by two interaction partners.
In addition to being based on two-person behavior demonstrations, the learned interaction models also inherently
capture the important spatial relationships and synchrony
of body movements between two interacting partners. At
runtime, the robot’s reaction to the human’s motion can
be efﬁciently extracted from the IM in real-time. Therefore
the presented approach is well suited for collaborative tasks
requiring continuous body movement coordination of human
and robot.
Our interaction models are composed of an Hidden
Markov Model and a set of IMs. At runtime, the HMM
is able to reliably select the appropriate interaction example
from the set of training data. In particular, the HMM exhibits
enough hysteresis in order to avoid an undesirable ”jumping”
between different interaction examples in consecutive frames
that we observed in alternative action recognition methods
we experimented with.

An important feature of our system is spatiotemporal generalization of trained behaviors. We evaluated our approach
using one motion captured demonstration of a cooperative
Lego rocket assembly task as well as additional 13 motion
capture recordings of human-human interactions as validation data. Between the 13 task executions, the handover
positions of the manipulated object were varied both in
height. In a simulation environment, the recorded motions of
the observed agent were applied to a simulated human while
the responses of a simulated robot were computed using
our interaction model. The simulated robot’s motions were
then compared to the motions of the human assistant in the
validation cases. Fig. 2 depicts the robot’s response in three
executions of the assembly tasks including all substasks. In
the ﬁgure, blue trajectories depict the height of the human
hand during validation while red trajectories show hand hight
in the demonstration used to train the interaction model. The
robot’s adapted gripper height is shown in green.
Temporal generalization is achieved using a two stage process. First, the user’s motion is matched against interaction
demonstrations using the HMM and, then, aligned locally
in the corresponding local posture space using DTW. Fig.
2 shows three repetitions of the Lego assembly task with
varying execution speeds, with a time difference between
the slowest and fastest task completion of ∼ 20 s. The actual
execution time in the validation cases is indicated by the
blue trajectory. As can be seen in the Figure, the shapes of
the red trajectory (selection of an appropriate IM) and the
green trajectory (adaption of the selected IM to the current
situation) closely match the shape of the blue trajectory. This
shows that our method is able to maintain a close temporal
synchrony between the movements of the human and the
robot even if the task execution time is quite different from
the training example.

R EFERENCES
[1] E. S. L. Ho, T. Komura, and C.-L. Tai, “Spatial relationship preserving
character motion adaptation,” ACM Trans. Graph., vol. 29, no. 4, jul
2010.
[2] H. Ben Amor, D. Vogt, M. Ewerton, E. Berger, B. Jung, and J. Peters,
“Learning responsive robot behavior by imitation,” in 2013 IEEE/RSJ
International Conference on Intelligent Robots and Systems. IEEE,
nov 2013, pp. 3257–3264.
[3] D. Vogt, B. Lorenz, S. Grehl, and B. Jung, “Behavior generation for
interactive virtual humans using context-dependent interaction meshes
and automated constraint extraction,” Computer Animation and Virtual
Worlds, vol. 26, no. 3-4, pp. 227–235, may 2015.

A PPENDIX
A high-resolution video demonstrating the
method on a tube assembly task can be
https://youtu.be/_2qcU4FcGyE.
A video showcasing our methodology in a
task using multiple Kinect V2 sensors can be
https://youtu.be/KhcvUUO-ZE0.

IV. C ONCLUSION
In the video we demonstrate our learning from demonstration system that enables robots to seamlessly interact

143

proposed
found at
handover
found at

2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Daejeon Convention Center
October 9-14, 2016, Daejeon, Korea

Estimating Perturbations from Experience using Neural Networks and
Information Transfer
Erik Berger1 , David Vogt1 , Steve Grehl1 , Bernhard Jung1 , Heni Ben Amor2
Abstract— In order to ensure safe operation, robots must be
able to reliably detect behavior perturbations that result from
unexpected physical interactions with their environment and
human co-workers. While some robots provide firmware force
sensors that generate rough force estimates, more accurate force
measurements are usually achieved with dedicated force-torque
sensors. However, such sensors are often heavy, expensive and
require an additional power supply. In the case of lightweight manipulators, the already limited payload capabilities
may be reduced in a significant way. This paper presents an
experience-based approach for accurately estimating external
forces being applied to a robot without the need for a forcetorque sensor. Using Information Transfer, a subset of sensors
relevant to the executed behavior are identified from a larger
set of internal sensors. Models mapping robot sensor data
to force-torque measurements are learned using a neural
network. These models can be used to predict the magnitude
and direction of perturbations from affordable, proprioceptive
sensors only. Experiments with a UR5 robot show that our
method yields force estimates with accuracy comparable to a
dedicated force-torque sensor. Moreover, our method yields a
substantial improvement in accuracy over force-torque values
provided by the robot firmware.

Fig. 1.
For safe behavior execution, robots must be able to reliably
detect perturbations resulting from unexpected physical interactions with
their environment and human co-workers.

Recent advances in deep learning research have produced
powerful neural network models for feature extraction and
nonlinear regression [4]. In this paper, we investigate such
deep learning techniques as the core component of our
methodology. To this end, we will contrast feature extraction
using autoencoders to our previous approach using Transfer
Entropy [5]. For modeling the evolution of sensor values
in time, we will employ different neural network architectures, including feedforward and recurrent neural networks.
One potential advantage of neural networks over previous
approaches, is their scalability to large numbers of input
dimensions and large numbers of data points, as well as their
ability to represent both feature extraction and regression
within the same framework.
In the remainder of this paper, we will introduce our
methodology for perturbation estimation and show how
neural networks can be used to model the evolution of sensor
values in time. We will present a number of experiments to
identify performance of different neural network architectures in this application domain and evaluate the accuracy of
these results.

I. I NTRODUCTION
The ability to sense the environment is a vital requirement
for intelligent and safe robotics. Modern sensors, such as
force-torque sensors (FT) can be used to measure external
influences on a robot and, in turn, generate adequate responses. However, it is often difficult to distinguish between
natural, behavior-related fluctuations in the sensor readings
and external perturbations that are caused by forces or
collisions applied by the outside world. Dynamic tasks in
particular can cause significant variation in sensor readings
that could potentially be mistaken for external influence.
In recent years, various methods have been proposed for
behavior-specific estimation of external perturbations [1][2].
In prior work, we have introduced a new methodology for
estimating forces from experience [3]. We have shown that
nonlinear state prediction and machine learning can be used
to generate accurate estimates of external perturbations, even
in the absence of force measuring sensors. First, a model
of the expected sensory feedback during a physical task is
learned. During runtime, expected sensations are compared
to measured sensor values and the difference is transformed
into a perturbation estimate. Our results showed that feature
extraction is a key component to the above methodology.

II. R ELATED W ORK
Robots that engage in physical interactions with humans
and objects need to regulate the forces exchanged with
their environment. This requires methods for estimating the
occurring intrinsic and external forces to allow for appropriate robot responses. Recent developments in compliant

1 Institute of Computer Science, Technical University Bergakademie
Freiberg, Bernhard-von-Cotta-Str. 2, 09599 Freiberg, Germany
2 School of Computing, Informatics and, Decision Systems Engineering,
Arizona State University, 699 S Mill Ave, Tempe, AZ 85281, USA

978-1-5090-3762-9/16/$31.00 ©2016 IEEE

176

Data Acquisition
Time
FT Data
Robot Data

Model Generation
Intrinsic
FT- Model

Total
FT-Model

Perturbation Estimation
Intrinsic
FT-Prediction

Total
FT-Prediction

Perturbation Value

Realtime Robot Data

Fig. 2. An overview of the presented machine learning approach. During an offline training phase, the robot sensor data together with information about
the time, force, and torque are recorded (Section III-A). The recorded data is analyzed for features and used to learn two models (Section III-B). In order to
estimate external influences, the robot’s realtime sensor data is used to predict intrinsic (Section III-C) and total force-torque measurements(Section III-D).
The difference between both is used as estimation of the actual external perturbation.

The depicted robot provides 104 different measurements
from a multitude of internal sensors, e.g., the applied current
or the joint encoder state. In our approach, sensor readings
generated from these sensors of the UR5 are used to learn a
model that predicts FT-values measured by the FT150. The
force-torque sensor is used in this context to provide ground
truth data about all measured forces acting on the robot.
The basic rationale underlying our approach is that accurate estimates of forces applied on the robot can be generated
by fusing information and evidence from a large number
of low-cost sensors. Note that these sensors do not have to
be related to force estimation. A crucial component in our
methodology is the identification of relevant features that
are used for learning the predictive models. Figure 2 show
an overview of the approach.
The first step of the presented approach is to record
training data representing the evolution of sensor values for
the particular behavior. In contrast to our previous work [3],
[2] no labeled data is recorded. Instead, the values of the
FT150 are used as ground truth data for the actual force
and torque. Next, the recorded data is analyzed for relevant features. As presented in [10], classical dimensionality
reduction methods such as Principal Component Analysis
fail to identify behavior-specific features. Instead, Transfer
Entropy [5] (TE) is utilized to extract the most relevant
features. In turn, these features are used to train two neural
network models. The first model is used to predict the
natural, behavior-related dynamics and is therefore called
intrinsic FT-model. The second network is trained to predict
total FT-values acting on the robot (intrinsic dynamics +
external perturbations) and is called total FT-model.
The difference between total and intrinsic prediction is
used to estimate the magnitude and direction of external
perturbations applied by a human or through a collision.
Since both models require only robot sensor data to predict
the actual FT-values, no expensive and heavy FT-sensor is
required during runtime. In the following, each step of the
presented approach will be explained in more detail.

control have lead to the emergence of robots with joint
torque sensing and feedback control [6]. For measuring external forces and perturbations, however, typically additional
force-torque (FT) sensors are used. Such sensors are often
expensive, add weight to the robot, and require additional
power supply. Hence, various authors have suggested using
algorithmic approaches for inferring applied forces. In [7],
a depth camera is used in order to estimate applied forces.
Using a depth camera allows for generic contact locations
on the robot. In [1] machine learning methods are used
to extract an inverse dynamics model for a cable-driven
robot manipulator. Measuring the difference between predicted controls from the inverse dynamics model and the
executed controls provides an estimate for external forces
applied on the robot. Using machine learning for force-based
robot control has also been suggested a number of other
publications. In [8] a robot learns to adapt its motion by
anticipating human intentions from force measurements only.
In [9], a method for learning force-based manipulation skills
from demonstrations was presented. The approach generated
variable-impedance control strategies thereby producing the
necessary compliance for handling deformable objects.
The work presented in the remainder of our paper focuses
on different aspect: how can a robot learn to estimate forces
from experience? In contrast to earlier discussed papers, we
learn behavior-specific models for intrinsic and total force
estimation. To increase the accuracy, an additional FT-sensor
is used during training these models. During runtime the FTsensor is not available anymore and therefore is predicted by
the learned models from the robot’s sensor data only. Finally,
the difference between intrinsic and total forces is used to
predict the magnitude and direction of external perturbations.
III. A PPROACH
The goal of the presented approach is to learn a behaviorspecific perturbation model which is able to predict the readings of a FT-sensor from previous experience. Specifically,
a FT-sensor with six degrees of freedom (Robotiq FT150) is
mounted close to the tool center point (TCP) of a robotic
arm (Universal Robots UR5). Figure 1 shows the principal
setup including a gripper for pick-and-place operations.

A. Data Acquisition
The realtime interface of the UR5 provides 104 different sensors, containing a variety of different information
177

Measured

sources, e.g., the mainboard voltage, control, target and
actual joint values. Some of these sources are relevant to
the FT prediction task while others are redundant or noninformative. Hence we first identify a smaller set of relevant,
informative sensors. To this end, the robot sensor values
s = (s1 , . . . , s104 ) and the FT-values f = (f1 , . . . , f6 )
are recorded during the execution of a behavior. Since the
FT150 data rate is less than the UR5’s 125Hz, FT-values
need to be interpolated to generate intermediate values. We
use Dynamic Mode Decomposition (DMD) [11], a nonlinear
interpolation method as described in [2]. Additionally, the
time index r is recorded. During behavior execution the time
index increases and is reset after each repetition. This time
index will be relevant for the later feature selection step of
the intrinsic FT-model.
For all following experiments a pick and place behavior
was executed on the robot for 60 seconds. During this process, the behavior was repeated about 16 times. The recorded
data R = ((s, f , r)1 ; . . . ; (s, f , r)n ) represents training data
consisting of n = 7500 equidistant samples with no external
perturbations. In addition, two other data sets T1 and T2
with a length of 60 seconds were recorded. In contrast to the
training data, T1 was perturbed by a human during the last
half of its execution, while T2 was continuously perturbed.

FF

TD

NARX

REC

All sensors
50

Force [N]

25

0

-25

-50

Autoencoder features
50

Force [N]

25

0

-25

-50

TE features
50

Force [N]

25

0

-25

B. Model Learning
-50

In the following section, different neural network architectures for dynamical systems modeling are used to model
the recorded robot dynamics. More specifically, a feedforward (FF), a time-delay (TD), a recurrent (REC), and
a nonlinear autoregressive network with exogenous inputs
(NARX) are employed. For the sake of reproducibility, each
network was generated with one hidden layer containing
six neurons. Training was performed with the LevenbergMarquardt method. In order to allow for a dynamic response,
all but the FF network require setting a temporal delay
parameter d. The TD network has a delay on the input
weights while the REC network layer has a delayed recurrent
connection to itself. In addition to delayed input weights,
the NARX network makes use of previous predictions. For
a more detailed description of the different neural network
architectures, the reader is referred to [12]. All network
architectures have been trained to map input data s to output
data f .

0

10

20

30

40

50

60

Time [s]

Fig. 3.
The different predictions of f2 ∈ T1 for different neural
network architectures trained with the data set R. Using all sensors (top),
autoencoders (middle), or TE feature selection (bottom) influences the
accuracy of the predictions. Only the sensors selected by utilizing TE was
able to suppress the external perturbations in the second half of the data
set.
Discarded
Selected

0.12

TEr

0.1
0.08
0.06
0.04
0.02
0
0

10

20

30

40

50

Sensors

60

70

80

90

100

Fig. 4. 10 sensors (highlighted green) with 50% of the overall TE are
selected as features for training. The remaining 94 sensors (highlighted
blue) are classified as less important for predicting the actual phase and
therefore being discarded.

C. Intrinsic FT-Prediction
In the following section, all neural networks are trained
with R and a delay of d = 2. The goal is to predict the
intrinsic state for the semi-perturbed data set T1. First, the
neural networks is trained using all sensors. The resulting
predictions for f2 can be seen in Figure 3. As long as no
perturbation occurs, all networks are approximately predicting the correct value. However, during the second half of T1
all networks fail to predict the correct intrinsic state. This is
due to the fact that the 104 input sensors contain non-relevant
information that may obfuscate important features. Hence,

feature extraction methods are needed to identify relevant
features.
A feature extraction approach that is gaining popularity, is
the use of autoencoders [4]. An autoencoder consists of an
encoder and a decoder component, both of which are neural
networks.
Broadly speaking, the encoder maps the input data to a
smaller set of hidden neurons while the decoder tries to
178

reconstruct the original input. This process can be stacked to
reduce the dimensionality of the input data through a stepwise layering. Using autoencoders, we reduce 104 sensors
to 50 values using the first encoder, and then to 10 values
by stacking a second encoder. For the encoding process, a
logistic sigmoid function was used while a linear transfer
function was utilized for the decoders. As a result, the 104 dimensional input data s is reduced to a 10 dimensional feature
data set. Next, this feature data was used to train the neural
networks. The resulting predictions for f2 ∈ T1 can be seen
in Figure 3 middle. Especially during the perturbation phase,
the overall accuracy increases since irrelevant information is
discarded. However, a problem of autoencoders is that the
temporal influence and correlation of variables is not taken
into account.
To predict the time-dependent intrinsic values of the FTsensor the actual phase of the behavior is taken into account.
TE is used as a measure of predictability and information
flow between the robots sensor values and the relative time
by TEr = T E(s, r). The TE from J to I is defined as
X
p(i + 1|i, j)
p(i + 1, i, j)log2
T E(I, J) =
,
p(i + 1|i)

Discarded
Selected

0.08

TEf

0.06

0.04

0.02

0
0

10

20

30

40

50

60

70

80

90

100

Sensors

Fig. 5. 9 sensors (highlighted green) with 50% of the overall TE are
selected as features for training. The remaining 95 sensors (highlighted blue)
are classified as less important for predicting the actual total FT-values and
therefore being discarded.

40

Force [N]

20

0

-20

-40

Measured
All Sensors
Autoencoders
TE features

-60

i∈I,j∈J

0

where (i1 , . . . , iq ) ∈ I and (j1 , . . . , jq ) ∈ J are the possible
states of quantized time-series data and the function p(·|·)
describes the conditional probability. For a more detailed
derivation the interested reader is referred to [5]. The resulting TEr describes how strong each sensor of the robot
is influenced by the relative time. Sensors with a high TE
are assumed to be beneficial for predicting the relative time
and therefore the actual phase of the behavior. Figure 4
shows the normalized and ordered TE values of TEr. As
can be seen, only 10 sensors with at least 50% overall TE
are selected for training the neural networks. These sensors
are, in particular, target and control values of the robot’s joint
states (e.g. position and velocity), which are independent of
external influences and therefore are good predictors for the
actual phase of the behavior. The resulting predictions for
this subset of sensors can be seen in Figure 3 bottom. In
contrast to the previous results, the selected sensors are able
to accurately predict the intrinsic fluctuations. Also during
external perturbations the predicted intrinsic forces are not
influenced. The difference between the actual measured total
FT-values and the intrinsic ones can be used to estimate the
magnitude and direction of an external perturbation.

10

20

30

40

50

60

Time [s]

Fig. 6.
The different predictions of f2 ∈ T1 for a recurrent neural
network which was trained with the data sets R and T2. Using all sensors
(red), autoencoders (yellow), or TE feature selection (purple) influences the
accuracy of the predictions.

Figure 5 shows the normalized and ordered TE values of
TEf . As can be seen in the figure, a small set of 9 sensors
contains more than 50% of the overall TE information. These
are especially measured values (e.g. current values close to
the TCP) and no more target and control values. This is
due to the fact that external perturbations does not perturb
target/control values but the measured actual ones instead.
The resulting mean squared errors for the different network
predictions of f ∈ T1 are shown in Table I. As can be seen,
utilizing TE for feature selection outperforms the usage of
autoencoders or all sensors. Furthermore, for NARX and
REC networks using autoencoders further deteriorates the
results. A possible explanation for this effect is that the objective function of autoencoders only focuses on the amount
of information retained by using the generated features. This
may be detrimental in various physical tasks in which some

D. Total FT-Prediction
In contrast to the prediction of the intrinsic FT-values explained in the previous section the total FT-values are not exclusively dependent on the state of the robot. Consequently, it
is important that the neural networks are additionally trained
with information about how external perturbations influence
the sensors of the robot. To this end, the neural networks are
trained with R and the perturbed data set T2. Furthermore,
feature selection is not calculate with respect to relative time
anymore. Instead, the TE is calculated between the robots
sensor values and each FT-value by TEf = |T E(s, f )|.

TABLE I
T HE MEAN SQUARED ERRORS RESULTING FROM DIFFERENT INPUTS AND
NEURAL NETWORK ARCHITECTURES .

179

All Sensors

Autoencoder

TE Features

FF

155.24

72.62

25.14

TD

95.48

81.45

14.82

NARX

22.43

62.34

7.33

REC

13.74

42.48

3.41

Firmware

sensors have limited variability but strong influence on the
task. The best result is obtained by using TE features and the
REC network architecture. Predictions of f2 ∈ T1 generated
by the REC network are compared to using all sensors and
autoencoders in Figure 6. Given these results, the following
experiment utilizes the proposed TE feature selection method
combined with recurrent neural networks.

Measurement

Prediction

Force [N]

50

0

-50

IV. E XPERIMENTS
Different experiments have been conducted to validate the
proposed method. To this end, the data set T1 introduced in
Section III-A was used.

50

Force [N]

A. Accuracy of Estimates
First, the RECo model is evaluated by investigating the
mean absolute error (MAE) in comparison to ground truth
data of the FT150 and an intrinsic FT-sensor included in
the firmware of the UR5. This sensor makes use of joint
torques and a kinematic model of the robot to predict the
FT-values at the TCP. In order to get comparable results,
the firmware was calibrated to the mass and size of the
FT150. For the FT-sensor, the manufacturer specifies a force
accuracy of 25 N at the TCP and a detection time of 250 ms.
The different force estimates for each dimension can be
seen in Figure 7. The estimates provided by the firmware
sensor follow the general trend but exhibit significant noise.
By contrast, the estimates of the RECo model are close to
the ground truth data of the FT150. The accuracy of the
firmware sensor resulted in a MAE of 26.7401 N which is
slightly below the 25 N specified by the manufacturer. In
addition, considering the mentioned detection time delay of
250 ms did not decrease the MAE score. In comparison,
the MAE of the RECo model is 3.8336 N. Furthermore,
the detection time is less than 10 ms (on a dual core with
3.2 GHz) and scales with the performance of the computer
system. Increasing the network delay from d = 2 to d = 125
further reduces the MAE to 1.9417 N. Consequently, the
model requires one second of continuous sensor data to start
the prediction while the detection time slightly increases to
15 ms. However, in order to keep the robot reactive from
the beginning, a minimal delay of d = 2 was used for all
experiments. Additionally, the RECo model also provides
better torque estimates (1.0761 N m) when compared to the
firmware sensor (5.6735 N m).

0

-50

-100

60

Force [N]

40

20

0

-20

0

10

20

30

40

50

60

Time [s]

Fig. 7. The x (top), y (middle) and z (bottom) force values of the firmware
FT-sensor (green), the FT150 sensor (blue), and the prediction of the learned
model (purple). The predicted values provide a tighter approximation of the
measured ground truth data.
x-direction

y-direction

z-direction

25
20

Force [N]

15
10
5
0
-5
-10

0

B. Perturbation Estimation

10

20

30

40

50

60

Time [s]

The intrinsic forces need to be predicted in order to dissect
external perturbations from the predicted total forces. For this
task, the RECi model is used. Figure 8 shows the resulting
intrinsic force predictions. As can be seen in Figure 8,
the intrinsic forces are not affected by perturbations in the
second half of the recording. Finally, the perturbation value
shown in Figure 9 is defined as the difference between
total and intrinsic FT-values. The length and direction of
the perturbation value is used to estimate the magnitude
and direction of external perturbations. Generated estimates
only represent the external perturbations applied by humans,

Fig. 8. The intrinsic force predictions are not influenced by the external
perturbations.

collisions or other external factors. As a result, the proposed
method can be applied during runtime without making use
of a FT-sensor in order to estimate total, intrinsic and in
consequence external FT-values from previous experience.
A video of some further experiments can be found here 1 .
1 https://youtu.be/60ue0X25S6k

180

x-direction
y-direction
z-direction

40

light-weight manipulators with a limited payload.A further
strength of the presented approach is that no prior knowledge
of the robot kinematics, dynamics, or sensor characteristics
is required. As a result, the approach generalizes to arbitrary
robot platforms. We have shown that, without further adjustment, usual neural network architectures produce adequate
estimates of the intrinsic, external, and total FT-values.
Adapting the network properties, for instance by increasing
the network delay, could further increase the estimation
accuracy. A limitation of the approach is that the robot needs
to perform the same behavior during training and runtime
estimation. However, early results on the generalization
capability of this approach show that it generalizes to mild
variations of the behavior. A more in-depth evaluation of the
generalization ability will be conducted in future work.

Perturbation [N]

20

0

-20

-40

-60

0

10

20

30

40

50

60

Time [s]

Fig. 9. The difference between the total and intrinsic forces represent
the perturbation value which is the FT-value applied to the robot from its
environment.
Perturbation

No Perturbation

50

R EFERENCES

Force [N]

40

30

[1] A. Colome, D. Pardo, G. Alenya, and C. Torras, “External force
estimation during compliant robot manipulation,” in Robotics and
Automation (ICRA), 2013 IEEE International Conference on, May
2013, pp. 3535–3540.
[2] E. Berger, M. Sastuba, D. Vogt, B. Jung, and H. B. Amor, “Estimation
of perturbations in robotic behavior using dynamic mode decomposition,” Advanced Robotics, vol. 29, no. 5, pp. 331–343, 2015.
[3] E. Berger, S. Grehl, D. Vogt, B. Jung, and H. Ben Amor, “Experiencebased torque estimation for an industrial robot,” in Proceedings of the
IEEE International Conference on Robotics and Automation (ICRA),
2016.
[4] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol,
“Stacked denoising autoencoders: Learning useful representations in a
deep network with a local denoising criterion,” J. Mach. Learn. Res.,
vol. 11, pp. 3371–3408, Dec. 2010.
[5] T. Schreiber, “Measuring information transfer,” Physical Review Letters, vol. 85, no. 2, pp. 461–464, 2000.
[6] R. Bischoff, J. Kurth, G. Schreiber, R. Koeppe, A. Albu-Schäffer,
A. Beyer, O. Eiberger, S. Haddadin, A. Stemmer, G. Grunwald,
and G. Hirzinger, “The KUKA-DLR lightweight robot arm - a
new reference platform for robotics research and manufacturing,” in
ISR/ROBOTIK 2010, Proceedings for the joint conference of ISR.
VDE Verlag, 2010, pp. 1–8.
[7] E. Magrini, F. Flacco, and A. De Luca, “Control of generalized contact
motion and force in physical human-robot interaction,” in Robotics
and Automation (ICRA), 2015 IEEE International Conference on, May
2015, pp. 2298–2304.
[8] E. Gribovskaya, A. Kheddar, and A. Billard, “Motion learning and
adaptive impedance for robot control during physical interaction with
humans.” in Proceedings of the IEEE International Conference on
Robotics and Automation (ICRA). IEEE, 2011, pp. 4326–4332.
[9] A. Lee, H. Lu, A. Gupta, S. Levine, and P. Abbeel, “Learning
force-based manipulation of deformable objects from multiple demonstrations,” in Proceedings of the IEEE International Conference on
Robotics and Automation (ICRA), 2015.
[10] E. Berger, D. Müller, D. Vogt, B. Jung, and H. Ben Amor, “Transfer
entropy for feature extraction in physical human-robot interaction:
Detecting perturbations from low-cost sensors,” in Humanoids’14,
2014.
[11] M. R. Jovanovi, P. J. Schmid, and J. W. Nichols, “Sparsity-promoting
dynamic mode decomposition,” Physics of Fluids (1994-present),
vol. 26, no. 2, 2014.
[12] M. Nrgaard, O. E. Ravn, N. K. Poulsen, and L. K. Hansen, Neural
Networks for Modelling and Control of Dynamic Systems: A Practitioner’s Handbook, 1st ed. Secaucus, NJ, USA: Springer-Verlag New
York, Inc., 2000.

20

10

0

0

5

10

15

20

Time [s]

Fig. 10.
behavior.

Non-perturbed and perturbed execution of a pick and place

C. Discussion
As can be seen in Figure 7 (bottom), the force applied
on the robot is estimated to be about 23 N. This is due to
the weight of the gripper mounted on top of the FT-sensor
– the manufacturer specifies a weight of 2.3 kg. A strength
of the presented approach is that, no prior knowledge of
the robots kinematics, dynamics, sensor instrumentation, or
parameters is required. As a result, our approach can quickly
be applied to any kind of robot platform. In addition, no
hard thresholds for detecting external perturbations need to
be set. Figure 10 illustrates this point. The green trajectory
shows the force readings during a normal execution of a pick
and place behavior. As can be seen, forces of 22 ± 3N are
generated. In contrast to that, the blue trajectory shows an
execution with different degrees of perturbations.
V. C ONCLUSION
In this paper, we have described a methodology for estimating forces from experience and showed how to use neural
network models in order to generate accurate estimates of
external perturbations. The main advantage of the presented
approach is that the learned models are able to map low-cost
robot sensor data to accurate FT measurements. Hence, no
FT-sensor is required during runtime and consequently the
robot’s weight is reduced. This is particularly beneficial for

181

2015 IEEE 14th International Conference on Machine Learning and Applications

Measuring and Modelling Delays in Robot Manipulators for Temporally Precise
Control using Machine Learning
Thomas Timm Andersen∗ , Heni Ben Amor† , Nils Axel Andersen∗ and Ole Ravn∗
∗ Department of Automation and Control, DTU Electrical Engineering
Technical University of Denmark, DK-2800 Kgs. Lyngby, Denmark. {ttan, naa, or}@elektro.dtu.dk
† Institute for Robotics and Intelligent Machines, College of Computing
Georgia Tech, Atlanta, GA 30332, USA. hbenamor@cc.gatech.edu

Abstract—Latencies and delays play an important role in
temporally precise robot control. During dynamic tasks in
particular, a robot has to account for inherent delays to reach
manipulated objects in time. The different types of occurring
delays are typically convoluted and thereby hard to measure
and separate. In this paper, we present a data-driven methodology for separating and modelling inherent delays during robot
control. We show how both actuation and response delays can
be modelled using modern machine learning methods. The
resulting models can be used to predict the delays as well as the
uncertainty of the prediction. Experiments on two widely used
robot platforms show signiﬁcant actuation and response delays
in standard control loops. Predictive models can, therefore, be
used to reason about expected delays and improve temporal
accuracy during control. The approach can easily be used on
different robot platforms.
Keywords-Robot control; Automation; Machine learning algorithms;

Figure 1. Temporally precise control of an industrial robot is realized
by modelling the inherent delay in the system. The picture depicts a fast
robot movement during data acquisition. Recorded data is processed using
machine learning algorithms to generate predictive models for system and
response delay.

I. I NTRODUCTION
For robots to engage in complex physical interactions with
their environment, efﬁcient and precise action generation
and execution methods are needed. Manipulation of small
objects such as screws and bolts, for example, requires spatially precise movements. However, in dynamically changing
environments, spatial precision alone is often insufﬁcient
to achieve the goals of the task. In order to intercept a
rolling ball on the table, for instance, a robot has to perform
temporally precise control—the right action command has
to be executed at the right time. Yet, by their very nature,
actuation commands are never instantaneously executed.
Delays and latencies, therefore, play an important role
in temporally precise control and can occur at different
locations in the robot control loop. Actuation delay is the
delay type that most roboticists are aware of. When an
action command is sent to the robot’s controller, it takes
a short while to process the command and calculate the
required joint motor input. Imagine a welding robot with
an uncompensated actuation delay of 50 ms, working an
object on a fairly slow-moving conveyor belt with a speed
of 0.5 m/s. The incurred delay would result in a tracking
error of 2.5 cm, which could easily destroy a product, or at
the very least result in a suboptimal result.
978-1-5090-0287-0/15 $31.00 © 2015 IEEE
DOI 10.1109/ICMLA.2015.98

A different type of delay is the response delay which
measures the amount of time until a real-world event is
sensed, processed and updated in memory. Response delay
is usually assumed zero, as one would naturally assume that
this is sampled and transmitted instantaneously whenever a
motion occurs. However, since there is a sampling clock and
since the controller also needs some time to pack the data
for transmission, the response delay can be a non-negligible
amount of time. An important implication of the response
delay is the discrepancy between the robot’s belief of its
own state and the true value of that state. When data is
received from the robot, indicating that the robot is at a
certain position moving with some velocity, the data is in
reality describing a state in the past.
In order to effectively act in dynamic environments and
reason about timing, a robot has to be aware of both the
actuation delay as well as the response delay. Sadly, such
information is not readily accessible form the robotics company, and no method is currently available for identifying
it. This has lead many researches to develop their own
168

controllers, but this is rarely an opportunity for industrial
users.
Safety during operation is the most crucial issue for robot
controllers, but each robotic company may has different
strategies which affect the architecture of the robot controller. It is therefore necessary to consider the controller
as a black box from which we must learn the controllerdependent delay characteristics. Direct measurement of these
delays is typically difﬁcult, since the different delay types are
convoluted and hard to separate. An important challenge is
therefore the question of how to separate these two delays as,
depending on the executed task, a robot has to compensate
for a different type of delay.
In this paper, we present a methodology for measuring
and modelling the inherent delays during robot control. We
introduce an experimental setup which allows us to collect
evidence for both the actuation delay, as well as the response
delay. The collected data is then used to learn controllerdependent predictive models of each type of delay. The
learned predictive models can be used by a robot to reason
about timing and perform temporally precise control.
The contributions of this publication are three-fold. First,
we provide a generic method for measuring the actuation
and response delay of a robot manipulator. Due to its datadriven nature, the method can be used on a variety of
actuators. Second, we show how existing machine learning
methods can be used to model and predict the inherent delay.
Finally, we show modelling results for two widely used robot
platforms, namely the Kuka KR 5 Sixx and the Universal
Robots’ UR10 robot. The acquired data is made publicly
available to the robotics community [1].

devices etc. In [6] a methodology for estimating delays is
presented, which focuses on VR application domains.
In robotics, the delay inherent to control loops can have
a detrimental impact on system performance. This is particularly true for sensor-based control used in autonomous
robots. Visual servoing of a robot, for example, can be
sensitive to the delays introduced through image acquisition
and processing [9]. Similarly, delays in proprioception can
produce instabilities during dynamic motion generation. In
[2], a dynamically smooth controller has been proposed that
can deal with delay in proprioceptive readings. However,
the approach assumes constant and known time-delay. A
major milestone in robot control with time-delay was the
ROTEX experiment [8]. Here, extended Kalman ﬁlters and
graphical representation were used to estimate the state of
objects in space, thereby enabling sensor-based long-range
teleoperation. How to effectively deal with such communication delays has been a central research question in robotic
tele-operation. Delays in robot control loops are not limited
to sensor measurements only. A prominent approach for
dealing with actuation delays is the Smith Predictor [17].
The Smith Predictor assumes a model of the plant, e.g.
robot system, and can become unstable in the presence of
model inaccuracies. A different approach has been proposed
in [3]. A neural network was ﬁrst trained to predict the state
of mobile robots based on positions, orientations, and the
previously issued action commands. The decision making
process was, then, based on predicted states instead of perceived states, e.g. sensor readings. The approach presented in
our paper follows a similar line of thought. However, instead
of predicting speciﬁc states of the robot, we are interested
in predicting the delay occurring at different parts of the
control loop.

II. R ELATED WORK
Modelling time delays is a vital research topic in computer
network engineering. In order to ensure fast communication
over large computer networks, various models have been put
forward to model the mean delay experienced by a message
while it is moving through a network [15]. These analytic
models typically require the introduction of assumptions,
e.g. Kleinrock’s independence assumption [11], to make
them tractable. Yet, since the network communication is
based on a limited number of communication protocols, it
is reasonable to use and constantly reﬁne such analytic approaches. Another domain in which latencies and delays play
a vital role is virtual reality (VR). As noted in [6], latencies
lead to a sensory mismatch between ocular and vestibular
information, can reduce the subjective sense of presence, and
most importantly, can change the pattern of behavior such
that users make more errors during speeded reaching, grasping, or object tracking. In VR applications, measuring and
modelling delays can be very challenging, since the delay
can heavily vary based on the involved software components,
e.g., rendering engine, as well as highly heterogeneous
hardware components, e.g., data gloves, wands, tracking

III. M ETHODOLOGY
In this section, we describe a data-driven methodology
for modelling delays in robotic manipulators. We show how
to acquire evidence for different types of delays and how
this information can be used in conjunction with machine
learning methods to produce predictive models for control.
A. Measuring the delay
The purpose of the presented method is to establish
the actuation and response delay that a high-level control
program can expect when issuing commands to a robotic
controller. To measure these delays, we need to synchronize
the issuing of commands with the control loop of the robot
controller. To this end, we use the published current state of
the robot, which most controllers send out in each control
cycle.
The overall system setup which will be used in the
remainder of the paper is depicted in Figure 2 (left). A highlevel control program is running on a computer, which sends
the commands to the robot control box. The control box,

169

Actuation
Delay

Transmission
Delay

Computer
Transmission
Delay

Robot

Control Box

Gyro/IMU

Response
Delay

Processing Delay

Figure 2. Left: Delays during the control of a robot manipulator. Transmission delay affects information ﬂow between main control computer and the
robot control box. Actuation delay and response delay are introduced in the communication between the control box and the physical robot. Right: For
delay modelling an external sensor is mounted, e.g. a gyroscope, to measure discrepancies between command times and execution times.

in turn, calculates and issues the low-level commands that
drive the robot. The delay between the high-level controller
and the control box will be referred to as the transmission
delay. The transmission delay has already been extensively
studied in computer networking [15] and will thus not be
treated in this paper. It is particularly crucial in tele-operation
scenarios, in which the high-level controller and the robot
control box may be separated by thousands of kilometers.
In this paper, however, we focus on the delays incurred
between the control box and the robot manipulator. A command that is received by the control box from the high-level
program at time t = 0 is typically only executed after a delay
of 1 . This is the actuation delay. Similarly, once a command
is executed by the robot at time t = 1 , it takes another delay
of 2 until the motion is reﬂected in the controllers memory
and transmitted to the high-level program running on the
central computer. This is the response delay.
The fundamental idea of our approach is to compare time
stamps at the moment a command is issued, the moment the
command is executed, and the moment the command gets
reﬂected in the published state of a robot. To this end, it is
important to know the ground truth about the true timing
of the robot movement. This is realized using an external
apparatus in our setup, e.g., a gyroscope or accelerometer,
see Figure 2 (right).
1) Determining ground truth: Since we want to measure
the delay of the robot, we need a reliable and accurate
method of measuring robot motion. The method needs to
measure the current motion without adding a signiﬁcant
delay of its own. This can be achieved by imposing a
signiﬁcantly higher sampling rate than the robot controller.
We use microelectromechanical (MEMS) gyroscopes, or
angular rate sensors, for the revolving joints, and MEMS
accelerometers for prismatic joints. They offer very high
sampling rates of several orders of magnitude higher than
many robot controllers publish (e.g. several kHz for affordable sensors), and practically no delay from motion
to available measurement. Such sensors cannot readily be

used to infer where in the kinematical chain a motion
has occurred, hence measurements have to be performed a
single joint at a time. Gyroscope measurements often come
with signiﬁcant noise, while accelerometer measurements
suffer from drift. However, both of these issues can be
compensated for using simple ofﬂine ﬁltering in-between
measurement and training the model.
2) Acquiring measurements: As mentioned before, our
approach is based on comparing time stamps throughout the
robot control loop. To this end, we use the published state
from the robot as the main sample clock and reference.
An experimental trial starts at t = 0 upon reception of a
ﬁrst package from the controller. The system time stamp
is recorded as soon as data is read, and the byte-encoded
package is stored for later parsing to extract the current joint
state. Upon reception, a command is sent instructing the
robot to start moving a single joint, which we monitor with
our angular rate sensor or accelerometer. The commanded
movement consists of a rapid acceleration in one direction,
followed by a fast deceleration before returning to return
to the starting pose. The entire motion trial takes about a
second, and all packages received until the robot stands
still are stored. Sensor readings from the external sensor
are stored by the central computer in order to identify the
ground truth time stamp of the moment in which the robot
moved.
There are several perturbations that can lead to variations
in the incurred delays, in particular physical perturbations.
For instance, the force resulting from the gravitational pull
on the robot varies with the joint conﬁguration of the robot,
just as the direction of motion effects whether the motor
needs to work against or along gravity. The different size of
motors and gearing in the robot also yields varying results.
These perturbations lead to varying static and kinetic friction
in the moving parts of a robot. This variation in turn leads
to a varying actuation delay.
As the magnitude of the static friction is usually larger
than that of the kinetic friction, we assume that the delay is

170

B. Learning Predictive Models of Delay

mostly affected by the robot’s joint conﬁguration when the
motion starts. We assume that the effect by the other joints
during a motion after the static friction has been overcome
can be neglected. A similar assumption of joint independence if often employed on the joint position controller when
using Independent joint control [14].
To acquire a representative data set for modelling delays,
we therefore need to map out the delay of each joint for
all the different joint combinations, moving in both positive
and negative direction. To capture variance in the delay, each
combination of joint conﬁguration and direction should be
measured several times.

Next, we want to use the recorded data in order to
learn predictive models of robot latencies. Once a predictive
model is learned, it can be used by a robot to infer the most
likely delay in a given situation. A common approach in
robot control is to use a path planner running on the central
computer to generate a starting joint conﬁguration and an
execution time of the trajectory. To ﬁnd the actual real time
that the robot will use to get to the goal state, we can query
the learned predictive models for each moving joint. The
individual delay is then added to the execution time of each
joint to identify the real execution time.
As input features for the model we use the starting joint
conﬁguration of the robot. As mentioned before, forces
acting on the robot vary depending on the joint conﬁguration
and impact in particular the actuation delay. The output of
the model is the expected delay. We learn individual models
for the actuation delay and the response delay, since these
two delays are unrelated. In line with the assumption of
independence between the joints, a separate model is learned
for each joint. Introducing the above structured approach,
allows for accurate predictions of the delay. To evaluate how
a uniﬁed model, predicting the delay of all joints, performs,
such a model is also trained.
The goal of learning is to generate predictive models
that can generalize to new situations and lead to accurate
predictions of the expected latencies. To this end, we use
three different machine learning methods, namely neural
networks (NN) [4], regression trees (RT) [5], and Gaussian
processes (GPR) [12]. We use these methods as they can
all effectively recover nonlinear relationships between input
and output data.
In our speciﬁc implementation, we used a feed-forward
neural network with 30 neurons in a single hidden layer.
Learning was performed using the Levenberg-Marquardt
[10] algorithm. In contrast, the regression tree method hierarchically partitions the training data into a set of partitions
each of which is modelled through a simple linear model.
Both NNs and the RTs produce a single result and do not
provide information about the uncertainty in the predicted
value. In contrast to that, GPR can learn probabilistic, nonlinear mappings between two data sets. Due to the inherent
noise and related phenomena, uncertainty handling is a
crucial issue when dealing with delays.
By providing the mean and the variance of any prediction,
the GPR approach allows us to reason about uncertainty of
our prediction. Together, mean and variance form Gaussian
probability distribution indicating the expected range of
predictions. This information can potentially be exploited to
generate upper- and lower-bounds for the expected delays,
which is in contrast to both NN and RT.
As both NN, RT, and GPR are well known machine
learning methods and we do not add anything to these

3) Filtering data and computing delays: When extracting
the delays, we evaluate the difference between the recorded
data. Before doing that, thought, we use a high order lowpass FIR ﬁlter (Figure 3) on the data from the angular rate
sensor and correct for any drifting of the accelerometer,
based on data recorded while the sensor was held stationary
on the robot.
Frequency (Hz)
0

500

1000

1500

2000

2500

3000

3500
30

Speed unfiltered
Speed filtered
PSD Unfiltered
PSD Filtered

20
10
0

0

-10
-20

Magnitude (dB)

Speed (deg/sec)

5

-30
-40

-5
0

10

20

30

40

50

60

Time (sec)

Figure 3. Gyroscope readings are ﬁltered using a FIR ﬁlter. A 60 second
datastream (green), recorded without moving the robot, is passed through
the ﬁlter to remove noise (blue). The frequency component of the data
before and after ﬁltering is shown in red and black, calculated using Welch’s
Power Spectral Density (PSD) estimate [18]

To calculate the delay, we evaluate our two data series
generated in each trial; the speed output from the robot
controller and the ﬁltered sensor data. The actuation delay is
the difference between the moment a command is sent to the
robot and the moment a sensor registers the motion, while
the response delay is the difference between the moment a
sensor registers motion and the moment it is reﬂected in the
robot’s current speed data. Both are calculated while taking
into account the transmission delay from Figure 2 (left).
Even when ﬁltering out the noise, it can be challenging
to establish the exact moment in time when the sensor
determines that a motion has started as the measured speed is
hardly ever zero. Instead we identify extrema of our recorded
data to detect the time difference between the set target
speed, the measured speed, and the reported current speed.

171

moving 10 times in both positive and negative direction
in 1,920 different joint conﬁgurations. A total of 33,500
trials were performed on each robot in order to generate
a comprehensive dataset, to be released to the public [1].
For purposes of machine learning, only a subset of the data
was later used.
To be able to compare the performance of the two robots,
we used the same 1,920 physical joint conﬁguration (i.e.
all links vertical) for both robots rather than using the
same joint values. This is a necessity since the DenavitHartenberg parameters of the robots are not identical and
the home position varies, thus positive joint rotation on one
robot might lead to negative joint rotation on the other.
Sampling only a subspace of the robots’ total workspace
does not introduce bias in the data, but rather limits the
model to predict delays within that subspace. By sampling
more poses, the model can routinely be extended to cover
the entire workspace if needed.

Figure 4. The Universal Robot UR10 with mounted measuring equipment.
The enclosure keeps the sensor at a stable temperature thus avoiding
temperature-related drift in measurements.

methods, the theory behind them will not be covered further
in this paper.
IV. R ESULTS
A. Experimental setup

B. Delay output

In our experiments, we model the performance of both a
Kuka KR 5 sixx (Figure 1) and a Universal Robot UR10
(Figure 4). To generate the training data, we mounted a
MPU6000 combined angular rate sensor and accelerometer
to the end-effector. To avoid temperature-related drifts, we

As explained in Section III-A3, delays are determined
by evaluation of the extrema of the recorded motions. An
example of how the delays vary for the two robots can be
seen on Figure 6. The distribution of the actuation delays can
be seen on Figure 7, while a boxplot showing the individual
delays per joint is shown on Figure 8. The same plots for
the reaction delays can be seen on Figure 9 and Figure 10,
respectively.

Speed (deg/sec)

100
Commanded speed
Measured speed
Reported speed
Actuation delay
Response Delay

50

0

C. Model comparison

-50

The extracted delays were used to train and validate
models based on different machine learning algorithms,
namely NN, RT, and GPR. For the NN and RT algorithms,
we used the standard MatLab implementation, while we used
GPstuff [16] for the GPR implementation. The starting joint
conﬁguration, the actuated joint, and the rotational direction
were used as input. The delays that were measured at each
input combination were used for training and testing, using
k-fold cross validation with 10 folds to limit overﬁtting
the data and to give an insight on how the model will
generalize to an independent dataset. The mean squared error
(MSE) from each fold were averaged together and used as
a measure of how well the model predicts delays. Models
for predicting both the delay of individual joints, as well as
a combined model that can predict the delay of all joints
were trained. The mean error of each model is derived by
taking the square root of the MSE and is shown in Table I
and II. The tables also shows the resulting mean error if the
delay was assumed that of the median of the corresponding
boxplots. This gives an indication of the performance of the
trained models. Lower values indicate better generalization
capabilities, while larger mean error values indicate poor
prediction performance.

-100
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

Time (sec)

Figure 5. Typical plot of logged data from a single trial. 33,500 trials
were completed on each robot.

mount the sensor on the robot in an enclosure with low
heat conductivity and then let the sensor warm up before
measurements. Since these robots only have revolute joints,
only the angular rate sensor is used, which outputs data at
a rate of 8 kHz.
To collect the data used for training the model, we perform
a series of short trials, wherein the robot is commanded
to perform a fast acceleration and deceleration motion. For
controlling the Kuka robot, the Kuka RSI [7] protocol is
used. It operates with a sample rate of 83.3 Hz (12 ms).
As argued in [13], the UR10 is controlled using URScript
SpeedJ commands. It operates with a sample rate of 125 Hz
(8 ms).
An example trajectory, along with typical outcome of
a trial, can be seen on Figure 5. The plots clearly show
a signiﬁcant time difference in the commanded speed, the
reported speed and the measured speed. To capture the variations of the delays, we performed trials on 4 different joints,

172

Joint 2
Joint 3
angle (deg) angle (deg)

25
-15

Actuation
Delay (s)

-55
0.12

0.03

0.10
0.08
0.06

0.02
0.01
0

-130
-160
45
-5
-35
-75

Actuation
Delay (s)

-80
-110
65

-70
-100

0.065

Response
Delay (s)

-50

Response
Delay (s)

Joint 2
Joint 3
angle (deg) angle (deg)

-20

0.03
0.02
0.01
0

0.045
0.025
0.005

Figure 6. Actuation and response delay for joint 3 moving in positive direction as a function of varying joint 2 and 3. The red graph is the mean and the
gray area is ±2 standard deviations, corresponding to a 95% conﬁdence interval. Note the different y axis interval. Left: Kuka. Right: Universal Robot.
6000

Number of trials in bin

Number of trials in bin

6000
5000
4000
3000
2000
1000
0
0.075

5000
4000
3000
2000
1000
0

0.08

0.085

0.09

0.095

0.1

0.105

0

0.005

0.01

0.015

Delay (s)

Figure 7.

0.02

0.025

0.03

0.035

0.04

Delay (s)

Combined distribution of the actuation delay of all joints. Note the different x axis interval. Left: Kuka. Right: Universal Robot.
0.05

0.115
0.04

Delay (s)

Delay (s)

0.105

0.095

0.03

0.02

0.085

0.01

0.075
Joint 1

Figure 8.

Joint 2

Joint 3

Joint 5

All joints

Joint 1

Joint 2

Joint 3

Joint 5

All joints

Boxplot of individual joint’s actuation delay. Note the different y axis interval. Left: Kuka. Right: Universal Robot.

Number of trials in bin

Number of trials in bin

6000
5000
4000
3000
2000
1000
0

8000
6000
4000
2000
0

0

0.005

0.01

0.015

0.02

0.025

0.03

0.035

Delay (s)

Figure 9.

0

0.005

0.01

0.015

0.02

0.025

Delay (s)

Combined distribution of the response delay of all joints. Left: Kuka. Right: Universal Robot.

173

0.03

0.035

0.03

Delay (s)

Delay (s)

0.03

0.02

0.01

0.01

0.00

0.00
Joint 1

Joint 2

Figure 10.

Joint 3

Joint 5

All joints

Joint 1

Joint 1
2.27
1.79
1.99
1.85
Joint 1
6.18
4.08
4.63
5.01

Joint 2
2.68
2.55
2.48
2.27
Joint 2
4.64
5.32
5.41
4.89

Joint 3
4.61
4.74
3.74
4.30
Joint 3
6.08
3.86
4.28
3.68

Joint 1
2.13
1.70
1.86
1.63
Joint 1
4.82
4.11
4.66
3.88

Joint 2
2.37
2.32
2.21
2.17
Joint 2
2.00
2.48
2.44
2.44

Joint 3
4.30
4.68
3.62
4.23
Joint 3
4.08
4.24
4.47
3.75

Joint 3

Joint 5

All joints

Joint 5
3.51
3.37
3.76
3.39
Joint 5
2.59
2.49
2.72
2.47

Combined
3.67
3.21
3.33
3.48
Combined
5.33
3.12
3.42
3.36

the delay and assuming the delay constant at the median of
each boxplot would thus decrease the error to within 0.3
cm for a delay within ±6 ms. If we include the whiskers
of the boxplot, corresponding to ∼ ±2.7σ or 99.3% of the
data, the worst case error would within 0.85 cm for a delay
within ±17 ms.

Average
3.35
3.13
3.06
3.06
Average
4.96
3.77
4.09
3.88

Figure 8 also shows that on both robots, it is joint 2 that
has the highest delay. This is the shoulder joint, and the
one that lifts the most. This supports our theory that gravity
inﬂuences the actuation delay. Figure 10 suggests that the
response delay on the other hand is not varying between
the joints. This is not surprising, as the response delay, as
mentioned previously, is largely incurred by the sampling
clock, packing of data and transmission. This most likely
happens simultaneously for each joint.

Table II
M EAN E RROR IN MILLISECONDS OF MODEL FIT FOR REACTION DELAY.
Kuka
Median
NN
RT
GPR
UR
Median
NN
RT
GPR

Joint 2

Boxplot of individual joint’s response delay. Left: Kuka. Right: Universal Robot.

Table I
M EAN ERROR IN MILLISECONDS OF MODEL FIT FOR ACTUATION DELAY.
Kuka
Median
NN
RT
GPR
UR
Median
NN
RT
GPR

0.02

Joint 5
3.09
2.22
2.31
3.11
Joint 4
4.90
5.22
5.84
5.20

Combined
3.33
2.36
2.37
2.63
Combined
5.09
4.84
5.33
5.05

Average
3.05
2.65
2.47
2.75
Average
4.18
4.01
4.35
3.82

The seemingly correlation between actuation and response
delay on Figure 6 is a consequence of the relatively low
temporal resolution of the robot controller data. This is also
why it is more dominant on the Kuka robot. As the sum of
the actuation and response delay will always be a multiple
of the sample period, an actuation delay a few ms below the
mean at a speciﬁc pose will result in a response delay a few
ms above the mean at that pose.
A surprising ﬁnding on Figure 9 is that the response delay
for the Kuka robot is more than one sample period, which
suggests that sampling and transmission of data takes place
in separate sample clock cycles.

V. D ISCUSSION
A. Evaluating the two robots’ delays
As it can be seen on Figure 7, the actuation delay of the
Kuka is signiﬁcantly higher than on the Universal Robot,
even factoring in the higher sample period; the average delay
for the Kuka is 7.5 sample periods vs. 2.5 sample periods
for the UR. If we relate the ﬁgure to the example from the
introduction, where a welding robot need to weld an object
on a conveyor belt moving at 0.5 m/s, our claim that it is important to compensate for the delay is clearly justiﬁed. The
Kuka robot would, without compensation, make a welding
seam displaced 4.5cm ± 0.5cm from the target, while the
Universal Robot would miss with 0.75cm − 1.25cm.
A deeper look into the actuation delays, which is on
Figure 8, shows that the delays in general only vary with
a few ms for each joint. Using our method for measuring

B. Evaluating the models’ performance
All of the models are able to predict the delays very
accurately to within a mean error of 5ms and it is thus
difﬁcult to say anything conclusive about which model is
best. Though all of the models would have a mean error
less than 0.35 cm if used for a typical task like welding,
which is an improvement of more than a factor 12 for the
Kuka robot and almost a factor 3 for the Universal Robot,
compared to using the controllers and not assuming any
delay. Comparing the learned models with measuring the
delay and assuming it to be static shows an improvement

174

between 6 and 24%.

[3] S. Behnke, A. Egorova, A. Gloye, R. Rojas, and M. Simon.
Predicting away robot control latency. In RoboCup 2003:
Robot Soccer World Cup VII, Lecture Notes in Computer
Science, pages 712–719. Springer Berlin Heidelberg, 2004.

The response delay for the Universal Robot shows the
least beneﬁt from modeling. This is most likely due to the
fact that the spread of the delays are so small. The missing
improvement with machine learning is thus a result of the
median delay yield a very good guess, and not a result of
the models being poor at learning those delays.

[4] C. M. Bishop. Neural Networks for Pattern Recognition.
Oxford University Press, Inc., New York, NY, USA, 1995.
[5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁcation and Regression Trees. Wadsworth and Brooks, Monterey,
CA, 1984.

It is worth noticing that the mean error in some cases
are signiﬁcantly higher for the Universal Robot models
than those of the Kuka robot. This correspond with Figure
6, where the conﬁdence interval is much broader for the
Universal Robot than for the Kuka.
It should also be noted that GPR does not only supply
a prediction of the delay, but also outputs a measure of
uncertainty, which is not reﬂected in the tables. For the
Universal Robot’s large variance, this is certainly an added
bonus.

[6] M. Di Luca. New method to measure end-to-end delay of virtual reality. Presence: Teleoper. Virtual Environ., 19(6):569–
584, December 2010.
[7] KUKA Robot Group. KUKA.Ethernet RSI XML 1.1, kst
ethernet rsi xml 1.1 v1 en edition, 12 2007.
[8] G. Hirzinger, K. Landzettel, and Ch. Fagerer. Telerobotics
with large time delays-the rotex experience. In Intelligent
Robots and Systems ’94. ’Advanced Robotic Systems and
the Real World’, IROS ’94. Proceedings of the IEEE/RSJ/GI
International Conference on, volume 1, pages 571–578 vol.1,
Sep 1994.

VI. C ONCLUSION
In this paper we presented a methodology for measuring
and separating actuation and response delays in robot control
loops. In addition, we introduced a data-driven approach
for modelling inherent delays using machine learning algorithms. We showed that the introduced models can be
efﬁciently used to predict occurring delays during temporally
precise control.
Real world experiments were used to identify latencies
in two widely used robot platforms. The measured delay
showed a large potential for improving temporal precision,
with more than a factor 12 improvement for one of the
robots.
All the employed machine learning algorithms showed
similar abilities to further improve the accuracy, with no
algorithm showing signiﬁcantly better accuracy than the
others. Still, Gaussian processes seem to be better suited
for this task, since they provide a probability distribution
over the expected delay. In turn, such a distribution can be
used to reason about upper- and lower-bounds in temporal
precision.
In our future work we will investigate how inverse models
of time delay can be learned. Given a speciﬁc time constraint
during a control task, an inverse model can be queried for
the most appropriate action which will meet the goals of the
task while ensuring time constraints.

[9] A.J. Koivo and N. Houshangi. Real-time vision feedback for servoing robotic manipulator with self-tuning controller. Systems, Man and Cybernetics, IEEE Transactions on,
21(1):134–142, Jan 1991.
[10] D. W. Marquardt. An algorithm for least-squares estimation of
nonlinear parameters. SIAM Journal on Applied Mathematics,
11(2):431–441, 1963.
[11] A. Popescu and D. Constantinescu. On kleinrocks independence assumption. In DemetresD. Kouvatsos, editor, Network
Performance Engineering, volume 5233 of Lecture Notes in
Computer Science, pages 1–13. Springer Berlin Heidelberg,
2011.
[12] C. E. Rasmussen and Ch. K. I. Williams. Gaussian Processes
for Machine Learning. The MIT Press, 2005.
[13] O. Ravn, N. A. Andersen, and T. T. Andersen. Ur10
performance analysis. Technical report, Technical University
of Denmark, Department of Electrical Engineering, 2014.
[14] M. W. Spong, S. Hutchinson, and M. Vidyasagar. Robot
modeling and control. John Wiley & Sons New York, 2006.
[15] A. S. Tanenbaum and D. J. Wetherall. Computer Networks.
Prentice Hall, 5th edition, 2011.
[16] Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi
Jylänki, Ville Tolvanen, and Aki Vehtari. Gpstuff: Bayesian
modeling with gaussian processes. The Journal of Machine
Learning Research, 14(1):1175–1179, 2013.

R EFERENCES
[1] http://aut.elektro.dtu.dk/staff/ttan/delay.html.

[17] P.D. Welch. A controller to overcome dead time. ISA Journal,
6(2):28–33, 1959.

[2] S. Bahrami and M. Namvar. Motion tracking in robotic
manipulators in presence of delay in measurements. In
Robotics and Automation (ICRA), 2010 IEEE International
Conference on, pages 3884–3889, May 2010.

[18] P.D. Welch. A direct digital method of power spectrum
estimation. IBM Journal of Research and Development,
5(2):141–156, 1961.

175

Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)

Sparse Latent Space Policy Search
Kevin Sebastian Luck

Joni Pajarinen

Erik Berger

Arizona State University
Interactive Robotics Lab
AZ 85281 Tempe, USA
mail@kevin-luck.net

Aalto University
Intelligent Robotics Group
02150 Espoo, Finland
Joni.Pajarinen@aalto.ﬁ

Technical University Bergakademie Freiberg
Institute of Computer Science
09599 Freiberg, Germany
erik.berger@informatik.tu-freiberg.de

Ville Kyrki

Heni Ben Amor

Aalto University
Intelligent Robotics Group
02150 Espoo, Finland
ville.kyrki@aalto.ﬁ

Arizona State University
Interactive Robotics Lab
AZ 85281 Tempe, USA
hbenamor@asu.edu
in a lower-dimensional latent space for control which, in
turn, reduces cognitive effort and training time during skill
acquisition. The existence of synergies has been reported
in a variety of human motor tasks, e.g., grasping (Santello,
Flanders, and Soechting 1998), walking (Wang, O’Dwyer,
and Halaki 2013), or balancing (Torres-Oviedo and Ting
2010).
Recently, various synergy-inspired strategies have been
put forward to improve the efﬁciency of RL for motor
skill acquisition (Bitzer, Howard, and Vijayakumar 2010;
Kolter and Ng 2007). Typically, these approaches use dimensionality reduction as a pre-processing step in order to
extract a lower-dimensional latent space of control variables.
However, extracting the latent space using standard dimensionality reduction techniques requires a signiﬁcantly large
training set of (approximate) solutions, prior simulations,
or human demonstrations. Even if such data exists, it may
drastically bias the search by limiting it to the subspace of
initially provided solutions. In our previous work, we introduced an alternative approach called latent space policy
search that tightly integrates RL and dimensionality reduction (Luck et al. 2014). Using an expectation-maximization
(EM) framework (Dempster, Laird, and Rubin 1977) we
presented a latent space policy search algorithm that iteratively reﬁnes both the estimates of the low-dimensional latent space, as well as the policy parameters. Only samples
produced during the search process were used.
In this paper, we propose a different kind of latent space
policy search approach, which similarly to our previous
work combines RL and dimensionality reduction, but which
also allows for prior structural knowledge to be included.
Our method is based on the Variational Bayes (VB) (Neumann 2011; van de Meent et al. 2015) framework. Variational Bayes is a Bayesian generalization of the expectationmaximization algorithm, which returns a distribution over
optimal parameters instead of a single point estimate. It is a
powerful framework for approximating integrals that would
otherwise be intractable. Our RL algorithm exploits these
properties in order to (1) perform efﬁcient policy search,
(2) infer the low-dimensional latent space of the task, and

Abstract
Computational agents often need to learn policies that
involve many control variables, e.g., a robot needs to
control several joints simultaneously. Learning a policy with a high number of parameters, however, usually requires a large number of training samples. We
introduce a reinforcement learning method for sampleefﬁcient policy search that exploits correlations between
control variables. Such correlations are particularly frequent in motor skill learning tasks. The introduced
method uses Variational Inference to estimate policy
parameters, while at the same time uncovering a lowdimensional latent space of controls. Prior knowledge
about the task and the structure of the learning agent
can be provided by specifying groups of potentially
correlated parameters. This information is then used to
impose sparsity constraints on the mapping between
the high-dimensional space of controls and a lowerdimensional latent space. In experiments with a simulated bi-manual manipulator, the new approach effectively identiﬁes synergies between joints, performs efﬁcient low-dimensional policy search, and outperforms
state-of-the-art policy search methods.

Introduction
Reinforcement learning (RL) is a promising approach to automated motor skill acquisition (Peters et al. 2011). Instead
of a human hand-coding speciﬁc controllers, an agent autonomously explores the task at hand through trial-and-error
and learns necessary movements. Yet, reinforcement learning of motor skills is also considered to be a challenging
problem, since it requires sample-efﬁcient learning in highdimensional state and action spaces. A possible strategy to
address this challenge can be found in the human motor
control literature (Bernstein 1967). Research on human motor control provides evidence for motor synergies; joint coactivations of a set of muscles from a smaller number of neural commands. The reduction in involved parameters results
c 2016, Association for the Advancement of Artiﬁcial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

1911

Time
Time

Group 1
Group 2
Group 3
Group 4

Samples

Figure 1: The main idea of Group Factor Policy Search: A number of variables, for example the joints of an arm or leg of
a NAO robot, form one group. Given several of such groups for the action vector (left matrix) the transformation matrix W
can be divided in several submatrices corresponding to those groups. Subsequently each factor, given by a column in W,
encodes information for all groups, e.g. four in the example given above. Factors may be non-zero for all groups, for a subset
of groups, for exactly one group or zero for all groups. In the ﬁgure, grey areas correspond to non-zero values and white areas
to zero values in the sparse transformation matrix. The transformation matrix is multiplied by the latent variables given by
Z̃ = (z̃1 , · · · , z̃t , · · · , z̃T ) distributed by z̃t ∼ N (0, trace(φ(st , t)φ(st , t)T )I).


(3) incorporate prior structural information. Prior knowledge about locality of synergies can be included by specifying distinct groups of correlated sub-components. Often
such prior knowledge about groups of variables, e.g. coactivated joints and limbs, is readily available from the mechanical structure of a system. Structural prior knowledge
is also common in other application domains. For example,
in a wireless network the network topology deﬁnes receiver
groups (Sagduyu and Ephremides 2004).
Our approach draws inspiration and incorporates ideas
from Factor Analysis, in particular Group Factor Analysis (Klami et al. 2015), as can be seen in Fig. 1. Groups
of variables, e.g., robot joints grouped into arms and legs,
are provided as prior structural knowledge by a user. A factorized control policy is then learned through RL, which includes a transformation matrix W. The transformation matrix holds factors that describe dependencies between either
all of the groups or a subset of them. The individual factors
can be regarded as synergies among the joints of the robot.
We will show that the resulting algorithm effectively ties
together prior structural knowledge, latent space identiﬁcation, and policy search in a coherent way.

Eτ [r = 1] =

p(τ, θ)p(r = 1|τ)dθdτ,

(1)

where the probability of the trajectory p(τ, θ) contains the
(stochastic) policy, r is a binary variable indicating maximum reward, and p(r = 1|τ) ∝ exp {−c (τ)} (Toussaint
2009) is the conditional probability of receiving maximum
expected reward given a cost function.
Assuming the Markov property and the independence of
actions, the probability of a trajectory can be written as
p(τ, θ) = p(θ)p(s1 )

T


p(st+1 |st , at )π(at |st , θ).

(2)

t=1

The stochastic policy π(at |st , θ) depends on the parameters θ for which we additionally introduce prior distributions
p(θ). This formulation will subsequently be used for structuring the policy model. The prior distributions may also depend on hyperparameters – for reasons of clarity, however,
we will omit any such parameters below. Furthermore, we
assume that the initial state distribution p(s1 ) and transition
dynamics p(st+1 |st , at ) are unknown but ﬁxed. Thus, they
will cancel out as constant values.

Policy Search
Group Factor Policy Search

Policy search methods try to ﬁnd an optimal policy for an
agent which acts in an uncertain world with an unknown
world model. At each time step t the agent executes an action at in state st and moves to the next state st+1 with probability p(st+1 |st , at ). After executing a certain number of
actions, the agent receives a reward feedback given by an
unknown reward function based on the performed execution
trace (or trajectory/history) τ = (s1 , a1 , . . . , sT , aT , sT +1 ).
The overall objective in policy search is to maximize the expected reward over trajectories and policy parameters θ. For
bounded rewards, maximizing expected reward is equivalent
to maximizing the probability of a binary reward r (Toussaint and Storkey 2006):

We will now introduce a new policy search method, called
Group Factor Policy Search (GrouPS ), that uncovers the latent space on-the-ﬂy based on prior structural information.
In this section, we discuss how to incrementally improve the
policy and the actual form of the new policy model. We parameterize the policy using Group Factor Analysis (Klami
et al. 2015) in order to utilize prior information about the
parameters and their correlations. Since our policy is a linear stochastic model with prior distributions, we ﬁrst present
a novel general Variational Inference framework for policy search that takes priors into account. Subsequently, we
discuss how the policy is parameterized, and ﬁnally show

1912

the policy model update equations for Group Factor Policy
Search which we derive using the introduced Variational Inference method.

Input: Reward function R (·) and initializations of
parameters. Choose number of latent
dimension n. Set ﬁxed hyper-parameters
aτ̃ , bτ̃ , aα , bα , σ 2 and deﬁne groupings.

Variational Inference for Policy Search

1

In each iteration our new policy search method samples a
distribution over trajectories pold (τ) using the current policy,
and based on pold (τ) ﬁnds a new policy which maximizes a
lower bound on the expected reward. This is repeated until
convergence.
In order to ﬁnd a new policy based on samples from the
old one, we introduce the sampling distribution pold (τ) and
the approximated parameter distribution q(θ) (deﬁned later)
into Equation 1. By applying the log-function and using
Jensen’s inequality (Kober and Peters 2009; Bishop 2006,
Eq. (1.115)) we derive the lower bound

p(τ, θ)
log
pold (τ)q(θ)
p(r = 1|τ)dθdτ
pold (τ)q(θ)



p(τ, θ)
p(r = 1|τ)dθdτ.
≥
pold (τ)q(θ) log
pold (τ)q(θ)
(3)

2
3
4
5
6

7
8
9
10
11
12
13
14
15

Since pold (τ) was generated using the old policy it does not
depend on θ and we can simplify the lower bound to



p(τ, θ)
p(r = 1|τ)dθdτ
pold (τ)q(θ) log
pold (τ)q(θ)

= const +
pold (τ)q(θ)
(4)
⎛
⎞
T
	
p (θ)
π(at |θ, st )
⎜
⎟
t=1
⎜
⎟ p(r = 1|τ)dθdτ,
· log ⎝
⎠
q(θ)

16
17
18
19


pold (τ) log

T

t=1

p(r = 1|τ)
π(at , θ|st )
dτdθ−j ,

R

Initialization of q-distribution
while not converged do
Update q (M) with Eq. (16)
Update q (W)
  with Eq. (19)
Update q Z̃ with Eq. (22)

Update q (α) with Eq. (12)
Update q (τ̃ ) with Eq. (25)
M = Eq(M) [M]
W = Eq(W) [W]
α = Eq(α) [α]
τ̃ = Eq(τ̃ ) [τ̃ ]

20

Result: Linear weights M for the feature vector φ,
representing the ﬁnal policy. The columns
of W represents the factors of the latent
space.
Algorithm 1: Outline of the Group Factor Policy
Search (GrouPS) algorithm.

where the constant term can be ignored for the maximization
	
of this term. By assuming the factorization q(θ) = qi (θi )
for the parameters and applying the Variational Bayes approach, we get the approximated distributions of the parameters:
 
log qj (θj ) = const +
qi (θi )
θ−j i=j

while reward not converged do
for h=1:H do # Sample H rollouts
for t=1:T do
at = Wi Zφ + Mφ + Eφ
with Z ∼ N (0, I) and E ∼ N (0, τ̃ ),
where τ̃ (m) = τ̃m I
Execute action at
Observe and store reward R (τ)

et al. 2015). The main idea of GFA is to introduce prior
distributions for the parameters, in particular a prior for
a structured transformation matrix W. The transformation
matrix, responsible for mapping between a low-dimensional
subspace and the original high-dimensional space, is forced
to be sparse and constructed using prior knowledge about
grouping of the dimensions, that is, W is a concatenation of
transform matrices W(m) for each group m. For example,
if the dimensions of a vector represent the joints of a legged
robot, we can group joints belonging to the same leg into the
same group (see e.g. Fig. 1).
Applying the idea of Group Factor Analysis for directed
sampling leads to a linear model, i.e. a stochastic policy


(m)
(m)
φ (st , t) ,
(7)
at = W(m) Zt + M(m) + Et

(5)

where the parameter vector θ−j contains all parameters ex
 is given by the intecept θj . The normalization constant R
gral


 = pold (τ)p(r = 1|τ)dτ.
R
(6)

(m)

∈ RDm ×1 is a linear
where, for group m, the action at
projection of a feature vector φ (st , t) ∈ Rp×1 . Each dimension of the feature vector is given by a basis function,
which may depend on the current state and/or time. In the
remainder of the paper, we will write φ instead of φ (s, t)

Formulation of Group Factor Policy Search
In order to identify sets of correlated variables during policy
search, we use a linear stochastic policy of a form similar
to the model used in Group Factor Analysis (GFA) (Klami

1913

for simplicity, even though there is an implicit dependency
of φ on the current state of a trajectory. W(m) ∈ RDm ×l
is a transformation matrix mapping from the l-dimensional
subspace to the original space. Each entry of the latent matrix Zt ∈ Rl×p is distributed according to a standard normal
distribution where N (0, 1), M(m) ∈ RDm ×p is the mean
(m)
matrix, and the entries of the noise matrix Et ∈ RDm ×p
−1
are distributed by N (0, τ̃m ).
We can derive a stochastic policy from the model deﬁned
in Equation 7. Since
Zt φ ∼ N (0, trace(φφT )I)

Figure 2: Graphical model in Plate notation for Group Factor Policy Search. The basis functions φ(st , t) as well as the
(m)
action vector at are observed. Equation 9 shows the de(m)
pendencies for at . The latent variables z̃t depend on the
feature vector as stated in Equation (8) . The parameter αm
might either be given by a Gamma distribution as stated in
Equation (12) or by a log-linear model with dependencies
on parameters U and V.

(8)

holds (see e.g. (Luck et al. 2014)), we can substitute Zt φ by
the random variable z̃t ∈ Rl×1 resulting in the policy
π(at |θ, st ) =

 ⎞

⎛
T

M
Tr
φφ

(9)
(m) 
N ⎝at W(m) z̃t + M(m) φ,
I⎠ .
τ̃m

m=1
If we take a closer look at the latent space given by Wz̃t
we ﬁrst ﬁnd that the length of each factor is determined by
φ(st , t)22 . Secondly, a factor may be non-zero only for
one or a subset of groups as can be seen in Fig. 1. This leads
to a sparse transformation matrix and specialized factors and
dimensions.
As mentioned before, the form of our linear model in
Equation 7 above is based on Group Factor Analysis. While
GFA typically maps a vector from the latent space to the
high-dimensional space, we map here a matrix from the latent space to the original space and then use this matrix as
a linear policy on the feature vectors. GFA does not apply
factor analysis (see e.g. (Harman 1976)) on each group of
variables separately, but instead introduces a sparsity prior
on the transformation matrix W thereby forcing correlations
between groups:
p (W|α) =

M 
K D
m






(m) 
−1
N wd,k 0, αm,k
,

The hyper-parameters aα and bα are ﬁxed and set to a small
positive value. The prior distributions given above will lead
to three kind of factors: (1) factors which are nonzero for
only one group, (2) factors which are nonzero for several
groups or (3) factors which are zero. In addition to the standard GFA prior distributions above, we introduce further
prior distributions for M and z̃ such that all prior distributions are given with
 




z̃ ∼ N 0, Tr φφT I ,
M ∼ N Mold , σ 2 I ,


τ̃m ∼ G aτ̃ , bτ̃ .
αm,k ∼ G (aα , bα ) ,
Fig. 2 shows a graphical model of Group Factor Policy
Search, given by the distributions stated above. Instead of
Z the latent variable z̃t is used, which depends on φ(st , t)
given a state and a point in time.

(10)

Derivation of Update Equations

m=1 k=1 d=1

with M being number of groups, Dm the number of dimensions of the m-th group and K the number of factors, i.e.
number of columns of W. The precision α is given by a
log-linear model with

We assume ﬁxed hyper-parameters aα , bα , aτ̃ and bτ̃ for
the distributions which we determine using the Variational
Inference method presented earlier, assuming a factorization
of the q-distributions

log α = UVT + μu 1T + 1μT
v,

q (θ) = q(Z̃)q (W) q (τ̃ ) q (M) q (α)

(11)

where U ∈ RM ×R , V ∈ RK×R and μu ∈ RM as well as
μv ∈ RK model the mean proﬁle. R deﬁnes the rank of the
linear model and is chosen R  min (M, K). However, for
the special case of R = min (M, K) the precision is given
by a simple gamma distribution (Klami et al. 2015)


α
(12)
q (αm,k ) = G aα
m , bm,k
with parameters
α
aα
m =a +

Dm
,
2

T
	
q(z̃t ) with
and additionally the assumption q(Z̃) =
Z̃:,t = z̃t .
By using the factorization given above and the Variational Inference rule for deriving the parameter distribution
in Equation (5), we can derive the approximated parameter
distributions, which maximize the expected reward.
The approximated distribution for the mean matrix is
given by a multiplicative normal distribution



M D
m


(m) T  M
M
qM (M) =
N mj,: μmj , Σj
(16)

(13)


1
(m) T (m)
α
.
bα
wk
m,k = b + Eq(W) wk
2

(15)

(14)

m=1 j=1

1914

where the mean and covariance parameters in dependency
of the group and dimension are given by
 −2
ΣM
I+
j = σ
⎡
⎞⎤⎞−1
⎛
T
T

φφ
p(r
=
1|τ)
⎝
 Eτ̃ [τ̃m ]⎠⎦⎠

Ep(τ) ⎣

R
Tr φφT

and
Z̃
μZ̃
t = Σt ·
⎛

T 
⎞
(m)
(m)
(m)
M E
W
a
−
M
φ

W
t
⎜
⎟


⎝
⎠.
−1
T
Tr φφ Eτ̃ [τ̃m ]
m=1

(24)

t=1

Unlike the other distributions, the precision is given by a
multiplicative gamma distribution


M

1
1 
qτ̃ (τ̃ ) =
(25)
G τ̃m |aτ̃ + Dm T, bτ̃ + bτ̃m
2
2
m=1

(17)
and
T
M moldj,:
=
Σ
·
+ ΣM
μM
j
j ·
mj
σ2

⎤


⎡
(m)
T φ a(m) − E
Ez̃ [z̃t ]

w wj,:
t,j
p(r
=
1|τ)
⎦


Ep(τ) ⎣
−1

R
Tr φφT E [τ̃ ]
t=1

τ̃

with one ﬁxed parameter and one variable parameter. Esti
mation of the parameter bτ̃m is the most complex and computationally expensive operation given by

T
−1 
p(r = 1|τ)  
(m) T (m)
T
τ̃ 
Tr φφ
at
at
bm = Ep(τ)

R
t=1


(m) T
− 2at
EM M(m) φ

T


T
+ 2Ez̃ [z̃t ] EW W(m) EM M(m) φ


(m) T
EW W(m) Ez̃ [z̃t ]
− 2at


T
+ φT EM M(m) M(m) φ




T
+ Tr EW W(m) W(m) Covz̃ [z̃t ]



T
T
+Ez̃ [z̃t ] EW W(m) W(m) Ez̃ [z̃t ] .

m

(18)
with mj,: given by the j-th row of M.
The q-distribution for the transformation matrix is similarly given by


M D
m


(m) T W
W
(19)
N wj,: |μmj , Σm
qW (W) =
m=1 j=1

with the mean and covariance parameters


p(r = 1|τ)
W
Σm = Ep(τ)

R
⎞⎤⎞−1
⎛


T

Ez̃ z̃t z̃T
⎝
¯ m,K ⎠⎦⎠ ,
 t

+ ᾱ
−1
T
E
Tr
φφ
[τ̃
]
t=1
τ̃ m
and

(20)

(26)



p(r = 1|τ)

R


 
(m)
(m)
T
T
at,j − EM mj,: φ Ez̃ [z̃t ]



.
−1
T
Tr
φφ
Eτ̃ [τ̃m ]
t=1

Algorithm

W
μW
mj = Σm · Ep(τ)

Algorithm 1 summarizes all update steps for performing
Group Factor Policy Search. The reward function R (·),
number n of latent dimensions, and a set of hyperparameters need to be provided by the user.

(21)

Evaluation

¯ m,K ) =
¯ m,K is given by diag (ᾱ
The diagonal matrix ᾱ
(αm,1 , αm,2 , · · · , αm,K ). The distribution for the latent
variables Z̃ depends on the trajectory and time. Hence the
reward can be seen as a probabilistic weight R̃ of a multiplicative normal distribution. However, since we assume independent latent variables z̃ht we can ignore the reward and
get
H
T

  


Z̃
N z̃ht |μZ̃
,
Σ
R̃
,
(22)
qZ̃ Z̃ =
t
t

For evaluations and experiments the expectation Ep(τ) [·]
used above in Eq.(16-20,25) was approximated by a sample
mean,
H
1 
Ep(τ) [f (τ)] ≈
f (τi )
(27)
H i=1
as proposed in (Kober and Peters 2009), where τi is the ith of the H realized trajectories and f (τ) a function value,
vector or matrix for τi and will be replaced by the parameter
approximations given above.

t=1

with time-dependent parameters
 
−1
Z̃
Σt = Tr φφT
I+


T

Setup of the Evaluation
 ⎞−1

(m)
M E
W(m)

W W


 −1  ⎠
T
Eτ̃m τ̃m
m=1 Tr φφ

For the comparison between the above presented GrouPS algorithm and previous policy search algorithms, a simulated
task of a bi-manual robot operating in a planar task space
was used. Each of the two arms (see Fig. 3) has six degreesof-freedom and the same base for the ﬁrst joint. The initial

(23)
,

1915

has been experimentally validated in both simulated and
physical robotic experiments (Kober and Peters 2011). PePPEr is also based on EM and incorporates policy search and
dimensionality reduction, but without priors and thus without a structured transformation matrix. For comparison with
PePPEr and PoWER the GrouPS algorithm was evaluated in
three different conﬁgurations: (1) One group which contains
all joints of both arms, (2) two groups, where each group
contains the joints of one arm and (3) four groups with two
groups per arm and joints 1-4 in one and joints 5-6 in the
second group. The hyper-parameters of GrouPS were set to
aτ̃ = bτ̃ = 1000, aα = bα = 1 and σ 2 = 100. No optimizations of the hyper-parameters were performed. Furthermore,
to prevent early convergence and collapsing of the distributions due to small sample sizes the parameter W and τ̃ are
resized after each iteration by a factor of 1.5. The same is
done after √
each iteration for PePPEr. However, the factor
was set to 1.09 since higher numbers lead to divergence
in the parameters of the algorithm with unstable and divergent results. PePPEr was implemented as presented in (Luck
et al. 2014) and in each iteration 20 inner iterations for the
optimizations of the parameters were used. The same setup
was used for GrouPS and for both algorithms the number
of latent dimensions were set to six. The static variance parameter for PoWER as presented in (Kober and Peters 2009)
and the initial variance of the other algorithms were all set
to 101.5 , also for NAC with learning parameter set to 0.5. In
each iteration, we sampled 30 trajectories and evaluated the
trajectories based on the reward function

Figure 3: Two simulated arms with six degrees-of-freedom
and the same base in their initial position. Each end effector
has a desired position for each time step, s shown by the
green and red dots. The ﬁnal position at time step 25 is given
by the coordinate (0, 4). The numbers represent the joints
with l for left and r for right.
180
1 Group
POWER
NAC
2 Groups
PePPEr
4 Groups

160

Sum. Distances

140
120
100
80
60
40

R(τ) =

20
0

0

200

400

600

800

25


exp (−  effl (at ) − posl (t) 2 )

t=1

1000

(28)

· exp (−  effr (at ) − posr (t) 2 ) ,

Iterations

where the function effl (at ) returns the position of the left
end effector given the action vector and posl (t) the corresponding desired goal position for time point t. effr (at ) and
posr (t) return the actual and desired positions, respectively,
for the right end effector. Then the 15 best trajectories are
chosen for the computation of the parameters for each algorithm as described in (Kober and Peters 2009).

Figure 4: Comparison between PePPEr, PoWER, Natural
Actor-Critic and three instances of the GrouPS algorithm on
the presented simulated task. Values correspond to the summarized distances between each end effector and its desired
position given the current policy for the iteration. The mean
value as well as the standard deviations are shown.

Results

conﬁguration of the arms is presented in Fig. 3 as well as
the desired positions for each end effector (tip of an arm).
At each of the 25 time steps we give a different goal position for each arm’s end effector, starting from the left for the
left arm and starting from the right for the right arm, with
the same ﬁnal position at (0, 4) for both arms. In this task,
the 12 dimensions of the action vector a represent the joint
angles for each arm. For the basis functions eleven isotropic
Gaussian distributions were used with φi (t) = N (t|μφ
i , 3)
for t ∈ {1, 2, . . . , 24, 25}. In total, 132 parameters have to
be estimated given M ∈ R12×11 .
As reference algorithms PoWER (Kober and Peters
2009), Natural Actor-Critic (NAC) (Peters and Schaal 2008)
and PePPEr (Luck et al. 2014) were chosen: NAC is a policy
gradient method while PoWER is an efﬁcient policy search
method based on expectation maximization (EM). PoWER

Fig. 4 depicts the results of the explained experiment. For
each algorithm ten different runs were executed and both
mean and standard deviation computed. As can be seen in
the ﬁgure, PePPEr outperforms both PoWER and NAC, as
well as our method in case only one group spanning all
variables is used. However, using two groups (one for each
arm) already leads to comparable performance. Finally, the
GrouPS algorithm with 4 different groups signiﬁcantly outperforms the comparison methods.

Importance of the Choice of Groups
In order to investigate the effect of choosing joint groups
we conducted an additional experiment. Our working hypothesis throughout the paper is that structural information about inherent groups of correlated variables will im-

1916

180
Swap1
Swap2
Swap3
4 Groups

160

Sum. Distances

140
120
100
80
60
40
20
0

0

200

400

600

800

Figure 7: Final policy found by the GrouPS algorithm after
100 iterations. A high reward is given if the head as well as
the left foot of the robot are high above the ground.

1000

Iterations

Figure 5: Comparison between the original chosen four
groups and three permutations of the Groups. Values correspond to the summarized distance between each end effector
and its desired position for each time step given the current
policy for the iteration.

Fig. 6. All new groupings (resulting from above swaps)
are clearly outperformed by the original partition. This result corroborates our assumption that a proper selection of
groups can ameliorate the performance of the policy search
algorithm.

180
Swap4
Swap5
4 Groups

160

Sum. Distances

140

Experiment: Lifting a Leg
To test the GrouPS algorithm in experiments following the
real world closely, we reproduced the experiment stated in
(Luck et al. 2014): We simulate a NAO robot (Gouaillier
et al. 2008) using the V-REP framework (Rohmer, Singh,
and Freese 2013) in the task of lifting its left leg without
falling. The same reward function was used as presented in
(Luck et al. 2014, Eq. (22)) with parameters α = 5, β = 10,
γ = 10 and λmax = 6. The V-REP framework (Rohmer,
Singh, and Freese 2013) allows for simulations with high
physical accuracy by utilizing the bullet physics library. In
this experiment, the actions represent the 26 joint velocities
for each of the 15 points in time. Again, for feature functions
Gaussian distributions were used and the same parameters
for GrouPS were chosen like given in the evaluation above.
We ran GrouPS for 100 iterations. In each iteration, we
used a set of 20 samples, of which ten were randomly selected from the set of 20 in the previous iteration and ten
generated by the current policy. We used ten best samples
out of this set of 20 for computing the new policy parameters. The groups were created in such a manner that the joints
of each arm or leg form a single group as well as the joints of
the head. The results are given in Fig. 7, where we ﬁnd that
the GrouPS algorithm is able to ﬁnd a satisfactory solution
even with a relatively small number of samples: the head and
left leg of the NAO robot are at high positions corresponding
to a high reward.

120
100
80
60
40
20
0

0

200

400

600

800

1000

Iterations

Figure 6: Comparison between the original grouping and
two other variants with a different splitting point. Again, the
values represent the summarized distances and shaded ares
corresponds to the standard deviation given ten executions.

prove the search. Conversely, if we provide wrong information about groupings the performance of the algorithm should deteriorate. To evaluate this hypothesis,
we took the original partitioning of the joints into four
groups and swapped two, later three pairs of joints randomly. As described above, the original group partitioning
is {(1l, 2l, 3l, 4l), (5l, 6l), (1r, 2r, 3r, 4r), (5r, 6r)}.
Performing two random swaps between the left and
right side results in {(1l, 2l, 2r, 4l), (5l, 5r), (1r, 3l, 3r, 4r)
, (6l, 6r)} (Fig. 6, Swap4). For three swaps the resulting partition is {(1l, 6r, 2r, 4l), (3r, 6l), (1r, 3l, 5l, 4r), (5r, 2l)}
(Fig. 6, Swap5). Furthermore, three other groupings with
different splitting points were evaluated: {(1l, 2l),
(3l, 4l, 5l, 6l), (1r, 2r), (3r, 4r, 5r, 6r)} (Fig. 5, Swap1),
{(1l, 2l), (3l, 4l), (5l, 6l), (1r, 2r), (3r, 4r), (5r, 6r)} (Fig.
5, Swap2) and {(1l, 2l, 3l), (4l, 5l, 6l), (1r, 2r, 3r),
(4r, 5r, 6r)} (Fig. 5, Swap3). The result of executing
GrouPS with these groupings can be seen in Fig. 5 and

Conclusion and Future Work
In this paper, we introduced a novel algorithm for reinforcement learning in low-dimensional latent spaces. To this
end, we derived a Variational Inference framework for policy search that takes prior structural information into account. The resulting policy search algorithm can efﬁciently
learn new policy parameters, while also uncovering the underlying latent space of solutions, and incorporating prior

1917

knowledge about groups of correlated parameters. In experiments using motor skill learning tasks, we showed that the
introduced GrouPS algorithm efﬁciently learns new motor
skills. It signiﬁcantly outperformed state-of-the-art policy
search methods, whenever prior information about structural
groups was provided.
So far, the dimensionality of the latent space needs to be
provided as a parameter to the reinforcement learning algorithm. We plan to investigate automatic adjustments of the
dimensionality using current rewards. In this paper, we focused on intra-group correlations. In future work, we plan to
investigate correlations among extracted group factors, e.g.,
correlations between arms and legs.

Neumann, G. 2011. Variational inference for policy search
in changing situations. In Proceedings of the 28th International Conference on Machine Learning (ICML), 817–824.
Peters, J., and Schaal, S. 2008. Natural actor-critic. Neurocomputing 71(7):1180–1190.
Peters, J.; Mülling, K.; Kober, J.; Nguyen-Tuong, D.; and
Krömer, O. 2011. Towards motor skill learning for robotics.
In Robotics Research. Springer. 469–482.
Rohmer, E.; Singh, S. P.; and Freese, M. 2013. V-REP: A
versatile and scalable robot simulation framework. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 1321–1326. IEEE.
Sagduyu, Y. E., and Ephremides, A. 2004. The problem of
medium access control in wireless sensor networks. IEEE
Wireless Communications 11(6):44–53.
Santello, M.; Flanders, M.; and Soechting, J. 1998. Postural
hand synergies for tool use. The Journal of Neuroscience
18(23).
Torres-Oviedo, G., and Ting, L. H. 2010. Subject-speciﬁc
muscle synergies in human balance control are consistent
across different biomechanical contexts. Journal of Neurophysiology 103(6):3084–3098.
Toussaint, M., and Storkey, A. 2006. Probabilistic inference
for solving discrete and continuous state Markov Decision
Processes. In Proceedings of the 23rd International Conference on Machine Learning (ICML), 945–952.
Toussaint, M. 2009. Robot trajectory optimization using
approximate inference. In Proceedings of the 26th annual
International Conference on Machine Learning (ICML),
1049–1056. ACM.
van de Meent, J.-W.; Tolpin, D.; Paige, B.; and Wood, F.
2015. Black-box policy search with probabilistic programs.
arXiv preprint arXiv:1507.04635.
Wang, X.; O’Dwyer, N.; and Halaki, M. 2013. A review on
the coordinative structure of human walking and the application of principal component analysis. Neural Regeneration
Research 8(7):662–670.

Acknowledgments
J.Pajarinen and V.Kyrki were supported by the Academy of
Finland, decision 271394.

References
Bernstein, N. A. 1967. The co-ordination and regulation of
movements. Pergamon Press.
Bishop, C. M. 2006. Pattern recognition and machine learning. Springer.
Bitzer, S.; Howard, M.; and Vijayakumar, S. 2010. Using
dimensionality reduction to exploit constraints in reinforcement learning. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
3219–3225. IEEE.
Dempster, A. P.; Laird, N. M.; and Rubin, D. B. 1977.
Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society. Series B
(Methodological) 39(1):1–38.
Gouaillier, D.; Hugel, V.; Blazevic, P.; Kilner, C.; Monceaux, J.; Lafourcade, P.; Marnier, B.; Serre, J.; and Maisonnier, B. 2008. The nao humanoid: a combination of performance and affordability. arXiv preprint arXiv:0807.3223.
Harman, H. H. 1976. Modern factor analysis. University of
Chicago Press.
Klami, A.; Virtanen, S.; Leppaaho, E.; and Kaski, S. 2015.
Group factor analysis. IEEE Transactions on Neural Networks and Learning Systems 26(9):2136–2147.
Kober, J., and Peters, J. 2009. Policy search for motor primitives in robotics. In Advances in Neural Information Processing Systems (NIPS), 849–856.
Kober, J., and Peters, J. 2011. Policy search for motor primitives in robotics. Machine Learning 84(1):171–203.
Kolter, J. Z., and Ng, A. Y. 2007. Learning omnidirectional
path following using dimensionality reduction. In Proceedings of the Robotics: Science and Systems (R:SS) conference. The MIT Press.
Luck, K. S.; Neumann, G.; Berger, E.; Peters, J.; and
Ben Amor, H. 2014. Latent space policy search for
robotics. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 1434–
1440. IEEE.

1918

2014 14th IEEE-RAS International Conference on
Humanoid Robots (Humanoids)
November 18-20, 2014. Madrid, Spain

Learning Interaction for Collaborative Tasks with Probabilistic
Movement Primitives
Guilherme Maeda1 , Marco Ewerton1 , Rudolf Lioutikov1 , Heni Ben Amor2 , Jan Peters1,3 , Gerhard Neumann1

Abstract— This paper proposes a probabilistic framework
based on movement primitives for robots that work in collaboration with a human coworker. Since the human coworker
can execute a variety of unforeseen tasks a requirement of our
system is that the robot assistant must be able to adapt and
learn new skills on-demand, without the need of an expert
programmer. Thus, this paper leverages on the framework
of imitation learning and its application to human-robot interaction using the concept of Interaction Primitives (IPs).
We introduce the use of Probabilistic Movement Primitives
(ProMPs) to devise an interaction method that both recognizes
the action of a human and generates the appropriate movement
primitive of the robot assistant. We evaluate our method
on experiments using a lightweight arm interacting with a
human partner and also using motion capture trajectories of
two humans assembling a box. The advantages of ProMPs in
relation to the original formulation for interaction are exposed
and compared.

Fig. 1. Illustration of two collaborative tasks where a semi-autonomous
robot helps a worker assembling a box. The robot must predict what is
the action to execute, to hand over the screw driver or to hold the box. Its
movement must also be coordinated relative to the location at which the
human worker executes the task.

I. I NTRODUCTION

to learn the interaction and to adapt to a variety of unforeseen
tasks without the need of an expert programmer.
Motivated by the described scenario, this work proposes
the use of imitation learning [1] in the context of collaboration. Imitation learning has been widely used as a method to
overcome the expensive programming of autonomous robots.
Only recently, however, its application for physical interaction has been introduced under the concept of Interaction
Primitives (IP) by Lee at al. in [2], defined as skills that
allow robots to engage in collaborative activities with a
human partner by Ben Amor et al. in [3].
Leveraging on the framework of [3], our approach is based
on probabilistically modeling the interaction using a distribution of observed trajectories. We propose using Probabilistic
Movement Primitives (ProMPs) [4] for modeling such a
distribution. In a manufacturing scenario such a distribution
of trajectories can be obtained by observing how two coworkers assemble a product, several times throughout the
day, providing a rich data set for imitation learning. Such a
collection of trajectories is used to create a prior model of the
interaction in a lower dimensional weight space. The model
is then used to recognize the intention of the observed agent
and to generate the movement primitive of the unobserved
agent given the same observations. The movement primitive
of the unobserved agent can then be used to control a robot
assistant.
The main contribution of this paper is the introduction
of the Probabilistic Movement Primitives [4] in the context
of imitation learning for human-robot interaction and action

While the traditional use of robots is to replace humans in dangerous and repetitive tasks we motivate this
paper by semi-autonomous robots that assist humans. Semiautonomous robots have the ability to physically interact
with the human in order to achieve a task in a collaborative
manner. The assembly of products in factories, the aiding of
the elderly at home, the control of actuated prosthetics, and
the shared control in tele-operated repetitive processes are
just a few examples of application.
Only recently, physical human-robot interaction became
possible due advances in robot design and safe, compliant control. As a consequence, algorithms for collaborative
robots are still in the early stages of development. Assistance
poses a variety of challenges related to the human presence.
For example, Fig. 1 illustrates a robot assistant that helps a
human to assemble a box. The robot must not only predict
what is the most probable action to be executed based on
the observations of the worker (to hand over a screw driver
or to hold the box) but also the robot movement must be
coordinated with the worker movement. Pre-programming a
robot for all possible tasks that a worker may eventually need
assistance with is unfeasible. A robot assistant must be able
1 Intelligent Autonomous Systems Lab, Technische Universitaet Darmstadt, 64289 Darmstadt Germany. Correspondence should be addressed to

maeda@ias.tu-darmstadt.de
2 Institute for Robotics and Intelligent Machines, Georgia Institute of
Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA.
3 Max Planck Institute for Intelligent Systems Spemannstr. 38, 72076
Tuebingen, Germany

978-1-4799-7174-9/14/$31.00 ©2014 IEEE

527

recognition. We will show how Interaction ProMPs can
be applied to address the three main problems previously
illustrated in Fig. 1, that is: (a) learning a collaborative model
by imitation learning and thus avoiding expert programming,
(b) the ability to recognize a task by observing the worker,
and (c) the coordination of the assistant movement in relation
to the worker movement. We also show the advantages of
ProMPs over the original DMP-based framework [3], and
present an algorithm for aligning data using local optimization in order to avoid the issue of slope constraints typical
of dynamic time warping.

in classifying interactions. However, this often requires a
substantial set of training data. In particular for humanoid
motion generation with many degrees-of-freedom, it is often
challenging to acquire sufficiently large and general data sets.
For more efficient learning and generalization, various
authors investigated the projection of the original trajectories into a new, low-dimensional space where correlations
between the agents are easier to unravel. Llorens et al. [11]
show how such a low-dimensional interaction space can
be used to implement an assistive robot arm. Similarly in
[7], probabilistic principal component analysis is used to
find a shared latent space. Dynamic Movement Primitives
(DMPs) [12] allows for a low-dimensional, adaptive representation of a trajectory. The general idea is to encode a
recorded trajectory as dynamical systems, which can be used
to generate different variations of the original movement. In
the context of interaction, Prada et al. [13] present a modified
version of DMPs, that adapts the trajectory of one agent to a
time-varying goal. By setting the goal to the wrist of another
agent, the method can be used to generate handover motions.
Although graphical models and HMMs have been successfully used for action and intention recognition in a
discretized symbolic level, the generation of trajectories
for the continuous dynamic control of the robot is usually
addressed by a different level of representation (e.g. a lowerlevel HMM [6] or DMPs). In relation to the previously
cited works, here, we propose a framework based solely
on a continuous movement representation that is used to
both recognize actions and generate trajectories in the form
of movement primitives; mainly leveraging on DMP-based
Interaction Primitives [3] and Probabilistic Movement Primitives (ProMPs) [4]. By using ProMPs rather than DMPs our
prosed method naturally correlates different agents directly
in the same space in which observations are made, since
observations of a task are usually given by their trajectories.
This is an advantage in relation to the original framework
of [3] since the representation of collaboration in the space
of accelerations/forces due to the use of DMPs obfuscates
the algorithm and increases its sensitivity to noise in the
observations.

II. R ELATED W ORK
The data-driven analysis and classification of interactions
between multiple persons has long been addressed within the
computer vision community. In particular visual surveillance
tasks, e.g., tracking of multiple pedestrians, require methods
for identifying the occurrence and type of person-to-person
interactions. In a seminal paper, Oliver et al. [5] show
that hidden Markov models (HMMs), and more generally
graphical models, are suited for representing the mutual
dependencies of the behaviors between interacting agents.
Graphical models have gained popularity in the field of
human-robot interaction as they naturally include temporal
information into the inference process and the Bayesian
semantics provides a simple way to encode prior knowledge.
In [6], Lee et al. use a hierarchical HMM to learn and
represent responsive robot behaviors. In their approach, a
high-level HMM identifies the current state of the interaction and triggers low-level HMMs which correspond to
the robot’s motor primitives. In order to ensure that the
robot adapts to the movement of the human partner, virtual
springs are attached between markers on the human body and
corresponding positions on the robot. In a similar vein, Ben
Amor et al. [7] use a path-map HMM to model interactions
in cooperative tasks. In their approach, a backbone of shared
hidden states correlates the actions of the interacting agents.
Tanaka et al. [8] use a Markov model to predict the
positions of a worker in an assembly line. The space in
which the worker moves is discretized into different regions.
A Gaussian mixture model relates positions to procedures.
Using this information a robot, then, delivers tools and
parts to a human worker along the assembly line. Besides
HMMs, other probabilistic graphical models have also been
used to address interaction tasks. Koppula et al. [9] use a
conditional random field with sub-activities, human poses,
object affordances and object locations over time. Inference
on the graphical model, allows a robot to anticipate human
activity and choose a corresponding, preprogrammed robot
response. Wang et al. [10] propose the intention-driven
dynamics model, which models human intentions as latent
states in graphical model. Intentions can be modeled as
discrete variables, e.g., action labels, or continuous variables,
e.g., an object’s final position. The transitions between latent
states and the mapping from latent states to observations
are modeled via Gaussian Processes. As evidenced by
these works, graphical models can be very powerful tools

III. P ROPOSED M ETHOD
This section briefly introduces Probabilistic Movement
Primitives for a single degree of freedom as presented in [4]
and proposes its extension for interaction and collaboration.
Although not covered in this work, in its original proposition,
the design of a feedback controller that tracks the distribution
of trajectories is also part of ProMPs and the interested
reader is referred to [4] for details; here we assume the
existence of a human-safe standard feedback controller such
as a low-gain PD controller. This section also exposes the
main characteristics of the interaction framework based on
DMPs in [3] and its relation to the approach of this paper.
Finally, a simple local optimization algorithm is proposed
for aligning several demonstrations provided by a human.
528

where w̄d is the augmented weight vector corresponding
to the d-th demonstration, wp is the n-dimensional column
vector of weights of the p-th DOF of the observed agent, and
wq is the vector of weights of the q-th DOF of the controlled
agent. The mean and covariance are then computed by
stacking all demonstration weights

A. ProMPs for a Single DOF
For the purposes of the following derivations we generically refer to each joint or Cartesian coordinates of a human
or robot simply as a degree of freedom (DOF) with position
q and velocity q̇. Starting with the case of a single DOF, we
denote y(t) = [q(t) q̇(t)]T and a trajectory as a sequence
τ = {y(t)}t=0,...T . We adopt linear regression with n
Gaussian basis functions ψ. The state vector y(t) can then
be represented by a n-dimensional column vector of weights
w as
 


ψ(t)
q(t)
=
y(t) =
w + y ,
(1)
q̇(t)
ψ̇(t)

µw = mean([w̄1 , ..., w̄d , , ..., w̄D ]T ),
Σw = Cov([w̄1 , ..., w̄d , , ..., w̄D ]T ),

where D is the number of demonstrations.
Gaussian conditioning can then be applied on-line as each
new observation is made using recursive updates in the form
−
∗
T −
µ+
w = µw + K(y (t) − Ht µw )

where Ψt = [ψ(t), ψ̇(t)]T is a 2×n dimensional timedependent basis matrix and y ∼N (0, Σy ) is zero-mean i.i.d.
Gaussian noise. The probability of observing the whole
trajectory is then
p(τ |w) =

T
Y

N (y(t)|Ψt w, Σy ).

(5)

−
T −
Σ+
w = Σw − K(Ht Σw )

(6)

T
∗
T +
−1
K = Σ−
,
w Ht (Σy + Ht Σw Ht )

where K is the Kalman gain matrix, y ∗ (t) is the observed
value at time t, Σ∗y is the measurement noise, and the upperscripts − and + the values before and after the update. The
observation matrix Ht is block diagonal and each diagonal
entry contains the 2×n basis [ψ(t), ψ̇(t)]T for each observed
joint


Ψt . . . 0

.. 
..
(7)
Ht =  ...
.
. 

(2)

0

Similar to DMPs the speed of the execution of the movement is decoupled from the speed of the original trajectory
by using a phase variable z(t). The phase variable replaces
the time in order to control the location of the basis functions
with ψ(z(t)). For simplicity we will use z(t) = t such that
ψ(t) = ψ(z(t)) while remembering that any monotonically
increasing function can be used [4].
Each trajectory is now represented by a low-dimensional
vector w since the number of basis is usually much smaller
than the number of time steps. Trajectory variations obtained
by different demonstrations are captured by defining the
distribution over the weights p(w|θ), where θ is the learning
parameter. The probability of the trajectory becomes
Z
p(τ |θ) = p(τ |w)p(w|θ)dw.
(3)

0

...

Ψt

In the collaboration case only measurements of the observed agent are provided. By maintaining consistency with
definition (4) where the entries of the observed agent
come before the controlled agent, the mean is then µw =
[µow µcw ]T and the observation matrix Ht is partitioned as


(Ψot )(1,1)
0
0
0



0
(Ψot )(P,P )
0
0 


Ht = 
 (8)
0 
0
0
0c(1,1)



0
0
0
0c(Q,Q)

So far θ captures the correlation among the weights within
the trajectory and between demonstrations of the same DOF.

where each zero entry is of 2×n dimension. Note that if only
positions of the observed agent are provided (Ψot )(p,p) =
[ψ(t), 0(t)]T .
In general, since (6) is a full state linear estimator, any
partial combination of observations (for example when y ∗
only contains positions, or velocities, or a mixture of both)
provides the optimal estimate of states µ+
w and their uncertainty Σ+
.
w

B. ProMPs for Collaboration
The key aspect for the realization of the interaction primitives is the introduction of a parameter θ that captures the
correlation of all DOFs of multiple agents. Assuming that
the distribution of trajectories of different agents is normal,
then p(w; θ) = N (w|µw , Σw ). Under this assumption we
redefine the vector of weights w to account for all degrees of
freedom of multiple-agents. Following the definitions in [3]
we will refer to the assisted human as the observed agent,
and assume that he/she provides the observed DOFs of the
model (e.g by motion capture). The robot will be referred to
as the controlled agent.
For an observed agent with P DOFs and a controlled
agent with Q DOFs, we construct a row weight vector by
concatenating the trajectory weights

C. Action Recognition for Primitive Activation
Here we use the ProMP framework in a multi-task scenario
where each task is one encoded by one interaction primitive.
Consider a specific task s ∈ {1, .., K} and assume that
for each task an Interaction ProMP has been generated as it
was proposed in section III-B. Using the recursive notation
of (6), the upper script (·)− refers to the parameters of the
Interaction ProMP updated up to the previous observation,
−
that is θs− = {µ−
w , Σw }s . The probability of the observation

T c
w̄d = {[w1T , ...wpT , ..., wPT ]o , [w1T , ...wqT , ..., wQ
] } (4)

529

at a subsequent time t given the parameters θs− of one of
the tasks is
Z
p(y ∗ (t); θs− ) =
p(y ∗ (t)|Ψt w, Σ∗y )p(w|θs− )dw (9)

E. Time Warping with Local Optimization
One issue of imitation learning for trajectories is that
multiple demonstrations provided by humans are usually,
sometimes severely, warped in time. Demonstrations must
be unwarped or time-aligned before the distribution of the
weights can be computed. Here we propose aligning trajectories by taking one of the demonstrations as a reference yr
and using local optimization of the time warping function
with
tj+1
(15)
= v0j + g(v j )tjw ,
w

−
∗
= N (y ∗ (t)|Ψt µ−
w , Ψt Σw Ψt + Σy ). (10)

The task s can now be recognized by applying Bayes rule
p(y ∗ (t)|θs− )p(s)
,
p(s|y ∗ (t)) = PK
−
∗
k=1 p(y (t)|θk )p(k)

(11)

where tjw represents a vector containing the warped time of
demonstration yw at the j-th iteration of the optimization.
We propose g as a smooth, linear Gaussian-basis-model
with P weights v j = [v1j , ..., vPj ] as the parameters to be
optimized. The extra parameter v0j is used to shift the time
which is useful when the reference and warped trajectories
are, in fact, identical but start at different instants. The
optimization is initialized with v0j = 0 and tjw = tr for
j = 1. The parameters v j are optimized with gradient descent
to decrease the absolute cumulative distance between the
reference and warped trajectories

where p(s) is the initial probability of the task (e.g. p(s) =
1/K for uniform distribution). We will evaluate Eqs. (9)-(11)
using real collaboration data in the experimental section of
this paper.
D. Relation to Interaction DMPs
It is now straightforward to relate our proposed method
with the previous interaction primitives based on DMPs
[3]. The principal difference is that in the framework of
interaction DMPs the weights are mapped from the forcing
function f (t) as opposed to the positions q(t). Using the
linear basis-function model
f (t) = ψ(t)T w,

v = arg min
v

(12)

(16)

k=0

(13)
IV. E XPERIMENTS

where g is the goal attractor, αy , βy are user-defined parameters that characterize the spring-damper behavior and τ
controls the speed of execution. For details on DMPs please
refer to [12] and references therein.
When using imitation learning a demonstration is executed
and measurements are usually given in the form of positions,
which must be differentiated twice such that the forcing
function can be computed
f (t) = q̈/τ 2 − αy (βy (g − q) − q̇/τ ).

|yr (tr (k)) − yw (v0j + g(v j )tjw )|.

While Dynamic Time Warping (DTW) [14] is widely
used for such problems, our local method forces alignment
without “jumping” the indexes of the warped time vector
which is an usual outcome of DTW and renders unrealistic
and non-smooth trajectories. While this problem is usually
minimized by imposing a slope constraint [14], the use
of a smooth function g not only avoids the tunning of
this parameter but also preserves the overall shape of the
trajectory.

where ψ(t) are the normalized Gaussian basis functions.
Similarly to the ProMP case a distribution of weights p(w)
is learned based on several demonstrations of a task.
For each DOF, the forcing function adds a nonlinear
behavior on the movement which complements a linear and
stable spring-damper system
q̈ = [αy (βy (g − q) − q̇/τ ) + f (t)]τ 2 ,

K
X

This section presents results on a simple simulated scenario to compare the differences between the original work
of Interaction DMPs with Interaction ProMPs. Next, we
evaluate the accuracy of Interaction ProMPs for generating
reference trajectories for an anthropomorphic robot arm
conditioned on the movement of a human. Finally, we will
show experimental results with Interaction ProMPs used in
a collaborative scenario of a box assembly to both recognize
and predict the action of two collaborators.

(14)

A. Comparison with Interaction DMPs

Referring back to (6) the Gaussian conditioning is now
based on the observation of forces or accelerations, that
is y ∗ (t) = f (q̈, (·), t)∗ . As our evaluations will show,
the fact that forces are usually computed using second
derivatives of the position can be restrictive for applications
with asynchronous or sparse measurements as the observed
accelerations needed for conditioning are hard to obtain in
this case. In contrast, in the ProMP framework, it is possible
to directly condition on the observed quantities, i.e., the
position of the agent.

In a typical interaction task the observations of a coworker
might arrive asynchronously, at irregular periods of time, for
example, when the measurement signal is prone to interruption (a typical case is occlusion in motion capture systems).
Fig. 2 (a) illustrates a simple case where both observed and
controlled agents have a single joint each. The training data
was created by sketching two sets of trajectories on a PC
screen using a computer mouse. We than use these two sets
as proxies of the observed and controlled agents resulting
on the initial distribution of trajectories (in blue). The upper
530

Current prediction

Initial distribution

Agent A (Observed)

0.8

0.8

0.6

0.6

X amplitude

X amplitude

Agent A (Observed)

0.4
0.2
0

Sparse observations

-0.2
0.5

1

1.5
2
2.5
Time (s)
Agent B (Controlled)

0.4
0.2
0
0

3

0.8

0.8

0.6

0.6

0.4
0.2
0

Test

Noisy observation

-0.2

X amplitude

X amplitude

0

Training

0.5

1

1.5
2
2.5
Time (s)
Agent B (Controlled)

3

0.4

(a)

0.2
0

-0.2

-0.2
0

0.5

1

1.5
2
Time (s)

2.5

0

3

0.5

1

1.5
2
Time (s)

2.5

Z

3

(b)

(a)

Y
Fig. 2. Two scenarios where the Interaction ProMPs are advantageous
over Interaction DMPs. (a) Sparse and asynchronous observations. (b) Noisy
stream of observed data (σ 2 = 0.04). The patches represent the ± 2σ
deviations from the mean.

0.14

0.14

0.12

0.12

0.1
0.08
0.06

0.06
0.04
0.02

40
60
80
100
Observed trajectory (%)

Fig. 4. An interactive task where the robot has to point at the same position
previously pointed by the human. The robot, however, has no exteroceptive
sensors and its predicted trajectory is based solely on the correlation with
the observed human movement. (a) The nine positions used to create the
Interaction ProMP (dot markers) and the extra nine positions used to test
the method (cross markers). (b) An example where the human points at the
test position #1 and the robot points to the same position.

0.1

0.02

20

var:0
var:0.01
var:0.02
var:0.03
var:0.04

0.08

0.04

0

(b)

Interaction ProMP
0.16

RMS prediction error

RMS prediction error

Interaction DMP
0.16

0

20

40
60
80
100
Observed trajectory (%)

avoided with ProMPs.
Fig. 3 compares the prediction error over the whole trajectory of Interaction DMPs and ProMPs given the same noisy
observed data. With DMPs the error is greatly influenced by
the amount of noise while ProMPs show much less sensitivity. For the case where the full trajectory of collaborator
A is observed (indicated by the arrow) the prediction error
increased by a factor of five times using the Interaction
DMPs when noise ranged from a clean signal to a signal
of noise variance 0.04. In contrast, the error deteriorates by
a factor of two with Interaction ProMPs.

Fig. 3. Root-mean-square prediction error of the movement of collaborator
B as a function of the number of observed samples of the trajectory of
collaborator A. The different bars indicate the amount of noise added to the
observation of the position of collaborator A.

plot shows the prediction (in green) of the observed agent
after observing four measurements. Note that following (4)
the predicted mean µ+
w has the dimension of the augmented
weight vector, that is, if each single-DOF agent trajectory
is encoded by n basis functions µ+
w is a column vector of
size 2n. The bottom figure depicts the predicted distribution
of the controlled agent. Note that the same experiment can
not be reproduced with Interaction DMPs as the second
derivative on such sparse measurement is hard to compute
and introduce innacuracies on the representation of the true
force.
In Fig. 2 (b) the ProMP is being conditioned on a constant
synchronous stream of noisy position measurements. The
plot shows the case where the true trajectory is corrupted
with a Gaussian noise with variance σ 2 = 0.04. Interaction
DMPs suffer from noisy position measurements as the observation must be differentiated twice to compute the forcing
function. While low-pass filters alleviate this problem, the
introduction of phase lag is an issue that can be potentially

B. Robot Control with Interaction ProMPs
We evaluated the ability of Interaction ProMPs in generating the appropriate movement primitive for controlling
a robot based on observations of a human partner. The
experiment consisted in measuring the [x, y, z] trajectory
coordinates of the wrist of an observed agent via motion
capture1 while pointing at a certain position on a table
placed in front of our robot (a 7-DOF anthropomorphic
arm with a 5-finger hand). Then, the robot was moved in
gravity compensation mode to point with its index finger
at the same position on the table while its joint positions
were being recorded (kinesthetic teaching). This pair of
1 All human positions were measured in relation to the world reference
frame located at the torso of the robot (as shown in Fig. 4(b)).

531

60

q2(deg)

q1(deg)

Robot

40
20
0
-20
0

-0.2

3

0

1

2
Time (s)

4
Time (s)

6

1

2
Time (s)

120

0

70

100

-20

80

-40

60
50
40

20
0

60
40

2

4
Time (s)

6

20
0

4
Time (s)

6

2
Time (s)

110 40
100 20

-60

90 0

-100
0

2

4
Time (s)

6

-0.2

3

120 60

-80
2

1

z (m)
0

1

2 2 4 4 6 6
Time
Time
(s) (s)

-0.3

2
Time (s)

3

-0.4

0

1

-22 80

56.99120

50 0

-24 70

56.985100

45-20

-26 60

56.98 80

40-40

-28 50
-30 40

80-20
0 0

-0.25

-0.35

-0.1
0

3

0.1
0

0.65
0

80

30
2

-0.4

3

-0.2

0.2

-32 30
0 0

56.975 60
56.97 40

2 2 4 4 6 6
Time
Time
(s) (s)

56.965 20
0 0

2
Time (s)

3

y(m)
q5(deg)

2
Time (s)

q4(deg)

1

q3(deg)

0

0.8
0.75
0.7

-0.35

-0.1

0.65

-0.3

0.3

x(m)
q4(deg)

0

0.4

0.9
0.85

Predicted robot trajectory

q3(deg)

0.1

0.7

-0.25

0.95

Observed
-0.15

q7(deg)

0.2

z (m)

0.8
0.75

-0.2

y (m)

0.3

0.5

q2(deg)

0.4

0.9

Prediction

1

q6(deg)

0.95

Initial distribution

-0.15

x (m)

0.5

Predicted robot trajectory

q5(deg)
q1(deg)

1

0.85

Observed

Prediction

y (m)

x (m)

Human

Initial distribution

-10

35-60

-15

30-80
-100
25
0 0

2 2 4 4 6 6
Time
Time
(s) (s)

-20
2 2 4 4 6 6
Time
Time
(s) (s)

-25

(b)

(a)

Fig. 5. The upper row shows the human Cartesian coordinates of the wrist. The bottom row shows the first four joints of the 7-DOF robotic arm. (a)
Conditioned results of the test position #6. (b) Conditioned results of the test position #8.

XY pointing error (cm)

trajectories formed by the Cartesian positions of the wrist
and the joint positions of the arm where then mapped into
the weight space and concatenated as in Eq. (4). In total,
nine different positions were pointed to collect training data,
sparsely covering an approximate circular area of diameter
30 cm. The pointed positions are shown in Fig. 4(a) by the
dots.
After creating the Interaction ProMPs, as described in
Section III-B, we defined extra nine marked positions shown
in Fig. 4(a) by the crosses. The human then pointed at one
of the crosses while motion capture was used to measure the
trajectory of the wrist. These observations were then used
to condition the Interaction ProMP to predict trajectories for
each joint of the robot, whose mean values where used as
reference for a standard trajectory tracking inverse dynamics
feedback controller with low gains.
Fig. 4(b) shows one example of the interactive task where
the human pointed to the position marked by the cross #1,
which was not part of the training; the robot was capable of
pointing to the same position. Note that the robot was not
provided with any exteroceptive feedback, such as cameras,
to reveal the location of the pointed position. Although
the robot was not directly “aware” of the position of the
pointed cross, the interaction primitive provides the robot the
capability to predict what movement to make based solely
on the observed trajectories of the partner.
Figure 5 shows two examples on the conditioned interaction primitives when the human pointed at positions #6 in (a)
and #8 in (b) (refer back to Fig. 4(a) for the physical position
of the crosses). The first row in each subplot shows the
[x, y, z] coordinates of the wrist. The second row shows the
first four joints of the robot, starting from the shoulder joint.
Since we are only interested in the final pointing position, the
interaction primitive was conditioned on the final measurements of the wrist position. As positions #6 and #8 were
physically distant from each other, the difference between
their predicted trajectories were quite large in relation to
each other, roughly covering the whole span of the prior

3
2.5
2
1.5
1
0.5
0

0

1

2

3

4
5
6
Test number (#)
(Cross markers)

7

8

9

10

Fig. 6. Accuracy of the pointed positions by the robot when using the test
positions given by the cross markers. The error was computed by taking the
actual pointed position and the true position of the markers on the table.

distribution (in blue) for some certain DOFs of the arm.
Figure 6 shows the distance error on the plane of the
table between the position pointed by the robot and its true
position. The robot was able to reasonably point even at
locations at the limits of the training data such as position
#1, #7, and #8 (see Fig. 4). The maximum error was of
3 cm, or 10% in relation to the total area covered by the
training points (approximately a circle of diameter 30 cm).
The experiments show that the physical movement of the
robot is clearly conditioned by the position indicated by the
human (see the accompanying video2 ).
Note that this not a precision positioning experiment; the
markers on the wrist were not fixed in a rigid, repeatable
manner, neither the finger of the robot could be positioned
with milimetric precision during the kinesthetic teaching
phase. The framework of Interaction ProMPs allows, however, to seamlessly integrate additional sensing to increase
accuracy in precision tasks. This is naturally achieved adjusting the observation vector y ∗ in (6) and the zero entries in
(8) to include new sensory information such as the reference
position of a hole in which the robot must insert a peg.
C. Action Recognition for Primitive Activation
While in the previous experiments interaction primitives
were evaluated for the case of a single task, here we show
2 also

532

available from http://youtu.be/2Ok6KQQQDNQ

Agent A

Agent B

Agent A

Agent A

Agent B

Agent B

Screwing
Hand over

Start
Start
Holdiing box

Start

Start

(a) Hand over

Reaching box

Pick screw driver

Grasping plate

Box flipped

Pick screw

Start

Start

(c) Plate insertion

(b) Fastening

Fig. 7. Three collaborative tasks involved when assembling a box by two co-workers. From left to right, the photos show the hand over of a screw, the
fastening of the screw where one agent grasps the screw driver while the other holds the box steadily, and the insertion of a plate, which requires one
agent to flip the box such that the slot becomes accessible to the other agent. The distribution of aligned demonstrations for each task are shown under
their respective photos. The plot shows the covariance in the x-y plane at each corresponding z height.

Current prediction

Initial distribution

how Interaction ProMPs can be used for recognizing the
action of the observed agent and to select the appropriate
desired movement primitive of the controlled agent. This
capability allows the robot to maintain a library of several
tasks encoded as Interaction ProMPs and to activate the
appropriate primitive based on the observation of the current
task.
As shown in the photos of Fig. 7, we collected collaborative data in the form of the Cartesian coordinate positions
of the wrists of two humans assembling a box. As in the
previous experiment of section IV-B, all measurements were
taken in relation to the torso of the robot. The collaborator
on the right plays the role of the observed agent while the
collaborator at the left plays the role of the controlled agent.
The controlled agent will be referred to as the predicted agent
since he/she can not be controlled.
In the ”hand-over” task shown in Fig. 7(a), the observed
agent stretches his hand as a gesture to request a screw.
The predicted agent then grasps a screw sitting on the table
and hand it over to the collaborator. In the ”fastening” task
shown in Fig. 7(b), the observed agent grasps an electrical
screwdriver. The predicted agent reacts by holding the box
firmly while the observed agent fastens the screw. In the
”plate insertion” task shown in Fig. 7(c), the observed agent
grasps the bottom plate of the box. The predicted agent then
flips the box such that the slots to which the plate slides in
are directed towards the observed agent.
Each task is repeated 40 times. All trajectories are aligned
using the method described in Section III-E. The aligned
trajectories are shown in Fig. 7 under their respective photos
as three-dimensional plots for each of the tasks where the
covariance in x-y directions are shown at the corresponding

60

70

55

60

Observed

10
0

40

45

z (cm)

y (cm)

x (cm)

50
50

40

-10
-20

35
30
20
0

-30

30
2

4
Time (s)

6

25
0

2

4
Time (s)

6

-40
0

2

4
Time (s)

6

70

70

60

60

50

50

30

30

20
2
Time (s)

4

0

40

40

20
0

10

z (cm)

80

y (cm)

x (cm)

(a) Interaction model: fastening task

10
0

-10
-20
-30

2
Time (s)

4

-40
0

2
Time (s)

4

(b) Interaction model: hand over task

Fig. 8. Action recognition based on conditioning the movement primitives
of the observed agent. In this example the observations of the fastening task
also overlaps with the primitives of the hand over task.

heights (Z direction) of the movement. Interaction ProMPs
are created for each task using the distribution of aligned
trajectories.
We evaluated action recognition using Eqs. (9)-(11) on
the three presented tasks for box assembly. Fig. 8 shows
one evaluation as an example. Note from the figure that the
majority of observations indicate that the fastening task is
taking place. The last five observations (surrounded by the
ellipse), however, fits both tasks and could be a potential
source of ambiguity in task recognition. Even in those
533

Probability of the task

0.6
0.4
0.2

1
0.8
0.6

Likelihood of the task

0.8

0.8
hand_over
0.6
fastening
0.4
plate_insertion

hand_over
fastening
plate_insertion

fastening
plate_insertion

0.2

0
0
20
40
60
80
0.4
0
20
40
60
80
100
Number
of
observations
0.2
60
80Number
100 of observations
0
Number of observations
0

0

2

4

Probability of the task

(a)

6
8
10
Number of observations

100

12

14

16

VI. ACKNOWLEDGMENTS
The research leading to these results has received funding
from the European Community’s Seventh Framework Programmes (FP7-ICT-2013-10) under grant agreement 610878
(3rdHand) and (FP7-ICT-2009-6) under grant agreement
270327 (ComPLACS). The authors would like to acknowledge Filipe Veiga, Tucker Hermans and Serena Ivaldi for
their assistance during the preparation of this manuscript.

1
0.8
0.6
0.4
0.2
0
0

2

4

(b)

Probability of the task

automatically generate different Interaction ProMPs without
a priori hand labeling of multiple tasks. We are also investigating tasks in which some of the involved DOFs do not
correlate linearly and also when certain tasks do not induce
correlation. The later is especially true for tasks where the
movement of the agents are not related by causality.

6
8
10
Number of observations

12

14

16

R EFERENCES

1
0.8

[1] S. Schaal, “Is imitation learning the route to humanoid robots?” Trends
in cognitive sciences, vol. 3, no. 6, pp. 233–242, 1999.
[2] D. Lee, C. Ott, and Y. Nakamura, “Mimetic communication model
with compliant physical contact in humanhumanoid interaction,” The
International Journal of Robotics Research, vol. 29, no. 13, pp. 1684–
1704, 2010.
[3] H. Ben Amor, G. Neumann, S. Kamthe, O. Kroemer, and J. Peters,
“Interaction primitives for human-robot cooperation tasks,” in Proceedings of 2014 IEEE International Conference on Robotics and
Automation (ICRA), 2014.
[4] A. Paraschos, C. Daniel, J. Peters, and G. Neumann, “Probabilistic
movement primitives,” in Advances in Neural Information Processing
Systems (NIPS), 2013, pp. 2616–2624.
[5] N. Oliver, B. Rosario, and A. Pentland, “A bayesian computer vision
system for modeling human interactions,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 831–843,
Aug 2000.
[6] D. Lee, C. Ott, and Y. Nakamura, “Mimetic communication model
with compliant physical contact in human-humanoid interaction,” Int.
Journal of Robotics Research., vol. 29, no. 13, pp. 1684–1704, Nov.
2010.
[7] H. Ben Amor, D. Vogt, M. Ewerton, E. Berger, B. Jung, and J. Peters,
“Learning responsive robot behavior by imitation,” in Proceedings of
the 2013 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS), 2013, pp. 3257–3264.
[8] Y. Tanaka, J. Kinugawa, Y. Sugahara, and K. Kosuge, “Motion
planning with worker’s trajectory prediction for assembly task partner
robot,” in Proceedings of the 2012 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS). IEEE, 2012, pp. 1525–
1532.
[9] H. S. Koppula and A. Saxena, “Anticipating human activities using
object affordances for reactive robotic response.” in Robotics: Science
and Systems, 2013.
[10] Z. Wang, K. Mülling, M. P. Deisenroth, H. B. Amor, D. Vogt,
B. Schölkopf, and J. Peters, “Probabilistic movement modeling for
intention inference in human–robot interaction,” The International
Journal of Robotics Research, vol. 32, no. 7, pp. 841–858, 2013.
[11] B. Llorens-Bonilla and H. H. Asada, “A robot on the shoulder:
Coordinated human-wearable robot control using coloured petri nets
and partial least squares predictions,” in Proceedings of the 2014 IEEE
International Conference on Robotics and Automation, 2014.
[12] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal,
“Dynamical movement primitives: learning attractor models for motor
behaviors,” Neural computation, vol. 25, no. 2, pp. 328–373, 2013.
[13] M. Prada, A. Remazeilles, A. Koene, and S. Endo, “Dynamic movement primitives for human-robot interaction: comparison with human
behavioral observation,” in Proceedings of IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), 2013, pp. 1168–
1175.
[14] H. Sakoe and S. Chiba, “Dynamic programming algorithm optimization for spoken word recognition,” Acoustics, Speech and Signal
Processing, IEEE Transactions on, vol. 26, no. 1, pp. 43–49, 1978.

0.6
0.4
0.2
0
0

(c)

2

4

6
8
10
Number of observations

12

14

16

Fig. 9. Action recognition given three different Interaction ProMPs, one for
each task involved in assembling the box. The three Interaction ProMPs are
conditioned on the same observations of the observed agent. Probabilities
of tasks are shown as a function of the number of observations along the
trajectory of the observed agent. (a) Recognition of the hand over task. (b)
Recognition of the fastening task. (c) Recognition of the plate insertion task.

cases, ProMPs can clearly distinguish among tasks as shown
by the plots in Fig. 9 where the probabilities of the task
are given as a function of the number of observations.
Subplots (a), (b) and (c) show the task recognition for the
fastening, hand over and plant insertion tasks, respectively.
In general, we observed that 3-5 observations are required to
achieve a 100% certainty for each task. (The last part of the
accompanying video shows our method controlling the robot
assistant to assembly a box with recognition of two different
handover tasks).
V. C ONCLUSION
This paper introduced a method for collaboration suited
for new applications using semi-autonomous robots whose
movements must be coordinated with the movements of a
human partner. By leveraging on the original framework of
Interaction Primitives [3] we proposed the use of ProMPs
for the realization of primitives that capture the correlation
between trajectories of multiple agents. This work compared the main differences between DMPs and ProMPs for
interaction and advocates the later for applications where
measurements are noisy and/or prone to interruption. Using
a 7-DOF lightweight arm we evaluated the capability of
Interaction ProMPs in generating the appropriate primitive
for controlling the robot in an interactive task involving
a human partner. We also proposed a method for task
recognition that naturally fits the ProMP framework.
Our current work addresses the use of mixture-models to
534

Identifying Motion Capture Tracking Markers with Self-Organizing Maps
Matthias Weber∗

Heni Ben Amor†

Thomas Alexander‡

FGAN e.V., FKIE, Germany

TU Bergakademie Freiberg, Institute for Informatics, Germany

FGAN e.V., FKIE, Germany

A BSTRACT
Motion Capture (MoCap) describes methods and technologies for
the detection and measurement of human motion in all its intricacies. Most systems use markers to track points on a body. Especially with natural human motion data captured with passive systems (to not hinder the participant) deficiencies like low accuracy
of tracked points or even occluded markers might occur. Additionally, such MoCap data is often unlabeled. In consequence, the system does not provide information about which body landmarks the
points belong to. Self-organizing neural networks, especially selforganizing maps (SOMs), are capable of dealing with such problems. This work describes a method to model, initialize and train
such SOMs to track and label potentially noisy motion capture data.
Index Terms: H.1.2 [Models and Principles]: User/Machine
Systems—Human information processing; I.2.6 [Artificial Intelligence]: Learning—Connectionism and neural nets
1

I NTRODUCTION

Motion Capture (MoCap) enables a user to capture human motion
in all its intricacies. Usually, MoCap systems use markers attached
to anatomical landmarks to track certain points on the body. Active
markers, i.e. markers that emit or receive signals, are big or need
cables and therefore hinder natural motion. As a consequence, passive markers are often used. They are little reflector spheres that
do not disturb the participant. Unfortunately, MoCap data provided
by such passive systems often has deficiencies. Occluded markers
might occur leading to not seen points and therefore lost tracking
in subsequent frames. Additionally, MoCap data is usually unlabeled, i.e. no information about the markers is given. Neural networks are able to deal with such problems as they try to generalize
on the learned data. Those with self-organizing features, e.g. selforganizing maps (SOMs), can already represent a map of linked
structures like a human skeleton. In this work, a method will be
presented how to model, initialize and train such SOMs so that they
adapt to certain poses extracted from potentially noisy motion capture data and identify tracked points.
2

3

A LGORITHM D ESCRIPTION

In order to identify tracked markers and to fit a corresponding skeletal model, the presented algorithm first employs a principle components analysis (PCA) on the captured point cloud. The PCA transforms the data by projecting it onto a set of orthogonal axes indicating the directions of greatest variance in the data. When applied to
a captured standing posture with the arms reaching out, this leads to
a properly aligned and flattened point cloud. It is oriented such that
the arms reach out in the direction of the x axis. The feet and head
are oriented along the y axis. After this, a user-supplied reference
skeleton model is scaled to the extensions of the x and y axis. This
leads to an accurately aligned model with respect to the projected
point cloud in PCA space. The scaled skeleton model is then retransformed into the real-world 3-dimensional space and moved to
the median point of the test person’s point cloud. As a result, the
model is now aligned to the test person’s initial posture.
After this initial step, the SOM is adapted to each frame of the
captured animation. For this, the skeleton structure is considered as
a SOM with joints represented as neurons. To ensure constant segment lengths in this skeleton structure over time, first every joint’s
position is adapted so that the distance to its parent is kept nearly
the same as the distance in the initial scaled skeleton model built via
PCA. This way the skeleton structure stays the same, even though
the SOM might try to converge several joints to one point of attraction.
After preserving the segment lengths, the position of every
tracked marker is presented to the SOM in every training step, with
~x being the training vector. Distances to all prototype vectors ~m
are then computed, using Euclidean distance measure: k~x − ~mb k =
mini {k~x − ~mi k}. The neuron with its prototype closest to ~x is the
winning neuron b.
The prototype vectors are updated according to the following update rule, where t is the current iteration, ~m again is the prototype
vector and ε and σ are the learning rate and learning radius respectively. The e expression on the right side is a neighborhood kernel
centered at the winning neuron vector ~mb .

R ELATED W ORK

Research on Motion Capture is in particular focused on robust
marker tracking and skeleton fitting. For the former problem, [1]
proposed an extended Kalman filter for preprocessing in conjunction with a motion model based on exponential maps for later estimation of the human skeleton configuration from the tracked point
cloud. For the problem of skeleton fitting, many approaches revolve
around fitting a reference skeleton model into the data cloud by using least-squares methods [2]. In [3] global and local techniques
for skeleton fitting are presented. Global techniques consider the
whole skeleton at once, while local techniques perform adaptation
on a limited number of bones.
∗ e-mail:

m.weber@fgan.de
amor@informatik.tu-freiberg.de
‡ e-mail: alexander@fgan.de
† e-mail:

IEEE Virtual Reality 2008
8-12 March, Reno, Nevada, USA
978-1-4244-1971-5/08/$25.00 ©2008 IEEE



~mi (t + 1) = ~mi (t) + ε (~x − ~mi (t)) e

−

k~mb (t)−~mi (t)k2
2σ 2



(1)

This adaptation step moves the winner neuron towards the current
training point with the learning parameters ε and σ . All other
neurons are also adapted towards this point but with other, much
lower learning parameters. Such lower learning parameters seem to
achieve a good compromise between attraction to the current training point and not moving too far away from the point the neuron
normally belongs to.
Above steps are computed several times to properly adapt to a
posture. After adaptation has finished, the neurons share nearly the
same positions as the corresponding markers. Therefore, the markers can be identified and labeled using a simple nearest neighbors
computation. As soon as this step has finished, the process has been
completed for the current data frame. The model is now adapted to
the markers and the markers are labeled according to the neuron
names.

297

4 E VALUATION AND C ONCLUSION
For evaluating the algorithm we performed a set of experiments. A
participant performed several motions: Moving to an initial pose,
performing some arm movement and body movement, doing the
initial pose on several different places with different orientations.
Finally, movement while using a device was captured. This sums
up to 8 datasets with 14736 data entries, overall. First evaluation
revealed that the system captured a minimum of 4 and a maximum
of 21 markers, in average 17.51 markers with a standard deviation
of 2.02. This means that a good average number of markers, 17 to
18, was detected and that there were few outliers because of the low
standard deviation. Nonetheless, there were some bigger outliers,
with a minimum of only 4 markers.
A grid search was performed to investigate the effect of learning
parameters on the neural network. The evaluated parameters were
ε, σ and the parameter l pm which is used to inhibit the learning
capabilities of all non-winner neurons. The grid search performs
the learning algorithm for each set of parameters and records the
performed error. For computing an error the following metric was
used:
v
u
u n 2 1 N
E =t
+ ∑ k~xi −~si k
(2)
maxn
N i
with E being the error, ~xi the neuron vector, ~si the corresponding
sample tracking vector, n the number of iterations and maxn the
maximum possible number of iterations. As can be seen, the distances between the neurons and the tracking markers compose one
part of the error metric. The other part of the error is based on
the number of iterations necessary to achieve the adaptation. This
equation ensures that even if one of the error parts gets low but the
other is still high (like low number of iterations but high distance
error for the neural net), there is still a measurable error.
Figure 1 shows the results of the grid-search for an area that
is most interesting in terms of low error. The l pm parameter describes how much to inhibit non-winner neurons from learning. It
should be in a range from 0.2 to 1.0, from a nearly winner-takesall network (WTA) to a normal network with all neurons learning
the stimulus. For this scenario, an optimum of 0.4 was chosen,
i.e. non-winner neurons do not adapt too much to markers they do
not belong to. Therefore, this value promises a good adaptation on
consecutive frames in an animation.

pecially the PCA algorithm, as it is used here to calculate position
and orientation of the participant, was evaluated. In all cases the algorithm performed very well to find position and orientation. Even
the SOM was trained correctly for every situation.
After that, a part of the data set, that contained only few marker
occlusions (one or two markers occluded for only a few frames)
was used to train and adapt the algorithm. This data was additionally degraded with one marker being occluded for a longer time.
For this scenario, all markers were identified correctly. The addition for preserving the segment lengths proved very well in this
situation. Figure 2 shows a sequence of images for the initialization
and training steps and for some animation steps.

Figure 2: a) The initially set up model and initially trained SOM, b) the
initial model set to the trained SOM, c) and d) the model after some
animation steps.

Finally, the remaining data with a lot of markers being occluded
(every once a while up to only four markers were seen) was used
for adapting the SOM. This data even contained situations where
the whole left arm data was missing for approximately 2 seconds.
This was a very tough condition for the algorithm. Markers just
disappeared and popped up on completely different positions. Unfortunately, the SOM is not very well fitted to track a lot of missing markers over a longer period of time, and, for human motion,
even 2 seconds are a long period of time. During this time, the
SOM tried to adapt to the other markers and completely lost track
on its corresponding, but missing marker. It can even happen that
other neurons adapt strongly to a reappearing marker, because in
the mean-while it moved to their position and they also lost track to
their corresponding markers.
Concluding, our approach is very well suited for situations where
only a few markers (one or two markers on consecutive joints in a
chain) are occluded or if occlusion happens for only a few frames.
It can easily adopt to the posture and identify the markers 100%
correctly. However, for capture data with a lot of occluded markers
(e.g. a complete arm) our approach seems to be overstrained. The
SOM seems to be inappropriate for such situations. Still, we believe
that this approach can help to identify markers for slightly noisy
human motion data.
R EFERENCES

Figure 1: Results of the grid search for l pm = 0.4. A low error is
achieved at ε = 0.049 and σ = 0.03.

The participant was also orientated in arbitrary directions to
check the method’s capabilities to cope with such situations. Es-

298

[1] K. Dorfmüller-Ulhaas. Robust Optical User Motion Tracking Using a
Kalman Filter. Technical Report 2003-6, University of Augsburg, Institut für Informatik, Universitätsstr. 2, D-86159 Augsburg, May 2003.
[2] J. F. O’Brien, R. Bodenheimer, G. Brostow, and J. K. Hodgins. Automatic Joint Parameter Estimation from Magnetic Motion Capture Data.
In Graphics Interface, pages 53 – 60, 2000.
[3] M.-C. Silaghi, R. Plänkers, R. Boulic, P. Fua, and D. Thalmann. Local
and Global Skeleton Fitting Techniques for Optical Motion Capture. In
CAPTECH ’98: Proc. International Workshop on Modelling and Motion Capture Techniques for Virtual Environments, pages 26–40, 1998.

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

1

Occlusion-Aware Object Localization,
Segmentation and Pose Estimation
Samarth Brahmbhatt
http://www.cc.gatech.edu/~sbrahmbh

Heni Ben Amor

School of Interactive Computing
Georgia Institute of Technology
Atlanta, GA USA

http://henibenamor.weebly.com

Henrik Christensen
http://www.cc.gatech.edu/~hic

Abstract
We present a learning approach for localization and segmentation of objects in an
image in a manner that is robust to partial occlusion. Our algorithm produces a bounding
box around the full extent of the object and labels pixels in the interior that belong to the
object. Like existing segmentation aware detection approaches, we learn an appearance
model of the object and consider regions that do not fit this model as potential occlusions.
However, in addition to the established use of pairwise potentials for encouraging local
consistency, we use higher order potentials which capture information at the level of image segments. We also propose an efficient loss function that targets both localization and
segmentation performance. Our algorithm achieves 13.52% segmentation error and 0.81
area under the false-positive per image vs. recall curve on average over the challenging
CMU Kitchen Occlusion Dataset. This is a 42.44% decrease in segmentation error and
a 16.13% increase in localization performance compared to the state-of-the-art. Finally,
we show that the visibility labelling produced by our algorithm can make full 3D pose
estimation from a single image robust to occlusion.

1

Introduction and Related Work

In this paper we address the problem of localizing and segmenting partially occluded objects.
We do this by generating a bounding box around the full extent of the objects, while also segmenting the visible parts inside the box. This is different from semantic segmentation, which
typically does not provide information about the spatial position of labelled pixels inside the
object. While a lot of progress has been made in object detection [9, 13, 21], occlusion by
other objects still remains a challenge. A common theme is to model occlusion geometrically
or appearance-wise, thereby allowing it to contribute to the detection process. Wang et al.
[19] use a holistic Histogram of Oriented Gradients (HOG) template [6] to scan through the
image and use specially trained part templates for instances where some cells of the holistic template respond poorly. Girshick et al. [11] force the Deformable Part Model detector to
place a trained ‘occluder’ part in regions where the original parts respond weakly. The object
masks produced by both of these algorithms are only accurate up to the parts and hence not
usable for many applications e.g. edge-based 3D pose estimation. Xiang and Savarese [20]
c 2015. The copyright of this document resides with its authors.

It may be distributed unchanged freely in print or electronic forms.

Pages 80.1-80.13
DOI: https://dx.doi.org/10.5244/C.29.80

2

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

approximate object structure in 3D using planar parts. A Conditional Random Field (CRF)
is then used to reason about visibility of the parts when the 3D planes are projected to the
image. However, such methods work well only for large objects that can be approximated
with planar parts.
Our approach is entitled Segmentation and Detection using Higher-Order Potentials (SDHOP). It is based on discriminatively learned HOG templates for objects and occlusion.
Whereas the object templates model the objects of interest, the occlusion templates provide
discriminative support and do not model a specific occluder. Segmentation is done by considering not only the response of patches to these templates, but also the segmentation of
neighbouring patches through a CRF with higher-order connections that encompass image
regions.
We will compare our approach to two existing approaches that have been designed to
handle partial occlusion. Hsiao and Hebert [14] approximate all occluders by boxes and generate occlusion hypotheses by finding locations of mismatch between image gradient and
object model gradient. These hypotheses are then validated by the visibility of other points
of the object and by an occlusion prior which assumes all objects rest on the same planar
surface. Our algorithm does not need such assumptions which reduce the segmentation accuracy. Gao et al. [10] learn discriminative appearance models of the object and occlusion
seen during training. Segmentation is achieved by defining a CRF to assign binary labels
to patches based on their response to these two filters. We build on their work but add several important modifications that lead to better localization and segmentation performance.
Firstly, we replace the edge-based pairwise terms with 4-connected pairwise terms that are
better able to propagate visibility relations. Secondly, we introduce the use of higher-order
potentials defined over groups of patches, allowing us to reason at the level of image segments which contain much more information than pairs of patches. We also introduce a
new loss function for structured learning that targets both localization and segmentation performance but is still decomposable over the energy terms. Lastly, we introduce a simple
procedure to convert the granular patch-level object mask produced by the algorithm to a
fine pixel-level mask that can be used to make 3D pose estimation of detected objects robust to partial occlusion. Our algorithm outperforms these approaches (Hsiao and Hebert
[14], Gao et al. [10]) at both object localization and segmentation on the CMU Kitchen
Occlusion dataset as shown in Section 3.
The rest of the paper is organized as follows. Section 2 describes our proposed approach.
We present evaluations on standard datasets and our own laboratory dataset in Section 3 and
summarize in Section 4.

2

Method

The training phase for SD-HOP requires a set of images with different occlusions of the
object(s) of interest. Each training sample is (1) over-segmented and (2) annotated with a
bounding box around the full extent of the object and a binary segmentation of the area
inside the box into object vs. non-object pixels. Given these training images and labels,
we train a structured Support Vector Machine (SVM) that produces the HOG templates and
CRF weights. Figure 1 shows an overview of our approach.
Object segmentation is done by assigning binary labels to all HOG cells within the
bounding box, 1 for visible and 0 for occluded. Instead of making independent decisions
for every cell, we allow neighbouring cells to influence each other. Neighbour influence can

3

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION
Training

Learning

Object 1

Object 2

Labels

Solve Constrained
QP
Feature Extraction

Feature Vector
Find Most
Violated Constraint

Segmentation Pyramid
Testing

Model

HOG Feature Pyramid

Inference

Figure 1: Overview of our approach. Top: During training, images are segmented and features are extracted from pyramids of segmentations and HOG features. An SVM model is
learned by max-margin learning. Bottom: After training, the model can be used to infer a
bounding box and visible segments of the object.
take two forms: (1) pairwise terms (Rother et al. [17]) that impose a cost for 4-connected
neighbours to have different labels and (2) higher-order potentials (Kohli et al. [16]) that impose a cost for cells to have a different label than the dominant label in their segment of the
image. These segments are produced separately by an unsupervised segmentation algorithm.

2.1

Notation

The label for an object in an image x is represented as y = (p, v, a), where p is the bounding
box, v is a vector of binary variables indicating the visibility of HOG cells within p and
a ∈ [1, A] indexes the discrete viewpoint. p = (px , py , pσ ) indicates the position of the top
left corner and the level in a scale-space pyramid. The width and height of the box are
fixed per viewpoint as wa and ha HOG cells respectively. Hence v has wa · ha elements.
All training images are also over-segmented to collect statistics for higher-order potentials.
Any unsupervised algorithm can be used for this, e.g. Felzenszwalb and Huttenlocher [8]
and Arbelaez et al. [2].

2.2

Feature Extraction

Given an image x and a labelling y, a sparse joint feature vector Ψ(x, y) is formed by stacking
A vectors. Each of these vectors has features for a different discretized viewpoint. All vectors
except for the one corresponding to viewpoint a are zeroed out. Below, we describe the
components of this vector.
1. 31-dimensional HOG features are extracted for all cells of 8x8 pixels in p as described
in Felzenszwalb et al. [9]. The feature vector is is constructed by stacking two groups
which are formed by zeroing out different parts, similarly to Vedaldi and Zisserman
[18]. The visible group φv (x, p) has the HOG features zeroed out for cells labelled 0
and the occlusion group φnv (x, p) has them zeroed out for cells labelled 1.
2. Complemented visibility labels, to learn a prior for a cell to be labelled 0: [1wh − v].

4

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

3. Count c(p) of cells in bounding box p lying outside the image boundaries, to learn a
cost for truncation by the image boundary, similarly to Vedaldi and Zisserman [18].
4. Number of 4-connected neighbouring cells in the bounding box that have different
labels, to learn a pairwise cost.
5. Each segment in the bounding box obtained from unsupervised segmentation defines
a clique of cells. To learn higher-order potentials, we need a vector θHOP that captures the distribution of 0/1 label agreement within cliques. A vector θc ∈ RK+1 is
constructed for each clique c as (θc )k = 1 if ∑i∈c vi = k. The sum of all θc within p
gives θHOP . In practice, since cliques do not have the same size we employ the normalization strategy described in Gould [12] and transform statistics of all cliques to a
standard clique size K (K = 4 in our experiments).
6. The constant 1, used to learn a bias term for different viewpoints.

2.3

Learning

Suppose w is a vector of weights for elements of the joint feature vector. We define wT Ψ(x, y)
as the ‘energy’ of the labelling y. The aim of learning is to find w such that the energy of the
correct label is minimum. Hence we define the label predicted by the algorithm as
f (x; w) = y∗ = argmin wT Ψ(x, y)

(1)

y

We use a labelled dataset (xi , yi )Ni=1 and learn w by solving the following constrained Quadratic
Program (QP)
N
1
min kwk2 +C ∑ ξi
(2)
w,ξ 2
i=1
s.t.

wT (Ψ(xi , ŷi ) − Ψ(xi , yi )) + ξi ≥ ∆(yi , ŷi ) ∀i, ŷ ∈ Yi
ξi ≥ 0 ∀i
D2 w ≥ 0

Intuitively this formulation requires that the score wT Ψ(xi , yi ) of any ground truth labelled
image xi must be smaller than the score wT Ψ(xi , ŷi ) of any other labelling ŷi by the distance
between the two labellings ∆(yi , ŷi ) minus the slack variable ξi , where kwk2 and ξi are
minimized. The regularization constant C adjusts the importance of minimizing the slack
variables. The above formulation has exponential constraints for each training image. For
tractability, training is performed by using the cutting plane training algorithm of Joachims
et al. [15] which maintains a working set Yi of most violated constraints (MVCs) for each
image. Gould [12] adapts this algorithm for training higher-order potentials. It uses D2 as a
second order curvature constraint on the K + 1 weights for the higher-order potentials, which
forces them to make a concave lower envelope. This encourages most cells in the image
segments to agree in visibility labelling. D2 is an appropriately 0-padded (to the left and
right) version of


−1 2
1 0 ...
 ..

..
.
..
 .
.
. ..
.
0

. . . −1

2

−1

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

5

The distance between two labels y and ŷ is called the loss function. It depends on the amount
of overlap between the two bounding boxes and the Hamming distance between the visibility
labellings

T
T 
area(p p̂)
area(p p̂)
S
S · H(v, v̂)
+
(3)
∆(y, ŷ) = 1 −
area(p p̂)
area(p p̂)
The mean Hamming distance H(v, v̂) between two labellings v and v̂ (potentially having different sizes as they might belong to different viewpoints) is calculated after projecting them
to the lowest level of the pyramid. By construction of the loss function, the difference in segmentation starts contributing to the loss only after the two bounding boxes start overlapping
each other. It also has the nice property of decomposing over the energy terms, as described
in Section 2.4.1.

2.4

Inference

To perform the inference as described in Eq. 1 we have to search through Y = A × P × V
where A is the set of viewpoints, P is the set of all pyramid locations and V is the exponential
set of all combinations of visibility variables. We enumerate over A and P and use an s − t
mincut to search over V at every location.
By construction, the feature vector w can be decomposed into weight vectors for the
different viewpoints i.e. w = [w1 , w2 , . . . , wA ]. In the following description, we will consider
one viewpoint and omit the superscript for brevity of notation. w can also be decomposed
as [wv , wnv , w pr , wtrunc ,W, wHOP , wbias ] into the six components described in Section 2.2. We
define the following terms that are used to construct the graph shown in Figure 2(b). φi (x, p)
are the vectorized HOG features extracted at cell i in bounding box p. Unary terms Fi (p) =
wv,i T φi (x, p) and Bi (p) = wnv,i T φi (x, p) are the responses at cell i for object and occlusion
filters respectively. Ri = w pr,i is the prior for cell i to be labelled 0. Constant term C(y) =
wtrunc · c(p) + wbias is the sum of image boundary truncation cost and bias. E is the set of
4-connected neighbouring cells in p and W is the pairwise weight. C(p) is the set of all
cliques in p and ψc (vc ) is the higher-order potential for clique c having nodes with visibility
labels vc . Combining these terms, the energy for a particular labelling is formulated as
wh

E(x, y) = wT Ψ(x, y) = ∑ Fi (p)vi + Bi (p)(1 − vi ) + Ri (1 − vi )
i=1

+

∑
(i, j)∈E

(4)
W |vi − v j | +

∑

ψc (vc ) +C(y)

c∈C (p)

ψc (vc ), the higher-order potential for clique c is defined as mink=1...K (sk ∑i∈c vi + bk ), following Gould [12]. Intuitively, it is the lower envelope of a set of lines whose slope is defined
as sk = M
K ((wHOP )k − (wHOP )k−1 ) and intercept as bk = (wHOP )k − sk k (recall that wHOP is a
K + 1 dimensional weight vector). M is the size of the clique. The normalization in sk makes
the potential invariant to the size of the clique (refer to Gould [12] for details). Figure 2(a)
shows a sample higher-order potential curve for a clique of K cells.
Given an image, a location, and a viewpoint we use s − t mincut on the graph construction shown in Figure 2(b) to find the labelling v that minimizes the energy in Eq. 4. Each
variable vi , i ∈ {1, 2, . . . , wh} defines a node and each clique has K − 1 auxiliary nodes in
the graph, z1 . . . zK−1 . For a detailed derivation of this graph structure please see Boykov
and Kolmogorov [3] and Gould [12]. After the maxflow algorithm finishes, the nodes vi still
connected to s are labelled 1 and others are labelled 0.

6

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

(a)

(b)

Figure 2: (a): Concave higher-order potentials encouraging cells in a clique to have the same
binary label, (b): Construction of graph to compute the energy minimizing binary labelling
of cells by s − t mincut.
Algorithm 1 Response-transfer between object detectors in overlapping regions
for all o ∈ [1, L] do {L is the number of objects, ◦ denotes the Hadamard product}
for all p ∈ P do
B(p) = C(p) ◦ 1[V(p) 6= 0] {Transfer equation for all cells in p}
end for
y∗ = argminy wT Ψ(x, y)
C(y∗ (p)) = F(y∗ (p)) ◦ y∗ (v) {Update equations for all cells in p}
V(y∗ (p)) = o · y∗ (v)
end for

2.4.1

Loss-augmented Inference

Loss-augmented inference is an important part of the cutting plane training algorithm (‘separation oracle’ in Joachims et al. [15]) and is used to find the most violated constraints. It
is defined as yMVC = argminŷ wT Ψ(x, ŷ) − ∆(y, ŷ), where y is the ground-truth labelling.
Our formulation of the loss function makes it solvable with the same complexity as normal
inference (Eq. 1) by decomposing the loss over the terms in Eq 4. The first term of Eq. 3 is
added to C(y), while the second term is distributed across Fi (p) and Bi (p) in Eq. 4.

2.5

Detection of Multiple Objects

Multiple objects of interest might overlap. Running the individual object detectors separately
leaves regions of ambiguity in overlapping areas if multiple detectors mark the same location
as visible. We find that running one iteration of α-expansion (see Boykov et al. [4]) in
overlapping areas resolves ambiguities coherently. The detectors are run sequentially. We
maintain a label map V that stores for each cell the label of the object that last marked it
visible, and a collected response map C that stores for each cell the object filter response
(Fi (p)) from the object that last marked it visible. While running the location search for
object o, we transfer object filter responses from C to the occlusion filter response map
(B(p)) for the current object as described in Algorithm 1. This is effectively one iteration

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

7

of α-expansion (see supplementary material for details). It causes decisions in overlapping
regions to be made between responses of well-defined object filters rather than between
responses of an object filter and a generic occlusion filter.
Such response-transfer requires the object models to be compatible with each other. We
achieve this by training the object models together as if they were different viewpoint components of the same object. The bias term in the feature vector makes the filter responses of
different components comparable.

2.6

3D Pose Estimation

The basic principle of many model based 3D pose estimation algorithms is to fit a given
3D model of the object to its corresponding edges in the image e.g. in Choi and Christensen [5], the 3D CAD model is projected into the image and correspondences between
the projected model edges and image edges are set up. The pose is estimated by solving
an Iterative Re-weighted Least Squares (IRLS) problem. However, partial occlusion causes
these approaches to fail by introducing new edges. We make the algorithm robust to partial
occlusion by first identifying visible pixels of the object using SD-HOP and discarding correspondences outside the visibility mask. We call our extension of the algorithm Occlusion
Reasoning-IRLS (OR-IRLS).

3

Evaluation

We implemented SD-HOP in Matlab, with MVC search and inference implemented in CUDA
since they are massively parallel problems. Inference on a 640x480 image with 11 scales
takes 3s for a single object with a single viewpoint on our 3.4 GHz CPU and NVIDIA GT730 GPU.

3.1

Localization and Segmentation

We evaluated our approach on the CMU Kitchen Occlusion Dataset from Hsiao and Hebert
[14]. This dataset was chosen because (1) it provides extensive labelled training data in the
form of images with bounding boxes and object masks, and (2) the dataset is challenging
and offers the opportunity to compare against an algorithm designed specifically to handle
occlusion. For the localization task we generated false positives per image (FPPI) vs. recall
curves, while for the segmentation task we measured the mean segmentation error against
ground truth as defined by the Pascal VOC segmentation challenge in Everingham et al. [7].
C = 25 (see eq. 2) was chosen by 5-fold cross-validation. While both results are presented
for the single pose part of the dataset, multiple poses are easily handled in our algorithm as
different components of the feature vector. Figure 3 shows FPPI vs. recall curves compared
with those reported by the rLINE2d+OCLP algorithm of Hsiao and Hebert [14] and those
generated from our implementation of Gao et al. [10]. Table 1 presents segmentation errors
compared with Gao et al. [10]. Hsiao and Hebert [14] do not report a segmentation of the
object.
Figure 3 shows that while both SD-HOP and Gao et al. [10] have similar recall at 1.0
FPPI, SD-HOP consistently preforms better in terms of area under the curve (AUC). Averaged over the 8 objects, SD-HOP achieves 16.13% more AUC than Gao et al. [10]. Table 1

8

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION
Colander

Pitcher

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.2

rLINE2D+OCLP
SD-HOP
Gao et al.

0
0.2

0.4

0.6

0.8

1

0.4

0.2

rLINE2D+OCLP
SD-HOP
Gao et al.

0
0

Recall

1

Recall

1

0.2

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

0

Cup

Scissors

0.8

0.8

0.6

0.6

0.6

0.2

0
0.2

0.4

0.6

0.8

1

FPPI

0.4

0.2

rLINE2D+OCLP
SD-HOP
Gao et al.

0
0

Recall

0.8

0.6

Recall

0.8

Recall

1

0.4

0.2

0.4

0.6

0.8

1

FPPI

0.8

1

0.4

0.2

rLINE2D+OCLP
SD-HOP
Gao et al.

0
0

0.6

Shaker

1

rLINE2D+OCLP
SD-HOP
Gao et al.

0.4
FPPI

1

0.2

0.2

FPPI

1

0.4

rLINE2D+OCLP
SD-HOP
Gao et al.

0

FPPI

Saucepan

0.4

0.2

rLINE2D+OCLP
SD-HOP
Gao et al.

0

FPPI

Recall

Thermos

1

Recall

Recall

Bakingpan
1

rLINE2D+OCLP
SD-HOP
Gao et al.

0
0

0.2

0.4

0.6

0.8

1

0

0.2

FPPI

0.4

0.6

0.8

1

FPPI

Figure 3: Object localization results on the CMU Kitchen Occlusion dataset
Table 1: Mean object segmentation error
Object
Gao et al. [10] SD-HOP

Table 2: Mean 3D pose estimation error
Pose parameter IRLS OR-IRLS

Bakingpan

0.2904

0.1516

Colander

0.2095

0.1249

X (cm)

1.6874

0.5774

Cup

0.2144

0.1430

Y (cm)

1.4953

0.6516

Pitcher

0.2499

0.1131

Z (cm)

8.228

2.1506

Saucepan

0.1956

0.1103

Roll (degrees)

1.1711

0.7152

Scissors

0.2391

0.1649

Pitch (degrees)

7.9100

2.3191

Shaker

0.2654

0.1453

Yaw (degrees)

5.7712

2.6055

Thermos

0.2271

0.1285

shows that SD-HOP consistently outperforms Gao et al. [10] in terms of segmentation error, achieving 42.44% less segmentation error averaged over the 8 objects. Figure 5 shows
examples of the algorithm’s output on various images from the CMU Kitchen Occlusion
dataset.

3.2

Ablation Study

We conducted an ablation study on the ‘pitcher’ object of the CMU Kitchen Occlusion
dataset to determine the individual effect of our contributions. Using the loss function
from Gao et al. [10] caused the segmentation error to increase from 0.1131 to 0.1547 and
area under curve (AUC) of FPPI vs. recall to drop from 0.7877 to 0.7071. To discern the
effect of 4-connected pairwise terms we removed the higher order terms from the model too.
Using the pairwise terms as described in Gao et al. [10] caused the segmentation error to
increase from 0.1547 to 0.2499 and AUC to decrease from 0.7071 to 0.6414.
Lastly, to quantify the effect of higher order potentials, we compared the full SD-HOP
model against one with higher order potentials removed. Removing higher order potentials

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

9

Figure 4: 3D pose estimation. Left to right: Pose estimation with IRLS, SD-HOP raw
segmentation mask, SD-HOP refined segmentation mask, Pose estimation with OR-IRLS.
Best viewed in colour.
caused the segmentation error to increase from 0.1131 to 0.1430 and AUC to drop from
0.7877 to 0.7544. We hypothesize that for small objects like the ones in the CMU Kitchen
Occlusion dataset, 4-connected pairwise terms are almost as informative as higher order
terms. To check this hypothesis we tested the effect of removing higher order potentials on
a close-up dataset of 41 images of a pasta-box occluded by various amounts through various
household objects. Removing the higher order potentials caused the segmentation error to
increase from 0.1308 to 0.1516 and area under curve AUC to drop from 0.9546 to 0.9008.
This indicates that higher order terms are more useful for objects with larger and hence more
informative segments.

3.3

3D Pose Estimation

We collected 3D pose estimation results produced by IRLS and OR-IRLS on a dataset which
has 17 images of a car-door in an indoor environment. The ground truth pose for the cardoor
was obtained by an ALVAR marker alv [1]. Table 2 shows the mean errors in the six pose
parameters. To discern the effect of errors inherent in the pose estimation process from the
effect of occlusion reasoning, the pose of the cardoor was constant throughout the dataset,
with various partial occlusions being introduced.
The granular HOG cell-level mask produced by SD-HOP caused some important silhouette edges to be missed for pose estimation. To solve this problem we utilized the unsupervised segmentation done earlier for defining higher order terms. If more than 80% of the
area within a segment was marked 1, we marked the whole segment with 1. Since segments
follow object boundaries, this produced much cleaner masks for pose estimation. Figure 4
shows the masks and pose estimation results for an example image from the dataset, with
more such examples presented in the supplementary material. Note that the segmentation
errors mentioned in Table 1 use the raw masks.

4

Conclusion

We presented an algorithm (SD-HOP) that localizes partially occluded objects robustly and
segments their visible regions accurately. In contrast to previous approaches that model occlusion, our algorithm uses higher order potentials to reason at the level of image segments
and employs a loss function that targets both localization and segmentation performance. We
demonstrated that our algorithm outperforms existing approaches on both tasks, when evaluated on a challenging dataset. Finally, we have shown that the segmentation output from

10

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

Figure 5: Object localization and segmentation results on the CMU Kitchen Occlusion
dataset. Left: Image, Center: Raw mask from SD-HOP, Right: Refined mask from SD-HOP
SD-HOP can be used to improve pose estimation performance in the presence of occlusion.
Avenues of future research include (1) training from weakly labelled data i.e. without segmentations, (2) a post-training algorithm to make object models comparable without having
to train them together, and (3) using the occlusion information to reason about interactions
between objects in scene understanding applications.
We would like to acknowledge Ana Huamán Quispe’s help with implementing this system on a bimanual robot. The system was used to enable the robot to pick up partially visible
objects lying on a table.

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

11

References
[1] ALVAR tracking library.
http://virtual.vtt.fi/virtual/proj2/
multimedia/alvar/index.html. Acessed: 2015-05-03.
[2] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 33(5):898–916, 2011. URL http://ieeexplore.ieee.
org/xpl/login.jsp?tp=&arnumber=5557884.
[3] Yuri Boykov and Vladimir Kolmogorov. An experimental comparison of mincut/max-flow algorithms for energy minimization in vision. Pattern Analysis and
Machine Intelligence, IEEE Transactions on, 26(9):1124–1137, 2004. URL http:
//ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1316848.
[4] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization
via graph cuts. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23
(11):1222–1239, 2001. URL http://ieeexplore.ieee.org/xpl/login.
jsp?tp=&arnumber=969114.
[5] Changhyun Choi and Henrik I Christensen. Robust 3D visual tracking using particle filtering on the special Euclidean group: A combined approach of keypoint
and edge features. The International Journal of Robotics Research, 31(4):498–519,
2012. URL http://ijr.sagepub.com/content/early/2012/03/01/
0278364912437213.
[6] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886–893. IEEE, 2005. URL http:
//ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1467360.
[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal
Visual Object Classes (VOC) Challenge. International Journal of Computer Vision, 88
(2):303–338, June 2010. URL http://link.springer.com/article/10.
1007%2Fs11263-009-0275-4.
[8] Pedro F Felzenszwalb and Daniel P Huttenlocher.
Efficient graph-based image segmentation.
International Journal of Computer Vision, 59(2):167–
181, 2004. URL http://link.springer.com/article/10.1023%2FB%
3AVISI.0000022288.19776.77.
[9] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. Pattern Analysis and
Machine Intelligence, IEEE Transactions on, 32(9):1627–1645, 2010. URL http:
//ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5255236.
[10] Tianshi Gao, Benjamin Packer, and Daphne Koller. A segmentation-aware object detection model with occlusion handling. In Computer Vision and Pattern Recognition
(CVPR), 2011 IEEE Conference on, pages 1361–1368. IEEE, 2011. URL http:
//ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995623.

12

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

[11] Ross B Girshick, Pedro F Felzenszwalb, and David A Mcallester. Object detection with
grammar models. In Advances in Neural Information Processing Systems, pages 442–
450, 2011. URL http://citeseerx.ist.psu.edu/viewdoc/summary?
doi=10.1.1.231.2429.
[12] Stephen Gould. Max-margin learning for lower linear envelope potentials in binary
markov random fields. In Proceedings of the 28th International Conference on Machine
Learning (ICML-11), pages 193–200, 2011. URL http://ieeexplore.ieee.
org/xpl/articleDetails.jsp?arnumber=6945904.
[13] Stefan Hinterstoisser, Cedric Cagniart, Slobodan Ilic, Peter Sturm, Nassir Navab, Pascal Fua, and Vincent Lepetit. Gradient response maps for real-time detection of textureless objects. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34
(5):876–888, 2012. URL http://ieeexplore.ieee.org/xpls/abs_all.
jsp?arnumber=6042881.
[14] Edward Hsiao and Martial Hebert. Occlusion reasoning for object detection under arbitrary viewpoint. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3146–3153. IEEE, 2012. URL http://ieeexplore.ieee.
org/xpl/login.jsp?tp=&arnumber=6248048.
[15] Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu. Cutting-plane training
of structural SVMs. Machine Learning, 77(1):27–59, 2009. URL http://link.
springer.com/article/10.1007%2Fs10994-009-5108-8.
[16] Pushmeet Kohli, Philip HS Torr, et al. Robust higher order potentials for enforcing label consistency. International Journal of Computer Vision, 82(3):302–
324, 2009. URL http://ieeexplore.ieee.org/xpl/login.jsp?tp=
&arnumber=4587417.
[17] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. Grabcut: Interactive
foreground extraction using iterated graph cuts. In ACM Transactions on Graphics
(TOG), volume 23, pages 309–314. ACM, 2004. URL http://dl.acm.org/
citation.cfm?id=1015720.
[18] Andrea Vedaldi and Andrew Zisserman. Structured output regression for detection
with partial truncation. In Advances in neural information processing systems, pages
1928–1936, 2009.
[19] Xiaoyu Wang, Tony X Han, and Shuicheng Yan. An HOG-LBP human detector with
partial occlusion handling. In Computer Vision, 2009 IEEE 12th International Conference on, pages 32–39. IEEE, 2009. URL http://ieeexplore.ieee.org/
xpl/login.jsp?tp=&arnumber=5459207.
[20] Yu Xiang and Silvio Savarese. Object Detection by 3D Aspectlets and Occlusion Reasoning. In Computer Vision Workshops (ICCVW), 2013 IEEE International Conference
on, pages 530–537. IEEE, 2013. URL http://ieeexplore.ieee.org/xpl/
articleDetails.jsp?reload=true&arnumber=6755942.
[21] Menglong Zhu, Konstantinos G Derpanis, Yinfei Yang, Samarth Brahmbhatt, Mabel Zhang, Cody Phillips, Matthieu Lecce, and Kostas Daniilidis. Single image 3D object detection and pose estimation for grasping.
In Robotics and

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

13

Automation (ICRA), 2014 IEEE International Conference on, pages 3936–3943.
IEEE, 2014. URL http://www.cis.upenn.edu/~menglong/papers/
icra2014_object_grasping.pdf.

2014 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS 2014)
September 14-18, 2014, Chicago, IL, USA

Latent Space Policy Search for Robotics
Kevin Sebastian Luck1 , Gerhard Neumann1 , Erik Berger2 , Jan Peters1,4 and Heni Ben Amor3
Abstract— Learning motor skills for robots is a hard
task. In particular, a high number of degrees-of-freedom
in the robot can pose serious challenges to existing reinforcement learning methods, since it leads to a highdimensional search space. However, complex robots are
often intrinsically redundant systems and, therefore, can
be controlled using a latent manifold of much smaller
dimensionality. In this paper, we present a novel policy
search method that performs efficient reinforcement learning by uncovering the low-dimensional latent space of
actuator redundancies. In contrast to previous attempts
at combining reinforcement learning and dimensionality
reduction, our approach does not perform dimensionality
reduction as a preprocessing step but naturally combines
it with policy search. Our evaluations show that the new
approach outperforms existing algorithms for learning
motor skills with high-dimensional robots.

I. INTRODUCTION
Creating autonomous robots that can adapt to the current
task by interacting with their environment is an important vision of artificial intelligence. In recent years, many
successful applications of reinforcement learning (RL)
to complex robot tasks have been reported, including
autonomous helicopter flight [1], robot table-tennis [2],
or quadruped locomotion [3]. One of the most successful methods for learning such motor tasks is policy
search [4].
Policy search tries to directly uncover the parameters
of a given policy representation that yield high rewards.
In this paper we focus on policy search for robots with
a high number of degrees-of-freedom (DOF). Typically,
the number of parameters of our control policy heavily
depends on the number of DOFs of the robot. Hence,
we generally need a large number of evaluations to
learn acceptable policies. However, evaluating hundred
thousands of different policies on a real robot is often
infeasible due to wear and tear, the required logistics,
or space and time constraints. At the same time, many
1 Kevin S. Luck, Gerhard Neuman and Jan Peters are with the
Department of Computer Science, Technische Universität Darmstadt,
64289 Darmstadt, Germany

{luck, geri, peters}@ias.tu-darmstadt.de
2 Erik Berger is with the Department of Mathematics and Computer Science, Technische Universität Bergakademie Freiberg, 09599
Freiberg, Germany

Erik.Berger@informatik.tu-freiberg.de
3 Heni Ben Amor is with the Institute for Robotics and Intelligent
Machines, Georgia Institute of Technology, GA 30332, USA

hbenamor@cc.gatech.edu
4 Jan Peters is with the Max Planck Institute for Intelligent Systems,
72076 Tübingen, Germany

978-1-4799-6934-0/14/$31.00 ©2014 IEEE

Fig. 1: A NAO robot learns to lift up one leg and
stay balanced using a novel latent space policy search
method. The co-articulation of the joints, needed for
successful execution of the motor skill, is represented
in the low-dimensional latent space.

robot control tasks, such as motor skills, are highly
redundant in the controlled DOFs. Typically, the intrinsic
dimensionality of such movements is much smaller than
the actual controlled number of DOFs. Hence, robot
learning can be performed much more efficiently if we
can determine the lower-dimensional latent space of the
movement we want to learn.
In this paper we present an efficient policy search
algorithm for learning policies in low-dimensional latent
spaces. The learning algorithm produces control signals for high-dimensional robot systems by estimating
policies in a latent space with a significantly lower
number of dimensions. The latent space encodes correlations between the controlled DOFs of the robot.
The parameters of the policy as well as the projection
parameters of the latent space are efficiently estimated
from samples during the policy search iterations. The
key insight to our algorithm is that policy search as
well as dimensionality reduction can be integrated in an
expectation-maximization (EM) framework. As a result,

1434

we can formulate a coherent algorithmic approach that
naturally combines policy search and dimensionality
reduction.
In contrast to previous attempts for combining reinforcement learning and dimensionality reduction for
robotic applications, our approach does not perform dimensionality reduction as a preprocessing step. Instead,
the parameters of the latent space are adapted based on
the reward signal from the environment.
II. Related Work
Policy search has attracted considerable attention in the
robot learning community. An excellent overview of the
topic and detailed descriptions of various state-of-the-art
algorithms can be found in [5] and [4].
Previous combinations of dimensionality reduction
and policy search, typically use a clear separation between the reinforcement learning algorithm and the dimensionality reduction step. In [6], data from a simulator
was used in a preprocessing step to identify a possible
low-dimensional latent space of policies using Reduced
Rank Regression. Learning on the real robot was then
restricted to the extracted latent space. Similarly, Bitzer
et al. [7] used user-provided training data to learn a
low-dimensional subspace using linear and non-linear
dimensionality reduction for robot learning. Using dimensionality reduction as a preprocessing step, or as an
independent process that can be executed after several
iterations of reinforcement learning, may lead to serious
limitations. First, extracting the latent space as a preprocess requires a significantly large training set of
(approximate) solutions, prior simulations, or human
demonstrations. Even if such data is available, it can
be counterproductive to use it, since the reinforcement
learning algorithm cannot change the parameters of the
latent space in these approaches. For example, when
using human demonstrations, e.g., recorded joint configurations, to identify the latent space, the extracted
latent space might not be appropriate for controlling the
robot as we neglect the correspondence problem [8],
i.e., there is no one-to-one mapping of the human
joints to the robot joints. Hence, we need to adapt the
projection of the latent space during the reinforcement
learning process. Using dimensionality reduction as an
independent process also leads to a decreased learning
efficiency, since it neglects reward information when
identifying subspaces.
III. Policy Search
In the following section, we will introduce the general
problem statement for reinforcement learning and dimensionality reduction and introduce the notation that
will be used throughout the paper. For a more detailed
description of theses topics, the reader is referred to [9]
and [10].

A. Problem Statement
Reinforcement learning methods can be used to autonomously learn robot control strategies through the
interaction with an environment. Given the current state
st ∈ S a robot executes an action at ∈ A, transitions
into the state st+1 and receives a reward rt (st , at ). The
action selection process is governed by the control policy
π(at |st , t), which is specified as conditional probability
distribution over the actions given the current state st .
Generally, RL algorithms try to determine an optimal
policy which maximizes the expected reward.
In this paper, we will focus on policy search methods.
Policy search approaches typically use a parametrized
stochastic policy represented by a πθ (at |st , t) with parameters θ. A typical representation of the policy in robotics
is to use a Gaussian distribution as policy where the
mean depends linearly on an observed feature vector φ
of the task, e.g., the location of an object to grasp. The
goal of learning is to optimize the expected return of the
policy with parameters θ with
Z
J (θ) = pθ (τ) R (τ) dτ,
(1)
T

where the expectation integrates over all possible trajectories τ in the set T. Each trajectory τ = [s1:T +1 , a1:T ]
is specified by a sequence of length T of states and
actions. The return R (τ) of a trajectory is defined as the
accumulated immediate rewards rt , i.e.,
R (τ) =

T
X

rt (st , at ) + rT +1 (sT +1 ),

(2)

t=1

where rT +1 denotes the final reward for reaching state
sT +1 . Note that in many robot applications, the reward
function and the policy are explicitly modelled to be time
dependent. Due to the Markov property, the trajectory
distribution pθ (τ) can be written as
pθ (τ) = p(s1 )

T
Y

p (st+1 |st , at ) πθ (at |st , t) .

(3)

t=1

Reinforcement learning algorithms try to determine policy parameters θ that maximize Equation 1.
B. Expectation Maximization Approaches to Policy
Search
In contrast to traditional approaches to reinforcement
learning, EM-based methods formalize the policy search
problem as inference problem with latent variables.
They transform the rewards into an improper probability
distribution such that the reward can be interpreted as
(unnormalized) probability of a binary reward event. In
our discussion, we will assume that the rewards have
already been transformed to such a improper probability
distribution, i.e., the rewards are non-negative. As in

1435

the standard EM-algorithm, we can now optimize a
lower bound, that is in this case a lower bound on
the expected return, instead of optimizing the original
objective. According to Kober and Peters [11], the lower
bound of the expected return (1) is given by
Z
Lθold (θ) = pθold (τ) R (τ) log pθ (τ) dτ
T
 (4)
 T

X π

Q (st , at , t) log πθ∗ (at |st , t) ,
= IE pθ (τ) 
old

t=1

π

where Q is defined as the expected reward to come for
time step t, when the robot is in state st and execute
action at ,

 T

X
π

Q (s, a, t) = IE  rt˜ (st˜, at˜) |st = s, at = a . (5)

the exploration of the policy in a lower dimensional
latent space. This low-dimensional exploration z is then
projected in to the high-dimensional original space by
a projection matrix. In order to infer such a model
with latent variables, we can again use the expectation
maximization algorithm. This time we infer a structured
policy from the weighted data points. More specifically
we use the marginalization rule [15] to introduce a
hidden variable
z to our policy by specifying that
R
pθ (at |st , t) = Z pθ (at , z|st , t) dz. This step leads to a new
lower bound given by

 T
Z

X π
IE pθold (τ) 
Q (st , at , t) log
pθ (at , z|st , t) dz ≥
Z

t=1

IE pθold

t˜=t


 T
X π
(7)


(a
(s
log
p
,
z|s
,
t)
Q
,
a
,
t)
IE
 ,
θ
t
t
(τ) 
t t
q(z|at ,st )

t=1

π

In practice, Q (s, a, t) is estimated by a single rollout,
 P

T
i.e, Qπ st[i] , at[i] , t ≈ rt[i]
˜ , where i denotes the index of
t˜=t

the episode.
An important advantage of this approach is that the
policy update is formulated as a weighted maximum
likelihood (ML) estimate for the parameters θ, where
the reward to come Qπ (s, a, t) is used as weight for
the samples. Due to the weighted ML update, there
is no need for a user-specified learning rate which is
often a critical factor for achieving good performance in
policy gradient algorithms [12]. The policy is typically
modelled as linear policy with Gaussian noise. In the
PoWER [11] algorithm, this Gaussian noise is added to
the parameter vector of the policy, i.e.,
a = (M + E) φ.

(6)

Mφ is the mean of the policy and Eφ denotes a
Gaussian noise term that is either isotropic or anisotropically distributed. In our experiments, we will use the
more commonly used isotropic version of the noise. In
contrast to the standard formulation of PoWER [11],
we use matrix-variate normal
[13] for the
 distributions

exploration noise E ∼ Nd,p 0, σ2 I , where 0 has d rows
and p columns. We will use the notation Nd,p (·, ·) for
such matrix-variate normal distributions and N (·, ·) for
multi-variate normal distributions.
In the remainder of this paper, we will write the
stochastic policy πθ (at |st , t) as pθ (at |st , t) to ensure consistent notation.

pθ

IV. The PePPC Er Algorithm
In this section, we will describe the “Policy Search with
Probabilistic Principle Component Exploration” Algorithm (PePPC Er) for policy search in low-dimensional
latent spaces. We will first start with a short recap
of Probabilistic PCA, explain the relevant probability
distributions for the PePPC Er algorithm and derive the
EM update equations.
A. Revisiting Probabilistic PCA
Probabilistic Principal Component Analysis (PPCA) is
the probabilistic formulation of the PCA algorithm for
performing linear dimensionality reduction. PPCA relates a d-dimensional data point x ∈ Rd to a lowdimensional latent variable z ∈ Rn through a linear
Gaussian model
x = Wz + µ + 

C. Using Structured Policies with Latent Variables
Another important advantage of weighted ML updates,
is that we can use structured policy representations that
again include latent variables z. For example, mixture
models [14] or low-dimensional factor models can be
used. In our specific case, the latent variable defines

(at ,z|st ,t)

where the distribution q(z|at , st ) = pθold (at |st ,t) is given
old
by the posterior of the latent variables given the old
policy parameters θold . In this lower bound, the EMalgorithm is applied twice. First, to derive the policy update by weighted maximum likelihood estimates.
Second, we use EM to update the joint distribution
pθ (at , z|st , t) instead of the marginal.
While this lower bound can be used for any latent
variable model, we will discuss our specific case of
estimating projection parameters in more detail in the
following section.

(8)

where the latent variable z ∈ Rn is Gaussian distributed
according to p (z) = N (0, I). The transformation matrix
W ∈ Rd×n maps each low-dimensional vector z to the
high dimensional space. The matrix W spans a lowdimensional subspace and µ ∈ Rd is the mean of

1436

the high-dimensional distribution. A high dimensional
isotropic noise  ∈ Rd with zero mean and σ2 I variance
is added to this projection. The parameters of this
model are given by µ, σ2 and W and can efficiently be
estimated using an EM algorithm (see [10] for details).
However, PPCA is a unsupervised learning method
while policy search is supervised.
B. Deriving the Update Equations for PePPC Er
Building on the insights from PPCA, we can decompose
a stochastic policy into a low-dimensional distribution
and projection parameters for generating the required
high-dimensional action. More specifically, we can write


a = W ZT φ + Mφ + Eφ,
(9)
where W is a projection matrix. The terms Mφ and Eφ
are again the mean and the Gaussian noise term. The
term ZT φ with Z ∼ N p,n (0, I) generates an exploration
noise in a low-dimensional latent space, which is then
projected into the high-dimensional space of actions
via W. Due to the projection from the latent space
to the original high dimensional state, the uncorrelated
explorative action from the latent space becomes a correlated action in the high dimensional space. Hence, the
projection matrix W can be understood as a matrix that
defines synergies in the action space that are used for
correlated exploration. Both, the mean M of the policy
and the projection matrix W are learned by the policy
search algorithm. Given the model in Equation 9, we
can derive the expectation of our probability distribution
p (a) in a straight-forward fashion
h 
i
 
IE [a] = IE W ZT φ +Mφ + IE Eφ = Mφ.
|
{z
}
(10)
| {z }
0

0

Similarly, we can also use the properties of matrixvariate normal distributions [13] to get the covariance
h
i
cov (a) = IE (a − IE [a]) (a − IE [a])T
h
i
h
i
(11)
= IE WZT φφT ZWT + IE EφφT ET



= tr φφT WWT + σ2 I ,
where tr (·) denotes the trace of a matrix. From Equation 10 and Equation 11 it follows that the prior distribution over actions is




p (a) = N Mφ, tr φφT WWT + σ2 I .
(12)
Now, in order to apply EM, we have to determine
the posterior distribution p (Z|a) over matrices Z. The
posterior distribution can be simplified by treating ZT φ
as a latent variable. Since the result of this product is a
vector, we can use Bayes theorem for Gaussian variables


[15, p.93] to derive the posterior distribution p ZT φ|a .
Given both distributions




 
p ZT φ = N 0, tr φφT I
(13)

and


 


 
p a|ZT φ = N W ZT φ + Mφ, σ2 tr φφT I ,

(14)

the posterior distribution can be written as





pθold ZT φ|a = N CWT (a − Mφ) , Cσ2 tr φφT ,
(15)

−1
T
2
where C = σ I + W W . Given this posterior distribution, we can now determine the equations of the
expectation step
h
i
IE pθ (ZT φ|a) ZT φ = CWT (a − Mφ) ,
(16)
old


T 


IE pθ (ZT φ|a) ZT φ ZT φ = Cσ2 tr φφT
(17)
old
h
iT
h
i
+IE pθ (ZT φ|a) ZT φ IE pθ (ZT φ|a) ZT φ .
old
old
1. Maximization Step for M
We use a maximum likelihood estimate to identify the
value of M in each iteration. To this end, we calculate
the derivative of the log-likelihood function w.r.t. M,

∂ ln p (a)  −1  T
= D aφ − MφφT ,
(18)
∂M



where D = tr φφT WWT + σ2 I = DT . After inserting
this result into the EM policy search framework and set
the derivative to zero, we get
 T

X ∂ ln p (at ) Qπt 
0 = IE pθold (τ) 
(19)

∂M
t=1
 T
 T

−1
X at φT Qπ  
X φφT Qπ 
t 
t 




  IE pθold (τ) 
 
⇔ M = IE pθold (τ) 
T 
T 
t=1 tr φφ
t=1 tr φφ
such that M maximizes the log-likelihood function
ln p (a).
2. Maximization Step for W
For optimizing W we have to use the new lower bound
given in Equation 7 and set the derivative of this term
w.r.t W to zero. Accordingly the derivative can be
written as




−1
∂ ln p a, ZT φ
= − σ2 tr φφT
 ∂W

T

T

T 
−a ZT φ + WZT φ ZT φ + Mφ ZT φ
(20)
from which follows that the optimal value of W that
maximizes the log-likelihood is given by


h
iT
T
π
X
T (a − Mφ) IE
t

pθold (ZT φ|a) Z φ Qt 




W = IE pθold (τ) 
T


tr φφ
t=1




−1

T
T
T
π 

X

 T IE pθold (ZT φ|at ) Z φ Z φ Qt 
IE pθ (τ) 
 .


 old 

tr φφT

1437

t=1

(21)

3. Maximization Step for σ2
Similarly to the estimation
 of W,
 we can also derivate
the log-likelihood of ln p a, ZT φ with respect to σ2 in
order to identify a new estimate of σ2 with

Input: Initialized parameters σ20 , W0 and M0 and the
dimensionality n of the low dimensional
manifold. The function φ (st , t) represents the
feature vector for the policy.
repeat




∂ ln p a, ZT φ

Sampling:
for h=1:H do # Sample the H rollouts
for t=1:T do
aht = Wi ZT φ + Mi φ + Eφ


with Z ∼ N p,n (0, I) and E ∼ Nd,p 0, σ2i I
Execute action aht


Observe and store reward rt sht , aht

  2 
−1
d
2
T
+
2
σ
tr
φφ
∂σ2
2σ2

T 

T
a − WZ φ − Mφ a − WZT φ − Mφ . (22)
=−

Setting the above derivative to zero leads to the following maximum-likelihood estimate of the variance:
 T
X   T −1
1
2
tr φφ
σ = IE pθold (τ) 
d
t=1

(at − Mφ)T (at − Mφ)
h
i
−2 (at − Mφ)T WIE pθ (ZT φ|at ) ZT φ
old
h
i
 #

T
T
T
+tr IE pθ (ZT φ|at ) Z φφ Z W W Qπt
old

 T
−1

X π 
Qt  . (23)
IE pθ (τ) 

Calculate weights:
"
#
T
P
Qπ (s, a, t) = IE
rt˜ (st˜, at˜) |st = s, at = a
t˜=t

Expectation:
foreach aht do
h
i
Compute IE pθ (ZT φ|aht ) ZT φ with (16).
old


T 
with (17).
Compute IE pθ (ZT φ|aht ) ZT φ ZT φ
old

old

t=1

Maximization:
Compute Mi+1 with (19).
Compute Wi+1 with (21).
Compute σ2i+1 with (23).
until Mi ≈ Mi+1

C. Complete Algorithm
The resulting algorithm that implements all of the above
steps can be found in Alg. 1. The initial values for the
parameters σ2 , W and M can either be randomly chosen
or initialized using a PPCA on a set of demonstrations.
Additionally, the algorithm requires the number of latent
dimensions n as input. After convergence, a policy is
given by a weight matrix M which is multiplied by the
feature vector φ (s, t) to receive an action for a given
state and time.
V. Experiments
The PePPC Er Algorithm has been evaluated on a simulated and a real-world robot task. In this section, we will
describe the experimental setup of these evaluations and
present the achieved results in comparison to PoWER
and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [16] algorithm.
A. Learning Inverse Kinematics
In our first experiment, we will focus on learning inverse
kinematics. A range of methods exist for analytically
or numerically solving the inverse kinematics problem.
However, various researchers have also looked at inverse
kinematics from a machine learning point of view [17].
In our experiment, we use a simulated robot with
d hinge-joints and d + 1 segments. The goal of the
simulated robot is to track the position of a sphere
that is moving on a circle. Setting d to values higher
than two results in a redundant system with more DOF

Output: Linear weights M for the feature vector φ.

Algorithm 1: Policy Search with Probabilistic Principle Component Exploration in the Action Space
(PePPC Er)

than required to accomplish the task. To learn inverse
kinematics, we set the reward function to
rt (st , at ) = e−D ,

(24)

where D is the distance of the end-effector to the
target, when action at is executed. Then, we use PePPC Er
to determine a suitable policy for the task. During the
optimization process, PePPC Er uncovers the redundancies
of the system by determining the low-dimensional latent
space of joint angle configurations that lead to touching
the target. The latent space models the co-articulation
of different links. An example result of a learned policy
can be found in Fig. 2. As can be seen in the figure, a
20 linked robot arm successfully tracks the target along
a circular path.
We ran the explained setup with different specifications of policy search algorithms resulting in the
graph depicted in Fig. 3. The graph depicts the sum

1438

60

Sum. Distances

50

Fig. 2: A tentacle-like robot with 20 links tracks a target
along a circular path.

40

30

20

10

0

of distances of the end-effector to the target positions.
For a balance evaluation, we compared to two different implementations of the PoWER algorithm. In one
implementation the σ2 was static, while in the other
implementation an automatic adaptation of a diagonal
covariance matrix was performed. This feature was also
implemented in the PePPC Er algorithm, which results in
a slightly different update equation for σ2 . In each iteration 30 samples were drawn and executed on a simulated
20-linked robot. As features we used 19 time-dependent
Gaussians, so we had to estimate 380 parameters for 50
time steps. We repeated each experiment 10 times and
calculated the mean (bold lines) and standard deviation
of the results (light colors around the mean). The figure
shows that PePPC Er outperforms CMA-ES and PoWER.
In particular in the early iterations both policy search
methods perform comparatively well. At the same time,
we can see that both PoWER implementations start to
stagnate at around 50 iterations. PePPC Er continues to
reduce the distance to the targets.

1

2

3

4
5
6
7
8
9
Number of Latent Dimensions

10

11

12

Fig. 4: The mean sum of distances and the standard
deviation between the 450th and 500th iteration for an
12-linked robot. Five solutions were learned by PePPC Er
for different values of the dimensionality n of the latent
space.
latent space was set to n = 5. In order to evaluate
the effect of this parameter on the results, we repeated
the evaluation of PePPC Er with varying values for n in
an inverse kinematics task for a 12-linked robot, as
can be seen in Fig. 4. In the depicted graph, we can
see a bump in the average distance at around 5 and 9
dimensions. This is an interesting phenomenon of latent
space policy search: too small a value for n restricts the
search space, too high a value for n diminishes the effect
of dimensionality reduction. In our specific example, the
best value for n seems to be 4 or 5.
B. Learning to Stand on One Leg

Fig. 3: Comparison between PePPC Er, PoWER and
CMA-ES on the inverse kinematics task with a 20linked robot. In each iteration we executed 30 different
joint configurations on the simulated robot. For the static
PoWER we set σ = 15. For the dynamic PoWER and
PePPC Er we computed the diagonal covariance matrix.
In the above experiment, the dimensionality of the

We also performed a learning task on a real robot.
More specifically, we used PePPC Er to learn policies for
standing on one leg. The task of standing on one leg is
a synergistic motor skill that requires the co-articulation
of different body parts for successful execution. It is
often used in biomechanical studies on synergies and
low-dimensional control in humans, such as in [18]. In
our experiment, we set the episodic reward of the robot
proportional to the height of the right leg after execution
of the policy. Furthermore, we have to consider in our
reward function, that the head and the right foot of the
robot should not move a lot. Hence, the reward function
can be written as
R(h, rf , lf ) = exp {α · h + β · rf − γ · lf − λMAX } , (25)
where α, β, γ ∈ R+ , h is the height of the head, rf the
height of the right foot and lf the height of the left foot
in the final position. The constant λMAX is the maximal
possible value of the first part of the sum. The height of
the head is responsible for a low reward if the robot falls

1439

Acknowledgment
Policy 1

The research leading to these results has received
funding from the European Union under grant agreement
#270327 (CompLACS).

Policy 2

References

Fig. 5: Two different policies for standing on one leg
learned using latent space policy search. Only 100
samples were needed to learn policy 1.

during learning. As features, time-dependent Gaussians
were used in this experiment.Actions were represented
by the change in the 25 robot joint angles between two
consecutive time steps.
The goal in robot learning is to learn from few trials.
We therefore restricted the maximum number of samples
(executions on the robot) to 600 samples. For automation
and repeatability purposes, learning was performed in
a physics-based simulator. However, we want to stress
that, given the relatively small number of trials needed
by PePPC Er to learn a policy, we can also perform
learning directly on the real robot. Fig. 5 shows two
learned policies acquired using PePPC Er. Learning started
from random initializations and did not require any
demonstrations. Policy 1 was learned using a sample
size of 20 samples and 5 iterations, i.e., 100 execution
on the robot in total. We can see, that it results in a
smooth and stable motor skill. Policy 2 required 600
evaluations in total and allows the robot to lift the leg
even higher.
VI. CONCLUSIONS
In this paper we presented a novel policy search algorithm for robotics applications. The PePPC Er algorithm
determines the correlations between different joints of
the robot and uses the information for fast and efficient
reinforcement learning. The presented method combines
policy search and dimensionality reduction in a natural
way and has been derived from basic principles. Applications on a simulated and a real robot indicate that
the approach can be employed to learn new motor skills
for complex, redundant robots using a relatively small
number of trials on the robot. In our future work we
want to combine the introduced approach with imitation
learning, in order to start in a good region of the search
space. Additionally, we want to investigate methods for
identifying the dimensionality of the current task.

[1] A. Y. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse,
E. Berger, and E. Liang, “Autonomous inverted helicopter flight
via reinforcement learning,” in Proceedings of the International
Symposium on Experimental Robotics, 2004, pp. 363–372.
[2] K. Muelling, J. Kober, O. Kroemer, and J. Peters, “Learning to
select and generalize striking movements in robot table tennis,”
International Journal of Robotics Research, no. 3, pp. 263–279,
2013.
[3] J. Zico Kolter and A. Y. Ng, “The stanford littledog: A learning
and rapid replanning approach to quadruped locomotion,” Int. J.
Rob. Res., vol. 30, no. 2, pp. 150–174, Feb. 2011.
[4] J. Kober and J. Peters, “Reinforcement learning in robotics: a
survey,” in Reinforcement Learning. Springer Berlin Heidelberg,
2012, pp. 579–610.
[5] M. P. Deisenroth, G. Neumann, and J. Peters, “A survey on policy
search for robotics,” Foundations and Trends in Robotics, vol. 2,
no. 12, pp. 1–142, 2013.
[6] J. Z. Kolter and A. Y. Ng, “Learning omnidirectional path
following using dimensionality reduction,” in in Proceedings of
Robotics: Science and Systems, 2007.
[7] S. Bitzer, M. Howard, and S. Vijayakumar, “Using dimensionality reduction to exploit constraints in reinforcement learning,”
in Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on, Oct 2010, pp. 3219–3225.
[8] C. L. Nehaniv and K. Dautenhahn, “Imitation in animals and
artifacts,” K. Dautenhahn and C. L. Nehaniv, Eds. Cambridge,
MA, USA: MIT Press, 2002, ch. The Correspondence Problem,
pp. 41–61.
[9] R. S. Sutton and A. G. Barto, Reinforcement Learning: An
Introduction. MIT Press, 1998.
[10] M. E. Tipping and C. M. Bishop, “Probabilistic principal component analysis,” Journal of the Royal Statistical Society, Series
B, vol. 61, pp. 611–622, 1999.
[11] J. Kober and J. Peters, “Policy search for motor primitives in
robotics,” Machine Learning, vol. 84, no. 1-2, pp. 171–203, 2011.
[12] J. Peters and S. Schaal, “Reinforcement learning of motor skills
with policy gradients,” Neural networks, vol. 21, no. 4, pp. 682–
697, 2008.
[13] A. K. Gupta and D. K. Nagar, Matrix variate distributions. CRC
Press, 2000, vol. 104.
[14] C. Daniel, G. Neumann, and J. Peters, “Learning concurrent
motor skills in versatile solution spaces,” in Intelligent Robots
and Systems (IROS), 2012 IEEE/RSJ International Conference
on, Oct 2012, pp. 3591–3597.
[15] C. M. Bishop and N. M. Nasrabadi, Pattern recognition and
machine learning. Springer New York, 2006, vol. 1.
[16] N. Hansen, S. Muller, and P. Koumoutsakos, “Reducing the time
complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES).” Evolutionary Computation,
vol. 11, no. 1, pp. 1–18, 2003.
[17] K. Grochow, S. L. Martin, A. Hertzmann, and Z. Popović, “Stylebased inverse kinematics,” ACM Trans. Graph., vol. 23, no. 3,
pp. 522–531, Aug. 2004.
[18] G. Torres-Oviedo and L. H. Ting, “Subject-specific muscle
synergies in human balance control are consistent across different
biomechanical contexts,” Journal of neurophysiology, vol. 103,
no. 6, pp. 3084–3098, 2010.

1440

2015 IEEE International Conference on Robotics and Automation (ICRA)
Washington State Convention Center
Seattle, Washington, May 26-30, 2015

Learning Multiple Collaborative Tasks with a
Mixture of Interaction Primitives
Marco Ewerton1 , Gerhard Neumann1 , Rudolf Lioutikov1 , Heni Ben Amor2 , Jan Peters1,3 and Guilherme Maeda1

Abstract— Robots that interact with humans must learn to
not only adapt to different human partners but also to new
interactions. Such a form of learning can be achieved by
demonstrations and imitation. A recently introduced method
to learn interactions from demonstrations is the framework
of Interaction Primitives. While this framework is limited
to represent and generalize a single interaction pattern, in
practice, interactions between a human and a robot can consist
of many different patterns. To overcome this limitation this
paper proposes a Mixture of Interaction Primitives to learn
multiple interaction patterns from unlabeled demonstrations.
Specifically the proposed method uses Gaussian Mixture Models of Interaction Primitives to model nonlinear correlations
between the movements of the different agents. We validate
our algorithm with two experiments involving interactive tasks
between a human and a lightweight robotic arm. In the first,
we compare our proposed method with conventional Interaction
Primitives in a toy problem scenario where the robot and the
human are not linearly correlated. In the second, we present a
proof-of-concept experiment where the robot assists a human
in assembling a box.

I. I NTRODUCTION
Robots that can assist us in the industry, in the household,
in hospitals, etc. can be of great benefit to the society. The
variety of tasks in which a human may need assistance is,
however, practically unlimited. Thus, it is very hard (if not
impossible) to program a robot in the traditional way to assist
humans in scenarios that have not been exactly prespecified.
Learning from demonstrations is therefore a promising
idea. Based on this idea, Interaction Primitive (IP) is a
framework that has been recently proposed to alleviate the
problem of programming a robot for physical collaboration
and assistive tasks [1], [2]. At the core, IPs are primitives
that capture the correlation between the movements of two
agents—usually a human and a robot. Then, by observing
one of the agents, say the human, it is possible to infer the
controls for the robot such that collaboration can be achieved.
A main limitation of IPs is the assumption that the
movements of the human and the movements of the robot
assistant are linearly correlated. This assumption is reflected
in the underlying Gaussian distribution that is used to model
1 Intelligent Autonomous Systems Lab, Department of Computer Science,
Technische Universität Darmstadt, Hochschulstr. 10, 64289 Darmstadt,
Germany
{ewerton, neumann, lioutikov, peters,

maeda}@ias.tu-darmstadt.de
2 Institute of Robotics and Intelligent Machines, Georgia Institute
of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA

hbenamor@cc.gatech.edu
3 Max Planck Institute for Intelligent Systems, Spemannstr. 38, 72076
Tuebingen, Germany jan.peters@tuebingen.mpg.de

978-1-4799-6923-4/15/$31.00 ©2015 IEEE

Holding tool
Plate handover
Human trajectories

Screw handover
Robot trajectories

Plate handover
Holding tool
Screw handover
Plate handover
Holding tool
Screw handover

Human trajectories

Robot trajectories

Fig. 1. Illustration of a task consisting of multiple interaction patterns,
where each can be represented as an Interaction Primitive. In this work, we
want to learn multiple interaction patterns from an unlabeled data set of
interaction trajectories.

the demonstrations. While this assumption holds for tasks
that cover a small region of the workspace (a high-five task
in [1] or handover of objects in [2]), it limits the use of IPs in
two aspects. First, as illustrated in Fig. 1, a task such as the
assembly of a toolbox consists of several interaction patterns
that differ significantly from each other and therefore can not
be captured by a single Gaussian. Moreover, even within a
single interaction pattern, the correlation between the two
agents may be nonlinear, for example, if the movements of
the human are measured in the Cartesian space, while the
movements of the robot are measured in joint space.
Manually labeling each subtask (e.g. “plate handover",
“screw handover", “holding screw driver") is a way to model
interactions with multiple subtasks. Ideally, however, robots
should be able to identify different subtasks by themselves.
Moreover, it may not be clear to a human how to separate
a number of demonstrated interactions in different, linearly
correlated groups. Thus, a method to learn multiple interaction patterns from unlabeled demonstrations is necessary.
The main contribution of this paper is the development
of such a method. In particular, this work uses Gaussian
Mixture Models (GMMs) to create a Mixture of Interaction

1535

Probabilistic Movement Primitives [2].
The remainder of this paper is organized as follows.
Section II presents related work. In Section III, Probabilistic
Movement Primitives (ProMPs) and Interaction ProMPs are
briefly introduced, followed by the proposition of the main
contribution of this paper: a Mixture of Interaction ProMPs
based on Gaussian Mixture Models (GMMs). Section IV
evaluates the proposed method first on a toy problem that is
useful to clarify the characteristics of the method and then on
a practical application of a collaborative toolbox assembly.
Section V presents conclusions and ideas for future work.
II. R ELATED W ORK
Physical human-robot interaction poses the problem of
both action recognition and movement control. Interaction
dynamics need to be specified in a way that allows for
robust reproduction of the collaborative task under different
external disturbances, and a common approach is based on
direct force sensing or emulation. Rozo et al. [3] propose a
framework for haptic collaboration between a human and a
robot manipulator. Given a set of kinesthetic demonstrations,
their method learns a mapping between measured forces and
the impedance parameters used for actuating the robot, e.g.,
the stiffness of virtual springs governing the collaborative
task. In another force-based approach, Lawitzky et al. [4]
propose learning physical assistance in a collaborative transportation task. In the early learning phase, the robot uses the
measured force values to follow the human guidance during
the task. Recorded force and motion patterns are then used
to learn a Hidden Markov Model (HMM) which can predict
the human’s next action and over time the robot learns to
take over a more active role in the interaction. Kulvicius et
al. [5] also address a transportation task where the two agents
are modeled as two point particles coupled by a spring. The
forces applied by the other agent tell the robot how to adapt
its own trajectory.
Our work differs significantly from the cited works in the
sense that our method does not use nor emulate force signals,
but instead learns the correlation between the trajectories of
two agents. Correlating trajectories not only simplifies the
problem in terms of hardware and planning/control but also
allows us to correlate multi-agent movements that do not
generate force during the interaction, for example, the simple
gesture of asking and receiving an object.
Graphical models have also been used to describe interaction dynamics. In the computer vision community, HMMs
have been widely adopted to model interaction dynamics
from input video streams [6], [7]. As a result, graphical
models have also gained considerable attention in the field
of human-robot interaction. In [8], Hawkins and colleagues
use a Bayes network to improve the fluency in a joint
assembly task. The Bayes network learns to infer the current
state of the interaction, as well as task constraints and the
anticipated timing of human actions. Tanaka et al. [9] use
a Markov model to predict the positions of a worker in an
assembly line. Wang et al. [10] propose the Intention-Driven
Dynamics Model (IDDM) as a probabilistic graphical model

with observations, latent states and intentions where the
transitions between latent states and the mapping from latent
states to observations are modeled as Gaussian Processes.
Koppula et al. [11] use a conditional random field with
sub-activities, human poses, object affordances and object
locations over time. Inference on the graphical model allows
a robot to anticipate human activity and choose a corresponding, preprogrammed robot response. Lee et al. [12]
learn a hierarchical HMM which triggers action primitives
in response to observed behaviors of a human partner.
While very successful for classifying actions, graphical
models, however, may not be the best option when it comes
to generating motions. In [13], for example, the use of a
HMM with discrete states, although very successful in action
classification, introduces artifacts into the motion generation
part that hinders motion generalization. Therefore, a clear
problem in physical human-robot interaction is that while
graphical models may be suitable in the action recognition
domain, motion generation at the continuous level must also
be taken into account. Llorens et al. [14] present a hybrid
design for a robot to be used on the shoulder. In their work,
Petri Nets accounts for discrete control transitions while, at
the motion level, Partial Least Squares Regression has been
used to find the best action of the robot at future time steps.
Perhaps the principal distinction of our method is the
use of Interaction Primitives (IPs), introduced by Ben Amor
et al. [1], initially based on dynamical movement primitives [15] and later extended to Probabilistic Movement
Primitives [16] with action recognition in the work of Maeda
et al. [2]. As shown in [2], Interaction Primitives can be
used to not only recognize the action of an agent, but also
to coordinate the actions of a collaborator at the movement
level; thus overcoming in a single framework both layers of
discrete action recognition and continuous movement control. Differently from [2], where different interaction patterns
must be hand-labeled, our contribution is the unsupervised
learning of a Mixture of Interaction Primitives.
III. M IXTURE OF I NTERACTION P RIMITIVES
In this section, we will briefly discuss the Interaction
Primitive framework based on Probabilistic Movement Primitives [2], [16], followed by the presentation of the proposed
method, based on Gaussian Mixture Models.
A. Probabilistic Movement Primitives
A Probabilistic Movement Primitive (ProMP) [16] is a
movement representation based on a distribution over trajectories. The probabilistic formulation of a movement primitive
allows operations from probability theory to seamlessly
combine primitives, specify via points, and correlate joints
via conditioning. Given a number of demonstrations, ProMPs
are designed to capture the variance of the positions q and
velocities q̇ as well as the covariance between different joints.
For simplicity, let us first consider only the positions q
for one degree of freedom (DOF). The position qt at time
step t can be approximated by a linear combination of basis

1536

functions,
qt = ψtT w + ,

(1)

where  is Gaussian noise. The vector ψt contains the N
basis functions ψi , i ∈ {1, 2, 3, ..., N }, evaluated at time
step t where we will use the standard normalized Gaussian
basis functions.
The weight vector w is a compact representation of a
trajectory1 . Having recorded a number of trajectories of q,
we can infer a probability distribution over the weights w.
Typically, a single Gaussian distibution is used to represent
p(w). While a single w represents a single trajectory, we
can obtain a distribution p(q1:T ) over trajectories q1:T by
integrating w out,
Z
p(q1:T ) = p(q1:T |w)p(w)dw.
(2)

where K = Σw HtT (ΣD + HtT Σw Ht )−1 , ΣD is the
observation noise, and


(ψto )(1,1)
0
0
0



0
(ψto )(P,P ) 0
0 


(5)
Ht = 

0 
0
0
0c(1,1)



0
0
0
0c(Q,Q)
is the observation matrix where the unobserved states of the
robot are filled with zero bases. Here, the human and the
robot are assumed to have P and Q DOFs, respectively.
Now, by combining (1), (3) and (4), we can compute the
probability distribution over the trajectories q1:T given the
observation D. For a detailed implementation the interested
reader is referred to [2].
C. Mixture of Interaction ProMPs

If p(w) is a Gaussian, p(q1:T ) is also Gaussian. The distribution p(q1:T ) is called a Probabilistic Movement Primitive
(ProMP).
B. Interaction ProMP
An Interaction ProMP builds upon the ProMP formulation, with the fundamental difference that we will use a
distribution over the trajectories of all agents involved in
the interaction. Hence, q is multidimensional and contains
the positions in joint angles or Cartesian coordinates of all
agents. In this paper, we are interested in the interaction
between two agents, here defined as the observed agent
(human) and the controlled agent (robot). Thus, the vector q
is now given as q = [(q o )T , (q c )T ]T , where (·)o and (·)c
refer to the observed and controlled agent, respectively.
Let us suppose we have observed a sequence of positions
qto at m specific time steps t, m ≤ T . We will denote this
sequence by D. Given those observations, we want to infer
the most likely remaining trajectory of both the human and
the robot.
Defining w̄ = [woT , wcT ]T as an augmented vector that
contains the weights of the human and of the robot for
one demonstration, we write the conditional probability over
trajectories q1:T given the observations D of the human as
Z
p(q1:T |D) = p(q1:T |w̄)p(w̄|D)dw̄.
(3)
We compute a normal distribution from n demonstrations
by stacking several weight vectors [w̄1 , ..., w̄n ]T , one for
each demonstration, such that w̄ ∼ N (µw , Σw ). A posterior
distribution can be obtained after observing D with
µnew
= µw + K(D − HtT µw ),
w
Σnew
= Σw − K(HtT Σw ),
w

The goal of our method is to learn several interaction
patterns given the weight vectors that parameterize our unlabeled training trajectories. For this purpose, we learn a GMM
in the weight space, using the Expectation-Maximization
algorithm (EM) [17].
Assume a training set with n vectors w̄ representing the concatenated vectors of human-robot weights as
defined in section III-B. In order to implement EM
for a GMM with a number K of Gaussian mixture components, we need to implement the Expectation step and the Maximization step and iterate over
those steps until convergence of the probability distribution
over the weights, p(w̄; α1:K , µ1:K , Σ1:K ), where α1:K =
{α1 , α2 , · · · , αK }, µ1:K = {µ1 , µ2 , · · · , µK } and Σ1:K =
{Σ1 , Σ2 , · · · , ΣK }. Here, αk = p(k), µk and Σk are the
prior probability, the mean and the covariance matrix of
mixture component k, respectively. We initialize the parameters α1:K , µ1:K and Σ1:K using k-means clustering before
starting the Expectation-Maximization loop. The number K
of Gaussian mixture components is found by leave-one-out
cross-validation.
The mixture model can be formalized as
p(w̄) =

K
X

p(k)p(w̄|k) =

k=1

K
X

αk N (w̄; µk , Σk ).

(6)

k=1

Expectation step: Compute the responsibilities rik , where
rik is the probability of cluster k given weight vector w̄i ,
N (w̄i ; µk , Σk )αk
.
rik = p(k|w̄i ) = PK
l=1 αl N (w̄i ; µl , Σl )

(7)

Maximization step: Update the parameters αk , µk and
Σk of each cluster k, using

(4)
nk =

n
X

rik , αk =

i=1
1 In order to cope with the different speeds of execution during demonstration, the trajectories must be time-aligned before parameterization. The
interested reader is referred to [2] for details.

1537

nk
,
n

(8)

,

(9)

Pn
µk =

i=1 rik w̄i

nk

1
Σk =
nk

n
X

!
rik (w̄i − µk )(w̄i − µk )

T

.

(10)

i=1

Finally, we want to use our model to infer the trajectories
of the controlled agent given observations from the observed
agents. We need to find the posterior probability distribution
over trajectories q1:T given the observations D, as in Section
III-B.
In order to compute this posterior using our GMM prior,
first we find the most probable cluster k ∗ given the observation D, using the Bayes’ theorem. The posterior over the
clusters k given the observation D is given by
p(k|D) ∝ p(D|k)p(k),
where

Algorithm 1 Training
1) Parameterize demonstrated trajectories:
Find vector of weights w̄ for each trajectory, such that qt ≈
ψtT w̄.
2) Find GMM in parameter space, using EM:
Initialize GMM parameters α1:K , µ1:K and Σ1:K with kmeans clustering.
repeat
E step
N (w̄i ; µk , Σk )αk
rik = p(k|w̄i ) = PK
l=1 αl N (w̄i ; µl , Σl )

(11)

M step

Z
p(D|k) =

nk =

p(D|w̄)p(w̄|k)dw̄

n
X

rik , αk =

i=1

nk
n

Pn

and
p(w̄|k) = N (w̄; µk , Σk ).
∗

Thus the most probable cluster k given the observation
D is
k ∗ = arg max p(k|D).
(12)
k

The output of the proposed algorithm is the posterior
probability distribution over trajectories q1:T , conditioning
cluster k ∗ to the observation D,
Z
p (q1:T |D) =

p (q1:T |w̄) p (w̄|k ∗ , D) dw̄.

i=1 rik w̄i

µk =
1
Σk =
nk

n
X

nk
!
T

rik (w̄i − µk )(w̄i − µk )

i=1

until p(w̄; α1:K , µ1:K , Σ1:K ) converges

Algorithm 2 Inference
1) Find most probable cluster given observation:

(13)
p(k|D) ∝ p(D|k)p(k)

Algorithms 1 and 2 provide a compact description of the
proposed methods for training and inference, respectively.
IV. E XPERIMENTS
This section presents experimental results in two different
scenarios using a 7-DOF KUKA lightweight arm with a 5finger hand2 .
The goal of the first scenario is to expose the issue of
the original Interaction Primitives [1], [2] when dealing with
trajectories that have a clear multimodal distribution. In the
second scenario we propose a real application of our method
where the robot assistant acts as a third hand of a worker
assembling a toolbox (please, refer to the accompanying
video3 ).
A. Nonlinear Correlations between the Human and the
Robot on a Single Task
To expose the capability of our method of dealing with
multimodal distributions, we propose a toy problem where
a human specifies a position on a table and the robot must
point at the same position. The robot is not provided any
form of exteroceptive sensors; the only way it is capable to
generate the appropriate pointing trajectory is by correlating
2 Regarding the control of the robot, the design of a stochastic controller
capable of reproducing the distribution of trajectories is also part of ProMPs
and the interested reader is referred to [16] for details. Here we use a
compliant, human-safe standard inverse-dynamics based feedback controller.
3 Also available at http://youtu.be/9XwqW_V0bDw

k ∗ = arg max p(k|D)
k

2) Condition on observation, using cluster k ∗ :
Z
p(q1:T |D) = p(q1:T |w̄)p(w̄|k ∗ , D)dw̄

its movement with the trajectories of the human. As shown
in Fig. 2, however, we placed a pole in front of the robot
such that the robot can only achieve the position specified by
the human by moving either to the right or to the left of the
pole. This scenario forces the robot to assume quite different
configurations, depending on which side of the pole its arm
is moving around.
During demonstrations the robot was moved by kinesthetic
teaching to point at the same positions indicated by the
human (tracked by motion capture) without touching the
pole. For certain positions, as the one indicated by the
arrow in Fig. 2(a), only one demonstration was possible. For
other positions, both right and left demonstrations could be
provided as shown in Fig. 2(a) and 2(b). The demonstrations,
totaling 28 pairs of human-robot trajectories, resulted in a
multimodal distribution of right and left trajectory patterns
moving around the pole.
In this scenario, modeling the whole distribution over

1538

0.1

RMS error (m)

0.08
0.06
0.04
0.02
0

2

4

6

8

10

12

14

16

Number of clusters

Fig. 4.
(a)

Root Mean Square Error with models using up to 17 Gaussians.

(b)

Fig. 2. Experimental setup of a toy problem used to illustrate the properties
of the Mixture of Interaction Primitives. The robot is driven by kinesthetic
teaching to point at the positions specified by the human (pointed with
the wand). Certain pointed positions can be achieved by either moving the
arm to the right (a) or to left (b) of the pole placed on the table. Other
positions, such as the one indicated by the arrow, can only be achieved by
one interaction pattern.
ground truth
prediction

(a)

(b)

Fig. 3. Results of the predictions of the robot trajectories in Cartesian
space. Both subplots show the same ground truth trajectories generated
by driving the robot in kinesthetic teaching. The predictions are generated
by leave-one-out cross-validation on the whole data set comprised of 28
demonstrations. (a) Prediction using the conventional Interaction ProMPs
with a single Gaussian. (b) Prediction using the proposed method with a
mixture of Gaussians.

the parameters of the trajectories with one single Gaussian
(as in the original Interaction Primitive formulation) is not
capable of generalizing the movements of the robot to
other positions in a way that resembles the training, as the
original framework is limited by assuming a single pattern.
This limitation is clearly shown in Fig. 3(a), where several
trajectories generated by a single cluster GMM (as in the
original Interaction Primitive) cross over the middle of the
demonstrated trajectories, which, in fact, represents the mean
of the single Gaussian distribution.
Fig. 3(b) shows the predictions using the proposed method
with a mixture of Gaussians. By modeling the distribution
over the parameters of the trajectories using GMMs as
described in section III-C, a much better performance could
be achieved. The GMM assumption that the parameters are
only locally linear correlated seemed to represent the data
much more accurately. As shown in Fig. 4, this improvement
is quantified in terms of the Root Mean Square (RMS) Error
of the prediction of the trajectory in relation to the ground
truth using leave-one-out cross-validation over the whole data
set. The same figure also shows that there is a sharp decrease
in the RMS error up to six clusters, especially when taking
into account the variance among the 28 tests. Beyond seven

clusters it is observed that the prediction error fluctuates
around 4 cm. The experiments previously shown in Fig. 3(b)
were done with eight clusters.
B. Assembling a Box with a Robot Assistant
In this experiment, we recorded a number of demonstrations of different interaction patterns between a human
and the robot cooperating to assemble a box. We used the
same robot described in the previous experiment. During
demonstrations, the human wore a bracelet with markers
whose trajectories in Cartesian coordinates were recorded
by motion capture. Similarly to the first scenario, the robot
was moved in gravity compensation mode by another human
during the training phase and the trajectories of the robot in
joint space were recorded.
There are three interaction patterns. Each interaction pattern was demonstrated several times to reveal the variance of
the movements. In one of them, the human extends his/her
hand to receive a plate. The robot fetches a plate from a
stand and gives it to the human. In a second interaction, the
human fetches the screwdriver, the robot grasps and gives
a screw to the human as a pre-emptive collaborator would
do. The third type of interaction consists of giving/receiving
a screwdriver. Each interaction of plate handover, screw
handover and holding the screwdriver was demonstrated 15,
20, and 13 times, respectively. The pairs of trajectories of
each interaction are shown in Fig. 54 .
As described in section III, all training data are fed to
the algorithm resulting in 48 human-robot pairs of unlabeled
demonstrations as shown in the upper row of Fig. 7. The
presented method parameterizes the trajectories and performs
clustering in the parameter space in order to encode the
mixture of primitives. In the upper row of Fig. 7, each
mixture is represented by a different color. The human is
represented by the (x, y, z) Cartesian coordinates while the
robot is represented by the seven joints of the arm. The figure
shows the first four joints of the robot (starting from the
base).
4 Due to the experimental setup, for the sub-tasks of plate and screw
handover we added an initial hand-coded trajectory that runs before the
kinesthetic teaching effectively starts. These trajectories are used to make
the robot grasp and remove the plate or screw from their respective stands.
This is reflected in the figure as the deterministic part at the beginning of
the trajectory of the robot. This initial trajectory, however, has no effect on
the proposed method itself.

1539

human

human
human

robot

robot

robot

(a) Handing over a plate

(b) Handing over a screw

(c) Holding the screw driver

Joint RMS prediction error (deg)

Fig. 5. Demonstrations of the three different interactions and their respective trajectories. For the case of plate and screw handover the beginning of the
robot trajectory shows a deterministic part that accounts for the fact that the robot has to remove objects from their respective stands, which is not part of
the kinesthetic teaching and does not affect the algorithm in any sense.

40

30

20

10

0

1

2

3

4

5

6

7

8

Number of clusters

Fig. 6. Root Mean Square Error of the joint trajectories (averaged over all
tests) using a leave-one-out cross-validation as a function of the number of
clusters (mixture components). The plateau after three clusters seems to be
consistent with the training data since it consists of three distinct interaction
patterns.

Figure 6 shows the RMS prediction error averaged over
all tests as the number of mixture components increase. The
prediction is obtained by leave-one-out cross-validation over
the whole set of 48 demonstrations. As one would expect,
since the unlabeled data contains three distinct interaction
patterns, the improvement is clearly visible up to three
mixture components. No significant improvement is obtained
afterwards, thus the GMM with three mixture components
was selected for experiments.
In the inference/execution phase, the algorithm first computes the most probable Interaction Primitive mixture component based on the observation of the position of the wrist
of the human with (12). Using the same observation, we
then condition the most probable Interaction Primitive, which
allows computing a posterior distribution over trajectories for
all seven joints of the robot arm as in (13). Finally, the mean
of each joint posterior distribution is fed to a standard inverse

dynamics feedback tracking controller.
The lower row of Fig. 7 depicts the posterior distribution
for one test example where a three-cluster GMM was trained
with the other 47 trajectories. The GMM prior is shown in
gray where the patches of different clusters overlap. The
observation consists only of the final position of the wrist,
shown as asterisks in the figure. The black lines are the
ground truth trajectories of each degree of freedom. The
posterior, in red, is represented by its mean and by the region
inside ± two standard deviations. The mean of this posterior
is the most probable trajectory for each degree of freedom
given the observed end position of the wrist of the human.
We assembled the toolbox, consisting of seven parts and
12 screws, two times. The experiments demanded more than
40 executions of the Interaction Primitives. The selection of
the right mixture component was 100% correct. (Please refer
to the accompanying video).
We evaluated the precision of the interactions by computing the final position of the hand of the robot with
forward kinematics. The forward kinematics was fed with
the conditioned robot trajectories predicted by leave-oneout cross validation. The interactions of plate handover
and holding screwdriver resulted in mean error with two
standard deviations (mean error ±2σ) of 3.2 ± 2.6 cm and
2.1 ± 2.3 cm, respectively. We did not evaluate the precision
of the handover of the screw, as the position at which
the robot hands the screw is not correlated to the human
(please refer to the accompanying video). As an example,
Fig. 8 shows the robot executing the plate handover at three
different positions based on the location of the wrist marker.
Note that the postures of the arm are very different, although
they are all captured by the same Interaction Primitive.

1540

Fig. 7. Upper row: Mixture components represented by their mean trajectories and the region inside two standard deviations (µ ± 2σ). Each mixture
component is represented by a different color and corresponds to a different interaction pattern. The light gray trajectories are the training trajectories. Obs.:
The plots show only the part of the trajectories generated by kinesthetic teaching. Lower row: Posterior probability distribution (red) given observation
depicted by the blue asterisks. The GMM prior is shown in gray.

addressing the estimation of the phase of the execution of
the primitive for switching tasks in real time. Also, we
are addressing the use of the stochastic feedback controller
provided by the original ProMP work in [16]. Although this
work focused on human-robot trajectories, we are currently
considering extensions of our work where the human is
replaced by other variables of interest. For example, the
same framework can be used to correlate joint and endeffector trajectories of the same robot to learn nonlinear
forward/inverse kinematic models. Similarly the Mixture of
Interaction Primitives can be used to correlate the interaction
between motor commands and joint trajectories to learn
inverse dynamics models.
VI. ACKNOWLEDGMENTS

Fig. 8. Handover of a plate. Conditioning on three different positions of
the wrist (using motion capture) of a human coworker.

V. C ONCLUSIONS

The research leading to these results has received funding
from the project BIMROB of the “Forum für interdisziplinäre
Forschung” (FiF) of the TU Darmstadt, from the European Community’s Seventh Framework Programme (FP7ICT-2013-10) under grant agreement 610878 (3rdHand) and
from the European Community’s Seventh Framework Programme (FP7-ICT-2009-6) under grant agreement 270327
(ComPLACS).

In this paper we presented a Mixture of Interaction
Primitives where Gaussian Mixture Models are used to
model multiple interaction patterns from unlabeled data. The
multimodal prior probability distribution is obtained over parameterized demonstration trajectories of two agents working
in collaboration. During the execution, the algorithm selects
the mixture component with the highest probability given the
observation of the human, which is then conditioned to infer
the appropriate robot reaction. The proposed method is able
to learn and recognize multiple human-robot collaboration
tasks from an arbitrary number of demonstrations consisting
of unlabeled interaction patterns, what was not possible with
the previous Interaction Primitive framework.
In the context of human-robot interaction we are currently
1541

R EFERENCES
[1] H. Ben Amor, G. Neumann, S. Kamthe, O. Kroemer, and J. Peters,
“Interaction primitives for human-robot cooperation tasks,” in Proceedings of 2014 IEEE International Conference on Robotics and
Automation (ICRA), 2014.
[2] G. Maeda, M. Ewerton, R. Lioutikov, H. Ben Amor, J. Peters, and
G. Neumann, “Learning interaction for collaborative tasks with probabilistic movement primitives,” in Proceedings of the International
Conference on Humanoid Robots (HUMANOIDS), 2014.
[3] L. Rozo, S. Calinon, D. G. Caldwell, P. Jimenez, and C. Torras,
“Learning collaborative impedance-based robot behaviors,” in AAAI
Conference on Artificial Intelligence, Bellevue, Washington, USA,
2013.
[4] M. Lawitzky, J. Medina, D. Lee, and S. Hirche, “Feedback motion
planning and learning from demonstration in physical robotic assistance: differences and synergies,” in Intelligent Robots and Systems
(IROS), 2012 IEEE/RSJ International Conference on, Oct 2012, pp.
3646–3652.

[5] T. Kulvicius, M. Biehl, M. J. Aein, M. Tamosiunaite, and F. Wörgötter, “Interaction learning for dynamic movement primitives used in
cooperative robotic tasks,” Robotics and Autonomous Systems, vol. 61,
no. 12, pp. 1450–1459, 2013.
[6] M. Brand, N. Oliver, and A. Pentland, “Coupled hidden markov
models for complex action recognition,” in Proceedings of the 1997
Conference on Computer Vision and Pattern Recognition (CVPR ’97),
ser. CVPR ’97. Washington, DC, USA: IEEE Computer Society,
1997, pp. 994–.
[7] N. Oliver, B. Rosario, and A. Pentland, “A bayesian computer vision
system for modeling human interactions,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 831–843,
Aug 2000.
[8] K. Hawkins, N. Vo, S. Bansal, and A. F. Bobic, “Probabilistic human
action prediction and wait-sensitive planning for responsive humanrobot collaboration,” in Proceedings of the International Conference
on Humanoid Robots (HUMANOIDS), 2013.
[9] Y. Tanaka, J. Kinugawa, Y. Sugahara, and K. Kosuge, “Motion
planning with worker’s trajectory prediction for assembly task partner
robot,” in Proceedings of the 2012 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS). IEEE, 2012, pp. 1525–
1532.
[10] Z. Wang, K. Mülling, M. P. Deisenroth, H. Ben Amor, D. Vogt,
B. Schölkopf, and J. Peters, “Probabilistic movement modeling for
intention inference in human–robot interaction,” The International
Journal of Robotics Research, vol. 32, no. 7, pp. 841–858, 2013.
[11] H. S. Koppula and A. Saxena, “Anticipating human activities using
object affordances for reactive robotic response.” in Robotics: Science
and Systems, 2013.
[12] D. Lee, C. Ott, Y. Nakamura, and G. Hirzinger, “Physical human robot
interaction in imitation learning,” in Robotics and Automation (ICRA),
2011 IEEE International Conference on. IEEE, 2011, pp. 3439–3440.
[13] H. Ben Amor, D. Vogt, M. Ewerton, E. Berger, B. Jung, and J. Peters,
“Learning responsive robot behavior by imitation,” in Proceedings of
the 2013 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS), 2013, pp. 3257–3264.
[14] B. Llorens-Bonilla and H. H. Asada, “A robot on the shoulder:
Coordinated human-wearable robot control using coloured petri nets
and partial least squares predictions,” in Proceedings of the 2014 IEEE
International Conference on Robotics and Automation, 2014.
[15] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal,
“Dynamical movement primitives: learning attractor models for motor
behaviors,” Neural computation, vol. 25, no. 2, pp. 328–373, 2013.
[16] A. Paraschos, C. Daniel, J. Peters, and G. Neumann, “Probabilistic
movement primitives,” in Advances in Neural Information Processing
Systems (NIPS), 2013, pp. 2616–2624.
[17] C. M. Bishop et al., Pattern recognition and machine learning.
springer New York, 2006, vol. 1.

1542

The 18th IEEE International Symposium on
Robot and Human Interactive Communication
Toyama, Japan, Sept. 27-Oct. 2, 2009

WeIAH.14

Physical Interaction Learning: Behavior Adaptation in Cooperative
Human-Robot Tasks Involving Physical Contact
Shuhei Ikemoto*
Takashi Minato
Heni Ben Amor
JST, ERATO and
JST, ERATO
VR and Multimedia Group
Adaptive Machine Systems
Asada Project
TU Bergakademie Freiberg
Osaka University
Osaka, Japan
Freiberg, Germany
Osaka, Japan
Hiroshi Ishiguro
Bernhard Jung
JST, ERATO and
VR and Multimedia Group
Adaptive Machine System
TU Bergakademie Freiberg
Osaka University
Freiberg, Germany
Osaka, Japan

Abstract— In order for humans and robots to engage in
direct physical interaction several requirements have to be met.
Among others, robots need to be able to adapt their behavior
in order to facilitate the interaction with a human partner. This
can be achieved using machine learning techniques. However,
most machine learning scenarios to-date do not address the
question of how learning can be achieved for tightly coupled,
physical touch interactions between the learning agent and a
human partner. This paper presents an example for such human
in-the-loop learning scenarios and proposes a computationally
cheap learning algorithm for this purpose. The efficiency of
this method is evaluated in an experiment, where human care
givers help an android robot to stand up.

I. I NTRODUCTION
Robot technology has come a far way from large, unsafe
manufacturing machines, to highly sophisticated androids
with human-like appearance. As this technology continues
to improve, the application domains of robots also keep
coming closer to our everyday life. So far, the most common
type of robots, namely industrial robots have primarily
inhabited dedicated working environments in factories. For
a human, entering such a workspace can result in severe
injuries. Recent robotic developments, however, are more
and more targeted at domestic environments and assistive
tasks, where human-robot interaction is indispensable. For
humans and robots to share a common living environment,
several requirements need to be met. First, all physical
contact between the interaction partners needs to be safe,
in particular meaning that the human being is never harmed.
Next, the robot needs to be able to adapt its behavior to the
environment and the actions of the human partner. Ideally, the
robot should also learn from previous interaction experiences
and modify the behavior according to received critiques.
Learning and adaptation has been intensively studied in
the robotics community. In particular, imitation learning has
proved to be a promising way of teaching new skills without
the need for tedious manual programming. However, research
*Corresponding author. Email: ikemoto@ed.ams.eng.osaka-u.ac.jp

978-1-4244-5081-7/09/$26.00 ©2009 IEEE

in this area mostly considers scenarios where teacher and
learner rely on communication interfaces such as motion
capture or use simple symbolic communication. In contrast,
recent research such as [5] has considered the direct physical
interaction between the communication partners, based on
kinesthetic and haptic feedback. While these works constitute an important step towards the meaningful coexistence
and interaction of humans and robots, the robot is mostly
assumed to take a passive role during the learning task.
In this paper we introduce a physical human robot interaction scenario with a tight coupling between the human
instructor and the learning robot. Inspired by the parenting
behavior observed in humans, a test subject is asked to
physically assist a state-of-the-art robot in a standing up
motion. Both human and robot need to adapt their behavior,
such that they can cooperatively solve the task. In particular,
this also means that the robot needs to react appropriately to
the force applied by the human instructor. After each trial,
the human can judge whether the interaction was successful
or not and the resulting critique is used by a machine learning
algorithm to update the behavior of the robot. As learning
progresses, the robot creates a behavioral model, which
implicitly includes the actions of the human counterpart. To
ensure that safety is always guaranteed, the robot is equipped
with pneumatically actuated flexible-joints. The robot joints
have a high flexibility in response to externally applied force
and allow for both passive and active reactions.
We argue, that human-in-the-loop learning scenarios, such
as the one presented here, will be particularly interesting
in the future, as they can help to strengthen the mutual
relationship between humans and robots. Ideally, this will
lead to a higher acceptance of robotic agents in our society.
II. R ELATED W ORK
Important aspects of Physical Human Robot Interaction
(PHRI) have been investigated in a perspective research
project conducted by European Network of Excellence (EURON) [3]. The project’s objective was to lay out and dis-

504

Fig. 1. Left: Overview of the physical interaction learning approach used in this paper: after physical interaction the human judges whether the interaction
was successful or not. This information is stored in the memory and later used for learning. Right: the flexible-joint humanoid robot used in the experiments
of this paper.

cuss important requirements for safe and dependable robots
involved in PHRI. Initial approaches to achieving these
requirements are currently being addressed in a follow-up
research project called PHRIENDS1 . The primary goal of
the project is to design robots that are intrinsically safe, in
order to reduce risks and fatalities in industrial manufacturing
workplaces. For this, new actuator concepts, safety measures
and control algorithms are being developed, which take the
presence of human subjects into account. The results of
this project are also relevant to applications outside the
manufacturing industry. However, learning and adaptation
between humans and robots is not in the focus of the project.
In [7], Khatib et al. discuss basic capabilities needed to
enable robots to operate in human-populated environments.
In particular, they discuss how mobile robots can calculate
collision free paths and manipulate surrounding objects.
For this, they characterize free space using an elastic strip
approach. The described robots, however, were not expected
to get into direct (physical) contact with surrounding human
subjects. The importance of direct physical interaction was
highlighted in the Haptic Creature project [10]. The project
investigates the role of affective touch in fostering the
companionship between humans and robots. Another attempt
to close human-robot interaction is the work presented in
[6]. Here, Kosuge et al. present a robot that can dance with
a human by adaptively changing the dance steps according
to the force/moment applied on it. In [1], Berger et al. use
kinesthetic interactions to teach new behaviors to a small
humanoid robot. Additionally, the behavior can be further
optimized with respect to a given criterion in simulation. In
this learning scheme the robot is a purely passive interaction
partner and only acts after learning is finished. Similar
approaches to teaching new skills have also been employed in
[2] and [9] using different learning methods, i.e. Continuous
Time Recurrent Neural Networks and Gaussian Mixture
Models respectively. Odashima and colleagues developed a
robot that can have direct physical contact with humans. The
1 Physical

Human-Robot Interaction which is Dependable and Safe

robot is intended for care tasks, such as carrying injured
persons to nearby physicians. The robot can also learn new
behaviors and assistive tasks by observing human experts
performing them. However, this learning does not take place
during interaction, but in offline sessions using immersive
virtual environments. In this paper we present experiments
with a soft skin robot that is involved in close physical
interaction with a human caregiver. In contrast to the above
research, both human and robot play an active role in the
interaction. Further, they both learn to adapt their behavior
to the interaction partner so as to achieve a common goal.
This tight coupling of robot and human learning and coadaptation is a distinctive feature and is the main focus of
the work to be presented.
III. P HYSICAL I NTERACTION L EARNING A PPROACH
The goal of interaction learning is to improve the cooperation of humans and robots while they are working to
achieve a common goal. In Figure 1 we see an overview
of the learning scheme employed in this paper. After an
initial physical interaction between a human and a robot,
the human is given the chance to evaluate the behavior of
the robot. More precisely, the human can judge whether
the interaction was successful or not. The feedback can be
done in various ways, such as through touching or through a
simple graphical user interface. Once the critique information
is collected by the robot system, it is stored in a memory
database. The memory’s task is to collect information about
recent successful interactions and manage the data for the
later learning step. The idea is based on the human short-term
memory. It allows us to optimize the set of training examples
used for learning, in order to improve learning quality.
After a number of interactions, the learning system queries
the memory for a set of new training data. The data is
then projected onto a low-dimensional manifold using dimensional reduction techniques. There are three justifications
for this step. First, dimensional reduction allows it for
a reduction of the space in which learning takes place.
From this follows that learning can be much faster and

505

Fig. 2. The three desired postures used in the stand-up behavior of the experiment. The task of learning is to determine ideal switching conditions between
the desired postures.

more efficient. Although the system described here does
not aim at biological plausibility, it can still be argued
that dimensional reduction also takes place in the human
brain. Biologically inspired neural networks such as SelfOrganizing Maps perform dimensional reduction and create
topographic maps such as those found in the human and
animal brain. Dimensional reduction generally helps to detect
meaningful low-dimensional structures in high-dimensional
inputs. Second, dimensional reduction allows us to visualize
and understand the adaptation taking place during interaction. This is particularly helpful for later review and analysis
purposes. Finally, it also reduces the negative influence of
outliers on learning. The inputs to the dimensional reduction
step are high-dimensional state vectors describing the robot’s
postures during the interaction. Output is a low-dimensional
posture space.
Once the state vectors are projected onto a lowdimensional manifold, we group the resulting points into
sets according to the action performed in that state. Thus,
we get for each possible action a set of states in which the
corresponding action should be triggered. For each action,
a Gaussian Mixture Model is learned. The model encodes a
probability density function of the learned state vectors. By
computing the likelihood of a given state vector p in a GMM
of action A, we can estimate how likely it is that the robot
should perform action A when in posture p. The learned
models are then used during the next physical interaction
trial to determine the robot’s actions. For this, each new
posture is projected into the low-dimensional posture space.
Then, the likelihood of the projected point for each GMM
is computed. Following a maximum-likelihood rationale the
action corresponding to the GMM with the highest likelihood
is then executed by the robot.
With each iteration of the above learning loop, the robot’s
adapts it’s model more and more towards successful interactions. The result is a smoother and easier cooperative
behavior between the human and the robot.
A. The CB2 Robot
The robot used in this study is called Child-robot with
Biomimetic Body or CB2 [8]. The robot has the following

features:
•

•

•
•

It is 130 cm high and weighs about 33 kg.
It has 56 degrees of freedom (DOFs).
All joints, apart from the joints used to move the eyes
and eyelids, are driven by pneumatic actuators.
All joints, apart from the joints used to move the fingers,
have potentiometers.

The joints have low mechanical impedance due to the compressibility of air. The joints can also be made completely
passive if the system discontinues the air compression during
robot motion. This helps the robot to perform passive motion
during physical interaction and helps to ensure the human
helper’s safety. This is in contrast to most other robots, where
the joints are driven by electric motors with decelerators. Due
to the flexible actuators, the joints produce smooth-looking
motion, even when the input signal changes drastically. This
feature of the CB2 robot is used to realize complex motions
using a simple control architecture. More specifically, full
body motions of the robot are realized by switching between
a set of successive desired postures. Each posture is described
by a posture vector x , with each entry of the vector denoting
the angular value of a particular joint. A low-level controller
is implemented by PD-control of angular values. Each time
the desired posture is drastically switched, large drive torques
are generated, resulting in active force applied to the human
caregiver. As the robot’s posture approaches the desired
posture, the passive motion gradually becomes the dominant
motion of the robot.
In Figure 2 we see how the rising-up behavior used
throughout this paper is realized in our control architecture.
The behavior is realized by switching between three desired
postures. At first glance this approach renders the specification of the robot motion extremely simple. However, the
switching times are highly dependent on the human interaction counterpart. More specifically, the switching times
depend on the anatomy and skills of the human. This means
that the robot has to adapt the switching times to his partner’s
characteristics while the interaction is going on.

506

B. Learning Method
In the uprising task explained in this paper, the goal of
learning is to determine an ideal timing for switching actions
s ∈ S between different desired postures. This is achieved by
learning three different probabilistic low-dimensional posture
models, one for the case that no switching occurs, one for
the first switching action and one for the second switching
action.
At each time step of an interaction between a human and
the robot, the posture of the latter and the current desired
posture is recorded. The robot posture r is a 52 dimensional
vector coding the current angular value of each joint. After
the interaction is finished, the postures are stored in the
memory. The memory database holds information of the
last 10 interactions. Although there are many possible ways
how new data is integrated in to the database, the general
policy used here is:“new data overwrites old data, successful
interactions overwrite failed interactions”.
After 10 interactions, the training data from the memory
is used for learning. First, dimensional reduction is applied
on the data. While many methods can be applied for this
task, we used PCA in this paper. To perform PCA, the mean
rm is subtracted from all recorded posture vectors and the
covariance matrix M of the resulting points is computed. A
singular value decomposition (SVD) on M yields matrices
U,V and W , such that:
M = UWV T

(1)

The columns of matrix V contain orthonormal vectors called
the eigenvectors or principal components of matrix M. The
matrix W is a diagonal matrix containing the singular values.
Each principal component (PC) has a corresponding singular
value which indicates how much information of the data set
it covers. The first few PC’s are then used as the axes of
our lower dimensional PCA space. Given a new data point
we can compute its coordinates in PCA space by subtracting
the mean and calculating the dot product for each of the
principal components.
Next, we compute a GMM for each of the three switching
classes. For this we divide the projected data points into
distinct sets. If no switching occured, then the corresponding
point is assigned to the first data set. Otherwise, it is assigned
to one of the other two sets. For each set of projected points,
we learn a probability density function by a weighted sum
of K Gaussian distributions:
K

p(x) =

∑ πk

p(x|k)

(2)

k=1

with πk being the weight of the k-th Gaussian and p(x|k) being the conditional density function. The conditional density
function is a d-dimensional Gaussian distribution:
−1
1
T
1
(3)
e− 2 ((x−µk ) Ck (x−µk ))
p(x|k) = √ d p
2π
det(Ck )
with mean µk and covariance matrix Ck . The above p(x|k)
can also be written as N (x|µk ,Ck ). To estimate the parameters {µk ,Ck , πk } for each of the Gaussian kernels the

Fig. 3. Interaction data of several stand-up interactions projected into a
low-dimensional posture space. Each point corresponds to one posture of
the robot.

Expectation-Maximization (EM) [4] algorithm is used. Frotunately, performing the EM algorithm in low-dimensional
spaces improves the convergence of the algorithm.
After learning, we end up with three GMMs coding
three probability density functions p1 (x), p2 (x), p3 (x). Each
probability density function can be used to determine the
probability of a point in low-dimensional posture space
with respect to a particular switching action. For example,
computing p2 (r) for a given projected robot posture r, returns
the likelihood of the robot having to switch from the second
to the third desired posture when being in state r.
When the next interaction with the human is started, the
robot can use the new learned model to decide in which
state it is and which desired posture to take on. For this,
the current joint values are projected onto the learned lowdimensional posture space. The result is a d-dimensional
point. The optimal desired next switching action can be
computed in a maximum-likelihood fashion using:
snext = argmax ps (x)

(4)

s∈S

In each step of the control loop, the robot calculates snext
and sends the angular values of the corresponding desired
posture to a low-level controller. The controller then computes the needed joint torques to take on this posture. After
the interaction is finished, the human critique information
is collected and used to update the memory. After that, the
learning loop is repeated. In Figure 3 we see an example of
a set of interactions projected onto a low-dimensional space.
Each point in the plot represents one posture of the CB2 robot
during an interaction. The points were colored according to
the desired posture which was active in that particular time
step.
IV. E XPERIMENT AND R ESULTS
In order to investigate tightly coupled adaptation and the
learning scheme proposed in this paper, we conducted an
PHRI experiment using the rising-up interaction introduced
earlier. In the experiment two subjects were asked to assist
the robot in standing up. The first test-subject was part of

507

Fig. 4. Sequential snapshots of the first (top) and last (bottom) interaction of the test subjects with the robot. Left we see the expert user, right the
beginner. The white curve depicts the change in the robot’s hips position. The center figures of each sequential snapshot shows how the robot learns in
both cases to have a strong contact between the feet and the ground.

the research team working on the CB2 robot and, thus,
often exposed to interactions with the robot. This test-subject
will be referred to as expert subject in the following. The
second user, has not been exposed to similar interactions in
the past and will be referred to as beginner. The subjects
had to repeatedly help the robot. After every 10 trials, the
accumulated data in the memory was used for learning a
new model, according to the learning scheme described in
Section III-B. In total 30 interactions with 2 learning steps
in between were conducted.
Figure 4 shows sequential snapshots of first and last
interactions for each subject. The upper row of pictures
shows the first interactions, while the lower row of pictures
shows interactions after learning. The white dashed line
indicates the height of the hips in each snapshot. In the
figures we can observe a smoother transition of the hip
height after the learning interaction, than before the learning
interaction. Especially, in the center figures, we can see a
strong contact between feet and the ground and an increased
hip height after learning, in contrast to the poor contact with
ungainly leg posture shown before learning. How much the
human helped the robot in the task, and how the human
evaluates the robot performance can be a subjective matter.
Therefore, in our evaluation we focus only on whether the
robot motion is refined such that inefficient and jerky motions
are avoided.
Figure 5 shows three interactions for each user which were
projected into the low-dimensional posture space. Each interaction is represented by a curve which reflects the robot’s
postures during the interaction. The black curve indicates the
robot’s postures during the initial phase, while the red and
green curve indicate the robot’s postures after the first and
second learning step respectively. In the case of the expert
user (left), we can clearly see that with each learning step, the
interaction becomes smoother and shorter. In the initial phase
the robot motion oscillates around the point (0.5, −2.0)T . It
might be caused by inefficient robot motion as discussed
above. After each learning step, the robot motion becomes

Fig. 5. Projected interactions in the low-dimensional posture space. Left:
expert. Right: beginner.

smoother and more efficient, and the corresponding trajectory
becomes smoother and shorter. In the diagram describing the
beginner subject (right), this feature is not similarly obvious.
In order to confirm the above discussion we quantified
the robot motion using the posture change norm. The posture change norm a of the robot was calculated using the
Euclidean distance between the data of t and t − 1 in the
posture space X defined by using each joint angle as a base.
a(t) =k x (t) − x (t−1) k2 , x ∈ X .

(5)

Computing the posture change norm at each time step of
the interaction results in the time series depicted in Figure 6.
The black time series’ show the posture change norm during
the initial interaction phase. We can see various sudden peaks
indicating large changes in the robot posture and, thus, nonsmooth motion. In particular in the case of the expert user, we
can find high peaks (around 1000 msec). This is undesirable,
as large changes in the robot posture result from strong
forces acting on it. The green and red time series’ show
the evolution of the norm after each learning step. With
each learning step, the amount and number of peaks in the
time series is reduced. In other words, the fluctuations in the
posture change norm decrease leading to a smoother and a
more efficient motion. This supports the hypothesis, that the
introduced learning method improves PHRI.
A statistical analysis of the data further underlines the
above hypothesis. For this, we computed the sum and vari-

508

Fig. 6. Evolution of the posture change norm with increasing number of
learning steps.

Fig. 7. The sum and stddev of the posture change norm of the expert
and beginner subject. With each learning step, the amount of these two
values decreases and the movement of the robot becomes smoother and
more synchronized with that of the subject.

ance of the posture change norm during the interactions.
Figure 7 shows the evolution of these values with each
learning step. In both cases, we see that the sum and variance
of the posture change norm decreased as the experiment
progressed. These results are backed by a t-test confirming
that the difference between the situation before and after
learning is statistically significant.
In the light of the above results, an interesting question
was raised: “Are the measured differences really due to
a symmetric learning process, in which both human and
robot adapt their behavior?”. In other words: does the robot
learning system have any effect on the evolution of interaction? To investigate this question, we conducted a baseline
experiment, in which we repeated the introduced experiment
in slightly different setting. This time, learning in the robot
was turned off, and fixed time steps were used for switching
between the postures. Thus, the only kind of adaptation that
is possible in this scenario, is the adaptation of the human
towards the robot. Comparing these baseline results with the
previously achieved results showed a significant difference,
answering the above question, and affirming that the results
achieved with the introduced probabilistic low-dimensional
posture maps system are due to a bilateral learning process
taking placing in both the human and the robot.
V. C ONCLUSION

be achieved through coordinated actions involving physical
contact. For this, we introduced a simple machine learning
algorithm for adapting the behavior of the robot according
to received critique from the human interaction partner. The
method has a low computational load, can be run online
while the interaction with the robot is going on and needs
relatively few training data. In contrast to previous work in
this field, the robot in this study is in close physical contact
with the human partner and plays an active role during the
execution of the cooperative task. The CB2 robot, through
its flexible-joint design and soft silicone skin, is particularly
suited for such tasks, as physical interactions become more
”natural” and lifelike. In an experiment inspired by the
parenting behavior in humans, we were able to show that
the proposed learning method results in measurable improvements of the interaction. Quantitative evaluations based on
the posture change norm confirm the significance of these
improvements.
In the future, we aim at investigating more complex
interactive behaviors. Additionally, we hope to include an
exploration phase into the learning algorithm, in which the
robot can try out different variants of a behavior in order to
find out, which one is best suited for the interaction partner.
Further, we hope to include non-binary feedback from the
human by using touch sensors or other input devices.
R EFERENCES
[1] E. Berger, H. Ben Amor, D. Vogt, and B. Jung. Towards a simulator for
imitation learning with kinesthetic bootstrapping. In in Proceedings of
the SIMPAR 2008 International Conference on Simulation Modeling
and Programming for Autonomous Robots, Workshop on The Universe
of RoboCup Simulators, Proceedings CD, 2008.
[2] S. Calinon and A. Billard. What is the teacher’s role in robot programming by demonstration? - Toward benchmarks for improved learning.
Interaction Studies. Special Issue on Psychological Benchmarks in
Human-Robot Interaction, 8(3):441–464, 2007.
[3] Agostino De Santis, Bruno Siciliano, Alessandro De Luca, and Antonio Bicchi. An atlas of physical human-robot interaction. Mechanism
and Machine Theory, 43(3):253–270, March 2008.
[4] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood
from Incomplete Data via the EM Algorithm. Journal of the Royal
Statistical Society. Series B (Methodological), 39(1):1–38, 1977.
[5] M. Hersch, F. Guenter, S. Calinon, and A. Billard. Dynamical system
modulation for robot learning via kinesthetic demonstrations. IEEE
Trans. on Robotics, 2008.
[6] Y. Hirata K. Kosuge, T. Hayashi and R. Tobiyama. Dance partner
robot -ms dancer-. In Proceedings of the 2003 IEEE/RSJ International
Conference on Intelligent Robots and Systems, 2003.
[7] O. Khatib, K. Yokoi, O. Brock, K. Chang, and A. Casal. Robots in
human environments. In in Proceedings of the 1st Workshop on Robot
Motion and Control, pages 213–221, 1999.
[8] T. Minato, Y. Yoshikawa, T. Noda, S. Ikemoto, H. Ishiguro, and
M. Asada. Cb2: A child robot with biomimetic body for cognitive
developmental robotics. In Proceedings of the 2003 IEEE-RAS/RSJ
International Conference on Humanoid Robots. Humanoids ’07, 2007.
[9] J. Tani, R. Nishimoto, J. Namikawa, and M. Ito. Codevelopmental
learning between human and humanoid robot using dynamic neural
network model. IEEE Transactions on System, Man and Cybernetics,
38(1):43–59, 2008.
[10] S. Yohanan and K. E. MacLean. The haptic creature project: Social
human-robot interaction through affective touch. In in Proceedings of
the AISB 2008 Symposium on the Reign of Catz & Dogs: the Second
AISB Symposium on the Role of Virtual Creatures in a Computerised
Society, volume 1, pages 7–11, 2008.

In this paper we presented a physical human-robot interaction scenario where successful task completion can only

509

2016 IEEE International Conference on Robotics and Automation (ICRA)
Stockholm, Sweden, May 16-21, 2016

Experience-based Torque Estimation for an Industrial Robot
Erik Berger1 , Steve Grehl1 , David Vogt1 , Bernhard Jung1 , Heni Ben Amor2

Abstract— Robotic manipulation tasks often require the control of forces and torques exerted on external objects. This paper presents a machine learning approach for estimating forces
when no force sensors are present on the robot platform. In
the training phase, the robot executes the desired manipulation
tasks under controlled conditions with systematically varied
parameter sets. All internal sensor data, in the presented case
from more than 100 sensors, as well as the force exerted by the
robot are recorded. Using Transfer Entropy, a statistical model
is learned that identifies the subset of sensors relevant for torque
estimation in the given task. At runtime, the model is used to
accurately estimate the torques exerted during manipulations of
the demonstrated kind. The feasibility of the approach is shown
in a setting where a robotic manipulator operates a torque
wrench to fasten a screw nut. Torque estimates with an accuracy
of well below ±1 N m are achieved. A strength of the presented
model is that no prior knowledge of the robot’s kinematics,
mass distribution or sensor instrumentation is required.

I. I NTRODUCTION
Physical interaction between a robot and its environment
requires accurate approaches for measuring and assessing
forces applied by the robot to external objects and vice
versa. For example, in human-robot interaction scenarios,
exchanged forces may indicate unwanted collisions or result from intended human guidance. Accurate measurement
of external forces is also important in manipulation tasks
involving tool-usage. Humans often rely on sensory stimuli
in order to estimate the state of a manipulation process, e.g.,
how hard a screw has been tightened.
This paper addresses the problem of measuring forces
during a manipulation task. Force-torque (FT) sensors are
often the first choice for measuring the forces exerted in
interactions with external objects. However, these sensors
are limited to a specific location. The spatial distribution
and cost of these sensors therefore need to be carefully
balanced. In dynamic tasks, such as manipulation tasks,
it may also become challenging to distinguish between
different kinds of forces such as, for instance, forces caused
by the task, sensor noise, or external perturbations of the
current behavior. Furthermore, modern robots, such as the
UR5 robotic arm, come with built-in, purely software-based
capabilities for force sensing utilizing a mass-acceleration
model. However, such methods require detailed knowledge
about the kinematics and mass distribution of the robot. A
drawback of such a method is that it quickly deteriorates in
estimation accuracy when the model is imprecise, e.g. due
to additionally attached equipment. In particluar, when the

Fig. 1. A 3-finger gripper mounted on an UR5 robotic arm is utilizing a
usual torque wrench.

robot is extended by a gripper or a tool, the mass distribution
needs to be re-calibrated accurately. In case of the UR5 robot,
the manufacturer specifies a force accuracy of ±25 N for the
robot’s tool center point (TCP). Also, the manufacturer warns
that such a kind of force estimation provides no protection
against momentum.
In contrast to the approaches mentioned above, and motivated by the ability of a trained mechanic to estimate a screw
nut’s tightening torque from prior experience, this paper
proposes an experience-based approach that does not require
prior information about the robot platform. Task models
are learned during a training phase based on the available
sensor data, without relying on any further knowledge, such
as mass distribution or robot kinematics. During runtime,
expected and measured sensor values are compared and
detected discrepancies are turned into force estimates. A
specific manipulation scenario is investigated, in which a
UR5 learns to adjust a screw nut using a torque wrench.
Figure 1 shows the principle setup which is elaborated below.

II. R ELATED W ORK

1 Institute of Computer Science, Technical University Bergakademie
Freiberg, Bernhard-von-Cotta-Str. 2, 09599 Freiberg, Germany
2 School of Computing, Informatics and, Decision Systems Engineering,
Arizona State University, 699 S Mill Ave, Tempe, AZ 85281, USA

978-1-4673-8026-3/16/$31.00 ©2016 IEEE

Robots that engage in physical interactions with humans
and objects need to regulate the forces exchanged with their
144

Data Acquisition
Time
Torque
Sensor Data

Model Generation
Phase
Feature
Space

Torque
Feature
Space

Realtime Sensor Data

Feature Space Projection
Phase Estimation

Torque Estimation

Fig. 2. An overview of the presented machine learning approach. During an offline training phase, sensor data together with information about the actual
time and torque is recorded (section III-A) to generate two low-dimensional Feature Spaces (section III-B). In order to estimate the actual torque, realtime
sensor data is projected into this spaces and compared with the training data (section III-C). The most similar matching is used as estimation of the actual
torque.

environment. This requires methods for estimating the occurring forces as well as methods for adapting the robot behavior
accordingly. Recent developments in compliant control have
lead to the emergence of robots with joint torque sensing
and feedback control [1]. For measuring external forces and
perturbations, however, typically additional force sensors,
e.g. force-torque sensors are used. Such sensors are often
expensive, add weight to the robot, and are limited in their
spatial resolution. Hence, various authors have suggested
using algorithmic approaches for inferring applied forces.
In [2], a depth camera is used in order to estimate applied
forces. Using a depth camera allows for generic contact
locations on the robot. In [3] machine learning methods
are used to extract an inverse dynamics model for a cabledriven robot manipulator. Measuring the difference between
predicted controls from the inverse dynamics model and the
executed controls provides an estimate for external forces
applied on the robot. A major challenge for such approaches
however, is training an inverse dynamics model that is
general enough to be applied to different situations. Using
machine learning for force-based robot control has also been
suggested a number of other publications. In [4] a robot
learns to adapt its motion by anticipating human intentions
from force measurements only. In [5], a method for learning
force-based manipulation skills from demonstrations was
presented. The approach generated variable-impedance control strategies thereby producing the necessary compliance
for handling deformable objects.
The work presented in the remainder of our paper focuses
on different aspect: how can a robot learn to estimate forces
from experience? In contrast to earlier discussed papers, we
learn behavior-specific models for force estimation, that are
narrower in scope, but accurate in results.

Fig. 3. A 3-finger gripper mounted on a UR5 robotic arm turns a usual
torque wrench about 45◦ . The applied torque (black) results in a preload
force (blue) which is countered by the force conducted exerted by a pressure
spring (green).

given tightening torque, a 3-fingered gripper mounted on a
robotic arm is used to adjust the torque utilizing a custom
torque wrench. The goal is to generate a behavior-specific
model, which is able to estimate the torque from previous
experience.
The first step of the presented approach is to record
example data representing the evolution of sensor values for
different torque values. In the training phase, the values of
all sensors available on the robot together with the actually
exerted torque and time stamps are recorded during a 45◦
tightening movement. This is performed multiple times for
different preconfigured torques. The recorded data is used to
generate a model consisting of two components, the PhaseFeature Space (P-FS), and the Torque-Feature Space (TFS). These components are low-dimensional embeddings of
the original high-dimensional data, that are extracted using
Transfer Entropy [6] (TE) and Principal Component Analysis
(PCA). During task execution, the P-FS is used to identify
the phase of the behavior while the T-FS estimates the

III. A PPROACH
An overview of the presented approach is shown in
Figure 2, while the setup is further explained in Figure 3. The
physical interaction considered is the tightening of a screw
nut by using a wrench. Hence, the torque results in a preload
force which is countered by a pressure spring. To secure a
145

applied torque based on the selected phase. To this end,
the real-time sensor data is compared to sensor readings
acquired during the training phase. Finally, the tightening
torque is estimated, by determining the training data point
with the highest similarity to the actual sensor readings. In
the following, each step of the presented approach will be
explained in more detail.

o[33...50]
p[33...50]

A. Data Acquisition
Similar to a skilled mechanic who is able to estimate a
screw nuts torque from previous experience, the robot needs
training data of the behavior. The realtime interface of the
UR5 provides overall 105 different sensors, containing less
useful information, e.g., the actual mainboard voltage and
redundant data as control, target and actual joint values.
However, the goal is to identify the sensors which are most
significant for estimating the behavior phase and the exerted
torque.
For this, the m = 105 sensor values r = (r1 , . . . , rm ),
are recorded during behavior execution. Additionally, the
relative time w and the preconfigured torque v are recorded
and stored in a m + 2 dimensional vector s = (r, w, v). In
contrast to the torque v, the relative time w increases during
behavior execution and is reset after. Consequently, as the
time for each training phase remains the same, w contains
always the same data for each single recording. However, one
behavior execution is recorded for two seconds with 125 Hz.
The resulting training data S = (s1 ; . . . ; sn ) represents the
training data for one possible torque v consisting of n = 250
equidistant samples.
To estimate the tightening torque, multiple behavior executions for varying configurations are recorded. First, the
screw nut was tightened with 6.0 N m and equidistantly
increased by 1.0 N m up to 15.0 N m. In the following,
the recorded training data D = (S1 ; . . . ; Sk ) for k = 10
different torque configurations is used to estimate the torque.
To achieve a higher level of accuracy without resulting in
a time consuming training phase, virtual training sets are
interpolated. For this, Dynamic Mode Decomposition (DMD)
[7] is utilized . A detailed explanation of DMD and how it is
used in the context of sensor data interpolation can be found
in [8].

Angle Sensors

Velocity Sensors

Current Sensors

Fig. 4. The Transfer Entropy for the actual angle, velocity and current
sensors is calculated for the torque applied to the screw nut (blue) and the
relative time (orange).

Radian

Ampere

6Nm

10Nm

15Nm

4
3
2
1

-1.3
-1.4
-1.5
-1.6
0

125

250

Timestep
Fig. 5. The peak TE sensor streams. Top: Different torques result in
varying sensor readings for the current sensor. Bottom: The angle sensor is
not affected by the torque but increases over time.

The main idea is that sensors with a high TE w.r.t. the
robot’s behavior are deemed more influential and relevant.
The formula for the TE between j and i is defined as
|i|−1

T E(j, i) =

X
t=1

p(it+1 , it , jt )log2

p(it+1 |it , jt )
,
p(it+1 |it )

(1)

where the function p describes the conditional probability.
Utilizing Formula 1 the TE between the relative time and
the sensors p(q) and for the torque and the sensor o(q) is
calculated by

B. Model Generation
The recorded data D is investigated in order to extract relevant features. For this, two relevance values o =
(o1 , . . . , om )T and p = (p1 , . . . , pm )T are computed for
each sensor. On the one hand o describes how strong the
sensor is influenced by the preconfigured torque v = D·,m+2
and on the other hand p describes how strong a sensor is
influenced by the relative time w = D·,m+1 . The main idea
is that sensor with a high TE are beneficial for estimating
the corresponding value. In previous work [9], TE has been
used to solve related tasks for perturbation detection during
human-robot interaction. In the present work, TE is used as
a measure of predictability and information flow between the
relative time or torque and the evolution of sensor readings.

p(q) = T E(w, D·,q )
o(q) = T E(v, D·,q ),

(2)

where q ∈ [1 . . . m]. Figure 4 shows the resulting TE for
the actual angle, velocity and current sensors contained in
p33...50 and o33...50 . For o the highest TE values was found
for the current sensors. The sensor with the highest TE o49
is visualized in the top of Figure 5. Obviously, the sensor
stream differs for varying torques. The angle and velocity
sensor share likewise less TE. Though, for p the TE is almost
completely contained in two angle sensors. The sensor with
the peak TE o35 is visualized in the bottom of Figure 5.
As can be seen, the sensor stream is nearly the same for
146

different training data. Hence, independent from the applied
torque, the sensor values are increasing over time. Therefore,
the sensor is suitable for estimating the actual phase of the
motor skill.
Next, the sensors with at least 90% of the overall TE are
used to select a subset of sensors from D in order to build
a feature space with
ω : Rm → Rm̂

are almost the same. This effect is utilized in order to further
decrease the computational effort. For this, the training data
in the first row k = 1 of Y∗ is compared with X by applying
Formula 5. The resulting path p∗1 represents the current phase
of the behavior and the last element in p∗1 the estimated
actual state of the robot. Due to the similarity of the phases,
the search space for all other rows in Y∗ is reduced by
applying a hill climbing approach. The starting point of the
search space for the remaining training sets is at b∗ ∈ p∗1 . By
applying Formula 5, the hill climbing method is searching
the neighbors of b∗ and stops when no reduced costs can be
found. Since p∗1 contains the global minimum it is assumed
that, due to their similarity, p∗2 , . . . , p∗k only contain global
minima. Finally, all phases are stored in P∗ = (p∗1 , . . . , p∗k ).

(3)

where m̂ ≤ m. The threshold is determined empirically but
can be changed in order to adapt the computational effort.
However, sensors which are not influenced by the relative
time are ignored during phase estimation while sensor not
related to the torque are ignored during torque estimation.
Finally, the dimensionality of the feature spaces is reduced
even further by applying well-known PCA. During the further procedure, the low dimensional embedding of ω(o) is
used for the phase estimation and therefore is called P-FS.
In contrast to that, the dimensional reduced ω(p) is used to
predict the torque and is called T-FS.

D. Torque Estimation
In the following, the torque is estimated based on the
previous phase estimation. By projecting the current time
window into the T-FS resulting in X̂ = (x̂1 , . . . , x̂t ) the
data can be compared to the low dimensional training data


ŷ1,1 · · · ŷ1,n

.. 
..
(6)
Ŷ =  ...
.
. 

C. Phase Estimation
Due to small time shifts, occurring between multiple
executions of a motor skill, the relative time is not sufficient
to estimate the current phase of a behavior. Because of
that, in contrast to the training data, the realtime data does
not contain information about the relative time. Instead, the
current phase is estimated from the P-FS which among other
sensors is strongly affected from joint positions.
For the estimation of the current phase of the behavior
a time window with size t of the actual sensor stream is
projected into the P-FS resulting in X = (x1 , . . . , xt ). In
previous work [8], the Subsequence Dynamic Time Warping
technique (SDTW) [10] was used for measuring the similarity between two sequences. In the case presented here,
the low dimensional sequence X is compared with the low
dimensional training data in the rows of the P-FS


y1,1 · · · y1,n

.. 
..
Y =  ...
(4)
.
. 
yk,1

···

ŷk,1

t
X

l∗ = argmin

t
X

c(x̂i , ŷl,P∗ (k,i) )

(7)

l∈[1:k] i=1

where l∗ ∈ [1 . . . k]. The preconfigured torque recorded
during execution of the training data vl∗ is used as estimation
of the actual one. Since, the robot performs the movement
during the estimation phase, the torque continuous increases.
This makes a realtime approach necessary. As mentioned
above, the training data is interpolated which effectivley
increases the number of datasets k. In order to further
decrease the computational demands, the previous mentioned
hill climbing method is utilized. Instead of computing k
possible matches the algorithm has a maximal computation
time in which it searches l∗ from random start positions in
l. This time is set to a value less than the data rate provided
by the robot, which in the case of the UR5 is 125 Hz. This
ensures that the robot always know its actual state and is
able to stop when a certain torque is reached. Obviously,
the computational demands depend to the size of the time
window t. However, in the following experiments 8 ms are
sufficient for estimating the torque utilizing the introduced
method.

yk,n

c(xi , yb+i )

ŷk,n

where n = 250, k = 10 and n ≥ t and ŷ is the low
dimensional projection of a single timestep. Comparing the
time window with the complete matrix Ŷ is time consuming.
In order to reduce the computational effort the comparison
is shrunk to the actual phases P∗ calculated in the previous
section. This reduces the number of computations from
(n − t) · k to k. In order to estimate the torque, the training
data l∗ with the highest correspondence to the actual data X̂
is calculated by

where n = 250, k = 10, n ≥ t and y is the low dimensional
projection of a single time step. Since the UR5 provides
equidistant sensor readings, the optimization problem of
the SDTW is reduced to finding the optimal path p∗ =
(b∗ , . . . , b∗ + t − 1), where b∗ is given by
b∗ = argmin

···

(5)

b∈[0:(n−t)] i=1

and c is a local distance measure, which in our case is the
Euclidean distance c = |x − y|. In contrast to SDTW no
accumulated cost matrix or warping need to be computed
what results in significant less computational effort.
As illustrated in the bottom of Figure 5, an advantage of
the presented phase estimation method is that the phases between the k recordings are just slightly shifted and therefore
147

TABLE I
T HE MEAN ABSOLUTE ERRORS IN N m RESULTING FROM DIFFERENT

Relative Time in [s]

2

P-FS
¬P-FS
∀P-FS
Actual Phase

FEATURE SPACES FOR DIFFERENT SIGNAL - TO - NOISE RATIOS .

1

25

T HE

ERROR SIGNAL WAS GENERATED UTILIZING WHITE NOISE .

125

250

Timestep

SN R

T-FS MAE

¬T-FS MAE

∀T-FS MAE

20

0.0

1.05

0.53

10

0.0

1.34

0.89

5

0.0

1.42

1.03

2.5

0.01

1.74

1.18

1.0

0.07

2.51

1.75

11

Torque in [Nm]

Fig. 6.
Phase estimation during behavior execution utilizing different
sensor selection schemes. P-FS is generated from the sensors with 90%
of the overall Transfer Entropy (orange) while ¬P-FS is generated from the
remaining sensors (blue) and ∀P-FS from all sensors (green). As can be
seen, only P-FS is able to predict the actual phase (black) accurately.

IV. E XPERIMENTS
To validate the proposed method different experiments are
conducted. Therefore ten training data sets are recorded. For
a more accurate torque estimation, the training data sets are
further interpolated after feature extraction. The resulting
data set contains 901 equidistant samples for torques in between [6.0 N m . . . 15.0 N m]. The sensors selection process
is evaluated and the reliability is proven by investigating the
absolute error. Consequently, the accuracy of the presented
results is 0.01 N m.

Estimated Torque
Configured Torque

10

9

0

125

Timestep

250

Fig. 7. The torque estimation during realtime behavior execution. The screw
nut is preconfigured with 9.6 N m (green). The torque estimation (blue) fails
at the beginning because of the small size of the captured realtime data.
After 0.2 s (highlighted area) the estimation gets close to the correct value.
The accuracy of the estimation directly depends to the size of the recorded
realtime data and gets close to the real torque after 2 s.

A. Feature Selection Evaluation
The evaluation of the P-FS is done by comparing it to all
sensors ∀P-FS and the negation of it ¬P-FS. The torque is
randomly chosen and low-dimensional projections of the last
25 recordings is provided. Figure 6 shows the resulting phase
estimation for the three different groups. As presumed, the
original P-FS is able to estimate the current phase accurately
while both other groups fail at certain time steps. Although,
there is nearly no difference between the result of ¬P-FS and
∀P-FS. This is due to the fact that suitable sensors form a
small subsets, identified by the introduced feature selection.
This shows that the resulting feature space P-FS is suitable,
to estimate the current phase of the behavior.
Next, the quality of the selected sensors for the torque
estimation is evaluated. Therefore, the data set for a torque
of 8.0 N m is selected from the training data. In order to
prove the robustness of the T-FS it is disturbed with white
noise of the following signal-to-noise ratio:
SN R =

2
σŶ

k,n

2
σnoise

,

mations. That proofs that the selected sensors are a suitable
choice for torque estimation.
B. Realtime Torque Estimation
In the final experiment a torque of 9.6 N m was set. The
goal is to identify this torque as fast as possible in order
to avoid an additional tightening. As mentioned the realtime
interface of the UR5 provides 105 different sensors 125 times
per second. Each sample is first projected into the P-FS and
appended to the previous ones. For the P-FS the size of
this low-dimensional segment is set to a size of 25 values
what, as illustrated in Figure 6, results in accurate phase
estimation results. However, as illustrated in Figure 6 before
the segment contains 25 elements, no phase estimation is
solved. Instead, the relative time is used in order to allow at
least a approximate torque estimation. Utilizing the actual
phase, the T-FS is used to estimate the actual torque by
comparing the realtime projection with the training data. As
can be seen in Figure 7 the torque estimation fails at the
beginning, due to the small size of the projected segment. In
contrast to the estimation of the phase, the torque estimation
accuracy is increased by using larger segments. In more
detail, the size of the low-dimensional projection window
is not restricted. As can be seen in Figure 7 after 0.2 s the
estimated torque matches the preconfigured one. In order to

(8)

where σ is the standard deviation. As done previously ∀T-FS
and ¬T-FS are generated in order to evaluate the selected
sensors in the T-FS. Table I shows the resulting mean
absolute errors for different levels of noise.
Similar to the phase estimation, the T-FS produces the
best results while ¬T-FS and ∀T-FS result in incorrect esti148

avoid an additional tightening of the screw nut, the robot may
stop the behavior execution or perform a reverse motion to
adjust the screw nut to the initial value. However, after 2 s
the torque was estimated with sufficient accuracy.
This confirms the assumption, that the generated feature
spaces can be applied in order to estimate the actual torque
from previous experience. In the following section, the
experimental results are discussed.

feature space is used to estimate the actual phase of the
motor skill by comparing realtime and training data. The
second feature space estimates the actual torque, again by
comparing realtime with training data. By taking into account
the previously calculated phase estimate, the computational
effort for torque estimation is reduced.
As explained in the discussion, the quality of the extracted
models varies over time. In some cases, this could lead to TE
values that vary for different phases of the behavior. Thus, a
phase-dependent selection of the optimal subset of samples
could be a beneficial extension of the proposed approach in
order to increase the overall accuracy of the torque estimate.
Nonetheless, with the present implementation torques were
accurately estimated, achieving a quality that compares to
force estimation capabilities achieved by humans.

C. Discussion
As can be seen in Figure 6 the accuracy of the torque
estimation is time dependent. This comes from the size and
the actual values within the recorded time window. From
time step 5 to about 60 the estimation overshoots the correct
torque and converges slowly afterwards. This is due to the
quality of the data recorded during the training phase. Taking
into account Figure 5 top, one can see that the data model up
to time step 40 contains overlapping sensor data. This leads
to a less accurate model for this subsequence of the behavior.
Past time step 40 the sensor data is clearly separated and
therefore has a better estimation quality. Consequently, as the
behavior continues, the phase shifts to a region with higher
accuracy and improves the overall estimation results. The
overall estimation converges slowly, since the time window
still contains the less accurate data from the beginning of the
estimation.
Taking a closer look at Figure 6, the offset after 0.5 s is
0.3 N m and after 2.0 s still about 0.1 N m. The length of the
torque wrench from the original tool center point is exactly
Nm
0.23 m. This results in an accuracy of 1.3 N = 0.3
0.23 m after
0.1 N m
0.5 s and 0.43 N = 0.23 m after 2.0 s. Thus, the proposed
model learning approach is more accurate than the mentioned
force accuracy of ±25 N specified by the manufacturer.
A limitation of the approach is that the robot needs to
be configured exactly the same way between offline training
and realtime estimation. However, in industrial settings this
usually is the case.

R EFERENCES
[1] R. Bischoff, J. Kurth, G. Schreiber, R. Koeppe, A. Albu-Schäffer,
A. Beyer, O. Eiberger, S. Haddadin, A. Stemmer, G. Grunwald,
and G. Hirzinger, “The KUKA-DLR lightweight robot arm - a
new reference platform for robotics research and manufacturing,” in
ISR/ROBOTIK 2010, Proceedings for the joint conference of ISR.
VDE Verlag, 2010, pp. 1–8.
[2] E. Magrini, F. Flacco, and A. De Luca, “Control of generalized contact
motion and force in physical human-robot interaction,” in Robotics
and Automation (ICRA), 2015 IEEE International Conference on, May
2015, pp. 2298–2304.
[3] A. Colome, D. Pardo, G. Alenya, and C. Torras, “External force
estimation during compliant robot manipulation,” in Robotics and
Automation (ICRA), 2013 IEEE International Conference on, May
2013, pp. 3535–3540.
[4] E. Gribovskaya, A. Kheddar, and A. Billard, “Motion learning and
adaptive impedance for robot control during physical interaction with
humans.” in Proceedings of the IEEE International Conference on
Robotics and Automation (ICRA). IEEE, 2011, pp. 4326–4332.
[5] A. Lee, H. Lu, A. Gupta, S. Levine, and P. Abbeel, “Learning
force-based manipulation of deformable objects from multiple demonstrations,” in Proceedings of the IEEE International Conference on
Robotics and Automation (ICRA), 2015.
[6] T. Schreiber, “Measuring information transfer,” Physical Review Letters, vol. 85, no. 2, pp. 461–464, 2000.
[7] M. R. Jovanovi, P. J. Schmid, and J. W. Nichols, “Sparsity-promoting
dynamic mode decomposition,” Physics of Fluids (1994-present),
vol. 26, no. 2, 2014.
[8] E. Berger, M. Sastuba, D. Vogt, B. Jung, and H. B. Amor, “Estimation
of perturbations in robotic behavior using dynamic mode decomposition,” Advanced Robotics, vol. 29, no. 5, pp. 331–343, 2015.
[9] E. Berger, D. Müller, D. Vogt, B. Jung, and H. Ben Amor, “Transfer
entropy for feature extraction in physical human-robot interaction:
Detecting perturbations from low-cost sensors,” in Humanoids’14,
2014.
[10] H. Sakoe, “Dynamic programming algorithm optimization for spoken
word recognition,” IEEE Transactions on Acoustics, Speech, and
Signal Processing, vol. 26, pp. 43–49, 1978.

V. C ONCLUSION
In order to provide a robot with a sense of force, an
approach for torque estimation from prior experience was
presented. Instead of using dedicated force sensors, data from
all sensors available on the robot is recorded, including e.g.
angle, velocity and current sensors. Using Transfer Entropy,
a model consisting of two feature spaces is learned from
the recorded sensor data. During runtime, sensor data is
captured and projected into these feature spaces. The first

149

Grasp Recognition with Uncalibrated Data Gloves A Comparison of Classification Methods
Guido Heumer∗

Heni Ben Amor†

Matthias Weber‡

Bernhard Jung§

VR and Multimedia Group
Institute of Informatics
TU Bergakademie Freiberg
Germany

A BSTRACT
This paper presents a comparison of various classification methods for the problem of recognizing grasp types involved in object
manipulations performed with a data glove. Conventional wisdom
holds that data gloves need calibration in order to obtain accurate
results. However, calibration is a time-consuming process, inherently user-specific, and its results are often not perfect. In contrast,
the present study aims at evaluating recognition methods that do
not require prior calibration of the data glove, by using raw sensor readings as input features and mapping them directly to different categories of hand shapes. An experiment was carried out,
where test persons wearing a data glove had to grasp physical objects of different shapes corresponding to the various grasp types of
the Schlesinger taxonomy. The collected data was analyzed with
28 classifiers including different types of neural networks, decision
trees, Bayes nets, and lazy learners. Each classifier was analyzed
in six different settings, representing various application scenarios
with differing generalization demands. The results of this work
are twofold: (1) We show that a reasonably well to highly reliable
recognition of grasp types can be achieved – depending on whether
or not the glove user is among those training the classifier – even
with uncalibrated data gloves. (2) We identify the best performing classification methods for recognition of various grasp types.
To conclude, cumbersome calibration processes before productive
usage of data gloves can be spared in many situations.
Keywords: Data Glove, Calibration, Grasp Recognition, Classification Methods
Index Terms:
I.3.6 [Computer Graphics]: Methodology and
Techniques—Interaction techniques; I.3.7 [Computer Graphics]:
Three-Dimensional Graphics and Realism—[Virtual Reality]
1

I NTRODUCTION

A desirable goal for many applications of immersive VR is the support of natural virtual object manipulations that closely resemble
the manipulation of real objects. Natural object manipulations are
e.g. fundamental in virtual prototyping for accurate simulation of
the operation or assembly of virtual product models [24]. Similarly, the imitation of a VR user’s manipulation of virtual objects
has been proposed as a means for programming assembly robots by
demonstration [1] and for generating virtual character animations
that faithfully reproduce the user’s interactions with scene objects
[13]. To support such natural manipulations, it is crucial that the
∗ e-mail:

guido.heumer@informatik.tu-freiberg.de
† e-mail: amor@informatik.tu-freiberg.de
‡ e-mail: matthias.weber@informatik.tu-freiberg.de
§ e-mail: jung@informatik.tu-freiberg.de

IEEE Virtual Reality Conference 2007
March 10 - 14, Charlotte, North Carolina, USA
1-4244-0906-3/07/$20.00 ©2007 IEEE

VR system is able to differentiate between various types of human
grasping.
VR-based manipulations of virtual objects are commonly facilitated through data glove-type input devices, such as Immersion’s
Cyberglove. In order to recognize a user-performed grasp, the sensor readings of the data glove have to be processed, analyzed and
matched towards one of a set of known grasp types. Typically, before using the data gloves a time-consuming calibration phase is
needed in order to account for differences in hand size and proportion when mapping from raw sensor readings to joint angles of the
user’s hand. How an optimal calibration can be achieved is still
an unsettled question. The more accurate methods rely on external
vision systems, which themselves need to be calibrated.
A main motivation for the work described here is to find out
whether it is possible to recognize a range of hand shape types during manipulations directly from raw sensor input without the intermediate joint angle representations. If successful, the cumbersome
calibration phase could be spared, enabling an immediate productive use of data gloves in immersive VR systems in many situations
where reliable classification of hand shapes (rather than exact reconstruction of joint angle values) is sufficient for the application.
A second motivation for this work is to evaluate the performance
of different classification methods. The aim is to identify the classifier (or family of classifiers) which is suited best for the problem
domain of “grasp recognition from raw data glove sensor data”.
The reason behind this are the so-called “no free lunch” (NFL) theorems [23]. The NFL theorems state, that averaged over all possible
problems, all algorithms perform exactly equal. As a consequence,
there can not be one classification technique, which is optimal for
all classification tasks. However, when restricting the classification problem to a particular domain, there might well be a classifier
(or a set of classifiers) which outperforms all others. This leads to
the conclusion that selecting a good classifier should be based on
an empirical evaluation in the respective problem domain. Following this reasoning we systematically evaluated several classification
techniques for this problem domain. We have experimented with a
total of 28 classifiers in various settings to find out which classifiers are suited best for this type of problem. The settings reflect
different possible use-cases and application scenarios. In this way,
informed decisions can be made about whether or not to use a particular classifier in a given VR scenario.
2

R ELATED W ORK

Various research in the fields of medicine, robotics, developmental psychology and VR has led to the formulation of grasp taxonomies: categorizations of grasps based on form or function. An
early taxonomy is described in Schlesinger’s work on constructing artificial hands [20]. He characterized which functionalities in
prosthetic hands are needed to grasp certain objects. Building on
this work, Taylor and Schwartz [21] defined English names for the
most important grasps investigated by Schlesinger: cylindrical, tip,
hook, palmar, spherical and lateral grip (see Figure 1 for examples).

19

Napier [16] researched the basic task requirements of grasps and
differentiated between two basic grasp types: the power grip which
clamps an object firmly under usage of the palm and the precision
grip where the thumb and other fingers pinch the object. Later,
Cutkosky [7] investigated optimal grasp operations in factories and
developed a taxonomy for categorizing feasible grasp types in this
domain.
In order to enable the computer to recognize and match a userperformed grasp onto a corresponding class from the taxonomy,
techniques from the area of pattern classification can be applied. In
Friedrich et al. [11] a Neural Network classifier and the Cutkosky
taxonomy were used for this purpose, yielding a classification rate
of about 90% for grasps performed using a data glove. According
to Ekvall and Kragic [9], a Hidden Markov Model (HMM) based
method was even able to achieve recognition rates of close to 100%
for single user settings. However, recognition rates dropped significantly (to about 70%) for settings with multiple users. In Aleotti and Caselli [1] a nearest neighbor classifier is used in conjunction with heuristic rules. The task of these rules is to disambiguate
between similar grasps. In this way, recognition rates of 94% for
seen users (users trained with) and 82% for unseen users (users not
trained with) were achieved. Applying a classifier to unseen users
always bears the risk of significantly lower recognition rates. This
stems from the fact that even identical postures can produce different sensor values when the sizes of the subjects’ hands vary. One
way to tackle this problem is to perform a calibration process as in
Kahlesz et al. [14]. However, this process can be complex, timeconsuming and in itself error-prone. In a recent paper by Borst
and Indugula on realistic virtual grasping [4], it was noticed that
even time-consuming calibration procedures do not produce accurate results. Another way to solve this problem is to use classification algorithms which are able to generalize over a large set of
users. However, it is still an open question which classification
techniques can achieve such generalization as no thorough comparison has been conducted so far. Another interesting question which
needs further investigation is the performance of classification techniques in different settings; e.g when new objects are grasped.
In contrast to previous research, the work presented in this paper
does not focus on a particular classification algorithm or a particular
setting. Instead, we try to compare the performance of a wide range
of classifiers in several settings within the domain of grasp classification. Such a comprehensive evaluation enables us to draw various
conclusions about the applicability and the success of classification
with uncalibrated data gloves.
3

DATA ACQUISITION

In the data acquisition phase of our study, sensor value data was
captured from a couple of users, performing all the grasps of
Schlesinger’s taxonomy [20] on several real objects. After recording the raw data, a first analysis was done on the basis of Sammons
mapping of a Self-Organizing Map [15]. The experiment setup and
the results of the data analysis are presented after a short illustration
of how the hand posture is measured by the type of data glove used.
3.1 Data Glove
The data glove used for recording was a 22-sensor wireless Cyberglove 2 by Immersion, Inc. (see Figure 1). This type of data glove
measures hand posture through a number of resistive bend-sensing
sensors which are placed in key locations (mostly joint positions)
on a stretch fabric glove. Each of the sensors measures its amount
of bending around one axis (the flat side) in the form of an 8-bit
value between 0 and 255, which is almost linearly proportional to
the bend angle.
It is important to note that the measured sensor values do not
directly represent finger joint angle values. For the mapping from

20

sensor values to actual joint angles, a complex calibration and conversion process is necessary which involves several pitfalls. In the
easiest case of measuring the flexion of the interphalangeal joints,
a direct linear conversion from one sensor value to a joint angle can
be performed. This involves an offset value (sensor value when the
joint is considered straight) and a gain factor (multiplicative factor
to convert bend value to radians/degrees). Even for this simplest
method of mapping, offset and gain values have to be determined
for each single sensor in a tedious process. Due to cross-couplings
between the sensors, however, more complex forms of calibration
are necessary to achieve a satisfactory fidelity. In Kahlesz et al. [14]
some recent calibration techniques are summarized. Since our objective was to spare any calibration process, the raw sensor readings, as transmitted by the Cyberglove 2, were used directly as feature vector for hand posture classification.
3.2 Experiment Setup
For each of the six grasp types, four objects of various shapes were
grasped. Table 1 lists the objects used for each grasp type, Figure 1
shows pictures of some of these objects.
Grasp Type
Cylindrical
Hook
Lateral
Palmar
Spherical
Tip

Objects
Bottle, Hammer, Flower Pot, Coffee Jug
Plastic Case, Toolbox, Backpack, Bag
Floppy Disk, Key, ID Card, CD Cover
Small Box, Matchbox, Tape Roller, PDA
Tennis Ball, Egg-shaped Case, Bowl, Mouse
Nail, Pencil, Small Eraser, PDA

Table 1: The grasp types and corresponding trial objects.

For each object, two trial sequences were performed. In the
first sequence, the object was grasped five times, with the subject’s
grasping hand starting from a fixed position on the table. During
this whole sequence the subject sat at the table. In the second sequence, the object was again grasped five times. This time, however, the hand starting position was varied randomly, as was the object’s orientation on the table. For larger objects, like the tool box,
the subject was standing during this sequence. The captured data
consisted of all the 22 glove sensor values representing the hand
posture at the “peak” moment of the grasp – as opposed to a sequence of sensor values of a full grasping movement. This moment
means the time the test subject’s hand firmly held the object and the
fingers were at rest. The peak moment was determined manually
by the test subject by pressing a button on the dataglove.
The whole data acquisition process was conducted with six test
subjects. Each subject was adult, male and right-handed. In total,
6 (test subjects) × 24 (objects) × 10 (grasps per test subject and
object) = 1440 data items were collected.
3.3 Data Visualization
Before the classifier evaluation was executed, it proved interesting
to first take a “glimpse” at the data. This helped to get an idea of
the problem difficulty, to assess the quality of the recorded data or
to make first decisions concerning a suitable classifier. Typically,
the recorded data is in a higher-dimensional space which makes a
human inspection difficult. For inspection and analysis purposes
it is more convenient to create a visual representation of the experiment data. This can be achieved through a projection of the
high-dimensional pattern space onto two dimensions. Common
techniques for such projection tasks are the Self-Organizing Map
and Sammons’ mapping [15]. Figure 2 shows a projection of our
experiment data using a combination of the two techniques. The
projection is distance preserving, which means that points which
are near to each other in the higher dimensional space will also be

Cylindrical

Hook

Lateral

Palmar

Spherical

Tip

Figure 1: Images of some of the objects used during the grasping experiments with their corresponding grasp types.

spherical
spherical spherical
spherical
palmar
cylindrical
cylindrical
tip
tip

cylindrical
cylindrical

spherical

tip
tip

palmar

palmar
palmar

spherical

palmar

palmar
tip

spherical

cylindrical

cylindrical

tip

tip

tip

tip

tip
palmar
tip
palmar
palmar
palmar

hook
hook

hook
cylindrical

tip

palmar

lateral

tip

hook

hook

hook
tip

hook
lateral
lateral
hook
hook hook
lateral
hook

hook
hook

cylindrical
lateral
lateral
cylindrical
cylindrical
lateral lateral
hook

Figure 2: Sammons mapping of a Self-Organizing Map representing
the data projected onto a two-dimensional space.

near to each other in the projection. It can be seen that the spherical grasp forms a particular region in the upper part of the map.
This region can neatly be separated from regions representing other
grasps, which indicates that classification of this grasp type is particularly easy. Although other grasp types also occupy particular
regions on the map, they are much more intermixed. This yields
complex decision boundaries. For example, the classes “tip” and
“palmar” cannot cleanly be separated from each other. What also
becomes visible, is that “cylindrical” grasps are scattered throughout the map. This fact is particularly interesting, as it exemplifies
the hard separability of the Schlesinger grasps based on hand shape
only. It follows from this observation that we can expect classification to be more error-prone for cylindrical grasps.
4

C LASSIFIER E VALUATION

A set of 28 different classifiers from the freely available Weka data
mining software package [22] has been evaluated. These were run
in an “out of the box” fashion, i.e. with default settings and without
any parameter optimization. The tested classifiers can be broadly
divided into five categories – probabilistic methods, function approximators, lazy learners, trees, and rule sets: (1) Probabilistic

methods such as the naive Bayes classifier [10] or Bayes nets [6]
learn to discriminate between classes by building up probability
models of each class. Using Bayesian inference the probability of a
new data item belonging to a particular class can be computed. For
example, the naive Bayes classifier learns a model of the training
data by estimating class probabilities and conditional probabilities
of the variables. Together with the Bayes theorem, these values
can be used to compute the probability of a data item belonging to
a particular class. The only “naive” assumption (hence the name)
that is being made, is that all variables of a data item are mutually
independent. (2) Function approximators learn the parameters of a
function which takes the new data item as an input and returns the
class as an output. Well-known representatives of this type of algorithm are Multilayer Perceptrons and Radial Basis Networks [2].
Here, the approximated function is represented by a set of interconnected neurons. The back-propagation algorithm [12] can be used
to train such networks in order to minimize the squared error of approximation. (3) Lazy learning techniques [3] postpone any type
of learning until a request for classification of a new data item is
received. When such a request is received, a database of previously
seen examples is searched for a set of examples, which are closest
to the new item (w.r.t. a given distance metrics). (4) Tree classifiers, such as decision trees [17], try to break up the classification
task into a hierarchy of simple decisions at whose end the final decision determines the class. As the name suggests, this hierarchy
has the form of a tree, whose nodes represent local decisions, while
leafs represent the classes. (5) Finally, rule induction methods [18]
create sets of logical rules for determining the class of a particular
item. Ridor [19], or “Ripple Down Rule”, is a representative algorithm from this class. Ridor requires that the data is incrementally
supplied to the training set. Data items which conflict with previously learned rules are seen as exceptions. These are then treated
by patching the rule locally for the particular item.
Another category, namely meta learning techniques [5] were
only roughly examined in preliminary tests and are not included
in the final results. These are techniques such as boosting or bagging that aim to create powerful classifiers through a combination
of several simpler ones. Depending on the algorithm hierarchies,
cascades or ensembles of base classifiers are used for classification.
Since meta classifiers can be built from essentially arbitrary combinations of simpler classifiers, they are inherently more complicated
to evaluate and several choices of base classifiers would have to be
looked at. This will, however, be subject of future work.

21

4.1 Design & Method
To determine which classifier is suited best for the domain of classifying raw sensor data, a comprehensive, systematic classifier evaluation was performed. We examined a set of classifiers in 6 different
settings, formed by a permutation of the values of two situational
variables (see Tables 2 and 3), putting different generalization demands on the classifiers. One variable (objects) determined whether
the objects grasped in the test set were seen, i.e. grasp examples
with this objects were used for training, versus unseen, where no
grasp examples with this object were used during training. Note,
that even in the seen case, training and test sets always were disjoint. This means, a grasp example used during training was never
used for testing as well. The other variable (user) determined the
user group, i.e. which set of users’ grasp examples were taken for
training. For this variable, three different cases were investigated:
individual and group, where training data of only one user or the
full group of users, respectively, were used for training and testing.
And a third case, unseen, where data of all users except one was
used for training, and data of the left-out user was used for testing.
The property of disjoint training and test sets also holds true in all
three cases.
Value
seen

Meaning
classifier trained and tested
with the same set of objects

unseen

classifier trained and tested
with different sets of objects

Example Scenario
applications with a
given, fixed set of
objects, e.g. a tool set
applications
where
scene objects change or
are of modifiable form,
e.g. CAD

For all settings, the number of data splits into test and training
set and the respective set sizes per split are summarized in Table 4.
Also a rough indication of how the test set was formed is given in
the middle column. More detail about the settings and the exact
method of how test and training set were generated are given in the
following subsections. Readers not interested in this level of detail
might want to skip to the presentation of results in section 4.2.
Setting
(user, objects)
#1 - individual,
seen objects
#2 - individual,
unseen objects
#3 - group,
seen objects
#4 - group, unseen objects
#5 - unseen,
seen objects
#6 - unseen,
unseen objects

Data Splitting
6 splits of data of one user.
test set - two random examples per object.
8 splits (2 series of 4) of data
of one user. test set - one random object per grasp type.
6 splits - as in #1 but data of
all users. test set - two random examples per object and
user.
8 splits as in #2 but data of all
users.
6 splits (one per user). test set
- all data of one user.
12 splits (2 series of 6). test
set - from user splits (as in
#5) pick one random object
per grasp type.

Train.
Set
192

Test
Set
48

180

60

1152

288

1080

360

1200

240

900

60

Table 4: Splitting of data into test and training sets for the different
settings.

Table 2: Investigated values of the variable ’objects’.

Value
individual
group
unseen

Meaning
classifier trained and tested
with a specific user
classifier trained with a
group of users and tested
with a group member
classifier trained with several users but tested with a
user not in the group

Example Scenario
single operator system
work group
public installation,
e.g. game

Table 3: Investigated values of the variable ’user’.

For each of the six settings, several pairs of disjoint training and
test sets were generated by splitting the complete data in an adequate way. Each classifier was trained and tested with each pair
(or data split), and the average rate of correct classifications for
each classifier over all these tests was determined. The feature (input) vector for classification consisted of all 22 sensor values which
were not weighted. The output of the classifier was an index value,
indicating one of the six grasp types. Note, that since all data items
were taken from valid examples of the different grasp types, there
was no rejection class. The right answer was always one of the six
Schlesinger grasp types.
Classifier performance was measured in the percentage of correct classifications. When two classifiers had the same average
performance, the classifier with the smaller standard deviation was
considered better. Additionally, for each setting, a set of best classifiers was established by selecting all classifiers that performed not
significantly different than the best classifier. To determine the significance of differences between classifiers the McNemar’s test was
used (for an overview on significance tests, see [8]).

22

4.1.1 Individual user, seen objects
Data of only one user is regarded and the same set of objects is used
for testing and training. This corresponds to an application, where
the system is trained for a specific user (and this user only) and all
objects to be interacted with are known in advance. In comparison
with conventional (calibrated) classification, this would correspond
to a perfect glove calibration being available for a particular user
and an additional training session having been performed, where
all objects later to be interacted with are trained into the system.
To generate disjoint training and test sets for this setting, all data
of one user was taken and split evenly into two sets, so that the
respective numbers of examples for each grasp and object stayed
the same. Since ten data samples for each object were recorded –
five with a fixed starting position and five with a variable starting
position – two samples of each object (one with each type of starting
position) were chosen for the test set (48 data items), while the
others formed the training set (192 data items). Overall, six splits
were generated in this way, by randomly chosing the test items.
4.1.2 Individual user, unseen objects
Again, data of only one user is regarded, however tests were always
performed with unseen objects, i.e. no data examples of the object
used for the tests have been used for training. This corresponds
to an application, where the system was trained for a specific user,
however the objects used during the interaction are not previously
known.
Training and test sets were generated, by randomly choosing one
object for each grasp type and using the examples of these object as
test set, whereas the data of the other objects was used as training
set. This way, for each user a series of eight splits was generated, so
that each object of one grasp type was used exactly twice for testing.
The training sets consisted of 180 data items, whereas the test sets
were 60 items large. The combinations between grasp types, i.e.

which object of each grasp type was chosen, were random. It was
ensured, however, that each permutation only occurred once.
4.1.3 Seen group of users, seen objects
Similar to 4.1.1 (individual user, seen objects), however data examples of all users were used for training and testing. This corresponds
to an application that is set up to work with a certain group of users
and the objects used for interaction are known in advance. Note
that this setting, similar to the settings below, is already beyond
the scope of calibration-based approaches as these require system
knowledge of individual users.
Data splitting was done as in the individual user case, but with
data of all users instead of one. Additionally, it was made sure
that the same number of examples from each user was chosen. For
each object and user combination two examples were chosen for
the test set (288 data items), while the remaining examples formed
the training set (1152 data items).
4.1.4 Seen group of users, unseen objects
This setting is similar to 4.1.2 (individual user, unseen objects).
However, data of all users were used for training and testing instead of data from just one user. This corresponds to an application
that is set up to work with a certain group of users and the objects
used for interaction are not known in advance.
Training and test sets were generated by creating eight splits,
where in each split data of one randomly picked object (per grasp
type) forms the test set (360 data items), whereas data from the remaining objects composes the training set (1080 data items). Again,
it was made sure that no permutation was repeated and that each object was part of the test sets exactly twice.
4.1.5 Unseen users, seen objects
In this setting, no data of the test user was contained in the training set, i.e. the classifier was not trained with data from the test
user. This corresponds to an application, where the system was
trained with data from a group of users and another (previously unseen user) then uses the system, for example in public installations,
etc. All objects used during tests were seen before by the classifier
(grasped by other users) during training, i.e. for this type of application the objects of the interaction need to be known in advance.
Here, for each user, a pair of datasets was generated where the
training set contained sensor data from all users except this one
(1200 data items), and the test set consisted of all data from this
user (240 data items). Since data was acquired from six different
users, this resulted in six different disjoint splits.
4.1.6 Unseen users, unseen objects
In this setting neither the objects nor the user involved in testing
were seen by the classifier during training. This corresponds to applications where the users and interaction objects are not known in
advance. This setting puts high demands on the classifier’s generalization capabilities, but can satisfy the broadest area of use cases.
For generating test sets and training sets the data was first split
into disjoint sets for each user as in 4.1.5. Then, for each of these
user splits two random object splits were generated, where for each
grasp type one object was picked. Data from this object was removed from the training sets, whereas only the data of this object
remained in the test set. This resulted in twelve data splits overall
with a test set size of 60 data items and a training set size of 900
data items.
4.2 Results
For every evaluated classifier in each data split of each setting, the
percentage of correctly classified examples has been determined.

Due to space limitations this is too much information to be presented here. Hence, the results have been summarized by determining average classification rates for each setting and the corresponding standard deviation, see Table 5. Each setting is represented by two columns (average and standard deviation). Following the cross validation results, in the next two columns are the
average performance over all settings and the standard deviation
of this total average. In the final column, the average run time
per test of each classification algorithm is given in milliseconds.
This value is of course dependent on the used hardware and for
this reason is only to be seen as a relative comparison between
the several algorithms. Grey table cells indicate for each setting
the classifier with the highest accuracy; table cells shaded in light
gray indicate classification methods whose accuracies vary only insignificantly (according to a McNemar’s test) from the best performing classifier. The interested reader can find the complete results of the study as well as the captured data on the Web under
http://vr.tu-freiberg.de/grasping/.
In the following, the results for each of the settings are summarized:
• individual user, seen objects – With 99.48% achieved by
the Kstar algorithm, a highly reliable classification rate can
be reached for the case where the objects grasped are known
in advance and the user trained the system individually. A
not significantly worse performance can be reached with IB1,
RBF Network, Multilayer Perceptron, LMT, Simple Logistic,
NNge, and SMO. Since KStar has a relatively long runtime,
in more time critical applications IB1 would be the next-best
choice or, if an even shorter runtime is needed, Multilayer
Perceptron.
• individual user, unseen objects – In the case, where the
objects grasped are not known (and trained in) in advance,
more generalization ability is needed, and the classification
rate drops down to 83.82%. The best classification rate was
achieved with SMO, which also has a relatively short runtime.
Not significantly worse performed IB1 and Multilayer Perceptron.
• group of users, seen objects – In the case, where a whole
group of users trained the system and an unspecified member
of the group uses it, a classification rate of 99.07%, almost as
good as for the single user case, can be achieved. Best performed IB1, followed by KStar and Multilayer Perceptron,
which would be the best choice, if a short test runtime is important.
• group of users, unseen objects – Again, for unseen objects
the classification rate drops, in this case to 84.62%. The best
classifier in this setting clearly was IB1 with all other classifiers performing significantly worse.
• unseen users, seen objects – For the case, where the user is
unseen to the system but the objects are known in advance,
a similar classification rate as for the unseen objects cases is
achieved. With 81.74% SMO performed best, followed by the
not significantly worse Multilayer Perceptron, IB1, Random
Forest and LMT classifiers.
• unseen users, unseen objects – In this case, where the user
as well as the objects are unseen, the classification rate drops
further down to 71.67%, achieved by the SMO classifier. This
reflects the rather high demand on generalization abilities of
the classifiers. Similar performance was achieved by Multilayer Perceptron, IB1, Simple Logistic and RBF Network
classifiers.

23

24

lazy
functions
functions
lazy
trees
functions
trees
functions
functions
rules
rules
trees
trees
bayes
rules
trees
bayes
rules
bayes
bayes
bayes
trees
lazy
bayes
rules
rules
trees
rules

IB1
MultilayerPerceptron
SMO
KStar
LMT
SimpleLogistic
RandomForest
RBFNetwork
Logistic
NNge
PART
J48
NBTree
BayesNet
Ridor
REPTree
NaiveBayes
JRip
NaiveBayesUpdateable
NaiveBayesSimple
NaiveBayesMultinomial
RandomTree
LWL
ComplementNaiveBayes
DecisionTable
OneR
DecisionStump
ConjunctiveRule

Individual User,
Seen Object
Avg.
Stdev.
99.13
0.72
98.38
0.9
97.11
1.56
99.48
0.78
97.28
1.11
97.28
1.11
96.99
1.18
98.56
1.08
96.01
1.44
97.22
1.2
91.55
2.72
93
3.17
94.74
2.63
95.55
2.77
90.97
2.62
88.66
2.9
93.69
3.35
86
2.81
93.64
3.28
93.23
3.34
84.55
5.04
84.96
2.96
78.88
5.76
71.99
10.03
80.03
3.28
53.36
5.92
32.7
0.56
32.87
0.57

Individual User,
Unseen Object
Avg.
Stdev.
83.68
5.12
82.4
5.27
83.82
5.01
79.66
6.74
76.98
3.09
76.53
2.73
71.98
7.06
70.49
8.93
72.92
3.76
68.13
5.06
66.6
5.93
66.7
7.9
66.04
8.09
68.06
5.82
64.48
5.6
65.49
4.6
71.01
4.84
58.48
3.15
70.8
4.56
70.73
4.36
65.28
4.12
56.56
6.31
63.79
3.51
60.28
7.85
46.7
6.78
40.9
6.72
30.35
0.94
29.48
1.11

Group,
Seen Object
Avg.
Stdev.
99.07
0.72
96.88
0.82
93.64
0.84
98.67
0.89
95.02
0.78
93.81
0.97
96.82
0.86
91.61
1.69
92.3
1.23
92.01
2
92.48
1.22
90.45
1.66
91.26
1.06
85.42
0.76
89.18
2.52
87.15
1.26
76.62
0.61
90.45
1.4
76.62
0.68
76.62
0.68
72.57
0.69
83.57
1.58
65.86
1.43
63.43
0.9
76.39
2.17
42.71
1.32
31.89
1.17
32.12
0.82

Group,
Unseen Object
Avg.
Stdev.
84.62
4.99
80.24
4.45
81.74
4.48
81.81
3.04
77.12
5.71
79.2
5.22
74.34
5.89
77.47
5.27
77.71
4.19
67.92
8.03
67.29
4.58
68.44
6.52
69.76
5.17
63.3
6.14
66.74
5.55
64.58
5.61
62.71
7.99
65.63
4.14
62.33
8.18
62.33
8.18
58.33
5.61
56.6
6.34
55.31
6.99
53.23
6.35
52.05
7.27
30.04
5.99
31.28
2.24
29.62
3.91

Unseen User,
Seen Object
Avg.
Stdev.
80.07
6.26
81.04
3.69
81.74
7.17
77.29
5.73
79.17
4.53
76.53
7.72
79.86
3.89
76.32
11.6
73.89
8.26
71.53
8.66
73.2
11.38
69.24
10.43
63.13
11.41
69.72
10.01
66.53
8.75
65
9.03
68.96
13.42
64.31
11.83
69.17
13.62
69.17
13.62
66.32
14.63
58.4
8.7
59.86
8.27
60.07
8.77
50.62
6.89
32.71
7.41
29.72
4.16
29.31
2.48

Unseen User,
Unseen Object
Avg.
Stdev.
68.06
11.72
71.53
11.02
71.67
12.81
64.86
11.31
63.47
14.41
67.22
15.49
66.11
8.86
67.08
16.13
64.31
12.15
57.92
14.41
55.28
13.54
59.31
14.55
52.92
8.77
61.11
9.17
54.03
14.27
58.19
12.5
61.81
11.38
56.81
11.58
61.81
11.51
61.95
11.76
58.06
13.76
46.25
13.47
51.94
12.77
55
12.93
41.11
14.98
28.75
9.67
28.61
7.27
30.42
6.44

Cross
Validation
Avg.
98.61
97.15
93.54
98.54
95.07
92.5
96.74
90.35
92.71
92.15
91.74
90.83
91.18
84.03
87.64
88.13
76.32
89.31
76.32
76.25
72.29
82.22
65
63.13
76.53
43.89
32.85
32.36

Avg.
87.61
86.8
86.18
85.76
83.44
83.3
83.26
81.7
81.41
78.13
76.88
76.85
75.58
75.31
74.22
73.89
73.02
73
72.96
72.9
68.2
66.94
62.95
61.02
60.49
38.91
31.06
30.88

Stdev.
11.89
10.58
8.99
13.41
12.65
11.24
13.33
11.86
12.21
15.33
15.04
14.03
16.58
13.02
14.73
13.41
10.83
14.96
10.87
10.71
9.27
16.07
8.72
6.18
16.46
8.87
1.58
1.52

Total

Table 5: Results of the classifier evaluation with best classifier in setting and not significantly different and therefore additional best classifiers .

Classifier
Category

Classifier

[ms/test]
2.25662
0.06072
0.07422
29.62138
0.09188
0.03572
0.03631
0.14287
0.04405
0.26511
0.03631
0.03492
0.09505
0.08255
0.02937
0.01409
0.23733
0.01607
0.27007
0.07322
0.03651
0.02738
21.36623
0.03631
0.04068
0.02600
0.01449
0.02004

Runtime

To obtain an additional measure for classifier performance, a 10fold cross-validation on the complete data set was performed. To
cross-validate a data set, it is split in k (where k is 10 in this case)
subsets. Then, k tests are conducted where the k-th subset is used
as the test set for the classifier and the other k-1 subsets are used as
the training data. The total classification rate is then computed as
the average of the classification rates of all k tests.
In the column labeled “Total”, the average value of the average
performances in the different settings is denoted. This is an indication of how well a classifier performs overall, hence the table has
been sorted by this value. The IB1 algorithm leads the table with
87.61% classification rate. The total standard deviation indicates
how strongly classifier performance varies over the different settings. Note, that this is not the average of the standard deviations
for the various settings. The average performances of the best classifiers for each setting are also displayed comparatively in Figure 3.
100%

tip
cyl
sph
pal
hook
lat

tip

cyl

sph

pal

hook

lat

0.9698
0.0077
0.0042
0.0205
0.0012
0.0011

0.0030
0.9329
0.0032
0.0024
0.0176
0.0002

0.0006
0.0106
0.9890
0.0006
0.0004
0.0000

0.0214
0.0086
0.0033
0.9765
0.0000
0.0002

0.0038
0.0375
0.0000
0.0000
0.9732
0.0028

0.0015
0.0028
0.0002
0.0001
0.0076
0.9957

Table 6: The normalized overall confusion matrix for IB1. (Row =
shown example, Column = classified as; column names are abbreviations of tip, cylindrical, spherical, palmar, hook, lateral)

well as between tip and palmar grasps is problematic. In contrast,
spherical and lateral are very well distinguishable from all the other
grasp types.
4.3 Discussion

80%

60%

ind. User, ind. User, User group, User group, unseen User,unseen User,
seen Obj. unseen Obj. seen Obj. unseen Obj. seen Obj. unseen Obj.

Figure 3: Comparison of average performance of best classifiers for
each setting

Average classifier runtimes (per test) have been determined by
summarizing the runtimes for all tests and dividing them by the
number of tests. They are given in the last column. As can be seen,
most runtimes stay in the same order of magnitude. The only exceptions are the lazy evaluators, which have a relatively long runtime,
since a lot of training examples need to be considered during the
tests. This runtime difference will also further increase with larger
training set sizes.
For further analysis, the confusion matrix of all classification
results for the best classifier IB1 was investigated. Each entry in
this matrix shows the number of recognized categories when a certain grasp is shown to the classifier. For a better understanding of
the matrix, these numbers were normalized by dividing by the total
number of examples for a certain grasp type. The resulting confusion matrix is presented in Table 6.
The entries in the diagonal of the matrix represent the percentage of which grasps were identified correctly. Since, as has been
shown, the IB1 classifier performs quite well, these values are reasonably high. The other values in the matrix describe the percentage of misclassifications between the shown and classified grasp
category. They indicate probable difficulties when distinguishing
between certain grasps. As predicted by the Sammon’s mapping in
Section 3.3 the distinction between cylindrical and hook grasps as

It has been shown that grasp recognition based on uncalibrated data
glove sensor input can be performed in a very reliable way in specific scenarios, where the group of users that uses the system and the
objects that are used during interaction are known in advance. Each
user would then need to train the system, by performing grasps for
each object occurring in the scenario.
For cases were either potential users are unknown or the grasped
objects are not determined in advance, with a recognition rate of
about 80%, still an acceptable performance can be achieved for applications where occasional misclassifications are not critical. This
might be the case for example public gaming or other entertainment
installations.
From the average performance of the examined classifiers a list
of classification algorithms that are suited best for the examined
problem domain has been compiled. Statistical significance tests
suggest that these algorithms perform significantly better than the
others in all examined settings. Furthermore, it can be seen that
several classifier categories are suited better for this problem domain than others. For example, whereas most function approximators perform reasonably well, Bayesian classifiers in all cases
yielded unsatisfactory results. Similarly, most tree (with the exception of LMT and Random Forest) and rule-based classifiers (with
the exception of NNge) resulted in comparably low recognition
rates when applied to the full problem of distinguishing between
all six Schlesinger grasps.
The overall best performing classifier IB1, as indicated by the
confusion matrix in Table 6, is most prone to misclassifications between grasps of types palmar and tip, and resp., cylindrical and
hook. To a certain extent, these misclassifications can be attributed
to an inherently hard separability of these grasp types based on hand
shape only (cf. Sammon’s Mapping in Section 3.3). However, an
examination of the confusion matrices of all classifiers reveals that
certain classifiers clearly outperform IB1 and the other classifiers
when applied to the specific problem of distinguishing between
these hard cases only: For differentiating between tip and palmar
grasps, the REPTree and the rule-based ZeroR classifier yielded significantly better recognition rates than all other classifiers. The best
separation of palmar and tip graps was achieved by the BayesNet
classifier. These observations suggest that a clever combination of
classifiers could lead to even higher recognitions rates.
A possibly surprising result was that, while LWL (Local
Weighted Learning) did not perform well, the other lazy evaluation algorithms were shown to be the best choice of classifiers in
the domain “grasp recognition from raw data glove sensor data”.

25

5

C ONCLUSION

We introduced a systematic approach for the evaluation of classification techniques for recognizing grasps performed with a data
glove. In particular, we distinguish between 6 settings that make
different assumptions about the user groups and objects to be
grasped. A large number of classifiers was compared to draw informed conclusions about achievable recognitions rates in the different settings. Through this, our approach extends previous reports
on classifier performance that focused on particular classifiers and
settings.
An interesting result of our evaluation is that calibration-free
classification of grasps can be performed with reasonable to high
reliability for a number of different settings. Surprisingly, algorithms based on lazy learning outperform most of other algorithms
in this domain. With respect to the average classification rate over
all settings, the IB1 (instance based learner) outperformed all other
tested algorithms. This is a suprising result, as lazy learners do
not perform any processing of the collected data, which supposedly
results in lower generalization ability. Considering the low complexity of implementation, these algorithms seem to be well suited
for many VR applications. However, in applications with extreme
realtime demands, e.g. requiring over 1000 classifications per second, faster techniques such as the Multilayer Perceptron are better
suited. While the total average classification rate of MLP’s in our
experiments was only less than 1% worse than that of IB1, it had
an approximately 35 times shorter runtime.
As indicated by the data analysis in section 3.3, the grasps of the
Schlesinger taxonomy are hard to distinguish based on hand shape
alone, e.g. palmar and tip. This makes classification based on uncalibrated sensor data – and, presumably, similarly for joint angles values – a challenging task. For our study, this had the advantage that
differences between the various classifiers turned out more clearly
in the analysis. To achieve better recognition rates, the inclusion
of higher level features such as finger bending indexes or contact
points will be considered in future work. Other taxonomies with
clearer differences between classes in terms of hand poses might
also increase recognition rates.
As our results for evaluating the confusion matrices of classifiers show (see section 4.3), some classifiers perform better at distinguishing specific grasp types than others. The strengths of one
classifier could compensate the weaknesses of another, so that their
combination into a well-tuned meta classifier might lead to a powerful hybrid classification scheme. In future work an in-depth evaluation of various hybrid and meta-classifiers will be performed,
which might result in a classifier that distinguishes well between
all grasp categories, even the ones which are more difficult to separate. A combination of IB1 (the best overall classifier) with either
the ZeroR rules or the REPTree tree classifier and additionally the
BayesNet classifier would be a first recommendation. Additionally a thorough parameter optimization process of the applied base
classifiers might yield even higher recognition rates than the ones
presented here.
Concluding, we believe that the demonstrated approach of
calibration-free data glove operation combined with out-of-the-box
application of classifiers from a freely available data mining software package greatly simplifies the usage of data gloves.
ACKNOWLEDGEMENTS
The research described in this contribution is partially supported by
the DFG (Deutsche Forschungsgemeinschaft) in the Virtual Workers project.
R EFERENCES
[1] J. Aleotti and S. Caselli. Grasp Recognition in Virtual Reality for
Robot Pregrasp Planning by Demonstration. In IEEE Intl. Conf. on
Intelligent Robotics and Automation, 2006.

26

[2] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, November 1995.
[3] G. Bontempi, M. Birattari, and H. Bersini. Lazy learning: a logical
method for supervised learning. In New Learning Paradigms in Soft
Computing, pages 97–136. Physica-Verlag, Heidelberg, 2002.
[4] C. W. Borst and A. P. Indugula. Realistic Virtual Grasping. In VR
’05: Proceedings of the 2005 IEEE Conference on Virtual Reality,
pages 91–98, 2005.
[5] P. K. Chan and S. J. Stolfo. On the accuracy of meta-learning for scalable data mining. Journal of Intelligent Information Systems, 8(1):5–
28, 1997.
[6] G. F. Cooper and E. Herskovits. A bayesian method for the induction
of probabilistic networks from data. Machine Learning, 9(4):309–347,
1992.
[7] M. Cutkosky. On Grasp Choice, Grasp Models and the Design of
Hands for Manufacturing Tasks. IEEE Trans. on Robotics and Automation, 5(3), 1989.
[8] T. G. Dietterich. Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms. Neural Computation,
10(7):1895–1924, 1998.
[9] S. Ekvall and D. Kragic. Grasp Recognition for Programming by
Demonstration Tasks. In IEEE International Conference on Robotics
and Automation, 2005.
[10] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classifiers. Mach. Learn., 29(2-3):131–163, 1997.
[11] H. Friedrich, V. Grossmann, M. Ehrenmann, O. Rogalla, R. Zöllner,
and R. Dillmann. Towards Cognitive Elementary Operators: Grasp
Classification using Neural Network Classifiers. In IASTED International Conference on Intelligent Systems and Control, 1999.
[12] S. Haykin. Neural Networks: A Comprehensive Foundation. Prentice
Hall PTR, Upper Saddle River, NJ, USA, 1994.
[13] B. Jung, H. B. Amor, G. Heumer, and M. Weber. From motion capture
to action capture: A review of imitation learning techniques and their
application to VR-based character animation. In Proceedings VRST
2006 - Thirteenth ACM Symposium on Virtual Reality Software and
Technology, pages 145–154, 2006.
[14] F. Kahlesz, G. Zachmann, and R. Klein. Visual-Fidelity Dataglove
Calibration. In Computer Graphics International (CGI), pages 403–
410. IEEE Computer Society Press, 2004.
[15] S. Kaski. Data Exploration Using Self-Organizing Maps. Acta Polytechnica Scandinavica, Mathematics, Computing and Management in
Engineering Series No. 82, March 1997.
[16] J. Napier. The Prehensile Movements of the Human Hand. The Journal of Bone and Joint Surgery, 38b(4):902–913, 1956.
[17] J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1993.
[18] J. R. Quinlan, P. J. Compton, K. A. Horn, and L. Lazarus. Inductive
knowledge acquisition: a case study. In Proceedings of the Second
Australian Conference on Applications of Expert Systems, pages 137–
156, Boston, MA, USA, 1987. Addison-Wesley Longman Publishing.
[19] D. Richards. Ripple down rules: a technique for acquiring knowledge.
In Decision making support systems: achievements, trends and challenges for the new decade, pages 207–226. Idea Group Publishing,
Hershey, PA, USA, 2002.
[20] G. Schlesinger. Der Mechanische Aufbau der Künstlichen Glieder.
In M. Borchardt et al., editors, Ersatzglieder und Arbeitshilfen für
Kriegsbeschädigte und Unfallverletzte, pages 321–661. SpringerVerlag: Berlin, Germany, 1919.
[21] C. Taylor and R. Schwarz. The Anatomy and Mechanics of the Human
Hand. Artificial Limbs, 2:22–35, 1955.
[22] I. H. Witten and E. Frank. Data Mining: Practical machine learning
tools and techniques. Morgan Kaufmann, San Francisco, 2nd edition,
2005.
[23] D. H. Wolpert and W. G. Macready. No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation, 1(1):67–
82, April 1997.
[24] G. Zachmann and A. Rettig. Natural and robust interaction in virtual
assembly simulation. In Eighth ISPE International Conference on
Concurrent Engineering: Research and Applications (ISPE/CE2001),
pages 425–434, 2001.

Auton Robot (2014) 36:1–3
DOI 10.1007/s10514-013-9379-3

Special issue on autonomous grasping and manipulation
Heni Ben Amor · Ashutosh Saxena · Nicolas Hudson ·
Jan Peters

Published online: 27 November 2013
© Springer Science+Business Media New York 2013

Grasping and manipulation of objects are essential motor
skills for robots to interact with their environment and perform meaningful, physical tasks. Since the dawn of robotics,
grasping and manipulation have formed a core research field
with a large number of dedicated publications. The field has
reached an important milestone in recent years as various
robots can now reliably perform basic grasps on unknown
objects. However, these robots are still far from being capable of human-level manipulation skills including in-hand or
bimanual manipulation of objects, interactions with nonrigid objects, and multi-object tasks such as stacking and
tool-usage. Progress on such advanced manipulation skills
is slowed down by requiring a successful combination of a
multitude of different methods and technologies, e.g., robust
vision, tactile feedback, grasp stability analysis, modeling of
uncertainty, learning, long-term planning, and much more.
In order to address these difficult issues, there have been an
increasing number of governmental research programs such
as the European projects DEXMART, GeRT and GRASP, and
H. Ben Amor (B)
Robotics and Intelligent Machines, Georgia Institute of
Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA
e-mail: hbenamor@cc.gatech.edu
A. Saxena
Cornell University, 4159 Upson Hall, Ithaca, NY 14853, USA
e-mail: asaxena@cs.cornell.edu
N. Hudson
Jet Propulsion Laboratory, M/S 82-105 4800 Oak Grove Drive,
Pasadena, CA 91109, USA
e-mail: nicolas.h.hudson@jpl.nasa.gov
J. Peters
Technical University Darmstadt, Hochschulstr. 10 ,
64289 Darmstadt, Germany
e-mail: mail@jan-peters.net

the American DARPA Autonomous Robotic Manipulation
(ARM) project. This increased interest has become apparent
in several international workshops at important robotics conferences, such as the well-attended workshop “Beyond Robot
Grasping” at IROS 2012 in Portugal. Hence, this special issue
of the Autonomous Robots journal aims at presenting important recent success stories in the development of advanced
robot grasping and manipulation abilities. The issue covers
a wide range of different papers that are representative of the
current state-of-the-art within the field. Papers were solicited
with an open call that was circulated in the 4 months preceding the deadline. As a result, we have received 37 submissions
to the special issue which were rigorously reviewed by up to
four reviewers as well as by at least one of the guest editors.
Altogether twelve papers were selected for publication in
this special issue. We are in particular happy to include four
papers which detail the approach and goal of the DARPA
ARM project as well as detailed descriptions of the developed methods.
We start with the paper “The DARPA Autonomous
Robotic Arm (ARM) Program - Synopsis” by Hacket et
al. which provides a general overview of the recent ARM
project. The paper motivates the decisions made during the
ARM project and gives insights into its history, structure and
current state.
In the paper “An Autonomous Manipulation System based
on Force Control and Optimization”, Righetti et al. introduce a manipulation architecture that uses force-torque control, variable compliance and optimization methods to realize
robust grasping. The architecture includes components for
calibration, perception, motion planning, motion primitive
execution, inverse kinematics and force control, and, therefore, addresses a number of vital sub-tasks of grasping and
manipulation. The efficiency of this approach was demonstrated during competitions in the DARPA ARM project,

123

2

where Righetti and colleagues were consistently ranked
among the top two teams.
The system of another top team at the DARPA ARM
project is described in the paper “Model-Based Autonomous
System for Performing Dextrous Human-Level Manipulation Tasks” by Hudson et al. from NASA JPL. The paper discusses the challenges faced during the project and the related
competition and presents system architecture for autonomous
bimanual manipulation. The required perceptual capabilities
including mapping, object detection and object tracking are
described in detail in the first part of the paper. The second
part of the paper focuses on planning, control and action execution. The paper provides important insights into the design
choices that were made during the development of the system, as well as their advantages and drawbacks.
In the last paper on ARM entitled “Learning of Grasp
Selection based on Shape-Templates”, Herzog et al. present
an algorithm for selecting good grasp poses for unknown
objects from point cloud data. The approach uses a local
grasp shape descriptor to encode suitable grasp locations on
an object. This descriptor is used together with kinesthetic
teaching methods in order to create a grasp library of stable
grasps. The approach also includes a learning component that
allows feedback on success or failure of a grasp to be used
for adapting the library. The robustness of the approach is
demonstrated in an extensive experiment with a variety of
household objects.
The next three papers of the special issue highlight recent
advances in the design of robotic actuators for grasping.
The paper “Design and Control of a Three-Fingered TendonDriven Robotic Hand with Active and Passive Tendons” by
Ozawa et al. presents a new three-fingered robotic hand and a
set of corresponding controllers. The paper discusses important aspects in the design of tendon-driven robotic fingers and
shows how active and passive tendons can be used to realize variable stiffness control. The approach was validated
by demonstrating five different types of stable grasps on a
variety of objects.
Another type of robot grippers is presented in the paper “A
Compliant Self-Adaptive Gripper with Proprioceptive Haptic Feedback” by Belizle et al. The presented gripper features
compliant joints, under actuation and a haptic interface. In
addition to the technical description of this actuator, Belizle
and colleagues also present a theoretical model using a quasistatic analysis. Finally, they also demonstrate the advantageous features of the new gripper in extensive simulated and
real experimental results.
A more bio-inspired approach to the design grippers is
introduced in the paper “A Variable Compliance Soft Gripper” by Giannaccini et al. The paper presents a novel tentaclelike gripper that has a large degree of variability in its shape.
In addition to the shape the authors also show how the compli-

123

Auton Robot (2014) 36:1–3

ance of the gripper can be changed according to the requirements of the task.
The remaining five papers focus on the algorithmic and
theoretical modelling of grasping and manipulation. In “An
Active Sensing Strategy for Contact Location without Tactile
Sensors Using Robot Geometry and Kinematics” Lee et al.
describe new methods for locating contacts without relying
on specialized sensors. A geometric estimation of the contact
point between the robot actuator and the environment is used
in conjunction with a control strategy in order to improve
estimation accuracy. Experimental results show that the estimated forces are more accurate that those achieved using
force-torque controllers.
The paper “Teaching Robots to Cooperate with Humans
in Dynamic Manipulation Tasks Based on Multi-Modal
Human-in-the-Loop Approach” by Peternel et al. focuses on
compliant robotic manipulation in the presence of a human
interaction partner. In this approach, a human demonstrator
first tele-operates a robot arm using a motion capture setup
in order to provide training data for a subsequent imitation
learning step. However, in contrast to previous work on imitation learning, not only the position of the human’s hand is
recorded, but also the human’s muscle activation. This information is, in turn, used to modulate the stiffness of the robot.
The approach is verified in a cooperative wood sawing task
where a human and a robot have to collaborate.
In “Autonomously Learning to Visually Detect Where
Manipulation Will Succeed”, Nguyen et al. turn towards
active learning of visual classifiers. They introduce a methodology for predicting successful locations for manipulation
based on visual features. A robot autonomously generates
training data by acting in his environment. The resulting
data set is then processed via dimensionality reduction and
Support Vector Machines. A set of experiments with a PR2
robot show how this methodology can be used by a robot to
autonomously improve the success rates during daily tasks,
e.g., turning a switch.
The paper “Object Search by Manipulation” by Dogar et
al. addresses the question of how to search for an object in
a cluttered environment. In such a situation a robot needs
to push away occluding objects in order to find what it is
looking for. Dogar and colleagues show that even a greedy
approach to pushing objects away can be optimal under certain conditions. They also present a second algorithm which
approaches polynomial time complexity and produces optimal plans under all situations. Both algorithms are evaluated on a real-world mobile robot platform. Additionally, the
authors provide a Markov Decision Problem formulation of
the problem and present a partial proof of optimality.‘
Finally, the paper “Analyzing Dexterous Hands using a
Parallel Robots Framework” by Borras et al. adapts and
extends an existing mathematical framework from the par-

Auton Robot (2014) 36:1–3

allel robotics literature to analyze underactuacted robotic
hands. The authors apply the framework to analyze a simple
example hand. In this analysis they show how the underactuation design parameters such as the transmission ratio and
the stiffness constants of the finger joints can modify the size
of the feasible workspace.
All 12 papers present significant developments in robot
grasping and manipulation and we hope that you will enjoy
reading them as much as we did.

Heni Ben Amor is a Research
Scientist at the Robotics and
Intelligent Machines Institute
at GeorgiaTech in Atlanta. He
received his M.Sc. from the
University of Koblenz-Landau,
and his Ph.D. from the Technische Universitaet Bergakademie
Freiberg. Heni has been a visiting
researcher with the Intelligent
Robotics Group of the University of Osaka, Japan and a postdoctoral scholar with the Technical University Darmstadt, Germany. His research has won several awards such as the Bernhard-v.-Cotta Award 2011 and the CoTeSys
Best Paper Award at IEEE RO-MAN 2009. Since 2011, he is also a recipient of the Daimler-Benz Fellowship. His research interests include
robotics, virtual reality, machine learning, motor skill learning and
human–robot interaction.

Ashutosh Saxena is an assistant professor in the Computer
Science department at Cornell
University. His research interests include machine learning,
robotics and computer vision. He
received his MS in 2006 and
Ph.D. in 2009 from Stanford University, and his B.Tech. in 2004
from Indian Institute of Technology (IIT) Kanpur. He has also
won best paper awards in 3DRR,
RSS and IEEE ACE. He was
named a co-chair of IEEE technical committee on robot learning.
He was a recipient of National Talent Scholar award in India and Google
Faculty award in 2011. He was named Alred P. Sloan Fellow in 2011,
a Microsoft Faculty Fellow in 2012, and received a NSF Career award
in 2013.

3
Nicolas Hudson is currently a
member of technical staff in the
Robotic Manipulation and Sampling group at the Jet Propulsion
Laboratory, California Institute
of Technology. He received the
BE(Hons) degree from the University of Canterbury in 2002,
the M.Eng degree from the California Institute of Technology in
2004, and a Ph.D. in Mechanical Engineering from the California Institute of Technology
in 2009. Dr. Hudson received
the 2012 NASA Early Career
Achievement Medal for technical achievement in autonomous manipulation, and a 2012 NASA Group Achievement Award as part of the
Autonomous Robotic Manipulation software team. He is the Task Manager of the JPL-Caltech DARPA ARM-S task.

Jan Peters is a full professor
(W3) at the Technische Universitaet Darmstadt and senior
research scientist at the MaxPlanck Institute for Intelligent
Systems. Jan Peters has received
the Dick Volz Best 2007 US PhD
Thesis Runner Up Award, the
2012 Robotics: Science & Systems - Early Career Spotlight,
the Young Investigator Award of
the International Neural Network
Society, and the IEEE Robotics
& Automation Society’s Early
Career Award. Jan Peters has
received four Master’s degrees in Computer Science, Electrical,
Mechanical and Aerospace Engineering at TU Munich and FernUni
Hagen in Germany, at the National University of Singapore (NUS) and
the University of Southern California (USC) as well as a Computer
Science Ph.D. from USC. Jan Peters has also been a researcher in Germany at DLR, TU Munich and the Max Planck Institute for Biological
Cybernetics, in Japan at ATR, at USC and at both NUS and Siemens
Advanced Engineering in Singapore.

123

Intelligent Exploration for Genetic Algorithms
Using Self-Organizing Maps in Evolutionary Computation
Heni Ben Amor

Achim Rettinger

Universität Koblenz-Landau
Universtitätsstraße 1
56070 Koblenz, Germany

Universität Koblenz-Landau
Universtitätsstraße 1
56070 Koblenz, Germany

amor@uni-koblenz.de

achim@uni-koblenz.de

ABSTRACT

guided by the ﬁtness of the current parent generation. During an optimization run thousands of individuals, each one
representing a possible solution, are generated, evaluated
and by chance recombined to produce oﬀspring. Information from previous generations is only implicitly and partially preserved in the current genome.
This bears the risk of a regeneration of individuals that
have already been seen in the search process. Even more
problematic is the fact that the search can be negatively
aﬀected by genetic drift. As a consequence, big parts of the
search space, potentially containing the global optimum, will
never be explored.
In this work we try to show that there is an eﬃcient way
not only to monitor the whole evolution process but also to
extract valuable data from it (see section 4) which is used
to guide the search process (see section 5). This task is
achieved by adaptive operators utilizing data, mined by a
Self-Organizing Map (SOM), from individuals of previous
generations. The evaluation of our approach proves that
GASOM is a well suited tool for addressing the issue of
premature convergence in GAs (see section 6). Finally we
point out how other problems can be solved by GASOM (see
section 7).

Exploration vs. exploitation is a well known issue in Evolutionary Algorithms. Accordingly, an unbalanced search
can lead to premature convergence. GASOM, a novel Genetic Algorithm, addresses this problem by intelligent exploration techniques. The approach uses Self-Organizing Maps
to mine data from the evolution process. The information
obtained is successfully utilized to enhance the search strategy and confront genetic drift. This way, local optima are
avoided and exploratory power is maintained. The evaluation of GASOM on well known problems shows that it
eﬀectively prevents premature convergence and seeks the
global optimum. Particularly on deceptive and misleading
functions it showed outstanding performance. Additionally,
representing the search history by the Self-Organizing Map
provides a visually pleasing insight into the state and course
of evolution.

Categories and Subject Descriptors
I.1.2.8 [Computing Methodologies]: Problem Solving,
Control Methods, and Search

General Terms
Algorithms

2. BACKGROUND

Keywords

2.1 Premature Convergence and the Loss of
Diversity

Genetic Algorithm, Self-Organizing Map, Exploration vs.
Exploitation, Diversity, Premature Convergence, Genetic Drift

1.

INTRODUCTION

Techniques from the ﬁeld of Evolutionary Computation,
in this case Genetic Algorithms (GA), have been proven to
be well suited for ﬁnding global optima in complex search
spaces. Using a population of individuals evolving over numerous generations as a metaphor the search is iteratively

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
GECCO’05, June 25–29, 2005, Washington, DC, USA.
Copyright 2005 ACM 1-59593-010-8/05/0006 ...$5.00.

A critical problem when dealing with Evolutionary Algorithms (EA) is the phenomenon of premature convergence.
According to [4], premature convergence occurs when the
population of a GA reaches a suboptimal state where genetic operators can no longer produce oﬀspring which outperforms their parents. In that case, the search process will
likely be trapped in a region containing a non-global optimum. A simple yet popular explanation for the occurence
of this phenomenon is the loss of diversity. In that context
diversity refers to the (genetic) variation of the population
members. Previous work on tackling premature convergence
was mostly centered on diversiﬁcation techniques. Various
enhanced strategies for diversity maintenance have been proposed which target the diﬀerent stages of the evolution process.
Selection
Some techniques try to prevent selection from being too
much biased towards high ﬁtness individuals. Usually this is

1531

2.2 The Self-Organizing Map

done by pre-processing the ﬁtness values prior to the selection procedure. Rank scaling [5], for instance, ranks the individuals according to their raw objective value. This avoids
the possibility of a small number of highly ﬁt individuals
dominating the reproduction process. On the other hand,
so called sharing methods [14] force an individual to share its
ﬁtness with other population members occupying the same
niche.

The SOM [7] is a class of artiﬁcial neural networks which
has proven to be a valuable tool in analysis and visualization of high-dimensional data. Based on unsupervised learning the SOM performs a non-linear mapping from a highdimensional input space onto a normally two-dimensional
grid.
More precisely speaking, the SOM consists of a set U of
units or neurons arranged on a regular grid. Each unit i ∈ U
is assigned a prototype vector mi = [mi1 , mi2 , ..., min ] where
n is the number of dimensions of the input space. The neurons are connected to adjacent neurons by a neighborhood
relation which constitutes the topology of the map. Usually, a rectangular or hexagonal topology is used. Throughout this paper the rectangular topology will be used as this
facilitates the visualization of each unit’s prototype vector.
The SOM has successfully been applied in a wide range
of research areas covering data mining, pattern recognition
and most interestingly in the analysis and control of complex
systems.

Mating
Another strategy to maintain high diversity is to apply restriction and encouragement to the selection of the mating
partners. In [1] incest is prevented by prohibiting crossover
of genetically similar individuals. More speciﬁcally, mating
partners with a Hamming distance below a given threshold
were not authorized for crossover. Seduction [13] tries to
mimic the mating behavior of sexually reproducing organisms. Here, the mating decision is based upon an attraction
value computed between the partners.
Replacement
In steady-state GAs the replacement strategy determines
which elements will lose their place in the population when
new chromosomes are inserted. A well designed replacement condition can keep promising genetic material from
getting lost. In deterministic crowding [9], each child replaces the most similar parent if it has higher ﬁtness. As
parents and children typically are very similar, unfair competition between individuals occupying diﬀerent niches is
avoided. Recently, a hybrid replacement scheme was proposed by Lozano et al[8]. They try to replace chromosomes
which perform poorly with respect to both ﬁtness and contribution to diversity.

3. THE GASOM APPROACH
GASOM stands for “Genetic Algorithm using Self-Organizing Maps.” This novel approach aims at avoiding the
problems of GAs discussed in section 2.1 through information obtained from the evolution process by a SOM. These
are its main features:
1. An explicit representation of the search history
2. A ﬁtness evaluation promoting novelty
3. A reseeding operator preserving exploratory power

Although the extensions described above have been shown
to partially improve the eﬃciency of the search, they make
simplifying assumptions by attacking rather the symptoms
of premature convergence than the real cause [6].
First of all, high diversity does not imply better GA performance. This is tightly coupled to the question of exploration
vs. exploitation. Enforcing diversity in the early phases of
evolution ensures a broad exploration of the search space.
However, in phases when high exploitation is needed, such
promotion could be counterproductive or even destructive.
Finding a modiﬁable balance between exploration and exploitation is the key to this issue.
Furthermore, once diversity is lost exploratory power cannot be solely regained by selection, mating or replacement of
current individuals. Additionally, high mutation rates capable of increasing the diversity again are known to be mainly
destructive. Hence there is need for an intelligent operator, which is able to reintroduce fresh and potential genetic
material into the gene pool, even in highly converged populations.
Finally, previous approaches tried to ensure high genetic
variability for each generation individually. Considering only
the diversity of the current population, important regions of
the solution space are possibly neglected, while others are
revisited numerous times. As will be shown in this paper,
the search can be controlled and guided more eﬃciently if
diversity is considered throughout the whole evolution process. We refer to this as novelty.

4. A control mechanism balancing exploration and exploitation
GASOM is based on a standard steady-state GA. In the next
two sections we will elaborate on its improvements in detail.
First, we will show in section 4 how SOMs can be used to
monitor, document and analyze the behavior of EAs during
an optimization run. The representation of the search history developed in this section, will help to gain insight into
phenomena occurring during evolution and extract useful
information from it. Section 5 comments on how the extracted information can be used to guide the search process
eﬃciently.

4. MINING THE EVOLUTIONARY SEARCH
Although working in an algorithmically simple manner,
EAs can produce vast amounts of data during an optimization run. In each of the numerous generations a large number of chromosomes is generated and evaluated. In traditional GAs only the current population is stored. But with
the help of data from the previous generations you could
gain valuable insight into the way EAs work and the problems they encounter. Possibly, we could draw important
conclusions on how to improve their eﬃciency and guide the
search online. Thus, there is need for an intelligent and
eﬃcient way of mining and storing the data during computation in real time. Necessary requirements are for example, processing incoming data such that a new chromosome should not lead to an entire recalculation of previously

1532

where t is the current iteration, α(t) is the learning
rate at iteration t and hbi (t) is a neighborhood kernel
centered at the winning unit:
«
„
r − r  2
(3)
hbi (t) = exp −
2σ 2 (t)

computed knowledge. In other words, the knowledge acquisition should be incremental. Furthermore, the employed
technique should be able to analyze data of arbitrarily high
dimensions and independent from the number of data entities processed. Additionally, the computational overhead
caused by its application should be scalable according to the
needs. Finally, to facilitate human introspection and qualitative analysis, the applied technique should support some
kind of graphical output.
The SOM algorithm introduced in 2.2 meets the discussed
requirements. It projects the data samples onto a twodimensional lattice. In contrast to many other multidimensional scaling methods, this projection does not have to be
repeated when new data points are evaluated. It can be
stored as a table and has low memory demands. The tabular representation facilitates the quantitative analysis of
the recorded data. It can also easily be turned into various
visualization forms including bitmaps, histograms or trajectories. In [12] SOM’s have already been used to visulaize
various aspects of evolutionary search.
Before applying, the SOM has to be appropriately trained
in order to represent the envisioned data set. In this case
the focus is on the genotype or solution space. Training the
SOM with a large number of points from this space yields a
two-dimensional projection of it.

where r and r  denote the positions of the BMU and
the neuron i on the SOM grid. Both α(t) and σ(t) are
monotonically decreasing functions of time.
Step 2 and 3 are repeated until the maximum number of iterations is reached. Once training is ﬁnished, each unit stands
for a particular region of the solution space. It represents
all chromosomes to which it is closest.
The learning process, as described above, is of stochastic
nature. It also expects various parameters like the learning
rate to be set by the user. Consequently, the diﬀerence in
accuracy between two trained maps can vary heavily. If we
would apply a map of mediocre accuracy to analyze a given
data set, we would probably draw wrong conclusions. Hence
we need to ensure that a map is of reliable quality before we
can use it for the envisioned tasks.
In this work a simple test procedure is employed to test
the quality of a SOM.We randomly generate a set of new
chromosomes and project them onto the map. The projection is done by ﬁnding the appropriate BMU for each of
these chromosomes, as deﬁned by equation 1. We keep track
how many times each neuron was activated as the BMU. Let
E(i) be a function, that returns the number of times a neuron i was activated. Ideally, the activation frequency of all
neurons is equal:

4.1 Training the SOM
In this section how to train a SOM and how to use it to
analyze the population is discussed. The training is carried
out only once before the start of the GA. As long as the
chromosome length and the representation form (binary vs.
real coded) does not change, a trained SOM can be stored
and reused. In the training phase a large number of diﬀerent individuals from the genotype space are shown to the
map. In our experiments a set of 100.000 diﬀerent individuals and a map with 10x10 neurons were used.1 The number
n of weights in the prototype vectors equals the number of
genes in a chromosome. The training comprised the following steps:

E(i) = E(j)

4.2 Mapping the Population
Now, we can employ the trained SOM to classify incoming data/chromosomes. For this we assign each evaluated
individual to the nearest map unit, as already done before
in the optimization step. To store the resulting information,
we will use two frequency tables. The ﬁrst table, which we
will call the population distribution table, holds the activation frequencies with respect to only those individuals that
are currently in the population. In the following the activation frequency of a neuron i in the population distribution
table, will be denoted by Ep (i). The second table, called the
search history table, stores the activation frequencies with
respect to all individuals evaluated during evolution. To access the information from this table we will use the function
Eh (i).
Examining the ﬁrst table, we can assess the diversity in
the current population. The higher the number of diﬀerent
activated neurons the higher the diversity. In contrast to
this, the search history table gives us an insight into the
course of evolution. If for instance a particular neuron was
rarely activated, we can deduce that the associated region
was not suﬃciently explored by the GA. But, before we get
to the analysis phase, we have to ﬁnd ways to represent the
contents of both the SOM and the frequency tables.

2. Stimulus and Response: A sample chromosome x
is randomly chosen from the genotype space. In this
step it is crucial that the random selection of chromosomes obeys a uniform distribution. If not, the
learning could be biased towards a particular region
of the search space. Distances to all prototype vectors
are then computed via some distance measure (usually Euclidean distance). The winning neuron b, also
called best-matching unit (BMU), is the map unit with
prototype closest to x.
i

(1)

3. Adaptation: The prototype vectors are then updated
according to the following update rule:
mi (t + 1) = mi (t) + α(t)hbi (t)[x − mi (t)]

(4)

Thus, in order to ensure reliable analysis, we take a SOM
which ﬁts requirement 4, best. If it does not, we assume
that the learning was biased.

1. Initialization: Choose random values for the weights
mi1 , mi2 , ..., min of the prototype vectors. As the only
restriction, the weight vectors mi have to be diﬀerent.

x − mb  = min{x − mi }

∀i, j ∈ U

(2)

1
The words unit and neuron are used here in a synonymical
way.

1533

(a)

(b)

(c)

Figure 1: A trained SOM in prototype representation(a), ﬁtness landscape representation(b) and search
history representation(c)

4.3 Representing the Search History
Although the procedure outlined above eﬀectively processes the chromosome data set it still causes diﬃculties to
the human user to interpret the results. This is mainly due
to the numerical output of the procedure. Interpretation
becomes easier if we use visual representation forms.

version of the prototype oriented representation from 1a. In
addition, every activation of a unit in the search history table is plotted as a black dot. It is slightly perturbed for
easier identiﬁcation of multiple activations. Each of these
dots represents the projection of an evaluated chromosome
onto the map. These additional plots tell us which parts of
the search space have been explored by the GA.

In ﬁgure 1a a trained SOM of 10x10 units can be seen.
Training was eﬀectuated with binary chromosomes of 16
bits. Each unit is represented by a pattern of 16 pixel visualized as 4x4 blocks. A pattern denotes the underlying
prototype or weight vector of that particular unit. Each
black pixel in the pattern refers to a 0 while a white pixel
refers to 1 in the weight of the prototype vector. For instance, if a unit is depicted by an entirely white block the
corresponding weight vector is ’1111111111111111’.

In ﬁgure 1c it can be observed, that the used GA effectively escaped the regions of low ﬁtness. The search is
mostly centered around regions of high ﬁtness in the lower
right and the upper left corner of the SOM. Furthermore, it
can be observed that a few neurons dominate most of the
activity, whereas a large number of neurons in the middle of
the map have not even been activated once.
What looks like the optimal search strategy at the ﬁrst
sight is in fact an undesirable behaviour of a GA. Fitness
landscapes, GAs are normally applied to, are almost always discontinuous, deceptive or have numerous local optima. Otherwise there are techniques better suited for ﬁnding global optima like simple hill-climbing for this example.
But there is no way that a GA that does not explore such
big parts of the search space and loses its exploratory power
this fast can succeed in a diﬃcult task.

If you take a closer look at the map, you detect that there
is an axis of blackish units beginning at the lower left corner and reaching to the upper right corner. The prototype
vectors of these units have mostly black pixels. This is due
to the fact that in the training process genotypically similar
chromosomes are clustered onto neighbouring neurons.
Now, we apply the above SOM to a 16bit 1s-counting
(Max1) problem. In this problem, the ﬁtness of an individual equals the number of 1s in its chromosome. We visualize
the ﬁtness distribution on the map by assigning each neuron
a color according to the ﬁtness of the corresponding weight
vector. This yields a low resolution picture of the ﬁtness
landscape as can be seen in ﬁgure 1b. Good units have a
light color while worse ones have darker color. The ﬁgure
conﬁrms the observation made before. The diagonal axis
beginning at the left lower corner can now clearly be seen
as a dark cluster of low ﬁtness. We would expect a GA to
avoid this region and seek out the higher ﬁtness regions in
the other corners.

Generation 20 Generation 30 Generation 70
Figure 2: Visualization of Genetic Drift
Figure 2 visualizes three diﬀerent states of a population
distribution table during the optimization of a multimodal
function. The color coding used in the ﬁgures above refers
to the activation count of each neuron in the population
spread table. The darker the color of a neuron the lower
its activation count. We see that in generation 20 two different neurons have a high activation count (visualized as

To check this last assumption, we will overlay the information from the search history table onto the map. The
information was gathered during a run of a simple GA on
the Max1 problem. In ﬁgure 1c one sees a slightly faded

1534

white pixels). These neurons correspond to the subspaces
containing the function’s global optima. In generation 30
genetic drift focuses the search on one of the two subspaces
(the upper left white unit becomes gray). Finally, in generation 70 we notice that almost all neurons have low activation
frequencies except of one single peak. At that time there is
no more competition among subspaces on the map. The
exploratory power and diversity is lost.
Up to this point we have shown that the population distribution table and the search history table are powerful tools
to monitor the search process of EAs. The information is
mined by SOMs and stored in the tables while meeting all
requirements in terms of computational complexity.

5.

then vanish from the population, giving place to individuals
with higher novelty. Thus, an increase in exploitation will
also eﬀect an increase in exploration.

5.2 Confronting Genetic Drift with Reseeding
To regain lost diversity some GA variants employ reseeding operations. For instance, the CHC algorithm [2] reseeds
a set of random individuals to restore variation in highly
converged populations. In [11] the reseeding operation is
based on previously encountered high ﬁtness points.
GASOM performs reseeding by introducing individuals
with a high novelty factor into the population. Such individuals correspond to the neurons with low activation count
in the search history table. Reseeding involves two steps.
First, a so called reseeding-pool is created. The reseedingpool consists of a set of buckets, each of which stands for
a particular neuron on the SOM. The buckets are ﬁlled by
randomly created new chromosomes. Each newly created
chromosome is assigned to the bucket which represents its
BMU. The requirements we have set up for SOMs in section 4.1 guarantee that the buckets will be uniformly ﬁlled.
Then, the neurons with lowest activation frequencies are
determined. If needed, individuals are sampled (without
replacement) from the respective buckets and inserted into
the population. To keep the population size constant the
individual from the population with the lowest ﬁtness has
to be killed instead. Metaphorically speaking the GASOM
reseeding operator could be viewed as a resettlement of individuals with a low ﬁtness from overcrowded areas to sparsely
populated regions.
To evaluate the performance of this approach, we conducted a few experiments on a 16bit Max1 function. In these
tests we compared it to other strategies, namely randomreseeding and reseeding of old individuals. A standard generational GA was used with a population size of 100 individuals. At each generation 10 individuals were requested from
the reseeding operator. To assess their quality we compared
them with other chromosomes, but did not insert them into
the population.

ENHANCING THE
EVOLUTIONARY SEARCH

Based on the information gained from the analysis and
the representations described above one can devise strategies to tackle the problems outlined in section 2.1 and 4.3.
Steps taken in GASOM towards this goal is the topic of this
section.
If at some instance in time evolution focuses too much on
a particular set of individuals and the exploratory power is
at risk, we can try to guide it towards unexplored regions of
the solution space. By looking up the search history table,
the least explored subspaces can be easily determined. In
subsection 5.1 it will be discussed how the tables holding
activation frequencies can be used to develop a novelty promoting ﬁtness evaluation. This new ﬁtness evaluation will
encourage the exploration of previously neglected solutions.
However, this alone will not be enough to tackle premature
convergence. As already motivated in subsection 2.1 a powerful operator is needed in order to reintroduce fresh and
potential genetic material. This will be explained in subsection 5.2. In subsection 5.3 the question of exploration vs.
exploitation will be addressed. A rough measurement for
the ratio of exploration and exploitation is introduced and
used to balance the evolutionary search.

5.1 Adapting the Fitness Evaluation

10.5
GASOM
Random

In GASOM the ﬁtness of a chromosome is computed by
the use of two ranks. The ﬁrst rank results from ordering
the population with respect to the objective value (ﬁtness).
The second rank orders individuals by their novelty. Novelty
here refers to the number of similar chromosomes already encountered during evolution. To determine the novelty factor
of a chromosome we use the following function:
novelty(c) = 1/Eh (bc )

10

Hamming Distance

9.5

9

8.5

8

(5)

7.5

where c is the current chromosome and bc is its BMU on
the trained Self-Organizing Map. Put in words, novelty is
the inverse of the activation frequency in the search history
table. The sum of both ranks is then used as the individual’s
ﬁtness score.
By applying the latter ﬁtness assignment scheme an individual is given two chances to survive in the population.
The ﬁrst possibility is a good performance on the posed
(optimization) problem. The second possibility a contribution to the exploration of new subspaces. In phases of high
exploitation the activation frequencies of exploited regions
will increase drastically. As a result the novelty factor of the
corresponding individuals will decrease. Many of them will

7

0

50

100

150

200

250

300

350

400

450

500

Generations

Figure 3: Average Hamming distance of a reseeded
individual to all previously evaluated individuals
In ﬁgure 3 a plot of the average Hamming distance of reseeded chromosomes to all previously encountered chromosomes can be seen. Random-reseeding maintains an average
distance of approximately 8 bits throughout all of the 500
generations. The SOM based reseeding starts with similar
values in the ﬁrst few generations. The average distance

1535

end of the search process it is favorably to make the most
out of the already found best solutions. So exploitation is
the better choice. In other words, the degree of exploitation
should be monotonically decreasing and the degree of exploration should be monotonically increasing during the search
run, respectively.
The novelty based ﬁtness evaluation, as pointed out in
section 5.1, is a good starting point for balancing exploration
and exploitation. So far the two ranks are equally weighted
to produce the ﬁnal ﬁtness of an individual. This results in
an implicitly well balanced exploration and exploitation. If a
dynamic behaviour with high exploration in the beginning
and high exploitation in the end is aspired explicitly the
ranks could be weighted accordingly. This concept is already
implemented for the reseeding operator (see section 5.2). So,
how many individuals should be reseeded in a speciﬁc time
step?
First, the amount of exploration in the current state of
the GA is assessed by counting the number of diﬀerent activated neurons in the population distribution table. Second,
a balance function is used to determine the number of neurons which ideally should be activated in the current state of
the search process. This optimal number of activated units
r is calculated by

then steadily increases up to approximately 10 bits . This
phenomenon occurs due to the fact that the quality of reseeding becomes better the more activations are stored in
the search history table.
Table 1: Occurrence number of minimal Hamming
distance (reseeded vs. all encountered individuals)
Hamming Distance in Bits
Method
0
1
2
3
>3
Old Indiv.
500
0
0
0
0
Random
199 275 25 1
0
SOM
6
241 246 7
0
In the next experiment at each generation the minimal
Hamming distance between the reseeded individuals and
the encountered individuals is measured. Table 1 shows the
number of times each distance was measured. Of course reseeding of old individuals always results in a minimal Hamming distance of 0, as the reseeded individuals are all previously seen. A more sensible comparison with respect to novelty can be done by comparing random- and SOM-reseeding.
While the random strategy reseeded in 199 generations a
previously seen chromosome, the SOM based strategy did
so in only 6 generations. The table also shows, that our
new approach reseeded most of the time chromosomes which
have at least a Hamming distance of 2 bits to all previously
encountered chromosomes.

r=

„
1−

n
nmax

«
∗ ||U ||

(6)

where n is the number of ﬁtness evaluations already performed, nmax is the maximal number of ﬁtness evaluations
and ||U || is the total number of neurons of the SOM. The
reseed fraction equals the diﬀerence between the ideal and
measured neuron count.
Only through the eﬀective combination of both the new
ﬁtness assignment scheme and the SOM reseeding operator
an eﬃcient exploration strategy can be achieved. If the reseeding operator would be used alone, the introduced new
individuals would in most cases die immediately and disapear from the population because novelty is not taken into
account. On the other hand, by actively generating novel
chromosomes one does not have to rely on crossover and
mutation that only by chance might produce new genetic
material.

Table 2: Occurence number of minimal Hamming
distance (reseeded vs. population individuals)
Hamming Distance in Bits
Method
0
1
2
3
4
5 >6
Old Indiv.
173 183 104 24
9
8
1
Random
7
55 230 185 23
0
0
SOM
0
0
30 134 256 74
6
The same experiment was then repeated (see table 2).
This time however, we measured the Hamming distance between the reseeded individuals and the individuals in the
current population. The results conﬁrm our previous observations. The SOM based reseeding never reintroduced an
individual which was already in the population, while the
other techniques did so in 7 generations (random) and 173
generations (old Indiv.).
We can conclude that the SOM-reseeding strategy eﬀectively maintains high exploratory power. Please note that
although reseeding of old individuals performed poorly in
the above tests it has the positive side eﬀect of keeping the
number of ﬁtness evaluations low.

6. EVALUATION AND RESULTS
GASOM was evaluated using three well known problems,
namely a Royal Road function [10], the Deceptive F9 function from [9] and the generalized Rosenbrock’s function[5].
The chromosome length was 64 bits, 24 bits and 60 bits
respectively.
The following parameter setting was used in all test runs.
A mutation rate pm = 0.005 and a crossover rate of pc =
0.7 with two point crossover. Selection was accomplished
by binary tournament selection. The population size was
set to 50 individuals while the maximum number of ﬁtness
evaluations was 50.000 evaluations. To reduce statistical
noise, each function was run 100 times and the results were
averaged.
First a set of experiments was conducted, in which the
contribution of the constituent parts of GASOM to the overall performance was assessed. The performance measure
used is the average of the best ﬁtness found at the end of
each run. Table 3 shows the results of this experiments.

5.3 Balancing Exploration and Exploitation
The limiting factor of a GA search run is in most cases the
numbers of ﬁtness evaluation. Fitness evaluations consume
a lot of time in real world application and might even involve
testing by an expert. Thus, the goal of every GA should be
to get the best results with regard to a limited number of
ﬁtness evaluations. The key for an eﬃcient search is the
balance between exploration and exploitation. In the beginning of the search premature convergence should be avoided
before having covered as much of the search space as possible. In this phase exploration must be enforced while at the

1536

maturely. In GASOM the average ﬁtness makes small oscillations while increasing in a nearly linear fashion. The
oscillations are due to the interplay between the new exploration techniques and the greediness of the GA.

Table 3: Average best ﬁtness of GASOM variants
Method
Deceptive F9 Royal Road Rosenbrock
Novelty
24,19
167,20
0,02575
Reseeding
28,28
185,76
0,00119
Nov. & Res.
30,00
220,56
0,00028

Table 5: Number of times global optimum was found
(out of 100)
Method
Deceptive F9 Royal Road Rosenbrock
SteadyState
0
6
0
Det. Crowding
44
50
0
Hybrid
3
77
0
GASOM
100
72
0

160
GASOM
Steady State
Det. Crowding
Hybrid

140

Average Fitness

120

100

80

Table 5 shows the number of times the global optimum
was found on the used test functions. Our method achieves
outstanding results on the Deceptive F9 function. In all 100
runs it was able to ﬁnd a global optimum. The runner-up
on this function (Deterministic Crowding) was able to ﬁnd
the global optimum in 44 times, while the other GAs did
so in less than 5 times. The intelligent exploration scheme
signiﬁcantly improved the eﬃciency of the search process
on deceptive functions. This was conﬁrmed by other experiments we conducted. The analysis of the search history
table gave insight into why GASOM performed better on
this type of problems. Typically, on deceptive problems the
search is lead away from the global optimum. Consequently,
the corresponding units in the search history table will be
activated less. The reseeding operator reintroduces individuals from this part of the search space, while the novelty
based ﬁtness assignment scheme gives them a chance to survive and reproduce.
Although diversity is not the primary goal of our proposed
GA, it should implicitly lead to a balance of diversity. In
order to measure the population diversity we used the following function[3]:
PL
i=1 min(Fi , 1 − Fi )
(7)
d(P ) =
L/2

60

40

20

0
0

5000

10000

15000

20000

25000

30000

35000

40000

45000

50000

Fitness Evaluations

Figure 4: Performance on Royal Road

Best results are highlighted in bold. It can clearly be seen,
that the best performance is achieved when novelty and and
reseeding are used in combination. The synergetic eﬀect between the new ﬁtness assignment scheme and the reseeding
operator leads to a signiﬁcant improvement. This conﬁrms
the assumption made in section 5.3 that both proposed elements are needed for an eﬃcient exploration strategy.
Next, another set of experiments was conducted in which
we compared the performance of GASOM to a standard
steady-state GA, deterministic crowding [9] and the novel
hybrid replacement scheme of Lozano et al. [8]. These variants have already been shown to signiﬁcantly improve the
performance of evolutionary search. Table 4 shows the results of this experiment.

and
Table 4: Average ﬁtness of diﬀerent GAs
Method
Deceptive F9 Royal Road Rosenbrock
SteadyState
24,00
95,44
0,08394
Det. Crowding
26,88
188,72
0,00067
Hybrid
22,54
227,20
0,09671
GASOM
30,00
220,56
0,00028
Both, on the Deceptive F8 function and the Rosenbrock
function GASOM yields the best results out of the tested
GAs. On the Royal Road function the hybrid method performed slightly better. However, its performance deteriorates on the other two functions. We may observe that GASOM achieves a good trade-oﬀ in performance on diﬀerent
problems and gives more stable results. Due to its improved
exploration ability, GASOM is able to drive the evolution
towards the most promising new regions.
Figure 4 shows plots of the average ﬁtness during a run
of the Royal Road function. Both the simple steady-state
GA and the GA with hybrid replacement exhibit the typical
ﬁtness jumps as new building blocks are rapidly propagated
among population individuals. Deterministic crowding has
a very neat looking monotic behavior but convergences pre-

PN
Fi =

j=1

Pi (j)

(8)
N
where N equals the population size and L is the chromosome
length in bits. Pi (j) returns the allele at the j th gene of
the chromosome. Figure 5 shows the change in diversity
during an optimizationn run of the Royal Road function. It
can be seen, that GASOM eﬀectively balances the diversity
according to the phase of evolution. In the beginning it
maintains high diversity, which then linearly drops to nearly
0 in the later phases of exploitation.

7. CONCLUSIONS AND FUTURE WORK
In this work we proposed an intelligent exploration technique for GAs. The approach uses SOMs to analyize data
of the evolution process and utilize it to enhance the search
strategy. This lead to the development of a GA with four
new core components: an explicit representation of the search
history, a ﬁtness evaluation promoting novelty, a reseeding
operator preserving exploratory power and a control mechanism balancing exploration and exploitation. This framework, called GASOM, was tested against several other ap-

1537

0.9

and comments. We are also grateful to the anonymous reviewers for their constructive and helpful comments.

GASOM
Steady State
Det. Crowding
Hybrid

0.8

0.7

The authors are partially supported by the German research foundation DFG.

Diversity

0.6

0.5

9. REFERENCES

0.4

0.3

0.2

0.1

0
0

5000

10000

15000

20000

25000

30000

35000

40000

45000

50000

Fitness Evaluations

Figure 5: Diversity on Royal Road

proaches and proved to be well suited for preventing premature convergence.
GASOM performed considerably well on deceptive problems where other GAs tend to get stuck in local optima.
The employed reseeding operator succesfully resisted genetic
drift. Hence, exploratory power was maintained where necessary. Besides that the SOM oﬀers a neat tool for visualizing the state of evolution.
An obvious reﬁnement of the discussed algorithm would
be an explicit adaption of the ﬁtness assignment. This could
be achieved by weighting the inﬂuence of the novelty rank
according to the measured degree of exploration (see section
5.3). Another starting point for straight forward ﬁne-tuning
is the balance function. Up to now only a linearly decreasing function was used. Exponential decline might be more
eﬀective. Another minor adjustment could make GASOM
ﬁnd as many diﬀerent optima as possible instead of focusing
on one global optimum. Once more the key would be an intelligent compromise between exploration and exploitation.
A more fundamental issue is the size of the SOM. The
eﬀect of varying the number of neurons on the performance
has not been investigated so far. An optimal trade-oﬀ between accuracy of the search history table and computational demands is desirable.
Furthermore, we aim at adjusting GASOM to multi-objective
problems. A promising study could be the identiﬁcation of
each unit’s part of the Pareto front. The same methods used
in GASOM for balancing exploration and exploitation (see
section 5.3) could be used to balance the diﬀerent objective
functions, accordingly. For this task other dimensionality reduction techniques might be more appropriate than SOMs.
Finally, we plan to apply GASOM in machine learning
environments. More speciﬁcally we are working on its application to robot control in the RoboCup domain.

8.

ACKNOWLEDGMENTS.

We would like to thank our advisors Oliver Obst and Jan
Murray for their encouragement and the stimulating discussions. Thanks to Thomas Kleemann for his insightful hints

1538

[1] L. Eschelman and J. Schaﬀer. Preventing premature
convergence in genetic algorithms by preventing
incest. In Proceedings of the Fourth International
Conference on Genetic Algorithms, pages 115–122.
Morgan Kaufmann, 1991.
[2] L. Eshelman. The chc adaptive search algorithm: How
to have safe search when engaging in nontraditional
genetic recombination. In Foundations of Genetic
Algorithms, pages 256–283. Morgan Kaufmann, 1991.
[3] C. Fernandes and A. Rosa. A study on non-random
mating and varying population size in genetic
algorithms using a royal road function. In Proceedings
of the 2001 Congress on Evolutionary Computation
CEC2001, pages 60–66. IEEE Press, 2001.
[4] D. Fogel. An introduction to simulated evolutionary
optimization. IEEE Transaction on Neural Networks,
5(1):3–14, 1994.
[5] D. A. Goldberg. Genetic Algorithms in Search,
Optimization, and Machine Learning. Addison-Wesley,
1989.
[6] J. Hu, K. Seo, Z. Fan, R. Rosenberg, and
E. Goodman. Hemo: A sustainable multi-objective
evolutionary optimization framework. In Proc. 2003
Genetic and Evolutionary Computing Conference.
Springer Verlag, July 2003.
[7] T. Kohonen, T. Kohonen, M. R. Schroeder, and T. S.
Huang. Self-Organizing Maps. Springer-Verlag New
York, Inc., 2001.
[8] M. Lozano, F. Herrera, and J.R.Cano. Replacement
strategies to preserve useful diversity in steady-state
genetic algorithms. In Press, March 2004.
[9] S. W. Mahfoud. Niching methods for genetic
algorithms. PhD thesis, University of Illinois at
Urbana-Champaign, Urbana, IL, USA, 1995.
[10] M. Mitchell, S. Forrest, and J. H. Holland. The royal
road for genetic algorithms: Fitness landscapes and ga
performance. In F. J. Varela and P. Bourgine, editors,
Toward a Practice of Autonomous System:
Proceedings of the First European Conference on
Artificial Life, pages 245–254, 1991.
[11] K. Rasheed. GADO: A Genetic Algorithm for
Continuous Design Optimization. PhD thesis, Rutgers
University, New Brunswick, NJ, 1998.
[12] G. Romero, J.J.Merelo, P. Castillo, J.G.Castellano,
and M. Arenas. Genetic algorithm visualization using
self-organizing maps. In Parallel Problem Solving from
Nature - PPSN VII, pages 442–451. Springer Verlag,
2002.
[13] E. Ronald. When selection meets seduction. In
Proceedings of the Sixth International Conference on
Genetic Algorithms. Morgan Kaufmann, 1995.
[14] B. Sareni and L. Kraehenbuehl. Fitness sharing and
niching methods revisited. IEEE Transaction on
Evolutionary Computation, 2(3):97–106, September
1998.

Physical Human–
Robot Interaction
Mutual Learning and Adaptation

By Shuhei Ikemoto, Heni Ben Amor,
Takashi Minato, Bernhard Jung,
and Hiroshi Ishiguro

©istockphoto.com/maxuser

C

lose physical interaction between robots and humans is a particularly challenging aspect
of robot development. For successful interaction and cooperation, the robot must have
the ability to adapt its behavior to the human counterpart. Based on our earlier work, we
present and evaluate a computationally efficient machine learning algorithm that is well
suited for such close-contact interaction scenarios. We show that this algorithm helps to
improve the quality of the interaction between a robot and a human caregiver. To this end, we present
two human-in-the-loop learning scenarios that are inspired by human parenting behavior, namely,
an assisted standing-up task and an assisted walking task.
Human–Robot Interaction and Cooperation
Until recently, robotic systems mostly remained in the realm of industrial applications and academic
research. However, in recent years, robotics technology has significantly matured and produced
highly realistic android robots. As a result of this ongoing process, the application domains of robots
have slowly expanded into domestic environments, offices, and other human-inhabited locations. In
turn, interaction and cooperation between humans and robots has become an increasingly important
and, at the same time, challenging aspect of robot development. Particularly challenging is the physical interaction and cooperation between humans and robots. For such interaction to be successful
and meaningful, the following technical difficulties need to be addressed:

Digital Object Identifier 10.1109/MRA.2011.2181676
Date of publication: 29 February 2012

24

•

IEEE ROBOTICS & AUTOMATION MAGAZINE

•

DECEMBER 2012

1070-9932/12/$31.00©2012IEEE

guaranteeing safety at all times
ensuring that the robot reacts appropriately to the force
applied by the human interaction partner
●● improving the behavior of the robot using a machine learning algorithm in a physical human–robot interaction
(PHRI).
In our previous research [1], we presented a PHRI scenario
in which we addressed the above topics. Inspired by the parenting behavior observed in humans, a test subject was asked
to physically assist a state-of-the-art robot in a standing-up
task. In such a situation, both the human and the robot are
required to adapt their behaviors to cooperatively complete
the task. However, most machine learning scenarios to date
do not address the question of how learning can be achieved
for tightly coupled, physical interactions between a learning
agent and a human partner. Building on the results in [2], we
present an extended evaluation and discussion of such
human-in-the-loop learning scenarios.
To realize learning and adaptation on the robot’s side, we
employ a computationally efficient learning algorithm based
on a dimensional reduction technique. In particular, after each
trial, the human can judge whether the interaction was successful, then the judgment is used in a machine learning algorithm to apply a dimensional reduction technique and update
the behavior of the robot. As learning progresses, the robot
creates a behavioral model, which implicitly includes the
actions of the human counterpart.
At the same time, refining the motions of the robot during a physical interaction requires the motions of the
human to be improved, because the two motions influence
each other. Hence, the human counterpart is part of the
learning system and overall dynamics. To analyze the efficiency of the proposed learning algorithm and the effect of
human habituation to the robot during such close-contact
interactions, we perform a set of PHRI experiments. In
addition to the assisted standing-up interaction scenario
presented in [2], we also present and discuss the first results
based on a novel interaction scenario. More specifically, we
present an assisted walking task in which a human caregiver must assist a humanoid robot while walking.
We believe that human-in-the-loop learning scenarios,
such as that presented herein, will be particularly interesting
in the future because they can help to strengthen the mutual
relationship between humans and robots. Ideally, this will
lead to a higher acceptance of robotic agents in society.
●●
●●

Related Research
Important aspects of PHRIs have been investigated in a perspective research project conducted by the European Network
of Excellence (EURON) [3]. The objective of the project was
to present and discuss important requirements for safe and
dependable robots involved in PHRIs. Initial approaches for
achieving these requirements are currently being addressed in
a follow-up research project called PHRIENDS (a PHRI that
is dependable and safe). To reduce risks and fatalities in
industrial manufacturing workplaces, the primary goal of the

PHRIENDS project is to design robots that are intrinsically
safe. This requires the development of new actuator concepts,
safety measures, and control algorithms, which take the presence of human subjects into account. The results of this project are also relevant to applications outside the manufacturing
industry. However, learning and adaptation between humans
and robots is not the focus of the PHRIENDS project.
Khatib et al. [4] discussed the basic capabilities needed to
enable robots to operate in human-populated environments.
In particular, they discussed how mobile robots can calculate
collision-free paths and manipulate surrounding objects. In
their approach, they characterized free space using an elastic
strip approach. However, the described robots were not
expected to come into
direct (physical) contact
with the surrounding
Close physical interaction
human subjects. The
importance of direct physbetween robots and
ical interaction was highlighted in the haptic
humans is a particularly
creature project [5], which
investigated the role of
challenging aspect
affective touch in fostering
t h e c omp a n i on s h ip
of robot development.
between humans and
robots. In an attempt to
improve human–robot
interaction, Kosuge et al.
presented a robot that can dance with a human by adaptively
changing the dance steps according to the force/moment
applied to the robot [6]. Amor et al. [7] used kinesthetic interactions to teach new behaviors to a small humanoid robot.
Furthermore, the behavior of the robot may be optimized with
respect to a given criterion in simulation. In this learning
scheme, the robot is a purely passive interaction partner and
acts only after the learning process is complete. Similar
approaches to teaching new skills have also been reported in
[8] and [9] using different learning methods, i.e., continuous
time-recurrent neural networks and Gaussian mixture models
(GMMs), respectively. Odashima et al. [10] developed a robot
that can come into direct physical contact with humans. This
robot is intended for caregiving tasks such as carrying injured
persons to a nearby physician. The robot can also learn new
behaviors and assistive tasks by observing human experts as
they perform these tasks. However, this learning does not take
place during interactions but rather in offline sessions using
immersive virtual environments. In [11], Evrard et al. present a
humanoid robot with the ability to perform a collaborative
manipulation task together with a human operator. In a teaching phase, the robot is first teleoperated using a force-feedback
device. The recorded forces and positions are then used to
learn a controller for the collaborative task. The main hypothesis underlying this approach is that the intentions of the
human interaction partner can be guessed from haptic cues. In
[12], physical interactions between a robot’s hand and the hand
of a human are modeled by recording their distances. The
DECEmber 2012

•

IEEE ROBOTICS & AUTOMATION MAGAZINE

•

25

distances are then encoded in a hidden Markov
Until recently, robotic
model (HMM), which in
systems mostly remained in turn is used to synthesize
similar hand contacts. A
recent survey on modern
the realm of industrial
approaches to physical
and tactile human–robot
applications and academic
interaction can be found
in [13].
research.
In this article, we present experiments with a
flexiblejoint robot that is
involved in close physical interaction with a human caregiver.
In contrast to the above research, both human and robot play
an active role in the interaction to learn and adapt their behaviors to their partner so as to achieve a common goal. This tight
coupling of robot and human learning and coadaptation is a
unique feature and is the primary contribution of the present
study. We assume that it is important to focus on the active
role in the interaction because the forces generated during the

Memory

Learning

Critique

DR

GMM

Physical Interaction
(a)

(b)
Figure 1.  (a) Overview of the physical interaction learning
approach. After physical interaction, the human judges whether
the interaction was successful. This information is stored in the
robot’s memory and used for later learning. (b) Flexible-joint
humanoid robot used in the experiments in this study. (Photos
courtesy of ERATO Asada Project.)

26

•

IEEE ROBOTICS & AUTOMATION MAGAZINE

•

DECEMBER 2012

active behavior of the robot influence the behavior of the
human, which in turn influences the passive behavior of the
robot. In addition, these active and passive roles cannot be
clearly separated because the robot and the human influence
each other when they are in physical contact.
Physical Interaction Learning Approach
The goal of interaction learning is to improve the cooperation
of humans and robots while they are working to achieve a
common goal. Figure 1 shows an overview of the learning
scheme used in this article. After an initial physical interaction
between a human and a robot, the human is given the chance
to evaluate the behavior of the robot. More precisely, the
human can judge whether the interaction was a success or
failure (binary evaluation). The feedback can be provided in
various ways, such as through touch or through a simple
graphical user interface. Once the evaluation information is
collected by the robot system, it is stored in a database in the
memory. The memory collects information about recent successful interactions and manages the data for the subsequent
learning step. This allows us to optimize the set of training
examples used for learning to improve learning quality. Figure
1 shows the human-in-the-loop learning system considered
in this article, where the behavior of the human influences the
behavior of the robot and, simultaneously, the behavior of the
robot influences the behavior of the human. Furthermore, the
behavior of the robot changes as learning progresses, which in
turn influences the behavior of the human and its physical
support. This system demonstrates one of the applications of a
tightly coupled physical interaction.
After a number of interactions, the learning system
queries the memory for a new set of training data. The data
are then projected onto a low-dimensional manifold using
dimensional reduction techniques. There are three justifications for this step. First, dimensional reduction allows a
reduction of the space in which learning takes place. Thus,
the learning can be much faster and more efficient. In addition, dimensional reduction generally helps to detect meaningful low-dimensional structures in high-dimensional
inputs. Second, dimensional reduction allows us to visualize
and understand the adaptation taking place during interaction. This is particularly helpful for later review and analysis
purposes. Finally, dimensional reduction reduces the negative influence of outliers on learning. The inputs to the
dimensional reduction step are high-dimensional state vectors describing the postures of the robot during the interaction. The output is a low-dimensional posture space.
Once the state vectors are projected onto a low-dimensional manifold, we group the resulting points into sets
according to the action performed in that state. Thus, we
obtain for each possible action a set of states in which the corresponding action should be triggered. For each action, a
GMM is learned. The model encodes a probability density
function of the learned state vectors. The ideal number of
Gaussian mixtures is estimated using the Bayesian information criterion (BIC) [14].

By computing the likelihood of a given state vector p in
a GMM of action A, we can estimate how likely it is that
the robot should perform action A when in posture p. The
learned models are then used during the next physical
interaction trial to determine the actions of the robot.
Here, each new posture is projected into the low-dimensional posture space. Then, the likelihood of the projected
point for each GMM is computed. Following a maximumlikelihood rationale, the action corresponding to the
GMM with the highest likelihood is executed by the robot.
With each iteration of the above learning loop, the robot
adapts its model more and more toward successful interactions. The result is a smoother and easier cooperative behavior between the human and the robot.

of the vector denoting the
angular value of a particThe human counterpart is
ular joint. A low-level
controller is implemented
part of the learning system
by the proportional-integral-differential (PID)
and overall dynamics.
control of angular values.
Each time the desired
posture is switched drastically, large drive torques are generated, resulting in an
active force being applied to the human caregiver. As the
posture of the robot approaches the desired posture,
the passive motion gradually becomes the dominant
motion of the robot because the amount of error in the
angular control gradually becomes smaller.
Figure 3 shows how the examined standing-up task is
realized using the proposed control architecture. The
behavior is realized by switching between three desired
postures. At first glance, the specifications of the robot
motion appear to be extremely simple. However, the
switching times are highly dependent on the human
interaction. More specifically, the switching times
depend on the anatomy and skills of the human. This
means that the robot has to adapt the switching times to
the characteristics of its partner during the period of
interaction. In addition, it must be noted that this
motion cannot be performed by the robot if a human
does not assist in its execution.

The CB2 Robot
The robot used in this study is called the child–robot with
biomimetic body, or CB2 [15]. The robot has the following
features.
●● Its height is 130 cm, and its mass is approximately 33 kg.
●● The degree of freedom (DOF) is 56.
●● The supplied air pressure is 0.6 MPa.
●● The efficient torque of the knee is theoretically 28.6 N∙m.
●● All joints, apart from the joints used to move the eyes and
eyelids, are driven by pneumatic actuators.
●● 
All joints, apart from the joints used to move the fingers,
are equipped with potentiometers.
The joints have low mechanical impedance because of
the compressibility of air. The joints
can also be made to be completely
passive if the system discontinues air
Behavior System of CB2
compression during robot motion.
Switching Mechanism
This helps the robot to perform passive motions during physical interx *(i ) (i = 1, 2, . . . , n)
action and helps to ensure the safety
Current Posture
Desired Posture
of the human partner. This is in conx
d{x 1, x 2, . . ., x k}
x * d{x *1, x *2, . . ., x *k }
Control System of CB2
trast to most other robots, in which
the joints are driven by electric
d
motors with decelerators. The flexi+
D
dt
+
*
x1
Joint 1
ble actuators enable the joints to
x1
–
+
P
produce seemingly smooth motions,
even when the input signal changes
d
D
+
drastically. This feature of the CB2
dt
+
*
x2
Joint 2
x
2
robot is used to realize complex
–
+
P
motions using the simple control
.
architecture [1] depicted in Figure 2.
.
.
More specifically, full body motions
of the robot are realized by switching
d
D
+
+
dt
between a set of successive desired
*
xk
Joint k
xk
postures. Furthermore, the flexible
–
+
P
actuators enable motions generated
by this simple control architecture to
be adaptively changed in response to
2
an applied force from the human Figure 2.  Control architecture of the CB robot. The desired posture is encoded as a
vector x* of angular values. Using a PID controller, drive torques are generated to attain
partner. Each posture is described by the desired posture. The switching mechanism changes between a set of different desired
a posture vector x, with each entry postures to achieve complex robot motions.
DECEMBER 2012

•

IEEE ROBOTICS & AUTOMATION MAGAZINE

•

27

Switching 1

Switching 2

When the Hands Are
Pulled Up by the Human

When the Legs Are
Bent More Than in
Posture 2

Posture 1

Posture 2

Posture 3

Figure 3.  The three desired postures used in the standing-up task of the experiment. The learning task is to determine the ideal
switching conditions between the desired postures. (Photo courtesy of ERATO Asada Project.)

Learning Method
In the standing-up task, the goal of learning is to determine
the ideal timing for switching actions x ) ! X ) 3 X between
different desired postures. Here, x ) is a desired posture, X )
is a set of desired postures prepared for control, and X is a
posture space that is constructed from all joint angles. This
is achieved by learning three different probabilistic lowdimensional posture models: 1) for the case in which no
switching occurs, 2) for the first switching action, and 3) for
the second switching action.
At each time step of an interaction between the human
and the robot, the realized posture and the current desired
posture of the robot are recorded. The robot posture r is a
52-dimensional vector that codes the current angular value
of each joint. After the interaction is complete, the postures are stored in a database in the memory. The database
holds the information for the last ten interactions.
Although there are several possible ways to integrate this
new data into the database, the general policy used here is
this new data overwrite old data, and successful interactions overwrite failed interactions.
After ten interactions, the training data from the
memory are used for learning. The goal of the learning
is to construct a model that indicates when the robot
should switch actions by changing the current desired
posture. This rule is described by a mapping from the
current posture of the robot to the desired posture that
the robot should use. To realize this map, we use a
GMM that can construct a probabilistic model. Therefore, the objective model of the learning is a probabilistic model that indicates the likelihood of desired
postures in the current state.
First, dimensional reduction is applied to the data
because a 52-dimensional vector has too many dimensions to learn the model. Although a number of methods
can be applied for this task, in this article, we used a
28

•

IEEE ROBOTICS & AUTOMATION MAGAZINE

•

DECEMBER 2012

principal component analysis (PCA). To perform the
PCA, the mean rm is subtracted from all recorded posture
vectors, and the covariance matrix M of the resulting
points is computed. A singular value decomposition
(SVD) on M yields matrices U, V, and W, such that
M = UWV T. 	(1)

	

The columns of matrix V contain orthonormal vectors,
also known as the eigenvectors or principal components
(PCs), of matrix M. The matrix W is a diagonal matrix
containing singular values. Each PC has a corresponding
singular value that indicates how much information of
the data set is covered by a specific PC. The first few PCs
are then used as the axes of the lower-dimensional PCA
space. Given a new data point, we can compute its coordinates in PCA space by subtracting the mean and calculating the dot product for each of the PCs.
Next, we compute a GMM for each of the three switching classes. Here, we divide the projected data points into
distinct sets. If no switching occurred, then the
corresponding point is assigned to the first data set. Otherwise, the corresponding point is assigned to one of the
other two sets. For each set of projected points, we learn a
probability density function by a weighted sum of
K Gaussian distributions:
p^x h =

	

K

/ r k p^x kh, 	(2)

k =1

with r k being the weight of the kth Gaussian and p ^ x kh
being the conditional density function. The conditional density function is a d-dimensional Gaussian distribution:
	

p ^x kh =

2r

d

1^
T -1
h
1
e - 2 (x -n k) C k (x -n k) , 	(3)
det (C k)

	

x )next = argmax p s (x) . 	(4)
x) ! X)

In each step of the control loop, the robot calculates snext
and sends the angular values of the corresponding desired
posture to a low-level controller. The controller then computes the needed joint torques to take on this posture. After
the interaction is complete, the human evaluation information is collected and used to update the memory. The learning loop is then repeated. The above algorithm is closely
related to HMMs [17]. At the same time, however, our algorithm deviates in various ways from HMM. More specifically, we do not learn the sequencing of states in our system.
As a result, no explicit transition probabilities between the
states are modeled.
Figure 4 shows an example of a set of interactions
projected onto a low-dimensional space. Each point in the
plot represents one posture of the CB2 robot during an
interaction. The points were colored according to the
desired posture that was active during that particular
time step.
Experiment and Results
To investigate tightly coupled adaptation and the learning
scheme proposed in this article, we conducted a PHRI
experiment using the interaction for the standing-up task
introduced earlier. In particular, we considered the following question: “Does the learning algorithm lead to a
symmetric learning process, in which both human and
robot adapt their behaviors?” Furthermore, we wanted to
measure the contribution of the learning algorithm to any
improvement in the interaction. This required a careful

Second Principal Component

with mean n k and covariance matrix C k . The above
p ^ x kh can also be written as N ^ x n k, C kh . The expectation-maximization (EM) [16] algorithm is used to estimate
the parameters " n k, C k, r k , for each of the Gaussian
kernels. Fortunately, performing the EM algorithm in lowdimensional spaces improves the convergence of the
algorithm.
After the learning process, we end up with three GMMs
coding three probability density functions, namely, p1(x),
p2(x), and p3(x). In our experiments, each GMM had between
five and ten Gaussians. Each probability density function can
be used to determine the probability of a point in a lowdimensional posture space with respect to a particular switching action. For example, computing p2(r) for a given
projected robot posture, r, returns the likelihood of the robot
having to switch from the second to the third desired posture
when the robot is in state r.
When the next interaction with the human starts, the
robot can use the newly learned model to decide its current
state and the desired posture. Here, the current joint values
are projected onto the learned low-dimensional posture
space. The result is a d-dimensional point. The optimal
desired subsequent switching action can be computed in a
maximum-likelihood fashion as follows:

Initial Desired Posture
Desired Posture 1
Desired Posture 2

First Principal Component
Figure 4.  Interaction data for the standing-up task projected into
a low-dimensional posture space. Each point corresponds to one
posture of the robot.

experiment design that would allow us to distinguish
between learning-based adaptation and adaptation due to
human habituation to the robot.
The experiment was split into three independent parts.
Throughout the experiment, five subjects were asked to
repeatedly assist the robot in standing up. In the first part,
after every ten trials, the accumulated data in the memory
were used for learning a new model, according to the learning scheme described in the “Physical Interaction Learning
Approach: Learning Method” section. In total, 30 interactions with two intermediate learning steps were performed.
In the second part of the experiment, learning by the robot
was disabled and fixed time steps were used for switching
between the postures. In this baseline scenario, the only type
of adaptation that was possible was the adaptation of the
human to the robot. In the third and final part, learning was
once again enabled (the results of the first part were not
included; hence, learning started from the beginning again).
The experimental design ensures that we have baseline data,
allowing us to compare the results of the interactions with
and without learning. In addition, by performing the baseline experiment between the learning experiments, we
ensure that the user is already familiar with the robot. Thus,
we rule out any distortion of the baseline result because of
unfamiliarity.
To determine the ideal number of PCs on which to project
the 52-dimensional posture vector of the robot, intrinsic
dimensionality estimation techniques can be used [18] as a
criterion. A simple estimation technique is based on the analysis of eigenvalues, which store the amount of information
that is captured by each of the PCs. Hence, the eigenvalues
determine how many PCs are needed to retain a specific percentage of information found in the data set. In our implementation, we automatically determine the number of PCs
that capture more than 85% of the information in the data set.
For our standing-up data set, this resulted in a projection onto
two PCs.
DECEMBER 2012

•

IEEE ROBOTICS & AUTOMATION MAGAZINE

•

29

(a)

(b)
Figure 5.  Sequential photographs of the (a) first and (b) last interactions of the test subjects with the robot. The white curve depicts
the change in position of the robot’s hips. The center photograph of each sequence shows how the robot learns to maintain firm
contact between its feet and the ground for both subjects. (Photos courtesy of ERATO Asada Project.)

After Training

Before Training
2.0
Principal Component 2

Principal Component 2

2.0
1.5
1.0
0.5
0.0
–0.5

1.5
1.0
0.5
0.0
–0.5

–1.0 –0.5 0.0 0.5 1.0 1.5 2.0
Principal Component 1

2.5

–1.0 –0.5 0.0 0.5 1.0 1.5 2.0
Principal Component 1

3.0

(a)
Before Training

1.5

1.0
0.5
0.0
–0.5
–1.0
–1.5
–2.0
–1.0 –0.5 0.0 0.5 1.0 1.5 2.0
Principal Component 1
(c)

3.0

2.5

3.0

(b)

Principal Component 2

Principal Component 2

1.5

2.5

2.5

3.0

After Training

1.0
0.5
0.0
–0.5
–1.0
–1.5
–2.0
–1.0 –0.5 0.0 0.5 1.0 1.5 2.0
Principal Component 1
(d)

Figure 6.  Projected interactions in the low-dimensional posture space: (a) and (b) the interaction trajectories for the first subject
before and after learning and (c) and (d) the interaction trajectories for the second subject. In both the cases, the trajectories become
smoother after learning and sudden jumps and knots are reduced. Furthermore, the trajectories become V-shaped, clearly indicating a
smooth transition between the three desired postures.

30

•

IEEE ROBOTICS & AUTOMATION MAGAZINE

•

DECEMBER 2012

	

a^ t h = x^ t h -x^t -1h 2, x ! X. 	(5)

Computing the posture change norm at each time step
of the interaction results in the time series depicted in
Figure 7. The solid line shows the posture change norm

during the initial interaction phase. We can see a sudden
peak indicating a large change in the robot posture and,
consequently, a nonsmooth motion. This is
undesirable because large
In recent years, robotics
changes in the robot posture result from strong
technology significantly
forces acting on the
robot. The other lines
matured and produced
show the evolution of the
norm after each learning
highly realistic android
step. With each learning
step, the number of
robots.
peaks in the time series is
reduced. In other words,
the fluctuations in the posture change norm decrease, leading to a smoother and more efficient motion.
A statistical analysis of the data further underlines the
above hypothesis. Here, we computed the mean and standard deviation of the summation of the posture change norm
during the interactions. Figure 8 shows the evolution of these
values with each learning step. For all subjects, we see that
the mean and standard deviation of the posture change norm

Posture Change Norm

2.5
Initial
After Learning 1
After Learning 2

2.0
1.5
1.0
0.5
0.0

0

10 20 30 40 50 60 70 80 90 100
Time
(a)

2.0
Posture Change Norm

Figure 5 shows sequential photographs of the interactions
of two test subjects. Figure 5(a) shows the initial interaction,
whereas Figure 5(b) shows the interaction after learning. The
white dashed line indicates the height of the hips in each
snapshot. In the figures, we can observe a smoother transition
of the hip height after the learning interaction, when compared with that before the learning interaction. In particular,
the center photographs reveal strong contact between the feet
and the ground and an increased hip height after learning, in
contrast to the poor contact with the ungainly leg posture
beforehand. Since the degree to which the human helped the
robot in the task and the evaluation of the robot performance
are somewhat subjective, in our evaluation, we focus only on
whether the robot motion is refined to the degree that inefficient and jerky motions are avoided.
Figure 6 shows the interaction trajectories for two users
before and after learning. Each trajectory was computed by
projecting the robot postures into the low-dimensional posture space. Before learning, the trajectories contain loops
and are partially linear. These linear pieces of the trajectories
are due to jerky movements and large changes in the robot
postures. In particular, for the first user, the variance in the
trajectory decreases after learning. The trajectories become
more similar and take on a V-shaped form. This can be
explained by the fact that the interaction consists of three
desired postures. Therefore, in successful trials, the interaction leads the robot from a starting posture to an intermediate posture and then to a final posture, as shown in Figure 3.
In a low-dimensional space, the result is a V-shaped or triangular-shaped trajectory. This allows us to qualitatively
evaluate the efficiency and naturalness of the interaction by
analyzing the smoothness and shape of the low-dimensional
trajectories. For example, in the case of the second subject,
the trajectories before learning contain large loops at the
point ^1.7, - 1.5 hT , which is the low-dimensional coordinate of the second desired posture. This phenomenon can
easily be explained if we take into account our previous
analysis. In the initial trials, the robot has poor contact with
the floor and the legs are often not symmetrically arranged
when reaching the second desired posture. As a result, lifting the robot becomes more difficult for the human and
involves slight modifications of the robot posture to make
the feet more stable. This interrupts the flow of the standing-up task and increases the interaction burden for the
human caregiver.
To confirm the above discussion, we quantified the robot
motion using the posture change norm. The posture change
norm a of the robot motion was calculated using the Euclidean distance between the data of t and t - 1 in the posture
space X defined using each joint angle as a base:

Initial
After Learning 1
After Learning 2

1.5
1.0
0.5
0.0

0

10 20 30 40 50 60 70 80 90 100
Time
(b)

Figure 7.  Evolution of the posture change norm during one
learning experiment. The solid, dotted, and dashed lines show
the evolution of the value when the robot has not yet learned,
after the first intermediate learning step, and after the second
intermediate learning step, respectively. (a) Subject 1 and
(b) Subject 2.

DECEMBER 2012

•

IEEE ROBOTICS & AUTOMATION MAGAZINE

•

31

Summation of Posture Change Norms
Summation of Posture Change Norms
Summation of Posture Change Norms

20
0.0325
15

10

5

1

2

3
4
Subject Number
(a)

20

2.32e–4
1.18e–4

5.19e–4
4.96e–4

5

0.0873

15

10

5

20

1

2

3
4
Subject Number
(b)

0.0049
0.022
0.033
0.0073

15

5

3.05e–7
1.36e–5 4.63e–4
8.27e–8
0.0028
0.0021

10

5

1

2

3
4
Subject Number
(c)

5

Figure 8.  Mean and standard deviation of the summation of
the posture change norm of test subjects in the (a) baseline,
(b) first learning, and (c) final training experiments. The dark
gray, white, and light gray bars indicate the mean and standard
deviation values during each of the intermediate learning steps
(after every ten trials). In (a), the baseline experiment, only
Subject 2 shows a significant improvement after all trials. In
(b) the first learning experiment, Subjects 2, 4, and 5 show
significant improvements. In (c) the final experiment, the
interaction with the robot improved for all subjects. With each
learning trial, the indicated values decrease, and the movement
of the robot becomes smoother and more synchronized with
that of the subject.

32

•

IEEE ROBOTICS & AUTOMATION MAGAZINE

•

DECEMBER 2012

decreased as the experiment progressed. In the baseline
experiment, only one subject was able to significantly
improve the interactions, where statistical significance is
computed using a t test. None of the other subjects were able
to improve their interactions. In the first experiment, in
which the proposed learning system is used, three subjects
show significant improvement. Finally, in the second learning experiment, all of the subjects showed significant
improvement in their interactions. This indicates that while a
human can adapt to a robot and thus improve their interactions (as in the baseline experiment), this adaptation can be
significantly improved by empowering the robot with learning capabilities (first and second learning experiments). We
also analyzed the maximum values of the posture change
norm during the interaction. Figure 9 shows the change in
the maximum posture change norm during each learning
phase of the baseline experiment and the first learning experiment. No significant difference in the maximum posture
change norm is observed in the baseline experiment. On the
other hand, in the learning experiment, there are large
changes in the maximum posture change norm. For all subjects, the values drastically decrease after learning.
Still, one possible implication from above results cannot be ruled out by the experiments performed so far.
Specifically, it remains unclear how much the learning
system contributes to the improvement of interaction. A
possible argument would be that the observed improvements are due to the long-term habituation and experience with the robot. If this argument is true, then we
should see a similar improvement of interactions as above,
even if we simply repeat the baseline experiment (where
learning is disabled) three times in a row. To investigate
this question, we performed the aforementioned experiment (three times baseline) with all subjects. For the subjects, the experiment looked exactly the same as the other
experiments: the difference was not transparent.
Figure 10 compares the summation of posture change
norm between the first and the third baseline experiment.
In each of the experiments, only one subject made significant improvement during the intermediate learning
steps. On the whole, although for some subjects slight
improvement was visible (notably Subject 5), the results
are not as comprehensive as when learning is enabled.
This means that, while long-term habituation and experience aids the learning process, it is not sufficient for a
general improvement in PHRI.
Discussion
The following observations are based on the results of the
above experiments. First, when learning and adaptation were
only possible on the side of the human caregiver, generally, little or no improvement could be measured. However, even in
this asymmetric learning situation, at least one subject was able
to adapt to the robot so as to significantly improve the interaction quality. This shows the human ability to quickly adapt to
new situations and motor tasks. The second observation is that

Summation of Posture Change Norm

Maximum Posture Change Norm

3
2.5
2
1.5
1
0.5
0

1

2

3
4
Subject Number

5

20

15

0.0029

10

0

1

2

3
4
Subject Number

(a)

(a)
Summation of Posture Change Norm

Maximum Posture Change Norm

3
2.5
2
1.5
1
0.5
0

1

2

3
4
Subject Number
(b)

5

5

20

0.0313

15

10

0

1

2

3
4
Subject Number

5

(b)

Figure 9.  Change in the maximum posture change norm in
each phase during (a) the baseline and (b) the first learning
experiments. No significant difference in the maximum posture
change norm is observed in the baseline experiment. On the
other hand, there are large changes in the maximum posture
change norm in the learning experiment. For all subjects, the
values decrease drastically after learning.

Figure 10.  Baseline experiment repeated three times in a row
to investigate whether improvement can be made without the
robot’s learning system enabled. In each of the experiments
only one subject made a significant improvement. On the
whole, although for some subjects slight improvement is visible
(notably Subject 5), the results are not as comprehensive
as when learning is enabled. (a) First baseline and (b) third
baseline experiments.

the interaction quality significantly improved in the first learning experiment, and the improvement was even more remarkable during the second learning experiment. These results
support our working hypothesis that the proposed learning
system facilitates PHRI. Another interesting observation is that
the human adaptation to the robot occurred in stages throughout the experiment. At the beginning of the experiment, the
users were intimidated by the robot and the experimental
setup. However, during the course of the experiment, the test
subjects became more and more comfortable with the situation and the robot dynamics. As a result, the test subjects
found it easier to interact with the robot. This suggests that
algorithms for improving PHRI can be made more efficient if
the familiarization of the human with the robot is taken into
account. A special familiarization phase, in which the human
caregiver becomes accustomed to the robot before any cooperative tasks, might be one approach. Another method by which
to familiarize the human with the robot might be a well-

designed interaction protocol that involves tasks that are
intended only to familiarize the human with the robot. An
interesting feature of the proposed algorithm is the ability to
monitor the progress of learning as trajectories in a lowdimensional space. The results of this study indicate that the
trajectories converge toward a V-shaped pattern for the standing-up task. Furthermore, the trajectories, after learning,
appear to have particular points or bottlenecks through which
they pass. This is reminiscent of the study by Kuniyoshi et al.
[19] in which it was shown that the dynamic motions for a particular task often have a bottleneck in the state space. This bottleneck is the result of the interaction of the human body and
the environment. Kuniyoshi et al. referred to this property as
knack and showed that the knack can be exploited to efficiently control a humanoid robot. In the proposed PHRI scenario, the dynamics of the robot strongly depends on the
dynamics of the human caregiver. A knack may be said to
appear in PHRI because of the strong coupling between the
DECEMBER 2012

•

IEEE ROBOTICS & AUTOMATION MAGAZINE

•

33

Summation of Posture Change Norm

norms between each phase (a phase
consists of ten trials) in each experiment (one baseline and one learning experiment). As opposed to the
baseline experiment, we can see that
the posture change norms decrease
when learning is enabled. Note that
the baseline experiment was performed after the learning experiment to account for the human’s
habituation.
Figure 11.  The assisted walking task where a human caregiver assists the robot in his or
her attempt to perform several walking steps. (Photo courtesy of ERATO Asada Project.)
These early results show that
the proposed human-in- the-loop
human and the robot and the resulting joint dynamics. In learning system is not limited to the uprising interaction
other words, the human can be regarded as a changing envi- and that other types of interactions can be realized. At
ronment that constraints the robot dynamics. Note that, the same time, in our experiments, we found that the
although only the posture of the robot was used to create the robot often failed to keep up when the human demontrajectories, we can still discern a knack that is based on joint strator drastically increased or reduced the speed of his
dynamics. However, it can be argued that posture information or her walking gaits. This is due to the reactive nature of
is not sufficient enough to draw final conclusions about the estimating the joint dynamics from the postures only.
joint dynamics. To address this question, we are currently To keep up with a human interaction partner in this sceinvestigating a different cooperative PHRI task, namely that of nario, the robot must be more predictive in its estimation
of the joint dynamics. One possible approach to overassisted walking as can be seen in Figure 11.
come
this problem is to include sensor information into
In this scenario, the human caregiver must assist the
the
probabilistic
low-dimensional posture models. That
robot while the latter is trying to walk. Similar to the
is,
the
state
of
the
robot would be based on the current
standing-up task, the assisted walking is realized using
joint
angles
as
well
as the information gathered from the
three desired postures: left leg up, standing, and right leg
sensors
under
the
skin.
In this case, switching between
up. These postures are repeated in a predetermined order
"
"
"
one
posture
and
another
would also be influenced by the
(standing left leg up standing right leg up) to creamount
of
pressure
exerted
by the human caregiver on
ate a cyclic walking motion. During an interaction sesthe
robot’s
body,
e.g.,
the
arms
during assisted walking.
sion, the human assists the robot in performing four
Further
studies
are
underway
to obtain a conclusive
cycles of the latter sequence. For a fast assessment of the
answer
to
these
questions.
applicability of our approach to different scenarios, we
performed an experiment using the same setup and
parameters as for the standing-up task. However, in this Conclusions
case, we had only one test subject performing 30 interac- In this article, we presented a PHRI scenario in which suctions with learning enabled and 30 interactions as base- cessful task completion can only be achieved through coordiline. Figure 12 shows the comparison of posture change nated actions involving physical contact. We introduced a
simple machine learning algorithm for adapting the behavior
of the robot according to an evaluation by a human interaction partner. This method has a low computational load and
90
can be run online during the interaction with the robot and
requires relatively few training data. In contrast to previous
85
research in this field, the robot considered in this study is in
80
close physical contact with the human partner and plays an
active role during the performance of the cooperative task.
75
The CB2 robot, through its flexible-joint design and soft silicone skin, is particularly well suited to such tasks because
70
physical interactions become more natural and lifelike. In an
experiment inspired by parenting behavior in humans, we
65
were able to show that the proposed learning method results
60
in measurable improvements of interaction. Quantitative
Baseline
Learning
evaluations based on the posture change norm confirm the
significance of these improvements.
Figure 12.  Summation of posture change norm for baseline
Thus far, the control system used herein has three paramand learning experiments in the assisted walking task. Each bar
eters: the set of desired postures, the feedback gains, and the
corresponds to a phase of ten interactions with the robot.
34

•

IEEE ROBOTICS & AUTOMATION MAGAZINE

•

DECEMBER 2012

switching rule. In this article, we focused on learning the
switching rule only. However, for more complex interaction
scenarios it might be important to adapt all of these parameters. Another limitation of the proposed learning algorithm
is the use of binary evaluation information. As a result, optimization of the parameters in a gradient descent manner is
not possible. Another drawback of binary evaluation information is that only positive feedback examples are retained
for use in the learning set while negative feedback examples
are removed from the learning set. With respect to the first
limitation, the desired postures and feedback gains can be
regarded as attractors and velocities in a low-dimensional
space. Amor et al. [7] have shown that such attractors can be
efficiently learned in a low-dimensional space while incorporating kinesthetic assistance provided by the user. In the
future, we therefore hope to integrate such a method into
the proposed PHRI algorithm. As for the second limitation,
we are considering the use of pressure sensors on the body
of the robot. The amount of pressure issued by the caregiver
can then be used as an approximate evaluation information.
This allows for a finer grained reward value and, consequently, the use of modern optimization algorithms. Pressure sensors are also helpful to distinguish whether the
human is currently in contact with the robot.
In summary, this study provided interesting insights into
the dynamics of PHRIs. The combination of a softbody
robot and an efficient learning scheme is an important step
toward responsive robots that share a common living space
with humans.
References
[1] S. Ikemoto, T. Minato, and H. Ishiguro, “Analysis of physical human-robot
interaction for motor learning with physical help,” Appl. Bionics Biomech.
(Special Issue on Humanoid Robots), vol. 5, no. 4, pp. 213–223, 2008.
[2] S. Ikemoto, H. B. Amor, T. Minato, H. Ishiguro, and B. Jung, “Physical
interaction learning: Behavior adaptation in cooperative human-robot tasks
involving physical contact,” in Proc. IEEE Int. Symp. Robot and Human Interactive Communication (Ro-Man), Sept. 2009, pp. 504–509.
[3] A. De Santis, B. Siciliano, A. De Luca, and A. Bicchi, “An atlas of physical
human-robot interaction,” Mechanism Mach. Theory, vol. 43, no. 3,
pp. 253–270, Mar. 2008.
[4] O. Khatib, K. Yokoi, O. Brock, K. Chang, and A. Casal, “Robots in human
environments,” in Proc. 1st Workshop Robot Motion and Control, 1999,
pp. 213–221.
[5] S. Yohanan and K. E. MacLean, “The haptic creature project: Social
human-robot interaction through affective touch,” in Proc. AISB Symp. Reign
of Catz & Dogs: The 2nd AISB Symp. Role of Virtual Creatures in a Computerised Society, 2008, vol. 1, pp. 7–11.
[6] K. Kosuge, T. Hayashi, Y. Hirata, and R. Tobiyama, “Dance partner robot—
MS-danSer,” in Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems, 2003,
vol. 3, pp. 3459–3464.
[7] H. B. Amor, E. Berger, D. Vogt, and B. Jung, “Kinesthetic bootstrapping:
Teaching motor skills to humanoid robots through physical interaction,” KI
2009: Advances in Artificial Intelligence (ser. Lecture Notes in Artificial Intelligence), B. Mertsching, Ed. Berlin, Germany: Springer-Verlag, 2009,
pp. 492–499.

[8] J. Tani, R. Nishimoto, J. Namikawa, and M. Ito, “Codevelopmental learning
between human and humanoid robot using dynamic neural network model,”
IEEE Trans. Syst., Man, Cybern., vol. 38, no. 1, pp. 43–59, 2008.
[9] S. Calinon and A. Billard, “What is the teacher’s role in robot programming by demonstration?—Toward benchmarks for improved learning” Interaction Studies (Special Issue on Psychological Benchmarks in Human-Robot
Interaction), vol. 8, no. 3, pp. 441–464, 2007.
[10] T. Odashima, M. Onishi, K. Tahara, K. Takagi, F. Asano, Y. Kato,
H. Nakashima, Y. Kobayashi, T. Mukai, Z. W. Luo, and S. Hosoe, “A soft
human-interactive robot ri-man,” in Proc. IEEE/RSJ Int. Conf. Intelligent
Robotics and Systems (IROS, v018), 2006, p. 1–1 (video session).
[11] P. Evrard, E. Gribovskaya, S. Calinon, A. Billard, and A. Kheddar, “Teaching physical collaborative tasks: Object-lifting case study with a humanoid,” in
Proc. IEEE-RAS Int. Conf. Humanoid Robots (Humanoids), Dec. 2009,
pp. 399–404.
[12] D. Lee, C. Ott, and Y. Nakamura, “Mimetic communication with impedance control for physical human-robot interaction,” in Proc. IEEE Int. Conf.
Robotics and Automation (ICRA), 2009, pp. 1535–1542.
[13] B. D. Argall and A. G. Billard, “A survey of tactile human-robot interactions,” Robot. Autonom. Syst., vol. 58, no. 10, pp. 1159–1176, 2010.
[14] G. Schwarz, “Estimating the dimension of a model,” Ann. Statist, vol. 6,
no. 2, pp. 461–464, 1978.
[15] T. Minato, Y. Yoshikawa, T. Noda, S. Ikemoto, H. Ishiguro, and M. Asada,
“Cb2: A child robot with biomimetic body for cognitive developmental robotics,” in Proc. IEEE-RAS/RSJ Int. Conf. Humanoid Robots (Humanoids), 2007,
pp. 557–562.
[16] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood from
incomplete data via the EM algorithm,” J. R. Statist. Soc. Series B (Methodological), vol. 39, no. 1, pp. 1–38, 1977.
[17] L. R. Rabiner, “A tutorial on hidden Markov models and selected applications in speech recognition,” Proc. IEEE, vol. 77, no. 2, pp. 257–285, Feb. 1989.
[18] K. Fukunaga and D. Olsen, “An algorithm for finding intrinsic dimensionality of data,” IEEE Trans. Comput., vol. 20, no. 2, pp. 176–183, 1971.
[19] Y. Kuniyoshi, Y. Ohmura, K. Terada, A. Nagakubo, S. Eitoku, and
T. Yamamoto, “Embodied basis of invariant features in execution and perception of whole body dynamic actions—Knacks and focuses of rolland-rise
motion,” Robot. Autonom. Syst., vol. 48, no. 4, pp. 189–201, 2004.

Shuhei Ikemoto, Department of Multimedia Engineering,
Osaka University, Japan. E-mail: ikemoto@ist.osaka-u.ac.jp.
Heni Ben Amor, Intelligent Autonomous Systems Group, Technische Universitaet Darmstadt, Germany. E-mail: amor@ias.
tu-darmstadt.de.
Takashi Minato, ATR Hiroshi Ishiguro Laboratory, Kyoto,
Japan. E-mail: minato@atr.jp.
Bernhard Jung, Virtual Reality and Multimedia Group, Technische Universität Bergakademie Freiberg, Germany. E-mail:
jung@informatik.tu-freiberg.de.
Hiroshi Ishiguro, Department of Systems Innovation, Osaka
University, Japan. E-mail: ishiguro@is.sys.es.osaka-u.ac.jp.

DECEMBER 2012

•

IEEE ROBOTICS & AUTOMATION MAGAZINE

•

35

Directing Policy Search with Interactively Taught Via-Points
Yannick Schroecker

College of Computing
Georgia Tech
Atlanta, GA 30332, USA

yschroecker3@gatech.edu

Heni Ben Amor

Interactive Robotics Lab
Arizona State University
Tempe, AZ 85282, USA

hbenamor@asu.edu

ABSTRACT

athomaz@ece.utexas.edu

be difficult due to the available input modalities. Recording the desired trajectories using teleoperation or kinesthetic
demonstration can be difficult for the user and thus often
leads to undesirable pauses, sprints or imperfections. Second, providing full trajectories as demonstrations does not
allow the user to put focus on critical segments and limits
exploration for segments that the user cannot demonstrate
as well as other parts of the trajectory. Third, demonstrations are typically only provided as an initialization of the
policy search process. Refining a learned policy and removing undesirable effects usually requires a repeated recording
of the entire demonstration and cannot be limited to specific
aspects of the task. This problem is amplified if the policy
search process has already been started as the learned policy
can usually not be combined with new demonstrations.
To address these issues, we propose a method that uses
soft via-points to initialize and interactively shape the policy
search process. Recent research [1] has shown that via-point
based representations can be efficiently obtained by a human teacher in the form of demonstrations and provide the
teacher with a more natural way of teaching the desired trajectory. This kind of demonstration can achieve smoother
trajectories by separating the act of moving the robot as a
teacher from the intended trajectory that the robot should
follow. Furthermore, it allows the teacher to focus on the
most important aspects of the trajectory. In this paper, we
introduce a method to use demonstrations in this form with
policy search methods based on Dynamic Movement Primitives. More specifically, we look at policy search methods
that can optimize DMP parameters and thereby improve
the policy by evaluating parameters sampled around the
current best estimate of the optimal policy. This class of
policy search algorithms has shown great promise and includes methods such as PoWER [14], REPS [20], policy
search based on CMA-ES [8, 22] and PI2 [24]. By combining
these two approaches we allow the teacher to demonstrate
the salient points of a trajectory in a natural way while
autonomously learning and improving the shape of the trajectory between those via-points. Our method achieves this
goal by learning a single and smooth trajectory that adheres
to the demonstrated via-points. Furthermore, we introduce
a method to modify existing policy distributions by selecting
the most likely trajectories based on the provided demonstration. Modifying the trajectory distribution in this way
allows the user to provide corrections to existing policies and
thereby shape the learning process. These corrections can
be applied to policies learned by demonstration, either continuous demonstrations or via-point based demonstrations,

Policy search has been successfully applied to robot motor
learning problems. However, for moderately complex tasks
the necessity of good heuristics or initialization still arises.
One method that has been used to alleviate this problem is
to utilize demonstrations obtained by a human teacher as
a starting point for policy search in the space of trajectories. In this paper we describe an alternative way of giving
demonstrations as soft via-points and show how they can be
used for initialization as well as for active corrections during
the learning process. With this approach, we restrict the
search space to trajectories that will be close to the taught
via-points at the taught time and thereby significantly reduce the number of samples necessary to learn a good policy.
We show with a simulated robot arm that our method can
efficiently learn to insert an object in a hole with just a minimal demonstration and evaluate our method further on a
synthetic letter reproduction task.

Keywords
Reinforcement Learning, Learning from Demonstration, Reinforcement Learning for Motor Skills, Dynamic Movement
Primitives, Keyframe Demonstrations

1.

Andrea Thomaz

Department of ECE
University of Texas at Austin
Austin, TX 78701, USA

INTRODUCTION

Robotics research in recent years has been working towards employing robots in unknown and often unstructured
environments. However, controlling a robot in these environments poses major difficulties as the robot has to adapt
its actions to the environment and needs to perform tasks
with incomplete knowledge of the domain. Reinforcement
learning and policy search methods in particular have shown
great promise for autonomous learning of motor skills as trajectories. However, due to the high dimensionality and size
of the state-action space, the required amount of samples
can be prohibitive. A prominent approach to overcome this
challenge is to use Learning from Demonstration to obtain
an initial trajectory and to ensure that the learning process
will quickly converge to the right optimum, see e.g. [15, 18,
4]. Unfortunately, this approach suffers from three major
issues: First, providing trajectories as demonstrations can
Appears in: Proceedings of the 15th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2016),
J. Thangarajah, K. Tuyls, C. Jonker, S. Marsella (eds.),
May 9–13, 2016, Singapore.
c 2016, International Foundation for Autonomous Agents and
Copyright 
Multiagent Systems (www.ifaamas.org). All rights reserved.

1052

as well as to policies learned during the autonomous learning process. Finally, we propose to use models of via-points
to handle variations of tasks that differ in parameters such
as the position and orientation of key-objects in a manipulation task. We show that this facilitates efficient contextual
policy search [13, 16] and allows to learn classes of tasks.

2.

Utilizing feedback from human teachers has been investigated in the field of interactive reinforcement learning.
Knox and Stone [12] introduce TAMER+RL, a reinforcement learning framework that utilizes a reward signal obtained by a human teacher in order to learn a regression
model that can fulfill a role similar to a Q-function. Another example is given by Griffith et al. [7] who have introduced Policy Shaping. Policy shaping is utilizing rewards
obtained by a human teacher in order to learn a separate
policy. This policy can then be combined with the policy
learned by standard reinforcement learning methods. Judah
et al. [11] propose an integrated approach which optimizes a
modified objective function based on the reward as well as
on human feedback in the form of binary labels. All three
methods utilize feedback provided by a human teacher but
are different from our approach in that the feedback takes
the shape of a reward-like signal instead of demonstrations.
One can see both, the learning from demonstration based
approach as well as the interactive reinforcement learning
approach as belonging to a generalized class of algorithms
that utilize insight obtained by a human teacher in order to
improve the policy. In this view, the interactive reinforcement learning is online and less structured whereas the classical learning from demonstration based approach is offline
and utilizes structured feedback. Our approach is structured
as well but can be used in an offline manner as well as online.
Another representation of distributions over trajectories
that can be restricted to go through specified via points
is called Probabilistic Movement Primitives and has been
proposed in [19]. Probabilistic Movement Primitives define
feed-forward trajectories directly as combination of basisfunctions and define operations on Gaussian distributions
over such trajectories. The conditioning operation defined
in this approach is similar to the operation for distributions
over DMPs introduced in section 3.2. However, while it
is likely that Probabilistic Movement Primitives could also
be used with our approach, we are focusing on Dynamic
Movement Primitives as they are more popular and better
understood.

RELATED WORK

The work presented in this paper is focusing on policy
search for robotics. For a survey of this topic, see Deisenroth et al. [6]. Specifically, we are looking at utilizing policy search methods for learning Dynamic Movement Primitives [10] that share the characteristic that they are sampling
directly from the current estimate of the optimal policy. One
example for this is the Covariance Matrix Adaption Evolution Strategy (CMA-ES) which has been used in [22] in
order to learn dynamic movement primitives based on a reward signal. In [14], Kober et al. propose Policy Learning
by Weighting Exploration with the Returns (PoWER) which
uses regression weighted by rewards in order to obtain a new
policy. Another method in this class is PI2 [24] which uses
path integrals to improve on the current policy.
In our evaluation, we use Episodic Relative Entropy Policy Search (REPS) [20] as the underlying policy search
method. Peters et al. derive a sample based approximation of the optimal distribution of DMP parameters given
trajectory- and reward samples as well as a bound on the
KL-divergence between the optimized distribution and the
distribution from which the trajectories were sampled. Using this approximation, the mean can iteratively be updated
using weighted linear regression over the sampled DMP parameters and the variance can be updated by the weighted
sample variance. The proper weights can be calculated by
solving a convex optimization problem based on the bound
on the KL-divergence and the sampled parameters and rewards. We refer to [20] for details.
In this paper, we consider an approach based on directing the policy search with demonstrations obtained by a
human teacher. The field of Learning from Demonstration(LfD) has been extensively covered in [4] and LfD methods have successfully been combined with policy search for
Dynamic Movement Primitives [15, 18]. In particular, we
are looking at recording partial demonstrations in the form
of via-points which have a long history in trajectory generation. Teaching via-points by demonstrations is also known as
keyframe demonstration and has been shown to constitute a
user-friendly and efficient way to obtain demonstrations [1].
Wada et al. extract via-points from a continuous demonstration and show that these can be used to create a trajectory
that minimizes torque change [25]. Miyamoto et al. extend
this approach and apply it to learning robot motor skills [17].
Bitzer et al. introduce an approach that combines keyframe
demonstrations with reinforcement learning by learning a
lower-dimensional manifold to simplify the state-space for a
non-episodic reinforcement learner [3]. This method differs
from our approach in that it only learns a simpler state-space
representation and does not learn a heuristic for specific trajectories. Utilizing corrections in order to change a policy
learned from demonstration has been introduced by Argall
et al. who propose to use tactile corrections [2]. While this
approach is interesting, it cannot be straight-forwardly integrated into autonomous policy search algorithms such as
the ones utilized in our approach.

3.

APPROACH

In this paper, we want to utilize a human teacher in order
to provide corrections and suggestions before and during the
learning process. Our goal is to use this information to help
the learning algorithm converge to a better solution after
seeing fewer samples. Specifically, we propose a setup where
the teacher is observing the reinforcement learning process
and can, before starting the learning process or in-between
iterations, inspect the current estimate of the best policy
and provide suggestions by physically or remotely moving
the robot. Suggestions are recorded as soft via-points y ∗
that the trajectories have to pass through at a specified time
t∗ . In the case of corrections, the time t∗ can be naturally
recorded by having the user stop the robot during an execution in order to provide the correction. This process is illustrated in Algorithm 1. In the case of initial demonstrations,
the time t∗ can be estimated manually based on domain
knowledge. While choosing the right t∗ in this case may
require some thought, we have found that simple heuristics
such as distributing via-points equally in time or choosing
t∗ to be proportional to the spatial distance of the via-point
are usually sufficient.
We base our method on episodic policy search in the space

1053

of trajectories represented as Dynamic Movement Primitives
(DMPs) [10, 21] which we describe in detail in section 3.1.
These algorithms optimize the parameters of the DMP w.r.t.
a given reward function, often by a weighted average with
weights obtained based on a transformation of the rewards.
The parameters of the DMP uniquely define a policy and
thus, we loosely refer to the parameters θ as policy and
to the distribution over parameters π(θ) as policy distribution. To incorporate demonstrations and corrections into
this framework, we use the given via-points as a heuristic to
guide the learning process to the solution without directly
modifying the given objective function. To this end, we obtain a modified policy distribution and sample only trajectories that are close to the demonstrated via-points. By modifying the policy directly, the effects of demonstrations and
corrections are immediate and the learning process never
samples trajectories that are far from the demonstrated viapoints. This leads to faster convergence and safer samples.
As we modify the policy directly, we require policy search
methods that improve upon the given policy in independent iterations based on samples obtained directly from the
policy such as PoWER [14], REPS [20] and CMA-ES [22]
which obtain a new policy based on a weighted average of
the samples obtained from the old policy distribution as well
as PI2 [24].
In many cases it can be desirable to consider parameterized tasks as this allows us to learn variations of a task instead of optimizing for a single trajectory. For example, the
optimal trajectory in a manipulation task may depend on
the location of key objects. One way to solve tasks such as
these is to learn a linear model of the parameters µπ = Aπ Φ
for some features Φ to serve as the mean of the policy distribution [16]. To handle parameterized tasks, we generalize
our notion of via-points to models that are linear in Φ, i.e.
y ∗ = BΦ where B is either derived from domain knowledge
or learned with linear regression.

jectory separately from the shape of the trajectory. Furthermore, DMPs have the concept of a phase which is a function
of time which can be adapted by changing the time-scale.
This is meant to reduce the dependency on the time. DMPs
are defined as


ẏ
ÿ = τ 2 α β(g − y) −
+ τ 2 fw (z),
(1)
τ
ż = −τ αz z.

where τ is the time scaling parameter and αz is a parameter that shapes the phase function. α and β are parameters
that are analogous to the gains of a PD-controller and define
how the system is drawn to the goal of the trajectory which
is defined by g. fw (z) is the forcing function which determines the shape of the trajectory and is defined as a mixture
of radial basis functions with parameters K ∈ N, c, h ∈ RK
PK
.
i=1 ϕi (z)wi
z,
(3)
fw (z) = P
K
i=1 ϕi (z)


1 (z − ci )2
.
ϕi (z) = exp −
.
(4)
2
hi
Commonly, the weights wi ; 0 ≤ i < K are taken as the
parameters of the DMP that are learned by demonstration
or autonomously whereas the other parameters are given
as hyper-parameters. However, in some cases the goals are
not known. We therefore include the goal position in the
. g
parameters θ = ( w
). K then determines the dimensionality of the parameter-space. In this paper, we are utilizing Gaussian distributions over DMP parameters as policy
distributions and optimize this distribution with respect to
the reward. The mean of this distribution π will be given
as a linear function of the features of the task parameters:
π(θ) = N (θ, Aπ Φ, Σπ ).

3.2

3.1

Obtaining a Sample Distribution

Assuming that we have via-points obtained from demonstration as described above, we derive a modified policy distribution for the underlying policy search that passes close
to this via-point at the specified time t∗ :
.
πH (θ) = p(θ|t∗ , y ∗ ) ∝ p(y ∗ |t∗ , σy I, θ)p(θ|t∗ ).
(5)

Algorithm 1 Policy search with interactive demonstrations
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

(2)

Initialize π (0) (θ) ← N (θ; µπ(0) , Σπ(0) )
Obtain initial via-points y∗ , t∗ from demonstration
for y ∗ , t∗ ← y∗ , t∗ do
π (0) (θ) ← p(θ|y ∗ , t∗ , µπ(0) , Σπ(0) ) (see Eqs. 18-21)
end for
for k ← 1 to N iterations do
π (k) (θ) ← policy search iteration(π (k) )
Execute trajectory defined by mean parameters µπ(k)
while teacher stops execution do
Record stop time t∗
Let teacher move the robot, obtain correction y ∗
π (k) (θ) ← p(θ|y ∗ , t∗ , µπ(k) , Σπ(k) )
Execute mean trajectory defined by µπ(k)
end while
end for

The latter distribution over parameters θ denotes the prior
of where we assume that the human teacher would want
the samples to lie. A possible prior is the current policy
N (θ|µπ(k) , Σπ(k) ) where µπ(k) = Aπ(k) Φ. This prior is reasonable as we want our samples to still follow the current
policy where no via points are given. The modified sampling
distribution then depends on the previous policy distribution
and is given by
πH (θ) = p(θ|t∗ , y ∗ , Aπ(k) , Σπ(k) )
∝ p (y ∗ |t∗ , σy I, θ)π(θ|Aπ(k) , Σπ(k) ) .

(6)

The former distribution p(y ∗ |t∗ , σy I, θ) denotes the probability of a given DMP going through the specified via point
with a specified variance σy . This distribution is dependent
on the trajectory that the DMP is following. As DMPs are
defining accelerations as a linear differential equation, they
can be solved for y given a starting position and velocity in
order to obtain an estimate of the position at any given time.
Note that the solution will not be exact as a real robotic system always has noise and the DMP will react to that noise

Dynamic Movement Primitives

Policy search relies on optimizing the parameters of a
parametric policy representation. One such policy representation are Dynamic Movement Primitives which have been
introduced by Ijspeert et al. in [10]. DMPs are given as dynamical systems that are attracted by a goal position while
following a superimposed trajectory. As such they have the
property that they can adjust the goal position of the tra-

1054

102

Reward

101
0
−101
−102
(a)

−104

Figure 1: a) Rest position of the robot. The robot has to
find a trajectory that puts the object in the box. b) Viapoint with the key placed in front of the hole, as it was given
to the robot. All trajectories have to be close to this viapoint which gives significant aid for finding the opening in
the box.

y=

1
−τ α



,

t

y = (1 0)

ht (s)



(10)

0
τ 2 (αβg+Ψ(z(s))T w)



(11)

t

Z
gαβ

ht (s)
0

N
X
i=0

0
τ2



ds

(12)
AH
!

t

Z
wi

Ψi (z(s))ht (s)
0

0
τ2



ds

= Mt θ,
where

0

.. . .
.
.

θ1

..
.

,

(17)

Σπ(k) MtT∗

(σy I+
−1
Mt∗ Σπ(k) MtT∗
Mt∗ Σπ(k) ,


.
= ΣH MtT∗ σy−1 IB + Σ−1
A (k) .
π (k) π

(18)
(19)
(20)

(21)

Where the application of the Woodbury matrix identity in
Eq. 20 allows for numerically stable computation of ΣH .
This sampling distribution can then be used in place of the
policy in order to obtain the samples used in the next iteration of the policy search as described in Algorithm 1.

(13)

Mt = ( m0 m1 ... mN +1 ) ,
Z t

m0 =
αβht (s) τ02 ds,
0
Z t
mi;0<i<N +1 =
Ψi−1 (z(s))ht (s)

Mt ···

..
.

= Σπ(k) −

0

= (1 0)

0

πH (θ) = N (θ|AH Φ, ΣH ),


.
T −1
∗ σy IMt∗
+
M
ΣH = Σ−1
t
(k)
π


ds

4500

Now we can calculate the sampling distribution as the posterior distribution of the likelihood p(y ∗ |t∗ , σy I, θ), encoding
information about the via-point, and the prior distribution π
which consists of the previously learned policy. For Gaussian
distributions, the sampling distribution is therefore given as

(9)

This equation is linear w.r.t the parameters θ and can therefore be written as:
Z

3600

p(y ∗ |t∗ , σy I, θ) = N (y ∗ |Mt∗ θ, σy I).

(8)

ϕi (z)z
.
.
Ψi (z) = PK
j=0 ϕj (z)

1800
2700
Samples

where θj denotes the parameters of the DMP for dimension
j. Therefore for DMPs, we can write the likelihood distribution p(y ∗ |t∗ , σy I, θ) as

where
0

900

Note that this equation is for a single dimension but can
straightforwardly be extended to multiple dimensions by extending M diagonally and adding rows to θ such that
! θ0 !
Mt 0 ···

0



0

Figure 2: Rewards obtained over 20 trials during the learning process with and without via-points, plotted on a scale
that is linear from -10 to 10 and logarithmic outside of that
span. In 18 out of 20 trials, the learner with the via-point
finds the opening in the box and obtains a reward close to
zero. This results in a low standard deviation (error bars).
Without the via-point the learning process converges to a
local optimum.

as well as inaccuracies in the underlying controller. Only the
goal position is being tracked exactly. However, using this
estimate is reasonable as long as the noise of the estimate is
significantly smaller than the inaccuracies introduced by the
human teacher when recording a demonstration. Assuming
that we start from a rest position where the position y0 and
velocity ẏ0 are zero (we make this assumption for the sake
of simplicity. See the appendix for a derivation of the sampling distribution for arbitrary y0 and ẏ0 ), the dynamical
system can be solved for the position y using Duhamel’s
principle [23]:
Z t


y
0
=
ht (s) τ 2 (αβg+Ψ(z(s))T w) ds,
(7)
ẏ

. (t−s) −τ 2 αβ
ht (s) = e
.
z(t) = e−τ αz t ,

With via-point
Without via-point

−103

(b)

(14)

4.
(15)
0
τ2



ds.

EXPERIMENTS

In section 3, we showed how to derive a modified sampling distribution based on via-points with timing information that are provided by a human demonstration. In this
section, we first utilize a simulated robot arm to show that

(16)

1055

−3
−4
−5

Reward

−6
−7
−8
−9

Continuous demonstration
Via-point demonstration

−10
(a)

−11

(b)

Figure 3: a) Mean trajectory of initial policy (bottom) derived from via-points (center). The trajectory is smooth
and barely deviates from the desired trajectory (top) even
in-between via-points. b) Mean trajectory (bottom) of initial policy derived from a continuous demonstration (center).
Especially the last letter shows how the trajectory mimics
imperfections of the demonstration.

500

1000
Samples

1500

2000

Figure 4: Comparison of reward obtained within 2000 samples, averaged over 30 trials. Error bars are showing the
standard deviation. The reward function is defined as the
minimum squared distance to the goal position. Learning initialized with via-points consistently outperforms the
learning process initialized with a continuous demonstration.

such a modified sampling distribution can drastically improve the outcome of a reinforcement learner by having it
learn how to insert an object in a hole while requiring only
minimal information from the user. We then utilize a word
reproduction task in order to provide a comparison to continuous demonstrations and to exhaustively analyze the keyproperties of our algorithm.

4.1

0

provided via-point is further away from the box than the
local optima.

4.2

Letter Writing

To further investigate the properties of this algorithm we
are evaluating our approach on a letter reproduction task as
well. Variations of this task have been used in the past to
evaluate different properties of Dynamic Movement Primitives as it has many similarities with learning trajectories
for robot arms while allowing for intuitive visualization, easy
recording of demonstrations and thus good conditions for a
comparison to conventional demonstrations as well as fast
execution [10, 9]. The objective of the letter reproduction
task is to learn trajectories for both dimensions which, when
followed by a simulated pen, can accurately reproduce a sequence of letters. We are representing those trajectories as
DMPs with a 60 dimensional weight-vector for each dimension and initialize the policy by a continuous demonstration
or via-points before optimizing it using REPS. For the policy search, we defined the reward function as the number of
104
. Note that
overlapping black pixels: − #IntersectingP
ixels+1
solving this task requires the learning algorithm to learn trajectories that are far more complex than in the previous task
while utilizing a sparser reward signal. First, we will compare our algorithm to learning trajectories from continuous
demonstrations as a baseline, then we will evaluate the use
of corrections after the learning process has started and finally we will show that linear models of via-points can be
used to learn policies that can reproduce trajectories with
different rotation and scaling factors.

Object Insertion with a Robot Arm

In our first experiment, we are evaluating the influence
of a small number of demonstrated via-points on the reinforcement learning process and show that it can drastically
improve the convergence of the policy search. To this end,
we are utilizing a simulated 7 DoF robot arm and have it
learn how to insert an object into a hole without any knowledge about the environment. We provide a single via-point
(see Figure 1b) and show that this is sufficient to lead the
learning process to the right solution which cannot reliably
be found without a demonstration. To learn this task, we
are utilizing our method with REPS as the underlying policy search algorithm to optimize the distribution over trajectories. These trajectories are represented by 7 Dynamic
Movement Primitives with 6-dimensional weights leading to
a 49-dimensional action vector θ. For evaluation, the outcome of the learning process is averaged over 20 trials with
30 policy search iterations per trial and 150 samples for each
iterations. As can be seen in Figure 2, the reward curve observed by using the via-point is converging to a value close
to 0 and therefore the distance of the trajectory to the goal
position is converging to 0 as well. The learning process
initialized with a zero mean policy, on the other hand, is
converging to different values. This can easily be explained
by the local optima that arise around the box, i.e. the robot
is converging to solutions where the end-effector is pressing
against the middle of other the sides of the box in order to
get closer to the goal position. As a consequence, it stops
exploring the box and does not find the opening. Note that
the learning process always exceeds the initial reward obtained by our approach as the reinforcement learner always
finds at least the closest points outside of the box which
cannot be derived by the provided via-point alone, i.e. the

4.2.1

Comparison to Continuous Demonstrations

The first instance of the letter reproduction task compares
learning initialized with a normal distribution around a continuous demonstration to learning initialized on a fixed set
of via-points at equi-distant points in time. In this experiment we show that despite the lack of information between
via-points, our approach will converge to a more accurate
solution in fewer iterations. To obtain an initial policy from
a continuous demonstration, we use standard least squares

1056

Reward
(a)

(b)

Figure 5: a) Mean trajectory of the initial policy (bottom)
derived by using only every second via-point (center). This
trajectory omits whole letters when compared to the desired
trajectory (top). b) Trajectory after 3 iterations (bottom),
when the second half of the via-points have been added (center). The policy now shows the desired word and can be
improved in future iterations. The “r” is not fully formed
immediately after the correction due to learned behavior.

100
200
300

0

500

1000
Samples

500
800
1300

1500

2000

Figure 6: Comparison of rewards obtained with the rest
of the via-points added after different numbers of samples.
Early demonstrations are clearly better than late demonstrations but late demonstrations can still be effective.

be used to modify an already trained policy. This is especially useful in situations where the optimal trajectory is not
immediately apparent to the user. To investigate the impact
of providing via-points at later iterations we are looking at a
variation of the letter writing task where only every second
of the initial via-points are provided from the start. Figure
5a shows that this constitutes a far worse initial policy and
that we would expect large gains by providing the second
half of the via-points. As can be seen in Figure 5b as well
as in the reward curve in Figure 6, via-points provided at a
later point can indeed improve the policy significantly.
However, while giving the via-points after some number
of iterations can still cause a significant jump in reward, the
size of this jump decreases for later iterations. After some
time, the modified sample distribution will even decrease
the performance of the learning process. This effect can be
attributed to a mismatch of the chosen prior distribution,
i.e. the current policy, with the optimal prior distribution
which is the unknown policy according to which the human
teacher is sampling his via-points. As the learning process
converges to a sub optimal policy, it is impossible to sample trajectories that adhere to both, the learned policy as
well as the specified via-points. Depending on the value of
the variance parameter, the learner can then either sam-

to learn the mean and then add an initial variance of 103 for
the weights and 5 for the goals of the DMPs. We have found
these values to yield the best result in the continuous case.
For policy search initialized with via-points, we start with a
multivariate normal distribution with mean 0 and a variance
of 105 for the weights and 500 for the goal positions. Note
that the higher initial variance is necessary as conditioning
on the via-points will otherwise lead to an overly narrow
distribution with each added via-point decreasing the variance of the policy. This initial distribution is then used
as the prior distribution to obtain a modified initial policy
based on the via-points that can be seen in Figure 3a. In
Figures 3a and b, it can be seen that the initial policy derived from the via-points is very smooth whereas the initial
policy derived from a continuous demonstration mirrors the
imperfections of that demonstration. Note that these imperfections are often much larger in practice as policy search is
unnecessary in domains where given demonstrations already
solve the task perfectly. Furthermore, the figures show that
the errors introduced by missing information between the
via-points is of the same order as the errors that can be
introduced by linear regression and that the mean of the
initial policy is already describing a good trajectory which
leads to a fast learning process. Finally, Figure 4 shows
that our approach yields both, higher initial reward as well
as higher final reward and therefore better trajectories before and after the learning process, when compared to the
baseline. Note that the higher initial reward is tied to the
amount of exploration that is necessary in the beginning and
can, in many cases, be a desirable property when it comes to
safe exploration. While our approach only explores the areas
in-between the via-points, a reinforcement learner that has
been initialized with a continuous demonstration has to explore around the full trajectory. It is possible to reduce the
exploration around the continuous demonstrations; however
this would also reduce the final reward that can be obtained.

4.2.2

−3.5
−4.0
−4.5
−5.0
−5.5
−6.0
−6.5
−7.0
−7.5
−8.0

(a)

(b)

Figure 7: Example of giving via-points interactively. a)
Original policy in comparison to the desired trajectory (top)
and the via-point that has been provided as a correction
(bottom). b) Mean of the modified policy. The trajectory
goes through the via-point (top) and thereby matches the
desired trajectory more closely (bottom).

Active Corrections

One important aspect of the approach presented in this
paper is that via-points can be provided at any time and can

1057

Reward

−2.8

5.

−3.0

In this paper we introduced an approach to utilize partial demonstrations to interactively guide the policy search
process. We have shown that our approach of using soft viapoints significantly outperforms continuous demonstrations
when used to initialize the learning process. Furthermore,
our results show that our approach can be used to guide the
learning process in an interactive way, utilizing the knowledge gained from observing the robot to change specific aspects of the policy. Finally, we have shown that we can use
linear models of via-points to generalize over variations of a
task.
One key insight is that when applying our method during the learning process, the results are largely dependent on
the covariance of the already learned policy. While sampling
completely new trajectories ensures that the robot continues exploration and does not return to the original trajectory
after reaching the via-point, it also requires a prior distribution that specifies sensible trajectories. In our approach,
this distribution is given by the policy search. In the future, we plan to investigate other prior distributions based
on different models of how humans give via-points to allow
providing corrections to already converged policies. Furthermore, for each via-point the user is required to specify an
exact point in time. We plan to relax this assumption and
allow the user to specify distributions in time as the importance of preserving the timing is heavily dependent on the
task. Finally, the method presented in this paper is based on
the assumption that the policy is a Gaussian distribution.
While this can be a reasonable assumption, there are cases
where other distributions would be preferable. Multi-modal
policies, for example, can be used to learn tasks with multiple solutions [5]. In the future, we would like to explore
this avenue and extend our approach to different types of
policies.

−3.2
−3.4
−3.6
−3.8

Parameterized task
Non-parametric task

−4.0
−4.2

0

2000

4000
6000
Samples

8000

10000

Figure 8: Reward obtained by contextual REPS shows that
via-point models are an effective initialization. Rewards on
the non-parametric task are displayed for comparison. The
initial reward is identical due to the linear model and is
improved upon significantly in both cases.

ple degenerate trajectories from low-probability areas of the
current policy or ignore the given suggestion. The variance
parameter is thus a measure of safety and specified how far
the new policy can stray from the learned policy in order
to adhere to the correction. To avoid this effect, via-points
should always be given while the uncertainty in the current
policy is relatively high in comparison to the deviation of
the via-points from this policy. Note that in this experiment, the via-points are already known from the start even
if the agent doesn’t utilize them. This allowed us to analyze the effect of giving late demonstrations without having
to account for the human factor. However, in practice, late
demonstrations should be given depending on trajectories
sampled from the current policy. This way, the user can
actively shape the trajectory and guide the learning process
to the right solution. We illustrate the process of providing
via-points interactively and show the impact of a correction
in Figure 7.

4.2.3

CONCLUSION

Acknowledgments
This work was conducted as a part of the OpenLabs project
1436618 sponsored by PSA Peugeot and partially funded
under ONR grant number N000141410003.

Evaluation of Learning with Linear Models

For the third experiment, we investigated the properties
of learning a parametric generalization of the above task
with a linear via-point model. We are looking at learning
a model that can recreate words with respect to rotation
and scaling of the target image. Note that similar kinds of
parametric tasks can be found in practice, where the parameters often denote the location and orientation of an
object. We are assuming that good features are known as
this would be a necessity for obtaining a via-point model
in practice. In this case we are using the feature vector
(sx cos(θ), sy cos(θ), sx sin(θ), sy sin(θ), 1) with sx and sy representing the scaling and lie between 0.6 and 1.4 whereas θ
represents the rotation of the target image which lies between -45 and 45 degrees. The linear via-point models are
then given w.r.t. these features and are used for learning a
linear Gaussian policy using locally linear weighted regression and contextual REPS. Figure 8 shows that the initial
policy adapts to the task parameters and that it can be used
in conjunction with contextual REPS to obtain a similar reward curve for arbitrary rotations and scaling as for a single
instance of the task.

APPENDIX
A.

DERIVING πH (θ) FOR ARBITRARY INITIAL POSITIONS AND VELOCITIES

In section 3, we derived a sampling distribution under the
assumption that y0 = 0 and ẏ0 = 0. However, while y0 = 0
can be assumed w.l.o.g., ẏ0 = 0 is only true for trajectories
that start from a rest position. Here, we derive an equivalent
sampling distribution for the general case. In the general
case, the closed form for dynamic movement primitives is
given by
y
ẏ

Z

t

=
+e

e
0

t


(t−s)

0
1
−τ 2 αβ −τ α

0
1
−τ 2 αβ −τ α





0
τ 2 (αβg+Ψ(z(s))T w)



ds


y0
ẏ0



.

(22)

And therefore
y = Mt θ + c

1058

(23)

[10] A. J. A. Ijspeert, J. Nakanishi, and S. Schaal.
Learning attractor landscapes for learning motor
primitives. In Advances in Neural Information
Processing Systems (NIPS), pages 1547–1554, 2002.
[11] K. Judah, S. Roy, A. Fern, and T. G. Dietterich.
Reinforcement Learning Via Practice and Critique
Advice. AAAI, 2010.
[12] W. B. Knox and P. Stone. Reinforcement learning
from simultaneous human and MDP reward categories
and subject descriptors. In Autonomous Agents and
Multiagent Systems (AAMAS), pages 475–482, 2012.
[13] J. Kober, E. Oztop, and J. Peters. Reinforcement
learning to adjust robot movements to new situations.
In Robotics: Science and Systems (RSS), 2010.
[14] J. Kober and J. Peters. Policy search for motor
primitives in robotics. In Advances in Neural
Information Processing Systems (NIPS)., pages
849–856, 2008.
[15] P. Kormushev, S. Calinon, and D. G. Caldwell. Robot
motor skill coordination with EM-based reinforcement
learning. In Intelligent Robots and Systems (IROS),
pages 3232–3237, 2010.
[16] A. G. Kupcsik, M. P. Deisenroth, J. Peters, and
G. Neumann. Data-Efficient Generalization of Robot
Skills with Contextual Policy Search. In AAAI
Conference on Artificial Intelligence (AAAI), 2013.
[17] H. Miyamoto, S. Schaal, F. Gandolfo, H. Gomi,
Y. Koike, R. Osu, E. Nakano, Y. Wada, and
M. Kawato. A Kendama learning robot based on
bi-directional theory. Neural Networks,
9(8):1281–1302, 1996.
[18] K. Mülling, J. Kober, O. Kroemer, and J. Peters.
Learning to select and generalize striking movements
in robot table tennis. International Journal of
Robotics Research, 32(3):263–279, 2013.
[19] A. Paraschos, C. Daniel, J. Peters, and G. Neumann.
Probabilistic movement primitives. In Advances in
Neural Information Processing Systems (NIPS), pages
2616–2624, 2013.
[20] J. Peters, K. Mülling, and Y. Altun. Relative Entropy
Policy Search. In AAAI Conference on Artificial
Intelligence (AAAI), pages 1607–1612, 2010.
[21] S. Schaal. Dynamic movement primitives - a
framework for motor control in humans and humanoid
robotics. In Adaptive Motion of Animals and
Machines, pages 261–280. Springer Tokyo, 2003.
[22] F. Stulp and O. Sigaud. Path integral policy
improvement with covariance matrix adaptation. In
International Conference on Machine Learning
(ICML), pages 281–288, 2012.
[23] G. Teschl. Ordinary differential equations and
dynamical systems, volume 140. American
Mathematical Soc., 2012.
[24] E. Theodorou, J. Buchli, and S. Schaal. Learning
policy improvements with path integrals. In
International Conference on Artificial Intelligence and
Statistics (AISTATS), pages 828–835, 2010.
[25] Y. Wada, Y. Koike, E. Vatikiotis-Bateson, and
M. Kawato. A computational model for cursive
handwriting based on the minimization principle. In
Advances in Neural Information Processing Systems
(NIPS), pages 727–734, 1993.

where

t
.
c = (1 0)e

0

1
−τ 2 αβ −τ α
∗


y0
ẏ0



.

(24)

∗

The sampling distribution p(θ|t , y ) = N (θ|AH Φ, ΣH ) is
then computed with the modified likelihood distribution
p(y ∗ |t∗ , θ) = N (y ∗ |Mt∗ θ + c, σy I).

(25)

The mean of this distribution differs slightly from the distribution derived in section 3 so that


AH Φ = ΣH MtT∗ σy−1 I (BΦ − c) + Σ−1
A
(26)
(k) Φ .
(k)
π
π
Note that we can assume w.l.o.g. that Φ is of the form
Φ = ( Φ1 Φ2 ··· 1 )T . The sampling distribution is then defined by
.
(27)
ΣH = Σπ(k) − Σπ(k) MtT∗
−1

T
Mt∗ Σπ(k) ,
σy I + Mt∗ Σπ(k) Mt∗


.
A (k) .
AH = ΣH MtT∗ σy−1 I (B − ( 0 ... 0 c )) + Σ−1
π (k) π
(28)

REFERENCES
[1] B. Akgun, M. Cakmak, J. W. Yoo, and A. L. Thomaz.
Trajectories and keyframes for kinesthetic teaching: a
human-robot interaction perspective. In International
Conference on Human-Robot Interaction, pages
391–398, 2012.
[2] B. Argall, E. Sauser, and A. Billard. Policy adaptation
through tactile correction. In Annual Convention of
the Society for the Study of Artificial Intelligence and
Simulation of Behaviour (AISB), 2010.
[3] S. Bitzer, M. Howard, and S. Vijayakumar. Using
dimensionality reduction to exploit constraints in
reinforcement learning. In International Conference on
Intelligent Robots and Systems (IROS), pages
3219–3225, 2010.
[4] S. Chernova and A. L. Thomaz. Robot learning from
human teachers. Synthesis Lectures on Artificial
Intelligence and Machine Learning, 8(3):1–121, 2014.
[5] C. Daniel, G. Neumann, and J. Peters. Hierarchical
relative entropy policy search. In International
Conference on Artificial Intelligence and Statistics
(AISTATS), 2012.
[6] M. P. Deisenroth, G. Neumann, and J. Peters. A
survey on policy search for robotics. Foundations and
Trends in Robotics, 2(1-2):1–142, 2013.
[7] S. Griffith, K. Subramanian, J. Scholz, C. Isbell, and
A. L. Thomaz. Policy shaping: integrating human
feedback with reinforcement learning. In Advances in
Neural Information Processing Systems (NIPS), pages
2625–2633, 2013.
[8] N. Hansen and A. Ostermeier. Adapting arbitrary
normal mutation distributions in evolution strategies:
the covariance matrix adaptation. In International
Conference on Evolutionary Computation (ICEC),
pages 312–317, 1996.
[9] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor,
and S. Schaal. Dynamical movement primitives:
learning attractor models for motor behaviors. Neural
Computation, 25(2):328–373, 2013.

1059

2014 14th IEEE-RAS International Conference on
Humanoid Robots (Humanoids)
November 18-20, 2014. Madrid, Spain

Online Multi-Camera Registration for Bimanual Workspace Trajectories
Neil T. Dantam

Heni Ben Amor

Henrik I. Christensen

Mike Stilman

Abstract— We demonstrate that millimeter-level bimanual
manipulation accuracy can be achieved without the static
camera registration typically required for visual servoing. We
register multiple cameras online, converging in seconds, by
visually tracking features on the robot hands and filtering the
result. Then, we compute and track continuous-velocity relative
workspace trajectories for the end-effector. We demonstrate the
approach using Schunk LWA4 and SDH manipulators and Logitech C920 cameras, showing accurate relative positioning for
pen-capping and object hand-off tasks. Our filtering software
is available under a permissive license.1

I. I NTRODUCTION
Visual feedback of hand movements provides rich information that can be used to correct for errors and improve manipulation accuracy. Recent evidence suggests that humans
use visual feedback of the hand to guide reach and grasp
tasks [15]. Continuously tracking and monitoring the state of
the hand allows us to dynamically accommodate to internal
and external perturbations, e.g., muscle impairments, thereby
achieving a high degree of robustness during manipulation
tasks.
In robotics, using visual feedback depends on a kinematic registration between the camera and the manipulators.
Typically, this is viewed as a static task: registration is
computed offline and assumed to be constant. In reality,
camera registration changes during operation due to external
perturbations, wear and tear, or even human repositioning.
For example, during the recent DARPA Robotics Challenge
trials, impacts from falls resulted in camera issues which
significantly affected the robot behavior [10]. The pose registration process should be treated as a dynamic task in which
the involved parameters are continuously updated. Such an
online approach to pose registration is challenging, since it
requires the constant visibility of a calibration reference and
sufficient accuracy to perform manipulation tasks.
Bimanual manipulation requires accurate coordination of
both end-effectors. To perform smooth and accurate bimanual manipulation, we propose an online estimation and
control approach that combines (1) visual tracking of the
manipulators, (2) co-estimation of poses for cameras and
end-effectors using a special Euclidean group median and
extended Kalman filter, and (3) continuous geometric interpolation on the special Euclidean group. Our key insight is
to combine perception and control online, using the robot

Fig. 1. Bimanual Schunk LWA4/SDH capping a pen using visual feedback
from online camera registration and end-effector tracking.

body frame as a reference. This work extends the singlecamera and manipulator registration presented in [5] to multicamera and multi-manipulator estimation, and it integrates
the spherical blending approach of [6] to enable continuous
motion of the manipulator in the workspace.
II. R ELATED W ORK
Typical camera registration methods collect a set of calibration data using an external reference object, compute
the calibration, then proceed assuming the calibration is
static. OpenCV determines camera registration from point
correspondences, typically using a chessboard [12]. Pradeep,
et. al, develop a camera and arm calibration approach based
on bundle adjustment and demonstrate it on the PR2 robot
[13]. This approach requires approximately 20 minutes to
collect data and another 20 minutes for computation, a
challenge for handling changing pose online.
Visual servo control incorporates camera feedback into
robot motion control [1], [2]. The two main types of visual
servoing are image-based visual servo control (IBVS), which
operates on features in the 2D image, and position-based
visual servo control, which operates on 3D parameters. Both
of these methods assume a given camera registration. While
IBVS is locally stable with regard to pose errors, under
PBVS, even small pose errors can result in large tracking
error [1]. Compared to both IBVS and PVBS, our method
requires no initial camera registration, instead estimating
the registration online. Additionally, compared to IBVS,
we estimate the full kinematics of the camera and robot,

The authors are with the Institute for Robotics and Intelligent
Machines, Georgia Institute of Technology, Atlanta, GA 30332,
USA.
ntd@gatech.edu, hbenamor@cc.gatech.edu,

hic@cc.gatech.edu
1 software

available at http://github.com/golems/reflex

978-1-4799-7174-9/14/$31.00 ©2014 IEEE

588

and thus can directly follow workspace reference poses and
trajectories, such as [6], rather than being limited to imagespace reference points and trajectories.
Other recent work has explored online visual parameter
identification. A single-camera and single-arm version of the
approach in this paper was presented in [5]. [11] tracks a
robot arm to identify encoder offsets. This method assumes
a given camera registration, but is also tolerant to some
registration error. In contrast, our work identifies the camera
registration online. Though we do not explicitly consider
encoder offsets, our method is empirically robust to offsets
◦
of even 30 (see Sect. V). [9] considers bimanual arm and
object tracking with vision and tactile feedback. Though the
hardware and implementation differ from work presented in
this paper, we obtain similar accuracy using inexpensive webcams and without tactile sensing. [16] uses maps generated
from a Simultaneous Localization and Mapping (SLAM)
algorithm to calibrate a depth sensor. In our approach, unlike
typical environments for SLAM, the object to which we
are trying to register our camera – the manipulator – will
necessarily be in motion.

A. Asynchronous Pose Co-Estimation
Each camera image provides pose measurements for visible end-effector features. To reduce estimation latency, we
process and filter the measurements from each camera asynchronously as they arrive rather than collecting images from
all cameras at a fixed timestep.
The kinematic chain through the manipulator, feature, and
camera is defined as:
0

Swi ⊗ wiSwi0 ⊗ wiSfp = bScj ⊗ cjSfp

b

(1)

where bSwi is the encoder-measured pose of wrist i in body
0
frame, wiSwi0 is the estimated offset pose of wrist i, wiSfp is
the encoder-measured transform from wrist i to feature p on
the hand, bScj is the estimated pose registration of camera j,
and cjSfp is the visually-measured pose feature p in camera
j. For a depiction of the setup see Fig. 3.
Based on (1), we produce measurements for wrist offset
wi 0
Swi and camera registration bScj :
w
iS 0
]
wi
bg
Scj

III. E STIMATION : P OSES OF C AMERAS AND H ANDS

0

= (bSwi )−1 ⊗ bd
Scj ⊗ cjSfp ⊗ (wiSfp )−1

(2)

wi0

(3)

b

= Swi ⊗

w
iS 0
[
wi

⊗

cj

−1

Sfp ⊗ ( Sfp )

iS 0 is the wrist offset measurement from this image
where w]
wi
Scj is the camera registration measurement,
and feature, bg
d
w
iS 0 is the currently estimated wrist offset, and b
[
Scj is the
wi
currently estimated camera pose.

We estimate poses of multiple cameras and manipulators
by visually detecting the 3D pose of each manipulator. First,
we detect texture features on the end-effector and fit a
transform, providing an instantaneous estimate of camera and
hand pose. To obtain sufficient accuracy for manipulation we
then combine median and extended Kalman filtering of these
poses.
To use the robot body as a reference for camera registration, we track the 3D pose of features on the end-effector.
These 3D poses can be estimated with marker-based [14]
and model-based approaches [3]. Marker-based approaches
require attaching fiducials to known locations on the robot,
such as the fingers. Model-based tracking, on the other hand,
requires accurate polygon meshes of the tracked object. In
our implementation, we use the ALVAR library [14] for
marker-based tracking.
For computational reasons, we used the dual quaternion
representation for the special Euclidean group SE(3). Compared to matrices, the dual quaternion has lower dimensionality and is more easily normalized, both advantages for our
filtering implementation. Compared to the unit quaternion
plus translation vector representation, dual quaternions are
more convenient for algebraic manipulation because they are
chained through multiplication. For Euclidean transformations, we use the conventional coordinate notation where
the leading superscript denotes the parent frame and the
following subscript denotes the child frame, i.e., aSb gives
the origin of b relative to a. The transformation aSb followed
by bSc is given as the dual quaternion multiplication aSb ⊗
b
Sc = aSc . We represent an orientation quaternion as aqb , a
translation vector as axb , a rotational velocity as aωb , and the
combined translational and rotational velocity as aχ˙ b .

B. SE(3) Median and Extended Kalman Filter
We apply median and extended Kalman filtering in the
special Euclidean group SE(3) to the measurements for
g
iS 0 and camera registration b
Scj , similar to
wrist offset w]
wi
the approach in [5]. First, to reject outliers, we compute the
median measurement over a sliding time window. Then, we
use an extended Kalman Filter over time to compute optimal
pose estimates under a Gaussian noise assumption.
To compute the median of orientation q over the sliding window, the structure of rotations in SO(3) offers a
convenient distance metric between two orientations: the
angle between them. Using this geometric interpretation, the
median orientation q is the orientation with minimum angular
distance to all other orientations.
n
X
q = arg min
| ln(qi∗ ⊗ qj )|
(4)
qi ∈Q

j=0

The median translation v is the conventional geometric
median, the translation with minimum Euclidean distance to
all other translations:
n
X
v = arg min
|vi − vj |
(5)
vi ∈V

j=0

In the extended Kalman filter, we consider state x composed of a quaternion q, a translation vector v, and the
translational and rotational velocities, v̇ and ω:
x = (q, v) = [qx , qy , qz , qw , vx , vy , vz , ωx , ωy , ωz , v̇x , v̇y , v̇z ]
589

er
S

er

Se` (t1 )

Relative Trajectory

erS˙
e`

e` ,

e`
Sw`
b
Sw`
bS ˙
w`

w
\
iS 0 , bd
Sc j
w
i

EKF

=
=

0

Sw0 ⊗ w`Sw`

e`

=

`

b

Ser ⊗ erSe` ⊗ e`Sw`
Ser ⊗ erS˙e` ⊗ e`Sw`

Sr , Ṡr

J+



b

ẋr
ωr



Workspace Control


x − xr
− kx
− kφ (J + J − I)φ
∗
ln (q ⊗ qr )
φ̇r (joint velocity)



EKF

wiS


w0
i



bS

wiS
w0
i



bS

median



cj

median

cj

w
^
iS 0
w
i



bg
Sc j



...

wiS
w0
 ^
i

0



...



bg
Sc j

0

0

(bSwi )−1 ⊗ bScj ⊗ cjSf` ⊗ (wiSf` )−1

ROBOT

0

image

Swi ⊗ wiSw0 ⊗ wiSf` ⊗ (cjSf` )−1

b

i

cj

Sf0 . . . cjSfn

Feature Detector

^
iS 0 and camera
Block diagram of the control system. 3D feature poses cjSfp are detected from visual data. Instantaneous wrist offsets w
wi
registrations bg
Scj are computed. Then the median of these poses is taken over a sliding window and subsequently Kalman-filtered. The filtered poses are
used to track a relative left-right workspace trajectory, and the Jacobian damped-least squares gives the reference joint velocities φ̇r .
Fig. 2.

b

Sc 0

b

Swr

wr
S

0
wr

0
wr
Ser

0
wr
Sf

c0

Sf3

3

Frame Source:
Encoders
Visions
Filter

Fig. 3.

Setup for a dual-arm and dual-camera system. The kinematic frames are shown for one of the arms and cameras.

Now, we find the process Jacobian F . The translation
portion is a diagonal matrix of the translational velocity. For
the orientation portion, we find the quaternion derivative q̇
from the rotational velocity:

The measurement z is the median pose from the sliding
window:
z = (q, v) = [qx , qy , qz , qw , vx , vy , vz ]
The general EKF prediction step for time k is:
x̂k|k−1 = f (x̂k−1 )

∂f 
Fk−1 =
∂x x̂k−1|k−1
T
Pk|k−1 = Fk−1 Pk−1|k−1 Fk−1
+ Qk−1

q̇ =

(6)

1
ω⊗q
2

(10)

(7)
This quaternion multiplication can be converted into the
following matrix multiplication:

(8)

where x̂ is the estimated state, f (x) is the process model, F
is the Jacobian of f , P is the state covariance matrix, and
Q is the process noise model.
The process model integrates the translational and rotational velocity, staying in the SE(3) manifold using the dual
quaternion exponential of the twist Ω:


Ω(ω, v̇, v) = ω, v × ω + v̇ 


∆t
f (x) = exp
Ω ⊗ (q, v)
(9)
2

1
1
ω ⊗ q = Mr (q) ω
2
2

qw
qz
−qz qw
Mr (q) = 
 qy −qx
−qx −qy


−qy
qx 

qw 
−qz

(11)

Note that we omit the w column of the typical quaternion
multiplication matrix because the w element of rotational
velocity ω is zero.
590

This gives the following process 13 × 13 Jacobian F :



1
I4×4
0
0
2 ∆tMr q
 0
I3×3
0
∆tI3×3 

F =
(12)
 0
0
I3×3
0 
0
0
0
I3×3

IV. C ONTROL : C ONTINUOUS W ORKSPACE
T RAJECTORIES
To perform smooth, bimanual motion, we compute a
relative workspace trajectory between the two manipulators,
transform the relative pose and velocity of the trajectory
to the body frame, then compute joint velocities using the
Jacobian damped least squares pseudoinverse.
We compute a relative trajectory for the two endeffectors using the spherical parabolic blends described
in [6]. This provides a straight-line, constant-axis, and
continuous-velocity workspace path for the end-effector by
blending subsequent spherical linear interpolation segments.
Given a list of relative left-right waypoint poses and times,
er
Se` (t0 ), . . . , erSe` (tn ), we compute the reference left-right
pose and velocity as a function of time: erSe` (t), erχ˙ e` (t).
From the relative reference pose erSe` and velocity erχ˙ el
between the left and right end-effectors, we control the left
arm in workspace, by first converting the relative pose and
velocity to the body frame, then computing the Jacobian
damped-least-squares inverse kinematics solution.
The left-arm wrist pose bSw` follows directly from the
kinematic chain through the right arm:

Now we consider the EKF correction step. The general
form is:
ẑk = h(x̂k|k−1 )

∂h 
Hk =
∂x x̂k|k−1

(13)
(14)

yk = v(zk , ẑ)
Sk =
Hk Pk|k−1 =

Hk Pk|k−1 HkT
Sk KkT

(15)
+ Rk

(16)
(17)

x̂k|k = p(x̂k|k−1 , Kk yk )

(18)

Pk|k = (I − Kk Hk )Pk|k−1

(19)

where z is the measurement, h is the measurement model,
H is the Jacobian of h, ẑ is the estimated measurement, R
is the measurement noise model, and K is the Kalman gain,
v is a function to compute measurement residual, and p is a
function to compute the state update.
We compute the EKF residuals and state updates using
relative quaternions to remain in SE(3) without needing
additional normalization. The observation h(x) is a pose
estimate:

b

Sw` = bSer ⊗ erSe` ⊗ e`Sw`

e`

(20)

We compute the measurement residual based on the relative rotation between the measured and estimated pose:
v(z, ẑ) = (yq , yv )

b

Sw` = bSer ⊗ erSe` ⊗ e`Sw`
0
d er

˙
˙
b
b
( Se ⊗ e`Sw` )
⇒ Sw` = Ser ⊗ (erSe` ⊗ e`Sw` ) + bSer ⊗
dt !`
*0 e

e`
˙ = bSe ⊗ erSe ⊗ e`S˙
⇒ bSw
r
`
`
 w` + rS˙ e` ⊗ Sw`


yq = ln zq ⊗ ẑq∗ ⊗ q
yv = zv − ẑv

(21)

where yq is the orientation part of the residual and yv the
translation part. Note that ln zq ⊗ ẑq∗ corresponds to a
velocity in the direction of the relative transform between
the actual and expected pose measurement and that we can
consider yq as a quaternion derivative. Then, the update
function will integrate the pose portion of y, again using
the exponential of the twist. First, we find the twist corresponding to the product of the Kalman gain K and the
measurement residual y:

˙ = bSe ⊗ erS˙ e ⊗ e`Sw
⇒ bSw
r
`
`
`

7 indicates that S cancels to zero, and we assume the
where 
S
right arm and left fingers are stationary (0 = bS˙er = e`S˙w` ).
Relative motion with both arms moving could be computed
by including the nonzero derivative bS˙er in the computation.
We can use the product rule for this derivation because
dual quaternion poses are chained through multiplication.
Using the quaternion plus translation vector representation,

(22)

Then, we integrate estimated pose using the exponential
of this twist:


∆t
(x(q,v) )k|k = exp
Ω ⊗ (q, v)
(23)
2
Finally, the velocity component of innovation y is scaled
and added:
(xω,v̇ )k|k = xω,v̇ + (Ky)ω,v̇

(26)

0

(Ky)φ = (Ky)q ⊗ q ∗
Ω(Ky, v) = ((Ky)φ , v × (Ky)φ + (Ky)v )

(25)

Next, we compute the body-frame feedforward reference
velocity, aχ˙ b . Since there is only one changing frame, erSe` ,
we could find the corresponding body frame motion by
rotating the velocity. However, the typical computation is
notationally cumbersome [4, p140].2 Instead, we find an
elegant and more general solution by merely taking the
derivative of the pose:

h(x) = (q, v)
H = I7×7

0

Sw` = e`Sw`0 ⊗ w`Sw`

2 The complexity of the velocity transformation notation in [4, p140]
stems from its representation using Gibbs’s vector calculus which decouples the quaternion multiplication into separate dot and cross products.
Hamilton’s and Study’s classical quaternion and dual quaternion notation is
simpler and more elegant for this kinematic computation. A similar computation is also possible using transformation matrices and their derivatives, but
these matrices are more difficult to normalize than quaternions, increasing
numerical error.

(24)
591

Fig. 4. Manipulation error using only encoders for position feedback.
Without using visual feedback, there is a 15mm relative positioning error
between the two end-effectors.

chaining is not a multiplication, so an equivalent derivation
would be more complex.
Velocity and the dual quaternion derivative are related as
follows:

1

(27)
quaternion, q

1
dR(S)
= ω ⊗ R(S)
dt
2

dD(S)
1
dR(S)
=
ẋ ⊗ R(S) + x ⊗
dt
2
dt

Fig. 5. Testing relative positioning accuracy by aligning the end-effectors.
Incorporating visual feedback and online registration reduces manipulation
error from 15mm to ≈ 2mm.

where R(S) is the real part of S, D(S) is the dual part of
S, ω is rotational velocity, and x is translation.
Finally, we compute reference joint velocities using the
Jacobian damped least squares with a nullspace projection
to keep joints near the zero position:
 


ẋr
x − xr
+
φ̇r = J
− kx
−kφ (J + J −I)φ (28)
ωr
ln (q ⊗ qr∗ )

x
y
z
w

0.5

0

-0.5

0

1

2

3

4 5 6
time (s)

7

0.5

translation, x

where x is the actual translation, q is the actual orientation
quaternion, xr is the reference translation, qr is the reference
orientation quaternion, ω is the actual rotational velocity,
ωr is the reference rotational velocity, kx is the workspace
position error gain, kφ is the null-space projection gain, and φ
is the configuration. We then use joint-level velocity control
to track the reference joint velocities φ̇r . A block diagram
depicting the components of the control system and their
interplay can be found in Fig. 2.

We implement this approach on a pair of Schunk LWA4
manipulators with SDH end-effectors, and use a pair of
Logitech C920 webcams to track the robot and objects.
Our estimation and control software is implemented as a
distributed system using the Ach real-time communication
library [7]. The Schunk LWA4 has seven degrees of freedom and uses harmonic drives, which enable repeatable
positioning precision of ±0.15mm [8]. However, absolute
positioning accuracy is subject to encoder offset calibration
and link rigidity. In practice, we achieve ±15mm accuracy
when using only the joint encoders for feedback, as can be
seen in Fig. 4. The Logitech C920 provides a resolution of
1920x1080 at 15 frames per second. To measure ground-truth
distances, we use a ruler and meter-stick.
To test the relative positioning accuracy of our implementation, we servo the end-effectors to a reference zero relative
alignment, Fig. 5, and then measure the actual relative error
between the two end-effectors. We conduct this test using

9 10

8

9 10

x
y
z

0

-0.5

V. E XPERIMENTS

8

0

1

2

3

4 5 6
time (s)

7

Fig. 6. Relative trajectory of erSe` between left and right end-effectors
for pen-capping. The trajectory has constant acceleration, constant velocity,
and constant deceleration segments.

only encoder feedback, then with visual feedback. We also
◦
repeat the test injecting encoder error of 15 at the initial
◦
◦
shoulder joint, 30 at the shoulder, and 15 at both the
shoulder and elbow. The results of this test are summarized
in Table I.
In addition, we use this method to perform the pen-capping
task shown in Fig. 3 and the object hand off task shown in
Fig. 7. The relative trajectory of erSe` for the pen-capping
task is plotted in Fig. 6
VI. D ISCUSSION
The results of Sect. V show that this method achieves
bimanual positioning accuracy of a few millimeters without
◦
static camera registration and even with significant (30 )
error in the joint encoders.
592

TABLE I
P OSITIONING T EST R ESULTS (mm)

No Offset
◦
shoulder: 15
◦
shoulder: 30
◦
shoulder & elbow: 15

Fig. 7.

Mean
encoder
visual
16.5
2.2
155
2.8
280
1.3
240
0.95

VII. C ONCLUSION
We have presented an online method to identify multiple
camera and manipulator poses and track continuous relative
trajectories for bimanual manipulation tasks. This is useful
for the typical case where camera registration is not static
but changes due to model error, disturbances, or wear and
tear. The key point is to track both manipulators, and follow
a trajectory based on the visually estimated relative 3D
pose between the end-effectors. By combining median and
Kalman filtering, we are able to achieve millimeter-level
manipulation accuracy. We have shown in our experiments
that online registration can be used to improve positioning
accuracy during bimanual manipulation tasks where successful operation depends on relative end-effector pose.
This method uses feedback only from joint encoders and
visual tracking of the robot hand. Further improvements
could be made by including force and tactile sensing and
by visually tracking in-hand objects.

Std. Dev.
encoder
visual
0.5
0.94
0.6
0.78
0
0.95
0
1.1

An object hand-off task.

R EFERENCES

There are a variety of error sources that we address in this
system. For the kinematics, error from encoder offsets in the
arm, imprecise link lengths, and flexing of links all contribute
inaccurate kinematic pose estimates. For perception, error
from inaccurate camera intrinsics, imprecise fiducial sizes,
offsets in object models, and noise in the image all contribute to error in visual pose estimates. To achieve accurate
manipulation, we must account for these potential sources of
error.
The position of the tracked features on the robot has
an important effect on error correction. Kinematic errors
between the robot body origin and the tracked features, e.g.,
due to flex or encoder offsets, are incorporated into the
camera registration and handled through the servo loop. Error
between the observed features and the end-effector cannot be
corrected. Thus, it is better to track features as close to the
end-effector as possible. Consequently, we placed the fiducial
markers on the fingers of the SDH end-effector.
One source of error for manipulation that we do not
address is error in grasping. Because we track only the robot
hand, any error in the relative pose between the hand and
grasped object is not corrected. In reality, when grasping
an object, the object itself becomes the robot’s end-effector.
Thus, to accurately manipulate in-hand objects, it would be
better to track the objects themselves. Since a grasped object
is likely to be partially occluded, model-based tracking such
as [3], which is robust to occlusions, is a potential approach.
A crucial additional consideration in manipulation is force
and tactile sensing. Using visual feedback without force and
tactile sensing already reduces the error to a few millimeters
and allows the robot to perform tasks such as pen capping
and object hand-off. However, considering the generated
contact forces during the manipulation would further improve
performance and allow even more accurate operation, in
particular during the post-contact phase. This is a key area
for improvement in this approach.

[1] François Chaumette and Seth Hutchinson. Visual servo control, part I:
Basic approaches. Robotics and Automation Magazine, 13(4):82–90,
2006.
[2] François Chaumette and Seth Hutchinson. Visual servo control,
part II: Advanced approaches. Robotics and Automation Magazine,
14(1):109–118, 2007.
[3] Changhyun Choi and Henrik I Christensen. Robust 3d visual tracking
using particle filtering on the special euclidean group: A combined
approach of keypoint and edge features. The International Journal of
Robotics Research, 31(4):498–519, 2012.
[4] J. Craig. Introduction to Robotics: Mechanics and Control. Pearson,
3rd edition, 2005.
[5] N. Dantam, H. Ben Amor, H. Christensen, and M. Stilman. Online
camera registration for robot manipulation (presented). In International Symposium on Experimental Robotics, 2014.
[6] N. Dantam and M. Stilman. Spherical parabolic blends for robot
workspace trajectories (accepted). In International Conference on
Intelligent Robots and Systems, 2014.
[7] Neil Dantam, Daniel Lofaro, Ayonga Hereid, Paul Oh, Aaron Ames,
and Mike Stilman. Multiprocess communication and control software
for humanoid robots (accepted). Robotics and Automation Magazine,
2014.
[8] Schunk GmbH. Dextrous lightweight arm LWA 4D, technical data.
http://mobile.schunk-microsite.com/en/produkte/
produkte/dextrous-lightweight-arm-lwa-4d.html.
[9] Paul Hebert, Nicolas Hudson, Jeremy Ma, and Joel W Burdick. Dual
arm estimation for coordinated bimanual manipulation. In Intl. Conf.
on Robotics and Automation, pages 120–125. IEEE, 2013.
[10] Sungmoon Joo and Michael Grey. DRC-Hubo retrospective, January
2014. Personal Communication.
[11] Matthew Klingensmith, Thomas Galluzzo, Christopher Dellin,
Moslem Kazemi, J. Andrew (Drew) Bagnell, and Nancy Pollard.
Closed-loop servoing using real-time markerless arm tracking. In Intl.
Conf. on Robotics and Automation (Humanoids Workshop), May 2013.
[12] OpenCV API Reference. http://docs.opencv.org/master/
modules/refman.html.
[13] Vijay Pradeep, Kurt Konolige, and Eric Berger. Calibrating a multiarm multi-sensor robot: A bundle adjustment approach. In Experimental Robotics, pages 211–225. Springer, 2014.
[14] Kari Rainio and Alain Boyer. ALVAR – A Library for Virtual and
Augmented Reality User’s Manual. VTT Augmented Reality Team,
December 2013.
[15] Jeffrey A. Saunders and David C. Knill. Humans use continuous visual
feedback from the hand to control both the direction and distance of
pointing movements. Experimental Brain Research, 162(4):458–473,
2005.
[16] A. Teichman, S. Miller, and S. Thrun. Unsupervised intrinsic calibration of depth sensors via slam. In Robotics: Science and Systems
(RSS), 2013.

593

2012 12th IEEE-RAS International Conference on Humanoid Robots
Nov.29-Dec.1, 2012. Business Innovation Center Osaka, Japan

Point Cloud Completion Using Extrusions
Oliver Kroemer, Heni Ben Amor, Marco Ewerton, and Jan Peters
Intelligent Autonomous Systems
Technische Universitaet Darmstadt
Email: {oli, amor, peters}@ias.tu-darmstadt.de

Abstract—In this paper, we propose modelling objects using
extrusion-based representations, which can be used to complete
partial point clouds. These extrusion-based representations are
particularly well-suited for modelling basic household objects
that robots will often need to manipulate.
In order to efficiently complete a partial point cloud, we
first detect planar reflection symmetries. These symmetries are
then used to determine initial candidates for extruded shapes
in the point clouds. These candidate solutions are then used
to locally search for a suitable set of parameters to complete
the point cloud. The proposed method was tested on real
data of household objects and it successfully detected the
extruded shapes of the objects. By using the extrusion-based
representation, the system could accurately capture various
details of the objects’ shapes.

20
40
60
80
100
120
140
160
0

50

100

150

200

I. I NTRODUCTION
In the future, service robots working in everyday environments will need to grasp and manipulate a wide range
of different objects. Given these unstructured environments,
many of the encountered objects will be novel to the robot,
and their complete shapes will initially be unknown. This
shape information is however vital for successfully and
efficiently manipulating the objects. Hence, the robot will
need to autonomously determine the shapes of novel objects.
One approach to acquiring a suitable 3D model is to scan
the object from different perspectives by either shifting the
object or moving around the object [1]. The information from
these multiple perspectives can then be accumulated to form
a 3D model. Although this approach can acquire accurate
object models, the process of acquiring multiple images is a
time-consuming and non-trivial task.
Alternatively, the robot can attempt to predict the shape
of an object from only a single perspective. The partial
model acquired from one perspective can be completed by
detecting patterns in the observed shape and extending these
patterns into the occluded regions [2]. For example, planes
of symmetry can be detected and, subsequently, used to
complete the point cloud accordingly [3].
In this paper, we show how the partial point clouds of
objects can be completed by detecting extruded shapes.
Extruded 3D shapes are 2D shapes that have been extended
into the third dimension along a particular path, such as a line
segment (linear extrusion) or circle (rotational extrusion). For
example, a cube is a linearly extruded square, and a sphere
is a rotationally extruded circle.
Our overall approach to detecting extruded shapes is similar to the shape from symmetry framework proposed by Thrun

978-1-4673-1369-8/12/$31.00 ©2012 IEEE

Figure 1. The top left image shows a heart-shaped box. The top right
image shows the depth data collected from the heart-shaped box. The bottom
images show the object models obtained using the extrusion-based approach
to point cloud completion.

and Wegbreit [4]. The robot first searches for planes of symmetry in the partial point cloud, which are entailed by both
linear and rotational symmetries. The detected symmetries
are then used to initialize local searches for suitable extrusion
parameters. Finally, the detected extrusions are evaluated
according to a scoring system, and used to complete the point
cloud when applicable. The main contribution of this paper is
therefore the use of extrusion-based, rather than symmetrybased, representations.
Extrusion-based representations are well-suited for robots
working in everyday environments, wherein many objects are
manufactured. Not only are linear and rotational extrusions
often used to design objects, but they are also common in the
manufacturing process. As a result, many everyday objects
have basic extruded shapes. In this paper, we will be focusing
on completing the point clouds of basic objects that can be
described by single extrusions.
Although the proposed approach is similar to the shape
from symmetry method, extrusion-based representations can
complete some point clouds that the symmetry-based representations cannot. For example, the extrusion-based approach
can complete surfaces by extruding edges, as illustrated in
Fig. 1. The heart-shaped box has two curved surfaces that
were not visible in the original point cloud. However, by

680

extruding the curved edges observed along the top of the
box, the extrusion-based approach could complete these parts
of the point cloud. The symmetry-based approach relies on
projecting observed surfaces into occluded regions. Given
that all of the observed surfaces are flat, this approach cannot
complete the curved surfaces.
A similar problem can occur when completing the point
cloud of a rectangular box when only two of the six sides
are observed. The extrusion-based approach can extrude one
surface according to the other in order to complete the entire
box. The symmetry-based approach would, however, have
problems completing the third pair of opposing surfaces.
There are obviously also situations in which a symmetrical
object cannot be modeled by an extrusion-based representation. Thus, these two representations complement each other
and could be used together.
The proposed method is explained in Section II, which
also includes an overview of related work in completing
point clouds for robot applications. The applicability of
the extrusion-based approach is demonstrated in Section
III, wherein the robot successfully completes the shapes of
various household objects from a single perspective.
II. E XTRUSION - BASED P OINT-C LOUD C OMPLETION
In this section, we explain how the observed point clouds
can be completed by detecting extruded shapes in the partial
point cloud. We begin by giving a brief overview of methods
used in robotics applications to complete partial point clouds.
In Section II-B, we detail the extrusion-based representation,
which is flexible enough to model a wide range of objects
shapes. Sections II-C to II-F explain how one can search for
extrusions in a partial point cloud.
The proposed method assumes that the robot’s vision
system acquires structured point clouds; i.e., each point cloud
corresponds to a pixel in a 2D grid. This form of data can be
acquired from dense stereo, time-of-flight camera, Kinect, or
other active stereo cameras.
A. Point Cloud Completion in Robotics
Determining the shape of an object from a single view is
a common problem in robotics. One approach to solving this
problem is to provide the robot with a library of previously
scanned models, which it can then fit into the observed scene
[5, 6, 7, 8]. This approach allows the robot to accurately
reconstruct the scene. However, it also relies on the robot
having a model of the object, and requires searching through
a large library of known objects. In the field of computer
vision, Pauly et al. [9] presented a method for completing
the shape of objects using the models of objects with similar
shapes. Hence, a smaller library of objects could be used, as
the objects generalize to novel objects.
Another approach is to fit primitive shapes, such as cubes
and cylinders or superquadrics, to the partial view [10].
Primitive shapes cannot only be used to represent simple
objects, but also parts of more complex objects [11]. The
primitive shapes are generally parameterized such that they



	
















Figure 2. The figure illustrates how the initial extrusion hypotheses are
generated. The top row corresponds to rotational extrusions, and the bottom
row demonstrates linear extrusions. (A) The 3D objects of a tube and a box
are shown. (B) A top view of the objects, as well as the detected plane of
symmetry indicated by the dashed line. (C) The point clouds are divided
into two regions, indicated by red and green. The arrows in the top image
indicate the observed surface normals for these regions, which are used to
divide the points. (D) The ICP algorithm is used to slide one region into
the other. The rigid body transformation found by the ICP algorithm is then
used to compute the extrusion parameters.

can be adapted to model a range of similar object parts. The
ability to adapt the primitives in this manner is important, as
the additional flexibility allows the model to capture more
details of the object.
Point cloud completion can also be performed by predicting the full shape of an object from symmetry [4, 3].
As already mentioned, this approach is the most similar
to the one presented in this paper. Thrun and Wegbreit
proposed a hierarchy of symmetries that can be detected
from a partial view and, subsequently, used to complete the
point cloud [4]. They begin by performing a grid search over
the entire object for valid local symmetries, followed by a
local optimization of the symmetry parameters using the hill
climbing algorithm. The resulting candidate symmetries are
evaluated using a scoring system based on the probability of
observing the completed point cloud. The symmetry-based
approach to point cloud completion was extended to robot
manipulation by Bohg et al. [3].
B. Extrusion-based Object Representations
The goal of the work presented in this paper is to detect
extruded shapes of objects from a single perspective. In this
manner, the robot can attempt to complete the shapes of the
objects in occluded region, such that the model can be used
for manipulating the object.
An extruded shape consists of two components: the profile
and the path. The profile is the basic 2D shape that the 3D
extruded shape is based on. The path is the line indicating
how the profile is extended into the third dimension. In this
paper, we focus on paths defined by straight line segments
(linear extrusions) and circles (rotational extrusions). This
family of shapes allows us to represent a wide range of
different primitive shapes, including spheres, rectangular
prisms, cylinders, and cones.

681

However, the robot will also encounter more complex
extruded shapes. Hence, we need the representation to be
flexible enough to capture these shapes in detail. We achieve
this goal by representing the profile of the object as a 2D
point cloud. This low-dimensional point cloud can be used
to represent a wide range of shapes. The profile can, thus,
also be directly obtained from the observed 3D point cloud.
The path of a linear extrusion is defined by a 3D coordinate
system and the length of the extrusion. The direction of the
extrusion is always in the coordinate frame’s z-direction, and
the 2D profile defines the shape in the x-y plane. For a
rotational extrusion, we define the axis of rotation as a 3D
line. The 2D profile points define the location along the axis
of rotation, as well as the radial distance from this axis.
C. Detecting Planes of Symmetry
One important characteristic of both linear and rotational
extrusions is that they result in symmetric shapes; i.e. the extrusions entail a mirror symmetry. Therefore, to find extruded
shapes in the point cloud, we begin by first searching for
planar reflection symmetries. Instead of using a grid search
to detect these symmetries [4, 3], we adopt the fast votingbased approach proposed by Mitra et al. [12].
First, the normal vector and curvature are computed for
each point in the point cloud. The normal vector can be
obtained by computing the eigenvectors for a local neighborhood of points. The normal direction is given by the eigenvector with the smallest eigenvalue, which points towards the
camera. The eigenvectors can then be computed for the local
neighborhood of normal directions. The resulting two largest
eigenvalues are used to approximate the local curvature.
Subsequently, each point is compared with every other
point in the cloud in order to find pair-wise symmetries. The
plane of symmetry for two points xa and xb is located at
the midpoint 0.5(xa + xb ), with a normal aligned with the
direction xb − xa . However, this pair-wise symmetry is only
considered valid if the points’ normals are also reflected in
this plane, and the difference in curvature values between the
two points is below a given threshold.
The parameters of each valid plane of symmetry are treated
as one vote. The goal is therefore to find parameter settings
with many votes, which correspond to large symmetric
regions in the point clouds. In order to find these planes of
symmetries, the distribution of symmetry plane parameters
is modelled as a kernel density estimate [12]. All of the
modes of this distribution can then be found using mean-shift
clustering [13]. The corresponding symmetry parameters
form the basis for local searches for extruded shapes.
D. Computing Initial Extrusion Parameters using ICP
Given a plane of symmetry, a set of extrusion parameters
needs to be computed. The steps used to perform this
computation are outlined in Fig. 2.
We begin by dividing the point cloud into two parts
according to the plane of symmetry. To detect extrusions,
we divide the points according to which side of the plane of

symmetry the point lies on. To detect rotations, we separate
the points according to their normals n and the symmetry
plane’s normal p. If the inner product pT n is greater than
zero, the point is assigned to the first region and otherwise
it is assigned to the second region. The dividing of the point
cloud into two regions is illustrated in Fig. 2C.
Once the point cloud has been divided into two parts,
we need to find a rigid-body transformation that shifts one
of the parts to the pose of the other. We use the iterative
closest point (ICP) algorithm to compute this transformation
[14]. The computed transformation should have the same
effect as sliding the part along the extrusion’s path. Hence, a
linear extrusion should result in a translation, and a rotational
extrusion should result in a rotation about an axis (see Fig.
2D). Given the transformation computed using ICP, we can
compute the direction of the path for the linear extrusions,
and the axis of rotation for rotational extrusions.
It should be noted that the symmetry-detection algorithm
proposed by Mitra et al. [12] also uses ICP to detect symmetries more accurately. However, their approach searches
for symmetries, and then uses ICP to find the same type
of symmetry more accurately. Instead, our approach first
detects planar reflection symmetries, and then uses ICP to
find extrusions, which are a different type of pattern. In this
manner, we exploit the self-similarity property of extruded
shapes.
E. Local Search
Given an initial hypothesis for an extrusion, the parameter
and profile can usually be further improved using a local
search. The 2D point cloud for representing the extrusion’s
profile also needs to be determined at this stage.
For linear extrusions, the profile will be a plane of points
that is orthogonal to the path of the extrusion. Hence, we
must align the direction of the path d with the normal of
the profile plane. We perform this local alignment using an
iterative procedure. We first define the set of profile points
as those points that have a normal n such that −dT n > τ ,
where 1 > τ > 0 is a threshold value. Given this set of
profile points, the direction of the path is updated as the
negative mean of the profile points’ normals. The set of
profile points can then again be updated according to the
update path direction. In order to improve the robustness
of this process, we begin with a low threshold value, e.g.,
τ = 0.5, and increase the value in each iteration up to a
maximum value, e.g. τ = 0.95. After the path direction has
been aligned with the normal of one of the object’s sides, we
need to fit a plane to this side. We first find the location of
this plane along the path direction by computing the position
of the current profile points in this direction and selecting
the mode. The points that are near to this plane are then
used to define the final set of profile points. The direction of
the extrusion path is given by the normal of this plane. The
length of the extrusion path is set according to the length
of the surfaces that are orthogonal to the direction of the
extrusion.

682

Figure 3. The columns correspond to the results for a cup, a funnel, a pot, a heart-shaped box, a roll of toilet paper, and a box respectively. The first
row shows a picture of each object. The second row shows the depth image taken of the object, which corresponds to the partial point cloud. Darker
red regions are further away than yellow regions, and blue regions have a depth of zero. The bottom two rows show the reconstructed object shape from
different angles. These reconstructions were made from one perspective of the object and the 3D meshes were not post-processed.

For rotational extrusions, we want to set the rotational
axis such that many of the profile points are mapped onto
each other. We begin by projecting the data into the 2D
profile space according to the initial hypothesis, and marking
points in dense regions as profile points. For each of the
m profile points, located at 3D points x1:m , we compute
the position along the rotation axis a1:n and the distance
from the rotation axis r1:m to the point. We also compute
the normalized direction d1:m from each point to the closest
point on the axis. For the ith data point,
Pmwe now compute
Pm the
locally weighted mean radius r̂i = j=1 wij rj / k=1 wik
where the weight wij is given by wij = exp(−(ai −aj )2 /v 2 )
and v is a length scale parameter. This radius value r̂i is an
approximation of the desired radius at this point along the
axis. Using this desired radius, we create a new 3D point
y i = xi + r̂i di . Once all y 1:n have been computed, we fit
a line to these points, which then becomes the new axis of
rotation. Note that the point y i will be closer to xi than the
axis of rotation if ri > r̂i and, thus, will draw the axis of
rotation closer to xi . Similar to the linear extrusion case, we
iterate over these steps multiple times to acquire a suitable
axis of rotation. The final profile for the rotational extrusion
can be smoothed by using local averaging over the radii
component.
F. Visibility Score
In the final stage of the extrusion detection process, we
assign each candidate extrusion a score in order to select
the one that best represents the robot’s observations. Similar
to the scoring systems used in symmetry-based approaches
[4, 3], the score is defined according to how well the

completed point cloud matches the observed scene. Our basic
scoring system is based on the depth images obtained from
the camera, such as the one shown in Fig. 3. This data
structure is similar to a z-buffer in computer graphics, and
indicates which regions in space are occluded. Using the 3D
positions of the observed points, and their pixel location in
the z-buffer, we can compute a projection matrix P between
the 3D camera space and the 2D z-buffer pixel space.
The partial point cloud is first completed according to the
extrusion parameters currently being evaluated. In order to
compare different extrusions in a fair manner, each point in
the profile should be used to generate the same number of
extruded points. The completed point cloud is then projected
into the z-buffer space using the projection matrix P . Each
projected point is assigned to the nearest pixel in the z-buffer.
A projected point is assigned a score according to its
z value in the camera coordinate frame zc and the depth
value of the assigned z-buffer pixel zp . We also define
a length scale parameter h. If the depth values are close
together kzp − zc k < 2h, then the point provides evidence
for the extrusion and, hence, is assigned a positive score of
exp(−0.5(zp − zc )2 /h2 ). If the point has a depth greater
than the z-buffer zp − zc > 2h, its location corresponds to
an occluded region and, hence, it is assigned a score of zero.
Finally, if the point has a depth that is less than the z-buffer
zp − zc < −2h, then it contradicts the observed scene and,
hence, it is assigned a negative score, e.g. −3. The score
for the entire point cloud is given by the sum of the scores
obtained by the individual points, divided by the number of
points in the profile.
The extrusion with the largest score is used to complete

683

the partial point cloud, which can then be used to create a
3D model for manipulating the object.

2
1.8

III. E XPERIMENT

1.6

Probability Density

The proposed method was implemented and applied to a
set of common household objects. The results show that the
method can detect the extruded shapes in the point clouds,
and even capture details of the objects’ shapes.
A. Setup and Results
In this experiment, we evaluated the accuracy of the
extrusion paths found by the proposed method. In particular,
we measured the errors in the radii of rotational extrusions
and the path lengths of linear extrusions.
Using a standard Kinect camera, we collected 30 partial
point clouds of common household objects. The objects were
placed individually on a table, and the table was segmented
out of the point cloud. The segmented point cloud was then
subsampled to obtain around 3000 points. The approach
presented in Section II was then used to complete each of
the partial point clouds. The desired type of extrusion was
pre-specified for each object.
The actual lengths and radii of the extrusions were also
measured manually using the point cloud. By measuring
these distances directly from the point clouds, cameraspecific calibration errors do not affect our results. The
measured distances were then compared to those found by
the point cloud completion method.
The robot found suitable extrusions for 28 of the 30
images (93% success rate). In one of the failed trials, the
robot extruded an incorrect surface. In the other failed
trial, the robot did not find a suitable axis of rotation for
describing the shape of the object. In the successful trials,
the error in the computed distances could be measured. The
distribution over these errors is shown in Fig. 4. The mode
and mean of the distribution are at −1.36 mm and −1.19
mm respectively. The standard deviation of the distribution
is 2.23 mm. Detecting the planar symmetries for each object
in the experiments took on average 19.9 seconds.
Fig. 3 shows a set of models obtained using the proposed
method. For the rotationally extruded objects, a stochastic
optimization was used to improve the alignment of the axis
of rotation and increase the visibility score. The 3D models
were generated from the completed point clouds using the
ball-pivot algorthim [15]. Some of the objects may seem
shorter than the actual object, which is a result of the table
segmentation. Standard post-processing methods, such as
smoothing, were not applied to the point clouds, in order
to display the quality of the profiles more clearly. For a real
application, we recommend post-processing the 3D model.
B. Discussion
The results show that the extrusion-based approach could
accurately complete the objects’ shapes in most of the trials.
In practice, the accuracy of the model would decrease due
to other sources of error, such as the camera calibration.

1.4
1.2
1
0.8
0.6
0.4
0.2
0
−2

−1.5

−1

−0.5

0

0.5

1

1.5

2

Extrusion Error (cm)
Figure 4. The distribution over errors in extrusion lengths and radii. The
distribution was modelled using a kernel density estimate with a Gaussian
kernel with a width of σ = 0.1cm. A negative value indicates that the
extruded shape found by the proposed method was smaller than the actual
size.

However, the computed models should still be suffficiently
accurate for performing coarse manipulations with the objects.
The results also show that the voting-based symmetry
detection method performs well even when applied to noisy
data. The detected planes of symmetry allowed the robot
to find valid extrusions in most of the objects used in this
experiment.
The computed extrusions are slightly biased towards being
too small. This may be a result of the relatively large
penalization for overestimating the size of the extrusion.
However, a slight bias is also to be expected for some objects,
such as the cup. The point cloud of the cup contains points
from both the outside and the inside of the cup. Hence, a
rotational extrusion that maps the front-outer points onto the
back-inner points would achieve a higher visibility score,
but would also result in a smaller cup radius than the actual
radius.
The models in Fig. 3 show the importance of using a
flexible profile representation. The extrusions are capable of
modelling details, such as the lip of the pot and the hole in
the middle of the toilet paper roll. The quality of the profiles
could be improved by reincorporating more points from the
original point cloud, once a valid set of extrusion parameters
has been found. Fine details, such as the texture of the cup,
are obviously lost due to noise in the data.
One shortcoming of the current method is that it can sometimes detect degenerate extrusions when a linear extrusion is
applied to a rotationally extruded object, or vice versa. For
example a rotational extrusion applied to a box may detect a
cylinder that fits into the shape of the box. We plan to address
this problem in the future in order to render the method more
robust.
The method is also able to cope with additional parts of

684

R EFERENCES

Figure 5. The top left image show the watering can. The top right image
shows the depth image taken of the watering can. Darker red regions are
further away than yellow regions, and blue regions have a depth of zero.
The bottom images show the results of applying the proposed point cloud
completion approach to the watering can’s partial point cloud. The watering
can is an example of an object that consists of multiple extruded parts.

the object that are not extruded, as shown by the handle
of the cup. In the future, we will investigate how a robot
can robustly decompose more complex objects into multiple
extruded parts, using the method proposed by Mitra et al.
[12]. An early result of this approach applied to a watering
can is shown in Fig. 5. Although the system failed to
complete the top handle, and incorrectly completed the side
handle, these initial results are promising.
Overall, the experiment has demonstrated that the proposed method could detect most of the extruded shapes and,
thus, accurately complete the point clouds.
IV. C ONCLUSION
In this paper, we investigated how point clouds of basic
objects can be represented and completed by linear and
rotational extrusions. These extrusions are represented in
a flexible manner, which allows them to accurately model
a wide range of shapes. By detecting local symmetries in
partial point clouds, we can search for extrusions in an
efficient manner, and use these extrusions to complete the
point cloud. In the experiment, the proposed method was
applied to point clouds obtained from real household objects,
and successfully completed most of the partial point clouds.
In the future, we plan to use the proposed method in order
to plan grasps on novel objects. In particular, the proposed
method allows us to compute contact points for occluded
regions of the object.
ACKNOWLEDGEMENTS
The project receives funding from the European Community’s Seventh Framework Programme under grant agreement
n° ICT- 248273 GeRT.

[1] M. Krainin, P. Henry, X. Ren, and D. Fox, “Manipulator
and object tracking for in-hand 3d object modeling.,” I.
J. Robotic Res., vol. 30, no. 11, pp. 1311–1327, 2011.
[2] T. P. Breckon and R. B. Fisher, “Amodal volume
completion: 3d visual completion,” Comput. Vis. Image
Underst., vol. 99, pp. 499–526, Sept. 2005.
[3] J. Bohg, M. Johnson-Roberson, B. León, J. Felip,
X. Gratal, N. Bergström, D. Kragic, and A. Morales,
“Mind the Gap - Robotic Grasping under Incomplete
Observation,” in ICRA 2011, May 2011.
[4] S. Thrun and B. Wegbreit, “Shape from symmetry,” in
ICCV 2005, pp. 1824–1831, IEEE, 2005.
[5] S. Savarese and F.-F. Li, “3d generic object categorization, localization and pose estimation,” in ICCV 2007,
pp. 1–8, 2007.
[6] R. Detry, N. Pugeault, and J. Piater, “A probabilistic
framework for 3D visual object representation,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 31, no. 10,
pp. 1790–1803, 2009.
[7] A. Aldoma, N. Blodow, D. Gossow, S. Gedikli,
R. Rusu, M. Vincze, and G. Bradski, “Cad-model
recognition and 6 dof pose,” in ICCV 2011, 3D Representation and Recognition (3dRR11), 11/2011 2011.
[8] G. Biegelbauer and M. Vincze, “Efficient 3d object detection by fitting superquadrics to range image data for
robot’s object manipulation,” in ICRA 2007, pp. 1086
–1091, 2007.
[9] M. Pauly, N. J. Mitra, J. Giesen, M. Gross, and
L. Guibas, “Example-based 3d scan completion,” in
Symposium on Geometry Processing, pp. 23–32, 2005.
[10] Z. C. Marton, L. C. Goron, R. B. Rusu, and M. Beetz,
“Reconstruction and Verification of 3D Object Models
for Grasping,” in ISRR 2009, (Lucerne, Switzerland),
2009.
[11] C. Goldfeder, P. K. Allen, C. Lackner, and R. Pelossof,
“Grasp planning via decomposition trees,” in ICRA’07,
pp. 4679–4684, 2007.
[12] N. J. Mitra, L. Guibas, and M. Pauly, “Partial and approximate symmetry detection for 3d geometry,” ACM
Transactions on Graphics (SIGGRAPH), vol. 25, no. 3,
pp. 560–568, 2006.
[13] D. Comaniciu and P. Meer, “Mean shift: a robust
approach toward feature space analysis,” IEEE Trans.
Pattern Analysis and Machine Intelligence, vol. 24,
pp. 603 –619, may 2002.
[14] Y. Chen and G. Medioni, “Object modeling by registration of multiple range images,” in ICRA 1991,
pp. 2724–2729, 1991.
[15] F. Bernardini, J. Mittleman, H. Rushmeier, C. Silva,
G. Taubin, and S. Member, “The ball-pivoting algorithm for surface reconstruction,” IEEE Trans. Visualization and Computer Graphics, vol. 5, pp. 349–359,
1999.

685

Probabilistic movement modeling for
intention inference in human–robot
interaction

The International Journal of
Robotics Research
32(7) 841–858
© The Author(s) 2013
Reprints and permissions:
sagepub.co.uk/journalsPermissions.nav
DOI: 10.1177/0278364913478447
ijr.sagepub.com

Zhikun Wang1,2 , Katharina Mülling1,2 , Marc Peter Deisenroth2 , Heni Ben Amor2 ,
David Vogt3 , Bernhard Schölkopf1 and Jan Peters1,2
Abstract
Intention inference can be an essential step toward efficient human–robot interaction. For this purpose, we propose
the Intention-Driven Dynamics Model (IDDM) to probabilistically model the generative process of movements that are
directed by the intention. The IDDM allows the intention to be inferred from observed movements using Bayes’ theorem.
The IDDM simultaneously finds a latent state representation of noisy and high-dimensional observations, and models
the intention-driven dynamics in the latent states. As most robotics applications are subject to real-time constraints, we
develop an efficient online algorithm that allows for real-time intention inference. Two human–robot interaction scenarios,
i.e. target prediction for robot table tennis and action recognition for interactive humanoid robots, are used to evaluate
the performance of our inference algorithm. In both intention inference tasks, the proposed algorithm achieves substantial
improvements over support vector machines and Gaussian processes.
Keywords
approximate inference, Gaussian process, intention inference

1. Introduction
Recent advances in sensors and algorithms allow for robots
with improved perception abilities. For example, robots can
now recognize human poses in real time using depth cameras (Shotton et al., 2011), which can enhance the robot’s
ability to interact with humans. However, effective perception alone may not be sufficient for human–robot interaction (HRI), since the robot’s reactions ideally depend on
the underlying intention of the human’s action, including
the others’ goal, target, desire, and plan (Simon, 1982).
Human beings rely heavily on the skill of intention inference (for example, in sports, games, and social interaction)
and can improve the ability of intent prediction by training. For example, skilled tennis players are usually trained
to possess substantially better anticipation than amateurs
(Williams et al., 2002). This observation raises the question of how robots can learn to infer the human’s underlying
intention from movements.
In this article, we focus on intention inference from a
movement based on modeling how the dynamics of a movement are governed by the intention. This idea is inspired by
the hypothesis that a human movement usually follows a
goal-directed policy (Baker et al., 2009; Friesen and Rao,
2011). The resulting dynamics model allows to estimate the
probability distribution over intentions from observations

using Bayes’ theorem and to update the belief as additional observation is obtained. The human movement considered here is represented by a time series of observations,
which makes discrete-time dynamics models a straightforward choice for movement modeling and intention inference. In a robotics scenario, we often rely on noisy and
high-dimensional sensor data. However, the intrinsic states
are typically not observable, and may have lower dimensions. Therefore, we seek a latent state representation of the
relevant information in the data, and then model how the
intention governs the dynamics in this latent state space,
as shown in Figure 1(b). The resulting model jointly learns
both the latent state representation and the dynamics in the
state space.
Designing a parametric dynamics model is difficult due
to the complexity of human movement, e.g., its unknown
nonlinear and stochastic nature. To address this issue, Gaussian processes (GPs) (see, e.g., Rasmussen and Williams,
1 Max

Planck Institute for Intelligent Systems, Tübingen, Germany
Universität Darmstadt, Darmstadt, Germany
3 Technical University Bergakademie Freiberg, Freiberg, Germany
2 Technische

Corresponding author:
Zhikun Wang, Max Planck Institute for Intelligent Systems, Spemannstraße 38, 72076 Tübingen, Germany.
Email: zhikun.wang@tuebingen.mpg.de

842

The International Journal of Robotics Research 32(7)

g
x1

x2

x3

z1

z2

z3

···

(a) GPDM

x1

x2

x3

z1

z2

z3

···

(b) IDDM

Fig. 1. Graphical models of the Gaussian Process Dynamical Model (GPDM) and the proposed Intention-Driven Dynamics Model
(IDDM), where we denote the intention by g, state by xt , and observation by zt . The proposed model explicitly incorporates the
intention as an input to the transition function (Wang et al., 2012b).

2006) have been successfully applied to modeling human
dynamics. For example, the Gaussian Process Dynamical
Model (GPDM) proposed by Wang et al. (2008) uses GPs
for modeling the generative process of human motion with
a nonlinear dynamical system, as shown in Figure 1(a).
Since the GP is a probabilistic non-parametric model, the
unknown structure of the human moment can be inferred
from data, while maintaining posterior uncertainty about
the learned model itself.
As an extension to the GPDM, we propose the IntentionDriven Dynamics Model (IDDM), which models the generative process of intention-driven movements. The dynamics
in the latent states are driven by the intention of the human
action/behavior, as shown in Figure 1(b). The IDDM can
simultaneously find a good latent state representation of
noisy and high-dimensional observations and describe the
dynamics in the latent state space. The dynamics in latent
state and the mapping from latent state to observations
are described by GP models. Using the learned generative
model, the human intention can be inferred from an ongoing movement using Bayesian inference. However, exact
intention inference is not tractable due to the nonlinear and
nonparametric GP transition model. Therefore, we propose
an efficient approximate inference algorithm to infer the
intention of a human partner.
The remainder of the article is organized as follows. First,
in the remainder of this section, we illustrate the considered
scenarios (Section 1.1) and discuss the related work (Section 1.2). Subsequently, we present the IDDM and address
the problem of its training in Section 2. In Section 3, we
study approximate algorithms for intention inference and
extend them to online inference in Section 4. We evaluate
the performance of the proposed methods in the two scenarios, i.e. target prediction in robot table tennis and action
recognition, in Sections 5 and 6. Finally, we summarize
our contributions and discuss properties of the IDDM in
Section 7.

1.1. Considered scenarios
To verify the feasibility of the proposed methods, we discuss two representative scenarios where intention inference
plays an important role in human–robot interactions:
(1) Target inference in robot table tennis. We consider
human–robot table tennis games (Mülling et al., 2011),
where the robot plays against a human opponent as shown in
Figure 2(a). The robot’s hardware constraints often impose
strong limitations on its flexibility in such a high-speed
scenario; for example, the Barrett WAM robot arm often
cannot reach incoming balls due to a lack of time caused
by acceleration and torque limits for the biomimetic robot
table tennis player described by Mülling et al. (2011). The
robot is kinematically capable of reaching a large hitting
plane with pre-defined hitting movements such as forehand,
middle, and backhand stroke movements that are capable
of returning the ball shot into their corresponding hitting
regions. However, movement initiation requires an early
decision on the type of movement. In practice, it appears
that to achieve the required velocity for returning the ball,
this decision needs to be taken at least 80 ms before the
opponent returns the ball (Wang et al., 2011b). Hence, it is
necessary to choose the hitting movement before the opponent’s racket has even touched the ball. This choice can
be made based on inference of the target location where
the opponent intends to return the ball from his incomplete
stroke movement. We show that the IDDM can improve the
prediction of the human player’s intended target over a baseline method based on GP regression, and can thus expand
the robot’s hitting region substantially by utilizing multiple
hitting movements.
(2) Action recognition for interactive humanoid robots.
In this setting, we use our IDDM to recognize the actions
of the human, as shown in Figure 2(b), which can improve
the interaction capabilities of a robot (Jenkins et al., 2007).

Wang et al.

843

(a) Robot table tennis

(b) Interactive humanoid robot

Fig. 2. Two examples of HRI scenarios where intention inference plays an important role: (a) target prediction in robot table tennis
games, and (b) action recognition for HRI.

In order to realize natural and compelling interactions, the
robot needs to correctly recognize the actions of its human
partner. In turn, this ability allows the robot to react in a
proactive manner. We show that the IDDM has the potential to identify the action from movements in a simplified
scenario.
In most robotics applications, including the scenarios
discussed above, the decision making systems are subject
to real-time constraints and need to deal with a stream
of data. Moreover, the human’s intention may vary over
time. To address these issues, we propose an algorithm for
online intention inference. The online algorithm can process the stream data and fulfill the real-time requirements.
In the experiments, the proposed online intention inference
algorithm achieved over four times acceleration over our
previous method in Wang et al. (2012b).

1.2. Related work
We review methods for intention inference and for modeling human movements that are related to the proposed
IDDM and inference methods.

1.2.1. Intention inference Inference of intentions has been
investigated in different settings. Most of previous work
relies on probabilistic reasoning.
Intention inference with discrete states and actions has
been studied extensively, using hidden Markov models
(HMMs) to model and predict human behavior where different dynamics models were adopted to the corresponding behaviors (Pentland and Liu, 1999). Online learning
of intentional motion patterns and prediction of intentions
based on HMMs was proposed by Vasquez et al. (2008),
which allows efficient inference in real time. The HMM can
be learned incrementally to cope with new motion patterns
in parallel with prediction (Vasquez et al., 2009).
Probabilistic approaches to plan recognition in artificial
intelligence (Liao et al., 2007) typically represent plans as
policies in terms of state-action pairs. When the intention is

to maximize an unknown utility function, inverse reinforcement learning (IRL) infers the underlying utility function
from an agent’s behavior (Abbeel and Ng, 2004). IRL has
also been applied to model intention-driven behavior. For
instance, maximum entropy IRL (Ziebart et al., 2008) has
been used to model goal-directed trajectories of pedestrians
(Ziebart et al., 2009) and target-driven pointing trajectories
(Ziebart et al., 2012).
In cognitive science, Bayesian models were used for
inferring goals from behavior by Rao et al. (2004), where
a policy conditional on the agent’s goal is learned to represent the behavior. Bayesian models can be used to interpret
the agent’s behavior and predict its behavior in a similar
environment with the learned model Baker et al. (2006).
In a recent work (Friesen and Rao, 2011), a computational framework was proposed to model gaze following,
where GPs are used to model the dynamics with actions
driven by a goal. These methods assume that the states
can be observed. However, in practice the states are often
not well-defined or not observable for complex human
movement.
One can also consider the intention inference jointly with
decision making, such as autonomous driving (Bandyopadhyay et al., 2012), control (Hauser, 2012), or navigation in
human crowds (Kuderer et al., 2012). For example, when
the state space is finite, the problem can be formulated as a
partially observable Markov decision process (Kurniawati
et al., 2011) and solved efficiently (Wang et al., 2012a).
In contrast, our method assumes that the robot’s decision
does not influence the intention of the human and considers intention inference and decision making separately,
which allows us to efficiently deal with high-dimensional
data stream and fulfill the real-time constraints.
1.2.2. GPDM and extensions Observations of human
movements often consist of high-dimensional features.
Determining a low-dimensional latent state space is
an important issue for understanding observed actions.
The Gaussian Process Latent Variable Model (GPLVM)
(Lawrence, 2004) finds the most likely latent variables

844

while marginalizing out the function mapping from latent
to observed space. The resulting latent variable representation allows to model the dynamics in a low-dimensional
space. For example, the GPDM (Wang et al., 2008) uses an
additional GP transition model for the dynamics of human
motion on the latent state space.
In robotics applications, the GPLVM can also be used
for learning dynamical system motor primitives (Ijspeert
et al., 2002) in a low-dimensional latent space, to achieve
robust dynamics and fast learning (Bitzer and Vijayakumar,
2009). Non-parametric dynamics models are also applied
for tracking a small robotic blimp with two cameras (Ko
and Fox, 2009), where GP-Bayes filters were proposed for
efficient filtering. In a follow-up work (Ko and Fox, 2011),
the model is learned based on the GPLVM, so that the latent
states need not be provided for learning.
The use of a GP transition model renders exact inference in the GPDM and, hence, in the IDDM, analytically
intractable. Nevertheless, approximate inference methods
have been successfully applied based on filtering and
smoothing in nonlinear dynamical systems. For the GPDM
and its extensions, approximate inference can be achieved
using particle filters (GP-PFs), extended Kalman filters
(GP-EKFs), and unscented Kalman filters (GP-UKFs) as
proposed by Ko and Fox (2009). GP assumed density filters
(GP-ADFs) for efficient GP filtering, and general smoothing in GPDMs were proposed by Deisenroth et al. (2009)
and Deisenroth et al. (2012), respectively. These filtering
and smoothing techniques allow the use of the expectation–
maximization (EM) framework for approximate inference
(Ghahramani and Roweis, 1999; Turner et al., 2010; Wang
et al., 2012b).

2. Intention-Driven Dynamics Model
We propose the IDDM, which is an extension of the
GPDM (Wang et al., 2008). The GPDM is a non-parametric
approach to learning the transition function in the latent
state space and the measurement mapping from states to
observations simultaneously. As shown in Figure 1(a), the
transition function in the GPDM is only determined by the
latent state. However, in the applications considered in this
paper, the underlying intention, as an important drive of
human movements, can hardly be discovered directly from
the observations. Considering that the dynamics can be
substantially different when the actions are based on different intentions, we propose the IDDM. As shown in Figure
1(b), the IDDM explicitly incorporates the intention into the
transition function in the latent state space. This dynamics model was inspired by the hypothesis that the human
action is directed by the goal (Baker et al., 2009; Friesen and
Rao, 2011). For example, in table tennis, the player swings
the racket in order to return the ball to an intended target.
The target is, hence, a driving factor in the dynamics of the
racket.

The International Journal of Robotics Research 32(7)

We present the proposed model and address the problem
of its training in this section. Later, in Section 3, we study
approximate algorithms for intention inference, and extend
it for online inference in Section 4.

2.1. Measurement and transition models
In the proposed IDDM, one set of GPs models the transition function in the latent space conditioned on the intention
g. A second set of GPs models the measurement mapping from the latent states x and the observations z. For
notational simplicity, we assume the intention variable g is
discrete or a scalar. The model and method can easily generalize to multi-variate intention variables. We detail both
the measurement and transition models in the following.
This article extensively uses properties of the GPs, e.g.
predictive distribution and marginal likelihood. We refer
to Rasmussen and Williams (2006) for a comprehensive
introduction to GPs.
2.1.1. Measurement model The observations of a movement are a time series z1:T  [z1 , . . . , zT ], where zt ∈ RDz .
In the proposed generative model, we assume that an observation zt ∈ RDz is generated by a latent state variable
xt ∈ RDx according to
zt = Wh(xt ) +Wnz,t ,

nz,t ∼ N ( 0, Sz ) ,

(1)

where the diagonal matrix W = diag( w1 , . . . , wDz ) scales
the outputs of h( xt ). The scaling parameters W allow for
dealing with raw features that are measured in different
units, such as positions and velocities. We place a GP prior
distribution on each dimension of the unknown function
h, which is marginalized out during learning and inference. The GP prior GP( mz ( ·) , kz ( ·, ·) ) is fully specified by
a mean function mz ( ·) and a positive semidefinite covariance (kernel) function kz ( ·, ·). Without specific prior knowledge on the latent state space, we use the same mean and
covariance function for the GP prior on every dimension of
the unknown measurement function h, and use the noise
(co)variance Sz = s2z I. The predictive probability of the
observations zt is given by a Gaussian distribution zt ∼
N ( mz ( xt ) , z ( xt ) ) , where the predictive mean and covariance are computed based on training inputs Xz and outputs
Yz , given by
mz ( xt ) = Yz K−1
z kz ( xt ) ,
 z ( xt ) =
σz2 ( xt )

(2)

σz2 ( xt ) I,

= kz ( xt , xt ) −kz ( xt )

(3)
T

K−1
z k z ( xt ) ,

(4)

where we use the shorthand notation kz ( xt ) to represent the
cross-covariance vector between h( Xz ) and h( xt ), and use
Kz to represent the kernel matrix of Xz .
2.1.2. Transition model We consider first-order Markov
transition model, see Figure 1(b), with a latent transition

Wang et al.

845

function f, such that
xt+1 = f(xt , g) +nx,t ,

nx,t ∼ N ( 0, Sx ) .

factors W in Equation (1). In the context of target prediction
in table tennis games, we use the linear kernel
(5)

The state xt+1 at time t + 1 depends on the latent state
xt at time t as well as on the intention g. We place a
GP prior GP( mx ( ·) , kx ( ·, ·) ) on every dimension of f with
shared mean and covariance functions. Subsequently, the
predictive distribution of the latent state xt+1 conditioned
on the current state xt and intention g is a Gaussian distribution given by xt+1 ∼ N ( mx ( [xt , g]) , x ( [xt , g]) ) based
on training inputs Xx and outputs Yx , with
mx ([xt , g]) =Yx K−1
x kx ([xt , g]) ,
 x ([xt , g]) =σx2 ([xt , g]) I,
σx2 ([xt , g]) =kx ([xt , g], [xt , g]) −kx ([xt , g])T

(6)
(7)
K−1
x kx ([xt , g]) ,
(8)

where Kx is the kernel matrix of training data Xx =
[[x1 , g1 ], . . . , [xn , gn ]]. The transition function f may also
depend on environment inputs u, e.g. controls or motor
commands. We assume that environment inputs are observable and omit them in the description of model for notational simplicity.

2.2. Covariance functions
By convention, we use GP prior mean functions that are
zero everywhere for notational simplicity, i.e. mz ( ·) ≡ 0 and
mx ( ·) ≡ 0. Hence, the model is determined by the covariance functions kz ( ·, ·) and kx ( ·, ·), which will be motivated
in the following.
The underlying dynamics of human motion are usually
nonlinear. To account for nonlinearities, we use a flexible Gaussian tensor-product covariance function for the
dynamics, i.e.
kx ([xi , gi ], [xj , gj ]; α) = kx ( xi , xj ; α) kx ( gi , gj ; α) +knoise
(9)

 α2
α3
2
2
= α1 exp − 2 xi − xj  − 2 ( gi − gj ) + α4 δij ,
where α = [α1 , α2 , α3 , α4 ] is the set of all hyperparameters,
and δ is the Kronecker delta function. When the intention
g is a discrete variable, we set the hyperparameter α 3 = ∞
such that kx ( gi , gj ; α) ≡ δij .
The covariance function for the measurement mapping
from the state space to observation space is chosen depending on the task. For example, the GPDM of Wang et al.
(2008) uses an isotropic Gaussian covariance function


(10)
kz (x, x ; β) = exp − β21 x − x 2 + β2 δx,x ,
parameterized by the hyperparameters β, as, intuitively, the
latent states that generate human poses lie on a nonlinear
manifold. Note that the hyperparameters β do not contain
the signal variance, which is parameterized by the scaling

kz (x, x ; β) = xT x + β1 δx,x ,

(11)

as the observations are already low-dimensional, but subject
to substantial noise.

2.3. Learning the IDDM
The proposed IDDM can be learned from a training data set
D = {Z, g} of J movements and corresponding intentions.
Each movement Zj consists of a time series of observations
j
j
given by Zj = [z1 , . . . , zT ]T . We construct the overall observation matrix Z by vertically concatenating the observation
matrices Z1 , . . . , ZJ , and the overall intention matrix g from
g1 , . . . , gJ . In the robot table tennis example, one movement corresponds to a stroke of the opponent, represented
by a time series of observed racket and ball configurations.
We assume that the intention g can be obtained for training,
for example by post-processing the data. In the robot table
tennis example, the observed intention corresponds to the
target where the opponent returns the ball to (see Figure 5
for an illustration). In the table-tennis training data, we can
obtained the target’s coordinates by post-processing. In the
action recognition, the label of action is provided directly in
the training data.
Similar to the GPDM (Wang et al., 2008), we find maximum a posteriori (MAP) estimates of the latent states X.
Alternative learning methods and an empirical comparison
can be found in (Turner et al., 2010; Damianou et al., 2011).
Given the model hyperparameters, the posterior distribution
of latent states X can be decomposed into the probability of
the observations given the states and the probability of the
states given the intention, i.e.
p( X|Z, g, α, β, W) ∝ p( Z|X, β, W) p( X|g, α) ,

(12)

both obtained by the GP marginal likelihood (Rasmussen
and Williams, 2006). The GP marginal probability of the
observations Z given the latent states X is given by a
Gaussian distribution



M
T T
,
exp − 12 tr K−1
p( Z|X, β, W)= √ |W|
z ZWW Z
D
MD
(2π )

z |Kz | z

(13)
where M  JT is the length of observations Z, and Kz is
the kernel matrix computed by the kernel function kz ( ·, ·).
Given the intention g, the sequence of latent states X has a
Gaussian probability
p( X|g, α) = p( X1 ) p( X2:T |X1:T−1 , g, α)

 1  −1
T
1)
= √ p(X
,
exp
−
tr
K
X
X
2:T
x
2:T
2
D
mD
(2π )

x |Kx | x

(14)
where Xt , t ∈ {1, . . . , T} is constructed by vertically concatenating state matrices x1t , . . . , xJt , m  J ( T − 1) is the

846

The International Journal of Robotics Research 32(7)

length of X2:T , and Kx is the kernel matrix of X2:T , computed by the kernel function kx ( ·, ·). We use a Gaussian
prior distribution on the initial states X1 .
Based on Equations (13) to (14), the MAP estimates
of the states are obtained by maximizing the posterior
in Equation (12). In practice, we minimize the negative
log-posterior


T T
− M log |W|
L( X) = D2z log |Kz | + 12 tr K−1
z ZWW Z


Dx
T
+ 2 log |Kx | + 12 tr K−1
x X2:T X2:T


+ 12 tr X1 XT1 + const
(15)
with respect to the states X, using the scaled conjugate
gradient (SCG) method (Møller, 1993).

Algorithm 1: Learning the model hyperparameters
α, β, and W by maximizing the marginal likelihood,
using the Monte Carlo EM algorithm.
Input : Data: D = {Z, g}
Input : Number of EM iterations: L
Output: Model hyperparameters:  = {α, β, W}
1 for l ← 1 to L do
2
for i ← 1 to I do
3
Initialize X by its MAP estimate ;
4
Draw sample X(i) from p( X|Z, g, ) using
HMC;

5
Maximize 1I Ii=1 log p( Z, X(i) |g, ) with respect
to  using SCG;

2.4. Learning hyperparameters
A reliable approach to learning the hyperparameters  =
{α, β, W} is to maximize the marginal likelihood

p( Z|g, ) = p( Z, X|g, ) dX,
(16)
which can be achieved approximately by using the EM
algorithm (Bishop, 2006). The EM algorithm computes the
posterior distribution of states q( X) = p( X|Z, g, ), given
in Equation (12), in the expectation (E) step, and updates
the hyperparameters by maximizing the expected likelihood
Eq [p( Z, X|g, ) ] in the maximization (M) step. However,
the posterior distribution q( X) is difficult to compute in
the IDDM. Following Wang et al. (2008), we draw samples
of the states X(1) , . . . , X(I) from the posterior distribution
using hybrid Monte Carlo (Andrieu et al., 2003) and hence,
the likelihood is estimated via Monte Carlo integration
according to
1
p( Z, X(i) |g, ) .
I i=1
I

Eq [p( Z, X|g, ) ] ≈

(17)

In the M step, we use SCG to update the hyperparameters.
In practice, we choose the number of samples I = 50 and
the number of EM iterations L = 10. Although this procedure, as described in Algorithm 1, is time-demanding, in
practice, we can learn the hyperparameters off-line.
In practice, the maximum likelihood estimate of the
hyperparameters may lead to over-fitting. For the IDDM, we
found that the noise variance α4 in Equation (9) of the transition model is occasionally underestimated, e.g. α4 < e−6 ,
as Algorithm 1 estimates it based on only a few samples.
The underestimated noise variance may prevent the learned
model from generalizing to test data that have significant
deviation from the training data. This phenomenon of overconfidence has been discussed by Lawrence (2005) and
Wang et al. (2008). To alleviate this problem, we add a small
constant e−3 to the learned noise variance α4 .
The model also depends on the hyperparameter Dx , i.e.
the dimensionality of the latent state space. Choosing an

appropriate Dx is important. If the dimensionality is too
small, the latent states cannot recover the observations,
which leads to significant prediction errors. On the other
hand, a high-dimensional state space results in redundancy
and can cause a drop in performance and computational
efficiency. Nevertheless, model selection, based on crossvalidation for example, is conducted before learning and
applying the model.
To summarize, the model M = {X, } can be learned
from a data set D. Subsequently, we use the model to infer
the unobserved intention of a new ongoing movement, as
described in the following section.

3. Approximate intention inference
After learning the model M from the training data set
D, the intention g can be inferred from a sequence of
new observations z1:T . For notational simplicity, we do not
explicitly condition on the model M and the data set D.
The measurement model defined in Equation (1) scales the
observations by a diagonal matrix W. Therefore, we preprocess every received observation with the scaling matrix
W and omit W hereafter as well.
The IDDM models the generative process of movements, represented by observations z1:T , given an intention
g. Using Bayes’ rule, we estimate the posterior probability (belief) on an intention g from observations z1:T . The
posterior is given by
p( z1:T |g) p( g)
p( z1:T )

∝ p( g) p( z1:T , x1:T |g) dx1:T ,

p( g|z1:T ) =

(18)
(19)

where computing the marginal likelihood p( z1:T |g) requires
to integrate out the latent states x1:T . Exactly computing the
posterior in Equation (19) is not tractable due to the use of
nonlinear GP transition model. Hence, we resort to approximate inference. In Wang et al. (2012b), we introduced an
EM algorithm for finding the maximum likelihood estimate

Wang et al.

847

of the intention. However, this point estimate may not suffice for the reactive policies of the robot that also take into
account the uncertainty in the intention inference (Wang
et al., 2011a,b; Bandyopadhyay et al., 2012). For example,
in the table tennis task, the robot may need to choose the
optimal time to initiate its hitting movement, and such a
choice is ideally made based on how certain the prediction
of target is (Wang et al., 2011b). In this article, we extend
our previous inference method (Wang et al., 2012b), such
that the uncertainty about the intention is explicitly modeled
and taken into account when making decisions.
The key challenge in estimating the belief in Equation
(19) is integrating out the latent states x1:T . A common
approximation to the log marginal posterior is to compute a lower bound B( g) ≤ log p( g|z1:T ) based on Jensen’s
inequality (Bishop, 2006). The bound is given by
(20)
B( g)  Eq [log p( z1:T , x1:T , g) ] + H( q)
= log p( g|z1:T ) −KL (q||p( x1:T |z1:T , g) )
≤ log p( g|z1:T ) ,
which holds for any distribution q( x1:T ) on the latent
states. Here, the Kullback–Leibler (KL) divergence
KL (q||p( x1:T |z1:T , g) ) determines how well B( g) can
approximate the belief. Based on this approximation, the
inference problem consists of two steps, namely, (a) finding
an approximation q( x1:T ) ≈ p( x1:T |z1:T , g), and (b) computing the approximate belief B( g). When using the EM
algorithm for the maximum likelihood estimate of the intention g, as in (Wang et al., 2012b), the E step and M step
correspond to these two steps, respectively.
For step (a), we approximate the posterior of latent
states p( x1:T |z1:T , g) by a Gaussian distribution q( x1:T ).
For this purpose, we use the forward–backward smoothing
method proposed by Deisenroth et al. (2009, 2012), which
is based on moment matching. Typically, Gaussian moment
matching provides credible error bars, i.e. it is robust to
incoherent estimates. The resulting approximate distribution q that we use in the lower bound B in Equation (20)
is given by
q(x1:T ) = N ( μq ,  q ) ≈ p(x1:T |z1:T , g) ,

(21)

with the mean and block-tridiagonal covariance matrix
⎡ x ⎤
μ1|T
⎢ .. ⎥
μq = ⎣ . ⎦ ,
μxT|T
⎤
⎡ x
0
 1|T  x1,2|T
⎥
⎢ x
.
..
⎥
⎢ 2,1|T . .
.
⎥,
⎢
q = ⎢
(22)
⎥
..
..
x
⎣
.
.
 T−1,T|T ⎦
0
 xT,T−1|T  xT|T
where we only need to consider the cross-covariance
between consecutive states.1

For step (b), based on the approximation q( x1:T ), the
posterior belief p( g|z1:T ) can then be approximated by the
lower bound B( g) in Equation (20).
In the following, we first detail step (a), i.e. the computation of q for our IDDM, in Section 3.1. Subsequently,
we discuss step (b), i.e. efficient belief estimation, in Section 3.2.

3.1. Filtering and smoothing in the IDDM
To obtain the posterior distribution p( x1:T |z1:T , g), approximate filtering and smoothing with GPs are crucial in
our proposed IDDM. We place a Gaussian prior on
the initial state x1 . Subsequently, Gaussian approximations q( xt−1 , xt ) of p( xt−1 , xt |z1:T , g) for t = 2, . . . , T
are computed. We explicitly determine the marginals
p( xt |z1:T , g) for t = 1, . . . , T, and the cross-covariance
terms cov[xt−1 , xt |z1:T , g], t = 2, . . . , T. These steps yield
a Gaussian approximation with a block-tri-diagonal covariance matrix, see Equation (22). These computations are
based on forward–backward smoothing (GP-RTSS) as proposed by Deisenroth et al., (2012).
As a first step, we compute the posterior distributions
p( xt |z1:T , g) with t = 1, . . . , T. To compute these posteriors using Bayesian forward–backward smoothing in
the IDDM, it suffices to compute both joint distributions
p( xt−1 , xt |z1:t−1 , g) and p( xt , zt |z1:t−1 , g). The Gaussian filtering and smoothing updates can be expressed solely in
terms of means and (cross-)covariances of these joint distributions (Deisenroth and Ohlsson, 2011; Deisenroth et al.,
2012). Hence, we have
z
−1
( zt − μzt|t−1 ) ,
μxt|t = μxt|t−1 +  xz
t|t−1 (  t|t−1 )

 xt|t

=

μxt−1|T
 xt|T

=
=

z
−1 zx
 xt|t−1 −  xz
 t|t−1 ,
t|t−1 (  t|t−1 )
x
x
x
μt−1|t−1 + Jt−1 ( μt|T − μt|t−1 ) ,
 xt−1|t−1 + Jt−1 (  xt|T −  xt|t−1 ) JTt−1 ,

(23)
(24)
(25)
(26)

where we define
Jt−1 =  xt−1,t|t−1 (  xt|t−1 )−1 .

(27)

In the following, we first detail the computations required
for a Gaussian approximation of the joint distribution
p( xt−1 , xt |z1:t−1 , g) using moment matching. Here, we
approximate the joint distribution p( xt−1 , xt |z1:t−1 , g) by the
Gaussian
  x

 x
μt−1|t−1
 t−1|t−1  xt−1,t|t−1
,
.
(28)
N
μxt|t−1
 xt,t−1|t−1
 xt|t−1
Without loss of generality, the marginal distribution
N ( xt−1 | μxt−1|t−1 ,  xt−1|t−1 ), which corresponds to the filter
distribution at time step t − 1, is assumed known. We compute the remaining elements of the mean and covariance in
Equation (28) in the following paragraphs. We will derive
our results for the more general case where we have a joint
Gaussian distribution p( xt−1 , xt , g|z1:t−1 ). The known mean

848

The International Journal of Robotics Research 32(7)

and covariance of distribution p( xt−1 , g|z1:t−1 ) are given by
˜ t−1|t−1 , respectively,
μ̃t−1|t−1 = [( μxt−1|t−1 )T , μTg ]T and 
˜
where the covariance matrix  t−1|t−1 is block-diagonal with
blocks  xt−1|t−1 and  g . By setting the mean μg = g and
 g = 0, we obtain the results from Wang et al. (2012b).
For convenience, we define x̃ = [xT , g]T .
Using the law of iterated expectations, the ath dimension
of the predictive mean of the marginal p(xt |z1:t−1 ) is given
as


(29)
( μxt|t−1 )a = Ext−1 Efa [fa (x̃t−1 ) |x̃t−1 ]|z1:t−1

= max (x̃t−1 ) p(x̃t−1 |z1:t−1 ) d x̃t−1 ,
where we substituted the posterior GP mean function for
the inner expectation. Note that if g is given then  g = 0.
Writing out the posterior mean function and defining γa :=
K−1
x ya , with yai , i = 1, . . . , M, being the training targets of
the GP with target dimension a, we obtain
( μxt|t−1 )a = q
 γa ,

(30)

where we define

T
q = kx ([xt−1 , g], Xx ) p( x̃t−1 |z1:t−1 ) d x̃t−1 .

(31)

Here, Xx denotes the set of the M GP training inputs x̃i =
[xTi , gii ]T of the transition GP. Since kx is a Gaussian kernel,
we can solve the integral in Equation (31) analytically and
obtain the vector q with entries qi with i = 1, . . . , M as
1


qi = α1 ||− 2 exp − 12 ζ Ti ( )−1 ζ i ,

ζ i = x̃i − μ̃t−1|t−1 ,

−1

 =  t−1|t−1 

(32)
+ I,

(33)

where  is a diagonal matrix of concatenated length scales
α2 I and α3 I. By applying the law of total variances, the
x
of the marginal predictive covariance matrix
entries σab
x
 t|t−1 in Equation (28) are given by

x
σab

⎧ T x
T
⎪
⎨ γa ( Q − qq ) γb
= γT ( Qx − qqT ) γ +α1
b
⎪
⎩ a
−tr( ( Kx +α4 I)−1 Qx ) + α4

if a = b,
(34)
if a = b.

We define the entries of Qx ∈ RM×M as
Qxij =

kxa ([xi , gi ], [μ̃t−1|t−1 ]) kxb ( [xj , gj ], [μ̃t−1|t−1 ])
√
|R|
 1 T −1 
exp 2 zij T zij

with
−1
˜ t−1|t−1 ( −1
R := 
a + b ) +I,
−1

−1
˜
T =( −1
a + b +  t−1|t−1 ) ,
−1
zij := −1
a ( x̃i − μ̃t−1|t−1 ) +b ( x̃j − μ̃t−1|t−1 ) .

For a detailed derivation, we refer to Deisenroth (2010);
Deisenroth et al. (2012).
To fully determine the joint Gaussian distribution
in Equation (28), the cross-covariance  xt−1,t|t−1 =
cov[xt−1 , xt |z1:t−1 , g] is given as the upper part of the
cross-covariance
cov[xt−1 , xt , g|z1:t−1 ] =

M


˜ t−1|t−1 −1
γai qai 

i=1

×( x̃i − μ̃t−1|t−1 ) ,
when we set μg = g and  g = 0. Note that q and  are
defined in Equations (32) and (33), respectively.
Up to now, we have computed a Gaussian approximation to the joint probability distribution p( xt−1 , xt |z1:t−1 , g).
Let us now have closer look at the second joint distribution p( xt , zt |z1:t−1 , g), which is the missing contribution
for Gaussian smoothing (Deisenroth and Ohlsson, 2011),
see Equations (23) to (26). To determine a Gaussian
approximation

 x   x
μt|t−1
 t|t−1  xz
t|t−1
,
(35)
N
μzt|t−1
 zx
 zt|t−1
t|t−1
to p( xt , zt |z1:t−1 , g) it remains to compute the mean and
the covariance of the marginal distribution p( zt |z1:t−1 , g)
and the cross-covariance terms cov[xt , zt |z1:t−1 , g]. We omit
these computations for the nonlinear Gaussian kernel as
they are very similar to the computations to determine the
joint distribution p( xt−1 , xt |z1:t−1 , g).
For the linear measurement kernel in Equation (11), we
compute the marginal mean μzt|t−1 in Equation (35) for
observation dimension a = 1, . . . , Dz according to

Eh,xt−1 [ha ( xt ) |z1:t−1 , g] = m( xt ) p( xt |z1:t−1 , g) dxt

= xTt XTz p( xt |z1:t−1 , g) dxt ξ a = qT ξ a ,
(36)
q = Xz μxt|t−1 .
Here, Xz comprises the training inputs for the measurement
model and ξ a = K−1
z Yza , where Yza are the training targets
z
of
of the ath dimension, a = 1, . . . , Dz . The elements σab
z
the marginal covariance matrix t|t−1 in Equation (35) are
given as
⎧ T z
T
if a = b,
⎪
⎨ ξ a ( Q − qq ) ξ a
 −1 z 
z
x
x
x
T
σab = 
+μ
( μt|t−1 ) −tr Kz Q
⎪
⎩ t|t−1T t|t−1
+ξ a ( Qz −qqT ) ξ a
if a = b,
(37)
a, b = 1, . . . , Dz , where we define

z
Q = Xz xt xTt Xz T p(xt |z1:t−1 , g)
dxt = Xz ( xt|t−1 + μxt|t−1 (μxt|t−1 )T ) Xz T .

Wang et al.

849

The cross-covariance  xz
t|t−1 = cov[xt , zt |z1:t−1 , g] in Equation (35) is given as
x
T
 xz
t|t−1 =  t|t−1 Xz ξ a

(38)

for all observed dimensions a = 1, . . . , Dz . The mean μzt|t−1
in Equation (36), the covariance matrix  zt|t−1 in Equation
(37), and the cross-covariance in Equation (38) fully determine the Gaussian distribution in Equation (35). Hence,
following Deisenroth and Ohlsson (2011), we can now
compute the latent state posteriors (filter and smoothing
distributions) according to Equations (23) to (26).
These smoothing updates in Equations (23) to (26)
yield the marginals of our Gaussian approximation to
p( x1:T |z1:T , g), see Equation (22). The missing crosscovariances  xt−1,t|T of p( x1:T |z1:T , g) that finally fully determine the block-tridiagonal covariance matrix in Equation
(22) are given by
 xt−1,t|T = Jt−1  xt|T ,

(39)

where Jt−1 is given in Equation (27). For detailed derivations, we refer to Deisenroth (2010).
These computations conclude step (1) on lower-bounding
the posterior distribution on the intention, see Equation
(20), i.e. the computation of the approximate distribution q
in Equation (21). It remains to compute the bound B itself,
which is described in the following.

3.2. Estimating the belief on intention
For a given intention g, we compute a Gaussian approximation q( x1:T ) to the posterior p( x1:T |z1:T , g), given by

q(xt , xt+1 ) = N

  x
μxt|T
 t|T
,
μxt+1|T
 xt+1,t|T

 xt,t+1|T
 xt+1|T


(40)

for t = 1, . . . , T − 1. The belief p( g|z1:T ) ≈ exp( B( g) ) is
estimated using Equation (20), where the computation can
be decomposed according to
B( g) =

T−1

t=1

Eq [log p( xt+1 |xt , g) ] +p( g) +H( q) +const.



Qt (g)

(41)
Here the smoothing distribution q( x1:T |g) ≈ p( x1:T |z1:T , g)
is computed given the intention g. As we only need to
estimate the unnormalized belief, the constant term needs
not to be computed. The entropy H( q) of the Gaussian
distribution q can be computed analytically, and is given by
H( q) =


1
TDx + TDx log( 2π ) + log | q | .
2

(42)

We define
Qt ( g)  Eq [log p( xt+1 |xt , g) ]
(43)

=
q(xt , xt+1 ) log p(xt+1 |xt , g) dxt+1 dxt

=
q(xt , xt+1 ) log (p(xt+1 |xt , g) q(xt ) ) dxt+1 dxt




−

≈q̃(xt ,xt+1 )

q(xt ) log q(xt ) dxt ,

where p(xt+1 |xt , g) q(xt ) can be approximated by a Gaussian distribution q̃( xt , xt+1 ) = N ( μq̃ ,  q̃ ) based on moment
matching (Quiñonero-Candela et al., 2003). Here, we only
compute the diagonal elements in the covariance matrix of
 q̃ . As a result, Equation (43) is approximated as
Qt ( g) ≈ KL( q(xt , xt+1 ) ||q̃(xt , xt+1 ) ) +H( q(xt , xt+1 ) )
+H( q( xt ) ) ,
(44)
where H( q) is the entropy of the distribution q and
KL( q||q̃) is the KL divergence between q and q̃, both
of which are Gaussians. The KL divergence also has a
closed-form expression, given by

1
tr( q̃−1  q ) +( μq − μq̃ )T  q̃−1 ( μq − μq̃ )
KL( q||q̃) =
2

| q |
− log
+ const.
(45)
| q̃ |
As a result, we can compute the unnormalized belief B( g)
for a given intention g approximately according to Equation
(41).
We aim to determine the posterior distribution p( g|z1:T )
of the intention g. Using the posterior distribution instead
of point estimates allows us to express uncertainty about
the inferred intention g. Computing Gaussian approximations of the posterior distributions can be done using the
unscented transformation (Deisenroth et al., 2012), for
instance. However, when the posterior is not unimodal, a
Gaussian approximation may lose important information.
Particle filtering can preserve all of the modes (Ko and
Fox, 2009), but will not be sufficiently efficient due to
the real-time constraints. As we focus on one-dimensional
intentions in this article, we advocate the discretization of
intention. For example, in the table tennis task, the intention (opponent’s target position) is a bounded scalar variable
g ∈ [gmin, gmax ], where the bounds are given by physical
constraints such as the table width and the length of robot
arm. We uniformly choose {v1 , . . . , vK } from [gmin , gmax ]
and represent intention by the index, i.e. g ∈ {1, . . . , K}.

3.3. Discussion of the approximate inference
method
To summarize, the algorithm for computing the posterior distribution over discrete or discretized intentions g

850

Algorithm 2: Inference of the discretized intentions by
computing the posterior probabilities for every value of
the intention.
Input : Observations x1:T
Output: Posterior probabilities for every intention
value g ∈ {1, . . . , K}
1 foreach g ∈ {1, . . . , K} do
2
Compute smoothing distribution
q(x1:T ) ≈ p(x1:T |z1:T , g) ;
3
Compute the value of
B( g) = Eq [log p(x1:T |g) ] + log p( g) using the
approximation in Equation (44) ;
4

Estimate the posterior

p( g|x1:T ) ≈ exp B(g)/( Kg =1 exp B( g )).

is given in Algorithm 2. The smoothing distribution q
defined in Equation (40) depends on the current estimate of
intention g.
However, it is often time-demanding to enumerate the
intention g and compute the smoothing distribution q for
each g individually. The computational complexity of the
smoothing step in Algorithm 2 is O( TK( D3z + Dx D2z +
N 2 D3x ) ) when using the linear kernel function for the measurement mapping, and O( TK( D3z + N 2 Dx ( D2x + D2z ) ) )
when using the Gaussian kernel function, where T is the
number of observations obtained, K the number of (discretized) intentions, N the number of training data, and Dx
and Dz the dimensionality of state and observation. The
complexity of computing the belief is O( TKN 2 D2x ). The
computational efficiency can be improved to meet the tight
time constraints in robotic applications by introducing further approximations, such as adopting GP pseudo-inputs to
reduce the size of training data N (Quiñonero-Candela and
Rasmussen, 2005; Snelson and Ghahramani, 2006), using
dimensionality reduction or feature selection techniques to
obtain a small number of features Dz (Ding and Peng, 2005;
van der Maaten et al., 2009), and reducing the sample size
K of intention g. However, the dependence of complexity
on the number of observations T still prevents the algorithm from being applied to online scenarios. For these, T
keeps growing as new observations come, whereas observations obtained a long time ago do not provide as much
information as recent ones. To address this issue, we will
introduce an approximation in the online inference method
in Section 4.

4. Online intention inference
The introduced inference algorithm can be seen as a batch
algorithm that relies on the segmentation of human movements. However, in online human–robot interaction, the
intention inference algorithm faces new challenges to deal

The International Journal of Robotics Research 32(7)

g

···

xt−1

xt

xt+1

zt−1

zt

zt+1

···

Fig. 3. The graphical model of the IDDM in an online manner,
which can handle a stream of observations.

with the stream of observations. The complexity of Algorithm 2 grows with the number of existing observations,
which does not fulfill the real-time requirements of an
online method. In addition, the intention can vary over time
in an online inference scenario. For example, the intended
targets in table tennis games vary between strokes. Hence,
the online method should model and track the change of
intention.
To address these issues, we generalize the inference
method to an online scenario. That is, the observations are
obtained constantly, and the belief on the intention is reestimated after receiving a new observation. A computational bottleneck in the batch method is that the smoothing
distribution q is computed for every value of intention. For
efficient inference, we compute a marginal smoothing distribution q according to current belief on intention p( g), i.e.
we integrate out the intention,

p(g) qg (x1:t ) .
(46)
q( x1:t ) 
g

The online inference algorithm then estimates the belief
Bt ( g) on the intention based on the marginal smoothing
distribution q after receiving an observation, which can be
sufficiently efficient for real-time intention inference with a
small sacrifice in accuracy.
Based on the marginal smoothing distribution, we update
the belief on intention using dynamic programming, which
will be discussed as follows.

4.1. Online inference using dynamic
programming
Assuming the marginal smoothing distribution q is given,
we develop an online inference method using dynamic programming (see Figure 3). The method maintains the belief
(i.e. log of the unnormalized posterior) of the intention
g based on the obtained observations z1:t−1 according to
Equation (41), given by
Bt−1 ( g) ≈ Eq [log p( g, x1:t−1 ) ] + const.

(47)

Wang et al.

851

Here, we consider discretized intentions g ∈ {1, . . . , K},
and write the belief Bt−1 as a vector of length K. For a new
observation zt , we decompose p( g, x1:t ) according to
p( g, x1:t ) = p( xt |xt−1 , g) p( g, x1:t−1 ) .

(48)

Algorithm 3: The online algorithm for the inference of
discrete intention g ∈ {1, . . . , K}.
1
2
3

As a result, the belief Bt becomes

4

(49)
Bt ( g) = Eq [log p( g, x1:t ) ] + const
= Eq [log p( xt |xt−1 , g) ]+Eq [log p( g, x1:t−1 ) ]+const
(50)
= Eq [log p( xt |xt−1 , g) ] + Bt−1 ( g) +const,

6

7

(51)
8

which is in a recursive form and can be computed efficiently
using dynamic programming. Given a new observation zt ,
the belief is updated based on Eq [log p( xt |xt−1 , g) ], which
is computed according to Equations
(43) to (44). The belief

Bt is then normalized, i.e. g exp( Bt ( g) ) = 1.
In addition, the intention can vary over time in an online
inference scenario. As the new observation zt can be more
informative than the previous observations z1:t−1 , we introduce a forgetting factor  to shrink the belief Bt−1 . The
recursive formula of the belief is subsequently given by
Bt ( g) = Eq [log p( xt |xt−1 , g) ] +( 1 − ) Bt−1 ( g) ,

5

(52)

where the shrinking factor  determines how fast the algorithm forgets the previous observations.

9

10
11

Obtain the initial observation z1 ;
Initialize the approximate distribution q(x1) ;
Initialize B1 (g) = log p(g) according to the prior ;
for t = 2, 3, . . . do
Obtain the observation zt ;
Compute marginal filtering distribution q(xt)
according to current belief Bt−1 ;
Update marginal smoothing distribution q(xt−1 )
according to current belief Bt−1 ;
foreach gt = {1, . . . , K} do
Compute B0 (g) = Qt−1 (g) using the
approximation in Equation (44);
Update the belief Bt = B0 +(1 − ) Bt−1 ;
Normalize the belief


exp(
B
(g))
;
Bt ← Bt − log
t
g

we compute the mean μg and variance σg2 according to the
belief Bt−1 . As a result, the marginal smoothing distribution
is given by

(54)
q( xt−1:t ) ≈ qg ( xt−1:t ) N ( g|μg , σg2 ) dg,
which is computed using moment matching.

4.2. Marginal smoothing distribution
The inference method relies on the smoothing distribution
q at time t, which in turn depends on the intention belief
Bt−1 . In analogy to the EM algorithm, we iteratively update
the belief on intention B and the smoothing distribution
q. However, full forward–backward smoothing on x1:t is
impractical as the computational complexity grows when
we obtain more observations. Full smoothing is also unnecessary since we do not update the previous belief B1:t−1 on
the intention. Hence, given a new observation zt , we only
need to compute q(xt−1:t ), which requires a single-step forward filtering and a single-step backward smoothing, based
on the current belief Bt−1 .
The filtering and smoothing need to integrate out the
uncertainty in the intention. For discrete intentions, we can
simply compute the smoothing distributions qg for every
value of intention gt−1 , and average over them

qg (xt−1:t ) pt−1 ( g) ,
(53)
q(xt−1:t ) ∝
g

where the belief pt−1 ( g) ∝ exp(Bt−1 ( g)). The resulting
distribution q will still be a Gaussian distribution.
For continuous intentions, enumerating the discretized
intention may be inefficient. To address this problem, we
use the moment matching to approximate the distribution on intention by a Gaussian distribution, which is also
adopted in the filtering and smoothing method. Specifically,

4.3. Discussion of the online inference method
The online inference algorithm described in Algorithm 3
iteratively updates the belief of intention and latent states.
The computational complexity of the smoothing step in
Algorithm 3 is O(D3z + Dx D2z + N 2 D3x ) when using the
linear kernel function for the measurement mapping, and
O(D3z + N 2 (Dx D2x + D3x )) when using the Gaussian kernel function, which no longer depends on the number of
observations T and the number of intentions K. The complexity of computing the belief is O(KN 2 D2x ). Compared to
the batch algorithm, the efficiency is improved by a factor
of T.
To summarize, we proposed an efficient online method
for intention inference from a new movement. The online
method updates the belief of the intention by taking into
account both the current belief and the new evidence (i.e.
new observation). We list the employed approximations in
both the batch and online inference methods in Table 1.

5. Target prediction for robot table tennis
Playing table tennis is a challenging task for robots and,
hence, has been used by many researchers as a benchmark
task in robotics (Billingsley, 1984; Anderson, 1988; Fässler
et al., 1990; Matsushima et al., 2005; Mülling et al., 2011).
Up to now, none of the groups that have been working

852

The International Journal of Robotics Research 32(7)

Table 1. Important approximations employed in the batch and online inference.
Batch
Belief p( g|z1:T )
Approx. belief B( g)
Distr. p( x1:T |z1:T , g)
Stream of observations

Online

Jensen’s lower bound B( g); cf. Equation (20)
Moment matching; cf. Equation (44)
q( x1:T |g) for each g
q( x1:T ) for all g; cf. Equation (53)
Sliding window
Recursive update; cf. Equation (51)

on robot table tennis ever reached the levels of a young
child, despite having robots with better perception, processing power, and accuracy than humans (Mülling et al.,
2011). Likely explanations for this performance gap are (i)
the human ability to predict hitting points from opponent
movements and (ii) the robustness of human hitting movements (Mülling et al., 2011). In this article, we focus on the
first issue: anticipation of the hitting region from opponent
movements.
Using the proposed method, we can predict where the
ball is likely to be shot before the opponent hits the ball,
which gives the robot a head start of more than 200 ms
additional time to initiate its movement.2 This additional
time can be crucial due to robot’s hardware constraints, for
example, acceleration and torque limits in the considered
setting (Mülling et al., 2011).
Note that the predicted intention is only used to choose
a hitting type, e.g. forehand, middle, or backhand. Finetuning of the robot’s movement can be done when the robot
is adjusted to the forehand/middle/backhand preparation
pose and once the returned ball can be reliably predicted
from the ball’s trajectory alone. Hence, a certain amount
of intention prediction error is tolerable since the robot
can apply small changes to its basic hitting plan based
on the ball’s trajectory. However, the robot cannot return
the ball outside the corresponding hitting region once it is
adjusted to a preparation pose.3 Therefore, prediction accuracy directly influences the performance of the robot player
(Wang et al., 2011b).

5.1. Experimental setting
Our anticipation system has been evaluated in conjunction with the biomimetic robot table tennis player (Mülling
et al., 2011), as this setup allowed exhibiting how much
of an advantage such a system may offer. We expect that
the system will help similarly or more when deployed
within our skill learning framework (Mülling et al., 2013)
as well as many of the recent table tennis learning systems
(Matsushima et al., 2005; Yang et al., 2010; Huang et al.,
2013).
We used a Barrett WAM robot arm to play table tennis against human players. The robot’s hardware constraints impose strong limitations on its acceleration, which
severely restricts its movement abilities. This limitation can
best be illustrated using typical table tennis stroke movements as shown in Figure 4, see Ramanantsoa and Durey

(1994) and Mülling et al. (2011), which consist of four
stages, namely awaiting stage, preparation stage, hitting
stage, and finishing stage. In the awaiting stage, the ball
moves toward the opponent and is returned by the opponent. The robot player moves to the awaiting pose and stays
there during this stage. The preparation stage starts when
the hitting movement is chosen according to the predicted
opponent’s target. The arm swings backward to a preparation pose. The robot requires sufficient time to execute a
ball-hitting plan in the hitting stage. To achieve the required
velocity for returning the ball in the hitting stage, movement
initiation to an appropriate preparation pose in the preparation stage is needed, which is often before the opponent hits
the ball. The robot player uses different preparation poses
for different hitting plans. Hence, it is necessary to choose
among them based on modeling the opponent’s preference
(Wang et al., 2011a) and inference of the opponent’s target
location for the ball (Wang et al., 2011b).
The robot perceives the ball and the opponent’s racket
in real-time, using seven Prosilica GE640C cameras. These
cameras were synchronized and calibrated to the coordinate system of the robot. The ball tracking system uses four
cameras to capture the ball on both courts of the table (Lampert and Peters, 2012). The racket tracking system provides
the information of the opponent’s racket, i.e. the position
and orientation (Wang et al., 2011b). As a result, the observation zt includes the ball’s position and velocity as well
as the opponent’s racket position, velocity, and orientation
before the human plays the ball. For the anticipation system
described here, we process the observations every 80 ms.
Here, the position and velocity of the ball were processed
online with an extended Kalman filter, based on a known
physical model (Mülling et al., 2011). However, the same
smoothing method cannot be applied to the racket’s trajectory, as its dynamics are directed by the unknown intended
target. Therefore, the obtained states of the racket were subject to substantial noise and the model has to be robust
to this noise. The proposed inference method can jointly
smooth on the racket’s trajectory, given by the smoothing
distribution q, and infer the intended target, given by the
belief B.
In our setting, the robot always chooses its hitting point
on a virtual hitting plane, which is 80 cm behind the table,
as shown in Figure 5. We define the human’s intended target g as the intersection of the returned ball’s trajectory
with the robot’s virtual hitting plane. As the x-coordinate
(see Figure 5) is most important for choosing among forehand/middle/backhand hitting plans (Wang et al., 2011b),

Wang et al.

853

(a) Awaiting stage

(b) Preparation stage

(c) Hitting stage

(d) Finishing stage

Fig. 4. The four stages of a typical table tennis ball rally are shown with the red curve representing the ball trajectories. Blue trajectories
depict the typical racket movements of players. The racket of human player is to the left of the table in the pictures. (Figures are adapted
from Mülling et al. (2011).)

Fig. 5. The robot’s hitting point is the intersection of the coming ball’s trajectory and the virtual hitting plane 80 cm behind the
table. (Figure adapted from Mülling et al. (2011).)

the intention g considered here is the x-coordinate of the
hitting point. Physical limitations of the robot restrict the
x-coordinate to the range to ±1.2 m from the robot’s base
(table is 1.52 m wide).
To evaluate the performance of the target prediction, we
collected a data set with recorded stroke movements from
different human players. The true targets were obtained
from the ball tracking system. The data set was divided
into a training set of 100 plays and a test set of 126 plays.
The standard deviation of the target coordinate in the test
set is 102.2 cm. A straightforward approach to prediction
is to learn a mapping from the features zt (including the
position, orientation, and velocity of the racket and the position and velocity of the ball) to the target g. We compared
our method with this baseline Gaussian process regression

(GPR) using a Gaussian kernel with automatic relevance
determination (Rasmussen and Williams, 2006). We considered using a sliding window on the sequence of observations, and conducted model selection to choose the optimal
window size. The best accuracy of GPR was achieved when
using a sliding window of size two, i.e. the input features
consist of zt−1 and zt . The hyperparameters were learned
by maximizing the marginal likelihood of training data,
following the standard routine (Rasmussen and Williams,
2006).
For every recorded play, we compared the performance of
the proposed IDDM intention inference and the GPR prediction at 80 ms, 160 ms, 240 ms, and 320 ms before the
opponent hits the ball. Note that this time step was only used
such that the algorithms could be compared, and that the
algorithms were not aware of the hitting time of the opponent in advance. We evaluated both the batch algorithm and
online algorithm.

5.2. Results
As demonstrated in Figure 6, the proposed IDDM model
outperformed the GPR baseline. At 80 ms before the opponent hit the ball, the batch algorithm resulted in a mean
absolute error of 31.5 cm, which achieved a 11.3% improvement over the GPR, whose average error was 35.6 cm. The
online algorithm had a mean absolute error of 32.5 cm,
which also outperformed GPR by an 8.5% improvement
in the accuracy. One model-free naive intention prediction
is to always predict the median of the intentions in the
training set. This naive prediction model caused an error

854

The International Journal of Robotics Research 32(7)

Online

Batch

GPR

Mean absolute error in cm

40

35

30

25

20

320

240
160
time in ms before stroke

80

Fig. 6. Mean absolute error of the ball’s target with standard error
of the mean. The algorithms use the observations obtained before
the opponent has hit the ball.

Our results demonstrated that the IDDM can improve the
target prediction in robot table tennis and choose the correct
hitting plan.
We have developed a proof-of-concept prototype system in which the robot is equipped with three pre-defined
hitting movements, i.e. forehand, middle, and backhand
movements, with their hitting regions shown in Figure 7.
As exhibited in Figure 8, our method allows the robot to
choose the responding hitting movement before the opponent has hit the ball himself, which is often necessary
for the robot to have sufficient time to execute the hitting movement, and substantially expands the robot’s overall hitting region to cover almost the entire accessible
workspace.5 Furthermore, we expect that the method can
further enhance the robot’s capability when equipped with
more and self-improving hitting primitives (Mülling et al.,
2013).

6. Action recognition in HRI
Table 2. The mean absolute errors (in cm) with standard error
of the mean of the goal inference made 80 ms before the opponent hits the ball, where Dx denotes the dimensionality of the state
space.
Kernel

Dx = 3

Dx = 4

Dx = 5

Dx = 6

Linear
Gaussian

41.5 ± 3.0
38.5 ± 2.7

31.5 ± 2.2
34.2 ± 2.5

35.4 ± 2.4
34.4 ± 2.7

37.0 ± 2.6
37.3 ± 2.7

of 78.8 cm. Hence, both the GPR and IDDM substantially
outperformed naive goal prediction.
The online algorithm, with a shrinking factor  = 0.2
given in Equation (52), took on average 70 ms to process
every observation, which can potentially fulfill the realtime requirements of 80 ms. The batch algorithm used a
sliding window of size 4, and took on average 300 ms to
process every observation. The online algorithm was significantly faster than the batch algorithm, with a small loss
in accuracy.4 Nevertheless, a certain amount of error is tolerable since the robot can apply small changes to its basic
hitting plan based on the ball’s trajectory. Therefore, we
advocate the use of the online algorithm for applications
with tight real-time constraints.
We performed model selection to determine the covariance function kz , which can be either an isotropic Gaussian
kernel, see Equation (10), or a linear kernel, see Equation
(11). Furthermore, we performed model selection to find
the dimension Dx of the latent states. In the experiments, the
model was selected by cross-validation on the training set.
The best model under consideration was with a linear kernel and a four-dimensional latent state space. Experiments
on the test set verified the model selection result, as shown
in Table 2.

To realize safe and meaningful HRI, it is important that
robots can recognize the human’s action. The advent of
robust, marker-less motion capture techniques (Shotton
et al., 2011) has provided us with the technology to record
the full skeletal configuration of the human during HRI.
Nevertheless, recognition of the human’s action from this
high-dimensional data set poses serious challenges.
In this paper, we show that the IDDM has the potential to infer the intention of actions from movements in
a simplified scenario. Using a Microsoft Kinect camera,
we recorded the 32-dimensional skeletal configuration of
a human during the execution of a set of actions namely:
crouching (C), jumping (J), kick-high (KH), kick-low (KL),
defense (D), punch-high (PH), punch-low (PL), and turnkick (TK). For each type of action we collected a training
set consisting of 10 repetitions and a test set of three repetitions. The system down-sampled the output of Kinect and
processed three skeletal configurations per second.
In this task, the intention g is a discrete variable and
corresponds to the type of action. Action recognition can
be regarded as a classification problem. We compared our
proposed algorithms to support vector machines (SVMs),
see Schölkopf and Smola (2001), and multi-class Gaussian
process classification (GPC), see Khan et al. (2012). We
used off-the-shelf toolboxes, i.e. LIBSVM (Chang and Lin,
2011) and catLGM,6 , and followed their standard routines
for prediction.
The algorithms made a prediction after observing a new
skeletal configuration. The batch algorithm used a sliding
window of length n = 5, i.e. it recognized actions based on
the recent n observations. We chose the IDDM with a linear covariance function for the covariance function kz of the
measurement GP and a two-dimensional latent state space.
The batch algorithm achieved a precision of 83.8%, which
outperformed SVM (77.5%) and GPC (79.4%) using the
same sliding windows. The online algorithm achieved the

Wang et al.

855

(a) Forehand pose

(b) Middle pose

(c) Backhand pose

Fig. 7. Preparation poses of the three pre-defined hitting movements in the prototype system, i.e. (a) forehand, (b) middle, and (c)
backhand. The shadowed areas represent the corresponding hitting regions.

approx. 320ms
0.5

posterior

0.4

approx. 160ms

approx. 80ms (before hit)

backhand
middle
forehand

0.3
0.2
0.1
0

−0.2

0.4

X

−0.2

0.4

X

−0.2

0.4

X

Fig. 8. Bar plots show the distribution of the target (X coordinate) at approximately 320 ms, 160 ms, and 80 ms before the player hits
the ball. The prediction becomes more and more certain as the player finishes the stroke, and the robot later chose the middle hitting
movement accordingly.

precision of 83.0% with significantly reduced computation
time. We observed that both the SVM and GPC confused
crouching with jumping, as they were similar in the early
and late stages. In contrast, the IDDM could distinguish
between crouching (C) and jumping (J) from their different

dynamics, which became clearly separable while the human
performed the actions.
The batch algorithm needs to choose the size of sliding windows, which influences both the accuracy and efficiency. As shown in Table 3, the batch algorithm could yield

856

The International Journal of Robotics Research 32(7)

Table 3. Comparison of the accuracy and efficiency using different algorithms for the action recognition task. Here, n denotes the
size of sliding windows and  is the shrinking factor of the online
method.
Algorithm

Accuracy

Time (s)

SVM (n = 5)
GPC (n = 5)
Batch (n = 4)
Batch (n = 5)
Batch (n = 6)
Online ( = 0.3)
Online (
 = 0.2)
Online ( = 0.1)

77.5%
79.4%
79.0%
83.8%
83.0%
83.0%
83.0%
82.6%

<0.01
>1
0.27
0.32
0.39
0.07
0.07
0.07

real-time action recognition at a rate of 3 Hz with a sliding window of size 5. The online algorithm, as shown in
Table 3, achieved a speedup of over a factor of four compared with the batch algorithm with a sliding window. The
online algorithm relies on the shrinking factor  in Equation (52), which describes how likely the type of actions is
expected to change. We also found that the performance of
the online algorithm is not sensitive to this parameter.

7. Discussion
In this article, we have proposed the IDDM, a latentvariable model for inferring intentions from observed
human movements. We have introduced efficient approximate inference algorithms that allow for real-time inference. Our contributions include: (1) suggesting the IDDM,
which simultaneously finds a latent state representation of
noisy and high-dimensional observations and models the
dynamics that are driven by the intention; (2) introducing an online algorithm to efficiently infer the human’s
intention from an ongoing movement; (3) verifying the
proposed model in two HRI scenarios. In particular, we
have considered target inference in robot table tennis and
action recognition for interactive robots. In these two scenarios, we show that modeling the intention-driven dynamics can achieve better predictions than algorithms without
modeling the dynamics.
The proposed method outperformed the GPR in the robot
table tennis scenario and SVM and GPC in the action recognition scenario. Nevertheless, we would not draw the overstated conclusion that IDDM is a better model than SVM
or GP based on these empirical results, as this discussion
would be a comparison of generative and discriminative
models. The performance of IDDM and SVM/GP should
be studied on a case-by-case basis. However, two important properties of these approaches should be noticed: (1)
computational efficiency and (2) robustness to measurement noise. First, the IDDM is often more computationally
demanding than GP and SVM. Nevertheless, the proposed
online inference method, and described possible approximations, make the IDDM applicable to real-time scenarios.

As demonstrated in the prototype robot table tennis system,
the IDDM was successfully used in a real system with tight
time constraints. Second, the IDDM is generally less prone
to measurement noise than SVM/GP, as it models the noise
in the generative process of observations.
In conclusion, the IDDM takes into account the generative process of movements in which the intention is the
driving factor. Hence, we advocate the use of IDDM when
the movement is indeed driven by the intention (or target
to predict), as the IDDM captures the causal relationship of
the intention and the observed movements.
Notes
1. We use the short-hand notation adb|c where a = μ denotes
the mean μ and a =  denotes the covariance, b denotes
the time step of interest, c denotes the time step up to which
we consider measurements, and d ∈ {x, z} denotes either the
latent space (x) or the observed space (z).
2. Our methods allows the robot to initiate its movement at least
80 ms before the opponent hits the ball. As the ball can usually be reliably predicted more than 120 ms after the opponent
returns, the robot could gain more than 200 ms additional
execution time by using our prediction method.
3. See the video at http://robot-learning.de/Research/ Probabilis
ticMovementModeling
4. The reason is that the online algorithm only updates the
smoothing distribution q( xt−1:t ) instead of the entire smoothing distribution q( x1:T ), and, hence, reduces the time complexity by a factor of T, see Sections 3.3 and 4.3.
5. See the video at http://robot-learning.de/Research/ Probabilis
ticMovementModeling
6. See http://www.cs.ubc.ca/∼emtiyaz/software/catLGM.html

Funding
Part of the research leading to these results has received funding from the European Community’s Seventh Framework Programme (grant agreement numbers ICT-270327 [CompLACS]
and ICT-248273 [GeRT]).

Acknowledgment
We thank Abdeslam Boularias for valuable discussions on this
work.

References
Abbeel P and Ng A (2004) Apprenticeship learning via
inverse reinforcement learning. In: International Conference
on Machine Learning.
Anderson R (1988) A Robot Ping-Pong Player: Experiment in
Real-time Intelligent Control. Cambridge, MA: MIT Press.
Andrieu C, De Freitas N, Doucet A and Jordan M (2003) An introduction to MCMC for machine learning. Machine Learning 50:
5–43.
Baker C, Saxe R and Tenenbaum J (2009) Action understanding
as inverse planning. Cognition 113(3): 329–349.
Baker C, Tenenbaum J and Saxe R (2006) Bayesian models of
human action understanding. In: Advances in Neural Information Processing Systems.

Wang et al.

Bandyopadhyay T, Won KS, Frazzoli E, Hsu D, Lee WS and Rus
D (2012) Intention-aware motion planning. In: International
Workshop on the Algorithmic Foundations of Robotics.
Billingsley J (1984) Machineroe joins new title fight. Practical
Robotics (May/June): 14–16.
Bishop C (2006) Pattern Recognition and Machine Learning.
New York: Springer.
Bitzer S and Vijayakumar S (2009) Latent spaces for dynamic
movement primitives. In: IEEE-RAS International Conference
on Humanoid Robots. IEEE, pp. 574–581.
Chang CC and Lin CJ (2011) LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology 2: 27:1–27:27. Software available at
http://www.csie.ntu.edu.tw/∼cjlin/libsvm.
Damianou A, Titsias M and Lawrence N (2011) Variational
Gaussian process dynamical systems. In: Advances in Neural
Information Processing Systems.
Deisenroth M (2010) Efficient Reinforcement Learning using
Gaussian Processes. KIT Scientific Publ.
Deisenroth M, Huber M and Hanebeck U (2009) Analytic
moment-based Gaussian process filtering. In: International
Conference on Machine Learning.
Deisenroth M and Ohlsson H (2011) A general perspective on
Gaussian filtering and smoothing. In: American Control Conference.
Deisenroth M, Turner R, Huber M, Hanebeck U and Rasmussen
C (2012) Robust filtering and smoothing with Gaussian processes. Transactions on Automatic Control 57: 1865–1871.
Ding C and Peng H (2005) Minimum redundancy feature selection
from microarray gene expression data. Journal of Bioinformatics and Computational Biology 3: 185–205.
Fässler H, Beyer H and Wen J (1990) A robot ping pong player:
optimized mechanics, high performance 3D vision, and intelligent sensor control. Robotersysteme 6: 161–170.
Friesen A and Rao R (2011) Gaze following as goal inference:
A Bayesian model. In: Annual Conference of the Cognitive
Science Society.
Ghahramani Z and Roweis S (1999) Learning nonlinear dynamical systems using an em algorithm. In: Advances in Neural
Information Processing Systems.
Hauser K (2012) Recognition, prediction, and planning for
assisted teleoperation of freeform tasks. In: Proceedings of
Robotics: Science & Systems.
Huang Y, Xu D, Tan M and Su H (2013) Adding active learning to LWR for ping-pong playing robot. IEEE Transactions on
Control Systems Technology, accepted for publication.
Ijspeert A, Nakanishi J and Schaal S (2002) Movement imitation with nonlinear dynamical systems in humanoid robots. In:
IEEE International Conference on Robotics and Automation.
Jenkins O, Serrano G and Loper M (2007) Interactive human
pose and action recognition using dynamical motion primitives.
International Journal of Humanoid Robotics 4: 365–386.
Khan M, Mohamed S, Marlin B and Murphy K (2012) A stickbreaking likelihood for categorical data analysis with latent
Gaussian models. In: International Conference on Artificial
Intelligence and Statistics.
Ko J and Fox D (2009) GP-BayesFilters: Bayesian filtering
using Gaussian process prediction and observation models.
Autonomous Robots 27: 75–90.
Ko J and Fox D (2011) Learning GP-BayesFilters via Gaussian
process latent variable models. Autonomous Robots 30: 3–23.

857

Kuderer M, Kretzschmar H, Sprunk C and Burgard W (2012)
Feature-based prediction of trajectories for socially compliant
navigation. In: Proceedings of Robotics: Science & Systems.
Kurniawati H, Du Y, Hsu D and Lee W (2011) Motion planning
under uncertainty for robotic tasks with long time horizons. The
International Journal of Robotics Research 30: 308–323.
Lampert CH and Peters J (2012) Real-time detection of colored
objects in multiple camera streams with off-the-shelf hardware components. Journal of Real-Time Image Processing
7: 31–41.
Lawrence N (2004) Gaussian process latent variable models for
visualization of high dimensional data. In: Advances in Neural
Information Processing Systems.
Lawrence N (2005) Probabilistic non-linear principal component
analysis with Gaussian process latent variable models. The
Journal of Machine Learning Research 6: 1783–1816.
Liao L, Patterson D, Fox D and Kautz H (2007) Learning and
inferring transportation routines. Artificial Intelligence 171:
311–331.
Matsushima M, Hashimoto T, Takeuchi M and Miyazaki F (2005)
A learning approach to robotic table tennis. IEEE Transactions
on Robotics 21: 767–771.
Møller M (1993) A scaled conjugate gradient algorithm for fast
supervised learning. Neural Networks 6: 525–533.
Mülling K, Kober J, Kroemer O and Peters J (2013) Learning to
select and generalize striking movements in robot table tennis.
The International Journal of Robotics Research, accepted for
publication.
Mülling K, Kober J and Peters J (2011) A biomimetic approach to
robot table tennis. Adaptive Behavior 19: 359–376.
Pentland A and Liu A (1999) Modeling and prediction of human
behavior. Neural Computation 11: 229–242.
Quiñonero-Candela J, Girard A, Larsen J and Rasmussen C
(2003) Propagation of uncertainty in Bayesian kernel models—
application to multiple-step ahead forecasting. In: IEEE International Conference on Acoustics, Speech, and Signal Processing.
Quiñonero-Candela J and Rasmussen C (2005) A unifying view of
sparse approximate Gaussian process regression. The Journal
of Machine Learning Research 6: 1939–1959.
Ramanantsoa M and Durey A (1994) Towards a stroke construction model. International Journal of Table Tennis Science 2:
97–114.
Rao R, Shon A and Meltzoff A (2004) A Bayesian model of
imitation in infants and robots. In: Imitation and Social Learning in Robots, Humans, and Animals. Cambridge: Cambridge
University Press, pp. 217–247.
Rasmussen C and Williams C (2006) Gaussian Processes for
Machine Learning. Cambridge, MA: MIT Press.
Schölkopf B and Smola A (2001) Learning with Kernels: Support
Vector Machines, Regularization, Optimization, and Beyond.
Cambridge, MA: MIT Press.
Shotton J, Fitzgibbon A, Cook M, et al. (2011) Real-time human
pose recognition in parts from single depth images. In: IEEE
Conference on Computer Vision and Pattern Recognition.
Simon M (1982) Understanding Human Action: Social Explanation and the Vision of Social Science. State University of
New York Press.
Snelson E and Ghahramani Z (2006) Sparse Gaussian processes
using pseudo-inputs. In: Advances in Neural Information Processing Systems.

858

Turner R, Deisenroth M and Rasmussen C (2010) State-space
inference and learning with Gaussian processes. In: International Conference on Artificial Intelligence and Statistics.
van der Maaten L, Postma E and van den Herik J (2009) Dimensionality reduction: A comparative review. Journal of Machine
Learning Research 10: 1–41.
Vasquez D, Fraichard T, Aycard O and Laugier C (2008) Intentional motion on-line learning and prediction. Machine Vision
and Applications 19: 411–425.
Vasquez D, Fraichard T and Laugier C (2009) Growing hidden
Markov models: An incremental tool for learning and predicting human and vehicle motion. The International Journal of
Robotics Research 28: 1486–1506.
Wang J, Fleet D and Hertzmann A (2008) Gaussian process
dynamical models for human motion. IEEE Transactions on
Pattern Analysis and Machine Intelligence 30: 283–298.
Wang Y, Won K, Hsu D and Lee W (2012a) Monte Carlo
Bayesian reinforcement learning. In: International Conference
on Machine Learning.
Wang Z, Boularias A, Mülling K and Peters J (2011a) Balancing safety and exploitability in opponent modeling. In: AAAI
Conference on Artificial Intelligence.
Wang Z, Deisenroth M, Amor H, Vogt D, Schölkopf B and Peters
J (2012b) Probabilistic modeling of human movements for

The International Journal of Robotics Research 32(7)

intention inference. In: Proceedings of Robotics: Science and
Systems.
Wang Z, Lampert C, Mülling K, Schölkopf B and Peters J
(2011b) Learning anticipation policies for robot table tennis.
In: IEEE/RSJ International Conference on Intelligent Robots
and Systems.
Williams A, Ward P, Knowles J and Smeeton N (2002) Anticipation skill in a real-world task: Measurement, training,
and transfer in tennis. Journal of Experimental Psychology
8: 259.
Yang P, Xu D, Wang H and Zhang Z (2010) Control system design
for a 5-DOF table tennis robot. In: International Conference on
Control Automation Robotics & Vision, pp. 1731–1735.
Ziebart B, Dey A and Bagnell J (2012) Probabilistic pointing target prediction via inverse optimal control. In: ACM
International Conference on Intelligent User Interfaces, pp.
1–10.
Ziebart B, Maas A, Bagnell J and Dey A (2008) Maximum
entropy inverse reinforcement learning. In: AAAI Conference
on Artificial Intelligence, pp. 1433–1438.
Ziebart B, Ratliff N, Gallagher G, et al. (2009) Planning-based
prediction for pedestrians. In: IEEE/RSJ International
Conference
on
Intelligent
Robots
and
Systems,
pp. 3931–3936.

2016 IEEE International Conference on Automation Science and Engineering (CASE)
Fort Worth, TX, USA, August 21-24, 2016

Combining arm and hand metrics for sensible grasp selection
Ana Huamán Quispe

Heni Ben Amor

Henrik I. Christensen

Abstract— In this paper we propose an approach to robot
grasp prioritization based on a combined arm-and-hand metric.
Most traditional approaches evaluate grasps based on handcentric metrics such as force-closure, finger spread, contact
surface area and similar measures. While these are certainly
important factors to predict the robustness of a grasp, they
do not carry information on the feasibility of the reaching
action needed to execute the grasp. Based on our observations
of physical pick-up experiments, we suggest that the execution
success of a pick-up task is partially dependant on the easiness
of the reaching movement. We present our metric, which
combines 2 measures involving arm-kinematics and an existing
hand heuristic metric. Results of simulated as well as physical
experiments in our robot, Crichton, are presented.

I. I NTRODUCTION
Manipulation skills are of paramount importance for humans. They allow us to interact with objects and use them to
accomplish desired tasks, such as activities of daily living.
Unsurprisingly, a natural milestone in robotics is to achieve
manipulation skills comparable to these shown by humans.
Once we are able to endow a robot with the capacity of autonomously solving manipulation tasks as simple as picking
up objects, and when these solutions are indistinguishable
from the solutions that normal humans arrive at, then we
will be able to say that we truly understand how actions are
controlled [24].
In reality, there is not yet a definitive answer to how
humans plan the grasping of objects. Grasping, the most
elemental skill for manipulation, is complex itself due to
the fact that many factors come into consideration to select
a solution. In order to pick up a rock, a robot must answer
questions such as : From what direction should I approach
it? Should I grab it on that outcropping closest to my left?
Which posture will allow me to reach it? and so on [24].
For sequential tasks, such as pick-and-place, handover or
pouring, the factors further accumulate, with grasp selection
being just an intermediate step to accomplish the final goal.
In this work, we focus on the simplest type of tasks: Pick-up.
From the point of view of robotics, there exist a vast
amount of research devoted to manipulation. Interestingly,
most of the work tends to focus on either the hand-centric
side of manipulation (grasping) or on the arm-centric aspect
(arm trajectory planning). On the hand-centric area, the main
focus falls on grasp synthesis: Finding robust grasps that
allow to hold an object while being resistant to possible
disturbances. The candidate grasps are then ranked according
to metrics that measure desired characteristics such as forceclosure, or human-based heuristics [5]. No consideration is
given to the reach action that precedes the grasp itself. On the
Institute
for
Robotics
and
gia
Institute
of
Technology,

Intelligent
Machines,
Atlanta,
GA
30332,

GeorUSA.

ahuaman@us.toyota-itc.com, hbenamor@asu.edu,
hic@cc.gatech.edu
978-1-5090-2409-4/16/$31.00 ©2016 IEEE

Fig. 1: A simple pick-up task: Crichton must reach the
Pringles container and raise it from the table 10 cm. For this
particular example, the planner found 130 candidate grasps.
Our metric orders these grasps considering the arm comfort
and the closeness of the hand and the object’s center of mass.
The sample grasps shown above were rated the best, the
worst and the average candidates.
other hand, arm-centric approaches concentrate on finding
arm trajectories that allow the robot to reach an object. It
is either assumed that a feasible grasp pose will happen to
be reached [29], or that the grasp will be generated along
during arm planning based on heuristic-based or hard-coded
constraints [4, 30].
Focusing on only arm- or hand- aspects of a pick-up
task makes sense, but in reality, both aspects are deeply
intertwined. Human studies show that when humans select
grasps, they consider how comfortable their arms will be
once the task is finished [25]. Further, the grasp pose chosen
determines the arm trajectory to be followed. In simulation
experiments, the arm movement preceding the grasp itself
might not matter; however, in real environments, the reaching
action more often than not plays an important role: If an
optimal grasp requires a complicated arm pose to reach it,
then actuation inaccuracies can become more prominent,
affecting the final hand pose with respect to the object
and hence the finger contacts location. Also the grasp can
become more sensitive to perceptual errors (i.e. object pose).
Finally, and not less important, a hard-to-reach arm pose is
incompatible with how humans reach objects.
In this work, we introduce a combined measure approach
that takes into account both arm and hand heuristic metrics.
The work presented here is a continuation of our previous
study [15] in which we use superquadrics to represent unknown objects and generate grasps for them using a heuristic
approach. Namely, this paper presents 3 contributions: (1) A

1170

small-scale analysis of the force-closure properties of the
candidate grasps obtained using [15]. Our results suggest
that most of these grasps exhibit force-closure properties,
which make them good candidates for further tasks. (2)
Our proposed combined arm-hand metric. (3) Physical and
simulation results of pick-up tasks that show the advantages
of ranking grasps according to a metric considering the arm
into the grasp evaluation.
The rest of this paper is organized as follows: Section II
present relevant work in the area of grasp synthesis and grasp
selection. In Section III we define the pick-up task studied in
this paper and the assumptions and constraints considered.
In Section IV presents a summarized overview of the work
presented already in [15]. In particular, subsection IV-B
presents the force-closure analysis we mentioned. In Section
V we introduce our combined arm-hand metric to rank
the candidate grasps. Section VI shows the results of both
simulation and physical tests performed on 10 household
objects. We conclude this paper with section VII, where
we provide some discussion regarding future work, and the
advantages and shortcomings of our approach.
II. R ELATED W ORK
In this section we review work concerning grasp synthesis
and grasp selection. For a more detailed review of previous
research in the area, we suggest the interested reader to
consult the excellent reviews from Bohg [5] and Sahbani
[26].
Pioneering work on grasp selection was developed by
Cutkosky [7], who observed that humans select grasps in
order to satisfy 3 main types of constraints: Hand constraints,
object constraints and task-based constraints. As pointed out
by Bohg et al. in [5], there is little work on task-dependent
grasping when compared to work focused on the first two
constraints. Hence, the main goal for a planner is to find a
grasp such that the robot can approach the object and execute
the said grasp, without further regard of what the robot will
do once the object is picked.
Grasp generation methods vary widely depending on the
assumptions considered. In the case of grasp planning for
known objects, Ciocarlie et al. [6] presented the concept
of eigengrasps, which was exploited to generate candidate
grasps searching in a low dimensional hand posture space
using their GraspIt! simulator. Diankov generated grasps by
sampling the surfaces of object meshes and using the normals
at the sample points to guide the approach direction of the
hand [8]. Approaches using primitive representations were
also proposed such that the grasp generation depends on
the particular primitive characterization: Miller et al. [20]
proposed to use a set of primitive shapes (cylinder, box,
ball) to decompose complex objects. Huebner and Kragic
[16] used bounding boxes, Przybylski et al. proposed the
Medial Axis representation [22], Goldfeder et al. [12] used
superquadrics due to their versatility to express different
geometry types with only 5 parameters.
In all the cases mentioned, the grasps are generated offline
and stored in a database for future use. These grasps are
usually ranked based on their force-closure properties, which
theoretically express the robustness and stability of a grasp.

One of the most popular metrics (ǫ) was proposed by Ferrari
and Canny [9]. However, it has been noted by different
authors that analytical metrics do not guaranteee a stable
grasp when executed in a real robot. This can be explained
by the fact that these classical metrics consider assumptions
that don’t always hold true in real scenarios (dynamics,
perceptual and modelling inaccuracies, friction conditions).
On the other hand, studies that consider human heuristics
to guide grasp search have shown remarkable results, outperforming classical approaches. In [2], Balasubramanian
observed that when humans kinestetically teach a robot how
to grasp objects, they strongly tend to align the robotic hand
along one of the object’s principal axis, which later results
in more robust grasps. The author termed skewness to the
metric measuring the axis deviation. In [23], Przybylski et
al. combine the latter metric with ǫ and use it to rank grasps
produced with GraspIt!. Berenson et al. [3] proposed a score
combining 3 measures: ǫ, object clearance and the robot
relative position to the object.
In this work we are interested in manipulation of unknown
objects. Multiple approaches of this kind have flourished
during the last few years, particularly due to the advent
of affordable RGB-D sensors. Since the 3D information is
partial and noisy, classical approaches to grasp generation
cannot be directly used. Rather, most of the current work
uses heuristics to guide grasp generation based on local representation of the object geometry features (or global features
if the object shape is approximated). In [13], Hsiao et al.
use the bounding box of the object segmented pointcloud to
calculate grasp approach directions using a set of heuristics.
We should notice that for most of these approaches, their
effectiveness can only be verified empirically.
Finally, we stressed the importance of choosing a grasp
taking into account the arm kinematics as to encourage
grasps that are easily reachable. As we mentioned, usually the problem of grasp planning is considered isolated
from arm planning, although there are a few exceptions:
Vahrenkamp et al. proposed Grasp-RRT [30] in order to
perform both grasp and arm planning combined. In a similar
vein, Roa et al. also proposed an approach that solve both
problems simultaneously [10]. Both approaches focus on
reaching tasks. On the same vein, Berenson et al. proposed
the use of Task Space Regions [4] that allow to plan arm
movements while also searching grasps. However, the main
disadvantage of this approach is that the object needs to be
known beforehand and the task regions must be explicitly
defined by the user. Additionally, no preference can be
chosen among which grasp pose is better for the given task.
The focus of this paper is on enabling the robot to produce in
a short planning time simple, comfortable pick-up plans for
novel objects for which no previous information is required.
III. P ROBLEM D EFINITION AND A SSUMPTIONS
Given a robot manipulator R and a novel object O, R
must perform a pick-up task consisting on reaching O from
a given start pose w Ts within its workspace and transport it
a short distance above the table.
The upper left image on Figure 2 depicts the robot setup
to be used on the experiments. R uses its left 7-DOF arm

1171

RGBD sensor

Fig. 2: Manipulation planning pipeline: a partial point cloud of the object is first analyzed for symmetries and then turned
into a superquadric representation. Grasps at the object are generated and then prioritized according to our arm-hand metric.
If a grasp produces a feasible reach movement, then execution proceeds.
and the 3-fingered hand attached as end effector. R obtains
perceptual information from an Asus Xtion sensor mounted
on top of its torso. The table area that the robot can perceive
and that is reachable by the left arm is 50cm×60cm (approximately marked by the colored circles). For the experiments,
the pick-up tasks will be restricted to this area. The 10
household objects to be used are shown in the first row of
Figure 3. The robot has no previous information of these
objects.
A. Pipeline

IV. G RASP G ENERATION FOR U NKNOWN O BJECTS
A. Object Representation and Grasp Generation using Superquadrics (SQ)
As it was explained Section III, the perceptual input of
our robot consists of a one-view pointcloud of the object to
be picked up. In this paper we use our superquadric-based
approach, previously presented in [15] to quickly reconstruct
simple objects and generate a discrete set of candidate
grasps homogeneously distributed on the object’s surface.
Example of some common objects and their superquadric
approximations are shown in Figure 3.

The pipeline for our system is shown in Figure 2. An
object is set in front of the robot and it has to pick it up
from the table. The following steps happen:
1) The RGBD sensor captures the current pointcloud and
approximates the unknown object to a superquadric. A
table mesh is also generated.
2) The environmental information (object + table) is sent
to the planner. A set of candidate grasps is generated.
3) Our metric is calculated for each candidate grasp. The
grasps are stored in a queue ordered with respect to
their metric value.
4) The planner pops the grasp with the biggest metric in
the queue. If a feasible pick-up arm path is obtained,
then it is executed. Otherwise, the next grasp is tried.
This continues until a grasp with a feasible reach path
is popped.
Ideally, the planner should test no more than one grasp.
The goal of the metric is to maximize the probability that
the first grasp (with maximum metric value) is robust and
likely to produce a feasible arm path, preferably short and
planned in a small amount of time.
In Section IV we present a brief recap of steps 1 and
2, which have already been presented in [15]. We also
summarily present a study of the robustness of the grasps
obtained with our heuristic grasp generation approach. In
Section V we present our proposed metric (step 3) and how
it can be used to select the most adequate grasp (from the
candidate grasp set) for the pick-up task at hand.

Fig. 3: Examples of superquadrics with different shapes. A
variety of shapes can be represented using a small number
of parameters.
The candidate grasp generation, as explained in [15], is
standard: First, the object surface is sampled into points
and their corresponding normals. The normals are used as
approach directions for the robot hand, whereas the points
are used as the initial Tool Center Point. Finally, the fingers
closing direction is aligned to the smallest SQ principal axis.
Once this is done, the hand pose is completely defined with
respect to the object and the only left step is to close the
fingers till they contact with the object. If there is collision
with the environment, we retract the hand along the normal
and try again till the only collision detected happens between
the fingers and the object. If a grasp has less than 4 contact
points or has not a single IK solution, it is discarded,
otherwise it is stored.
B. Analysis of Grasp Robustness
The grasp generation approach from IV-A is a standard
approach that in practice produces good grasps. In this

1172

% of force-closure grasps

100
90
80
70
60
50
40
30
20
10
0

100 100 100 100 100 98.38 100
93.75 92

r
s
s
e
e
e
p
ll
it
Ba heez offe lushi ingle leane aisin te cu con
C P Pr e c
R hi low
C
t
l
i
W Ye
Wh

Fig. 4: % of candidate grasps with non-disturbance force
closure (ǫ > 0.001). Tests for 9 objects from database

Average of P (fc )

section, we seek to illustrate how “good” these grasps are
by using a force-closure metric.
Ideally, metrics seek to measure how robust a grasp is
in keeping the object in place when faced with external
disturbances. However, grasping is a dynamic process in
which many variables involved cannot be properly modelled (friction models, object model inaccuracies, dynamics
between the fingers and the object). Surveys of grasping
literature cite as many as 24 grasp metrics [11].
In this section we use the metric proposed in [31],
which measures force-closure under disturbances. The metric
consists on evaluating the ǫ metric proposed by Ferrari
and Canny [9] on a given grasp while introducing small
disturbances in the pose of the object to be grasped. The
metric is the ratio of force-closure grasps with respect to
the total number of disturbed grasps. As the authors clearly
state, this metric is computationally expensive to be applied
in online grasping. We use it here to get an idea of how
reliable are the grasps that the approach on Section IV-A
produces.
We evaluate the grasps generated for the 10 objects
considered. We implemented our approach inIV-A in our
simulator based on the DART dynamic library [18]. Once the
candidate grasps are generated, they are evaluated according
to [31]. We also present results of the grasps metrics based
on the simple ǫ metric, for comparison reasons. The metric
calculations were done using GraspIt! [19]. The average
results over the sets of candidate grasps per each object are
shown in Figure 4 and Figure 5. In average, the candidate
grasp sets range between 30-120 grasps, depending on the
object size and location. For the disturbed metric, we use a
disturbance of 1 cm on x,y and z and a rotation disturbance
of 5o in a random axis (this is similar to the original paper
experiment setup). It is worth noticing that in the original
paper, the authors apply the disturbance to the object and
then try to execute the grasp. If the fingers collide early
with the object, then a recovery procedure is attempted to
correct the grasp. In our case, we apply the disturbance to
the robot hand, not the object. Secondly, if an early collision
is detected, our system does not attempt to correct the grasp
but considers it a failure. We consider this to be a more
realistic assumption.
Results in Figure 4 show that according to the classical ǫ metric, the grasps generated from IV-A are mostly
force-closure grasps. Figure 5 shows the average results of
evaluating the candidate grasps using the disturbed metric.
We expect these values to be lower than those in Figure 4
since they consider perturbations, nonetheless high enough
that they guarantee that some resistance to pose error is
present. Indeed, Figure 5 shows average of force-closure
results in over 70% of the disturbed grasps. While these are
by no means exhaustive results, we can infer that for the
object tested, the candidate grasps obtained have a reasonable
likelihood of being robust.

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.87
0.82

0.79
0.72 0.72

0.69

0.68
0.65

0.49

r
s
s
e
e
e
p
ll
it
Ba heez offe lushi ingle leane aisin te cu con
C P Pr e c
R hi low
C
t
l
i
W
e
Y
Wh

Fig. 5: Mean and standard deviation of the P (fc ) metric
averaged for all the candidate grasps per each of 9 test
objects.
grasp in a random order until a solution is found. However,
arm planning can be a time-consuming process, particularly
when using sampling-based methods. It is therefore desirable
to first evaluate grasps that are more likely to produce a
solution. Since there is likely more than one solution in G,
it is preferable to choose grasps such that the solution is
quickly found. We propose to use a combination of arm and
grasp metrics to prioritize the grasps.
A. Arm Metric (ma )
When humans perform pick-up tasks, they select a grasp
such that their arm is comfortable at the end of the reaching
movement. This inherently simple phenomenon, known as
the end comfort effect, has been observed in adult humans
as well as in other primates [25].

V. G RASP P RIORITIZATION
Once a set of feasible grasps G is generated, paths for
reaching and picking up the object must be produced. A
brute-force approach would be to exhaustively try each

Fig. 6: Possible IK solutions for two grasps.
Our proposed arm-centric metric intends to capture the
comfort factor for a given grasp. For illustration purposes,

1173

please see Figure 6. It depicts a simulated version of the
robot arm grasping a cylinder in 2 poses. Since our arm
is redundant, there exist more than one way to execute
these grasps. Intuitively, the arm (human or robotic) is more
comfortable if it has high freedom of movement (i.e. If the
arm is free to do some “shaking” while keeping the hand in
the grasp pose, without colliding with the environment and
without hitting its joint limits).
Formally, for a given grasp g applied on an object located
at w To we define our arm metric as the number of collisionfree inverse kinematic solutions that allow the hand to
execute g.
(

qi is collision-free
F K(qi ) × (g.h To ) = w To
(1)
The IK solutions (Q) are calculated by discretizing the
redundant value [27], hence for every grasp, the same number
of possible IK solutions are generated. In a previous study
[14], we originally proposed that instead of just using the
number of inverse kinematic solutions, we use the average
of the manipulability [32] of these solutions:

ma (g) = |Q| such that ∀qi ∈ Q

|Q|

1 X
manipulability(qi )
|Q| i=1

(2)

However, upon performing physical experiments, we noticed that in general, |Q| is a more direct measurement of
how comfortable or easy to reach is a determined grasp pose.
If a grasp pose has a lower number of IK solutions, it is
expected that the manipulability of these arm configurations
will equally be low, while more solutions generally translate
to a higher freedom of movement.
B. Grasp Metric (mg )
The arm-centric metric presented above only considers the
arm comfort. Consider the scenario in Figure 7, where 3
candidate grasps are depicted for a cylindrical object. Let us
assume that these grasps have similar ma values, hence they
are all deemed equally desirable. From human experience,
we can all agree that the second grasp is the most likely to
be stable since the hand is closer to the center of mass of
the object being held. We incorporate this heuristic on the
proposed grasp metric.
Our second metric attempts to favor grasps that hold the
object near its center of gravity. We propose to quantify
this heuristic as the distance between the object’s center of
mass and the hand’s approach direction vector. We select
this metric because it is easy to calculate, as it is just the
distance between a line and a point. This metric is similar
to the existing metric B1 [17], which measures the distance
between the center of the contact polygon and the center of
mass of the object. We don’t use this metric mainly because
our system does not provide finger contact information.
C. Arm + Grasp Metric
Now that we have both metrics, we must use them in a
meaningful way. A direct way to do this could be using a
weighted sum of both. However, both metrics have different

Fig. 7: Examples of similar grasps with a different distance
from the hand approach direction and the object center of
mass
units (ma is adimensional and mg has length units), hence
adding them does not have a real meaning. We order the
candidate grasps in the following steps.
• Calculate the mean µ and the standard deviation σ of
the ma metric over all the candidate grasps.
• Divide the grasps in 4 groups, in a similar manner as
in [17]:
1) Very good ma quality: ma (g) > µ + σ
2) Good ma quality: µ < ma (g) < µ + σ
3) Fair ma quality: µ − σ < ma (g) < µ
4) Bad ma quality: ma (g) < µ − σ
• Within each of the 4 groups, order the grasps according
to mg .
• The final ordered set of grasps will contain 4 ma -based
ordered sets (very good, good, fair and bad), inside each
of which grasps are ordered according to mg .
VI. E XPERIMENTS AND R ESULTS
In Section V we presented our metric combining arm and
grasp measurements. We hypothesize that the grasps ranked
highly were more likely to: (1) Produce a feasible arm plan.
(2) The arm plan would take less time with respect to the
other candidates since the arm end-pose was considered in
the metric. (3) The produced plan would likely be successful
in real-execution.
In subsection VI-A we evaluate the first two hypothesis
and present comparative results for the 9 objects considered.
In subsection VI-B we present results of experiments that
validate hypothesis (3).
A. Simulation Results
The goal of the simulation tests is to evaluate how effective
our metric is in prioritizing grasps that (1) Produce a feasible
reach plan and (2) Plan the said reach plan in a relative short
time with respect to the other candidates.
The simulation tests were conducted as follows: For each
of the 10 objects considered, we generated 100 randomized
pick-up tests. In each test, the object was placed randomly on
the table within the workspace area of the robot’s left arm.
Our planning module generated a set of candidate grasps,
and ranked them according to the metric presented. Then, in
order to obtain comparative results, the best and the worst
grasp -according to our metric - were passed to the path
planner to generate the reaching and pick-up path necessary
to accomplish the task. Finally, 3 statistics of the plans were
obtained:

1174

Success rate: If the grasp selected produced a feasible
reach plan.
• Planning time: Time taken to generate a path, if it exists.
The kinematic simulation was performed using the DART
library [18]. The results are shown in Table I.
•

Slippage: The object slipped from the hand’s reach
while the fingers were closing (yellow cone).
• The hand knocked up the object while trying to reach
it (white cup).
TABLE II: Results of grasp selection using the proposed
metrics for 10 household objects
•

TABLE I: Simulation results of 100 randomized scenarios
per each object
Object
1.
2.
3.
4.
5.
6.
7.
8.
9.

Pringles
Cheezit
Coffee
White cup
Ball
Plushie
Raisins
White cleaner
Yellow cone

Success
Best
Worst
84.2%
91%
100%
84%
100%
100%
99%
99%
100%

52.6%
31%
20.7%
54%
12%
3%
47%
35%
46%

Object
1.
2.
3.
4.
5.

Plan time(s)
Best Worst
0.88
1.31
0.88
0.88
0.77
0.83
0.83
0.93
0.83

4.62
1.33
1.70
5.46
10.37
0.84
6.64
1.84
2.65

Pringles
Cheezit
Coffee
White cup
Ball

Success rate
10/10
9/10
9/10
7/8
8/10

Object
6. Plushie
7. Raisins
8. White cleaner
9. Yellow cone
10. Milk

Success rate
8/10
10/10
9/10
9/10
8/10

Additional videos of the robot experiments are available at http://www.cc.gatech.edu/˜ahuaman3/
manip.html

We draw the following conclusions:
• Selecting the best grasp according to our metric in
general yields feasible solutions in more than 90% of
the cases. This means that by using our metric, in
90% of the time, the first grasp selected will produce
a solution, avoiding having to try additional grasps. In
contrast, the grasps qualified as the worst present lower
success rates.
• The planning times for the best grasps according to our
metric are mostly shorter than the planning times for
the worst grasps in factors bigger than 2 for most of
the cases.
The simulation results suggest that using our metric help
select a grasp that in turn will produce a path that is generated
in a short period of time and will allow the arm to finish in
a comfortable configuration.

Fig. 8: Grasping results on Pringles: 10 / 10 successes

B. Experimental Results
The simulation results are illustrative in the sense that they
allow us to compare the ideal performance of the grasps
generated in the absence of perception errors, actuation
inaccuracies and dynamic effects between the hand and
the object (i.e. slippage, gravity effect after picking-up). To
evaluate how well the grasps selected by our metric worked,
we performed a small-scale set of physical experiments
with the 10 objects shown in Figure 3. We performed 10
experiments per each object, consisting on positioning the
evaluated object in 9 fixed locations on the table (colored
markers) and one random final location. Similarly to the
simulation experiments, the object was rotated randomly.
The experiments outcome is shown in Table II and in
Figures 8-12. As it can be observed, for most of the objects
the success rate is consistently higher than 80%, which
according to [1] is considered a realistic success threshold
for automatically generated grasps assuming ideal laboratory
conditions. The failures we observed (9 out of 98 grasps)
were mostly caused by the following causes:
• Inexact modelling: The superquadric approximation had
considerable error in certain poses where the pointcloud
was not dense enough (ball).

Fig. 9: Grasping results on ball: 8/10 successes

Fig. 10: Grasping results on white cleaner: 9/10 successes
VII. C ONCLUSION
In this paper, we propose the approach of selecting
grasps for pick-up actions considering arm- and grasp-centric
metrics. Given a previously unknown object, the method
synthesizes online a grasp that allows a comfortable final
arm pose. The novelty of this approach resides neither in the
arm metric nor in the grasp measure but in the combination

1175

Fig. 11: Grasping results on milk carton: 8/10

Fig. 12: Grasping results on raisins: 10/10 successes
of both and in the recognition that for real scenarios, a
strategy that considers both factors is important to increase
the probability of a successful execution. We also showed
simulation results that suggest that using a standard heuristic
approach to generate candidate grasps indeed produce grasps
that can be expected to be robust, based on an existing metric
that considers 3D pose inaccuracies.
The paper presented here shows only a simple application
of our metric used for pick-up tasks. Our work further
investigates the utility of our approach for slightly more
complex tasks such as pick-and-place and handovers. From
preliminary results, we expect our metric to be equally useful
in these cases. This is the subject of some current work not
addressed in this document due to space constraints.
R EFERENCES
[1] R. Balasubramanian, L. Xu, P. Brook, J. Smith, and Y. Matsuoka.
Physical human interactive guidance: Identifying grasping principles
from human-planned grasps. Transactions on Robotics, (99):1–12,
2012.
[2] R. Balasubramanian, L. Xu, P. D Brook, J.R. Smith, and Y. Matsuoka.
Physical human interactive guidance: Identifying grasping principles
from human-planned grasps. In The Human Hand as an Inspiration
for Robot Hand Development. Springer, 2014.
[3] D. Berenson, R. Diankov, K. Nishiwaki, S. Kagami, and J. Kuffner.
Grasp planning in complex scenes. In 7th IEEE-RAS Int. Conf. on
Humanoid Robots, 2007.
[4] D. Berenson, S. Srinivasa, and J. Kuffner. Task space regions: A framework for pose-constrained manipulation planning. The International
Journal of Robotics Research, page 0278364910396389, 2011.
[5] J. Bohg, A. Morales, T. Asfour, and D. Kragic. Data-driven grasp
synthesis: A survey. IEEE Transactions on Robotics, 2014.
[6] M. Ciocarlie, C. Goldfeder, and P. Allen. Dimensionality reduction for
hand-independent dexterous robotic grasping. In IEEE/RSJ Int. Conf.
on Intelligent Robots and Systems (IROS), pages 3270–3275, 2007.
[7] M.R. Cutkosky. On grasp choice, grasp models, and the design of
hands for manufacturing tasks. IEEE Transactions on Robotics and
Automation, 1989.
[8] R. Diankov and J. Kuffner. Openrave: A planning architecture for
autonomous robotics. Robotics Institute, Pittsburgh, PA, CMU-RI-TR08-34, 79, 2008.
[9] C. Ferrari and J. Canny. Planning optimal grasps. In IEEE Int. Conf.
on Robotics and Automation (ICRA), 1992.

[10] J. Fontanals, B.A. Dang-Vu, O. Porges, J. Rosell, and M. Roa. Integrated grasp and motion planning using independent contact regions.
14th IEEE-RAS Int. Conf. on Humanoid Robots (Humanoids), 2014.
[11] A. Goins, R. Carpenter, W. Wong, and R. Balasubramanian. Evaluating
the efficacy of grasp metrics for utilization in a gaussian process-based
grasp predictor. In International Conference on Intelligent Robots and
Systems (IROS), pages 3353–3360. IEEE, 2014.
[12] C. Goldfeder, P.K. Allen, C. Lackner, and R. Pelossof. Grasp
planning via decomposition trees. In IEEE Int. Conf. on Robotics
and Automation (ICRA), pages 4679–4684, 2007.
[13] K. Hsiao, S. Chitta, M. T. Ciocarlie, and E. G. Jones. Contact-reactive
grasping of objects with partial shape information. In IEEE/RSJ Int.
Conf. on Intelligent Robots and Systems (IROS), volume 99, page 124,
2010.
[14] A. Huamán Quispe, H. Ben Amor, H. Christensen, and M. Stilman.
Grasping for a purpose: Using task goals for efficient manipulation
planning. arXiv preprint arXiv:1603.04338, 2016.
[15] A. Huamán Quispe, B. Milville, MA. Gutiérrez, C. Erdogan, M. Stilman, HI. Christensen, and H. Ben Amor. Exploiting symmetries and
extrusions for grasping household objects. IEEE Int. Conf. on Robotics
and Automation (ICRA)(to appear), 2015.
[16] K. Huebner and D. Kragic. Selection of robot pre-grasps using boxbased shape approximation. In IEEE/RSJ Int. Conf. on Intelligent
Robots and Systems (IROS), 2008.
[17] B. León, C. Rubert, J. Sancho-Bru, and A. Morales. Characterization
of grasp quality measures for evaluating robotic hands prehension. In
International Conference on Robotics and Automation (ICRA), pages
3688–3693. IEEE, 2014.
[18] C.K. Liu and S. Jain. A Short Tutorial on Multibody Dynamics.
Technical Report GIT-GVU-15-01-1, Georgia Institute of Technology,
School of Interactive Computing, 08 2012.
[19] Andrew T Miller and Peter K Allen. Graspit! a versatile simulator for
robotic grasping. Robotics & Automation Magazine, IEEE, 11(4):110–
122, 2004.
[20] A.T. Miller, S. Knoop, H.I. Christensen, and P.K. Allen. Automatic
grasp planning using shape primitives. In IEEE Int. Conf. on Robotics
and Automation,(ICRA), 2003.
[21] M. Pilu and R.B. Fisher. Equal-distance sampling of superellipse
models. University of Edinburgh, Department of Artificial Intelligence,
1995.
[22] M. Przybylski, T. Asfour, and R. Dillmann. Unions of balls for shape
approximation in robot grasping. In IEEE/RSJ Int. Conf. on Intelligent
Robots and Systems (IROS), 2010.
[23] M. Przybylski, T. Asfour, R. Dillmann, R. Gilster, and H. Deubel.
Human-inspired selection of grasp hypotheses for execution on a
humanoid robot. In 11th IEEE-RAS Int. Conf. on Humanoid Robots,
2011.
[24] D. Rosenbaum, R. Cohen, R. Meulenbroek, and J. Vaughan. Plans for
grasping objects. In Motor control and learning, pages 9–25. Springer,
2006.
[25] D. A. Rosenbaum, C. M. van Heugten, and G. E. Caldwell. From
cognition to biomechanics and back: The end-state comfort effect and
the middle-is-faster effect. Acta psychologica, 94(1):59–85, 1996.
[26] A. Sahbani, S. El-Khoury, and P. Bidaud. An overview of 3d
object grasp synthesis algorithms. Robotics and Autonomous Systems,
60(3):326–336, 2012.
[27] M. Shimizu, H. Kakuya, W. Yoon, K. Kitagaki, and K. Kosuge.
Analytical inverse kinematic computation for 7-DOF redundant manipulators with joint limits and its application to redundancy resolution.
Transactions on Robotics, 24(5):1131–1142, 2008.
[28] F. Solina and R. Bajcsy. Recovery of parametric models from range
images: The case for superquadrics with global deformations. Transactions on Pattern Analysis and Machine Intelligence, 12(2):131–147,
1990.
[29] F. Stulp, E. Theodorou, and S. Schaal. Reinforcement learning with
sequences of motion primitives for robust manipulation. Transactions
on Robotics, 28(6):1360–1370, 2012.
[30] N. Vahrenkamp, T. Asfour, and R. Dillmann. Simultaneous grasp and
motion planning: Humanoid robot armar-iii. Robotics & Automation
Magazine, 2012.
[31] J. Weisz and P.K. Allen. Pose error robust grasping from contact
wrench space metrics. In International Conference on Robotics and
Automation (ICRA), pages 557–562. IEEE, 2012.
[32] T. Yoshikawa. Manipulability of robotic mechanisms. The International Journal of Robotics Research, 4(2):3–9, 1985.

1176

The 23rd IEEE International Symposium on Robot and
Human Interactive Communication
August 25-29, 2014. Edinburgh, Scotland, UK,

Dynamic Mode Decomposition for Perturbation Estimation
in Human Robot Interaction
Erik Berger1 , Mark Sastuba2 , David Vogt1 , Bernhard Jung1 , Heni Ben Amor3
Abstract— In many settings, e.g. physical human-robot interaction, robotic behavior must be made robust against more or
less spontaneous application of external forces. Typically, this
problem is tackled by means of special purpose force sensors
which are, however, not available on many robotic platforms.
In contrast, we propose a machine learning approach suitable
for more common, although often noisy sensors. This machine
learning approach makes use of Dynamic Mode Decomposition
(DMD) which is able to extract the dynamics of a nonlinear
system. It is therefore well suited to separate noise from regular
oscillations in sensor readings during cyclic robot movements
under different behavior configurations. We demonstrate the
feasibility of our approach with an example where physical
forces are exerted on a humanoid robot during walking. In
a training phase, a snapshot based DMD model for behavior
specific parameter configurations is learned. During task execution the robot must detect and estimate the external forces
exerted by a human interaction partner. We compare the DMDbased approach to other interpolation schemes and show that
the former outperforms the latter particularly in the presence
of sensor noise. We conclude that DMD which has so far
been mostly used in other fields of science, particularly fluid
mechanics, is also a highly promising method for robotics.

Fig. 1: A NAO robot detects the existence and amount of
external perturbations applied by a human interaction partner.
The direction and amount of the perturbation is used to infer
human guidance, e.g., walk backwards.

I. I NTRODUCTION
Robots need accurate and efficient sensing capabilities in
order to react to influences from the environment. This is
particularly true for robots that are engaging in joint physical
activities with a human partner. In such scenarios, forces
and torques applied by the human can severely perturb the
execution of a motor skill and need to be accounted for in the
decision making process. In order to appropriately respond
to a perturbation, a robot needs to detect both the occurrence
of such an event as well as the degree by which it occurred.
One way of implementing such detection is to use readings
from a special purpose sensor, e.g., force-torque sensor, along
with a thresholding method. However, such sensors are often
heavy, expensive and prone to error. In practice many sensors
return non-zero readings even when the robot merely moves.
Distinguishing between external, human perturbations and
natural variation in the sensor values can therefore become
a challenging task.
In this paper, we present a machine learning approach to
robot sensing that is well suited for identifying external
influences caused by a human partner as shown in Figure 1.

The approach focuses on learning probabilistic, behaviorspecific models of regular oscillations in sensor readings
during motor skill execution. These models are used to (1)
identify perturbations by detecting irregularities in sensor
readings that cannot be explained by the inherent noise,
and (2) to generate a continuous estimate of the amount of
external perturbation. Due to the data-driven nature of the
approach, no detection threshold needs to be provided by
the user. The presented perturbation filter can be regarded as
a virtual force sensor that produces a continuous estimate of
external forces. To this end, we use Sparse Dynamic Mode
Decomposition to learn a model of the system dynamics
during the robot execution of a specific motor skill. During
human-robot interaction, the model is then used to determine
the existence and amount of irregularities in the sensor
readings. By modeling the correlations as well as the timedependent variation in the original sensor values, our filter
can robustly deal with uncertainties in estimating the human
physical influence on the robot. During task execution, the
estimated perturbation value can be used to compensate for
the external forces or infer the intended guidance of a human
interaction partner. Experiments on a real robot show that

1 Institute of Computer Science, Technical University Bergakademie
Freiberg, Bernhard-von-Cotta-Str. 2, 09599 Freiberg, Germany
2 Institute of Mechanics and Fluid Dynamics, Technical University
Bergakademie Freiberg, Lampadiusstr. 4, 09599 Freiberg, Germany
3 Institute for Robotics and Intelligent Machines, Georgia Institute of
Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA

978-1-4799-6765-0/14/$31.00 ©2014 IEEE

593

Perturbation Filter

External Perturbation

Perturbation Detection

Reaction

Perturbation Estimation
Prediction

Sensor
Data

Behavior
Behavior
Parameter
Parameter

Costs

Perturbation
Value

Evaluation

Fig. 2: An overview of the presented machine learning approach. An external perturbation is filtered by using a previously
learned predictive model of behavior parameters. After detecting a perturbation the strength and direction is estimated in
the behavior parameter space. The resulting perturbation value can be used for an adequate reaction.

learned models can be used to accurately determine even
small disturbances.

of a human partner in close-contact interaction scenarios.
The parameters of the interaction model are updated using
binary evaluation information obtained from the human. The
approach significantly improves physical interactions, but is
limited to learning timing information.
Stückler et al. [10] present a cooperative transportation
task where a robot follows the human guidance using arm
compliance. In doing so, the robot recognizes the desired
walking direction through visual observation of the object
being transported. A similar setting has been investigated by
Yokoyama et al. [11]. They use a HRP-2P humanoid robot
equipped with a biped locomotion controller and an aural
human interface to carry a large panel together with a human.
Forces measured with sensors on the wrists are utilized to
derive the walking direction. Similarly, Bussy et al. [12] also
use force-torque sensors on the wrists to adapt the robot
behavior during object transportation tasks. Lawitzky et al.
[13] also shows how load sharing and role allocation can be
used to balance the contribution of each interaction partner
depending on the current situation.
The main drawback of the above approaches is that they
require special aural and visual input devices or force sensors
which are not present on many robot platforms. Additionally,
none of the approaches using force-torque sensors addresses
the problem of uncertainty in the provided measurements.
As a result, all of these approaches assume high-quality
sensing capabilities and low-speed execution of the joint
motor task. In contrast to the above approaches, we propose
a new filtering algorithm that can learn the natural variation
in sensor values during a motor skill. Using predictive models learned by Dynamic Mode Decomposition, the filtering
algorithm also estimates the perturbation which best explains
an observed set of new sensor values.

II. R ELATED W ORK
A popular approach to the design of human-robot interaction (HRI) is the use of mediating artefacts, such as
pendants, joysticks or mobile phones [1]. The approach
allows a programmer to pre-specify a set of tasks, commands
and corresponding robot reactions. Since communication is
mediated through the artefact, no filtering or interpretation
of the human commands is required.
In recent years, more natural and intuitive approaches to HRI
have gained popularity. Various researchers have proposed
the so-called soft robotics paradigm: compliant robots that
“can cooperate in a safe manner with humans” [2]. An important robot control method for realizing such a compliance
is impedance control [3]. Impedance control can be used
to allow for touch based interaction and human guidance.
To this end, impedance controllers require accurate sensing
capabilities, in the form of force-torque sensors. However,
such sensors are typically heavy, expensive and suffer from
noise. Other sensors, such as current based torque sensors
are even more prone to issues related to noise and drift.
Still, the ability to sense physical influences is at the core of
recent advances made in the field of HRI. For example, Lee
et al. [4] use impedance control and force-torque sensors in
order to realize human-robot interaction during programming
by demonstration tasks. Wang et al. [5] present a robot that
can adapt its dancing steps based on the external forces
exerted by a human dance partner. Ben Amor et al. [6] uses
touch information to teach new motor skills to a humanoid
robot. Touch information is therefore only used to collect
data for subsequent learning of a robotic motor skill. Robot
learning approaches that are based on such kinesthetic teachin have gained considerable attention in the literature, with
similar results reported in [7] and [8]. A different approach
aiming at joint physical activities between humans and robots
has been reported in [9]. Ikemoto et al. use Gaussian mixture
models to adapt the timing of a humanoid robot to that

III. APPROACH
In our approach the robot recognizes and automatically
differs between strength and direction of external perturbations which may be caused by a human interaction partner.
An overview of the approach can be seen in Figure 2. First,
594

we record training data for a behavior with different parameter configurations, e.g. walking with varying step lengths,
in a controlled environment without external perturbations.
The training data is used to learn a dynamic model utilizing
a state of the art interpolation method from fluid dynamics
(DMD).
During behavior execution an external perturbation is detected by comparing the recorded training data with the
current sensor data. For estimating the perturbation value the
current sensor readings are compared to new sensor values
generated from the learned model. The perturbation value
is then calculated from the difference between the current
behavior parameter and the behavior parameter of the sensor
characteristic with the highest compliance to the current
sensor characteristic.
In the following, we will address each step of our approach in
more detail. Subsequently, we will describe how perturbation
detection, model learning and perturbation estimation are
realized in order to allow a whole variety of HRI scenarios.

a*

b*
Y
X

Fig. 3: Given the recorded data (black) and the partial
observation (red), we calculate the optimal warping path p∗
between a∗ and b∗ .

subsequence X. This technique is also known as subsequence
dynamic time warping (SDTW) [15]. To find the optimal
subsequence we first have to calculate the accumulated cost
matrix D, which for SDTW is defined as

A. Recording Training Data
The first step in our approach is to record training data
that reflects the evolution of sensor values during regular
execution of a motor skill. It is important to record several
executions of the behavior, since motor skills can often be
executed with different parameters, e.g., varying step lengths
during walking. However, since we use machine learning
methods, we will later see that the number of required
training data can be limited to about five examples.
Each recorded example contains training data sampled with
100Hz for one repetition of the modelled robot behavior. In
our specific case of training a perturbation filter for walking,
we record both the center of mass (CoM) and the proper
acceleration of the robot for four seconds. Acquiring training
data requires less than one minute in total.

D(n, 1) =

c(xk , y1 ), n ∈ [1 : N ],

D(1, m) = c(x1 , ym ), m ∈ [2 : M ],
D(n, m) = min{D(n − 1, m − 1), D(n − 1, m),
D(n, m − 1)} + c(xn , ym )
where c is a local distance measure, which in our case is
defined as c = |x − y|. The goal of the SDTW algorithm is
to determine the path with minimal overall costs C ending
at (b∗ , M ), where b∗ is given by
b∗ = argmin D(N, b).

(2)

b∈[1:M ]

To determine the warping path p∗ = (p1 , . . . , pL ) starting
at p1 = (a∗ , 1) and ending at pL = (b∗ , M ) a dynamic
programming recursion is used. As illustrated in Figure 3 the
resulting path p∗ represents the optimal subsequence of X in
Y. As a result SDTW can be used to estimate the current state
of a behavior using a subset of temporally measured sensor
values which are mapped to the recorded data. In more detail,
we use the subsequence p∗ as prediction of sensor values at
the current state.

Since we are dealing with time-varying data, it is important
to estimate the phase of the robot during the execution of
a motor skill. Depending on the phase, e.g., the left leg
is lifted, the variance in the sensor readings can change
drastically. To determine the current phase, a time window
of sensor values is captured and temporally aligned to the
training data. To this end, we use the dynamic time warping
technique (DTW) [14]. DTW is a time series alignment
algorithm for measuring the similarity between two temporal
sequences X = (x1 , . . . , xN ) and Y = (y1 , . . . , yM ) of
length N ∈ N and M ∈ N. In our specific case, the goal is to
find the optimal correspondence between the sensor data Y
recorded during the training phase and the currently observed
sequence X, where M is much larger then N .
Due to this significant difference in length of X and Y, we
formulate our task as finding a subsequence
Y(a∗ : b∗ ) = (ya∗ , ya∗ +1 , . . . , yb∗ )
∗

n
X
k=1

B. Phase Estimation

∗

p*

C. Perturbation Detection
Due to uncertainties in the real world, a motor skill is never
twice executed in exactly the same way. To accommodate for
such natural noise in the behavior, we use learned, behaviorspecific information about the temporal evolution of sensor
variances.
Different approaches can be used to learn such a probabilistic
model. One solution is to use Gaussian Process Regression
(GPR) [16]. An important advantage of GPR is the ability
to learn a probabilistic model from a small set of training

(1)

∗

with 1 ≤ a ≤ b ≤ M , where a is the starting index and
b∗ is the end index that optimally fits to the corresponding
595

Prediction

Measurement

knowledge of A is not required for the following variant:
xN = a0 x0 +a1 x1 +· · ·+aN −1 xN −1 +r. The final snapshot
xN can be expressed as a linear combination of the previous
ones [x0 , . . . , xN −1 ] by computing the weighting factors
[a0 , . . . , aN −1 ], considering the residual r is minimized in
a least squares sense, to form the companion matrix


0
a0
1 0
a1 



..  ∈ CN ×N .
.
.
.. ..
(3)
S=
. 



1 0 aN −2 
1 aN −1

Sensor Value

Standard Deviation

0

10

20

30

40

50

0

10

20

30

40

50

Time Step [1/100s]

Fig. 4: After estimating the current phase of the behavior
the deviation between the measured and predicted sensor
values can be used to detect external influences. Left: There
is no external perturbation. Right: An external perturbation
is detected.

In [17] the author describes a more robust solution, which is
achieved by applying a singular value decomposition on K1
such that K1 = U ΣW ∗ . The full-rank matrix S̃ ∈ CN ×N
is determined on the subspace spanned by the orthogonal
basis vectors U of K1 , described by S̃ = U ∗ K2 W Σ−1 .
Solving the eigenvalue problem S̃µ = λµ leads to a subset
of complex eigenvectors µ. The DMD modes are defined
by Φ = U µ, which implies a mapping of the eigenvectors
µ ∈ CN ×N from a lower dimensional space to a higher
dimensional space CM ×N . The complex eigenvalues λ contain growth/decay rates δ = <[log(λ)]/∆t and frequencies
f = =[log(λ)]/(2π∆t) of the corresponding DMD modes
Φ. The temporal behavior of the DMD modes is contained
in the Vandermonde matrix Vand , which is formed by

−1 
1 λ1 · · · λN
1
1 λ2 · · · λN −1 
2


(4)
Vand =  .
..  .
..
..
 ..
.
. 
.
−1
1 λN · · · λN
N

data. The main drawback of this approach is the large computational effort. Another, computationally less expensive
solution is to compute the standard deviation σ for each time
step of the recorded data separately.
Given a probabilistic model as described above, we can
detect a perturbation by calculating the likelihood of the
current sensor readings. In our implementation, we trigger a
detection when the sensor values are outside of the computed
standard deviation σ. Figure 4 shows an example for a
regular and a disturbed execution of a behavior.
D. Modelling Robot Dynamics using Dynamic Mode Decomposition
In this section we use Dynamic Mode Decomposition
(DMD) to learn a predictive model describing the change
in sensor values under different behavior parameters. DMD
is a novel data processing technique from fluid dynamics and
was introduced in [17] and [18]. Once a DMD is learned,
it can be applied to simulated sensor values under different
parameter conditions.
DMD presents a modal decomposition for nonlinear flows
and features the extraction of coherent structures with a
single frequency and growth/decay rate. It computes a linear
model which approximates the underlying nonlinear dynamics. Given is an equidistant snapshot sequence N + 1
of an observable x = (u1 , . . . , uM )∗ ∈ CM ×1 which is
stacked into two matrices K1 = [x0 . . . , xN −1 ] ∈ CM ×N
and K2 = [x1 . . . , xN ] ∈ CM ×N . The matrices K1 and
K2 are shifted by one time step ∆t and can be linked
via the mapping matrix (system matrix) A ∈ CM ×M such
that K2 = AK1 = K1 S. Since the data stem from
experiments, the system matrix A is unknown and for a very
large system it is computationally impossible to solve the
eigenvalue problem directly as well as to fulfill the storage
demand [19]. The idea is to solve an approximate eigenvalue
problem by projecting A onto an N -dimensional Krylow
subspace and to compute the eigenvalues and eigenvectors
of the resulting low-rank operator as described in [20]. One
type of Krylow methods is the Arnoldi algorithm and the

The DMD modes Φ must be scaled in order to perform a data
recalculation of the first snapshot sequence K1 = ΦDα Vand .
Therefore, having a look into Vand shows that the first
snapshot x0 is independent from temporal behavior since
λ = [λ1 , . . . , λN ] = 1. The scaling factors α = [α1 . . . αN ]∗
are calculated by ΦDα = x0 , where Dα = diag{a}.
A new solution to find the scaling vectors α was introduced
in [21]. Here, α is obtained by considering the temporal
growth/decay rates of the DMD modes in order to approximate the entire data sequence K1 optimally. Therefore the
problem can be brought into the following form
2

min J(α) = |ΣW ∗ − µDα Vand |F
α

(5)

which is a convex optimization problem. Its solution leads
to
∗ ))−1 diag(V
∗
α = ((µ∗ µ) ◦ (Vand Vand
and W Σ µ)

(6)

where the over line denotes the complex-conjugate of a vector/matrix. However, the key challenge is to identify a subset
of DMD modes that captures the most important dynamic
structures in order to achieve a good quality approximation.
To solve that problem, the sparsity-promoting dynamic mode
decomposition (SDMD) [21] was developed. The sparsity
structure of the vector of amplitudes α is fixed in order
596

Standard Deviation

Prediction

Measurement

4

6

8

10

12

Sensor Value

−3

C

Time Step [1/100s]

400

300

200

100

−4
100

200

300

400

Time Step [1/100s]

500

α

N
X

|αi | ,

Time Step [1/100s]

(7)

2

4

300

200

100

−4

i=1

0

Interpolated CoM

400

to determine the optimal values of the non-zero amplitudes.
Therefore the objective function J(α) is extended with an
additional term such that
min J(α) + γ

−2

600

Fig. 5: External perturbations which differ in strength and
direction are increasing the overall warping costs C during
behavior execution.

x 10

Trained CoM

−2

0

2

4

Step Length [cm]

where γ denotes a regularization parameter that focuses
on the sparsity of the vector α. As a result, instead of
considering only the modes with largest amplitudes, the
sparsity-promoting DMD aims to identify the modes that
have the strongest influence on the entire time sequence.
The lower the number of non-zero amplitudes, the more the
sparse-promoting DMD concentrates on the low-frequency
modes. As already mentioned, the data presented here stems
from low cost sensors which may be affected by disturbance.
Hence, forcing a low number of non-zero amplitudes in α
can reduce the influence of noise in the approximation.
For our implementation of DMD in a human robot interaction
scenario, the snapshot data N +1 is represented by the sensor
data recorded during training data acquisition. Each column
of the snapshot matrices K1 and K2 contains a fixed number
of sensor values, i.e., the longitudinal CoM.

Fig. 6: DMD is used to generate new sensor values for
unknown parameter settings. Top: The training data which
consists of five equidistant samples of the longitudinal CoM
during walking. Bottom: The longitudinal CoM is interpolated with an interval of 0.01cm resulting in predictions for
800 possible parameter configurations.

current readings. For this task, we make use of the previously described SDTW method. As mentioned the SDTW
finds the optimal warping path p∗ for a currently measured
subsequence X to a previous recorded dataset Y. Whenever a
perturbation is detected, we perform iterative optimization by
generating predictions using a DMD model and calculating
the warping costs using SDTW. The goal of this optimization
process is to identify the behavior parameter that would best
explain the currently observed sensor values. Optimization
is performed using a stochastic optimization technique, i.e,
Covariance Matrix Adaptation Evolution Strategy (CMAES). The warping costs C generated by SDTW are used
as objective function. Figure 5 shows the warping costs C
calculated during a walking task.
The behavior parameter which produces least costs C is
regarded as the true behavior parameter if human forces are
taken into account. Accordingly, we can generate an estimate

E. Calculating a Continuous Measure of Perturbation
If the deviation between measured and predicted sensor
value is larger than the allowed variance σ we assume that
an external perturbation is influencing the execution of the
behavior. However, the question remains: how strong is the
external perturbation?
To estimate the strength of the perturbation, we simulate
different behavior parameters using the learned DMD model
and select the one that produces sensor values similar to our
597

Standard Deviation
a

20

longitudinal CoM

M RE

15

DMD
SDMD
LWR
Spline
Cubic

10

5

0

CoM

Prediction

c

Proper Acceleration

Measurement
b

d

Time Step

Fig. 9: Perturbation detection during walking using the
robot’s longitudinal CoM. Top Left: Slight push from the
front. Top Right: Strong push from the front. Bottom Left:
Slight push from the back. Bottom Right: Strong push from
the back.

Fig. 7: The DMD techniques are compared with a set of
classical interpolation schemes. Left: The DMD shows the
highest accuracy for the CoM. Right: In presence of high
noise, which is the case for proper acceleration estimates,
SDMD produces higher accuracy than DMD or classical
interpolation schemes.

the same data generation techniques as above. Again, the
original DMD uses all extracted modes to predict new sensor
values. However, these predictions are corrupted by the fact
that some of these extracted modes mainly contain noise. As
a result, the prediction performance of DMD deteriorates to
about the same level as classical interpolation schemes. In
contrast, SDMD concentrates on the three DMD modes that
best approximate the sensor data. In this case, one mode was
set to zero which obviously contained strong noise. However,
because of its smaller MRE, we use the DMD model in
conjunction with the CoM for the following experiments.

for the human forces by calculating the difference between
the behavior parameter used to control the robot and the
behavior parameter identified by the learned model.
IV. E XPERIMENTS
In the following experiments we use DMD, SDMD and
classical interpolation schemes to learn a model of a robot’s
walking gait. Furthermore, we evaluate and compare the
quality of each of these models. The best model is then used
to detect and estimate external perturbations during a human
robot interaction task.
A. Prediction Quality

B. Perturbation Detection

For the evaluation of DMD and SDMD we make use of
a walking dataset recorded on a Nao robot. The longitudinal
CoM was recorded for a walking gait with five different
equidistant step lengths between −4cm and +4cm. The
data is recorded with 100Hz for four seconds. Both the
DMD and SDMD algorithms were applied on this dataset,
resulting in four DMD modes. Given the learned models,
the goal is to generate new sensor values for step lengths
that were not recorded during training. Figure 6 shows
the five training samples of the longitudinal CoM and the
generated model which was interpolated with an interval of
0.01cm. To evaluate the precision of the generated data we
additionally recorded test samples with step lengths in an
interval of 1cm and measured their mean relative error MRE
w.r.t. the corresponding generated data. We also compared
the results with a set of classical interpolation schemes.
For the CoM, Figure 7 shows that DMD results in the
highest accuracy among all methods. SDMD reduces the
number of used modes to three and results in a slightly
less accurate model. Since, we want to work with low cost
sensors which may have significant noise, we additionally
recorded the robot’s longitudinal acceleration and applied

In the following experiments we detect external perturbations while the robot performs a walking gait with a step
length of 0.5cm for 35 seconds. During slowly walking the
human perturbs the robot by touching and pushing it as
shown in Figure 8. While Figure 8a, 8c show slight pushes,
which just marginally disturb the walking gait. We also
applied strong pushes as shown in Figure 8b, 8d. Especially,
the strong push from the back as shown in Figure 8d
significantly affected the robot‘s stability during walking.
We use the DMD model to generate the predicted sensor
values for the current step length. During behavior execution
the longitudinal CoM is measured with 100Hz and saved
in a sliding window with 10 measurements. To estimate the
current walking phase, we calculate the optimal warping path
from this subsequence in the predicted data using SDTW.
The resulting path is used as time dependent prediction of
the longitudinal CoM for the currently measured values.
Figure 9 shows the measured and predicted longitudinal CoM
for the external perturbations a-d as shown in Figure 8.
A perturbation is detected when the measured longitudinal
CoM is outside the variance of the predicted one.
598

(a) Slight Push Front

(b) Strong Push Front

(c) Slight Push Back

(d) Strong Push Back

Perturbation Value [cm]

Fig. 8: The human touches and pushes the robot during the execution of a walking behavior. The estimated perturbation
values differ in strength and direction and reflect the amount of force applied on the robot.

2

a

b

c

d

1
0
−1
−2
0

500

1000

1500

2000

2500

Time Step [1 / 100s]

3000

3500

Fig. 11: The perturbation value for the external perturbation a-d is the difference between the predicted parameter with
minimal costs and the current behavior parameter. Perturbation d produces a large oscillation which is dampened over time.

C. Perturbation Estimation

a

b

c

d

0.2

C

0.15
0.1

If a perturbation is detected, we have to find another
behavior parameter and its corresponding sensor evolution,
which has minimal mapping costs C for the SDTW. As
mentioned before, there are several different approaches for
this minimization problem. However, to prove the correctness
of our approach we compute C for each step length of the
learned DMD model. Figure 10 shows the overall costs C
for all possible step lengths of our DMD model during the
peeks of the external perturbations as shown in Figure 8ad. Obviously, pushes from the front produces minimal costs
for negative step lengths while pushes from the back lead
to positive step lengths. As a result, the parameters with
minimal costs can be seen as behavior parameters which
counteract the external perturbation. Finally, the perturbation
value is calculated from the difference of the current step
length of 0.5cm and the predicted step length. Since the
behavior parameter is specified in cm the measuring unit for
the perturbation value is also in cm. The perturbation value
for the complete behavior execution is shown in Figure 11.

0.05
0
0.4

C

0.3
0.2
0.1
0
−4

−2

0

2

4 −4

−2

0

2

4

Step Length [cm]

Fig. 10: The overall costs C for all possible parameters
during the peaks of the external perturbations a-d. The step
length which produces the minimal costs (black crosses)
is the predicted step length which is used to calculate the
perturbation value.

599

of this approach to industry-grade robots and collaborative
assembly tasks.
R EFERENCES
[1] P. Rouanet, P.-Y. Oudeyer, F. Danieau, and D. Filliat, “The Impact
of Human-Robot Interfaces on the Learning of Visual Objects,” IEEE
Transactions on Robotics, vol. 29, no. 2, pp. 525–541, Apr. 2013.
[2] A. Albu-Schäffer, O. Eiberger, M. Fuchs, M. Grebenstein, S. Haddadin, C. Ott, A. Stemmer, T. Wimböck, S. Wolf, C. Borst, and
G. Hirzinger, “Anthropomorphic soft robotics from torque control to variable intrinsic compliance,” in Robotics Research, ser.
Springer Tracts in Advanced Robotics, C. Pradalier, R. Siegwart, and
G. Hirzinger, Eds. Springer Berlin Heidelberg, 2011, vol. 70, pp.
185–207.
[3] S. Haddadin, Towards Safe Robots - Approaching Asimov’s 1st Law.
Springer, 2014.
[4] D. Lee and C. Ott, “Incremental kinesthetic teaching of motion
primitives using the motion refinement tube,” Autonomous Robots,
vol. 31, no. 2-3, pp. 115–131, 2011.
[5] H. Wang and K. Kosuge, “Control of a robot dancer for enhancing
haptic human-robot interaction in waltz,” IEEE Trans. Haptics, vol. 5,
no. 3, pp. 264–273, Jan. 2012.
[6] H. Ben Amor, E. Berger, D. Vogt, and B. Jung, “Kinesthetic bootstrapping: Teaching motor skills to humanoid robots through physical
interaction,” in KI 2009: Advances in Artificial Intelligence. Springer
Berlin Heidelberg, 2009, pp. 492–499.
[7] S. Calinon, Robot programming by demonstration: A probabilistic
approach. EPFL Press, 2009.
[8] J. Kober and J. Peters, “Policy search for motor primitives in robotics,”
Machine Learning, vol. 84, no. 1, pp. 171–203, 2011.
[9] S. Ikemoto, H. Ben Amor, T. Minato, B. Jung, and H. Ishiguro,
“Physical human-robot interaction: Mutual learning and adaptation,”
IEEE Robotics & Automation Magazine, vol. 19, no. 4, pp. 24–35,
2012.
[10] J. Stückler and S. Behnke, “Following human guidance to cooperatively carry a large object,” in Humanoids’11, 2011, pp. 218–223.
[11] K. Yokoyama, H. Handa, T. Isozumi, Y. Fukase, K. Kaneko, F. Kanehiro, Y. Kawai, F. Tomita, and H. Hirukawa, “Cooperative works by
a human and a humanoid robot,” in Proceedings. ICRA ’03. IEEE
International Conference on Robotics and Automation 2003., vol. 3,
2003, pp. 2985–2991 vol.3.
[12] A. Bussy, P. Gergondet, A. Kheddar, F. Keith, and A. Crosnier,
“Proactive behavior of a humanoid robot in a haptic transportation
task with a human partner,” in RO-MAN, 2012 IEEE. IEEE, 2012,
pp. 962–967.
[13] M. Lawitzky, A. Mortl, and S. Hirche, “Load sharing in human-robot
cooperative manipulation,” in RO-MAN, 2010 IEEE, 2010, pp. 185–
191.
[14] H. Sakoe, “Dynamic programming algorithm optimization for spoken
word recognition,” IEEE Transactions on Acoustics, Speech, and
Signal Processing, vol. 26, pp. 43–49, 1978.
[15] M. Müller, Information Retrieval for Music and Motion. Secaucus,
NJ, USA: Springer-Verlag New York, Inc., 2007.
[16] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for
Machine Learning (Adaptive Computation and Machine Learning).
The MIT Press, 2005.
[17] P. J. Schmid, “Dynamic mode decomposition of numerical and experimental data,” Journal of Fluid Mechanics, vol. 656, pp. 5–28, 8
2010.
[18] C. W. Rowley, I. Igor Mez, I. Shervin Bagher, R. Philipp Schlatte, and
D. S. Henningson, “Spectral analysis of nonlinear flows,” Journal of
Fluid Mechanics, vol. 641, no. -1, pp. 115–127, 2009.
[19] S. Bagheri, “Analysis and control of transitional shear flows using
global modes,” Ph.D. dissertation, KTH, Mechanics, 2010.
[20] K. Chen, J. Tu, and C. Rowley, “Variants of dynamic mode decomposition: Boundary condition, koopman, and fourier analyses,” Journal
of Nonlinear Science, vol. 22, no. 6, pp. 887–915, 2012.
[21] M. R. Jovanovi, P. J. Schmid, and J. W. Nichols, “Sparsity-promoting
dynamic mode decomposition,” Physics of Fluids (1994-present),
vol. 26, no. 2, 2014.
[22] E. Berger, D. Vogt, N. Haji-Ghassemi, B. Jung, and H. Ben Amor,
“Inferring guidance information in cooperative human-robot tasks,”
in Proceedings of the International Conference on Humanoid Robots
(HUMANOIDS), 2013.

Fig. 12: During a cooperative transportation task, a humanoid
robot continuously estimates the amount and direction of
external perturbations in order to react to human guidance.

D. Reaction
Our approach can be used in scenarios where a robot has to
detect and react to external perturbations in order to fulfill a
specified task. As shown in Figure 12 it can be used to follow
the human guidance in a cooperative transportation task as
investigated in our previous publication [22]. A video of the
functionality can be found under this link1 . Furthermore, our
approach can be used to implement collision detection and
safety constrains. In addition, the method can also be used to
measure the weight of a carried object during a manipulation
task. In general, behavior specific filtering allows for a
variety of close contact interactions with the environment.
V. C ONCLUSION
In this paper, we presented a new approach for learning
behavior-specific filters that can be used to accurately identify human physical influences on a robot. The approach uses
DTW and DMD/SDMD in order to (1) detect an external
perturbation, and (2) to quantify the amount of external
perturbations. The generated perturbation value can then be
used by a robot to adapt its movements to the applied forces
or interpret a human command such as “walk backwards”.
In our experiments we showed that the learned perturbation
filter can be used to accurately estimate touch information
from noisy, low-cost sensors. Our approach produces a
continuous perturbation value that can be used to detect even
subtle physical interactions with a human partner. Since we
are using a data-driven approach, no thresholds need to be
defined by the user. At the core of our approach lies Dynamic
Mode Decomposition, which so far has mostly been used
in other fields of science, particularly fluid mechanics. We
conclude that DMD is also a highly promising method
for robotics. For future work, we hope to hierarchically
combine several filters in a mixture-of-experts approach, in
order to generalize perturbation estimation to new, untrained
behaviors. We are currently also investigating the application
1 http://youtu.be/48y0hEix2fY

600

2015 IEEE International Conference on Robotics and Automation (ICRA)
Washington State Convention Center
Seattle, Washington, May 26-30, 2015

Exploiting Symmetries and Extrusions for Grasping
Household Objects
Ana Huamán Quispe1

Benoı̂t Milville1
Marco A. Gutiérrez2
Can Erdogan1
Henrik Christensen1
Heni Ben Amor1

Mike Stilman†

Abstract—In this paper we present an approach for creating
complete shape representations from a single depth image for
robot grasping. We introduce algorithms for completing partial
point clouds based on the analysis of symmetry and extrusion
patterns in observed shapes. Identified patterns are used to
generate a complete mesh of the object, which is, in turn, used for
grasp planning. The approach allows robots to predict the shape
of objects and include invisible regions into the grasp planning
step. We show that the identification of shape patterns, such
as extrusions, can be used for fast generation and optimization
of grasps. Finally, we present experiments performed with our
humanoid robot executing pick-up tasks based on single depth
images and discuss the applications and shortcomings of our
approach.

I. I NTRODUCTION
The ability to grasp and manipulate objects is an important skill for autonomous robots. Many important tasks, e.g.,
assisting humans in household environments, require robots
to reliably plan and execute grasps on surrounding objects.
To generate plans for manipulation tasks, information about
the shape of the object is required. A frequent approach to
grasp planning is to use a database of polygonal meshes
representing the different objects that the robot can manipulate
[8]. Such information about object geometry can be used by
grasp planners to synthesize an appropriate hand shape and
orientation for physical interaction. While this approach is
valid for structured domains with a small set of different
objects, it does not scale to unstructured environments in
which many objects may have never been seen before.
Other approaches to grasp planning employ depth cameras
to acquire 3D point clouds of new objects, which in turn are
used to generate grasps. Since the point clouds are acquired
from a specific perspective, they only hold partial shape
information about the visible frontal part. Using only partial
point clouds to plan manipulation tasks can be very limiting,
since many grasps involve placing fingers on opposite sides of
an object. To fill any gaps and produce a complete point cloud,
multiple images can be acquired by either iteratively moving
the camera or the object. This process is time-consuming and
1 Institute

gia

Institute

for
of

Robotics
and
Intelligent
Machines,
Technology,
Atlanta,
GA
30332,

GeorUSA.

ahuaman3@gatech.edu, cerdogan@cc.gatech.edu,
benoit.milville@gadz.org, hic@cc.gatech.edu,
hbenamor@cc.gatech.edu
2 Robotics and Artificial Laboratory, Univesity of Extremadura, Cáceres,
10003, Spain. marcog@unex.es

978-1-4799-6923-4/15/$31.00 ©2015 IEEE

Fig. 1: Extracted information of rotational symmetries in the
object is used to create a complete shape from a partial point
cloud. The generated mesh is used by a grasp planner to
generate a continuous set of grasps around the symmetry axis.

introduces new challenges such as the precise matching of the
individual point clouds of each view.
Alternatively, the robot can use geometric cues to predict the
shape of the object in unseen regions. Through the analysis
of inherent shape properties such as mirror symmetries and
rotational extrusions, estimates of the complete point cloud
can be generated from a single image. The extracted symmetry
parameters can be used to extend observed shape patterns, e.g.,
the profile curve of an object, to occluded regions.
In this paper, we show how compact object representations for manipulation tasks can be generated from a partial
point cloud. Given a single RGB-D image, we generate
a complete mesh model of the observed object as well
as additional shape information, e.g., axis of symmetry or
superquadric approximations. We show that these compact
representations can be later exploited for the fast synthesis
of a continuous set of grasps. In turn, the set is used to
plan robot manipulation tasks. Our approach builds both upon
recent developments in symmetry-based [3, 18], as well as
extrusion-based object representations [16]. Symmetry-based
representations mirror observed object parts into occluded
regions. Extrusion-based approaches, on the other hand, try

3702

to identify a two-dimensional profile which can be linearly
or rotationally extruded to complete an object. In this work
we show how symmetries and extrusions can be used to
extract two different types of object representations, namely
superquadric approximations and 2D shape profiles. We also
show how these representations can used to generate grasps
on the object.
The rest of this paper is organized as follows: Section
II summarizes relevant literature. Section III introduces two
compact object representations that are based on detecting
symmetries and extrusions. Section IV shows how compact
object representations based on extrusion patterns can be
exploited for fast grasp planning with a small number of
parameters. Section V presents experimental results of the
object completion, as well as its application to robot grasping
tasks. Finally in Section VI we discuss our approach and its
advantages and shortcomings.
II. R ELATED W ORK
For a robot to physically interact with its environment,
algorithms for both grasp planning and perception are required.
Traditional approaches for grasp generation are often based
on fitting 3D CAD models to the observed scene [14, 15].
Such an approach, however, cannot be used to grasp novel
objects since it requires accurate, prior knowledge about the
shape. With the advent of depth cameras, various researchers
have turned towards point cloud representations for perception
and grasp planning. Huebner et al. [11] showed that bounding
boxes computed from point clouds can be used to grasp novel
objects. In a similar vein, Jiang et al. [12] proposed a socalled grasping-rectangle representation which can be used
to infer the best grasp parameters given an RGB-D image
of a novel object (given an offline training step). Przybylski
et al. [21] showed simulation results in which a medial axis
representation of objects can be used to find successful grasps
without compromising on the approximation quality. Other
than boxes and spheres [17], superquadrics [9] have also been
considered for grasping applications given their compactness
and ability to represent many diverse shapes with a limited
number of parameters. Recently, Duncan et al. presented a
fast hierarchical approach to fit superquadrics online [5].
On the side of grasp generation, a popular metric used to
predict grasp robustness is the ǫ metric proposed by Ferrari
and Canny [6]. While many popular grasp generators, such
as GraspIt! use this metric to evaluate and refine the grasp
search, it has been noted [4] that a grasp with a good metric
does not translate to a robust grasp in a real-world execution.
Researchers such as Hsiao [10] and Balasubramanian [1] have
shown that grasps obtained using simple human heuristics can
produce comparable or even better results when evaluated in
a real, non-simulated environment.
A real world scenario - contrary to a simulated one presents its own set of challenges: errors in perception, control
and modeling must be considered and might render an optimal
simulated grasp into an infeasible one. Regarding incomplete
perceptual information, such as one-view point clouds for a

given object, Bohg et al. [3] proposed a simple approach that
exploits the symmetry of most common household objects to
predict the full shape of an object on a tabletop scenario.
Following Bohg’s observation that most common household
objects present similar characteristics (such as symmetry,
extrusion-like geometry and primitive shapes), we use them
to approximate the shape of objects. This is also useful in the
event of occlusion, in which a complete point cloud is not
available.
III. G ENERATING C OMPACT O BJECT R EPRESENTATIONS
FROM S INGLE RGB-D I MAGES
In this section, we present two compact representations of
objects that can be generated from partial point clouds. These
representations can be used to plan grasps on objects involving
regions of the point cloud that are currently invisible. As a
result, a wider range of grasps can be planned, including, for
example, side grasps which are based on an opposition of
fingers placed at the front (seen) and the back (unseen) of the
object.
We will first present a superquadric representation which is
based on determining symmetries in point clouds. After that,
we will turn towards a more detailed representation which
makes use of rotational symmetries and linear extrusions to
characterize an object.
A. Superquadric Representation
Superquadrics are a family of geometric shapes that can
represent a wide range of diverse objects. The equation describing superquadrics in their canonical form can be written
as
  2
 ǫ2
x ǫ2  y  ǫ22 ǫ1  z  ǫ21
F (x) =
+
+
= 1.
a
b
c

(1)

where a,b,c are the scaling factors along the principal axes,
ǫ1 is the shape factor of the superquadric cross section in
a plane orthogonal to XY containing the axis Z, and ǫ2 is
the shape factor of the superquadric cross section in a plane
parallel to XY. If a general transformation is considered,
then the total number of parameters required to define a
superquadric is 11 (the 6 additional being the rotational and
translational degrees-of-freedom (DoFs) {x, y, z, ρ, ψ, θ}). By
minimizing the error between each point and the general
superquadric equation, a shape that best fits the point cloud
can be obtained:
min
k

n √
X

k=0

2
abcF ǫ1 (x; Λ) − 1

(2)

As mentioned in Section II, superquadrics have previously
been used to generate grasp configurations for simple objects
[2, 22]. Most of these approaches assume that the complete
shape of the object is given or that the parameters can
be learned beforehand. However, when working with depth
cameras this is not a reasonable assumption to make. In
recent work, Duncan et al. [5] presented a superquadric fitting

3703

Hypotheses

Initial Estimation

Optimization

Fig. 3: The three steps used for optimizing the axis of
extrusion. First, we generate hypotheses by analysing pairs of
points. The resulting estimates are used to produce an initial
estimate of the axis of extrusion. Finally, optimization is used
to improve the extrusion axis.

choose suitable candidates for task execution. For example,
detecting the axis of symmetry in a rotationally symmetric
object allows us to rotate any feasible grasp around this axis.

Fig. 2: An example for the superquadric fitting with symmetry
analysis (middle) and without it (bottom).
approach which uses a voxel representation to reduce the
computational complexity of the task. We found that this
approach worked well when the segmented point cloud of
the object had a good viewing point (i.e. the front, side
and top of the object were seen). For point clouds in which
only one side of the object was seen (i.e. only front), the
performance quickly deteriorated, producing fitting parameters
that in many cases exceeded greatly the original dimensions
of the objects. While this could be partially alleviated by hardcoding limits in the dimension of the axes, this is not practical
when dealing with novel objects, for which we might not know
the dimensions beforehand.
Inspired by work presented by Bohg et al. [3], we added
an additional pre-processing step to the superquadric minimization process. Instead of using the original point cloud as
input, we generated a mirrored version (see Fig. 2) by finding
an optimal symmetry plane perpendicular to the table where
the object resides (for more details of this process, please refer
to the original paper [3]).
B. Object Completion from Extrusions
Planning task-specific grasps requires information about
the complete shape of the object to be manipulated. Many
household objects are based on extrusions. Indeed many
modelling and manufacturing systems use linear and rotational
extrusions in a hierarchy to generate the models used for
manufacturing. Uncovering extrusions in partial point clouds
can therefore help to generate a complete point cloud from a
partial observation. In addition, this knowledge can be used to
create a large set of feasible grasps from which a planner can

In this paper, extrusion detection is performed using a threestep approach, see Fig. 3 for an overview of the approach using
rotational extrusions. In the first step, we use points from the
partial point clouds to generate hypotheses for the extrusion
axis. In the case of rotational extrusions, we randomly sample
pairs of points and use the normal of each point to create a
line. Each pair of lines is intersected and the resulting point
is used as a hypothesis for the axis of extrusion. Fig. 3 shows
an example for points sampled from a cylindrical object. To
account for noise, we use the midpoint of the line connecting
the closest points, in case the two lines do not intersect.
The collected hypotheses points are then used to create an
initial estimate of the axis of extrusion. To this end, we fit a
line into the set of hypotheses using linear least-squares. The
RANSAC [7] algorithm is further used to reduce the influence
of outliers. Given this initial estimate, we perform optimization
to produce a more accurate axis of extrusion. Specifically, we
use the dynamic hill climbing algorithm [23] to search for an
axis of extrusion which reduces the dispersion of points along
the profile of the object. In every iteration, the axis of extrusion
is used to rotate all points of the partial point cloud back onto a
plane. We then estimate the density of the points using a kernel
density estimator [20]. By maximizing the density using the
hill climbing algorithm, we can reduce the dispersion of the
projected points, thereby recreating the profile of the object.
However, performing a kernel density estimation in each step
of the optimization process is computationally expensive and
does not scale to large point clouds. The following method
is, therefore, a discrete approximation of the kernel density,
which produced accurate results in practice while at the same
time being fast.
We create an approximation of the kernel density estimator
by creating a grid over the projected point cloud. The number
of cells used in our experiments varied between 5 and 30
cells in each dimension. For each cell i ∈ {1, .., M } we count
the number of points ci that lie within. We then calculate the
average of the differences to neighbouring cells j ∈ {1, .., N }.
The overall objective function of the optimization can be

3704

Iteration 10

Iteration 2

Iteration 50

20

25
15

20
15

10

10
5
5
0

0
0

1

2

3

4

5

6

7

Fig. 4: Density estimation at different stages of optimization.
At the beginning of the optimization, the projected points are
highly dispersed. The axis of extrusion is then changed to
minimize the dispersion, such that the outer profile of the
object emerges as can be seen in iteration 50. On the right
side we can see the object to which the profile belongs.
written as
E=

M N
1 1 XX
||ci − cj ||
MN i j

Fig. 6: Extracted object profile for the linearly extruded
objects. The extracted profiles are used to create a complete
point cloud.
(3)

where E is the energy to be minimized. Fig. 4 shows three
iterations during the optimization of the axis of extrusion.
Dark areas correspond to regions of high density of points,
while lighter areas correspond to low density regions. In early
iterations, the estimate of the axis does not produce a clear
profile when points are projected (rotationally) onto a plane.
In iteration ten, we can see that high density regions start
forming. After fifty iterations, an approximate profile of the
object starts to emerge.
After optimization is finished, we regard the projected points
as the profile of the object and rotationally extrude them
around the axis of extrusion to generate a complete point
cloud. Fig. 5 shows a set of household objects, the recorded
depth images, as well as the reconstructed complete meshes.
Given the completed point cloud, we reconstructed the meshes
using Poisson surface reconstruction [13].
For the case of linear extrusions along an axis, a different
method for the estimation of the initial axis of extrusion needs
to be used. For linear extrusions, we compare the normal
vectors of pairs of points and generate a hypothesis if the
difference between the normals is below a threshold. The
resulting set of hypothesis can then be clustered, such that each
cluster represents a possible axis of extrusions. For example,
for a box, up to six clusters can be found.
Note, that in our approach we use a point cloud to represent
the profile of an extrusion. For revolute objects, the profile
defines the outer curve of the object, which can be rotated
around the axis of extrusion to generate the complete shape.
For linear extrusions, the cloud represents the basic 2D shape
which can be extruded to form the object. Fig. 6 shows the
extracted object profiles for objects with linear extrusions.
IV. U SING C OMPACT O BJECT R EPRESENTATIONS FOR
G RASP P LANNING
Grasp planning greatly benefits from the completed point
clouds. A complete point cloud can be triangulated and used as
an input to existing grasp generation and planning algorithms.
In contrast to the partial point cloud, the completed and
triangulated mesh can be used to perform collision checks

and evaluate grasp quality using existing metrics. In contrast,
traditional grasp quality metrics cannot be directly applied to
partial point clouds. Similarly, having a complete mesh allows
a grasp planner to evaluate a large variety of grasps, which can
then be pruned based on task constraints. However, generating
many grasps often involves repeated applications of grasp optimization methods which can be computationally demanding,
in particular in the presence of many degrees-of-freedom in
the robot arm and hand. Extracted shape information from
extrusions can be used to improve the efficiency of this process
by significantly reducing the number of degrees-of-freedom of
the problem.
The main insight of this section is that hand shapes during
object grasping are invariant to movements along the axis of
extrusion. As long as the robot hand moves along the axis of
extrusion, no expensive replanning of the hand shape is necessary. In the case of linear extrusions, the robot hand can move
up and down the axis of extrusion without having to change
the hand shape. Similarly, in the case of rotational extrusions,
the hand can be rotated around the axis of extrusion. This
knowledge can be exploited during grasp generation in order to
turn each single detected grasp into a continuous set of grasps.
Subsequently, we present a specific example how information
about extrusions can be used to reduce the dimensionality and
complexity of a grasp re-planning task.
Fig. 7a shows a scenario, in which a grasp is executed
on a rotationally symmetric object. The grasp has a low
manipulability index which is not sufficient to achieve the task
constraints. Typically, this means that a new grasp and arm
pose needs to be planned, which involves (sampling-based)
optimization in the high-dimensional space of joint angles.
Given that the grasp is performed on a rotationally symmetric object, the grasp generation can be modeled as an
inverse kinematics problem where the goal is to determine
an arm configuration q that is collision free. The output is
constrained by the end-effector position on the object and the
corresponding inverse kinematics solution. The end-effector
pose x can be parametrized by (1) the rotation around the axis
of extrusion φ and (2) the distance along the axis of extrusion

3705

Fig. 5: Reconstruction of rotationally symmetric household objects. The top row shows a photo of the object. The middle
row shows the corresponding depth image recorded using a Microsoft Kinect. The bottom row shows the completed mesh.
Reconstruction was performed from a single image through the analysis of extrusions.
α, x = pose(φ, α). The inverse kinematics solution q with a
7-DoF arm for an end-effector pose x can be parametrized by
an additional variable θ which represents the angle between
the wrist-elbow-shoulder plane and the ground, q = IK(x, θ).
At each iteration i, the new arm position is computed
using an updated grasp position from the parameter space
{φi−1 ± δφ , αi−1 ± δα } and the corresponding inverse kinematics parametrized by {θi−1 − δθ , θi−1 , θi−1 + δθ }. Let P
represent the full space of the variables φ, α, and θ. The
algorithm iteratively updates these parameters by determining
which tuple leads to the maximum manipulability [19]. This
is realized by solving for the following objective
q
(4)
q i = argmax det(J(q)J T (q))
{φ,α,θ}∈P

where q = IK(pose(φ, α), θ). The sequence in Fig. 7 shows
several snapshots during this optimization. In this scenario,
the robot grasps a rotationally symmetric bottle. The initial
random grasp sample in Fig. 7a yields a manipulability of
0.268 which is then improved in Fig. 7d leading to a value of
0.540. To optimize the manipulability, the planner iteratively
changes the grasp position on the robot with the φ and
α parameters, and the inverse kinematics parameter θ. This
optimization can be performed efficiently since, the highdimensional configuration space of the hand does not need
to be represented thanks to the extracted symmetries. Instead,
a three-dimensional space of parameters {φ, α, θ} ∈ P is used.
V. E XPERIMENTAL R ESULTS
In this section, we present a set of experiments which
we conducted to evaluate the proposed approach. The first

set of experiments focuses on the complexity and accuracy
of point cloud completion when generating compact object
representations. The second set of experiments shows the
application of the approach to grasp planning on a humanoid
robot. The used humanoid robot is based on Schunk LWA3
arms with 7 DoF. A Schunk gripper with a maximum aperture
of 7cm was used. Partial point clouds were recorded using a
Microsoft Kinect camera.
A. Accuracy of Fit
We first analyzed the accuracy of fit of the two presented
compact object representations. For extrusions, we collected a
set of rotationally symmetric meshes from internet databases
from which we generated partial point clouds. We then cut
out a partial point cloud representing 30% of the data and
simulated Kinect-like noise by adding holes and noise to
the dataset. The partial cloud was then completed using the
extrusion detection methods from Sec. III-B. To measure the
accuracy, we compared the completed clouds to the original
mesh of the object. On average, the approach produced an
error (distance of points to mesh) of 2mm, where objects had
a diameter between 10 − 20cm. Analysis of the extrusions
required on average 200ms.
For superquadric fitting we conducted a similar experiment.
However, in this case we noticed larger variations in the
reconstructed shapes depending on the perspective of the
camera to the object. We therefore placed each object at one
of five different locations in front of the camera and measured
the run time of the algorithm including symmetry analysis and
without it. As depicted in Tab. I, the fitting time is shorter when
additional points are added via symmetry analysis. While this

3706

Fig. 7: Grasp manipulability optimization along the axis of extrusion. Since the object is symmetric, the same hand configuration
can be rotated around the object (A-B, C-D). At the same time, the extra DOF in the inverse kinematics solution is also utilized
to maximize manipulability (B-C).
may seem unintuitive, we found that the superquadric shape
has more constraints when considering mirrored points. As
a result, the optimization process required for fitting quickly
settles on a good solution.

to the invariance along the axis of extrusion. Images of the
executed grasp and the experimental setup can be found in
Fig. 8.
TABLE II: Experimental results, 3 trials per object per location

TABLE I: Comparison of fitting times
Object
Apple
Milk
Jam
Raisins
Creamer

Input
Symmetry
Plain
Symmetry
Plain
Symmetry
Plain
Symmetry
Plain
Symmetry
Plain

P1
0.02
0.14
0.20
0.42
0.06
0.08
0.29
0.36
0.15
0.65

P2
0.13
0.17
0.07
0.56
0.11
0.29
0.25
0.40
0.15
0.09

P3
0.01
0.06
0.03
0.27
0.13
0.10
0.31
0.43
0.22
0.39

P4
0.06
0.06
0.05
0.53
0.08
0.08
0.14
0.43
0.13
0.26

P5
0.07
0.06
0.04
0.06
0.21
0.11
0.27
0.32
0.14
0.29

Avg. Time
0.05s
0.098s
0.078s
0.368s
0.118s
0.132s
0.252s
0.388s
0.158s
0.336s

Extrusion
Success
SQ

Extrusion
Grasps
SQ

Location
B4
C3
C4
B4
C3
C4
B4
C3
C4
B4
C3
C4

Creamer
100%
0%
100%
100%
100%
100%
1040
800
1270
11
7
10

Dove
100%
0%
100%
100%
100%
100%
900
400
320
11
3
5

Roll
0%
0%
100%
0%
66%
0%
1200
2200
800
7
1
5

Micro
100%
0%
100%
100%
100%
100%
640
800
1020
11
7
13

B. Robot Grasping Experiments
Next, we conducted an experiment in which a humanoid
robot was used to grasp household objects located in front of
it. We also placed several other objects as clutter on the table.
Given the depth image all objects were reconstructed using
compact object representations. After that, the robot planned
and executed grasps using the normal at a point as an approach
direction and the method described in Sec. IV for ensuring
manipulability and obstacle avoidance. We conducted trials
with 4 objects which were placed at 4 different locations on
the table. Each trial was repeated three times. A grasp was
regarded successful if the robot was able to lift the object.
Tab. II summarizes the results of the experiment. We can see
that the approach using superquadrics performs well on most
objects with the exeption of the roll. In contrast, the extrusionbased approach seems to have difficulties with a specific
location (C3). Analyzing the robot executions, we found that
superquadric approach typically leads to approximate shapes
which are slightly larger than the original object. Hence,
the executed grasp includes a ”buffer” zone that allows it
to succeed in the presence of sensor and calibration noise.
Grasps planned for the shapes generated by the symmetry
detection, however, are tighly fit to the object. This often lead
to premature contact with the object during grasp execution.
In Tab. II we also see the number of different grasps found
using the two approaches. We can see that the symmetry based
approach leads to a larger number of different grasps, due

VI. D ISCUSSION AND C ONCLUSION
In this paper we introduced methods for generating compact
and complete object representations that are particularly useful
for robot grasping applications. The approach exploits natural
patterns found in many shapes, e.g., symmetries, linear extrusions, and rotational extrusions to generate a complete mesh
from a single depth image. We also showed that the extraction
of this information can be used to improve the efficiency and
quality of the grasp planning step. The work presented in
this paper can be seen as a first step towards shape priors
that can be used by a robot to generate hypotheses about the
shape of an object in invisible regions. Other cues, such as
curvature and texture may also be helpful in predicting the
complete shape from partial observations. At the moment the
introduced approach is limited to household objects, which
are often based on linear and rotational extrusions. However,
it can also be extended to work in a hierarchy to complete
more complex objects. In future work, we hope to investigate
this aspect in more detail.
The performed robot experiments showed that the approach
can be used to create a variety of grasps. In particular, we
can generate grasps that extend to parts of the object that are
not seen. This is in contrast to other methods which limit the
approach direction of the robot to the visible part of the object.
We have shown in the experiments that the method can be
used to reconstruct objects in a cluttered scene without prior

3707

Fig. 8: Grasps on household objects generated via grasp planning on compact object representations. All objects on the table
were reconstructed. Objects that were not grasps were regarded as obstacles to be avoided during the manipulation task.
information. Yet, the additional information gained by creating
complete meshes also imposes additional requirements on the
accuracy of the robot controller. Planning grasps with more
accurate reconstructions of the observed object means that the
robot needs to be very precise in the task execution. So far,
we do not have a model of the inherent sensor and actuation
noise. We hope to investigate Bayesian approaches to object
fitting, which would allow us to use information about the
uncertainty during task execution.
ACKNOWLEDGMENTS
This work is dedicated to the memory of Mike Stilman,
whose enthusiasm for making robots do cool things will
always be remembered.
R EFERENCES
[1] R. Balasubramanian, L. Xu, P. D. Brook, J.R. Smith, and Y. Matsuoka.
Human-guided grasp measures improve grasp robustness on physical
robot. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2010.
[2] G. Biegelbauer and M. Vincze. Efficient ”3d” object detection by fitting
superquadrics to range image data for robot’s object manipulation. In
IEEE Int. Conf. on Robotics and Automation (ICRA), 2007.
[3] J. Bohg, M. Johnson-Roberson, B. León, J. Felip, X. Gratal,
N. Bergstrom, D. Kragic, and A. Morales. Mind the gap: Robotic
grasping under incomplete observation. In IEEE Int. Conf. on Robotics
and Automation (ICRA), 2011.
[4] R. Diankov. Automated Construction of Robotic Manipulation Programs. PhD thesis, Robotics Institute, Carnegie Mellon University, 2010.
[5] K. Duncan, S. Sarkar, R. Alqasemi, and R. Dubey. Multi-scale
superquadric fitting for efficient shape and pose recovery of unknown
objects. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2013.
[6] C. Ferrari and J. Canny. Planning optimal grasps. In IEEE Int. Conf.
on Robotics and Automation (ICRA), 1992.
[7] M. A. Fischler and R. C. Bolles. Random sample consensus: A paradigm
for model fitting with applications to image analysis and automated
cartography. Communications of the ACM, 1981.
[8] C. Goldfeder and P. Allen. Data-driven grasping. Autonomous Robots,
2011.
[9] C. Goldfeder, P. Allen, C. Lackner, and R. Pelossof. Grasp planning via
decomposition trees. In IEEE Int. Conf. on Robotics and Automation
(ICRA), 2007.
[10] K. Hsiao, S. Chitta, M. Ciocarlie, and E. Jones. Contact-reactive
grasping of objects with partial shape information. In IEEE/RSJ Int.
Conf. on Intelligent Robots and Systems (IROS), 2010.
[11] K. Huebner and D. Kragic. Selection of robot pre-grasps using boxbased shape approximation. In IEEE/RSJ Int. Conf. on Intelligent Robots
and Systems (IROS), 2008.
[12] Y. Jiang, S. Moseson, and A. Saxena. Efficient grasping from rgbd
images: Learning using a new rectangle representation. In IEEE Int.
Conf. on Robotics and Automation (ICRA), 2011.
[13] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface reconstruction.
In Proc. of the Fourth Eurographics Symposium on Geometry Processing, 2006.

[14] U. Klank, D. Pangercic, R.B. Rusu, and M. Beetz. Real-time cad model
matching for mobile manipulation and grasping. In 9th IEEE-RAS Int.
Conf. on Humanoid Robots (Humanoids), 2009.
[15] D. Kragic, A. Miller, and P. Allen. Real-time tracking meets online grasp
planning. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2001.
[16] O. Kroemer, H. Ben Amor, M. Ewerton, and J. Peters. Point cloud completion using extrusions. In Int. Conf. on Humanoid Robots(Humanoids),
2012.
[17] A. Miller, S. Knoop, H. I. Christensen, and P. Allen. Automatic grasp
planning using shape primitives. In IEEE Int. Conf. on Robotics and
Automation (ICRA), 2003.
[18] Niloy J. Mitra, Leonidas J. Guibas, and Mark Pauly. Partial and
approximate symmetry detection for 3d geometry. In ACM SIGGRAPH
2006 Papers, SIGGRAPH ’06, pages 560–568, New York, NY, USA,
2006. ACM.
[19] Y. Nakamura and H. Hanafusa. Inverse kinematic solutions with
singularity robustness for robot manipulator control. Journal of dynamic
systems, measurement and control, 1986.
[20] E. Parzen. On estimation of a probability density function and mode.
The Annals of Mathematical Statistics, 1962.
[21] M. Przybylski, T. Asfour, and R. Dillmann. Unions of balls for shape
approximation in robot grasping. In IEEE/RSJ Int. Conf. on Intelligent
Robots and Systems (IROS), 2010.
[22] F. Solina and R. Bajcsy. Recovery of parametric models from range
images: The case for superquadrics with global deformations. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 1990.
[23] D. Yuret and M. de la Maza. Dynamic hill climbing: Overcoming the
limitations of optimization techniques. In Second Turkish Symposium
on Artificial Intelligence and Neural Networks, 1993.

3708

From Motion Capture to Action Capture:
A Review of Imitation Learning Techniques and their
Application to VR-based Character Animation
Bernhard Jung, Heni Ben Amor, Guido Heumer, Matthias Weber
VR and Multimedia Group
TU Bergakademie Freiberg
Freiberg, Germany

jung|amor|guido.heumer|matthias.weber@informatik.tu-freiberg.de
ABSTRACT

for virtual characters. Animations recorded from a human
actor through motion capture are natural, lifelike, and contain even subtle details of human movement. Today, motion
capturing is used in a variety of applications, ranging from
virtual reality environments to video games and movies. In
recent years, however, the virtual worlds found in such applications have become more and more complex, thus making
various limitations of motion capture visible. For example,
there is a current trend in the computer graphics community towards increased interactivity: virtual characters have
to be able to react appropriately to sensations from their
environment or the user. For this, the animations have to
be changed at runtime; a process which is very difficult to do
with motion capture data. A partial solution to this problem, motion graphs, is proposed in [13]. Another recent
trend is the use of physical models in order to enrich the
virtual worlds with a higher amount of realism and responsiveness. Again, online modifications of recorded motions
are needed, in order to have the animation of a virtual character change according to the physical forces acting on it.
One approach for combining physical models with motion
capture data has been proposed in [29]. Recent movie productions also have started to use large groups of autonomous
characters, each of which is equipped with a basic set of motion captured skills. However, these motions are not flexible
enough in order to be changed according to the context they
are triggered in. For example, a recorded motion for attacking a normal sized opponent might be rendered obsolete, if
the current opponent is only half as tall.
A more flexible synthesis of motions can be achieved by
means of behavioral animation techniques. Here, the animation is represented through a model or controller, instead of
a set of motion data. Applying the controller in each simulation step yields the desired animation. Unfortunately, creating good controllers is often a difficult process based on
complex search and optimization techniques. Additionally,
by relying on such optimization or planning processes, the
user sacrifices a large amount of control over the resulting
animation. As a consequence, the animator is often required
to put significant effort into parameter tweaking in order to
achieve the desired end result of naturally appearing animations.
In this paper, we introduce a new approach to animation synthesis that we call action capture. Action capture is
a VR-based method that not only tracks the user’s movements but also his or her interactions with scene objects.

We present a novel method for virtual character animation that we call action capture. In this approach, virtual
characters learn to imitate the actions of Virtual Reality
(VR) users by tracking not only the users’ movements but
also their interactions with scene objects. Action capture
builds on conventional motion capture but differs from it
in that higher-level action representations are transferred
rather than low-level motion data. As an advantage, the
learned actions can often be naturally applied to varying
situations, thus avoiding retargetting problems of motion
capture. The idea of action capture is inspired by human
imitation learning; related methods have been investigated
for a longer time in robotics. The paper reviews the relevant literature in these areas before framing the concept of
action capture in the context of VR-based character animation. We also present an example in which the actions of a
VR user are transferred to a virtual worker.

Categories and Subject Descriptors
I.2.6 [Artificial Intelligence]: Learning; I.3.6 [Computer
Graphics]: Methodology and Techniques—Interaction techniques; I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism—Virtual Reality

General Terms
Algorithms

Keywords
Virtual Reality, Motion Capture, Imitation Learning, Character Animation

1.

INTRODUCTION

Since its inception, motion capture has proved to be a
powerful and natural way of producing complex animations

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
VRST’06, November 1–3, 2006, Limassol, Cyprus.
Copyright 2006 ACM 1-59593-321-2/06/0011 ...$5.00.

145

The goal of action capture is to combine the power of motion capture for creating natural and complex animations
with the flexibility and dynamic responses of behavioral animation. To this end, imitation learning techniques are used
in order to build faithful, yet situationally adaptable models of recorded movements that preserve interactions with
scene objects under varying environmental conditions. Imitation learning has been investigated for a longer time in
the cognitive and neuro sciences as well as in the fields of
robotics and Artificial Intelligence, where related methods
have been applied as a means of programming robots by
demonstration. We review and discuss some of the relevant publications in these areas from a computer animation
point of view. Building upon the results of this discussion,
we present a basic framework for action capture and suggest
possible extensions. Finally, we present first results from an
ongoing research project in which manipulation tasks performed in VR are used to generate the animation of virtual
humans.

2.

virtual characters. Such animation methods have in recent
years become popular in movies and interactive games under
the name of behavioral or interactive animation [22, 27].
Note that humans are highly capable of transferring learned
actions to slightly different task environments as detailed in
the above example. The following section therefore reviews
empirical findings on human imitation learning.

3. IMITATION IN HUMANS AND ANIMALS
According to Thorndike [26] imitation is: “from an act
witnessed learn to do an act”1 . Imitation is a powerful ability of humans and higher animals, which enables them to
acquire new skills by observing actions of others. Young
children for example are able to learn a number of social behaviors by observing their parents. Sport students can learn
how to do a complex tennis serve by watching a teacher
repeating it a few times. Imitation enables us to quickly
acquire new skills without going through a lengthy trialand-error process. As a result humans are very flexible and
adaptive to changes in their environment. When confronted
with new situations, environments or cultures we can rapidly
learn a set of skills which might be crucial for progress or
even survival. For the purpose of introducing the term action capture in later sections, we will focus on the imitation
of ‘actions’. Following Arbib’s equation ‘action = movement
+ goal / expectation’ [3], with ‘action’ we refer to a goaldirected intentional motor behavior.
Bakker and Kuniyoshi [4] identify three requirements for
the process of imitation:

MOTIVATION

Extending conventional motion capture, action capture
aims to take advantage of increasingly available, complete
VR systems for the purpose of virtual character animation. Similar to motion capture, the user’s movements are
recorded by means of position trackers and data-gloves. However, the recorded movements are abstracted to higher-level
action representations that also account for user interactions
with scene objects. Captured actions are later reproduced
by virtual characters using behavioral animation techniques.
The resulting animations naturally include interactions with
scene objects. As further advantage, actions can be reproduced by virtual characters of different sizes and body proportions as well as in situations where the task environment
differs from the original recording situation.
To illustrate the above advantages over conventional motion capture, consider the following example of a VR system
for evaluating the interior design of a car’s virtual prototype. To test the prototype’s design, the user may interactively simulate the handling of the steering wheel, stick shift,
radio controls, and other instruments. For ergonomic analyzes, these procedures are later to be repeated by virtual
humans of different sizes and body proportions. Using conventional motion capture, the recorded movements would
need retargetting to each differently sized virtual human;
although powerful methods have been proposed for this retargetting step, e.g. based on spacetime constraints [28], this
process still requires an explicit modeling effort by the human animator [9]. To complicate – and add realism to – the
example, the ergonomic analyzes might further reveal a flaw
in the car’s interior design resulting e.g. in the repositioning
of some instrument in the car’s prototype. Again, the goal
of action capture is to reuse the learned actions; i.e. data
captured of the VR user e.g. twisting a knob in its original position should still be valid for synthesizing the virtual
characters’ animations when the knob’s position is slightly
altered in the next version of the virtual prototype. In the
case of motion capture, the knob’s repositioning would again
require retargetting of the movement data; in contrast, in
the case of action capture, a reusable, abstract animation
command such as twist(knob-1) would play an integral role
when synthesizing the desired animation. A prerequisite for
action capture is thus a certain degree of autonomy of the

1. Observation The action of a teacher is observed and
processed.
2. Representation The action is represented through
an internal model.
3. Reproduction Based on the internal model, the situation, and environment, an appropriate variant of the
action is reproduced.
The process of observation involves an abstraction step, which
can happen at different levels of cognitive complexity. For
example, it might involve dissecting the seen action into simpler components which are part of the imitator’s repertoire
of skills. At a higher level of complexity, it involves analyzing the relevant environmental information accompanying
the action, such as manipulated tools. At the highest level
of complexity, observation also includes an act of ‘understanding’. Here, the imitator infers the goals and intentions
behind the perceived action. The observed action is then
represented through an internal model. For this, a mapping
from the teacher’s body onto the student’s body has to be
applied. Each body part of the teacher has to be put in relation to the student’s own body part, such that the internal
model can afterwards be used to replicate the observed action. In the literature, this is known as the ‘correspondence
problem’ [18].
Piaget’s studies regarding imitation processes in children
led to the distinction between two forms of imitation ([20],
see also [4]): conservative and true imitation. Conservative
imitation means imitating an action through already available behaviors. True imitation acts on a higher level and
1
Although there exist more recent definitions in the literature, many of them are conflicting or even confusing.

146

generates new behaviors to accurately imitate actions with
a greater understanding of what such actions are about.
Meltzoff and colleagues (see [16] and [21]) studied the imitative learning abilities of infants and came up with a four
stage progression of imitative abilities. In the first stage, a
so-called body babbling phase, the infant explores its body
and learns how specific muscle movements achieve elementary body configurations. The result is an internal model of
the infant’s own body. Adding to this, the infant learns a
set of motor primitives which can afterwards be connected
to achieve complex movements. After the body babbling
phase, the imitative abilities progress as follows:

Figure 1: The process of imitation
when it performed the action itself. For example, when seeing someone grasping food, the monkey activated the same
neuron that would in another situation make it perform a
grasp for food itself. This established for the first time a
direct connection between action observation and action execution. It was also shown that mirror neurons only fire for
goal-directed movements with a target. They do not respond
to seeing objects alone or a mimicked action in absence of a
target. This suggests that “the difference between imitation
and understanding is that, in the case of imitation, the observed act is not only internally represented, but must also
be externally manifested”[23]. Although this view is still debated in the neuroscience literature, it surely highlights the
tight bonds between action understanding and imitation.

1. Imitation of Body Movements In this stage, the
infant uses it’s body parts to imitate observed body
movements or facial acts. First they activate the corresponding body part, then they correct their imitative
response until they converge on the accurate match.
2. Imitation of Actions on Objects In this stage,
infants learn to imitate the manipulation of objects
which are external to their body. This includes playing with toys in a variety of contexts.
3. Inferring Intentions This is the highest form of imitative learning. It requires inferring the goals and
intentions of the demonstrator from his observed behavior. In such a case, even an unsuccessful act can
be correctly imitated.

4. A REVIEW OF COMPUTATIONAL
APPROACHES TO IMITATION
LEARNING

In the imitation of body movements phase, the child imitates
observed movements of a person by mapping them to one (or
a set) of it’s own motor primitives. Such a behavior is often
called ‘mimicry’; the mere reproduction of movements without having the same intention or goal. In the imitation of action on objects stage, the infant is able to learn manipulation
tasks. In this stage, the imitative behavior becomes more
goal-oriented. Although the child might not infer the purpose of the manipulation task, it is able to understand the
basic steps involved and reproduce the task under different
conditions. Obviously, this needs a more sophisticated internal model, in which relations between actions and objects
are also stored. The internal model must also be complex
enough to include models of the physics of passive objects.
For example, a child might have to represent that bigger objects are typically heavier than smaller ones. Finally, in the
last phase infants are able to understand seen actions and
infer goals and intentions behind them. In such a case, the
internal representation might only include the ends but not
the means to achieve them. Representing an action through
such high-level models enables for high ability of generalization. If the goal is clear, an action can be imitated even in
a different context such as an unseen situation.
From the above, it becomes obvious that action understanding plays a vital role in imitation. A recent hypothesis in the neurophysiological literature suggests, that action
understanding and action execution are based on a shared
neural substrate. This, so called ‘direct-matching hypothesis’ was advanced by Rizzolatti and colleagues [23] as a
consequence of the accumulating empirical evidence for the
existence of mirror systems in humans and monkeys. Mirror
systems or more specifically mirror neurons were first discovered in a sector of the premotor cortex of monkeys. Interestingly, these neurons fired both when the monkey saw another (living) individual performing a particular action and

Recent years have seen a growing interest in computational models of imitation learning, mainly in the field of
robotics as a method of ‘Programming by Demonstration
(PbD)’; the edited collections of Dautenhahn & Nehaniv [7]
and Billard & Siegwart [5] provide general overviews.
The following review of selected techniques for computational imitation learning is based on the work by Bakker
and Kuniyoshi [4] and on our discussion in Section 3. The
review is structured through raising the following questions:
When observing, who and how should be observed? How
are the observed actions represented? And how can the
seen action be converted to the observer’s actuators (correspondence problem)? When reproducing the actions, how
to adapt the actions to the current context? And are the
motor primitives used in actions learned or explicitly programmed? Figure 1 shows the process of imitation and the
questions involved.
Further, w.r.t. the classes of imitative abilities identified
by Meltzoff et al. [16], cf. Section 3: Does the observer imitate body movements (trajectories)? Do the imitated actions involve the manipulation of objects? And finally, does
the observer infer the intentions and imitate the intentions
behind an observed action?
In this section, we answer the above questions for a number of recent publications on imitation learning. The chosen
publications reflect different approaches to the problem and
can roughly be categorized in two groups: biologically inspired approaches and engineering oriented approaches.

4.1 Biologically Inspired Approaches
Mataric [14, 15] describes a biologically inspired, behaviorbased approach for imitation learning in embodied agents,

147

Table 1: Comparison of different biologically inspired approaches to imitation.
Mataric [14, 15]
Rao et al. [21]
Oztop and Arbib [19]
Observation
-How?
2D vision, magnetic
synthetic vision
3D simulator,
trackers, exoskeleton
2D vision
-What?
upper body postures of
grid location of
hand state trajectory
user
virtual teacher
during grasping
Representation
-How to represent?
sequences of
forward/inverse models using
neural network in
visuo-motor primitives
state transition probabilities
core mirror system
classify as best
computing state transition
matching in core
-How to convert to
observer’s actuators?
matching basic behavior
probabilities
mirror system
Reproduction
-How to adapt to
(not applicable)
Bayesian inference
neural networks reactive to
and probability maximization
visual input
current context?
programmed / learned
programmed
programmed
-Learned or programmed
motor primitives?
Imitation of
-Body Movements?
yes
yes
no
-Actions on Objects?
no
no
yes
no
yes
no
-Inferred Intentions?

which could be robots or virtual characters. The agents
dispose of a pre-programmed – or alternatively, learned –
set of basic behaviors called “perceptual-motor primitives”
which serve to segment and continuously classify the movements of the human trainer as well as to generate the agent’s
movement. The primitives can further be parametrized with
metric values such that they are sufficient, when properly
combined, for generating a large range of complex behaviors. Imitation learning thus creates new skills as novel sequences and superpositions of classified primitives. As in
motion capture, the goal of her imitation learning approach
consists of the humanoid agents repeating the movements of
the human trainer. An advantage of the behavior-based approach over motion capture is that it allows the robot a certain degree of freedom in the interpretation of the observed
movement and thus naturally generalizes over varying body
sizes. The reported setting is limited to the imitation of arm
movements, tracked e.g. visually or using magnetic markers,
and does not involve interactions with scene objects.
Rao and colleagues [21] propose a probabilistic framework
in which they formalize Meltzoff and Moores’s four-stage
progression model of imitative abilities in infants [16] (see
Section 3). They apply this model for learning how to solve
a (simulated) maze task through imitation. In the body
babbling phase, the imitating agent first wanders through
the maze by performing random actions. The frequencies of
the outcomes of each state-action pair are recorded in order to compute probabilities. These probabilities represent
a forward model of the environment: a model predicting the
next state of a system, given the current state and the action
to be executed. Then, the imitator observes a sequence of
states (grid positions) the teacher went through in order to
solve the maze. From this sequence, a so called inverse model
is learned: a model that tells what action to choose given
the current state and the desired goal. Reproduction of
the seen action is accomplished using probabilistic inference
techniques. Using these techniques and the internal models

(forward and inverse), the imitator is even able to infer the
intent of the teacher. Due to the power of Bayesian inference in dealing with uncertainty, noise, and missing data, a
high level of generalization can be achieved. However, due
to the simplicity of the maze domain, imitation did not include complex interaction with scene objects. The primitive
behaviors for moving in the maze such as move north were
also pre-programmed.
Oztop and Arbib [19] present a detailed computational
model of reaching and grasping that integrates many neurophysiological findings of the monkey’s brain. In particular, they develop a computational account of the workings of the mirror neuron system, a brain region activated
both when an action is observed and when the action is performed. Their computational model consists of three ‘grand
schemas’: Schema 1, ‘reach and grasp’, takes input from
the vision system, extracts the object position and object
‘affordances’ such as size; it further encapsulates motor programs to execute the grasp. Schema 2, ‘visual analysis of
hand state’, also takes input from the vision system but is
concerned with the extraction of ‘hand state’ information.
The extracted hand state includes e.g. time based information about the distance and angle between hand and target
object, wrist velocity, and several hand shape parameters
such as aperture. Finally, schema 3, ‘core mirror circuit’,
takes input from the two other schemas and is responsible
for the generation of action code to control the grasping
action. To test the model, they implemented a simple virtual environment where a stylized (monkey or human) arm
simulates the grasping of an object. For further validation,
they also describe experimental work with 2D vision input.
Overall, the article focusses on the logic of the mirror neuron system rather than a more complete imitation system.
Furthermore, actions other than grasping for which mirror
system activity has been reported are not accounted for by
the model.

148

Table 2: Comparison of different engineering oriented approaches to imitation.
Aleotti et al.[1]
Buchsbaum&Blumberg [6]
Dillman et al. [8]
Observation
-How?
-What?
Representation
-How to represent?
-How to convert to
observer’s actuators?
Reproduction
-How to adapt to
current context?
-Learned or programmed
motor primitives?
Imitation of
-Body Movements?
-Actions on Objects?
-Inferred Intentions?

data glove, magnetic trackers,
2D vision system
hand position and
hand posture of user

positions of body parts of
other virtual character

data glove, force sensors,
magnetic trackers, cameras
hand trajectory and
hand posture of user

classes of pre-programmed
tasks for movement synthesis
classify tasks

path through pose
graph
search for similar path
in own graph

tree structure of
elementary actions
set of heuristic rules for
sensor control

(not addressed)

(not addressed)

programmed

programmed

parametrization of
pre-defined programs
programmed

no
yes
no

yes
yes
yes

no
yes
no

4.2 Engineering Oriented Approaches

synthetic vision

and dynamic grasps. The segmented elementary actions are
then stored in a tree structure of operations. In turn to
map this representation of operations to a robot, first the
human grasp is mapped to robotic grippers (with less fingers) by “calculating an optimal group of coupled fingers”,
which exert a force in a common direction. To generate
the arm trajectory, a set of logical rules for selecting sensor constraints, like force thresholds, etc. is used. The rules
are based on the current context of operation like approach
phase, grasp, retract phase, etc. The parameters generated
by this mapping are then used to trigger and parametrize a
pre-defined robot program, which, after a first step of testing the mapped trajectory, executes the movement in the
real world. Overall this approach allows to instruct a robot
to execute previously specified tasks in a specific way, as
shown by the user. However, neither new movement or operation types can be learned, nor are inferences made about
the intention of demonstrated tasks.
Buchsbaum and Blumberg apply imitation learning to
a computer animation setting where one virtual character
learns the actions of another one [6]. Two virtual articulated
characters in the shape of mice observe each other through a
simple form of synthetic vision. The characters perceive each
other as a set of color-coded dots, which represent the position of the various body parts, like finger tips, nose, shoulders, etc. The motor system for animation of these figures
is based on a multi-resolution variant of a standard motiongraph they call posegraph. This graph consists of a number
of pre-defined poses as nodes with the edges defining valid
transitions between poses. Basic movements are defined as
paths through this graph. When one character observes the
other’s motion it perceives the global positions of several
key body parts through the synthetic vision system. After
transforming these absolute positions to body-root-relative
positions the input pose is compared to the character’s own
known poses by use of a euclidean distance metrics. Movements are segmented at certain transitory poses, such as

Aleotti et al. [1] use the Programming by Demonstration
paradigm to instruct a robot in picking and placing objects
in a scene. Instruction and first-time reproduction occur
in a virtual environment, enriched with vibro-tactile feedback and visual fixtures. Observation of the user is achieved
via a data glove and a magnetic position tracking system.
The demonstrated action sequence is represented by instantiating a hierarchical structure of classes describing possible
basic and high level tasks. Segmentation is achieved with
a third-party software provided with the data glove. It can
determine when grasps take place and when they stop. Using this representation they map the high level tasks to a
robot and control it in a virtual environment. If the result is
sufficient, the real robot repeats this action sequence. The
pick-and-place tasks are pre-programmed. The mapping of
actions from the human to the robot succeeds through high
level representations that are independent of either agent’s
geometry. A prerequisite is of course that pre-programmed
task models are available.
Dillman et al. [8] present an approach of PbD for humanoid service robots. The laying of a table is considered
as example scenario, which involves mainly pick and place
operations. User actions are tracked via a data glove, fitted
with force sensors, and magnetic position trackers. Additionally an active trinocular camera head observes the user
visually. Hand movements are segmented into elementary
actions like moves, static and dynamic grasps, which are
in turn chunked into semantically related groups, e.g. approach phase, grasp phase and release phase. Segmentation takes place in a two-phase process. In the first phase,
the hand trajectory is segmented at times of contact between hand and object by analyzing the force values with
a threshold based algorithm. In the second phase, where
the actions during a grasp are segmented and analyzed, actions are classified into three classes of force value profiles by
processing force sensor input: static grasps, external forces

149

4.4 Summary

standing, which are assumed to always be taken between
movements. Each observed movement is then compared to
the character’s own known movements by searching for a
path through the posegraph with a minimal overall distance
value. The movement represented by this path is then identified by the character as the observed movement. Reasoning
about the intentions behind observed actions is enabled by
a hierarchically organized action system, which is composed
of individual behavior units, referred to as action tuples.
Hereby an action consisting of one or more movements is
annotated with trigger contexts, optional objects of attention and do-until context, that determine when the action
finishes. Reasoning is again performed by searching a path
through this hierarchy from a top-level motivation to the
observed movements at the leaf nodes. Along the path the
annotated constraints are attempted to be matched. This
way a reasoning from observed movements towards the underlying motivations is possible. The downside of this model
is, however, that everything from the behavior hierarchy to
the identified poses is predefined by the developers. New
movements could theoretically be identified and stored as
new paths through the motion-graph. The basic poses the
movement consists of, however, still have to be pre-defined.
A further restriction lies in the restriction to characters with
the same skeletal structure, with the correspondence of body
parts of one character to another being hard-wired. It remains unclear how basic movements could be adapted to
dynamic changes in the situation, e.g. the target object of
a reach motion changing its position.

In recent years an increasing body of research on imitation in artificial characters and robots has been published.
Not all of the problems tackled in these publications are of
interest for achieving action capture. Still, it becomes obvious from the above review that some common difficulties
have been attacked and (partial) solutions proposed. Applications of these solutions on real-world robots show remarkable results. It also becomes obvious from the review, that
different approaches to imitation exist. Often, however, the
following assumptions can be found made in computational
realizations of imitation learning: (1) The learning agent
is equipped with a repertoire of primitive behaviors/motor
skills/actions (2) Imitation involves finding new combinations or sequences of these primitive behaviors. (3) Imitated
actions are represented through complex structures such as
inverse models or plans. (4) The correspondence problem,
i.e. the mapping of observed movements to the imitator’s
body is tackled through abstraction of movements to actions /behaviors to replicate the effects of actions rather
than outer appearance of motion.
With action capture we aim at finding an approach to
imitation which is particularly suitable for virtual reality
and computer animation applications. With the hope of
building on achievements of the community, we summarized
in this section some of the influential papers on imitation.
Tables 1 and 2 summarize in tabular form how each of the
papers addressed the questions posed for the review.

5. ACTION CAPTURE

4.3 Related Work

Based on the above discussion on previous work on imitation learning we are now in a position to frame an adaptation to virtual environments that we call action capture.
One way to conceptualize action capture is as an extension
of motion capture where the human trainer’s performance is
recorded not (only) at the lower level of movements but at
the higher level of actions, particularly actions involving the
manipulation of scene objects. Another way to conceptualize action capture is as a learning method for behavior-based
virtual characters that empowers virtual characters to learn
novel complex behaviors from basic behaviors by imitating
a human trainer. Whichever conceptualization is chosen, a
feature of action capture is that the learned actions / behaviors are inherently adaptable to situations that are similar
but not necessarily identical to the learning situation.
In the following we will first present a basic framework
for action capture. The basic framework can be seen as
a translation of operational approaches to imitation learning in robotics to immersive VR; it is further restricted to
actions on objects, i.e. manipulation tasks. We will then
discuss possible choices of action representations and point
out directions for extending the framework.

Some other interesting work on imitation has been carried out by Ijspeert et al. [11]. Using a technique called locally weighted regression they approximate the trajectories
of a human demonstrator. The approximated trajectory is
stored as a set of nonlinear differential equations which form
a control policy for a humanoid robot. The power of this
approach was shown by having a humanoid robot with 30
degrees of freedom imitate a tennis forehand and backhand
swing. The ALICE system proposed by Alissandrakis et
al. [2] was also able to achieve imitation in robots. This
system mainly focusses on solving the correspondence problem. To this end, a so called ‘correspondence library’, which
maps actions, states and effects of a teacher to those of the
imitator, was introduced. A generation mechanism proposes
for each observed action a candidate corresponding action.
Using a specified metrics, the system then decides whether
to apply an action from the correspondence library or from
the generation mechanism. In the case where the generation mechanism proposes a better corresponding action the
library gets updated. In Nakanishi et al. [17] it was also
demonstrated that imitation can be used to teach a biped
robot complex locomotion and self stabilization skills. The
learned locomotion controllers enabled the robot to walk
over surfaces with different friction properties without loosing balance. In contrast to the mainly robotics centered
research, the work of Kopp and Graeser [12] mainly focused
on virtual embodied agents. They proposed a motor control framework which is based on a combination of coupled
forward and inverse models, and graph-based representations. The framework enables the imitator to predict and execute the teacher’s most probable next move by traversing a
graph-based representation called ‘motor command graph’.

5.1 A Basic Framework for Action Capture
Action capture is a VR-based method for recording the
actions of a human VR user and later reproducing these actions by virtual characters. In general, with ‘action’ we refer to any kind of intentional motor behavior. For the basic
framework, we restrict ‘actions’ to manipulations of scene
objects. Actions are decomposable into primitive actions
which correspond to basic behaviors of the virtual charac-

150

Concerning the parametrization of actions in the representation, varying degrees of abstraction can be imagined which
will correspond to different generalization capabilities. For
example, in a simple case, the virtual character may have
to perform its actions in the same task environment, on
identical objects, which are in a same or similar initial configuration as demonstrated before by the human VR user.
Here, it may suffice to parametrize the action representation
with an internal identifier of the manipulated object, e.g.
grasp(block-1). In more challenging task environments, the
virtual character might be expected to repeat the demonstrated not necessarily on identical objects but on objects
of the same type. Then the action representation might be
parametrized with something like grasp(block(green)) which
will require some additional instantiation mechanism during
action reproduction.
Another aspect in the choice of the action representation
concerns a trade-off between generality and accuracy. At one
end of the spectrum is task-level imitation where actions are
just parametrized with their target objects. Task-level imitation is very general and relatively easy to adapt to novel
situations but may result in movements which are unfaithful
to the original movement. For example, the result of capturing a soccer player kicking the ball might be a very stiff
and robot-like kick, when the action is applied on a virtual
character. Important properties of the motion such as timing, grace, style, and realism might get lost when focusing
on high generalization solely. Thus, when faithfulness of the
animation is important, then it may be useful to parametrize
the action representation further with movement and posture data. Movements and postures could be represented in
a variety of ways, including joint angle values, trajectories,
results from Principal Component Analyzes, contact points
of hands with scene objects, symbolic descriptions of shape
or movement, etc. Furthermore, one way to cope with the
tradeoff between generalization and faithfulness of the animation could be the use of multi-level representations that
specialize on different levels of detail. Representations at
higher levels can achieve high generalization, while representations at lower levels can focus on including subtle features
of the original movement into the action.

ters. The setting for action capture thus consists of:
• Virtual environment: which supports its interactive
manipulation by a human user. In particular, whenever the user manipulates a scene object, the virtual
environment can detect this manipulation and generate a corresponding event, e.g., as simple cases, pushed(
button-1) or grasped(block-2).
• Human teacher: who performs an action or a sequence
of actions in the virtual environment. The human
teacher’s actions are typically tracked using standard
VR input devices such as position trackers and data
gloves although in principle alternative methods e.g.
based on visual input are also possible.
• Virtual character (learner): who observes the teacher’s
actions and learns to repeat them. The virtual character’s body is assumed to be similar to the teacher’s
body, i.e. humanoid. This assumption ensures a more
or less straightforward mapping of the teacher’s body
parts to the virtual character’s body, thus simplifying
the solution to the correspondence problem. The virtual character’s body size and proportions may however differ from the human VR user. The virtual character further is equipped with a repertoire of basic behaviors, e.g. for pushing a button or grasping an object. These basic behaviors may be further parametrized, e.g. with a target position, a target object, or
a hand shape to be assumed during a manipulation
action.
The teacher’s actions are tracked and abstracted to action representations that allow a later reproduction of the
actions. The phases of action capture and reproduction are:
1. Action capture: during which the teacher’s movements
are tracked, segmented, classified as actions, and stored
as high-level representations of the action or action sequence. The action representation should at least be
expressed at the level of basic behaviors. Segmentation and classification of the teacher’s movements can
be informed by the events thrown by the VR system
whenever interesting parts of a scene manipulation occur. Different choices of action representations are discussed below.

5.3 Extending the Basic Framework

2. Action reproduction: where the action’s representation is mapped to behaviors of the virtual character
and the behaviors are executed.

In the basic framework of action capture, we restricted
‘action’ to the manipulation of scene objects from the more
general view of ‘action’ as any kind of intentional motor behavior. The pragmatic reason for this limitation lay in a desired reduction in complexity and the fact that VR systems
extend motion capture systems exactly with capabilities for
processing interactions with scene objects. Using Meltzoff’s
et al. classification ([16], also see Sec. 3), other levels of imitation learning include the mimicking of body movements
and imitation at the level of inferring intentions. To simply
mimic the movements of a human teacher, standard motion
capture equipment without immersion in an virtual environment suffices; however, to count as ‘action capture’, the
recorded movements should be abstracted to the level of actions. One way of addressing the challenging problem of
inferring the intentions behind seen actions could involve
the integration of further task and domain knowledge into
the action capture process.

5.2 Action Representations
The choice of the action representation in general depends
on the task environment and expected degree of faithfulness
of the animation; it is thus application-specific. However,
the internal representation of the observed actions should
at least be expressed at the level of basic behaviors, i.e. at a
higher level than movement data. An action representation
may however include movement data, e.g. default postures
that parametrize actions/behaviors. The internal representation should also allow for sequences and hierarchical organization of actions/behaviors. Means for representing parallel actions are useful if e.g. two-handed manipulations are
considered in the application scenario.

151

Action Capture

Action Reproduction
Grasp
Taxonomy

Reach Detection

Plans

"Grasp Object"

...

"Move Object"

Release Detection
Grasp Detection
Grasp Classification

Action
Representation

Behaviors

Open Hand
Reach

Close Hand

Contact Points

User Hand

grasped?

Object

Motor
Programs
Annotated
Objects

decrease
r_thumb1
flexion

increase
r_thumb2
flexion

...

Figure 2: Illustration of Action Capture and Action Reproduction in a prototypical implementation

6.

AN EXAMPLE: THE VIRTUAL
WORKERS PROJECT

termined: tracking position and orientation values of the
upper limbs, finger joint angle values, and collision points
of the user’s hands with the virtual objects. The grasp is
then classified w.r.t. a grasp taxonomy to provide an abstract description of hand shape. In our current prototype,
Schlesinger’s grasp taxonomy [24] as summarized by Taylor and Schwarz [25] is used. In other work, we report how
recognition of the Schlesinger grasps can be achieved reliably and in real-time even with uncalibrated data gloves
[10]. Figure 3 gives an example of the various informations
recorded in a grasp event. In the next step of motion analysis, grasp events are processed in order to dynamically generate an abstract action representation or plan. A plan is
a specification of sequential or parallel actions that can be
executed by the behavioral animation system used for action reproduction. In the current implementation, plans are
generated via a simple template-based approach that maps

In the Virtual Workers project we aim at giving virtual
humans the ability to learn and imitate object manipulations, particularly manipulations that involve different types
of grasping. A major goal is to make the learned animations
robust against dynamic scene changes such as repositioned
or resized objects. In an example scenario, a virtual worker
is located in a virtual environment that consists of various objects including a hammer, screwdrivers, etc. (see Figure 5). The user, who acts as teacher for the virtual worker,
interacts with the environment on a wall-sized stereo projection. User movements are tracked by means of a 22 sensor
data glove and an optical tracking system. For interaction
with the virtual objects, a model of the user hand is projected into the virtual scene and checked for collisions with
the virtual objects. The user can grasp objects and move
them around or perform pre-defined functions on them, e.g.
push a button, etc. These user actions are later reproduced
by the virtual worker through means of action capture techniques.
Figure 2 illustrates the main components of the prototypical system architecture for action capture and reproduction. The action capture modules extract significant events
from the continuous user interaction and generate higher
level representations of the actions. Action reproduction is
achieved through a multi-layer behavioral animation architecture. Action capture and reproduction modules as well as
action representations may optionally refer to a grasp taxonomy which provides symbolic descriptions of hand shapes
during grasping. Similarly, a database of object annotations may optionally be used that provides default information of typical grasp positions and orientations in an objectcentered coordinate frame. These main components of the
system architecture will now be described in more detail.
In the action capture phase, user interactions are first examined for relevant events. In the chosen scenario, these
events correspond to grasp-related user actions and are generated e.g. when an object is grasped, released again, or
when a reach motion is initiated. E.g. when a grasp is
detected, first basic features of the user interaction are de-

Figure 3: Example grasp event
<event-sequence>
<event timestamp="3.895" type="grasp">
<low-level>
<joint-angle joint-id="r_index1">19.8306 -0.0865252
0.678805 0.729203</joint-angle>
<contact-point joint-id="sensor_r_index1">
<object>Ball-1</object>
<pos>0.00730081 -0.0734563 0.0135953</pos>
</contact-point>
<object-ids> Ball-1 </object-ids>
<hand-transform>0.0205884 0.211408 -0.97718 0
0.0805939 0.973855 0.212386 0
0.996533 -0.0831275 0.00301189 0
-0.1502 -0.626599 0.917001 1
</hand-transform>
<hand>right</hand>
</low-level>
<high-level>
<taxonomy>schlesinger</taxonomy>
<category>spherical</category>
</high-level>
</event>
...
</event-sequence>

152

events to actions. The example plan in Figure 4 specifies the
actions to replicate a complex grasping action. The plan describes the parallel execution of two behaviors for reaching
and opening of the hand; then follows a hand closing behavior that results in a spherical grasp of the scene object.

Figure 4: Example of a dynamically generated plan:
grasping an object
<plan type="captured">
<parallel>
<behavior>
<type>Reach</type>
<param name="object">Ball-1</param>
</behavior>
<behavior>
<type>GraspOpen</type>
<param name="preshape">spherical</param>
</behavior>
</parallel>
<behavior>
<type>GraspClose</type>
<param name="grasp-type">spherical</param>
</behavior>
</plan>

Figure 5: User demonstrating a grasp and the virtual human imitating it

Reproduction of captured actions through the virtual worker is achieved via a multi-level behavioral animation approach. At the highest level, predefined plans describe the
decomposition of complex animations into combinations of
primitive actions. Predefined plans contain useful, often reoccurring action sequences, such as grasping an object, pick
and place operations, etc. Primitive actions are executed by
parametrizable mid-level behaviors, such as reaching a goal
position, closing or opening the hand, etc. Behavior execution integrates collision sensor feedback, such that e.g. a
hand closing behavior stops when fingers make contact with
an object. Finally, low-level motor programs are responsible
for controlling body movements.
The dynamically generated plans resulting from the action capture phase must be composed of actions which can
be reproduced by the virtual worker. I.e. each action in a
captured plan must correspond to either a behavior or a
predefined plan. Captured plans can however combine the
predefined plans and behaviors in novel ways. In this way,
action capture enables the virtual agent to learn new tasks
as novel action combinations from a built-in behavior repertoire.
Note that in the particular plan of Figure 4, the grasp
action is specified in a comparatively abstract manner. Detailed information about hand position and orientation, joint
angles, contact points, and timing available in the grasp
events are deliberately missing in the plan. Through this,
plan execution becomes robust against certain variations in
the scene configuration such as changes to object location
and size as well as differently sized virtual humans. Alternatively, the plan might have included such detailed information – at the cost of robustness against scene alterations but
possibly gaining more accuracy w.r.t. the style of the original grasping action of the user. The deep exploration of this
design space of action representation, and more generally,
action capture is topic of on-going and future work.

7. CONCLUSION
In this paper, we have motivated and introduced an extension to motion capture, called action capture, which has its
roots in imitation learning. Action capture aims at recording flexible, responsive, and adaptable animations suitable
for interactive and physics-based virtual worlds. In particular, it extends motion capture systems with capabilities for
processing interactions with scene objects.
We first presented previous work on imitation learning,
which then helped to specify a basic framework of required
features. We also described an extension to the basic framework, which includes additional, more complex features. Finally, we presented the ‘Virtual Workers’ project: an example application in which virtual agents learn manipulation
tasks through action capture.
One of the main goals of this paper was to show that imitation can be a powerful tool for animating virtual characters.
Although there exists already a variety of publications on
imitation learning, much of the published work focuses on
topics such as robotics or biological models. By introducing a special framework for action capture, we limited the
research questions to the ones which are of interest to the
VR community. We believe that action capture will prove
particularly beneficial in virtual prototyping settings that
require the automated generation of animations for many
variants of prototypes and virtual humans.

8. ACKNOWLEDGMENTS
The research described in this contribution is partially
supported by the DFG (Deutsche Forschungsgemeinschaft)
in the Virtual Workers project.

153

9.

REFERENCES

[15] M. Mataric. Sensory-motor primitives as a basis for
learning by imitation: Linking perception to action
and biology to robotics. In Dautenhahn and Nehaniv
[7].
[16] A. N. Meltzoff. The Human Infant as Imitative
Generalist: A 20-year Progress Report on Infant
Imitation with Implications for Comparative
Psychology. In Social Learning in Animals: The Roots
of Culture, pages 347–370, 1996.
[17] J. Nakanishi, J. Morimoto, G. Endo, G. Cheng,
S. Schaal, and M. Kawato. Learning from
Demonstration and Adaptation of Biped Locomotion.
Robotics and Autonomous Systems, 47(2-3):79–91,
2004.
[18] C. Nehaniv and K. Dautenhahn. The Correspondence
Problem. In Dautenhahn and Nehaniv [7], pages
41–61.
[19] E. Oztop and M. Arbib. Schema design and
implementation of the grasp-related mirror neuron
system. Biological Cybernetics, 87:116–140, 2002.
[20] J. Piaget. Play, Dreams and Imitation in Childhood.
New York: W. W. Norton, 1962.
[21] R. Rao, A. P. Shon, and A. N. Meltzoff. A Bayesian
Model of Imitation in Infants and Robots. In
Imitation and Social Learning in Robots, Humans and
Animals: Behavioural, Social and Communicative
Dimensions, 2004.
[22] C. Reynolds. Flocks, Herds and Schools: A
Distributed Behavioural Model. Computer Graphics,
21(4):25–34, 1987.
[23] G. Rizzolatti, L. Fogassi, and V. Gallese.
Neurophysiological Mechanisms Underlying the
Understanding and Imitation of Action. Nature
Reviews Neuroscience, pages 661–770, September
2001.
[24] G. Schlesinger. Der Mechanische Aufbau der
Künstlichen Glieder. In M. Borchardt et al., editors,
Ersatzglieder und Arbeitshilfen für Kriegsbeschädigte
und Unfallverletzte, pages 321–661. Springer-Verlag:
Berlin, Germany, 1919.
[25] C. Taylor and R. Schwarz. The Anatomy and
Mechanics of the Human Hand. Artificial Limbs,
2:22–35, 1955.
[26] E. L. Thorndike. Animal Intelligence: An
Experimental Study of the Associative Processes in
Animals. Psychological Review Monographs, 8, 1898.
[27] B. Tomlinson. From Linear to Interactive Animation:
How Autonomous Characters Change the Process and
Product of Animating. ACM Computers In
Entertainment, 3(1), 2005.
[28] A. Witkin and M. Kass. Spacetime Constraints. In
SIGGRAPH’88 Conference Proceedings, volume 22,
pages 159–168. ACM, 1988.
[29] V. B. Zordan, A. Majkowska, B. Chiu, and M. Fast.
Dynamic Response for Motion Capture Animation.
ACM Transactions on Graphics, 24(3):697–701, 2005.

[1] J. Aleotti, S. Caselli, and M. Reggiani. Leveraging on
a Virtual Environment for Robot Programming by
Demonstration. In IEEE/RSJ Intl. Conf. on
Intelligent Robots and Systems IROS 2003, Workshop
on Robot Programming by Demonstration, Las Vegas
(USA), 2003.
[2] A. Alissandrakis, C. L. Nehaniv, and K. Dautenhahn.
Imitation with ALICE: Learning to Imitate
Corresponding Actions Across Dissimilar
Embodiments. IEEE Trans. Systems, Man and
Cybernetics, 32(4):482–296, 2002.
[3] M. A. Arbib. The mirror system, imitation, and the
evolution of language. In Dautenhahn and Nehaniv [7].
[4] P. Bakker and Y. Kuniyoshi. Robot see, Robot do: An
Overview of Robot Imitation. In AISB96 Workshop:
Learning in Robots and Animals, pages 3–11, 1996.
[5] A. Billard and R. Siegwart, editors. Special Issue on
Robot Learning from Demonstration, volume 47 of
Robotics and Autonomous Systems, 2004.
[6] D. Buchsbaum and B. Blumberg. Imitation as a First
Step to Social Learning In Synthetic Characters: A
Graph-based Approach. In SCA ’05: Proceedings of
the 2005 ACM SIGGRAPH/Eurographics Symposium
on Computer Animation, pages 9–18, New York, NY,
USA, 2005. ACM Press.
[7] K. Dautenhahn and C. Nehaniv, editors. Imitation in
Animals and Artifacts. MIT Press, 2002.
[8] R. Dillmann, M. Ehrenmann, P. Steinhaus,
O. Rogalla, and R. Zöllner. Human Friendly
Programming of Humanoid Robots: The German
Collaborative Research Center. In IARP 2002, 2002.
[9] M. Gleicher. Retargetting Motion to New Characters.
In SIGGRAPH’98 Conference Proceedings, Computer
Graphics Annual Conference Series, pages 33–42.
ACM, 1998.
[10] G. Heumer, H. Ben Amor, M. Weber, and B. Jung.
Calibration-free Recognition of Grasp Types - A
Comparison of Classification Methods. In Proceedings
Dritter Workshop Virtuelle und Erweiterte Realität
der GI-Fachgruppe VR/AR, 2006.
[11] A. Ijspeert, J. Nakanishi, and S. Schaal. Movement
Imitation with Nonlinear Dynamical Systems in
Humanoid Robots. In IEEE International Conference
on Robotics and Automation, 2002.
[12] S. Kopp and O. Graeser. Imitation Learning and
Response Facilitation in Embodied Agents. In
Intelligent Virtual Agents 2006, LNAI, pages 28–41.
Springer-Verlag, Berlin, 2006.
[13] L. Kovar, M. Gleicher, and F. Pighin. Motion Graphs.
ACM Transactions on Graphics, 21(3), 2002.
[14] M. Mataric. Getting Humanoids to Move and Imitate.
IEEE Intelligent Systems, pages 18–24, July/August
2000.

154

2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China

Interaction Primitives for Human-Robot Cooperation Tasks
Heni Ben Amor1 , Gerhard Neumann2 , Sanket Kamthe2 , Oliver Kroemer2 , Jan Peters2

Abstract— To engage in cooperative activities with human
partners, robots have to possess basic interactive abilities
and skills. However, programming such interactive skills is a
challenging task, as each interaction partner can have different
timing or an alternative way of executing movements. In this
paper, we propose to learn interaction skills by observing how
two humans engage in a similar task. To this end, we introduce
a new representation called Interaction Primitives. Interaction
primitives build on the framework of dynamic motor primitives
(DMPs) by maintaining a distribution over the parameters of
the DMP. With this distribution, we can learn the inherent
correlations of cooperative activities which allow us to infer the
behavior of the partner and to participate in the cooperation.
We will provide algorithms for synchronizing and adapting the
behavior of humans and robots during joint physical activities.

I. INTRODUCTION
Creating autonomous robots that assist humans in situations of daily life has always been among the most important
visions in robotics research. Such human-friendly assistive
robotics requires robots with dexterous manipulation abilities
and safe compliant control as well as algorithms for humanrobot interaction during skill acquisition. Today, however,
most robots have limited interaction capabilities and are not
prepared to appropriately respond to the movements and
behaviors of their human partners. The main reason for
this limitation is the fact that programming robots for such
interaction scenarios is notoriously hard, as it is difficult to
foresee many possible actions and responses of the human
counterpart.
Over the last ten years, the field of imitation learning [12]
has made tremendous progress. In imitation learning, a user
does not specify the robot’s movements using traditional
programming languages. Instead, he only provides one or
more demonstrations of the desired behavior. Based on
these demonstrations, the robot autonomously generates a
control program that allows it to generalize the skill to
different situations. Imitation learning has been successfully
used to learn a wide range of tasks in robotics [2], such
as basic robot walking [4], [5], driving robot cars [10],
object manipulation [9], and helicopter manoeuvring [1].
A particularly successful approach to imitation learning is
based on Dynamic Motor Primitives (DMPs)[6]. DMPs use
dynamical systems as a way of representing control policies,
which can be generalized to new situations. Several motor
1 Institute

for Robotics and Intelligent Machines, Georgia Institute of
Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA
2 Intelligent Autonomous Systems, Department of Computer Science,
Technical University Darmstadt, Hochschulstr. 10, 64289 Darmstadt, Germany

978-1-4799-3685-4/14/$31.00 ©2014 IEEE

Fig. 1. A human-robot interaction scenario as investigated in this paper.
A robot needs to learn when and how to interact with a human partner.
Programming such a behavior manually is a time-consuming and errorprone process, as it hard to foresee how the interaction partner will behave.

primitives can be chained together to realize more complex
movement sequences.
In this paper, we generalize the concept of imitation learning to human-robot interaction scenarios. In particular, we
learn interactive motor skills, which allow anthropomorphic
robots to engage in joint physical activities with a human
partner. To this end, the movements of two humans are
recorded using motion capture and subsequently used to
learn a compact model of the observed interaction. In the
remainder of this paper, we will call such a model an
Interaction Primitive (IP). A learned IP is used by a robot
to engage in a similar interaction with a human partner. The
main contribution of this paper is to provide the theoretical
foundations of interaction primitives and their algorithmic
realization. We will discuss the general setup and introduce
three core components, namely methods for (1) phase estimation, (2) learning of predictive DMP distributions, and
(3) correlation the movements of two agents. Using examples from handwriting synthesis and human-robot interaction
tasks, we will clarify how these components relate to each
other. Finally, we will apply our approach to real-world
interaction scenarios using motion capture systems.
II. RELATED WORK
Finding simple and natural ways of specifying robot control programs is a focal point in robotics. Imitation learning,
also known as programming by demonstration, has been
proposed as a possible solution to this problem [12]. Most approaches to imitation learning obtain a control policy which

2831

encodes the behavior demonstrated by the user. The policy
can subsequently be used to generate a similar behavior that
is adapted to the current situation. Another way of encoding
policies is to use statistical modeling methods. For example,
in the mimesis model [8] a continuous hidden Markov
model is used for encoding the teacher’s demonstrations.
A similar approach to motion generation is presented by
Calinon et al. [3] who used Gaussian mixture regression to
learn gestures.
The methods discussed so far are limited to single agent
imitation learning scenarios. Recently, various attempts have
been undertaken for using machine learning in human-robotinteraction scenarios. In [13], an extension of the Gaussian
process dynamics model was used to infer the intention of
a human player during a table-tennis game. Through the
analysis of the human player’s movement, a robot player
was able to determine the position to which the ball will be
returned. This predictive ability allowed the robot to initiate
its movements even before the human hit the ball. In [7],
Gaussian mixture models were used to adapt the timing of
a humanoid robot to a human partner in close-contact interaction scenarios. The parameters of the interaction model
were updated using binary evaluation information obtained
from the human. While the approach allowed for humanin-the-loop learning and adaptation, it did not include any
imitation of observed interactions. In a similar vein, the
work in [8] showed how a robot can be actively involved
in learning how to interact with a human partner. The
robot performed a previously learned motion pattern and
observed the partner’s reaction to it. Learning was realized
by recognizing the observed reaction and by encoding the
action-reaction patterns in a HMM. The HMM was then used
to synthesize similar interactions.
In our approach, learning of motion and interaction are
not split into two separate parts. Instead, we learn one integrated interaction primitive which can directly synthesize an
appropriate movement in response to an observed movement
of the human partner. Furthermore, instead of modelling
symbolic action-reaction pairs, our approach models the joint
movement of a continuous level. This continuous control
is realized through the use of DMPs as the underlying
representation. By introducing probabilistic distributions and
bayesian inference in the context of DMPs, we obtain a new
set of tools for predicting and reacting to human movements.
III. INTERACTION PRIMITIVES
The goal of learning an Interaction Primitive is to obtain
a compact representation of a joint physical activity between
two persons and use it in human robot interaction. An interaction primitive specifies how a person adapts his movements
to the movement of the interaction partner, and vice versa.
For example, in a handing-over task, the receiving person
adapts his arm movements to the reaching motion of the
person performing the handing-over. In this paper, we propose an imitation learning approach for learning such interaction primitives. First, one or several demonstrations of the
interaction task are performed by two human demonstrators

in a motion capture environment. Using the motion capture
data, we extract an interaction primitive which specifies the
reciprocal dependencies during the execution of the task.
Finally, the learned model is used by a robot to engage in a
similar interaction with a human partner. An IP should also
be applicable to a wide variety of related interactions, for
example, handing over an object at different locations. An
example of an interaction of a humanoid robot with a human
partner performing a high-five movement is given in Fig. 2.
At the core of our approach is a new representation
for interaction tasks. An IP can formally be regarded as a
special type of DMP which represents a joint activity of
two interaction partners. After an IP is learned from the
demonstration data, it is used to control a robot in a similar
task. For the sake of notational clarity, we will refer to
the first interaction partner, i.e., the human, as the observed
agent, while the second interaction partner, i.e., the robot,
will be called controlled agent.
The IP performs three steps to infer an appropriate way
of reacting to the movement of the observed agent.
1) Phase Estimation: The actions of the interaction partners are executed in synchrony. In particular, the robot
adapt its timing such that it matches the timing of the
human partner. For this synchronization, we need to
identify the current phase of the interaction
2) Predictive DMP distributions: As a next step, we
compute predictions over the behavior of an agent
given a partial trajectory τ o of the observed agent.
To do so, we use a probabilistic approach and model
a distribution p(θ) over the parameters of the DMPs.
This distribution can be conditioned on a new, partial
observation, i.e., to obtain an updated parameter distribution p(θ|τ o ). We use samples of this distribution
to predict the future behavior of the agent.
3) Correlating both Agents: In order to perform a successful cooperation, the movement of the robot needs
to be correlated with the movement of the human.
Such operation is a straightforward extension to the
predictive DMP distributions. Instead of conditioning
on the observation of all DoFs, we only condition on
the DoFs of the observed agent, and, hence, we also
obtain a distribution over the DMP parameters of the
controlled agent, that can be used to control the robot.
In the following, we will address each of the steps above
in detail. First, we will recapitulate the basic properties and
components of DMPs. Subsequently, we will describe how
phase estimation, adaptation, and correlation can be realized
within the DMP framework in order to produce an interactive
motor primitive.
A. Dynamic Motor Primitives
A DMP is an adaptive representation of a trajectory
representing a human or robot movement [6]. In this section,
we will give a brief recap of DMPs. The general idea is to
encode a recorded trajectory as dynamical systems, which
can be used to generate different variations of the original
movement. As a result, a robot can generalize a demonstrated

2832

Motion Capture

Interaction Primitive

Human-Robot Interaction

Fig. 2. An overview of the presented machine learning approach. Left: Using motion capture we first record the movements of two persons during
an interaction task. Center: Given the recorded motion capture data, we learn an interaction primitive specifying each persons’ movement as well as the
dependencies between them. Right: During human-robot interaction, the learned interaction primitive is used by the robot to adapt its behavior to that of
his human interaction partner.

movement to new situations that may arise. Formally, a DMP
can be written as a dynamical system
ÿ = (αy (βy (g − y) − ((ẏ)/τ )) + f (x)) τ 2

(1)

where y is a state variable such as the joint angle to be
controlled, g is the corresponding goal state, and τ is a time
scaling factor. The first set of terms represents a criticallydamped linear system with constant coefficients αy and βy .
The last term is called the forcing function
Pm
i=1 ψi (x)wi x
= φ(x)T w
(2)
f (x) = P
m
j=1 ψj (x)
where ψi (x) are Gaussian basis functions and w the corresponding weight vectors. The basis functions only depend on
the phase variable x, which is the state of a canonical system
shared by all degrees of feedom (DoFs). The canonical
system acts as a timer to synchronize the different movement
components. It has the form ẋ = −αx xτ , where x0 = 1 at
the beginning of the motion and, thereafter, it decays towards
zero. The elements of the weight vector w are denoted as
shape-parameters, as they determine the acceleration profile
of the movement, and, hence, indirectly also the shape of
the movement. Typically, we learn a separate set of shape
parameters w as well as the goal attractor g for each DoF.
The goal attractor g can be used to change the target position
of the movement while the time scaling parameter τ can be
used to change the execution speed of the movement.
The weight parameters w of the DMP can be straightforwardly obtained from observed trajectories {y1:T , ẏ1:T , ÿ1:T }
by first computing the forcing function that would reproduce
the given trajectory, i.e.,
Fi =

1
ÿi − αy (βy (g − yi ) − ẏi /τ ).
τ2

(3)

Subsequently, we can solve the system Φw = F in a least
squares sense, i.e.,
w = (ΦT Φ)−1 ΦT F ,

(4)

B. Phase Estimation by Dynamic Time Warping
For a joint activity to succeed, the movement of the
interaction partners needs to be temporally aligned. During
the execution of human-robot interaction, the robot observes
a partial movement of the human counterpart. Given this
partial movement sequence, we need to determine the current
state of the interaction. This is achieved by determining the
current value of the phase variable x. To this end, we will use
the dynamic time warping (DTW) algorithm [11]. DTW is a
method for the alignment of time series data. Given two time
series u = (u1 , · · · , uN ) and v = (v1 , · · · , vM ) of size N
and M , DTW finds optimal correspondences between data
points, such that a given distance function D is minimized.
This task can be formulated as finding an alignment between
a reference trajectory u and an observed subsequence v. In
our specific case, the reference trajectory is the movement of
the observed agent during the original demonstration of the
task, and the query trajectory is the currently seen partial
movement sequence of the human interaction partner. We
define an alignment π as a set of tuples (π1 , π2 ) specifying
a correspondence between point π1 of the first time series
and point π2 of the second time series. To find such an
alignment, we first calculate the accumulated cost matrix,
which is defined as
m
• D(1, m) = Σk=1 c(u1 , vk ), m ∈ [1 : M ],
n
• D(n, 1) = Σk=1 c(uk , v1 ), n ∈ [1 : N ],
• D(n, m) = min{D(n − 1, m − 1), D(n, m − 1),
D(n − 1, m)} + c(un , vm )
where c is a local distance measure, which is often set to the
squared Euclidean distance, i.e., c = ||u−v||2 . In the original
DTW formulation, finding the optimal alignment is cast as
the problem of finding a path from (1, 1) to (N, M ) producing minimal costs according to the accumulated cost matrix.
This optimization is achieved using a dynamic programming
recursion. The DTW approach above assumes that both time
series have approximately the same length. However, in our
case we want to match a partial movement to the reference
movement. To this end, we modify the DTW algorithm and
determine the path with minimal distance starting at (1, 1)
and ending at (n∗ , M ), where n∗ is given by

where Φ is a matrix containing of basis vectors for all time
steps, i.e., Φt = φTt = φ(xt ).
2833

n∗ = argmin D(n, M ).
n

(5)

Training
Partial
Completion

Fig. 3. Phase estimation and pattern completion using DTW and DMPs.
Given the partial observation (black), we estimate the current phase, and
use it to generate the unseen part (red) of the letter. The goal does not have
to be specified and is estimated alongside the other parameters.

The index n∗ reflects the frame in the reference movement
which produces minimal costs with respect to the observed
query movement. As a result it can be used to estimate the
current value of the phase variable x of the canonical system.
More specifically, calculating (n∗ /N ) yields an estimated
of the relative time that has passed, assuming a constant
sampling frequency. Scaling this term nonlinearly yields an
estimate of the phase x of the canonical system

 ∗ 
n
τ .
(6)
x = exp −αx
N
A simple example for the explained phase estimation algorithm can be see Fig 3. The grey trajectories show the
demonstrated handwriting samples for which DMPs have
been learned. The black trajectories show a new, partial
handwriting sample. Using DTW, we can identify the phase
of the DMP and then use it to automatically complete the
observed letter. To this end, we can set the starting position
of the DMP to the last point in the partial trajectory, and
set the phase according to the estimated x. By specifying
any goal position g, we can generate the missing part of the
observed movement.
C. Predictive Distributions over DMPs
In this section we will introduce predictive distributions
over DMP parameters that can be used to predict the behavior
of an agent given a partial observed trajectory. We will first
describe how to generate such predictive distribution for a
single agent and later show in the next section how this
model can be easily extended to infer a control policy for
the controlled agent in an interaction scenario.
In our probabilistic approach, we model a distribution
p(θ) over the parameters of a DMP. In order to also
estimate the target position of the movement, we include
the shape parameters wi as well as the goal attractors gi
for all DoFs i in the parameter vector θ of the DMP, i.e.,
θ = [wT1 , g1 , . . . , wTN , gN ], where N is the number of DoFs
for the agent. Given the parameter vector samples θ [j] of
multiple demonstrations j = 1 . . . S, we estimate a Gaussian
distribution over the parameter θ, i.e., p(θ) = N (θ|µθ , Σθ ),
with
PS
PS
[j]
[j]
− µ)(θ [j] − µ)T
j=1 (θ
j=1 θ
µθ =
, Σθ =
.
S
S
(7)

In order to obtain a predictive distribution, we observe a
partial trajectory τ o = y 1:t∗ up to time point t∗ and our goal
is to estimate the distribution p(θ|τ o ) over the parameters
θ of the DMP. These parameters can be used to predict
the remaining movement y t∗ :T of the observed agent. The
updated distribution p(θ|τ o ) can simply be computed by
applying Bayes rule, i.e.,
p(θ|τ o ) ∝ p(τ o |θ)p(θ).

(8)

In order to compute this operation, we first have to discuss
how the likelihood p(τ o |θ) can be implemented. The parameters of a DMP directly specify the forcing function,
however, we also include the goal positions gi in the forcing
function fi for the ith DoF. Therefore, we reformulate the
forcing function, i.e., for a single degree of freedom i, we
will write the forcing function fi (t) as
fi (xt ) = φTt wi + αy βy gi


wi
T
= [φt , αy βy ]
.
gi

(9)
(10)

The forcing function can be written in matrix form for all
DoFs of the observed agent and for all time steps 1 ≤ t ≤ t∗ ,
i.e.,



 w1
Φ̃ 0 . . . . . 

 0 Φ̃ 0 . .   g1 

  .. 
= Ωθ,
(11)
F = .
.. .. ..  
. 

 ..
. . . 
 wN 
0 . . . . . Φ̃
gN
{z
}
|
Ω

where the t-th row Φ̃t,· = [φTt , αy βy ] of Φ̃ contains the basis
functions for time step t and a constant as basis for the goal
attractor gi . The matrix Ω contains the basis functions for all
DoFs on its block-diagonal. The vector F contains the value
of the forcing function for all time steps and all degrees
of freedom, i.e., F = [f T1 , . . . , f TN ]T , where f i contains
the values of the forcing function for all time steps for the
ith DoF. By concatenating the forcing vectors f i for the
single DoFs and by writing Ω as a block-diagonal matrix, we
achieve that the parameters of all DoF can be concatenated
in the vector θ.
Given an observed trajectory y 1:t∗ , we can also compute
the forcing function vector F ∗ from the observation, i.e., the
elements of the F ∗ vector are given by
fi∗ (xt ) =

1
o¨i (t) − αy (−βy oi (t) − o˙i (t)/τ ).
τ2

(12)

Now, we can use a Gaussian observation model for the
likelihood
p(τ o |θ) = N (F ∗ |Ωθ, σ 2 I),
(13)
where σ 2 is the observation noise variance which will act as
regularizer in the subsequent conditioning.
In order to perform the conditioning, we first write
p(τ o , θ) = p(τ o |θ)p(θ) as a joint Gaussian distribution

2834

0.5

attraction-point

0

y-axis [m]

attraction-point

goal

-0.5
-1
-1.5

Training
Partial
Completion

-2

and, subsequently, apply Gaussian conditioning. The joint
distribution is given as

 ∗
Ωθ,
A
ΩΣθ
F
,
(14)
p(τ o , θ) = N
θ µθ , Σθ ΩT
Σθ
with A = σ 2 I + ΩΣθ ΩT .
The conditional distribution p(θ|τ o ) is now also Gaussian
with mean and variance
= µθ + Σθ ΩT A−1 (F ∗ − Ωµθ ),

Σθ|τ o

= Σθ − Σθ ΩT A−1 ΩΣθ .

-1.5

-1

Observed
Agent

Fig. 4. Given a set of training trajectories (in gray) we learn a predictive
distribution over the DMP weights. The distribution can then be used to
sample new trajectories with a similar shape. In this example, DTW is used
to determine the current phase of a partially observed trajectory (black). The
completions of this trajectory are performed by estimating the most likely
distribution of DMP weights.

µθ|τ o

-2

(15)

Using the above distribution we can compute the most
likely weights µθ|τ o of the DMPs for any observed partial
movement. In Fig. 4, we see an example of using the above
procedure for estimating weights from partial observations.
On the left side, we see the demonstrations that have been
used to train the DMPs. The different demonstrations reflect
different versions of the same alphabet letter. On the right
side, we see the partial observation (black) of a new handwritten sample as well as the automatic reconstruction using
a DMP with estimated weights.
D. Correlating two Agents with Predictive Distributions
Correlating the controlled agent with its interaction partner
is now a straightforward extension of the predictive DMP
framework. We now assume that we have two agents, the
observed and the controlled agent. In order to capture the
correlation between the agents, we use a combined parameter
vector θ = [θ To , θ Tc ]T which contains the DMP parameters
θ o of the observed and the parameters θ c of the controlled
agent. We can now use the approach for obtaining a predictive distribution p(θ|τ o ), however, in contrast to the previous
section, the observed trajectory only contains the DoFs of the
observed agent. Hence, in order to write the forcing vector
F = Ωθ o in terms of the complete parameter vector θ, we
need to append a zero-matrix to the feature matrix Ω, i.e.,
F = Ω̃θ, with Ω̃ = [Ω, 0]. The conditioning equations given
in (15) can now be applied straightforwardly by replacing Ω

-0.5

0

0.5

x-axis [m]

1

1.5

2

Controlled
Agent

Fig. 5. A simple simulation setup for studying Interaction Primitives. Two
opposing (robot) arms of different kinematic structure (2 links vs. 3 links)
execute a high-five movement to a specific goal. Using optimal control
methods we can calculate optimal trajectories that reach the goal, while at
the same time being attracted by “attraction points”. The resulting data set
contains strong correlations between the movements of the two agents.

with Ω̃. The estimated parameter vector θ can, thus, be used
to to predict the remaining movement of the observed agent
(using θ o ) but also to control the robot in the current situation
(using θ c ). Hence, its behavior is always related the behavior
of the human interaction partner.
IV. E XPERIMENTS
To evaluate our approach under controlled and varying
conditions, we designed a simple simulation environment
that can be used to study interaction scenarios. The simulation consists of two opposing robot arms with different
kinematic structures as can be seen in Fig. 5. The robot
on the left side (the observed agent) consists of two links
which are connected through a hinge joint, while the robot
on the right side (the controlled agent) has three links and
two joints. The task of the robots is to execute a high-five
movement. In order to generate training data for this task we
first synthesize a set of training trajectories for both agents.
Using optimal control we determine for each agent a joint
angle trajectory which brings it from its start position to a
specified goal position. Attractor points are added in order to
generate realistic-looking high-five movements. During the
synthesis of training data, both the goal position and the
attractor positions are varied.
Given this training data, we learn an IP capturing the
mutual dependencies of the agents during the high-five
movement. After learning, we the use the IP to control the
three linked robot arm. Given partial joint angle trajectories
of the observed agent, we use the IP and conditioning to (1)
determine the most likely current goal, (2) the ideal joint
angle configurations of the controlled agent.
Fig. 6 depicts task space trajectories for the controlled
agent after conditioning. On the left side we see the results
of conditioning when 40% of the movement of the observed
agent is seen. On the right side are the results after conditioning with 60% of the data. Each trajectory is a possible
prediction of the task space movement of the agent. The

2835

0.0
2

y -a x is [m ]

y-axis [m]

-0.1
1

0

0

25

50

75

100

0

25

50

75

-0.3
-0.4

100

-0.5

x-axis [m]

Fig. 6. The uncertainty over the goal position shrinks with increasing
amount of data points. Left: distribution after observing 40% of the
partners movements. Right: distribution after observing 60% of the partners
movements
16

Controlled
Observed

14
12

-0.6

1.5

1.6

1.7

x - ax i s[ m ]

1.8

1.9

Fig. 8. Difference between ground truth and predicted task space goals.
Blue circles show the true positions of the goals. Red circles depict the
predicted goals after observing 60% of the interaction.

10
8

Side

MSE

-0.2

6
4
2
0

0

10

20

30

40

50

60

70

80

90

100

Middle

Percentage of trajectory [%]
Fig. 7. Mean squared error based on different percentage of partially
observed trajectories of the interaction partner. The red curve shows the
accuracy of predicting the movements of the observed agent from partial
trajectories. The black curve shows the accuracy in inferring the right
reaction in response to the observed movement.

figure shows that the uncertainty significantly shrinks when
we transition fom 40% to 60% in this example.
To further analyze the accuracy of prediction, we generated a new test data set consisting of interactions to different
goal positions that were not part of the original training data.
We then calculated the mean squared error (MSE) between
the predicted joint angle trajectories generated from the IP
and the ground-truth data. Fig. 7 shows the evolution of
the MSE for observed partial trajectories of increasing size.
We can see that the MSE in the prediction of the observed
agent significantly drops at around 20% and again at 60%. A
similar trend, albeit with higher variance, can also be seen in
the predictions for the ideal response of the controlled agent.
The plot suggests that after seeing 60% of the movement of
the interaction partner we can already roughly infer the goal
he is aiming at.
We also analyzed the difference in task space between the
inferred and the ground-truth data for the controlled agent.
More specifically, we evaluated how far the predicted goal
is from the true goal of the trajectory. Fig. 8 shows the
true goals and the inferred goals in task space after seeing
60% of the movements of the observed agent. We can see
that the predicted goals are in close proximity to the true
goal locations. Please note, that the depicted positions are
in task space while the inferred values are in joint space.
The depicted position is calculated by performing forward

(a)

(b)

(c)

Fig. 9. Two reactions synthesized from an Interaction Primitive for a
handing-over task. The humanoid on the left side (controlled agent) is
controlled through the IP and has to receive an object from the humanoid
on the right side. Depending on the location where the object is handed
over, the reaction of the controlled agent is changed. Training of the IP was
performed using motion capture data of two humans.

kinematics using the inferred joint angle data. As a result,
even small errors in the joint angle of the first link will
propagate and produce larger errors at the end-effector. Still,
the results indicate that the approach can be efficiently used
for intention inference in human-robot tasks. By predicting
the most likely goal of the human interaction partner, the
robot can proactively initiate his own response.
We also conducted experiments using real motion capture
data collected from the interactions of two human subjects.
In a first experiment we asked two humans to demonstrate a
“handing-over” in which the first subject hands a cup to the
second subject. The demonstrations were then used to learn
an IP. However, in order to cope with the high dimensionality
of the dataset, we applied Principal Component Analysis
(PCA) as a pre-processing step, in order to project the data
onto a five-dimensional space. After training, we tested the
learned IP on a set of data points that have not been used
during training. Fig. 9 shows two example situations, where
the controlled agent (left humanoid) automatically infers the
optimal reaction to the behavior of his interaction partner
(right humanoid). In the first sequence the controlled agent

2836

Fig. 10. A frame sequence from a high-five interaction between a human and a humanoid. The robot automatically reacts to the movement of the human
and estimates the appripriate location of the executed high-five. The human interaction partner is tracked using an OptiTrack motion capture system.

correctly turns to the left side to receive an object. In contrast
to that, in the second sequence, the agent reaches for the
middle in order to properly react to the observed movement.
Finally, we performed a set of interaction experiments on
a real humanoid robot. The humanoid has two arms with 7
DoF each. During the experiment we used one arm with four
DoFs. More specifically, we trained an Interaction Primitive
for the high-five. Again, we collected motion capture data
from two humans for training this IP. After training, the
robot used the IP to predict the joint configuration at the goal
position as well as the weight parameters of the DMP. Fig. 10
shows an example interaction realized via the presented
approach. Using prediction in this task is important, as it
helps the robot to match the timing of the human interaction
partner. Notice that the starting location of the robot is quite
far from the rest poses in the learning database.
V. C ONCLUSION
In this paper, we proposed the novel Interaction Primitive
framework based on DMPs and introduced a set of basic
algorithmic tools for synchronizing, adapting, and correlating
motor primitives between cooperating agents. The research
introduced here lays the foundation for imitation learning
methods that are geared towards multi-agent scenarios. We
showed how demonstrations recorded from two interacting
persons can be used to learn an interaction primitive, which
specifies both the executed movements, as well as the correlations in the executed movements. The introduced phase
estimation method based on dynamic time warp proved to
very important for applying a learned interaction primitive in
new situations. Timing is a highly variable parameter, which
varies among different persons, but can also vary depending
on the current mood or fatigue.
In future work, we plan to concentrate on more complex
interaction scenarios, which are composes of several interaction primitives. These primitives could executed in sequence
or in a hierarchy in order to produce complex interactions
with a human partner. We are also already working on using
the interaction primitive framework for predicting the most
likely future movements of a human interaction partner. The
underlying idea is that the same representation which is
used for movement synthesis can also be used for movement

prediction. The predicted actions of the human could then be
integrated into the action selection process of the robot, in
order to avoid any dangerous situations.
VI. ACKNOWLEDGEMENTS
The work presented in this paper is funded through the
Daimler-and-Benz Foundation and the European Communitys Seventh Framework Programme under the grant agreement n ICT-600716 (CoDyCo).
R EFERENCES
[1] P. Abbeel, A. Coates, and A. Ng. Autonomous Helicopter Aerobatics
through Apprenticeship Learning. International Journal of Robotic
Research, 29:1608–1639, 2010.
[2] A. Billard, S. Calinon, R. Dillmann, and S. Schaal. Survey: Robot
Programming by Demonstration. In Handbook of Robotics, volume
chapter 59. MIT Press, 2008.
[3] S. Calinon, E.L. Sauser, A.G. Billard, and D.G. Caldwell. Evaluation
of a probabilistic approach to learn and reproduce gestures by imitation. In Proc. IEEE Intl Conf. on Robotics and Automation (ICRA),
pages 2381–2388, Anchorage, Alaska, USA, May 2010.
[4] R. Chalodhorn, D. Grimes, K. Grochow, and R. Rao. Learning to
walk through imitation. In Proceedings of the 20th international joint
conference on Artifical intelligence, IJCAI’07, pages 2084–2090, San
Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.
[5] D. Grimes, R. Chalodhorn, and R. Rao. Dynamic imitation in a
humanoid robot through nonparametric probabilistic inference. In In
Proceedings of Robotics: Science and Systems (RSS). MIT Press, 2006.
[6] A. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal.
Dynamical movement primitives: Learning attractor models for motor
behaviors. Neural Comput., 25(2):328–373, February 2013.
[7] S. Ikemoto, H Ben Amor, T. Minato, B. Jung, and H. Ishiguro.
Physical human-robot interaction: Mutual learning and adaptation.
IEEE Robotics and Automation Magazine, 19(4):24–35, Dec.
[8] D. Lee, C. Ott, and Y. Nakamura. Mimetic communication model
with compliant physical contact in human-humanoid interaction. Int.
Journal of Robotics Research., 29(13):1684–1704, November 2010.
[9] M. Mühlig, M. Gienger, and J. Steil. Interactive imitation learning of
object movement skills. Autonomous Robots, 32:97–114, 2012.
[10] D. Pomerleau. ALVINN: an autonomous land vehicle in a neural network. In David S. Touretzky, editor, Advances in Neural Information
Processing Systems 1, pages 305–313. San Francisco, CA: Morgan
Kaufmann, 1989.
[11] H. Sakoe and S. Chiba. Dynamic programming algorithm optimization
for spoken word recognition. Acoustics, Speech and Signal Processing,
IEEE Transactions on, 26(1):43–49, 1978.
[12] S. Schaal. Is imitation learning the route to humanoid robots? Trends
in Cognitive Sciences, 3:233–242, 1999.
[13] Z. Wang, M. Deisenroth, H. Ben Amor, D. Vogt, B. Schoelkopf, and
J Peters. Probabilistic modeling of human dynamics for intention
inference. In Proceedings of Robotics: Science and Systems (R:SS),
2012.

2837

2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)
Windsor Oceanico Hotel, Rio de Janeiro, Brazil, November 1-4, 2016

Traffic Light Status Detection Using Movement Patterns of Vehicles
Joseph Campbell1 , Heni Ben Amor1 , Marcelo H. Ang Jr.2 , and Georgios Fainekos1

Abstract— Vision-based methods for detecting the status of
traffic lights used in autonomous vehicles may be unreliable due
to occluded views, poor lighting conditions, or a dependence
on unavailable high-precision meta-data, which is troublesome
in such a safety-critical application. This paper proposes a
complementary detection approach based on an entirely new
source of information: the movement patterns of other nearby
vehicles. This approach is robust to traditional sources of error,
and may serve as a viable supplemental detection method. Several different classification models are presented for inferring
traffic light status based on these patterns. Their performance
is evaluated over real and simulated data sets, resulting in up
to 97% accuracy in each set.

Fig. 1: Example of traffic light occlusion by other vehicles.
The light is obscured in the left image but becomes visible
in the right image as the traffic starts moving.

I. I NTRODUCTION
One of the many challenges facing autonomous vehicles is
the ability to safely navigate complex environments such as
intersections while maintaining compliance with local traffic
regulations. Vehicles and pedestrians with varying directions
of travel cross paths while being guided by traffic lights
that are optimized for identification by human drivers. This
issue has been partially addressed with the introduction of
intelligent traffic light systems which actively communicate
their signal to nearby vehicles through Vehicular Ad Hoc
Networks [1], [2]. Nonetheless, as observed by [3], such
systems have thus far been limited to small-scale academic
experiments and a timely integration into current road networks seems unlikely.
It is for this reason that recent work has focused on the
real-time identification of traffic light signals via visionbased systems [3], [4]. This approach can work well but
is subject to errors that can lead to the misidentification
of the status of traffic light signals. These errors can arise
due to poor lighting conditions which interfere with the
camera sensor or an obstructed view resulting from a dirty
lens or another vehicle, as demonstrated in Fig. 1. This can
potentially lead to disastrous results – an autonomous vehicle
erroneously passing through an intersection could find itself
in a situation in which it is unable to avoid a collision.
This paper proposes a complementary traffic light identification system based on an alternative source of information:
the behavior of other nearby vehicles based on positional
data. Conceptually, the system infers the status of a traffic
light from the movements of other vehicles around or in the

corresponding intersection. The advantage of such a system
is not that it has fewer failure-inducing cases than a visionbased system, but rather that they are different failure cases.
Ideally, this system will be paired with a vision-based one
such that they complement each other and reduce the total
points of failure.
Consider the following scenario: a traffic light is nonfunctional due to an extenuating circumstance. As is typical
on US roads, a law enforcement officer is directing traffic
through the intersection. A typical vision-based recognition
system is of no help in this scenario, however, by observing
when other vehicles start to pass through the intersection and
from which direction, the proposed system can infer which
traffic the officer is allowing to pass through the intersection.
Similarly, consider a situation in which a vehicle stops at a
red light behind a larger vehicle that is occluding the traffic
light. Suppose the preceding vehicle suffers a mechanical
failure and is blocking traffic; the traffic light cycles through
its phases and surrounding traffic bypasses the offending
vehicle in other lanes. Once again, a vision-based system
would not be of assistance in this scenario; however, the
proposed system may indicate that the traffic light is green
and, therefore, alternative action should be taken.
The contributions of this paper are as follows: we present
a system for predicting the state of a traffic light based on
the spatial movement of nearby vehicles, and evaluate its
effectiveness in simulated and real-world conditions.
II. R ELATED W ORK
Vision-based traffic light detection systems have been
widely analyzed in previous works. The majority of these
works have focused purely on image recognition [5], [6],
[7], [8]. Of particular interest, however, are those that seek
to minimize the risk posed by errors inherent to vision-based
detection systems. In [4], the authors propose using a detailed
map of traffic lights to act as prior knowledge so that the

1 J. Campbell, H. B. Amor, and G. Fainekos are with the School
of Computing, Informatics, and Decision Systems Engineering, Arizona
State University, Tempe, AZ 85281, USA. {jacampb1, hbenamor,

fainekos}@asu.edu
2 M. H. Ang Jr. is with the National University of Singapore, Kent Ridge,
Singapore. mpeangh@asu.edu
This material is based upon work supported by the National Science
Foundation EAPSI Fellowship under Grant No. 1515589 and CPS 1446730.

978-1-5090-1889-5/16/$31.00 ©2016 IEEE

283


T
gn,t = xn,t , yn,t .

A

If we place the target vehicle at the origin of a Cartesian
coordinate plane with the positive x-axis extending towards
the front of the vehicle and the positive y-axis extending
towards the left-hand side of the vehicle, then x is the
distance along the x-axis from the target vehicle to the
observed vehicle n. Similarly, y is the distance along the yaxis to vehicle n. This yields a conditional probability of the
following form, where N is the total number of observable
vehicles at time t,

3
1

2
x
y

(1)

B

Fig. 2: A scenario in which vehicle A’s current position is
ambiguous, as there are multiple paths it could have taken
which could be used to infer different traffic light states.

p(zt |g1,t , g2,t , ..., gN,t ) = p(zt |g1:N,t ).

(2)

However, this approach has a potential drawback which is
visualized in Fig. 2. If vehicle A is an observable vehicle at
time t, it may have taken several different paths to arrive at
this position: path 1, 2, or 3. Each of these paths could result
in a different traffic light state zt . For example, if vehicle B
is our target vehicle and we are observing A, zt could be
green if vehicle A followed path 2, red if path 1, and either
green or red if path 3 (depending on local traffic regulations
for right-on-red turns). This leads to an ambiguous situation,
in which the state of vehicle A at this point in time does not
necessarily help us determine zt .
We can alleviate this problem if we consider a temporal
trace of the position. We could alter the vehicle state to
include information on the position over time in the form
of velocity,

detection system knows when it should be able to see traffic
lights. If traffic lights are not detected at an expected position,
the autonomous vehicle can take preventative action such as
slowing down under the assumption that the light is red or
yellow. However, this could have unintended consequences
since if the light is green this action may result in a collision
with human-operated vehicles due to unpredictability.
Similarly, in [3] the authors acknowledge the difficulties in
building a purely vision-based traffic light detection system
and so augment theirs with prior map knowledge as well
as temporal information. While yielding good results, the
system still fails to identify traffic light signals in certain
cases. Indeed, the authors indicate that a possible approach
for improvement would be to introduce 3-dimensional LIDAR data into the mix in order to improve recognition of
the traffic lights themselves.
In [9], the authors use the movement patterns of pedestrians to apply semantic labels to the environment. They
infer the location of pedestrian crossings, sidewalks, and
building entrances and exits based on the activity patterns
of pedestrians. This is similar in spirit, if not in execution,
to the labeling of traffic lights based on vehicle movement
patterns introduced in this paper.


T
gn,t = xn,t , yn,t , ẋn,t , ẏn,t .

(3)

This is susceptible to the same ambiguity problem, however. In the example from Fig. 2, if path 2 resulted from
A accelerating through a light which recently turned green,
then the velocity at time t could be roughly the same for all
paths. The same holds true when acceleration is considered:
T

gn,t = xn,t , yn,t , ẋn,t , ẏn,t , ẍn,t , ÿn,t .

(4)

A more effective approach is to consider the state of
vehicle n not just for a single time step t, but rather over
a time window, i.e., t − 1, t − 2, and so on. If we consider
a window size of T time steps in the past, then we can
represent the state of vehicle n as a time series s at time t:

III. P ROBLEM F ORMULATION
We can formalize the problem from a probabilistic perspective as follows: let Z be a discrete random variable which
represents the state of a traffic light with respect to a target
vehicle. The specific value of Z is denoted by z, and in this
paper can take the value of either green or red. The goal is
to then determine the probability that a traffic light is either
green or red with respect to our target vehicle at a specific
point in time t: p(Zt = zt ). To simplify the notation, from
this point on we will refer to this probability as simply p(zt ).
Clearly, we cannot determine an accurate probability for
p(zt ) without additional information. Therefore, we would
like to consider observations of nearby vehicles when determining this probability. The simplest approach is to consider
the spatial position of every nearby vehicle independently at
each point in time. We define the state g of vehicle n at time
t as:

sn,t = gn,t , gn,t−1 , ..., gn,t−Tn

(5)

p(zt |s1,t , s2,t , ..., sN,t ) = p(zt |s1:N,t ).

(6)

If observations are ideal, then the entire path for vehicle A
is now taken into account and there is no more ambiguity. In
practice, this may not be the case and the effective window
size Tn may vary from vehicle to vehicle. For example, A
may only enter the sensor range of our target vehicle once
it reaches the position depicted in Fig. 2.
Problem: Given a set of observations L of nearby vehicles, determine p(zt |L) under the following assumptions.
284

1) L is either a set of independent vehicle states g, or a
set of independent series of states s.
2) Each series s may contain a variable number of states
corresponding to sequential time points.
3) At least one vehicle must be observed for at least one
time step.
In practice, Assumption 2 is not strong as this is implicitly
satisfied by a Bayesian tracking algorithm in this paper.

B. Bidirectional Long Short Term Memory Networks
Standard RNNs suffer from a problem known as the
vanishing gradient [15], in which the hidden layer node
weights for previous inputs converge to zero over time, thus
preventing an RNN from effectively learning from inputs
that span a long time period. A variant of the RNN known
as the Long Short Term Memory (LSTM) [16] network was
designed to mitigate this problem by introducing the concept
of LSTM nodes that are more effective at retaining previous
values. Furthermore, the Bidirectional Long Short Term
Memory (BLSTM) network was shown to be exceptionally
well-suited for sequence classification [17].
The bidirectional aspect of a BLSTM network is a concept lifted from Bidirectional Recurrent Neural Networks
(BRNNs) [18]. In a BRNN, two RNNs – one processing
the input series forward in time and one backward in time
– are connected to the same output layer. This architecture
yields greater prediction accuracy as it predicts based on past
inputs as well as future inputs.

IV. M ETHODOLOGY
In order to find the probability in Eq. (6), we adopt a
Bayesian interpretation which yields the following posterior:
p(s1:N,t |zt )p(zt )
.
(7)
p(s1:N,t )
We employ a discriminative model to find this posterior
directly. Artificial neural networks (ANNs) have excellent
predictive capabilities given nonlinear input-output mappings, particularly when applying a classification label to
a time series input. Depending on the cost function and
network complexity, ANNs can also accurately approximate
a Bayesian posterior directly [10], in our case p(zt |s1:N,t ).
p(zt |s1:N,t ) =

V. E XPERIMENTS
A. Experimental Setup

A. Artificial Neural Networks
Artificial neural networks are mathematical models capable of accurately approximating any continuous function [11]. In this work, we employ ANNs in a supervised
pattern classification capacity to approximate the posterior
probability in Eq. (2). In other words, the input to the
network is the feature vector gn,t and the expected output is
zt . Each observed vehicle state gn,t is treated as independent,
and the goal is to learn which states correspond to each
value of zt . In a feed-forward neural network (FFNN), the
nodes are not allowed to form cycles. This is suitable for
simple pattern classification, however, it is not ideal when
we would like to consider some inputs as dependent and
use multiple inputs to derive a single output. This is the case
when approximating the posterior probability in Eq. (6), as it
is conditional on a time series of vehicle states sn,t as defined
in Eq. (5). This is known as sequence classification [12], and
recurrent neural networks have been previously used with
great effect [13]. A recurrent neural network (RNNs) [14]
is a type of neural network that is allowed to form cyclical
connections among hidden layer nodes.
We use RNNs to estimate p(zt |sn,t ) by generating a single
output zt from a sequence of feature vectors gn,t , which
together form the time series sn,t of state vectors for vehicle
n. However, this is not the same as the posterior probability
defined in Eq. (6) which is conditional on all observed
vehicles, not just one. The feature vector to our network must
be a constant size. This rules out simply concatenating the
feature vectors of all observed vehicles, since the number
of observed vehicles may vary at any given time. Instead,
we take the mean probability of p(zt |sn,t ) for all observed
vehicles at time t and use that as an approximation:
PN
p̂(zt |sn,t )
p̂(zt |s1:N,t ) ≈ n=1
.
(8)
N

In order to evaluate how well these networks can approximate the posterior probabilities, we collected two sets
of data with which to perform experiments. The first set
was generated from real-world sensor data collected by an
autonomous vehicle from 56 intersections in the vicinity of
the National University of Singapore campus in Singapore.
Spatial point cloud data was collected with a SICK LMS 151
LIDAR sensor operating at 50Hz. As there is no ground truth
available with which to form the vehicle time series, the data
was fed through a two-stage vehicle tracking algorithm.
The first stage decomposes the point cloud data into
a subset of clusters, in which each cluster consists of a
collection of points in close proximity to each other. The
clusters are then tracked over several measurement frames
to yield an average spatial position and a velocity vector
for a given point in time [19]. The second stage treats
these independent measurements as observations to a particle
filter-based multi-target tracking algorithm [20]. Vehicle time
series are then derived from the particle filters and downsampled to 10Hz. Supervised labels were manually generated
from camera inputs collected simultaneously with the LIDAR
data. This process yielded a data set consisting of 1011
unique time series.
Despite the inherent value of real-world data, there is
a limit to how much can be collected. Additionally, due
to practical constraints, we could only collect data from
nearby intersections which limits how well we can generalize. Therefore, we turned to synthetic data generated
with the SUMO traffic simulator [21]. Road networks for
13 intersections were generated from OpenStreetMap data:
3 in Tempe, Arizona, 2 in New York City, New York,
and 8 in Singapore. Simulations were run in which traffic
passed through the intersections from each direction and
either traveled straight, turned left, or turned right. Vehicles
285

Classifier

Feature Set

1-NN
1-NN
1-NN
FFNN
FFNN
FFNN
BLSTM
BLSTM
BLSTM

x, y
x, y, ẋ, ẏ
x, y, ẋ, ẏ, ẍ, ÿ
x, y
x, y, ẋ, ẏ
x, y, ẋ, ẏ, ẍ, ÿ
x, y
x, y, ẋ, ẏ
x, y, ẋ, ẏ, ẍ, ÿ

Real
(No Track)
0.678
0.910
0.669
0.850

Real

Sim

Sim-Real

0.853
0.977
0.943
0.697
0.897
0.899
0.655
0.790
0.782

0.737
0.968
0.972
0.655
0.796
0.862
0.764
0.870
0.908

0.719
0.934
0.969
0.690
0.774
0.852
0.683
0.740
0.804

Sim
(Noisy)
0.813
838
0.833
0.642
0.692
0.695
0.765
0.874
0.863

Sim-Real
(Noisy)
0.599
0.648
0.627
0.684
0.716
0.709
0.679
0.742
0.718

TABLE I: The mean test accuracy for 1-Nearest Neighbor (1-NN), Feed-forward Neural Network (FFNN), and Bidirectional
Long Short Term Memory (BLSTM) classifiers. The best classifier for each data set is highlighted in green, while the
classifiers that are not significantly different are highlighted in yellow.

were uniformly distributed to one of three behavior models:
aggressive, average, and submissive.
SUMO is capable of writing floating car data (FCD)
output, which contains the position, velocity, and heading of
every vehicle at each sampling interval for the duration of the
simulation. To correspond with the real data set, the sampling
interval was fixed to 10Hz. This data was then transformed
with respect to a chosen target vehicle, and used to generate
state vectors for each other vehicle within a 50m sensor range
of the target. Since the FCD data includes a vehicle identifier,
these states can then be assembled into a time series for
each vehicle. These time series’ were segmented in order to
coincide with the states of the intersection’s traffic light and
labeled as either green or red. This process yielded a data
set consisting of 2311 unique time series.
The BLSTM network used in these experiments is composed of an input layer followed by two parallel LSTM
layers with 32 nodes each; one layer processes the input
sequence forward and one layer backwards. The output from
the LSTM layers is concatenated into a dropout layer with a
0.5 drop rate. The FFNN is a standard feed-forward network
with 3 hidden layers and 256 nodes per layer. In both
networks, the size of the input layer is dependent on the
number of vehicle state variables, while the output layer
always consists of two nodes in order to produce a one-hot
encoding of zt . The networks are trained using RMSProp
backpropagation with categorical cross-entropy loss and a
softmax activation function. Additionally, a simple K-Nearest
Neighbor algorithm was evaluated to serve as another point
of comparison. For all experiments, K = 1.

the Bayesian tracking had a significant impact on the 1-NN
performance. Thus, we created a Real (No Track) data set
with only the raw measurements obtained by the clustering
algorithm. However, despite slightly reduced performance,
the 1-NN classifier is still the best performer on this data set.
Results for BLSTM and feature sets containing acceleration
are not included for this data set as they require the time
series information provided by the tracking algorithm.
Furthermore, 1-NN has the highest classification accuracy
on the Simulation data set. This seems to indicate that the
Sim data set is a good approximation of the real data set
since it yields similar results, but on further analysis the test
sample distribution between the two data sets is strikingly
different. This can be observed in the first figure of each
row in Fig. 3. The Real data set is heavily skewed, with a
large portion of the test samples coming from intersections
where only a small number of vehicles were observed for a
short period of time. Meanwhile, the Sim data has a much
flatter distribution over a wider domain.
In order to determine whether this distribution has a prominent effect on classification accuracy, we ran an optimization algorithm to minimize the Kullback-Leibler divergence
between the distributions of the two data sets. This was
accomplished by using the truncation factor of a random
portion of the time series as the search parameter. The initial
KL divergence between the Real and Sim data sets is 1.35,
however, after this optimization routine that was reduced
to 0.09. The resulting data set is referred to as Sim-Real,
and it can be seen in Fig. 3 that the associated test sample
distribution is similar to that of the Real data set. As in
the other data sets, the 1-NN classifier is again the best
performer on the Sim-Real data and indicates robustness
to changes in the test sample distribution. Additionally,
since the simulation data yields similar results to the realworld data and is capable of closely approximating the realworld observation distribution, we consider it an accurate
representation of the real-world data.

B. Results and Discussion
The first experiment of interest is to evaluate the relative
performance of each classifier on our data sets, the results
of which are shown in Table I. The classification accuracy is
evaluated for the posterior probabilities produced by both the
FFNN and BLSTM classifiers, with a train/validation/test set
split of 60%/20%/20%, as well as the 1-NN classifier with a
80%/20% train/test split. This experiment reveals that despite
being the simplest, the 1-NN classifier performs significantly
better than all other classifiers on the Real data set with a
97% classification rate. Since this is an unexpected result,
we were interested in whether the noise reduction caused by

The only time we observed the 1-NN classifier perform
poorly is on data sets with a considerable amount of noise.
Gaussian noise with a standard deviation of 2.0 was applied
to all values in the Sim and Sim-Real data sets, resulting
in Noisy variations. The results in Table I show that 1286

0

0.8

10
20
30
Num obs. vehicles
1

1

800
600
400
200
10
20
Mean obs. length (s)

Sim
Confidence

0.6

2
4
6
8
Num obs. vehicles
Test accuracy

Num samples

Real
Confidence

0.6

10
20
30
Num obs. vehicles

1,000

0

0.8

Test accuracy

2,000

1

Test accuracy

4,000

Real
Sim
Sim-Real

Test accuracy

Num samples

1
6,000

0.8
0.6

0.8
0.6

2
4
6
Mean obs. length (s)

10
20
Mean obs. length (s)

Fig. 3: The first figure in each row shows the distribution of test samples in each data set according to the number of
observed vehicles per time step, and the mean observation length among all observed vehicles per time step. The remaining
figures in each row show the BLSTM (full feature set) test accuracy for real data (solid blue line) and simulation data (solid
green line) at each time step for both criteria. The red dashed indicates the prediction confidence at each time step.
Feature Set
x, y, ẋ, ẏ
x, y, ẋ, ẏ, ẍ, ÿ
x, y, ẋ, ẏ
x, y, ẋ, ẏ, ẍ, ÿ
x, y, ẋ, ẏ
x, y, ẋ, ẏ, ẍ, ÿ

NN yielded a considerably worse classification accuracy in
this scenario, while BLSTM was largely unaffected by the
additional noise and achieved the best accuracy with 87%
and 74% on the Sim and Sim-Real data sets respectively.
The results in Table I also allow us to examine the impact
of the different vehicle states defined in Eqs. (1), (3), and (4)
on the overall accuracy. The first observation we can make is
that the addition of velocity information into the feature set
results in a statistically significant (p-value < 0.05) increase
in accuracy for every classifier on every data set. This is
a strong result, and in line with the hypothesis that the
introduction of velocity information will help alleviate the
intersection ambiguity problem. However, it is interesting
to note that the addition of acceleration information does
not always lead to a further increase in accuracy. The noisy
data sets, in particular, actually exhibit either a statistically
significant decrease in accuracy or no change at all. This
suggests that we can reduce the complexity of our classifiers
without penalizing accuracy on noisy data sets by leaving
acceleration out of the feature set.

α
1
1
2
2
3
3

Mean Real
0.693
0.718
0.837
0.842
0.877
0.876

Mean Sim
0.861
0.866
0.863
0.870
0.868
0.876

TABLE II: The BLSTM mean test accuracy for all time steps
with at least α observed vehicles.

the observations that have occurred before the current time
step are considered. The mean classification accuracy for all
time steps is shown in Table II.
With further analysis, it is evident that a significant number
of misclassified time steps occur when only one vehicle
is observed. As more distinct vehicles are observed, the
classification accuracy increases, which is an intuitive result.
This is visualized in the top row of Fig. 3. If we examine
the distribution of test samples over the number of observed
vehicles, it is clear that a large portion of the samples
occur when only one vehicle is observable. Taking this into
consideration, if the test accuracy is evaluated only for time
steps in which two or more vehicles are observed, then the
accuracy increases from 71% to 84% on the Real data set as
shown in Table II. In contrast, the simulation data has a flatter
distribution, and as a result the corresponding accuracy does
not see a proportional increase. With the positive correlation
between accuracy and the number of vehicles, we might also
expect such a relationship between the classification accuracy
and the length of time that vehicles are observed. The plots
in the second row of Fig. 3 show a positive correlation,
indicating that this is true to some extent.

The second experiment is designed to test how the BLSTM
classifier would perform in a realistic scenario. In real-world
use, we do not have access to the full vehicle time series
in the data sets; we only have the vehicle observations that
have occurred until the current time step. The network is
first trained with the full vehicle time series from all but one
of the intersections. With the remaining time series, time
is treated as a discrete value and incremented in steps. At
every time step, the network is used to estimate the mean
temporal probability in Eq. (8) from the vehicle time series
that have had an observation within the past 3 seconds. Only
287

16

the same, we envision that the best use of our system is to
combine it with a traditional vision-based method. Different
failure cases suggests that the systems will complement each
other, and result in a more robust detection system.

x

14
12
10
8

R EFERENCES
10 8

6
y

4

2

[1] N. Maslekar, M. Boussedjra, J. Mouzna, and H. Labiod, “Vanet based
adaptive traffic signal control,” in Vehicular Technology Conference
(VTC Spring), 2011 IEEE 73rd. IEEE, 2011, pp. 1–5.
[2] V. Gradinescu, C. Gorgorin, R. Diaconescu, V. Cristea, and L. Iftode,
“Adaptive traffic lights using car-to-car communication,” in Vehicular
Technology Conference, 2007. VTC2007-Spring. IEEE 65th. IEEE,
2007, pp. 21–25.
[3] J. Levinson, J. Askeland, J. Dolson, and S. Thrun, “Traffic light
mapping, localization, and state detection for autonomous vehicles,” in
Robotics and Automation (ICRA), 2011 IEEE International Conference
on. IEEE, 2011, pp. 5784–5791.
[4] N. Fairfield and C. Urmson, “Traffic light mapping and detection,” in
Robotics and Automation (ICRA), 2011 IEEE International Conference
on. IEEE, 2011, pp. 5421–5426.
[5] J.-H. Park and C.-s. Jeong, “Real-time signal light detection,” in 2008
Second International Conference on Future Generation Communication and Networking Symposia. IEEE, 2008, pp. 139–142.
[6] H. Tae-Hyun, J. In-Hak, and C. Seong-Ik, “Detection of traffic lights
for vision-based car navigation system,” in Advances in Image and
Video Technology. Springer, 2006, pp. 682–691.
[7] F. Lindner, U. Kressel, and S. Kaelberer, “Robust recognition of traffic
signals,” in Intelligent Vehicles Symposium, 2004 IEEE. IEEE, 2004,
pp. 49–53.
[8] R. De Charette and F. Nashashibi, “Real time visual traffic lights
recognition based on spot light detection and adaptive traffic lights
templates,” in Intelligent Vehicles Symposium, 2009 IEEE. IEEE,
2009, pp. 358–363.
[9] B. Qin, Z. J. Chong, T. Bandyopadhyay, M. H. Ang, E. Frazzoli,
and D. Rus, “Learning pedestrian activities for semantic mapping,” in
Robotics and Automation (ICRA), 2014 IEEE International Conference
on. IEEE, 2014, pp. 6062–6069.
[10] M. D. Richard and R. P. Lippmann, “Neural network classifiers
estimate bayesian a posteriori probabilities,” Neural computation,
vol. 3, no. 4, pp. 461–483, 1991.
[11] K.-I. Funahashi, “On the approximate realization of continuous mappings by neural networks,” Neural networks, vol. 2, no. 3, pp. 183–192,
1989.
[12] Z. Xing, J. Pei, and E. Keogh, “A brief survey on sequence classification,” SIGKDD Explor. Newsl., vol. 12, no. 1, pp. 40–48, Nov.
2010.
[13] A. Graves, “Sequence transduction with recurrent neural networks,”
ICML Representation Learning Worksop, 2012.
[14] J. Schmidhuber, “Deep learning in neural networks: An overview,”
Neural Networks, vol. 61, pp. 85–117, 2015.
[15] Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term dependencies with gradient descent is difficult,” IEEE transactions on neural
networks, vol. 5, no. 2, pp. 157–166, 1994.
[16] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[17] A. Graves and J. Schmidhuber, “Framewise phoneme classification
with bidirectional lstm and other neural network architectures,” Neural
Networks, vol. 18, no. 5, pp. 602–610, 2005.
[18] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,” Signal Processing, IEEE Transactions on, vol. 45, no. 11, pp.
2673–2681, 1997.
[19] X. Shen, S.-W. Kim, and M. Ang, “Spatio-temporal motion features
for laser-based moving objects detection and tracking,” in Intelligent
Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on. IEEE, 2014, pp. 4253–4259.
[20] D. Schulz, W. Burgard, D. Fox, and A. B. Cremers, “Tracking multiple
moving targets with a mobile robot using particle filters and statistical
data association,” in Robotics and Automation, 2001. Proceedings
2001 ICRA. IEEE International Conference on, vol. 2. IEEE, 2001,
pp. 1665–1670.
[21] D. Krajzewicz, J. Erdmann, M. Behrisch, and L. Bieker, “Recent development and applications of SUMO - Simulation of Urban MObility,”
International Journal On Advances in Systems and Measurements,
vol. 5, no. 3&4, pp. 128–138, December 2012.

Fig. 4: Scenario in which pedestrians mistaken for a vehicle
result in a mis-classification with high confidence. The
camera image is on the left, and the corresponding time series
given by the particle filter is on the right.

Furthermore, there is also a positive relationship between
the accuracy and the BLSTM classifier’s prediction confidence, which we define as the maximum probability among
all values of zt . In other words, as the classifier observes
more vehicles it grows more confident in the prediction and
this results in a higher classification accuracy. However, there
are instances in which this does not hold true. Specifically,
it can be seen that the accuracy is poor while the prediction
confidence is high for the Real data set when the mean
observation length is between 4s and 5s in Fig. 3. On further
analysis, this occurred when the target vehicle was stopped in
front of pedestrians crossing a red light as shown in Fig. 4.
A single vehicle was tracked for several seconds moving
directly in front of the target vehicle with an average speed
of 2.83m/s. The most likely scenario is that the pedestrians
crossing the street were mistaken for a vehicle turning left,
which resulted in a prediction of a green light when in fact,
the light was red.
VI. C ONCLUSION
This paper has shown that it is possible to accurately
infer the current state of a traffic light by analyzing the
spatial movements of nearby vehicles with respect to a
target vehicle. This method was evaluated on real-world
data gathered in Singapore and synthetic data generated
from a traffic simulator. In both cases, encouraging results
were achieved with three different classifiers: a feed-forward
neural network, a bidirectional long short-term memory
network, and a nearest neighbor classifier. It was found
that in most tested scenarios, a nearest neighbor classifier
obtained the best classification results. However, if the data
is particularly noisy, better accuracy may be achieved with
a BLSTM classifier.
Similar to a vision-based approach, the methodology presented here has failure cases in which inference produces
wrong results. The most obvious case is when no vehicles
are in observation range, however, it was also seen that in
some scenarios more than one vehicle may need to be in
observation range in order to make an accurate prediction.
Likewise, there are specific instances in which the inference
may be wrong if other vehicles are only observed for an
extremely brief period of time. However, since the failure
cases for our approach and a vision-based approach are not
288

2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems
October 7-12, 2012. Vilamoura, Algarve, Portugal

Generalization of Human Grasping for Multi-Fingered Robot Hands
Heni Ben Amor, Oliver Kroemer, Ulrich Hillenbrand, Gerhard Neumann, and Jan Peters

Abstract— Multi-fingered robot grasping is a challenging
problem that is difficult to tackle using hand-coded programs.
In this paper we present an imitation learning approach for
learning and generalizing grasping skills based on human
demonstrations. To this end, we split the task of synthesizing
a grasping motion into three parts: (1) learning efficient grasp
representations from human demonstrations, (2) warping contact points onto new objects, and (3) optimizing and executing
the reach-and-grasp movements. We learn low-dimensional
latent grasp spaces for different grasp types, which form the
basis for a novel extension to dynamic motor primitives. These
latent-space dynamic motor primitives are used to synthesize
entire reach-and-grasp movements. We evaluated our method
on a real humanoid robot. The results of the experiment
demonstrate the robustness and versatility of our approach.

I. I NTRODUCTION
The ability to grasp is a fundamental motor skill for
humans and a prerequisite for performing a wide range of
object manipulations. Therefore, grasping is also a fundamental requirement for robot assistants, if they are to perform
meaningful tasks in human environments. Although there
have been many advances in robot grasping, determining
how to perform grasps on novel objects using multi-fingered
hands still remains an open and challenging problem.
A lot of research has been conducted on robot grippers
with few degrees of freedom (DoF) which may not be
particularly versatile. However, the number of robot hands
developed with multiple fingers has been steadily increasing in recent years. This progress comes at the cost of
a much higher dimensionality of the control problem and,
therefore, more challenges for movement generation. Hard
coded grasping strategies will typically result in unreliable
robot controllers that can not sufficiently adapt to changes
in the environment, such as the object’s shape or pose.
Such hard coded strategies will also often lead to unnatural
‘robotic looking’ grasps, that do not account for the increased
sophistication of the hardware. Alternative approaches, such
as the optimization of grasps using stochastic optimization
techniques, are computationally expensive and require the
specification of a grasp quality metric [27]. Defining an
adequate grasp metric is often hard to do, as it requires specifying intuitive concepts in a mathematical form. Additionally,
such approaches typically do not consider the whole reachand-grasp movement but exclusively concentrate on the hand

Fig. 1. The Justin robot learns to grasp and lift-up a mug by imitation. The
reach-and-grasp movement is learned from human demonstrations. Latentspace dynamic motor primitives generalize the learned movement to new
situations.

Heni Ben Amor, Oliver Kroemer, Gerhard Neumann and Jan Peters
are with the Technische Universitaet Darmstadt, Intelligent Autonomous
Systems, Darmstadt, Germany. {amor, kroemer, neumann,

A. Related Work

peters}@ias.tu-darmstadt.de
Ulrich Hillenbrand is with the German Aerospace Center - DLR,
Institute of Robotics and Mechatronics, Oberpfaffenhofen, Germany.

Ulrich.Hillenbrand@dlr.de
978-1-4673-1736-8/12/S31.00 ©2012 IEEE

configuration at the goal.
In this paper, we present an imitation learning approach for
grasp synthesis. Imitation learning allows a human to easily
program a humanoid robot [3], and also to transfer implicit
knowledge to the robot. Instead of programming elaborate
grasping strategies, we use machine learning techniques to
successfully synthesize new grasps from human demonstration. The benefits of this approach are threefold. First, the
computational complexity of the task is significantly reduced
by using the human demonstrations along with compact lowdimensional representations thereof. Second, the approach
allows us to imitate human behavior throughout the entire
reach-and-grasp movement, resulting in seamless, naturallooking motions. Typical transitions between a discrete set
of hand shapes, as can be found in traditional approaches,
are thus avoided. Finally, this approach also allows the user
to have control over the type of grasp that is executed.
By providing demonstrations of only one particular grasp
type, the synthesis algorithm can be used to generate distinct
grasps e.g., only lateral, surrounding, or tripod grasp. The use
of assorted grasps can considerably improve the robustness
of the grasping strategy as the robot can choose a grasp type
which is appropriate for the current task.

In order to generalize human grasping movements, we
need to understand how humans perform grasps. Human
grasping motions consist of two components: the reaching
motion of the arm for transporting the hand, and the motions

2043

Grasp Type Learning

DB
Contact
Warping

Grasp Spaces

Human Demonstration

Grasp Spaces
Grasp
Optimizer

LS-DMP

Grasp
Conﬁguration

Fig. 2. An overview of the proposed approach. The contact points of a known object are warped on the current object. Using the resulting positions, an
optimizer finds the ideal configuration of the hand during the grasp. The optimizer uses low-dimensional grasp spaces learned from human demonstrations.
Finally, a latent space dynamic motor primitive robustly executes the optimized reach-and-grasp motion. The approach is data-driven and can be used to
train and execute different types grasps.

of the fingers for shaping the hand [16], [17]. These two
components are synchronized during the grasping movement
[7]. For example, at around 75% of the movement duration,
the hand reaches its preshape posture and the fingers begin
to close [15]. At this point in time, the reaching motion of
the hand shifts into a low velocity movement phase.
Early studies into human hand control assumed muscles
and joints as being controlled individually by the central
nervous system [26], [19]. However, more recent studies have
found evidence suggesting that the fingers are controlled using hand synergies [23], [2] — i.e., the controlled movements
of the fingers are synchronized.
According to this view, fingers are moved “synergistically”
thereby reducing the number of DoF needed for controlling
the hand. Such hand synergies can be modeled as projections
of the hand configuration space into lower-dimensional subspaces [20] such as the principal components. Movements
along the first principal component of this subspace result
in a basic opening and closing behavior of the hand. The
second and higher-order principal components refine this
motion and allow for more precise shaping of the hand [20],
[24], see Fig. 3. Although the majority of the variation in
the finger configurations is within the first two principal
components, higher-order principal components also contain
important information for accurately executing grasps [23].
The gain in grasp accuracy does, however, plateau at around
five dimensions [22], [20]. Therefore, the space of human
hand synergies during grasping can be well represented by
a five-dimensional subspace.
Following this idea, various researchers have used dimensionality reduction techniques to find finger synergies
in recorded human grasps [4], [8]. Once a low-dimensional
representation of finger synergies is found, it can be used
to synthesize new grasps in a generate-and-test fashion. For
example, the authors of [8] use Simulated Annealing to find

an optimal grasp on a new object while taking into account
the finger synergies. Common to such approaches is the
use of a grasp metric [27] that estimates the quality of a
potential solution candidate. However, such metrics can be
computationally demanding and rely on having an accurate
model of the objects. In general, it is difficult to define
a grasp metric that includes both, physical aspects of the
grasps (such as the stability) as well as functional aspects
that depend upon the following manipulations.
Alternative approaches to grasp synthesis predict the success probability of grasps for different parts of the object. For
example, good grasping regions are estimated from recorded
2D images of the object in [25]. A labeled training set
of objects including the grasping region is subsequently
produced by using a ray-tracing algorithm. The resulting
dataset is then used to train a probabilistic model of the
ideal grasping region. The learned model, in turn, allows
a robot to automatically identify suitable grasping regions
based on visual features. In a similar vein, Boularias et al.
[6] use a combination of local features and Markov Random
Fields to infer good grasping regions from recorded point
clouds. Given an inferred grasping region, the reach-andgrasp motion still needs to be generated using a set of
heuristics. Additionally, this approach does not address the
problems of how to shape the hand and where to place the
finger contacts.
Tegin et. al. [28] also used imitation learning from human
demonstration to extract different grasp types. However,
they do not model the whole reach-and-grasp movement
and circumvent the high-dimensionality problem by using
simpler manipulators.
II. O UR A PPROACH
In our approach, we address the challenges of robot grasping by decomposing the task into three different stages: (1)

2044

2. Principal Component

learning efficient grasp representations from human demonstrations, (2) warping contact points onto new objects, and
(3) optimizing and executing the synchronized reach-andgrasp movements.
An overview of the proposed approach can be seen in
Fig. 2. The contact points of a known object are first warped
onto the current object using the techniques in Sec. II-B.
The warped contact points are then used by the optimizer
to identify all parameters needed for executing the grasp,
i.e., the configuration of the fingers and the position and
orientation of the hand. The optimization is performed in
low-dimensional grasp spaces which are learned from human
demonstrations. Finally, the reach-and-grasp movement is
executed using a novel extension to dynamic motor primitive
[14] called latent-space dynamical systems motor primitive
(LS-DMP).
A. Learning Grasp Types from Human Demonstration
Using human demonstrations as reference when synthesizing robot grasps can help to narrow down the set of
solutions and increase the visual appeal of the generated
grasp. At the same time, a discrete set of example grasps
can also heavily limit the power of such an approach. To
overcome this problem, we use dimensionality reduction
techniques on the set of human demonstrations in order
to infer the low-dimensional grasp space. To this end, we
recorded the movements of nine test-subjects, where each test
subject was asked to perform reach-and-grasp actions on a
set of provided objects. We subsequently performed Principal
Component Analysis (PCA) on the dataset, projecting it onto
five principal components. This choice of dimensionality is
based on research on the physiology of the human hand
[22], [20] which suggested that five principle components
are sufficient for accurately modeling the movements of the
human hand.
The resulting grasp space is a compact representation
of the recorded grasps as it models the synergies between
the different fingers and finger segments. The first principal
component, for example, encodes the opening and closing
of the hand. Fig. 3 shows grasps from the space spanned by
the first two principal components.
The above approach yields general grasp spaces that do
not give the user control over the grasp type to be executed
by the robot. However, for many tasks it is important to favor
a particular grasp type over another when synthesizing the
robot movements. For example, for carrying a pen one can
use a tip grasp, while for writing with the pen an extension
grasp is better suited. Hence, in a second experiment with the
same test subjects we learned grasp spaces for specific grasp
types, such as lateral grasps or tripod grasps. To determine
the grasp space, we devised a grasp taxonomy [10] consisting
of twelve grasp types and recorded specific datasets for each
of these types. The datasets were subsequently used to learn
grasp spaces for the specific grasp type.
Due to the differences in kinematics of the human and
robot hand, there are multiple ways to map the hand state to
the robot state, also known as the correspondence problem

1. Principal Component
Fig. 3. The space spanned by the first two principal components of human
recorded grasps applied to the robot hand. The first component describes the
opening and closing of the hand. The second principal component modulates
the shape of the grasp.

in the robotics literature [9]. In this paper, we solve the
correspondence problem by dividing the generalization of
grasps into two parts, i.e., the reproduction of the hand shape
and the adaptation of Cartesian contact points. The reproduction of the hand shape is realized by directly mapping the
human joint angles to the robot hand. For the index, middle
and ring fingers this results in an accurate mapping with
robot hand configurations similar to the demonstrated human
hand shapes. In order to map the thumb, an additional offset
needed to be added to the carpometacarpal joint. Using this
type of mapping, the reproduced hand shapes will be similar
to those of the human. The generalization of the Cartesian
contact points is achieved by the contact warping algorithm
described in Sec. II-B. The two generalizations in Cartesian
space and in joint space are then reconciled through the
optimization process explained in Sec. II-D.
B. Generalizing Grasps through Contact Warping
In this section, we introduce the contact warping algorithm. This algorithm allows the robot to adapt given contact
points from a known object to a novel object. As a result, we
can generalize demonstrated contact points to new situations.
Assume that we are given two 3D shapes from the same
semantic/functional category through dense sets of range data
points. In our approach, the process of shape warping, that
is, computing a mapping from the source shape to the target
shape, has been broken down into three steps.

2045

1) Rigid alignment of source and target shapes, such
that semantically/functionally corresponding points get
close to each other.

2) Assignment of correspondences between points from
the source shape and points on the target shape.
3) Interpolation of correspondences to a continuous (but
possibly non-smooth) mapping.
The alignment step involves sampling and aligning many
surflet pairs, i.e., pairs of surface points and their local
normals, from source and target shapes. The estimation of
relative clusters of the pose parameters is obtained from the
surflet-pair alignments [11], [12].
Since the alignment of source and target shapes has
brought corresponding parts close to each other, we can
again rely on the local surface description by surflets to
find correspondences, based on proximity of points and
alignment of normal vectors. The correspondence assignment
that we have used here is an improved version of the method
described in [11]. In this approach, correspondences were
assigned for each source surflet independently into the set
of target surflets. For strong shape variations or unfavorable
alignment between source and target, such an approach could
result in a confusion of similar parts.
In order to cope with larger shape variation, some interaction between assignments of neighboring points has to be
introduced. We have, therefore, formulated correspondence
search as an optimal assignment problem. In this formulation, interaction between assignments of different points is
enforced through uniqueness constraints.
Let {x1 , . . . , xN } be points from the source shape, transformed to align with the target shape; let {y1 , . . . , yN } be
points from the target shape.1 Assignment of source point i
to target point j is expressed as an assignment matrix,

1 if i is assigned to j,
aij =
(1)
0 otherwise.
Furthermore, let dij = kxi − yj k be the Euclidean distances
between source and target points and cij = ni · mj be the
angle cosines between the unit normal vectors ni and mj at
source point i and target point j, respectively.
The objective is to minimize the sum of distances between
correspondences, i.e., mutually assigned points,
D(a11 , . . . , a1N , a21 , . . . , aN N ) =

N
N X
X

dij aij ,

(2)

subject to the constraints
aij = 1 ∀j ∈ {1, . . . , N } ,

(3)

i=1

i.e., to assign every target point to exactly one source point,
N
X

aij = 1 ∀i ∈ {1, . . . , N } ,

(4)

j=1

i.e., to assign every source point to exactly one target point,
and
cij aij ≥ 0 ∀i, j ∈ {1, . . . , N } ,
(5)
1 An

i.e., to assign only between points with inter-normal angle
of ≤ 90 degrees. The two equality constraints (3) and
(4) mediate the desired interaction between assignments of
different points. The inequality constraint (5) can exclude
points from being assigned and, therefore, the problem may
become infeasible. Thus, we have to add imaginary source
and target points x0 and y0 which have no position and no
normal direction. They can be accommodated by appending
large entries d0j and di0 to the distance matrix, which larger
than all real distances in the data set, as well as zero entries
c0j = ci0 = 0 to the angle cosine matrix. These imaginary
points can be assigned to all real points with a penalty, which
is chosen such that only points without a compatible partner
will receive this imaginary assignment. We subsequently
minimize the cost function
C(a01 , . . . , a0N , a10 , . . . , aN N )
= D(a11 , . . . , a1N , a21 , . . . , aN N )
+

N
X
i=1

i=1 j=1

N
X

Fig. 4. Mug warping example. A dense set of surface points from the
source mug (top row) and their mappings to the target mug (bottom row)
are colored to code their three Cartesian source coordinates (three columns).

equal number N of points from source and target shapes can always
be re-sampled from the original data sets.

di0 ai0 +

N
X

(6)

d0j a0j .

j=1

For solving this constrained optimization problem, we use
the interior-point algorithm, which is guaranteed to find an
optimal solution in polynomial time [30].
Finally, point correspondences are interpolated to obtain
a continuous (but possibly non-smooth) mapping of points
from the source domain to the target domain. More theory
and systematic evaluations of the procedure are given in [13].
Fig. 4 shows an example of a dense set of surface points
warped between two mugs. A warp of the contact points of
an actual grasp from the source to the target mug is shown
on the left of Fig. 2.
C. Latent Space Dynamic Motor Primitives
In order to execute different grasps, the robot requires
a suitable representation of the grasping actions. Ideally,
the grasping action should be straightforward to learn from

2046

a couple of human demonstrations and easily adapted to
various objects and changes in the object locations. The
action representation should also ensure that the components
of the grasping movement are synchronized. The dynamical
systems motor primitives (DMPs) representation fulfills all
of the above requirements [14]. DMPs have been widely
adopted in the robotics community, and are well-known
for their use in imitation learning [21], [18]. The DMP
framework represents the movements of the robot as a set of
dynamical systems
ÿ = αz (βz τ −2 (g − y) − τ −1 ẏ) + aτ −2 f (x, θ1:N )
where y is a state variable, g is the corresponding goal state,
and τ is a time scale. The first set of terms represents a
critically-damped linear system with constant coefficients αz
and βz . The last term, with amplitude coefficient a = g − y0 ,
incorporates a shaping function
PN
i=1 ψi (x)θi x
,
f (x, θ1:N ) = P
N
j=1 ψj (x)

where ψi (x) are Gaussian basis functions, and the weight
parameters θ1:N define the general shape of the movements.
The weight parameters θ1:N are straightforward to learn
from a single human demonstration of a goal directed
movement. The variable x is the state of a canonical system
shared by all DoFs. The canonical system acts as a timer
to synchronize the different movement components. It has
the form ẋ = −τ x, where x0 = 1 at the beginning of
the motion and thereafter decays towards zero. The metaparameters g, a, and τ can be used to generalize the learned
DMP to new situations. For example, the goal state g of
the reaching movement is defined by the position of the
object and the desired grasp. We explain how the DMP goal
meta-parameters are computed for new objects in Sec. II-D.
However, we need first to define how the finger trajectories
can be encoded as DMPs, such that they generalize to new
situations in a human-like manner.
Representing and generalizing the motions of the fingers
is a challenging task due to the high dimensionality of the
finger-configuration space. A naive solution would be to
assign one DMP to each joint [19]. However, as previously
discussed in Sec. I-A, humans seem to generalize their
movement trajectories within lower-dimensional spaces of
the finger configurations, and not at the level of each joint
independently [23], [20]. If the robot’s generalization of the
grasping action does not resemble the human’s execution,
implicit information contained within the human demonstrations is lost. Therefore, in order to facilitate behavioral
cloning of human movements, the DMPs for multi-fingered
hands should be realized in a lower dimensional space. In
addition, overfitting is avoided by representing the movement
in a lower-dimensional space.
In particular, the DMPs can be defined in the latent
spaces learned in Sec. II-A. As such spaces are learned
from complete trajectories of the grasping movements, they
also include the finger configurations needed for representing

the hand during the approach and preshaping phases of the
action, as well as the final grasps [20]. We use a DMP for
each of the latent space dimensions as well as DMPs for
the wrist position and orientation. The weight parameters
for these DMPs can be learned from human demonstrations
by first projecting the tracked motions into the latent space
and subsequently learning the weights from the resulting
trajectory. Thus, the same data that is used to learn the
latent space can be reused for learning the weight parameters.
The resulting latent-space DMPs, as well as the reaching
movement’s DMPs, are linked to the same canonical system,
thus, ensuring that they remain synchronized. The output of
the latent-space DMPs is afterwards mapped back into the
high-dimensional joint space by the PCA projection. In this
manner, the grasping action can be executed seamlessly, and
the robot can begin closing its fingers before the hand has
reached its final position.
Thus, we have defined a human-like representation of
the grasping movements that can be acquired by imitation
learning. Given this DMP representation, the robot still needs
to determine the meta-parameters for new situations. This
process is described in the next section.
D. Estimating the Goal Parameters
In order to generalize the latent-space DMPs to new
objects, we need to estimate the goal state g for each latentspace dimension, as well as the orientation of the hand
for a new set of contact points which we have acquired
from contact warping as discussed in Sec. II-B. We use
one contact-point per finger, where the contact point is
always located at the finger tip. Each point is specified in
Cartesian coordinates. As we have four fingers, this results
into a 12-dimensional task space vector xC . Additionally,
we also want to estimate the position and orientation of the
hand in the world coordinate frame. We therefore add six
virtual joints v, i.e., three translational and three rotational
joints. We will denote the transformation matrix, which is
defined by these six virtual joints, as T(v). We define the
finger tip position vector x1:4 as the concatenation of all
four finger tip positions. This vector is a function of the
transformation matrix T(v) and the joint configurations of
the fingers q = m + Kg, i.e.,
φW (y) = T(v)φH (m + Kg).
The vector m represents the mean of the PCA transformation
and K is given by the first five eigenvectors. The function
φH (q) calculates the finger tip-positions in the local hand
coordinate frame. This setup is an inverse kinematics problem with the difference that we want to optimize the joint
positions of the fingers in the latent space instead of directly
optimizing the joint positions q. Thus, the inverse kinematics
problem is over-constrained as we have twelve task variables
and only eleven degrees of freedom. Therefore, instead of
the standard Jacobian pseudo-inverse solution, we need to
employ a different approach.
Our task is to estimate the optimal configuration y∗ =
∗
[v , g∗ ] of the hand, which consists of the orientation and

2047

the latent space coordinates, such that the squared distance
between finger-tip positions x1:4 and the contact points xC
is minimal, i.e.,
y∗

=

[v∗ , g∗ ] = argmaxy L(y),

L(y)

=

−(φW (y) − xC )T C−1 (φW (y) − xC ) (7)
+yT Wy.

The matrix W = diag(w) defines a damping or regularization term for the step-size of y, and C = diag(c)
defines the inverse precision for each task variable. The
Jacobian J = ∂φW /∂y for this problem can be obtained
straightforwardly, i.e., for the derivation w.r.t v it is given
by the standard geometric Jacobian and for the derivation
w.r.t the latent variable g it is given by Jl = Jq K, where Jq
denotes the geometric Jacobian.
We will solve the optimization problem given in Equation
(7) by iteratively applying a least squares solution. Given
the current hand configuration yk and the desired finger-tip
positions xC , the update step for the hand configuration is
therefore given by
∆yk = (JT CJ + W )−1 JT C (xC − φW (yk )) .

(8)

As we have to solve an overconstrained inverse kinematics
setting, in contrast to the more common underconstrained
inverse kinematics setting, we use the left-pseudo inverse
in Equation (8). This update equation also corresponds to
a Bayesian view on inverse kinematics [29]. We repeat the
update until convergence in order to get the optimal hand
configuration y∗ . We always start our optimization from an
initial posture where the hand is pointing downwards.
III. S ETUP AND E VALUATIONS
To evaluate the proposed approach, we conducted a set
of experiments using the Rollin’ Justin robot platform [5].
Justin is a mobile humanoid robot system with an upperbody including 43 actuated degrees-of-freedom (DoF). In our
experiments we controlled 22 DoF pertaining to the Torso
(3 DoF), the right arm (7 DoF), and the four-fingered right
hand (12 DoF). The experiments were performed both in
simulation and on the physical robot.
A. Simulation Results
In the first experiment, we evaluated the performance
and the results of our approach in a simulated environment
for the Justin robot. As explained in Sec. II, we trained
individual LS-DMPs for each of the principal components of
the demonstrated reach-and-grasp movement. Fig. 5 shows
the latent-space trajectories for three out of the five principal
components of the hand shape. The example trajectories are
depicted in blue, while the trajectory learned by the LSDMP is depicted in red. This figure reveals an interesting
insight into the nature of the recorded human reach-and-grasp
movements: many of the example trajectories have a distinct
sigmoid shape that has a bell-shaped velocity profile. This
insight corresponds to the results in [1] , which showed that

Fig. 6. The Justin robot executes a reach-and-grasp movement in simulation. Using the trained LS-DMP a new trajectory (red) to the target
object is synthesized. The optimal hand position and orientation (shown
as a coordinate system) is estimated along with the optimal hand shape in
latent-space.

humans perform point-to-point reaching movements such
that the velocity profile along the path can be characterized
by a symmetric bell-shape. Our results indicate that a similar
property holds for the latent space trajectories of the hand
shape during a reach and grasp.
After learning, we first executed the LS-DMP in simulation. Fig. 6 shows the start and end configuration during one
run of the algorithm. The red curve depicts the trajectory of
the hand as generated by the LS-DMP, while the displayed
coordinate system shows the estimated hand orientation of
the robot. To evaluate the accuracy of the produced grasping
motions, we repeatedly changed the position and orientation
of the target object and measured the distance between
the warped contact points on the object and the fingertip
positions. Ideally, the fingertips should always coincide with
the contact points. Tab. I shows the average distance of the
fingers to the warped contact points after executing a reachand-grasp movement. We also varied the grasp spaces in
order to evaluate the effect of the grasp type on the the
resulting hand shape. The grasp space indicated by Multi in
Tab. I was learned using all available human demonstrations.
This grasp space encompasses a wide range of variations of
the human hand. As can be seen in the table, we achieved
the most accurate results by using this grasp space. In this
case, the average error is about 7mm.
It should be noted that the fingers of the robot are much
larger than human fingers and have a width of about 3cm.
Given the size of the robot’s fingers, the produced error only
corresponds to about a quarter of the finger width. The table
clearly shows that changing the grasp type results in higher
average error. This increased error is to be expected, as we
constrained the space of possible solutions to a specific grasp
type. At the same time, visual inspection of the resulting
grasps shows that this error does not deteriorate the quality
of the resulting grasps, as will be seen in the next section.
B. Real Robot Experiments
We also conducted experiments with the real Rollin’ Justin
robot. Three different types of mugs were used during
the experiments. After placing a mug on a table in front
of the robot, all information about the pose of the mug

2048

2

1.5

1.5

1.5

1

0.5

0

−0.5

Eigenvector 5

2

Eigenvector 3

Eigenvector 1

2

1

0.5

0

0

0.2

0.4

0.6

0.8

1

−0.5

1

0.5

0

0

0.2

0.4

Time

0.6

0.8

1

−0.5

0

0.2

Time

0.4

0.6

0.8

1

Time

Fig. 5. The plots show example trajectories (blue), and the mean trajectory (red), for three (out of five) latent space dimensions during the closing of the
hand. The trajectories have been shifted and scaled to start at zero and end at one, in order to allow for easier comparison of their shapes. As the scaled
trajectories have similar shapes, they can be represented by individual DMPs and easily learned from human demonstrations.

TABLE I

Tripod

Surrounding

Lateral

Multi
0.007

Tripod
0.013

Surrounding
0.0157

Lateral
0.014

was estimated using a Kinect camera and the techniques
explained in [12]. Subsequently, using the contact warping
techniques from Sec. II-B the contact points from a known
mug were warped onto the currently seen mug. The resulting
contact points were subsequently fed into the optimizer to
estimate all parameters that are needed to execute the reachand-grasp movement. The estimation of all parameters using
the algorithm in Sec. II-D takes about one to five seconds.
We performed about 20 repetitions of this experiment with
the different mugs placed at various positions and heights.
Additionally, we included a lifting-up motion to our movement, in order to evaluate whether the resulting grasp was
stable or not. In all of the repetitions the robot was able to
successfully grasp and lift-up the observed object.
Furthermore, we also executed the reach-and-grasp movements using grasp spaces belonging to different grasp types.
No change was made to the structure or other parameters of
the algorithm. The only difference between each execution
run was the grasp space to be loaded. Fig. 7 shows three of
the grasp types used in our taxonomy along with the result of
applying them to the Justin robot. The figure clearly shows
that changing the grasp type can have a significant effect
on the appearance of the executed grasp. For example, we
can see that the use of the tripod-grasp results in delicate
grasps with little finger opposition, while surrounding grasps
lead to more caging grasps with various finger oppositions.
Our approach exploits the redundancy in hand configurations
and allows desired grasp types to be set according to the
requirements of the manipulation task that is going to be
executed. Fig. 8 shows a sequence of pictures captured
from one of the reach-and-grasp movements executed on the
real robot. Reach-and-grasp movements for different grasp
types and situations are shown in the video submitted as
supplemental material.

Robot

Grasp Type
Avg. Error (m)

Human

AVERAGE DISTANCE BETWEEN WARPED CONTACT POINTS AND
FINGERTIPS AFTER GRASPING .

Fig. 7. The three grasp types lateral, surrounding, and tripod from our
taxonomy are demonstrated by a human and later reproduced by the Justin
robot. All parameters of the reach-and-grasp movement, such as the shape
of the hand, its position, and orientation are automatically determined using
latent space dynamic motor primitives.

IV. C ONCLUSION
In this paper, we presented a new approach for imitation and generalization of human grasping skills for multifingered robots. The approach is fully data-driven and learns
from human demonstrations. As a result, it can be used
to easily program new grasp types into a robot – the user
only needs to perform a set of example grasps. In addition
to stable grasps on the object, this approach also leads to
visually appealing hand configurations of the robot. Contact
points from a known object are processed by a contact
warping technique in order to estimate good contact points
on a new object.
We, furthermore, presented latent-space dynamic motor
primitives as an extension to dynamic motor primitives that
explicitly models synergies between different body parts.
This significantly reduces the number of parameters needed
to control systems with many DoF such as the human hand.
Additionally, we have presented a principled optimization
scheme that exploits the low-dimensional grasp spaces to
estimate all parameters of the reach and grasp movement.

2049

Fig. 8. A sequence of images showing the execution of a reach-and-grasp movement by the Justin humanoid robot. The executed latent-space dynamic
motor primitive was learned by imitation. The type of the grasp to be executed can be varied according to the requirements of the task to subsequently
executed. New grasp types can be trained within minutes by recording a new set of human demonstrations.

The proposed methods were evaluated both in simulation
and on the real Justin robot. The experiments exhibited the
robustness of the approach with respect to changes in the
environment. In all of the experiments on the real, physical
robot, the method successfully generated reach-and-grasp
movements for lifting up the seen object.
ACKNOWLEDGMENT
We thank Florian Schmidt and Christoph Borst from the
DLR - German Aerospace Center for their help with the
Justin robot and for their valuable comments and suggestions.
H. Ben Amor was supported by a grant from the Daimlerund-Benz Foundation. The project receives funding from
the European Community’s Seventh Framework Programme
under grant agreement n ICT- 248273 GeRT.
R EFERENCES
[1] W. Abend, E. Bizzi, and P. Morasso. Human arm trajectory formation.
Brain : a journal of neurology, 105(Pt 2):331–348, jun 1982.
[2] M. Arbib, T. Iberall, and D. Lyons. Coordinated control programs for
movements of the hand. Experimental brain research, pages 111–129,
1985.
[3] H. Ben Amor. Imitation learning of motor skills for synthetic humanoids. PhD Thesis, Technische Universitaet Bergakademie Freiberg,
Freiberg, Germany, 2011.
[4] H. Ben Amor, G. Heumer, B. Jung, and A. Vitzthum. Grasp synthesis
from low-dimensional probabilistic grasp models. Comput. Animat.
Virtual Worlds, 19(3-4):445–454, sep 2008.
[5] C. Borst, T. Wimbock, F. Schmidt, M. Fuchs, B. Brunner, F. Zacharias,
P. R. Giordano, R. Konietschke, W. Sepp, S. Fuchs, C. Rink, A. AlbuSchaffer, and G. Hirzinger. Rollin’ justin - mobile platform with
variable base. In Robotics and Automation, 2009. ICRA ’09. IEEE
International Conference on, pages 1597 –1598, may 2009.
[6] A. Boularias, O. Kroemer, and J. Peters. Learning robot grasping
from 3-d images with markov random fields. In 2011 IEEE/RSJ
International Conference on Intelligent Robots and Systems, IROS
2011, pages 1548–1553, 2011.
[7] S Chieffi and M Gentilucci. Coordination between the transport and
the grasp components during prehension movements. Experimental
Brain Research, pages 471–477, 1993.
[8] M. T. Ciocarlie and P. K. Allen. Hand posture subspaces for dexterous
robotic grasping. Int. J. Rob. Res., 28(7):851–867, July 2009.
[9] K. Dautenhahn and C. L. Nehaniv. Imitation in Animals and Artifacts.
MIT Press, Campridge, 2002.
[10] G. Heumer. Simulation, Erfassung und Analyse direkter Objektmanipulationen in virtuellen Umgebungen. PhD Thesis, Technische
Universitaet Bergakademie Freiberg, Freiberg, Germany, 2011.
[11] U. Hillenbrand. Non-parametric 3d shape warping. In Pattern
Recognition (ICPR), 2010 20th International Conference on, pages
2656 –2659, 2010.

[12] U. Hillenbrand and A. Fuchs. An experimental study of four variants
of pose clustering from dense range data. Computer Vision and Image
Understanding, 115(10):1427 – 1448, 2011.
[13] U. Hillenbrand and M. A. Roa. Transferring functional grasps through
contact warping and local replanning. In 2012 IEEE/RSJ International
Conference on Intelligent Robots and Systems, IROS, 2012.
[14] A.J. Ijspeert, J. Nakanishi, and S. Schaal. Movement imitation
with nonlinear dynamical systems in humanoid robots. In Robotics
and Automation, 2002. Proceedings. ICRA ’02. IEEE International
Conference on, volume 2, pages 1398 –1403, 2002.
[15] M Jeannerod. The timing of natural prehension movements. Journal
of Motor Behavior, 16(3):235–254, 1984.
[16] M. Jeannerod. Perspectives of Motor Behaviour and Its Neural Basis,
chapter Grasping Objects: The Hand as a Pattern Recognition Device.
1997.
[17] M. Jeannerod. Sensorimotor Control of Grasping: Physiology and
Pathophysiology, chapter The study of hand movements during grasping. A historical perspective. Cambridge University Press, 2009.
[18] J. Kober, B. Mohler, and J. Peters. Learning perceptual coupling for
motor primitives. In Intelligent Robots and Systems, 2008. IROS 2008.
IEEE/RSJ International Conference on, pages 834 –839, sept. 2008.
[19] R. N. Lemon. Neural control of dexterity: what has been achieved?
Exp Brain Res, 128:6–12+, 1999.
[20] C. R. Mason, J. E. Gomez, and T. J. Ebner. Hand Synergies
During Reach-to-Grasp. Journal of Neurophysiology, 86(6):2896–
2910, December 2001.
[21] J. Nakanishi, J. Morimoto, G. Endo, G. Cheng, S. Schaal, and
M. Kawato. Learning from demonstration and adaptation of biped
locomotion. Robotics and Autonomous Systems, 47:79?–91, 2004.
[22] M. Saleh, K. Takahashi, and N.G. Hatsopoulos. Encoding of coordinated reach and grasp trajectories in primary motor cortex. J Neurosci,
32(4):1220–32, 2012.
[23] M. Santello, M. Flanders, and J. F. Soechting. Postural Hand Synergies
for Tool Use. The Journal of Neuroscience, 18(23):10105–10115,
December 1998.
[24] M. Santello and J. F. Soechting. Gradual molding of the hand to object
contours. Journal of neurophysiology, 79(3):1307–1320, March 1998.
[25] A. Saxena, J. Driemeyer, and A. Y. Ng. Robotic grasping of novel
objects using vision. Int. J. Rob. Res., 27(2):157–173, feb 2008.
[26] M H Schieber. How might the motor cortex individuate movements?
Trends Neurosci, 13(11):440–5, 1990.
[27] R. Suárez, M. Roa, and J. Cornella. Grasp quality measures. Technical
report, Technical University of Catalonia, 2006.
[28] Johan Tegin, Staffan Ekvall, Danica Kragic, Jan Wikander, and Boyko
Iliev. Demonstration-based learning and control for automatic grasping. Intelligent Service Robotics, 2009.
[29] M. Toussaint and C. Goerick. A bayesian view on motor control and
planning. In From Motor Learning to Interaction Learning in Robots,
pages 227–252. 2010.
[30] R. J. Vanderbei. Linear Programming: Foundations and Extensions.
Springer, 2001.

2050

