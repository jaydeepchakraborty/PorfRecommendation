Proceedings of the 2014 Winter Simulation Conference
A. Tolk, S. Y. Diallo, I. O. Ryzhov, L. Yilmaz, S. Buckley, and J. A. Miller, eds.

SNAT: SIMULATION–BASED SEARCH FOR
NAVIGATION SAFETY. THE CASE OF SINGAPORE STRAIT
Xingyi Chen

Giulia Pedrielli

Department of Industrial System Engineering
National University of Singapore
1 Engineering Drive 2, 117576, SINGAPORE

Centre for Maritime Studies
National University of Singapore
12 Prince George’s Park, 118411, SINGAPORE

Szu Hui Ng
Department of Industrial System Engineering
National University of Singapore
1 Engineering Drive 2, 117576, SINGAPORE

ABSTRACT
As the bottleneck of the shipping routes from the Indian Ocean to the Pacific Ocean, the Singapore
Strait is handling high daily traffic volume. In order to enhance navigational safety and reduce
collisions at sea, several approaches have been proposed. However, most of the contributions adopt
deterministic algorithms, failing to consider the stochasticity due to human behaviors of ship captains.
Moreover, the effectiveness of these approaches is hindered by the fact that their focus is on providing
a globally optimal safe set of trajectories to all vessels involved in encounter situations, almost
neglecting each captain’s perspective. We propose Safe Navigation Assistance Tool (SNAT), a
simulation–based search algorithm to assist the captain by suggesting highly safe and robust
maneuver strategies for conflict avoidance. Extensive numerical experimentation were performed
proving the effectiveness of SNAT in reducing the number of conflicts, with respect to real data
provided by the Automatic Identification System (AIS).
1

INTRODUCTION

The Singapore Strait is a 105-kilometer long, 16-kilometer wide corridor connecting the Strait of
Malacca and the South China Sea (Figure 1). As one of the busiest straits in the world, it is crossed by
about 400 vessels at any one time (MPA 2013). Hence, navigational safety is a fundamental aspect to
consider. The International Maritime Organization (IMO), the main regulatory body for sea traffic
management, has proposed the International Regulations for Preventing Collisions at Sea (Colregs).
Colregs regulates maneuver strategies with the scope to decrease the accident occurrence.
With compliance to Colregs and other economic criteria, good marine practice for taking
maneuvers should fulfill the following: (1) the course alteration should be bounded between 15
degrees and 60 degrees, since the course alterations less than 15 degrees might be misleading for
navigational systems (and therefore may lead to collisions) and course alterations larger than 60
degrees are not recommended for efficiency reasons; (2) maneuvers to starboard (right) side should be
favored over maneuvers to port (left) side; (3) speed alterations should be performed only in case no
alternative strategy exists to avoid conflicts; (4) speed decrease should be preferred to speed increase;
(5) extreme changes in speed should not be performed.
Nonetheless, given the remarkable environmental and operational costs both the marine
authorities and ship companies incur due to marine accidents and safety violations, a considerable
effort has been dedicated to the improvement of the available technologies for tracking vessels in
crowded sea regions as well as to the development of effective models and methods to support the

978-1-4799-7486-3/14/$31.00 ©2014 IEEE

1819

Chen, Pedrielli, and Ng
land offices and vessel captains predicting and avoiding conflict situations. The Automatic
Identification System (AIS) provides real–time data through VHF (Very High Frequency) signals,
which include vessel name, length, weight, speed and course. With this technology, a wide range of
software tools have been developed to provide navigational guidance based upon AIS data.

Figure 1 : Map of the Singapore Strait.
However, despite the tremendous efforts of the Maritime and Port Authority (MPA) of Singapore
to monitor traffic at the Singapore Strait and the noticeable technological advancement, the conflict
rate is still high. According to Weng et al. (2012), the average number of conflicts occurring at the
Singapore Strait is about 2000 per month. Among the conflicts, some are indeed severe accidents such
as the recent collision between two bulk carriers in the Singapore Strait on Feb 10 (2014) which was
the third collision in 13 days; resulting in a total of 680 metric tons of fuel being spilled (The Straits
Times, 2014). Nonetheless, less severe conflicts must be the focus of any support system in order to
avoid much worse situations.
Although many contributions can be found in the area of conflict avoidance in multi-ship
encounter situations, most of them adopt deterministic approaches which do not take into
consideration the stochasticity of ship movements due to the unpredictable behavior of the ship
captain.
This work proposes a simulation–based search algorithm named Safe Navigation Assistance Tool
(SNAT) which, installed on a vessel, pursues the objective to effectively reduce conflicts under
stochastic scenarios determined by the random behaviors of the vessel captain and of the surrounding
vessels. In particular, SNAT was developed with the scope of being integrated with the on board
navigation support devices.
The remainder of the paper is structured as follows: section 2 presents the problem that we want
to address through this study, followed by section 3 which is the proposed methodology.
Experimental conditions and results are explained in section 4. Section 5 concludes the paper and
provides directions for future research.
2

EXISTING APPROACHES AND CONTRIBUTION

Navigation safety algorithms seek a set of optimal maneuver (usually referred to as maneuver
strategy) which avoids possible conflict situations while minimizing a defined utility function. The
maneuver strategies proposed to ship captains can consist of a course change, speed change or a
combination of both. A course change can be either a positive change to the right or a negative change
to the left, while a speed change can be either a decrease or an increase in the vessel velocity.

1820

Chen, Pedrielli, and Ng
Two main approaches can be individuated in the literature related to planning optimal ship
trajectories in encounter situations: (1) deterministic approaches based on differential games
(Lisowski 2007); (2) heuristic approaches with the application of evolutionary algorithms
(Michalewicz et al. 2000).
The former model multi-ship encounter situations as a dynamic game with each vessel as a player
having its own maneuver strategies. This approach captures the dynamic interaction between vessels.
Indeed, when one vessel changes its maneuver strategy other vessels react accordingly. However,
high computational complexity is its major drawback.
The latter family of approaches uses mechanisms such as reproduction, mutation and
recombination to generate candidate maneuver strategies and it finds the optimal set of ship
trajectories according to a given fitness function. However, the optimal trajectory for each ship is
found based on pre-determined trajectories of other ships, i.e., other ships’ motion parameters are
assumed to be constant and this assumption can lead to large biases. Despite this shortfall, while most
studies do not consider Colregs, the most recent contributions in this family take Colregs into
consideration while searching for the strategy to suggest.
The optimal solution generated by these two families of approaches is represented by the set of
safe trajectories for all ships in the domain considered by the search procedures (i.e., the monitored
area). In fact, most of current studies focus on methodologies that provide a globally optimal safe set
of trajectories for all vessels in the monitored area, i.e., the optimality is defined with respect to a
global utility. However, little attention is directed towards examining random human behaviors and
their impact on the effectiveness of the developed procedure. Nonetheless, random behavior is a
fundamental aspect to consider as it reflects the preference of the ship captain for choosing a
maneuver strategy maximizing its own utility which might differ from the global utility considered by
the mentioned algorithms. In fact, it is very likely for ship captains to choose their own preferred
maneuver strategies, which may undermine the effectiveness of safe trajectories proposed by current
approaches.
The method proposed in this paper, as the recent evolutionary studies, considers Colregs
regulations, i.e., it suggests to the vessel having SNAT in place, whenever possible, a set of optimal
maneuvers which are also Colregs compliant.
SNAT has two further distinguishing features with respect to the main analyzed algorithms: (1) it
considers the random vessels moves as well as the stochasticity due to the human behavior of ship
captains, and (2) simulation is actively integrated with the search algorithm. In particular, as will be
detailed in section 3.2, each candidate strategy is tested against several simulation replications
enabling the evaluation of the strategy safety level leading, eventually, to the search of further
solutions. This aspect of the proposed procedure is relevant since it characterizes both the safety level
of the generated solution and its robustness with respect to different future scenarios.
3

SNAT OVERVIEW

SNAT is an integrated simulation search algorithm that suggests optimal maneuvers required for an
individual vessel (referred to as own vessel in the rest of the study) to avoid conflicts with surrounding
vessels (referred to as target vessels in the rest of the study). Specifically, a search algorithm
generates candidate maneuvers and interacts with a discrete time simulator which emulates the
vessel’s dynamics and evaluates the quality of the proposed solution. Being at its first
implementation, SNAT was not installed on board yet. Hence, we integrated the SNAT algorithm into
a simulation–based evaluation procedure, in order to provide an estimate of its effectiveness in
decreasing the number of conflicts when in place. In the evaluation procedure simulation replaces the
real world emulating future positions of own and target vessels. Section 3.1. illustrates the main
assumptions underlying the SNAT simulation optimization algorithm, and section 3.2 describes the
SNAT steps with more detail. Finally, the evaluation procedure is described in section 3.3.

1821

Chen, Pedrielli, and Ng
3.1

Model Assumptions

SNAT recognizes conflict situations using the concept of ship domain defined as the surrounding
effective area that the ship captain wants to keep clear of other ships or fixed objects (Goodwin 1975).
Several regular shapes are adopted in the literature to model the domain: Goldwell (1982) consider an
ellipse, whereas Wang et al. (2009) propose an eclipse with semi-minor axis equal to 1.6 times the
length of the vessel and semi-major axis which is 4 times the length of the vessel. The use of ship
domain enables conflict avoidance on top of collision avoidance, as the ship domain violation
(conflict) occurs before the contact between two vessels (collision).
The ship domain shape adopted in this study is the circular domain shape with a radius equal to 4
times the length of the vessel and a conflict is detected if the straight-line distance between own vessel
and the target vessels is less than the radius of the shape. This choice leads to the detection of
potential conflicts that may not be captured if an eclipse ship domain is used and take precautions
accordingly.
When potential conflicts are detected, the probability that ship captains will take immediate
maneuvers, referred to as 𝑃𝑃𝑀𝑀 , is assumed inversely proportional to the time to earliest conflict or, in
navigational terms, time to closest point of approach (referred to as TCPA in the rest of the paper).
More specifically, we assume ship captains will perform immediate maneuver if the conflict detected
is within a defined threshold interval of 10 minutes (i.e., 𝑃𝑃𝑀𝑀 = 1.00 in case 𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇 ≤ 10 as it is shown
in Table 1). If the time to the detected conflict is larger than this threshold, the probabilities that ship
captains will take immediate maneuvers were set according to expert opinion. In particular, three
scenarios were designed as shown in Table 1. Scenario 1 refers to the optimistic case in which the ship
captain performs immediate maneuvers, according to SNAT suggestion, 80% of the times if the
TCPA is between 10 and 20 minutes and 60% if TCPA is between 20 and 30 minutes. Scenario 2
represents the situation in which the ship captain performs immediate maneuvers 75% of the times if
the TCPA is between 10 and 20 minutes and 55% if TCPA is between 20 and 30 minutes. Finally, in
scenario 3, the ship captain performs immediate maneuvers 70% of the times if the TCPA is between
10 and 20 minutes and 50% if TCPA is between 20 and 30 minutes.
Table 1 : PM values.
Probability

Scenario 1

Scenario 2

Scenario 3

𝑃𝑃𝑀𝑀 |𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇 ≤ 10 𝑚𝑚𝑚𝑚𝑚𝑚

1.00

1.00

1.00

0.80

0.75

0.70

0.60

0.55

0.50

𝑃𝑃𝑀𝑀 |10 𝑚𝑚𝑚𝑚𝑚𝑚 < 𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇 ≤ 20 𝑚𝑚𝑚𝑚𝑚𝑚
𝑃𝑃𝑀𝑀 |20 𝑚𝑚𝑚𝑚𝑚𝑚 < 𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇 ≤ 30

We model the unpredictable maneuver that the ship captain performs due to its individual
preferences through the probability of a deliberate change in course and/or speed computed according
to the estimated distribution of the change in course/speed. Herein, deliberate refers to a voluntary
maneuver characterized by a change in either speed or course at least 1.5 standard deviations away
from average changes estimated for each vessel. The considered reference value was chosen because
minor deviations may be due to external factors such as wind, water flow and sensitivity of AIS
receivers.
We distinguished the following random maneuver: (A) change in course associated with
probability 𝑃𝑃𝐶𝐶 = 0.37, (A.1) change in course with right turn associated with a probability 𝑃𝑃𝑅𝑅 |𝑃𝑃𝐶𝐶 =
0.49, (A.2) change in course with left turn associated with a probability 𝑃𝑃𝐿𝐿 |𝑃𝑃𝐶𝐶 = 0.51, (B) change in
speed associated with probability 𝑃𝑃𝑆𝑆 = 0.12, (B.1) decrease in speed with associated probability
𝑃𝑃𝐷𝐷 |𝑃𝑃𝑆𝑆 = 0.73, and (B.2) speed increase with associated with a probability 𝑃𝑃𝐼𝐼 |𝑃𝑃𝑆𝑆 = 0.27.
In order to estimate the aforementioned probabilities, the AIS data for 1640 vessels over a 48–
hours period at the Singapore Strait were collected summing up to over 250,000 records. The traced
positions were used to evaluate the changes in course and speed between two consecutive AIS
updates. In order to define a distribution for each of the aforementioned four maneuvers, the data were
tested against 15 distributions at 5% significance level and distributions with the highest goodness of
1822

Chen, Pedrielli, and Ng
fit were selected. In particular, distributions for degree of course change to the right, degree of course
change to the left and percentage of speed decrease were assigned a normal distribution, while the test
for the distribution of percentage of speed increase suggested a lognormal fit. Parameters identified
for all maneuver options are summarized in Table 2; all the values are computed with respect to the
average changes estimated from the AIS data.
Table 2: Distributions for different maneuver options.
Maneuver Option

Best-fit
Distribution

Turn Right

𝝈𝝈

Unit
Measure

Normal

𝝁𝝁

103.05195

46.42626

[°]

Turn Left

Normal

-98.53705

45.74620

[°]

Decrease Speed

Normal

-0.57881

0.19179

[knots]

Increase Speed

Lognormal

0.47164

0.28727

[knots]

3.2 SNAT Detailed Procedure
As outlined in Figure 2, once installed on the vessel, SNAT will receive AIS data collection in case of
conflict detection. At this point the SNAT procedure starts with the following steps: (1) Maneuver
Strategy Generation Procedure: if a conflict is detected, the optimization algorithm seeks the best
maneuver minimizing a utility function which is based on the Colregs best practices; (2) Maneuver
Evaluation Procedure: the probability of future conflicts given the candidate maneuver strategy is
estimated through several simulation replications. In case the probability of conflict is below a
predefined threshold level SNAT returns the solution to the on–board devices and it will be run again
at the next detected conflict.

Figure 2 : SNAT algorithm overview.
When a conflict is detected, SNAT searches for optimal changes in course ∆CR,t (right turn) or
∆CL,t (left turn) and change in speed ∆SD,t (speed decrease) or ∆SI,t (speed increase) such that the own
vessel can avoid potential conflicts. One of the features of SNAT is that it explicitly considers Colregs
1823

Chen, Pedrielli, and Ng
regulations, i.e., whenever possible, the algorithm proposes as optimal strategy a set of maneuvers
which are Colregs compliant (Table 3).
Table 3: Search preference for maneuvers according to Colregs.
Preference
Maneuver Strategy
Rank 1
Rank 2
Rank 3
Rank 4
Rank 5
Rank 6
Rank 7
Rank 8

∆CR,t : from 15° to 60°

∆CL,t : from − 15° to − 60°

∆CR,t : from 15° to 60° and ∆SD,t : from-2 knots to-10 knots
∆CL,t : from − 15° to − 60° and ∆SD,t : from-2 knots to-10
k
∆C : from 15° to 60° and ∆S : from 2 knots to 10 knots
R,t

I,t

∆CL,t : from − 15° to − 60° and ∆SI,t : from 2 knots to 10 knots
∆SD,t : from − 2 knots to − 10 knots
∆SI,t : from 2 knots to 10 knots

The objective function guiding the search is formulated as:
𝑀𝑀𝑀𝑀𝑀𝑀 𝑧𝑧 = �∆𝐶𝐶𝑅𝑅,𝑡𝑡 − 𝐹𝐹𝑅𝑅,𝐿𝐿 ∆𝐶𝐶𝐿𝐿,𝑡𝑡 � + 𝐹𝐹𝐶𝐶,𝑆𝑆 �𝐹𝐹𝐷𝐷,𝐼𝐼 ∆𝑆𝑆𝐼𝐼,𝑡𝑡 − ∆𝑆𝑆𝐷𝐷,𝑡𝑡 �,

(1)

where FR,L is the preference factor of a right turn over a left turn; FD,I is the preference factor of
speed decrease over a speed increase; and FC,S is the preference factor of a course change over a speed
change. While minimizing the degree of change, the suggested maneuver strategies should avoid all
conflicts in a time interval shorter than CTmax defined as the maximum between the TCPA related to
the detected conflict and a fixed safety time factor which is an input parameter. This value determines
the safety interval from current time.
The choice of the objective function is in line with the IMO considerations (best practices in
section 1) and the maneuver strategies suggested comply whenever possible with Colregs according to
the strategy ranking shown in Table 3. In fact, function (1) leads to (a) the minimization of the
required change in course ∆CR,t (∆CL,t ) and speed ∆SD,t (∆SI,t ) at each discrete time step t, (b) the
avoidance of the earliest conflict detected, and (c) it ensures there are no conflicts in the time interval
CTmax.

Maneuver Strategy Generation Procedure
The algorithm first checks for strategies with higher rank (where the rank is defined with respect to
the objective function value, Table 3). SNAT starts searching for a course change to the right from
15° to 60° with a 1° increment at each iteration in case the maneuver is detected as infeasible (i.e., it
does not avoid conflict). If all Rank 1 maneuver strategies fail, SNAT proceeds to strategies with
lower rank. Since the preferred strategies will always be examined first, the first feasible maneuver
strategy found by SNAT is also the optimal strategy. In the worst case scenario, if all higher rank
strategies are not feasible, and the least favored strategy of increasing speed by 10 knots is the optimal
solution, the total number of iterations that need to be performed results as follows:
𝐼𝐼𝑤𝑤 = ∑8𝑖𝑖=1 𝐼𝐼 𝑟𝑟𝑟𝑟 = 46 + 46 + 46 × 9 + 46 × 9 + 46 × 9 + 46 × 9 + 9 + 9 = 1766,

where 𝐼𝐼𝑤𝑤 stands for number of iterations in the worst case scenario and 𝐼𝐼 𝑟𝑟𝑟𝑟 stands for the number of
iterations in rank i.
In situations where no feasible solutions can be found in the search space of 1766 solutions,
SNAT will perform the following analysis.
First, SNAT will check the TCPA related to the detected conflicts.
If TCPA is found to be greater than 20 minutes, maneuvers will be postponed to subsequent time
steps, i.e., when TCPA becomes smaller. The 20 minutes threshold was set because, from pilot
experiments, about 80% of conflicts detected when SNAT is not in use have TCPAs less than or equal
to 20 minutes. Hence, SNAT should avoid conflicts with TCPAs less than or equal to 20 minutes

1824

Chen, Pedrielli, and Ng
since they occur more often, while for conflicts with TCPAs greater than 20 minutes a delay in
maneuver is still acceptable.
When TCPA is shorter than 20 minutes and no feasible solutions can be found given the current
search space, the search space will be expanded to allow course changes between 15 and 70 degrees
and speed changes between 2 and 15 knots. The expanded search space will then consist of 3276
solutions:
𝐼𝐼𝐸𝐸 = ∑8𝑖𝑖=1 𝐼𝐼 𝑟𝑟𝑟𝑟𝑟𝑟 = 56 + 56 + 56 × 14 + 56 × 14 + 56 × 14 + 56 × 14 + 14 + 14 = 3276,

where 𝐼𝐼𝐸𝐸 is the number of iterations in the expanded search space, and 𝐼𝐼 𝑟𝑟𝑟𝑟𝑟𝑟 is the number of strategies
in the expanded rank i.

Maneuver Evaluation Procedure
In order to evaluate the feasibility of the maneuver, at each generated solution, N simulation
replications are used to estimate the probability of conflict. These simulations have a maximum length
of CTmax and the candidate solution is not generating conflicts with respect to the i-th replication if
the straight-line distance between own vessel and all target vessels is greater than the safety distance
for the next CTmax = 20 minutes. In order to provide safer maneuvers under stochastic scenarios, a
conservative approach was adopted while evaluating candidate strategies. In particular, the safety
distance used by the evaluation procedure is variable: the initial safety distance is set to be 5.5 times
the length of vessel (instead of 4 as stated in section 3.1). If the criterion is too stringent for any
feasible strategy to be found, lower safety factors of 5.0, 4.5 and 4.0 are used subsequently. To
decrease the required computational time, if a conflict is detected before CTmax , the simulation is
terminated and SNAT records a failure for that replication.
Once the replications have been performed, a maneuver strategy is declared feasible if it can
avoid conflicts (failures) with all target vessels in future time CTmax with some defined probability
(1 − 𝛼𝛼)% . The candidate strategy is then evaluated in terms of the % ratio between the number of
replications with zero conflicts over total number of replications (N). In case the effectiveness is
smaller than the value (1 − 𝛼𝛼)% , the proposed strategy is declared infeasible and the generation
procedure is restarted. In case, given the reduced safety time and expanded search space, no feasible
solution can be found, SNAT will not suggest any maneuver and it will give advice of the probability
of conflict (𝛽𝛽 ≥ (1 − 𝛼𝛼)%).

SNAT Evaluation Procedure
In order to evaluate its effectiveness, SNAT algorithm was integrated into a simulation–based
evaluation procedure as the one represented in Figure 3.

Figure 3: SNAT Evaluation Procedure.

1825

Chen, Pedrielli, and Ng
At every step of the evaluation procedure (Figure 3), the current simulation time (replacing the
real–time when AIS data are received), is referred to as T. At every time T, the own vessel position is
generated and potential conflicts are searched up to a future time (monitor time) of MTmax. The safety
distance used to detect conflicts is set to be 4 times the length of the own vessel (section 3.1). To
predict conflicts, the simulation advances on the monitoring time clock 𝑀𝑀𝑀𝑀 (while the simulation time
stays at T) for 𝑇𝑇 ≤ 𝑀𝑀𝑀𝑀 ≤ MTmax using a time step equal to mt ∗.
In case conflicts are detected, the SNAT procedure starts.
Once the maneuver has been generated, the simulation is used to update the position of the own
vessel accordingly. In particular, a change is made according to the solution proposed by SNAT only
if the ship captain decides to take immediate action, otherwise they remain unchanged. The update
procedure is different for target vessels whose parameters are updated according to probabilities and
distributions of random vessel behaviors reported in section 3.1 (Table 2).
The updated course and speed for own vessel C∗,t and S∗,t and for target vessels Cj,t and Sj,t (𝑗𝑗 =
1, … , 𝐽𝐽 ) are returned to the simulator to calculate updated longitude and latitude of vessels at
simulation time T, i.e., X ∗,T , Y∗T , X j,T , Yj,T , respectively. The same quantities X ∗,MT , Y∗,MT , X j,MT , Yj,MT
are computed at monitor time MT for future conflict detection.
At this point, time T is incremented with a fixed step t ∗ emulating the frequency at which AIS
data are received.
The simulation considers ship captains’ reaction to conflicts detected as well as their tendency to
take maneuvers under normal circumstances (i.e., when there are no conflicts) introducing further
randomness. In particular, in potential conflict situations, after the earliest conflict is detected and the
ship captain is about to implement the maneuver, the actual movement is affected by a random
variable. More specifically, when a conflict is detected, SNAT will check whether its TCPA reaches
the threshold of 10 minutes, if the TCPA is less than or equal to 10 min, SNAT assumes the ship
captain will accept maneuvers suggested and take immediate actions and will update the position
accordingly. However, if the threshold is not reached, SNAT will update the position according to the
scenarios in Table 1. In case no actions are taken by the captain SNAT will re-compute maneuver
strategies at the next time step. Also the random behaviors of target vessels in normal circumstances
were included according to identified probabilities and distributions presented in Table 2 in order to
generate the next vessel position.
4

NUMERICAL RESULTS

Experiments were conducted to test the effectiveness of SNAT in conflict avoidance under stochastic
scenarios. Section 4.1 reports the detailed parameter settings for the experimental conditions, while
the obtained results to evaluate the effectiveness of the approach are shown in section 4.2. In all
experiments, real AIS data received from the Singapore Strait from longitude 102° to 105° and
latitude 1° to 2° over 48 hours were used to set the initial vessels locations.
4.1

Parameters setting

The simulation duration was set to be 3 hours, i.e., the average time a vessel spends in regions of the
Singapore Strait controlled by Singapore Vessel Traffic Service (VTS). Part of the simulation control
parameters were determined from pilot experiments to trade-off between computational complexity
and efficiency. Different experimental conditions were obtained separating the AIS data stream into
16 sub-streams each of 3hrs duration. In fact, because of the average stay duration in the strait is 3hrs,
the subsequent streams were considered independent. The 16 data sets contain vessel information
such as name, length, position, speed and course which will be used to set initial vessel parameters at
the start of each simulation run. Each data set is used for 1 simulation experiment resulting in a total
of 16 experimental conditions each defined by a different initial condition (i.e., relative position of the
own vessel vs target vessels).
Concerning time increments 𝑡𝑡 ∗ and 𝑚𝑚𝑚𝑚 ∗ (section 3.2), a value of 30s was set to be the reference
line since it represents the average rate of data updates received from AIS receivers (Aarsather et al.
2009). However, preliminary tests on increments of 1min and 3min led us to set 𝑚𝑚𝑚𝑚 ∗ 𝑎𝑎𝑎𝑎𝑎𝑎 𝑡𝑡 ∗ = 1𝑚𝑚𝑚𝑚𝑚𝑚
1826

Chen, Pedrielli, and Ng
thus balancing the computational efficiency and the miss-out rate (i.e., the ratio between the number
of conflict not detected over the test runs and the total number of existing conflicts in the tests runs).
The maximum monitor time 𝑀𝑀𝑀𝑀𝑚𝑚𝑚𝑚𝑚𝑚 was set to be 30min since out of 186 conflicts detected over
16 test runs, when SNAT is not in place, 148 (i.e. about 80%) have TCPAs less than 25 minutes. Any
further increase in 𝑀𝑀𝑀𝑀𝑚𝑚𝑚𝑚𝑚𝑚 to provide early warning is a waste of computational effort since the ship
captain will always have to react to earlier conflicts first. The safety time 𝐶𝐶𝐶𝐶𝑚𝑚𝑚𝑚𝑚𝑚 was set to be either
greater than TCPA or 15min such that maneuver strategies suggested by SNAT will ensure no
conflicts are present at least in the next 15 minutes. Concerning the maneuver generation, 10
replications (N = 10) and a (1 − 𝛼𝛼)% =90% were used to evaluate each candidate maneuver strategy.
Concerning the pilot random behavior, the probability PM was set to the values in the scenario 3
illustrated in section 3.1 (Table 1).
4.2

Numerical results

The effectiveness of SNAT was tested against stochastic scenarios in which ship captains’
reaction to conflict(s) are subject to randomness as well as motion parameters (speed and course) of
target vessels are subject to random changes.
To test the effectiveness of SNAT under situations where ship captains may not take immediate
reactions once a conflict is detected, 16 experimental conditions were conducted and the set of vessel
parameters first captured by AIS receiver over a 3-hour period were used as initial input parameters
for the simulation. From simulation results, initialization bias is observed when, at the beginning of
simulation, the initial position of own vessel is too close to target vessels. In these conditions,
conflicts detected within the first 5 minutes of simulation were removed when assessing the
effectiveness of SNAT.
Dispersed

Moderate

Crowded

Figure 4: Average number of conflicts detected with and without SNAT.
The tests were repeated for 100 macro-replications and the average number of conflicts detected
using maneuver strategies suggested by SNAT was compared with average number of conflicts
detected if SNAT is not in place, as shown in Figure 4.
The 16 experimental conditions (Test Run in Figure 4) were grouped according to the initial
positions of vessels obtaining three macro cases: the “Dispersed” case in which own vessel is not
crowded by target vessels at the initial setting; the “Crowded” case in which own vessel is very
crowded by target vessels at the initial setting; and the “Moderate” case which consists of scenarios
between the two extremes.
It can be observed that, the performance of SNAT is consistent in all 16 conditions, i.e., the
number of conflicts is strictly increasing as the traffic intensity increases. The same pattern cannot be
recognized in the case where SNAT is not in place.

1827

Chen, Pedrielli, and Ng
To quantify the overall effectiveness of SNAT, we computed the expected number of conflicts
1

∑100 𝐶𝐶

𝑖𝑖=1 𝑖𝑖𝑖𝑖
over 3-hour duration as ∑16
%, where 𝐶𝐶𝑗𝑗 is the number of conflicts detected under the jth
16 𝑗𝑗=1 100
experimental condition under the i-th macro-replication. We compared this value with the number of
conflicts when SNAT is not in place and computed the % improvement associated to the application
of SNAT, as the the % decrease in the number of conflicts which is shown in Table 4.
We notice that the percentage reduction in the number of conflicts reaches its maximum in test
Run 2 under “Dispersed” scenario, of 100.00%, whereas its minimum is in Test Run 14 under
“Crowded” scenario of 76.47%.

Figure 5: Effect of the initial conditions.
Table 4 : % reduction in number of conflicts in different scenarios.
Scenario

% Reduction in Conflicts

Dispersed

100.00

Moderate

100.00

Crowded

94.68

We first highlight the evidence that SNAT is never worse than the without SNAT situation.
Further analysing the obtained results we observed that SNAT is sensitive to the initial conditions.
Figure 5 shows the initial positions of own vessel (cross mark) and target vessel (diamond marks) for
condition 2 and condition 14, as well as the respective performance of SNAT.
As it can be observed, many conflicts cannot be avoided in test run 14. This, however, is due to
the fact that the vessels is starting off in an overcrowded region when the SNAT procedure is
activated, making conflicts unavoidable (refer to the top right panel in Figure 5).
5

CONCLUSIONS

Navigation safety is becoming a major concern for the maritime authorities. Current approaches are
not sufficient in guaranteeing conflicts avoidance as they do not consider randomness. Moreover, the
available regulations to control the ship captain’s maneuver proposed by the maritime authorities are
usually neglected. The method proposed in this research tries to fill this gap. SNAT is a stochastic
simulation–based search algorithm which provides robust optimal maneuver strategies to minimize
conflict situations. Numerical results show that the effectiveness of the proposed solution varies based
1828

Chen, Pedrielli, and Ng
on the number of vessels involved in the encounter situation and their initial relative positions.
However, the effectiveness of SNAT is proven remarkable under all the considered conditions.
Several extensions are under investigation. Within the scope of this study, the maneuvers taken by
ship captains are assumed to take immediate effect. However, in real settings, the effective reaction
time of vessels may depend on several factors such as the type of vessel, its length, weight and speed
as well as human reaction time of ship captains.
Further improvements can be made by updating vessel information based on real-time AIS data.
Trace-driven simulation can then be adopted to further improve effectiveness under stochastic
scenarios by capturing real–time motion parameters of target vessels and develop optimal maneuver
strategies based on real-time data instead of modelling the random behavior as a stochastic variable.
Moreover, especially in light of the possible integration of SNAT with the guidance support
devices on board, the computational efficiency becomes a key issue. The need of running simulations
to evaluate the robustness leads to the requirement of reducing the evaluation time. Furthermore, the
computational effort required by the actual search procedure has to be remarkably reduced.
Metamodels replacing the simulation as well as more efficient search algorithms are under
investigation.
ACKNOWLEDGMENTS
The authors would like to acknowledge Mr. Kister Thomas from the School of Computing at National
University of Singapore for providing the Automatic Identification System (AIS) data which form the
basis of the study object of this paper.
This work was partially supported by the NOL (Neptune Orient Lines) Fellowship Programme in
Singapore through the grant R–702–000–010–720.
REFERENCES
Aarsather, K.G., and T. Moan. 2009. “Estimating navigation patterns from AIS.” The Journal of
Navigation 62: 587–607. Cambridge: Cambridge Journals.
Goodwin, E.M. 1975. “A statistical study of ship domains.” The Journal of Navigation 28: 329–341.
Cambridge: Cambridge Journals.
Goldwell, T.G. 1982. “Marine traffic behaviour in restricted waters.” The Journal of Navigation 36:
431–444. Cambridge: Cambridge Journals.
Lisowski, J. 2007. “The dynamic game models of safe navigation.” International Journal on Marine
Navigation and Safety of Sea Transportation 1.1: 12–18.
MPA. 2013. Maritime and Port Authority of Singapore. Accessed March 16.
http://www.mpa.gov.sg/sites/maritime_singapore/what_is_maritime_singapore/other_facts_you_
may_not_know.page.
Michalewicz, Z., and R. Smierzchalski. 2000. “Modelling of a ship trajectory in collision situations at
sea by evolutionary algorithm.” IEEE Transactions on Evolutionary Computation 3.4: 227–241.
Szlapczynski, R. 2006. “A unified measure of collision risk derived from the concept of a ship
domain.” The Journal of Navigation 59: 477–490. Cambridge: Cambridge Journals.
Szlapczynski, R. 2011. “Evolutionary sets of safe ship trajectories: A New Approach to Collision
Avoidance.” The Journal of Navigation 64: 169–181. Cambridge: Cambridge Journals.
The Straits Times. 2014. 3rd bunker spill in 13 Days after another vessel collision. Accessed March 16.
http://shipandbunker.com/news/apac/937706-singapore-3rd-bunker-spill-in-13-days-afteranother-vessel-collision.
Wang, N., X. Meng, Q. Xu, and Z. Wang. 2009. “A unified analytical framework for ship domains.”
Journal of Navigation 62.4: 643–655.
Weng, J., Q. Meng, and X. Qu. 2012. “Vessel collision frequency estimation in the Singapore Strait.”
Journal of Navigation 65.2: 207–221.

1829

Chen, Pedrielli, and Ng
AUTHOR BIOGRAPHIES
XINGYI CHEN is currently an undergraduate in Industrial and System Engineering at the University
of Singapore. As an undergraduate, she did research on the use of simulation for safety in navigation.
She is currently working on the topic of multi-disciplinary optimization. Her email address is
a0069870@nus.edu.sg.
GIULIA PEDRIELLI is Research Fellow for the Centre for Maritime Studies at the National
University of Singapore. She received her M.S and Ph.D. in Mechanical Engineering from Politecnico
di Milano. Her research focuses on simulation-optimization based on math programming and budget
allocation techniques applied to maritime systems. Her email address is cmsgp@nus.edu.sg.
SZU HUI NG is an Associate Professor in the Department of Industrial and Systems Engineering at
the National University of Singapore. She holds B.S., M.S., and Ph.D. degrees in Industrial and
Operations Engineering from the University of Michigan. Her research interests include computer
simulation modeling and analysis, design of experiments, and quality and reliability engineering. She
is a member of IEEE and INFORMS and a senior member of IIE. Her email address is
isensh@nus.edu.sg.

1830

Proceedings of the 2012 Winter Simulation Conference
C. Laroque, J. Himmelspach, R. Pasupathy, O. Rose, and A. M. Uhrmacher, eds.

Time Buffer for Approximate Optimization of Production Systems: Concept, Applications and
Structural Results
Giulia Pedrielli
Politecnico di Milano
Via Giuseppe La Masa 1
20156 Milano, ITALY

ABSTRACT
Simulation Optimization is acquiring always more interest within the simulation community. In this
ﬁeld, Mathematical Programming Representation (MPR) has been applied for both simulation and sample
path-based optimization of production systems performance. Although in the traditional literature these
systems have been represented by means of Integer Programming (IP) models, recently, approximate Linear
Programming (LP) models have been proposed to optimize and evaluate the performance of a category of
production systems. This work deals with LP models developed based on the Time Buffer (TB) variable
whose concept, applicability and structural properties will be presented. Moreover the models convergence,
within the Sample Average Approximation (SAA) framework, will be characterized.
1

Contributions

MPR is deeply different from traditional simulation optimization techniques (Fu, Glover, and April 2005)
since more information can be obtained from a single simulation (optimization) run (Chan and Schruben
2008).
TB-based LP models are here applied to approximately solve manufacturing optimization problems
(Buzacott and Shantikumar 1993). These models, iteratively solved using the SAA approach, result in the
optimal TB conﬁguration and the related performance estimates (Sect. 3). These two outputs form the
basis to derive the integer solution the TB is approximating. Indeed, the TB solution in the continuous
domain is strongly related to a unique solution in the discrete domain.
Two applications of the TB have been developed so far: (1) approximation of the buffer capacity in an
open ﬂow line, (2) approximation of the number of pallets in a loop line. The optimization problems were
the Buffer Allocation Problem (BAP) and the Pallet Sizing Problem (PSP) respectively. Although a model
for the BAP was already proposed in (Alﬁeri and Matta 2012), for this case, the second order properties
of the approximate model variables, the SAA solution approach and the convergence properties constitute
a result of this work (Sect. 4).
Formulations are not reported for space limitations, hence refer to (Alﬁeri and Matta 2012).
2

Time Buffer for Simulation and Optimization

The TB is a continuous variable deﬁned in the approximate model to replace the discrete variable deﬁned
in the original IP model. In general, to approximate a discrete variable with a TB, it must be possible to
formally describe its effects on the events characterizing the system dynamics. This holds when the system
dynamics can be formulated as a set of max-plus type equations, (Buzacott and Shantikumar 1993).
In the BAP case, for example, the space buffer capacity continuous counterpart has to be modelled. The
capacity effect is to delay (anticipate) the time a customer enters the workstation upstream the buffer (start
event). Hence, the start event time represents the continuous variable to trigger by means of the TB that
978-1-4673-4780-8/12/$31.00 ©2012 IEEE

Pedrielli
will, directly, delay or anticipate this event as the space buffer, indirectly, does. Moreover, for the open line
case, referring to the work of Shanthikumar et al., (Shanthikumar and Yao 1991), it was proved that: (1)
the TB is increasing convex in the processing times if the processing times are convex in the parameters
characterizing their distribution, (2) the completion time is decreasing convex in the TB and increasing
convex in the processing times if these are increasing and convex in the parameters characterizing their
distribution.
3

Simulation Optimization Algorithm

The TB simulation optimization models are iteratively solved to ﬁnd the optimal TB conﬁguration and its
discrete counterpart, following the steps described below.
1. Initialization: Iteration k = 0. (1) set the parameters describing the system and the simulation
optimization conﬁguration parameters (e.g., the number of machines, the run length), (2) set the
target average completion time, (3) generate the sample path of processing and arrival times.
2. System Conﬁguration Generation: Solve the LP approximate optimization model, obtaining the
samplepath-optimal TB. Go to Step 3.
3. System Performance Evaluation: Feed the approximate simulation model with the initialization
data and the TB’s obtained from the previous step. Solve the approximate simulation model. If
k > 0 and the stopping condition is met, derive the sp-approximate integer solution and exit the
procedure. Otherwise increase the sample path size and go to Step 2.
4

Convergence Properties

Let ε-SBAP deﬁne the ﬁnite Sample path approximate BAP problem to be solved for the ε-optimal solution
in terms of TB, i.e. the solution characterized by a completion time higher than the target of maximum ε,
with ε going to 0 as the size of the sample path n → ∞, P-almost surely.
For this problem, the asymptotic convergence was characterized for: (1) the feasible region, (2) the set
of minimizers (in terms of TB solutions), (3) the value of the objective function. Based on the properties
of the LP models, duality theory and epi-convergence theory, the asymptotic convergence for all the cases
was proved.
Moreover, adopting the Large Deviation Theory ((Dembo and Zeitouni 2009), (Shapiro 1996)), it was
proved that, in case the central limit theorem holds for the objective function and the constraints, the
probability of obtaining a sample path solution which is not the optimal for the inﬁnite sample path problem
goes to 0 with exponential rate.
REFERENCES
Alﬁeri, A., and A. Matta. 2012. “Mathematical Programming Formulations for Approximate Simulation
of Multistage Production Systems”. European Journal of Operational Research 219 (3): 773–783.
Buzacott, J., and J. Shantikumar. 1993. Stochastic Models of Manufacturing Systems. Prentice–Hall.
Chan, W., and L. Schruben. 2008. “Optimization models of Discrete–Event System Dynamics”. Operations
Research 56 (5): 1218–1237.
Dembo, A., and O. Zeitouni. 2009. Large Deviations Techniques and Applications. Stochastic Modelling
and Applied Probability. Springer.
Fu, M., F. Glover, and J. April. 2005. “Simulation optimization: a review, new developments, and
applications”. 83–95.
Shanthikumar, J. G., and D. D. Yao. 1991. “Strong Stochastic Convexity: Closure Properties and Applications”. Journal of Applied Probability 28 (1): pp. 131–145.
Shapiro, A. 1996. “Simulation Based Optimization–Convergence Analysis and Statistical Inference”.
Stochastic Models 12 (3): 425–454.

Proceedings of the 2015 Winter Simulation Conference
L. Yilmaz, W. K. V. Chan, I. Moon, T. M. K. Roeder, C. Macal, and M. D. Rossetti, eds.

DISCRETE EVENT OPTIMIZATION: SINGLE–RUN INTEGRATED
SIMULATION–OPTIMIZATION USING MATHEMATICAL PROGRAMMING
Giulia Pedrielli

Andrea Matta

Centre for Maritime Studies
National University of Singapore
15 Prince George’s Park
Singapore, SG 118414, SINGAPORE

Department of Industrial Engineering and Management
Shanghai Jiao Tong University
800 Dong Chuan Road
Shanghai, 200240, CHINA

Arianna Alfieri
Department of Management and Production Engineering
Politecnico di Torino
Corso Duca degli Abruzzi 24
10129 Torino, ITALY

ABSTRACT
Optimization of discrete event systems conventionally uses simulation as a black–box oracle to estimate
performance at design points generated by a separate optimization algorithm. This decoupled approach fails
to exploit an important advantage: simulation codes are white-boxes, at least to their creators. In fact, the
full integration of the simulation model and the optimization algorithm is possible in many situations. In
this contribution, a framework previously proposed by the authors, based on the mathematical programming
methodology, is presented under a wider perspective. We show how to derive mathematical models for
solving optimization problems while simultaneously considering the dynamics of the system to be optimized.
Concerning the solution methodology, we refer back to retrospective optimization (RO) and sample path
optimization (SPO) settings. Advantages and drawbacks deriving from the use of mathematical programming
as work models within the RO (SPO) framework will be analyzed and its convergence properties will be
discussed.
1

INTRODUCTION

Mathematical programming representations can be used to describe the dynamics of discrete event systems
(DESs) using a set of equations and an objective function (Schruben 2000, Chan and Schruben 2008a),
which drives the model to execute all the events as soon as possible and the optimal solution of such
mathematical programming models represents the simulated system dynamics.
In case the DES becomes too complex, the mathematical programming approach might become impractical
as no equation can describe certain behaviors or, in case they exist, the resulting mathematical model is
computationally intractable (e.g., complex flows, presence of real time dispatching rules). Nevertheless,
for many systems of interest, linear programming models are sufficient to describe the system dynamics
(Chan and Schruben 2008a, Chan and Schruben 2008b, Alfieri and Matta 2012b).
The possibility to have an analytical description of system dynamics makes mathematical programming
representation an appealing technique for analyzing in a formal way the simulated systems. Simulation
output is generated by means of equations rather than a set of logical rules embedded in a computer code.

978-1-4673-9743-8/15/$31.00 ©2015 IEEE

3557

Pedrielli, Matta, and Alfieri
This enriches the types of analysis that can be performed on the simulation output (Pedrielli 2013, Matta
et al. 2014, Chan and Schruben 2008a).
In addition to the aforementioned aspects, the use of mathematical programming for simulation allows
to integrate simulation and optimization since the system dynamics constraints can be integrated in a
mathematical programming model developed for optimization purposes. However, such integrated models
are no longer LPs (as they usually are for simulation) and Mixed Integer Linear Programming (MILP) have
to be considered with the related computational burden. Approximate LP models to simulate and optimize
DESs can be developed to overcome this difficulty. Alfieri and Matta (2012a) propose an approximation
consisting in modeling queues as time buffers (TB), i.e., temporal lags between two events, instead of the
traditional space buffers. This approximation preserves the model linearity even when used for optimization
and it allows to avoid the presence of two decoupled modules iteratively interacting with each other.
The output of the simulation module is the input for the optimization in an iterative procedure which
continues until the optimal solution is found or a predefined stopping condition is satisfied (Spall 2003). For
the optimization module, a vast choice of methods is available (Fu et al. 2005); the most common ones are
response surface methodology, stochastic approximation, random search, sample path optimization (SPO)
and its iterative version, retrospective optimization (RO).
In case mathematical programming models are used for simulation–optimization, then SPO and RO
are commonly adopted (Healy and Schruben 1991, Plambeck et al. 1996, Robinson 1996). Examples of
implementation of the integrated simulation–optimization framework with SPO as solution methodology
can be found in (Alfieri and Matta 2012a) that solve the buffer allocation problem in open tandem queuing
systems; Alfieri et al. (2013) approximately select the maximum number of jobs in a closed tandem queuing
systems; in (Weiss and Stolletz 2015) the discrete buffer allocation problem is solved by means of a tailored
Benders’ decomposition technique. In a slightly different framework from the one proposed by the authors,
Kolb and Gttlich (2015) solve the continuous buffer space allocation problem modeling a hybrid system,
discrete in the machine states and continuous in the material flow. The same concept is exploited in (Tan
2015) for the optimal production flow rate control problem for a continuous material flow system with an
unreliable station and deterministic demand.
In general, the motivation for proposing mathematical programming to integrate simulation and optimization
stands in that the integration of the system dynamics with the optimization represents a fundamental step
towards the realization of efficient algorithms which can rely on the information coming from a white–box
model (refer to section 3). Also the ability to model a simulation–optimization problem as a mathematical
programming model, within the Sample Path Optimization framework, enables the convergence study.
In addition, the fact that simulation and optimization both work on the same sample path might lead to
reduction in the required effort in terms of number of observations required to solve the problem (e.g.,
the number of jobs to simulate, the number of patients visiting hospital facilites). However, to make this
statement applicable to the computational effort, more efficient algorithms need to be developed.
The recent contributions have several aspects in common. Similarly to (Plambeck et al. 1996), a
single, fairly long, run is considered and mathematical programming techniques are used to solve sample–
path integrated simulation–optimization problems. Several challenges are still to be tackled as well as
opportunities of improvement can be reached. Convergence of the methodology is addressed only in
the pioneer work of Robinson (1996), but no constraints are considered in that case and mathematical
programming is not explicitly considered as simulation technique. In fact, Robinson (1996) does not refer to
integrated simulation–optimization models and an external oracle producing consistent output is assumed to
be available. Nevertheless, this new framework inherits issues from Robinson (1996) as the demonstration
of the advantage of static sampling versus iterative sampling (Healy and Schruben 1991).
In light of the aforementioned aspects, this paper presents an overview of mathematical programming for
integrated simulation–optimization by presenting the approach, the main results from the existing literature
and describing the main challenges/opportunities. Furthermore, a convergence proof scheme is provided
for the integrated framework proposed by the authors.

3558

Pedrielli, Matta, and Alfieri
2

DEO: INTEGRATED SIMULATION–OPTIMIZATION

Discrete Event Optimization (DEO) refers to a framework for the integrated simulation–optimization of
DES. At this stage of development of the framework, we consider systems such as supply chains or
manufacturing systems. In this context, the topology of the DES we consider can be represented by a
queueing network with the set of servers J = {0, . . . , J + 1} and the set of possible transaction routes for
job i (i ∈ N = {0, . . . , n}) between servers, represented by Qi = {( j, j′ )| j, j′ ∈ J} , ∀i. For each pair ( j, j′ ),
the arc connecting j and j′ belongs to Qi if and only if job i can directly flow from node j to node j′ . The
source node, represented by index j = 0, is the server j having no predecessor. The sink node is, instead,
the server j having no successor and it is indexed by J + 1. The source node represents an infinite external
arrival stream of customers, whereas the sink node is the output gate through which jobs are released from
the network. We consider a general setting in which no explicit condition has to be imposed over the
system layout (i.e., output can be linear, as well as merge and split points or cycles can be modeled).
ξ
ξ
Let Ei j and ei j denote the events occurring in the system and the related occurrence times, respectively,
where ξ ∈ T is the event type (e.g., T = {a, s, f } as arrival, start of process or departure of a job from a
server, respectively), and the pair (i, j) indicates the job i and the server j the event refers to. We assume
that job i at server j undergoes a process activity with duration bounded by a start event Eisj occurring at
time esij and a completion event Eifj occurring at time eifj ; the duration of the process is ti j and, in case of
stochastic DES, {ti j } may follow some known statistical distributions.
i
n More generally, the flow of oeach job i is determined by the occurrence of a set of events W =
ξ
ξ
Iξ
Ei j , ξ ∈ T, ( j, j′ ) or ( j′ , j) ∈ Qi , i ∈ N. Each event Ei j in the set Wi has a set Wi j of the input events,
Oξ

i.e., triggering events, and a set Wi j of the output events, i.e., triggered events. Notice that elements in
Iξ

Oξ

the sets Wi j and Wi j might not be in the set Wi . An example of this is when the triggering (triggered)
′
′
event is related to a job i 6= i, i ∈ N. According to Matta et al. (2014), we provide a set of definitions
which will be useful in the subsequent explanations.
Definition 1 (Event Relationship Graph Lite, ERGL) An ERGL is an oriented weighted graph where the set
ξ
ξ
of nodes W = {Ei j , i ∈ N, j ∈ J, ξ ∈ T} represents the set of events Ei j occurring in the system. Each node
ξ

is assigned a value equal to the time ei j when the associated event occurs. Directed arcs connect different
o


n

ξ
ξ′
ξ
ξ′
event pairs Ei j , Ei′ j′ and the set of arcs E =
Ei j , Ei′ j′ , i, i′ ∈ N, j, j′ ∈ J, ξ , ξ ′ ∈ T represents the
ξ

Ei j

precedence relationships between events. Each arc can be assigned a weight w

ξ′

Ei′ j′

that can be continuous
ξ ,i, j

(positive or negative) or binary. In order to simplify the notation, we refer to weights as wξ ′ ,i′ , j′ .
ξ

ξ′

ξ

ξ′

Definition 2 (Connected Events) Let ei j and ei′ j′ be the times when the events Ei j and Ei′ j′ occur, respectively.



 ′ 
ξ
Iξ
Oξ ′
Oξ V ξ
Iξ ′
These two events are connected if and only if Ei′ j′ ∈ Wi j ∪ Wi j
Ei j ∈ Wi′ j′ ∪ Wi′ j′
. In particular,
 ′

′
′
V
Oξ
ξ
Iξ
ξ
ξ
ξ
if Ei′ j′ ∈ Wi j Ei j ∈ Wi′ j′ , the connection establishes that event Ei′ j′ can trigger event Ei j . Similarly,


ξ
Oξ
ξ
ξ′
Iξ ′ V ξ ′
if Ei j ∈ Wi′ j′ Ei′ j′ ∈ Wi j , event Ei j can trigger event Ei′ j′ .
Definition 3 (Controlled ERGL) Given a set WN ⊆ W of natural events (events determined by physical
constraints, e.g., a job cannot enter the system before its arrival times, and it cannot be processed by two
servers at the same time), a controlled ERGL is the ordered set of events WCN containing all the elements
in WN and the set WC ⊆ W \ WN of control events. Elements in WCN are connected through natural arcs
(EN ⊆ E) and directed control arcs (EC ⊆ E). The weight associated to each control arc can be either

3559

Pedrielli, Matta, and Alfieri
ξij

ξij

continuous, denoted as sξ ′ i′ j′ and referred to as time buffer, or binary weight, with κξ ′ i′ j′ indicating the
associated binary value.
The values associated to the nodes of the resulting graph and the control weights can be translated into
decision variables in an optimization problem, while natural weights, are fixed input parameters. Under
this new perspective,
n o the optimization of a system corresponds to the search of the best set of control
n o
ξ
ξ
C
events W = Ei j , and the set of arcs (and related weights) such that the resulting occurrence times ei j
satisfy some target performance. Using the mathematical programming formalism, we can formulate the
optimization problem as (Matta et al. 2014):
min

∑

αυ eυ +

υ∈W

∑

∑

p (eν ) ≤
ν∈WC
eifj ≥ esij + ti j
ξ′
ei′ j′

(βν sν + γν κν ) + ϑ ε

(1)

ν∈EC
∗

µ +ε

(2)

∀Eisj , Eifj ∈ WN


ξ
ξij
ξ
ξ′
≥ e i j − q wξ ′ i ′ j ′
∀Ei j , Ei′ j′ ∈ WCN

(3)
(4)

Equation (1) is the objective function, having as decision variables the event times eυ , and the control
parameters (weights in the ERG graph). Function (1) can consider a single or multiple objectives depending
on the values of αυ , βν and γν that are known function coefficients. The term ϑ ε serves the purpose to
penalize finite sample path solutions that do not meet the desired performance (i.e., violate the constraint (2)
if the decision variable ε is not considered). This penalization approach has impact on the implementation
of the algorithm but not on the asymptotic properties. Equation (2) is the performance constraints, where
µ ∗ is the target performance and p is any function of the control event times. The natural dynamics linking
event times if no control is added are represented by constraints (3), stating that customer i cannot leave stage
j (eifj ) before accessing the server (esij ) and completing the service (ti j ). Parameters ti j form the collection
of realizations of random variables characterizing the queueing process (e.g., arrival times, processing
times). These values translate the weights between the nodes Eisj (starting event) and Eifj (departure event).
According to the definitions in the previous section, we assume to know the probabilistic characterization
of the input stochastic processes. Hence, we can generate ti j as realizations of known random variables.
ξ
ξ′
Constraints (4) refer to control constraints. Variables ei, j and ei′ j′ represent the time occurrence of two
ξij

events relating job i′ on stage j′ and job i on stage j that are linked by control q(wξ ′ i′ j′ ). If the relationship
ξij

ξij

between the two event times is boolean, function q has the form (1 − κξ ′ i′ j′ ) · M, where κξ ′ i′ j′ is a binary
decision variable and M is a large number. Instead, in case of continuous relationship, q is a function of
ξij
the continuous variable sξ ′ i′ j′ time buffer.
The main feature of DEO models is that they are based on events rather than on states, which generally
grow faster than events. Notice that when βν = 0 and γν = 0 ∀ν in equation (1) and the performance
constraint is not present, the model becomes a simulation model while when αυ = 0 ∀υ is null, we have
an optimization model.
Notice that, since i = 1, . . . , n, the model is a function of n. Hence, it is apparent that, as the simulation
length increases (i.e., n increaes), also the number of decision variables and constraints increases.
3

SOLUTION APPROACH

The DEO models in (Weiss and Stolletz 2015, Kolb and Gttlich 2015, Tan 2015) are solved in the framework
of SPO (Robinson 1996). In SPO, a single sample path is generated and the problem is solved using
techniques from deterministic optimization. The length of the sample size impacts on the quality of the
obtained results.
3560

Pedrielli, Matta, and Alfieri
SPO can be interpreted as a single–iteration RO, i.e., as a single large deterministic problem solved
to optimality (Healy and Schruben 1991). Specifically, RO is an iterative technique requiring the solution
of a sequence of sample-path problems of increasing size (Jin and Schmeiser 2003). At each step l, a
sample-path problem Pl is solved using the information obtained from the previous iterations. The solution
at iteration l has an error tolerance εl from the optimum and, combined with the solutions from previous
iterations, it is used to compute the new retrospective candidate. The algorithm terminates when a stopping
condition is met, otherwise the sample size is increased and the error tolerance decreased. Large error
tolerance values at the beginning of the procedure allow for rough but fast solutions that are then used in
the subsequent iterations as warm start solutions.
The use of mathematical programming for solving sample–path problems affects the implementation
of RO. Nevertheless, the retrospective one could be an interesting framework to investigate to seek more
efficient algorithms for the solution of DEO models. Indeed, the complexity of DEO models increases
ξ
with the size of the sample path due to the events times decision variables (i.e., {ei j }). As a result, the
computational time for solving long sample-path might be too large because of the model size. Under this
perspective, a warm start, as in the RO approach, could speed up the solution algorithm: the algorithm
might be iteratively stopped before the optimal solution is reached and the suboptimal solution might be
used as starting point for the next iteration executed with an increased sample path. Using mathematical
programming techniques could help to control the tolerance error in solving the deterministic sample-path
problem.
Independently from the iterative aspect, the solution of the deterministic problem strongly impacts
on the efficiency, in terms of computation time, of the algorithm. In (Alfieri and Matta 2012a, Alfieri
et al. 2013) an LP model is solved by state–of–the–art algorithms such as the simplex-based methods or
interior points methods. Nevertheless, the LP structure could still be exploited to develop more efficient
solution algorithms. For example, gradient–based search methods (Spall 2003, Edelkamp and Schroed
2011) or column–row generation procedures (Muter et al. 2012) could be developed and tailored for the
DEO framework. The two approaches could also be considered together in a unique solution framework
in which the reduced LP of the column–row generation algorithm is solved by gradient–based methods.
4

ASYMPTOTIC PROPERTIES

Herein, we present the asymptotic analysis of our proposed integrated simulation–optimization algorithm.
The basic idea to prove the convergence is to consider our algorithm as a case of Sample Path Optimization and
use the results in (Robinson 1996) that provides the asymptotic characterization of sample path optimization
algorithms. In order to do so, the proposed analysis is made of thee main parts: 1) analysis of the second
order properties in the context of simulation and optimization; 2) analysis of the feasible region asymptotic
behavior; 3) application of the results in (Robinson 1996) to the integrated simulation–optimization model.
The second order properties of the considered optimization models and related simulation models guarantee
the regularity conditions at the basis for constraint classification and existence results. The second part of
the analysis is required since Robinson (1996) does not consider stochastically constrained problems. Once
part 1) and 2) are characterized, we can apply the main results in (Robinson 1996) and prove convergence
in our setting.
4.1 Second Order Properties
In this section, we will refer to the time buffer approximate models presented in section 2, i.e., we will
assume γν = 0 ∀ν ∈ EC . In particular, we will analyze optimization models (αυ = 0, ∀υ) and will assume the
function of the involved time buffers is simply the sum of the time buffer components, i.e., βν = 1 ∀ν ∈ EC .
As it will be shown in section 5, this type of objective function reflects several applications.
In this study, we exploit the mathematical programming framework and the concepts presented in Yao
and Shanthikumar (1991). In particular, SIL(sp), SICX(sp), SICV(sp), SDL(sp), SDCX(sp), and SDCV(sp)

3561

Pedrielli, Matta, and Alfieri
represent monotone convexity and concavity notions. They refer to stochastic increasing and linear,
stochastic increasing and convex, stochastic increasing and concave, stochastic decreasing linear, stochastic
decreasing and convex, stochastic decreasing and concave, in the sample path (sp) sense, respectively (Yao
and Shanthikumar (1991)).
We indicate with Fn ⊂ Xn × Rn+ × R+ the feasible region for the approximate optimization problem
(for a finite sample path of size n), where Xn is the domain for the time buffer sν , Rn+ is the domain for
the finishing times and R+ the domain for the ε. Since the results will basically focus on the time buffer
sν rather than the event times eν , it is useful to define the projection of the feasible region Fn onto the time
buffer space Xn . We will refer to this set representing the sample path feasible time-buffer configurations
as Σn . Moreover, to simplify the notation and focus on the sample size n, we will refer to the time buffers
as sn , dropping the ν subscript.
The primal (on the left) and the dual (on the right) approximate optimization models, in their matrix
forms, are the following:
′

min Sn = 1 sn + ϑ · ε
s.t.
AD eν ≥ b1 (τ, υ)
P

max b1 (τ, υ)uD + b2 (τ, υ)uP − µ ∗ · ν
′

2

A [eν |sn ] ≥ b (τ, υ)
ε − ∑ p (eν ) ≥ −µ ∗

(5)

A D uD ≤ 0

(6)

′
AP u

(7)

≤1
ν ≤ϑ
P

ν∈WC

eν ≥ 0, ε ≥ 0, sn ∈ Xn
′

u ≥ 0, ν ≥ 0

′

The vector v = [eν |sn ] , where [·|·]′ is the row vector obtained by the concatenation of two column vectors,
is the set of decision variables of the primal model, i.e., the time buffer s and the event times e, while
′
u = [uD |uP ] and ν represent
 the dual variables.
The matrix A = AD |AP is an l × m dimension matrix, where l represents the number of constraints
not including the performance constraint(s) (2) and m the number of decision variables. According to the
definitions provided in section 2, constraints (6) and (7)
 are the same as (3) and (4), respectively.
The m-dimensional vector of the right hand side b = b1 |b2 = {b1 , b2 , . . . , bm } consists of the realization
of random variables, in compact form, Ai (i = 1, . . . , n) and Bi j (i = 1, . . . , n j = 1, . . . , J). These random
variables are assumed to follow univariate distributions (following the approach in Shaked and Shanthikumar
(2007)) Ai ∼ V A (υ) and Bi j ∼ V B (τ), respectively. The link between the realizations and the parameters
of the sampling distribution is made explicit through the notation b (τ, υ). In section 5, some examples
will show that these stochastic variables can model arrival times as well as processing times of jobs in a
production system. Nevertheless, several processes can apply to the same definition.
The objective of the primal problem, Sn (sn (b(τ, υ), µ ∗ ), ε(b(τ, υ), µ ∗ )), is a function of the time buffer
sn , which is itself a function of the right hand side b.
The same modeling approach can be applied for the simulation model.
′

min χ = 1 eν

′

max q(τ, υ) u

s.t.
Aeν ≤ −q(τ, υ)
eν ≥ 0

′

A [u] ≤ 1
u≥0

A is the same matrix as in the approximate optimization model while vector q(τ, υ) is the concatenation of
vectors t and sn , i.e., q(τ,υ) =
	 [−t(τ, υ)|sn (τ, υ)] . The notation q (τ, υ) has exactly the same interpretation
of b (τ, υ). Event times ei j are the variables of the primal model, while u represents the vector of dual
variables. We will refer to the primal objective function as χ(sn , t). Furthermore, when interested in
studying the behavior of function χ with respect to only sn or t we will use χ(sn , ·) (sn fixed, t variable)
3562

Pedrielli, Matta, and Alfieri
and χ(·, t) (sn variable, t fixed).
The formulation just presented allows to explicitly indicate the set of control constraints (equations (4))
having the time buffers as right hand side and the natural dynamics constraints (equations (3)) not containing
it. Again, the dependency of both t and sn on the parameters τ, υ is made explicit through the notation
t(τ, υ) and sn (τ, υ).
In the following, we will consider only random variable Bi j , i.e., Ai = 0, ∀i. This will simplify the
notation in t(τ), sn (τ), b(τ) and q(τ). Moreover, the parameter of interest τ will be defined in a convex
set T (e.g., an interval real line).
In order to proceed with the analysis, we need to make some assumptions on the form of the performance
function p (eν ) and on the system meeting the target performance as n → ∞.
Assumption 1 p (eν ) is a convex function of eν and we estimate E (p (eν )) through a sample average of
the realizations of the function values in the sample path.
Assumption 2 The system under analysis is stationary and the target performance µ ∗ is such that µ ∗ ≥ µmin
being µmin the best performance that can be reached by the system in a steady state.
Property 1 (Second order properties for Sn ) If {b(τ)} ∈ SICX(sp), then {Sn∗ (b)} ∈ SICX(sp).
Proof.
The proof simply relies on the strong duality that holds in the case of LP models as approximate
time buffer models are. According to strong duality, we can prove that the function is increasing and
convex, namely (increasing):
′

′

′

Sn∗ (b1 ) = b1 u(1)∗ − µ ∗ ν (1)∗ ≤ b2 u(1)∗ − µ ∗ ν (1)∗ ≤ b2 u(2)∗ − µ ∗ ν (2)∗ = Sn∗ (b2 ),
whereas, for convexity:
′

′

Sn∗ (β b1 + (1 − β )b2 ) = (β b1 + (1 − β )b2 )u(1)∗ − µ ∗ ν (1)∗
′

′

= β b1 u(1)∗ + (1 − β )b2 u(1)∗ − µ ∗ ν (1)∗
 ′

 ′

≤ β b1 u(2)∗ − µ ∗ ν (2)∗ + (1 − β ) b2 u(3)∗ − µ ∗ ν (3)∗
= β Sn∗ (b1 ) + (1 − β )Sn∗ (b2 ).
The SICX property is a result from Shaked and Shanthikumar (1988) (Proposition 3.2, pag. 433).
Property 2 (Second order properties for χ) If the time buffer sequence {sn (τ)} ∈ SICX(sp), then the
approximate simulation objective function χ ∗ (sn , ·) ∈ SDCX(sp). If the processing time sequence {t(τ)} ∈
SICX(sp), then the approximate simulation objective function χ ∗ (·, t) ∈ SICX(sp).
Proof.

The same reasoning of Property 1 applies.

Corollary 1 The average performance {µ̂(sn , t)} ∈ SICX(sp) in the processing times t. The average
performance {µ̂(sn , t)} ∈ SDCX(sp) in the time buffer sn .
Proof.
According to Assumption 1, the expected performance is estimated by convex operations. Since
χ ∗ is SICX(sp) in the processing times and SDCX(sp) in the time buffers (Property 2) and both SICX(sp)
and SDCX(sp) are closed with respect to monotonic convex operations (Yao and Shanthikumar 1991), then
the average performance is SICX(sp) with respect to the processing times and SDCX(sp) with respect to
the time buffer.

3563

Pedrielli, Matta, and Alfieri
4.2 Constraints Characterization
Let Λ(s, B) and Λ̂(sn , b) be the expected value of the target performance µ and its estimator, namely:
Λ(s, B) , EB [µ],

Λ̂(sn , b) ,

1 n
∑ p(eν (i))
n i=1

(8)

Let λ (s, B) and λ̂ (s, b) be the difference between the expected value of the actual performance and the
target performance and its estimator, namely:
λ (s, B) , Λ(s, B) − µ ∗ ,

λ̂ (sn , b) , Λ̂(sn , b) − µ ∗ .

(9)

Estimates defined in (8) and (9) are sample averages. The expected values are functions of the time buffer
s and of the collection of random variables B. We will denote the expected values with Λ(s, B) and λ (s, B)
when we want to stress that the described property is related to the considered random variables (i.e., the
distribution taken into account), whereas the notation Λ(s, ·) and λ (s, ·) will be adopted in case the property
is independent from the distributions.
Lemma 1 The following holds for the approximate optimization model: (i) Λ(s, B) is Lipschitz continuous
in the domain of s for PB almost all τ ∈ R||τ|| . Then there exists a function Π : R||τ|| → R such that
||Λ(s1 , B) − Λ(s2 , B)|| ≤ Π(τ)||s1 − s2 ||, for PB almost all τ ∈ R||τ|| and such Π(τ) is integrable; (ii) The
moment generating function of Π(τ), denoted as MΠ(τ) (l), is finite for all the l in a neighborhood of 0.
Proof.
Note that Π(τ) is a stochastic function since it is related to the distance between random variables
||s1 − s2 ||. Corollary 1 proves that Λ(s, B) is convex in the parameters τ of distribution V B of B, hence
it is always possible to define function Π. In addition, if the system is stationary and µ ∗ ≥ µmin , then
Λ(s, B) < ∞ with probability 1, hence function Π is integrable. Corollary 1 also guarantees that, if the
difference ||s1 − s2 || is finite, the difference ||Λ(s1 , B) − Λ(s2 , B)|| is also finite.
For convexity, finiteness and compactness of the set Σ (section 4), the moment generating functions of
Π(τ) is finite in a neighborhood of 0 (Billingsley 1999).
We are now ready to characterize the relationship between λ̂ (·, ·) and λ (·, ·). First, we separately
analyze λ̂ (·, ·) and λ (·, ·), in order to verify their properties. We then study the distance between the two
functions as the sample path increases.
Proposition 1 Let the function π = E [Π(τ)] be the expectation of Π(τ) and πn be the sample average
approximation of E [Π(τ)], i.e., πn ,

1
n

n

∑ Π j (τ): 1) If function λ (s) is bounded on Σ, λ̂ (sn , ·) is PB –almost
j=1

surely bounded on Σ. 2) If function λ (s, ·) is Lipschitz continuous on Σ, λ̂ (sn , ·) is PB –almost surely
Lipschitz continuous on Σ.
Proof.
Let s0 ∈ Σ be a time buffer configuration. Lemma 1 leads to the following chain of inequalities
for the infinite sample path problem:
E [λ (s, B)] ≤
E [|λ (s0 , B)| + Π(τ)||s − s0 ||]
0 ≤ |λ (s0 , B)| + E [Π] maxs1 ,s2 ∈Σ ||s1 − s2 || =
<∞
=
|λ (s0 , B)| + π maxs1 ,s2 ∈Σ ||s1 − s2 ||

(10)

The inequalities |λ (s0 , B)| < ∞ and Π < ∞ hold because of Lemma 1, whereas maxs1 ,s2 ∈Σ ||s1 − s2 || (the
deviation of the set Σ) is finite because the set is compact. This proves equation (10) holds, i.e., |λ (s0 , B)|
and π are both finite. The chain of equalities E [λ (s, B)] = E [ε] = 0 is guaranteed by any target performance
satisfying µ ∗ ≥ µmin . In fact, from Lemma 4, as n → ∞, we are guaranteed the solution set is not empty
and ε ∗ = 0 exists.
3564

Pedrielli, Matta, and Alfieri
The same result can be proved for function λ̂n (s, b). We can rewrite the chain of inequalities for the
finite sample path problem. Simply by considering that in the second inequality wehhave α iinstead of 0.
In the finite sample path case, we cannot guarantee that ε̂ ∗ = 0, i.e., in general, E λ̂ (s, b) = E [ε̂] = 0
does not hold.
Lipschitz continuity holds as a consequence of Corollary 1. Indeed, function λ (s, B) is increasing
convex in the processing times, realization of the random variables in B, and it is decreasing convex in the
multidimensional array s (the time buffer).
n
o
Lemma 2 sup |λ (s, B) − λ̂ (s, B)| : s ∈ Σ → 0 as n → ∞ almost surely. As a result, λ̂ (s) converges to
λ (s) uniformly on Σ almost surely.
Proof.

The proof relies on the result from Proposition 1.

Lemma 2 shows that the sample average estimator is an unbiased estimator of the performance expected
value and, as a result, the sample path constraint set converges to the true constraints set (Shapiro 2003).
4.3 Asymptotic Convergence
In the following, we will use the notation s and S to indicate the infinite sample path solution and objective
function, respectively, while sn and Sn will indicate the optimal time buffer and the objective function
of the finite sample path case, as in the previous sections. In order to apply the fundamental results in
(Robinson 1996), we need to show that Assumptions A–B in Definitions 2.2 and 2.3 in (Robinson 1996)
hold and this is proved by
Property 3 (Assumption A, Definition 2.2 in (Robinson 1996), page 517) Function Sn satisfies the following
conditions: (a) for each 1 ≤ n < ∞, Sn is lower semi–continuous. (b) Sn →epi S , i.e., it epiconverges to
S.
Proof.
The function Sn is lower semi–continuous iff the related epigraph, ESn , is closed (Attouch 1984).
The epigraph of the function Sn represents the set of solutions having a value of the objective function
smaller than some predefined φ > Sn∗ , i.e., ESn = {(sn , φ ) : Sn (sn ) ≤ φ }. In particular, let sn be a feasible
solution and φ be a real positive value. The function Sn : Σn → R+ is linear in sn and it is defined over
the compact set Σn . As a result, there exists an arbitrarily small δ (φ ) ∈ R such that the neighborhood of
(feasible) solutions sn + δ are outside the epigraph ESn , thus ESn is a closed set. This proves that S is
lower semi–continuous.
The function Sn →epi S if, in addition to lower–semicontinuity, it uniformly converges to the infinite
sample path function S on compact subsets of the function domain Σ. Since the random variables in
B are such that P (Bi j ≥ ∞) = 0, then the objective function is finite S ≤ ∞ a.s., hence the function is
proper. In addition, the objective function is linear in s, convex in the processing times (Property 1) and it
is defined over a compact set Σn . Since, from Lemma 2, we know that Σn → Σ uniformly, we have uniform
convergence.
Property 4 (Assumption B, Definition 2.3 in (Robinson 1996), page 517) S is proper and the set of
minimizers of the finite sample path optimization problem S∗n = {sn ∈ Σn |Sn = Sn∗ } is not empty and it
is compact.
Proof.
The problem in (5) always admits a feasible solution for construction. Let µmin be the best
performance that can be reached by the system in a steady state. If the system is stationary, a finite µmin
exists. As a result, the problem solution set is non–empty. Under Assumption 2, as the sample path size
goes to ∞, the optimal solution is characterized by ε ∗ = 0.

3565

Pedrielli, Matta, and Alfieri
Theorem 1 (Convergence of S , from Theorem 3.2 in (Robinson 1996), page 519) The minimizer s∗n
epi–converges to the infinite sample path solution s∗ with probability 1. The related objective function
value Sn∗ converges uniformly to the infinite sample path objective function value S ∗ .
Proof.
From Property 4, the optimal solution to the infinite–sample path optimization problem exists
and it is finite, s∗ . The ε-problem is developed in a way such that a minimum to the sample path problem
always exists, i.e., for each n, the sample–path solution sn is such that sn ∈ S∗n . As n → ∞ this solution
converges to a limiting value s.
Let Sn∗ be the value of the objective function at the optimum: Sn∗ = inf Sn . Function Sn satisfies
epi–convergence (Property 3 and 4). Given Lemma 2, we can use epi–convergence to prove that S∗n
converges to S∗ , i.e., s∗n converges to s∗ .
Let Γ be the set defined as Γ = {b : sup {bl } = ∞} (note that Γ has measure 0 under Assumption 2). For
every realization b of the processing times such that b ∈
/ Γ, the following holds (this result is in Theorem
3.2 in (Robinson 1996), page 519):
(a)
(b)

S ∗ ≤ lim sup Sn∗ ;
if sn is a sequence converging to s and if, for each n, sn ∈ S∗n , then s ∈ S∗ .

Epi–convergence results in s ∈ S∗ , hence the sample path optimal solution converges to the infinite sample
path optimal solution.
5

NUMERICAL ANALYSIS

Herein, we analyze the impact of the complexity and the size of the problem together with the main
parameters. In order to do so, we consider the multi–stage systems in Table 1.
Table 1: Test Problems

System
Multi–Stage
Kanban–Controlled
Base–Stock–Controlled
Extended–Kanban–Controlled

Objective
Minimize Buffer Capacity
Minimize Kanban Tokens
Minimize Base Stock Level
Minimize the Base Stock Level & Kanban Tokens

Performance
System Throughput
Service Level
Service Level
Service Level

We solved these problems by approximating buffer capacities, kanban tokens and inventory levels with
the related time-buffers. More details concerning the implementation of the framework to these cases
are available in (Pedrielli et al. 2015, Alfieri and Matta 2012a). Figure 1 shows the convergence of the
objective (S ∗ ) function and the related computational effort when the systems in Table 1 are subject to
different saturation conditions. In particular, in Figure 1(a), the multi–stage system is required to produce
at a rate of 0.65 [jobs/time unit], and, equivalently, we set an arrival rate into the pull system equal to 0.65
[jobs/time unit]. Figure 1(b) refers to a rate of 0.9 [jobs/time unit]. All the Test Problems refer to three-stage
systems, i.e., J = 3. From Figure 1, we can observe that the multi–stage buffer allocation problem (MS in
Figure 1(a)), shows the fastest convergence, weather base–stock and kanban systems appear to be more
oscillating. This phenomenon can be brought back to the higher influence of the initial conditions on
kanban and base–stock systems, which cause the solution to be more influenced by the initial generation
of random variables negatively affecting the rates. Figure 1(b) shows that the convergence is faster, with
respect to the previous cases; this is due to the fact that more stringent constraints dramatically reduce
the solution space making the procedure converge faster. We also observed the required solution time.
Systems of increasing complexity also require an increased computational power (e.g., EKCS), whereas
more constrained systems correspond to less computational time.

3566

Pedrielli, Matta, and Alfieri

(a) µ ∗ = 0.65

(b) µ ∗ = 0.90

Figure 1: Empirical Convergence

In these experiments, even for a small number of entities, the computational effort shows an exponential
growth. As already stated in section 3, techniques that can still make use of information coming from the
mathematical model, but use faster solution algorithms, will be beneficial to the framework.
6

CONCLUSIONS

In this paper, we gave a comprehensive overview of the integrated simulation–optimization framework
based on mathematical programming representations. The main definitions and guidelines to develop
integrated simulation–optimization models are provided together with the algorithm to solve them. The
asymptotic properties and examples presented encourage the design and analysis of efficient methodologies
for the solution of integrated simulation–optimization models. Indeed, if, on the one hand, the presented
algorithm shows empirical convergence, its implementation requires to solve mathematical programming
models. More efficient techniques can be used which exploit the mathematical modeling while providing
a more efficient procedure (i.e., leading to possibly faster convergence rates). Such procedures might be
applied not only to the approximate but also to the original problems. In particular, time buffer models
could be interpreted as low fidelity versions of their integer analogue. As such, the approximate integer
solution from the time buffer model, might indeed be used as initial solution for the integer counterpart.
Nevertheless to make such framework effective, algorithmic efficiency needs to be improved.
REFERENCES
Alfieri, A., and A. Matta. 2012a. “Mathematical programming formulations for approximate simulation of
multistage production systems”. European Journal of Operational Research 219 (3): 773 – 783.
Alfieri, A., and A. Matta. 2012b. “A Time-Based Decomposition Algorithm for Fast Simulation with
Mathematical Programming Models”. In Proceedings of the 2012 Winter Simulation Conference, edited
by C. Laroque, J. Himmelspach, R. Pasupathy, O. Rose, and A. M. Uhrmacher: Berlin, Germany.
Alfieri, A., A. Matta, and G. Pedrielli. 2013. “Mathematical Programming formulations for approximate
simulation optimization of closed–loop systems”. Annals of Operations Research.
Attouch, H. 1984. Variational Convergence of Functions and Operators. Pitman, London.
Billingsley, P. 1999. Convergence of probability measures; 2nd ed. Wiley Series in Probability and Statistics.
Hoboken, NJ: Wiley.
Chan, W., and L. Schruben. 2008a. “Optimization models of Discrete–Event System Dynamics”. Operations
Research 56 (5): 1218–1237.
Chan, W. K., and L. W. Schruben. 2008b. “Mathematical programming models of closed tandem queueing
networks”. ACM Transactions on Modeling and Computer Simulation 19 (1).
Edelkamp, S., and S. Schroed. 2011. Heuristic Search: Theory and Applications. Morgan Kaufmann.

3567

Pedrielli, Matta, and Alfieri
Fu, M., F. Glover, and J. April. 2005. “Simulation optimization: a review, new developments, and
applications”. In Proceedings of the 2005 Winter Simulation Conference.
Healy, K., and L. W. Schruben. 1991. “Retrospective simulation response optimization”. In Proceedings
of the 23rd conference on Winter simulation.
Jin, J., and B. Schmeiser. 2003. “Simulation-based Retrospective Optimization of Stochastic Systems: a
Family of Algorithms”. In Proceedings of the 2003 Winter Simulation Conference.
Kolb, O., and S. Gttlich. 2015. “A continuous buffer allocation model using stochastic processes”. European
Journal of Operational Research 242 (3): 865 – 874.
Matta, A., G. Pedrielli, and A. Alfieri. 2014. “ERG Lite: Event Based Modeling for SimulationOptimization
of Control Policies in Discrete Event Systems”. In Proceedings of the 2014 Winter Simulation Conference.
Muter, I., S. Birbil, and K. Bulbul. 2012. “Simultaneous Column-and-Row Generation for large-scale linear
programs with column-dependent-rows”. Mathematical Programming 142(1-2):47–82.
Pedrielli, G. 2013. Discrete Event Systems Simulation–Optimization: Time Buffer Framework. Ph. D. thesis,
Mechanical Engineering Department, Politecnico di Milano, Italy.
Pedrielli, G., A. Matta, and A. Alfieri. 2015. “Integrated Simulation–Optimization of Pull Control Systems”.
International Journal of Production Research:To Appear.
Plambeck, E. L., B. ruey Fu, S. M. Robinson, and R. Suri. 1996. “Sample-path optimization of convex
stochastic performance functions”. Mathematical Programming 75:137–176.
Robinson, S. 1996. “Analysis of Sample–Path Optimization”. Mathematics of Operations Research 21:513–
528.
Schruben, L. W. 2000. “Mathematical Programming Models of Discrete Event System Dynamics”. In
Proceedings of the 2000 Winter Simulation Conference.
Shaked, M., and J. Shanthikumar. 2007. Stochastic orders. Springer series in statistics. Springer.
Shaked, M., and J. G. Shanthikumar. 1988. “Stochastic Convexity and Its Applications”. Advances in
Applied Probability 20 (2): pp. 427–446.
Shapiro, A. 2003. “Monte Carlo sampling methods”. Handbooks in operations research and management
science 10:353–425.
Spall, J. C. 2003. Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control.
Wiley.
Tan, B. 2015. “Mathematical programming representations of the dynamics of continuous-flow production
systems”. IIE Transactions 47 (2): 173–189.
Weiss, S., and R. Stolletz. 2015. “Buffer allocation in stochastic flow lines via sample-based optimization
with initial bounds”. OR Spectrum:1–34.
Yao, D., and J. G. Shanthikumar. 1991. “Strong stochastic convexity: Closure properties and applications”.
Journal of Applied Probability 28 (1): pp. 131–145.
AUTHOR BIOGRAPHIES
GIULIA PEDRIELLI is Research Fellow for the Centre for Maritime Studies at the National University
of Singapore. Her research focuses on stochastic simulation-optimization in both single and multiple–
objectives framework. Her email address is cmsgp@nus.edu.sg.
ANDREA MATTA is Distinguished Professor at the Institute of Industrial Engineering at Shanghai Jiao
Tong University, where he currently teaches stochastic models and simulation. His research area includes
analysis and design of manufacturing and health care systems. His email address is matta@sjtu.edu.cn.
ARIANNA ALFIERI is Associate Professor at Politecnico di Torino, where she currently teaches production planning and control and system simulation. Her research area includes scheduling and planning
in production and transportation systems. Her email address is arianna.alfieri@polito.it.

3568

Proceedings of the 2016 Winter Simulation Conference
T. M. K. Roeder, P. I. Frazier, R. Szechtman, E. Zhou, T. Huschka, and S. E. Chick, eds.

HYBRID ORDER PICKING STRATEGIES FOR
FASHION E-COMMERCE WAREHOUSE SYSTEMS
Giulia Pedrielli
Albert Vinsensius
Ek Peng Chew
Loo Hay Lee

Alessandro Duri

Department of Industrial & Systems Engineering
National University of Singapore
1 Engineering Drive 2
Singapore, 117576, SINGAPORE

Operations Department
ZALORA
182 Clemenceau Ave
Singapore, 239923, SINGAPORE

Haobin Li
Institute of High Performance Computing
Agency for Science, Technology & Research (A*STAR)
1 Fusionopolis Way, #16-16 Connexis
Singapore, 138632, SINGAPORE

ABSTRACT
E-commerce has become an increasingly relevant business in Southeast Asia. Effective warehouse management in terms of order picking is a key competitive advantage in this industry. Fashion products are
particularly difficult to efficiently manage in a warehouse as they have high demand variability, with a short
shelf-life and very little replenishment. In this work, after a detailed analysis of demand and physical layout
of the warehouse, we propose: (1) a new pick list generation algorithm considering aspects such as work
balancing and picking time minimization, and (2) a family of picking strategies accounting for possible
order configurations and warehouse layout. The main contribution of this work is in the development
of hybrid order picking strategies: a combination of zone-based and order-based picking with batching.
Simulation is used to assess the performance of these strategies. We have found that these hybrid strategies
outperform FIFO order picking often employed in industry.
1

INTRODUCTION

The fast fashion e-commerce industry has been booming in recent years especially in Southeast Asia, with
many players in the industry ranging from large corporations to small start-ups and online shops owned
by individuals. In large fast fashion e-commerce corporations, stocks are kept in large warehouses. Such
warehouses not only act as an inventory buffer in order to ensure availability but are, in fact, crucial to
meet service levels in terms of delivery time. As such, the efficient management of such a warehouse
- in terms of supply, order picking and location - represent a key competitive advantage to a company
competing in this industry. Moreover, fast fashion products have unique characteristics which make them
more challenging to manage efficiently in a warehouse. Their demand is highly variable and unpredictable,
both in terms of volume and value, their short shelf-life makes obsolescence a vital concern, and they are
seldom replenished where out-of-stock items are usually replaced with new collections instead of restocked.

978-1-5090-4486-3/16/$31.00 ©2016 IEEE

2250

Pedrielli, Duri, Vinsensius, Chew, Lee, and Li
As a result of these characteristics, warehouse management becomes challenging due to the difficulty
in identifying highly frequent or stable inventory, deciding on the inventory put-away (i.e. assignment
of inventory to locations), and order picking strategy to generate pick lists for the pickers to fulfill the
orders. In such a volatile and highly-variable environment, a single order picking strategy is unlikely to
satisfy the demand profile faced by such an e-commerce company. After an initial analysis of the demand
profiles and the physical layout of the warehouse, we realize that the warehouse operations and, especially,
the picking process represent the most significant component in the warehouse operating costs. With the
aim of improving the picking process, we propose an approach which relies on the characteristics of the
orders faced by the company. Since the picking process is an operational issue, our algorithm needs to
be implementable in a short time, thus hindering the possibility of using mathematical programming and
traditional optimization techniques to assign picking jobs to the warehouse operators.
Therefore, in this paper, we propose (1) a new fast algorithm to generate pick lists which takes into
consideration various aspects such as work balancing and pick time minimization, and (2) a family of picking
strategies taking into account the possible order configurations as well as the physical warehouse layout.
In order to assess and evaluate the performance of the various order picking strategies, we propose the use
of a discrete-event simulation as a general approach which can easily be extended to more complicated
and specialized warehouse layout.
2

WAREHOUSE OPERATIONS: A REVIEW

Warehouse operations and management as been a relevant research area for a considerable time and is
still relevant today. Gu, Goetschalckx, and McGinnis (2007) and Gu, Goetschalckx, and McGinnis (2010)
provide detailed reviews on warehouse operations including order picking. Order picking is the most
expensive operations in a warehouse as it is very labor and time consuming (Frazelle 2001). The objective
of the order picking system (OPS) is to maximize the service level (e.g. in terms of order lead time) subject
to resource constraints, given the warehouse layout and inventory storage locations. Since the bulk of the
OPS time is spent on traveling, minimizing item pick cycle time (i.e. item and order retrieval time) is an
equivalent objective (de Koster, Le-Duc, and Roodbergen 2007). It has also been found that the throughput
of the overall OPS is inversely proportional to the cycle time (Manzini, Gamberi, Persona, and Regattieri
2007). Hence the problem becomes providing the optimal wave size, batching, item-picker assignment and
routing for these pickers to retrieve the assigned items such that the item cycle time is minimized.
While optimal routing is desirable, this may not be suitable in practice as some pickers might find
the optimal routing illogical or counter-intuitive (Gademann and Velde 2005). Moreover, when there are
multiple candidate locations for a single item, a multitude of complications arise in determining the optimal
routing and that there are scare research done on this issue even though this scenario is often found in
practice. With a more complex warehouse structure and inventory locations, routing heuristics such as
S-shaped, return, mid-point, largest gap, and combined (hybrid) are more popular especially in practice
where a solution has to be obtained quickly and that a satisfactory solution is sufficient. Note that many
of these previous study assume a unit load. Moving forward, the rise of e-commerce poses a renewed
challenge to optimize warehouse picking operations where less-than-unit-load picking (or even single-item
picking) becomes very common. Since heuristics is preferred in practice, the routing used in this work is
based on the nomenclature of the inventory locations, resulting in a routing which is similar to the return
and S-shaped heuristics.
Order batching is often done to release a wave of orders to be picked. The batching problem in itself
is a complex problem which can significantly impact the performance of the OPS. This batching creates
a partition either between orders or between pickers/storage locations (otherwise known as zoning). The
difficulty in optimizing the batches comes from the fact that the cycle time is not known until the batch has
been created and the routing assigned (Gu, Goetschalckx, and McGinnis 2007). These batching problems
are often solved with heuristics with a variation of order-closeness metric, with the objective of batching
similar or close orders together (Elsayed and Unal 1989). The order proximity batching problem is studied

2251

Pedrielli, Duri, Vinsensius, Chew, Lee, and Li
by Gademann, Van Den Berg, and Van Der Hoff (2001), Hwang and Kim (2005), and de Koster, Van der
Poort, and Wolters (1999). In terms of performance, Petersen, Aase, and Heiser (2004) compared various
order picking strategies and found that batching often yields the lowest cycle time especially when the
order sizes are small. de Koster, Van der Poort, and Wolters (1999) compared various seed and savings
heuristics for manual warehouses and concluded that even a simple order batching method yields significant
improvement from FIFO order picking and that the performance of these heuristics depends largely on the
capacity of the carts utilized.
Simulation has often been used to optimize warehouse design and operations. A high-level manual
order-picking warehouse design has been analysed through simulation by Altarazi, Ammouri, and Alzubi
(2012). In terms of operations, simulation has been used to evaluate order-picking models solved with genetic
algorithm (Chang, Liu, Liu, and Xin 2007). Yoo, Cho, and Yücesan (2010) employ nested partitioning
and optimal computing budget allocation methods with simulation to optimize supply chain performance
on a strategic level while reducing computational loads. Gagliardi, Renaud, and Ruiz (2007) developed
a discrete-event simulation model to evaluate storage space strategies in a high-throughput warehouse.
Similarly, Faria and Reis (2015) employed a discrete-event simulation model to evaluate various storage
and routing strategies to improve order picking performance. Following these works, we have developed
a discrete-event simulation model to evaluate the proposed hybrid picking strategies.
3

METHODOLOGY

We first analyzed the demand profile of the company and noticed that, in the scope of choosing the picking
policy, it is important to understand how orders are characterized in terms of number of items and, in case
of multi-item orders, how these are distributed throughout the warehouse.
Assuming a randomized put-away strategy, which is sensible for a fashion warehouse, we analyzed
the warehouse density looking at the different zones (the zones referred to in this paper do not reflect
the real warehouse). As a result of the randomized put-away strategy, SKUs may have several physical
locations. Table 1 shows the distribution of the items in the warehouse. In particular, we can see that
the majority of the orders are single item orders and therefore we should design picking strategies which
improve the efficiency in fulfilling these orders. Nevertheless, a remarkable number of orders is multi-item
and these form the largest picking volume in terms of units of items. Moreover, it looks clear how most
of the orders are spread in different zones. Consequently, the picking has to consider an efficient way to
manage multi-zone items. A possibility, as two of the strategies indeed propose, is to use a sorting station
to consolidate orders from different zones, nevertheless, in this case we will need to control the maximum
number of orders released for picking at any one time (called batch of incoming orders later on) in order
to control the loading and waiting time at the sorting station.
Table 1: Orders Characterization.
Item Type

%items

%orders

Single Zone
Multi-Zone
Single Item

6.182
76.781
17.037

5.995
50.99
43.006

This work considers a manual picker-to-part order picking system where a picker collects a batch of
orders instead of a single order or item as in single-command operations. We assume that a layout of the
warehouse exists, along with the inventory locations, and that a list of orders is available and ready to be
picked. The pick list generation procedure generates a master pick list which is the set of picking lists
for single pickers. The pick list generation process is detailed in section 3.1. Four picking strategies are
compared using a discrete-event simulation. A simple First-In-First-Out (FIFO) strategy acts as the base

2252

Pedrielli, Duri, Vinsensius, Chew, Lee, and Li
case as this the strategy commonly implemented in industry to simplify the order picking process. The
four picking strategies are detailed in section 3.2. The overall proposed process is as such:
1. Batch incoming orders.
2. Group orders into three types: (1) multi-item single-zone, (2) multi-item multi-zone, and (3)
single-item.
3. If the picking strategy is not the FIFO base case, generate order queues corresponding to the three
order types.
4. Implement picking process and generate pick list based on the picking selected strategy.
3.1 Pick List Generation
In this section, we describe the pick list generation process through the phases of order batching, queue
generation and order assignment, and finally picker routing and pick list generation.
Batch Creation
Receives input of orders and generates the batches of orders. Simulation will be
employed as a method to study the impact of batch size on performance and establish the optimal
batch size. The batch size is a function of tote capacity, number of orders and system capacity
(number of pickers, etc.) such that the batch size results in 3 hours’ worth of picking activities.
This is also known as the pick wave. The difficulty is that it is not known how to determine this
batch size a priori.
Queue Generation
Receives the orders, number of pickers, number of totes, totes capacity, and inventory
locations. The output of this phase is the queue which associates the inventory location (UID)
with the items in the orders. The criteria for the queue generation are as such: (1) ”Queue
Dependency”: inventory in similar locations (zones, floor, etc.) are assigned to the same queue;
(2) Order characteristics: express (high priority), single-item and multi-item orders are placed
in separate queues; (3) ”Virtual Backlog”: items in locations are virtually reserved through the
assignment process in order to prevent pickers from being directed to an eventually empty location.
This process can be thought of as an order classification process which generates three types of
order based on the three criteria above. The three types of queue generated are Type 1: multi-item
single-zone, Type 2: multi-item multi-zone, and Type 3: single-item.
Inventory Reservation Procedure
Locations containing the desired item are identified and the first
location UID is assigned. Virtual reservation is done by decrementing the availability of the item
in that location.
Pick List Generation
Receives as input the picking strategy, queues generated, number of pickers and
the tote capacity (which determines the maximum size of the pick list). The output is then the
master pick lists which is the set of pick lists for individual pickers, complete with routing (items
sequencing).
In essence, after order batching, the orders are grouped into queues based on their characteristics and
the master pick list is generated by cutting each queue based on the suggested location UID (through the
inventory reservation procedure) according to the picker capacity. Algorithm 1 details these procedures.
3.2 Picking Strategies
The picking strategy is incorporated into the pick list generation. We propose four different picking
strategies. Strategy A represents the reference base case as it does not actually rely on any pick list
generation algorithm but simply pick the orders in a FIFO manner.
Strategy A (Pure Order Picking) In this base case strategy, we adopt an “order-based” picking
where orders are processed and picked sequentially in a FIFO manner, without any type of queue generation
or order classification.
2253

Pedrielli, Duri, Vinsensius, Chew, Lee, and Li
Algorithm 1 Queue and pick list generation algorithm.
Queue Generation (Allocation)
Identify item locations
Type 1 orders Create list of multi-item single-zone orders. Each order reports all possible fulfilment
zones.
Type 2 orders Create list of multi-item multi-zone orders. Each order is characterized by an array of
pairs of zones and items.
Type 3 orders Create list of single-item orders and find zones fulfilling single-item orders.
Pick List Generation (Assignment)
For each order in Type 1
While(not all items assigned && zones not empty)
If(item is available in zone)
Inventory reservation procedure
Else
Eliminate zone and check next zone
Type 1 orders
End If
End While
If(zone is empty)
Add order to Type 2
End If
End For
For each order in Type 2
For each item in order
Select zone with most number of items for the same order
While(item not assigned)
If(item available in zone)
Inventory reservation procedure
Type 2 orders
Else
Eliminate zone and check next zone
End If
End While
End For
End For
For each order in Type 3
Select zone with lowest load
If(item available in zone)
Inventory reservation procedure
Type 3 orders
Else
Eliminate zone and check zone with next lowest load
End If
End For
Strategy B (Hybrid Order Picking) In this strategy, each order is picked by a single picker and
the pickers have to travel across zones to fulfill the orders and, as such, there is no sorting required after
the picking. In literature, this is often identified as a pick-and-pass and sort-while-picking order picking.
•

Type 3 orders: Single-item orders are separated into a special queue for fast processing.

2254

Pedrielli, Duri, Vinsensius, Chew, Lee, and Li
•

•

Type 1 orders: Multi-item orders which can be fulfilled in the same zone are grouped together
under queues corresponding to a particular zone. A master pick list based on the tote capacity
and the suggested picking route (item sequence) is generated. The location assignment and path
generation is performed by Algorithm 1. Note that orders are not broken up into individual items
(i.e. sorting happens on the tote itself) and completed orders are sent directly to the outbound.
Type 2 orders: Multi-item orders which cannot be fulfilled in the same zone (hence multi-zone) are
grouped into another queue. They are partially picked in a zone before moving on to subsequent
zone(s) until completion.

Strategy C (Hybrid Zone Picking) In this strategy, orders in the batch is broken up into items which
are located in the same zones such that each picker stays in a particular zone. At the end of the picking
process, the picked items are sorted to the corresponding orders. In literature, this is often identified as
a pick-then-sort order picking. The complication with this strategy is that there exists another sub-batch
which controls the granularity of the sorting process. A larger sub-batch results in a higher pick density
(hence potentially lower picking cycle time) but a longer consolidation and sorting time.
•
•

•

Type 3 orders: Single-item orders are separated into a special queue for fast processing.
Type 1 orders: Multi-item orders which can be fulfilled in the same zone are grouped together
under queues corresponding to a particular zone. A master pick list based on the tote capacity
and the suggested picking route (item sequence) is generated. The location assignment and path
generation is performed by Algorithm 1. Note that orders are not broken up into individual items
(i.e. sorting happens on the tote itself) and completed orders are sent directly to the outbound.
Type 2 orders: Multi-item orders which cannot be fulfilled in the same zone (hence multi-zone)
are batched into sub-batches of predefined size and broken up into individual items which are then
grouped into their fulfillment zones. The queues generated correspond to the zones. The pickers
stay in a particular zone during picking. These orders will be consolidated and then sorted after
picking.

Strategy D (Pure Zone Picking) This strategy is similar to the hybrid zone picking but does not
differentiate between multi-items orders which can be fulfilled in a single zone and those which can only
be fulfilled by multiple zones. Sorting after picking is also required for this strategy.
•
•

4

Type 3 orders: Single-item orders are separated into a special queue for fast processing.
Type 1 and Type 2 orders: Multi-item orders are batched into sub-batches of predefined size and
broken up into individual items which are then grouped into their fulfilment zones. The queues
generated correspond to the zones. The pickers stay in a particular zone during picking. These
orders will be consolidated and then sorted after picking.

SIMULATION MODEL

The simulation model employed is a discrete-event simulation based on the Object-Oriented Discrete-Event
Simulation (subsequently referred to as “O2DES”) framework, developed by one of the authors, written in
C# on Microsoft Visual Studio IDE. In this framework, the simulation entities are partitioned into three major
categories namely Dynamics, Events and Statics. Static elements have constant properties which describe
the fixed physical warehouse layout. Dynamic elements have modifiable properties describing mutable
entities such as pickers, SKUs, and picking lists. Finally, Events modify the states of the Dynamic entities
and are executed sequentially according to a Future-Events List. This O2DES framework allows us to
comply closely to the widely-accepted discrete-event simulation framework while exploiting object-oriented
programming features such as inheritance and dynamic binding for flexibility.
The pick list generation module, which implements the proposed picking strategies, as well as the
shortest-path routing module, based on Dijkstra’s algorithm, are also integrated into the simulation model.

2255

Pedrielli, Duri, Vinsensius, Chew, Lee, and Li
The pick list generation module takes in the orders and generates the batches and pick lists based on the
selected strategy parameters. The routing module determines the travel time required based on the locations
of the items in the pick list. These two modules combined result in routes which are similar to those
generated with a return and S-shaped heuristics for order picking routing.
The discrete-event simulation model is used to evaluate and compare the various strategies. Firstly,
actual demand data and inventory locations are obtained. Two weeks of demand data is used; this forms
the source of stochasticity in the experiment. The physical layout of the warehouse is also translated into
the simulation model; this physical layout is constant throughout all the experimental runs. For each set of
demand data, the simulation is run using each of the picking strategies to obtain the desired output statistics
(described in detail in Section 5). Parameters such as tote capacity, batch size, pickers, and sorting rate are
subjected to a design of experiment as described in Section 5.2. Figure 1 illustrates the experiment flow.
Selected Strategy

Selected Parameters
influences
decision on

Demand Data
Inventory Locations

Discrete-Event
Simulation Model

Output
Statistics

Physical Layout

Figure 1: Simulation Experiment Flowchart.

5

FASHION WAREHOUSE CASE STUDY

ZALORA is one of the main e-commerce companies in Southeast Asia. A complexity for ZALORA is
caused by the fact that the content of the customers’ orders is particularly variable. As a consequence, as
opposed to most of the warehouse in other sectors, it is difficult to identify fast-moving items and perform
categorizations of the inventory using traditional ABC techniques. As a result, for the experiments, we
consider a randomized inventory storage, i.e. SKUs are allocated to multiple and randomly positioned
inventory locations. In the case constructed with the company, we consider a warehouse with 8 zones.
The density per zone resulting from the random put-away is derived as the ratio between the number of
items in a zone to the total number of items (in all zones). We took as a reference a specific warehouse
and studied the order profile for two weeks. From this study, we were able to identify the profile in terms
of single-item orders, multi-item orders and, within the multi-item orders, the single-zone and multi-zone
orders. This information is very important for the allocation of operators to the different order types and
also to establish the capacity of the different types of totes.
The simulation is constructed based on a real warehouse layout, inventory locations and demand data.
The simulation model is validated against the real-world picking cycle time and pick list size based on the
FIFO strategy. The warehouse layout evaluated consists of 725 rows, 68886 racks (i.e. possible storage
locations), and 132378 items. The number of daily orders evaluated is in the range of 3000 and 6000
orders and the number of pickers available is as high as 150 operators. For confidentiality, these values
do not reflect the actual values implemented in the company but are in the same order of magnitude. In
essence, the stochasticity in this simulation comes from a sample of the real-world demand data as well
as the real-world randomized inventory locations.
The objectives of the experimentation phase are presented in two stages:
1. Size the system capacity in terms of manpower required to complete the regular daily demand;
2. Given the manpower available, identify a good setting in terms of policy parameters. In particular,
we are interested in:
2256

Pedrielli, Duri, Vinsensius, Chew, Lee, and Li
•Capacity of the totes bringing items;
•Capacity of the carts bringing orders;
•Master Batch size, representing the number of orders the system processes simultaneously to
perform the assignment to the operators in the warehouse (i.e., to form the pick lists). This
parameter has a lower bound (degenerate, which is the number of operators) and an upper bound
corresponding to the capacity given by the number of deployed operators and the capacity of
the carts/totes. If the batch size is below the capacity in terms of totes, we will have idle
operators which is something we need to avoid. Therefore, there is a strong correlation between
the Master Batch Size and the capacities aforementioned;
•Sorting Rate: represents the number of items per time unit that a sorter is able to produce;
•Number of Sorters: represents the number of operators at the sorting station;
•Maximum Order Batch Size, which defines how many orders destined to the sorting station
can be simultaneously put in the picking system. This parameter is important to control the
waiting time at the sorting station and it is highly correlated to the number of sorting stations
and the sorting rate.
When dealing with the second objective, we established with the company three main performance measures
which are important to consider:
•
•

•

Picking Cycle Time [sec/item]: represents the average time for an item to be picked. It is important
to notice that the larger the capacity of the totes/carts, the larger will be the cycle time;
average items per tote
%]: represents the ratio between the capacity of
Average Tote Utilization [
tote capacity
the tote and the number of items carried. This represents a good indicator of the work balance.
Low saturation implies that some operators will be idle or finish ahead of time;
average orders per cart
Average Cart Utilization [
%]: this is the same as the previous indicator but
cart capacity
for orders.

In the following section (5.1) we show the performance of the strategies with respect to the Stage 1 problem,
i.e. the manpower sizing, while in the subsequent section (5.2) we use the input information related to the
operators to perform a policy parameters optimization. Specifically, the second stage task was performed
using Design of Experiment since the company was able to provide us with the possible values of the
policy parameters due to technical and physical constraints over the parameters.
5.1 Manpower Allocation
Since labor cost constitute a significant portion of warehouse operating costs, we try to minimize the number
of pickers for each of the strategies. The first issue that has to be solved for the allocation problem was
the assignment of operators to each order type. The idea is to assign workers to the different order types
based on the average total relative cycle cycle time required by each type with respect to the total average
cycle time. In order to evaluate these average cycle times, we ran simulation by considering an over-sized
system. The results are displayed in Table 2 below.
Considering the ratios in the last column of Table 2, we performed a simple enumeration over the
total number of available operators according to the data provided by the company. Based on the data of
11 days of demand, we bootstrapped the data using an empirical discrete density in order to generate a
statistically significant input. Under the advice of the company, we ensured that the allocated manpower is
able to complete all the picking tasks for each day within 10 hours with a 95% probability. As suggested
by the company, we started with 40 pickers and derived the following allocation shown in Table 3 below.
It is noteworthy that Strategy C and D, despite requiring a lower number of operators, necessitate sorting.
As a result, at this point of the analysis we cannot conclude which strategy is superior considering the

2257

Pedrielli, Duri, Vinsensius, Chew, Lee, and Li
Table 2: Average Picking Cycle Time Estimation.
Strategy

Order Type

Cycle time [min/item]

Cycle Time Multiplier
ci / ∑i=1,...3 ci

A

Order
Multi-Item Single Zone
Multi-Item Multi Zone
Single Item
Multi-Item Single Zone
Multi-Item Multi Zone
Single Item
Multi-Item
Single Item

1.127
0.874
1.008
1.101
0.874
0.583
1.095
0.752
1.101

1.000
0.293
0.338
0.369
0.293
0.195
0.367
0.252
0.369

B

C
D

Table 3: Required Manpower.
Strategy

Order Type

# Workers

A

Order
Multi-Item Single Zone
Multi-Item Multi Zone
Single Item
Multi-Item Single Zone
Multi-Item Multi Zone
Single Item
Multi-Item
Single

26
2
17
4
1
15
4
16
3

B

C
D

manpower requirements; only Strategy A appears to be dominated. In the next part of the experimentation,
we will focus on this comparison by considering the remaining picking policy parameters in order to provide
insights on dominance of the different strategies.
5.2 Strategy Comparison
Based on discussions with the company, we were able to identify a relatively small discrete set of possible
feasible policy parametrization. Therefore, we constructed a Design of Experiment as depicted in Table
4 below (note that the values do not refer to the actual values implemented in the company). In the
experimental runs, we collect the following output as Key Performance Indicators (KPIs):
•
•

•

Cycle Time: for the example case, the company established a threshold for the cycle time for each
item i, ci = 54 [sec/item];
Average Tote Utilization: this indicator determines the productivity of the tote equipment and
measures the efficiency of warehouse operations. This KPI is not applicable for Strategy A. For
the company, the threshold average tote utilization should be at least 75%;
Average Cart Utilization: this KPI determines the productivity of cart equipment. This KPI is not
applicable for Strategy D. For this indicator, the optimal average cart utilization should be at least
90%.

2258

Pedrielli, Duri, Vinsensius, Chew, Lee, and Li
The aforementioned design of experiment aims to determine the set of parameters for which the desired
level of performance is achieved. In Table 5, the average results across all experimental conditions using a
particular demand data for each strategy are reported. In particular, the “Satisfactory Performance Level”
indicates the proportion (in terms of percentage) of tested experimental conditions in which a strategy is
able to meet all the thresholds suggested by the company. We observe that the only strategy that meets all
the requirements is Strategy D, under the settings with maximum item totes capacity 40 and maximum
order batch size of 70.
Table 4: Testing Configurations (factors levels for a full factorial design).
Item Totes
Capacity

Order Totes
Capacity

Master Batch
Size

Max Orders
Batch Size

Sorting
Rate

No. of
Sorters

(20, 40)

(6, 12)

(500, 700)

(30, 70)

(3, 7)

(1, 3)

Table 5: Average Strategy Performance Over All Configurations.
Archived Percentage
Average Cycle Time [sec/item]
Average Tote Utilization
Average Cart Utilization
Satisfactory Performance Level

Strategy A

Strategy B

Strategy C

Strategy D

70
N.A.
99.70%
0%

62.75
92%
86.80%
0%

52.46
58%
46.70%
0%

51.88
58%
N.A.
12.5%

Although Strategy A performs the best in terms of cart utilization, it does not meet the requirement
for average cycle time, which is around 70 seconds per item picked on average. For Strategy B, it does
not meet requirement of the total cycle time, which is nearly 63 seconds per item picked on average. It is
apparent how strategy A and B require a larger number of operators in order to reach the same performance
of the other two strategies (considering also the operators at the sorting station). Therefore, at this phase
of the experiment these two strategies are dominated by C and D. For Strategy C, it meets requirement for
total cycle time with 52 seconds per item (still higher than that of Strategy D) but none of the simulation
cases satisfies the requirement for cart utilization, which is only 47% on average.
Observing the effects analysis from the performed experiments, we found that:
•

•

The cart (tote) capacity has an important impact over the system performance: a smaller capacity
requires pickers to run more times to fulfill total orders picking, thus increasing the picking time.
As a result, we should maximize the capacity provided that the picker can handle the cart in the
same amount of time.
The master-batch size is a key for the balance between picking and sorting. A very large batch
size, will provide larger saturation, but results in longer waiting times at the sorting station. As a
consequence, this size has to be set carefully.

Considering the results, after taking the desired KPIs and manpower allocation into consideration,
Strategy D is the best strategy for the company based on this example scenario. In terms of the KPIs,
Strategy D yields a significant reduction in average cycle time while not being absolutely disadvantaged
in terms of tote utilization. As for manpower allocation, Strategy D requires less manpower to handle the
same warehouse operations and thus reduces the related labor cost. Nevertheless, we need to consider that
at this stage of development of the simulation model, we are considering the sorting process simply as a
delay, i.e., the detailed sorting process is not modeled within the simulator. Nevertheless, sorting a large
number of items, as strategy D requires, can be particularly problematic, due to the complexity for the

2259

Pedrielli, Duri, Vinsensius, Chew, Lee, and Li
operator to identify and match items. For this reason, strategy C should be considered since it lowers the
number of items directed to sorting.
Another important aspect resides in the low saturation obtained both in strategy C and D. This result is
due to the sorting station which represents the system bottleneck. Due to the presence of the order batch,
the pickers cannot load to many items to avoid congestion at the sorting and this generates waiting times.
Again, the efficiency of the sorting station is relevant for both strategies and we are currently in the process
of designing a sorting process and the required technology.
6

CONCLUSION

This work contributes in the proposal of four innovative picking strategies, which were designed for an
e-commerce company under the consideration that the main impact for the picking process resides in trying
to pool together orders which are located in the same zone independently from the volume. The proposed
approach has been rigorously evaluated using simulation, thus providing a tool to the company to evaluate
the different strategies and we have found that the pure zone picking is the best.
From this work, we see that there is an indication that having a high picking density results in a
higher picking performance. As such, future work is being performed to use learning techniques in order
to maximize the inventory density by performing inventory put-away based on correlation between items
instead of volume (differently from typical ABC classification).
REFERENCES
Altarazi, S. A., M. M. Ammouri, and E. Alzubi. 2012. “Concurrent Manual-Order Picking Warehouses
Design via Simulation”. IIE Annual Conference. Proceedings:1.
Chang, F.-l., D.-d. Liu, Z.-x. Liu, and Z. Xin. 2007. “Research on Order Picking Optimization Problem of
Automated Warehouse”. Systems Engineering - Theory & Practice Online 27 (2): 139–143.
de Koster, R., T. Le-Duc, and K. J. Roodbergen. 2007. “Design and Control of Warehouse Order Picking:
A Literature Review”. European Journal of Operational Research 182 (2): 481–501.
de Koster, R., E. S. Van der Poort, and M. Wolters. 1999. “Efficient Orderbatching Methods in Warehouses”.
International Journal of Production Research 37 (7): 1479–1504.
Elsayed, E. A., and O. I. Unal. 1989. “Order Batching Algorithms and Travel-Time Estimation for Automated
Storage/Retrieval Systems”. International Journal of Production Research 27 (7): 1097–1114.
Faria, F., and V. Reis. 2015. “An Original Simulation Model to Improve the Order Picking Performance:
Case Study of an Automated Warehouse”. Volume 9335, 689–703.
Frazelle, E. 2001. World-Class Warehousing and Material Handling. New York: McGraw-Hill.
Gademann, N., J. P. Van Den Berg, and H. H. Van Der Hoff. 2001. “An Order Batching Algorithm for
Wave Picking in a Parallel-Aisle Warehouse”. IIE Transactions 33 (5): 385–398.
Gademann, N., and S. Velde. 2005. “Order Batching to Minimize Total Travel Time in a Parallel-Aisle
Warehouse”. IIE Transactions 37 (1): 63–75.
Gagliardi, J. P., J. Renaud, and A. Ruiz. 2007. “A Simulation Model to Improve Warehouse Operations”. In
Proceedings of the 2007 Winter Simulation Conference, edited by S. G. Henderson, B. Biller, M. Hsieh,
J. Shortle, J. D. Tew, and R. R. Barton, 2012–2018. Piscataway, New Jersey: Institute of Electrical and
Electronics Engineers, Inc.
Gu, J., M. Goetschalckx, and L. F. McGinnis. 2007. “Research on Warehouse Operation: A Comprehensive
Review”. European Journal of Operational Research 177 (1): 1–21.
Gu, J., M. Goetschalckx, and L. F. McGinnis. 2010. “Research on Warehouse Design and Performance
Evaluation: A Comprehensive review”. European Journal of Operational Research 203 (3): 539–549.
Hwang, H., and D. G. Kim. 2005. “Order-Batching Heuristics Based on Cluster Analysis in a Low-Level
Picker-to-Part Warehousing System”. International Journal of Production Research 43 (17): 3657–3670.

2260

Pedrielli, Duri, Vinsensius, Chew, Lee, and Li
Manzini, R., M. Gamberi, A. Persona, and A. Regattieri. 2007. “Design of a Class Based Storage Picker to
Product Order Picking System”. The International Journal of Advanced Manufacturing Technology 32
(7): 811–821.
Petersen, C. G., G. R. Aase, and D. R. Heiser. 2004. “Improving Order-Picking Performance Through the
Implementation of Class-Based Storage”. International Journal of Physical Distribution & Logistics
Management 34 (7): 534–544.
Yoo, T., H. Cho, and E. Yücesan. 2010. “Hybrid Algorithm for Discrete Event Simulation Based Supply
Chain Optimization”. Expert Systems with Applications 37 (3): 2354 – 2361.
AUTHOR BIOGRAPHIES
GIULIA PEDRIELLI is a Research Fellow for the Department of Industrial & Systems Engineering
at the National University of Singapore. Her research focuses on stochastic simulation-optimization in
both single and multiple–objectives framework. She is developing her research in meta-model based
simulation optimization and learning for simulation and simulation optimization. Her email address is
giulia.pedrielli.85@gmail.com.
ALESSANDRO DURI is the regional Director of Operations for ZALORA gfg. He focuses on the
everyday coordination of 7 DCs, customer service and photo shoot production hubs throughout seven
countries in Southeast Asia. He also manages the upstream section of the supply chain, liaising with
suppliers worldwide, with a goal to bring an unprecedented product offering and customer experience to
all of Southeast Asias online fashion shoppers. His email address is alessandro.duri@zalora.com.
VINSENSIUS ALBERT is a PhD Candidate in the Department of Industrial & Systems Engineering
(ISE), National University of Singapore (NUS). He received his Bachelor of Engineering (ISE) with Honors (Highest Distinction) from NUS in 2015 and is currently a recipient of NUS Research Scholarship. His
research interest is in the field of supply chain, logistics and optimization, employing a combination of operations research, computational, and simulation methods. His email address is vinsensius.albert@u.nus.edu.
EK PENG CHEW received his Ph.D. in Industrial Engineering from the Georgia Institute of Technology,
USA. He is currently an Associate Professor and Deputy Head (Undergraduate Studies) in the Department
of Industrial and Systems Engineering at the National University of Singapore. He was a Visiting Scholar
and a Visiting Professor, respectively, at the Georgia Institute of Technology and University of British
Columbia in 2006. His current research areas are in port logistics and maritime transportation, simulation
optimization and inventory management. His email address is isecep@nus.edu.sg.
LOO HAY LEE is an Associate Professor in the Department of Industrial and Systems Engineering at
National University of Singapore. He received his B.S (Electrical Engineering) degree from the National
Taiwan University in 1992 and his S.M and PhD degrees in 1994 and 1997 from Harvard University.
His research focuses on the simulation-based optimization, maritime logistics which includes port operations and the modelling and analysis for the logistics and supply chain system. His email address is
iseleelh@nus.edu.sg.
HAOBIN LI is a Scientist for the Institute of High Performance Computing, A*STAR Singapore. He
received his B.Eng degree (1st Class Honors) in 2009 from the Department of Industrial and Systems
Engineering at National University of Singapore, with minor in computer science; and Ph.D. degree from
the same department in 2014. He has research interests in operations research, simulation optimization
and designing high performance optimization tools with application on logistics and maritime studies. His
email address is lihb@ihpc.a-star.edu.sg.

2261

Ann Oper Res (2015) 231:105–127
DOI 10.1007/s10479-013-1480-7

Mathematical programming models for joint
simulation–optimization applied to closed queueing
networks
Arianna Alfieri · Andrea Matta · Giulia Pedrielli

Published online: 11 October 2013
© Springer Science+Business Media New York 2013

Abstract The optimization of stochastic Discrete Event Systems (DESs) is a critical and
difficult task. The search for the optimal system configuration (optimization problem) requires the assessment of the system performance (simulation problem), resulting in a
simulation–optimization problem. In the past ten years, a noticeable research effort has
been devoted to this area. Recently, mathematical programming has been proposed to integrate simulation and optimization for multi-stage open queueing networks. This paper
proposes the application of this approach to closed queueing networks. In particular, the optimal pallet allocation problem is tackled through linear mathematical programming models
for simulation–optimization.
Keywords Simulation–optimization · Mathematical programming · Loop manufacturing
systems · Discrete event systems

1 Introduction
Simulation–optimization of Discrete Event Systems (DESs) is typically carried out by using
two different and separate modules: a simulation module for the evaluation of the system
performance and an optimization module for the generation of the candidate solutions.
Several methods can be used for optimization (Fu et al. 2005; Healy and Schruben 1991;
Chick et al. 2003), as, for example, Response Surface Methodology (RSM) (Myers et al.
2009; Montgomery 2005), stochastic approximation (Kushner and Yin 1997), ranking and
selection (Boesel et al. 2003), meta-heuristics (Hong and Nelson 2006) and mathematical
programming (Robinson 1996). The output of the optimization module is a system configuration that is then given as input to the simulation module in order to evaluate its quality

A. Alfieri
Politecnico di Torino, Corso Duca degli Abruzzi 24, 10129 Torino, Italy

B

A. Matta · G. Pedrielli ( )
Politecnico di Milano, via Giuseppe La Masa 1, 20156 Milano, Italy
e-mail: giulia.pedrielli@mail.polimi.it

106

Ann Oper Res (2015) 231:105–127

in terms of feasibility and related system performance. The simulation module is usually a
discrete event simulator taking as input the outcome of the optimization module and generating the performance of the system as output (Law 2007). The system performance is the
input for the next iteration of the optimization module. This iterative procedure continues
until the optimal solution is found or a predefined stopping condition is met (Kleijnen 2008;
Spall 2003).
This loop usually requires a large number of iterations to reach a good solution and this
behaviour can be partially ascribed to the lack of system dynamics modelling within the
optimization module, justifying the presence of an external performance evaluator.
An alternative way to simulate a DES is to use mathematical programming (Schruben
2000). The system behaviour is described by a set of equations (constraints) and the solution
of the resulting mathematical model represents a single simulation run (Chan and Schruben
2008a).
The use of mathematical programming to simulate DESs naturally leads towards a deeper
integration between simulation and optimization. Indeed, if the system dynamics can be described by means of constraints, these can be embedded within the optimization model.
Under this perspective, the simulation module is used in sequence (and not in loop) with the
optimizer to provide the evaluation of the system performance given the optimal configuration. As a result, a single simulation-optimization iteration is needed to obtain the optimal
solution.
However, if optimization is considered, mathematical models are no longer Linear Programming (LP) models (as they usually are for simulation) and Integer Programming (IP)
models have to be considered with the related computational burden (Garey and Johnson
1979; Brodsky et al. 2003; Ming-Guang et al. 2002).
Approximate LP models to simulate and optimize DESs have been proposed to overcome
this difficulty (Matta 2008). In Alfieri and Matta (2012), an approximate representation of a
class of multi-stage production systems with finite buffer capacities is presented. The proposed approximation consists in modelling queues as Time Buffers (TB), i.e., temporal lags
between two events, instead of using the traditional Space Buffers. This approximation preserves the linearity of the mathematical model even when used for optimization purposes. In
addition, an analysis of the structural properties and relationship of the approximate models
with the corresponding exact formulations is proposed.
This paper extends the approximation developed in Alfieri and Matta (2012) to the simulation and optimization of closed queueing networks, which have been deeply studied in the
literature (Matta and Chefson 2005; Chan and Schruben 2008b; Dallery and Liberopoulos
2000; Maggio et al. 2009a; Gershwin and Werner 2007).
Specifically, approximate LP formulations are proposed to evaluate the performance of
closed queueing networks and to find the optimal configuration in terms of the number of
customers populating the line at any given time (constant in loop systems), i.e., the minimum
number of customers that allows to satisfy a predefined level of throughput. We refer to
this optimization problem as Pallet Allocation Problem (PAP). Although the PAP is not
combinatorial, as the Buffer Allocation Problem is (Dolgui et al. 2010), the stochasticity
affecting the arrivals and service times, the high correlation between the events that occur in
the stages of the line, and the non monotonicity of the throughput curve with respect to the
number of pallets (Maggio et al. 2009b; Gershwin and Werner 2007) make the modelling of
loop systems a difficult task.
In addition, the solution of the single loop PAP that we propose represents a first step
towards the solution of the multiple–loop case (Zhang 2006).
The rest of the paper is organized as follows: Sect. 2 describes the problem, the notation
and the main assumptions. Section 3 deals with the different types of blocking that can occur

Ann Oper Res (2015) 231:105–127

107

in closed networks. In Sects. 4 and 5 optimization and simulation models are presented,
respectively. Section 6 focuses on the algorithms designed to solve the proposed models.
The numerical results out of experiments from random generated problem instances are
commented in Sect. 7. Section 8 concludes the paper.

2 Assumptions and notations
A closed queueing network is a system in which the number p of customers (the loop population) is kept constant. This type of system appears frequently in factories. Manufacturing
processes which utilize pallets or fixtures can be viewed as loops since the number of pallets/fixtures circulating in the system remains constant. Similarly, control policies such as
CONWIP and kanban create conceptual loops by limiting the number of parts in the system.
Several contributions to the analysis and optimization of closed queueing networks can be
found in the literature, proving the relevance of this special class of discrete event systems
(Dallery and Frein 1989; Matta and Chefson 2005; Chan and Schruben 2008b; Bouhchouch
et al. 1993; Maggio et al. 2009a; Gershwin and Werner 2007).
The class of closed queueing networks analysed in this work considers J machines decoupled by J buffers with finite capacity (cj ) and a single customer type. Each machine j ,
exception made for the first stage, j = 1, has an upstream finite capacity buffer, j − 1. The
buffer J closes the network and it is the upstream queue of the first stage and the downstream
queue of the last server.
We will refer to customer and part interchangeably assuming that each customer requires
exactly one part.
Each customer i (i = 1, . . . , n) arrives at the system at time ai and can be processed by
the first machine only in case part i − p has been released from the last machine (being
p the number of pallets). As a result, the release time of part i in the loop system is the
maximum between its arrival time ai and the time when part i − p leaves the system. After
having been processed by the first machine, parts go to the second machine and so forth
until the last operation is performed on the last machine J . Once a part has completed the
last operation, it is stored in the buffer J and it is released only when a new customer is
available to be processed. We assume that no scheduling decision has to be considered, thus
part i + 1 is processed after part i.
Customer i has to wait in queue j − 1 if the machine in stage j is busy in serving another
customer k (with k < i).
Machines are modelled as perfectly reliable. The service times {tij } of each customer i
at each server j are known in advance from a random sampling or from a specific sample
path, whereas transportation times are considered negligible or already included in service
times.

3 Blocking phenomena in closed queueing networks
The closed queueing networks described in Sect. 2 can be affected by blocking phenomena
due to:
– Buffers with finite capacity;
– Constant number of pallets allowed.

108

Ann Oper Res (2015) 231:105–127

These two factors lead to three types of blocking phenomena: (1) deadlock, (2) server
blocked because the downstream buffer is full (server blocking), (3) part blocked from being
released from the system because of constant pallets (customer blocking). In the following
each of these phenomena is separately described.
Deadlock In case the number of pallets circulating in the system is equal to the total capacity of the buffers, we incur in the deadlock phenomenon. If this condition occurs, then
all servers are blocked. In order to avoid this situation, the maximum number of pallets P
that can circulate in the system is bounded by the following quantity:
P=

J


cj − 1.

(1)

j =1

Then at least a free position within the closed network has to be free in order to avoid this
blocking.
Server blocking Each customer will be allowed to enter and being processed on the j th machine only if the number of parts in the downstream stage is strictly less than the
capacity cj of the j -th queue. The capacity cj is discrete and it includes the customer in
service at stage j + 1. In this paper the Server Blocking is modelled assuming blocking
before service (BBS) control rule. This type of blocking was already considered in Alfieri
and Matta (2012) when modelling open queueing networks with finite buffer capacities and
stochastic processing times.
Customer blocking Parts to be released from the network can be blocked, i.e., forced to
wait in the last queue to keep the number of parts constant. This happens when no new
part is ready to enter the system when a part is completed. To model this particular type of
blocking, an additional virtual server, J + 1, was added to the J stages in the loop.
This server has processing time equal to 0 for each part and it is directly followed by the
first machine (without decoupling buffer). This machine is blocked only in case it holds a
customer and the first server is busy, i.e., Blocking After Service (BAS) rule is assumed for
the virtual server.
A physical interpretation of the virtual machine is that it synchronizes a part entering
the first stage with a corresponding customer leaving stage J , thus modelling blocking phenomena due to the constant number of customers in the line. Under a modelling perspective,
i.e., in the scope of developing the mathematical models for simulation–optimization, the
virtual machine enables the separation between the constraints modelling blocking due to
the finite buffer capacity and the constraints modelling the blocking ascribed to the constant
number of customers populating the system. This separation is fundamental to obtain the
linear approximation of the simulation–optimization models.
Figure 1 represents the “real” system, i.e., the network with J stages, and the “modelled”
system, i.e., the system with the additional virtual server. The two systems are “equivalent”
in the sense established by Definition 1.
Definition 1 Equivalent Systems The modelled system is equivalent to the real system if,
under the same sequences {ai } and {tij }, they produce exactly the same sample path.
Property 1 establishes the relationship between modelled and real system.

Ann Oper Res (2015) 231:105–127

109

Fig. 1 Equivalent loop representations

Property 1 Let cj and cj be the capacity for the last stage in the modelled and in the real
loop system, respectively.
The modelled and the real systems are equivalent iff cj = cj − 1.
Proof The last server in the modelled system, j = J +1, has no downstream queue, then it is
blocked only if the first machine is busy. Since, by definition of virtual server, ti,J +1 = 0, ∀i,
the customer entering the virtual machine is not forced to wait a predefined amount of time.
As a result, the virtual server can be interpreted as the last buffer slot of the network and
yi,J +1 is the time when part i leaves buffer J in the real system. Since the last buffer slot is,
with the virtual machine, considered separately, the total buffer slots downstream server J
are cj + 1. Hence, the modelled and real system are equivalent only when cj + 1 = cj . 

4 Optimization models
In this section, approximate LP formulations are proposed to find the optimal number of
pallets populating the line, i.e., the minimum number of customers that can circulate into
the system honouring a predefined target mean throughput ϑ that we estimate as:
ϑ̂ =

n−d
,
yn,J +1 − yd,J +1

(2)

where yn,J +1 is the finishing time of the last part at the virtual machine J + 1 in the simulated
sample path and corresponds to the time when the last customer leaves the system. The
parameter d represents the end of the system warm-up (Law 2007).
4.1 Exact optimization model
The pallet allocation problem can be formulated as the following IP model:
min

P

k=1

ek · zk · k

(3)

110

Ann Oper Res (2015) 231:105–127

s.t.
yi1 ≥ ai + ti1
yi+1,j − yij ≥ ti+1,j

(5)

∀i, j = 1, . . . , J

(6)

j = 1, . . . , J, i = 1, . . . , n − cj

yi+k,1 − yi,J +1 ≥ zk · ti+k,1 − (1 − zk )M
yi,J +1 − yi+k−1,1 ≥ −(1 − zk )M
μ̂ =

(4)

∀j, i = 1, . . . , n − 1

yi,j +1 − yij ≥ ti,j +1
yi+cj ,j − yi,j +1 ≥ ti+cj ,j

∀i

∀k, i = 1, . . . , n − k

∀k, i = 1, . . . , n − k

yn,J +1 − yd,J +1
≤ μ∗
n−d
P


zk = 1

(7)
(8)
(9)
(10)
(11)

k=1

The decision variables {yij } represent the finishing times of parts i = 1, . . . , n at servers
j = 1, . . . , J + 1 (J + 1 being the virtual server). The binary decision variable zk is equal to
1 if a number of pallets equal to k (k = 1, . . . , P ) is assigned to the line. The objective is to
minimize the total cost obtained multiplying k for the cost ek to have k pallets in the line. In
the following, without loss of generality, we will assume ek = 1, ∀k.
Constraints (4) do not allow the service of customer i at the first machine to finish before
its arrival time ai plus its service time. A machine cannot serve two consecutive customers
at the same time (5) and a customer cannot be processed by two different servers at once (6).
Constraints (7) prevent a customer to leave a machine if the immediate downstream buffer
is full.
Constraints (8) and (9) keep the number of parts in the system at a value k equal to the
number of pallets assigned to the line. These constraints are made redundant when zk = 0,
by subtracting the big–M from the right hand side. Constraints (7) are separated from (8)
and (9) thanks to the virtual machine as outlined in Sect. 3.
Constraint (10) bounds the performance measure to the limit μ∗ . Finishing times can
assume only positive values in the real domain because the arrival times are non negative
parameters.
Finally, only one value k can be chosen, as stated by Eq. (11).
4.2 Approximate optimization model
In order to approximate the IP optimization model described in Sect. 4.1, the time buffer
concept presented in Alfieri and Matta (2012) needs to be adapted to be applied to closed
queueing networks.
4.2.1 Time buffer for closed queueing networks
If p customers circulate in the line, two events are directly influenced by the value of p: the
start event of part i + p (γ in Fig. 2), and the finish event of part i + p − 1 (β in Fig. 2)
occurring at stage j = 1 at times xi+p,1 and yi+p−1 , respectively.
As a result, two different time buffer types, s and b, need to be introduced. The time
buffer s can anticipate xi+p,1 and b plays the same role with respect to yi+p−1,1 . The arrow
from node β to node i in Fig. 2 represents the influence between the departure time of

Ann Oper Res (2015) 231:105–127

111

Fig. 2 Closed loop system behaviour
Fig. 3 Time buffer sk with
k=γ −α

customer i from the system (yi,J +1 ) and the finishing time of customer i + p − 1 at the first
server (yi+p−1,1 ). The arrow from node i to node γ represents the influence of yi,J +1 over
xi+p,1 . Formally, given two customers, α and γ , to be served in the sequence α → γ , a time
length of s forces the starting time of customer γ at the first stage (denoted as xγ ,1 ) and the
finishing time of customer α at the last stage (denoted as yα,J +1 ) in the following way:
sγ −α ≥ yα,J +1 − xγ ,1 .
Indeed, customer γ can start s time units before customer α leaves the system (Fig. 3).
When s = 0, customer γ is forced to wait customer α to leave the system before entering,
i.e., no more than γ − α parts can circulate in the loop.
Given two customers α and β, with α < β < γ , a time length of b forces the departure
time of customer α from the system (denoted as yα,J +1 ) and the finishing time of customer
β at the first stage (denoted as yβ,1 ) in the following way:
b ≥ yβ,1 − yα,J +1 .
Customer α can exit b time units before customer β finishes its process in the first stage
(Fig. 4). When b = 0, customer α cannot be released until customer β leaves the first station,
i.e., at least β − α customers circulate in the loop.
It is worth mentioning that the time buffer concept is different from that adopted in manual assembly lines, mainly related to dimensioning the speed of the conveyor between two
adjacent machines. It also differs from the slack time in PERT graphs, which can be defined
as the time available between the estimated completion time of the job and its due date.
4.2.2 Time buffer model
In the approximate optimization model, the decision variable zk is replaced by variables sk
and bk . Consequently, we need to consider whether the minimization of sk or the minimization of bk has to be taken into account.

112

Ann Oper Res (2015) 231:105–127

Fig. 4 Time buffer bk with
k=γ −β

In the case we minimize sk , the approximate optimization objective function results as
follows:
min

P


sk .

(12)

k=1

The larger the decision variable sk is, the higher the importance of having a number of pallets
k in the system will be.
In case bk is the decision variable to be optimized, the following objective function must
be used:
min

P


bk .

(13)

k=1

As for sk , if bk is large the importance of having a number of pallets k in the system will be
high.
The approximate optimization model constraints can be easily devised from those defined
for the exact formulation (4)–(11) by removing constraint (11) and replacing (8) and (9) by:
yi+k,1 − yi,J +1 ≥ ti+k,1 − sk
yi,J +1 ≥ yi+k−1,1 − bk

k = 1, . . . , P , i = 1, . . . , n − k

i = 1, . . . , n − k + 1, k = 1, . . . , P

(14)
(15)

4.2.3 Stochastic time buffer model
When mathematical programming models are adopted for the simulation–optimization of
multi-stage open queueing networks (Alfieri and Matta 2012), a sample path based framework can be used to solve the approximate optimization model (Robinson 1996; Pedrielli
2013).
The approximate models presented in the previous sections have been solved running
multiple independent replications, obtaining a set of optimal solutions in terms of time buffer
s (or b, depending on the objective function adopted).
However, the interpretation of the results coming from the different replications represents an issue difficult to tackle (Healy and Schruben 1991).
As an example, we might perform the sample path optimization for R = 5 replicates,
obtaining R optimal solutions {Z∗1 , . . . , Z∗R }. In case the variables are integer, the average
solution, Z̄, might be infeasible, i.e., not integer. Moreover, even if the average solution
is integer, it might be such that μ̂r (Z̄) ≥ μ∗ , where μ∗ is the target performance, hence
resulting infeasible. The same considerations hold for the approximate optimization model.
We propose the application of stochastic programming (Birge and Louveaux 1997; Higle
and Sen 1996) to solve the PAP problem, tackling the described interpretation issue.

Ann Oper Res (2015) 231:105–127

113

Two-stage stochastic programming approaches are based on the separation of the set of
decisions variables in first-stage decisions, to be taken before the observation of any of the
uncertain elements, and second-stage decisions, to be taken after the occurrence of uncertain
events.
In our case, the first stage decisions correspond to the time buffers sk or bk , according
to the adopted objective function (i.e., min s or min b). The second stage decisions are represented by the finishing times {yij } and the time buffer which is not minimized at the first
stage.
Let Ω be the set of all the considered scenarios. Each scenario ω ∈ Ω is completely
defined by the realizations of the stochastic parameters {ai (ω)} and {tij (ω)}.
The variable ylate (we call it late since it represents the delay with respect to the finishing
time needed to reach the target performance) was introduced to link the first and second
stage decisions:


(16)
ylate (ω) = yn (ω) − yd (ω) − μ(n − d),
where μ = 1/ϑ . The expected value of the variable ylate (ω) is:




1  
E ylate (ω) =
yn (ω) − yd (ω) − μ(n − d) .
|Ω| ω∈Ω

(17)

This equation represents the difference between the expected target finishing time (μ(n −
d)), given as input, and the finishing time obtained from the optimization of the {yij (ω)} in
the second stage (yn (ω) − yd (ω)).

The first stage problem, minimizing sk , can be written as follows:
min
P




sk + α · E ylate (ω)

(18)

k=1

s.t.
sk ≥ 0 ∀k

(19)

The term α · E[ylate (ω)] avoids solutions in which the target finishing time constraint is not
satisfied (i.e., ylate (ω) > 0). The coefficient α is an input parameter representing the weight
of the stochastic part.
The second stage problem(s), one for each scenario, can be written as follows:


ylate (ω) = min yn (ω) − yd (ω) − μ(n − d)
s.t.
yi1 (ω) ≥ ai (ω) + ti1 (ω)

∀i

yi+1,j (ω) − yij (ω) ≥ ti+1,j (ω) ∀j, i = 1, . . . , n − 1
yi,j +1 (ω) − yij (ω) ≥ ti,j +1 (ω)
yi+cj ,j (ω) − yi,j +1 (ω) ≥ ti+cj ,j (ω)

∀i, j = 1, . . . , J

j = 1, . . . , J, i = 1, . . . n − cj

yi+k,1 (ω) − yi,J +1 (ω) ≥ ti+k,1 (ω) − sk
yi,J +1 (ω) − yi+k−1,1 (ω) ≥ −bk (ω)

i = 1, . . . , n − k, ∀k

i = 1, . . . , n − k + 1, ∀k

114

Ann Oper Res (2015) 231:105–127

All constraints are exactly the same as those presented in Sect. 4.2. The only difference is
that each variable and parameter
depends on the scenario ω.
If we minimize the function bk , the first and second stage models change as follows.
First stage problem
min
P




bk + α · E ylate (ω)

(20)

k=1

s.t.
bk ≥ 0

∀k

(21)

Second stage problem The second stage problem differs from the one previously presented
only in the constraints involving the time buffers that become:
yi+k,1 (ω) − yi,J +1 (ω) ≥ ti + k, 1(ω) − sk (ω)
yi,J +1 (ω) − yi+k−1,1 (ω) ≥ −bk

i = 1, . . . , n − k, ∀k

i = 1, . . . , n − k + 1, ∀k

The solution of the stochastic approximate optimization model is a unique time buffer [here
and now solution (Birge and Louveaux 1997)].
4.3 Optimization models properties
The structural properties of the optimization models presented in the previous section are
discussed in the following.
Proposition 1 There always exists an optimal solution in which {sk }k≤P is a non-increasing
sequence and {bk }k≤P is a non-decreasing sequence of non negative real values.
Proof Consider constraints (14); customer i and customer i + k are related through:
yi+k,1 − ti+k,1 ≥ yi,J +1 − sk .

(22)

On the other hand yi,J +1 must also satisfy:
yi+k+1,1 − ti+k+1,1 ≥ yi,J +1 − sk+1 .

(23)

Because of customer sequence constraints (5), the following condition must hold:
yi+k,1 − ti+k,1 ≤ yi+k+1,1 − ti+k+1,1 .

(24)

If sk ≤ sk+1 , it would allow customer i + k + 1 to start service, at stage 1, before customer
i + k. However, customer i + k must precede customer i + k + 1 (Eq. (24)); hence, the
additional time sk+1 − sk can never be used by customer i + k + 1 to start. As a result, there
always exists an optimal solution in which sk ≥ sk+1 .

The proof does not change in the case we minimize the function bk , therefore it is not
reported.


Ann Oper Res (2015) 231:105–127

115


Proposition 2 The solution obtained minimizing k sk corresponds, in the space domain,
to the one with the minimum number ofpallets.
The solution obtained minimizing k bk corresponds, in the space domain, to the one
with the maximum number of pallets.
Proof Consider the following inequalities obtained manipulating constraints (14) and (15):
−bk+1 ≤ yi,J +1 − xi+k,1 ≤ sk .

(25)


If k sk is minimized, the interval defined in (25) is bounded from above. In other words,
given a target performance μ, the solution p̃s we obtain minimizing the sum of sk is the
one with the lowest turnaround time, defined as the interval between the time xi1 when the
customer i enters the loop and the time yi,J +1 when it exits. Since the turnaround time is
monotonic with respect to the number of pallets in the system (Proposition 3, Sect. 5.3), the
solution with the lower turnaround time corresponds to the one with the lower number of
pallets.

If k bk is minimized, the opposite situation happens leading to the solution with the
higher turnaround time.


5 Simulation models
The models for optimization presented in Sect. 4 only provide an estimate of the average
throughput lower bound (Alfieri and Matta 2012). Indeed, the values {yij } obtained once
the models are solved are feasible but there is no guarantee that they are the optimal (i.e.,
minimized) since they do not appear explicitly in the objective function. As a result, if we
refer to Eq. (2), the value of ϑ is overestimated. In case the performance need to be precisely
assessed, simulation models have to be developed (Schruben 2000). In this section, the exact
and approximate models to simulate loop systems are presented together with their structural
properties.
Simulation models receive, as input, data characterizing the arrival process of customers
into the system {ai }, the service time of each customer on each machine of the closed network {tij } and the number of pallets populating the system (in terms of the integer value p
or time buffers). Specifically, pallets or time buffer values are assumed to be the results of
the solution of the optimization models (exact or approximate, respectively).
The output of the model are the finishing times {yij } of the customers on every machine
of the line, which are used to estimate the throughput of the system ϑ (using Eq. (2)).
5.1 Exact simulation model
A closed queueing network with J stages can be simulated by the following LP model (Chan
and Schruben 2008b).
min
n 
J

i=1 j =1

s.t.

yij

(26)

116

Ann Oper Res (2015) 231:105–127

(4)–(7)
yi+p,1 − yi,J +1 ≥ ti+p,1
yi,J +1 − yi+p−1,1 ≥ 0

i = 1, . . . , n − p

i = 1, . . . , n − p + 1

yij ≥ 0

(27)
(28)

i, j

Constraints (27) limit the maximum number of parts in the system to p, while constraints
(28) impose that there must at least p parts in the system: the i-th part is forced to wait for
the (i + p − 1)-th part to exit the first machine before leaving the system.
5.2 Approximate simulation model
The approximate mathematical model for simulation differs from the exact one only in constraints (27) and (28) that are replaced by (14) and (15), respectively. However, in case of
simulation, both {sk } and {bk } are known in advance (as the result of the optimization model)
and the only decision variables are the finishing times {yij }.
5.3 Simulation models properties
It is possible to univocally describe a sample path resulting from the solution of the simulation model by the tuple Q = (P , E ), where P and E represent the closed–loop system
configuration and the customer characteristics. Specifically, P represents the number of pallets in the system. In the case we refer to the approximate model P = {S , B}, where S is the
vector s containing time buffer capacities {sk } and B is the vector b containing time buffer
capacities {bk }. The quantity E is an n × (J + 1) matrix that contains the arrival times {ai }
and the service times {tij }.
The number of pallets p populating the system is strongly related to the customer
turnaround time defined as the time elapsing between the moment the customer enters the
network (xi1 ) until its departure (yi,J +1 ). This relationship is defined in Proposition 3.
Proposition 3 The turnaround time fi of customer i is a monotonic function of the number
of pallets p.
Proof Let the function fi be defined as follows:
fi = yi,J +1 − xi,1 .

(29)

Let p be the fixed number of customers circulating in the loop and assume that the capacity
of each buffer satisfies cj > n, ∀j , i.e., each buffer could host all the parts visiting the closed
system during the simulation run. In other words, we assume no deadlock or server blocking
can happen.
We can derive xi1 from the time xi+1,1 when customer i + 1 enters the system in the
following way:
xi1 = max{xi+1,1 − ti1 , ai }.

(30)

Assuming ai = 0 ∀i, the following holds:
xi1 = xi+1,1 − ti1 .

(31)

Ann Oper Res (2015) 231:105–127

117

Considering together equations (29) and (31), we can write:




k=p−1

fi = yi,J +1 − xi+p,1 −

	
ti+k,1 .

(32)

k=1

From constraints (27) and (28), yi,J +1 = xi+p,1 ; hence, Eq. (32) becomes:


k=p−1

fi =

ti+k,1

∀i.

(33)

k=1

As p increases, the function fi increases as well; hence, it is a monotonic function in the
number of pallets.

Since the throughput is an increasing and then decreasing function of the number of
pallets (Gershwin and Werner 2007), it follows from Proposition 3 that the throughput is
non monotonic with respect to the turnaround time. As a result, the same throughput ϑ can
be reached with different values of fi .
If the approximate model is considered, the time buffers {sk } and {bk } are characterized
in Remark 1.
Remark 1 Let Q = (S , B, E ) be an approximate formulation of a closed queueing network
sample path. The objective function value χ obtained solving the approximate simulation
model is a monotonic function in the time buffers sk and bk .
The time buffers can also be exploited to derive a configuration in terms of the (near) optimal number of pallets that populate the system. We will refer to this approximate solution
as p̃.
To compute p̃, we consider that, if sk (bk ) is positive, at least k pallets are needed in
the system. In addition, when approximate simulation is run, sk (bk ) directly influence the
values taken by the decision variables {yij }, which are used to compute p̃. Specifically, for
every part i, the condition yi,J +1 ≤ yi+k,1 is verified and the maximum value of k for which
the condition holds is collected as ki . Indeed, if this condition holds, at least k pallets need
to be allocated to the system. The approximate solution is then computed as:
p̃ = max ki
i

(34)

One of the most relevant differences between the exact and approximate models is that the
approximate models do not keep the number of parts in the system constant. The solution
p̃ will then be, in general, different from the solution we would obtain solving the exact
optimization model.

6 Algorithm
The procedure of single run simulation–optimization can be described as follows:
1. Initialization
(a) Set the parameters describing the system: number of buffers and machines (J ), buffer
capacities (cj );

118

Ann Oper Res (2015) 231:105–127

(b)
(c)
(d)
(e)

Set the simulation length as the number of parts to simulate (n);
Set the warm–up length (d);
Set the target throughput value ϑ ;
Generate the sample path, i.e., parameters tij and ai , from a predefined probability
distribution;
2. Approximate Optimization
Feed the mathematical model defined by equations (12) or (13), (4)–(7), (14)–(15) and
(10) with the input parameters from initialization. Solve the LP approximate optimization
model and store the arrays s and b.
3. Approximate Simulation
Feed the approximate simulation model with the following data:
– initialization data except for the target throughput (the same data used for the approximate optimization);
– optimal arrays s and b.
(a) Solve the approximate simulation model defined by equations (4)–(7) and (14)–(15).
Store the values of y1 (vector containing the finishing time of each part i at the first
machine) and yJ+1 (vector containing the finishing time of each part i at the last
machine).
(b) Compute the approximate integer solution using Eq. (34).
In the case stochastic programming approach is adopted, the procedure presented has to be
modified as follows:
– Initialization: define the number of scenarios |Ω| (the probability of each scenario will
be equal to 1/Ω) and generate the sample path, i.e., parameters tij (ω) and ai (ω), from a
predefined probability distribution for each scenario.
– Approximate optimization: feed the two-stage mathematical model. Solve the LP approximate optimization model and store the optimal vectors s and b.

7 Numerical results
Numerical experiments on random generated instances have been carried out on three test
cases. The simulation–optimization methodology is applied under both multiple replication
and stochastic programming approach. The same generated instances were also used to feed
standard simulation models developed in Arena© . The results obtained from standard simulation are the same as those obtained from the mathematical programming models, thus
validating the correctness of the mathematical formulations.
7.1 Experimental settings
The multiple replication optimization has been performed over sample paths characterized
by n = 5000, d = 2000 and different values of target throughput (ϑ ∗ ). For each target ϑ ∗ , 10
independent replications were run using min b and min s as objective function, alternatively.
For each replication, the exact optimization model was run to compute the optimal sample
path solution.
The stochastic programming approach was characterized by |Ω| = 20 independent scenarios (for each value of the target throughput ϑ ∗ ); each scenario was characterized by a
probability α = 0.05 and a number of parts equal to n = 3000, d = 1000.

Ann Oper Res (2015) 231:105–127

119

The customers are assumed to arrive at time ai = 0, ∀i, i.e., all customers are available
to be processed at time 0.
Table 4 contains the results of the approximate optimization with stochastic programming for all the experiment sets. In particular, column ϑ ∗ is the target throughput given as
input to the approximate optimization model, pI P represents the optimal solution obtained
solutions
obtained solvsolving the IP model. Columns p̃s and p˜b contain the approximate


ing the stochastic programming models when the function k sk or k bk is minimized,
respectively.
Columns ϑs and ϑb are the throughput obtained solving the approximate simulation

∗
∗
model,
 provided the optimal solution s and b (corresponding to the minimization of sk
and bk ), respectively. More specifically, ϑs and ϑb have been computed solving the approximate simulation model with a sample path size n = 31500, d = 5000.
For all the experiments, the CPU time has been about 2 minutes for a single run of
5000 parts in the multiple replicate case, and 5 minutes for the solution of the stochastic
problem.
7.2 Experiment set #1
The first experiment set is characterized by a number of machines J = 4. The buffer capacities are identical and equal to cj = 5, j = 1, . . . , 4. The processing times for each machine
were generated from an exponential distribution with parameters τ = 6 (representing the
mean), for stages j = 1, 2, 4, and τ = 7 for stage j = 3.
Exact simulation results The system has been studied solving the exact simulation model
for every pallet configuration, p = 1, . . . , P with P = 19. Numerical results are presented in
Fig. 5(a), showing the mean throughput ϑ (dotted line), together with the quadratic fit (plain
line), from the 10 independent replications. The half width related to the 95 % confidence
interval for the average throughput was estimated to be less than 0.0354. The maximum
throughput is achieved when the number of pallets allocated to the system is around 11
(based on the 10 sample paths).
Simulation–optimization results Three target throughput levels were tested: ϑ ∗ = (0.063,
0.106, 0.114).
Figure 5(b) represents the results obtained from the 10 replications of simulation–
optimization. The value of throughput ϑ (y axis) and the corresponding approximate number
of pallets computed from the optimization models (x axis) are reported. Both the minimization of s and b are considered. The red dots represent the throughputs obtained simulating
the optimal time buffer configuration when s is minimized (corresponding to the x value
p̃s ), whereas the green dots have the same meaning but refer to the minimization of b (corresponding to the x value p̃b ). The minimization of b, given the target throughput ϑ ∗ , leads
to a solution that is symmetric to the one obtained by minimizing s. More specifically, the
solution obtained minimizing b, for the same throughput, has the largest number of pallets. This result confirms Proposition 2. In addition, as the target throughput increases, approaching the maximum throughput, this difference almost vanishes. When ϑ = ϑ max and
the throughput curve has no flatness, there is only one configuration returning the required
value of the throughput. Hence, we expect that, for values of throughput approaching the
maximum, there is no difference between minimizing s or b.
Table 1 reports the exact integer solution obtained running the IP model (pI P ) for each
of the 10 replications and the approximate solutions obtained running the LP models for

120

Ann Oper Res (2015) 231:105–127

Fig. 5 Results of ES#1
(exponential distribution)


optimization
(p̃s being the result of the minimization of k sk , while p̃b refers to the minimization of k bk ). The target throughput and the throughput obtained from the simulation
are reported as well (ϑ ∗ , ϑ s and ϑ b , respectively). The approximate solution obtained minimizing s is always extremely close to the exact one. Moreover, as the required throughput
approaches the maximum, the approximate solutions coming from the minimization of b
are remarkably close to the exact optimum as well.
Table 4 reports the results obtained applying the stochastic programming approach (refer
to the set of rows ES#1). Also in this case, the solution is close to the exact one, showing
the robustness of the approach.
For the case ϑ ∗ = 0.063, the value of ϑb (the throughput obtained from the simulation
model when solved using the optimal time buffer solution b∗ ) is lower than ϑ ∗ , i.e., it does
not satisfy the throughput constraint. This cannot happen in the case simulation and optimization work on the same sample path. However, in the case of stochastic programming,
the optimization model (returning b∗ ) and the simulation model (returning ϑb ) work on
different samples and this explains the obtained result.
7.3 Experiment set #2
The second experiment set, as the first one, was designed with J = 4 machines. The buffers
capacities were set to c1 = 15, c2 = 5 and c3 = c4 = 10. The processing times were gener-

Ann Oper Res (2015) 231:105–127
Table 1 Approximate
optimization results with multiple
replication—experiment set ES#1

121
Run

ϑ

pI P

p˜s

ϑs

p˜b

ϑb

1

0.063

2

2

0.063

17

0.063

2

0.063

2

2

0.063

17

0.063

3

0.063

2

2

0.063

17

0.063

4

0.063

2

2

0.063

17

0.063

5

0.063

2

2

0.063

17

0.063

6

0.063

2

2

0.063

17

0.063

7

0.063

2

2

0.063

17

0.063

8

0.063

2

2

0.063

17

0.063

9

0.063

2

2

0.063

17

0.063

10

0.063

2

2

0.063

17

0.063

1

0.106

7

7

0.107

12

0.106

2

0.106

6

6

0.106

11

0.106

3

0.106

6

6

0.106

13

0.106

4

0.106

7

7

0.106

12

0.106

5

0.106

7

7

0.106

12

0.106

6

0.106

6

7

0.107

13

0.106

7

0.106

7

7

0.106

13

0.106

8

0.106

6

7

0.106

12

0.106

9

0.106

6

6

0.107

12

0.106

10

0.106

7

8

0.107

12

0.106

1

0.114

9

10

0.114

9

0.114

2

0.114

8

8

0.114

10

0.114

3

0.114

8

9

0.114

10

0.114

4

0.114

9

10

0.114

10

0.114

5

0.114

9

9

0.114

10

0.114

6

0.114

9

9

0.114

11

0.114

7

0.114

9

9

0.114

11

0.114

8

0.114

8

9

0.114

10

0.114

9

0.114

8

8

0.114

10

0.114

10

0.114

9

9

0.114

8

0.114

ated from a uniform distribution for every machine (τ1 = 0.5, τ2 = 1.5, being τ1 the mean
and τ2 the coefficient of variation).
Exact simulation results The system was studied solving the exact simulation model for
each pallet configuration. Figure 6(a) shows the mean throughput together with the quadratic
fit. The half width related to the 95 % confidence interval for the average throughput was
estimated to be less than 0.0157. The throughput curve, in this case, is flat and multiple
equivalent solutions (characterized by the same throughput) are present. In particular, solutions with p = (19, 20, 21, 22) pallets give the same average throughput.

122

Ann Oper Res (2015) 231:105–127

Fig. 6 Results of ES#2 (uniform
distribution)

Simulation–optimization results Three target throughput levels were tested: ϑ ∗ = (0.148,
0.268, 0.358). Figure 6(b) reports the results obtained applying the multiple replication approach.
Also in this case p̃s and p̃b are symmetric and this result confirms Proposition 2.
Table 2 reports the exact integer solution obtained running the IP model for each of
the 10 replications and those obtained from the LP models. As in ES#1, the approximate
solution obtained minimizing s is always extremely close to the exact one. Moreover, as the
required throughput approaches the maximum, the approximate solutions coming from the
minimization of b are closer to the exact optimum as well. However, because of the curve
flatness, the phenomenon is less evident than in ES#1.
Table 4 (block ES#2) details the results from the stochastic programming approach. It is
worth to comment the results obtained for ϑ ∗ = 0.358. In the third row, the solution p̃s = 25
is “far” from the pI P = 16. This is justified by the fact that this test case was characterized
by a throughput curve particularly flat. In cases the throughput has this behaviour, the time
approximation is always less effective.
7.4 Experiment set #3
The third test case is identical to the first experiment set, but the processing times were
generated from a uniform distribution (τ1 = 6, τ2 = 0.175 for j = 1, 2, 4 and τ1 = 7, τ2 =

Ann Oper Res (2015) 231:105–127
Table 2 Approximate
optimization results with multiple
replication—experiment set ES#2

123
Run

ϑ

pI P

p˜s

ϑs

p˜b

ϑb

1

0.148

2

1

0.148

38

0.148

2

0.148

2

1

0.148

38

0.148

3

0.148

2

1

0.149

38

0.148

4

0.148

2

1

0.148

38

0.148

5

0.148

2

1

0.148

38

0.148

6

0.148

2

1

0.148

38

0.148

7

0.148

2

1

0.148

38

0.148

8

0.148

2

1

0.148

38

0.148

9

0.148

2

1

0.148

38

0.148

10

0.148

2

1

0.148

38

0.148

1

0.268

4

3

0.268

37

0.268

2

0.268

4

3

0.269

37

0.268

3

0.268

4

3

0.269

37

0.268

4

0.268

4

3

0.268

37

0.268

5

0.268

4

3

0.268

37

0.268

6

0.268

4

3

0.269

37

0.268

7

0.268

4

3

0.268

37

0.268

8

0.268

4

3

0.268

37

0.268

9

0.268

4

3

0.268

37

0.268

10

0.268

4

3

0.268

37

0.268

1

0.358

15

15

0.358

22

0.358

2

0.358

15

19

0.359

22

0.358

3

0.358

14

16

0.359

26

0.358

4

0.358

16

17

0.358

31

0.358

5

0.358

13

13

0.358

31

0.358

6

0.358

13

14

0.360

30

0.358

7

0.358

18

18

0.358

29

0.358

8

0.358

13

16

0.359

30

0.358

9

0.358

13

13

0.359

28

0.358

10

0.358

14

15

0.360

29

0.358

0.175 for j = 3, being τ1 is the mean and τ2 the coefficient of variation). Notice that the
means of the processing times are exactly equal to those in ES#1.
Exact simulation results The system was studied solving the exact simulation model for
each pallet configuration. Numerical results are presented in Fig. 7(a), showing the mean
throughput and the quadratic fit. The half width related to the 95 % confidence interval for
the average throughput was estimated to be less than 0.01. Also in this case, as in the second
experiment set, the curve is flat. High throughputs are obtained in the range p = (6, . . . , 15).

124

Ann Oper Res (2015) 231:105–127

Fig. 7 Results of ES#1 (uniform
distribution)

Simulation–optimization results Three target throughput levels were tested: ϑ ∗ = (0.095,
0.114, 0.143). Results of the multiple replication are reported in Fig. 7(b). The approximate
solutions confirm the considerations already made for the other experimental settings.
Table 3 reports the exact integer solution obtained running the IP model for each of
the 10 replications together with the approximate solutions obtained from the LP models.
The approximate
and the exact solutions are again
 very close when the approximate model

minimizes k sk . In case of minimization of k bk , instead, a (near) maximum throughput
is required to have such closeness between approximate and exact solution.
Also in this case, Table 4 (block ES#3) shows that the solution obtained minimizing s is
close to the exact one.

8 Conclusion
In this paper we extended the simulation–optimization approach based on mathematical programming to closed queueing networks. The fundamental difference from previous works is
that two sets of continuous decision variables are needed to represent the customer blocking
behaviour.
With respect to the newly introduced decision variables, the models were analysed and
different possible objective functions were considered. The effects of the different objective
functions on the optimal continuous and integer solutions were studied.

Ann Oper Res (2015) 231:105–127
Table 3 Approximate
optimization results with multiple
replication—experiment set ES#3

125
Run

ϑ

pI P

p˜s

ϑs

p˜b

ϑb

1

0.095

2

2

0.095

17

0.095

2

0.095

2

2

0.095

17

0.095

3

0.095

2

2

0.095

17

0.095

4

0.095

2

2

0.095

17

0.095

5

0.095

2

2

0.095

17

0.095

6

0.095

2

2

0.095

17

0.095

7

0.095

2

2

0.095

17

0.095

8

0.095

2

2

0.095

17

0.095

9

0.095

2

2

0.095

17

0.095

10

0.095

2

2

0.095

17

0.095

1

0.114

3

2

0.114

17

0.114

2

0.114

3

2

0.114

16

0.114

3

0.114

3

4

0.114

16

0.114

4

0.114

3

2

0.114

17

0.114

5

0.114

3

2

0.114

17

0.114

6

0.114

3

2

0.114

17

0.114

7

0.114

3

2

0.114

16

0.114

8

0.114

3

2

0.114

17

0.114

9

0.114

3

2

0.114

16

0.114

10

0.114

3

2

0.114

17

0.114

1

0.143

5

5

0.143

15

0.144

2

0.143

9

10

0.143

13

0.167

3

0.143

9

10

0.143

14

0.143

4

0.143

10

10

0.143

13

0.143

5

0.143

10

9

0.143

10

0.143

6

0.143

10

9

0.143

15

0.143

7

0.143

10

9

0.143

14

0.143

8

0.143

10

10

0.143

13

0.143

9

0.143

10

10

0.143

14

0.143

10

0.143

10

10

0.143

14

0.143

The main drawbacks related to the multiple replications optimization approach were
highlighted and modifications to the developed models were proposed leading to a twostage stochastic programming approach. In both cases (multiple replication and stochastic
programming), the models for optimization and for simulation were integrated to define the
algorithm of simulation–optimization for loop systems.
Future research will be devoted to investigate the formal relationships between the approximate and the exact solutions and to develop different decomposition techniques to reduce the computational effort needed to solve the LP models, thus enhancing the possibility
to efficiently tackle larger instances. Moreover, the solution of the single loop PAP that we
propose represents a first step towards the solution of the multiple–loop case.

126
Table 4 Approximate
optimization results with
stochastic programming

Ann Oper Res (2015) 231:105–127
Experiment Set

ϑ∗

ES#1

0.063

2

2

0.065

17

0.056

0.106

7

7

0.109

11

0.11

0.114

11

10

0.132

11

0.114

ES#2

ES#3

pI P

p˜s

ϑs

p˜b

ϑb

0.148

2

1

0.151

37

0.148

0.268

4

3

0.268

35

0.270

0.358

16

25

0.360

28

0.359

0.094

2

1

0.095

17

0.094

0.114

3

2

0.114

17

0.114

0.143

10

9

0.143

14

0.143

References
Alfieri, A., & Matta, A. (2012). Mathematical programming formulations for approximate simulation of multistage production systems. European Journal of Operational Research. doi:10.1016/j.ejor.2011.12.044.
Birge, J., & Louveaux, F. (1997). Introduction to stochastic programming. Springer series in operations
research and financial engineering series. Berlin: Springer.
Boesel, J. N., Barry, L., & Kim, S. (2003). Using ranking and selection to “clean up” after simulation optimization. Operations Research, 51, 814–825. doi:10.1287/opre.51.5.814.16751.
Bouhchouch, A., Frein, Y., & Dallery, Y. (1993). Analysis of a closed-loop manufacturing system with finite
buffers. Applied Stochastic Models and Data Analysis, 9(2), 111–125. doi:10.1002/asm.3150090205
Brodsky, A., Pedersen, J., & Wagner, A. (2003). On the complexity of buffer allocation in message passing
systems. Journal of Parallel and Distributed Computing, 65(6), 692–713.
Chan, W., & Schruben, L. (2008a). Optimization models of discrete–event system dynamics. Operations
Research, 56(5), 1218–1237.
Chan, W. K., & Schruben, L. W. (2008b). Mathematical programming models of closed tandem
queueing networks. ACM Transactions on Modeling and Computer Simulation, 19(1). doi:10.1016/
j.jpdc.2004.10.009.
Chick, S., Schmeiser, B., Sánchez, P. J., Ferrin, D., Morrice, D. J., & Jin, J. (2003). Simulation-based retrospective optimization of stochastic systems: a family of algorithms. In Proceedings of the 2003 winter:
Vol. 1. Simulation conference, 2003 (pp. 543–547).
Dallery, Y., & Frein, Y. (1989). A decomposition method for approximate analysis of closed queueing networks with blocking. In Queueing networks with blocking (pp. 193–216).
Dallery, Y., & Liberopoulos, G. (2000). Extended kanban control system: combining kanban and base stock.
IIE Transactions, 32, 369–386.
Dolgui, A., Eremeev, A., & Sygaev, V. (2010). A problem of buffer allocation in production lines: complexity analysis and algorithms. In 3rd international conference on metaheuristics and nature inspired
computing, Djerba, Tunisia.
Fu, M., Glover, F., & April, J. (2005). In Simulation optimization: a review, new developments, and applications (pp. 83–95).
Garey, M., & Johnson, D. (1979). Computers and intractability: a guide to the theory of NP-completeness.
New York: Freeman
Gershwin, S. B., & Werner, L. (2007). An approximate analytical method for evaluating the performance of
closed-loop flow systems with unreliable machines and finite buffers. International Journal of Production Research, 45, 3085–3111.
Healy, K., & Schruben, L. W. (1991). Retrospective simulation response optimization. In Proceedings of the
23rd conference on winter simulation, WSC ’91 (pp. 901–906). Washington: IEEE Computer Society.
Higle, J., & Sen, S. (1996). Stochastic decomposition: a statistical method for large scale stochastic linear
programming. Nonconvex optimization and its applications. Norwell: Kluwer Academic.
Hong, L. J., & Nelson, B. L. (2006). Discrete optimization via simulation using compass. Operations Research, 54(1), 115–129.
Kleijnen, J. P. C. (2008). Design and analysis of simulation experiments. International series in operations
research & management science: Vol. 111. Berlin: Springer.
Kushner, H., & Yin, G. (1997). Stochastic approximation algorithms and applications. Berlin: Springer.

Ann Oper Res (2015) 231:105–127

127

Law, A. (2007). Simulation modeling and analysis (4th ed.). New York: McGraw-Hill.
Maggio, N., Matta, A., Gershwin, S., & Tolio, T. (2009a). A decomposition approximation for three-machine
closed-loop production systems with unreliable machines, finite buffers and a fixed populatio. IIE Transactions, 41(6), 562–574.
Maggio, N., Matta, A., Gershwin, S. B., & Tolio, T. (2009b). A decomposition approximation for threemachine closed-loop production systems with unreliable machines, finite buffers and a fixed population.
IIE Transactions, 41(6), 562–574.
Matta, A. (2008). Simulation optimization with mathematical programming representation of discrete event
systems. In S. J. Mason, R. R. Hill, L. Monch, O. Rose, T. Jefferson, & J. W. Fowler (Eds.), Proceedings of the 2008 winter simulation conference Piscataway (pp. 1393–1400). New Jersey: Institute of
Electrical and Electronics Engineers, Inc.
Matta, A., & Chefson, R. (2005). Formal properties of closed flow lines with limited buffer capacities and
random processing times. In Proceedings of the European simulation and modelling conference, Porto,
Portugal (pp. 190–194).
Ming-Guang, H., Pao-Long, C., & Ying-Chyi, C. (2002). Buffer allocation in flow-shop-type production
systems with general arrival and service patterns. Computers & Operations Research, 29(2), 103–121.
Montgomery, D. (2005). Progettazione e analisi degli esperimenti. Istruzione scientifica. New York:
McGraw-Hill.
Myers, R., Montgomery, D., & Anderson-Cook, C. (2009). Response surface methodology: process and
product optimization using designed experiments. Wiley series in probability and statistics. New York:
Wiley.
Pedrielli, G. (2013). Discrete event systems simulation–optimization: time buffer framework. PhD thesis,
Mechanical Engineering Department, Politecnico di Milano, Italy.
Robinson, S. (1996). Analysis of sample–path optimization. Mathematics of Operations Research, 21, 513–
528.
Schruben, L. W. (2000). Mathematical programming models of discrete event system dynamics. In J. A.
Joines, R. R. Bartona, K. Kang, & P. A. Fishwick (Eds.), Proceedings of the 2000 winter simulation
conference Piscataway (pp. 381–385). New Jersey: Institute of Electrical and Electronics Engineers,
Inc.
Spall, J. C. (2003). Introduction to stochastic search and optimization. New York: Wiley.
Zhang, Z. (2006). Analysis and design of manufacturing systems with multiple-loop structures. PhD thesis,
Mechanical Engineering Department, Massachussetts Institute of Technology, Berkeley.

Proceedings of the 2014 Winter Simulation Conference
A. Tolk, S. Y. Diallo, I. O. Ryzhov, L. Yilmaz, S. Buckley, and J. A. Miller, eds.

EVENT RELATIONSHIP GRAPH LITE: EVENT BASED MODELING FOR
SIMULATION–OPTIMIZATION OF CONTROL POLICIES IN DISCRETE EVENT SYSTEMS
Andrea Matta

Giulia Pedrielli

Shanghai Jiao Tong University
800 Dong Chuan Road
Shanghai, 200240, CHINA

National University of Singapore
12 Prince George’s Park
Singapore, 118411, SINGAPORE

Arianna Alfieri
Politecnico di Torino
24 corso Duca degli Abruzzi
Torino, 10129, ITALY

ABSTRACT
Simulation–optimization has received a spectacular attention in the past decade. However, the theory still
cannot meet the requirements from practice. Decision makers ask for methods solving a variety of problems
with diverse aggregations and objectives. To answer these needs, the interchange of solution procedures
becomes a key requirement as well as the development of (1) general modeling methodologies able to
represent, extend and modify simulation–optimization as a unique problem, (2) mapping procedures between
formalisms to enable the use of different tools. However, no formalism treats simulation–optimization as
an integrated problem. This work aims at partially filling this gap by proposing a formalism based upon
Event Relationship Graphs (ERGs) to represent the system dynamics, the problem decision variables and
the constraints. The formalism can be adopted for simulation–optimization of control policies governing a
queueing network. The optimization of a Kanban Control System is proposed to show the whole approach
and its potential benefits.
1

INTRODUCTION

Simulation–optimization theory and practice still have to converge for satisfying the needs of decision
makers, who try to solve complex problems in limited time (Fu 2002). At various stages of the decision
process, problems are characterized by remarkably different levels of detail. Moreover, the decision maker
considers different variables and performance to control as well as several costs/profits (Schruben 2010,
Schruben 2013). For instance, at the early stage of a decision process, rough models are enough for
identifying the most promising alternatives, notwithstanding the need to have more accurate representations
as the best designs need to be compared. New methods are needed to support the decision maker solving
this variety of problems. In light of this, the possibility to interchange solution procedures while solving
a specific problem would represent a spectacular advantage. However, there exist neither a simulation nor
an optimization tool which can serve such a purpose.
In fact, these needs reveal two main gaps that scientific research has not addressed yet: (1) modeling
gap substantiated in the lack of a general modeling methodology able to represent and easily extend/modify
simulation–optimization problems; (2) lack of mapping procedures to transform the simulation-optimization
model into different languages, thus enabling the use of multiple tools to solve it.
978-1-4799-7486-3/14/$31.00 ©2014 IEEE

3983

Matta, Pedrielli, and Alfieri
Concerning the modeling methodologies, most of the research was devoted to develop simulation
or optimization rather than simulation–optimization formalisms. Schruben (1983) proposed the Event
Relationship Graph (ERG), a general language for modeling and simulation of discrete event systems
(DESs). ERGs have been demonstrated to be able to simulate a Turing machine and have been successfully
applied to the evaluation of the performance of DESs. Moreover, ERG solving optimization problems
were proposed (Savage et al. 2005, Chan 2005). In Liu et al. (2012), an ERG is automatically generated
from real time data to first simulate and then optimize the system. The LEGO framework was proposed to
develop simulation model components using ERGs (Buss and Sanchez 2002). In the computing area, several
modeling techniques are used for simulation and property verification, but few have the ERG modeling
power and generality. An ERG can model a petri net but not vice versa. The well known GSMP (Generalized
Semi-Markov Process) and DEVS (Discrete Event System Specification (Zeigler 1976)) formalisms have
the same modeling power of ERG (Savage, Schruben, and Yücesan 2005) but they are not easy to use in
many practical applications.
State based formalisms (e.g., finite state automata) manifest their drawbacks when complex systems
composed of several components are modeled due to the state space growth, which is typically faster than
the increase in the number of events (Cassandras and Lafortune 2008, Cao and Zhang 2008). Also DEVS
and GSMP (Iglehart and Shedler 1983), despite their generality, suffer from the state space growth problem.
Concerning the mapping procedures, Chan and Schruben (2008) proved a general scheme to translate
ERG models into their mathematical programming counterpart. However, this was done in the scope of
simulation. The translation into mathematical programming was then extended to simulation–optimization
of multi–stage tandem queueing systems in Matta (2008), Alfieri and Matta (2012). Recently, Latorre and
Jiménez (2013) proposed a tree–based petri net model (modeling formalism) to solve a resource allocation
problem.
This paper explores the possibility of developing an event–based modeling language for DES simulationoptimization problems using ERGs. More specifically, a restricted class of ERGs (namely Event Relationship
Graph Lite, ERGL or ERG Lite) is used to simultaneously represent the dynamics of the DES together
with the constraints and the decision variables of the optimization problem. This integrated ERG contains
most of the information (the objective function can be implicitly included only in particular cases) needed
to estimate the performance and to solve the optimization problem(s) related to the DES underlying the
developed graph. This preliminary study deals with a subclass of ERGs. Because of this restriction, the
class of optimization problems under investigation is confined to the selection of the optimal control policy
for multi–stage queueing systems. Despite the current limitations of ERGLs in terms of expressiveness of
the formalism, the class of DESs and the problems they can model is vast and relevant in practice (e.g.,
buffer allocation problems, maintenance policies, production control policies, etc.).
The contribution of this work is twofold. The proposed simplified ERG is proven to be capable of
representing the useful information for solving the simulation–optimization problems of a controlled DES.
Either a simulation or a simulation–optimization model with different levels of approximation are generated
using mathematical programming. A second contribution is the clear distinction between system modeling
and control modeling. This paper formally defines the system object of the simulation–optimization as a
controlled ERGL, i.e., the union of a natural model (the DES without any control) and the control model
(the control mechanism added to govern the DES). The decomposability is only possible thanks to the use
of a unique formalism able to handle both structural and control information.
2

GENERAL NOTATION

The topology of the DES we consider can be represented by a queueing network with the set of servers
J = {0, . . . , J + 1} and the set of possible transaction routes for job i (i ∈ N = {0, . . . , n}) between servers,
represented by Qi = {( j, j0 )| j, j0 ∈ J} , ∀i. For each pair ( j, j0 ), the arc connecting j and j0 belongs to Qi
if and only if job i can directly flow from node j to node j0 . The source node, represented by index j = 0,
is the server j having no predecessors. The sink node is, instead, the server j having no successors and it

3984

Matta, Pedrielli, and Alfieri
is indexed by J + 1. The source node represents an infinite external arrival stream of customers, whereas
the sink node is the output gate through which jobs are released from the network.
A stage includes a single server, its upstream buffer and output buffer B j , both operating under a
specified control policy. Buffers may have either finite or infinite capacity. We consider a general setting
in which no explicit condition has to be imposed over the system layout, i.e., stage j is not the j−th on
which each job is processed, but it simply represents the stage label. Analogously, job i is not the i−th in
the sequence.
ξ
ξ
Let Ei j and ei j denote the events occurring in the system and their occurrence times, respectively, where
ξ ∈ T is the event type, and the pair (i, j) indicates the job i and the stage j the event refers to. We assume
that job i at stage j undergoes a process activity with duration bounded by a start event Eisj occurring at
time esij and a completion event Eifj occurring at time eifj ; the duration of the process is ti j and, in case of
stochastic DES, {ti j } may follow some known statistical distributions.
i
More
generally, the
n
o flow of each job i is determined by the occurrence of a set of events W =
ξ
ξ
Iξ
Ei j , ξ ∈ T, j ∈ J , i ∈ N. Each event Ei j in the set Wi has a set Wi j of the input events, i.e., triggering
Oξ

Iξ

events, and a set Wi j of the output events, i.e., triggered events. Notice that elements in the sets Wi j and
Oξ

Wi j might not be in the set Wi .
3

EVENT RELATIONSHIP GRAPH LITE
ξ

The DES previously described can be modeled through a graph whose set of nodes W is the set of events Ei j ,
whereas the set E of arcs represents the precedence relationships between events. According to Schruben
(1983), we can define a subclass of ERGs in the scope of the presented research.
Definition 1 (Event Relationship Graph Lite, ERGL) An ERGL is an oriented weighted graph where the
ξ
set of nodes W = {(i, j, ξ ), i ∈ N, j ∈ J, ξ ∈ T} contains the events Ei j occurring in the system. Each node
ξ

is assigned a value equal to the time ei j when the event occurs. Directed arcs connect different event


ξ
ξ0
pairs Ei j , Ei0 j0 and the set of arcs E = {((ξ , i, j) , (ξ 0 , i0 , j0 )) , i, i0 ∈ N, j, j0 ∈ J, ξ , ξ 0 ∈ T} represents the
ξij

precedence relationships between events. Each arc can be assigned a weight wξ 0 i0 j0 that can be continuous
(positive or negative) or binary.
Iξ

ξ

Oξ

In an ERGL, each event Ei j has a set of triggering events Wi j as well as a set of triggered events Wi j . In
 0

ξ
ξ
ξ 0 i0 j0
ξ0
Iξ
case of arc with continuous weight, the value of each node satisfies ei j ≥ maxξ 0 ,i0 , j0 ei0 j0 + wξ i j : Ei0 j0 ∈ Wi j .
 0

ξ
ξ
ξ 0 i0 j 0
ξ0
Iξ
This becomes ei j ≥ maxξ 0 ,i0 , j0 ei0 j0 · wξ i j : Ei0 j0 ∈ Wi j in case of binary connections. Concerning the set
E, if an arc is assigned a binary weight, we interpret the arc as active when the associated weight is equal
to 1 and deactivated otherwise. If an arc is deactivated, it does not establish any relationship between the
connected events. Arcs with continuous weights are always active arcs.
Remark 1 ERGLs are a simpler subclass of ERGs that does not contain conditional arcs and in which the
system state is implicitly defined by the event sequence. This characteristic makes ERGLs convenient to
represent queuing systems.
4

DESIGNING A CONTROL SYSTEM THROUGH ERGLs

ERGs have been proven to be spectacularly effective in the simulation of DESs. In this paper, using the
ERGL subclass, we extend their use to optimization. Under the ERGL representation perspective, modeling
a system corresponds to create and connect events, i.e., populating the graph. The system dynamics is then
a result of the relationships between events in the ERGL.

3985

Matta, Pedrielli, and Alfieri
ξ

ξ0

ξ

ξ0

Definition 2 (Connected Events) Let ei j and ei0 j0 be the times when the events Ei j and Ei0 j0 occur, respectively.
 0 



ξ
Iξ
Oξ V ξ
Iξ 0
Oξ 0
These two events are connected if and only if Ei0 j0 ∈ Wi j ∪ Wi j
Ei j ∈ Wi0 j0 ∪ Wi0 j0
. In particular,

 0
0
0
V
ξ
Iξ
ξ
Oξ
ξ
ξ
if Ei0 j0 ∈ Wi j Ei j ∈ Wi0 j0 , the connection establishes that event Ei0 j0 can trigger event Ei j . Analogously,


ξ
Iξ 0 V ξ 0
Oξ
ξ
ξ0
if Ei j ∈ Wi0 j0 Ei0 j0 ∈ Wi j , event Ei j can trigger event Ei0 j0 .
In the following sections, we investigate how to create a basic ERGL model (referred to as natural
model) and how to extend it to include control mechanisms.
4.1 Natural Event Model
We define as natural a system that evolves solely according to its physical constraints. In this system,
entities are supposed to flow according
to their process
plan. Referring to definition 1, this system can be

S  Iξ
Oξ
i
mapped into a graph in which ξ , j Wi j ∪ Wi j ⊆ W , ξ ∈ T, j ∈ J, i.e., only events related to the same
job are connected. This reflects the assumption that the natural knowledge is related to the processes each
job has to undergo, and no control policy has been established yet. The arcs of the natural ERGL can be
weighted only through continuous weights.
Figure 1 reports the graph for a serial multi–stage line. Only the events related to the same job are connected
(i.e., event Eisj precedes event Eifj and event Eifj precedes event Ei,s j+1 ). This example shows that, as long as
no sequence is defined between jobs, the natural ERGL is a disconnected graph. A direct consequence of
this graph feature is that the natural model cannot be used to estimate the performance of the underlying
system.

Figure 1: Single server multi–stage tandem line: the natural graph.

4.2 Control Event Model
The natural model represents a system that evolves accordingly to the process sequences assigned to each
job. However, any system needs a control to enable the entities flow. Considering the general event–based
model in definition 1, a controlled ERGL can be defined as follows.
Definition 3 (Controlled ERGL) Given a set WN of natural events, a controlled ERGL is an ordered set
WCN of events that contains all the elements in WN ⊆ W and adds the set WC ⊆ W of control events.
Elements in WCN are connected through natural arcs (EN ⊆ E) and control arcs (EC ⊆ E). Control arcs
ξij
are directed arcs associated to either continuous weight sξ 0 i0 j0 , referred to as time buffer, or binary weight,
ξij

with κξ 0 i0 j0 indicating the associated binary value.
Definition 3 implies that a control policy can be represented within an ERGL only if it can be mapped
into a unique event sequence. This assumption is valid for a large category of state–based policies as
long as artificial control events are created to represent the system reaching an observable state. However,
this assumption fails for more complex time–dependent policies that are based on measures not directly
3986

Matta, Pedrielli, and Alfieri
related to a unique system state. In this case, more complex formalisms (such as the full ERGs) should be
considered. GSMPs also allow to model such more complex situations. However, if GSMPs are considered,
the state based modeling leads to an impractical increase in the number of nodes in the resulting graph.
It is clear that the design of a controlled ERGL requires the insertion of new active control arcs EC
between already existing event times and, possibly, the insertion of new events (control events, WC ).
Control arc insertion The representation of a sequencing or routing control policy only requires
the addition of connections between times of natural events to enable their connection according to the
order established by the policy. As an example, consider the natural model in Figure 1. A sequencing rule
forcing job with label i to be processed before job labeled i + 1 at each stage j (for each i and j) will add
arcs between nodes eifj and esi+1, j , as depicted in Figure 2(a).
Control nodes insertion The addition of new arcs is not sufficient when the adopted control policy
does not only deal with the order of events, but further new synchronization mechanisms (conditional
arcs in the full ERG) need to be added to modify the flow of entities. A typical example are blocking
based control policies generated by finite capacity buffers in the line, as well as kanban tokens. In these
cases, the introduction of control events is required. As an example, consider a multi–stage line in which
a maximum capacity of c j entities is assigned to the j–th stage. A release event is needed to block the
occurrence of the (natural) start event of job i + c j at stage j (esi+c j , j ) until the release of the job i to the
stage j + 1 has occurred, i.e., the control release event eri, j+1 is responsible for triggering the control event
eri+c j , j that, eventually, triggers the natural event esi+c j , j . Under a state–based modeling perspective, this
means to prevent job i to enter the queue of stage j if the queue level is equal to c j , i.e., the buffer is full.
The addition of a new control event implies adding new control arcs to connect the added node with the rest
of the graph. This can also require the replacement of a natural arc with new control arcs. An additional
example is depicted in Figure 2(b), where thick arrows refer to newly added control arcs, thick circles to
newly added control events, and dashed arrows refer to removed natural arcs. In the natural model, the
starting event time of job i at stage j (esij ) can occur only after the finishing event time for job i − 1 at
f
the same stage (ei−1,
j ). The new control mechanism breaks this connection. It is the new control event
f
f
f ,i−k,h
s
ecij , triggered by the natural events ei−1,
j and ei−k,h , to trigger the event ei j . The continuous weight sc,i, j
f
on the control arc (( f , i − k, h) , (c, i, j)) forces a delay between ei−k,h
and ecij . As a result, according to
definition 1, job i can start only at the maximum between the finish of job i − 1 at the same stage and the
f ,i−k,h
finish time of job i − k at stage h plus the interval sc,i,
j .

(a) Single server multi–stage tandem line: sequenced jobs.

(b) Example of replacing natural mechanisms.

Figure 2: Adding a control mechanism to the ERGL.

The complexity of the proposed ERGL is strongly related to the type of connections that need to be
defined between event times. In general, the growth of the number of nodes in the model is linear in the
number of servers, jobs and policies. The same does not hold for the arcs, whose growth is linear only
in the number of servers and considered policies, while it is polynomial with order larger than 1 in the
number of jobs.
3987

Matta, Pedrielli, and Alfieri
The structure of the ERGL can be used as a means to determine whether a specific control policy might
generate deadlocks in the system, or simply result ineffective as it does not modify the entity flow deriving
from the natural graph. These two aspects are shortly investigated in the remainder of the section.
Deadlock detection In a system modeled through ERGL, a deadlock can occur when a cyclic
relationship between nodes exists (necessary condition). Hence, checking if a control mechanism may lead
to a deadlock means to check if the insertion of new arcs and/or new nodes (and related arcs) creates a
cycle in the previously acyclic graph.
Redundancy detection We define a control mechanism as redundant when the resulting ERGL is
characterized by duplicated connections between the same event pairs. In such cases, the added control
does not change the entities flow and it is then ineffective in controlling the system dynamics.
Infeasibility and redundancy can be efficiently verified using the concept of transitive closure (van
Leeuwen 1990, Aho, Garey, and Ullman 1972, Habib, Morvan, and Rampon 1993) guaranteeing the
detectability of deadlocks and redundancies also in complex cases.
The obtained ERGL considers both the natural and controlled dynamics of the system. The natural
dynamics depends on the defined process sequences, whereas the controlled dynamics depends on the
policies introduced to govern the flow of entities. Different control policies lead to different ERGLs
structures, i.e., different control event set WC and control arc sets EC and different weights s and/or κ.
The indisputable advantage of the proposed approach is that an ERGL can be considered as an optimization
model when WC , EC and associated weights s and κ are interpreted as sets of decision variables. The next
section investigates this aspect.
5

OPTIMIZING A CONTROLLED ERGL

The optimization of a control system can be performed, according to the classical simulation–optimization
approach, through a search module generating the candidate values θ for the parameters of the control
policy to be optimized, and using simulation as a black–box tool for estimating an implicit function l.
Using the mathematical programming formalism, we can formulate the optimization problem P as:
P : min
θ ∈Θ

l(θ )

(1)

where θ is the vector of decision variables and Θ defines the constraint set on θ (Fu, Glover, and April 2005).
If stochastic systems are considered, function l (θ ) can be interpreted as an expectation, i.e., l = E[L(θ , Ω)],
where Ω represents a probability space governing the stochastic processes that characterize the system
(e.g., inter arrival times, processing times). Furthermore, the problem could be constrained to satisfy a
predefined level of some performance measures (e.g., satisfaction of the service levels, throughput targets).
The approach we propose is not in contrast with this framework: the solution generated by the search
procedure θ can be mapped onto control events and arcs (and related weights) following the rules discussed
in section 4. The obtained ERGL has all the information for estimating function l, i.e., it behaves as a pure
simulation model and, as such, it can be used to obtain an estimate of the system performance (Savage,
Schruben, and Yücesan 2005).
However, the advantage of the presented modeling approach relies in the possibility to interpret control
nodes, arcs and weights as decision variables in an optimization problem instead of fixed input parameters
to be received from an external search procedure. This interpretation exploits the completeness of the ERG
formalism and makes the ERGL a simulation–optimization modeling language.
Under this new perspective,
corresponds to the search of the best set
o optimization of a control policy
n o
n the
ξ
C = Eξ
of control events W
i j and related occurrence times ei j , as well as the set of arcs (and related
weights) such that the ERGL has the best associated value for the selected objective function.
Such a simulation–optimization model can be “solved” in many ways based on the characteristics of
the objective function (1) (Chan and Schruben 2008). In this paper, we propose the use of mathematical

3988

Matta, Pedrielli, and Alfieri
programming. In particular, we introduce the way to, automatically, map ERGLs in a set of equations
deriving the integrated mathematical programming model for simulation-optimization.
5.1 Objective functions
When all the control nodes, arcs and related weights are established, the optimization problem related to
the ERGL can be simply brought back to a simulation problem and the objective function is that proposed
in Chan and Schruben (2008):
min

∑

eυ

(2)

υ∈W

The objective function (2), together with the constraints generated from the ERGL (see section 5.2), defines
a mathematical programming representation of the simulation model of the system. Advantages of this
formulation have been investigated in Chan and Schruben (2008) and Matta (2008).
The problem is more challenging when either weights, arcs or nodes have to be determined. In particular,
two main cases can be distinguished: (1) optimization of a given control policy, (2) identification and
optimization of the best control policy. In both cases, the event times are variables to be optimized as in
the simulation case.
Optimization of a given control policy If all nodes and arcs have already been established, the
problem can be brought back to the choice of the set of optimal arc weights (the control policy parameters).
A general objective function can be defined as follows:
f (e) + h(s) + g(κ)

min

(3)

where f , h and g are real functions of the event times e, the time buffers s (if present), and the boolean
activations κ (if defined in the ERGL), all characterized according to definition 3. Depending on the type
of considered events, several objective functions can be defined. In case κ is defined, the problem is a
MILP. In the simple case in which f , g and h are summations we obtain:
min

∑

αυ eυ +

∑

(βν sν + γν κν ) ,

(4)

ν∈EC

υ∈W

where αυ , βν and γν are known function coefficients. Function (2) is a special case of (4) for βν , γν = 0,
∀ν, and corresponds to minimizing a function that depends only on the event times. The case αυ = 0, ∀υ,
corresponds, instead, to the minimization of a function of the control parameters (s, κ).
Identification and optimization of the best control policy If the design of the ERGL is not given,
the optimal control mechanisms WC have to be selected together with the control parameters s and/or
κ making the problem more complex. Similarly to the previous case, we can use the general objective
function in equation (3). The main difference with the previous case relies in the need to add the activation
binary decision variables which take value 1 if the event e is added, the arc with continuous weight s is
chosen or the binary connection with weight κ is included in the ERGL. It is clear the need of these new
elements to create the ERGL: in the previous model the same elements were implicitly contained within
the sets defining the graph structure. We can notice that this problem is always a MILP thus making its
solution more challenging.
The control of DES performance can also be included in the form of stochastic constraints forcing the
system to achieve a configuration that meets a predefined target. An expected performance is a function of
event times e and control variables s and κ. Examples of stochastic constraints are the expected value of
customers waiting in queue forced to be lower than a threshold, the expected system throughput in a shop
floor or the expected service level in a serial supply chain forced to be greater than a predefined value.
Being concerned with optimization, stochastic constraints are not treated in Chan and Schruben (2008),
Chan (2005). Performance constraints are modeled by introducing the following relationship:

∑

p (eν ) ≤ ϑ ∗ ,

ν∈WC

3989

Matta, Pedrielli, and Alfieri
where ϑ ∗ is the target performance and p is any function of the control event times.
5.2 From ERGL to mathematical programming constraints
The constraints characterizing the optimization model can be partitioned into two categories: linear dynamics
constraints and control constraints.
Linear dynamics constraints.
These constraints involve only continuous variables and map the
relationships established by the natural arcs in the control model. An example of this type of constraints
is the following:
eifj ≥ esij + ti j ,
stating that customer i cannot leave stage j (eifj ) before accessing the server (esij ) and completing the service
(ti j ). The same relationships were proposed within the LP formulations by Chan and Schruben (2008).
The right hand side is a vector of values that usually are realizations of random variables and represents
the weight of the arc connecting the nodes esij and eifj in the example.
A procedure to automatically generate the nodes and arcs referring to linear dynamics constraint in the
ERGL was proposed in Chan and Schruben (2008), Chan (2005). Thus, we refer to these works for
translating the natural model.
Control constraints. Control constraints relate the occurrence times of events in presence of control
variables and derive from the translation of control arcs and weights (i.e., control parameters). Control
parameters can be either discrete or continuous and, when discrete, their translation leads to non linear
constraints. Also for this type of constraints, a general procedure to translate the graph into the mathematical
programming model has been proposed in Chan and Schruben (2008), Chan (2005). However, their models
only refer to simulation, hence control arcs are treated as input parameters instead of decision variables.
An example of this type of constraints is the following:


ξ0
ξ
ξij
(5)
ei0 j0 ≥ ei j − q wξ 0 i0 j0 ,
ξ0

where ei, j and ei0 j0 are the time occurrence of two events relating job i on stage j and job i0 on stage j0
ξ

ξij

that are linked by control q(wξ 0 i0 j0 ). If the relationship between the two event times is boolean, function q
ξij

ξij

has the form (1 − κξ 0 i0 j0 ) · M, where κξ 0 i0 j0 is a binary decision variable and M is a large number. Instead,
ξij

in case of continuous relationship, q is a function of the continuous variable sξ 0 i0 j0 time buffer.
6

APPLICATION: KANBAN CONTROL SYSTEM (KCS)

A three–stage queueing network managed by a kanban policy is represented in Figure 3. Each stage j

Figure 3: Three–stage queueing network with Kanban Control System.

( j = 1, . . . , 3) is composed of a server (represented by a circle) with an incoming infinite capacity buffer
3990

Matta, Pedrielli, and Alfieri
and a synchronization station consisting of two queues: K j+1 containing the kanban tokens of stage j + 1,
and B j containing the finished parts from stage j. At the last stage, D3 contains the external demands
(Liberopoulos and Dallery 2000). A fixed and discrete number of kanban tokens K j ∈ K j = {K Lj , . . . , KUj }
is associated to stage j.

Figure 4: Kanban control system: event graph.

The natural ERGL related to the KCS is a multi–stage tandem line with fixed job sequence and infinite
buffer capacities like the one depicted in Figure 2(a). Specifically, the nodes are: esij , ∀i, j, the time when the
start event Eisj occurs, and eifj , ∀i, j, the time of the finish (departure) event Eifj . Each node esij is connected
to a node eifj through a natural delay arc whose weight is the service time of job i at stage j, ti j . Event Eifj
f
s
s
can trigger the start event Ei+1,
j , if there are queueing jobs. As a result, nodes ei+1, j and ei j are connected.
Assuming a known process sequence, job i starts being processed on stage j + 1 once the activities at
server j are completed (dashed arrow connecting eifj and esi, j+1 , Figure 4). When KCS is considered, these
dashed connections
n oneed to be “broken down” to be replaced by control connections and a new set of
control events erij model the release of jobs i = 1, . . . , n from stage j − 1 to stage j. This new control
event breaks the aforementioned connection replacing it with two control arcs (( f , i, j) , (r, i, j + 1)) and
((r, i, j + 1) , (s, i, j + 1)). The resulting graph has the control nodes and arcs with thick borders in Figure
4 and, as a result of the control model, each customer, after being processed by stage j, can be released
to the next stage only if a free kanban is available. As an example, if a single kanban token is assigned
to stage j, customer i + 1 can be released to stage j only if customer i has already been released to the
next stage. Nodes eri, j+1 and eri+1, j are then connected through a control arc whose “weight” is the binary
 D	
r,i, j+1
variable κr,i+1,
represent the demand signal constraining each job
j = 1. Finally, the control nodes ei
not to leave the system before the related demand has occurred. The described connections (arcs) can be
mapped, as described in section 5, to the following constraints:

eifj − esij ≥ ti j

∀i, j

(6)

esi+1, j − eifj ≥ 0
eri, j+1 − eifj ≥ 0
esij − erij ≥ 0
eri,J+1 ≥ eD
i

∀i, j

(7)

∀i, j

(8)

∀i, j

(9)

∀i

(10)



r,i, j+1
eri+k, j − eri, j+1 ≥ 1 − κr,i+k,
j ·M

∀ j, k ∈ K j , i = 1, . . . , n − k

(11)

j+1
eri+k, j − eri, j+1 ≥ −sr,i,
r,i+k, j

∀ j, k ∈ K j , i = 1, . . . , n − k

(12)

3991

Matta, Pedrielli, and Alfieri
To optimize the described system, we considered the following objective functions:


Z I = min ∑ ∑ esij + eifj + erij

Z II = min ∑ ∑
κ

j∈J i∈N

Z

III

= min ∑ ∑
s

∑

j+1
sr,i,
r,i+k, j

Z

IV

j∈J i∈N k∈K j

∑

r,i, j+1
κr,i+k,
j ·k

j∈J i∈N k∈K j

= min ∑ ∑

∑

αi jr eri, j+1 − eri, j



j∈J i∈N k∈K j

Function Z I minimizes the sum of all the event times without constraining the number of kanban tokens
(the time buffer). Functions Z II and Z III minimize the total number of kanban tokens and the amount
of time buffer allocated to each stage of the line, respectively. Function Z IV minimizes the cost of the
waiting time at each stage. Since we consider identical jobs, αi jr depends only on stage and event type,
i.e., αi jr = α jr , ∀i. We added the following constraint to control the lateness of the jobs:

1
∗
eri,J+1 − eD
∑
i ≤ϑ
n i∈N

(13)

Furthermore, we set the event times related to the arrival of the demand for job i at the times tiD , which is
D
the realization of a random variable representing the demand process, i.e. eD
i = ti , ∀i.

(a) Average service level.

(b) Average waiting time.
Figure 5: Objective functions: the comparison through the “benchmark.”

Processing times were assumed lognormal and identical for each stage (µ = 2.73, σ = 0.274) as demand
inter arrival times (µ = 3.4, σ = 1.624). The target performance was set to ϑ = 0.1 corresponding to a
91% service level (i.e., the ratio between the demand on time and the total demand). Figures 5(a)–5(b)
report the value for the service level and the average total waiting time obtained over 20 independent
simulation–optimization replications (x-axis corresponds to the replication), respectively. It is clear how a
“dominant” objective function exists with respect to both measures. In Figure 5(a), the system with the
highest service level is obtained from objective function Z I . This objective function has no practical use
in terms of policy choice since its implementation would require the control of the time of all the events,
3992

Matta, Pedrielli, and Alfieri
which is impossible in a stochastic setting. Nonetheless, such a “policy” provides an upper bound on the
service level performance, hence constituting the benchmark on this measure. Another useful benchmark
is given by function Z IV that provides quite a good lower bound on the service level. This is expected
since the minimization of the waiting times (and inventory costs consequently) is negatively correlated to
the service level. The two systems having minimum time buffer and minimum number of kanbans are in
between, with kanban (i.e., Z II ) performing better.
Figure 5(b) reports the results on the average system waiting time (the process time ti j is not included in
the statistic). Also in this case, we can identify a benchmark policy, i.e., Z IV which is also the worst policy
in terms of service level. Moreover, the stability of the response over different sample paths suggests Z IV
is a good estimate of the minimum expected waiting time in the system. However, as policy Z I , Z IV cannot
be implemented in practice since it requires to control the waiting time between each job pair, implying
perfect knowledge of the, stochastic, service times. Also in this case, the time buffer and the kanban
control policies are in between. Another interesting aspect can be noticed: from Figure 5(b), the kanban
policy appears to generate an unstable signal with picks in some replications. This is due to the fact that
the number of needed kanban tokens can only change through discrete steps. For particular realizations
of the stochastic variables, the number of kanban tokens has to rise from K to K + 1, resulting in a jump
in the system performance. This phenomenon is mitigated when the time buffer policy is in place as
the continuous time buffer can react with small changes to the external conditions avoiding picks in the
response.
7

CONCLUSIONS

A new modeling approach has been presented that relies on a graph based system representation. We define
a subclass of ERG (ERG Lite), downsizing the level of complexity of the category of systems we can
represent. This enables the integration of the description of the system dynamics and the optimization of
the control policy that governs the system. Each controlled ERG Lite is meant to be the union of a natural
model and a control model, enhancing the modularity and flexibility of the proposed language. A mapping
procedure is proposed to transform the ERG Lite model into the mathematical programming counterpart.
The advantage of this approach relies in the possibility to interpret arcs and nodes in the ERG Lite as
decision variables, thus integrating the simulation and the optimization in the same model. Hence, solving
the simulation–optimization model can be interpreted as identifying the “best” ERG Lite with respect to
some specified objective. The presented methodology is applied to a multi–stage kanban controlled system
to show its effectiveness in generating and solving different simulation–optimization problems.
REFERENCES
Aho, A., M. Garey, and J. Ullman. 1972. “The Transitive Reduction of a Directed Graph”. SIAM Journal
on Computing 1 (2): 131–137.
Alfieri, A., and A. Matta. 2012. “Mathematical Programming Formulations for Approximate Simulation
of Multistage Production Systems”. European Journal of Operational Research 219 (3): 773–783.
Buss, A. H., and P. J. Sanchez. 2002. “Modeling Very Large Scale Systems: Building Complex Models with
LEGOs (Listener Event Graph Objects)”. In Proceedings of the 2002 Winter Simulation Conference,
edited by E. Y ucesan, C.-H. Chen, J. L. Snowdon, and J. M. Charnes, 732–737. Piscataway, New
Jersey: Institute of Electrical and Electronics Engineers, Inc.
Cao, X.-R., and J. Zhang. 2008. “Event-Based Optimization of Markov Systems”. IEEE Transactions on
Automatic Control 53 (4): 1076–1082.
Cassandras, C. G., and S. Lafortune. 2008. Introduction to discrete event systems. Springer.
Chan, W., and L. Schruben. 2008. “Optimization models of Discrete–Event System Dynamics”. Operations
Research 56 (5): 1218–1237.

3993

Matta, Pedrielli, and Alfieri
Chan, W. K. 2005. Mathematical Programming Representations of Discrete-Event System Dynamics. Ph.
D. thesis, University of California, Berkeley.
Fu, M. 2002. “Optimization for Simulation: Theory vs. Practice”. Journal on Computing 14 (3): 192–215.
Fu, M., F. Glover, and J. April. 2005. “Simulation optimization: a review, new developments, and
applications”. In Proceedings of the 2005 Winter Simulation Conference, edited by M. E. Kuhl,
N. M. Steiger, F. B. Armstrong, and J. A. Joines, 83–95: Piscataway, New Jersey: Institute of Electrical
and Electronics Engineers, Inc.
Habib, M., M. Morvan, and J.-X. Rampon. 1993. “On the calculation of transitive reduction—closure of
orders”. Discrete Mathematics 111 (1-3): 289–303.
Iglehart, D. L., and G. S. Shedler. 1983. “Simulation of non–Markovian systems”. IBM Journal of Research
and Development 27 (5): 472 – 479.
Latorre, J.-I., and E. Jiménez. 2013. “Simulation-based optimization of discrete event systems with alternative
structural configurations using distributed computation and the Petri net paradigm”. SIMULATION 89
(11): 1310–1334.
Liberopoulos, G., and Y. Dallery. 2000. “A unified framework for pull control mechanisms in multi–stage
manufacturing systems”. Annals of Operations Research 93 (1–4): 325–355.
Liu, Y., H. Zhang, C. Li, and R. J. Jiao. 2012. “Workflow simulation for operational decision support using
event graph through process mining”. Decision Support Systems 52 (3): 685 – 697.
Matta, A. 2008. “Simulation Optimization with Mathematical Programming Representation Of Discrete
Event Systems”. In Proceedings of the 2008 Winter Simulation Conference, edited by S. J. Mason,
R. R. Hill, L. Monch, O. Rose, T. Jefferson, and J. W. Fowler, 1393–1400. Piscataway, New Jersey:
Institute of Electrical and Electronics Engineers, Inc.
Savage, E. L., L. W. Schruben, and E. Yücesan. 2005. “On the Generality of Event-Graph Models”.
INFORMS Journal on Computing 17 (1): 3–9.
Schruben, L. 2010. “Simulation modeling for analysis”. ACM Transactions on Modeling and Computer
Simulation 20 (1): Article 2.
Schruben, L. 2013. “Simulation modeling, experimenting, analysis, and implementation”. In Proceedings
of the 2013 Winter Simulation Conference, edited by R. Pasupathy, S.-H. Kim, A. Tolk, R. Hill, and
M. E. Kuhl, 678–690. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, Inc.
Schruben, L. W. 1983. “Simulation modeling with event graphs”. Communications of the ACM 26 (11):
957 – 963.
van Leeuwen, J. 1990. “Graph algorithms”. In Handbook of Theoretical Computer Science: Algorithms
and Complexity, edited by J. van Leeuwen, Volume A, 525–631. MIP Press.
Zeigler, B. P. 1976. Theory of Modeling and Simulation. Wiley.
AUTHOR BIOGRAPHIES
ANDREA MATTA is Distinguished Professor at the Department of Industrial Engineering & Management at
Shanghai Jiao Tong University. He currently teaches stochastic models and simulation. His research area includes analysis and design of manufacturing and healthcare systems. His email address is matta@sjtu.edu.cn.
GIULIA PEDRIELLI is Research Fellow for the Centre for Maritime Studies at the National University
of Singapore. Her research focuses on simulation-optimization based on math programming and budget
allocation techniques applied to maritime systems. Her email address is cmsgp@nus.edu.sg.
ARIANNA ALFIERI is Associate Professor at Politecnico di Torino, where she currently teaches production planning and control and system simulation. Her research area includes scheduling and planning
in production and transportation systems. Her email address is arianna.alfieri@polito.it.

3994

Proceedings of the 2016 Winter Simulation Conference
T. M. K. Roeder, P. I. Frazier, R. Szechtman, E. Zhou, T. Huschka, and S. E. Chick, eds.

G-STAR: A NEW KRIGING-BASED TRUST REGION METHOD FOR GLOBAL
OPTIMIZATION
Giulia Pedrielli
Szu Hui Ng
Department of Industrial & Systems Engineering
National University of Singapore
1 Engineering Drive, 4
SG 258127, SINGAPORE

ABSTRACT
Trust region methods are an efficient technique to identify good solutions when the sampling effort needs
to be controlled due to the cost of running simulation. Meta-model based applications of trust region
methods have already been proposed and their convergence has been characterized. Nevertheless, these
approaches keep the strongly local characteristic of the original trust region method. This is not desirable
in that information generated at local level are “lost” as the search progresses. A first consequence is that
the search technique cannot guarantee global convergence. We propose a global version of the trust region
method, the Global Stochastic Trust Augmented Region (G-STAR). The trust region is used to focus the
simulation effort and balance between exploration and exploitation. Such an algorithm focuses the sampling
effort in trust regions sequentially generated by adopting an extended Expected Improvement criterion.
This paper presents the algorithm and the preliminary numerical results.
1

INTRODUCTION & PROBLEM STATEMENT

We consider a single objective minimization problem defined over a compact set X. The deterministic
d–dimensional objective function f : x ∈ X ⊂ ℜd → f (x) ∈ ℜ can only be observed with noise from a
simulator at any point x. Our goal is to find a global minimum of f : X → ℜ, where X ⊆ ℜd solving:
P : min f (x)
s.to
x∈X
In order to find a solution to P, we refer to meta-model based algorithms for simulation-optimization and
we are specifically interested in two classes of approaches: (1) trust region methods producing locally
converging procedures, and (2) meta-model based simulation optimization for global optimization. In fact,
G-STAR bridges these two families of approaches aiming at providing to the class (1) a global convergence
property. Such a link is established through the construction of ensembles of meta-models which are used
to sequentially drive the search for the global optimum.
The Trust Region method (TR) was initially proposed as a deterministic nonlinear programming method
with a similar framework to response surface methodology (RSM) (Nocedal and Wright 2006, Myers and
Anderson-Cook 2009). At each iteration, the approach defines a trust region around the current centroid,
it builds a quadratic approximation model, finds the optimal solution within the trust region (based on this
quadratic model) and then moves to the optimal solution (if it is satisfactory by certain criterion). The
size of trust region is automatically determined by the algorithm. In addition, it can be proved to converge
to a stationary point of the objective function. Chang et al. (2013) proposed the stochastic version of
this algorithm, the Stochastic Trust Region Gradient-Free Method (STRONG) for simulation optimization
978-1-5090-4486-3/16/$31.00 ©2016 IEEE

803

Pedrielli and Ng
with continuous decision variables. STRONG combines the traditional Response Surface Methodology
framework with the trust region method for deterministic optimization to achieve convergence property
and it provides an automated procedure to implement RSM. STRONG is proved by the authors to have
the potential of solving high-dimensional problems efficiently. The results in the paper are particularly
interesting. Nevertheless, similar to the deterministic case, information generated locally in terms of
response surfaces, is only used to make local decisions, hindering the possibility to search for global
solutions.
On the other hand, meta-model based global methods exploit the information coming from the simulation
by iteratively improving the estimate of a global model of the response surface to optimize. Since this
response surface is constructed upon a simulation model, it is referred to as meta–model. These procedures
typically use the meta–model to compute an indicator (e.g., the Expected Improvement, Knowledge Gradient)
which guides the search, by iteratively suggesting the next point(s) to sample with the objective to improve
the meta-model and eventually identify the global optimum. The Efficient Global Optimization Algorithm
(EGO) represents one of the most adopted procedures in this family for the case of deterministic f (x).
It uses the Expected Improvement, to choose the next point(s) and a ordinary kriging model to fit the
response. Sequential Kriging Optimization (SKO) extends EGO to the stochastic case by adapting both the
meta–model form as well as the criterion to guide the search. TSSO and, more recently, eTSSO, extend
SKO by providing advanced sampling and budget allocation criteria as well as the possibility to handle
non-homogeneous variance (Quan et al. 2013, Pedrielli and Ng 2015, Pedrielli and Ng 2016).
Meta-model-based approaches provide a way to model the global information about the response surface,
thus leading to global optima. For this reason it is appealing to try integrate them in a TR framework.
However, it is not straightforward to integrate the global meta-model into the trust region scheme. This is
because, in the trust region approach, a local model is generated within the current visited region. As a
result, the search progresses by generating local models and using a unique model as in the meta-model
literature would contradict the TR approach. A way to integrate the global information with multiple local
models is therefore necessary to exploit the TR framework, while maintaining a global information.
In this regard, meta-model ensembles represent an attractive alternative to be able too mix globally
and locally generated models as it is our target (Goel et al. 2007, Zerpa et al. 2005). In this context, the
meta-model ensemble is similar to model averaging, which serves as an approach to account for model
uncertainty. Goel et al. (2007), Müller and Piché (2011) and Goel et al. (2007) suggest different approaches
for determining the weights of models in the combination. In order to apply ensembles to guide the search
in a TR based algorithm, we need to modify two main aspects. Firstly, in TR approaches, models are
dynamically generated as the search progresses along with the related trust regions and they are not known
in advance and, hence, to adapt to this, the ensemble model is developed and estimated dynamically.
Secondly, the weights, computed at each iteration in the optimization procedure, need to be defined as a
function of the location x, and hence, an optimization problem needs to be solved at each iteration which
assigns to each model a different weight based on the fitting performance.
G-STAR, as a global meta-model based method, iteratively updates a unique global model which
helps in identifying centroids of trust regions to exploit. Subsequently, as a TR-method, iteratively builds
meta-models within the generated trust regions. The generated models are subsequently integrated forming
the meta-model ensemble which is used to predict the best point at each iteration. Specifically, the ensemble
is constructed with weights associated to each model that reduce to 0 exponentially fast as we get farther
from the model trust region.
2

THE G-STAR ALGORITHM

In order to provide a global feature to the trust region algorithm, G-STAR makes use of kriging as metamodel. This choice is due to the flexibility of kriging models in representing different shapes and, in
the scope of optimization, the presence of several indicators for sampling, which we use to support the
meta-model based search.

804

Pedrielli and Ng
G-STAR is developed around three main routines: (1) initialization, (2) search, and (3) ensemble construction.
An outline of the algorithm is provided in Figure 1.

Figure 1: G-STAR Overview.
The main steps of G-STAR are outlined below:
1. Initialization Procedure:
• Construct a first Global Model with n0 sampled points from X;
• Use the Modified Expected Improvement (mEI), first proposed in (Quan et al. 2013), to derive
the first centroid x∗1 ;
• Construct a trust region of size ∆0 and estimate the first local model. Set the number of
iterations to k = 1, and update the number of trust regions at iteration k, L (k) = 1.
2. Search Procedure
• Sample the centroid of the next candidate trust region, xk+1 , according to the Trust Region
expected improvement criterion proposed in this paper. This criterion samples either inside the
last created trust region or in the feasible region X excluding the already sampled points in the
set S as well as the generated trust region.
• Evaluate the function f¯ (xk ) by running mmin simulation replications at that new candidate
centroid xk+1 ;
• Conduct the modified ratio-comparison and sufficient-reduction tests in order to examine the
current xk+1 .
– In case the candidate is accepted, update the centroid of the trust region x∗k+1 and the
size of the trust region ∆L(k+1) . A new local model is created by uniformly sampling nBk+1
points within the trust region. The set of sampled points at iteration k + 1, Sk+1 , is updated
nB
by adding the centroid xk+1 and the uniformly sampled points {xi }i=1k+1 . The number of
trust regions is updated L (k + 1) ← L (k) + 1;
– If the tests are not passed, then the trust region is reduced in size and the local model is
updated considering the new sampled point.

805

Pedrielli and Ng
3. Ensemble Procedure
• According to the Ensemble procedure, the simulation budget remaining from the search step
is allocated to the already sampled points and the local model(s) as well as the global response
are re-estimated accordingly. This means that all local models and the global model can be
potentially re-estimated any time a point within the trust region or a centroid receives additional
simulation budget. Specifically, Wi,k represents the number of simulation replications allocated
to point i up to iteration k. We will use mk = Wk −Wk−1 and mik = Wik −Wi,k−1 , i ∈ S, to refer
to the number of simulation replications allocated at iteration k and the number of simulation
replications allocated to point i at iteration k, respectively.
As a result, we can produce the local forecasts for each trust region fˆi` (x0 ) , i = 1, . . . , L (k + 1)
as well as the global prediction fˆg (x0 );
• We use the produced predictions to compute the ensemble of the meta-models by deriving
the weights wg (x; θ ) for all the models.
Given a total simulation budget T , the procedure stops when the used budget is Wk = T . In order to run
G-STAR, an initial size of the trust region ∆0 needs to be provided as input parameter and used every time
a new trust region is generated. At the end, the sampled point with the lowest mean is selected.
In the following part of the paper, we will go into the details of the algorithm components starting form
the Ensemble Procedure, section 2.1, whereas the Search Procedure will be further detailed in section 2.2.
2.1 Estimation of the Ensemble of Surrogates
In this section, we present the modeling technique applied in this paper. In particular, we aim at generating
a meta-model ensemble by sequentially assigning weights to the different meta-models, developed in the
different trust regions, proportionally to the Mean Squared Error estimated throughout the sampled space.
In the context of this paper, the set of meta-models is not known beforehand and, instead, we dynamically
update a global surrogate model by using the function evaluations at the sampled centroids, while we estimate
a local surrogate model in each of the trust regions being generated around the centroids. As it will be
further detailed in section 2.2, the centroids are sampled according to the expected improvement criterion.
As a result, similarly to the literature on meta-model based simulation-optimization, we interpret these
points as representative of the global response characteristics. In fact, they are sampled in the scope of
exploring and exploiting the search space.
The idea behind this approach is that, while the global model gives us a chance to explore the entire
solution space, the local models focus on locally improving the global fit by considering only the points
within the trust regions and neglecting other sampled regions. As a result of this, we expect that the local
kriging model will be particularly accurate within the trust region but not outside. This consideration is
relevant in the scope of computing the weights to assign to each estimated meta-model, as it will be in the
next paragraphs.
As a result of the weighted model update, the response prediction can be formulated as it follows (L (k)
represents the number of trust regions created up to iteration k):
L(k)

fˆe (x) =

∑ wi (x; θ ) fˆi` (x) +

i=1

L(k)

!

1 − ∑ wi (x; θ )

fˆg (x) , x ∈ X.

(2.1)

i=1

Where fˆe (x) represents the ensemble prediction at the generic point x in the feasible space, and it is derived
based on fˆi` , referring to the local models estimated using the points sampled in the i–th trust regions and
fˆg (x) estimated using the centroids generated throughout the algorithm iterates.
Despite the local models and the global model are estimated according to different sampled points, the fit
is always global and this strategy is used in order to simplify the construction of a smooth ensemble of
806

Pedrielli and Ng
surrogates. Indeed, even if the prediction is global, local models will be given a very low weight outside
their TR, while still guaranteeing smoothness of the overall prediction. Specifically, all these predictions,
following the meta-models ensemble literature, are averaged by means of a series of weights wi (x; θ ) which
are a function of the points location x and parametrized over θ , which controls the rate at which the local
model weight vanishes. It has to be noted that, as highlighted in Figure 1, at the end of the search procedure
we may not generate a new trust region. For this reason, generally, L (k) 6= k and L (k + 1) ≥ L (k).
One peculiar aspect for the prediction proposed in equation (2.1) is that we treat the global model
differently from the local surrogates. The main reason is that the global model is estimated independently
from the local surrogates in that it only uses the function values at the centroids (which are not used
to estimate the trust region models). The choice of using centroids, sampled according to the global
expected improvement (equation (2.5)), is in line with the traditional literature on meta-model based global
optimization, as already mentioned. In fact, the main role of the global model in G-STAR is to guarantee
global exploration of the solution space which is realized through the generation of centroids, as it will
be clearer in section 2.2.1. In fact, the global response enables us to consider the entire feasible space
even when no local model is defined. On the other hand, the availability of meta-models enables the
computation of sampling criteria at global level, thus avoiding local minima as typical of the original trust
region algorithm.
In the next part, we briefly describe the base meta-model used by G-STAR.
Models Estimation Differently from the traditional literature on meta-models ensembles, in this paper
each model adopted is in the family of Modified Nugget Effect Kriging (MNEK) model (Yin et al. 2011).
The choice of this meta-model is motivated by the stochastic nature of simulation and non-homogeneous
variance. As previously mentioned, the choice of kriging models was performed for the flexibility in terms
of shapes that can be produced if compared to polynomial forms. At global level, the choice of a kriging
model is necessary in the scope of capturing a global trend. On the other hand, at local level, simpler
models might also be considered. In particular, as in the tradtional TR literature, first and second order
models might be locally estimated. The proposed approach does not lose in generality assuming different
local models. However, Gaussian models represent an advantage in view of developing a closed form
estimation for the MSE of the meta-model ensemble.
We assume that the function values f (x) are realizations of a random process and a statistical model
represents the response surface, namely:
f (x) = Z (x) + ξ (x) , x ∈ X

(2.2)

where Z describes the mean and ξ describes the random noise process. As in the deterministic case, we
further model Z (x) as a Gaussian process with covariance function τ 2V , where
τ 2 is the process variance

2
and V the matrix of process correlation; formally, Z (x) ∼ GP µ (x) , τ V .
A commonly adopted correlation function V = (Kθ (xi − x j ))ki=1 is the d–dimensional separable version of
the power exponential family of functional forms which is characterized by smooth response. The noise
ξ (x) is assumed to be distributed with zero mean and covariance function σξ2Vξ , where Vξ denotes the
matrix of sampling correlations. Error variances are generally not constant and they may depend on x (i.e.,
the heteroscedastic case is considered). With independent sampling (i.e., no CRN), Vξ is diagonal, and
equation (2.2) reduces to the independent sampling noise model Yin et al. (2011). The general form of
equation (2.2) is similar to the form proposed in Ankenman et al. (2010).
As shown in Yin et al. (2011), the predictor for (2.2) at the point x, given k points have been already
sampled, is:

h
−1 iT 
T
k
v
−1
−1 1 − 1 Vξ +V


ei  f¯i
(2.3)
fˆ (mk , x, θ ) = ∑ vT Vξ +V
ei + 1T Vξ +V
−1
1T Vξ +V
1
i=1

807

Pedrielli and Ng
where, mk represents the total number of simulation replications performed up to iteration k. f̄ is the k–
dimensional vector of the averaged
function values

 at the already sampled points. v is the correlation vector,
2
2
−ν·dx,x
T
−ν·dx,x
k , where, dx,x represents the euclidean distance between
1 ···e
specifically, v (mk , x, ν) = e
i
point x at which the prediction is performed and the already sampled locations xi , i = 1, . . . , k. The vector
ei has size k (being k the number of sampled points) and its elements are all equal to 0 except the i–th
element which is equal to 1.
Weights Derivation Since G-STAR dynamically generates local models, each iteration will potentially
have a different number of meta-models. Since the local models are meant to be fit for the specific trust
region, intuitively, they should receive a larger weight within the related trust region and such a weight
should vanish to 0 as the distance from the region increases (in terms of distance from the centroid). In
light of these observations, we propose to weight each meta-model according to the following form:


d
2
∑ (xl − ri,l ) 

l=1
 , i = 1, . . . , L (k)

(2.4)
wi (x; θ ) = exp −θi

2|Bi |
i=L(k)

Here, L (k) refers to the number of trust regions at iteration k, the parameters {θi }i=1 will be estimated
by minimizing the MSE of the ensemble and they represent an additional degree of freedom to optimize
the rate of decay of the weight in the space. The vector ri represents the d-dimensional centroid of the
trust region and x is the generic point being considered. Finally, Bi represents the trust region and | · |
refers to the size of the trust region. The inspiration of the weight comes from the Gaussian density, where
the deviation is interpreted as the size of the trust region.
2.2 Search Procedure
2.2.1 The Trust Region-Expected Improvement (EIπ̃Bk )
At each iteration k a global response model and a new local response model are both constructed. We
use them as support to identify the next trust region centroid as it is common in a trust–region approach.
Specifically, at iteration k, we consider the current centroid xk , the corresponding trust region BL(k) xk , ∆L(k)
of size ∆k , the global response model estimated with the sampled centroids and the last estimated local
model over the current trust region.
In order to find the next candidate centroid, we propose the trust region-modified Expected Improvement
indicator (EIπ̃Bk ), the indicator is formulated as follows:
EIπ̃Bk = 1g(∆L(k) )≤0 EIπ̃k (X) + 1g(∆L(k) )>0 EIπ̃k (B)

(2.5)

∗
In the above equation, EIπ̃k is defined in a different way based on the fact that xk+1
will be sampled within
or outside the trust region. In fact, due to the construction of the algorithm, the local and global criterion
are exclusive and the activation of one of the two is guided by the indicator function 1g(∆L(k) )≥0 . We refer
to the component EIπ̃k (X) in equation 2.5 as Global EI , whereas the Local EI is EIπ̃k (B).
We will sample according to the global (or local) EI when the following function is less (larger) than
or equal to 0:




∆L(k)
+ε −1
(2.6)
g ∆L(k) =
∆0

Where, ε is a random variable (a uniform U (0, 1)). We can notice that this criterion guarantees that when
the size of the trust region ∆L(k) → 0, then the function g ≤ 0 no matter the realization of ε. Therefore a
808

Pedrielli and Ng
point for the next centroid will be created outside the current trust region. The search is then brought back
by construction to the global search typical of meta-model based simulation optimization, thus guaranteeing
the global convergence.
When ∆L(k) > 0, instead, the probability of sampling inside the trust region is proportional to the size of
the trust region with respect to the initial size ∆0 (which is an input parameter of the algorithm). The idea
is that, at the beginning we have large probability to sample inside the trust region and this probability
decreases with the iterations
 (as the trust region reduces in size). In case the sampling happens outside the
trust region, i.e., g ∆L(k) ≤ 0, then the criterion is equivalent to the one in (Pedrielli and Ng 2015) and
the EI is computed considering the entire feasible space excluding the current trust region, BL(k) , and the
set of already sampled points:


	
xk+1 ∈ arg
max
EIπ̃k := Eπ̃k max f¯ (x∗k ) − fˆg (x) , 0
(2.7)
x∈X∈
/ (S∪BL(k) )
In equation (2.7), X represents the entire feasible
space and S the set of already sampled points. xk∗ refers to

the point with the best function value f¯g xk∗ up to iteration k, while fˆ (x) is the predicted global function
value at the generic point x.
When the point is sampled inside the trust region, the same expected improvement is computed, but
within the trust region, i.e., X ← Bk . In this sense, we look for the best point within the trust region since
we believe it is a promising region, and we use the expected improvement locally. We highlight that several
approaches may be used in order to search locally. Nevertheless, the expected improvement gives us the
possibility to leverage on the produced local model when sampling, making use of the Gaussian process
definition to derive the indicator.
In general, it can be argued that the Expected Improvement is a criterion which leads to both exploration
and exploitation, which means that the EI may choose to sample very close to a previous trust region or
inside an already existing trust region even when the condition g (∆k ) ≤ 0 is verified and we would like
to get out from the trust region. Nevertheless, it is relevant to highlight that the global EI looks for new
centroids and the presence of previously generated trust regions where the global EI may sample does not
contradict the aim of the global search for which G-STAR is proposed. Simply, overlapping trust regions
will be generated. Also, the local and global sampling criteria will never choose a point from the same
set. This guarantees G-STAR to reach density in the sampling space as the number of iterations k → ∞.
Indeed, by construction,
 the algorithm will never choose the same centroid twice and, on the other hand,
the condition g ∆L(k) ≤ 0 will guarantee exhaustive search in the sampling space.
By construction, the following property holds:
Property 2.1. For any iteration k, we have: fˆke (xk ) − fˆke (xk+1 ) ≥ EIπ̃Bk , where fˆke is the prediction computed
from the meta-model ensemble and xk+1 represents the candidate centroid, i.e., the point solving 2.5.
The property can be proved just considering the definition of expected improvement in equation (2.7).
This fact will be particularly useful in the derivation of the sufficient reduction test to establish whether to
move the centroid of the current trust region at the generic iteration k.
The proposed sampling criterion is one of the differences between G-STAR and STRONG, which looks for
a Cauchy point in the local region and does not provide a global search criterion. Due to this characteristic
of G-STAR, the algorithm can reach global convergence. Indeed the global criterion will always be active
when the size of the trust region goes to 0. This enables us to use the convergence results in the previous
work of the authors (Pedrielli and Ng 2016).
2.2.2 Acceptance/rejection of the new centroid
As a TR algorithm, G-STAR automatically generates a new centroid/ updates the trust region based on the
results from the Ratio-Comparison (RC) and Sufficient Reduction SR tests. Despite in the original paper
(Chang et al. 2013), the authors do not use as response surface a kriging model, it can be argued that the
two tests play the same role in G-STAR as in STRONG: the RC test verifies the consistency between the
809

Pedrielli and Ng
prediction and the simulated values and this is consistent with the idea of model variance in the kriging
literature. The sufficient reduction test, compares the actual difference between two consecutively sampled
points and the expected difference, which is the expected improvement in the context of meta-model based
search.
The RC test fails when the simulated difference is much lower than the predicted difference between
¯ k )− f¯k (xk+1 )
.
the function value at two consecutive centroids. The RC test is (Chang et al. 2013):ρk = ffˆke (x
ˆe
k (xk )− f k (xk+1 )
Given two input parameters 0 < η0 < η1 < 1, if ρk < η0 , then the RC test is failed. The prediction fˆk here
refers to the meta-model ensemble.
The only difference with respect to (Chang et al. 2013) here is that we are considering xk+1 , i.e., the current
centroid as a maximizer of the expected improvement instead of a Cauchy point. This is in line with our
overall aim of making the trust region a global search algorithm. The Cauchy point is generated with
gradient information, therefore it is subject to local convergence issues. Instead, the expected improvement
is proven to converge to the global optimum. This does not modify the validity of the test in conceptual
terms: since we are still using a meta-model to emulate the response surface, we are interested in verifying
the quality of such a model.
On the other hand, even when the models are consistent, we might not update the centroid if the
improvement is not significant. This is verified by the SR test. The sufficient reduction test is identical to
¯
¯ k+1 )−η 0 ζk
.
the one proposed in (Chang et al. 2013): t ∗ = fk (xk )− fk (x
Sk
where Sk is the pooled sample variance of xk+1 and xk and, by construction, ζk := EIπ̃Bk , i.e., the value
of the expected improvement. The variance adopted for the test is a pooled estimator: Sk2 =
Sk2 (xk+1 ,n0 )
n0



Sk2 (xk ,nk )
nk

+

. The degrees of freedom of the resulting t–statistics for the adopted approach are:d f = Sk4 ·

2
2 −1
(Sk2 (xk ,nk )/nk )
(Sk2 (xk+1 ,n0 )/n20 )
+
It is important to notice that the test is not based on the predicted
nk
n0

variance but the sampled variance at the point to be explored, where nk represents the budget dedicated
to the sampling of the point. The sufficient reduction test is passed in case t ∗ > t1−αk ,d f (Myers and
Anderson-Cook 2009).
It has to be noted that, due to Property 2.1, we are able to translate the criterion originally presented in
(Chang et al. 2013), to the case of sufficient reduction in equation (2.2.2) in G-STAR. This is due to the
fact that we are able to bound the maximum expected improvement at each algorithm iteration and this is
made possible by the use of meta-models.
2.3 Ensemble Update
After the centroid-tests, G-STAR uses the enlargement coefficient γ2 > 1 and the shrinkage coefficient
0 < γ1 < 1 for modifying the size of the trust region. G-STAR will perform the following steps:
•

•

If the candidate point passes the SR/RC Test, then the centroid is moved, i.e., x∗k+1 ← xk+1 and the
TR around the point is enlarged using ∆L(k+1) = γ2 · ∆L(k) . Once a trust region is determined nBk
points are sampled within the trust region according to a Latin Hypercube sampling design in order
to have a first estimate of the local model. Such a sampling choice is originates by the procedure
we use to initially estimate a model in the surrogate literature, i.e., through the choice of n0 initial
sampling points Kleijnen (2008).
If the point fails either RC or SR tests we have two scenarios:
– If xk+1 ∈ B: The trust region is reduced according to ∆L(k+1) = γ1 · ∆L(k) and the centroid is
not updated.
– if xk+1 ∈
/ B: the solution is accepted, x∗k+1 ← xk+1 , and the trust region is enlarged (favor
exploration), i.e., ∆L(k+1) = γ2 · ∆L(k) .

810

Pedrielli and Ng
It is important to highlight that the shirkage of trust region in G-STAR does not lead to the construction
of a new meta-model. A result of the shrinkage is the increase in the sampling density (in terms of ratio
between sampled points and size of sampling space) and this leads intuitively to an improved fit of the
kriging model. It may be argued that simply increasing sampling in the trust region without reducing the
size will lead to the same effect of improving the model fit, due to the properties of kriging. However,
reducing the size of the trust region can have the positive effect of increasing the speed of exploration of
the space by reducing the probability of sampling within the TR (equation 2.5). Nevertheless, it is has to
be noted that, wile TR reduction in the original algorithm is required to fit the local model, in our case it
just helps in increasing exploration.
Sequential Update of the Meta-model Ensemble The sequential update of the response surface
happens in two steps: (1) update of the global and local model(s), (2) update the set of sampled points and
subsequent update of the weights. Specifically, at each iteration of the algorithm we estimate the parameters
of:(A) The global response surface considering only the centroid(s) generated by the global component
of the Expected improvement in equation (2.7). This implies that, in case the sampling is local, then no
estimation of the global parameters is performed; (B) Generate nBL(k) Latin Hypercube samples in the trust
region BL(k) and update the estimate of all the local models having trust regions intersecting with the set
of new sampled points. This results in an improvement of multiple local models every time a sampling
region is formed according to the latin hypercube sampling as described in section 2.3. For the update of
the overlapping trust regions, we always consider the initial size ∆0 .; (C) Evaluation: once the new points
in the trust region have been sampled, we may have remaining budget for the iteration to improve the
model estimation. Specifically, the total budget to allocate to the evaluation stage is established according
to the criterion presented in (Pedrielli and Ng 2015, Pedrielli and Ng 2016). In particular, allocating budget
is important for two reasons: (1) computational effort to dedicate to a specific iteration of the algorithm,
(2) computational effort to dedicate to each sampled solution to improve the response estimate. (Pedrielli
and Ng 2015) solves (1), while Optimal Computing Budget Allocation (OCBA) scheme is used for (2).
After simulation, all involved trust regions and the global model may be re-estimated; (D) Recompute the
optimal weights for all the regions solving the MSE problem in (2.8).
We propose to minimize the empirical MSE to define the weights to associate to the different models.
In particular, at each iteration, all the weights parametrized by θ in equation (2.8) at each local region and,
consequently, the weight of the global model are updated by solving the following optimization problem:
2
1
fˆke (x) − f¯ (x) =
∑
θ
θ |S|
x∈S
!
#
!2
"
L(k)
L(k)
∑ wi (x; θ ) fˆi` (x) + 1 − ∑ wi (x; θ ) fˆg (x) − f¯ (x)
min MSEk (θ ) = min

1
∑
|S| x∈S

i=1

(2.8)

i=1

When the global prediction is pretty good, we will expect θ in the weighting function to grow in order
to taper down the effect of the local model very quickly around the centroid in order to let the global
model play the major role in the prediction. It is apparent that the MSE problem just formulated will
increase its computational demand when the iterations k and, consequently, the number of trust regions
L (k) remarkably grow.
It is also noteworthy, that multiple local models are updated along the way, and this may be computationally expensive as the number of local models increases with the number of iterations. Updating
schemes which consider the likelihood of a trust region to contain the global optimimum in
3

PRELIMINARY NUMERICAL RESULTS

In this section, we show the behavior of G-STAR over a simple 1–dimensional case. Since G-STAR
introduces a global characterization to trust region based approaches, we are interested in investigating the
workings of the algorithm and its empirical properties in terms of convergence to the global optimum for a
811

Pedrielli and Ng
multi-modal response surface as the one in the example. In order to test the performance of the algorithm,
we compare G-STAR with another global algorithm developed by the authors, i.e., eTSSO (Pedrielli and Ng
2016). In fact, since TR algorithms are not able to guarantee global convergence, a more fair comparison
is between G-STAR and globally converging procedures. Specifically, we used the following function as
test example in this paper:
f (x) = (2x + 9.96) cos(13x − 0.26)

(3.1)

This function has a global minimum in x∗ = 0.746 with function value y∗ := y (x∗ ) = −11.45 and a local
minimum in x = 0.2628, with X = [0, 1]. As noise, we applied to the function an additive Gaussian
Process ξ (x) with mean 0 and diagonal variance covariance matrix with diagonal elements (δ represents
the magnitude of the noise): σξ2 (x) = δ · x. Figure 2 shows three sequential iterations of G-STAR. In
particular, a set of n0 = 3 points are sampled in x0 = [0.4551, 0.61723, 0.9951] and a first global model
estimation is performed. With this first global model, the first centroid x = 0.7072 for the trust region
with size ∆0 = 0.3499 (where ∆0 is an input parameter). Figure 2b shows the fitting of the first local
model in the trust region defined by the centroid xk∗ and the lower and upper bounds x1l , x1u . As expected,
the local model fits particularly well the true function within the trust region, but poorly outside of its
estimation interval. As the algorithm progresses (Figures 2c-2d), we notice that the meta-model ensemble
is incrementally improved due to the generation of new local models. The proposed methodology for
ensembles produces a surrogate which basically conforms with the local model in the trust region, while
following the global model outside it. We can notice that, at the 2nd iteration no update of the global model

(a) Weights generated.

(b) 1st Iteration.

(c) 2nd Iteration.

(d) 3rd Iteration.

Figure 2: Sequential Ensemble Construction.
is performed since the new centroid is generated by means of the local EI (2.5). Moreover, since the trust
region changes at each iteration, we do not observe any shrinkage.
G-STAR looks particularly promising when low budget values are considered. Indeed, in such a case, the
algorithm can effectively take advantage of a local good model to improve the global one, whereas the
original optimizer has to deal with a low quality global model. It is important to notice that, as shown
in the figures, when the number of iteration progresses, the global model will tend to have more weight
that any local model. This is asymptotically correct due to the fact that, as the iterations go to infinity we
will sample, by construction, an infinite number of centroids. Therefore, the global model will be dense in

812

Pedrielli and Ng
Common Parameters
n0
3
mmin
10
T
200
δ
0.1

G-STAR Parameters
γ1
0.6
γ2
1.05
η0
0.2
η1
0.8
α
0.05
nBk
4

|x − x∗ |

Average
Std Err |x − x∗ |
Average |y − y∗ |
Std Err |y − y∗ |
PCS

eTSSO
0.45
0.0122
0.919
0.019
7%

GSTAR
0.0194
0.021
0.433
0.046
66%

(b) Comparison between eTSSO and G-STAR
(a) Algorithms Parametrization

the solution space. Considering the local models being generated, as the iterations k → ∞, the optimized
parameter θ will tend to become larger to reduce the influence of the local model.
In this preliminary analysis, we had as target to understand the behavior of the algorithm, particularly in
the way it sequentially estimates the response surface, and compare it with the previous algorithm designed
and extended by the authors, i.e., the Two Stage Sequential Optimization algorithm (TSSO).
In order to run TSSO and G-STAR we adopted the parametrization in Table 1a. Therefore, nBk = 4
points are used to estimate the local models.
In the first set of results, we show the workings of the algorithm in terms of response surface estimation.
In particular, we show how progressively the ensemble of surrogates fits the true function.
In order to compare the performance of G-STAR and eTSSO (Pedrielli and Ng 2016), we fixed the
same initial points and we adopted the same random number stream throughout the algorithm execution.
We compare the two algorithms based on three KPI’s: The location error, i.e., |x − x∗ | computed as the
euclidean distance between the solution produced by the algorithms and the true global optimum. We
also report the error |y − y∗ | which measures the precision in the estimation of the value of the function at
the global optimum. Moreover, we report the results in terms of Probability of Correct Selection (PCS)
which is computed as the percentage over the 100 macro-replications, that the algorithm chooses the global
optimum instead of the local solution in x = 0.2628. Table 1b shows the performance of the two algorithms.
It is apparent how, under a very limited budget, as it is in the case of the experiment performed,
G-STAR shows a statistically significant improvement in the function estimation and, as a result, in the
identification of the global minimum.
4

CONCLUSIONS

This paper proposed the Global Stochastic Trust Augmented Region Algorithm for the first time. The
main idea of G-STAR is to extend trust-region methods to provide them a global optimization perspective,
while contributing in the area of ensemble of surrogates by providing a new way to dynamically assign
location-dependent weights which represents a novelty in the surrogate literature where models are typically
weighted by constant factors all through the response surface. The development of the algorithm highlights
the issues in sampling, where local and global conditions need to be verified when generating a candidate
point. In order to do so, we create an index which guarantees us to escape local solutions. This extension
of the Expected Improvement criterion is particularly suitable when meta-models are available as it makes
explicit use of them. The preliminary numerical analysis shows that, when the budget is relatively small,
G-STAR provides a structural advantage with respect to the algorithms previously developed by the authors,
thus making the further development of the technique an interesting topic to investigate, especially in higher
dimensions. Different local model types can be tested of simpler natural such as second or first order
models. When Gaussian models are used, then G-STAR might take particular advantage of the closed
form derivation of the MSE of the meta-model ensemble. The finite time performance analysis should be
performed to provide theoretical strengths of the ensembles approach. Also, several parameters need to be
tuned, adaptive versions of G-STAR should be studied to avoid their manual setting.

813

Pedrielli and Ng
REFERENCES
Ankenman, B., B. L. Nelson, and J. Staum. 2010. “Stochastic kriging for simulation metamodeling”.
Operations research 58 (2): 371–382.
Chang, K.-H., L. J. Hong, and H. Wan. 2013. “Stochastic trust-region response-surface method (strong)-a
new response-surface framework for simulation optimization”. INFORMS Journal on Computing 25
(2): 230–243.
Goel, T., R. T. Haftka, W. Shyy, and N. V. Queipo. 2007. “Ensemble of surrogates”. Structural and
Multidisciplinary Optimization 33 (3): 199–216.
Kleijnen, J. P. C. 2008. Design and analysis of simulation experiments, Volume 111 of International Series
in Operations Research & Management science. Springer.
Müller, J., and R. Piché. 2011. “Mixture surrogate models based on Dempster-Shafer theory for global
optimization problems”. Journal of Global Optimization 51 (1): 79–104.
Myers, R. H., and C. M. Anderson-Cook. 2009. Response surface methodology: process and product
optimization using designed experiments, Volume 705. John Wiley & Sons.
Nocedal, J., and S. Wright. 2006. Numerical optimization. Springer Science & Business Media.
Pedrielli, G., and S. H. Ng. 2015. “Kriging-based Simulation Optimization: A Stochastic Recursion
Perspective”. In Proceedings of the 2015 Winter Simulation Conference, L. Yilmaz, W. K. V. Chan,
I. Moon, T. M. K. Roeder, C. Macal, and M. D. Rossetti, eds., 3834–3845. Piscataway, New Jersey:
Institute of Electrical and Electronics Engineers, Inc.
Pedrielli, G., and S. H. Ng. 2016. “Two Stage Sequential Optimization Approach: Generalization and
Asymptotic Properties”. Submitted to IIE Transactions. Under Review.
Quan, N., J. Yin, S. H. Ng, and L. H. Lee. 2013. “Simulation optimization via kriging: a sequential search
using expected improvement with computing budget constraints”. Iie Transactions 45 (7): 763–780.
Yin, J., S. H. Ng, and K. M. Ng. 2011. “Kriging metamodel with modified nugget-effect: The heteroscedastic
variance case”. Computers & Industrial Engineering 61 (3): 760–777.
Zerpa, L. E., N. V. Queipo, S. Pintos, and J.-L. Salager. 2005. “An optimization methodology of alkaline–
surfactant–polymer flooding processes using field scale numerical simulation and multiple surrogates”.
Journal of Petroleum Science and Engineering 47 (3): 197–208.
AUTHOR BIOGRAPHIES
GIULIA PEDRIELLI is Research Fellow for the Department of Industrial & Systems Engineering at the National University of Singapore. Her research focuses on stochastic simulation-optimization in both single and
multiple–objectives framework. She is developing her research in meta-model based simulation optimization
and learning for simulation and simulation optimization. Her email address is giulia.pedrielli.85@gmail.com.
SZU HUI NG is an Associate Professor in the Department of Industrial and Systems Engineering at the
National University of Singapore. She holds B.S., M.S. and Ph.D. degrees in Industrial and Operations
Engineering from the University of Michigan. Her research interests include computer simulation modeling
and analysis, design of experiments and quality and reliability engineering. She is a member of IEEE
and INFORMS, and a senior member of IIE. Her email address is isensh@nus.edu.sg and her website is
http://www.ise.nus.edu.sg/staff/ngsh/index.html.

814

Proceedings of the 2016 Winter Simulation Conference
T. M. K. Roeder, P. I. Frazier, R. Szechtman, E. Zhou, T. Huschka, and S. E. Chick, eds.

CONSTRAINED OPTIMIZATON FOR HOSPITAL BED ALLOCATION VIA DISCRETE
EVENT SIMULATION WITH NESTED PARTITIONS
Loo Hay Lee
Giulia Pedrielli

Nugroho A. Pujowidianto
Home Printing Solutions
HP Inc.
138 Depot Road
109683, SINGAPORE

Department of Industrial & Systems Engineering
National University of Singapore
1 Engineering Drive 2
117576, SINGAPORE

Chun-Hung Chen

Haobin Li

Department of Systems Engineering
and Operations Research
George Mason University
4400 University Drive, MS 4A6
Fairfax, VA 22030, USA

Department of Computing Science
Institute of High Performance Computing
A*STAR Singapore
1 Fusionopolis Way #16-16 Connexis
138632, SINGAPORE

ABSTRACT
This paper aims to further motivate the use of simulation of complex systems in optimizing healthcare
operations under uncertainty. One argument to use optimization only such as mathematical programming
instead of simulation optimization in making decisions is the ability of the former to account for constraints
and to consider a large number of alternatives. However, current state-of-the art of simulation optimization
has opened the possibilities of using both simulation and optimization in the case of multiple performance
measures. We consider the case of hospital bed allocation and give an example on how a stochastically
constrained optimization via simulation can be applied. Nested Partitions are used for the search algorithm
and combined with OCBA-CO, an efficient simulation budget allocation, as simulation is time-consuming.
1

INTRODUCTION

Stochastic simulation and optimization are two powerful tools in decision making. Simulation enables
decision makers to understand complex systems and evaluate their performances under uncertainty. Optimization is useful as it considers wider range of solutions in selecting the best instead of pre-generated
alternatives in a simulation study. Simulation optimization or optimization via simulation are able to capture
both benefits of stochastic simulation and optimization (Fu 2002). At the same time, there are still some
challenges in integrating simulation optimization into practice as described in Fu et al. (2014).
Out of many real-world problems, the application of simulation to healthcare is ubiquitous (Jun, Jacobson,
and Swisher 1999). There have been some applications of simulation optimization to healthcare but their
numbers are relatively fewer than simulation case studies. Brailsford et al. (2007) combine discrete-event
simulation with a metaheuristic called Ant Colony Optimization (ACO) to determine the optimal screening
policies in addressing diabetic retinopathy. Tanfani, Testi, and Alvarez (2010) use simulation optimization
to determine the optimal plan for the Operating Room.

978-1-5090-4486-3/16/$31.00 ©2016 IEEE

1916

Pujowidianto, Lee, Chen, Li, and Pedrielli
In this paper, we consider the bed allocation problem which is not new but remains challenging. Poor bed
allocation could cause overcrowding in Emergency Department leading to increase in mortality (Sprivulis
et al. 2006). Despite the wide range of literatures on the bed allocation problem, only few works use
simulation optimization techniques. Wang et al. (2015) applied multi-objective optimization via simulation
on the bed allocation problem. Zhang et al. (2012) to address the issue of long-term care planning.
Holm, Lurås, and Dahl (2013) and Keshtkar, Salimifard, and Faghih (2015) consider both simulation and
optimization. Most of other literatures either use only optimization or only simulation. Those which use
only optimization aim to find the best way of allocating beds by considering a huge number of possibilities
(Ridge et al. 1998, Teow and Tan 2008). It is possible to consider uncertainties in optimization using
stochastic programming. At the same time, some decision makers may need to use simulation to evaluate
performance measures which do not have closed-form expressions such as the number of bed overflow. In
addition, simulation is able to evaluate policy to model the hospital complexities or preferences (Goldman,
Knappenberger, and Eller 1968). Simulation studies have given useful insights on bed allocation (El-Darzi
et al. 1998, Harper and Shahani 2002, Akkerman and Knip 2004, Cochran and Bharti 2006). However,
the use of only simulation is limited as it only considers a pre-determined set of alternatives which may
not be the real best solution.
One of the challenges of simulating complex service systems such as healthcare is the amount of
times required as multiples replications are needed to get better estimates of the performance measures of
interest. Lapierre et al. (1999) mentioned that once a valid simulation model for the bed allocation problem
is obtained, we could compare among the alternatives using ranking and selection procedure (R&S) that
efficiently determines the number of replications. R&S have been shown to be much more efficient than
if the simulation budget is equally distributed among the alternatives. Another challenge is to consider
constraints in the performance measures separately instead of lumping it into a single objective. These
two challenges can be tackled by integrating R&S with optimization methods. For example, Ahmed and
Alkhamis (2009) integrates the constrained R&S procedure by Andradóttir, Goldsman, and Kim (2005) with
the search algorithm (Alkhamis and Ahmed 2004) to find the optimal number of staffs in the emergency
department.
In this paper, we apply the proposed allocation procedure (OCBA-CO) and Nested Partitions method
in finding the best feasible bed configuration which consists the number of beds for each specialty. This
provides an alternative for decision makers who wish to model the problem as a constrained optimization
instead of a single objective problem or a multi-objective problem. For the case where all performance
measures are equally important, Wang et al. (2015) provides an excellent example on how to efficiently
obtain the set of non-dominated solutions.
2

PROBLEM STATEMENT

2.1 Sample of System Description and Modeling
We consider a hypothetical setting as described in Pujowidianto et al. (2012). The bed management unit
is open 24 hours daily and we consider two sources to the bed management unit, namely the emergency
patients and the elective patients. For the emergency patients arrivals, we treat their service time in the
emergency department as a one lump sum. We assume that the service time distribution is uniform across
different levels of patient acuity. Some patients from the emergency department will then be admitted to
the bed management unit based on the historical probability of admission for different types of patient
attributes. We assume that there is no physical limit for the number of patients in the emergency department.
For both the emergency and the elective patients, we consider 5 different specialties.
We use non-stationary Poisson process to model both the emergency patients and the elective arrivals.
The length of stay is exponentially distributed. The more critical patients receive a higher priority in
practice. However, for simplicity, we do not consider different patient acuity levels and so first-in first-out
(FIFO) is used as the queue discipline. In addition, we assume that the travelling time to the ward and

1917

Pujowidianto, Lee, Chen, Li, and Pedrielli
the cleaning time are incorporated inside the length of stay. Figure 1 shows the process flow in modeling
the bed allocation problem. It is possible to allow for an overflow when the bed with the correct specialty
for a particular patient is not available. This is governed by the overflow protocol in Table 1. For each
specialty, there are at most three other specialties where the overflow can be allowed.

Figure 1: Process Flow.
Table 1: Overflow protocol.
Specialty
Medicine
Cardiac
Oncology
Surgery
Orthopedic

1st Overflow
Oncology
Medicine
Medicine
Medicine
Surgery

2nd Overflow
Cardiac
Surgery
Surgery
Oncology
Medicine

3rd Overflow
Not Applicable
Orthopedic
Orthopedic
Cardiac
Not Applicable

2.2 Problem Description
Let c be the total number of specialties considered while xm and um are the number of available bed and
the number of occupied bed respectively at a given fixed time of the day for the specialty m = 1, . . . , c.
We consider three daily performance measures, namely the bed occupancy rate (BOR), the 99th percentile
of the turn-around-time (TAT99 ), and the number of overflow (O). BOR is defined as the total number of
c
m=1 um
occupied beds at a particular time of the day divided by the total number of bed, i.e. BOR = ∑
. In
∑cm=1 xm
this study, the parameters xm and um are measured at 6 a.m. Let r p , a p , and t p be the time of bed request,
the time of the admission to the bed management unit, and the turn-around-time for patient p ∈ W where
W is the set of all possible patients. The turn-around-time for patient p is measured from the time of bed
request to the time the patient is admitted, i.e. t p = a p − r p . TAT99 can then be obtained by taking the 99th
percentile of t p of all patients p ∈ W in a day. The daily number of overflow represents the number of
mismatched between the specialty of the patients and that of the bed. It is measured in terms of percentage
of the number of overflow with respect to the total number of admitted patients in a given day.
Our goal is to determine the best feasible bed configuration, that is to find the configuration xi = [x1 . . . xc ]
among k designs, i.e. i = 1, . . . , k, which returns the largest BOR while ensuring the 99th percentile of the
turn-around-time and the number of overflow are less than the maximum limits γ1 and γ2 as described in
the following

1918

Pujowidianto, Lee, Chen, Li, and Pedrielli

maxxi BOR subject to TAT99 ≤ γ1 , O ≤ γ2 .

(1)

Due to the uncertainties in the patients arrival time and the length-of-stay, the values of BOR and
TAT99 need to be estimated via simulation. Let Hi jd be the BOR in the j-th simulation replication
and in the d-th day for the bed configuration i, hi = E j [Ed [Hi jd ]]. Similarly, the simulation sample for
TAT99 is Gi1 jd , gi1 = E j [Ed [Gi1 jd ]] and that for the percentage of overflow is Gi2 jd , gi2 = E j [Ed [Gi2 jd ]].
i
The comparison of the bed configurations are based on sample means, i.e. Ĥi = N1i ∑Nj=1
( D1 ∑D
d=1 Hi jd )
1 Ni
1 Ni
1 D
1 D
, Ĝi1 = Ni ∑ j=1 ( D ∑d=1 Gi1 jd ), and Ĝi2 = Ni ∑ j=1 ( D ∑d=1 Gi2 jd ) where Ni is the number of simulation
samples for bed configurations i and D is the number of simulated days. We assume that simulating the
system for D days excluding the warm-up period is sufficient to represent the original system. The key for
an efficient comparison is then on the determination of Ni .
We note that the problem in (1) can be modified depending on the goal of the decision maker. For
example, we can consider an additional constraint if the BOR should not exceed 85%. Gorunescu, McClean,
and Millard (2002) show that hospitals need to keep 10 − 15% emptiness to maintain the service efficiency
using queuing model. This is in line with the finding in Bagust, Place, and Posnett (1999) that regular
shortages can occur if the average bed occupancy unit is 90% or more.
3

PROPOSED METHOD

Our goal is to propose a procedure that allows hospital decision makers to select the best feasible bed
allocation design, i.e. the design which optimizes the main objective while satisfies all constraints. Both
the main objective and the multiple constraint measures need to be estimated via simulation. This is done
by integrating a constrained ranking and selection procedure for efficiently allocating the simulation budget
in comparing the designs and a search algorithm for generating the next sets of designs to be compared as
shown in Figure 2.
We use the Optimal Computing Budget Allocation for Constrained Optimization (OCBA-CO) which
efficiently allocates the simulation budget to the critical designs based on the means and variances in
selecting the best feasible alternative. When the number of alternatives is small enough for all designs to
be simulated, we can use this procedure directly. The sequential algorithm for implementing OCBA-CO
can be found in Lee et al. (2012).
For the searching algorithm, we use Nested Partitions method by Shi and Ólafsson (2000). In Nested
Partitions, the search space is partitioned into several regions. In each region, design points are randomly
sampled. Based on these samples, the most promising region is determined based on the promising index.
Once a region is declared as the most promising region, it will be further partitioned in the next iteration. The
other region will be aggregated as one partition called as the surrounding region. The most promising region
can be defined as the area where the best feasible alternative is located. This matches the characteristics
of OCBA-CO which emphasizes on selecting the best among a fixed number of alternatives instead of
accurately estimating the performance of each alternative. To avoid being trapped in a local optimal, Nested
Partitions allows backtracking if the best alternative at the current iteration is not located to any partitions
of the previous iterations most promising region.
4

NUMERICAL EXAMPLES

For the simulation, we use 4 warm-up days and afterwards 90 working days are simulated. The parameters
for the emergency patients arrivals are taken from Ahmed and Alkhamis (2009) as they are easier to generate.
Table 2 shows the arrival rates for each time period. The service time in the emergency department is
exponentially distributed with mean of 180 minutes. Aside from the arrival rates and the service time,
we adapt the data from the work in a Singapore hospital by Calugcug et al. (2009). For the emergency
patients, 64% of them are admitted. Table 3 shows the length of days for each specialty together with the
breakdown of the admitted emergency patients and elective patients for each specialty.
1919

Pujowidianto, Lee, Chen, Li, and Pedrielli

Figure 2: The framework for integrating Nested Partitions and OCBA-CO.
In this example, 5 specialties are considered and the average bed occupancy rate (BOR), the average
percentile of the turn-around-time (TAT99 ), and the number of overflow are measured. The maximum
limit for the TAT99 is γ1 = 480 (in minutes). For the number of overflow, several values of the maximum
limit are used to see the effect of the selection of the overflow limit. For the setting where only one value
is used, the limit of overflow is defined as 30%.

99th

4.1 Selection from a small number of alternatives
Pujowidianto et al. (2012) considered a simple case where there are only 5 alternatives. All designs are
simulated and there is no search needed. The constrained ranking and selection approach we use, namely
OCBA-CO and the commonly used Equal Allocation (EA) are being compared. The measurement of
effectiveness is the probability correct selection (PCS) which is estimated by the fraction of obtaining
Table 2: The arrival rates for each time period.
Time
Emergency Patients
Elective Patients

0
5.3
0

2
3.8
0

4
3
0

6
4.8
0

8
7
0.2

1920

10
8.3
0.4

12
9
0.7

14
7.8
4.7

16
7.8
5.3

18
8
3.2

20
6.5
0.8

22
3.3
0.3

Pujowidianto, Lee, Chen, Li, and Pedrielli
Table 3: The simulation parameters for each specialty.
Length of Stays (days)
Proportion of Admitted Emergency Patients
Proportion of Elective Patients

Medicine
6.3
50%
14%

Cardiac
3.8
14%
22%

Oncology
9.1
5%
20%

Surgery
4.8
18%
28%

Orthopedic
11.2
13%
16%

correct selection out of a pre-determined number of independent experiments. The results show that
OCBA-CO performs better than EA.
4.2 Selection from a large number of alternatives
In this paper, we consider the case where the number of alternatives is huge. For each specialty, the
minimum number of bed is 5 while the maximum number of bed is 500. In other word, the search space
is Θ = [5, 500]5 as there are 5 specialties. This translates to 3.002 × 1013 alternatives. Thus, a searching
algorithm is needed as it is virtually impossible to simulate all alternatives.
For the settings of Nested Partitions, we divide each axis of the most promising region into two.
In other word, there are 25 subregions as with 5 considered specialties. The first experiment shows the
result where 1 sample is taken from each region. The total computing budget for the first iteration is 215.
Subsequently, we increase the total computing budget by 50 in each of the iteration of Nested Partitions. For
the OCBA-CO, we run 5 initial replications for each design considered. Afterwards, there is an increment
of 50 replications to be allocated to the designs until the total computing budget in each of the NP iteration
is exhausted. Figure 3 shows that NP+OCBA-CO is able to converge in terms of the main objective value
as the search algorithm progresses.

Figure 3: Convergence of NP+OCBA-CO in terms of the main objective BOR.
In addition, we run different values of the limits to observe the effect of these limits to the total number
of bed changes. Table 4 shows the effect of the limit on TAT99 while the effect of overflow limit can be
seen in Table 5. As expected, a stricter requirement results in a solution with higher total number of beds.
The solution to the case with a lower turn-around-time (TAT99 ) limit has a higher total number of beds so
as to reduce the waiting time. Similarly, reducing the allowed percentage of overflow follows in a higher
total number of beds due to the reduction in flexibility which decreases the pooling effect.

1921

Pujowidianto, Lee, Chen, Li, and Pedrielli
Table 4: Effect of TAT99 limit when the overflow limit is 50%.
Turn-around-time (TAT99 ) limit
480
360

Total number of bed
705
824

Table 5: Effect of overflow limit when the TAT99 limit is 480 minutes.
Percentage of overflow limit
50%
30%
10%
5

Total number of bed
705
802
920

CONCLUSION

In this paper, we formulate the bed allocation problem as a stochastically constrained optimization via
simulation. This allows the consideration of uncertainties embedded in patients arrival and service time
and the constraints on some of the performance measures. As simulation is computationally intensive,
we apply the OCBA procedure for constrained optimization. We provide an alternative for addressing the
stochastically constrained optimization via a black-box simulation by integrating OCBA-CO with Nested
Partitions method for selecting the best design given a huge discrete search space. The integrated procedure
is the methodological contribution and it is able to provide a guideline on how to select the best feasible
bed configuration.
The desire of the paper is to provide more motivations for hospital decision makers to use simulation
optimization as it is able to incorporate constraints in the performance measures. For those who prefer to
incorporate the constraints into a single objective, the constrained ranking and selection (R&S)procedure
by Hu and Andradóttir (2014) can be used. In terms of how to implement the integration of simulation,
R&S, and optimization algorithm, one can refer to SimOpt by Pasupathy and Henderson (2011) which
provides abundant examples on how to code them. One can also use the framework by Li et al. (2015)
which proposes an object-oriented discrete event simulation modeling for ease of development. Their
modeling paradigm facilitates the integration of simulation, efficient simulation budget allocation methods,
and search algorithms.
In practice, the parameters can be updated to model arrival and service time characteristics in a more
realistic manner. The potential configurations can be obtained by both the users preference and by searching
algorithms for randomly sampling the configurations out of the possible combinations. In addition, the
schedule of the elective patients in this paper is assumed to be given. When necessary, the model can
be extended to capture the interaction between the elective patients and the patients entering the bed
management unit from the emergency department. It is possible to do other what-if scenarios such as
changing the order of the overflow protocol. In the case where a single optimal solution is not preferred,
the OCBA method for selecting optimal subset can be explored. These show the flexibilities of simulation
optimization in addressing bed allocation problem.
ACKNOWLEDGMENTS
This work has been supported in part by National Science Foundation under Awards ECCS-1462409,
CMMI-1462787 and CMMI-1233376. The authors would like to thank three anonymous reviewers and
the committee for their constructive comments.
REFERENCES
Ahmed, M. A., and T. M. Alkhamis. 2009. “Simulation Optimization for an Emergency Department
Healthcare Unit in Kuwait”. European Journal of Operational Research 198 (3): 936–942.

1922

Pujowidianto, Lee, Chen, Li, and Pedrielli
Akkerman, R., and M. Knip. 2004. “Reallocation of Beds to Reduce Waiting Time for Cardiac Surgery”.
Health Care Management Science 7 (2): 119–126.
Alkhamis, T. M., and M. A. Ahmed. 2004. “Sequential Stochastic Comparison Algorithm for Simulation
Optimization”. Engineering Optimization 36 (5): 513–524.
Andradóttir, S., D. Goldsman, and S.-H. Kim. 2005. “Finding the Best in the Presence of a Stochastic
Constraint”. In Proceedings of the 2005 Winter Simulation Conference, edited by M. E. Kuhl, N. M.
Steiger, F. B. Armstrong, and J. A. Joines, 732–738. Piscataway, New Jersey: Institute of Electrical
and Electronics Engineers, Inc.
Bagust, A., M. Place, and J. W. Posnett. 1999. “Dynamics of Bed Use in Accommodating Emergency
Admissions: Stochastic Simulation Model”. BMJ 319 (7203): 155–158.
Brailsford, S. C., W. J. Gutjahr, M. S. Rauner, and W. Zeppelzauer. 2007. “Combined Discrete-Event
Simulation and Ant Colony Optimisation Approach for Selecting Optimal Screening Policies for
Diabetic Retinopathy”. Computational Management Science 4 (1): 59–83.
Calugcug, C., P. Koe, S. Lam, M. Cordova, and Y. Yu. 2009. “Bed Capacity Planning”. Technical report, IE
3100R Systems Design Project, Department of Industrial and Systems Engineering, National University
of Singapore, Singapore.
Cochran, J. K., and A. Bharti. 2006. “Stochastic Bed Balancing of an Obstetrics Hospital”. Health Care
Management Science 9 (1): 31–45.
El-Darzi, E., C. Vasilakis, T. Chaussalet, and P. H. Millard. 1998. “A Simulation Modelling Approach to
Evaluating Length of Stay, Occupancy, Emptiness and Bed Blocking in a Hospital Geriatric Department”.
Health Care Management Science 1 (2): 143–149.
Fu, M. C. 2002. “Optimization for Simulation: Theory vs. Practice”. INFORMS Journal on Computing 14
(3): 192–215.
Fu, M. C., G. Bayraksan, S. G. Henderson, B. L. Nelson, W. B. Powell, I. O. Ryzhov, and B. Thengvall. 2014.
“Simulation Optimization: A panel on the State of the Art in Research and Practice”. In Proceedings
of the 2014 Winter Simulation Conference, edited by A. Tolk, S. Y. Diallo, I. O. Ryzhov, L. Yilmaz,
S. Buckley, and J. A. Miller, 3696–3706. Piscataway, New Jersey: Institute of Electrical and Electronics
Engineers, Inc.
Goldman, J., H. A. Knappenberger, and J. C. Eller. 1968. “Evaluating Bed Allocation Policy with Computer
Simulation”. Health Services Research 3 (2): 119.
Gorunescu, F., S. I. McClean, and P. H. Millard. 2002. “Using a Queueing Model to Help Plan Bed
Allocation in a Department of Geriatric Medicine”. Health Care Management Science 5 (4): 307–312.
Harper, P. R., and A. K. Shahani. 2002. “Modelling for the Planning and Management of Bed Capacities
in Hospitals”. Journal of the Operational Research Society 53 (1): 11–18.
Holm, L. B., H. Lurås, and F. A. Dahl. 2013. “Improving Hospital Bed Utilisation through Simulation
and Optimisation: With Application to a 40% Increase in Patient Volume in a Norwegian General
Hospital”. International Journal of Medical Informatics 82 (2): 80–89.
Hu, L., and S. Andradóttir. 2014. “A Penalty Function Approach for Simulation Optimization with Stochastic
Constraints”. In Proceedings of the 2014 Winter Simulation Conference, edited by A. Tolk, S. Y. Diallo,
I. O. Ryzhov, L. Yilmaz, S. Buckley, and J. A. Miller, 3730–3736. Piscataway, New Jersey: Institute
of Electrical and Electronics Engineers, Inc.
Jun, J., S. Jacobson, and J. Swisher. 1999. “Application of Discrete-Event Simulation in Health Care
Clinics: A Survey”. Journal of the Operational Research Society 50 (2): 109–123.
Keshtkar, L., K. Salimifard, and N. Faghih. 2015. “A Simulation Optimization Approach for Resource
Allocation in an Emergency Department”. QScience Connect 2015 (1): 8.
Lapierre, S. D., D. Goldsman, R. Cochran, and J. DuBow. 1999. “Bed Allocation Techniques Based on
Census Data”. Socio-Economic Planning Sciences 33 (1): 25–38.

1923

Pujowidianto, Lee, Chen, Li, and Pedrielli
Lee, L. H., N. A. Pujowidianto, L.-W. Li, C. Chen, and C. M. Yap. 2012. “Approximate Simulation Budget
Allocation for Selecting the Best Design in the Presence of Stochastic Constraints”. IEEE Transactions
on Automatic Control 57 (11): 2940–2945.
Li, H., Y. Zhu, Y. Chen, G. Pedrielli, and N. A. Pujowidianto. 2015. “The Object-oriented Discrete
Event Simulation Modeling: A Case Study on Aircraft Spare Part Management”. In Proceedings of
the 2015 Winter Simulation Conference, edited by L. Yilmaz, W. K. V. Chan, I. Moon, T. M. K.
Roeder, C. Macal, and M. D. Rossetti, 3514–3525. Piscataway, New Jersey: Institute of Electrical and
Electronics Engineers, Inc.
Pasupathy, R and Henderson, S. G. 2011. “SimOpt”. Accessed Apr. 6, 2016. http://www.simopt.org.
Pasupathy, R., and S. G. Henderson. 2011. “SimOpt: A Library of Simulation Optimization Problems”. In
Proceedings of the 2011 Winter Simulation Conference, edited by S. Jain, R. Creasey, J. Himmelspach,
K. White, and M. Fu, 4075–4085. Piscataway, New Jersey: Institute of Electrical and Electronics
Engineers, Inc.
Pujowidianto, N. A., L. H. Lee, C. M. Yap, and C. H. Chen. 2012. “Efficient Simulation-Based Comparison
for Hospital Bed Allocation”. In Proceedings of the IIE Asian Conference 2012, 229–236.
Ridge, J. C., S. K. Jones, M. S. Nielsen, and A. K. Shahani. 1998. “Capacity Planning for Intensive Care
Units”. European Journal of Operational Research 105 (2): 346–355.
Shi, L., and S. Ólafsson. 2000. “Nested Partitions Method for Global Optimization”. Operations Research 48
(3): 390–407.
Sprivulis, P. C., J.-A. Da Silva, I. G. Jacobs, A. R. L. Frazer, and G. A. Jelinek. 2006. “The Association between
Hospital Overcrowding and Mortality among Patients Admitted via Western Australian Emergency
Departments”. Medical Journal of Australia 184 (5): 208.
Tanfani, E., A. Testi, and R. Alvarez. 2010. “Operating Room Planning Considering Stochastic Surgery
Durations”. International Journal of Health Management and Information 1 (2): 167–183.
Teow, K. L., and W. S. Tan. 2008. “Allocation of Hospital Beds in an Existing Hospital”. Journal of
Operations and Logistics 2:2.
Wang, Y., L. H. Lee, E. P. Chew, S. S. W. Lam, S. K. Low, M. E. H. Ong, and H. Li. 2015. “Multi-objective
Optimization for a Hospital Inpatient Flow Process via Discrete Event Simulation”. In Proceedings
of the 2015 Winter Simulation Conference, edited by L. Yilmaz, W. K. V. Chan, I. Moon, T. M. K.
Roeder, C. Macal, and M. D. Rossetti, 3622–3631. Piscataway, New Jersey: Institute of Electrical and
Electronics Engineers, Inc.
Zhang, Y., M. L. Puterman, M. Nelson, and D. Atkins. 2012. “A Simulation Optimization Approach to
Long-Term Care Capacity Planning”. Operations Research 60 (2): 249–261.
AUTHOR BIOGRAPHIES
NUGROHO A. PUJOWIDIANTO is an R&D Writing System Engineer in the Home Printing Solutions
at HP Inc. He received his B.Eng. (Mechanical Engineering) degree from Nanyang Technological University in 2006 and his Ph.D. degree from the Department of Industrial and Systems Engineering, National
University of Singapore in 2013. His research interests include simulation optimization and its application
in healthcare. His email address is nugroho@hp.com.
LOO HAY LEE is an Associate Professor and Program Director in the Department of Industrial and
Systems Engineering, National University of Singapore. He received his B. S. (Electrical Engineering)
degree from the National Taiwan University in 1992 and his S. M. and Ph. D. degrees in 1994 and 1997
from Harvard University. He is currently a senior member of IEEE, a committee member of ORSS, and a
member of INFORMS. His research interests include production planning and control, logistics and vehicle
routing, supply chain modeling, simulation-based optimization, and evolutionary computation. His email
address is iseleelh@nus.edu.sg.

1924

Pujowidianto, Lee, Chen, Li, and Pedrielli
CHUN-HUNG CHEN is a Professor of Systems Engineering and Operations Research at George Mason
University and is also affiliated with National Taiwan University. Dr. Chen has led research projects in
stochastic simulation and optimization, sponsored by the NSF, NSC, FAA, Air Force, and NASA. He served
as Co-Editor of the Proceedings of the 2002 Winter Simulation Conference and Program Co-Chair for 2007
Informs Simulation Society Workshop. He has served on the editorial boards of IEEE Transactions on
Automatic Control, IEEE Transactions on Automation Science and Engineering, IIE Transactions, Journal
of Simulation Modeling Practice and Theory, and International Journal of Simulation and Process Modeling. He received his Ph.D. degree from Harvard University in 1994. His email address is cchen9@gmu.edu.
HAOBIN LI is a Scientist in Institute of High Performance Computing, under Agency for Science, Technology and Research (A*STAR) of Singapore. He received his B.Eng. degree (1st Class Honors) in 2009
from the Department of Industrial and Systems Engineering at National University of Singapore, with minor
in Computer Science; and Ph.D. degree from the same department in 2014. He has research interests in
operation research, simulation optimization and designing high performance optimization tools which are
ready for practical industrial use. His email address is lihb@ihpc.a-star.edu.sg.
GIULIA PEDRIELLI is a Research Fellow in the Department of Industrial and Systems Engineering, National University of Singapore. Her research focuses on stochastic simulation-optimization in both single and
multipleobjectives framework. She is developing her research in meta-model based simulation optimization
and learning for simulation and simulation optimization. Her email address is giulia.pedrielli.85@gmail.com.

1925

Proceedings of the 2016 Winter Simulation Conference
T. M. K. Roeder, P. I. Frazier, R. Szechtman, E. Zhou, T. Huschka, and S. E. Chick, eds.

DISCRETE EVENT OPTIMIZATION: WORKSTATION AND BUFFER ALLOCATION
PROBLEM IN MANUFACTURING FLOW LINES
Giulia Pedrielli

Mengyi Zhang
Andrea Matta
Department of Industrial Engineering & Management
School of Mechanical Engineering
Shanghai Jiao Tong University
800 Dongchuan Road
Shanghai, 200240, CHINA

School of Computing, Informatics,
& Decision Systems Engineering
Arizona State University
699 S Mill Avenue
Tempe, AZ 85281, USA

ABSTRACT
Resource and buffer allocation problems are well-known topics in manufacturing system research. A proper
allocation of resource and space can significantly improve the system performance and reduce the investment
cost. However, few works consider the joint problem because of its complexity. Recent research has shown
that Discrete Event Optimization (DEO) framework, an integrated simulation-optimization approach based
on mathematical programming, can be used to optimize buffer allocation of production lines, such as
open and closed flow lines and pull controlled manufacturing systems. This paper proposes mathematical
programming models for solving the joint workstation and buffer allocation problem in manufacturing
flow lines constrained to a given target throughput. The problem is formulated in two different ways: an
exact model using mixed integer linear programming formulation and approximate models using linear
programming formulation. Numerical analysis shows that efficiency and accuracy can be both achieved by
using approximate formulations in a math-heuristic procedure.
1

INTRODUCTION

Both resource allocation problems and buffer allocation problems are well-known topics of research in
manufacturing system design. Among all related literatures, only a few works dealt with joint resource and
buffer allocation problems, and all of them solved joint problems assuming the system layout is given, i.e.
finding out the service rate and buffer capacity of each workstation other than designing the networks. The
joint optimization of both server and buffer allocation of a single station system was solved by Shanthikumar
and Yao (1987). Hillier and So (1995) proposed an enumeration method to find the optimal number of
servers and buffer capacities in open networks. Spinellis, Papadopoulos, and Smith (2000) used a simulated
annealing algorithm to solve the same joint allocation optimization problem for long production lines.
Woensel et al. (2010) discussed the problem of acyclic configured M/G/c/K queuing networks under the
assumption of Poisson arrivals and exponential service rates. To solve this problem, they used Lagrangian
relaxation to approximate the joint buffer and server optimization problem, Powell’s search method to
optimize the relaxed problem, and a two-moment approximation to compute the mean throughput.
The whole problem analyzed in this paper can be decomposed as workstation allocation, workload
allocation and buffer allocation. Workstation allocation and workload allocation are both resource allocation
problems in manufacturing systems. Some examples of resource allocation are the server allocation, the
assembly line balancing, the machine grouping problems in cellular manufacturing, the machine loading
and tool allocation problems in flexible manufacturing systems. The server allocation problem has the goal
to allocate parallel servers at each workstation (Boxma, Kan, and Vliet 1990). Assembly line balancing

978-1-5090-4486-3/16/$31.00 ©2016 IEEE

2879

Zhang, Matta, and Pedrielli
problems partition tasks among workstations arranged along a flow oriented material handling equipment
constrained by a cycle time, where the number of workstations is fixed. The survey of Becker and
Scholl (2006) summarized assembly line balancing problems and the related methods for optimization and
evaluation. Machine grouping problems in cellular manufacturing systems allocate different machines to
each cell to maximize compatibility between machines and parts or to seek a trade-off between machine
cost and intercell movement cost (Gunasingh and Lashkari 1989). Machine loading and tool allocation
problems in flexible manufacturing systems help to decide which tools and operations of each part family
are allocated to which machine (Sarin and Chen 1987). The resources allocated in these problems are
machines. Demir, Tunali, and Eliiyi (2014) reviewed 110 articles on buffer allocation problems. Most of
them proposed different algorithms like simulated annealing, tabu search and evolutionary algorithms for
solving buffer allocation problems having an objective function of the throughput maximization. Only a
few articles solved the optimization under the objective of minimization of total buffer space.
The joint problem to be solved in this paper, however, differs from all problems in previous works.
Existing literatures deal with either resource allocation, or buffer allocation or joint server and buffer
allocations of manufacturing systems where the number of workstations is fixed. This paper will help to
design an open flow line by providing the total number of workstations, the workload at each workstation
and the buffer capacity at each stage subject to a target throughput. Therefore, the analyzed problem is
more general than others because it embraces three different problems related one each other.
Manufacturing systems are Discrete Event Systems (DES) whose simulation process can be analytically
modeled into Mathematical Programming (MP) formulation (Chan and Schruben 2003). The times at which
events occur in the simulation is the solution of this MP under the objective of minimization of the sum
of all event times. Solving the MP model means finding the evolution trajectory of the system during
the simulation. Optimization constraints like limited buffer capacity and a target throughput can also
be formulated in the MP. This Discrete Event Optimization (DEO) framework, an integrated simulationoptimization method based on the MP, is proposed to optimize buffer allocation problem for a class of
queuing systems, such as open flow lines (Matta 2008) and pull control manufacturing systems (Pedrielli,
Matta, and Alfieri 2015b). The optimal solution from this method is the global optimal based on one
simulation sample path. Other examples of enhancement of the DEO approach can be found in Tan (2015),
Stolletz and Weiss (2013). Pedrielli (2013); Pedrielli, Matta, and Alfieri (2015a); Pedrielli, Matta, and
Alfieri (2016) proposed a more general framework, i.e. not tailored to a specific simulation optimization
problem. Specifically, Pedrielli, Matta, and Alfieri (2016) proposed a formal procedure that encompasses all
the steps from the description of Event Relationship graphs (ERGs) for simulation-optimization (ERGLite
formalism) to the generation of the mathematical programming formulation.
In this paper, we use the DEO framework to solve the joint workstation and buffer allocation problem.
This work differs from previous researches in two aspects: (1) it presents a Mixed Integer Linear Programming
(MILP) formulation as an exact representation of the joint workstation and buffer allocation optimization.
(2) it develops a math-heuristic algorithm consisting of three steps based on Linear Programming (LP)
approximate models of the system. This algorithm finds out the number of workstations in the first step and
then allocates workload and buffer space. The MILP formulation of the joint problem, the LP approximate
models for workstation allocation and the math-heuristic algorithm are original contributions.
This paper is organized as follows. The problem is described in the next section. The exact model and
the approximate models using Mathematical Programming Representation (MPR) of DES for simulationoptimization are formulated in section 3. The three-step math-heuristic algorithm is introduced in section
4. Section 5 reports the application of the math-heuristic to some test cases. Finally, conclusions are drawn
in the last section.
2

PROBLEM DESCRIPTION

Manufacturing systems considered in this paper are open flow lines composed of workstations and buffers
(Figure 1). Processing times are randomly distributed, thus workstations can be assumed perfectly reliable.

2880

Zhang, Matta, and Pedrielli
Parts are processed from the first workstation to the last one sequentially. A workstation cannot process
more than one part at a certain time, and a part cannot be processed by more than one workstation at the
same time. Parts wait in the ( j − 1)th buffer when jth workstation is working on a previous part. Because
of random processing times and limited buffer capacities, workstations can be either working, starving or
blocked. The last workstation is never blocked.

Figure 1: Example of open flow line with 3 workstations.
We want to minimize the investment cost of the system while guaranteeing a minimum production rate.
The number of workstations influences the production rate. Since it is assumed that the total processing
time to complete a part is given, the longer the line, the higher the throughput. However, the workload and
buffer allocation problems change depending on the number of workstations. For instance, a 10-workstation
line requires allocating 9 buffers, and splitting the process cycle in 10 partitions, but a 2-workstation line
only requires allocating 1 buffer, and splitting the process cycle in 2 partitions. Therefore, workload and
buffer allocations are two problems nested in the workstation allocation problem. It is clear that a joint
optimization can be more effective.
Parameters for solving this joint problem are the expected total processing times of parts, distributions
of processing times at each stage, the target throughput and the random numbers used to generate processing
times in simulation. Expected total processing times of parts can be different, which makes the method
presented in this paper be proper also to flexible manufacturing lines. Furthermore, processing times may
not necessarily be exponential, which is usually an assumption in other researches on open flow lines.
The joint design problem will provide the number of workstations needed, the workload allocated among
workstations and the buffer capacity at each stage given a target throughput where the total cost of the flow
line is minimized. Workload of a workstation is defined as the proportion of expected processing time at
the workstation, therefore it is a real number between 0 and 1. The workload allocated to workstations can
also be limited by additional constraints related to manufacturing process, e.g. a bottleneck workstation,
minimum workload at some workstations because of some special processing techniques, etc. The objective
function, i.e. the total cost of a flow line, consists of workstation cost and buffer cost, and unit costs of
both workstation and buffer space are given.
3

MODELING

3.1 Notation
In this section, according to the approach in Pedrielli, Matta, and Alfieri (2016), we present the ERGL
model (Figure 2), which is then explained through the related integrated MPR with the notations below.
Parameters
UM : the upper bound of workstation number.
UB : the upper bound of buffer capacity at each stage.
N : the total number of parts in simulation experiment.
D : the number of parts in warm-up period.
CM : the unit workstation cost.
CB : the unit buffer capacity cost.
AM : the adjusted unit workstation cost parameter in the approximate model.

2881

Zhang, Matta, and Pedrielli
AB : the adjusted unit time buffer cost parameter in the approximate model.
Ai : the arrival time of ith part.
Ti : the expected total processing time of part i, which is the sum of processing time at all stages of the
part.
α ∗ : the target throughput.
zi, j : random numbers used to generate processing times ti, j .
Event time decision variables
ti, j ∈ [0, +∞) : the processing time of part i at workstation j.
Fi, j ∈ [0, +∞) : the finishing time of part i at workstation j.
Optimization decision variables
m j ∈ {0, 1} : if the jth workstation is allocated in the flow line, m j = 1. Otherwise m j = 0.
s j ∈ [0, 1) : workstation workload, the proportion of workload allocated at the jth workstation.
x j,k ∈ {0, 1} : if capacity of jth buffer is k − 1, x j,k = 1. Otherwise, x j,k = 0.
r j,k ∈ [0, +∞) : time buffer capacity of the jth workstation (in the approximate model).
3.2 Exact MILP Model
The joint workstation and buffer allocation problem can be formulated in an MILP model that integrates
both simulation and optimization. The model is formulated as follows:
UM −1 UB +1

UM

min{CM

∑ m j +CB

j=1

∑ ∑ (k − 1)x j,k }

j=1 k=1

Subject to:
UM

∑ sj = 1

(1)

j=1

s j ≤ m j,
m j−1 ≥ m j ,

∀ j = 1, 2, . . . ,UM

(2)

∀ j = 2, . . . ,UM

(3)

UB +1

∑

∀ j = 1, 2, . . . ,UM − 1

x j,k = 1,

(4)

k=1

∀ j = 1, 2, . . . ,UM , ∀i = 1, 2, . . . , N

ti, j = φ (Ti s j , zi, j ),

Fi,1 − ti,1 ≥ Ai ,

∀i = 1, 2, . . . , N

(5)
(6)

Fi+1, j − Fi, j − ti+1, j ≥ 0,

∀ j = 1, 2, . . . ,UM , ∀i = 1, 2, . . . , N − 1

(7)

Fi, j+1 − Fi, j − ti, j+1 ≥ 0,

∀ j = 1, 2, . . . ,UM − 1, ∀i = 1, 2, . . . , N

(8)

Fi+k, j − Fi, j+1 − ti+k, j + (1 − x j,k )M ≥ 0,

∀ j = 1, 2, . . . ,UM − 1, ∀k = 1, . . . ,UB + 1, ∀i = 1, 2, . . . (9)
N −D
≥ α∗
(10)
FN,UM − FD,UM

Constraint (1) states that the workload is completely allocated to the flow line. Constraints (2) describe
that when the workload allocated to a workstation is non-zero, this workstation is allocated to the system;
otherwise if the workload is zero, the related workstation is not allocated. Constraints (3) impose that all
workstations allocated are in the first part of the line, the time for parts passing through unused workstations
is 0 and this does not influence the manufacturing process in front. Constraints (6)-(9) describe the
2882

Zhang, Matta, and Pedrielli
production process which is presented with the ERGL model in Figure 2. Constraints (6) are derived from
arcs from Ai to Fi,1 , and it states that the ith part arrives at the line at time Ai . Constraints (7) are derived
from horizontal arcs in the ERGL model, and state that one machine cannot process more than one part
at a certain time. Constraints (8) are derived from vertical arcs in the ERGL model, and impose that a
part cannot be processed by more than one machine at the same time ((6)-(8) (Chan and Schruben 2003)).
Constraints (9) are derived from the arcs from Fi, j+1 to Fi+k, j , and describe that buffer capacity is finite: if
capacity of jth buffer is equal to k − 1 (which means x j,k = 1), part i + k cannot enter the jth workstation
before the ith part leaves the ( j + 1)th workstation ((4) and (9) in Matta (2008)). Constraint (10) states
that the designed production line should reach a minimum target throughput α ∗ .

Figure 2: ERGlite Representation.
Constraints (5) deal with random generation of processing times, which are a function φ of the expected
value Ti s j and the random numbers zi, j . As the value of the decision variable s j changes, also the generated
processing times are modified accordingly. As we solve an MILP problem, the function φ should be a
linear function of variables s j to keep low the complexity of the model. Specifically, s j and zi, j can be
combined in an additive or a multiplicative way:
•

•

Additive combination A function like φ = Ti (s j + zi, j ) can be used, where zi, j follows a zero-mean
distribution. For example, if zi, j follows a uniform distribution on (-0.1, 0.1) and ti, j = Ti (s j + zi, j ),
then ti, j also follows a uniform distribution on (Ti (s j − 0.1), Ti (s j + 0.1)).
Multiplicative combination A function like φ = Ti s j f (zi, j ) can be used. In this case, distributions
of zi, j and ti, j do not necessarily have the same shape. For example, if zi, j is uniformly distributed
in interval (0,1), and ti, j is assumed to follow an exponential distribution with a mean Ti s j , then
ti, j = −Ti s j ln(1 − zi, j ).

2883

Zhang, Matta, and Pedrielli
Other constraints can also be considered to be more consistent with industrial reality if additional
knowledge is available on the process. For example, constraints (11) impose that the second workstation is
the bottleneck of the system. Another useful constraint is (12) which gives a lower bound to the workload
of the jth workstation.
s2 ≥ s j ,

∀j

s j ≥ 0.2

(11)
(12)

By solving this MILP model, the global optimal can be obtained using a single-replication experiment
under the DEO framework. As the replication length increases, the optimal solution epi-converges to the
optimum (Pedrielli et al. 2016). However, the number of variables and the number of constraints increase
significantly as N, UM or UB increases. Specifically, the number of binary variables and the number of
continuous variables in the model are UMUB and 2NUM + UM , respectively. The number of constraints
containing binary variables is NUMUB , and the number of continuous constraints is 3NUM . Therefore,
when designing long production lines or when considering long simulations, the computational complexity
can be very high.
3.3 Approximate Model
One reason for high complexity of the MILP is the large number of integer variables. Thus, replacing these
variables by continuous ones is important for solving long production line design or long simulations in
reasonable computation time.
Constraints (2)-(4) and (9) and the objective function contain the binary variables m j and x j,k . By
using constraints (13), Matta (2008) introduced an LP approximate formulation of buffer allocation binary
variables x j,k .
Fi+k, j − Fi, j+1 ≥ ti+k, j − r j,k

(13)

If the capacity of jth buffer is not less than k (which is equivalent to r j,k > 0), part i + k can enter
machine j before part i leaves machine j + 1. A larger r j,k means higher necessity to have the kth slot.
Variables r j,k are also known as time buffer capacity and were extensively studied in Matta (2008); Pedrielli
(2013); Pedrielli, Matta, and Alfieri (2015a); Pedrielli, Matta, and Alfieri (2015b).
The total buffer capacity formula in the objective function is replaced by
UM −1 UB

∑ ∑ r j,k .

j=1 k=1

The objective function is changed into formula (14), in which the minimization of workstation number
is guaranteed by giving higher weight on additional workstations.
UM −1 UB

UM

min{AM

∑

js j + AB

j=1

∑ ∑ r j,k }

(14)

j=1 k=1

where AM and AB are adjusted unit cost parameters.
The main advantage of this approximate LP model is the higher efficiency compared with the exact
model, while the disadvantage is the loss of accuracy, especially for thebuffer allocation problem. Indeed,
minimizing (14) leads that upstream workstations have higher workloads, and therefore, higher buffer
capacities may be needed for such an unbalanced line. To solve this problem, a three-step math-heuristic
algorithm is introduced in section 4.
2884

Zhang, Matta, and Pedrielli
4

THREE-STEP MATH-HEURISTIC ALGORITHM

The algorithm iteratively uses two models to approximately find out the optimal system configuration. The
procedure is illustrated in Figure 3. The approximate model can be decomposed into two models. One
model solves the workstation number with infinite buffers. The second model solves the workload and
buffer allocation problem with the fixed workstation number NM . This decomposition works under the
assumption that workstation cost is much higher than buffer cost, which means adding an extra workstation
is never considered as a good solution if target throughput can be fulfilled by increasing buffer capacity.

Figure 3: Algorithm outline.
The workstation number model consists of constraints (1), (5)-(8) and(10) and the following objective
function:
UM

min{ ∑ js j }.
j=1

The workload and buffer allocation model consists of constraints (1),(5)-(8),(10) and (13) and the
following objective function:
M−1 UB

min{ ∑

∑ r j,k }.

j=1 k=1

The detailed algorithm is described as follows.

2885

Zhang, Matta, and Pedrielli
Algorithm 1
Step 1 The workstation number
Solve the workstation number model, and the solution is an approximate workstation number NM0 .
Step 2 First iteration of workload and buffer allocation
Workload and buffer allocation model is solved with NM = NM0 .
if this model is feasible then
b = true
else
b = f alse
end if
Step 3 Tuning
if b = true then
while b = true do
Solve the workload and buffer allocation model with NM = NM − 1.
if The model is infeasible then
b = f alse
end if
end while
Solve the workload and buffer allocation model with NM = NM + 1.
else
while b = f alse do
Solve the workload and buffer allocation model with NM = NM + 1.
if The model is feasible then
b = ture
end if
end while
end if
5

NUMERICAL ANALYSIS

In this section the application of the proposed math-heuristic algorithm is reported on three cases. In all the
cases, the distribution of processing times of the second workstation is a symmetric triangular distribution
with width = 0.2Ti , i.e. ti,2 are distributed on (Ti (s2 − 0.1), Ti (s2 + 0.1)), zi,2 follows a triangular distribution
with minimum value −0.1, maximum value 0.1 and the peak of the probability density function at 0.
Therefore, function φ in constraint (5) becomes
ti,2 = Ti (zi,2 + s2 )
Processing times at other workstations are exponentially distributed, i.e. zi, j follows a uniform distribution
in (0, 1) and the following expression is used:
ti, j = −Ti s j ln(1 − zi, j ), j 6= 2.
All parts arrive at time 0 (Ai = 0, ∀i). Expected total processing times are 1 time unit for 50% of the
parts or 0.5 time unit for 50% of the parts. Boundaries of the problem are UM = 10 and UB = 20. The total
part number N is equal to 20000, and the warm-up period consists of 500 parts (identified with Welch’s
approach). Unit workstation cost CM is 100 and unit buffer slot cost CB is 1. The same joint allocation
problem is also solved using OptQuest in Arena by running 1000 iterations, where each iteration executes
10 simulations for comparison. Results are verified by simulating the optimal system in Arena with 100000

2886

Zhang, Matta, and Pedrielli
parts and checking the satisfaction of the throughput constraint (the throughput values are presented in
column - α verified in Table 3 with a half width 95% confidence level less than 0.01).
The first case is the design of a flow line with a bottleneck at the second workstation (s2 ≥ s j ). The
target throughput is varied from 1.5 to 6 parts per time unit. Figure 4 shows the total costs of the systems
derived from themath-heuristic and OptQuest. The cost provided by the heuristic is lower than that provided
by OptQuest by 8.2% on the average and the gap can be up to 31.1%. In Table 1, we compare two system
configurations for α ∗ = 5 by using different methods.

Figure 4: Case 1: comparison of math-heuristic and OptQuest.
Table 1: Case 1: optimal configurations (α ∗ = 5).
Method
Math-Heuristic
OptQuest

Number of
Workstations
4
5

Total buffer
capacity
56
31

Workload
0.25, 0.26, 0.24, 0.25
0.23, 0.24, 0.17, 0.12, 0.24

Stage buffer
capacity
19, 19, 18
16, 6, 1, 8

Total
cost
456
531

The second case is the design of more unbalanced lines with a constraint of bottleneck at the second
workstation (s2 ≥ 1.2s j ). The target throughput is varied from 1.5 to 6 parts per time unit. Other parameters
and distribution assumptions are the same as in case 1. Figure 5 shows the total costs of systems derived
from the math-heuristic and OptQuest. The cost provided by the heuristic is lower than that provided by
OptQuest by 12.7% on the average and the gap can be up to 38.2%. In Table 2, we compare two system
configurations for α ∗ = 5 by using different methods.
Table 2: Case 2: optimal configurations (α = 5).
Method
Math-Heuristic

Number of
Workstations
5

Total buffer
capacity
22

OptQuest

7

71

Workload
0.20, 0.24, 0.19, 0.17, 0.20
0.16, 0.20,
0.05, 0.14, 0.14, 0.17, 0.14

Stage buffer
capacity
7, 6, 5, 4

Total
cost
522

20, 5, 20, 5, 1, 20

771

Table 3 shows that solutions provided by the math-heuristic in both cases can guarantee the throughput
target.
In the third case, we choose two tests, each from the last two cases. The two tests are repeated using
the math-heuristic algorithm with 10 different random sample paths. α ∗ = 6 is chosen from case 1, and
2887

Zhang, Matta, and Pedrielli

Figure 5: Case 2: comparison of math-heuristic and OptQuest.
Table 3: Throughputs verified using long simulation N = 100000.

α∗
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6

Case 1
α verified
α verified
of math-heuristic of OptQuest
1.9
1.9
2.1
2.1
2.5
2.8
3.2
3.2
3.6
3.7
4.3
4.2
4.6
4.6
5.0
5.2
5.7
5.7
6.1
6.5

Case 2
α verified
α verified
of math-heuristic of OptQuest
1.9
1.6
2.1
2.1
2.8
2.8
3.2
3.1
3.5
3.6
4.2
4.3
4.6
4.6
5.3
5.7
5.6
6.7
6.2
6.8

α ∗ = 4 is chosen from case 2. Table 4 and Table 5 show the results of these experiments. Similar with
the first two cases, all results provided by the math-heuristic are better than the OptQuest results. It is
possible to notice that the solution found by the heuristic is quite stable. Indeed, the number of allocated
workstations does not change in the ten replications, whereas the total allocated buffer ranges from 59 to
63 (α ∗ = 6) and from 13 to 15 (α ∗ = 4).
The computation time is around 10 minutes on average for solving one problem using the math-heuristic
algorithm, while the time for OptQuest is around 20 minutes. Experiments show that the proposed algorithm
is both efficient and accurate.
CONCLUSION
This work proposes different MPRs and a math-heuristic algorithm based on the MPRs for solving the joint
workstation and buffer allocation problems, both of which are global search methods. Numerical results
show that the proposed math-heuristic is both efficient and accurate. However, the exact model in large
scale cannot be solved in reasonable computational time. Future work will be dedicated to solve efficiently
the integrated simulation-optimization model by using decomposition approaches from MILP theory.

2888

Zhang, Matta, and Pedrielli
Table 4: Case 3: results of 10 different sample paths using the same parameters and constraints as in case
1 with α ∗ = 6.
Number of
Workstations
5
5
5
5
5
5
5
5
5
5

Total buffer
capacity
59
59
59
60
62
60
62
63
62
63

0.21,
0.21,
0.21,
0.21,
0.21,
0.21,
0.21,
0.21,
0.21,
0.21,

Workload
0.21, 0.2, 0.19, 0.19
0.21, 0.2, 0.19, 0.19
0.21, 0.2, 0.19, 0.19
0.21, 0.19, 0.19, 0.2
0.21, 0.19, 0.19, 0.2
0.21, 0.19, 0.19, 0.2
0.21, 0.19, 0.19, 0.2
0.21, 0.2, 0.19, 0.19
0.21, 0.19, 0.19, 0.2
0.21, 0.2, 0.19, 0.19

Stage buffer
capacity
19, 13, 14, 13,
18, 14, 14, 13
18, 14, 14, 13
19, 14, 14, 13
19, 15, 15, 13
18, 14, 14, 14
19, 14, 15, 14
19, 15, 15, 14,
19, 14, 16, 13
19, 15, 15, 14

α verified
6
6.1
6.1
6
6
6
6.1
6.1
6
6.1

Table 5: Case 3: results of 10 different sample paths using the same parameters and constraints as in case
2 with α ∗ = 4.
Number of
Workstations
4
4
4
4
4
4
4
4
4
4

Total buffer
capacity
14
14
14
14
14
15
13
13
14
14

0.24,
0.24,
0.24,
0.24,
0.24,
0.24,
0.24,
0.24,
0.24,
0.27,

Workload
0.29, 0.22,
0.29, 0.22,
0.29, 0.22,
0.29, 0.22,
0.29, 0.22,
0.29, 0.22,
0.29, 0.22,
0.29, 0.22,
0.29, 0.22,
0.27, 0.21,

0.25
0.25
0.25
0.25
0.25
0.25
0.25
0.25
0.25
0.25

Stage buffer
capacity
5, 5, 4
5, 5, 4
5, 5, 4
5, 5, 4
5, 5, 4
6, 5, 4
5, 4, 4
5, 4, 4
5, 5, 4
6, 4, 4

α verified
4.2
4.2
4.2
4.2
4.2
4.3
4.2
4.2
4.2
4.3

REFERENCES
Becker, C., and A. Scholl. 2006. “A Survey on Problems and Methods in Generalized Assembly Line
Balancing”. European Journal of Operational Research 168 (3): 694–715.
Boxma, O. J., A. R. Kan, and M. V. Vliet. 1990. “Machine Allocation Problems in Manufacturing Networks”.
European Journal of Operational Research 45 (1): 47–54.
Chan, W. K., and L. W. Schruben. 2003. “Properties of Discrete Event Systems from Their Mathematical
Programming Representations”. In Proceedings of the 2003 Winter Simulation Conference, edited by
S. Chick, P. J. Sanchez, D. Ferrin, and D. J. Morrice, 496–502. New Orleans, Louisiana: Institute of
Electrical and Electronics Engineers, Inc.
Demir, L., S. Tunali, and D. T. Eliiyi. 2014. “The State of the Art on Buffer Allocation Problem: a
Comprehensive Survey”. Journal of Intelligent Manufacturing 25 (3): 371–392.
Gunasingh, K. R., and R. S. Lashkari. 1989. “Machine Grouping Problem in Cellular Manufacturing
Systems–an Integer Programming Approach”. The International Journal of Production Research 27
(9): 1465–1473.
Hillier, F. S., and K. C. So. 1995. “On the Optimal Design of Tandem Queueing Systems with Finite
Buffers”. Queueing Systems 21 (3-4): 245–266.

2889

Zhang, Matta, and Pedrielli
Matta, A. 2008. “Simulation Optimization with Mathematical Programming Representation of Discrete
Event Systems”. In Proceedings of the 2008 Winter Simulation Conference, edited by T. Jefferson,
J. Fowler, S. Mason, R. Hill, L. Moench, and O. Rose, 1393–1400. Miami, Florida: Winter Simulation
Conference.
Pedrielli, G. 2013. Discrete Event Systems Simulation-Optimization: Time Buffer Framework. Ph. D. thesis,
Mechanical Engineering Department, Politecnico di Milano, Italy.
Pedrielli, G., A. Matta, and A. Alfieri. 2015a. “Discrete Event Optimization: Single-Run Integrated
Simulation-Optimization Using Mathematical Programming”. In Proceedings of the 2015 Winter Simulation Conference, edited by C. M. Macal, M. D. Rossetti, L. Yilmaz, I. Moon, W. K. Chan, and
T. Roeder, 3557–3568. Huntington Beach, California: Institute of Electrical and Electronics Engineers,
Inc.
Pedrielli, G., A. Matta, and A. Alfieri. 2015b. “Integrated Simulation-Optimization of Pull Control Systems”.
International Journal of Production Research 53 (14): 4317–4336.
Pedrielli, G., A. Matta, and A. Alfieri. 2016. “DEO: Integrated Simulation-Optimizatin of Queueing
Systems”. Working Paper.
Sarin, S. C., and C. S. Chen. 1987. “The Machine Loading and Tool Allocation Problem in a Flexible
Manufacturing System”. International Journal of Production Research 25 (7): 1081–1094.
Shanthikumar, J. G., and D. D. Yao. 1987. “Optimal Server Allocation in a System of Multi-Server Stations”.
Management Science 33 (9): 1173–1180.
Spinellis, D., C. Papadopoulos, and J. G. Smith. 2000. “Large Production Line Optimization Using Simulated
Annealing”. International Journal of Production Research 38 (3): 509–541.
Stolletz, R., and S. Weiss. 2013, June. “Buffer Allocation Using Exact Linear Programming Formulations
and Sampling Approaches”. In Preprints of the 2013 IFAC Conference on Manufacturing Modelling,
Management and Control. Saint Petersburg, Russia.
Tan, B. 2015. “Mathematical Programming Representations of the Dynamics of Continuous-Flow Production
Systems”. IIE Transactions 47 (2): 173–189.
Woensel, T. V., R. Andriansyah, F. Cruz, J. G. Smith, and L. Kerbache. 2010. “Buffer and Server Allocation
in General Multi-Server Queueing Networks”. International Transactions in Operational Research 17
(2): 257–286.
AUTHOR BIOGRAPHIES
MENGYI ZHANG is M.S. student of Department of Industrial Engineering and Management at Shanghai
Jiao Tong University. Her research focuses on simulation-optimization based on mathematical programming. Her email address is myra@sjtu.edu.cn.
ANDREA MATTA is Distinguished Professor at the Department of Industrial Engineering and Management at Shanghai Jiao Tong University, where he currently teaches stochastic models and simulation.
His research area includes analysis and design of manufacturing and health care systems. His email address
is matta@sjtu.edu.cn.
GIULIA PEDRIELLI is currently Assistant Professor for the School of Computing, Informatics, and Decision Systems Engineering at Arizona State University, and previously Research Fellow for the Department
of Industrial & Systems Engineering at National University of Singapore. Her research focuses on stochastic
simulation-optimization in both single and multipleobjectives framework. She is developing her research
in meta-model based simulation optimization and learning for simulation and simulation optimization. Her
email address is giulia.pedrielli.85@gmail.com.

2890

Proceedings of the 2016 Winter Simulation Conference
T. M. K. Roeder, P. I. Frazier, R. Szechtman, E. Zhou, T. Huschka, and S. E. Chick, eds.

A SIMULATION BASED CUT GENERATION APPROACH TO IMPROVE DEO EFFICIENCY:
THE BUFFER ALLOCATION CASE
Arianna Alfieri

Mengyi Zhang
Andrea Matta
Department of Industrial Engineering & Management
School of Mechanical Engineering
Shanghai Jiao Tong University
800 Dongchuan Road
Shanghai, 200240, CHINA

Department of Management &
Production Engineering
Politecnico di Torino
Corso Duca degli Abruzzi 24
Torino, 10129, Italy

Giulia Pedrielli
School of Computing, Informatics, & Decision Systems Engineering
Arizona State University
699 S Mill Avenue
Tempe, AZ 85281, USA

ABSTRACT
The stochastic Buffer Allocation Problem (BAP) is well known in several fields and it has been characterized
as NP-Hard. It deals with the optimal allocation of buffer spaces among stages of a system. Simulation
Optimization is a possible way to approximately solve the problem. In particular, we refer to the Discrete
Event Optimization (DEO). According to this approach, BAP simulation optimization can be modeled as
a Mixed Integer Programming model. Despite the advantages deriving from having a single model for
both simulation and optimization, its solution can be extremely demanding. In this work, we propose a
Benders decomposition approach to efficiently solve large DEO of BAP, in which cuts are generated by
simulation. Numerical experiment shows that the computation time can be significantly reduced by using
this approach.
Pedrielli, Matta, and Alfieri (2015) proposed a general DEO framework to model and optimize queueing
systems. The approach relies on the Event Relationship Graph Lite (ERG Lite) formalism to formulate
integrated simulation optimization mathematical programming models. ERG Lite is an extension of the
Event Relationship Graphs. The authors showed that the BAP can be solved by DEO (Matta 2008)
models that contain both simulation and optimization aspects. The simulation components control the event
times, by means of constraints dealing with the system dynamics. The optimization components, instead,
correspond to the binary variables and related constraints used for the capacity selection and minimization
of total buffer space.
Due to the presence of integer variables, the computation time required to solve the DEO model
significantly increases as the number of parts increases. However, a long simulation (i.e., a huge number of
parts) is needed to reduce the effect of the initialization bias and the uncertainty in the random components.
Weiss and Stolletz (2015) showed that the computation time could be substantially reduced by using a
Benders decomposition approach to solve the BAP. As a further step, this work targets the further reduction
of computation time by simulating, instead of solving, the mathematical programming problems, to generate
3710

Zhang, Matta, Alfieri, and Pedrielli
cuts in a Benders decomposition approach. Our approach is similar to the L-shape decomposition (Higle
and Sen 1991) with the main differences that we consider simulation optimization problems instead of two
stage stochastic ones, and we use simulation to generate cuts.
In general, Benders decomposition generates cuts by iterating between a master problem and a subproblem
(Benders 1962). The master problem contains the integer components and the generated cuts, while the
subproblem contains the continuous components and the integer variables as parameters. The procedure
can be summarized in three steps: 1) solve the master problem and use the variable values as input for the
subproblem; 2) solve the subproblem and use the value of the dual variables to generate either an optimality
or a feasibility cut; 3) add the generated cut to the master problem. At each iteration, the solution of the
master problem is a Lower Bound (LB) on the solution of the original problem, while the solution of the
subproblem is an Upper Bound (UB). The procedures stops when UB=LB.
In this work, we propose an innovative method that uses simulation, instead of mathematical programming, to generate cuts in Step 2. The variables and constraints of the subproblem are the simulation
variables and constraints in the original problem. The variables and constraints of the master problem
are the optimization variables and constraints of the original problem and the optimization cuts. Figure 1
reports a flowchart of the procedure for the DEO BAP model.
The main advantage of this
method is the significant improvement in computation time while solving long integrated simulation optimization Mixed Integer Linear Programming (MILP) models. In particular, 1) the master problem, although
it is a MILP, it is quite fast to solve
due to the small number of variables
Figure 1: Benders Decomposition approach for DEO.
and constraints and 2) the subproblem, which is a large LP, has a small
computation time since it is solved by using simulation instead of optimization techniques. This avoids
the exponential effort increase in the model size, since simulation shows an approximately linear increase.
As a result, much larger DEO models can be solved thus improving the quality of the solution.
REFERENCES
Benders, J. F. 1962. “Partitioning procedures for solving mixed-variables programming problems”. Numerische mathematik 4 (1): 238–252.
Higle, J. L., and S. Sen. 1991. “Stochastic decomposition: An algorithm for two-stage linear programs
with recourse”. Mathematics of operations research 16 (3): 650–669.
Matta, A. 2008. “Simulation Optimization with Mathematical Programming Representation of Discrete
Event Systems”. In Proceedings of the 2008 Winter Simulation Conference, edited by T. Jefferson,
J. Fowler, S. Mason, R. Hill, L. Moench, and O. Rose, 1393–1400. Miami, Florida: Winter Simulation
Conference.
Pedrielli, G., A. Matta, and A. Alfieri. 2015. “Discrete Event Optimization: Single-Run Integrated
Simulation-Optimization Using Mathematical Programming”. In Proceedings of the 2015 Winter Simulation Conference, edited by C. M. Macal, M. D. Rossetti, L. Yilmaz, I. Moon, W. K. Chan, and
T. Roeder, 3557–3568. Huntington Beach, California: Institute of Electrical and Electronics Engineers,
Inc.
Weiss, S., and R. Stolletz. 2015. “Buffer allocation in stochastic flow lines via sample-based optimization
with initial bounds”. OR Spectrum 37 (4): 869–902.

3711

Journal of Simulation (2012) 6, 237–252

r 2012 Operational Research Society Ltd. All rights reserved. 1747-7778/12
www.palgrave-journals.com/jos/

An HLA-based distributed simulation for
networked manufacturing systems analysis
G Pedrielli1*, M Sacco2, W Terkaj2 and T Tolio1
1
Politecnico di Milano, Dipartimento di Ingegneria Meccanica, MI, Italy; 2Istituto Tecnologie Industriali e
Automazione (ITIA), Consiglio Nazionale delle Ricerche (CNR), MI, Italy

Manufacturing systems can be thought as production networks nodes whose relations have a strong impact on design
and analysis of each system. Commercial simulators are already adopted to analyse complex networked systems, but the
development of a monolithic model can be too complex or infeasible when a detailed description of the nodes is not
available outside the ‘owner’ of the node. Then the problem can be decomposed modelling complex systems with
various simulators that interoperate in a synchronized manner. Herein, the integration of simulators is addressed by
taking as a reference the High Level Architecture (HLA). This paper proposes modiﬁcations to Commercial-off-theshelf Simulation Package Interoperability Product Development Group protocols and to use patterns of how HLA can
be effectively adopted to support Commercial Simulation Package interoperability: a new solution for the synchronous
entity passing problem and modiﬁcations to the Entity Transfer Speciﬁcation are presented. The resulting infrastructure
is validated and tested over an industrial case.
Journal of Simulation (2012) 6, 237–252. doi:10.1057/jos.2012.6; published online 15 June 2012
Keywords: complex manufacturing systems; discrete event simualtion; distributed simulation; HLA; interoperability reference models

1. Introduction
Nowadays, manufacturing companies have to face an
increasingly turbulent market characterized by demand
variability, unstable requirements from the clients and short
product lifecycles (Terkaj et al, 2009). In addition to the
problems related to the market, the performance of a
manufacturing system is deeply affected by its relations with
other systems. Indeed, every production system is not a
standalone unit, but a node in a production network
characterized by complex dynamics that should be considered
during the design and analysis phases (Wiendahl and Lutz,
2002). Both in the literature and in the industrial practice,
Discrete Event Simulation is used to analyse production and
logistics problems in various industrial domains (Law, 2007;
Smith, 2003). This is also due to the spreading of CommercialOff-The-Shelf (COTS) Simulation Packages (CSPs) that are
available on the market and provide a wide range of
functionalities (eg, visual building of the simulation model,
simulation run support, animation, etc).
However, the complexity of a simulation model becomes
hardly manageable when the relations between the nodes of
a production network must be considered, since the
modelling of a single node is already complex by itself and
requires speciﬁc expertise and information (Vàncza et al,
2008). Moreover, in real practice the developer of a
simulation model can hardly access all the information
*Correspondence: G Pedrielli, Politecnico di Milano, Dipartimento di
Ingegneria Meccanica, Via Giuseppe La Masa, 1 - 20155 Milano, Italy.
E-mail: giulia.pedrielli@mail.polimi.it

characterizing a production network because information
sharing is seen as a threat by most of the companies, thus
hindering the feasibility of developing a unique and
monolithic simulation model to evaluate the performance
of complex production networks.
A distributed simulation (DS) approach can be proposed
to face the aforementioned criticalities, but several problems
arise when trying to interoperate heterogeneous simulators
in real industrial cases, even though a standard like High
Level Architecture (HLA) (IEEE Standard a, b, c, 2000,
Kuhl et al, 1999 and IEEE Standard 2003) has already been
proposed to support the interoperability between simulators.
Indeed, the enhancement of HLA with additional complementary standards (Taylor et al, 2006a, b) and the deﬁnition
of a standard language for CSPs represent relevant and
not yet solved scientiﬁc and technical challenges (Hibino
et al, 2002; Straburger, 2001; Straburger, 2006b).
The realization of an effective integration between several
COTS is hindered by the lack of a unique simulation
language and a common standard for data modelling (Sacco
et al, 2011). Indeed, as long as different COTS do not share a
common language, the interaction between the HLA-Run
Time Infrastructure (RTI) and the simulator has to be
tackled in a dedicated way. An example is represented by the
work of Mustafee and Taylor (2006), who propose the
development of a manufacturing adapter to the simulation
software Simul8. The connection between the simulator and
the distributed environment is obtained, thanks to the CSP
controller middleware. This controller performs two speciﬁc
tasks: it communicates with Simul8 through its COM

238 Journal of Simulation Vol. 6, No. 4

interface and interacts with RTI using the HLA interface
speciﬁcation. In particular, the ﬁrst task is performed by
the Simul8 adapter. As highlighted in the aforementioned
paper, the adapter is tailored on the speciﬁc simulator and
the experience suggests that the development of general
purpose adapters may be more difﬁcult than it logically
seems.
On the other hand, the deﬁnition of a data model
describing the system under analysis would facilitate the
exchange of information between disparate software applications. In the manufacturing domain a lot of interest is
being given to standard information modelling (examples are
Core Manufacturing Simulation Data—CMSD—Lee et al,
2006, and Simulation Data Exchange—SDX—Sly and
Shreekanth, 2001).
Recently, the COTS Simulation Package InteroperabilityProduct Development Group (CSPI-PDG), within the
Simulation Interoperability Standards Organization (SISO),
worked on the deﬁnition of the CSP interoperability
problem (Interoperability Reference Models, IRMs) and
on a draft proposal for a standard to support the CSPs
interoperability (Entity Transfer Speciﬁcation, ETS). Nevertheless, an effective interoperability among CSPs is still far
to be reached in industrial contexts.
Boer and Verbraeck (2003) investigated the main beneﬁts
and criticalities related to the industrial application of HLA
by interviewing the actors involved in the problem
(eg, simulation model developers, software houses, HLA
experts). The results of the survey showed that CSPs vendors
do not see direct beneﬁts in using DS, whereas in industry
HLA is considered troublesome because of the lack of
experienced users and the complexity of the standard.
The remainder of this paper is structured as follows.
Section 2 presents preliminary results obtained by the
authors from the analysis of the literature on HLA applied
in civil domain. Section 3 presents the Problem Statement,
delving into the problem of CSP interoperability and the
IRMs. Section 4 analyses the literature and highlights some
of the open issues. Section 5 proposes a solution to the Type
A.2 IRM, a modiﬁcation to ETS, and a communication
protocol between the CSP and its adapter taking as a
reference the work already carried out by CSPI-PDG.
Section 6 addresses the implementation of the proposed
solution that is validated (Section 7) and then tested over a
realistic industrial case (Section 8). Finally, conclusions and
future developments are drawn in Section 9.

2. Literature review
Although HLA-based DS has received signiﬁcant attention
from the scientiﬁc research and is frequently adopted in
military applications, several issues arise when DS is
proposed outside the military environment (see Boer and
Verbraeck, 2003; Boer, 2005; Boer et al, 2006a b c; Alvarado

et al, 2008; Pedrielli et al, 2011). The literature related to the
application of HLA-based DS technique in the civil domain
and, in particular, the manufacturing domain was investigated aiming at:
 Individuating the speciﬁc ﬁelds in the civil domain where
DS is currently more applied.
 Identifying the motivations that lead to the use of DS
technique.
 Highlighting the main technical and scientiﬁc open issues
that still represent a hurdle for the use of DS.
The bibliography search was carried out by considering
the following keywords: Distributed Simulation, Operations
Research and Management, Commercial Simulation
Packages, Interoperability Reference Models, High Level
Architecture, Manufacturing Systems, Discrete Event Simulation, Manufacturing Application, Industrial Application
and Civil Applications. The use of these keywords brought
to identify 20 core papers based on the number of citations
(Fujimoto, 1998; Straburger, 2001; Linn and Chen, 2002;
Taylor et al, 2002, 2003, 2004; Banks et al, 2003; Boer and
Verbraeck, 2003; McLean et al, 2003; Straburger et al, 2003;
Wang et al, 2004, 2006; Boer, 2005; Boer et al, 2006a, b, c;
Lendermann, 2006; Straburger, 2006a; Lendermann 2007;
Zacharewicz et al, 2008; Liang et al, 2009; SISO-STD-0062010; Taylor, 2011; Yuan and Zhang, 2011). These papers
can be considered as introductory to the topic of DS in civil
domain. Starting from these papers the bibliographic search
followed the path of the citations, that is, works cited by the
core papers and papers citing the core ones were considered.
This search brought to the selection of 83 further papers.
The overall 103 papers were published mainly in the
following journals and conference proceedings: Advanced
Simulation Technologies Conference, European Simulation
Interoperability Workshop, European Simulation Symposium,
Information Sciences, International Journal of Production
Research, Journal of the Operational Society, Journal of
Simulation, Workshop on Principles of Advanced and Distributed Simulation and Winter Simulation Conference.
The collected papers addressing the HLA-based DS in
civil applications have been analysed and classiﬁed according
to the following criteria:
1. Speciﬁc ﬁeld of application of DS in the civil domain (eg,
Supply Chain Management (SCM), Health Care).
2. Motivation that justiﬁed the use of DS technique.
3. Technical issue that is addressed in the paper, proposing a
solution to an integration issue or enhancement to
services of the HLA architecture components.
Most of the papers can be classiﬁed according to more
than one criterion. In particular, over 70% of the papers
deal with technical issues, thus showing that HLA and DS
experts are putting a lot of effort in the enhancement and

G Pedrielli et al—An HLA-based distributed simulation for networked manufacturing systems analysis 239

extensions of HLA-based DS to face civil application
problems. In addition, more than 60% of the papers
propose the application of HLA-based DS in a speciﬁc
ﬁeld in the civil domain (eg, Rabe et al, 2006; Kubat et al,
2009).
Figure 1 shows which are the main ﬁelds of application of
DS: SCM (eg, Terzi and Cavalieri, 2004; Gan et al, 2007),
Manufacturing (eg, Taylor et al, 2005), Health Care (eg,
Mustafee et al, 2009) and Production Scheduling and
Maintenance (eg, Uygun, 2006). Figure 1 clearly shows that
much attention is paid to SCM (Rabe et al, 2006) and
Manufacturing, whereas Health Care is acquiring larger
interest in the recent years.
A further analysis was carried out by considering only the
papers related to the manufacturing domain, aiming at
evaluating whether the contributions addressed real industrial case applications or test cases applications. This analysis
was performed to have a measure of how the solutions
proposed within the scientiﬁc community have then been
implemented over real cases.
The results show that only 22% of the papers address a
real case. This result conﬁrms the outcomes obtained by
Boer (2005) in the analysis of the adoption of DS in the
manufacturing environment. Although solutions have been
developed for manufacturing domains, this technique is still
far from being adopted as an evaluation tool in industrial
companies because the end-users perceive HLA and DS as
an additional trouble rather than a promising approach
(Boer et al, 2006 c). As a consequence, a lot of effort is put in
the development of decision support systems that hide the
complexity of a distributed environment to the end user
(Raab et al, 2011).
Analyzing the contributor under the second criterion, the
main motivations leading to adopt an HLA-based DS for
civil applications were:
1. Complexity: The DS is used because a single developer is
not able to reproduce all the dynamics of the system that
consists of several complex subsystems.
2. Re-use: The DS is proposed to integrate pre-existing
simulators.

Figure 1

Domains of application.

3. Industrial secrecy: This motivation is similar to complexity, but in this case the main problem for realizing a
unique simulation model consists in the lack of shared
information.
Complexity is clearly the main reason for the use of DS
technique, as highlighted in McLean and Riddick, 2000;
Linn and Chen, 2002; Gan et al, 2007; Kubat and Uygun,
2007. On the other hand, the low percentage (E5.6% of all
papers analysed) of papers using DS to cope with industrial
secrecy can be partially traced back to the lack of real
industrial applications that still characterizes DS in civil
environment.
Concerning the third criterion two main technical issues
were identiﬁed analyzing the collected papers:
1. Integration of discrete event simulators (CSP): Several
discrete event simulators are put together and synchronized by means of the services offered by the HLA
infrastructure.
2. Enhancements to RTI services (please refer to Section 4.1
for more details on HLA integration infrastructure).
The integration of CSPs is the most addressed technical
issue, nonetheless the integration of real CSPs (ie, not
general purpose programming languages) still represents a
challenging topic. Figure 2 gives a picture of the main
simulators that have been adopted (eg, Park et al, 2005; Gan
et al, 2005; Wang et al, 2007). It can be noticed that CSP
Emulators (eg, Wang et al, 2006; Pedrielli et al, 2011) are still
one of the most used solutions because of the problems
related to interoperating real CSPs. These problems are
mainly caused by the lack of data and information mapping
between simulators and the possibility to interact (eg, send
and receive information, share the internal event list), while
the simulation is running.
The enhancement of the RTI services is another key
research topic (Fujimoto et al, 2007; Al-Zoubi and
Wainer, 2009). In particular, the scientiﬁc papers deal
mainly with two open issues: (1) Time management

Figure 2

CSP adopted.

240 Journal of Simulation Vol. 6, No. 4

(eg, Peschlow and Martini, 2007; Malik, 2010); (2) Data
distribution management (eg, Wainer et al, 2010; Yuan
and Zhang, 2011). Time Management has received more
attention because it strongly inﬂuences the computational
performance of the DS.
The following conclusions can be drawn from the
literature analysis, providing also the motivations of this
paper:
 There is a lack of DS applications in the real manufacturing environment.
 The interoperability of CSPs still represents a technical
challenging problem.
 The HLA architecture components (RTI services) must be
extended and adapted to civil applications.

3. Problem statement
The analysis of networked manufacturing systems by means
of distributed discrete event simulation is addressed by
developing distinct simulators, each representing a single
sub-system, and connecting them according to the relations
characterizing the network. The work presented in this paper
is aimed at supporting the design and analysis tasks (see
Section 1) for the following classes of manufacturing systems
(Colledani and Tolio, 2005):
 Assembly/disassembly systems, where assembly (disassembly) machines take different components (part) from
one or more input buffer(s) and produce a single part
(parts of different types), which is (are) placed in a
downstream buffer (different output buffers).
 Split/merge systems, where different part types are
managed. A split machine receives all parts from a
single upstream buffer and then places the processed
parts in different downstream buffers according to the
part type and the adopted policy. A merging machine
receives the parts of different type from more than one
upstream buffer, but places all the processed parts in a
single downstream buffer. The split/merge systems
differ from the assembly/disassembly ones because
parts cannot be modelled as components of the same
product. For further details please refer to Colledani
and Tolio (2005).
 Closed loop systems, where the last machine of the system
is linked with the ﬁrst machine and the number of parts in
the system remains constant.
 General production lines with dedicated (ﬂexible) machines performing only one or more operations.
The simulation of these classes of manufacturing systems
via a distributed approach is strongly related to the
representation of the part and information ﬂow between

the subsystems, thus rising the need to formalize this kind of
transfer.
In literature (Taylor et al, 2004), the part transfer has been
formalized through the deﬁnition of the entity passing
problem where the term entity refers to elements that are
dynamically created and moved during a simulation (Taylor
et al, 2006b). The main result in the formalization of entity
passing problem was presented by CSPI-PDG with the
deﬁnition of Type A IRM, namely Entity Transfer. Figure 6
outlines the basic idea behind both Type A.1 and Type A.2
IRMs. The manufacturing system is decomposed into
subsystems consisting of workstations and buffers, and each
subsystem is associated with a different simulation model.
Mi represents the model of the i-th production subsystem
and Enij/Exij represent the j-th entry/exit point in Mi. Qik is
associated with the k-th buffer in Mi, Wih stands for the h-th
workstation in Mi, whereas the arrows represent the ﬂow of
entities. In Figure 3, an entity can be transferred from
workstation W1a to buffer Q2a.
Type A.1 IRM (named ‘General Entity Transfer’) is
deﬁned as the transfer of entities from one model to another,
that is, an entity leaves from a given place in a sending model
(eg, M1) at time T1 and arrives at a given place in a receiving
model (eg, M2) at time T2 (T1pT2). The departure and
arrival places can be buffers, workstations and so on
(eg, W1a is a departure place and Q2a is an arrival place).
According to Type A.1 IRM the entity transfer is always
feasible, thus no authorization is required before passing an
entity from a model to another. The entity transfer
represented in Figure 3 can be of Type A.1 IRM only if
the capacity of Q2a is unbounded.
Type A.2 IRM (named ‘Bounded Receiving Element’) is
deﬁned as the relation between a generic element in the
sending model and a bounded element in receiving model, so
that the feasibility of the entity transfer depends on the state
of the receiving element. For instance, an entity transfer is
blocked when the queue of the receiving element is full, even
if the entity is ready to leave at time T1 and would attempt to
arrive at the bounded element at time T2. Therefore, the
information about the target element state is needed by the
sending model, since it could stop the entity transfer. The
entity transfer represented in Figure 3 is of Type A.2 IRM if
the capacity of Q2a is bounded.
Type A.3 IRM (named ‘Multiple Input Prioritization’)
represents the case where an element of the receiving model
can receive entities from more than one sending model.
A problem arises if entities coming from different places
arrive at the same time (ie, there are simultaneous events)

Figure 3

Entity transfer.

G Pedrielli et al—An HLA-based distributed simulation for networked manufacturing systems analysis 241

Figure 4

IRMs distribution.

and the receiving element is not able to receive all of them
(Wang et al, 2006; Peschlow and Martini, 2007).
The literature review showed that around 21% of the
papers taken into consideration dealt with IRMs. Figure 4
shows how the IRMs addressed are distributed within the
sample of papers facing them (Taylor et al, 2004, 2006a, b,
2008, 2009; Peschlow and Martini, 2007).
The largest part of the papers deal with the basic IRM,
that is, the general entity transfer, since most of the
applications is related to SCM (ie, queues can often be
modelled as inﬁnite capacity as they represent inventory,
production or distribution centres).
The situation is slightly different if the manufacturing
domain is analysed. Indeed, IRM Type A.2 is largely
adopted in the case of manufacturing applications. The
reason is that, when a production system is modelled, the
decoupling buffers between workstations must be usually
represented as ﬁnite capacity queues.
Merge systems (see Figure 7), can be modelled using
Type A IRM. In particular, the receiving model will start
only when the batch of components is available, this can
be done deﬁning the entity in the receiving model as a
batch. The communication can be represented by Type
A.1 and A.2 based on the receiving model buffer capacity
being inﬁnite or ﬁnite. The case of split systems (see
Figure 6) can be modelled by IRMs Type A.1 and A.2
depending on the receiving model queue capacity (ﬁnite
or inﬁnite).
In addition, in many applications, it is necessary to deal
with management policies (eg, shared resources assignment,
simultaneous events and entity priority) requiring a solution
to IRM Type A.3 and IRM Type B. However, the
management policies still represent a challenge both in
terms of problem formalization and proposed solutions
(Taylor, 2011).
These results motivate the research into the ﬁeld of the
formalization of the interoperability issues in manufacturing
domain: the deﬁnition of interoperability issues is still an
ongoing research topic and the standardization of a solution
to these problems is far from being reached.

Figure 5

Reference architecture.

4. The problem of entity transfer
Past research contributions presented solutions to face
both Type A.1 and Type A.2 IRMs (Section 3). In particular, a reference architecture (Figure 5) for the DS (Taylor
et al, 2006b) and a protocol to manage the communication
between the architecture components (Taylor et al, 2006a, b;
Straburger, 2006a) were proposed.
Section 4.1 presents the reference architecture adopted in
this work and highlights the open issues, whereas Section 4.2
introduces the communication protocols and highlights their
major drawbacks.

4.1 Reference architecture
The general architecture shown in Figure 5 is taken as a
reference throughout this work and it is mainly based on
architecture proposed by Taylor et al (2006b). A detailed
description of the architecture components can be found in
Taylor et al (2006a, b). Each federate consists of a COTS
CSP, a model that is executed by the CSP, and the
middleware that is a sort of adaptor interfacing the CSP
with the RTI. The relationship between CSP, the middleware
and the RTI consists of two communication ﬂows: (1)
middleware-RTI, (2) CSP-middleware. The middleware
translates the simulation information in a common format
so that the RTI can share it with the federation. In addition,
the middleware receives and sends information from/to the
CSP.
Two main issues arise when the simulation information is
translated for the RTI:
 A common time deﬁnition and resolution is necessary. For
example, the time should be deﬁned as being the time

242 Journal of Simulation Vol. 6, No. 4

when an entity exits a source model and then instantaneously arrives at the destination model (ie, the deﬁnition
of time implies zero transit time) (Taylor et al, 2006a).
Alternatively, it should be deﬁned including the notion of
travel time, in this case the entity would arrive to
destination with a delay equal to the transfer time.
 The representation of an entity depends on how the
simulation model is designed and implemented in a CSP.
Indeed, the names that the modellers use to represent the
same entity might be different. A similar problem can
arise for the deﬁnition of simple datatypes. For example,
some CSPs use 32-bit real numbers, whereas others use
64-bit (Taylor et al, 2006a).
Since the aforementioned issues are related to the adopted
CSP and the decisions taken by the modelers, two
simplifying hypotheses have been made in this paper: all
the models have the same time deﬁnition and resolution
(ie, there is a relationship between how time is represented in
one CSP and another) and a mapping relationship exists
between the entity representations in the various models.
This paper addresses the transfer mechanism that is used to
move an entity from one model to another thanks to the
communications between middleware and RTI, and between
CSP and middleware.
CSPI-PDG proposed the ETS Protocol (Taylor et al,
2006a, b) to manage communication at middleware-RTI
level (see Section ‘ETS protocol’). In this paper, a
communication protocol based on Simulation Messages is
proposed to manage the communication between a CSP and
its middleware (or adapter). The communication protocol
was conceived for the DS of network of Discrete Event
Manufacturing Systems characterized by the transfer of
parts in the presence of buffers with ﬁnite capacity. The
presence of Simulation Messages is the main difference
between the reference architecture in Figure 5 and the
architecture proposed in Taylor et al (2006a).

include the novel interaction classes and deﬁne them as
publish and/or subscribe. The Parameter Table was modiﬁed
to include the proposed parameters for the interactions and
the Datatype table was also modiﬁed. For further details
please refer to (Taylor et al, 2006b).
Straburger (2006a, b) highlighted some relevant drawbacks in the ETS standard proposal:
 It is not possible to differentiate multiple connections
between any two models.
 ETS suggested interaction hierarchy does not work: a
federate subscribing to the superclass will never receive the
values transmitted in the interaction parameters.
 The speciﬁcation of user-deﬁned attributes is placed into a
complex datatype, this introduces new room for interoperability challenges as all participating federates have to
be able to interpret all of the attributes.
 There are some possibilities for misinterpretation in
the deﬁnition of ‘Entity’ and ‘EntityType’ introducing
changes in FOMs, whenever a new entity type is talked
about.
Furthermore, the ETS was not designed to manage the
Type A.2 IRM and the interaction class hierarchy refers to
the entity transfer without taking into account any
information on the state of the receiving buffer (eg, Q2a
and Q2b in Figure 6). The industrial cases deﬁned in Section
3 can be modelled only if the ﬁrst drawback of the list is
properly addressed: the simulation of a manufacturing
system in a distributed way may ask for the representation
of multiple connections between the models, thus requiring
multiple entry points in a receiving model (eg, Model 2 in
Figure 6) and/or multiple exit points in a sending model (eg,
Model 1 in Figure 7). The ETS draft standard should be
modiﬁed to manage Type A.2, IRM as well.

4.2 ETS protocol
The ETS protocol proposed by CSPI-PDG deﬁnes the
communication between the sending model and the receiving
model (ModelA and ModelB in Figure 5, respectively) at
RTI level by means of a special hierarchy of interaction
classes. An interaction class is deﬁned as a template for a set
of characteristics (parameters) that are in common within a
group of interactions (refer to IEEE HLA standard, 2000).
The middleware of the sending model instantiates a speciﬁc
interaction class and sends it to the RTI whenever an entity
has to be transferred.
After developing the interaction class hierarchy, following
the HLA standard, the Simulation Object Model and
Federation Object Model (FOM) were developed to include
the novel interactions and their parameters. In particular
extensions were proposed to the Interaction Class Table to

Figure 6 Multiple part type production system (eg, disassembly
or split system)—Case I.

Figure 7 Multiple part type production system (eg, assembly
or merge systems)—Case II.

G Pedrielli et al—An HLA-based distributed simulation for networked manufacturing systems analysis 243

5. A solution proposal for the type A.2 IRM
This section presents the solutions proposed to cope with the
problems related to the Type A.2 IRM, as highlighted in
Section 3. In particular, Section 5.1 proposes a modiﬁcation
to ETS, whereas Section 5.2 describes how information
is sent (received) between the CSP and its middleware.
Section 5.3 shows the new protocol aimed at managing the
communication between a CSP and its middleware. Finally,
the hypothesis underlying this protocol to minimize the use
of zero lookahead is shown in Section 5.4.

5.1 Proposal to modify ETS
The ETS standard proposal (Taylor et al, 2006a, 2006b) is
modiﬁed by deﬁning a new class hierarchy. In particular,
different subclasses of the transferEntity class are deﬁned to
face the drawbacks of the ETS Protocol (Section 4.2). The
resulting class hierarchy consists of the following classes
(Figure 8):
 transferEntity, as already deﬁned in the ETS protocol.
This superclass allows the federate subscribing to all the
instances of entity transfer. The instantiation of this class
is related to visualization and monitoring tasks.
 TransferEntityFromFedSourceEx is a novel subclass
deﬁned for every exit point, where FedSourceEx stands
for the name or abbreviation of a speciﬁc exit point in the
sending model. This class is useful to group the instances
of the transferEntity that are related to the source
federate, so that the FedSourceEx can subscribe to all
these instances without explicitly naming them.
 TransferEntityFromFedSourceExToFedDestEn is a novel
subclass deﬁned for each pair of exit point (Ex) of the
source federate (FedSource) and entry point (En) of the
receiving federate (FedDest). This class is instantiated
both when a sending model needs to transfer a part to a
speciﬁc entry point in the receiving model, and when a
receiving model needs to share information about a buffer
state or about the receipt of a part from a speciﬁc exit
point in a sending model.
The models both publish and subscribe to this subclass
that was designed to create a private communication
channel between the sending and the receiving model.
Therefore, if an entry point in the receiving model is
connected with multiple federates/exit points, then the
receiving federate has to inform about the state of the
entry point by means of multiple interactions, each
dedicated to a speciﬁc federate/exit point. This communication strategy is not the most efﬁcient in a generic
case, but it offers the possibility to deliver customized
information and adopt different priorities for the various
federates/exit points. This becomes fundamental in real
industrial applications where information sharing among

Figure 8

Interaction class hierarchy.

different subsystems is seen as a threat, thus rising the
need to design a protocol that creates a one-to-one
communication between each pair of exit/entry point
inside the corresponding sending/receiving model.
The ETS Interaction class table was modiﬁed to
represent the transferEntityFromFedSourceEx and transferEntityFromFedSourceExToFedDestEn subclasses. The
Parameter Table was modiﬁed to include the parameters
of the novel interaction class transferEntityFromFedSourceExToFedDestEn. The introduced parameters are
presented below. The similarities with the parameters
included in the ETS Parameter Table are highlighted
where present.
 Entity: It is a parameter of complex datatype containing
the EntityName, that is used to communicate the type of
the entity, and the EntityNumber, that is used to
communicate the number of entities to be transferred.
The EntityName and EntityNumber play the role of the
EntityName and EntitySize deﬁned in ETS (Taylor et al,
2006a b), respectively.
 ReceivedEntity: It refers to the entity received by the
receiving federate and has the same type of the parameter
Entity.
 Buffer_Availability: It was designed to enable the communication about the buffer availability.
 SourcePriority: This parameter was designed to communicate the priority assigned to the entity source, so that the
infrastructure can be further extended to manage Type
A.3 IRM (Section 3).
 EntityTransferTime: It deﬁnes the simulation time when
the entity is transferred to the destination point, that is
the arrival time. In this work, the entity leaves the source
node and reaches the destination node at the same time,
since it is assumed that the transferred entity instantaneously arrives at destination (Section 4.1).

5.2 Simulation messages
Simulation Messages are designed to support the communication between a CSP and its middleware (Section 4.1),
that is, not on the HLA side. The function of the
communication protocol depends on the role played by the

244 Journal of Simulation Vol. 6, No. 4

federate. The sending federate uses the protocol for
communications concerning the need of sending an entity
to another model (outgoing communication) and/or information on the availability of the target receiving federate
(incoming communication). The receiving federate uses the
protocol for communications concerning the buffer state
and/or the acceptance of an entity (outgoing communication) and/or the receipt of an entity from other models
(incoming communication). Simulation Messages are
implemented as a class that is characterized by the following
attributes:
 Time referring to the simulation time when the message is
sent to the middleware from the CSP. This attribute is
used by the middleware to determine the TimeStamp of
the interaction that will be sent to the RTI.
 BoundedBuffer containing the information about the
availability of the bounded buffer in the receiving model.
 TransferEntityEvent representing the entity transfer event
scheduled in the sending model event list and contains the
information about the entity to be transferred and the
scheduled time for the event.
 ExternalArrivalEvent representing the external arrival
event that is scheduled in the receiving model. It contains
the information about the entity to be received and the
scheduled time for the event.
 ReceivedEntity representing the information about the
entity that was eventually accepted by the receiving
model.

5.3 Communication protocol
This section presents the communication protocol between
federates, whereas in Section 5.4 will deﬁne the hypotheses
needed to minimize the zero lookahead when applying the
proposed protocol.
Herein, the behaviour of the sending federate will be
analysed at ﬁrst, then the receiving federate will be taken
under consideration. Finally, an example will be described to
clarify how the protocol works.

Sending federate. The CSP of the sending federate sends
a message to its middleware whenever a TransferEntityEvent (Section 5.2) is scheduled, that is, the departure event
of an entity from the last workstation of the sending model
is added to the simulation event list. Then, the middleware
uses the attributes time and TransferEntityEvent to inform
the RTI about the need of passing an entity, while the
simulation keeps on running (the TransferEntityEvent
time corresponds to the EntityTransferTime presented in
Section 5.1).
The request to advance to EntityTransferTime is sent by
the middleware to the RTI as soon as all local events
scheduled for that time instant have been simulated.

After the time has advanced, the middleware can
inform the CSP of the sending model about the state of
the receiving buffer in the receiving model. If the receiving
buffer is not full, then the workstation can simulate the
TransferEntityEvent, otherwise it becomes blocked. From
the blocking instant until when the middleware informs
the sending model that the receiving buffer is not full, the
model keeps on sending requests for time advance at the
lookahead value.

Receiving federate. The CSP of the receiving federate
sends a message to its middleware whenever a change in the
buffer availability occurs. This message contains the
updated value of the attribute boundedBuffer representing
the availability of the buffer, that is, the number of
available slots. Then, the middleware communicates this
information to the RTI via interactions. In particular, the
information on the availability of the buffer represents a
ﬁeld of the timestamped interaction transferEntityFromFedSourceExToFedDestEn.
If the change in the buffer availability is due to the
arrival of an entity from another model, then the update
of the information does not imply zero lookahead and the
communication is characterized by deﬁning the entity
that has been accepted (ie, the ReceivedEntity attribute).
If the buffer state change is not related to an external
arrival, then the update of the buffer information may
imply a zero lookahead (Taylor et al, 2006b), whenever it
is not possible to determine an advisable a-priori lookahead for the federation (Section 5.4). After being
informed by the middleware that another federate needs
to transfer an entity, the receiving model actually
simulates the arrival of the entity only if the buffer is
not full, otherwise the arrival is not simulated and the
workstation in the sending model becomes blocked.
Example. The application of the Simulation Messages
can be better appreciated by presenting an example (see
Figure 9) that is characterized as follows: (1) the reference
production system is represented in Figure 3, (2) the buffer
Q2a at time t accommodates a number of parts, that is,
greater than zero and less than the buffer capacity and an
entity enters workstation W1a, (3) a departure event from
workstation W1a is scheduled for time t 0 ¼ t þ p, where p
represents the processing time of the leaving entity at
station W1a, (4) during the time interval (t, t 0 ), no event
happening in the federate M2 (local event) inﬂuences the
state of the buffer Q2a.
Since W1a is the last machine in model M1, the departure
event is also a TransferEntityEvent. Therefore, the CSP
sends a message to its middleware containing time (t) and the
TransferEntityEvent attributes. After receiving the message,
the middleware of the sending model informs the RTI via
interaction.

G Pedrielli et al—An HLA-based distributed simulation for networked manufacturing systems analysis 245

Figure 9

Communication protocol.

Once the RTI time advances to time t, the middleware of the receiving model receives the information
about the need of the sending model to transfer an entity
at time t 0 . Then, the middleware sends to the receiving
model a simulation message containing the ExternalArrivalEvent. The receiving model simulates the external
arrival as soon as the simulation time advances to t 0 and
all local events for that time have been simulated (since
the buffer Q2a is not full according to the example
settings). A message is sent to the middleware of the
receiving model containing the updated level of Q2a
(attribute BoundedBuffer) together with the information
concerning the recently accommodated part (attribute
ReceivedEntity).
Afterwards, the middleware sends two interactions to the
RTI: one is with a TimeStamp equal to t 0 and contains the
updated state of the buffer Q2a and the receipt of the entity,
the other contains the request of time advance to time t 0 .
Once the RTI reaches time t 0 , the middleware of the sending
model receives the information regarding the state of Q2a
and the received entity by means of the RTI. Since the entity
has been delivered to the receiving model, the station W1a is
not blocked by the middleware.

5.4 Formal characterization of the communication
protocol
This section deﬁnes which hypotheses are needed to
minimize the occurrence of zero lookahead if the communication protocol afore presented is adopted.
Let Eij(t, t 0 ) represent an external event scheduled in the
i-th federate j-th exit (entry) point at simulation time t,

where t can be, in general, smaller or equal to t 0 that
represents the simulation time when the event is
supposed to be simulated. An event scheduled into the
event list of a simulator is deﬁned as external if one of the
three following conditions holds:
 The realization of the event depends on the state of a
federate, that is, in general, different from the one that
scheduled the event. One example of external event has
been addressed in Section 5.3: when the sending federate
(model M1) wants to transfer a part to the receiving
federate (model M2), the possibility for the leaving event
to be simulated depends on the state of the queue of the
receiving federate.
 The simulation of the event leads to changes into the state
of other federates in the federation. This is the case when
the downstream machine to the ﬁrst buffer in the receiving
model takes a part from the buffer thus changing its
availability, this information must be delivered to sending
models that are willing to transfer an entity, the state of
the sending federate(s) will change depending on the
information delivered (W1a can be idle or blocked).
 The event is not scheduled by the simulator that will
simulate it, but is put into the simulation event list by the
middleware associated with the simulator. This is the case
of the External Arrival Event.
In this paper three types of external events are taken into
consideration:
 Entity transfer event: this event happens when a sending
federate wants to transfer a part to a receiving federate.

246 Journal of Simulation Vol. 6, No. 4

 Buffer availability change event: this is a departure event
from the workstation downstream the buffer representing
the entry point of the receiving model.
 External Arrival event: this event is scheduled by the
middleware inside the simulation event list of the receiving
federate every time a part has to be transferred.
If tot 0 it means that the simulation message can be sent
by the sending (receiving) model and received by the target
federate before the event contained in the message has to be
executed. When this happens it is possible to minimize the
use of the zero lookahead for the communication between
federates.
The federate sending the message can communicate with
tot 0 under the following conditions:
 The Entity transfer event is scheduled when the part enters
the machine in the sending model. In this case, the event is
put into the event list a number of time units before it
must be simulated, that is, at least equal to the processing
time of the workstation under analysis.
In the case the event is scheduled when the part leaves the
workstation, then the condition holds if there exists a
transfer time between the sending and the receiving model,
that is, larger than zero and no events affect the arrival of
the part once the transfer has started.
The conditions aforementioned are not unrealistic when a
manufacturing plant is simulated: both in the case the
event is scheduled before or after the processing activity,
the time between the departure from the exit point and the
arrival to the entry point is in general not negligible.
Nonetheless, in both the aforementioned cases, it is
required that no other external events are scheduled by
the same exit point during the interval (t, t 0 ). This can
happen when, after a leaving event has been scheduled, a
failure affects the machine. In this case, the information
related to the part to be transferred has already been
delivered and cannot be updated. As a consequence an
external arrival event will be scheduled in the receiving
model although the sending model will not be able to
deliver the part because of the machine failure. A solution
to this issue is part of the future developments of this
work (Section 9).
 It is possible to communicate in advance the Buffer
availability change event if the workstation processing the
part schedules the leave event in advance to its realization
and no other events are scheduled by the same workstation during the interval (t, t 0 ). However, the zero
lookahead cannot be avoided by the sending federate,
which cannot be aware of the downstream buffer changes
and then it will send update request at the lookahead
value.
 The zero lookahead can be avoided if the middleware of
the receiving model can schedule the External Arrival
event in advance and then inform the target federates on

the availability of the buffer in advance. This condition
can be satisﬁed based on the entity transfer event
characteristics.
In the case one or more of the conditions aforementioned
do not hold than the communication protocol shown in the
Section 5.3 implies the use of zero lookahead.
If the hypothesis that no additional external events must
be scheduled by the same exit (entry) point in a federate
(sending or receiving) within the time interval (t, t 0 ) is
relaxed, then the middleware should be able to arrange
incoming events in a queue and wait before delivering the
information to the simulator until when the most updated
information has been received. However, it is quite
straightforward to show that, in the worst case, the
middleware should wait until when the simulation time
reaches t 0 , and therefore all the time advance requests would
be performed at the zero lookahead. This relaxation is under
analysis by the authors (Section 9).

6. HLA-based infrastructure implementation
The HLA-based architecture shown in Figure 5 was
implemented as follows:
 MAK-RTI 3.3.2 (http://www.mak.com) was used as the
RTI component implementation.
 The middleware was developed in C þþ language
following the speciﬁcations deﬁned in A Solution
Proposal for Type A.2 IRM section and was named
SimulationMiddleware.
 The simulation models were developed using a CSP
emulator, thus following the approach suggested in Wang
et al (2006).
The FederateAmbassador and RTIAmbassador were provided by MAK-RTI as C þþ classes and were linked to the
SimulationMiddleware. Further extensions were needed to
implement the proposed modiﬁcation to ETS (see Section
‘Proposal to modify ETS) and the Simulation Messages (see
Section ‘Simulation messages’). The former required a
modiﬁcation to FederateAmbassador class, whereas the latter
led to the development of a new C þþ class. The
SimulationMiddleware was implemented to manage the
information contained in Simulation Messages.

7. Manufacturing production system: a test case
The experiments presented in this section were designed to
test the proposed entity passing solution (Section 5.1). The
test case refers to a factory that produces two part types
(Part_A and Part_B) and consists of two separated
manufacturing systems (Plant1 and Plant2). Plant1 executes
a set of manufacturing operations on both part types,

G Pedrielli et al—An HLA-based distributed simulation for networked manufacturing systems analysis 247

Table 1 Analysis of interactions
Type of interaction

Figure 10

Factory test case.

whereas Plant2 is divided into two production lines and each
line is dedicated to process a single part type. If the
performance of Plant1 and Plant2 is simulated via two
separated simulators, then the whole production system can
be simulated in a distributed way by representing Plant1 and
Plant2 as federates within a federation (see Figure 10). This
test case is characterized by Type A.2 IRM, because the
workstation W12 has to transfer parts of type A (type B) to
the bounded buffer Q21 (Q23) in the Part_A Line (Part_B
Line) of Plant2. For validation purposes the results from the
DS were compared with a monolithic reference implementation. Two simulators representing Plant1 and Plant2 were
developed using the CSP emulator mentioned in Section 6.
These simulators were integrated by means of the HLAbased infrastructure (DS approach). Furthermore, a monolithic simulator was built using the CSP emulator to
represent the overall factory (SA approach). Finally, a
monolithic simulator was built using Rockwell Arenas 12
to validate the CSP emulator (results of this validation are
out of scope, thus they are not reported). It is assumed that
all simulation models do not contain any stochastic element.
The experiments were designed as follows:
 Parts of type A arrive at Plant1 every two time units,
whereas parts of type B every time unit.
 The capacity of Q11 was set to a value large enough to
guarantee that it never becomes completely full.
 Three conditions of maximum capacity (ie, 1, 5 and 50)
were considered for the other buffers.
 Three conditions of processing times were considered for
the workstations. Equal to 1 for all the workstations in the
ﬁrst condition, then to 7 for workstation W11 and to 1 for
all the other workstations in the second condition, and
ﬁnally to 7 for workstations W21 and W23 and to 1 for all
the other workstations in the third condition.
Nine experimental conditions were obtained by combining
the buffer capacity and processing time conditions. For each
experimental condition the performance of the manufacturing system was simulated according to both the SA and the
DS approach. A simulation length of 10 000 time units was
set for all the experiments. A conservative synchronization
approach was adopted: given the deterministic nature of the
simulation, it was possible to evaluate the smallest interval
elapsing between two consecutive events and the lookahead
was set to that value (ie, 1 time unit) for all the experiments.

Time advance
Entity passing (part type A)
Entity passing (part type B)
Entity passing (A þ B)

Total number of interactions
Sending
federate

Receiving
federate

9999
715
1429
2144

10 000
1429
2857
4286

The experiments were run on a single machine Intel
Core2 Duo Processor T7250, 2.00 GHz and 2046 MBRAM. Both simulation approaches (DS and SA) returned
identical simulation output values (not reported for
reason of space) for each experiment, thus validating
the interoperability offered by the proposed solution for
Type A.2 IRM. However, the DS approach required a
signiﬁcantly higher (four times on average) computational
time compared with the SA approach because of the
overhead related to the services offered by RTI (Gan et al,
2000). The aforementioned result led to examine the
number of interactions sent during the simulation experiments, thus delving into the overhead related to the RTI
services.
Table 1 reports the number of interactions sent during the
DS of the experiments characterized by the ﬁrst condition of
buffer capacity and the third condition of processing times.
This experimental condition is the most critical in terms of
the number of interactions needed to synchronize the two
federates because the two slowest workstations are placed
in Plant 2 and therefore buffers Q21 and Q23 are frequently
full while workstation W12 is frequently blocked. As a
consequence, many interactions are needed to communicate
when the parts can be actually transferred according to the
state of buffers Q21 and Q23.
The receiving federate sends more entity passing interactions than the sending federate because the receiving
federate sends an interaction every time an entity is
received and/or the availability of the receiving buffer
changes, whereas the sending federate sends an interaction
only when a departure event is scheduled. Each part
arriving at the buffer, except the ﬁrst that is directly
assigned to the machine, causes two interactions: one to
communicate the receipt of the entity and the updated
buffer state, and another one to communicate the updated
buffer availability when the entity leaves the buffer to enter
the next workstation. A large number of time interactions
is necessary because a request for time advance is generated
at every time unit since the workstation W12 is almost
always blocked and the time advances at the lookahead
(conservative synchronization approach). This behaviour
highlights how the solution proposed in Section 5 needs to
be further improved.

248 Journal of Simulation Vol. 6, No. 4

8. Industrial case
This section aims at verifying if the proposed integration
infrastructure presented in Section 5 can help to better
evaluate the performance of complex manufacturing
systems as described in Section 3. The goal does not
consist in comparing the HLA-based DS with a monolithic simulation in terms of accuracy and computational
efﬁciency as already done in the previous section. Herein,
the attention is focused in the industrial ﬁeld represented
by sheet metal production, thanks to the collaboration
offered by the company Tenova Pomini. In this industrial
ﬁeld, the production systems are characterized by the
presence of at least two subsystems interacting with each
other (Figure 11). The Roll Milling System produces sheet
metal using milling rolls that are subject to wearing-out
process; once the rolls wear out (ie, at the end of the roll
life) there is the need to change them to avoid defects
in the laminates. The Roll Shop performs the grinding
process to recondition the worn out rolls. Tenova Pomini
is a designer and provider of this kind of systems.
If the attention is focused on the rolls, then the
resulting production system can be considered as a closed
loop (Figure 11). The Roll Milling System sends batches
of worn out rolls to the Roll Shop following a given
policy and receives reconditioned rolls back. Both Roll
Milling System and Roll Shop have ﬁnite capacity
buffers. This implies that it is necessary to check whether
the buffer in the system receiving the rolls has available
slots. The deadlock in the closed loop is avoided because
the number of rolls circulating in the system is less that
the number of available slots (taking into account also
the machines) and is constant.
The two subsystems forming a closed loop are strongly
related and their reciprocal inﬂuences should be considered to properly evaluate the performance of the
whole factory. However, the lack of shared information
between the owner of the Milling System and the Roll
Shop designer makes the realization of a monolithic
simulation model hard to obtain or even infeasible. In
particular, the Roll Milling System works according to
speciﬁc roll changing policies that are not shared with the
Roll Shop designer even if they play a key role in the
dynamics of the whole factory. Indeed, when a roll is
worn out, the remaining life of the other rolls is checked

Figure 11

Industrial case representation.

and if the remaining life of a roll is under a predeﬁned
threshold, then it is sent to the grinding system together
with the completely worn out rolls. The presence of a
policy determines a relation between different roll types,
since a roll can be sent to the grinding system depending
on the behaviour of other roll types.
When Tenova Pomini designs a Roll Shop, the owner of
the Roll Milling System provides aggregated information
about the yearly average demand of worn out rolls to be
reconditioned. Then the Roll Shop designer develops a
simulator for the roll grinding process with high level of
detail.
The hypothesis is made that the Roll Shop designer has
developed and validated a simulator using the CSP
emulator (see Section 6). Similarly, the Roll Milling
System owner has developed and validated a simulator
using the CSP emulator, modelling the milling process
and the roll changing policy with high level of detail;
however, the model of the Roll Milling System simulator
is not shared with the Roll Shop designer. The Service
Level (SL) is the typical key performance indicator for
evaluating the milling system and is deﬁned as the ratio
of the time during which the milling system is producing
laminates over the total time when the milling system
is available (ie, there is no failure). The SL would be
reduced if the Roll Milling System had to wait for
reconditioned rolls coming from the Roll Shop.
The Roll Shop designer has to evaluate the system
performance while taking into account the inﬂuence of the
Roll Milling System related to (1) the arrival rate of worn
out rolls from the Roll Milling System that is estimated from
the yearly aggregate demand of reconditioned rolls and (2)
the acceptance of the reconditioned rolls sent by the Roll
Shop (closed loop model).
The inﬂuence of the Roll Milling System can be
represented by a mathematical model inside the detailed
simulation model of the Roll Shop. This mathematical
model roughly reproduces the Roll Milling System by
generating the arrival of worn out rolls and accepting the
reconditioned ones. The realization of a simulation model as
described before will be referred to as Approach A. The main
drawbacks of Approach A consist in:
 The real behaviour of the Roll Milling System cannot be
precisely modelled since it is reduced to a black box
sending and receiving rolls (eg, the roll changing policies
are not modelled).
 The performance (eg, mean starvation time for every
station, mean level of roll buffers, etc) of the Roll Milling
System cannot be evaluated.
These drawbacks lead to a potentially inaccurate evaluation of the factory performance. The Roll Shop designer
could increase the level of detail of the mathematical model
to improve the completeness of the simulation model and the

G Pedrielli et al—An HLA-based distributed simulation for networked manufacturing systems analysis 249

accuracy of the estimated SL. However, the lack of shared
information hinders the feasibility of this enhancement. It
must be stressed that simulator of Approach A cannot be
considered as a proper monolithic simulator of the whole
factory, since the Roll Milling System is only poorly
modelled.
An alternative approach (Approach B) to evaluate the
performance of the whole factory can be developed
by adopting the proposed HLA-based Infrastructure to
integrate the simulators of the Roll Milling System and of
the Roll Shop. In this case, the mathematical model is
removed from the simulation model of the Roll Shop, since
the behaviour of the Roll Milling System is already modelled
by its simulator. Approach B enables to evaluate the impact
of the number of rolls on the system performance, so that
the Roll Milling System owner can optimize the investment
cost associated with the expensive rolls, whereas the Roll
Shop designer can design a more effective and efﬁcient
Roll Shop thus better meeting the needs of the customer.
The two approaches have been compared by designing
a set of experiments that are characterized as follows:
 Three experimental conditions are designed with reference
to the total number of rolls circulating in the whole
system. These three conditions are deﬁned as Low,
Medium and High level.
 The simulation run length was set to 6 months.
 The roll changing policy adopted for the Approach B
simulator has been kept ﬁxed throughout the experimentation.
The results of the experiments are shown in Table 2.
Approach A and Approach B are compared in terms of the
estimated SL. The results show that the difference between
the two approaches is larger for the High and Medium level
conditions. When the level of rolls is Low the roll changing
policy does not affect the overall performance of the
production system because the Roll Milling System is
frequently starved and therefore the estimations are similar.
In case of Medium and High level conditions the workload of
the rolls in the roll shop can be strongly inﬂuenced by the
roll changing policy, thus generating a higher difference in
the estimation between the two approaches.
On the basis of the analysis carried out so far, it was
decided to design further experiments to analyse the
behaviour of the system with different starting workload

Table 2 Service level results
Experimental
conditions

Approach A

Approach B

Percentage
difference

High level
Medium level
Low level

0.995
0.946
0.308

0.872
0.682
0.273

12.3
27.7
3.5

conditions, that is the number of rolls that are present in the
Roll Milling System when the simulation starts. These
experiments can be useful to analyse the ramp-up period and
select the roll changing policy that avoids the arising of
critical workload conditions. These additional experiments
can be carried out only adopting Approach B, since the
starting workload conditions cannot be modelled with
Approach A. Indeed, the mathematical model generates rolls
for the Roll Shop independently from the starting workload
conditions. Therefore, the mathematical model would
generate roll arrivals even if all the rolls are already located
in the Roll Shop, thus incorrectly increasing the number of
rolls in the whole system. This represents an additional
criticality of the Approach A that can be solved only using
Approach B.
The second set of experiments was designed as follows: (1)
Two types of roll circulate in the factory (RollType1 and
RollType2). The roll of type RollType2 has a longer roll life
than RollType1; (2) For each type of roll three levels of the
Starting Workload (ie, number of rolls) in the Roll Milling
System are considered; (3) Three simulation run lengths are
considered, that is, 1, 2 and 4 weeks; (4) The roll changing
policy is ﬁxed for all experiments; (5) the total number of
rolls is equal to the High level of the previous experimentation and is ﬁxed for all the experiments.
Figure 12 shows the main effects plot for the SL evaluated
by simulating the 27 resulting experimental conditions with
Approach B. The plot suggests a signiﬁcant inﬂuence of the
factor Starting Workload for RollType1. This roll type
assumes a key role because of its short roll life. If the
Starting Workload For RollType1 is Low, the Roll Shop can
hardly follow the frequent roll requests of RollType1 from
the Roll Milling System during the transient period and low
values of SL are observed. This transient phenomenon
occurs in all conditions of the simulation lengths, however, it
mitigates when the simulation length increases. Indeed the
SL tends to a stationary value, that is, independent from
the starting conditions. Nonetheless, this analysis can be
useful for the Roll Milling System owner that can
individuate critical conditions, thus designing roll changing
policies that avoid the occurrence of these situations during
the ramp up period.

9. Conclusions and future works
The simulation of complex manufacturing systems led to the
investigation of the use of DS technique in industrial
environment. The ﬁrst results of the literature review
performed by the authors were presented. The sample of
papers will be enlarged and further review will be performed
to obtain a clear picture of DS in manufacturing. The need
of simulating complex manufacturing systems led to
investigate the integration of CSPs based on HLA and to
propose a solution to the CSP interoperability problem by

250 Journal of Simulation Vol. 6, No. 4

Figure 12

Main effect on service level with Approach B.

addressing the Type A.2 IRM and modifying the ETS
protocol. In particular, Simulation Messages were designed
to manage the communication between a CSP and its
adaptor in case of Type A.2 IRM. Nonetheless, the
implementation can be extended to the case of simultaneous
events (Type A.3 IRM). The main hypotheses that have to
be satisﬁed to minimize the zero lookahead have been
described. Future work will be done to relax the hypothesis
of the absence of additional events when the ﬁrst simulation
message has been sent. The experiments showed the
feasibility of the use of the integrated simulators infrastructure. The last section showed the industrial beneﬁts that
can be reached by applying the HLA-based DS.
Further developments are needed to optimize the time
advance management. The effect of different lookahead
values should be investigated together with the use of
optimistic synchronization approaches. The analysis of the
number of interactions needed for the entity transfer
(see Section 7) highlights that further research on the
implementation of Type A.2 IRM is necessary as well. For
instance, it would be interesting to investigate the design of
protocols that do not force to send interaction at every time
unit to communicate the state of the federates, but enable the
interaction depending on the system state (Adaptive Communication Protocols).
Acknowledgements—The research reported in this paper has received
funding from the European Union Seventh Framework Programme
(FP7/2007-2013) under grant agreement No: NMP2 2010-228595,
Virtual Factory Framework (VFF). The authors would like to
thank Prof. S.J.E. Taylor for his on-going scientiﬁc support and Prof.
S. Straburger for the precious suggestions during the PADS workshop. Authors acknowledge Tenova Pomini for the deﬁnition of the
industrial case.

References
Alvarado JR, Osuna RV and Tuokko R (2008). Distributed
simulation in manufacturing using high level architecture.
International Federation for Information Processing 260(4):
121–126.
Al-Zoubi K and Wainer G (2009). Performing distributed simulation with restful web-services approach. In: Dunkin A,
Ingalls R, Yücesan E and Rossetti M et al (eds). Proceedings
of the 2009 Winter Simulation Conference, 13–16 December,
Austin, TX, pp 1323–1334.
Banks J et al (2003). The future of simulation industry. In:
Ferrin D, Morrice D, Sanchez P and Chick S (eds). Proceedings
of the 2003 Winter Simulation Conference, 7–10 December,
New Orleans, LA, pp 2033–2043.
Boer CA (2005). Distributed simulation in industry. PhD Thesis.
Erasmus Research Institute of Management (ERIM), Erasmus
University Rotterdam, the Netherlands.
Boer CA, De Bruin A and Verbraeck A (2006a). Distributed
simulation in industry: A survey—Part 1 The COTS vendors.
In: Nicol D, Fujimoto R, Lawson B and Liu J et al (eds).
Proceedings of the 2006 Winter Simulation Conference, 3–6
December, Monterey, CA, pp 1094–1102.
Boer CA, De Bruin A and Verbraeck A (2006b). Distributed
simulation in industry: A survey—Part 2 experts on
distributed simulation. In: Nicol D, Fujimoto R, Lawson B
and Liu J et al (eds). Proceedings of the 2006 Winter
Simulation Conference, 3–6 December, Monterey, CA,
pp 1061–1068.
Boer CA, De Bruin A and Verbraeck A (2006c). Distributed
simulation in industry: A survey—Part 3 the HLA standard in
industry. In: Nicol D, Fujimoto R, Lawson B and Liu J et al
(eds). Proceedings of the 2006 Winter Simulation Conference, 3–6
December, Monterey, CA, pp 1061–1068.
Boer CA and Verbraeck A (2003). Distributed simulation and
manufacturing: Distributed simulation with COTS simulation
packages. In: Ferrin D, Morrice D, Sanchez P and Chick S (eds).
Proceedings of the 2003 Winter Simulation Conference, 7–10
December, New Orleans, LA, pp 829–837.

G Pedrielli et al—An HLA-based distributed simulation for networked manufacturing systems analysis 251

Colledani M and Tolio T (2005). A decomposition method to
support the conﬁguration/reconﬁguration of production systems. CIRP Annals—Manufacturing Technology 54(1): 441–444.
Fujimoto RM (1998). Parallel and distributed simulation. In: Wiley
(ed). Handbook of Simulation. University of Michigan, Wiley
Online Library.
Fujimoto RM et al (2007). Ad Hoc distributed simulations.
Proceedings of the 21st International Workshop on Principles of
Advanced and Distributed Simulation, IEEE Computer Society,
pp 15–24.
Gan BP et al (2000). Distributed supply chain simulation across
enterprise boundaries. In: Fishwick P, Kang K, Joines J and
Barton R (eds). Proceedings of the 2000 Winter Simulation
Conference, 10–13 December, Orlando, FL, pp 1245–1251.
Gan BP et al (2005). Interoperating Autosched AP using the
high level architecture. In: Armstrong F, Joines J, Steiger N and
Kuhl N (eds). Proceedings of the 2005 Winter Simulation
Conference, 4–7 December, Orlando, FL.
Gan BP et al (2007). Architecture and performance of an HLAbased distributed decision support system for a semiconductor
supply chain. SimTech Technical Reports 7(4): 220–226.
Granoweiter L and Watrous B (2004). Challenges and Solutions for
Large Scale HLA Federations. Simulations Interoperamlity
workshop, Cambridge, MA, pp 3–38.
Hibino H et al (2002). Manufacturing adapter of distributed
simulation systems using HLA. In: Snowdon J, Charnes J,
Yücesan E and Chen CH (eds). Proceedings of the 2002 Winter
Simulation Conference, 8–11 December, San Diego, CA,
pp 1099–1107.
Kubat C and Uygun O (2007). HLA based distributed simulation model for integrated maintenance and production scheduling system in textile industry. In: Pham PT, Eldukhri EE
and Soroka A (eds). Proceedings of 3rd I*PROMS Virtual
International Conference, 2–13 July, Cardiff, England, pp
413–418.
Kubat C, Uygun O and Ozmetel E (2009). Scenario based
distributed manufacturing simulation using HLA technologies.
Information Sciences 79(10): 1533–1541.
Kuhl F, Dahmann J and Weatherly R (1999). Creating Computer
Simulation Systems: An introduction to the High Level Architecture. Prentice Hall PTR: Upper Saddle River, NJ.
Law AM (2007). Simulation modeling and Analysis 3rd edn,
McGraw-Hill: New York.
Lee YT, Leong S and Riddick F (2006). A core manufactur
ing simulation data information model for manufacturing
applications, http://citeseerx.ist.psu.edu/viewdoc/summary?
doi=10.1.1.161.6233. Manufacturing Systems Integration
Division National Institute of Standards and Technology
Gaithersburg, MD 20899-8260 USA. 301-975-5426, 301-9753550, 301-975-3892.
Lendermann P (2006). About the need for distributed simulation
technology for the resolution of real-world manufacturing and
logistics problems. In: Nicol D, Fujimoto R, Lawson B and
Liu J et al (eds). Proceedings of the 2006 Winter Simulation
Conference, 3–6 December, Monterey, CA, pp 1119–1128.
Lendermann P (2007). Panel: Distributed simulation in industry—
A real-world necessity or ivory tower fancy? In: Morse K,
Perumalla K and Riley G (eds). Proceedings of the 2007
Winter Simulation Conference, 15–17 June, San Diego, CA,
pp 1053–1062.
Liang Y, Cai W and Turner SJ (2009). Interfacing RePast with HLA
using a generic architecture for COTS simulation package
interoperability. Joint SISO/SCS Spring Simulation Interoperability Workshop, San Diego, CA.

Linn RL and Chen CS (2002). Development of distributed
simulation model for the transporter entity in a supply chain
process. In: Snowdon J, Charnes J, Yücesan E and Chen C-H
(eds). Proceedings of the 2002 Winter Simulation Conference,
8–11 December, San Diego, CA, pp 1319–1326.
Malik AW (2010). An optimistic parallel simulation for cloud
computing environments. SCS M&S Magazine 6(4): 1–9.
MAK RTI. http://www.mak.com.
McLean C et al (2003). Simulation standards: Current status, needs,
and future directions. In: Ferrin D, Morrice D, Sanchez P
and Chick S (eds). Proceedings of the 2003 Winter Simulation
Conference, 7–10 December, New Orleans, LA, pp 2019–2026.
McLean C and Riddick F (2000). Integration of manufacturing
simulations using HLA. Proceedings of the 2000 Advanced Simulation Technologies Conference (ASTC 2000), 14–17 October,
Cleveland, OH.
Mustafee N and Taylor SJE (2006). Investigating distributed
simulation with COTS simulation packages: Experiences with
Simul8 and the HLA. Proceedings of the 2006 Operational
Research Society Simulation Workshop (SW06), Leamington
Spa, pp 33–42.
Mustafee N, Taylor SJE, Katsaliaki K and Brailsford S
(2009). Facilitating the analysis of a UK national blood service
supply chain using distributed simulation. Simulation 85(2):
113–128.
Park J et al (2005). Addressing complexity using distributed simulation: A case study in spaceport modeling. In: Armstrong F,
Joines J, Steiger N and Kuhl N (eds). Proceedings of the 2005
Winter Simulation Conference, 4–7 December, Orlando, FL.
Pedrielli G et al (2011). Simulation of complex manufacturing
systems via HLA-based Infrastructure. Proceedings of the 2011
Workshop on Principles of Advanced and Distributed Simulation,
Nice, 14–17 June, Nice, pp 78–89.
Peschlow P and Martini P (2007). Efﬁcient analysis of simultaneous
events in distributed simulation. Proceedings of the 11th IEEE
International Symposium on Distributed Simulation and RealTime Applications (DS-RT ‘07), 22–24 October, Chania,
Greece, pp 244–251.
Raab M, Masik S and Schulze T (2011). Support system
for distributed HLA simulations in industrial applications
Workshop on Parallel Advanced and Distributed Simulation
87–93.
Rabe M, Jaekel FW and Weinaug H (2006). Supply chain
demonstrator based on federated models and HLA application.
Simulation und Visualisierung 2006. European Publishing House:
Ghent, Erlangen, San Diego, Delft, pp 329–338.
Sacco M et al (2011). VFF: Virtual Factory Manager. In: Shumaker
Randall (ed) Virtual and Mixed Reality—Systems and Application, Lecture Notes in Computer Science Vol. 6774, Springer:
Berlin Heidelberg, pp 397–406.
SISO CSPI-PDG. http://www.sisostds.org.
SISO COTS Simulation Package Interoperability Product Development Group (2010). Standard for Commercial-off-the-shelf
Simulation Package Interoperability Reference Models. SISOSTD-006-2010.
Sly D and Shreekanth M (2001). Simulation Data Exchange (SDX)
implementation and use. In: Peters BA, Smith JS, Medeiros
DJ and Rohrer MW (eds). Proceedings of the 2001 Winter
Simulation Conference, pp 1473–1477.
Smith JS (2003). Survey on the use of simulation for manufacturing
system design and operation. Journal of Manufacturing Systems
22(2): 157–171.
Straburger S (2001). Distributed simulation based on the high
level architecture in civilian application domains. PhD Thesis,

252 Journal of Simulation Vol. 6, No. 4

Computer Science Faculty, University Otto von Guericke,
Magdeburg.
Straburger S (2006a). The road to COTS-interoperability: from
generic HLA-interfaces towards plug-And-play capabilities.
In: Nicol D, Fujimoto R, Lawson B and Liu J et al (eds).
Proceedings of the 2006 Winter Simulation Conference, 3–6
December, Monterey, CA.
Straburger S (2006b). Overview about the high level architecture
for modeling and simulation and recent developments. Simulation News Europe 16(2): 5–14.
Straburger S, Schmidgall G and Haasis S (2003). Distributed
manufacturing simulation as an enabling technology for the
digital factory. Journal of Advanced Manufacturing System 2(1):
111–126.
Taylor SJE (2011). Realising parallel and distributed simulation in
industry: A roadmap. Proceedings of the 25th International
Workshop on Principles of Advanced and Distributed Simulation.
IEEE Computer Society, 14–17 June, Nice, p 1.
Taylor SJE, Bohli L, Wang X and Turner SJ (2005). Investigating
distributed simulation at the ford motor company. Proceedings
of the Ninth IEEE International Symposium on Distributed
Simulation and Real-Time Applications. IEEE Computer
Society, pp 139–147.
Taylor SJE et al (2002). Distributed simulation in industry:
Potentials and pitfalls. In: Snowdon J, Charnes J, Yücesan E
and Chen C-H (eds). Proceedings of the 2002 Winter Simulation
Conference, 8–11 December, San Diego, CA, pp 688–694.
Taylor SJE et al (2003). HLA-Cspif panel on commercial off-theshelf distributed simulation. In: Ferrin D, Morrice D, Sanchez P
and Chick S (eds). Proceedings of the 2003 Winter Simulation
Conference, 7–10 December, New Orleans, LA, pp 881–887.
Taylor SJE, Turner S and Low M (2004). A proposal for an entity
transfer speciﬁcation standard for COTS simulation package
interoperation Proceedings of the 2004 European Simulation
Interoperability Workshop. 28 June–1 July, Edinburgh, Scotland.
Taylor S et al (2006a). Developing interoperability standards for
distributed simulation and COTS simulation packages with the
CSPI PDG. In: Perrone LF et al (eds). Proceedings of the 2006
Winter Simulation Conference. Institute of Electrical and
Electronics Engineers: Piscataway, NJ, pp 1001–1110.
Taylor SJE, Wang X, Turner SJ and Low MYH (2006b).
Integrating heterogeneous distributed COTS discrete-event
simulation packages: An emerging standards-based approach.
IEEE Transactions on Systems, Man & Cybernetics 36(1):
109–122.
Taylor SJE, Turner SJ and Straburger S (2008). Guidelines for
commercial off-the-shelf simulation package interoperability. In:
Mason SJ et al (eds). Proceedings of the 2008 Winter Simulation
Conference. Association for Computing Machinery Press: New
York, NY, pp 193–204.
Taylor SJE et al (2009). Commercial-off-the-shelf simulation
package interoperability: Issues and futures. In: Tew J,
Barton R, Henderson J and Biller B et al (eds). Proceedings of
the 2009 Winter Simulation Conference, 13–16 December,
Washington, DC, USA.

Terkaj W, Tolio T and Valente A (2009). Designing manufacturing
ﬂexibility in dynamic production contexts. Design of Flexible
Production Systems Methodologies and Tools. Springer: Berlin
Heidelberg, pp 1–18.
Terzi S and Cavalieri S (2004). Simulation in the supply chain
context: A survey. Computers in Industry 53(1): 3–16.
Uygun Ö (2006). HLA-based distributed simulation model for
integrated maintenance and production scheduling system in
textile industry. International Symposium on Intelligent Manufacturing Systems.
Vàncza J, Egri P and Monostory L (2008). A coordination
mechanism for rolling horizon planning in supply networks.
CIRP Annals—Manufacturing Technology 57(1): 455–458.
Wainer G et al (2010). Standardizing DEVS model representation.
In: Discrete Event Modeling and Simulation: Theory and
Applications. CRC Press.
Wang X, Turner SJ, Low MYH and Gan BP (2004). A Generic
Architecture for the Integration of COTS Packages with the
HLA. In: Robinson S and Taylor SJE (eds). Proceedings of the
2004 Operational Research Society Simulation Workshop.
Association for Computing Machinery’s—Special Interest
Group for Simulation: Birmingham, UK, pp 225–233.
Wang X, Turner SJ and Taylor SJE (2006). COTS simulation
package (CSP) interoperability—A solution to synchronous
entity passing. Proceedings of the 20th International Workshop on
Principles of Advanced and Distributed Simulation. IEEE
Computer Society, pp 201–210.
Wang G, Chun J and Peng G (2007). Adapting arena into HLA:
Approach and experiment [A]. Proceedings of the IEEE
International Conference on Automation and Logistics [C].
Vol. 8, IEEE: Jinan, China, pp 1709–1713.
Wiendahl HP and Lutz S (2002). Production in networks. CIRP
Annals—Manufacturing Technology 51(2): 573–586.
Yuan Z and Zhang LM (2011). The viewable distributed simulation
linkage development tool based on factory mechanism. Applied
Mechanics and Materials 58(60): 1813–1818.
Zacharewicz G et al (2008). G-DEVS/HLA environment for
distributed simulations of workﬂows. SIMULATION: Transactions of the Society for Modeling and Simulation International
84(5): 197–213.
1516-2000 IEEE Standard for M&S—HLA—Framework and
rules. IEEE Computer Society, 2000, pp 1–22.
1516-2000 IEEE Standard for M&S—HLA—Federate Interface
Speciﬁcation. IEEE Computer Society, 2000, pp 1–29.
1516-2000 IEEE Standard for M&S—HLA—Object Model
Template (OMT). IEEE Computer Society, 2000, pp 1–30.
1516-2003 IEEE Recommended Practice for HLA FEDEP. IEEE
Computer Society, 2003, pp 1–79.

Received 22 September 2011;
accepted 9 February 2012 after one revision

Proceedings of the 2015 Winter Simulation Conference
L. Yilmaz, W. K. V. Chan, I. Moon, T. M. K. Roeder, C. Macal, and M. D. Rossetti, eds.

MULTI-OBJECTIVE MULTI-FIDELITY OPTIMIZATION WITH ORDINAL
TRANSFORMATION AND OPTIMAL SAMPLING
Haobin Li
Institute of High Performance Computing
Department of Computing Science
1 Fusionopolis Way, 138632, SINGAPORE

Yueqi Li
Loo Hay Lee
Ek Peng Chew
Department of Industrial and Systems Engineering
National University of Singapore
1 Engineering Drive 2
117576, SINGAPORE

Giulia Pedrielli

Chun-Hung Chen

Centre for Maritime Studies
National University of Singapore
12 Prince George’s Park
118411, SINGAPORE

Dept. of Systems Engineering & Operations Research
George Mason University
4400 University Drive, MS 4A6
Fairfax, 22030 VA, USA

ABSTRACT
In simulation–optimization, the accurate evaluation of candidate solutions can be obtained by running a
high–fidelity model, which is fully featured but time–consuming. Less expensive and lower fidelity models
can be particularly useful in simulation–optimization settings. However, the procedure has to account
for the inaccuracy of the low fidelity model. Xu et al. (2015) proposed the MO2 TOS, a Multi–fidelity
Optimization (MO) algorithm, which introduces the concept of ordinal transformation (OT) and uses optimal
sampling (OS) to exploit models of multiple fidelities for efficient optimization. In this paper, we propose
MO–MO2 TOS for the multi–objective case using the concepts of non–dominated sorting and crowding
distance to perform OT and OS in this setting. Numerical experiments show the satisfactory performance
of the procedure while analyzing the behavior of MO–MO2 TOS under different consistency scenarios of
the low–fidelity model. This analysis provides insights on future studies in this area.
1

INTRODUCTION

Simulation models have been widely applied for the analysis and optimization of complex system. This has
been particularly true in the past decades, also due to the explosive revolution of computing technology,
which has greatly facilitated the realization of running simulation models. In this setting, simulation–
optimization has received a remarkable attention as an effective set of techniques coupling simulation with
search procedures to identify strategies or designs leading to the best performance. Specifically, a search
algorithm which selects candidate solution(s) and a simulator which evaluates the objective performance
of selected solutions. These two components are iteratively called until a specific stopping criteria is met
(Fu et al. 2005).
However, in case simulation models have a high level of detail (i.e., they are high–fidelity models),
despite the accuracy in the performance estimate, they result in high computational effort. However, the

978-1-4673-9743-8/15/$31.00 ©2015 IEEE

3737

Li, Li, Pedrielli, Lee, Chew, and Chen
available simulation budget is limited in nature and this results in the possibility of evaluating only a
small fraction of candidates. If less detailed simulation models are available (i.e., low–fidelity models),
these might be run over a more extended set of solutions and the results might be used in combination
and in support of the, few, expensive simulations. Low fidelity models are by construction less accurate
than the high fidelity counterpart. Intuitively, the effectiveness of an approach which uses low fidelity
models is remarkably influenced by their consistency with the high–fidelity counterpart (Xu et al. 2014,
Xu et al. 2015): if the low–fidelity model contains significant bias, misleading results may be obtained.
Therefore, Xu et al. (2014) proposed Multi-fidelity Optimization with Ordinal Transformation and Optimal
Sampling (MO2 TOS), a framework for single objective simulation–optimization problems which combines
low and high–fidelity simulation. Specifically, it makes use of ordinal ranking of low–fidelity model
observations based on which samples are selected for high–fidelity evaluation. Xu et al. (2015) shows
that this procedure can efficiently perform optimization by reducing variances within a group of candidate
solutions and enlarged distances between groups.
Nevertheless, currently MO2 TOS can manage multi–objective optimization only in case the multiple
objectives are scalarized into a single value, which is possible only if the relative preference for each
objective is available. However, such preferences are not easy to define. Several algorithms have been
proposed for finding the Pareto set, e.g., NSGA-II (Deb et al. 2000), MO-PSO (Lee et al. 2014) and
MO-COMPASS (Li et al. 2015), but none them considers the possibility of using evaluations from models
with different fidelities.
In this paper, we firstly define the problem, and introduce some preliminary knowledge on the Pareto
optimality and the generic framework for the MO2 TOS algorithm. Then we develop the multi-objective
MO2 TOS (MO-MO2 TOS) and analyze it through numerical experiments.
2

PROBLEM DEFINITION

We refer to the family of deterministic multi–objective optimization problems defined on any finite solution
space Θ, i.e., kΘk < ∞, such that for any solution x ∈ Θ, we have a high–fidelity simulation model that
can be used to evaluate the performance
with sufficientaccuracy, i.e., without bias and noise. We denote

the high–fidelity results as g (x) = g(1) (x) , . . . , g(H) (x) , where H indicates the number of objectives.
According to the definition of Pareto optimality, we are aiming to find the Pareto set Π on Θ, namely
(Li et al. 2015):
Π (Θ) ≡ {x ∈ Θ |6 ∃y ∈ Θ, g(y) ≺ g(x)} ,
(1)
where, ‘≺’ denotes the dominance, i.e., g(y) ≺ g(x) if an only if
∀l ∈ {1, . . . , H}, g(l) (y) ≤ g(l) (x) and ∃l ∈ {1, . . . , H}, g(l) (y) < g(l) (x).

(2)

The high–fidelity model is time consuming, i.e., obtaining g (x) is computationally costly. However, a
low–fidelity simulation model is available giving an estimate of the true performance in a remarkably
shorter time than the high–fidelity one. We denote the low–fidelity result as g̃ (x) , ∀x ∈ Θ, and we model
the low–fidelity bias as follows (Xu et al. 2015):
g̃ (x) = g (x) + δx ,

(3)

where δx represents the bias of the low fidelity model corresponding to the feasible solution x.
Therefore, in this study, we propose an efficient procedure to exploit simulation models at both fidelity
levels in order to identify an estimated Pareto set denoted by Π̂ when the computational budget in terms of
number of high–fidelity evaluation is limited and remarkably lower than the number of candidate solutions.
Formally, if S ⊆ Θ is the set of solutions that are sampled for high–fidelity evaluation according to the
procedure, the resulting estimated Pareto Set is:
Π̂ (Θ) = {x ∈ S |6 ∃y ∈ S, g(y) ≺ g(x)} .
3738

(4)

Li, Li, Pedrielli, Lee, Chew, and Chen
MO–MO2 TOS FRAMEWORK

3

In Xu et al. (2015), the MO2 TOS algorithm is proposed for solving simulation–optimization problem with
single–objective. Grounding on this proposal, we propose the extension in Algorithm 1 that consists of
the same two fundamental steps as MO2 TOS: Ordinal Transformation (OT) as in Step 4, and the Optimal
Sampling (OS) as in Step 8 & 9. In particular, we modify the two essential procedures to make them
suitable for solving multi-objective problems.
Algorithm 1: Generic Procedure for MO2 TOS Algorithm
1
2
3
4
5

forall the x ∈ Θ do
Apply low-fidelity model to evaluate x, and obtain g̃ (x);
end
	

(OT): Sort all x ∈ Θ considering g̃ (x), as Θ = x(1) , . . . , x(|Θ|) ;
Following the sequence, evenly divide Θ into m groups, i.e., let K = {1, . . . , m} and
o
n
Θk = x(⌊ k−1 ·|Θ|+1⌋) , . . . , x(⌊ k ·|Θ|⌋) , ∀k ∈ K;
m

6
7
8
9
10
11
12
13

m

Let S = 0;
/
while not stopped do
(OS-1): Sample k∗ ∈ K, considering g (x) , ∀x ∈ Θk ∩ S, ∀k ∈ K;
(OS-2): Sample x∗ ∈ Θk∗ , considering g̃ (x) , ∀x ∈ Θk∗ \ S;
Apply high-fidelity model to evaluate x∗ , and obtain g (x∗ );
S ← S ∪ {x∗ };
end
With S, return Π̂ (Θ) by (4).

For OT, the low–fidelity model is used to derive a relative relationship (ranking) among candidate
solutions. Since the time for low–fidelity model is assumed to be negligible and the solution set is finite,
the low-fidelity model is run for all the candidates x ∈ Θ, which are subsequently ordered according to
increasing values of g̃ (x). Although affected by bias, we use the low–fidelity to transform the solution
space to the 1-dimensional ordinal space according to Xu et al. (2015). In a single–objective case, once
g̃ (x) is computed through the low–fidelity simulator the ordinal transformation ranks x based on the g̃ (x)
itself. However, in the multi–objective case, such a transformation is complicated by the need to consider
the concept of dominance. The solution of this issue represents the most important aspect in extending the
MO2 TOS to its multi–objective version.
Once the dominance–based OT has been performed, we can exploit the efficiency of the optimal
sampling scheme in Xu et al. (2015) to select the candidates for high–fidelity evaluations. Specifically, OS
is performed by grouping solutions according to the ranking resulting from OT. Compared with a random
grouping, the variability within a group of solutions decreases and differences between groups increase.
With OS, two decisions are to be made at every iteration, i.e., which group to select, and, within the
selected group, which solution to sample for high–fidelity evaluation. Moreover, from iteration to iteration,
the evaluated high-fidelity results are used to update the expected quality of each group of solutions.
In the next few sections we detail how to implement OT and OS for the multiple–objective case.
Herein, we do not consider scalarizing methods since they lead to the possibility to use MO2 TOS and no
extension is required in this case. The same holds for the hyper-volume concept in the objective space that
is dominated by a multi-objective solution (Bradstreet et al. 2008), and the solutions with larger dominated
hyper-volume (DHV) have higher order in the ranking. This is relatively fair comparison between solutions
that are non-dominated by each other. But, the DHV does not explicitly reflect the distribution of solutions
on the Pareto front. Besides, boundary points need to be identified to make the DHV finite, and the selection
of them could also affect the ordering.

3739

Li, Li, Pedrielli, Lee, Chew, and Chen
3.1 Ordinal Transform
Several ways for ordering solutions with multiple objectives have been proposed in the literature (Swisher
et al. 2003, Zitzler et al. 2003).
A particularly common method is the non-dominated sorting (NS), proposed by Deb et al. (2000),
based on which NSGA-II is developed (Deb et al. 2000). According to this concept, solutions are first
ranked according to the Pareto layers, and one dominating solution has higher rank than the dominated;
then, within each Pareto layer where solutions are non-dominated by each other, the crowding distance is
used to rank the solutions. The crowding distance reflects the density around a solution in the corresponding
Pareto layer, and the solution with lower density has higher rank in the layer.
Algorithm 2: Procedure for calculating the crowding–distance in Pareto layer P
1
2
3
4
5
6
7
8
9
10
11
12

forall the x ∈ P do
d (x) ← 0;
end
forall the l ∈ {1, . . . , H} do
∆(l) ← maxx∈Θ ĝ(l) (x) − minx∈Θ ĝ(l) (x);
	

Sort all x ∈ P by ĝ(l) (x) in ascending order, as P = x(1) , . . . , x(|L|) ;


d x(1) ← ∞ and d x(|L|) ← ∞;
forall the i ∈ {2, . . . , |L| − 1} do

 ĝ(l) (x(i+1) )−ĝ(l) (x(i−1) )
d x(i) ← d x(i) +
∆(l)
end
end
Return d (x) , ∀x ∈ P.

To be more specific, in MO2 TOS, given any solution set Ω, we assume ĝ (x) is the observed objective
values for x, i.e., ĝ (x) ← g (x) for high–fidelity evaluation, and ĝ (x) ← g̃ (x) for low–fidelity evaluation;
then P is any Pareto layer that contains solutions that are non-dominated by each other, i.e.,
P ⊆ Ω, s.t. ∄x, y ∈ P, ĝ (x) ≺ ĝ (y)

(5)

For each P, the procedure to compute the crowding-distance d (x) , ∀x ∈ P works as in Algorithm 2.
Subsequently, the NS rule can be described as follows:
	

Ω = x(1) , . . . , x(|Ω|) ,




(6)
s.t. 1 ≤ i < j ≤ |Ω| =⇒ ĝ x( j) ⊀ ĝ x(i) and d x(i) ≤ d x( j) .
The NS rule can be applied for the Step 4 in Algorithm 1 by letting Ω ← Θ and ĝ (x) ← g̃ (x) , ∀x ∈ Θ, so
as to form an OT procedure for multi–objective problems.
3.2 Optimal Sampling
Given an ordered set of items, solutions or groups, we propose to sample according to a truncated geometric
scheme parametrized over p ∈ [0, 1]. Let Φ be the ordered set containing the elements a(i) , ∀i ∈ {1, . . . , |Φ|},
where i refers to the order. The truncated geometric sampling (TGS) is described in Algorithm 3. With
this procedure, we guarantee that the higher rank (smaller i) items have larger probabilities to be sampled.
The TGS is used to developed both sub-procedures for the OS. As in Step 8 of Algorithm 1 (OS-1), we
need to sample a group Θk∗ from {Θ1 , . . . , Θm }. Algorithm 4 shows how TGS is utilized with an algorithm
parameter pg ∈ [0, 1], in sampling k∗ ∈ K considering g (x) , ∀x ∈ Θk ∩ S, ∀k ∈ K. The procedure firstly
3740

Li, Li, Pedrielli, Lee, Chew, and Chen
Algorithm 3: Procedure of truncated geometric sampling (TGS)
1
2
3
4
5
6
7
8

while true do
forall the i ∈ {1, . . . , |Φ|} do
Sample u from a uniform distribution on [0, 1];
if u < p then
Return a(i) .
end
end
end

ensures that each group is sampled at least once (Step 1 – 4). Then the groups are sorted according to the
averaged ranking performed using the high–fidelity results and TGS is applied to select the k∗ (Step 6 –
9).
Algorithm 4: Procedure of sampling k∗ ∈ K (OS-1)
1
2
3
4
5
6
7
8
9
10

forall the k ∈ K do
if Θk ∩ S = 0/ then
Return k∗ = k.
end
end
Sort S according to (6), in which Ω ← S and ĝ (x) ← g (x) , ∀x ∈ S;
Let I (x) denotes the order of x in S, and Ik ← ∑x∈Θ
 k ∩S I (x) / |Θk	∩ S| , ∀k;
Sort K according to Ik in ascending order, as K = k(1) , . . . , k(m) ;
Sample k∗ from K by TGS as in Algorithm 3, by Φ ← S and p ← pg ;
Return k∗ .

With TGS defined in Algorithm 3, the sampling of x∗ ∈ Θk∗ as in Step 9 of Algorithm 1 (OS-2) can be
straight-forward, by applying TGS with Φ ← Θk∗ \ S and p ← ps , where ps is another algorithm parameter.
The sequence of all x ∈ Θk∗ remains as by OT.
Therefore, we have defined a procedure of OS for multi-objective problems.
4

NUMERICAL RESULTS

Numerical experiments on MO–MO2 TOS algorithm were conducted in order to: (1) understand the effect
of the different algorithm parameters over the performance, (2) compare the performance of the procedure
against random search (i.e., an algorithm which does not use the low–fidelity model information).
Specifically, we focus on an optimization problem defined on the solution space Θ = {1, . . . , N} where
N = 10000. Each point is associated with the high–fidelity value:
i
h
(7)
g (x) = g(1) (x) , g(2) (x)
where g(1) (x) and g(2) (x) are randomly generated from a standard normal distribution, i.e., N (0, 1), for
all x ∈ Θ. Despite the high–fidelity evaluation is trivial, it is good enough to provide a testbed scenario
for finding Pareto solutions. Figure 1 shows the distribution of the solutions in the objective space. A
spectrum of colors are used to indicate different Pareto Layers, where the red color denotes the Pareto set.
Then, each solution has an low–fidelity estimation as g̃ (·):
i
h
(8)
g̃ (x) = g̃(1) (x) , g̃(2) (x) .
3741

Li, Li, Pedrielli, Lee, Chew, and Chen

Figure 1: Objective values of high-fidelity model (colored for Pareto layers).
According to the definition in Section 2, we randomly assign the low–fidelity values from the high–fidelity
model according to the following:

(9)
g̃(1) (x) = α1 · g(1) (x) + ε in which ε ∼ N 0, σ12 ,

(10)
g̃(2) (x) = α2 · g(2) (x) + ε in which ε ∼ N 0, σ22 .
This artificial construction of the low-fidelity response give us the chance, as it will be illustrated later on,
to construct scenarios with different “quality” of the low-fidelity model.
Intuitively, a better low-fidelity model has both coefficients α1 , α2 positive, i.e., the low-fidelity model
is indeed consistent with respect to the high-fidelity model. But, in case α1 < 0 or α2 < 0, it means that at
least one of the two low-fidelity objective values does not consist with respect to the high-fidelity model and
we can expect the ordinal transformation, and, consequently, the optimal sampling, illustrated in Sections
3.1–3.2 to be negatively affected.
The variance component σ12 , σ22 , characterizing the bias also hinders the algorithm performance. As
previously stated, by controlling the magnitude and variability of the bias, we can create several scenarios
which refer to different quality levels.
By varying the parameters of the low–fidelity model, we obtained the experimental conditions in Figure
2. In particular, as shown in the picture, we can separate the entire experiment space into 3 main scenario:
(1) positive correlation, i.e., α1 = α2 = 1, (2) positive and negative correlation, i.e., α1 = 1, α2 = −1 or
α1 = −1, α2 = 1 , and (3) negative correlation, i.e., α1 = α2 = −1. In the figures, the σ1 and σ2 are set to
0.2, but in the experiment we test different possible noise levels.
In the numerical test, we have a limited budget of 1000 evaluations with the high fidelity model, while
we performed 100 macro–replications of the algorithm procedure in order to derive the 25, 50, and 75
percentile of the DHV by the observed Pareto set resulting from the macro–replicated experimentation.
4.1 Effect of the parameters in the algorithm performance
As reported in Table 1, there are 32 conditions tested based on the scenarios shown in Figure 2.
4.1.1 Scenario 1: with α1 = α2 = 1
Not surprisingly, the most relevant factor in this case is represented by the low fidelity model variance.
Intuitively, the DHV increases for a low fidelity model characterized by lower variance. Indeed, a large
variance results in the diffusion of solutions, leading to groups containing dominated and non–dominated
3742

Li, Li, Pedrielli, Lee, Chew, and Chen

Figure 2: Distortions of Pareto layers & rankings in low-fidelity models under different scenarios.
Table 1: Experimental conditions for each low fidelity level
Level
Low
High

σ1
0.2
0.8

σ2
0.2
0.8

m
10
30

pg
0.2
0.8

ps
0.1
0.3

solutions. Such a condition, hinders the effectiveness of the ordinal transformation which is indeed based
on the group. Some solutions may jump out of its front and drop into other fronts nearby, and this would
decrease algorithm efficiency. Under scenario 1, the ranking obtained by OT is close to the true ranking
due to the positive correlation characterizing the relationship between the low and the high fidelity function
in both objectives.
4.1.2 Scenario 2: with α1 = 1, α2 = −1 or α1 = −1, α2 = 1
In these conditions, the number of groups m (as in Step 5 of Algorithm 1) and pg (as in Step 9 of Algorithm
4) have the most important impact over the DHV. In particular, smaller values of the parameters lead to
better performance. Intuitively, the algorithm tends to broadly explore the whole solution space. As we
can observe in Figure 2, when one low fidelity model is positively correlated with its high–fidelity model
while the other one is negatively correlated with its high fidelity model, the OT ranking is not effective:
solutions from the true Pareto set tend to distribute among different groups, and a low probability of group
selection tends to favor a spread search among different groups hence increasing the chance to identify
Pareto solutions.
4.1.3 Scenario 3: with α1 = α2 = −1
It is relevant to notice that the scenario with both low-fidelity models negatively correlated marks an important
difference between the single–objective optimization case in Xu et al. (2015) and the multi–objective case
of interest in this paper. In fact, when a single objective is of concern, in case the low–fidelity model is
“perfectly” negatively correlated with the high-fidelity model (i.e., α = −1), at the first iteration of the

3743

Li, Li, Pedrielli, Lee, Chew, and Chen
algorithm we might simply consider to revert the ordering. Afterwards the procedure works exactly as
in the positively correlated case. However, if multiple objectives are consider, the reverting of the Pareto
front will not produce the aforementioned results. In fact, solutions in different layers at the boundary
of the frontier will be assigned to different layers. This can be observed in Figure 2, where the Pareto
Solutions, despite concentrated in the area with smaller high-fidelity and larger low fidelity ranking, tend
to distribute in the solution space with several possible values of low-fidelity performance. Given this,
reverting the low fidelity evaluations as it would be natural is the single objective case, will not produce
the same performance observed in Section 4.1.1.
Specifically, we observed that factor m and pg have a major impact on the algorithm performance. This
is reasonable: larger values of m as well as pg lead to focus on the “bad observations” hence increasing
the chance to recognize a fairly good Pareto set. In fact, as previously stated, a large portion of solutions
in the true Pareto set would fall into the last group, before reversion of the OT ranking is performed, while
solutions from the last front in true Pareto set would probably fall into the first group, and solutions with
non-dominated or near objective performance would still stay close with each other. As a result, after
the re–ranking according to the high-fidelity model performance, the obtained groups should be consistent
with the high-fidelity model. Therefore, we prefer larger values of pg to focus on the best solutions and a
larger number of groups in order to maximize the chance to identify also misclassified solutions.
4.2 Benchmark Comparison
To show the algorithm performance and demonstrate the efficiency of MO–MO2 TOS, we compare it with
uniform random search applied to the cases in Figure 2. Based on the analysis in the previous section,
we focus on the effect of the parameter pg , which varies in the set {0.1, 0.5, 0.9}, with regard to each
scenario, ps , instead, was set to the value 0.1 for all tests. Concerning m, two levels were considered as
in the previous section, namely, 10 and 30.
Also in this case, for each experiment setting, 100 macro-replications were performed. The results
are reported in Figure 3–7 each corresponding to one of the three scenarios, respectively. Here, the x-axis
represents the number of sampled solutions and the y-axis represents the average DHV resulting from the
100 independent macro-replications.

(a) m = 10

(b) m = 30

Figure 3: Benchmark comparison for Scenario 1.
From Figure 3, we observe that for Scenario 1, compared to random sampling, MO–MO2 TOS performs
much better even for small values of sampled solutions (x–axis), which obtains a Pareto set close to the
true one with limited samples. This is reasonable since the low fidelity is particularly good under this
scenario. After OT, the ranking of solutions, which follows a similar sequence to the true one, facilitates the
algorithm to quickly focus on optimal or near optimal solutions. According to the testing result, with 95%
acceptance level, the performance of MO–MO2 TOS is statistically better than that of random sampling
for all cases, i.e., independently from the specific algorithm parameters. Despite this, we observe that
when m is low (Figure 3(a)) we prefer larger values of pg , and vice versa (Figure 3(b)). Figure 4 shows
the reason behind the performance of the proposed algorithm. In particular, m = 10 is considered. In the

3744

Li, Li, Pedrielli, Lee, Chew, and Chen

(a) pg = 0.1

(b) pg = 0.9

Figure 4: Sampling history for Scenario 1.
Figure, the red color corresponds to the solutions sampled at early stages of the procedure, while darker
colors correspond to solutions sampled in later stages. We observe that, when pg = 0.1 (Figure 4(a)), the
algorithms spreads the sampling among several groups at least at the first iterations and only towards the
end it focuses on the lower left group containing the Pareto solutions. On the contrary, when pg = 0.9, the
algorithm will focus on the first group, ending up with a more in–depth exploration of the best groups. As

(a) m = 10

(b) m = 30

Figure 5: Benchmark comparison for Scenario 2.
already observed in the previous section, Scenario 2 is the most critical for the performance. In particular,
we observe a much lower values for DHV which at the end of 1000 iterations compared with Scenario 1. The
difference between MO–MO2 TOS and random search is relatively small. The reason for this performance
resides in the fact that the Pareto solutions are spread among several groups (Figure 6). Hence, as we
observe from Figure 5, a lower value for both m and pg are preferable as they enable a more exploratory
search. In the Scenario 3 with both negative correlations, the number of groups plays a remarkable role
as we can observe from Figure 7. When m = 10, the performance of MO-MO2 TOS is worse than random
search at the beginning of the algorithm execution, and only if pg is set to a large value, the algorithm
outperform the random search in a later phase. This may be caused by the large diversity of solutions with
different performance within each group if m is too small (Figure 8(a)), for which the good solutions do
not gain priority (marked in black) in the group since they are badly observed in low-fidelity model. On the
contrary, when m = 30, it performs better than random search, especially when pg is large, because once
the high-fidelity evaluation re-rank a small group with higher priority, all solutions inside are eventually

3745

Li, Li, Pedrielli, Lee, Chew, and Chen

(a) pg = 0.1, m = 10

(b) pg = 0.9, m = 10

Figure 6: Sampling history for Scenario 2

(a) m = 10

(b) m = 30

Figure 7: Benchmark comparison for Scenario 3
explored. Therefore, the algorithm is able to identify a good Pareto set under such setting. In conclusion,
we suggest large values for both m and pg are preferred for this scenario.
5

CONCLUSIONS

In this paper, a stochastic search algorithm MO-MO2 TOS for multi-objective optimization is developed,
and a numerical experiment is designed to test its performance with uniform random search set as the
benchmark. It has been shown that MO-MO2 TOS can perform more efficiently than random search with
proper algorithm parameter setting, for all scenarios. Specifically, when only a small number of iterations
can be executed due to computing budget limit, the advantage of the proposed algorithm is obvious.
However, there is still significant difference between the different scenarios depending on the quality
of the low-fidelity models. Specifically, when the low-fidelity model is negatively correlated with its high
fidelity model, the ranking obtained from OT will be impaired and thereby affects the performance of
MO-MO2 TOS. In order to improve the algorithm efficiency, except for tuning the algorithm setting, other
approaches can be adopted to reduce the bias brought by low fidelity model. One possible approach is to
use the negative objective value of a solution if the low and high fidelity models turn out to be negatively
correlated. In other words, the first iterations of the algorithm should be used also to learn the correlation
and the noise describing the bias of the low-fidelity model in order to improve the algorithm performance
by dynamically changing the parameters.

3746

Li, Li, Pedrielli, Lee, Chew, and Chen

(a) pg = 0.9, m = 10

(b) pg = 0.9, m = 30

Figure 8: Sampling history for Scenario 3
In this work, we have discussed the application of MO-MO2 TOS to the case of deterministic optimization
problems. An additional direction would then be to consider the case of stochastic simulations. In this
setting, MO-MO2 TOS might still prove to be effective by integrating techniques such as Optimal Computing
Budget Allocation (Lee et al. 2010) to allocate simulation replications to the sampled configurations.
ACKNOWLEDGMENTS
Research supported in part by the grant (R-SMI-2013-MA-11) funded by the Singapore Maritime Institute.
REFERENCES
Bradstreet, L., L. While, and L. Barone. 2008. “A fast incremental hypervolume algorithm”. Evolutionary
Computation, IEEE Transactions on 12 (6): 714–723.
Deb, K., S. Agrawal, A. Pratap, and T. Meyarivan. 2000. “A fast elitist non-dominated sorting genetic
algorithm for multi-objective optimization: NSGA-II”. Lecture notes in computer science 1917:849–858.
Fu, M. C., F. W. Glover, and J. April. 2005. “Simulation optimization: a review, new developments, and
applications”. In Proceedings of the 37th conference on Winter simulation, 83–95. Winter Simulation
Conference.
Lee, L. H., E. P. Chew, S. Teng, and D. Goldsman. 2010. “Finding the non-dominated Pareto set for
multi-objective simulation models”. IIE Transactions 42 (9): 656–674.
Lee, L. H., E. P. Chew, Q. Yu, H. Li, and Y. Liu. 2014. “A study on multi-objective particle swarm
optimization with weighted scalarizing functions”. In Proceedings of the 2014 Winter Simulation
Conference, 3718–3729. IEEE Press.
Li, H., L. H. Lee, E. P. Chew, and P. Lendermann. 2015. “MO-COMPASS: A Fast Convergent Search
Algorithm for Multi-Objective Discrete Optimization via Simulation”. IIE Transactions (just-accepted).
Swisher, J. R., S. H. Jacobson, and E. Yücesan. 2003. “Discrete-event simulation optimization using
ranking, selection, and multiple comparison procedures: A survey”. ACM Transactions on Modeling
and Computer Simulation (TOMACS) 13 (2): 134–154.
Xu, J., S. Zhang, C. C. Huang, C.-H. Chen, L. H. Lee, and N. Celik. 2015. “MO2 TOS: Multi-fidelity
Optimization with Ordinal Transformation and Optimal Sampling”. submitted to Asia-Pacific Journal
of Operational Research.

3747

Li, Li, Pedrielli, Lee, Chew, and Chen
Xu, J., S. Zhang, E. Huang, C.-H. Chen, L. H. Lee, and N. Celik. 2014. “Efficient multi-fidelity simulation
optimization”. In Proceedings of the 2014 Winter Simulation Conference, 3940–3951. IEEE Press.
Zitzler, E., L. Thiele, M. Laumanns, C. M. Fonseca, and V. G. Da Fonseca. 2003. “Performance assessment
of multiobjective optimizers: an analysis and review”. Evolutionary Computation, IEEE Transactions
on 7 (2): 117–132.
AUTHOR BIOGRAPHIES
HAOBIN LI is a scientist in Institute of High Performance Computing, under Agency for Science, Technology and Research (A*STAR) of Singapore. He received his B.Eng. degree (1st Class Honors) in 2009
from the Department of Industrial and Systems Engineering at National University of Singapore, with
minor in computer science; and Ph.D. degree from the same department in 2014. He has research interests
in operation research, simulation optimization and designing high performance optimization tools which
are ready for practical industrial use. His email address is lihb@ihpc.a-star.edu.sg.
YUEQI LI is an undergraduate student in the Department of Industrial and Systems Engineering at National
University of Singapore. Her email address is liyueqi@u.nus.edu.
GIULIA PEDRIELLI is Research Fellow for the Centre for Maritime Studies at the National University
of Singapore. She received her Ph.D. (honors) from Politecnico di Milano in 2013. Her research focuses
on stochastic simulation-optimization in both single and multiple–objectives framework. Her email address
is cmsgp@nus.edu.sg.
LOO HAY LEE is Associate Professor and Deputy Head in the Department of Industrial and Systems
Engineering, National University of Singapore. He received his B. S. (Electrical Engineering) degree from
the National Taiwan University in 1992 and his S. M. and Ph. D. degrees in 1994 and 1997 from Harvard
University. He is currently a senior member of IEEE, a committee member of ORSS, and a member of
INFORMS. His research interests include production planning and control, logistics and vehicle routing,
supply chain modeling, simulation-based optimization, and evolutionary computation. His email address
is iseleelh@nus.edu.sg.
EK PENG CHEW is Associate Professor and Deputy Head in the Department of Industrial and Systems
Engineering, National University of Singapore. He received his Ph. D. degree from the Georgia Institute
of Technology. His research interests include logistics and inventory management, system modeling and
simulation, and system optimization. His email address is isecep@nus.edu.sg.
CHUN-HUNG CHEN is a Professor at the Department of Systems Engineering & Operations Research at
George Mason University. He received his Ph.D. degree in Engineering Sciences from Harvard University
in 1994. He is also affiliated with the National Taiwan University. His work was sponsored by NSF, NIH,
DOE, NASA, MDA, Air Force, and FAA. He has received several awards including National Thousand
Talents Award (2011), and the Best Automation Paper Award from the IEEE Conference on Robotics and
Automation (2003). His email address is cchen9@gmu.edu.

3748

Proceedings of the 2016 Winter Simulation Conference
T. M. K. Roeder, P. I. Frazier, R. Szechtman, E. Zhou, T. Huschka, and S. E. Chick, eds.

EMPIRICAL ANALYSIS OF THE PERFORMANCE OF VARIANCE ESTIMATORS IN
SEQUENTIAL SINGLE–RUN RANKING & SELECTION: THE CASE OF TIME DILATION
ALGORITHM
Haobin Li

Giulia Pedrielli,
Yinchao Zhu,
Loo Hay Lee,

IHPC
A*STAR
1 Fusionopolis Way
SINGAPORE 138632

Department of Industrial & Systems Engineering
National University of Singapore
1 Engineering Drive 2
SINGAPORE, 117576

ABSTRACT
Ranking and Selection has acquired an important role in the Simulation-Optimization field, where the
different alternatives can be evaluated by discrete event simulation (DES). Black box approaches have
dominated the literature by interpreting the DES as an oracle providing i.i.d. observations. Another relevant
family of algorithms, instead, runs each simulator once and observes time series. This paper focuses on
such a method, Time Dilation with Optimal Computing Budget Allocation (TD-OCBA), recently developed
by the authors. One critical aspect of TD-OCBA is estimating the response given correlated observations.
In this paper, we are specifically concerned with the estimator of the variance of the response which plays
a crucial role in simulation budget allocation. We propose an empirical analysis over the performance
impact on TD-OCBA of several variance estimators involved in resource allocation. Their performances
are discussed in the typical probability of correct selection (PCS) framework.
1

INTRODUCTION

In this paper, we are interested in the sequential stochastic selection methods (S3 M) which optimize the
Probability of Correct Selection (PCS). We assume that the number of candidate configurations is finite and
the one with best expected performance is to be identified. S3 M has been widely applied as an optimization
technique in presence of few number of discrete possible solutions in a stochastic environment.
In many applications, discrete event simulation (DES) models, as a major tool to provide prediction of
system performance, are used as oracle in R&S techniques to evaluate the system behaviour. DES models
are utilized mainly in two different ways: the output is taken either while the simulation is running a single
long replication, or from multiple shorter replications once they are completed.
The issues posed by these two families of algorithms are very different. In a multiple replication case
the length of the replication and the number of replications is a relevant decision, whereas in the single
run case, we need to consider the correlation between observation. In fact, single–run simulation outputs
a time series of realizations of performance indicator; while multiple replications provide a sequence of
independent realizations of the output. As a result, the output analysis for these two families of algorithms
is remarkably different (Schruben 1983).
S3 Ms algorithms based on independent multiple replications make use of simulation allocation rules
in order to determine the number of simulations to allocate to each of the systems. Optimal Computing
Budget Allocation is one of the most known procedures for budget allocation (Chen and Lee 2010). OCBA
optimizes the probability of correct selection (i.e., the probability that the system estimated as the best

978-1-5090-4486-3/16/$31.00 ©2016 IEEE

738

Pedrielli, Zhu, Lee, and Li
at the end of the procedure is the true best system) by allocating different number of replications to the
candidate solutions. Similar to most of the developed S3 M procedures, OCBA treats the simulator as a
black box not to be accessed while it is running.
Time Dilation (TD) falls in the family of single run S3 Ms. Specifically, multiple configurations are
concurrently simulated in a unique run and time series observations are used to allocate computational
resource to the different candidates (Schruben 1997). In its original version, TD does not have a fixed
rule of budget allocation which is instead taylored to the specific applications. Realizing the potential of
TD, authors recently developed TD-OCBA, which integrates TD and OCBA. As suggested in Pedrielli,
Zhu, and Lee (2015), the output from a single–run simulation does not necessarily follow i.i.d. assumption
required by OCBA. If a sample variance estimator is used for a non i.i.d. series, the bias will be significant
and the OCBA rule will be severely impacted by the variance estimation. Hence, the variance estimator
used in TD-OCBA becomes critical for the overall performance.
The estimation of the variance from simulation output has received a remarkable attention in the
OR literature, for the purpose of point and confidence interval estimation. Goldsman and Nelson (2006)
and the related literature provides estimators for correlated output such as non-overlapping/overlapping
Batched Means, Standardized Time Series Area Estimators, and Cramer-von Mises estimators. While these
estimators are asymptotically unbiased, we are interested in the behavior of the estimators given limited
output length when used for optimization purposes and not only for estimation.
In order to investigate these properties, in this paper we focus on the study of the variance estimators
and their influence on the TD-OCBA performance. Batch mean estimator, weighted area estimator and
weighted Cramer-von Mises estimators have been studied and the experimental results for their performance
in the typical PCS framework are shown and discussed.
The remainder of the paper is organized as follows: section 2 summarizes the main background of the
paper. Section 3 details the problem of interest. Candidate estimators’ performance are studied in section
4 for a theoretical case with known output distribution. Finally, section 5 closes the paper.
2

BACKGROUND

We would like to first introduce the notation used throughout the paper:
Table 1: Notations.
k
T
Ni
v, vi
σi
σ̂i
m
Bi
Yi, j
Ȳi
Ȳi, j
Ȳi, j,l
b
δb,i

total number of systems
total budget, in number of replications(OCBA) or observations (TD-OCBA)
budget assigned to system i
time scale, time scale for system i
standard deviation for system i
standard deviation estimation for system i
batch size
total number of batches for system i
jth observation from system i
i
sample mean performance of system i, Ȳi = ∑Nj=1
Yi, j
1 m
mean of jth batch of system i, m ∑l=1 Yi,( j−1)m+l
mean of first l observation in jth batch of system i, 1l ∑lp=1 Yi,( j−1)m+p
index of best performing system
performance difference between system i and best performing system, Ȳb − Ȳi

Time Dilation grounds in the idea of redefining simulation experiments to include both the models and
the systems being simulated. As a result of this, the simulation experimental unit of effort is rethought
as real time, rather than discrete runs, batches, regenerative cycles, etc. (Schruben et al. 2003, Swisher

739

Pedrielli, Zhu, Lee, and Li
et al. 2000). This has far–reaching implications. In an integrated simulation experiment, each point in the
experiment does not need to use the same units to measure the simulated time (say, hours or minutes), nor
do the time scales need to be constant (Schruben 2010, Schruben 2013).
Hence, systems that are performing better can have their simulated time scales contracted. The result is
that less simulation work is spent simulating the poorer performing systems; the better performing systems
are simulated longer during the same interval of real time. This general idea has been called time dilation
(Schruben 1997, Hyden et al. 2001, Hyden and Schruben 2000).
The clocks for design points that are performing relatively poorly can have their time scales dilated
(increased), decreasing the relative numbers of events that are executed for that design point. More effort
is spent simulating the winners than losers.
(1/λi ) · vi [GTU] = 1/λi [LTU]

(1)

Equation (1), formalizes the time–scale update. If 1/λi represents the inter–arrival time and the global
time unit (i.e., GTU) is [hours], we might want to move the time unit of a good system to minutes by
[LTU]
, i.e., the time scale, with vi = 1/60 in this
multiplying the inter–arrival by the conversion factor vi = [GTU]
example. By doing so, the simulator with associated time scale vi = 1/60 will execute events 60-times
faster than the candidates with associated time scale v j = 1. This is the basic idea of time dilation and
is the mechanism we exploit to differentiate computational budget among different solutions, instead of
choosing the number of replications. Mapping time–dilated performance back to global simulated time is
done simply by dividing each inter–event interval by the time–scale used simulating each point during that
interval.
Sequential techniques that change the probability of where to run the next simulation appear to be
well–suited for adaptation to the experimental unit of run time. In fact, the allocation ratio generated by a
simulation allocation rule can be used to modify the time scale assigned to a specific simulation experiment.
Optimal Computing Budget Allocation (OCBA) is one of the most successful techniques in the field of
stochastic assignment of simulation budget (Chen and Lee 2010). OCBA formulates the allocation decision
as a constrained maximization problem of the Probability of Correct Selection (PCS) subject to the total
budget limitation:
max PCS

N1 ,...,Nk

s.t. N1 + N2 + ... + Nk = T.

(2)

Ni ∈ N, i = 1, ..., k.
where PCS is the probability that the observed best system is the system with best expected performance.
We can solve the aforementioned problem and derive the following allocation (Chen and Lee 2010):

σi /δb,i 2
, i, j ∈ 1, 2, ..., k, and i 6= j 6= b,
σ j /δb, j
v
u k
u
Ni2
Nb = σb t ∑
2
i=1,i6=b σi

Ni
=
Nj



(3)

In order to fully utilize advantages from both TD and OCBA, in Pedrielli, Zhu, and Lee (2015), we
employ the rule from OCBA to update the time scale in time-dilation. In order to do so, we need to compute
the time scale ratio instead of reasoning in terms of number of replications as in equation (3). If we refer
to αi as the ratio of computational budget allocated to system i, i.e., αi = NTi , we can reformulate equation
(3) as:

740

Pedrielli, Zhu, Lee, and Li

σi /δb,i 2
, i, j ∈ 1, 2, ..., k, and i 6= j 6= b,
σ j /δb, j
v
u k
u
αi2
αb = σb t ∑
,
2
i=1,i6=b σi
αi
=
αj



(4)

k

∑ α p = 1.

p=1

and we put forward a more general single–run ordinal optimization procedure, TD-OCBA, integrating TD
and OCBA.
In order to implement TD-OCBA, each system is simulated according to a local clock and the overall
simulation has a shared clock (global clock). Initial events for all systems are scheduled at the same time.
And subsequent events will be scheduled to a common clock time stamp, by adding, timespan to wait
for next event in local clock times the time scale, to the current common clock time stamp. The overall
simulation always execute the event with earlier common clock time stamp. Update for the time scales
happens firstly when each of the candidates has generated at least one batch of output data. Then the update
will happen whenever there is a system that outputs m new observations. Finally the procedure will stop
when total number of observations for all systems reaches T . The system with best observed performance
will be selected. As previously highlighted, if a system is allocated a larger time scale vi , the time between
events will be increased (dilated) as well. As a result, being all systems subject to the same global time
clock, larger time scale configurations will be simulating less events given a fixed simulation interval.
Algorithm 1: Time Dilated Optimization & OCBA (TD-OCBA).
Initialization: Set the total budget T (number of observations);
l = 0, νi ← ν0 , di = 0;
while ∑ki=1 Ni ≤ T do
for i = 1, ..., k (parallel loop) do
Observe the sequence {Yi,di m+1 ,Yi,di m+2 , ...,Yi,di m+m } ;
Yi, j
i m+m
Update: Ȳi = ∑dj=1
;
d
m+m
i
√
σ̂i ← variance estimation for system i ;
Ni ← Ni + m;
di ← di + 1;
end
Choose b s.t. b ∈ arg mini=1,...,k Ȳi ;
vi ← αb /αi , i = 1, ..., k, determined in (4);
end
This procedure has been shown to have better performance than original TD and it can be applied to
more general problems. At this point, we want to further develop the TD-OCBA procedure to improve the
efficiency.
In order for the time scales to be correctly assigned, performance of the variance estimation is critical.
For single–run simulations, the assumption of i.i.d. on the observations is not valid for most cases. Thus
variance estimators that do not assume correlation structure are more suitable to be employed in the
TD-OCBA procedure. To the best of authors’ knowledge, Conway (1963) first concerned about variance
estimation for simulation and since then it has been an interest field of statistical analysis. With techniques
of batching and overlapping, several estimators have been developed in Schmeiser (1982), Goldsman et al.
741

Pedrielli, Zhu, Lee, and Li

Figure 1: Flow chart for Algorithm 1.
(1990), Goldsman et al. (1999), Alexopoulos et al. (2007). While the asymptotic consistency of these
estimators has been well studied, yet what affect the performance of TD-OCBA is the accuracy of the
estimator. Intuitively, given same budget, TD-OCBA running with estimator of more accurate estimation
should produce higher PCS. We are interested in this argument and thus this is the focus for this study.
3

VARIANCE ESTIMATION IN SINGLE RUN SIMULATION OPTIMIZATION

The interest of further studying the TD-OCBA lies in improving convergence rate of PCS. It is realized
that this problem can be decomposed into 2 layers. The first layer problem is the allocation rule. We have
not confirmed that use of original OCBA ratio guarantees optimized allocation for PCS. The optimization
can be formulated as:
max PCS
α1 ,...,αk

s.t. α1 + α2 + ... + αk = 1.

(5)

αi > 0, i = 1, ..., k.
The second layer problem is concerned with the estimation for the variance associated to the simulation
output since we do not know the correlation structure of the time series output generated by the simulator.
Arguably, the more accurate the estimator is, the closer the time scale allocation is to the asymptotic optimal
allocation.
While we need to use an asymptotically consistent indicator, we are also interested in the variance associated
to this estimator. The first is a well studied problem, and several consistent estimators have been proposed.
Alexopoulos et al. (2007) notes 3 aspects to assess the performance of variance estimator, namely: (1)
bias, (2) variance, and (3) MSE (sum of variance and the square of bias).
The main control parameters to estimate the variance from a series of observations are:
•
•
•
•

B indicating the number of batches;
m indicating the batch size;
ρ indicating the overlapping ratio, i.e., the portion of observations from the previous batch, which
are re-used to form current batch;
E = {1, 2, . . . , 5} the estimator type, chosen among the available estimators and applied to all
systems.

Now the second layer problem becomes, assigned the budget from level one, how to set the aforementioned
parameters in order to minimize the estimated MSE associated with the output, namely, for each system

742

Pedrielli, Zhu, Lee, and Li
i = 1, . . . , k:
Pi :

ˆ σ̂i2
min MSE



Bi ,mi ,ρi ,Ei

(6)

If the bias has been widely analyzed, the variance of such indicators has not been fully studied, however
it impacts on the optimization performance. Indeed, given the limited budget of observations, TD-OCBA
is sensitive to the accuracy of the estimators.
In this paper we investigate the effect of the bias and the variance of the estimators on the performance
of TD-OCBA, which represent our second layer problem.
In this work. we choose estimators with known bias and variance are valuable for the assessment. In
the search in literature, three families of estimators, namely batch mean estimator, weighted area estimator
and CvM estimator and their overlapped form appear to be of particular interest.
3.1 Batch Mean Estimator
Non–overlapping Batch Mean (NBM) estimator was first discussed in Schmeiser (1982). Instead of using
the standard variance estimator, the author groups observations into batches and estimates the variance
of the sequence using the variances of the sample average computed for each batch. For each system
configuration i, the estimator takes the following form:
σ̂i2 =

1 Bi
m Bi
(Ȳi,k − ∑ Ȳi,l )2 .
∑
Bi − 1 k=1
Bi l=1

(7)

This estimator represents the intuitive extension of the sample variance were the i.i.d. observations are
replaced by the means of the different batches and the sample average is still the average of all the
observations. We will refer to match mean estimator as N .
3.2 Standardized Time Series
Schruben (1983) proposes the concept of standardized time series. For each system, the standardization
involves converting the time series Yi,1 , ...,Yi,n into B batches with m outputs (Yi,1 , ...,Yi,m ),(Yi,m+1 , ...,Yi,2m ),...,
(Yi,(Bi −1)m+1 , ...,Yi,Bi m ) and performing the following transformation:
Ti, j (t) ≡

bmtc(Ȳi, j,bmtc − Ȳi, j,m )
√
σ m

(8)

for 0 ≤ t ≤ 1 and j = 1, 2, ..., Bi , where
Ȳi, j,l ≡

1 l
∑ Yi,( j−1)m+g
l g=1

Based on this revolutionary idea, several estimators for the variance of time series observations have been
proposed. We focus on two particular estimators: (1) the Weighted Area Estimator in section 3.3 and the
Batched Cramer Von Mises Estimator presented in section 3.4.
3.3 Weighted Area Estimator
Goldsman et al. (1990) introduced the Batched area estimator (BAE) with the following form:
(
!!)2
 
Bi
m
h
m
1
1
h
h
1
1
σ̂i2 =
.
∑ m ∑ f m √m h ∑ Yi,( j−1)m+l − m ∑ Yi,( j−1)m+l
Bi j=1
h=1
l=1
l=1
743

(9)

Pedrielli, Zhu, Lee, and Li
where f is the weighting function that satisfies: f (t) is continuous on interval [0, 1] and normalized so that
R
Var( 01 f (t)σ B0 (t)dt) = 1. B0 is a Brownian
bridge process on [0, 1].
√
We will refer to BAE taking f (t) = 12 as A ( f0 ).
3.4 CvM Estimator
Batched CvM estimator(CvM) has been introduced in Goldsman, Kang, and Seila (1999). It takes the
following form:

!!2 
 

Bi 
m
h
m
1
h
h
1
1
1
2
√
(10)
σ̂i =
∑ m ∑ g m
∑ Yi,( j−1)m+l − m ∑ Yi,( j−1)m+l  .
Bi j=1
m h l=1
h=1
l=1
where g is the weighting function that satisfies: g has a continuous and bounded second derivative on [0, 1]
R
and is normalized so that E[ 01 g(t)σ 2 B02 (t)dt] = σ 2 .
We will refer to CvM taking g(t) = 6 as C (g0 ).
3.5 Overlapping Estimators
As the name suggests, these estimators involve overlapping of the batches. For non-overlapping estimators,
batches are formed by each m new observations. In Alexopoulos et al. (2007), batches for overlapping
estimators consist of m − 1 observations from the most recent batch and only 1 observation is added to
form a new batch.
In the TD-OCBA approach, such an update would require an excessive computational load. Therefore,
we assume that the overlapping is determined by a ratio that the user (or the algorithm itself) can set
statically or dynamically.
Hence, we introduce the overlapping ratio parameter o. Differently from the approach proposed in
Alexopoulos et al. (2007), a new batch is formed from bomc new outputs and d(1 − o)me from the last
batch. The overlapping ratio proposed in Alexopoulos et al. (2007) is m−1
m .
All the estimators presented in the previous sections can be applied to overlapped batches without any
modification in the formulation.
While all the presented estimators are asymptotically unbiased, their bias terms and variance terms
differ. Table 2 is taken from Alexopoulos et al. (2007). These estimators are chosen as they have exact
approximate bias and variance.
Table 2: Approximate asymptotic bias and variance for estimators.
Nonoverlapping
N
A ( f0 )
C (g0 )

(m/γ) Bias
1
3
5

(b/σ 4 ) Var
2
2
0.8

Overlapping ratio =
N
A ( f0 )
C (g0 )

m−1
m

(m/γ) Bias
1
3
5

(b/σ 4 ) Var
1.333
0.686
0.419

From Table 2, we can observe that for N , A ( f0 ) and C (g0 ), both approximated bias and variance are
provided. Hence they are chosen to be incorporated in the TD-OCBA procedure to check the procedure
performance (required budget) against (m/γ) Bias and (b/σ 4 ) Var. The batch size m is also varied as we
are interested to see its effect on the budget.
4

NUMERICAL EXPERIMENTS & RESULTS

In this section, we use theoretical problems (i.e., problems for which we know the optimum as well as
the true variance of the output for each candidate solution) to test the performance of the estimators. In
particular, we generate two set of auto-regressive time series:

744

Pedrielli, Zhu, Lee, and Li
s1:

Yi,t+1 = ci + 0.5Yi,t + N(0, 3), where ci = i, i = 1, ..., 10.

s2:

Yi,t+1 = ci + 0.5Yi,t + N(0, 6), where ci = i, i = 1, ..., 10.

where the first group is considered to have small noise and second to have large noise.
The time series with smallest asymptotic mean is to be selected. The performance of an estimators is
assessed by looking at the budget required to reach a fixed PCS level.
The statistic of the PCS is obtained out of 1000 macro-replications of the TD-OCBA algorithm. In the
experiments, we observed that additional computational time required for the time scale update computation
is negligible. This is particularly true when the simulation is significantly slow (i.e., the largest part of
the computational time is required by the simulator itself). Hence, we can empirically conclude that the
computation of estimator does not represent a computational burden in a realistic simulation optimization
problem (realistic in terms of computational effort required by the simulation).
In the results, the budget reported is the average of the observations required for each of the macroreplications required to obtain a threshold PCS level. In the experiments we study the effect of the variance
estimators and the batch size as well as the overlapping ratio (refer to Problem (6)).
Tables 3 and 4 show the results of the relative experiments.
Table 3: Budget for PCS to reach 0.95 for series 1.
m
5
10
20
50

Nonoverlapping
N
A ( f0 ) C (g0 )
357.2 215.2
209.5
239.4 229.1
212.9
278.6 296.3
278.9
505.5 505.4
504.9

Overlapping ratio = 0.2
N
A ( f0 ) C (g0 )
367.1 216.7
207.4
230.0 226.7
212.3
277.2 292.5
278.4
505.2 505.8
507.0

Overlapping ratio = 0.5
N
A ( f0 ) C (g0 )
361.6 213.3
209.1
234.1 224.5
211.2
281.5 291.7
275.7
506.1 506.7
505.0

Overlapping ratio = 0.8
N
A ( f0 ) C (g0 )
341.3 211.3
206.1∗
221.5 216.8
209.0+
281.6 284.4
274.2+
+
505.8 504.7
506.7

Table 4: Budget for PCS to reach 0.90 for series 2.
m
5
10
20
50

Nonoverlapping
N
A ( f0 ) C (g0 )
1670.5 783.4
590.1+
785.8
680.7
506.7
587.6
631.2
513.0
690.6
743.5
695.7

Overlapping ratio = 0.2
N
A ( f0 ) C (g0 )
2037.4 913.5
623.0
738.3
672.2
499.1
560.8
633.2
505.0
677.3+ 740.0
688.7

Overlapping ratio = 0.5
N
A ( f0 ) C (g0 )
2983.2 985.0
664.1
793.2
641.3
488.8∗
595.6
602.3
501.7
688.8
731.7
686.6

Overlapping ratio = 0.8
N
A ( f0 ) C (g0 )
2984.9 882.7
648.8
836.3
593.9
490.8
603.9
565.4
498.0+
700.8
722.1
677.8

In Table 3 and 4, highlighted cell contains the best performer of the 3 estimators for the same batch size and
overlapping ratio. Non-overlapping can be inferred as having o = 0. + indicates the best performance in
one row, thus for same batch size, comparing both overlapping and non-overlapping ratio cases. ∗ denotes
the overall best performance both in the row and table, i.e., comparing against all the parameters.
Similar experiments have been conducted for the original TD-OCBA in (Pedrielli et al. 2015) that uses
the sample variance as estimator. In the original settings, the time scales are updated at each observation
and no batching is adopted. In this setup, the original algorithm required an average budget of 248.2
observations to achieve a PCS of 0.95 for the case of time series 1, and an average budget of 881.3 to
achieve a PCS level of 0.90 for the case of time series 2. It is noteworthy that, even though the original
algorithm has an updating frequency about 1 order of magnitude larger than the batch-based algorithm,
the original TD-OCBA with sample variance estimator is outperformed by TD-OCBA with C (g0 ) when
a proper batch size is chosen.
Further observations comparing the selected estimators can be drawn from the results.

745

Pedrielli, Zhu, Lee, and Li
The first is that the budget to reach fixed PCS is indeed related to the variance estimator. It is found
that budget requirement varies by quite much using different estimator. From highlights and marks, it is
noticed that C (g0 ) performs better then N and A ( f0 ) in most cases.
The interesting part is that, according to the theoretical results presented in (Alexopoulos, Argon,
Goldsman, Tokol, and Wilson 2007), C (g0 ) has the largest Bias (m/γ) and the lowest variance ((b/σ 4 )).
In other words we observe a positive relationship between the required budget and the variance of the
indicator, whereas the bias does not seem to influence the performance.
By comparing the performance of overlapping and non-overlapping estimators, the overlapping version
performs better most of the times. The overlapping coefficient impacts TD-OCBA in two ways: (1) the
frequency at which the time scale is updated. From the description of the TD-OCBA procedure, it is known
that time scales gets updated as soon as one new batch is formed, which results in the change in variance
estimation. So for experiments using overlapping estimators, the time scale get updated more often; (2)
overlapping estimators have lower associated variance than their non-overlapping counterpart and this may
contribute to the difference in efficiency.
Lastly, the batch size is affecting the performance as well. The impact of the batch size can be well
understood as it does not only affect the number of batches for each estimator, but it contributes to affect the
frequency as which time scales are affected. Nevertheless, the budget to reach fixed PCS is not monotonic
with the batch size. Though the estimator converges to true variance given infinitely large batch size, larger
batch size results in less number of update of time scale for limited budget. In other words given finite
budget, we have to trade off the batch estimator precision with the number of batch observations that can
be obtained.
5

CONCLUTIONS & FUTURE WORK

Ranking and Selection has acquired an important role in the Simulation-Optimization field, where the
different alternatives can be evaluated by discrete event simulation (DES). In fact, black box approaches
have dominated the literature by interpreting the DES as an oracle providing i.i.d. observations.
Another relevant family of algorithms, instead, considers an oracle which produces time series observations as each simulator is ran only once (i.e., no multiple replicates are performed). In this paper,
we focus on a specific single run sequential stochastic selection method (S3 M) recently developed by the
authors, Time Dilation with Optimal Computing Budget Allocation (TD–OCBA).
As a single run approach, one of the most critical aspects of TD–OCBA is to deal with the estimate of the
response given a series of correlated observations. In fact, in this paper, we derive a two-stage optimization
structure for the class of single-run R&S algorithms. In particular, at a first stage we choose how to allocate
the simulation budget to each candidate solution while at the second stage we want to minimize the MSE of
the variance estimator playing with the estimator type the batch size and the overlapping ratio. Due to the
difficulty in the estimation of the MSE, in this manuscript, we study the empirical performance of several
variance estimators, different batch sizes as well as overlapping ratios in order to enhance the TD-OCBA
procedure.
We propose an autoregressive process as case study and we observe a positive relationship between
the asymptotic variance of the estimators and the required budget to reach fixed PCS values.
Also, generally, the overlapped estimators perform better than the non-overlapping counterpart. This is
due to the fact that, given the same budget, more observations are available to the optimization procedure
and this results in a positive effect in the search algorithm.
The required budget is, instead, non-monotonic in the batch size m. This is due to the fact that if,
on the one hand larger batch size reduce the variance of the estimator, they also reduce the number of
observations available in the scope of the search procedure, thus giving issues to the optimization.
Current work is focusing on the formulation of the second stage problem in order to enhance the TDOCBA through an automated procedure that checks the MSE and returns a dynamic value for overlapping
ratio and batch size.

746

Pedrielli, Zhu, Lee, and Li
Also, TD-OCBA opens to alternative first level problem formulation: since single-run procedures are
easier to control in terms of running time, whereas the number of observations produces in a certain interval
is typically a random variable. In such a setting, in addition to the probability of correct selection, we
would like to formulate the problem in the framework of regret due to the fact that we do not know how
many iterations the algorithm will produce in a specific run-time. As a result, we want to guarantee a good
solution at any point in time.
REFERENCES
Alexopoulos, C., N. T. Argon, D. G. Goldsman, G. Tokol, and J. R. Wilson. 2007. “Overlapping Variance
Estimators for Simulation”. Operations Research 55 (6): 1090–1103.
Chen, C. H., and L. H. Lee. 2010. Stochastic Simulation Optimization: An Optimal Computing Budget
Allocation. World scientific.
Conway, R. W. 1963. “Some Tactical Problems in Digital Simulation”. Management Science 10 (1): 47–61.
Goldsman, D., K. Kang, and A. F. Seila. 1999. “Cramer-Von Mises Variance Estimators for Simulations”.
Operations Research 47 (2): 299–309.
Goldsman, D., M. Meketon, and L. Schruben. 1990. “Properties of Standardized Time Series Weighted
Area Variance Estimators”. Management Science 36 (5): 602–612.
Goldsman, D., and B. L. Nelson. 2006. “Chapter 15 Correlation-Based Methods for Output Analysis”. In
Simulation, Volume 13 of Handbooks in Operations Research and Management Science, 455 – 475.
Elsevier.
Hyden, P., and L. W. Schruben. 2000. “Improved Decision Processes Through Simultaneous Simulation and
Time Dilation”. In Proceedings of the 2000 Winter Simulation Conference, edited by K. K. J. A. Joines,
R. R. Burton and P. A. Fishwick, 743–748. Orlando, Florida: Institute of Electrical and Electronics
Engineers, Inc.
Hyden, P., L. W. Schruben, and T. Roeder. 2001. “Resource Graphs for Modeling Large-scale, Highly
Congested Systems”. In Proceedings of the 2001 Winter Simulation Conference, edited by D. J. M.
B. A. Peters, J. S. Smith and M. W. Rohrer, 523–529. Arlington, Virginia: Institute of Electrical and
Electronics Engineers, Inc.
Pedrielli, G., Y. Zhu, and L. H. Lee. 2015. “Single–run Simulation Optimization Through Time Dilation
and Optimal Computing Budget Allocation”. In Proceedings of the 10th Conference on Stochastic
Models of Manufacturing and Service Operations, 187–194.
Schmeiser, B. 1982. “Batch Size Effects in the Analysis of Simulation Output”. Operations Research 30
(3): 556–568.
Schruben, L. W. 1983. “Confidence Interval Estimation Using Standardized Time Series”. Operations
Research 31 (6): 1090–1108.
Schruben, L. W. 1997. “Simulation Optimization Using Simultaneous Replications and Event Time Dilation”.
In Proceedings of the 1997 Winter Simulation Conference, edited by D. H. W. S. Andradbttir, K. J. Healy
and B. L. Nelson, 177–180. Atlanta, Georgia: Institute of Electrical and Electronics Engineers, Inc.
Schruben, L. W. 2010. “Simulation Modeling for Analysis”. ACM Transactions on Modeling and Computer
Simulation (TOMACS) 20 (1): 2.
Schruben, L. W. 2013. “Simulation Modeling, Experimenting, Analysis, and Implementation”. In Proceedings of the 2013 Winter Simulation Conference, edited by A. T. R. H. R. Pasupathy, S.-H. Kim and
M. E. Kuhl, 678–690. Washington, D.C.: Institute of Electrical and Electronics Engineers, Inc.
Schruben, L. W., T. M. Roeder, W. K. Chan, P. Hyden, and M. Freimer. 2003. “Advanced Event Scheduling
Methodology”. In Proceedings of the 2003 Winter Simulation Conference, edited by D. F. S. Chick, P.
J. Snchez and D. J. Morrice, 159–165. New Orleans, Louisiana: Institute of Electrical and Electronics
Engineers, Inc.
Swisher, J. R., P. D. Hyden, S. H. Jacobson, and L. W. Schruben. 2000. “A Survey of Simulation Optimization
Techniques and Procedures”. In Proceedings of the 2000 Winter Simulation Conference, edited by K. K.

747

Pedrielli, Zhu, Lee, and Li
J. A. Joines, R. R. Burton and P. A. Fishwick, 119–128. Orlando, Florida: Institute of Electrical and
Electronics Engineers, Inc.
AUTHOR BIOGRAPHIES
GIULIA PEDRIELLI is Research Fellow for the Department of Industrial & Systems Engineering at the National University of Singapore. Her research focuses on stochastic simulation-optimization in both single and
multiple–objectives framework. She is developing her research in meta-model based simulation optimization
and learning for simulation and simulation optimization. Her email address is giulia.pedrielli.85@gmail.com.
YINCHAO ZHU is a research engineer in National University of Singapore. He received his B.Eng.
degree and B.Sci in 2012 from Engineering Science Program and Department of Mathematics at National
University of Singapore. He is currently pursuing his PhD degree in the Department of Industrial and
Systems Engineering. He has research interests in operations research and simulation optimization. His
email address is yinchao.zhu@u.nus.edu.
LOO HAY LEE is Associate Professor and Deputy Head for the Department of Industrial and Systems
Engineering, National University of Singapore. He received his B. S. (Electrical Engineering) degree from
the National Taiwan University in 1992 and his S. M. and Ph. D. degrees in 1994 and 1997 from Harvard
University. He is currently a senior member of IEEE, a committee member of ORSS, and a member of
INFORMS. His research interests include production planning and control, logistics and vehicle routing,
supply chain modeling, simulation-based optimization, and evolutionary computation. His email address
is iseleelh@nus.edu.sg.
HAOBIN LI is Scientist for the Institute of High Performance Computing, A*STAR Singapore. He
received his B.Eng. degree (1st Class Honors) in 2009 from the Department of Industrial and Systems
Engineering at National University of Singapore, with minor in computer science; and Ph.D. degree from
the same department in 2014. He has research interests in operations research, simulation optimization
and designing high performance optimization tools with application on logistics and maritime studies. His
email address is lihb@ihpc.a-star.edu.sg.

748

Simulation of complex manufacturing systems via
HLA-based infrastructure
Giulia Pedrielli, Paola Scavardone, Tullio Tolio

Marco Sacco, Walter Terkaj

Mechanical Department
Politecnico di Milano
Milan, 20156, ITALY
giulia.pedrielli@mail.polimi.it

Institute of Industrial Technologies and Automation (ITIA)
National Research Council (CNR)
Via Bassini 15
Milan, 20133, ITALY

Abstract— Manufacturing systems can be thought as production
networks nodes whose relations have a strong impact on design
and analysis of each system. One of the most common techniques
to support these tasks is Discrete-Event Simulation. The state-ofthe art commercial simulators are already adopted to analyze
complex networked systems, but the development of a monolithic
simulation model can be too complex or even infeasible when a
detailed description of the nodes is not available outside the
"owner" of the node. In these cases the problem can be
decomposed by modeling complex systems with various
simulators that interoperate in a synchronized manner. Herein,
the integration of simulators is addressed by taking as a reference
the High Level Architecture (HLA) and the research carried out
by Commercial-off-the-shelf Simulation Package Interoperability
(CSPI) Product Development Group (PDG). This paper proposes
modifications to CSPI-PDG protocols and to use patterns of how
HLA can be effectively adopted to support CSP interoperability:
a new solution for the synchronous entity passing problem and a
modification to the Entity Transfer Specification are presented.
The resulting infrastructure is validated and tested over a
realistic industrial case.
Complex Manufacturing Systems; Discrete Event Simualtion;
Distributed Simulation; HLA

I.

INTRODUCTION

Nowadays manufacturing companies have to face an
increasingly turbulent market characterized by demand
variability, unstable requirements from the clients and short
product lifecycles [14]. In addition to the problems related to
the market, the performance of a manufacturing system is
deeply affected by its relations with other systems in the
supply chain network. Indeed, every production system is not
a standalone unit, but a node in a production network
characterized by complex dynamics that should be considered
during the design and analysis phases [18]. Both in the
literature and in the industrial practice, Discrete Event
Simulation is used to analyze production and logistics
problems in various industrial domains [5] thanks to
Commercial-Off-The-Shelf (COTS) Simulation Packages
(CSPs) that are available on the market and provide a wide
range of functionalities (e.g. visual building of the simulation
model, simulation run support, animation, etc.). However, the
complexity of a simulation model becomes hardly manageable
when the relations between the nodes of a production network

must be considered, since the modeling of a single node is
already complex by itself and requires specific expertise and
information [15]. Moreover, in real practice the developer of a
simulation model can hardly access to all the information
characterizing a production network because information
sharing is seen as a threat by most of the companies, thus
hindering the feasibility of developing a unique and
monolithic simulation model to evaluate the performance of
complex production networks. In these cases a distributed
simulation approach can be proposed to face the
aforementioned criticalities. However, even though a standard
like HLA [4] has already been proposed to support the
interoperability between simulators, several problems arise
when trying to interoperate heterogeneous simulators in real
industrial cases, thus highlighting the need of enhancing HLA
with additional complementary standards [12]. Furthermore,
the definition of a standard language for CSPs represents a
relevant and not yet solved scientific challenge [3, 9, 10].
Recently, the COTS Simulation Package InteroperabilityProduct Development Group (CSPI-PDG), within the
Simulation Interoperability Standards Organization (SISO),
worked on the definition of the CSP interoperability problem
(Interoperability Reference Models, IRMs) and on a draft
proposal for a standard to support the CSPs interoperability
(Entity Transfer Specification, ETS). Nevertheless, an
effective interoperability among CSPs is still far to be reached
in industrial contexts.
Boer [1] investigated the main benefits and criticalities
related to the application of HLA in the industrial domain by
carrying out a survey aimed at various players involved in the
problem (e.g. simulation model developers, software houses,
HLA experts). The results of the survey showed that CSPs
vendors do not see direct benefits in using distributed
simulation, whereas in industry HLA is considered
troublesome because of the lack of experienced users and the
complexity of the standard. A literature review on HLA
applications in the civil domain was performed by the authors
of this paper. About 100 papers published during the last
decade were analyzed, showing that 30% of the works deal
with the industrial domain. However, only 6% of the papers
address industrial cases. The latter remark confirms the results
obtained by Boer. This analysis highlights the need of further
efforts to demonstrate the applicability of HLA and evaluate
the benefits of interoperability in the industrial field.

978-1-4577-1366-8/11/$26.00 ©2011 IEEE

The remainder of this paper is structured as follows.
Section II presents the Problem Statement, delving into the
problem of CSP interoperability and the IRMs. Section III
analyzes the literature and highlights some of the open issues.
Section IV proposes a solution to the Type A.2 IRM, a
modification to ETS, and a communication protocol between
the CSP and its adapter taking as a reference the work already
carried out by CSPI-PDG. Section V addresses the
implementation of the proposed solution that is validated
(Section VI) and then tested over a realistic industrial case
(Section VII). Finally, conclusions and future developments
are drawn in Section VIII.
II.

PROBLEM STATEMENT

The analysis of networked manufacturing systems by
means of distributed discrete event simulation is addressed by
developing distinct simulators for each subsystem, and
connecting them according to the relations characterizing the
network. The work presented in this paper is aimed at
supporting the design and analysis tasks (Section I) for the
following classes of manufacturing systems [2]:
•

Assembly/disassembly systems, where assembly
(disassembly) machines take different components
(part) from one or more input buffer(s) and produce a
single part (parts of different types) which is (are)
placed in a downstream buffer (different output
buffers).

•

Split/merge systems, where different part types are
managed. A split machine receives all parts from a
single upstream buffer and then places the processed
parts in different downstream buffers according to the
part type and the adopted policy. A merging machine
receives the parts of different type from more than one
upstream buffer, but places all the processed parts in a
single downstream buffer. The split/merge systems
differ from the assembly/disassembly ones because
parts cannot be modeled as components of the same
product. For further details please refer to [2].

•

•

Closed loop systems, where the last machine of the
system is linked with the first machine and the number
of parts in the system remains constant.
General production lines with dedicated (flexible)
machines.

The simulation of these classes of manufacturing systems
via a distributed approach is strongly related to the
representation of the part and information flow between the
subsystems, thus rising the need to formalize this kind of
transfer. In literature [11] the part transfer was formalized by
the definition of the entity passing problem where the term
entity refers to elements that are dynamically created and
moved during a simulation [9]. The main result in the
formalization of entity passing problem was presented by
CSPI-PDG with the definition of Type A IRM, namely Entity
Transfer.

Fig. 1 outlines the basic idea behind both Type A.1 and
Type A.2 IRMs. The manufacturing system is decomposed
into subsystems consisting of workstations and buffers, and
each subsystem is associated with a different simulation
model. Mi represents the model of the i-th production
subsystem and Enij/Exij represent the j-th entry/exit point in
Mi. Qik is associated with the k-th buffer in Mi, Wih stands for
the h-th workstation in Mi, whereas the arrows represent the
flow of entities. In Fig. 1, an entity can be transferred from
workstation W1a to buffer Q2a.
Type A.1 IRM (named “General Entity Transfer”) is
defined as the transfer of entities from one model to another,
i.e. an entity leaves from a given place in a sending model
(e.g. M1) at time T1 and arrives at a given place in a receiving
model (e.g. M2) at time T2 (T1 <= T2). The departure and
arrival places can be buffers, workstations, etc. (e.g. W1a is a
departure place and Q2a is an arrival place). According to Type
A.1 IRM the entity transfer is always feasible, thus no
authorization is required before passing an entity from a model
to another. The entity transfer represented in Fig. 1 can be of
Type A.1 IRM only if the capacity of Q2a is unbounded.
Type A.2 IRM (named “Bounded Receiving Element”) is
defined as the relation between a generic element in the
sending model and a bounded element in receiving model, so
that the feasibility of the entity transfer depends on the state of
the receiving element. For instance, an entity transfer is
blocked when the queue of the receiving element is full, even
if the entity is ready to leave at time T1 and would attempt to
arrive at the bounded element at time T2. Therefore, the
information about the target element state is needed by the
sending model, since it could stop the entity transfer. The
entity transfer represented in Fig. 1 is of Type A.2 IRM if the
capacity of Q2a is bounded.
Type A.3 IRM (named “Multiple Input Prioritization”)
represents the case where an element of the receiving model
can receive entities from more than one sending model. A
problem arises if entities coming from different places arrive
at the same time (i.e. there are simultaneous events) and the
receiving element is not able to receive all of them [12, 17].
III.

THE PROBLEM OF ENTITY TRANSFER

Past research contributions presented solutions to face both
Type A.1 and Type A.2 IRMs. In particular, a reference
architecture (Fig. 2) for the distributed simulation [12] and a
protocol to manage the communication between the
architecture components [9, 11, 12] were proposed.
Section III.A presents the reference architecture adopted in
this work and highlights the open issues, whereas Section III.B
introduces the communication protocols and highlights their
major drawbacks.

Figure 1.

Entity Transfer

hypotheses have been made in this paper: all the models have
the same time definition and resolution (i.e. there is a
relationship between how time is represented in one CSP and
another) and a mapping relationship exists between the entity
representations in the various models.
This paper addresses the transfer mechanism that is used to
move an entity from one model to another thanks to the
communications between middleware and RTI, and between
CSP and middleware. CSPI-PDG already proposed the Entity
Transfer Specification (ETS) Protocol [11, 12] to manage
communication at middleware-RTI level (see Section III.B).
Herein a communication protocol based on Simulation
Messages (see Fig. 2) is proposed to manage the
communication between a CSP and its middleware (or adapter)
when simulating a network of Discrete Event Manufacturing
Systems that is characterized by the transfer of parts in the
presence of buffers with finite capacity (see Section IV.B). The
presence of Simulation Messages is the main difference
between the reference architecture in Fig. 2 and the architecture
proposed in [12].

Figure 2. Reference Architecture

A. Reference Architecture
The general architecture shown in Fig. 2 is taken as a
reference throughout this work and is mainly based on
architecture proposed by Taylor et al. [12]. A detailed
description of the architecture components can be found in [11]
and [12].
Each federate consists of a COTS simulation package
(CSP), a model that is executed by the CSP, and the
middleware that is a sort of adaptor interfacing the CSP with
the Run Time Infrastructure (RTI). The relationship between
CSP, the middleware and the RTI consists of two
communication flows: (1) middleware-RTI, (2) CSPmiddleware. The middleware translates the simulation
information in a common format so that the RTI can share it
with the federation. In addition, the middleware receives and
sends information from/to the CSP.
Two main issues arise when the simulation information is
translated for the RTI:
•

•

A common time definition and resolution is necessary.
For example, the time should be defined as being the
time when an entity exits a source model and then
instantaneously arrives at the destination model (i.e.
the definition of time implies zero transit time) [11].
The representation of an entity depends on how the
simulation model is designed and implemented in a
CSP. Indeed, the names that the modelers use to
represent the same entity might be different. A similar
problem can arise for the definition of simple
datatypes. For example, some CSPs use 32-bit real
numbers while others use 64-bit [11].

Since the aforementioned issues are related to the adopted
CSP and the decisions taken by the modelers, two simplifying

B. ETS Protocol
The ETS protocol proposed by CSPI-PDG defines the
communication between the sending model and the receiving
model (ModelA and ModelB in Fig. 2, respectively) at RTI
level by means of a special hierarchy of interaction classes. An
interaction class is defined as a template for a set of
characteristics (parameters) that are in common within a group
of interactions [4]. The middleware of the sending model
instantiates a specific interaction class and sends it to the RTI
whenever an entity has to be transferred.
After developing the interaction class hierarchy, following
the HLA standard, the extensions to Object Model Template
(OMT), Simulation Object Model (SOM) and Federation
Object Model (FOM) were proposed by CSPI-PDG to include
the novel interactions and their parameters. In particular
extensions were proposed to the Interaction Class Table to
include the novel interaction classes and define them as
publish and/or subscribe. The Parameter Table was modified
to include the proposed parameters for the interactions and the
Datatype table was also modified. For further details please
refer to [12].
Straßburger [9] highlighted some relevant drawbacks in
the ETS standard proposal:
•

It is not possible to differentiate multiple connections
between any two models.

•

ETS suggested interaction hierarchy does not work: a
federate subscribing to the superclass will never
receive the values transmitted in the interaction
parameters.

•

The specification of user defined attributes is placed
into a complex datatype, this introduces new room for
interoperability challenges as all participating
federates have to be able to interpret all of the
attributes.

•

There are some possibilities for misinterpretation in
the definition of “Entity” and “EntityType”
introducing changes in FOMs whenever a new entity
type is talked about.

Furthermore, the ETS was not designed to manage the
Type A.2. IRM and the interaction class hierarchy refers to the
entity transfer without taking into account any information on
the state of the receiving buffer (e.g. Q2a and Q2b in Fig. 3).
The industrial cases defined in Section II can be modeled
only if the first drawback of the list is properly addressed and
if the ETS draft standard is modified to manage Type A.2.
IRM as well. Indeed, the simulation of a manufacturing
system in a distributed way may ask for the representation of
multiple connections between the models, thus requiring
multiple entry points in a receiving model (e.g. Model 2 in
Fig. 3) and/or multiple exit points in a sending model (e.g.
Model 1 in Fig. 4).
IV.

A SOLUTION PROPOSAL FOR THE TYPE A.2 IRM

Figure 5. Interaction Class Hierarchy

The resulting class hierarchy consists of the following
classes:
•

transferEntity, as already defined in the ETS protocol.
This superclass allows the federate subscribing to all
the instances of entity transfer. The instantiation of
this class is related to visualization and monitoring
tasks.

•

transferEntityFromFedSourceEx is a novel subclass
defined for every exit point, where FedSourceEx
stands for the name or abbreviation of a specific exit
point in the sending model. This class is useful to
group the instances of the transferEntity that are
related to the source federate, so that the FedSourceEx
can subscribe to all these instances without explicitly
naming them.

•

transferEntityFromFedSourceExToFedDestEn is a
novel subclass defined for every pair of exit point (Ex)
of the source federate (FedSource) and entry point
(En) of the receiving federate (FedDest). This class is
instantiated both when a sending model needs to
transfer a part to a specific entry point in the receiving
model, and when a receiving model needs to share
information about a buffer state or about the receipt of
a part from a specific exit point in a sending model.
The models both publish and subscribe to this subclass
that was designed to create a private communication
channel between the sending and the receiving
models. Therefore, if an entry point in the receiving
model is connected with multiple federates/exit points,
then the receiving federate has to inform about the
state of the entry point by means of multiple
interactions, each dedicated to a specific federate/exit
point. This communication strategy is not the most
efficient in a generic case, but it offers the possibility
to deliver customized information and adopt different
priorities for the various federates/exit points.

This section presents the solutions proposed to cope with
the problems related to the Type A.2 IRM, as highlighted in
Section III. In particular, Section IV.A proposes a modification
to ETS, whereas Section IV.B describes a new protocol aimed
at managing the communication between a CSP and its
middleware.
A. Proposal to modify Entity Transfer Specification
The ETS standard proposal [11, 12] is modified by
defining a new class hierarchy. In particular, different
subclasses of the transferEntity superclass are defined to face
the drawbacks presented in Section III.B (Fig. 5).

Figure 3. Multiple Part Type Production System (e.g. disassembly or split
system) - Case I

Figure 4. Multiple Part Type Production System (e.g. assembly or merge
systems) - Case II

The ETS Interaction class table was modified to represent
the
transferEntityFromFedSourceEx
and
transferEntityFromFedSourceExToFedDestEn
subclasses,
whereas the Parameter Table was modified to include the
parameters
of
the
novel
interaction
class
transferEntityFromFedSourceExToFedDestEn.
The parameters in the Parameter Table are defined as
follows and the similarities with the parameters included in the
ETS Parameter Table [12] are highlighted where present.
•

Entity. It is a parameter of complex datatype
containing the EntityName that is used to

communicate the type of the entity, and the
EntityNumber that is used to communicate the number
of entities to be transferred. The EntityName and
EntityNumber play the role of the EntityName and
EntitySize defined in ETS [11, 12], respectively.
•

ReceivedEntity. It refers to the entity received by the
receiving federate and has the same type of the
parameter Entity.

•

Buffer_Availability. It was defined to enable the
communication about the buffer state.

•

SourcePriority. This parameter was defined to
communicate the priority assigned to the entity source,
so that the infrastructure can be further extended to
manage Type A.3 IRM.

•

EntityTransferTime. It defines the simulation time
when the entity transfer starts. This time is equal to the
entity arrival time, since it is assumed that the
transferred entity instantaneously arrives at destination
(see the definition of time in Section III.A). The transit
time is not explicitly modeled, but it can be taken into
account if EntityTransferTime represents the entity
arrival time, thus considering the transit time as well.

B. Simulation Messages
Simulation Messages are designed to support the
communication between a CSP and its middleware (see
Section III.A) that is not on the HLA side. The choice of a
communication protocol depends on the role played by the
federate. In the case of sending federate the protocol manages
the communication concerning the need of sending an entity to
another model (outgoing communication) and/or the
possibility of sending the entity outside the model (incoming
communication). In the case of receiving federate the protocol
manages the communication concerning the buffer state and/or
the acceptance of an entity (outgoing communication) and/or
the receipt of an entity from other models (incoming
communication). Simulation Messages are implemented as a
class which is characterized by the following attributes:
•

time that refers to the simulation time when the
message is sent to the middleware from the CSP. This
attribute is used by the middleware to determine the
TimeStamp of the interaction that will be sent to the
RTI.

•

BoundedBuffer that contains the information about the
state of the bounded buffer in the receiving model.

•

TransferEntityEvent that represents the entity transfer
event scheduled in the sending model event list and
contains the information about the entity to be
transferred and the scheduled time for the event.

•

ExternalArrivalEvent that represents the external
arrival event that is scheduled in the receiving model.
It contains the information about the entity to be
received and the scheduled time for the event.

•

ReceivedEntity that represents the information about
the entity that was eventually accepted by the
receiving model.

The CSP of the sending federate sends a message to its
middleware whenever a TransferEntityEvent is scheduled (i.e.
the departure event of an entity from the last workstation of
the sending model is added to the simulation event list). Then,
the
middleware
uses
the
attributes
time
and
TransferEntityEvent to inform the RTI about the need of
passing an entity, while the simulation keeps on running.
When the simulation time arrives at the TransferEntityEvent
time (that corresponds to the EntityTransferTime presented in
Section IV.A) and after all the local events scheduled for that
time instant have been simulated, the request for the advance
to EntityTransferTime is sent by the middleware to the RTI.
After the time has advanced, the middleware informs the CSP
of the sending model about the state of the receiving buffer in
the receiving model. If the receiving buffer is not full, then the
workstation can simulate the TransferEntityEvent, otherwise it
becomes blocked. From the blocking instant until when the
middleware informs that the receiving buffer is not full, the
model keeps on sending requests for time advance at the
lookahead value. The CSP of the receiving federate sends a
message to its middleware whenever a change in the buffer
state occurs and this message contains the updated value of the
attribute boundedBuffer representing the state of the buffer.
Then, the middleware communicates to the RTI the state of
the buffer via interactions. If the change in the buffer state is
due to the arrival of an entity from another model, then the
update of the information does not imply zero lookahead and
the communication is characterized by defining the entity that
has been accepted (i.e. the ReceivedEntity attribute). If the
buffer state change is not related to an external arrival, then
the update of the buffer information implies a zero lookahead
[12] whenever it is not possible to determine an advisable apriori lookahead for the federation. After being informed by
the middleware that another federate needs to transfer an
entity, the receiving model actually simulates the arrival of the
entity only if the buffer is not full, otherwise the arrival is not
simulated and the workstation in the sending model becomes
blocked.
The application of the Simulation Messages can be better
appreciated by describing an example that is characterized as
follows: (1) the reference production system is represented as
in Fig. 1, (2) the buffer Q2a at time t accommodates a number
of parts that is greater than zero and lower than the buffer
capacity and an entity enters workstation W1a, (3) a departure
event from workstation W1a is scheduled for time t' = t + p,
where p represents the processing time of the leaving entity at
station W1a, (4) during the time interval (t, t'), no event
happening in the federate M2 (local event) influences the state
of the buffer Q2a.
Since W1a is the last machine in model M1, the departure
event is also a TransferEntityEvent. Therefore, the CSP sends
a message to its middleware containing time (t) and
TransferEntityEvent attributes. After receiving the message,
the middleware of the sending model informs the RTI via
interaction. When the RTI time advances to time t, the
middleware of the receiving model receives the information

about the need of the sending model to transfer an entity at
time t'. Then, the middleware sends to the receiving model a
simulation message containing the ExternalArrivalEvent. The
receiving model simulates the external arrival as soon as the
simulation time advances to t' and all local events for that time
have been simulated (since the buffer Q2a is not full according
to the example settings). A message is sent to the middleware
of the receiving model containing the updated values of the
Q2a state (attribute BoundedBuffer) together with the
information concerning the recently accommodated part
(attribute ReceivedEntity). Afterwards, the middleware sends
two interactions to the RTI: one is with a TimeStamp equal to
t' and contains the updated state of the buffer Q2a and the
receipt of the entity, the other contains the request of time
advance to time t'. When the RTI advances to time t', the
middleware of the sending model receives the information
regarding the state of Q2a and the received entity by means of
the RTI. Since the entity has been delivered to the receiving
model, the station W1a is not blocked by the middleware.
V.

HLA-BASED INFRASTRUCTURE IMPLEMENTATION

The HLA-based architecture shown in Fig. 2 was
implemented as follows:
•

MAK-RTI 3.3.2 [6] was used as the RTI component
implementation.

•

The middleware was developed in C++ language
following the specifications defined in Section IV and
was named SimulationMiddleware.

•

The simulation models were developed using a CSP
emulator, thus following the approach suggested in
[17].

The FederateAmbassador and RTIAmbassador were
provided by MAK-RTI as C++ classes and were linked to the
SimulationMiddleware. Further extensions were needed to
implement the proposed modification to ETS (see Section
IV.A) and the Simulation Messages (see Section IV.B). The
former required a modification to FederateAmbassador class,
whereas the latter led to the development of a new C++ class.
The SimulationMiddleware was implemented to manage the
information contained in Simulation Messages.
VI.

MANUFACTURING PRODUCTION SYSTEM: A TEST CASE

The experiments presented in this section were designed to
test the proposed entity passing solution (Section IV.A) .The
test case refers to a factory that produces two part types
(Part_A and Part_B) and consists of two separated
manufacturing systems (Plant1 and Plant2). Plant1 executes a
set of manufacturing operations on both part types, whereas
Plant2 is divided into two production lines and each line is
dedicated to process a single part type. If the performance of
Plant1 and Plant2 is simulated via two separated simulators,
then the whole production system can be simulated in a
distributed way by representing Plant1 and Plant2 as federates
within a federation (see Fig. 6).
This test case is characterized by Type A.2. IRM, because
the workstation W12 has to transfer parts of type A (type B) to

the bounded buffer Q21 (Q23) in the Part_A Line (Part_B Line)
of Plant2. For validation purposes the results from the
distributed simulation were compared to a monolithic
reference implementation. Two simulators representing Plant1
and Plant2 were developed using the CSP emulator mentioned
in Section V. These simulators were integrated by means of
the HLA-based infrastructure (DS approach). Furthermore, a
monolithic simulator was built using the CSP emulator to
represent the overall factory (SA approach). Finally a
monolithic simulator was built using Rockwell Arena® 12 to
validate the CSP emulator (results of this validation are not
reported since they are out of scope). It is assumed that all
simulation models do not contain any stochastic element. The
experiments were designed as follows:
•

Parts of type A arrive at Plant1 every two time units,
whereas parts of type B every time unit.

•

The capacity of Q11 was set to a value large enough to
guarantee that it never becomes completely full.

•

Three conditions of maximum capacity (i.e. 1, 5, 50)
were considered for the other buffers.

•

Three conditions of processing times were considered
for the workstations. Equal to 1 for all the
workstations in the first condition, then equal to 7 for
workstation W11 and to 1 for all the other workstations
in the second condition, and finally equal to 7 for
workstations W21 and W23 and to 1 for all the other
workstations in the third condition.

Nine experimental conditions were obtained by combining
the buffer capacity and processing time conditions. For each
experimental condition the performance of the manufacturing
system was simulated according to both the SA and the DS
approach. A simulation length of 10000 time units was set for
all the experiments. A conservative synchronization approach
was adopted, given the deterministic nature of the simulation,
it was possible to evaluate the smallest interval elapsing
between two consecutive events and the lookahead was set to
that value (i.e. 1 time unit) for all the experiments. The
experiments were run on a single machine Intel Core2 Duo
Processor T7250, 2.00 GHz and 2046 MB-RAM.
Both simulation approaches (DS and SA) returned identical
simulation output values for each experiment, thus validating
the interoperability offered by the proposed solution for Type
A.2 IRM. However, the DS approach required a significantly
higher (four times on average) computational time compared
to the DS approach because of the overhead related to the
services offered by RTI. The aforementioned result led to
examine the number of interactions sent during the simulation
experiments, thus delving into the overhead related to the RTI
services.

Figure 6.

Factory Test Case

Table I reports the number of interactions sent during the
distributed simulation of the experiments characterized by the
first condition of buffer capacity and the third condition of
processing times. This experimental condition is the most
critical in terms of the number of interactions needed to
synchronize the two federates because the two slowest
workstations are placed in Plant 2 and therefore buffers Q21
and Q23 are frequently full while workstation W12 is frequently
blocked. As a consequence, many interactions are needed to
communicate when the parts can be actually transferred
according to the state of buffers Q21 and Q23.
The receiving federate sends more entity passing
interactions than the sending federate because the receiving
federate sends an interaction every time an entity is received
and/or its state is changed, whereas the sending federate sends
an interaction only when a departure event is scheduled. Each
part arriving at the buffers, except the first that is directly
assigned to the machine, causes two interactions: one to
communicate the receipt of the entity and the updated buffer
state, and another one to communicate the updated buffer state
when the entity leaves the buffer to enter the next workstation.
A large number of time interactions is necessary because a
request for time advance is generated at every time unit since
the workstation W12 is almost always blocked and the time
advances at the lookahead. The number of time advance
interactions does not decrease if the slowest machine is placed
in Plant1, since the receiving federate is still not aware of
when the entity will arrive. This behavior highlights how the
solution proposed in Section IV needs to be further improved.
VII. INDUSTRIAL CASE
This section aims at verifying if the proposed integration
infrastructure presented in Section III can help to better
evaluate the performance of complex manufacturing systems
as described in Section II. The goal does not consist in
comparing the HLA-based distributed simulation with a
monolithic simulation in terms of accuracy and computational
efficiency as already done in Section VI. Herein, the attention
is focused on the industrial field represented by sheet metal
production, thanks to the collaboration offered by the
company Tenova Pomini. In this industrial field, the
production systems are characterized by the presence of at
least two subsystems interacting with each other (Fig. 7). The
Roll Milling system produces sheet metal using milling rolls
that are subject to wearing out process; once the rolls wear out
(i.e. at the end of the roll life) there is the need to change them
to avoid defects in the laminates. The Roll Shop performs the
grinding process to recondition the worn out rolls. Tenova
Pomini is a designer and provider of RollShop systems.
TABLE I.
Type of Interaction
Time Advance
Entity Passing (part type A)
Entity Passing (part type B)

ANALYSIS OF INTERACTIONS
Total Number of interactions
Sending Federate

Receiving Federate

9999
715
1429

10000
1429
2857

Figure 7. Industrial Case representation

If the attention is focused on the rolls, then the resulting
production system can be considered as a closed loop: the Roll
Milling system sends batches of worn out rolls to the Roll
Shop following a given policy and receives reconditioned rolls
back.
The two subsystems forming a closed loop are strongly
related and their reciprocal influences should be considered to
properly evaluate the performance of the whole factory.
However, the lack of shared information between the owner of
the Roll Milling system and the Roll Shop designer makes the
realization of a monolithic simulation model hard to obtain or
even infeasible. In particular, the Roll Milling system works
according to specific roll changing policies that are not shared
with the Roll Shop designer even if these policies play a key
role in the dynamics of the whole factory. Indeed, when a roll
is worn out, the remaining life of the other rolls is checked and
if the remaining life of a roll is under a predefined threshold,
then it is sent to the grinding system together with the
completely worn out rolls. Therefore, these policies determine
a relation between different roll types, since a roll can be sent
to the grinding system depending on the behavior of other roll
types.
When Tenova Pomini designs a Roll Shop, the owner of
the Roll Milling system provides aggregated information
about the yearly average demand of worn out rolls to be
reconditioned. Then the Roll Shop designer develops a
simulator for the roll grinding process with high level of
detail.
The hypothesis is made that the Roll Shop designer has
developed and validated a simulator using the CSP emulator
(see Section V). Similarly, the Roll Milling system owner has
developed and validated a simulator using the CSP emulator,
modeling the milling process and the roll changing policy with
high level of detail; however, the model of the Roll Milling
system simulator is not shared with the Roll Shop designer.
The Service Level (SL) is the typical key performance
indicator (KPI) for evaluating the Roll Milling system and is
defined as the ratio of the time during which the milling
system is producing laminates over the total time when the
milling system is available (i.e. there is no failure).
The Service Level would be reduced if the Roll Milling
system had to wait for reconditioned rolls coming from the
Roll Shop.
The Roll Shop designer has to evaluate the system
performance in terms of SL while taking into account the
influence of the Roll Milling system related to:

•

•

The arrival rate of worn out rolls from the Roll
Milling system that is estimated from the yearly
aggregate demand of reconditioned rolls.
The acceptance of the reconditioned rolls sent by the
Roll Shop (closed loop model).

The influence of the Roll Milling system can be modeled
by adding a virtual workstation inside the detailed simulation
model of the Roll Shop. This virtual workstation roughly
models the Roll Milling system by generating the arrival of
worn out rolls and accepting the reconditioned ones.
Therefore, the Service Level can be calculated with reference
to the performance of the virtual workstation. The realization
of a simulation model like this will be referred to as Approach
A. The main drawbacks of Approach A consist in:
•

The real behavior of the Roll Milling system cannot be
precisely modeled since it is reduced to a black box
sending and receiving rolls (e.g. the roll changing
policies are not modeled).

•

The performance (e.g. mean starvation time for every
station, mean level of roll buffers, etc) of the Roll
Milling system cannot be evaluated.

These drawbacks lead to a potentially inaccurate
evaluation of the factory performance. The Roll Shop designer
could increase the level of detail of the virtual workstation to
improve the completeness of its simulation model and the
accuracy of the estimated Service Level. However, the lack of
shared information hinders the feasibility of this enhancement.
It must be stressed that simulator of Approach A cannot be
considered as a proper monolithic simulator of the whole
factory, since the Roll Milling system is only poorly modeled.
An alternative approach (Approach B) to evaluate the
performance of the whole factory can be developed by
adopting the proposed HLA–based Infrastructure to integrate
the simulators of the Roll Milling system and of the Roll
Shop. In this case, the virtual workstation is removed from the
simulation model of the Roll Shop, since the behavior of the
Roll Milling system is already modeled by its simulator.
Approach B enables to evaluate the impact of the number of
rolls on the system performance, so that the Roll Milling
System owner can optimize the investment cost associated
with the expensive rolls, whereas the Roll Shop designer can
design a more effective and efficient system thus better
meeting the needs of the customers.
The two approaches have been compared by designing a
set of experiments that are characterized as follows:
•

Three experimental conditions are designed with
reference to the total number of rolls circulating in the
whole system. These three conditions are defined as
Low, Medium, High level.

•

The simulation run length was set to six months.

•

The roll changing policy adopted for the Approach B
simulator has been kept fixed throughout the
experimentation.

The results of the experiments are reported in Table II.

TABLE II.
Experimental
Conditions
High Level
Medium Level
Low Level

SERVICE LEVEL RESULTS

Approach A

Approach B

0.995
0.946
0.308

0.872
0.682
0.273

Percentage
difference
12.3
27.7
3.5

Approach A and Approach B are compared in terms of the
estimated Service Level. The results show that the difference
between the two approaches is larger for the High and Medium
level conditions. If the level of rolls is Low then the roll
changing policy does not affect the overall performance of the
production system because the Roll Milling system is
frequently waiting for reconditioned rolls and therefore the
estimations are similar. In case of Medium and High level
conditions the workload of the rolls in the Roll Shop can be
strongly influenced by the roll changing policy, thus
generating a higher difference in the estimation between the
two approaches.
Based on the analysis carried out so far, it was decided to
design further experiments to analyze the behavior of the
system with different starting workload conditions, i.e. the
number of rolls that are present in the Roll Milling system
when the simulation starts.
These experiments can be useful to analyze the ramp-up
period and select the roll changing policy that avoids the
arising of critical workload conditions.
These additional experiments can be carried out only
adopting Approach B, since the starting workload conditions
cannot be modeled with Approach A. Indeed, the virtual
workstation generates rolls for the Roll Shop independently
from the starting workload conditions. Therefore, the virtual
workstation would generate roll arrivals even if all the rolls
are already located in the Roll Shop, thus incorrectly
increasing the number of rolls in the whole factory. This
represents an additional criticality of the Approach A that can
be solved only using Approach B.
The second set of experiments was designed as follows:
•

Two types of roll circulate in the factory (RollType1
and RollType2). The roll of type RollType2 has a
longer roll life than RollType1.

•

For each type of roll three levels of the Starting
Workload (i.e. number of rolls) in the Roll Milling
system are considered.

•

Three simulation run lengths are considered, i.e. 1
week, 2 weeks and 4 weeks.

•

The roll changing policy is fixed for all experiments.

•

The total number of rolls is equal to the High level of
the previous experimentation and is fixed for all the
experiments.

Fig. 8 shows the main effects plot for the Service Level
evaluated by simulating the 27 resulting experimental
conditions with Approach B.

(FP7/2007-2013) under grant agreement No: NMP2 2010228595, Virtual Factory Framework (VFF). The authors would
like to thank Prof. S.J.E. Taylor for his on-going scientific
support and TENOVA POMINI for the definition of the
industrial case.
REFERENCES
[1]

[2]

Figure 8. Main Effect Plot for Service Level with Approach B

The plot suggests a significant influence of the factor
Starting Workload for RollType1. This roll type assumes a key
role because of its short roll life. If the Starting Workload for
RollType1 is Low, then the Roll Shop can hardly cope with the
frequent roll requests of RollType1 from the Roll Milling
system during the transient period and low values of SL are
observed. This transient phenomenon occurs in all conditions
of the simulation lengths, but it mitigates when the simulation
length increases. Indeed the SL tends to a stationary value that
is independent from the starting conditions. Nonetheless this
analysis can be useful for the Roll Milling system owner that
can individuate critical conditions, thus designing roll
changing policies that avoid the occurrence of these situations
during the ramp-up phase.

[3]

[4]
[5]
[6]
[7]

[8]
[9]

[10]

VIII. CONCLUSIONS AND FUTURE WORKS
The need of simulating complex manufacturing systems
led to investigate the integration of CSPs based on HLA and to
propose a solution to the CSP interoperability problem by
addressing the Type A.2 IRM and modifying the ETS
protocol. In particular, Simulation Messages were designed to
manage the communication between a CSP and its adaptor in
case of Type A.2 IRM. Nonetheless, the implementation can
be extended to the case of simultaneous events (Type A.3.
IRM). The experiments showed the feasibility of the use of the
integrated simulators infrastructure, but further developments
are needed to optimize the time advance management. The
effect of different lookahead values should be evaluated
together with the use of optimistic synchronization
approaches. The analysis of the number of interactions needed
for the entity transfer (see Section VI) highlights that further
research on the implementation of Type A.2 IRM is necessary
as well. For instance, it would be interesting to investigate the
design of protocols that do not force to send interaction at
every time unit to communicate the state of the federates, but
enable the interaction depending on the system state. Finally
Section VII showed how the HLA-based distributed
simulation can be exploited in the industrial domain.
ACKNOWLEDGMENT
The research reported in this paper has received funding
from the European Union Seventh Framework Programme

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

Boer, C.A. 2005. Distributed simulation in industry. PhD thesis.
Erasmus Research Institute of Management (ERIM), Erasmus University
Rotterdam, The Netherlands.
Colledani, M., T. Tolio. 2005. A Decomposition Method to Support the
Configuration/Reconfiguration of Production Systems. CIRP Annals Manufacturing Technology, vol. 54, Issue 1, pp. 441-444.
Hibino, H., Y. Fukuda, Y. Yura, K. Mitsuyuki, K. Kaneda. 2002.
Manufacturing Adapter Of Distributed Simulation Systems Using HLA.
In Proceedings of the 2002 winter simulation conference E. Yücesan,
C.H. Chen, J. L. Snowdon, and J. M. Charnes, eds, vol. 2, pp. 1099 1107.
IEEE 1516. 2000. Standard for Modeling and Simulation (M&S) High
Level Architecture (HLA).
Law, A.M., 2007. Simulation modeling and Analysis, 3rd ed. New York:
McGraw-Hill.
MAK RTI, www.mak.com.
McLean, C., F. Riddick. 2000. Integration of manufacturing simulations
using HLA. In Proceedings of the 2000 Advanced Simulation
Technologies Conference (ASTC 2000).
SISO CSPI-PDG www.sisostds.org.
Straßburger, S. 2006a. The road to COTS-interoperability: from generic
HLA-interfaces towards plug-And-play capabilities. In Proceedings of
the 2006 Winter Simulation Conference, L.F. Perrone, F.P. Wieland, J.
Liu, B.G. Lawson, D.M. Nicol, and R.M. Fujimoto, eds., pp.1111 - 1118.
Straßburger, S. 2006b. Overview about the high level architecture for
modeling and simulation and recent developments. In Simulation News
Europe 16(2), pp. 5-14.
Taylor, S.J.E., S. Turner, and M. Low. 2004. A Proposal for an Entity
Transfer Specification Standard for COTS Simulation Package
Interoperation. In Proceedings of the 2004 European Simulation
Interoperability Workshop. June 28 - July 1, 2004. Edinburgh, Scotland.
Taylor, S.J.E., Wang, X., Turner, S.J., Low, M.Y.H. 2006. Integrating
Heterogeneous Distributed COTS Discrete-Event Simulation Packages:
An Emerging Standards-Based Approach. IEEE Transactions on
Systems, Man & Cybernetics: Part A, 36, 1, pp. 109-122.
Taylor, S.J.E., N. Mustafee, S. Turner, K. Pan, S. Straßburger. 2009.
Commercial-off-the-shelf Simulation Package Interoperability: issues
and futures. In Proceedings of the 2009 Winter Simulation Conference
M. D. Rossetti, R. R. Hill, B. Johansson, A. Dunkin and R. G. Ingalls,
eds, pp. 203 - 215.
Terkaj, W., T. Tolio and A. Valente. 2009. Designing Manufacturing
Flexibility in Dynamic Production Contexts. In Design of Flexible
Production Systems Methodologies and Tools, Springer. DOI
10.1007/978-3-540-85414-2. pp. 1-18.
Vàncza, J., P. Egri, L. Monostory. 2008. A coordination mechanism for
rolling horizon planning in supply networks. CIRP Annals Manufacturing Technology, Volume 57, 455-458.
Sacco, M., Pedrazzoli, P., Terkaj, W. 2010. VFF: Virtual Factory
Framework. In Proceedings of 16th International Conference on
Concurrent Enterprising, Lugano, Switzerland.
Wang, X., S.J. Turner, S.J.E. Taylor. 2006. COTS Simulation Package
(CSP) Interoperability – A Solution to Synchronous Entity Passing. In
Proceedings of the 20th Workshop on Principles of Advanced and
Distributed Simulation (PADS'06), pp. 201 - 210.
Wiendahl, H.P., S. Lutz. 2002. Production in Networks. CIRP Annals Manufacturing
Technology,
vol.
51,
pp.
573-586.

Proceedings of the 2015 Winter Simulation Conference
L. Yilmaz, W. K. V. Chan, I. Moon, T. M. K. Roeder, C. Macal, and M. D. Rossetti, eds.

KRIGING–BASED SIMULATION–OPTIMIZATION:
A STOCHASTIC RECURSION PERSPECTIVE
Giulia Pedrielli

Szu Hui Ng

Centre for Maritime Studies
National University of Singapore
15 Prince George’s Park
Singapore, SG 118414, Singapore

Department of Industrial and Systems Engineering
National University of Singapore
1 Engineering Drive 2
Singapore, SG 117576, Singapore

ABSTRACT
Motivated by our recent extension of the Two–Stage Sequential Algorithm (eTSSO), we propose an
adaptation of the framework in Pasupathy et al. (2015) for the study of convergence of kriging–based
procedures. Specifically, we extend the proof scheme in Pasupathy et al. (2015) to the class of kriging–based
simulation–optimization algorithms. In particular, the asymptotic convergence and the convergence rate
of eTSSO are investigated by interpreting the kriging–based search as a stochastic recursion. We show
the parallelism between the two paradigms and exploit the deterministic counterpart of eTSSO, the more
famous Efficient Global Optimization (EGO) procedure, in order to derive eTSSO structural properties.
This work represents a first step towards a general proof framework for the asymptotic convergence and
convergence rate analysis of meta–model based simulation–optimization.
1

INTRODUCTION

In the last decade, simulation has been adopted as a means to iteratively evaluate the performance of the
solutions generated by search algorithms (Fu et al. 2008). In the literature, this coupling is referred to
as simulation–optimization. This approach has been proven particularly effective when complex functions
are of concern, and the search algorithm has to rely upon the performance estimation produced by the
simulation (typically treated as a black box oracle) at selected points (candidate solutions) since a closed
form of the function is not available (Fu et al. 2008).
In this work, we specifically refer to the family of simulation–optimization problems trying to find
the point x∗ satisfying x∗ ∈ arg minx∈X f (x), where X represents the design space (i.e., the feasible region
of the problem), and f (x) is the function whose measurements (with or without noise) can be obtained
only through simulation. More specifically, the scope of this manuscript is the one of meta–model based
simulation–optimization and, in particular, we will focus on kriging–based simulation–optimization.
The basic idea behind this family of algorithms is to exploit the information coming from the simulation
by iteratively improving the estimate of a model of the response surface to optimize. Since this response
surface is constructed upon a simulation model, it is referred to as meta–model. These procedures typically
use the meta–model to compute an indicator (e.g., the Expected Improvement, Knowledge Gradient) which
guides the search, iteratively suggesting the next point(s) to sample with the objective to identify the global
optimum.
The Efficient Global Optimization Algorithm (EGO) represents one of the most famous of this family of
procedures when the function f (x) is deterministic. It uses the Expected Improvement, to choose the next
point(s) and a ordinary kriging model to fit the response. Stochastic Kriging Optimization (SKO) extends
EGO to the stochastic case by adapting both the meta–model form as well as the criterion to guide the
search.

978-1-4673-9743-8/15/$31.00 ©2015 IEEE

3834

Pedrielli and Ng
Despite the important interest that kriging has raised both in the simulation and optimization community,
the analysis of the convergence properties of kriging based algorithms represents an active field of research.
Results on the asymptotic behavior of the EGO algorithm are provided in Locatelli (1997), and, more
recently, Bull (2011) provided the analysis of the convergence rates of meta–model based search in a more
general framework including the EGO.
The authors have previously worked in the stochastic settings by proposing the Two Stage Sequential
Search Optimization algorithm (TSSO) (Quan et al. 2013) and, lately, they have been further improving
the approach resulting in the extended–TSSO (eTSSO) (Pedrielli et al. 2015). This paper provides a
theoretical understanding of the eTSSO algorithm, by taking a different perspective on kriging–based
simulation–optimization. Specifically, we look at eTSSO as a stochastic recursion algorithm having EGO
as deterministic counterpart. In particular, the parallelism between kriging–based search and stochastic
recursion as well as the relationship between eTSSO and EGO are investigated to adapt the framework in
Pasupathy et al. (2015) for the convergence analysis of kriging–based optimization algorithms.
The remainder of the paper is structured as follows: section 2 gives the main notation and terminology
to support the preliminaries provided in section 3 where both the deterministic as well as the stochastic
kriging–based approaches are presented. Section 4 presents the proposed adaptation of the framework in
Pasupathy et al. (2015) to the case of stochastic–kriging based simulation–optimization. Section 5 offers
some numerical evidence of the asymptotic convergence and convergence rate of the studied stochastic
algorithm, while section 6 concludes the paper.
2

NOTATION AND TERMINOLOGY

In this section, we present some of the main definitions that will be used throughout the work.
We will refer to x as a vector of real numbers in a generic d–dimensional space X ⊆ ℜd . ei is defined in
ℜd and it denotes a unit vector whose i–th component is 1 and any other component is 0.
wp1

For a sequence of random variables, we say {Xn } −−→ x to mean that the stochastic sequence {Xn } converges
to x with probability 1.
For a sequence of real numbers {an }, we say that an = o (1) if limn→∞ an = 0, and an = O (1) if ∃c ∈ (0, ∞)
with |an | < c for large enough n. Finally, we say that an = Θ (1) if 0 < lim inf an ≤ lim sup an < ∞.
We also adopt the following definitions to characterize the convergence behaviour of the analysed algorithms.
 	
−x∗ ||
=
Definition 1 (Linear convergence) xkθ exhibits a linear(ℓ) convergence to x∗ if lim supk→∞ ||x||xk+1
∗
k −x ||
ℓ ∈ (0, 1)
The following definition characterizes the control of the sample size sequence we created for the
stochastic algorithm eTSSO.
Definition 2 (Geometric growth of a sequence) A sequence {mk } exhibits geometric growth if mk+1 =
c · mk , k = 1, 2, . . . for some c ∈ (1, ∞).
3

PRELIMINARIES

We consider a single objective minimization problem defined over a compact set X. The deterministic d–
dimensional objective function f : x ∈ X ⊂ ℜd → f (x) ∈ ℜ is here observed running simulation experiment(s)
at point x. Our goal is to find a global minimum of f : X → ℜ, where X ⊆ ℜd solving:
P : min f (x)
s.to
x∈X
In order to find a solution to P, we use a search procedure based on a meta–model of the response, whose
parameters are updated, as the search progresses, based on the simulation results. The model guides the
search predicting the function values at points where no simulation has been conducted yet.

3835

Pedrielli and Ng
3.1 Deterministic Problem
We start with the case in which the simulation response is obtained without noise, i.e., in the case of
deterministic optimization. In particular, suppose we wish to minimize an unknown function f , choosing
design points xk and the estimated minima xk∗ . We will refer to π as the statistical model estimating the
behavior of the unknown function f . This model can be interpreted as our belief about the unknown
function (Bull 2011). In this paper, we refer to the Efficient Global Optimization (EGO) algorithm, which,
iteratively maximizing the expected
improvement, generates a sequence of random design points {xk } and
	
estimated best solutions xk∗ taking value over the compact space X. At iteration k, the filtration Fk ,
defined as the sigma algebra σ (xi , f (xi ) : i ≤ k), represents the set of available information. According to
the EGO, given Fk , we will choose as estimated best solutions at iteration k, xk∗ , the point, among those
already sampled in the set S ⊆ X, having the best function value up to that iteration. The next point to
sample is selected in order to maximize the following Expected Improvement function (Jones et al. 1998):


(1)
EIπk (x, Fk ) := Eπ f (xk∗ ) − fˆ (x) |Fk

where, as already stated, xk∗ represents the sampled point with the minimum associated function value f xk∗
up to iteration k, and fˆ (x) is the predicted function value at the non sampled point x. Typically, the function
f is modeled as a stationary Gaussian process and we consider the values f (x) of the non–sampled points
x to be jointly Gaussian with mean and covariance parametrized through a constant τ and a d–dimensional
vector θ , namely:
Eπ [ f (x)] = µ, Covπ [ f (x) , f (y)] = τ 2 Kθ (x − y)

(2)

Having chosen a certain statistical model for f , each point x ∈ X \ S is associated with the following
predictor:

f (x) |( f (xi )i≤k ) ∼ N fˆ (x; θ ) , s2k (x; θ )
where:

1T V −1 f
1T V −1 1
fˆk (x, θ ) := µ̂k (θ ) + vT V −1 (f − µ̂k 1)
and
2 !
T
−1
1−1 V v
s2k (x, θ ) := τ 2 1 − vT V −1 v +
1T V −1 1
µ̂k (θ ) :=

(3)
(4)

(5)

where, 1 is a vector having all elements equal to 1, V = (Kθ (xi − x j ))ki=1 is the spatial variance–covariance
matrix and v = (Kθ (xk+1 − xi ))ki=1 represents the correlation vector. Following Yin et al. (2011), we use a
Gaussian kernel in equation (2), :

d
2 
Kθ (xi − x j ) := ∏ exp −θl xil − x jl

(6)

l=1

Under assumptions 1–4 page 2883 in Bull (2011), which are satisfied in the present context, the author
proves convergence rates for EGO. In particular, the author uses the Reproducing Kernel Hilbert Space
H (X) of functions over the space X constructed from the kernel
K
the convergence rates

 and establishes

of the loss function Lk (EIπk , Hθ (X) , R) := sup|| f ||H (X) ≤R Eπ f xk∗ − min fˆ over the ball of radius R, BR ,
θ
in H (X) after k steps as (Theorem 2, page 2887, (Bull 2011)):




sup Eπ f (xk∗ ) − fˆ (x) |Fk = O k−1/d .
Lk (EIπk , Hθ (X) , R) :=
(7)
|| f ||Hθ (X) ≤R

3836

Pedrielli and Ng
Under this result, considering the definitions
in section 2, EGO exhibits linear convergence rates. In

 
||xk+1 −x∗ ||
1 1/d
particular the rate is lim ||xk −x∗ || = O 1 − k
.
k→∞

3.2 Stochastic Problem

In the stochastic context, a simulation is only able to return a point estimate of f (xi ) for each replication run
at the point xi ∈ X ⊆ ℜd and not its true value as in the deterministic case presented in section 3.1. In this
setting, the authors refer to their recently proposed eTSSO (Pedrielli et al. 2015), a two–stage algorithm
which uses the first stage to select a new point and subsequently assesses the number of replications required
to re–evaluate the model parameters. The Modified Nugget Effect Kriging (MNEK) model (Yin et al.
2011) is used to estimate the function values at the non–sampled locations x ∈ X ∈
/ S. Subsequently, the
point xk is added to the set S if it maximizes the modified expected improvement function EIπ̃k (xi , Fk ), the
stochastic version of (1).
Again, we assume that f (xi ) are realizations of a random process and a statistical model π̃k represents the
stochastic counterpart of π in section 3.1, namely:
f (x) = Z (x) + ξ (x) , x ∈ X

(8)

where Z describes the mean and ξ describes the random noise process. As in the deterministic case, we
2
further model Z (x) as a Gaussian process with covariance function τ 2V , where
 τ is the process variance
2
and V the matrix of process correlation; formally, Z (x) is a GP µ (x) , τ V .
As already mentioned in section 3.1, a commonly adopted correlation function V = (Kθ (xi − x j ))ki=1 is the d–
dimensional separable version of the power exponential family of functional forms which is characterized by
smooth response (equation (6)). The noise ξ (x) is assumed to be distributed with zero mean and covariance
function σξ2Vξ , where Vξ denotes the matrix of sampling correlations. Error variances are generally not
constant and they may depend on x (i.e., the heteroscedastic case is considered). With independent sampling
(i.e., no CRN), Vξ is diagonal, and equation (8) reduces to the independent sampling noise model (Yin
et al. 2011). The general form of equation (8) is similar to the form proposed in Ankenman et al. (2010).
As shown in Yin et al. (2011), the predictor for (8) at the point x, given k points have been already
sampled, is:

h
iT 

T V +V −1 v
k
1
−
1
ξ
−1
−1


ei + 1T Vξ +V
(9)
ei  f¯i
fˆ (Wk , x, θ ) = ∑ vT Vξ +V
−1
T
1 Vξ +V
1
i=1
where, Wk represents the total number of simulation replications performed up to iteration k. f̄ is the
k–dimensional vector of the averaged
values at the already sampled points. v is the correlation
 function
2
2
T
−θ ·dx,x
1
vector, specifically, v (Wk , x, θ ) = e
· · · e−θ ·dx,xk , where, dx,xi represents the euclidean distance
between point x at which the prediction is performed and the already sampled locations xi , i = 1, . . . , k.
The vector ei has size k (being k the number of sampled points) and its elements are all equal to 0 except
the i–th element which is equal to 1. The optimal MSE results (Yin et al. 2011):




 T

T V ′ −1 v
T V ′ −1 v
1
−
1
1
−
1


 V ′ −1 v +
(10)
s2k (Wk , x, θ ) := c0 + τ 2 1 − v + 1

′ −1
′
T
1 V 1
1T V −1 1
′

where V = V + Vξ , and c0 is the nugget effect value which usually can be estimated from the sample
variance as ĉ0 = σ̂ 2 /Wi,k , where Wi,k represents the number of simulation replications allocated to point
3837

Pedrielli and Ng
Algorithm 1: eTSSO Algorithm
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

Initialization: Choose T , rmin , S : |S| = N0 , mi0 = rmin and W0 = N0 · m0 ;
for i = 1 . . . , N0 do
Run mi0 replications and return f¯0 (xi ) , σ̂ξ2,0 (xi );
end
Fit the MNEK model to the set of sample means and apply cross–validation to verify it.
Update T = T −W0 , k = 1.
while Wk−1 ≤ T do
Select xk ∈ arg maxx∈X∈S
S ← S ∪ xk ;
/ EIπ̃k (x),


Run rmin simulations to obtain f¯k (xk ) , σ̂ 2 (xk ) ;
ξ ,k

Use OCBA to determine σ̂ξ2,k and update the budget mk according to (12);
Wk = Wk−1 + mk ;
if (mk − rmin ) > 0 then
Apply equations (13)–(14) to allocate mik to the sampled points i ∈ S;
Run the simulation experiments according to the budget;
Fit the kriging model according to the updated information;
k = k + 1;
end
end

i up to iteration k. We will use mk = Wk −Wk−1 and mik = Wik −Wi,k−1 , i ∈ S, to refer to the number of
simulation replications allocated at iteration k and the number of simulation replications allocated to point
i at iteration k, respectively. In the rest of the paper, we will avoid the notation (Wk , ·, ·) when it is clear
we refer to the stochastic model and s2k will be referred to as extrinsic variance.
Algorithm 1 summarizes the steps of the eTSSO procedure. In its first stage, eTSSO, computes the
function EIπ̃k (xi , Fk ) over the set of non–sampled points X \ S (Pedrielli et al. 2015):
	


(11)
xk ∈ arg max EIπ̃k (xi , Fk ) := Eπ̃k max f¯ (xk∗ ) − fˆ (x) , 0
x∈X ∈S
/


∗

here, f¯ xk represents the response at the sampled points with the lowest average function value f¯ up to
iteration k, and fˆ (x) is a random variable with the mean given by kriging mean function and the variance
given by the spatial prediction uncertainty s2k (x, θ ). The point xk is sampled with rmin replications (with
rmin being an input parameter), and added to the set of already sampled points S.
At the second stage, eTSSO uses the Optimal Computing Budget Allocation (OCBA) technique (Chen
et al. 2000) to assign the available simulation replications. Specifically, the sequence of simulation
replications at each iteration is dynamically updated according to the following stochastic rule:
!
σ̂ξ2,k
⌋
(12)
mk = ⌊mk−1 1 + 2
σ̂ξ ,k + s2k (xk )
where, σ̂ξ2,k refers to the estimated variance of the sampled point receiving more budget according to
OCBA, whereas s2k (xk ), formulated in (10), refers the location maximizing the function EIπ̃k (xi , Fk ). The
k

total budget used up to iteration k is Wk = ∑ mk . The budget at the first iteration is set to the minimum
k=0

number of replications to sample a new point, i.e., m0 = rmin . Since N0 points are chosen for the initial
MNEK model fit, W0 = N0 · m0 and the remaining budget is T ← T −W0 . At the generic k–th iteration the
3838

Pedrielli and Ng
budget mk is allocated to the points in S using OCBA. In particular, equation (13) determines the relative
allocation between non–best designs and equation (14) is used to derive the relative allocation between the
best design and non best designs. If we refer to mik as the budget allocated to point i at iteration k, we
have (Chen et al. 2000):


σ̂ξ ,k (xi ) /δb,i 2
,
(13)
mik /m jk =
σ̂ξ ,k (x j ) /δb, j
v
u
u
m2ik
mbk = σ̂ξ ,k (xb ) t ∑
,
(14)
2
x∈S:x6=xb σ̂ξ ,k (xi )
At iteration k, xb is the design point with the lowest sample mean, mbk is the related number of replications
and σ̂ξ ,k (xb ) is the related sample standard deviation; mik is the number of replications performed at location
i and σ̂ξ ,k (xi ) is the estimated standard deviation at that point. δb,i is the difference between sample mean
at point i and the lowest sample mean.
4

MAIN RESULTS

We can interpret the EGO in Jones et al. (1998) adopting the perspective of stochastic recursion algorithms.
In particular, at the k–th iteration, manipulating the definition in (1), we have:


(15)
xk+1 = xk∗ + dist xk∗ , arg max EIπk (x)
x∈X\S

where the function dist (·, ·) is the vector of the distances between the components of xk∗ and the candidate
point xk+1 .
According to algorithm 1, we can interpret eTSSO as the stochastic counterpart of (15) and formulate
the related iteration as:


∗
∗
(16)
Xk+1 = Xk + dist Xk , arg max EIπ̃k (x)
x∈X\S

where, π̃k refers to the model in (8) which replaces π in (2).
The main difference between (15) and the typical recursion in a deterministic search algorithm, resides
in the filtration Fk . According to the traditional update:
xk+1 = xk + h (xk )

(17)

whereas, in (15), the entire sequence of the visited points is considered at each step k = 1, . . .:




(18)
xk+1 = g {xi }ki=1 + h {xi }ki=1





here, g {xi }ki=1 := xk∗ ∈ arg minx∈S f (x) and h {xi }ki=1 := dist xk∗ , arg maxx∈X\S EIπk .
In eTSSO, we estimate the components in the iteration (18) as it follows:




(19)
Xk+1 = G Wk , {Xi }ki=1 + H Wk , {Xi }ki=1




where, G Wk , {Xi }ki=1 := Xk∗ ∈ arg minx∈S f¯ (x), whereas H Wk , {Xi }ki=1 :=

:= dist Xk∗ , arg maxx∈X\S EIπ̃k . Wk represents the total simulation budget used up to iteration k.
In the following, we will exploit the proof framework proposed in Pasupathy et al. (2015) for stochastic
recursion, in order to study the eTSSO algorithm having the EGO as Deterministic Analogue (Jones et al.
1998). The following assumptions are required in the scope of showing our results.
3839

Pedrielli and Ng
Assumption 1 X is a compact space.
Assumption 2 Each dimension in the space is defined between [0, 1].
Assumption 2 simply requires normalization of the function domain X.
Assumption 3 The initial sample size is such to produce an initial fit of the MNEK model satisfying
cross-validation criteria.
Assumption 4 The Gaussian correlation function is adopted to model the spatial variance covariance matrix
V.
Assumption 4 is a sufficient condition for the existence of the derivative processes and it ensures that
the various variance–covariance matrices are positive definite, i.e., non singular. These will be used in
Lemma 1, which characterizes the expected improvement function in (1).
Assumption 5 The parameters τ and θ of the MNEK model are assumed known.
Assumption 6 The number of replications mk assigned at each iteration satisfies mk ≥ mk−1 , ∀k = 1, 2, . . .
and mk → ∞ as k → ∞. Moreover, for any ε > 0 there exists a δε ∈ (0, 1) and a k̄ε > 0 such that
ψ 2k L (mk−1 , ε) ≤ (δε )k , ∀k ≥ k̄ε , where L (·, ·) is strictly decreasing in mk−1 and non–increasing in ε.
Assumption 7 The true function f to be optimized over the compact space X has a unique minimum x∗ .
The following lemma characterizes the function h (x) in the EGO iterates.
Lemma 1 There exists κ ∈ ℜ such that, for any (x, y) ∈ X, ||h(x) − h(y)|| ≤ κ||x − y||.
Proof.
To guarantee the result, we have to prove that, given two sequences {xi }ki=1 and {x̃i }ki=1 corresponding to the filtration Fk and F˜k , respectively, which are close in terms of euclidean distance between the
related points, the resulting processes h (x) | f (xi )i≤k and h (x) | f (x̃i )i≤k are also close. From the definition in
equation (15), these sequences are generated according to the expected improvement. As a result, requiring
close sequences corresponds to guaranteeing the function EIπk is Lipschitz continuous.
We can formulate the expected improvement as (Locatelli 1997):
!
!!


∗ − µ̂ (θ )
f
x
f xk∗ − µ̂k (θ )
k
k
− ( f (xk∗ ) − µ̂k (θ )) 1 − Φ
,
(20)
EIπk (xi ; Fk ) = sk (xi , θ )φ
sk (xi , θ )
sk (xi , θ )
where φ and Φ represent the pdf and cdf of the normal distribution, respectively. Such a function is
Lipschitz continuous in case µ̂k (θ ) and sk (xi , θ ) have the form of equations (3) and (5), respectively.
dEIπ (xi ;F )

k
k
∀x ∈ X. Since we consider
To show Lipschitz continuity we have to guaranteethat
dx ∗ < ∞, 
f (xk )k −µ̂k (θ )
f (xk∗ )−µ̂k (θ )
and Φ
are the pdf and cdf of a
Gaussian processes, the components φ
sk (xi ,θ )
sk (xi ,θ )

normal distribution with finite mean and variance 0 < µ̂k (θ ) , s2k (xi , θ ) < ∞, and finite derivative. More
attention needs to be paid to the derivatives of µ̂k (θ ) and sk (xi , θ ). We rewrite in explicit form equation
(5):




k

sk (xi , θ ) = τ 1 −  ∑

k

d

− ∑ θ j (xi j −xh j )

∑e

j=1

h=1 g=1

d

2

e

− ∑ θ j (xg j −xh j )
j=1

2

1/2

−1 
rhg

(21)

It is apparent that under assumption 4, equation (21) is infinitely differentiable with respect to xi . The
−1
, ∀ (h, g) representing the terms of
finiteness of the derivative, however, depends on the values of rhg
the matrix V (h–th row, g–th column). Hence, the derivative will exist finite as long as the matrix V is
non–singular, i.e., if assumption 4 holds. The same can be proven for the mean component. We then
conclude function EIπk (xi ; Fk ) is Lipschitz continuous.

3840

Pedrielli and Ng
Lemma 2 The EGO algorithm which uses the Gaussian correlation form satisfies limk→∞ xk = x∗ . Moreover,
0
, i.e., it is independent from the specific initial set of points chosen
the convergence is uniform in {xi }Ni=1
to generate the first model estimate.
Proof.

From (21), we observe it is possible to bound the term sk (xi , θ ) from above as follows:
k

sk (xi , θ ) = τ 1 −

k

∑ ∑e

d

d

− ∑ θ j (xil −xhl )2 − ∑ θ j (xgl −xhl )

e

l=1

l=1

h=1 g=1

2

−1
rhg

!!1/2

≤τ

(22)

From Lemma 1, we have EIπk (xi ; Fk ) is Lipschitz continuous. Hence, EIπk (xi ; Fk ) is finite over the design
space and, given two points, |EIπk (xi ; Fk ) − EIπk (x j ; Fk ) | is finite as well. We can then consider the
following:
!

√
√
f xk∗ − µ̂k (θ )
(23)
≤ sk (xi , θ )/ 2π ≤ τ/ 2π,
EIπk (xi ; Fk ) ≤ sk (xi , θ )φ
sk (xi , θ )
where the last two inequalities are derived from the normality property and equation (22), respectively.
According to (22) and (23), we can use the results in Locatelli (1997). Indeed, we note that in (22),
the variance in the new point xi is a weighted sum of the estimates at the already sampled points, and
weights are a function of the euclidean distance between the candidate point and the sampled points. This
is consistent with the form recognized in Locatelli (1997), which we extended to the d–dimensional case
due to assumptions 2–6 and by replacing linear with euclidean distances. As a result of (23) and Lemma
1, we can apply Lemma 1 in Locatelli (1997) (page 60) using the Lipschitz constant c = τ, leading to the
following result ((Locatelli 1997), page 61):
lim max ||xi , x j || = 0.

k→∞ i, j∈S

(24)

Hence, the set of points at which the function is observed if the algorithm is never stopped, is dense in X,
proving convergence of the algorithm.
Concerning the uniform convergence result, there exists a number of initial points N0 such that the
convergence is guaranteed independently from the specific initial set (Bull 2011), and, under Assumption
3, we satisfy this condition.
Now, we are ready to characterize the behaviour of eTSSO as the stochastic counterpart of EGO.
Lemma 3 As the number of iterations k → ∞, under assumptions 6–7, the MNEK model π̃k approaches
its deterministic counterpart π.


Proof.
If assumption 6 holds, let us consider L = Tr σξ2Vξ , where Tr(·) is the trace of a matrix, and


let δε = 1/τ 2 . The function Tr σξ2Vξ satisfies the properties required to L : being an error function, it
is strictly decreasing in Wk . The assumption is stating that there exist a finite number of iterations such
k
that L ≤ δε /ψ 2 . This means that at each iteration, the algorithm produces estimates of L which are
decreasing with δε /ψ 2 . To show this holds, let us rewrite the covariance matrix:

  2
2
2

σξ (x1 )
1
e(−θ ·d12 ) · · · e(−θ ·d1k )
0
·
·
·
0
2
 (−θ ·d21
2
2 
W1,k τ

)
e
1
· · · e(−θ ·d2k )  
′

 ..
.
.
.


.
.
.
V = V +Vξ = 
(25)
. 
.
.
.
..
..
..
..
+




.
.
.
.
σξ2 (xk )
2
0
· · · · · · W τ2
e(−θ ·dk1 )
k,k
···
···
1
3841

Pedrielli and Ng
Here, di j represents the euclidean distance between two points (i, j), Wi,k represents the number of replications
performed at location i up to iteration k according to the eTSSO budget allocation scheme. Having assumed
knowledge on the kriging variance (assumption 5), the diagonal elements of the matrix Vξ are bounded by
1
 and ψ 2 = τ 2 (S refers to the set of sampled points).
decreasing values of δε /ψ 2 , where δε = 
maxx j ∈S W j,k
 −1
Apparently, δε ∈ (0, 1); moreover, maxx j ∈S W j,k τ 2
→ 0 as k → ∞, since maxx j ∈S W j,k → ∞ ∀k. Hence,
wp1

Vξ −−→ 0.



2 (x) .
Lemma 4 As the number of iterations k → ∞, fˆ (x|Fk ) ∼ N µ|Fk (x) , σ|F
k

Proof.
As the number of iterations k → ∞ we can apply Lemma 3 and the result in (Stein 1999) (Appendix
A). fˆ (x|Fk ) has normal distribution with mean and variance:

T !
1 − 1T V −1 v
k→∞
T
−1
T
−1
(26)
fˆ (Wk , x, θ ) −−−→ v V ei + 1 V
ei f¯ (x)
1T V −1 1

"
 #T

T V −1 v
T V −1 v
1
−
1
1
−
1
k→∞

s2k (Wk , x, θ ) −−−→ τ 2 1 − v + 1
V −1 v +
(27)
1T V −1 1
1T V −1 1

corresponding to the deterministic predictor and variance in (Yin et al. 2011) equivalent to (3)–(5).

At this point, we want to characterize the stochastic iteration in (16). In order to do so, we study
G (·, ·) as well as H (·, ·). If we consider G (·, ·), at each iteration, the best point is the one having the best
sampled mean and not the best true value, which cannot be evaluated, and the following property holds.


	

∗ ) − g(x∗ ) > δ
∗ ) For any δ > 0, and with k → ∞, sup
Pr
G(W
,
X
,
X
Property
1
(Convergence
of
G
W
k
k
x∈X
k
k
k

−1/2 
= O Wk Xk∗
.

Proof.
Here, Wk Xk∗ is the total budget allocated to point Xk∗ . Due to the geometric growth of the
replication in equation (12) Wk → ∞ when k → ∞. Due to Lemma 3, we simply exploit the Law of Large
numbers to argue that the sample average will converge to the true function value provided that the sampling
effort goes to infinity. In fact, we know that x∗ will be sampled infinitely often due to the fact that OCBA
rule is used to allocate the budget to each point to sample at each iteration. Due to the results in (Chen
et al. 2000), we know that the best point will be sampled infinitely often (Theorem 1, page 258). As a
result, Property 1 holds.
The next results focus on H(Wk , x) in order to characterize the eTSSO efficiency and consistency.
H (Wk , x) satisfies, for any ∆ > 0, that
Theorem 1 Let k → ∞ (i) As Wk → ∞, the estimator

supx∈X Pr {||H (Wk , x) − h (x) || > ∆} = O Wk−2α . (ii) If the sample sizes {Wi,k } satisfy Wi,k → ∞, then
wp1

||H (Wk , x) − h (x) || −−→ 0.

Proof.
Result (i) is a direct consequence of Lemma 3 and Lemma 4 when α = 21 . The sequence {Wi,k }
satisfies Wi,k → ∞ due to equation (12), and we can now simply recall the results in Lemma 2 to prove part
(ii).
At this point, we can exploit the results in Pasupathy et al. (2015) to study the efficiency of Algorithm
1 in section 3.2. Before presenting the main result, we note the following.
Property 2 (Characterization ofH (Wk , x)) Let k →∞, then the estimator H (Wk , x) satisfies
−1
supx∈X E (H (Wk , x) − h (x)) = Θ τ 2 · min (Wi,k )
.
3842

Pedrielli and Ng
Proof.
As already stated, Wk → ∞ when k → ∞. Under assumption 6 and the result in Lemma 1 EIπ̃k
is lower semi–continuous. Lemma 1 guarantees the function is finite over the space X. We can use the
results in Attouch (1984) to say that EIπ̃k epi–converges to EIπ when Wk → ∞. Concerning the optimal
location, Theorem 1, page 258 in (Chen et al. 2000) guarantees infinite budget will be allocated to the best
point. Then, given epi–convergence, we can simply apply Theorem 3.4 in Robinson (1996), to say that
the sequence EIπ̃k → EIπk and the sequence of selected points Xk ∈ arg max EIπ̃k → xk . This convergence
is determined by the model and the variance covariance matrix in equation (25). Hence, as a consequence
of Lemma 3 and Theorem 1, the property holds.
Theorem 2 (Convergence rate of Algorithm 1) Let us define ck := 1 + σ̂ 2

σ̂ξ2,k

+s2k (xk )
ξ ,k

and ℓ = 1 − 1k

1/d

. Given

that EGO exhibits linear convergence, for any ε > 0 satisfying ℓ + ε < 1 and as k → ∞, the following holds
for Ek = ||Xk − x∗ ||:

−1/2
)
(28)
if ck ∈ 1, ℓ−2 ,
Ek = O(Wk


−k
−1/2
−1/2
if ck ≥ ℓ−2 , Ek = O ck (ℓ + ε)−1
Wk
(29)

Proof.
According to Bull (2011) the error decay rate of EGO, under assumptions 1–4 page 2883, is


1 1/d
ℓ = 1− k
. According to theorem 1, we have that supx∈X Pr {||H (Wk , x) − h (x) || > ∆} = O Wk−2α
with α = 1/2. From (12), we observe that the coefficient ck for the geometric increase of the budget at
k=∞

each algorithm iteration satisfies ck ≤ 2, also ck −−→ 1. Since the budget increase is stochastic, we need to
consider both the case ck ∈ 1, ℓ−2 and ck ≥ ℓ−2 in (Pasupathy et al. 2015) and, considering the definitions
of ℓ and ck , we use Theorem 6.5 page 17 in Pasupathy et al. (2015) to prove the result.

It has to be noted that, in assumption 5, we stated that the parameters θ and τ 2 of the MNEK model are
known in advance. However, convergence is affected by the quality of the estimators (Kleijnen et al. 2012).
If MLEs are adopted, in deterministic settings, Bull (2011) proves convergence is guaranteed assuming to
be able to produce a bounded estimate of θ̂ . Intuitively, when the bias is consistent, the optimal location
should still be identified.
5

EMPIRICAL RESULTS

We empirically observed the convergence of eTSSO in the location and the function estimation, by evaluating
the number of iterations to let the euclidean distances |x − x∗ | and |y − y∗ | converge to 0, respectively (where
x∗ is the reachable optimum in a finite grid (Kleijnen et al. 2012)). This analysis provides empirical
evidence of eTSSO convergence even when the model parameters are sequentially estimated. We present
a 1–d, a 2–d tetra–modal, and the 3–d Hartmann function. For the 1–d case, we used the function:
f (x) = (2x + 9.96) cos(13x − 0.26) + ξ (x) .

We applied a normal random noise having variance σξ2 = δ · (x1 ), with δ = {0.1, 1.0, 10.0}.
We noticed that the convergence rate is affected by the noise level. In particular, in the case δ = 0.1,
the location is identified after 168 simulation replications (corresponding to 6 iterations of the eTSSO
algorithm), 7733 (corresponding to 13 iterations) in the case δ = 1.0 and 32094 (corresponding to 15
iterations) for the case δ = 10.0. It is also noteworthy how good performances are already reached for
much lower budgets: in the low noise case, an error of 0.001 in the location is reached after 80 simulation
replications (i.e., two algorithm iterations), for δ = 1.0, 136 replications are required (i.e., four iterations)
and in the case of δ = 10.0, 1094 replications were required, corresponding to 10 iterations.
For the 2–d case, we considered the following tetra–modal:


2
2 2
+ ξ (x) .
f (x1 , x2 ) = −5(1 − (2x1 − 1)2 )(1 − (2x2 − 1)2 )(4 + 2x1 − 1) 0.05(2x1 −1) − 0.05(2x2 −1)
3843

Pedrielli and Ng
Table 1: Parameters Ai j and Pi j of the Hartmann–3 function
Ai j
3
0.1
3
0.1

10
10
10
10

30
35
30
35

Pi j
0.3689
0.4699
0.1091
0.03815

0.117
0.4387
0.8732
0.5743

0.2673
0.747
0.5547
0.8828

Table 2: Theoretical versus Empirical convergence rates.

1–d
Tetra–Modal
Hartmann–3

Average Empirical Convergence Rate
δ = 0.1
δ = 1.0
δ = 10.0
0.891667 0.95378788
0.9619742
0.842135 0.97017507 0.96720977
0.587 0.46614123 1.28771458

Average Theoretical Convergence Rate
δ = 0.1
δ = 1.0
δ = 10.0
0.265934 0.120082
0.1253412
0.313284 0.269857
0.2251032
0.235702 0.229416
0.22369009

Both dimensions of the test function, x1 and x2 , are scaled to [0, 1]. The global minimum is located at
[0.85, 0.5] and has the value −7.098. We applied to the function a normal random noise with variance
σξ2 = δ · (|x1 | + |x2 |) and we tried three noise scenarios, namely δ = {0.1, 1.0, 10.0}.
In this case, the convergence is reached after 272 simulation replications (7 algorithm iterations) in the
case δ = 0.1, 1071 (17 iterations) replications are required for the case δ = 1.0, while 115852 replications
are required when δ = 10.0 corresponding to 44 iterations of eTSSO. As in the previous case, however,
reasonable result with an error in the order 10e(−3) are obtained already with a number of replications
equal to 220 (i.e., two iterations), 352 (i.e., six iterations) and 5955 (i.e., 13 iterations) for the three noise
levels δ = {0.1, 1.0, 10.0}, respectively. For the 3–d case, we adopt the following Hartmann–3 function:
"
#
4

3

i=1

j=1

f (x1 , x2 , x3 ) = − ∑ αi exp − ∑ Ai j (x j − Pi j )2 + ξ (x) .

Here, 0 ≤ xi ≤ 1 for i = 1, 2, 3; parameters α = (1.0, 1.2, 3.0, 3.2), and Ai j and Pi j given in Table 1.
The function has a global minimum at x∗ = [0.114614, 0.555649, 0.852547] with f (x∗ ) = −3.86278; the
function has three additional local minima. We applied to the function a normal random noise with variance
σξ2 = δ · (|x1 | + |x2 | + |x3 |) and we tried the following set of values for δ , {0.1, 1.0, 10.0}.
A similar behavior with respect to the previous cases was observed. Nevertheless, the increased dimension
leads to a slower convergence rate in the optimum location. Specifically, in this case, the convergence of
|x − x∗ | was reached only with budget 250000 (corresponding to 16 iterations). This has to be brought back
also to the noise of the Hartmann–3 which is larger than in the previous cases. Table 2 shows the average
convergence rate derived from the iterations of the algorithm where, as theoretical counterpart, we use
−1/2
. We considered k = 1, . . . , K ∗ , being K ∗ the iteration at which the algorithm reaches convergence.
Wk
6

CONCLUSIONS

eTSSO is a kriging–based algorithm recently proposed by the authors extending the TSSO. Its efficiency
builds upon a stochastic dynamic sampling rule which geometrically increases the budget to assign to each
algorithm iteration. Inspired by the proof framework in Pasupathy et al. (2015), we propose to apply it
to characterize consistency and efficiency of the eTSSO iterates. To do this, we exploit the properties of
the EGO algorithm and interpret eTSSO as its stochastic counterpart. This paper is a first step towards a
general approach for the performance analysis of meta–model based simulation–optimization algorithms.
According to the main theorem 2, eTSSO
is efficient only in case the coefficient for the geometric increase

−2
of the budget satisfies ck ∈ 1, ℓ . The stochastic nature of the budget allocation does not guarantee
3844

Pedrielli and Ng

this. Nevertheless, ck ∈ 1, ℓ−2 , and this makes the eTSSO allocation closer to the efficient case and the
empirical results sustain this argument. Further research will generalize the study to meta–model based
stochastic algorithms and consider sequential parameters estimation.
ACKNOWLEDGMENTS
Research supported in part by the grant (R-SMI-2013-MA-11) funded by the Singapore Maritime Institute.
REFERENCES
Ankenman, B., B. L. Nelson, and J. Staum. 2010. “Stochastic kriging for simulation metamodeling”.
Operations research 58 (2): 371–382.
Attouch, H. 1984. “Variational Convergence of Functions and Operators”.
Bull, A. D. 2011. “Convergence rates of efficient global optimization algorithms”. The Journal of Machine
Learning Research 12:2879–2904.
Chen, C.-H., J. Lin, E. Yücesan, and S. E. Chick. 2000. “Simulation budget allocation for further enhancing
the efficiency of ordinal optimization”. Discrete Event Dynamic Systems 10 (3): 251–270.
Fu, M. C., C.-H. Chen, and L. Shi. 2008. “Some topics for simulation optimization”. In Proceedings of
the 40th Conference on Winter Simulation, 27–38. Winter Simulation Conference.
Jones, D. R., M. Schonlau, and W. J. Welch. 1998. “Efficient global optimization of expensive black-box
functions”. Journal of Global optimization 13 (4): 455–492.
Kleijnen, J. P., W. van Beers, and I. Van Nieuwenhuyse. 2012. “Expected improvement in efficient global
optimization through bootstrapped kriging”. Journal of global optimization 54 (1): 59–73.
Locatelli, M. 1997. “Bayesian algorithms for one-dimensional global optimization”. Journal of Global
Optimization 10 (1): 57–76.
Pasupathy, R., P. Glynn, S. Ghosh, and F. Hashemi. 2015. “How much to sample in simulation–based
stochastic recursions”. http://lebox.vt.edu/users/pasupath/pasupath.htm.
Pedrielli, G., S. H. Ng, and C. Liu. 2015. “eTSSO: Adaptive Search Method for Stochastic Global Optimization Under Finite Budget”. Available at www.maritimestudies.nus.edu.sg/CMS
%Research%20Highlight%20-%20August%202014.pdf .
Quan, N., J. Yin, S. H. Ng, and L. H. Lee. 2013. “Simulation optimization via kriging: a sequential search
using expected improvement with computing budget constraints”. Iie Transactions 45 (7): 763–780.
Robinson, S. 1996. “Analysis of Sample–Path Optimization”. Mathematics of Operations Research 21:513–
528.
Stein, M. L. 1999. Interpolation of spatial data: some theory for kriging. Springer.
Yin, J., S. H. Ng, and K. M. Ng. 2011. “Kriging metamodel with modified nugget-effect: The heteroscedastic
variance case”. Computers & Industrial Engineering 61 (3): 760–777.
AUTHOR BIOGRAPHIES
GIULIA PEDRIELLI is Research Fellow for the Centre for Maritime Studies at the National University
of Singapore. Her research focuses on stochastic simulation-optimization in both single and multiple–
objectives framework. Her email address is cmsgp@nus.edu.sg.
SZU HUI NG is an Associate Professor in the Department of Industrial and Systems Engineering at the
National University of Singapore. She holds B.S., M.S. and Ph.D. degrees in Industrial and Operations
Engineering from the University of Michigan. Her research interests include computer simulation modeling
and analysis, design of experiments and quality and reliability engineering. She is a member of IEEE
and INFORMS, and a senior member of IIE. Her email address is isensh@nus.edu.sg and her website is
http://www.ise.nus.edu.sg/staff/ngsh/index.html.

3845

2014 IEEE International Conference on
Automation Science and Engineering (CASE)
Taipei, Taiwan, August 18-22, 2014

Time Buffer Control System for multi–stage production lines
Giulia Pedrielli1 , Arianna Alfieri2 and Andrea Matta3

Abstract— The Time Buffer Control System (TBCS) for
coordinating multi–stage systems is introduced in this paper.
The TBCS controls the release of jobs to synchronize the stages
of a production line. Specifically, a set of time intervals called
time buffers are adopted to delay the flow of jobs. The dynamics
of the TBCS is described as well as the structural properties
characterizing the policy. Numerical results are presented to
evaluate the performance of a production/inventory system
controlled by TBCS.

I. INTRODUCTION
For most production/distribution systems, the task of identifying cost effective replenishment strategies for raw materials, work in progress and finished goods’ inventories, aiming
at reaching a predefined service level (meeting customers’
due dates) while minimizing inventory costs, represents a key
activity for the operational performance of the manufacturing
system [1], [2]. The control of the production/inventory
system can be based on forecast or actual information and, in
the latter case, it might require continuous or periodic review
of the system state. Policies based on forecasts are usually
referred to as “open-loop” or push policies [3], [4], whereas
those in the second category are commonly addressed as
“closed-loop” or pull policies [2], [5], [6], [7], [8].
Simple closed–loop policies adopt a single control variable
(e.g., the level of WIP) and use a single rule to synchronize
system stages (i.e., release of the job in case the WIP is below
a predefined threshold level). They are usually characterized
by high operational efficiency, however, lessened by a low
responsiveness. Control strategies that use multiple control
variables as well as more complex synchronization rules,
have been proposed to improve the responsiveness. An example is the Extended Kanban control system [9], which uses
both demand and WIP level information to determine the job
release between stages. Despite the advantage deriving from
multiple information flows on both operational efficiency and
system responsiveness, the increase in the policy complexity
has to be considered [2], [10], [11], [12].
Under the perspective of the system dynamics, control
strategies can be considered as synchronization mechanisms
between stages. Hence, two approaches can be distinguished:
optimize the release time/reorder frequency (time–based
1 Giulia Pedrielli is with Centre for Maritime Studies, National University of Singapore, 12 Prince George’s Park, 118425 SINGAPORE

cmsgp@nus.edu.sg
2 Arianna Alfieri is with the Department of Management and Production
Engineering, Politecnico di Torino, Corso Duca degli Abruzzi 24, TORINO,
ITALY arianna.alfieri@polito.it
3 Andrea Matta is with the Department of Industrial Engineering & Management, School of Mechanical Engineering, Shanghai Jiao Tong University
(SJTU) Shanghai, 200240, P.R. CHINA matta@sjtu.edu.cn

978-1-4799-5283-0/14/$31.00 ©2014 IEEE

policies [13], [14], [15]) or the WIP level/reorder quantities
(WIP-based policies [9], [16], [17]). These two classes of
policies give remarkably different results when stochasticity
affects the system (e.g., demand, leadtime or production time
uncertainty) [3].
Several reasons make time–based preferable to WIP–based
control mechanisms [18]. Firstly, it is usually easier to decide
the reorder frequency rather than to compute the reorder
quantity. Secondly, the use of the release time (reorder
interval) as control variable simplifies the mathematical
representation of the problem with respect to the use of
the reorder quantity. Finally, given the demand pattern, it
is easier to compute the reorder quantities once the reorder
frequency has been decided than vice versa.
In the literature, the time based policies for the synchronization of production lines have been widely studied. Just to cite
a few examples, a cyclic schedule to control job release times
to meet a predefined target throughput level minimizing the
holding cost is proposed in [13], [14]. Instead, in [15], job
release times are chosen by comparing the waiting and the
process time of each job.
Since the information used to control job release is related
to the downstream stages (backward information), the coordination mechanism can be defined as time–based/pull–type.
These kind of policies are very effective in controlling the
WIP level, however they require a continuous review.
In this work, we propose a time–based mechanism, the
Time Buffer Control System (TBCS), for the control of
the release of jobs in multi–stage production lines. Two
main principles characterize this policy: (1) fixed–interval
review, (2) delay of job release based on backward and
forward information flows. The time buffer establishes the
time between consecutive “reviews” of the WIP at each stage
of the production line. Each job is released or delayed based
on the information it receives at each review from both the
upstream (“forward” information flow) and the downstream
(“backward” information flow) stages.
The proposed control system falls within the optimal reorder
interval policies in the inventory literature and within the
release time delay in the production literature. However,
differently from the presented strategies, the system state is
checked at predefined time intervals. This is, most of the
times, a more realistic setting with respect to the continuous
review policies since the implementation of a continuous
review might be too expensive or even infeasible. Moreover,
the use of forward information gives a “push” aspect to the
policy enabling the control of the maximum waiting time of
jobs at each stage.
The remainder of the paper is structured as follows: section II
393

outlines the contribution of the paper; section III introduces
the main notation, while the TBCS policy is detailed in
sections IV–VI. Section VII presents the evaluation of the
policy performance through random generated instances,
while section VIII concludes the paper.
II. CONTRIBUTION
This paper proposes a novel control mechanism for multi–
stage production lines, the Time Buffer Control System
(TBCS), based on the use of time buffers to synchronize
the stages of a tandem line. Time buffers behave as traffic
lights: at fixed and predefined time intervals, the buffer slots
at each stage are “checked” and activated in case they are
empty. A slot remains active, i.e., able to accept new jobs,
until the time buffer interval expires. As a result, buffer slots
at each stage provide a storing space and the related time
buffer establishes the “system pace” determining the time
interval at which buffer slots are checked and new jobs can
occupy free slots.
Controlling the time a job spends in each of the buffer slots
in the line increases the synchronization between different
stages possibly reducing both the WIP and waiting times.
Determining the parameters of TBCS implies the computation of both the “size” of the time buffer (i.e., how many
spots to assign to the traffic light), and the time between
consecutive checks on the buffer slots at each stage (i.e.,
how long spot lights remain active). TBCS parameters can
be computed solving LP models already proposed in the
literature [19], where time buffer is used as a mean to
approximate control policies. Mathematical programming
has been proposed in [20] to optimize the control parameters
in CONWIP flow lines. With respect to this work we
propose a different, time based policy and we model the
system dynamics using the events characterizing the system
instead of its state. This gives the possibility to avoid time
discretization to formulate the mathematical model.
III. GENERAL NOTATION
We refer to single class product multi–stage production
lines where the number of production stages is J. Stage
j = J + 1 represents the demand stage, i.e., the point at
which jobs are released to satisfy customer demand. Each
stage j is composed of a server M j ( j = 1, . . . , J) and the
related input buffer (B j ). Each server M j has a downstream
queue PFj containing the finished parts. The set (B j , M j , PFj )
determines stage j. At the last stage, J + 1, DJ+1 contains
the external
 demands.
	
Variables zi j represent the release times of job
	 i (i =
1, . . . , n) to stage j ( j = 1, . . . , J + 1). Variables xi j are the
starting event
 	times of job i at the stage j ( j = 1, . . . , J).
Variables yi j represent the time job i departs the j − th
stage ( j = 1, . . . , J) to enter stage j + 1 or to be released
from the system ( j = J + 1).
The times when demand for jobs	 occur {di } and the processing times at each stage ti j are problem parameters
that follow the stochastic process {Di } , i = 1, . . . , n, and
the sequence of stochastic processes {Ti } j , i = 1, . . . , n, j =

1, . . . , J, respectively.
Each stage is assigned a time buffer vector to control the
flow
of parts. 	
Specifically, a sequence of time intervals

s1 j , s2 j , . . . , sK j is given where K represents the maximum
number of jobs that can simultaneously populate stage j.
We want the system to satisfy a target average lateness:
α=

1 n
∑ (zi,J+1 − di ) ,
n i=1

(1)

where n is the total number of jobs that flow through the
system, zi,J+1 represents the time when the i–th job is
released from the system and di the time when the demand
for the i–th job is realized.
IV. TIME BUFFER CONCEPT
In this section we provide the main background related
to the time buffer concept which was created as an approximation mechanism for solving mathematical programming
models. More details can be found in the cited contributions.
Recently, mathematical programming has been proposed to
simultaneously simulate and optimize production/inventory
systems [21], [22]. The Buffer Allocation for multi–stage
production systems, the optimal Pallet Allocation for loop
systems, the Kanban and Basestock Allocation for serial
production/inventory systems have been tackled using this
approach ([19], [23], [24]). Moreover, in order to approximate the aforementioned models and decrease the required
computational effort, approximate Linear Programming (LP)
models were also proposed in [24]. Specifically, a set of
continuous decision variables, referred to as time buffers,
were used to approximate the finite buffer capacity, the
number of kanban tokens, the number of pallets and so forth
based on the specific optimization problem to solve, thus
simplifying the mathematical model for optimization. The
Time Buffer Control System (TBCS) uses the time buffer as
a continuous parameter to control the release of jobs between
consecutive stages and it is defined as follows [24]:
Definition 1: Consider two jobs a and b to be served in
the sequence a → b at a stage j of a serial production system.
Assume that job b has to be processed at stage h, whereas
job a undergoes its process at stage j and j > h. Job b is
blocked by job a if its starting time at stage h is greater
than or equal to the finishing time of job a at stage j. If we
allocate a positive time buffer between the two stages, the
starting time of customer b on server h can be anticipated as
follows:
xbh ≥ ya j − s,

(2)

where xbh is the starting time of customer b at stage h, ya j
is the finishing time of customer a at stage j and s is the
time buffer. Job b at stage h can start s time units before the
service of job a is completed at stage j. It is easy to notice
that, if s = 0, job a can block job h, indeed xbh ≥ ya j holds
in this case.
We use the time buffer to control the interval between the
release of job i to stage j + 1 and the release of job i + k to
394

stage j. Hence, equation (2) becomes [23]:
zi, j+1 − zi+k, j ≤ sk j

j = 1, . . . , J; k = 1, . . . , K

(3)

i = 1, . . . , n − k
Reading equation (3) as zi, j+1 ≤ zi+k, j + sk j , each time job i
completes its process at stage j, it has to be released
to stage

j + 1 not later than the mink=1,...,K zi+k, j + sk j , and sk j can
be seen as the maximum delay of job i. It also holds that
zi+k, j + sk j ≥ zi, j+1 , i.e., job i + k can be released to stage j
with a maximum advance of sk j time units with respect to
the release of job i to stage j + 1.
V. TIME BUFFER CONTROL SYSTEM
The basic idea of the TBCS is to check and update the
state of the system at fixed time intervals. Instead of using
the number of parts as a means to coordinate the flow of
jobs between production stages, the buffers are checked
at fixed time units (activation mechanism, section V-A).
Afterwards, in case free slots are available, the jobs waiting
to be processed at each stage j are moved towards the related
server M j (update mechanism, section V-B) keeping their
sequence. In fact, each buffer slot is logically assigned an
index l = K, . . . , 1, where the slot K is meant to be input
slot of stage j and the slot k = 1 represents the input slot to
M j . Jobs waiting to be released to stage j + 1 and jobs from
the previous stage j − 1 can be released according to the
availability of the input buffers of the target stages (release
mechanism, section V-C).
The Event Relationship Graph (ERG) in Figure 1 represents
the dynamic behavior of a single–stage system controlled
through TBCS. Five types of event are modeled: (1) A0 ,
arrival of jobs at the system; (2) T B1 , time buffer event
firing at a time interval equal to the time buffer value sk1 ;
(3) ui1 , update events firing each time a part enters a buffer
timed slot; (4) S1 , start event, fired when a job starts being
processed by server 1; (5) F1 , finish event, fired when the
process terminates. These three event types are triggered
by three fundamental mechanisms: Activation (section V-A),
Update (section V-B), and Release (section V-C).
A. Activation Mechanism
The time buffer is such that, in case K parts (where K is
the number of buffer slots at each stage) are present at the
stage, no additional jobs can be released to the stage itself,
i.e., from equation (3), zi+K, j ≥ zi, j+1 . The value of the time–
buffer sk j coordinates the flow of parts within each stage.
Specifically, the T B j event is fired at fixed time intervals
tbk j (k = 1, . . . , K) equal to tbk j = sk j − sk+1, j time units
(sk1 − sk+1,1 in Figure 1) and buffer slots are checked. For
the input buffer slot it holds tbK j = sK j .
Once the T B event has been fired, if the buffer in the
upstream stage PFj (PF0 in Figure 1) has waiting jobs and
there are free buffer slots, these can accommodate waiting
jobs. Specifically, in case slots indexed from l low ≥ 1 to
l up = K are free, in order to keep the jobs sequence, incoming
jobs occupy free slots for increasing values of l, i.e., the
first job in the upstream queue occupies position l low the

Fig. 1.

ERG for a TCBS in a single–stage production line

next job occupies l low + 1 and so forth until either upstream
queue is empty or the l up –th slot has been filled. During the
interval to the next T B–event, jobs are processed
within each
 	
stage according to the processing times ti j and the system
state is updated through the Update Mechanism presented in
section V-B.
B. Update Mechanism
When a buffer slot becomes free and the downstream slot
(having a larger index value) has a part, event ui j is fired
j
> 0) on the arc outgoing vertex
(condition (tsij == 0 &&tsi+1
j
ui j in Figure 1 (tsi represents the state of the i–th buffer slot
at the j–th stage and it is equal to 1 if the buffer slot i at
stage j accommodates a job and 0 otherwise) and the job
is moved “upward” (i.e., towards the buffer slot with lower
associated index) to the free buffer slot (update (tsij ++;tskj −
−) in Figure 1). The interval tbk j is updated according to
tbk j = sk, j − sk+1, j . The update process leads eventually the
job to reach slot k = 1 where it waits for at most tb1 j =
s1 j − s2 j . The update rule is justified by equation (3): the
considered reference time is the release to the stage, i.e., zi j .
Since the job has already spent the interval sk j in the k–th
buffer slot after its release, the allowed waiting time must be
adjusted accordingly. When the server becomes free, the job
in position k = 1 starts being processed, i.e., the start event
is fired (S1 in Figure 1).
C. Release Mechanism
After the processing time ti j has elapsed, the finish event
(Fj ) is fired, the server state is set to idle ((M1 + +),
Figure 1) and the processed job is sent to the downstream
queue ((PFj + +), (PF1 + +) in Figure 1). At this point,
the job waits until the time buffer triggering signal. The
trigger can be a backward update when the information
is transmitted from the downstream stage. This backward
update happens at the time
 when T B j+1 event is fired, i.e.,
every sk−1, j+1 − sk, j+1 = tbk, j+1 , k = 1, . . . , K time units
395

(refer to section V-B for the definition of tbk j ).
Furthermore, a signal is received from the stage the job has
to leave. In fact, a forward signal is generated when the time
buffer event T B j is fired , i.e., every sk−1, j − sk j , k = 1, . . . , K
time units. This information transmission “pushes” the job
that is to be released to the downstream stage.
As an example, let us assign to stage j the time buffer
sequence s j = s1 j , s2 j , s3 j . Since the time buffer size is K =
3, no more than 3 jobs are simultaneously present at stage
j. The time buffer event T B j will occur at tbi j , (i = 1, . . . , 3)
intervals s1 j − s2 j , s2 j − s3 j , s3 j , respectively. A possible
system dynamics is the following: at time t = s3 j a change
in the queue level at stage j is observed from Q to level
Q + 1. In fact a TB event was fired and the third buffer
slot was activated. As a result, a part is released to stage j.
Afterwards, a decrease in the level happens at time t +tbi, j+1 ,
i.e., a time buffer event of the downstream stage is fired and
a release to stage j + 1 occurs.
VI. PROPERTIES OF THE TBCS
Herein the impact of the time buffer parameters on the
departure times of jobs from different stages is formalized.
We use the fact that the TBCS can be modelled exploiting
the mathematical programming based approach shown by the
authors in [24]. In fact, the dynamics of the TBCS can be
formulated through evolution inequalities (making use of “+”
and “max” operators only) relating the timing of events in
the TBCS.
Proposition 1: In the TBCS the timing of the events are
related by the following mathematical relationships:



zi1 = max max zi−k,1 + sk−1,1 − sk1 ; zi+K,2 ; ai
(4)
k

i = 2, . . . , n

	
xi j = max zi j , yi−1, j i = 2, . . . , n ∀ j

(5)

yi j = xi j + ti j ∀i ∀ j

(6)

0

zifj = yi, j−1 i = 2, . . . , n j = 2, . . . , J − 1

00
zifj = min zi+k, j−1 + sk1, j−1 ∀i ∀ j
k+1,...K

0

zbi j = zi−K, j+1 i = 2, . . . , n j = 2, . . . , J − 1

00
zbi j = max zi−k, j + sk−1, j − sk j
∀i ∀ j
k

Release time of job i at stage j + 1 zi, j+1 appears also in the
equation linking job i and i + k + 1:
zi+k+1, j ≥ zi, j+1 − sk+1, j .

(7)
(8)
(9)

Terms zi+k, j and zi+k+1, j correspond to the release times, at
stage j, of jobs i + k and i + k + 1, respectively. Since job
i + k is served before customer i + k + 1 (to keep the job
sequence) at each stage, the following condition must hold:
zi+k, j ≤ zi+k+1, j .

(10)

n
 0 0
 00 00 o
zi j = max max zifj ; zbi j , min zifj ; zbi j

(11)

i = 2, . . . , n j = 1, . . . , J − 1



	
zi,J+1 = min min zi+k,J + sk,J , di ∀i

(12)

k

is released to the stage j (zi j ) and the time when the previous
job has completed its processing at the same stage yi−1, j .
Relations (6) model the finish of the processing for job i at
stage j that has to be the starting time xi j plus the processing
time ti j . Equations (7) and (8) represent the “forward” release
conditions. Each job i has to finish being processed before
being released to the next stage (constraint (7)). Constraints
(8) model the release due to the time buffer occurring at
the stage from which it has to be released. The TBCS
synchronizes the stages delaying the release information to
jobs k > i at the same stage, by a fixed interval.
Relations (9) and (10) represent the “backward” release
conditions. Each job i cannot enter the next stage if it is
full (constraint (9)). Constraints (10) model the coordination
effect of the time buffer on the release of each job to the
next stage. Also in this case, the TBCS is delaying the
transmission of the downstream
 release information by the
fixed interval sk−1, j+1 − sk, j+1 .
Finally, the actual release of job i must be greater than both
the finishing time on the previous stage and the release time
of job i + K. The time buffer coordination mechanism is
given by (8) and (10), it is implemented each time conditions
(7) and (9) are not binding and the release time is set to
00
00
the minimum between zbi j or zifj (condition (11)). Equations
(12) model the release of job i from the last stage to satisfy
customer demand.
The evolution equations presented in Proposition 1 lead to
the following property.
Property 1: There always exists a time buffer sequence,
solution of the evolution equations, which is not increasing
in k, i.e., sk j ≥ sk+1, j k = 1, . . . , K − 1, ∀ j.
Proof: Consider equations (8); job i and job i + k are
linked by:
zi+k, j ≥ zi, j+1 − sk j .

Proof: Equations (4) describes the release to the first
stage, which has to be the maximum between the finish of
the processing of the previous customer and the arrival time
of the customer itself (i.e., ai ). Relations (5) represent the
time at which job i can start being processed at machine j
(xi j ). This time is the maximum between the time when job i

If sk j ≤ sk+1, j holds, it would allow job i + k + 1 being
released to stage j before job i + k. Since job i + k must
precede job i + k + 1, the additional time sk+1, j − sk j can
never be used by job i + k + 1 for being released. Hence,
there always exists a solution to the evolution equations in
which sk j ≥ sk+1, j .
n0 o
Property 2: Consider the TBCS, with time buffer sk j
 	
0
in place of sk j and let ε (x0 , y0 , z0 ) and ε (x, y, z) be the
0
related sample paths. The following holds. If sk j > sk j , for
0
some k0 = 1, . . . , n and sk j = sk j , ∀ k 6= k0 , then: x0 ≤ x, y0 ≤ y,
and z0 ≤ z.
396

Proof: In the evolution equations, if sk j increases for all
k and j, constraints (8) allow release times zi, j+1 to decrease.
Since this is applicable to each j and k, also zn,J+1 (i.e., the
release to the demand stage) is allowed to decrease. If sk j
decrease for all k and j, constraints (8) force release times
zi, j+1 to increase. Since this is applicable to each stage j and
k, also zn,J+1 will increase.
Property 3: The service level of a TBCS with sk j >
0, ∀ k ≤ k̄ j is bounded from above by the kanban system
having k̄ j kanban tokens at the related stage j.
Proof: Consider equation (8). The related kanban
system has this relation defined only for the case k = k̄ j .
This means that there are fewer equations constraining the
system dynamics in a kanban system. It is a known result
that as the size of the feasible region decreases, the solution
cannot improve. As a consequence, the event time variables
will be forced to assume larger values. Since the service level
is a decreasing function of the release event times (equation
(1)), it cannot improve.
VII. NUMERICAL RESULTS
Numerical experiments on random generated instances
were designed to evaluate the performance of the TBCS
policy. In particular, the following measures were analyzed:
(1) average lateness, (2) average waiting time, and (3)
average number of parts at each stage j. We analyze a three–
stage system with a single product type and a single server
at each stage. All the jobs are assumed to be available at the
first stage at time t = 0. The processing times were assumed
to be distributed as a log–normal with mean µ = 2.732 and
variance σ 2 = 0.075, corresponding to a coefficient of variation cv = 0.1. Also the demand was distributed according
to a log–normal with: (1) µ = (2.87, 3.73) (high arrival rate,
low arrival rate, respectively), and (2) coefficient of variation
cv = (0.98, 0.28) (high variance case, low variance case,
respectively), hence resulting in 4 experimental conditions. In
all the experiments, the ratio between the average processing
time and the inter-arrival time was 95% for the high mean
experiments and 75% for the low mean experiments. 50
replications were run to evaluate the performance measures,
each with 5000 jobs and 2000 jobs considered as warmup
period.
Following the approach in [24], the time buffer parameters
in each case were generated solving a mathematical model
having as objective function the sum of the time buffer
components ∑Kk=1 ∑Jj=1 sk j and as constraints the (max, +)
equations (4)–(12).
Numerical results showed that the time buffer value is
deeply affected by the variability of the input and there
is a strong interaction effect of the mean and the variance
factors. Figures 2 and 3 show the main performance measures
computed simulating the TBCS policy under the described
conditions. In particular, the average number of customers in
queue and the average waiting time are reported in Figures
2 and 3 for the first and third (last) stage, respectively.
We can notice how the average waiting time is noticeably
influenced by the increase of the demand as well as the

Fig. 2.

Average Waiting Time and queue level at stage j = 1

average queue level. It is interesting to remark the effect of
the variance on the waiting time at the first stage (Figure 2).
At this stage, the effect of a variance increase is to decrease
the waiting time, contrarily to the effect on other stages. This
is due to the fact that, as the demand variability increases,
time buffers tend to be allocated to the last stage in order
to increase the system reactivity, whereas less time buffer is
allocated to the source point. This leads to reduced WIP and
waiting times. With respect to the service level, the results

Fig. 3.

Average Waiting Time and queue level at stage j = 3

showed how the TBCS performs better in cases of highly
saturated systems (service levels around 98%), whereas the
worst performance of the TBCS system was individuated for
the case with low variance and low mean (average service
level 82%). On the contrary, when the variance increases,
the time buffer solution seems more robust. In fact, the case
of maximum demand rate and maximum variance is the one
in which the TBCS performs better in terms of service level
(in all the experimentations the service level we obtained
from TBCS was in the interval (84.1%, 98.2%) with 95%
confidence).
A further set of experiments was run to compare the
performance of the TBCS policy and the Knaban control
system. In order to identify the kanban configuration, we
solved the exact optimization problem presented in [23]. We
ran both cases µ = (2.87, 3.73), keeping the coefficient of
variation at the low level, i.e., cv = 0.28. Results are shown
in Table I. In particular, Kanban(1) and TBCS(1) refer to the
results with low mean arrival rate for the Kanban and TBCS
policy, respectively. The entries Kanban(2) and TBCS(2)
refer to the high mean case. From the results we can observe
the higher performance of the proposed policy in terms of
both average and standard deviation of the system waiting
397

TABLE I
C OMPARISON BETWEEN K ANBAN AND TBCS CONTROL POLICY
Policy
Kanban(1)
Kanban(2)
TBCS(1)
TBCS(2)

¯
SL
93.280
91.240
82.942
92.676

σ̂SL
2.289
2.890
2.425
2.367

W¯T
6.494
6.945
3.229
5.247

σ̂W T
0.061
2.223
0.0421
1.167

Q̄
3.940
5.144
3.064
4.567

σ̂Q
0.003
0.757
0.013
0.403

time (WT) and average system queue (Q) (experiments were
run with 50 replications for each case). This is due to the
fact that TBCS is a time based policy, hence it reacts more
effectively to the demand rate bringing the system as close
as possible to the “demand pace”. This is also reflected by
lower variance in the results. Interestingly, the kanban system
outperforms the TBCS, in case of low average demand rate,
in terms of service level (SL) (second and third columns in
Table I). This can be brought back to the fact that, with low
demand rate, the TBCS policy, slowing down the system at
a lower pace, results in lower service levels.
VIII. CONCLUSIONS AND FURTHER
DEVELOPMENTS
This work provides the first results of a new time–based
control policy, the TBCS.
The numerical results showed a remarkable sensibility of the
value of the time buffers to changes in the demand variability.
This facts leads to a substantial overestimation of the amount
of required time buffer (i.e., the value associated to each
time buffer). As a result, the time buffer system tends to
behave similarly to a kanban system (property 3). A possible
way to improve TBCS is then to provide a dynamic version
of the policy, where parameters can change based on the
realizations of the processing times.
Furthermore, the authors are currently working on the generalization of the presented policy to multiple time buffer
types under two main objectives: improving the policy effectiveness and provide a framework for time based policies.
Concerning the latter aspects, it is being investigated whether
the time buffer concept can be adopted to characterize time–
based control mechanism and under which conditions it can
describe reorder–level based policies. The idea behind this
study is to create a unified framework for push–pull control
policies using the time, instead of the space, to model the
coordination effect of the different control systems.
R EFERENCES
[1] A. Federgruen and Y.-S. ZhengSource, “Optimal power-of-two replenishment strategies in capacitated generalproduction/distribution,”
Management Science, vol. 39, no. 6, pp. 710–727, 1993.
[2] M. L. Spearman and M. A. Zazanis, “Push and pull
production systems: Issues and comparisons,” Operations Research,
vol. 40, no. 3, pp. 521–532, 1992. [Online]. Available:
http://or.journal.informs.org/content/40/3/521.abstract
[3] C. Qi, A. Sivakumar, and S. Gershwin, “An efficient new job release
control methodology,” International Journal of Production Research,
vol. 47, no. 3, pp. 703–731, 2008.
[4] J. Orlicky, Materials Requirements Planning. McGraw- Hill, New
York, 1975.

[5] G. Liberopoulos, “Production release control: Paced, wip-based or
demand-driven? revisiting the push/pull and make-to-order/make-tostock distinctions,” in Handbook of Stochastic Models and Analysis
of Manufacturing System Operations, ser. International Series in
Operations Research & Management Science, J. M. Smith and B. Tan,
Eds. Springer New York, 2013, vol. 192, pp. 211–247.
[6] L. Krajewski, B. King, L. Ritzman, and D. Wong, “Kanban, mrp,
and shaping the manufacturing environment,” Management Science,
vol. 33, no. 1, pp. 39–57, 1987.
[7] L. Lee, “A comparative study of the push and pull productions
systems,” International Journal of Operations and Production Management, vol. 9, no. 4, pp. 5–18, 1989.
[8] J. Deleersnyder, T. Hodgson, R. King, P. OGrady, and A. Savva,
“Integrating kanban type pull systems and mrp type push systems:
Insights from a markovian model,” IEE Transactions, vol. 24, no. 3,
pp. 43–56, 1992.
[9] Y. Dallery and G. Liberopoulos, “Extended kanban control system:
combining kanban and base stock,” IIE Transactions, vol. 32, pp. 369–
386, 2000.
[10] W. Hopp and M. Spearman, Factory Physics. McGraw-Hill, 2007.
[11] A. M. Bonvik, C. Couch, and S. Gershwin, “A comparison of
production-line control mechanisms,” International Journal of Production Research, vol. 35, no. 3, pp. 789–804, 1997. [Online]. Available:
http://www.tandfonline.com/doi/abs/10.1080/002075497195713
[12] J. Deleersnyder, T. J. Hodgson, H. Muller-Malek, and P. J. O’Grady,
“Kanban controlled pull systems: An analytic approach,” Management
Science, vol. 35, no. 9, pp. 1079–1091, 1989.
[13] R. Bowmann, “Job release control using a cyclic schedue,” Production
& Operations Management, vol. 11, no. 2, pp. 274–287, 2002.
[14] S. C. Graves, H. C. Meal, D. Stefeck, and A. H. Zeghmi, “Scheduling
of reentrant flow shops,” Journal of Operations Management, vol. 3,
no. 4, pp. 197–203, 1983.
[15] K. Hadavi and M. S. Shahraray, “Release no job before its time,” in
Proceedings of the Third International Conference on Expert Systems
and the Leading Edge in Operations Management, 1989, pp. 381–399.
[16] J. Cochran and S.-S. Kim, “Optimum junction point location and
inventory levels in serial hybrid push/pull production systems,” International Journal of Production Research, vol. 36, no. 4, p. 11411155,
1998.
[17] R. Hall, Zero Inventories, ser. Dow Jones-Irwin. Homewood, IL,
1983.
[18] W. Maxwell and J. Muckstadt, “Establishing consistent and realistic
reorder intervals in production-distribution systems,” Operations Research, vol. 33, no. 6, pp. 1316–1341, 1985.
[19] A. Alfieri, A. Matta, and G. Pedrielli, “Optimization of pull control
systems: A time buffer approach,” Submitted for review to IIE Transactions, 2013.
[20] S. Helber, K. Schimmelpfeng, and R. Stolletz, “Setting inventory levels
of conwip flow lines via linear programming,” BuR Business Research
Journal, vol. 4, no. 1, pp. 98–115, 2011.
[21] L. W. Schruben, “Mathematical programming models of discrete event
system dynamics,” in Proceedings of the 2000 Winter Simulation
Conference, J. A. Joines, R. R. Bartona, K. Kang, and P. A. Fishwick,
Eds. Piscataway, New Jersey: Institute of Electrical and Electronics
Engineers, Inc., 2000, pp. 381–385.
[22] A. Matta, “Simulation optimization with mathematical programming
representation of discrete event systems,” in Proceedings of the 2008
Winter Simulation Conference, S. J. Mason, R. R. Hill, L. Monch,
O. Rose, T. Jefferson, and J. W. Fowler, Eds. Piscataway, New Jersey:
Institute of Electrical and Electronics Engineers, Inc., 2008, pp. 1393–
1400.
[23] A. Alfieri and A. Matta, “Mathematical programming representation
of pull controlled single-product serial manufacturing systems,”
Journal of Intelligent Manufacturing, vol. 23, no. 1, pp. 23–35, 2012.
[Online]. Available: http://dx.doi.org/10.1007/s10845-009-0371-x
[24] ——, “Mathematical programming formulations for approximate simulation of multistage production systems,” European Journal of Operational Research, vol. 219, no. 3, pp. 773 – 783, 2013.

398

Proceedings of the 2012 Winter Simulation Conference
C. Laroque, J. Himmelspach, R. Pasupathy, O. Rose, and A. M. Uhrmacher, eds.

Time Buffer for Approximate Optimization of Production Systems: Concept, Applications and
Structural Results
Giulia Pedrielli
Politecnico di Milano
Via Giuseppe La Masa 1
20156 Milano, ITALY

ABSTRACT
Simulation Optimization is acquiring always more interest within the simulation community. In this
ﬁeld, Mathematical Programming Representation (MPR) has been applied for both simulation and sample
path-based optimization of production systems performance. Although in the traditional literature these
systems have been represented by means of Integer Programming (IP) models, recently, approximate Linear
Programming (LP) models have been proposed to optimize and evaluate the performance of a category of
production systems. This work deals with LP models developed based on the Time Buffer (TB) variable
whose concept, applicability and structural properties will be presented. Moreover the models convergence,
within the Sample Average Approximation (SAA) framework, will be characterized.
1

Contributions

MPR is deeply different from traditional simulation optimization techniques (Fu, Glover, and April 2005)
since more information can be obtained from a single simulation (optimization) run (Chan and Schruben
2008).
TB-based LP models are here applied to approximately solve manufacturing optimization problems
(Buzacott and Shantikumar 1993). These models, iteratively solved using the SAA approach, result in the
optimal TB conﬁguration and the related performance estimates (Sect. 3). These two outputs form the
basis to derive the integer solution the TB is approximating. Indeed, the TB solution in the continuous
domain is strongly related to a unique solution in the discrete domain.
Two applications of the TB have been developed so far: (1) approximation of the buffer capacity in an
open ﬂow line, (2) approximation of the number of pallets in a loop line. The optimization problems were
the Buffer Allocation Problem (BAP) and the Pallet Sizing Problem (PSP) respectively. Although a model
for the BAP was already proposed in (Alﬁeri and Matta 2012), for this case, the second order properties
of the approximate model variables, the SAA solution approach and the convergence properties constitute
a result of this work (Sect. 4).
Formulations are not reported for space limitations, hence refer to (Alﬁeri and Matta 2012).
2

Time Buffer for Simulation and Optimization

The TB is a continuous variable deﬁned in the approximate model to replace the discrete variable deﬁned
in the original IP model. In general, to approximate a discrete variable with a TB, it must be possible to
formally describe its effects on the events characterizing the system dynamics. This holds when the system
dynamics can be formulated as a set of max-plus type equations, (Buzacott and Shantikumar 1993).
In the BAP case, for example, the space buffer capacity continuous counterpart has to be modelled. The
capacity effect is to delay (anticipate) the time a customer enters the workstation upstream the buffer (start
event). Hence, the start event time represents the continuous variable to trigger by means of the TB that
978-1-4673-4780-8/12/$31.00 ©2012 IEEE

Pedrielli
will, directly, delay or anticipate this event as the space buffer, indirectly, does. Moreover, for the open line
case, referring to the work of Shanthikumar et al., (Shanthikumar and Yao 1991), it was proved that: (1)
the TB is increasing convex in the processing times if the processing times are convex in the parameters
characterizing their distribution, (2) the completion time is decreasing convex in the TB and increasing
convex in the processing times if these are increasing and convex in the parameters characterizing their
distribution.
3

Simulation Optimization Algorithm

The TB simulation optimization models are iteratively solved to ﬁnd the optimal TB conﬁguration and its
discrete counterpart, following the steps described below.
1. Initialization: Iteration k = 0. (1) set the parameters describing the system and the simulation
optimization conﬁguration parameters (e.g., the number of machines, the run length), (2) set the
target average completion time, (3) generate the sample path of processing and arrival times.
2. System Conﬁguration Generation: Solve the LP approximate optimization model, obtaining the
samplepath-optimal TB. Go to Step 3.
3. System Performance Evaluation: Feed the approximate simulation model with the initialization
data and the TB’s obtained from the previous step. Solve the approximate simulation model. If
k > 0 and the stopping condition is met, derive the sp-approximate integer solution and exit the
procedure. Otherwise increase the sample path size and go to Step 2.
4

Convergence Properties

Let ε-SBAP deﬁne the ﬁnite Sample path approximate BAP problem to be solved for the ε-optimal solution
in terms of TB, i.e. the solution characterized by a completion time higher than the target of maximum ε,
with ε going to 0 as the size of the sample path n → ∞, P-almost surely.
For this problem, the asymptotic convergence was characterized for: (1) the feasible region, (2) the set
of minimizers (in terms of TB solutions), (3) the value of the objective function. Based on the properties
of the LP models, duality theory and epi-convergence theory, the asymptotic convergence for all the cases
was proved.
Moreover, adopting the Large Deviation Theory ((Dembo and Zeitouni 2009), (Shapiro 1996)), it was
proved that, in case the central limit theorem holds for the objective function and the constraints, the
probability of obtaining a sample path solution which is not the optimal for the inﬁnite sample path problem
goes to 0 with exponential rate.
REFERENCES
Alﬁeri, A., and A. Matta. 2012. “Mathematical Programming Formulations for Approximate Simulation
of Multistage Production Systems”. European Journal of Operational Research 219 (3): 773–783.
Buzacott, J., and J. Shantikumar. 1993. Stochastic Models of Manufacturing Systems. Prentice–Hall.
Chan, W., and L. Schruben. 2008. “Optimization models of Discrete–Event System Dynamics”. Operations
Research 56 (5): 1218–1237.
Dembo, A., and O. Zeitouni. 2009. Large Deviations Techniques and Applications. Stochastic Modelling
and Applied Probability. Springer.
Fu, M., F. Glover, and J. April. 2005. “Simulation optimization: a review, new developments, and
applications”. 83–95.
Shanthikumar, J. G., and D. D. Yao. 1991. “Strong Stochastic Convexity: Closure Properties and Applications”. Journal of Applied Probability 28 (1): pp. 131–145.
Shapiro, A. 1996. “Simulation Based Optimization–Convergence Analysis and Statistical Inference”.
Stochastic Models 12 (3): 425–454.

Proceedings of the 2015 Winter Simulation Conference
L. Yilmaz, W. K. V. Chan, I. Moon, T. M. K. Roeder, C. Macal, and M. D. Rossetti, eds.

THE OBJECT-ORIENTED DISCRETE EVENT SIMULATION MODELING:
A CASE STUDY ON AIRCRAFT SPARE PART MANAGEMENT
Haobin Li
Institute of High Performance Computing
Department of Computing Science
1 Fusionopolis Way, 138632, SINGAPORE

Yinchao Zhu
Yixin Chen
National University of Singapore
Department of Industrial and Systems Engineering
1 Engineering Drive 2, 117576, SINGAPORE

Giulia Pedrielli

Nugroho A. Pujowidianto

National University of Singapore.
Centre for Maritime Studies
15 Prince George’s Park, 118414, SINGAPORE

Hewlett-Packard Singapore
Business Printing Division
138 Depot Road, 109683, SINGAPORE

ABSTRACT
Object–Oriented DES (O2 DES) is an effort to implement the object oriented paradigm in the scope of ease
the development of discrete event simulation models in both education as well as industrial settings. In
particular, O2 DES offers several functionalities which support the integration of the tool with optimization
techniques, thus making it easier to the students to understand the concept of simulation–optimization. It
also supports the application of different variance reduction techniques such as budget allocation and time
dilation. In order to do so, the provided toolkit exploits the C# language and the .NET Framework and it
guarantees the efficient generation of DES models, as well as the effectiveness of the developed models
in being integrated with sampling solutions. We propose a case study related to the aircraft spare part
management problem to show case the main functionalities of the proposed tool.
1

INTRODUCTION

In the past decade, Discrete event simulation (DES) has established in many industrial realities as the
main tool for evaluation of the performance of critical portions of the system. More recently, it has been
integrated with optimization in many commercial software, in order to support decision makers in the
improvement/redesign of industrial systems. Examples of commercial software packages providing this
functionalities are Arena (Kelton et al. 1998), AutoMod (Muller 2011), FlexSim (Nordgren 2002). These
software packages provide a graphical interface to support the users building the model and they offer
animation features important to understand the dynamics of the system resulting from the generated model.
Simulation is also a key topic in education and DES in particular. Despite the fact that DES is event
based, most of the simulation software provided to students to start creating their first simulation models
are process–based, i.e., they do not explicitly expose the event and the events list (Kelton et al. 1998).
On the contrary, purely event –based simulation paradigm typically do not expose the state of the system,
which is, instead, implicitly defined through the events sequence (Schruben and Yucesan 1993).
In relationship to this, O2 DES represents the effort to provide students with a toolkit enabling, simultaneously,

978-1-4673-9743-8/15/$31.00 ©2015 IEEE

3514

Li, Zhu, Pedrielli, Pujowidianto, and Chen
the description of the sequence of events as well as the status of the entities of the model, thus mirroring
the contents of a simulation class.
In addition to this, we recognized that, despite optimization and variance reduction have been since long
integrated within the simulation class material, still the tools used by the student to develop models do not
enable interactive linkage with the optimization as well as some variance reduction functionality (Nelson
2013). Besides these functionalities, advancing technologies have influenced the latest developments in the
DES community. As computing capability increases, due to the emergence of parallel high-performance and
cloud infrastructures, not only the evaluation of significantly complex and large systems has been enabled,
but the active integration of simulation and optimization is a practical alternative to more traditional search
algorithms based on approximated analytical models of the system under analysis. Due to its object–oriented
characteristic, O2 DES simplifies the incorporation of such techniques making it even more useful to student
not only from engineering, but also from computer science background.
In fact, if optimization is considered in the setting of simulation, one of the main criticality raises from the
need to evaluate a large number of configurations when a solution needs to be identified (Fu 2002). Hence,
calling the simulator has to be fast. In particular, the set–up times due to the interaction of the simulation
with the optimization have to be reduced to the minimum (Fu et al. 2014). Nevertheless, this interaction is
only one of the main source of inefficiency in the current implementation of simulation–based optimization.
As an example of this inefficiency, several optimization architectures use a database to store a great amount
of simulation results, to make them available to the search procedure. Such a decoupled approach can
largely hinder the efficiency of the procedure. To facilitate the communication between simulator and other
layers in the optimization architecture, it is desired to have all parties to talk through interfaces under a
common framework. Certainly, one possible choice is to select a common programming language that is
robust to develop all parties.
Besides, the simulation modeling has to be more flexible and modular in order to enable the following
functionalities in an optimization setting: (1) starting from an initial configuration, new configurations
should be automatically generated driven by the optimization procedure; (2) simulation parameters such as
the replication length, the experiment time scale or the number of replications might be modified dynamically
and or interactively as the simulation progresses under to reduce the output variance.
The second point is strongly related to the problem of improving efficiency of simulation in optimization.
For the past two decades, many contributions have been proposed on how to efficiently allocate replications
for DES in the process of identifying optimal solutions (Chen et al. 1997). Almost at the same time,
Schruben (1997) proposed the concept of event time dilation, also aiming to improve the efficiency of the
optimization procedure. Recently, some have also initiated the research on parallel optimization to utilize
the advantages of multi-core computers and cloud computing (Fu et al. 2014). Despite the aforementioned
research effort, few commercial simulation modeling paradigms can support all the advanced features
mentioned above, because many simulators are rigid, and the interaction with the simulator while the
simulation is running is not a provided functionality.
Adopting an object–oriented paradigm can largely simplify the development of a simulation model
and enable the interaction and integration with the optimization component, by creating objects that the
simulation can expose to the optimization in order to be modified.
In this paper, we propose the first steps towards the development of a new paradigm for Object-Oriented
DES modeling, O2 DES, which we developed in C#. In our study, all the advanced features mentioned
above were taken developed and implemented. We hope that this work could inspire a new alternative and
serve as a reference for the DES simulation & Optimization community.
The remainder of the paper is structured as follows: section 2 provide the basis language and the main
reference literature at the basis and motivating our work. Section 3 presents the object–oriented simulation
paradigm, whose main components are described in section 4, while the main functionalities are the subject
of section 5. Section 6 shows a case study on the aircraft spare part management problem.The case study
shows the object–oriented procedure to generate a discrete event simulation model, while proving the

3515

Li, Zhu, Pedrielli, Pujowidianto, and Chen
easiness of incorporating the generated simulation model with optimization algorithms and two advanced
simulation budget control techniques. Finally, section 7, closes the paper.
2

BACKGROUND

DES models the dynamics of a system by executing by an ordered sequence of discrete events which
defines, at any point, the simulation events’ list (Banks and John S. Carson 1986, Schriber et al. 2014).
In a DES, an event occurs at a particular point in time and it may determine changes in the system state.
Specifically, a discrete event system might change its state only when an event triggered and the system
dynamics evolves according to the scheduled events list. DES is widely used in areas of optimization,
process improvement, and network analysis. In manufacturing industries, it is the main technique for the
analysis of reliability, capacity and maintenance improvements, as well as for evaluations of alternative
designs (e.g., plant expansions, capital investment options, or cycle time reduction and safety (Sharda et al.
2011)).
For DES modeling, many high–level simulation languages have been introduced due to the fact that
simulation programs are comparatively difficult to write in machine languages. A history of simulation
including the DES software development can be found in Nance and Sargent (2002). As an example,
SIMULA is programming language that was extended from ALGOL 60, a language specifically developed
for simulation, facilitating the formal description of layout and rules of operation of systems. SIMULA
uses the method of quasi–parallel processing for performing operations in “active phases” or “events” (Dahl
and Nygaard 1966).
Many commercial softwares have been developed enhancing the user interface and promoting the spread of
c developed based on SIMAN simulation
simulation in education and industries. As an example, Arena
,
language and CINEMA graphic libraries, uses a hierarchical approach to provide the user with an objectoriented simulation language and flexible system definition for end-user (Hammann and Markovitch 1995).
In addition to the generation of simulation–specific languages and commercial software, several open
source libraries have also been released developed in general purpose programming languages. Examples
are adevs in C++, MASON in Java and SimPy in Python. In particular, C# has been used to develop DES
software (Choi and Kang 2013). Some examples of C# DES software are SharpSim, DEVS#, Activity Cycle
Executor (ACE). However, the focus of the aforemetioned platform is performance evaluation, whereas
optimization is not of concerns.
As a result, despite numerous contributions have been provided in the field of performance evaluation
by means of DES, there is a need to develop a tool for efficient DES modeling which, at the same time,
enables incorporating the recent advances in the simulation optimization.
In particular, a new optimization–oriented simulation framework is required which try to satisfy the
following needs: 1) efficient and automated generation of alternative configurations; 2) run–time accessibility
to database to store scenario data and candidate solutions, as well as (3) for designing and incorporating
heuristics and various optimization algorithms; and 4) run–time access to simulation parameters for output
accuracy on-line control.
In light of these requirements, we propose O2 DES as an alternative way to build DES model in C#
language.
3

THE GENERAL FRAMEWORK

Any DES model consists of two major components: a system clock that indicates the time of the simulated
system, and a future events list (FEL) that stores all events scheduled to happen at a future clock time.
When a simulation model is running, the head event (i.e., the one with the earliest scheduled time) in the
FEL is selected for execution and the system clock is updated to the head event time. Specifically, when
an event is executed, some status property of the simulation model might change and new events may

3516

Li, Zhu, Pedrielli, Pujowidianto, and Chen

Figure 1: Class diagram of a DES framework.

Figure 2: Sequence diagram for scheduling a future event.
be scheduled in another future clock time. The procedure is repeated until the FEL becomes empty, or a
terminating condition for the simulation is met, e.g., a specified clock time, or number of events is reached.
The class diagram of the DES framework implemented in C# is shown in Figure 1. And we have also
attached the sequence diagrams for the core functions in the framework, i.e., scheduling a future event
(Figure 2) and executing a head event (Figure 3). Although it is simple and compact, the framework could
constitute the fundamentals for all DES models implemented with Object Oriented Programming (OOP)
paradigm.
In addition to this basic features, we further defined the main O2 DES components and functionalities
to meet the highlighted challenges at the end of section 2.
4

BASIC COMPONENTS

O2 DES

provides four main components the user can adopt to build a simulation–optimization model: (1)
Scenario and Static Components, (2) Status and Loads, (3) Event. It is noteworthy that, due to the use of
C#, we did not need to create objects for distributions or for the definition of queues (at least up to this
point). In fact, distributions as well as random number generation engines, are provided by means of the
C# library MathNet.Numerics provided in Visual Studio NuGet Packages. Also, queues can be modeled
by directly referring to the native libraries using the queue, stack or list objects.

3517

Li, Zhu, Pedrielli, Pujowidianto, and Chen

Figure 3: Sequence diagram for executing a head event.
4.1 Scenario & Static Components
The static components refer to all entities whose properties do not change during the simulation run. For
encapsulation purposes, in O2 DES, we also defined the Scenario object for each simulation model as a
means to contain the collection of static components.
As an example, refer to an M/M/n queue, in which we have two static components, i.e., customer type
and server type. The customer type is defined through its static inter–arrival time distribution, while the
server type is characterized by the distribution of the service times and the number of servers in the
considered problem. Then the Scenario object is created and it contains these static components and fully
characterizing the system configuration under analysis.
To the knowledge of the authors there is no modeling framework proposing the scenario class. Instead,
the system configuration is implied by the definitions of the simulation objects (e.g., resources, processes).
Nevertheless, it can be argued that the implicit scenario definition can make it more difficult, or hinder, the
possibility to define, and study, multiple scenarios. This is particularly relevant in an optimization setting,
where several alternatives might be defined and compared.
4.2 Status & Loads
Differently from the previous case, the Status and the Load are dynamic components, i.e., they refer to
the objects whose properties are updated during the simulation run, and transient objects (i.e., entities that
leave the system), respectively. Together, they represent the simulation runtime information. Usually they
are associated to one or more static components so that during the runtime, the Scenario properties can be
easily referred.
Specifically, if the simulated entity has a life cycle in the simulation which is shorter than the simulation
run length, it is referred to as a load and its class contains all the runtime properties of the entity, whereas
its static properties are encapsulated in a static component class.
The status, instead, refers to a set of properties that describes a snapshot of the simulated system (e.g.,
waiting queue, server status).
Once the loads and status classes have been defined, we can instantiate a scenario and link them to the
scenario. From an OOP perspective, the scenario encapsulates the load and the status instances, together
with the common method that manipulates the status when events are triggered (refer to section 4.3). Thus,
the modeling of events will focus on the logical dependency among the status changes. This has an effect
3518

Li, Zhu, Pedrielli, Pujowidianto, and Chen
on the coding of the simulation model, in that it makes the modeling clean and tight, improving readability
and maintainability of the code.
As an example, following the M/M/n queue previously described, although the customer type is static,
the individual customers (i.e., the occurrences) are transient entities as they arrive and leave the system as
simulation is running and each of them might have associated a certain statistics (e.g., cycle time, waiting
time). A buffer is a status entity in that its level varies as the simulation progresses. Note that the buffer
is also a static component, however, as such it is described by its, static, capacity attribute.
The .NET framework provides a rich library for organizing status variable in a well defined data
structure. For example, the classes of List, Stack and Queue as in the standard library, can be directly
applied to implement different types of waiting queues.
4.3 Events
The event is not a static neither a dynamic component since it only describes the procedures (methods) to
update the instances of load and status classes and it has not static neither dynamic properties.
Once scenario, load and status are defined, the event specifies a set of status changes following certain
logical statements, and it schedules one or more new events at a future time when necessary. In particular,
an event generating function takes as the input one or more occurrences of loads, and status, and it
modifies them by calling the status manipulating methods. As mentioned in Section 3, each generated
event encapsulates the entire procedure into a delegate object and puts into the FEL after appending a time
stamp corresponding to the scheduled execution time.
Each event can be decomposed into several sub-procedures that may happen at the same time, but the
execution of which depends on the status condition. In such cases, each sub-procedure is an atomized event
and it is triggered together with other atomized events for immediate execution. The example in Section
6 will clarify this point.
5

MAIN FUNCTIONALITIES

With respect to the provided definitions, a DES run instance, in order to be initialized, requires only two sets
of information: the scenario, i.e., the static components; and, for stochastic simulation, the random seed
(through the MathNet.Numerics library). O2 DES exposes several functionalities which were developed to
facilitate the integration of the models in an optimization environment and that we explain in the following.
•

•

•

Generation of multiple simulation models by defining multiple scenario instances. This feature
facilitates the integration of recursive stochastic search algorithms such as MO-COMPASS (Li et al.
2015), that samples alternative configurations of a base scenario to be simulated (examples to be
shown in Section 6);
Generation of multiple simulation replications by giving different random seeds as input, while
the scenario object remains the same across the replications. This functionality represents a
fundamental requirement to integrate techniques such as Optimal Computing Budget Allocation
to allocate replications to candidate solutions (i.e., scenario) in a sequential manner (Chen et al.
1997).
Concurrent simulation. Multiple simulation instances are simultaneously managed as a unique model
due to the synchronous central controller. It enables the utilization of Time Dilation (Schruben
1997) as an efficient variance reduction technique.

With O2 DES, both simulator and optimizer program can be compiled in the same executable, therefore
all communication are made through memory level and no file operation is required at all. In such a way,
the efficiency of the optimization infrastructure tremendously increases.

3519

Li, Zhu, Pedrielli, Pujowidianto, and Chen

Figure 4: Extended base class for DES model with support of time dilation.
5.1 Optimal Computing Budget Allocation
Although OCBA (Chen et al. 1997) has been proposed for nearly two decades, the technique has not been
integrated, to the knowledge of the authors, in any simulation package performing optimization tasks. In
fact, the integration of OCBA requires a control of the simulation execution: an initial simulation run is
conducted for all candidate solutions and the remaining budget is iteratively assigned to the promising
candidates sequentially allocating simulation replications as opposed to the traditional user–defined allocation
performed in commercial simulation packages. The proposed O2 DES could integrate OCBA making it
transparent to the user.
5.2 Time Dilation
The purpose of the concurrent simulation with time dilation (Schruben 1997) is similar to OCBA, i.e., to
improve the efficiency of the simulation optimization. However, it requires each underlying DES model
to expose its future events list and system clock to a centralized controller which works according to a
common clock time. The central controller is able to allocate the computational effort by executing more
often events from the events list of system configurations showing better performance.
O2 DES, as with OCBA, makes the implementation of time dilation particularly easy and transparent
to the user. Specifically, several fields and properties were defined at the level of the class of DES model:
we store the relevant information on the time series of frequencies in order to perform the updates (Figure
4). Two static functions (execute head event with time dilation, and run with time dilation) were defined
under the base class, taking as input multiple DES objects and selecting the different events list to simulate
based on their performance.
6

A CASE STUDY

In this section, we illustrate a case study on the aircraft spare part management to demonstrate how the
proposed modeling paradigm can be used for a real industrial problem. It is a complex scenario, where
many types of entities and their interactions are considered, and multiple operational rules are involved in
the formulating the process. Therefore, with O2 DES we have an explicit and systematic way to describe
the simulation model as following.

3520

Li, Zhu, Pedrielli, Pujowidianto, and Chen

Figure 5: Class diagram of the DES model for aircraft spare part management.
6.1 Simulation
Firstly, we define the static components of the DES in the left portion of the class diagram in Figure 5
represents all the static entities and their relationships. Herein, we defined the relationships between flight
schedules, their executing aircrafts and the spare-part rotables associated to the aircrafts, the airports
serving as both the origins and destinations of flights, as well as the location to store and repair the failures.
Since the model allows the spare-parts to be transferred between different airports, we also model the
logistic flights connecting pairs of the airports. Besides, the class diagram clearly shows the attributes of
each entity which are concerned with the DES model.
The right portion of Figure 5 represents, instead, the dynamic components of the model. As we can observe,
there are two types of logical entities with life-cycle (i.e., the loads) in the system, one is called the flight
job, i.e., an instance of flight trip, and the other is the repair job, i.e., the main entity to be processed in the
system. In addition, three sets of status variables were modeled, i.e., inventories, backorders, and repair
jobs for all types of spare–parts and at all airport locations. Correspondingly, we created three objects to
encapsulate each set of status variables and prepare the common updating methods for the event calls.
Figure 6 enumerates all the nine events defined in the model. Two of them on the right portion of the
digram are for the flight jobs; and the remaining seven events are for the repair jobs. The arrows show
the triggering relationships among all the events. Note that, among the nine events, the ones colored in
green utilize the encapsulated functions of the status variables (Figure 7) in which operational rules (as in
light blue) are define and could be replaced. To be more specific, we recognized the following nine main
events:
•

Depart: The a flight job is departing, it triggers an arrival event to be scheduled after its flight
hours. And if the flight job is forward direction, it also triggers a returning departure concerning
both flight hours and the stop over time, together with the next departure on the forward direction
concerning the flight frequency. For simplicity, we assume that the scheduled departure will not
be affected by any part failure;
3521

Li, Zhu, Pedrielli, Pujowidianto, and Chen

Figure 6: Code map of event generators for aircraft spare part management.
•

•

•

•

•

•

•

•

Arrive: When a flight job arrives, according to certain probabilities that are functions of the
flight hours and mean time to failure (MTTF) of each part, it triggers failures of parts which are
associated to the executing aircraft. Multiple repair jobs could be created and the “fail” event is
invoked immediately for each of them;
Fail: Once a part failure is observed, it tries to consume an inventory of corresponding spare part
at the local airport for replacement, then the failure is filled instantly. If not successful, it tries to
look for surrounding inventories considering certain rules (as in the right-top portion of Figure 7,
the method colored in light blue can be implemented by different rules), and find an inventory to
be consumed and transferred to the local airport, so that the failure is filled at a future time. In the
case of no finding, the repair job is pushed into a backorder locally for future process (right-bottom
portion of Figure 7). In any cases, the repair job triggers an event of “sent to repair” immediately
after the failure happens;
Send to Repair: The event calls a status encapsulated method to assign an airport to repair the failed
part (left portion of Figure 7, the method colored in light blue can be easily replaced). Basically,
the method look into the available repairing capacities at all the airports and the expected repairing
and transportation time, from which it selects the one costs the least. Then it sends the part and
schedule an event of “arrive to repair” for the underlying repair job;
Arrive to Repair: When a repair job arrives at the repairing location, it is pushed into the status
object for the repairs (left portion of Figure 7). Then, based on the availability of repairing capacity,
either it invokes a “start to repair” or it is joined into a local queue (also encapsulated in the status
object) for future process;
Start to Repair: The event triggers nothing more than a finishing event of the underlying repair job
called “send to replenish”, at a future time according to the mean time to repair (MTTR) of the
part;
Send to Replenish: The event pulls the repair job out of the status object. According the logistic
time, it schedules a “replenish” event in future. And if there is still repair job in the corresponding
queue, the “start to repair” event is invoked for it;
Replenish: Refer to the right portion of Figure 7. The replenished part will first look into the local
backorders, and make one unit of existing backorder filled by consuming itself. Otherwise, it is
pushed into the local inventory; and
Fill: The repair job is time stamped and archived.

3522

Li, Zhu, Pedrielli, Pujowidianto, and Chen

Figure 7: Code map of events with status objects for aircraft spare part management.
6.2 Optimization
The simulation model just presented can be used to efficiently solve the well-known spare parts allocation
problem. Specifically, in the spare parts allocation problem, the decision variables are Ii, j,t , t = 0, i.e.,
the amount of inventory to allocate to the site i, for the part type j, at time 0, i.e., the initial spare parts
inventory level for each part type. The objectives are typically two:
•
•

max ϑ1 , maximize the service level defined as the percentage of repair jobs which are re–filled
within a maximum interval specified as an attribute of its underlying part type;
min ϑ2 , minimize the total cost defined as the sum of capital costs for purchasing the spare–parts,
holding costs, logistic costs, repairing costs and penalty costs for excess of the maximum fill hours;

The optimization technique suggested exploits the functionalities of O2 DES. In fact, due to the possibility
of easily manipulating the scenario with the initial inventory level at all the airports as well as to access
the load information in run-time, both a stochastic selection procedure such as OCBA considering several
initial inventory configurations to choose the best, as well as a recursive procedure as MO–COMPASS can
be used in this scenario.
We firstly try a simple heuristic algorithm to solve the optimization problem. Starting from the solution
of zero inventories for each part at all locations, we abstract additional information from the simulator
to find out which part type and location cumulates the maximum number of unsatisfactoriness. Then in
the next solution, one additional inventory is allocated to the location for the specific part type. Until the
observed service level reaches 100%, we stop the procedure and report the Pareto solutions with either
higher service level or the lower cost. For all the three pilot run scenarios, the heuristic optimization can
be completed within a few minutes.
Then, we apply the MO-COMPASS (Li et al. 2015) as the optimizer. To form the integration, we
simply fetch the quantitative values sampled from the optimizer, and cast them into the Scenario object
which can be taken in by the simulator for evaluation. Then, the quantitative values from the simulation
results are fed back to the optimizer. The iteration repeats until satisfying results are obtained.

3523

Li, Zhu, Pedrielli, Pujowidianto, and Chen
It is observed that until the optimal performance converges, compared with the heuristic results with the
similar service level, the MO-COMPASS is able to further reduce the cost by 20%. However, it takes
longer time to complete the optimization.
7

CONCLUSION

This paper proposes a compact and flexible paradigm for building O2 DES model with C# programming
language. Such an effort results in an open toolkit for teaching DES simulation as well as simulation–based
optimization. The underlying framework as well as the three main components of the modeling paradigm are
discussed and the main functionalities enhancing the framework for simulation–optimization are reported
as well. O2 DES is demonstrated by a case study of aircraft spare part management problem.
With the case study, we also illustrated that the modeling paradigm facilitates the integration of the
simulator within a simulation optimization architecture, and make it feasible to incorporate advanced search
techniques as MO-COMPASS.
Future studies will focus on two areas. One is to apply the paradigm to model further complex
DES systems, e.g., health care, warehouse, transportation and other logistic problems. Another direction
is to enrich the general framework by integrating with more practical features concerning simulation–
optimization. We hope this study inspires a wide range of practical research on simulation modeling, and
benefit the industrial practitioners by providing them an efficient tool to analyze and improve productivities.
REFERENCES
Banks, J., and I. John S. Carson. 1986. “Introduction to discrete-event simulation”. In Proceedings of the
18th conference on Winter simulation, 17–23. 318253: ACM.
Chen, H.-C., C.-H. Chen, L. Dai, and E. Yucesan. 1997. “New development of optimal computing budget
allocation for discrete event simulation”. In Proceedings of the 1997 Winter Simulation Conference,
334 – 41.
Choi, B. K., and D. Kang. 2013. Modeling and Simulation of Discrete Event Systems. John Wiley & Sons.
Dahl, O.-J., and K. Nygaard. 1966. “SIMULA: an ALGOL-based simulation language”. Commun. ACM 9
(9): 671–678.
Fu, M. C. 2002, July. “Optimization for Simulation: Theory vs. Practice”. INFORMS Journal on Computing 14 (3): 192–215.
Fu, M. C., G. Bayraksan, S. G. Henderson, B. L. Nelson, W. B. Powell, I. O. Ryzhov, and B. Thengvall.
2014. “Simulation optimization: A panel on the state of the art in research and practice”. In Proceedings
of the 2014 Winter Simulation Conference, edited by A. Tolk, S. Y. Diallo, I. O. Ryzhov, L. Yilmaz,
S. Buckley, and J. A. Miller, 3696–3706. Piscataway, New Jersey: Institute of Electrical and Electronics
Engineers, Inc.
Hammann, J. E., and N. A. Markovitch. 1995. “Introduction to arena”. Winter Simulation Conference
Proceedings:519 – 523. Arena simulation system;Simulation analysis program;.
Kelton, W. D., R. P. Sadowski, and D. A. Sadowski. 1998. Simulation with ARENA, Volume 47.
WCB/McGraw-Hill New York.
Li, H., L. H. Lee, E. P. Chew, and P. Lendermann. 2015. “MO-COMPASS: A Fast Convergent Search
Algorithm for Multi-Objective Discrete Optimization via Simulation”. IIE Transactions (just-accepted).
Muller, D. 2011. “Automod - providing simulation solutions for over 25 years”. In Simulation Conference
(WSC), Proceedings of the 2011 Winter, 39–51.
Nance, R. E., and R. G. Sargent. 2002. “Perspectives on the Evolution of Simulation”. Operations Research 50
(1): 161–172.
Nelson, B. 2013. Foundations and methods of stochastic simulation: a first course, Volume 187. Springer
Science & Business Media.

3524

Li, Zhu, Pedrielli, Pujowidianto, and Chen
Nordgren, W. 2002. “Flexsim simulation environment”. In Simulation Conference, 2002. Proceedings of
the Winter, Volume 1, 250–252 vol.1.
Schriber, T. J., D. T. Brunner, and J. S. Smith. 2014. “Inside Discrete-event Simulation Software: How
It Works and Why It Matters”. In Proceedings of the 2014 Winter Simulation Conference, edited by
A. Tolk, S. Y. Diallo, I. O. Ryzhov, L. Yilmaz, S. Buckley, and J. A. Miller, 132–146. Piscataway,
New Jersey: Institute of Electrical and Electronics Engineers, Inc.
Schruben, L., and E. Yucesan. 1993. “Modeling paradigms for discrete event simulation”. Operations
Research Letters 13 (5): 265–75. 4525298 modeling paradigms discrete event simulation graph theory
simulation graphs model-based problem-solving environment.
Schruben, L. W. 1997. “Simulation optimization using simultaneous replications and event time dilation”.
In Proceedings of the 29th conference on Winter simulation, 177–180: IEEE Computer Society.
Sharda, B., S. J. Bury, J. Banks, and J. S. Carson. 2011. “Best practices for effective application of discrete
event simulation in the process industries Introduction to Discrete-Event Simulation.”. In Proceedings
of the 2011 Winter Simulation Conference (WSC 2011), 2315 – 24.
AUTHOR BIOGRAPHIES
HAOBIN LI is a scientist in Institute of High Performance Computing, under Agency for Science, Technology and Research (A*STAR) of Singapore. He received his B.Eng. degree (1st Class Honors) in 2009
from the Department of Industrial and Systems Engineering at National University of Singapore, with
minor in computer science; and Ph.D. degree from the same department in 2014. He has research interests
in operation research, simulation optimization and designing high performance optimization tools which
are ready for practical industrial use. His email address is lihb@ihpc.a-star.edu.sg.
YINCHAO ZHU is a research engineer in National University of Singapore. He received his B.Eng. degree
(1st Class Honors) and B.Sci in 2012 from Engineering Science Program and Department of Mathematics at
National University of Singapore. He is currently pursuing his PhD degree in the Department of Industrial
and Systems Engineering. He has research interests in operation research and simulation optimization. His
email address is zy@nus.edu.sg.
GIULIA PEDRIELLI is Research Fellow for the Centre for Maritime Studies at the National University
of Singapore. She received her Ph.D. degree from the Mechanical Engineering Department in Politecnico
di Milano in 2013 (with Honors). Her research focuses on stochastic simulation based-optimization in both
single and multiple–objectives framework. Her email address is cmsgp@nus.edu.sg.
NUGROHO A. PUJOWIDIANTO is an R&D Writing System Engineer in the Business Printing Division
at Hewlett-Packard Singapore. He received his B.Eng. (Mechanical Engineering) degree from Nanyang
Technological University in 2006 and his Ph.D. degree from the Department of Industrial and Systems
Engineering, National University of Singapore in 2013. His research interests include simulation optimization and its application in health care. His email address is nugroho@hp.com.
YIXIN CHEN is a final year undergraduate student in the Department of Industrial and Systems Engineering
at National University of Singapore. His email address is chen yixin29@u.nus.edu.

3525

