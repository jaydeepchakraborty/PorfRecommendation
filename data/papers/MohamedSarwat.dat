2013 IEEE 14th International Conference on Mobile Data Management

Social Networking and Mobility: A Data
Management Perspective
Mohamed Sarwat
Mohamed F. Mokbel
Department of Computer Science and Engineering, University of Minnesota
200 SE Union Street, Minneapolis, MN 55455, USA
sarwat@cs.umn.edu

mokbel@cs.umn.edu
PART I : Social Networking and Mobility: Why, What ?
• Social Networking and Mobility: Overview (5 minutes)
◦ The Effect of Social Networking on People’s Life.
◦ Mobility Changed the Computing Paradigm.
◦ Combining Social Networking and Mobility.
• The Rise of Location-based Social Networking (5 minutes)
◦ Overview of early LBSN attempts.
◦ The functionality of current LBSN systems.
◦ Is it all about the Check-In functionality?
• Location-based Social Networking: Data Trilogy (10 minutes)
◦ Social networking data
◦ Spatial/Spatio-Temporal Data
◦ Users Opinions Data
◦ Data trilogy to enrich the user experience.
• Combining Social Networking and Mobility: Perspectives (10 minutes)
◦ GeoSocial Search and Query Processing.
◦ GeoSocial Recommendation.
◦ GeoSocial Data Analytics.
◦ GeoSocial Media Visualization.
◦ GeoSocial Crowd-sourcing.

I. S EMINAR O UTLINE
Figures 1 and 2 give the seminar outline, which consists
of two main parts. The seminar presents the state-of-the-art
research, systems, and applications that combine both social
networking and mobility.
A. PART I: Social Networking and Mobility: Why, What ?
The outline of the first part is given in Figure 1. We start
by giving a quick overview of social networking services
(e.g., Facebook, Twitter), their evolution, and how they impact
the society [1], [2], [3], [4]. Similarly, we explain, through
examples and case studies, how the widespread of mobile
devices changed the computing paradigm in a way that impacted our daily life. We then illustrate how the marriage
of both social networking and mobility has led to the rise
of location-based social networking systems. We highlight
several attempts to combine social networking and mobility,
and then explain the functionality of current location-based
social networking systems, and we use Foursquare as a case
study. Therefore, we explain the richness of data generated
by merging both social networking and mobility [5], [6],
[7], [8]: (1) Social Networking data: represents the friendship
between different users as well as all sorts of social interactions
between users. (2) Spatial/Spatio-Temporal data: represents the
users geo-locations, venues (e.g., restaurant) geo-locations and
information about users visiting different places at different
times. (3) Opinions data: represents how much a user likes
the places she visits (e.g., Alice visited restaurant A and gave
it a rating of five). We then illustrate how different mixes of
this data trilogy are leveraged to explore new trends and build
novel applications [6], [7], [9], [10].

Fig. 1.

opinions data side by side in traditional recommender systems [9], [17], [18], [19]. We also highlight the research
works that leverage GeoSocial data points and trajectories for
travel and itinerary recommendations [20], [21]. (3) GeoSocial
Analytics: We give an overview of computational techniques
that analyze GeoSocial data [22], [23], [24], [25] to learn
more about human behavior, the societal and economical
consequences of such analysis [26], [27]. (4) GeoSocial Visualization: We motivate the need for visualizing GeoSocial Media
(e.g., Geo-Tagged Tweets, Geo-Tagged Videos) and present the
state-of-the-art GeoSocial visualization techniques [28], [29].
(5) GeoSocial Crowdsourcing: We give an overview of the
Volunteered Geographic Information (VGI) area and survey
the recent papers that address the Crowsourcing topic from
a geographic perspective [30], [31]. For all aforementioned
topics, we present results from recent research work, case
studies from hot mobile applications, and the anatomy of
built systems. Then, we highlight the main risks that result
from combining social networking and mobility (e.g., privacy).
Finally, we introduce possible research directions.

B. PART II: Combining Geo and Social: Different Perspectives
In the second part (outline in Figure 2), we present the stateof-the-art research that lies within the intersection of both social networking and mobility, from the following perspectives:
(1) GeoSocial Queries: That incorporates both the geo-location
and the social awareness in answering queries [11], [12], [13],
[14]. We present recent studies that show how both spatial
and social aspects can be combined to enhance the search
quality. (2) GeoSocial Recommendation: We present recent
studies which show that geo-location matters in recommender
systems [15], [16], and we manifest several techniques to
incorporate the spatial/spatio-temporal information and users
978-0-7695-4973-6/13 $26.00 © 2013 IEEE
DOI 10.1109/MDM.2013.102

Tutorial Outline: PART I (30 minutes).

II.

TARGET AUDIENCE

The seminar is targeted at mobile data and social networking
researchers, developers, and enthusiasts. Attending the seminar
does not require any prior knowledge as it starts by giving a
quick overview of both the social networking and mobility
concepts. This seminar lasts for 90 minutes and the detailed
5

[4] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. Mahoney, “Statistical
properties of community structure in large social and information
networks,” in WWW, 2008.
[5] M. Sarwat, J. Bao, A. Eldawy, J. J. Levandoski, A. Magdy, and M. F.
Mokbel, “Sindbad: A Location-based Social Networking System,” in
SIGMOD, 2012.
[6] Y. Cai and T. Xu, “Design, analysis, and implementation of a large-scale
real-time location-based information sharing system,” in MobiSys, 2008.
[7] Y. Chen, K. Jiang, Y. Zheng, C. Li, and N. Yu, “Trajectory simplification
method for location-based social networking services,” in GIS-LBSN,
2009.
[8] Y. Zheng, Y. Chen, X. Xie, and W.-Y. Ma, “Geolife2.0: A location-based
social networking service,” in MDM, 2009.
[9] K. Kodama, Y. Iijima, X. Guo, and Y. Ishikawa, “Skyline queries
based on user locations and preferences for making location-based
recommendations,” in GIS-LBSN, 2009.
[10] M. Ye, P. Yin, and W.-C. Lee, “Location recommendation for locationbased social networks,” in GIS, 2010.
[11] C. Chen, F. Li, B. C. Ooi, and S. Wu, “Ti: an efficient indexing
mechanism for real-time search on tweets,” in SIGMOD, 2011.
[12] C. C. CAO, J. She, Y. Tong, and L. Chen, “Whom to ask? jury selection
for decision making tasks on micro-blog services,” in VLDB, 2012.
[13] S. B. Roy and K. Chakrabarti, “Location-aware type ahead search on
spatial databases: semantics and efficiency,” in SIGMOD, 2011.
[14] J. R. Thomsen, M. L. Yiu, and C. S. Jensen, “Effective caching of
shortest paths for location-based services,” in SIGMOD, 2012.
[15] J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel, “LARS: A
Location-Aware Recommender System,” in ICDE, 2012.
[16] M. Sarwat, J. J. Levandoski, A. Eldawy, and M. F. Mokbel, “LARS*:
A Scalable and Efficient Location-Aware Recommender System,” in
TKDE, 2013.
[17] M.-H. Park et al, “Location-based recommendation system using
bayesian user’s preference model in mobile devices,” in UIC, 2007.
[18] Y. Takeuchi and M. Sugimoto, “An outdoor recommendation system
based on user location history,” in UIC, 2006.
[19] S. B. Roy et al, “Space efficiency in group recommendation,” VLDB
J., vol. 19, no. 6, pp. 877–900, 2010.
[20] Y. Zheng and X. Xie, “Learning travel recommendations from usergenerated GPS traces,” ACM Transactions on Intelligent Systems and
Technology (TIST), vol. 2, no. 1, p. 2, 2011.
[21] V. Zheng, Y. Zheng, X. Xie, and Q. Yang, “Collaborative Location and
Activity Recommendations with GPS History Data,” in WWW, 2010.
[22] E. Cho, S. A. Myers, and J. Leskovec, “Friendship and mobility: user
movement in location-based social networks,” in KDD, 2011.
[23] G. D. F. Morales, A. Gionis, and M. Sozio, “Social content matching
in mapreduce,” in VLDB, 2011.
[24] W. Lu, Y. Shen, S. Chen, and B. C. Ooi, “Efficient processing of k
nearest neighbor joins using mapreduce,” in VLDB, 2012.
[25] C. Zhang, L. Shou, K. Chen, G. Chen, and Y. Bei, “Evaluating geosocial influence in location-based social networks,” in CIKM, 2012.
[26] W. Lu and L. V. S. Lakshmanan, “Profit maximization over social
networks,” in ICDM, 2012.
[27] S. Bhagat, A. Goyal, and L. V. S. Lakshmanan, “Maximizing product
adoption in social networks,” in WSDM, 2012.
[28] A. D. Sarma, H. Lee, H. Gonzalez, J. Madhavan, and A. Y. Halevy,
“Efficient spatial sampling of large geographical tables,” in SIGMOD,
2012.
[29] M. D. Lieberman and H. Samet, “Supporting rapid processing and
interactive map-based exploration of streaming news,” in GIS, 2012.
[30] A. Efentakis, D. Theodorakis, and D. Pfoser, “Crowdsourcing computing resources for shortest-path computation,” in GIS, 2012.
[31] L. Kazemi and C. Shahabi, “Geocrowd: Enabling query answering with
spatial crowdsourcing,” in GIS, 2012.

PART II : Combining Geo & Social: Different Perspectives
• GeoSocial Search and Query Processing (10 minutes)
◦ GeoSocial News Feed Queries.
◦ GeoSocial Ranking Techniques.
◦ System Support for GeoSocial Query Processing.
• GeoSocial Recommendation Services (10 minutes)
◦ Location Recommendation.
◦ Travel Sequence/Itinerary Recommendation.
◦ Location-Aware Recommender Systems.
• GeoSocial Data Analytics (10 minutes)
◦ Analyzing Human Behavior
◦ Predicting GeoSocial Norms/Trends.
• GeoSocial Media Visualization (10 minutes)
◦ GeoSocial Visualization Techniques.
◦ Case Studies.
• GeoSocial Crowd-sourcing (10 minutes)
◦ Volunteered Geographic Information (VGI).
◦ Humans as Sensors.
◦ Case Studies.
• GeoScoial: Risks and Threats (5 minutes)
◦ User Geo-Location is sensitive Information.
◦ Protect User Privacy and Ensure Quality.
• Summary and Open Research Directions (5 minutes)

Fig. 2.

Tutorial Outline: PART II (60 minutes).

timing is given in Figures 1 and 2. By attending the seminar,
the audience are expected to get more familiar with the stateof-the-art research that lies in the intersection of both social
networking and mobility.
III.

B IOGRAPHY

Mohamed Sarwat is a doctoral candidate at the department of
Computer Science and Engineering, University of Minnesota. He
obtained his Bachelor’s degree in computer engineering from Cairo
University in 2007 and his Master’s degree in computer science from
University of Minnesota in 2011. His research interest lies in the
broad area of data and information management systems. Mohamed
has been awarded the University of Minnesota Doctoral Dissertation
Fellowship in 2012. His research work has been recognized by the
Best Research Paper Award in SSTD 2011.
Mohamed F. Mokbel (Ph.D., Purdue University, 2005, MS, B.Sc.,
Alexandria University, 1999, 1996) is an associate professor in the
Department of Computer Science and Engineering, University of
Minnesota. His main research interests focus on advancing the state
of the art in the design and implementation of database engines to
cope with the requirements of emerging applications (e.g., locationbased applications and sensor networks). His research work has
been recognized by three best paper awards at IEEE MASS 2008,
MDM 2009, and SSTD 2011. Mohamed is a recipient of the NSF
CAREER award 2010. He has actively participated in several program
committees for major conferences including ICDE, SIGMOD, VLDB,
SSTD, and ACM GIS. He is/was a program co-chair for ACM GIS
2008, 2009, and 2010. Mohamed is an ACM and IEEE member and
a founding member of ACM SIGSPATIAL.

R EFERENCES
[1]
[2]
[3]

C. Kadushin, Understanding Social Networks: Theories, Concepts, and
Findings. Oxford University Press, 2012.
D. Kempe, J. M. Kleinberg, and . Tardos, “Maximizing the spread of
influence through a social network,” in KDD, 2003.
J. Leskovec, D. P. Huttenlocher, and J. M. Kleinberg, “Predicting
positive and negative links in online social networks,” in WWW, 2010.

6

arXiv:1408.0325v1 [cs.SI] 2 Aug 2014

Matrix Factorization with Explicit Trust and Distrust
Relationships
Rana Forsati

Mehrdad Mahdavi

Shahid Beheshti University, G.C., Tehran, Iran

Michigan State University, Michigan, USA

r_forsati@sbu.ac.ir

mahdavim@cse.msu.edu

Mehrnoush Shamsfard

Mohamed Sarwat

Shahid Beheshti University, G.C., Tehran, Iran

University of Minnesota, Minneapolis, USA

m_shams@sbu.ac.ir

sarwat@cs.umn.edu

Abstract
With the advent of online social networks, recommender systems have became crucial for the
success of many online applications/services due to their significance role in tailoring these applications to user-specific needs or preferences. Despite their increasing popularity, in general recommender systems suffer from the data sparsity and the cold-start problems. To alleviate these issues,
in recent years there has been an upsurge of interest in exploiting social information such as trust
relations among users along with the rating data to improve the performance of recommender systems. The main motivation for exploiting trust information in recommendation process stems from
the observation that the ideas we are exposed to and the choices we make are significantly influenced by our social context. However, in large user communities, in addition to trust relations, the
distrust relations also exist between users. For instance, in Epinions the concepts of personal "web
of trust" and personal "block list" allow users to categorize their friends based on the quality of reviews into trusted and distrusted friends, respectively. Hence, it will be interesting to incorporate
this new source of information in recommendation as well. In contrast to the incorporation of trust
information in recommendation which is thriving, the potential of explicitly incorporating distrust
relations is almost unexplored. In this paper, we propose a matrix factorization based model for
recommendation in social rating networks that properly incorporates both trust and distrust relationships aiming to improve the quality of recommendations and mitigate the data sparsity and the
cold-start users issues. Through experiments on the Epinions data set, we show that our new algorithm outperforms its standard trust-enhanced or distrust-enhanced counterparts with respect to
accuracy, thereby demonstrating the positive effect that incorporation of explicit distrust information
can have on recommender systems.

1 Introduction
The huge amount of information available on the Web has made it increasingly challenging to cope
with this information overload and find the most relevant information one is really interested in. Recommender systems intend to provide users with recommendations of products they might appreciate,
taking into account their past ratings, purchase history, or interest. The recent proliferation of online
social networks have further enhanced the need for such systems. Therefore, it is obvious why such systems are indispensable for the success of many online applications such as Amazon, iTunes and Netflix
to guide the search process and help users to effectively find the information or products they are looking for [49]. Roughly speaking, the overarching goal of recommender systems is to identify a subset of
items (e.g. products, movies, books, music, news, and web pages) that are likely to be more interesting
to users based on their interests [13, 76, 16, 5].
In general, most widely used recommender systems (RS) can be broadly classified into contentbased (CB), collaborative filtering (CF), or hybrid methods [1]. In CB recommendation, one tries to
recommend items similar to those a given user preferred in the past. These methods usually rely on
the external information such as explicit item descriptions, user profiles, and/or the appropriate features extracted from items to analyze item similarity or user preference to provide recommendation.

1

In contrast, CF recommendation, the most popular method adopted by contemporary recommender
systems, is based on the core assumption that similar users on similar items express similar interest,
and it usually relies on the rating information to build a model out of the rating information in the past
without having access to external information required in CB methods. The hybrid approaches were
proposed that combine both CB and CF based recommenders to gain advantages and avoid certain
limitations of each type of systems [20, 64, 55, 48, 54, 67, 15].
The essence of CF lies in analyzing the neighborhood information of past users and items’ interactions in the user-item rating matrix to generate personalized recommendations based on the preferences of other users with similar behavior. CF has been shown to be an effective approach to recommender systems. The advantage of these types of recommender systems over content-based RS is that
the CF based methods do not require an explicit representation of the items in terms of features, but it
is based only on the judgments/ratings of the users. These CF algorithms are mainly divided into two
main categories [21]: memory-based methods (also known as neighborhood-based methods) [73, 9]
and model-based methods [26, 63, 65, 79]. Recently, another direction in CF considers how to combine memory-based and model-based approaches to take advantage of both types of methods, thereby
building a more accurate hybrid recommender system [56, 77, 32].
The heart of memory-based CF methods is the measurement of similarity based on ratings of items
given by users: either the similarity of users (user-oriented CF) [24], the similarity of items (itemsoriented CF) [61], or combined user-oriented and item-oriented collaborative filtering approaches to
overcome the limitations specific to either of them [74]. The user-oriented CF computes the similarity
among users, usually based on user profiles or past behavior, and seeks consistency in the predictions
among similar users [78, 26]. The item-oriented CF, on the other hand, allows input of additional itemwise information and is also capable of capturing the interactions among them. If the rating of an item
by a user is unavailable, collaborative-filtering methods estimate it by computing a weighted average of
known ratings of the items from the most similar users.
Memory-based collaborative filtering is most effective when users have expressed enough ratings
to have common ratings with other users, but it performs poorly for so-called cold-start users. Coldstart users are new users who have expressed only a few ratings. Thus, for memory based CF methods
to be effective, large amount of user-rating data are required. Unfortunately, due to the sparsity of the
user-item rating matrix, memory-based methods may fail to correctly identify the most similar users
or items, which in turn decreases the recommender accuracy. Another major issue that memory-based
methods suffer from is the scalability problem. The reason is essentially the fact that when the number of users and items are very large, which is common in many real world applications, the search to
identify k most similar neighbors of the active user is computationally burdensome. In summary, data
sparsity and non-scalability issues are two main issues current memory based methods suffer from.
To overcome the limitations of memory-based methods, model-based approaches have been proposed, which establish a model using the observed ratings that can interpret the given data and predict
the unknown ratings [1]. In contrast to the memory-based algorithms, model-based algorithms try
to model the users based on their past ratings and use these models to predict the ratings on unseen
items. In model-based CF the goal is to employ statistical and machine learning techniques to learn
models from the data and make recommendations based on the learned model. Methods in this category include aspect model [26, 63], clustering methods [30], Bayesian model [80], and low dimensional
linear factor models such as matrix factorization (MF) [66, 65, 79, 59]. Due to its efficiency in handling very huge data sets, matrix factorization based methods have become one of the most popular
models among the model-based methods, e.g. weighted low rank matrix factorization [65], weighted
nonnegative matrix factorization (WNMF) [79], maximum margin matrix factorization (MMMF) [66]
and probabilistic matrix factorization (PMF) [59]. These methods assume that user preferences can be
modeled by only a small number of latent factors [12] and all focus on fitting the user-item rating matrix
using low-rank approximations only based on the observed ratings. The recommender system we will
propose in this paper adhere to the model-based factorization paradigm.
Although latent factor models and in particular matrix factorization are able to generate high quality
recommendations, these techniques also suffer from the data sparsity problem in real-world scenarios
and fail to address users who rated only a few items. For instance, according to [61], the density of
non-missing ratings in most commercial recommender systems is less than one or even much less.
Therefore, it is unsatisfactory to rely predictions on such small amount of data which becomes more
challenging in the presence of large number of users or items. This observation necessitates tackling the
2

data sparsity problem in an affirmative manner to be able to generate more accurate recommendations.
One of the most prominent approaches to tackle the data sparsity problem is to compensate for the
lack of information in rating matrix with other sources of side information which are available to the
recommender system. For example, social media applications allow users to connect with each other
and to interact with items of interest such as songs, videos, pages, news, and groups. In such networks
the ideas we are exposed to and the choices we make are significantly influenced by our social context.
More specifically, users generally tend to connect with other users due to some commonalities they
share, often reflected in similar interests. Moreover, in many real-life applications it may be the case
that only social information about certain users is available while interaction data between the items
and those users has not yet been observed. Therefore, the social data accumulated in social networks
would be a rich source of information for the recommender system to utilize as side information to alleviate the data sparsity problem. To accomplish this goal, in recent years the trust-based recommender
systems became an emerging field to provide users personalized item recommendations based on the
historical ratings given by users and the trust relationships among users (e.g., social friends).
Social-enhanced recommendation systems are becoming of greater significance and practicality
with the increased availability of online reviews, ratings, friendship links, and follower relationships.
Moreover, many e-commerce and consumer review websites provide both reviews of products and a
social network structure among the reviewers. As an example, the e-commerce site Epinions [22] asks
its users to indicate which reviews/users they trust and use these trust information to rank the reviews
of products. Similar patterns can be found in online communities such as Slashdot in which millions of users post news and comment daily and are capable of tagging other users as friends/foes or
fans/freaks. Another example is the ski mountaineering site Moleskiing [3] which enables users to
share their opinions about the snow conditions of the different ski routes and also express how much
they trust the other users. Another well-known example is the FilmTrsut system [19], an online social
network that provides movie rating and review features to its users. The social networking component
of the website requires users to provide a trust rating for each person they add as a friend. Also users
on Wikipedia can vote for or against the nomination of others to adminship [7]. These websites have
come to play an important role in guiding users’ opinions on products and in many cases also influence their decisions in buying or not buying the product or service. The results of experiments in [11]
and of similar works confirm that a social network can be exploited to improve the quality of recommendations. From this point of view, traditional recommender systems that ignore the social structure
between users may no longer be suitable.
A fundamental assumption in social based recommender systems which has been adopted by almost all of the relevant literature is that if two users have friendship relation, then the recommendation
from his or her friends probably has higher trustworthiness than strangers. Therefore the goal becomes
how to combine the user-item rating matrix with the social/trust network of a user to boost the accuracy of recommendation system and alleviate the sparsity problem. Over the years, several studies have
addressed the issue of the transfer of trust among users in online social networks. These studies exploit
the fact that trust can be passed from one member to another in a social network, creating trust chains,
based on its propagative and transitive nature 1 . Therefore, some recommendation methods fusing social relations by regularization [29, 36, 42, 81] or factorization [41, 43, 59, 58, 65, 60, 57] were proposed
that exploit the trust relations in the social network.
Also, the results of incorporating the trust information in recommender systems is appealing and
has been the focus of many researchers in the last few years, but, in large user communities, besides the
trust relationship between users, the distrust relationships are also unavoidable. For example, Epinions
provided the feature that enables users to categorize other users in a personal web of trust list based
on their quality as a reviewer. Later on, this feature integrated with the concept of personal block list,
which reflects the members that are distrusted by a particular user. In other words, if a user encounters
a member whose reviews are consistently offensive, inaccurate, or otherwise low quality, she can add
that member to her block list. Therefore, it would be tempting to investigate whether or not distrust
information could be effectively utilized to boost the accuracy of recommender systems as well.
In contrast to trust information for which there has been a great research, the potential advantage/disadvantage of explicitly utilizing distrust information is almost unexplored. Recently, few
1 We note that while the concept of trust has been studied in many disciplines including sociology, psychology, economics, and
computer science from different perspectives, but the issue of propagation and transitivity have often been debated in literature
and different authors have reached different conclusions (see for example [62] for a thorough discussion)

3

attempts have been made to explicitly incorporate the distrust relations in recommendation process [22, 40, 69, 72], which demonstrated that the recommender systems can benefit from the proper
incorporation of distrust relations in social networks. However, despite these positive results, there
are some unique challenges involved in distrust-enhanced recommender systems. In particular, it has
proven challenging to model distrust propagation in a manner which is both logically consistent and
psychologically plausible. Furthermore, the naive modeling of distrust as negative trust raises a number of challenges- both algorithmic and philosophical. Finally, it is an open challenge how to incorporate trust and distrust relations in model-based methods simultaneously. This paper is concerned with
these questions and gives an affirmative solution to challenges involved with distrust-enhanced recommendation. In particular, the proposed method makes it possible to simultaneously incorporate both
trust and distrust relationships in recommender systems to increase the prediction accuracy. To the
best of our knowledge, this is the first work that models distrust relations into the matrix factorization
problem along with trust relations at the same time.
The main intuition behind the proposed algorithm is that one can interpret the distrust relations
between users as the dissimilarity in their preferences. In particular, when a user u distrusts another
user v, it indicates that user u disagrees with most of the opinions issued, or ratings made by user v.
Therefore, the latent features of user u obtained by matrix factorization must be as dissimilar as possible to v’s latent features. In other words, this intuition suggests to directly incorporate the distrust into
recommendation by considering distrust as reversing the deviation of latent features. However, when
combined with the trust relations between users, due to the contradictory role of trust and distrust relations in propagating social information in the matrix factorization process, this idea fails to effectively
capture both relations simultaneously. This statement also follows from the preliminary experimental
results in [69] for memory-based CF methods that demonstrated regarding distrust as an indication to
reverse deviations in not the right way to incorporate distrust.
To remedy this problem, we settle to a less ambitious goal and propose another method to facilitate
the learning from both types of relations. In particular, we try to learn latent features in a manner that
the latent features of users who are distrusted by the user u have a guaranteed minimum dissimilarity
gap from the worst dissimilarity of users who are trusted by user u. By this formulation, we ensure that
when user u agrees on an item with one of his trusted friends, he/she will disagree on the same item
with his distrusted friends with a minimum predefined margin. We note that this idea significantly
departs from the existing works in distrust-enhanced memory based recommender systems [69, 72],
that employ the distrust relations to either filter out or debug the trust relations to reduce the prediction
task to a trust-enhanced recommendation. In particular, the proposed method ranks the latent features
of trusted and distrusted friends of each user to reflect the effect of relation in factorization.
Summary of Contributions This work makes the following key contributions:
• A matrix factorization based algorithm for simultaneous incorporation of trust and distrust relationships in recommender systems. To the best of our knowledge, this is the first model-based
recommender algorithm that is able to leverage both types of relationships in recommendation.
• An efficient stochastic optimization algorithm to solve the optimization problem which makes
the proposed method scalable to large social networks.
• An empirical investigation of the consistency of the social relationships with rating information.
In particular, we examine to what extent trust and distrust relations between users are aligned
with the ratings they issued on items.
• An exhaustive set of experiments on Epinions data set to empirically evaluate the performance of
the proposed algorithm and demonstrate its merits and advantages.
• A detailed comparison of the proposed algorithm to the state-of-the-art trust/distrust enhanced
memory/model based recommender systems.
Outline The rest of this paper is organized as follows. In Section 2 we draw connections to and put our
work in context of some of the most recent work on social recommender systems. Section 3 formally

4

introduces the matrix factorization problem, an optimization based framework to solve it, and its extension to incorporate the trust relations between users. The proposed algorithm along with optimization
methods are discussed in Section 4. Section 5 includes our experimental result on Epinions data set
which demonstrates the merits of the proposed algorithm in alleviating data sparsity problem in rating
matrix and generating more accurate recommendations. Finally, Section 6 concludes the paper and
discusses few directions as future work.

2 Related Work on Social Recommendation
Earlier in the introduction, we discussed some of the main lines of research on recommender system;
here, we survey further lines of study that are most directly related to our work on social-enhanced
recommendation. Many successful algorithms have been developed over the past few years to incorporate social information in recommender systems. After reviewing trust-enhanced memory-based
approaches, we discuss some model-based approaches for recommendation in social networks with
trust relations. Finally, we review major approaches in distrust modeling and distrust-enhanced recommender systems.

2.1 Trust Enhanced Memory-based Recommendation
Social network data has been widely investigated in the memory-based approaches. These methods
typically explore the social network and find a neighborhood of users trusted (directly or indirectly) by a
user and perform the recommendation by aggregating their ratings. These methods use the transitivity
of trust and propagate trust to indirect neighbors in the social network [45, 47, 31, 27, 29, 28, 33].
In [45], a trust-aware collaborative filtering method for recommender systems is proposed. In this
work, the collaborative filtering process is informed by the reputation of users, which is computed by
propagating trust. [31] proposed a method based on the random walk algorithm to utilize social connection and other social annotations to improve recommendation accuracy. However, this method
does not utilize the rating information and is not applicable to constructing a random walk graph in
real data sets. TidalTrust [18] performs a modiÞed breadth first search in the trust network to compute
a prediction. To compute the trust value between user u and v who are not directly connected, TidalTrust aggregates the trust value between u’s direct neighbors and v weighted by the direct trust values
of u and its direct neighbors.
MoleTrust [45, 46, 80] does the same idea as TidalTrust, but MoleTrust considers all the raters up to a
fixed maximum-depth given as an input, independent of any specific user and item. The trust metric in
MoleTrust consists of two major steps. First, cycles in trust networks are removed. Therefore, removing
trust cycles beforehand from trust networks can significantly speed up the proposed algorithm because
every user only needs to be visited once to infer trust values. Second, trust values are calculated based
on the obtained directed acyclic graph by performing a simple graph random walk:
TrustWalker [27] combines trust-based and item-based recommendation to consider enough ratings without suffering from noisy data. Their experiments show that TrustWalker outperforms other
existing memory based approaches. Each random walk on the user trust graph returns a predicted
rating for user u on target item i . The probability of stopping is directly proportional to the similarity
between the target item and the most similar item j , weighted by the sigmoid function of step size k.
The more the similarity, the greater the probability of stopping and using the rating on item j as the
predicted rating for item i . As the step size increases, the probability of stopping decreases. Thus ratings by closer friends on similar items are considered more reliable than ratings on the target item by
friends further away.
We note that all these methods are neighborhood-based methods which employ only heuristic algorithms to generate recommendations. There are several problems with this approach. The relationship
between the trust network and the user-item matrix has not been studied systematically. Moreover,
these methods are not scalable to very large data sets since they may need to calculate the pairwise
user similarities and pairwise user trust scores.

5

2.2 Trust Enhanced Model-based Recommendation
Recently, researchers exploited matrix factorization techniques to learn latent features for users and
items from the observed ratings and fusing social relations among users with rating data as will be
detailed in Section 3. These methods can be divided into two types: regularization-based methods
and factorization-based methods. Here we review some existing matrix factorization algorithms that
incorporate trust information in the factorization process.
2.2.1 Regularization based Social Recommendation
Regularization based methods typically add regularization term to the loss function and minimize it.
Most recently, Ma [42] proposed an idea based on social regularized matrix factorization to make recommendation based on social network information. In this approach, the social regularization term is
added to the loss function, which measures the difference between the latent feature vector of a user
and those of his friends. The probability model similar to the model in [42] is proposed by Jamali [29].
The graph Laplacian regularization term of social relations is added into the loss function in [36] and
minimizes the loss function by alternative projection algorithm. Zhu et a l. [81] used the same model
in [36] and built graph Laplacian of social relations using three kinds of kernel functions. In [37], the
minimization problem is formulated as a low-rank semidefinite optimization problem.
2.2.2 Factorization based Social Recommendation
In factorization-based methods, social relationship between users are represented as social relation
matrix, which is factored as well as the rating matrix. The loss function is the weighted sum of the social
relation matrix factorization error and the rating matrix factorization error. For instance, SoRec [41]
incorporates the social network graph into probabilistic matrix factorization model by simultaneously
factorizing the user-item rating matrix and the social trust networks by sharing a common latent lowdimensional user feature matrix [37]. The experimental analysis shows that this method generates better recommendations than the non-social filtering algorithms [28]. However, the disadvantage of this
work is that although the usersÕ social network is integrated into the recommender systems by factorizing the social trust graph, the real world recommendation processes are not reflected in the model.
Two sets of different feature vectors are assumed for users which makes the interpretability of the model
very hard [28, 39]. This drawback not only causes lack of interpretability in the model, but also affects
the recommendation qualities. A better model named Social Trust Ensemble (STE) [39] is proposed
by the same authors, by making the latent features of a user’s direct neighbors affect the rating of the
user. Their method is a linear combination of basic matrix factorization approach and a social network
based approach. Experiments show that their model outperforms the basic matrix factorization based
approach and existing trust based approaches. However, in their model, the feature vectors of direct
neighbors of u affect the ratings of u instead of affecting the feature vector of u. This model does not
handle trust propagation. Another method for recommendation in social networks has been proposed
in [40]. This method is not a generative model and defines a loss function to be minimized. The main
disadvantage of this method is that it punishes the users with lots of social relations more than other
users. Finally, SocialMF [28] is a matrix factorization based model which incorporates social influence
by making the features of every user depend on the features of his/her direct neighbors in the social
network.

2.3 Distrust Enhanced Social Recommendation
In contrast to incorporation of trust relations, unfortunately most of the literature on social recommendation totally ignore the potential of distrust information in boosting the accuracy of recommendations. In particular, only recently few work started to investigate the rule of distrust information in
recommendation process both from theoretical and empirical viewpoints [22, 84, 51, 82, 40, 75, 69, 71,
68, 72]. Although these studies have shown that distrust information can be plentiful, but there is a
significant gap in clear understanding of distrust in recommender systems. The most important reasons for this shortage are the lack of data sets that contain distrust information and dearth of a unified
consensus on modeling and propagation of distrust.

6

Table 1: Summary of notations consistently used in the paper and their meaning.
Symbol
Meaning
U = {u 1 , · · · , u n }, n
I = {i 1 , · · · , i m }, m
k
R ∈ Rn×m
ΩR , |ΩR |
U ∈ Rn×k
V ∈ Rm×k
S ∈ {−1, +1}n×n
ΩS , |ΩS |
W ∈ Rn×n
+
N (u) ⊆ [n]
N+ (u) ⊆ [n]
N− (u) ⊆ [n]
D : Rk × Rk → R+

The set of users in system and the number of users
The set of items and the number of items
The dimension of latent features in factorization
The partially observed rating matrix
The set of observed entires in rating matrix and its size
The matrix of latent features for users
The matrix of latent features for items
The social network between n users
The set of extracted triplets from the social relations and its size
The pairwise similarity matrix between users
Neighbors of user u in the social graph
The set of trusted neighbors by user u in the social graph
The set of distrusted neighbors by user u in the social graph
The measurement function used to assess the similarly of latent features

A formal framework of trust propagation schemes, introducing the formal and computational treatment of distrust propagation has been developed in [22]. In an extension of this work, [82] proposed
clever adaptations in order to handle distrust and sinks such as trust decay and normalization. In [75], a
trust/distrust propagation algorithm called CloseLook is proposed, which is capable of using the same
kinds of trust propagation as the algorithm proposed by [22]. [34] extended the results by [22] using a
machine-learning framework (instead of the propagation algorithms based on an adjacency matrix)
to enable the evaluation of the most informative structural features for the prediction task of positive/negative links in online social networks. A comprehensive framework that computes trust/distrust
estimations for user pairs in the network using trust metrics is build in [71]: given two users in the trust
network, we can search for a path between them and propagate the trust scores along this path to obtain an estimation. When more than one path is available, we may single out the most relevant ones
(selection), and aggregation operators can then be used to combine the propagated trust scores into
one final trust score, according to different trust score propagation operators.
[40] was the first seminal work to demonstrate that the incorporation of distrust information could
be beneficial based on a model-based recommender system. In [71] and [72] the same question is
addressed in memory-based approaches. In particular, [72] embarked upon the distrust-enhanced recommendation and showed that with careful incorporation of distrust metric, distrust-enhanced recommender systems are able to outperform their trust-only counterparts. The main rational behind the
algorithm proposed in [72] is to employ the distrust information to debug or filter out the users’ propagated web of trust. It is also has been realized that the debugging methods must exhibit a moderate
behavior in order to be effective. [68] addressed the problem of considering the length of the paths that
connect two users for computing trust-distrust between them, according to the concept of trust decay.
This work also introduced several aggregation strategies for trust scores with variable path lengths
Finally we note that the aforementioned works try to either model or utilize the trust/distrust information. In recent years there has been an upsurge of interest in predicting the trust and distrust
relations in a social network [34, 14, 4, 53]. For instance, [34] casts the problem as a sign prediction
problem (i.e., +1 for friendship and -1 for opposition) and utilizes machine learning methods to predict
the sign of links in the social network. In [14] a new method is presented for computing both trust and
distrust by combining an inference algorithm that relies on a probabilistic interpretation of trust based
on random graphs with a modified spring-embedding algorithm to classify an edge. Another direction
of research is to examine the consistency of social relations with theories in social psychology [8, 35].
Our work significantly departs from these works on prediction or consistency analysis of social relations, and aims to effectively incorporate the distrust information in matrix factorization for effective
recommendation.

7

3 Matrix Factorization based Recommender Systems
This section provides a formal definition of collaborative filtering, the primary recommendation
method we are concerned with in this paper, followed by solution methods for low-rank factorization
that are proposed in the literature to address the problem.

3.1 Matrix Factorization for Recommendation
In collaborative filtering we assume that there is a set of n users U = {u 1 , · · · , u n } and a set of m items
I = {i 1 , · · · , i m } where each user u i expresses opinions about a set of items. In this paper, we assume
opinions are expressed through an explicit numeric rating (e.g., scale from one to five), but other rating
methods such as hyperlink clicks are possible as well. We are mainly interested in recommending a
set of items for an active user such that the user has not rated these items before. To this end, we are
aimed at learning a model from the existing ratings, i.e., offline phase, and then use the learned model to
generate recommendations for active users, i.e., online phase. The rating information is summarized in
an n × m matrix R ∈ Rn×m , 1 ≤ i ≤ n, 1 ≤ j ≤ m where the rows correspond to the users and the columns
correspond to the items and (p, q)th entry is the rate given by user u p to the item i q . We note that the
rating matrix is partially observed and it is sparse in most cases.
An efficient and effective approach to recommender systems is to factorize the user-item rating
matrix R by a multiplicative of k-rank matrices R ≈ UV> , where U ∈ Rn×k and V ∈ Rm×k utilize the
factorized user-specific and item-specific matrices, respectively, to make further missing data prediction. The main intuition behind a low-dimensional factor model is that there is only a small number of
factors influencing the preferences, and that a user’s preference vector is determined by how each factor applies to that user. This low rank assumption makes it possible to effectively recover the missing
entires in the rating matrix from the observed entries. We note that the celebrated Singular Value Decomposition (SVD) method to factorize the rating matrix R is not applicable here due to the fact that the
rating matrix is partially available and we are only allowed to utilize the observed entries in factorization process. There are two basic formulations to solve this problem: these are optimization based (see
e.g., [57, 37, 41, 33]) and probabilistic [50]. In the following subsections, we first review the optimization
based framework for matrix factorization and then discuss how it can be extended to incorporate trust
information.

3.2 Optimization based Matrix Factorization
Let ΩR be the set of observed ratings in the user-item matrix R ∈ Rn×m , i.e.,
ΩR = {(i , j ) ∈ [n] × [m] : R i j has been observed},
where n is the number of users and m is the number of items to be rated. In optimization based matrix
factorization, the goal is to learn the latent matrices U and V by solving the following optimization
problem:
#
"
´2 λ
λV
1 X ³
U
>
R i j −Ui ,: V j ,: +
kUkF +
kVkF ,
(1)
min L (U, V) =
U,V
2 (i , j )∈ΩR
2
2
where k · kF is the Frobenius norm of a matrix, i.e, kAkF =

qP P
n
m
i =1

j =1 |A i j |

2.

The optimization prob-

lem in (1) constitutes of three terms: the first term aims to minimize the inconsistency between the
observed entries and their corresponding value obtained by the factorized matrices. The last two terms
regularize the latent matrices for users and items, respectively. The parameters λU and λV are regularization parameters that are introduced to control the regularization of latent matrices U and V,
respectively. We would like to emphasize that the problem in (1) is non-convex jointly in both U and V.
However, despite its non-convexity, the formulation in (1) is widely used in practical collaborative filtering applications as the performance is competitive or better as compared to trace-norm minimization,
while scalability is much better. For example, as indicated in [33], to address the Netflix problem, (1)
has been applied with a fair amount of success to factorize data sets with 100 million ratings.

8

3.3 Matrix Factorization with Trust Side Information
Recently it has been shown that just relying on the rating matrix to build a recommender system is
not as accurate as expected. The main reason for this claim is the known cold-start users problem and
the sparsity of rating matrix. Cold-start users are one of the most important challenges in recommender
systems. Since cold-start users are more dependent on the social network compared to users with more
ratings, the effect of using trust propagation gets more important for cold-start users. Moreover, in
many real life systems a very large portion of users do not express any ratings, and they only participate
in the social network. Hence, using only the observed ratings does not allow to learn the user features.
One of the most prominent approaches to tackle the data sparsity problem in matrix factorization
is to compensate the lack of information in rating matrix with other sources of side information which
are available to the recommender system. It has been recently shown that social information such as
trust relationship between users is a rich source of side information to compensate for the sparsity. The
above mentioned traditional recommendation techniques are all based on working on the user-item
rating matrix, and ignore the abundant relationships among users. Trust-based recommendation usually involves constructing a trust network where nodes are users and edges represent the trust placed on
them. The goal of a trust-based recommendation system is to generate personalized recommendations
by aggregating the opinions of other users in the trust network. The intuition is that users tend to adopt
items recommended by trusted friends rather than strangers, and that trust is positively and strongly
correlated with user preferences. Recommendation techniques that analyze trust networks were found
to provide very accurate and highly personalized results.
To incorporate the social relations in the optimization problem formulated in (1), few papers [40, 29,
42, 37, 81] proposed the social regularization method which aims at keeping the latent vector of each
user similar to his/her neighbors in the social network. The proposed models force the user feature
vectors to be close to those of their neighbors to be able to learn the latent user features for users with
no or very few ratings [29]. More specifically, the optimization problem becomes as:
L (U, V) =

´2 λ
λV
1 X ³
U
kUkF +
kVkF
R i j −Ui>,: V j ,: +
2 (i , j )∈ΩR
2
2
°
°
°
n °
X
λS X
1
°
°
U j ,: °,
+
°Ui ,: −
°
2 i =1 °
|N (i )| j ∈N (i )

(2)

where λS is the social regularization parameter and N (i ) is the subset of users who has relationship
with i th user in the social graph.
The rationale behind social regularization idea is that every user’s taste is relatively similar to the
average taste of his friends in the social network. We note that using this idea, latent features of users
indirectly connected in the social network will be dependent and hence the trust gets propagated. A
more reasonable and realistic model should treat all friends differently based on how similar they are.
Let assume the weight of relationship between two users i and j is captured by Wi j where W ∈ Rn×n
demotes the social weight matrix. It is easy to extend the model in (2) to treat friends differently based
on the weight matrix W as:
L (U, V) =

´2 λ
1 X ³
λV
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
°
°
P
°
n °
λS X
j ∈N (i ) Wi j U j ,: °
°
+
°
°Ui ,: − P
°
2 i =1 °
j ∈N (i ) Wi j

(3)

An alternative formulation is to regularize each users’ fiends individually, resulting in the following objective function [42]:
L (U, V) =

´2 λ
1 X ³
λV
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
+

n
°
°2
λS X
Wi j °Ui ,: −U j ,: ° .
2 i , j =1

where we simply assumed that for any j ∉ N (i ), Wi j = 0.
9

As mentioned earlier, the objective function in L (U, V) is not jointly convex in both U and V but it
is convex in each of them fixing the other one. Therefore, to find a local solution one can stick to the
standard gradient descent method to find a solution in an iterative manner as follows:
Ut +1 ← Ut − η t ∇U L (U, V)|U=Ut ,V=Vt ,
Vt +1 ← Vt − η t ∇V L (U, V)|U=Ut ,V=Vt .

4 Matrix Factorization with Trust and Distrust Side Information
In this section we describe the proposed algorithm for social recommendation which is able to incorporate both trust and distrust relationships in the social network along with the partially observed rating
matrix. We then present two strategies to solve the derived optimization problem, one based on the
gradient descent optimization algorithm which generates more accurate solutions but it is computationally cumbersome, and another based on the stochastic gradient descent method which is computationally more efficient for large rating and social matrices but suffers from slow convergence rate.

4.1 Algorithm Description
As discussed before, the vast majority of related work in the field of matrix factorization for recommendation has primarily focussed on trust propagation and simply ignore the distrust information between
users, or intrinsically, are not capable of exploiting it. Now, we aim at developing a matrix factorization
based model for recommendation in social rating networks to utilize both trust and distrust relationships. We incorporate the trust/distrust relationship between users in our model to improve the quality
of recommendations. While intuition and experimental evidence indicate that trust is somewhat transitive, distrust is certainly not transitive. Thus, when we intend to propagate distrust through a network,
questions about transitivity and how to deal with conflicting information abound.
To inject social influence in our model, the basic idea is to find appropriate latent features for users
such that each user is brought closer to the users she/he trusts and separated apart from the users
that she/he distrusts and have different interests. We note that simply incorporating this idea in matrix
factorization by naively penalizing the similarity of each user’s latent features to his distrusted friends’
latent features fails to reach the desired goal. The main reason is that distrust is not as transitive as trust,
i.e. distrust can not directly replace trust in trust propagation approaches and utilizing distrust requires
careful consideration (trust is transitive, i.e., if user u trusts user v and v trusts w, there is a good chance
that u will trust w, but distrust is certainly not transitive, i.e., if u distrusts v and v distrusts w, then w
may be closer to u than v or maybe even farther away). It is noticeable that this statement is consistent
with the preliminary experimental results in [69] for memory-based CF methods that indicate regarding
distrust as an indication to reverse deviations in not the right way to incorporate distrust. Therefore we
pursue another approach to model the distrust in recommendation process.
The main intuition behind the proposed framework stems from the observation that the trust relations between users can be treated as agreement on items and distrust relations can be considered as
disagreement on items. Then, the question becomes how can we guarantee when a user agrees on an
item with one of his/her friends, he/she will disagree on the same item with his/her distrusted friends
with a reasonable margin. We note that this margin should be large enough to make it possible to distinguish between two types of friends. In terms of latent features, this observation translates to having
a margin between the similarity and dissimilarity of users’ latent features to his/her trusted and distrusted friends.
Alternatively, one can view the proposed method from the viewpoint of connectivity of latent features in a properly designated graph. Intuitively, certain features or groups of features should influence
how users connect in the social network, and thus it should be possible to learn a mapping from features to connectivity in the social network such that the mapping respects the underlying structure of
the social network. In the basic matrix factorization algorithm for recommendation, we can consider
the latent features as isolated vertices of a graph where there is no connection between nodes. This can
be generalized to the social-enhanced setting by considering the social graph as the underlying graph
between latent features with two types of edges (i.e., trust and distrust relations correspond to positive

10

(a) User trust netwrok

(b) User distrust netwrok

(c) Partially observed rating matrix

(d) Illustration of learned latent features

Figure 1: A simple example with seven users {u 1 , u 2 , · · · , u 7 } and six items {i 1 , i 2 , · · · , i 6 } to illustrate the
main intuition behind the proposed algorithm. The inputs of the algorithm are (a) trust network, (b)
distrust network, and (c) partially observed rating matrix R, respectively. As shown in (d) for user u 1 ,
the learned latent features for all his trusted friends {u 2 , u 4 , u 6 , u 7 } are closer to u 1 ’s latent features than
his distrusted friends {u 3 , u 5 } with a margin of 1.
and negative edges, respectively). Now the problem reduces to learning the latent features for each user
u such that users trusted by u in the social network (with positive edges) are close and users which are
distrusted by u (with negative edges) are more distant. Learning latent features in this manner respects
the inherent topology of the social network.
Figure 1 shows an example to illustrate the intuition behind the mentioned idea. For ease of exposition, we only consider the latent features for the user u 1 . From the trust network in Figure 1 (a) we
can see that user u 1 trusts the list of users N+ = {u 2 , u 4 , u 6 , u 7 } and from the distrust network in Figure 1
(b) we see that user u 1 distrusts the list of users N− = {u 3 , u 5 }. The goal is to learn the latent features
that obeys two goals, i) it minimizes the prediction error on observed entries in the rating matrix, ii) it
respects the underlying structure of the trust and distrust networks between users. In Figure 1 (d) the
latent features are depicted in the Euclidean space from the viewpoint of user u 1 . As shown in Figure 1
(d), for user u 1 , the latent features of his/her trusted friends N+ lie inside the solid circle centered at u 1
and the latent features of his/her distrusted friends N− lie outside the dashed circle. The gap between
two circles guarantees that always there exists a safe margin between u 1 ’s agreements with his trusted
and distrusted friends. One simple way to impose these constraints on the latent features of users is to
generate a set of triplets for any combination of trusted and distrusted friends ( e.g., one such triplet
for user u 1 can be constructed as (u 1 , u 2 , u 5 )) and force the margin constraint to hold for all extracted
triplets. This ensures that the minimum margin gap will definitely exist between the latent features
of all the trusted and distrusted friends as desired and makes it possible to incorporate both types of

11

relationships between users in the matrix factorization.
It is worthy to mention that similar to the social-enhanced recommender systems discussed before, the proposed algorithm is also based on hypotheses about the existence and the correlation of
trust/distrust relations and ratings in the data. The empirical investigation of correlation between social
relations and rating information has been the focus of a bulk of recent research including [83, 53, 38],
where the results reinforce the hypothesis that ratings from trusted people count more than those from
others and in particular distrusted neighbors. We have also conducted experiments as will be detailed
in Subsection 5.5, to empirically investigate the correlation/alignment between social relations and the
rating information issued by users which supports our strategy in exploiting the trust/distrust relations
in matrix factorization.
We now formalize the proposed solution. As the first ingredient, we need a measure to evaluate the
consistency between the latent features of users, i.e., the matrix U, and the trust and distrust constraints
existing between users in the social network. To this end, we introduce a monotonically increasing
convex loss function `(z) to measure the discrepancy between the latent features of different users. Let
u i , u j , and u k be three users in the model such that u i trusts u j but distrusts u k . The main intuition
behind the proposed framework is that the latent features of u i , i.e., Ui ,: must be more similar to u j ’s
latent features than latent features for user u k . For each such a triplet we penalize the objective function
by `(D(Ui ,: ,U j ,: ) − D(Ui ,: ,Uk,: )) where the function D : Rk × Rk 7→ R+ measures the similarity between
two latent vectors assigned to two different users, and ` : R 7→ R+ is a penalty function that is utilized
to assess the violation of latent vectors of trusted and distrusted users. Example loss functions include
hinge loss `(z) = max(0, 1−z) and logistic loss `(z) = log(1+e −z ) which are widely used convex surrogate
of 0-1 loss function in learning community.
Let ΩS denote the set of extracted triplets from the social relations, i.e.,
©
ª
ΩS = (i , j , k) ∈ [n] × [n] × [n] : S i j = 1 & S i k = −1 .
Here, a positive relationship means friends or a trusted relationship and a negative relationship means
foes or a distrust relationship. Then, our goal becomes to find a factorization of matrix R such that
the learned latent features of users are consistent with the constraints in ΩS where the consistency is
reflected in the loss function. This results in the following optimization problem:
L (U, V) =

´2 λ
λV
1 X ³
U
kUkF +
kVkF
R i j −Ui>,: V j ,: +
2 (i , j )∈ΩR
2
2
+

X
λS
`(D(Ui ,: ,U j ,: ) − D(Ui ,: ,Uk,: )).
|ΩS | (i , j ,k)∈ΩS

(4)

Let us make the above general formulation more specific by setting `(·) and D(·, ·) to be the hinge loss
and the Euclidian distance, respectively. Under these two assumptions, the objective can be formulated
as:
L (U, V) =

´2 λ
1 X ³
λV
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
{z
}
|
R(U,V)

X
¢
¡
λS
+
max 0, 1 − kUi ,: −U j ,: k2 + kUi ,: −Uk,: k2 .
|ΩS | (i , j ,k)∈ΩS

(5)

Here the constraints have been written in terms of hinge-losses over triplets, each consisting of a user,
his/her trusted friend and his/her distrusted friend. Solving the optimization problem in (5) outputs
the latent features for users and items that can utilized to estimate the missing values in the user-item
matrix. Comparing the formulation in (5) to the existing factorization-based methods discussed earlier reveals two main features of the proposed formulation. First, it aims to minimize the error on the
observed ratings and to respect the inherent structure of the social network among the users. The tradeoff between these two objectives is captured by the regularization parameter λS which is required to be
tuned effectively.

12

Algorithm 1 GD based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, ΩS
2: Output: U and V
3: for t = 1, . . . , T do
4:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
5:
Compute ∇U by Eq. 7
6:
Compute ∇V by Eq. 8
7:
Update:
Ut +1 = Ut − η t ∇U |U=Ut ,V=Vt
Vt +1 = Vt − η t ∇V |U=Ut ,V=Vt
8:
9:

end for
return UT +1 and VT +1 .

In a similar way, applying the logistic loss to the general formulation in (4) yields the following objective:
L (U, V) =

´2 λ
1 X ³
λV
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
+

X
¡
¡
¢¢
λS
log 1 + exp kUi ,: −Uk,: k2 − kUi ,: −U j ,: )k2 .
|ΩS | (i , j ,k)∈ΩS

(6)

Remark 1. We note that in several applications of recommender systems, besides the observed ratings, a
description of the users and/or the objects through attributes (e.g., gender, age) or measures of similarity
is available that could potentially benefit the process of recommendation (see e.g. [2] for few interesting
applications). In that case it is tempting to take advantage of both known ratings and descriptions to
model the preferences of users. A natural way to incorporate the available meta-data is to kernalize the
similarity measure between latent features based on a positive definite kernel between pairs that can be
deduced from the meta-data. More specifically, instead of simply using Euclidian distance as the similarity measure between latent features in (5), we can use the kernel matrix K obtained from the Laplacian of
the graph obtained from the meta-data to measure the similarity as:
¢
¡
¢> ¡
D(Ui ,: ,U j ,: ) = Ui ,: −U j ,: K Ui ,: −U j ,: ,
P
where K = (D − W)−1 , with D as a diagonal matrix with D i ,i = nj=1 Wi j . Here W captures the pairwise
weight between users in the similarity graph between users that is computed based on the available metadata about users.
Remark 2. We would like to emphasize that it is straightforward to generalize the proposed framework
to incorporate similarity and dissimilarity information between items. What we need is to extract the
triplets from the trust/distrust links between items and repeat the same process we did for users. This
will add another term to the objective in terms of latent features of items V as shown in the following
generalized formulation:
L (U, V) =

´2 λ
λV
1 X ³
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
+

X
¡
¢
λS
max 0, 1 − kUi ,: −U j ,: k2 + kUi ,: −Uk,: k2
|ΩS | (i , j ,k)∈ΩS

+

X
¡
¢
λI
max 0, 1 − kVi ,: − V j ,: k2 + kVi ,: − Vk,: k2 ,
|ΩI | (i , j ,k)∈ΩI

where λI is the regularization parameter and ΩI is the set of triplets extracted from the similar/dissimilar
links between items. The similarity/dissimilarity links between items can be constructed according to tags
issued by users or associated with items, and categories. For example, if two items are attached with a
13

same tag, there is a trust link between them and otherwise distrust link. Alternatively, trust/distrust links
can be extracted by measuring similarity/dissimilarity based on the item properties or profile if provided.
This can further improve the accuracy of recommendations.

4.2 Batch Gradient Descent based Optimization
In optimization for supervised machine learning, there exist two regimes in which popular algorithms
tend to operate: the stochastic approximation regime, which samples a small data set per iteration,
typically a single data point, and the batch or sample average approximation regime, in which larger
samples are used to compute an approximate gradient. The choice between these two extremes outlines the well-known tradeoff between inexpensive noisy steps and expensive but more reliable steps.
Two preliminary examples of these regimes are the Gradient Descent (GD) and the Stochastic Gradient
Descent (SGD) methods, respectively. Both GD and SGD methods starts with some initial point, and
iteratively updates the solution using the gradient information at intermediate solutions. The main
difference is that GD requires a full gradient information at each iteration while SGD only requires an
unbiased estimate of the full gradient which can be done by sampling
We now discuss the application of GD algorithm to solve the optimization problem in (5) as detailed
in Algorithm 1. Recall that the objective function is not jointly convex in both U and V. On the other
hand, the objective is convex in one parameter by fixing the other one. Therefore, we follow an iterative
method to minimize the objective. At each iteration, first by fixing V, we take a step in the direction of
the negative gradient for U and repeat the same process for V by fixing U.
For the ease of exposition, we introduce further notation. For any triplet (i , j , k) ∈ ΩS we note that
the kUi ,: −U j ,: k2 − kUi ,: −Uk,: k2 can be written as Tr(CU> U) where Tr(·) denotes the trace of the input
matrix and C is a sparse auxiliary matrix defined for each triplet with all entries equal to zero except:
Ci k = Cki = C j j = 1 and Ckk = Ci j = C j i = −1. Having defined this notation, we can write the objective
in (5) as:

L (U, V) = R(U, V) +

´
³
X
λV
λS
λU
kUkF +
kVkF +
max 0, 1 − Tr(Ckij U> U) .
2
2
|ΩS | (i , j ,k)∈ΩS

where Ckij is the C matrix defined above which is associated with triplet (i , j , k). To apply the GD
method, we need to compute the gradient of L (U, V) with respect to U and V which we denote by
∇U = ∇U L (U, V) and ∇V = ∇V L (U, V), respectively. We have:

∇U = ∇U R(U, V) + λU U −

X
λS
k
1
(UCk>
k
>
i j + UCi j )
|ΩS | (i , j ,k)∈ΩS [Tr(Ci j U U)<1]

(7)

where 1[·] is the indicator function which takes a value of one if its argument is true, and zero otherwise.
Similarly for ∇V we have:
∇V = ∇V R(U, V) + λV V

(8)

The main shortcoming of GD method is its high computational cost per iteration due to the gradient
computation (i.e., step (7)) which is expensive when the size of social constraints ΩS is large. We note
that the size of ΩS can be as large as O(n 3 ) by considering all triplets in the social graph. In the next subsection we provide an alternative solution to resolve this issue using the stochastic gradient descent and
mini-batch SGD methods which are more efficient than the GD method in terms of the computational
cost per iteration but with a slow convergence rate in terms of target approximation error.

4.3 Stochastic and Mini-batch Optimization
As discussed above, when the size of social network is very large, the size of ΩS may cause computational problems in solving the optimization problem in (5) using GD method. The reason is essentially
the fact that computing the gradient at each iteration requires to go through all the triplets in ΩS which
is infeasible for large networks. To alleviate this problem we propose a stochastic gradient based [52]

14

Algorithm 2 Mini-SGD based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, ΩS , min batch size B
2: Output: U and V
3: for t = 1, . . . , T do
4:
∇t ← 0
5:
for b = 1, . . . , B do
6:
(i , j , k) ← Sample random triplet from ΩS
7:
if (1 − kUi ,: −U j ,: )k2 + kUi ,: −Uk,: k2 > 0) then
8:
∇t ← Ut Ckij U>
t
9:
end if
10:
end for
11:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
12:
Update:
¶
µ
λS
∇t
Ut +1 = Ut − η t ∇U R(Ut , Vt ) + λU Ut +
B |ΩS |
13:

Update:
Vt +1 = Vt − η t (∇V R(Ut , Vt ) + λV Vt )

14:
15:

end for
return UT +1 and VT +1 .

method to solve the optimization problem. The main idea is to choose a fixed subset of triplets for
gradient computation instead of all |ΩS | triplets at each iteration [10]. More specifically, at each iteration, we sample B triplets uniformly at random from ΩS to compute the next solution. We note that
this strategy generates unbiased estimates of the true gradient and makes each iteration of algorithm
computationally more efficient compared to the full gradient counterpart. In the simplest case, SGD
algorithm, only one triplet is chosen at each iteration to generate an unbiased estimate of the full gradient. We note that in practice SGD is usually implemented based on data shuffling, i.e., making the
sequence of the training samples random and then training the model by going through the training
samples one by one. An intermediate solution, known as mini-batch SGD, chooses a subset of triplets
to compute the gradient. The promise is that by selecting more triplets at each iteration, on one hand
the variance of stochastic gradients decreases promotional to the number of sampled triplets, and on
the other hand the algorithm enjoys the light computational cost of basic SGD method.
The detailed steps of the algorithm are shown in Algorithm 2. The mini-batch SGD method improves the computational efficiency by grouping multiple constraints into a mini-batch and only updating the U and V once for each mini-batch. For brevity, we will refer to this algorithm as Mini-SGD.
More specifically, the Mini-SGD algorithm, instead of computing the full gradient over all triplets, samples B triplets uniformly at random from ΩS where 1 ≤ B ≤ |ΩS | is a parameter that needs to be provided
to the algorithm, and computes the stochastic gradient as:
∇t =

λS
B

X
(i , j ,k)∈ΩB

1[Tr(Ck

ij

k
(UCk>
i j + UCi j )
U>
t Ut )<1]

where ΩB is the set of B sampled triplets from ΩS . We note that
E[∇t ] =

X
λS
k
1
(UCk>
k
>
i j + UCi j ),
|ΩS | (i , j ,k)∈ΩS [Tr(Ci j Ut Ut )<1]

i.e., ∇t is an unbiased estimate of the full gradient in the right hand side. When B = |ΩS |, each iteration
handles the original objective function and Mini-SGD reduces to the batch GD algorithm. We note that
both GD and SGD share the same convergenceprate in terms of number of iterations in expectation for
non-smooth optimization problems (i.e., O(1/ T ) after T iterations), but SGD method requires much
less running time to convergence compared to the GD method due to the efficiency of its individual
iterations.

15

5 Experimental Results
In this section, we conduct exhaustive experiments to demonstrate the merits and advantages of the
proposed algorithm. We conduct the experiments on the well-known Epinions 2 data set, aiming to
accomplish and answer the following fundamental questions:
1. Prediction accuracy: How does the proposed algorithm perform in comparison to the stateof- the-art algorithms with/without incorporating trust and distrust relationships between users.
Whether or not the trust/distrust social network could help in making more accurate recommendations?
2. Correlation of social relations with rating information: To what extent, the trusted and distrusted friends of a user u are aligned with the ratings the user u issued for the reviews written by
his friends? A positive answer to this question indicates that two users will issue similar (dissimilar) ratings if they are connected by a trust (distrust) relation and prefer to behave similarly.
3. Model selection: What role do the regularization parameters λS , λU and λV play in the accuracy
of the proposed recommender system and what is the best strategy to tune these parameters?
4. Handling cold-start users: How does exploiting social relationships in prediction process affect
the performance of recommendation for cold-start users?
5. Trading trust for distrust: To what extent the distrust relations can compensate for the lack of
trust relations?
6. Efficiency of optimization: What is the trade-off between accuracy and efficiency by moving
from the gradient descent to the stochastic gradient descent with different batch sizes?
In the following subsections, we intend to answer these questions. We begin by introducing the data set
we use in our experiemnts and the metrics we employ to evaluate the results, followed by the detailed
experimental results.

5.1 Data Set Description and Experimental Setup
The Epinions data set We begin by discussing the data set we have chosen for our experiments. To
evaluate the proposed algorithm on trust and distrust-aware recommendations, we use the Epinions
data set [22], a popular e-commerce site and customer review website where users share opinions on
various types of items such as electronic products, companies, and movies, through writing reviews
about them or assigning a rating to the reviews written by other users. The rating values in Epinions are
discrete values ranging from Ònot helpfulÓ (1/5) to Òmost helpfulÓ (5/5). These ratings and reviews
would potentially influence future customers when they are about to decide whether a product is worth
buying or a movie is worth watching.
Epinions allows users to evaluate other users based on the quality of their reviews, and to make
trust and distrust relations with other users in addition to the ratings. Every member of Epinions can
maintain a "trust" list of people he/she trusts that is referred to as web of trust (social network with trust
relationships) based on the reviewers with consistent ratings or "distrust" list known as block list (social
network with distrust relationships) that presents reviewers whose reviews were consistently found to
be inaccurate or low quality. The fact that the data set contains explicit positive and negative relations
between users makes it very appropriate to study issues in trust- and distrust-enhanced recommender
systems. Epinions is thus an ideal source for experiments on social recommendation. We remark that
the Epinions data set only contains bivalent relations (i.e., contains only full trust and full distrust, and
no gradual statements).
To conduct the coming experiments, we sampled a subset of Epinions data set with n = 121, 240
users and m = 685, 621 different items. The total number of observed ratings in the sampled data set is
12,721,437 which approximately includes 0.02% of all entries in the rating matrix R which demonstrates
the sparsity of the rating matrix. We note that the selected items are the most frequently rated overall.
The statistics of the data set is given in Table 2. The social statistics of the this data source is summarized
2 http://www.trustlet.org/wiki/Epinions_datasets

16

Table 2: Statistics of sample data from Epinions data set used in our experiments.
Statistic
Quantity
Number of users
121,240
Number of items
685,621
Number of ratings
12,721,437
Number of trust relations
481,799
Number of distrust relations
96,823
Minimum number of ratings by users
1
Minimum number of ratings for items
1
Maximum number of ratings by users
148735
Maximum number of ratings for items
945
Average number of ratings by users
85.08
Average number of ratings for items
15.26

Table 3: Maximum and average trust and distrust relations for users in the sampled data set.
Statistics
Trust per user
Be Trusted per user
Max
Min
Average

1983
1
4.76

2941
0
4.76

Max
Min
Average

Distrust per user
1188
1
0.91

Be Distrusted per user
429
0
0.91

in Table 3. The frequencies of ratings for users is shown are Table 4. In the user distrust network, the
total number of issued distrust statements is 96,823. As to the user trust network, the total number of
issued trust statements is 481,799.
Experimental setup To better evaluate the effect of utilizing the social side information in recommendation accuracy, we employ different amount of training data 90%, 80% , 70% and 60% to create
four different training sets that are increasingly sparse but the social network remains the same in all of
them. Training data 90%, for example, means we randomly select 90% of the ratings from the sampled
Epinions data set as the training data to predict the remaining 10% of ratings. The random selection
was carried out 5 times independently to have a fair comparison. Also, since our preliminary results
on a smaller data set revealed that the hinge loss performs better than the exponential loss, in the rest
of experiments we stick to this loss function. However, we note the exponential loss is slightly faster in
optimizing the corresponding objective function thanks to its smoothness, but it was negligible considering its worse accuracy compared to the hinge loss. All implementations are in Matlab, and all
experiments were performed on a 4-core 2.0 GHZ of a load-free machine with a 12G of RAM.

5.2 Metrics
5.2.1 Metrics for rating prediction
We employ two well-known measures, the Mean Absolute Error (MAE) and the Root Mean Squared
Error (RMSE) [25] to measure the prediction accuracy of the proposed approach in comparison with
other basic collaborative filtering and trust/distrust-enhanced recommendation methods.
MAE is very appropriate and useful measure for evaluating prediction accuracy in offline tests [25,
45]. To calculate MAE, the predicted rating is compared with the real rating and the difference (in absolute value) considered as the prediction error. Then, these individual errors are averaged over all predictions to obtain the overall MAE value. More precisely, let T denote the set of ratings to be predicted,

17

Table 4: The frequencies of user’s rating.
# of Ratings
# of Users

0-10
4,198,074 (≈ 33%)

11-20
3,053,144 (≈ 24%)

21-30
2,289,858 (≈ 18%)

31-40
1,526,572 (≈ 12%)

41-50
534,300 (≈ 4.2%)

267,1

# of Ratings
# of Users

61-70
157,745 (≈ 1.24%)

71-80
143,752 (≈ 1.13%)

81-90
104,315 (≈ 0.82%)

91-100
43,252 (≈ 0.34%)

101-200
21,626 (≈ 0.17%)

2
10,68

i.e., T = {(i , j ) ∈ [n]×[m], R i j needs to be predicted} and let R̂ denote the prediction matrix obtained by
algorithm after factorization. Then,
P
MAE =

(i , j )∈T

|R i j − R̂ i j |

|T |

,

where R i j is the real rating assigned by the user i to the item j , and R̂ i j is the rating user i would assign
to the item j that is predicted by the algorithm .
The RMSE metric is defined as:
v
uP
¡
¢2
u
t (i , j )∈T R i j − R̂ i j
.
RMSE =
|T |
The first measure (MAE) considers every error of equal value, while the second one (RMSE) emphasizes larger errors. We would like to emphasize that even small improvements in RMSE are considered
valuable in the context of recommender systems. For example, the Netflix prize competition offered a
1,000,000 reward for a reduction of the RMSE by 10% [72].
5.2.2 Metrics for evaluating the correlation of ratings with trust/distrust relations
As part of our experiments, we investigate how the explicit trust/distrust relations between users in
the social network are aligned with the implicit trust/distrust relations between users conveyed from
the rating information. We use recall, Mean Average Precision (MAP) [44] and Normalized Discount
Cumulative Gain (NDCG) to evaluate the ranking results. Recall is defined as the number of relevant
friends divided by the total number of friends in the social network. Precision is defined as the number
of relevant friends (trusted or distrusted) divided by the number of friends in the social network. Given
a user u, let r i be the relevance score of the friend ranked at position i , where r i = 1 if the user is relevant
to the u and r i = 0 otherwise. Then we can compute the Average Precision (AP) as
P
r i × Precision@i
.
AP = i
# of relevant friends
MAP is the average of AP over all the users in the network.
NDCG is a normalization of the Discounted Cumulative Gain (DCG) measure. DCG is a weighted
sum of the degree of relevancy of the ranked users. The weight is a decreasing function of the rank
(position) of the user, and therefore called discount. NDCG normalizes DCG by the Ideal DCG (IDCG),
which is simply the DCG measure of the best ranking result. Thus NDCG measure is always a number
in [0, 1]. NDCG at position k is defined as:
NDCG@k = Zk

k
X
2r i − 1
i =1 log(i + 1)

where k is also called the scope, which means the number of top-ranked users presented to the user
and Zk is chosen such that the perfect ranking has a NDCG value of 1. We note that the base of the
logarithm does not matter for NDCG, since constant scaling will cancel out due to normalization. We
will assume it is the natural logarithm throughout this paper.

18

Figure 2: Grid Search to find the best values for λU and λC on the data set with 90% of rating information.

5.3 Model Selection
Tuning of parameters (a.k.a model selection in learning community) is a critical problem in most of the
learning problems. In some situations, the learning performance may drastically vary with different
choices of the parameters. There are three parameters in objective (5) that play very important role in
the effectivity of the proposed algorithm. These are λU , λV , and λS . Between these, λS controls how
much the proposed algorithm should incorporate the information of the social network in completing
the partially observed rating matrix. In the extreme case, a very small value for λS , the algorithm almost forgets the social information exists between the users and only utilizes the observed user-item
rating matrix for factorization. On the other hand, if we employ a very large value for λS , the social
network information will dominate the learning process, leading to a poorer performance. Therefore,
in order to not hurt the recommendation performance, we need to find a reasonable value for social
regularization parameter. To this end, we analyze how the combination of these parameters affect the
recommendation performance.
We conduct a grid search on the potential values of two parameters λS and λV to find the combination with best performance. Figure 2 shows the grid search results for these parameters on data set with
90% of training data where the optimal prediction accuracy is achieved at point (14.8, 11) with the optimal RMSE = 1.12. We would like to emphasize that we have done the cross validation for only pairs of
(λS , λV ) and (λS , λU ) because, (i) considering the grid search for the triplet (λS , λU , λV ) is computationally burdensome, (ii) and our preliminary experiments showed that λV and λU behave similarly with
respect to λS . Based on the results reported in Figure 2, in the remaining experiments, we set λS = 14.8,
λV = 11, and λU = 13 when the training is performed on the data set with 90% of rating information.
We repeat the same process to find out the best setting of regularization parameters for other data sets
with 80%, 70%, and 60% of rating data as well.

5.4 Baseline Methods
Here we briefly discuss the baseline algorithms that we intend to compare the proposed algorithm.
The baseline algorithms are chosen from both types of memory-based and model-based recommender
systems with different types of trust and distrust relations. In particular, we consider the following basic
algorithms:
• MF (matrix factorization based recommender): this is the basic matrix factorization based rec19

ommender formulated in the optimization problem in (1) which does not take the social data
into account.
• MF+T (matrix factorization with trust information): to exploit the trust relations between users
in matrix factorization, [40] relied on the fact that the distance between latent features of users
who trust each other must be minimized that can be formulated as the following objective:
min
U

n
X
1X
D(Ui ,: ,U j ,: ),
2 i =1 j ∈N+ (i )

where N+ (i ) is the set of users the i th user trusts in the social network (i.e., S i j = +1). By employing this intuition in the basic formulation in (1), [40] solves the following optimization problem:
"
#
´2 α X
n
X
1 X ³
λU
λV
>
min
R i j −Ui ,: V j ,: +
D(Ui ,: ,U j ,: ) +
kUkF +
kVkF .
U,V 2 (i , j )∈Ω
2 i =1 j ∈N+ (i )
2
2
R
• MF+D (matrix factorization with distrust information): the basic intuition behind the algorithm
proposed in [40] to exploit the distrust relations is as follows: if user u i distrusts user u j , then we
can assume that their corresponding latent features Ui ,: and U j ,: would have a large distance. As
a result we aim to maximize the following quantity for all users:
max
U

n
X
1X
D(Ui ,: ,U j ,: ),
2 i =1 j ∈N− (i )

where N− (i ) denotes the set of users the i th users distrusts (i.e, S i j = −1). Adding this term to the
basic optimization problem in (1) we obtain the following optimization problem:
"
#
´2 β X
n
X
1 X ³
λU
λV
>
min
D(Ui ,: ,U j ,: ) +
kUkF +
kVkF .
R i j −Ui ,: V j ,: −
U,V 2 (i , j )∈Ω
2 i =1 j ∈N− (i )
2
2
R
• MF+TD (matrix factorization with trust and distrust information): this algorithm stands for the
algorithm proposed in the present work. We note that there is no algorithm in the literature that
exploits both trust and distrust relations in factorization process simultaneously.
• NB (neighborhood-based recommender): this algorithm is the basic memory-based recommender algorithm that predicts a rating of a target item i for user u using a combination of the
ratings of neighbors of u (similar users) that already issued a rating for item i . Formally,
P
W 0 (R ui − R̄ u )
u 0 ∈N (u),Wuu 0 >0 uu
R̂ ui = R̄ u +
,
(9)
P
Wuu 0
u 0 ∈N (u),W 0
uu

where the pairwise weight Wuu 0 between pair of users (u, u 0 ) is calculated by Pearson’s correlation
coefficient [25]
• NB+T (neighborhood with trust information) [45, 17, 47]: the basic idea behind the trust based
recommender systems proposed in TidalTrsut [17] and MoleTrsut [45] is to limit the set of neighbors in (9) to the users who are trusted by user u. The distinguishing feature of these algorithms
is the mechanism of trust propagation to estimate the trust transitively for all the users. By adapting (9) to only consider trustworthy neighbors in predicting the new ratings we obtain:
P
W 0 (R ui − R̄ u )
u 0 ∈N+∗ (u),Wuu 0 >0 uu
,
(10)
R̂ ui = R̄ u +
P
W 0
u 0 ∈N ∗ (u),W 0 >0 uu
+

uu

where N+∗ (u) is the set of trusted neighbors of u in the social network with propagated trust relations (when there is no propagation we have N+∗ (u) = N+ (u)). We note that instead of Pearson’s
correlation coefficient as the wighting schema, we can infer the weights exploiting the social relation between the users. Since for the data set we consider in our experiments, the trust/distrust
relations are binary values, the social based pairwise distance would be simply the hamming distance between the binary vector representation of social relations of users. For implementation
details we refer to [70, Chapter 6].
20

Table 5: The consistency of implicit and explicit trust relations in the data set for different ranges of
ratings measured in terms of NDCG, recall, and MAP.
# of Ratings
NDCG@10 NDCG@20
Recall@10 Recall@20 Recall@40
MAP
0-20
21-40
41-60
61-80
≥ 81

0.083
0.108
0.117
0.120
0.135

0.078
0.103
0.112
0.117
0.126

0.054
0.080
0.083
0.088
0.091

0.092
0.125
0.128
0.132
0.151

0.156
0.198
0.225
0.230
0.253

0.140
0.190
0.208
0.230
0.244

Table 6: The consistency of implicit and explicit distrust relations in the data set for different ranges of
ratings measured in terms of NDCG, recall, and MAP.
# of Ratings
NDCG@10 NDCG@20
Recall@10 Recall@20 Recall@40
MAP
0-20
21-40
41-60
61-80
≥ 81

0.065
0.071
0.082
0.089
0.104

0.057
0.068
0.072
0.078
0.096

0.045
0.060
0.075
0.081
0.087

0.071
0.077
0.085
0.105
0.125

0.132
0.140
0.158
0.164
0.191

0.130
0.134
0.152
0.160
0.183

• NB+TD-F (neighborhood with trust information and distrust information as filtration) [69, 72]:
a simple strategy to use distrust relations in the recommendation is to filter out distrusted users
from the list of neighbors in predicting the ratings. As a result, we adapt (9) to exclude distrusted
users from the users’ propagated web of trust.
• NB+TD-D (neighborhood-based with trust information and integrated distrust information) [69,
72]: in the same spirit as the filtration strategy, we can use distrust relations to debug the trust
relations. More specifically, if user u trusts user v, v trusts w, and u distrusts w, then the latter
distrust relation contradicts the propagation of the trust from u to w and can be excluded from
the prediction. In this method distrust is used to debug the trust relations.

5.5 On the Consistency of Social Relations and Rating Information
As already mentioned, the Epinions website allows users to write reviews about products and services
and to rate reviews written by other users. Epinions also allows users to define their web of trust, i.e.
"reviewers whose reviews and ratings have been consistently found to be valuable" and their block list,
i.e. "reviewers whose reviews are found to be consistently inaccurate or not valuableÓ. Different intuitions on interpreting these social information will result in different models. The main rational behind
incorporating trust and distrust relations in recommendation process is to take the trust/distrust relations between users in the social network as the level of agreement between ratings assigned to reviews
by users 3 . Therefore, investigating the consistency or alignment between user ratings (implicit trust)
and trust/distrust relations in the social network (explicit trsut) become an important issue.
Here, we aim to empirically investigate whether or not there is a correlation between a user’s current
trustees/friends or distrusted friends and the ratings that user would assign to reviews issued by his
neighbors. Obviously, if there is no correlation between social context of a user and his/her ratings to
reviews written by his neighbors, then the social structure does not provide any advantage to the rating
information. On the other hand, if there exists such a correlation, then the social context could be
supplementary information to compensate for the lack of rating information to boost the accuracy of
recommendations.
The consistency of trust relations and rating information issued by users on the reviews written
by his trustees has been analyzed in [83, 23]. However, [83] also claimed that social trust (i.e., explicit
trust) and similarity between users based on their issued ratings (i.e., implicit trust) are not the same,
3 In the literature the similarity between users conveyed from the rating information issued by users and the direct relation in
the social network are usually referred to as the implicit and the explicit trust, respectively.

21

Table 7: The alignment rate of users in establishing trust/distrust relationships with future users in the
social network based on the majority vote of their current trusted/distrusted friends. The number of
trusted friends (+) and distrusted friends (-) are denoted by n + and n − , respectively. Here u denotes the
current user and w stands for a future user in the network.
Setting
Type of Relation (u
w) % of Relations Alignment Rate (%)
n+ > n−
n+ < n−
n + = n − > 0 or n + = n − = 0

+
+
+

48.80
2.54
1.15
8.02
39.49

92.09
8.15
17.88
83.42
-

and can be used complementary. According to [38], when comparing implicit social information with
explicit social information, the performance of using implicit information is slightly worse. We further
investigate the same question about the consistency of distrust relations and ratings issued by users to
their distrusted neighbors. The positive answer to this question can be interpreted as follows. Given
that user u is interested in item i , the chances that v, trusted (distrusted) by u, also likes this item i is
much higher (lower) than for user w not explicitly trusted (distrusted) by u.
To measure the similarity between users, there are several methods we can borrow in the literature.
In this paper, we adopt the most popular approach that is referred to as Pearson correlation coefficient
(PCC) P : U × U 7→ [−1, +1] [6, 47], which is defined as:
Pm
i =1 (R ui − R̄ u )(R vi − R̄ v )
, ∀u, v ∈ U ,
P (u, v) = qP
Pm
m
2
2
j =1 (R ui − R̄ u ) × j =1 (R vi − R̄ v )
where R̄ u and R̄ v are the average of ratings issued by users u and v, respectively. The PCC measures
the extent to which there is a linear relationship between the rating behaviors of the two users, the extreme values being -1 and 1. The similarity of two users becomes negative when users have completely
diverging ratings. We note that this quantity can be considered as the implicit trust between users that
is conveyed via ratings given by users.
To conduct this set of experiments, we first group all the users in the training data set based on
the number of ratings, and then measure the prediction accuracies of different user groups. Users are
grouped into five classes: "[1, 20)", "[20, 40)", "[40, 60)", "[60, 80)", and "> 81 ". In order to have a
comprehensive view of the ranking performance, we present the NDCG, recall and MAP scores of trust
and distrust alignments on the Epinions data set in Table 5 and Table 6, respectively. We note that
the data set we use in our experiments only contains bivalent trust values, i.e., -1 and +1, and it is not
possible to have an ordering on the list of friends (timestamp of relations would be an option to order
the friends but unfortunately it is not available in our data set). To compute the NDCG, we use the
ordering of trusted/distrusted friends which yields the best value.
On the positive side, we observe a clear trend of alignment between ratings assigned by a user and
the type of relation he has made in the social network. This observation coincides with our intuition.
Overall, when more ratings are observed for a user, the similarity calculation process will find more
accurate similar or dissimilar neighbors for this user since we have more information to represent or
interpret this user. Hence, by increasing the number of ratings, It is conceivable from the results in Tables 5 and 6 that the alignment between implicit and explicit neighbors becomes better. By comparing
the results in Tables 5 and 6 we can see that trust relations are slightly better aligned than the distrust
relations.
On the negative side, the results show that the NDCG on both types of relations is small. One explanation for this phenomenon is that the Epinions data set is not tightly bound to a specific application.
For example, a user may trust or distrust anther user based on his/her comments on a specific product
but they might have similar taste on other products. Furthermore, compared to other data sets such as
FilmTrusts, the Epinions data set is very sparse data set, and consequently it is relatively inaccurate to
rely on the rating information to compute the implicit trust relations. Finally, our approach to distinguish trust/distrust lists from the rating information is limited by the PCC trust metric we have utilized.
We conjecture that better trust metrics that is able to exploit other side information such as time and in22

Table 8: The accuracy of prediction of matrix factorization with three different methods measured in
terms of MAE and RMSE errors. The parameter k represents the number of latent features in factorization.
k
% of Training Measure
MF
MF+T
MF+D
MF+TD
10

60%
70%
80%
90%

20

60%
70%
80%
90%

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9813 ± 0.042
1.6050 ± 0.032
0.9462 ± 0.083
1.5327 ± 0.032
0.9150± 0.022
1.3824 ± 0.032
0.8921 ± 0.025
1.2166 ± 0.017

0.8561 ± 0.032
1.4125 ± 0.022
0.8332 ± 0.092
1.2407 ± 0.063
0.8206 ± 0.041
1.1906 ± 0.042
0.8158 ± 0.016
1.1403 ± 0.027

0.9720 ± 0.038
1.5036 ± 0.040
0.9241 ± 0.012
1.4405 ± 0.023
0.8722 ± 0.034
1.3155 ± 0.026
0.8736 ± 0.053
1.1869 ± 0.049

0.8310 ± 0.016
1.2294 ± 0.086
0.8206 ± 0.023
1.1562 ± 0.043
0.8113 ± 0.032
1.1061 ± 0.021
0.8025 ± 0.014
1.0872 ± 0.020

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9972 ± 0.016
1.6248 ± 0.014
0.9688 ± 0.019
1.5162 ± 0.016
0.9365 ± 0.025
1.4081 ± 0.015
0.9224 ± 0.016
1.2207 ± 0.0 18

0.8431 ± 0.018
1.3904 ± 0.042
0.8342 ± 0.062
1.2722 ± 0.027
0.8172 ± 0.011
1.1853 ± 0.023
0.8128 ± 0.021
1.1402 ± 0.026

0.9746 ± 0.060
1.5423 ± 0.046
0.9350 ± 0.022
1.4540 ± 0.075
0.8705 ± 0.016
1.3591 ± 0.073
0.8805 ± 0.032
1.1933 ± 0.028

0.8475 ± 0.012
1.1837 ± 0.023
0.8290 ± 0.034
1.1452 ± 0.016
0.8129 ± 0.025
1.1049 ± 0.082
0.8096 ± 0.010
1.0851 ± 0.011

teractional information would be helpful in distinguishing implicit trusted/distrusted friends, leading
to better alignment between implicit and explicit trust relations.
We also conduct experiments to evaluate the consistency of social network only based on the
trust/distrust relations between users. In particular, we investigate to what extent a users’ relations
are aligned with the opinion of his/her neighbors in the social network. More specifically, let u be a
user who is about to make a trust or distrust relation to another user v. We assume that n + number
of u’s neighbors trust v and n − number of u’s neighbors distrust v. We note that in the real data set
the distrust relations are hidden. To conduct this set of experiments, we randomly sample 30% of the
relations from the social network and use the remaining 70% to predict the type of sampled relations 4
by majority voting.
Table 7 shows the results on the consistency of social relations. We observe that in all cases there
is an alignment between the opinions of users’ friends and his/her own relation (92.09% and 83.42%
when the majority of friends trust and distrust the target user, respectively). This might be due to social
influence of people on social network, however, it is hard to justify the existence of such a correlation in
Epinions data set which includes reviews for diverse set of products and taste of users. One interesting
observation from the results reported in Table 7 is the case where the number of distrusted users dominates the number of trusted users (i.e., n − > n + ). While the distrust relations are private to other users,
but we can see that there is a significant alignment between users’s relation type and his distrusted
friends.

5.6 On the Power of Utilizing Social Relationships
We now turn to investigate the effect of utilizing social relationships between users on the accuracy
of recommendations in factorization-based methods. In other words, we would like to experimentally
evaluate whether incorporating distrust can indeed enhance the trust-based recommendation process.
To this end, we run four different MF (i.e., pure matrix factorization based algorithm), MF+T (i.e., matrix
factorization with only trust relationships), MF+D (i.e., matrix factorization with only distrust relationships), and MF+TD (i.e., the algorithm proposed here) algorithms on the data set. We run the algorithms
with k = 10 and k = 20 latent vector dimensions. As mentioned earlier, different amount of training data
90%, 80% , 70% and 60% has been used to create four different training sets that are increasingly sparse
but the social network remains the same in all of them. We evaluate all algorithms by both MAE and
4 A more realistic way would be to use the timestamp of relations to create the training and test sets.

23

RMSE measures.
Table 8 shows the MAE and RMSE errors for the four sampled data sets. First, as we expected, the
performance of all learning algorithms improves with an increasing number of training data. It is also
not surprising to see that the MF+T, MF+D, and MF+TD algorithms which exploit social side information perform better than the pure matrix factorization based MF algorithm. Second, the proposed
algorithm outperforms all other baseline algorithms for all the cases, indicating that it is effective to incorporate both types of social side information in recommendation. This result by itself indicates that
besides trust relationships in the social network, the distrust information is also a rich source of information and can be utilized in recommendation algorithms. We note that distrust information needs to
be incorporated carefully as its nature is totally different from trust information. Finally, it is noticeable
that the MF+T outperforms the MF+D algorithm due to huge number of trust relations to distrust relations in our data set. It is also remarkable that users are more likely to be influenced by their friends
to make trust relations than the distrust relations due to the private nature of distrust relations in Epinions data set. This might lead us to believe that distrust relations have better quality than trust relations
which requires a deeper investigation to be verified.

5.7 Comparison to Baseline Algorithms
Another question that is worthy of investigation is how state-of-the-art approaches perform compared
to the method proposed in this paper. To this end, we compare the performance of the MF-TD algorithm with the baseline algorithms introduced in Subsection 5.4. Table 9 contains the results of our
experiments with eight different algorithms on the data set with 90% of rating data. The second column
in the table represents the configuration of parameters used by each algorithm.
When we utilize trust/distrust relations in neighborhood-based algorithms, a crucial decision we
need to make is to which level the propagation must be performed (no propagation corresponds to the
single level propagation which only includes direct neighbors). Let p and q denote the level of propagation for trust and distrust relations, respectively. Let us first consider the trust propagation to decide
the value of p. We note that there is a tradeoff between accuracy and the level of trust propagation: the
longer propagation levels results in less accurate trust predictions. This is due the fact that when we use
longer propagation levels, the further away we are heading from each user, and consequently decrease
the confidence on the predictions. Obviously this affects the accuracy of the recommendations significantly. As a result, for the trust propagation we only consider single level propagation by choosing p = 1
(i.e, N+∗ = N+ ). We also note that since in the Epinions data set a user can not simultaneously trust
and distrust another user, in the neighborhood-based method with distrust relations, the debugging
only makes sense for propagated information. Therefore, we perform a three level distrust propagation
(q = 3) to constitute the set of distrusted users for each users. We note that the longer the propagation
levels, the more often distrust evidence can be found for a particular user, and hence the less neighbors will be left to participate in the recommendation process. For factorization based methods, the
value of regularization parameters, i.e., λU , λV , and λS , are determined by the procedure discussed in
Subsection 5.3.
The results of Table 9 reveal some interesting conclusions as summarized below:
• From Table 9, we can observe that for factorization-based methods, incorporating trust or distrust
information boost the performance of recommendation in terms of both accuracy measures. This
demonstrates the advantages of trust and distrust-aware recommendation algorithms. We also
can see that both MF+T and MF+D perform better than the non-social MF but the performance
of MF+T is significantly better than MF+D. As discussed before, this observation does not indicate
that the trust relations are more beneficial than the distrust relations as in our data set only 16.7%
of relations are distrust relations. The MF+TD algorithm that is able to employ both types of relations is significantly better than other algorithms that demonstrates the advantages of proposed
method to utilize trust and distrust relations.
• Looking at the results reported in Table 9, it can immediately be noticed that the incorporation
of trust and distrust information in neighborhood-based methods decreases the prediction error
but the improvement is not as significant as the factorization based methods. We note that for
the NB+T method with longer levels of propagation (p = 2, 3), our experiments revealed that the
accuracy remains almost same or gotten worse on both MAE and RMSE measures and this is
24

Table 9: Comparison with other popular methods. The reported values are the MAE and RMSE on the
data set with 90% of rating information. The values of parameters for each specific algorithm is included
in the second column.
Method
Parameter (s)
MAE
RMSE
MF
MF+T
MF+D
MF+TD

k = 10 and λU = λV = 5
k = 10, λU = λV = 5 , and α = 1
k = 10, λU = λV = 5 , and β = 10
k = 10, λU = 13, λV = 11 , and λS = 14.8

0.8921
0.8158
0.8736
0.8025

1.2166
1.1403
1.1852
1.0872

NB
NB+T
NB+TD-F
NB+TD-D

p =1
p = 1 and q = 3
p = 1 and q = 3

0.9381
0.8904
0.8692
0.8728

1.5275
1.3455
1.2455
1.2604

Table 10: The accuracy of handling cold-start users and the effect of social relations. The number of
leant features in this experiments is set to k = 10. The first column shows the number of cold-start
users sampled randomly from all users in the data set. For the cold-starts users all the ratings have
been excluded from the training data and used in the evaluation of three different algorithms.
% of Cold-start Users Measure
MF
MF+T MF+D MF+TD
30%
20%
10%
5%

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9923
1.7211
0.9812
1.7088
0.9334
1.4222
0.9134
1.3852

0.8824
1.5562
0.8805
1.4339
0.8477
1.3782
0.8292
1.2921

0.9721
1.6433
0.9505
1.6250
0.9182
1.4006
0.8633
1.3255

0.8533
1.4802
0.8472
1.2630
0.8322
1.2655
0.8280
1.2888

why we only report the results only for p = 1. In contrast, for distrust propagation we found out
that q = 3 has a visible impact on the performance of both filtering and debugging methods. We
would like to emphasize that for longer levels of distrust propagation in Epinions data set, i.e.,
q > 4, we found that the size of the set of distrusted users N−∗ (·) becomes large for most of users
which degrades the prediction accuracy. We also observe another interesting result about the
performance of NB+TD method with filtering and debugging strategies. We found that although
filtering generates slightly better predictions, NB+TD-F performs almost as good as the NB+TDD method. Although this observation does not suggest any of these methods as the method of
choice in incorporating distrust, we believe that the accuracy might differ from data set to data
set and it strongly depends on the propagation/aggregation strategy.
• Considering the results for both model-based and memory-based methods in Table 9, we can
conclude few interesting observations. First, we notice that factorization-based methods with
trust/distrust information perform better than the neighborhood based methods. Second, the
incorporation of trust and distrust relations in matrix factorization has significant improvement
compared to improvement achieved by memory-based methods. Although the type of filtration
or debugging strategy could significantly affect the accuracy of incorporating distrust in memorybased methods, but the main shortcoming of these methods comes from the fact that these algorithms somehow exclude the influence of distrusted users from the rating prediction. This stands
in stark contrast to the model proposed in this paper that ranks the neighbors based on the type
of relation. This observation necessitates to devise better algorithms for propagation and aggregation of trust/distrust information in memory-based methods.

25

Table 11: The accuracy of proposed algorithm on a data set with 390257 (≈ 90%) trust relations sampled
uniformly at random from all trust relations with varied number of distrust relations. The learning is
performed based on 90% of all ratings with k = 10 as the dimension of latent features.
Method # of Trust Relations # of Distrust Relations Measure
Accuracy
MF+TD

433,619 (≈ 90%)

9,682 (≈ 10%)
19,364 (≈ 20%)
29,047 (≈ 30%)
38,729 (≈ 40%)
48,411 (≈ 50%)
58,093 (≈ 60%)
67,776 (≈ 70%)
77,458 (≈ 80%)
87,140 (≈ 90%)
96,823 (= 100%)

MF+T

481,799 (= 100%)

0

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.8803 ± 0.051
1.2166 ± 0.028
0.8755 ± 0.033
1.1944 ± 0.042
0.8604 ± 0.036
1.1822 ± 0.081
0.8431 ± 0.047
1.1706± 0.055
0.8165± 0.056
1.1425± 0.091
0.8130± 0.035
1.1380± 0.046
0.8122 ± 0.041
1.1306 ± 0.042
0.8095 ± 0.036
1.1290 ± 0.085
0.8061 ± 0.044
1.1176 ± 0.067
0.8050 ± 0.052
1.1092 ± 0.063

MAE
RMSE

0.8158 ± 0.016
1.1403 ± 0.027

5.8 Handling Cold-start Users by Social Side Information
In this subsection, we demonstrate the use of social network to further illustrate the potential of proposed framework and the relevance of incorporating side information. To do so, as another set of our
experiments, we intend to examine the performance of proposed algorithm on clod-start users. Addressing cold-start users (i.e., users with few ratings or new users) is a very important for the success
of recommender systems due to huge number of this type of users in many real world systems. As a
result, handling cold-start users is one the main challenges in existing systems. To evaluate different
algorithms we randomly select 30%, 20%, 10%, and 5% as the cold-start users. For cold-start users, we
do not include any rating in the training data and consider all the ratings made by cold-start users as
testing data.
Table 10 shows the performance of above mentioned algorithms. As it is clear from the Table 10,
when the number of cold-start users is low with respect to the total number of users, say 5% of total
users, the affect of distrust relationships is negligible in prediction accuracy. But, when the number of
cold-start users is high, exploiting the trust and distrust relationships significantly improve the performance of recommendation. This result is interesting as it reveals that the lack of rating information for
cold-start and new users can be alleviated by incorporating the social relations of users, and in particular both trust and distrust relationships.

5.9 Trading Trust for Distrust Relationships
We also compare the potential benefit of trust relations to distrust relations in the proposed algorithm.
More specifically, we would like to see in what extent the distrust relations can compensate for the
lack of trust relations. We run the proposed algorithm with the subset of trust and distrust relations and
compare it to the algorithm which only utilizes all of the trust relations. To setup this set of experiments,
we randomly sample a subset of trust relations and gradually increase the amount of distrust relations
to see when the effect of distrust information compensate the effect of missed trust relations.

26

We sample 433,619 (approximately 90%) trust relations from the total 481,799 trust relations and
vary the number of distrust relations and feed to the proposed algorithm. Table 11 reports the accuracy
of proposed algorithm for different number of distrust relations in the data sets. All these samplings
have been done uniformly at random. We use 90% of all ratings for training and the remaining 10% for
evaluation, and set the dimension of latent features to k = 10. As it can be concluded from Table 11,
when we feed the proposed algorithm MF+TD with 90% of trust and 50% of the distrust relations, it
reveals very similar behavior to the trust-enhanced matrix factorization based method MF+T, which
only utilizes all the trust relations in factorization. This result is interesting in the sense that the distrust
information between users is as important as the trust information (we note that in this scenario the
number trust relations excluded from the training is almost same as the number of distrust relations
included). By increasing the number of distrust relations we can observe that the accuracy of recommendations increases as expected. In summary, this set of experiments validates that incorporating
distrust relations can indeed enhance the trust-based recommendation process and could be considered as a rich source of information to be exploited.

5.10 On the Impact of Batch Size in Stochastic Optimization
As mentioned earlier in the paper, directly solving the optimization problem in (5) using full gradient
descent method requires to go through all the triplets in the constraint set ΩS which could be computationally expensive due to the huge number of triplets in ΩS . To overcome this efficiency problem, one
can turn to stochastic gradient scent method which tries to generate unbiased estimates of the gradient
at each iteration in a much cheeper way by sampling a subset of triplets from ΩS .
To accomplish this goal, we perform gradient descent and stochastic gradient descent to solve the
optimization problem in (5) to find the matrices U and V following the updating equations derived in (7)
and (8). At each iteration t , the currently learned matrices Ut and Vt are used to predict the ratings in
the test set. In particular, at each iteration, we evaluate the RMSE and MAE on the test set, and terminate
training once the RMSE and MAE starts increasing, or the maximum number of iterations is reached.
We run the algorithm with latent vectors of dimension k = 10.
We compare the computational efficiency between proposed algorithm with GD and mini-batch
SGD with different batch sizes. We note that the GD updating rule can be considered as min-batch
SGD where the batch size B is deterministically set to be B = |ΩS | and simple SGD can be considered as
mini-batch SGD with B = 1. We remark that in contrast to GD method which uses all the triplets in ΩS
for gradient computation at each iteration, for SGD method due to uniform sampling over all tuples in
ΩS , some of the tuples may be used more than once and some of the tuples might never been used for
gradient computation.
Figures 3 and 4 show the convergence rate of four different updating rules in terms of the number of
iterations t for two different measures RMSE and RME, respectively. The first algorithm denoted by GD
runs the simple full gradient descent iteratively to optimize the objective. The other three algorithms
named SGD1, SGD2, and SGD3 in the figures use the batch sizes of B = 0.1 ∗ |ΩS |, B = 0.2 ∗ |ΩS |, and
B = 0.3 ∗ |ΩS |, respectively. In our experiments, due to very slow convergence of the basic SGD method
with B = 1 in comparison to other fours methods, we simply exclude its result from the discussion.
In terms of accuracy of predictions, from both Figures 3 and 4, we can conclude that the GD has
the best convergence and SGD3 has the worst convergence in all settings. This is because, although
all of the four algorithms use an unbiased estimate of the true gradient to update the solution at each
iteration, but the variance of each stochastic gradient is proportional to the size of the batch size B .
Therefore, for larger values of B , the variance of stochastic gradients is smaller and the algorithm convergences faster, but, for smaller values of B the algorithm suffers from high variance in stochastic
gradients and convergences slowly. We emphasize that this comparison holds for iteration complexity which is different from the computational complexity (running time) of individual iterations. More
specifically, each iteration of GD requires |ΩS | gradient computations, while for SGD we only need to
perform B ¿ |ΩS | gradient computations. In summary, SGD has lightweight iteration but requires more
iterations to converge. In contrast, GD takes expensive steps in much less number of iterations. From
Figures 3 and 4, it is noticeable that although a large number of iterations is usually needed to obtain a
solution of desirable accuracy using SGD, the lightweight computation per iteration makes SGD attractive for the optimization problem in (5) for large number of users. We also not that for the GD method,
the error is a monotonically decreasing function it terms of number of iterations t , but for the SGD

27

based methods this does not hold. This is because although SGD algorithm is guaranteed to converge
to an optimal solution (at least in expectation), but there is no guarantee that the stochastic gradients
provide a descent direction for the objective at each iteration due to the noise in computing gradients.
As a result, for few iterations we can see that the objective increases but finally it convergences as expected.

6 Conclusions and Future Works
In this paper, we have made a progress towards making distrust information beneficial in social recommendation problem. In particular, we have proposed a framework based on the matrix factorization
which is able to incorporate both trust and distrust relationships between users in factorization algorithm. We experimentally investigated the potential of distrust as a side information to overcome the
data sparsity and cold-start problems in traditional recommender systems. In summary, our results
showed that more accurate recommendations can be obtained by incorporating distrust relations, indicating that distrust information can indeed be beneficial for the recommendation process.
This work leaves few directions, both theoretically and empirically, as future work. From an empirical point of view, it would be interesting to extend our model for weighted social trust and distrust
relations. One challenge in this direction is that, as far as we know, there is no publicly available data set
that includes weighted (gradual) trust and distrust information. Also, the experimental results we have
conducted on the consistency of social relations with rating information hint at a number of potential
enhancements in future work. In particular, it would be interesting to further examine the correlation between implicit and explicit distrust information. An important challenge in this direction is to
develop better metrics to measure the implicit trust between users as the simple metrics such as Pearson correlation coefficient seem to be insufficient. Furthermore, since we only consider the distrust
between users, it would be easy to generalize our model in the same way to incorporate dissimilarity
between items and investigate how it works in practice. Also, our preliminary results indicated that
hinge loss almost performs better than the exponential loss, but from the optimization viewpoint, the
exponential loss is more attractive due to its smoothness. So, an interesting direction would be to use a
smoothed version of the hinge loss to gain from both optimization efficiency and algorithmic accuracy.

References
[1] Gediminas Adomavicius and Alexander Tuzhilin. Toward the next generation of recommender
systems: A survey of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge
and Data Engineering, 17(6):734–749, 2005.
[2] Deepak Agarwal and Bee-Chung Chen. flda: matrix factorization through latent dirichlet allocation. In Proceedings of the third ACM international conference on Web search and data mining,
pages 91–100. ACM, 2010.
[3] Paolo Avesani, Paolo Massa, and Roberto Tiella. A trust-enhanced recommender system application: Moleskiing. In Proceedings of the 2005 ACM symposium on Applied computing, pages 1589–
1593, 2005.
[4] Giacomo Bachi, Michele Coscia, Anna Monreale, and Fosca Giannotti. Classifying trust/distrust
relationships in online social networks. In Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International Confernece on Social Computing (SocialCom), pages
552–557. IEEE, 2012.
[5] Jesús Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Gutiérrez. Recommender
systems survey. Knowledge-Based Systems, 46:109–132, 2013.
[6] John S Breese, David Heckerman, and Carl Kadie. Empirical analysis of predictive algorithms for
collaborative filtering. In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence, pages 43–52. Morgan Kaufmann Publishers Inc., 1998.

28

[7] Moira Burke and Robert Kraut. Mopping up: modeling wikipedia promotion decisions. In Proceedings of the 2008 ACM conference on Computer supported cooperative work, pages 27–36. ACM,
2008.
[8] Dorwin Cartwright and Frank Harary. Structural balance: a generalization of heider’s theory. Psychological review, 63(5):277, 1956.
[9] Gang Chen, Fei Wang, and Changshui Zhang. Collaborative filtering using orthogonal nonnegative
matrix tri-factorization. Information Processing & Management, 45(3):368–379, 2009.
[10] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. Better mini-batch algorithms via
accelerated gradient methods. In NIPS, volume 24, pages 1647–1655, 2011.
[11] David Crandall, Dan Cosley, Daniel Huttenlocher, Jon Kleinberg, and Siddharth Suri. Feedback
effects between similarity and social influence in online communities. In Proceedings of the 14th
ACM SIGKDD international conference on Knowledge discovery and data mining, pages 160–168.
ACM, 2008.
[12] Sanjoy Dasgupta, Michael L Littman, and David McAllester. Pac generalization bounds for cotraining. Advances in neural information processing systems, 1:375–382, 2002.
[13] Mukund Deshpande and George Karypis. Item-based top-n recommendation algorithms. ACM
Transactions on Information Systems (TOIS), 22(1):143–177, 2004.
[14] Thomas DuBois, Jennifer Golbeck, and Aravind Srinivasan. Predicting trust and distrust in social
networks. In Privacy, security, risk and trust (passat), 2011 ieee third international conference on
and 2011 ieee third international conference on social computing (socialcom), pages 418–424. IEEE,
2011.
[15] Rana Forsati, Hanieh Mohammadi Doustdar, Mehrnoush Shamsfard, Andisheh Keikha, and Mohammad Reza Meybodi. A fuzzy co-clustering approach for hybrid recommender systems. International Journal of Hybrid Intelligent Systems, 10(2):71–81, 2013.
[16] Rana Forsati and Mohammad Reza Meybodi. Effective page recommendation algorithms based on
distributed learning automata and weighted association rules. Expert Systems with Applications,
37(2):1316–1330, 2010.
[17] Jennifer Golbeck. Computing and applying trust in web-based social networks. PhD thesis, 2005.
[18] Jennifer Golbeck. Generating predictive movie recommendations from trust in social networks.
Springer, 2006.
[19] Jennifer Golbeck and James Hendler. Filmtrust: Movie recommendations using trust in web-based
social networks. In Proceedings of the IEEE Consumer communications and networking conference,
volume 96. Citeseer, 2006.
[20] Nathaniel Good, J Ben Schafer, Joseph A Konstan, Al Borchers, Badrul Sarwar, Jon Herlocker, and
John Riedl. Combining collaborative filtering with personal agents for better recommendations.
In AAAI/IAAI, pages 439–446, 1999.
[21] Quanquan Gu, Jie Zhou, and Chris Ding. Collaborative filtering: Weighted nonnegative matrix
factorization incorporating user and item graphs. In SIAM SDM, pages 199–210, 2010.
[22] R. Guha, Ravi Kumar, Prabhakar Raghavan, and Andrew Tomkins. Propagation of trust and distrust. In Proceedings of the 13th International Conference on World Wide Web, pages 403–412. ACM,
2004.
[23] Guibing Guo, Jie Zhang, Daniel Thalmann, Anirban Basu, and Neil Yorke-Smith. From ratings to
trust: an empirical study of implicit trust in recommender systems. In SAC, 2014.
[24] Jonathan L Herlocker, Joseph A Konstan, Al Borchers, and John Riedl. An algorithmic framework
for performing collaborative filtering. In Proceedings of the 22nd annual international ACM SIGIR
conference on Research and development in information retrieval, pages 230–237. ACM, 1999.
29

[25] Jonathan L Herlocker, Joseph A Konstan, Loren G Terveen, and John T Riedl. Evaluating collaborative filtering recommender systems. ACM Transactions on Information Systems (TOIS), 22(1):5–53,
2004.
[26] Thomas Hofmann. Latent semantic models for collaborative filtering. ACM Transactions on Information Systems (TOIS), 22(1):89–115, 2004.
[27] Mohsen Jamali and Martin Ester. Trustwalker: a random walk model for combining trust-based
and item-based recommendation. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 397–406. ACM, 2009.
[28] Mohsen Jamali and Martin Ester. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the fourth ACM conference on Recommender
systems, pages 135–142. ACM, 2010.
[29] Mohsen Jamali and Martin Ester. A transitivity aware matrix factorization model for recommendation in social networks. In Proceedings of the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages 2644–2649. AAAI Press, 2011.
[30] Arnd Kohrs and Bernard Merialdo. Clustering for collaborative filtering applications. In In Computational Intelligence for Modelling, Control & Automation. IOS. Citeseer, 1999.
[31] Ioannis Konstas, Vassilios Stathopoulos, and Joemon M Jose. On social networks and collaborative
recommendation. In Proceedings of the 32nd international ACM SIGIR conference on Research and
development in information retrieval, pages 195–202. ACM, 2009.
[32] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering
model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 426–434. ACM, 2008.
[33] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender
systems. Computer, 42(8):30–37, 2009.
[34] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Predicting positive and negative links in
online social networks. In Proceedings of the 19th international conference on World wide web,
pages 641–650. ACM, 2010.
[35] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Signed networks in social media. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 1361–1370.
ACM, 2010.
[36] Wu-Jun Li and Dit-Yan Yeung. Relation regularized matrix factorization. IJCAI-09, 2009.
[37] Juntao Liu, Caihua Wu, and Wenyu Liu. Bayesian probabilistic matrix factorization with social
relations and item contents for recommendation. Decision Support Systems, 2013.
[38] Hao Ma. An experimental study on implicit social recommendation. In Proceedings of the 36th
international ACM SIGIR conference on Research and development in information retrieval, pages
73–82. ACM, 2013.
[39] Hao Ma, Irwin King, and Michael R Lyu. Learning to recommend with social trust ensemble. In
Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 203–210. ACM, 2009.
[40] Hao Ma, Michael R Lyu, and Irwin King. Learning to recommend with trust and distrust relationships. In Proceedings of the third ACM conference on Recommender systems, pages 189–196. ACM,
2009.
[41] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. Sorec: social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 931–940. ACM, 2008.

30

[42] Hao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and Irwin King. Recommender systems with
social regularization. In Proceedings of the fourth ACM international conference on Web search and
data mining, pages 287–296. ACM, 2011.
[43] Hao Ma, Tom Chao Zhou, Michael R Lyu, and Irwin King. Improving recommender systems by
incorporating social contextual information. ACM Transactions on Information Systems (TOIS),
29(2):9, 2011.
[44] Christopher D Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction to information
retrieval, volume 1. Cambridge university press Cambridge, 2008.
[45] Paolo Massa and Paolo Avesani. Trust-aware collaborative filtering for recommender systems. In
On the Move to Meaningful Internet Systems 2004: CoopIS, DOA, and ODBASE, pages 492–508.
Springer, 2004.
[46] Paolo Massa and Paolo Avesani. Controversial users demand local trust metrics: An experimental
study on epinions. com community. In Proceedings of the National Conference on artificial Intelligence, volume 20, page 121. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999,
2005.
[47] Paolo Massa and Paolo Avesani. Trust metrics in recommender systems. In Computing with Social
Trust, pages 259–285. Springer, 2009.
[48] Prem Melville, Raymond J Mooney, and Ramadass Nagarajan. Content-boosted collaborative filtering for improved recommendations. In AAAI/IAAI, pages 187–192, 2002.
[49] Bradley N Miller, Joseph A Konstan, and John Riedl. Pocketlens: Toward a personal recommender
system. ACM Transactions on Information Systems (TOIS), 22(3):437–476, 2004.
[50] Andriy Mnih and Ruslan Salakhutdinov. Probabilistic matrix factorization. In Advances in neural
information processing systems, pages 1257–1264, 2007.
[51] Uma Nalluri. Utility of distrust in online recommender systems. Technical report, 2008.
[52] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–
1609, 2009.
[53] Akshay Patil, Golnaz Ghasemiesfeh, Roozbeh Ebrahimi, and Jie Gao. Quantifying social influence
in epinions. HUMAN, 2(2):pp–67, 2013.
[54] Dmitry Pavlov and David M Pennock. A maximum entropy approach to collaborative filtering in
dynamic, sparse, high-dimensional domains. In NIPS, volume 2, pages 1441–1448, 2002.
[55] Michael J Pazzani. A framework for collaborative, content-based and demographic filtering. Artificial Intelligence Review, 13(5-6):393–408, 1999.
[56] David M Pennock, Eric Horvitz, Steve Lawrence, and C Lee Giles. Collaborative filtering by personality diagnosis: A hybrid memory-and model-based approach. In Proceedings of the Sixteenth
conference on Uncertainty in artificial intelligence, pages 473–480. Morgan Kaufmann Publishers
Inc., 2000.
[57] Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning, pages
713–719. ACM, 2005.
[58] Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using markov
chain monte carlo. In Proceedings of the 25th international conference on Machine learning, pages
880–887. ACM, 2008.
[59] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. Advances in neural
information processing systems, 20:1257–1264, 2008.
31

[60] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted boltzmann machines for
collaborative filtering. In Proceedings of the 24th international conference on Machine learning,
pages 791–798. ACM, 2007.
[61] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. Item-based collaborative filtering
recommendation algorithms. In Proceedings of the 10th international conference on World Wide
Web, pages 285–295. ACM, 2001.
[62] Wanita Sherchan, Surya Nepal, and Cecile Paris. A survey of trust in social networks. ACM Computing Surveys (CSUR), 45(4):47, 2013.
[63] Luo Si and Rong Jin. Flexible mixture model for collaborative filtering. In ICML, volume 3, pages
704–711, 2003.
[64] Ian Soboroff and Charles Nicholas. Combining content and collaboration in text filtering. In Proceedings of the IJCAI, volume 99, pages 86–91, 1999.
[65] Nathan Srebro, Tommi Jaakkola, et al. Weighted low-rank approximations. In ICML, volume 3,
pages 720–727, 2003.
[66] Nathan Srebro, Jason DM Rennie, and Tommi Jaakkola. Maximum-margin matrix factorization.
Advances in neural information processing systems, 17(5):1329–1336, 2005.
[67] Mojdeh Talabeigi, Rana Forsati, and Mohammad Reza Meybodi. A hybrid web recommender system based on cellular learning automata. In Granular Computing (GrC), 2010 IEEE International
Conference on, pages 453–458. IEEE, 2010.
[68] Nele Verbiest, Chris Cornelis, Patricia Victor, and Enrique Herrera-Viedma. Trust and distrust aggregation enhanced with path length incorporation. Fuzzy Sets and Systems, 202:61–74, 2012.
[69] Patricia Victor, Chris Cornelis, Martine De Cock, and Ankur Teredesai. Trust- and distrust-based
recommendations for controversial reviews. IEEE Intelligent Systems, 26(1):48–55, 2011.
[70] Patricia Victor, Chris Cornelis, and Martine De Cock. Trust networks for recommender systems,
volume 4. Springer, 2011.
[71] Patricia Victor, Chris Cornelis, Martine De Cock, and Enrique Herrera-Viedma. Practical aggregation operators for gradual trust and distrust. Fuzzy Sets and Systems, 184(1):126–147, 2011.
[72] Patricia Victor, Nele Verbiest, Chris Cornelis, and Martine De Cock. Enhancing the trust-based
recommendation process with explicit distrust. ACM Transactions on the Web (TWEB), 7(2):6, 2013.
[73] Fei Wang, Sheng Ma, Liuzhong Yang, and Tao Li. Recommendation on item graphs. In Data Mining, 2006. ICDM’06. Sixth International Conference on, pages 1119–1123. IEEE, 2006.
[74] Jun Wang, Arjen P De Vries, and Marcel JT Reinders. Unifying user-based and item-based collaborative filtering approaches by similarity fusion. In Proceedings of the 29th annual international
ACM SIGIR conference on Research and development in information retrieval, pages 501–508. ACM,
2006.
[75] Grzegorz Wierzowiecki and Adam Wierzbicki. Efficient and correct trust propagation using
closelook. In Web Intelligence and Intelligent Agent Technology (WI-IAT), 2010 IEEE/WIC/ACM International Conference on, volume 1, pages 676–681. IEEE, 2010.
[76] Lei Wu, Steven CH Hoi, Rong Jin, Jianke Zhu, and Nenghai Yu. Distance metric learning from
uncertain side information with application to automated photo tagging. In Proceedings of the
17th ACM international conference on Multimedia, pages 135–144. ACM, 2009.
[77] Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, Hua-Jun Zeng, Yong Yu, and Zheng Chen. Scalable collaborative filtering using cluster-based smoothing. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages
114–121. ACM, 2005.
32

[78] Kai Yu, Anton Schwaighofer, Volker Tresp, Xiaowei Xu, and H-P Kriegel. Probabilistic memorybased collaborative filtering. Knowledge and Data Engineering, IEEE Transactions on, 16(1):56–69,
2004.
[79] Sheng Zhang, Weihong Wang, James Ford, and Fillia Makedon. Learning from incomplete ratings
using non-negative matrix factorization. SIAM, 2006.
[80] Yi Zhang and Jonathan Koren. Efficient bayesian hierarchical user modeling for recommendation
system. In Proceedings of the 30th annual international ACM SIGIR conference on Research and
development in information retrieval, pages 47–54. ACM, 2007.
[81] Jianke Zhu, Hao Ma, Chun Chen, and Jiajun Bu. Social recommendation using low-rank semidefinite program. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.
[82] Cai-Nicolas Ziegler. On propagating interpersonal trust in social networks. In Computing with
Social Trust, pages 133–168. Springer, 2009.
[83] Cai-Nicolas Ziegler and Jennifer Golbeck. Investigating interactions of trust and interest similarity.
Decision Support Systems, 43(2):460–475, 2007.
[84] Cai-Nicolas Ziegler and Georg Lausen. Propagation models for trust and distrust in social networks. Information Systems Frontiers, 7(4-5):337–358, 2005.

33

2
1.9

1.8

1.8

1.7

1.7

1.6

1.6

RMSE Error

RMSE Error

2
1.9

1.5
1.4
1.3

1.5
1.4
1.3

1.2

1.2
GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

1.1
1
0

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

1.1
1
50

100
150
It e r a tio n N u m b e r

200

250

0

50

200

250

200

250

(b) 70% of Training Data

2

2

1.9

1.9

1.8

1.8

1.7

1.7

1.6

1.6

RMSE Error

RMSE Error

(a) 60% of Training Data

100
150
It e r a tio n N u m b e r

1.5
1.4
1.3

1.5
1.4
1.3

1.2

1.2
GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

1.1
1
0

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

1.1
1
50

100
150
It e r a tio n N u m b e r

200

250

0

(c) 80% of Training Data

50

100
150
It e r a tio n N u m b e r

(d) 90% of Training Data

Figure 3: Comparison of accuracy of prediction in terms of RMSE with GD and SGD with three varied
batch sizes.

34

1

0.95

0.95

0.9

0.9
M A E Error

M A E Error

1

0.85

0.8

0.85

0.8

0.75

0.75

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

0.7
0

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

0.7
50

100
150
It e r a tio n N u m b e r

200

250

0

50

200

250

200

250

(b) 70% of Training Data

1

1

0.95

0.95

0.9

0.9
RMSE Error

RMSE Error

(a) 60% of Training Data

100
150
It e r a tio n N u m b e r

0.85

0.8

0.85

0.8

0.75

0.75

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

0.7
0

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

0.7
50

100
150
It e r a tio n N u m b e r

200

250

(c) 80% of Training Data

0

50

100
150
It e r a tio n N u m b e r

(d) 90% of Training Data

Figure 4: Comparison of accuracy of prediction in terms of MAE with GD and SGD with three varied
batch sizes.

35

1384

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

LARS*: An Efficient and Scalable
Location-Aware Recommender System
Mohamed Sarwat, Justin J. Levandoski, Ahmed Eldawy, and Mohamed F. Mokbel
Abstract—This paper proposes LARS*, a location-aware recommender system that uses location-based ratings to produce
recommendations. Traditional recommender systems do not consider spatial properties of users nor items; LARS*, on the other hand,
supports a taxonomy of three novel classes of location-based ratings, namely, spatial ratings for non-spatial items, non-spatial ratings
for spatial items, and spatial ratings for spatial items. LARS* exploits user rating locations through user partitioning, a technique that
influences recommendations with ratings spatially close to querying users in a manner that maximizes system scalability while not
sacrificing recommendation quality. LARS* exploits item locations using travel penalty, a technique that favors recommendation
candidates closer in travel distance to querying users in a way that avoids exhaustive access to all spatial items. LARS* can apply
these techniques separately, or together, depending on the type of location-based rating available. Experimental evidence using
large-scale real-world data from both the Foursquare location-based social network and the MovieLens movie recommendation
system reveals that LARS* is efficient, scalable, and capable of producing recommendations twice as accurate compared to existing
recommendation approaches.
Index Terms—Recommender system, spatial, location, performance, efficiency, scalability, social

1

I NTRODUCTION

R

ECOMMENDER systems make use of community
opinions to help users identify useful items from a
considerably large search space (e.g., Amazon inventory [1],
Netflix movies1 ). The technique used by many of these systems is collaborative filtering (CF) [2], which analyzes past
community opinions to find correlations of similar users
and items to suggest k personalized items (e.g., movies)
to a querying user u. Community opinions are expressed
through explicit ratings represented by the triple (user, rating, item) that represents a user providing a numeric rating
for an item.
Currently, myriad applications can produce location-based
ratings that embed user and/or item locations. For example, location-based social networks (e.g., Foursquare2 and
Facebook Places [3]) allow users to “check-in” at spatial
destinations (e.g., restaurants) and rate their visit, thus are
capable of associating both user and item locations with ratings. Such ratings motivate an interesting new paradigm
of location-aware recommendations, whereby the recommender system exploits the spatial aspect of ratings when

1. Netflix: http://www.netflix.com.
2. Foursquare: http://foursquare.com.
• M. Sarwat, A. Eldawy, and M. F. Mokbel are with the Department
of Computer Science and Engineering, University of Minnesota,
Minneapolis, MN 55455 USA.
E-mail: {sarwat, eldawy, mokbel}@cs.umn.edu.
• J. J. Levandoski is with the Microsoft Research, Redmond, WA 98052-6399
USA. E-mail: justin.levandoski@microsoft.com.
Manuscript received 3 May 2012; revised 27 Nov. 2012; accepted
14 Jan. 2013. Date of publication 31 Jan. 2013; date of current version
29 May 2014.
Recommended for acceptance by J. Gehrke, B.C. Ooi, and E. Pitoura.
For information on obtaining reprints of this article, please send e-mail to:
reprints@ieee.org, and reference the Digital Object Identifier below.
Digital Object Identifier 10.1109/TKDE.2013.29

producing recommendations. Existing recommendation
techniques [4] assume ratings are represented by the (user,
rating, item) triple, thus are ill-equipped to produce locationaware recommendations.
In this paper, we propose LARS*, a novel locationaware recommender system built specifically to produce
high-quality location-based recommendations in an efficient manner. LARS* produces recommendations using a
taxonomy of three types of location-based ratings within a
single framework: (1) Spatial ratings for non-spatial items,
represented as a four-tuple (user, ulocation, rating, item),
where ulocation represents a user location, for example, a
user located at home rating a book; (2) non-spatial ratings
for spatial items, represented as a four-tuple (user, rating,
item, ilocation), where ilocation represents an item location,
for example, a user with unknown location rating a restaurant; (3) spatial ratings for spatial items, represented as a
five-tuple (user, ulocation, rating, item, ilocation), for example,
a user at his/her office rating a restaurant visited for lunch.
Traditional rating triples can be classified as non-spatial
ratings for non-spatial items and do not fit this taxonomy.

1.1 Motivation: A Study of Location-Based Ratings
The motivation for our work comes from analysis of two
real-world location-based rating datasets: (1) a subset of
the well-known MovieLens dataset [5] containing approximately 87K movie ratings associated with user zip codes
(i.e., spatial ratings for non-spatial items) and (2) data from
the Foursquare [6] location-based social network containing user visit data for 1M users to 643K venues across
the United States (i.e., spatial ratings for spatial items).
In our analysis we consistently observed two interesting properties that motivate the need for location-aware
recommendation techniques.

c 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
1041-4347 
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

Fig. 1. Preference locality in location-based ratings. (a) MovieLens

preference locality. (b) Foursquare preference locality.

Preference locality. Preference locality suggests users
from a spatial region (e.g., neighborhood) prefer items
(e.g., movies, destinations) that are manifestly different
than items preferred by users from other, even adjacent,
regions. Fig. 1(a) lists the top-4 movie genres using average MovieLens ratings of users from different U.S. states.
While each list is different, the top genres from Florida
differ vastly from the others. Florida’s list contains three
genres (“Fantasy", “Animation", “Musical") not in the other
lists. This difference implies movie preferences are unique
to specific spatial regions, and confirms previous work
from the New York Times [7] that analyzed Netflix user
queues across U.S. zip codes and found similar differences. Meanwhile, Fig. 1(b) summarizes our observation
of preference locality in Foursquare by depicting the visit
destinations for users from three adjacent Minnesota cities.
Each sample exhibits diverse behavior: users from Falcon
Heights, MN favor venues in St. Paul, MN (17% of visits)
Minneapolis (13%), and Roseville, MN (10%), while users
from Robbinsdale, MN prefer venues in Brooklyn Park, MN
(32%) and Robbinsdale (20%). Preference locality suggests
that recommendations should be influenced by locationbased ratings spatially close to the user. The intuition is that
localization influences recommendation using the unique
preferences found within the spatial region containing the
user.
Travel locality. Our second observation is that, when
recommended items are spatial, users tend to travel a
limited distance when visiting these venues. We refer
to this property as “travel locality.” In our analysis of
Foursquare data, we observed that 45% of users travel
10 miles or less, while 75% travel 50 miles or less. This
observation suggests that spatial items closer in travel
distance to a user should be given precedence as recommendation candidates. In other words, a recommendation
loses efficacy the further a querying user must travel
to visit the destination. Existing recommendation techniques do not consider travel locality, thus may recommend
users destinations with burdensome travel distances (e.g.,
a user in Chicago receiving restaurant recommendations
in Seattle).

1.2

Our Contribution: LARS* - A Location-Aware
Recommender System
Like traditional recommender systems, LARS* suggests k
items personalized for a querying user u. However, LARS*
is distinct in its ability to produce location-aware recommendations using each of the three types of location-based
rating within a single framework.

1385

LARS* produces recommendations using spatial ratings
for non-spatial items, i.e., the tuple (user, ulocation, rating, item), by employing a user partitioning technique that
exploits preference locality. This technique uses an adaptive
pyramid structure to partition ratings by their user location
attribute into spatial regions of varying sizes at different
hierarchies. For a querying user located in a region R, we
apply an existing collaborative filtering technique that utilizes only the ratings located in R. The challenge, however,
is to determine whether all regions in the pyramid must
be maintained in order to balance two contradicting factors: scalability and locality. Maintaining a large number of
regions increases locality (i.e., recommendations unique to
smaller spatial regions), yet adversely affects system scalability because each region requires storage and maintenance
of a collaborative filtering data structure necessary to produce recommendations (i.e., the recommender model). The
LARS* pyramid dynamically adapts to find the right pyramid shape that balances scalability and recommendation
locality.
LARS* produces recommendations using non-spatial ratings for spatial items, i.e., the tuple (user, rating, item, ilocation), by using travel penalty, a technique that exploits travel
locality. This technique penalizes recommendation candidates the further they are in travel distance to a querying
user. The challenge here is to avoid computing the travel
distance for all spatial items to produce the list of k recommendations, as this will greatly consume system resources.
LARS* addresses this challenge by employing an efficient
query processing framework capable of terminating early
once it discovers that the list of k answers cannot be altered
by processing more recommendation candidates. To produce recommendations using spatial ratings for spatial items,
i.e., the tuple (user, ulocation, rating, item, ilocation) LARS*
employs both the user partitioning and travel penalty techniques to address the user and item locations associated
with the ratings. This is a salient feature of LARS*, as
the two techniques can be used separately, or in concert,
depending on the location-based rating type available in
the system.
We experimentally evaluate LARS* using real locationbased ratings from Foursquare [6] and MovieLens [5], along
with a generated user workload of both snapshot and continuous queries. Our experiments show LARS* is scalable to
real large-scale recommendation scenarios. Since we have
access to real data, we also evaluate recommendation quality by building LARS* with 80% of the spatial ratings and
testing recommendation accuracy with the remaining 20%
of the (withheld) ratings. We find LARS* produces recommendations that are twice as accurate (i.e., able to better
predict user preferences) compared to traditional collaborative filtering. In summary, the contributions of this paper
are as follows:
•

•

We provide a novel classification of three types
of location-based ratings not supported by existing
recommender systems: spatial ratings for non-spatial
items, non-spatial ratings for spatial items, and spatial
ratings for spatial items.
We propose LARS*, a novel location-aware recommender system capable of using three classes of

1386

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

Fig. 3. Item-based similarity calculation.

(a)

(b)

Fig. 2. Item-based CF model generation. (a) Ratings matrix.

(b) Item-based CF model.

location-based ratings. Within LARS*, we propose:
(a) a user partitioning technique that exploits user
locations in a way that maximizes system scalability while not sacrificing recommendation locality and
(b) a travel penalty technique that exploits item locations and avoids exhaustively processing all spatial
recommendation candidates.
•
LARS* distinguishes itself from LARS [8] in the following points: (1) LARS* achieves higher locality
gain than LARS using a better user partitioning data
structure and algorithm. (2) LARS* exhibits a more
flexible tradeoff between locality and scalability. (3)
LARS* provides a more efficient way to maintain
the user partitioning structure, as opposed to LARS
expensive operations.
•
We provide experimental evidence that LARS* scales
to large-scale recommendation scenarios and provides better quality recommendations than traditional approaches.
This paper is organized as follows: Section 2 gives an
overview of LARS*. Sections 4, 5, and 6 cover LARS* recommendation techniques using spatial ratings for non-spatial
items, non-spatial ratings for spatial items, and spatial ratings
for spatial items, respectively. Section 7 provides experimental analysis. Section 8 covers related work, while Section 9
concludes the paper.

2

LARS* OVERVIEW

This section provides an overview of LARS* by discussing
the query model and the collaborative filtering method.

2.1 LARS* Query Model
Users (or applications) provide LARS* with a user id U,
numeric limit K, and location L; LARS* then returns K
recommended items to the user. LARS* supports both snapshot (i.e., one-time) queries and continuous queries, whereby
a user subscribes to LARS* and receives recommendation
updates as her location changes. The technique LARS*
uses to produce recommendations depends on the type of
location-based rating available in the system. Query processing support for each type of location-based rating is
discussed in Sections 4 to 6.
2.2 Item-Based Collaborative Filtering
LARS* uses item-based collaborative filtering (abbr. CF)
as its primary recommendation technique, chosen due to
its popularity and widespread adoption in commercial
systems (e.g., Amazon [1]). Collaborative filtering (CF)
assumes a set of n users U = {u1 , . . . , un } and a set of m

items I = {i1 , . . . , im }. Each user uj expresses opinions about
a set of items Iuj ⊆ I . Opinions can be a numeric rating
(e.g., the Netflix scale of one to five stars), or unary (e.g.,
Facebook “check-ins" [3]). Conceptually, ratings are represented as a matrix with users and items as dimensions, as
depicted in Fig. 2(a). Given a querying user u, CF produces
a set of k recommended items Ir ⊂ I that u is predicted to
like the most.
Phase I: Model Building. This phase computes a similarity score sim(ip ,iq ) for each pair of objects ip and iq
that have at least one common rating by the same user
(i.e., co-rated dimensions). Similarity computation is covered below. Using these scores, a model is built that stores
for each item i ∈ I , a list L of similar items ordered by a
similarity score sim(ip ,iq ), as depicted in Fig. 2(b). Building
2
this model is an O( RU ) process [1], where R and U are the
number of ratings and users, respectively. It is common to
truncate the model by storing, for each list L, only the n
most similar items with the highest similarity scores [9].
The value of n is referred to as the model size and is usually
much less than |I |.
Phase II: Recommendation Generation. Given a querying user u, recommendations are produced by computing
u’s predicted rating P(u,i) for each item i not rated by u [9]:

l∈L sim(i, l) ∗ ru,l
P(u,i) = 
(1)
l∈L |sim(i, l)|
Before this computation, we reduce each similarity list L to
contain only items rated by user u. The prediction is the sum
of ru,l , a user u’s rating for a related item l ∈ L weighted by
sim(i,l), the similarity of l to candidate item i, then normalized by the sum of similarity scores between i and l. The
user receives as recommendations the top-k items ranked
by P(u,i) .
Computing Similarity. To compute sim(ip , iq ), we represent each item as a vector in the user-rating space of
the rating matrix. For instance, Fig. 3 depicts vectors for
items ip and iq from the matrix in Fig. 2(a). Many similarity functions have been proposed (e.g., Pearson Correlation,
Cosine); we use the Cosine similarity in LARS* due to its
popularity:
sim(ip , iq ) =

ip · iq
ip iq 

(2)

This score is calculated using the vectors’ co-rated dimensions, e.g., the Cosine similarity between ip and iq in
Fig. 3 is .7 calculated using the circled co-rated dimensions.
Cosine distance is useful for numeric ratings (e.g., on a scale
[1,5]). For unary ratings, other similarity functions are used
(e.g., absolute sum [10]).
While we opt to use item-based CF in this paper, no
factors disqualify us from employing other recommendation techniques. For instance, we could easily employ

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

1387

Fig. 5. Example of Items Ratings Statistics Table.

Fig. 4. Pyramid data structure.

user-based CF [4], that uses correlations between users
(instead of items).

3

N ON -S PATIAL U SER R ATINGS FOR
N ON -S PATIAL I TEMS

The traditional item-based collaborative filtering (CF)
method is a special case of LARS*. CF takes as input the
classical rating triplet (user, rating, item) such that neither
the user location nor the item location are specified. In
such case, LARS* directly employs the traditional model
building phase (Phase-I in section 2) to calculate the similarity scores between all items. Moreover, recommendations
are produced to the users using the recommendation generation phase (Phase-II in section 2). During the rest of
the paper, we explain how LARS* incorporates either the
user spatial location or the item spatial location to serve
location-aware recommendations to the system users.

4

S PATIAL U SER R ATINGS FOR N ON -S PATIAL
I TEMS

This section describes how LARS* produces recommendations using spatial ratings for non-spatial items represented
by the tuple (user, ulocation, rating, item). The idea is to
exploit preference locality, i.e., the observation that user opinions are spatially unique (based on analysis in Section 1.1).
We identify three requirements for producing recommendations using spatial ratings for non-spatial items: (1) Locality:
recommendations should be influenced by those ratings
with user locations spatially close to the querying user
location (i.e., in a spatial neighborhood); (2) Scalability: the
recommendation procedure and data structure should scale
up to large number of users; (3) Influence: system users
should have the ability to control the size of the spatial
neighborhood (e.g., city block, zip code, or county) that
influences their recommendations.
LARS* achieves its requirements by employing a user
partitioning technique that maintains an adaptive pyramid
structure, where the shape of the adaptive pyramid is
driven by the three goals of locality, scalability, and influence.
The idea is to adaptively partition the rating tuples (user,
ulocation, rating, item) into spatial regions based on the ulocation attribute. Then, LARS* produces recommendations

using any existing collaborative filtering method (we use
item-based CF) over the remaining three attributes (user,
rating, item) of only the ratings within the spatial region
containing the querying user. We note that ratings can come
from users with varying tastes, and that our method only
forces collaborative filtering to produce personalized user
recommendations based only on ratings restricted to a specific spatial region. In this section, we describe the pyramid
structure in Section 4.1, query processing in Section 4.2, and
finally data structure maintenance in Section 4.3.

4.1 Data Structure
LARS* employs a partial in-memory pyramid structure [11]
(equivalent to a partial quad-tree [12]) as depicted in Fig. 4.
The pyramid decomposes the space into H levels (i.e., pyramid height). For a given level h, the space is partitioned
into 4h equal area grid cells. For example, at the pyramid
root (level 0), one grid cell represents the entire geographic
area, level 1 partitions space into four equi-area cells, and
so forth. We represent each cell with a unique identifier cid.
A rating may belong to up to H pyramid cells: one
per each pyramid level starting from the lowest maintained grid cell containing the embedded user location
up to the root level. To provide a tradeoff between recommendation locality and system scalability, the pyramid
data structure maintains three types of cells (see Fig. 4):
(1) Recommendation Model Cell (α-Cell), (2) Statistics Cell
(β-Cell), and (3) Empty Cell (γ -Cell), explained as follows:
Recommendation Model Cell (α-Cell). Each α-Cell
stores an item-based collaborative filtering model built
using only the spatial ratings with user locations contained
in the cell’s spatial region. Note that the root cell (level 0)
of the pyramid is an α-Cell and represents a “traditional"
(i.e., non-spatial) item-based collaborative filtering model.
Moreover, each α-Cell maintains statistics about all the ratings located within the spatial extents of the cell. Each
α-Cell Cp maintains a hash table that indexes all items (by
their IDs) that have been rated in this cell, named Items
Ratings Statistics Table. For each indexed item i in the Items
Ratings Statistics Table, we maintain four parameters; each
parameter represents the number of user ratings to item i in
each of the four children cells (i.e., C1 , C2 , C3 , and C4 ) of cell
Cp . An example of the maintained parameters is given in
Fig. 5. Assume that cell Cp contains ratings for three items
I1 , I2 , and I3 . Fig. 5 shows the maintained statistics for each
item in cell Cp . For example, for item I1 , the number of user
ratings located in child cell C1 , C2 , C3 , and C4 is equal to
109, 3200, 14, and 54, respectively. Similarly, the number of
user ratings is calculated for items I2 and I3 .
Statistics Cell (β-Cell). Like an α-Cell, a β-Cell maintains statistics (i.e., items ratings Statistics Table) about the
user/item ratings that are located within the spatial range

1388

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

of the cell. The only difference between an α-Cell and a
β-Cell is that a β-Cell does not maintain a collaborative
filtering (CF) model for the user/item ratings lying in its
boundaries. In consequence, a β-Cell is a light weight cell
such that it incurs less storage than an α-Cell. In favor of
system scalability, LARS* prefers a β-Cell over an α-Cell to
reduce the total system storage.
Empty Cell (γ -Cell). a γ -Cell is a cell that maintains
neither the statistics nor the recommendation model for the
ratings lying within its boundaries. a γ -Cell is the most
light weight cell among all cell types as it almost incurs
no storage overhead. Note that an α-Cell can have α-Cells,
β-Cells, or γ -Cells children. Also, a β-Cell can have α-Cells,
β-Cells, or γ -Cells children. However, a γ -Cell cannot have
any children.

4.1.1 Pyramid Structure Intuition
An α-Cell requires the highest storage and maintenance
overhead because it maintains a CF model as well as the
user/item ratings statistics. On the other hand, an α-Cell (as
opposed to β-Cell and γ -Cell) is the only cell that can be
leveraged to answer recommendation queries. A pyramid
structure that only contains α-Cells achieves the highest
recommendation locality, and this is why an α-Cell is considered the highly ranked cell type in LARS*. a β-Cell is
the secondly ranked cell type as it only maintains statistics
about the user/item ratings. The storage and maintenance
overhead incurred by a β-Cell is less expensive than an
α-Cell. The statistics maintained at a β-Cell determines
whether the children of that cell need to be maintained as
α-Cells to serve more localized recommendation. Finally, a
γ -Cell (lowest ranked cell type) has the least maintenance
cost, as neither a CF model nor statistics are maintained for
that cell. Moreover, a γ -Cell is a leaf cell in the pyramid.
LARS* upgrades (downgrades) a cell to a higher (lower)
cell rank, based on trade-offs between recommendation
locality and system scalability (discussed in Section 4.3). If
recommendation locality is preferred over scalability, more
α-Cells are maintained in the pyramid. On the other hand, if
scalability is favored over locality, more γ -Cells exist in the
pyramid. β-Cells comes as an intermediary stage between
α-Cells and γ -Cells to further increase the recommendation
locality whereas the system scalability is not quite affected.
We chose to employ a pyramid as it is a “spacepartitioning" structure that is guaranteed to completely
cover a given space. For our purposes, “data-partitioning"
structures (e.g., R-trees) are less ideal than a “spacepartitioning" structure for two main reasons: (1) “datapartitioning" structures index data points, and hence covers
only locations that are inserted in them. In other words,
“data-partitioning" structures are not guaranteed to completely cover a given space, which is not suitable for
queries issued in arbitrary spatial locations. (2) In contrast to “data-partitioning" structures (e.g., R-trees [13]),
“space partitioning" structures show better performance for
dynamic memory resident data [14]–[16].
4.1.2 LARS* versus LARS
Table 1 compares LARS* against LARS. Like LARS*,
LARS [8] employs a partial pyramid data structure to
support spatial user ratings for non-spatial items. LARS

TABLE 1
Comparison between LARS and LARS*

Detailed experimental evaluation results are provided in Section 7.

is different from LARS* in the following aspects: (1) As
shown in Table 1, LARS* maintains α-Cells, β-Cells, and γ Cells, whereas LARS only maintains α-Cells and γ -Cells. In
other words, LARS either merges or splits a pyramid cell
based on a tradeoff between scalability and recommendation locality. LARS* employs the same tradeoff and further
increases the recommendation locality by allowing for more
α-Cells to be maintained at lower pyramid levels. (2) As
opposed to LARS, LARS* does not perform a speculative
splitting operation to decide whether to maintain more
localized CF models. Instead, LARS* maintains extra statistics at each α-Cell and β-Cell that helps in quickly deciding
wether a CF model needs to be maintained at a child cell.
(3) As it turns out from Table 1, LARS* achieves higher recommendation locality than LARS. That is due to the fact
that LARS maintains a CF recommendation model in a cell
at pyramid level h if and only if a CF model, at its parent
cell at level h − 1, is also maintained. However, LARS* may
maintain an α-Cell at level h even though its parent cell,
at level h − 1, does not maintain a CF model, i.e., the parent cell is a β-Cell. In LARS*, the role of a β-Cell is to keep
the user/item ratings statistics that are used to quickly decide
whether the child cells needs to be γ -Cells or α-Cells. (4) As
given in Table 1, LARS* incurs more storage overhead than
LARS which is explained by the fact that LARS* maintains additional type of cell, i.e., β-Cells, whereas LARS
only maintains α-Cells and γ -Cells. In addition, LARS*
may also maintain more α-Cells than LARS does in order
to increase the recommendation locality. (5) Even though
LARS* may maintain more α-Cells than LARS besides the
extra statistics maintained at β-Cells, nonetheless LARS*
incurs less maintenance cost. That is due to the fact that
LARS* also reduces the maintenance overhead by avoiding
the expensive speculative splitting operation employed by
LARS maintenance algorithm. Instead, LARS* employs the
user/item ratings statistics maintained at either a β-Cell or an
α-Cell to quickly decide whether the cell children need to
maintain a CF model (i.e., upgraded to α-Cells), just needs
to maintain the statistics (i.e., become β-Cells), or perhaps
downgraded to γ -Cells.

4.2 Query Processing
Given a recommendation query (as described in
Section 2.1) with user location L and a limit K, LARS*
performs two query processing steps: (1) The user location

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

1389

L is used to find the lowest maintained α-Cell C in the
adaptive pyramid that contains L. This is done by hashing
the user location to retrieve the cell at the lowest level of
the pyramid. If an α-Cell is not maintained at the lowest
level, we return the nearest maintained ancestor α-Cell.
(2) The top-k recommended items are generated using the
item-based collaborative filtering technique (covered in
Section 2.2) using the model stored at C. As mentioned
earlier, the model in C is built using only the spatial ratings
associated with user locations within C.
In addition to traditional recommendation queries (i.e.,
snapshot queries), LARS* also supports continuous queries
and can account for the influence requirement as follows.
Continuous queries. LARS* evaluates a continuous
query in full once it is issued, and sends recommendations back to a user U as an initial answer. LARS* then
monitors the movement of U using her location updates.
As long as U does not cross the boundary of her current grid cell, LARS* does nothing as the initial answer is
still valid. Once U crosses a cell boundary, LARS* reevaluates the recommendation query for the new cell only if
the new cell is an α-Cell. In case the new cell is an αCell, LARS* only sends incremental updates [16] to the last
reported answer. Like snapshot queries, if a cell at level h is
not maintained, the query is temporarily transferred higher
in the pyramid to the nearest maintained ancestor α-Cell.
Note that since higher-level cells maintain larger spatial
regions, the continuous query will cross spatial boundaries less often, reducing the amount of recommendation
updates.
Influence level. LARS* addresses the influence requirement by allowing querying users to specify an optional
influence level (in addition to location L and limit K)
that controls the size of the spatial neighborhood used
to influence their recommendations. An influence level I
maps to a pyramid level and acts much like a “zoom"
level in Google or Bing maps (e.g., city block, neighborhood, entire city). The level I instructs LARS* to process the recommendation query starting from the grid
α-Cell containing the querying user location at level
I, instead of the lowest maintained grid α-Cell (the
default). An influence level of zero forces LARS* to use
the root cell of the pyramid, and thus act as a traditional (non-spatial) collaborative filtering recommender
system.

4.4 Main Idea
As time goes by, new users, ratings, and items will be added
to the system. This new data will both increase the size of
the collaborative filtering models maintained in the pyramid cells, as well as alter recommendations produced from
each cell. To account for these changes, LARS* performs
maintenance on a cell-by-cell basis. Maintenance is triggered for a cell C once it receives N% new ratings; the
percentage is computed from the number of existing ratings
in C. We do this because an appealing quality of collaborative filtering is that as a model matures (i.e., more data
is used to build the model), more updates are needed to
significantly change the top-k recommendations produced
from it [17]. Thus, maintenance is needed less often.
We note the following features of pyramid maintenance:
(1) Maintenance can be performed completely offline, i.e.,
LARS* can continue to produce recommendations using
the "old" pyramid cells while part of the pyramid is being
updated; (2) maintenance does not entail rebuilding the
whole pyramid at once, instead, only one cell is rebuilt at
a time; (3) maintenance is performed only after N% new
ratings are added to a pyramid cell, meaning maintenance
will be amortized over many operations.

4.3 Data Structure Maintenance
This section describes building and maintaining the pyramid data structure. Initially, to build the pyramid, all
location-based ratings currently in the system are used
to build a complete pyramid of height H, such that all
cells in all H levels are α-Cells and contain ratings statistics and a collaborative filtering model. The initial height
H is chosen according to the level of locality desired,
where the cells in the lowest pyramid level represent the
most localized regions. After this initial build, we invoke
a cell type maintenance step that scans all cells starting
from the lowest level h and downgrades cell types to
either (β-Cell or γ -Cell) if necessary (cell type switching is discussed in Section 4.5.2). We note that while
the original partial pyramid [11] was concerned with

4.5 Maintenance Algorithm
Algorithm 1 provides the pseudocode for the LARS* maintenance algorithm. The algorithm takes as input a pyramid cell C and level h, and includes three main steps:
Statistics Maintenance, Model Rebuild and Cell Child Quadrant
Maintenance, explained below.
Step I: Statistics Maintenance. The first step (line 4) is
to maintain the Items Ratings Statistics Table. The maintained
statistics are necessary for cell type switching decision,
especially when new location-based ratings enter the system. As the items ratings statistics table is implemented using
a hash table, then it can be queried and maintained in O(1)
time, requiring O(|IC |) space such that IC is the set of all
items rated at cell C and |IC | is the total number of items
in IC .

Algorithm 1 Pyramid maintenance algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

/* Called after cell C receives N% new ratings */
Function PyramidMaintenance(Cell C, Level h)
/* Step I: Statistics Maintenance*/
Maintain cell C statistics
/*Step II: Model Rebuild */
if (Cell C is an α -Cell) then
Rebuild item-based collaborative filtering model for cell C
end if
/*Step III: Cell Child Quadrant Maintenance */
if (C children quadrant q cells are α -Cells) then
CheckDownGradeToSCells(q,C) /* covered in Section 4.5.2 */
else if (C children quadrant q cells are γ -Cells) then
CheckUpGradeToSCells(q,C)
else
isSwitchedToMcells ← CheckUpGradeToMCells(q,C) /* covered in
Section 4.5.3 */
if (isSwitchedToMcells is False) then
CheckDownGradeToECells(q,C)
end if
end if
return

spatial queries over static data, it did not address pyramid
maintenance.

1390

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

Step II: Model Rebuild. The second step is to rebuild the
item-based collaborative filtering (CF) model for a cell C, as
described in Section 2.2 (line 7). The model is rebuilt at cell
C only if cell C is an α-Cell, otherwise (β-Cell or γ -Cell)
no CF recommendation model is maintained, and hence
the model rebuild step does not apply. Rebuilding the CF
model is necessary to allow the model to “evolve" as new
location-based ratings enter the system (e.g., accounting for
new items, ratings, or users). Given the cost of building the
2
item-based CF model is O( RU ) (per Section 2.2), the cost
of the model rebuild for a cell C at level h is

(R/4h )2
(U/4h )

(a)

R2

= 4h U ,

assuming ratings and users are uniformly distributed.
Step III: Cell Child Quadrant Maintenance. LARS*
invokes a maintenance step that may decide whether cell
C child quadrant need to be switched to a different cell
type based on trade-offs between scalability and locality.
The algorithm first checks if cell C child quadrant q at
level h + 1 is of type α-Cell (line 10). If that case holds,
LARS* considers quadrant q cells as candidates to be downgraded to β-Cells (calling function CheckDownGradeToSCells
on line 11). We provide details of the Downgrade α-Cells to
β-Cells operation in Section 4.5.2. On the other hand, if C
have a child quadrant of type γ -Cells at level h+1 (line 12),
LARS* considers upgrading cell C four children cells at
level h + 1 to β-Cells (calling function CheckUpGradeToSCells
on line 13). The Upgrade to β-Cells operation is covered in
Section 4.5.4. However, if C has a child quadrant of type βCells at level h+1 (line 12), LARS* first considers upgrading
cell C four children cells at level h + 1 from β-Cells to αCells (calling function CheckUpGradeToMCells on line 15). If
the children cells are not switched to α-Cells, LARS* then
considers downgrading them to γ -Cells (calling function
CheckDownGradeToECells on line 17). Cell Type switching
operations are performed completely in quadrants (i.e., four
equi-area cells with the same parent). We made this decision
for simplicity in maintaining the partial pyramid.

4.5.1 Recommendation Locality
In this section, we explain the notion of locality in recommendation that is essential to understand the cell type
switching (upgrade/downgrade) operations highlighted in
the PyramidMaintenance algorithm (algorithm 1). We use
the following example to give the intuition behind recommendation locality.
Running Example. Fig. 6 depicts a two-levels pyramid
in which Cp is the root cell and its children cells are C1 , C2 ,
C3 , and C4 . In the example, we assume eight users (U1 , U2 ,
. . . , and U8 ) have rated eight different items (I1 , I2 , . . . , and
I8 ). Fig. 6(b) gives the spatial distributions of users U1 , U2 ,
U3 , U4 , U5 , U6 , U7 , and U8 as well as the items that each
user rated.
Intuition. Consider the example given in Fig. 6. In cell
Cp , users U2 and U5 that belongs to the child cell C2 have
both rated items I2 and I5 . In that case, the similarity score
between items I2 and I5 in the item-based collaborative filtering CF model built at cell C2 is exactly the same as the
one in the CF model built at cell Cp . The last phenomenon
happened because items (i.e., I2 and I5 ) have been rated
by mostly users located in the same child cell, and hence
the recommendation model at the parent cell will not be

(b)

(c)
Fig. 6. Item ratings spatial distribution example. (a) Two-levels
pyramid. (b) Ratings distribution and recommendation models.
(c) Locality loss/gain at C_p.

different from the model at the children cells. In this case,
if the CF model at C2 is not maintained, LARS* does not
lose recommendation locality at all.
The opposite case happens when an item is rated by
users located in different pyramid cells (spatially skewed).
For example, item I4 is rated by users U2 , U4 , and U7 in
three different cells (C2 , C3 , and C4 ). In this case, U2 , U4 ,
and U7 are spatially skewed. Hence, the similarity score
between item I4 and other items at the children cells is different from the similarity score calculated at the parent cell
Cp because not all users that have rated item I4 exist in the
same child cell. Based on that, we observe the following:

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

TABLE 2
Summary of Mathematical Notations

Definition 2. Locality Loss (LGc )
LGc is defined as the total locality lost by downgrading cell c
four children cells to β-Cells (0 ≤ LGc ≤ 1). It is calculated
as the sum of all items locality loss normalized by the total
number of items |Ic | in cell c.

i∈Ic LGc,i
.
(4)
LGc =
|Ic |

Observation 1. The more the user/item ratings in a parent cell
C are geographically skewed, the higher the locality gained
from building the item-based CF model at the four children
cells.
The amount of locality gained/lost by maintaining the
child cells of a given pyramid cell depends on whether
the CF models at the child cells are similar to the CF
model built at the parent cell. In other words, LARS*
loses locality if the child cells are not maintained even
though the CF model at these cells produce different recommendations than the CF model at the parent cell. LARS*
leverages Observation 1 to determine the amount of locality
gained/lost due to maintaining an item-based CF model at
the four children. LARS* calculates the locality loss/gain as
follows:
Locality Loss/Gain. Table 2 gives the main mathematical notions used in calculating the recommendation locality
loss/gain. First, the Item Ratings Pairs Set (RPc,i ) is defined
as the set of all possible pairs of users that rated item i
in cell c. For example, in Fig. 6(c) the item ratings pairs
set for item I7 in cell Cp (RPCp ,I7 ) has three elements
(i.e., RPCp ,I7 ={
U3 , U6 ,
U3 , U7 ,
U6 , U7 }) as only users U1
and U7 have rated item I1 . Similarly, RPCp ,I2 is equal to
{
U6 , U7 } (i.e., Users U2 and U5 have rated item I2 ).
For each item, we define the Skewed Item Ratings Set
(RSc,i ) as the total number of user pairs in cell c that rated
item i such that each pair of users ∈ RSc,i do not exist
in the same child cell of c. For example, in Fig. 6(c), the
skewed item ratings set for item I2 in cell Cp (RSCP ,I2 ) is ∅
as all users that rated I2 , i.e., U2 and U5 are collocated in
the same child cell C2 . For I4 , the skewed item ratings set
RSCP ,I2 ={
U2 , U7 , 
U2 , U4 , 
U4 , U7 } as all users that rated
item I2 are located in different child cells,i.e., U2 at C2 , U4
at C4 , and U7 at C3 .
Given the aforementioned parameters, we calculate Item
Locality Loss (LGc,i ) for each item, as follows:
Definition 1. Item Locality Loss (LGc,i )
LGc,i is defined as the degree of locality lost for item i from
downgrading the four children of cell c to β-Cells, such that
0 ≤ LGc,i ≤ 1.
LGc,i =

|RSc,i |
.
|RPc,i |

1391

(3)

The value of both |RSc,i | and |RPc,i | can be easily extracted
using the items ratings statistics table. Then, we use the LGc,i
values calculated for all items in cell c in order to calculate
the overall Cell Locality loss (LGc ) from downgrading the
children cells of c to α-Cells.

The cell locality loss (or gain) is harnessed by LARS*
to determine whether the cell children need to be downgraded from α-Cell to β-Cell rank, upgraded from the
γ -Cell to β-Cell rank, or downgraded from β-Cell to γ -Cell
rank. During the rest of section 4, we explain the cell rank
upgrade/downgrade operations.

4.5.2 Downgrade α-Cells to β-Cells
That operation entails downgrading an entire quadrant of
cells from α-Cells to β-Cells at level h with a common parent at level h − 1. Downgrading α-Cells to β-Cells improves
scalability (i.e., storage and computational overhead) of
LARS*, as it reduces storage by discarding the item-based
collaborative filtering (CF) models of the the four children cells. Furthermore, downgrading α-Cells to β-Cells
leads to the following performance improvements: (a) less
maintenance cost, since less CF models are periodically
rebuilt, and (b) less continuous query processing computation,
as β-Cells do not maintain a CF model and if many β-Cells
cover a large spatial region, hence, for users crossing β-Cells
boundaries, we do not need to update the recommendation
query answer. Downgrading children cells from α-Cells to
β-Cells might hurt recommendation locality, since no CF
models are maintained at the granularity of the child cells
anymore.
At cell Cp , in order to determine whether to downgrade a quadrant q cells to β-Cells (i.e., function
CheckDownGradeToSCells on line 11 in Algorithm 1), we
calculate two percentage values: (1) locality_loss (see
equation 4), the amount of locality lost by (potentially)
downgrading the children cells to β-Cells, and (2) scalability_gain, the amount of scalability gained by (potentially)
downgrading the children cells to β-Cells. Details of calculating these percentages are covered next. When deciding
to downgrade cells to β-Cells, we define a system parameter M, a real number in the range [0,1] that defines a
tradeoff between scalability gain and locality loss. LARS*
downgrades a quadrant q cells to β-Cells (i.e., discards
quadrant q) if:
(1 − M) ∗ scalability_gain > M ∗ locality_loss.

(5)

A smaller M value implies gaining scalability is important and the system is willing to lose a large amount of
locality for small gains in scalability. Conversely, a larger
M value implies scalability is not a concern, and the
amount of locality lost must be small in order to allow
for β-Cells downgrade. At the extremes, setting M=0 (i.e.,
always switch to β-Cell) implies LARS* will function as
a traditional CF recommender system, while setting M=1
causes LARS* pyramid cells to all be α-Cells, i.e., LARS*
will employ a complete pyramid structure maintaining a
recommendation model at all cells at all levels.

1392

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

Calculating Locality Loss. To calculate the locality loss
at a cell Cp , LARS* leverages the Item Ratings Statistics Table
maintained in that cell. First, LARS* calculates the item
locality loss LGCp ,i for each item i in the cell Cp . Therefore,
LARS* aggregates the item locality loss values calculated
for each item i ∈ Cp , to finally deduce the global cell locality
loss LGCp .
Calculating scalability gain. Scalability gain is measured in storage and computation savings. We measure
scalability gain by summing the recommendation model
sizes for each of the downgraded (i.e., child) cells (abbr.
sizem ), and divide this value by the sum of sizem and the recommendation model size of the parent cell. We refer to this
percentage as the storage_gain. We also quantify computation
savings using storage gain as a surrogate measurement, as
computation is considered a direct function of the amount
of data in the system.
Cost. using the Items Ratings Statistics Table maintained
at cell Cp , the locality loss at cell Cp can be calculated in
O(|ICp |) time such that |ICp | represents the total number of
items in Cp . As scalability gain can be calculated in O(1)
time, then the total time cost of the Downgrade To β-Cells
operation is O(|ICp |).
Example. For the example given in Fig. 6(c), the locality loss of downgrading cell Cp four children cells
{C1 , C2 , C3 , C4 } to β-Cells is calculated as follows: First,
we retrieve the locality loss LGCp ,i for each item i ∈
{I1 , I2 , I3 , I4 , I5 , I6 , I7 , I8 }, from the maintained statistics at
cell Cp . As given in Fig. 6(c), LGCp ,I1 , LGCp ,I2 , LGCp ,I3 ,
LGCp ,I4 , LGCp ,I5 , LGCp ,I6 , LGCp ,I7 , and LGCp ,I8 are equal to 0.0,
0.0, 1.0, 1.0, 0.0, 0.666, 0.166, and 1.0, respectively. Then, we
calculate the overall locality loss at Cp (using equation 4),
LGCp by summing all the locality loss values of all items
and dividing the sum by the total number of items. Hence,
the scalability loss is equal to ( 0.0+1.0+1.0+0.0+0.666+0.1666+1.0
)
8
= 0.48 = 48%. To calculate scalability gain, assume the sum
of the model sizes for cells C1 to C4 and CP is 4GB, and
the sum of the model sizes for cells C1 to C4 is 2GB.
Then, the scalability gain is 24 =50%. Assuming M=0.7, then
(0.3 × 50) < (0.7 × 48), meaning that LARS* will not
downgrade cells C1 , C2 , C3 , C4 to β-Cells.

4.5.3 Upgrade β-Cells to α-Cells
Upgrading β-Cells to α-Cells operation entails upgrading the
cell type of a cell child quadrant at pyramid level h under
a cell at level h − 1, to α-Cells. Upgrading β-Cells to α-Cells
operation improves locality in LARS*, as it leads to maintaining a CF model at the children cells that represent
more granular spatial regions capable of producing recommendations unique to the smaller, more “local", spatial
regions. On the other hand, upgrading cells to α-Cells hurts
scalability by requiring storage and maintenance of more
item-based collaborative filtering models. The upgrade to
α-Cells operation also negatively affects continuous query
processing, since it creates more granular α-Cells causing user locations to cross α-Cell boundaries more often,
triggering recommendation updates.
To determine whether to upgrade a cell CP (quadrant q) four children cells to α-Cells (i.e., function
CheckUpGradeToMCells on line 15 of Algorithm 1). Two percentages are calculated: locality_gain and scalability_loss.

These values are the opposite of those calculated for the
Upgrade to β-Cells operation. LARS* change cell CP child
quadrant q to α-Cells only if the following condition holds:
M ∗ locality_gain > (1 − M) ∗ scalability_loss.

(6)

This equation represents the opposite criteria of that presented for Upgrade to β-Cells operation in Equation 5.
Calculating locality gain. To calculate the locality gain,
LARS* does not need to speculatively build the CF model
at the four children cells. The locality gain is calculated the
same way the locality loss is calculated in equation 4.
Calculating scalability loss. We calculate scalability loss
by estimating the storage necessary to maintain the children cells. Recall from Section 2.2 that the maximum size
of an item-based CF model is approximately n|I|, where n
is the model size. We can multiply n|I| by the number of
bytes needed to store an item in a CF model to find an
upper-bound storage size of each potentially Upgradeded to
α-Cell cell. The sum of these four estimated sizes (abbr. sizes )
divided by the sum of the size of the existing parent cell
and sizes represents the scalability loss metric.
Cost. Similar to the CheckDownGradeToSCells operation,
scalability loss is calculated in O(1) and locality gain can
be calculated in O(|ICp |) time. Then, the total time cost of
the CheckUpGradeToMCells operation is O(|ICp |).
Example. Consider the example given in Fig. 6(c).
Assume the cell Cp is an α-Cell and its four children C1 ,
C2 , C3 , and C4 are β-Cells. The locality gain (LGCp ) is calculated using equation 4 to be 0.48 (i.e., 48%) as depicted in
the table in Fig. 6(c). Further, assume that we estimate the
extra storage overhead for upgradinging the children cells
to α-Cells (i.e., storage loss) to be 50%. Assuming M=0.7,
then (0.7 × 48) > (0.3 × 50), meaning that LARS* will
decide to upgrade CP four children cells to α-Cells as locality
gain is significantly higher than scalability loss.

4.5.4 Downgrade β-Cells to γ -Cells and Vice Versa
Downgrading β-Cells to γ -Cells operation entails downgrading the cell type of a cell child quadrant at pyramid level h
under a cell at level h − 1, to γ -Cells (i.e., empty cells).
Downgrading the child quadrant type to γ -Cells means
that the maintained statistics are no more maintained in
the children cell, which definitely reduces the overhead of
maintaining the Item Ratings Statistics Table at these cells.
Even though γ -Cells incurs no maintenance overhead, however they reduce the amount of recommendation locality
that LARS* provides.
The decision of downgrading from β-Cells to γ -Cells is
taken based on a system parameter, named MAX_SLEVELS.
It is defined as the maximum number of consecutive
pyramid levels in which descendant cells can be β-Cells.
MAX_SLEVELS can take any value between zero and the
total height of the pyramid. A high value of MAX_SLEVELS
results in maintaining more β-Cells and less γ -Cells in
the pyramid. For example, in Fig. 4, MAX_SLEVELS is
set to two, and this is why if two consecutive pyramid
levels are β-Cells, the third level β-Cells are autotmatically downgraded to γ -Cells. For each β-Cell C, a counter,
called S-Levels Counter, is maintained. The S-Levels Counter
stores of the total number of consecutive levels in the direct
ancestry of cell C such that all these levels contains β-Cells.

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

At a β-Cell C, if the cell children are β-Cells, then we
compare the S-Levels Counter at the child cells with the
MAX_SLEVELS parameter. Note that the counter counts
only the consecutive S-Levels, so if some levels in the chain
are α-Cells the counter is reset to zero at the α-Cells levels. If
S-Levels Counter is greater than or equal to MAX_SLEVELS,
then the children cells of C are downgraded to γ -Cells.
Otherwise, cell C children cells are not downgraded to
γ -Cells. Similarly, LARS* also makes use of the same
S-Levels Counter to decide whether to upgrade γ -Cells to
β-Cells.

5

N ON -S PATIAL U SER R ATINGS FOR S PATIAL
I TEMS

This section describes how LARS* produces recommendations using non-spatial ratings for spatial items represented
by the tuple (user, rating, item, ilocation). The idea is to
exploit travel locality, i.e., the observation that users limit
their choice of spatial venues based on travel distance
(based on analysis in Section 1.1). Traditional (non-spatial)
recommendation techniques may produce recommendations with burdensome travel distances (e.g., hundreds
of miles away). LARS* produces recommendations within
reasonable travel distances by using travel penalty, a technique that penalizes the recommendation rank of items the
further in travel distance they are from a querying user.
Travel penalty may incur expensive computational overhead
by calculating travel distance to each item. Thus, LARS*
employs an efficient query processing technique capable
of early termination to produce the recommendations without calculating the travel distance to all items. Section 5.1
describes the query processing framework while Section 5.2
describes travel distance computation.

5.1 Query Processing
Query processing for spatial items using the travel penalty
technique employs a single system-wide item-based collaborative filtering model to generate the top-k recommendations by ranking each spatial item i for a querying user u
based on RecScore(u, i), computed as:
RecScore(u, i) = P(u, i) − TravelPenalty(u, i).

(7)

P(u, i) is the standard item-based CF predicted rating of
item i for user u (see Section 2.2). TravelPenalty(u, i) is the
road network travel distance between u and i normalized
to the same value range as the rating scale (e.g., [0, 5]).
When processing recommendations, we aim to avoid calculating Equation 7 for all candidate items to find the top-k
recommendations, which can become quite expensive given
the need to compute travel distances. To avoid such computation, we evaluate items in monotonically increasing
order of travel penalty (i.e., travel distance), enabling us to
use early termination principles from top-k query processing [18]–[20]. We now present the main idea of our query
processing algorithm and in the next section discuss how
to compute travel penalties in an increasing order of travel
distance.
Algorithm 2 provides the pseudo code of our query
processing algorithm that takes a querying user id U, a

1393

Algorithm 2 Travel Penalty Algorithm for Spatial Items
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

Function LARS*_SpatialItems(User U, Location L, Limit K)
/* Populate a list R with a set of K items*/
R←φ
for (K iterations) do
i ← Retrieve the item with the next lowest travel penalty (Section 5.2)
Insert i into R ordered by RecScore(U, i) computed by Equation 7
end for
LowestRecScore ← RecScore of the kth object in R
/*Retrieve items one by one in order of their penalty value */
while there are more items to process do
i ← Retrieve the next item in order of penalty score (Section 5.2)
MaxPossibleScore ← MAX_RATING - i.penalty
if MaxPossibleScore ≤ LowestRecScore then
return R /* early termination - end query processing */
end if
RecScore(U, i) ← P(U, i) - i.penalty /* Equation 7 */
if RecScore(U, i) > LowestRecScore then
Insert i into R ordered by RecScore(U, i)
LowestRecScore ← RecScore of the kth object in R
end if
end while
return R

location L, and a limit K as input, and returns the list R of
top-k recommended items. The algorithm starts by running
a k-nearest-neighbor algorithm to populate the list R with
k items with lowest travel penalty; R is sorted by the recommendation score computed using Equation 7. This initial
part is concluded by setting the lowest recommendation
score value (LowestRecScore) as the RecScore of the kth item
in R (Lines 3 to 8). Then, the algorithm starts to retrieve
items one by one in the order of their penalty score. This
can be done using an incremental k-nearest-neighbor algorithm, as will be described in the next section. For each item
i, we calculate the maximum possible recommendation score
that i can have by subtracting the travel penalty of i from
MAX_RATING, the maximum possible rating value in the
system, e.g., 5 (Line 12). If i cannot make it into the list
of top-k recommended items with this maximum possible
score, we immediately terminate the algorithm by returning R as the top-k recommendations without computing the
recommendation score (and travel distance) for more items
(Lines 13 to 15). The rationale here is that since we are
retrieving items in increasing order of their penalty and calculating the maximum score that any remaining item can
have, then there is no chance that any unprocessed item
can beat the lowest recommendation score in R. If the early
termination case does not arise, we continue to compute
the score for each item i using Equation 7, insert i into
R sorted by its score (removing the kth item if necessary),
and adjust the lowest recommendation value accordingly
(Lines 16 to 20).
Travel penalty requires very little maintenance. The only
maintenance necessary is to occasionally rebuild the single system-wide item-based collaborative filtering model
in order to account for new location-based ratings that
enter the system. Following the reasoning discussed in
Section 4.3, we rebuild the model after receiving N% new
ratings.

5.2 Incremental Travel Penalty Computation
This section gives an overview of two methods we implemented in LARS* to incrementally retrieve items one by one
ordered by their travel penalty. The two methods exhibit a

1394

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

trade-off between query processing efficiency and penalty
accuracy: (1) an online method that provides exact travel
penalties but is expensive to compute, and (2) an offline
heuristic method that is less exact but efficient in penalty
retrieval. Both methods can be employed interchangeably
in Line 11 of Algorithm 2.

5.2.1 Incremental KNN: An Exact Online Method
To calculate an exact travel penalty for a user u to item i,
we employ an incremental k-nearest-neighbor (KNN) technique [21]–[23]. Given a user location l, incremental KNN
algorithms return, on each invocation, the next item i nearest to u with regard to travel distance d. In our case, we
normalize distance d to the ratings scale to get the travel
penalty in Equation 7. Incremental KNN techniques exist
for both Euclidean distance [22] and (road) network distance [21], [23]. The advantage of using Incremental KNN
techniques is that they provide an exact travel distances
between a querying user’s location and each recommendation candidate item. The disadvantage is that distances
must be computed online at query runtime, which can be
expensive. For instance, the runtime complexity of retrieving a single item using incremental KNN in Euclidean space
is [22]: O(k + logN), where N and k are the number of total
items and items retrieved so far, respectively.
5.2.2 Penalty Grid: A Heuristic Offline Method
A more efficient, yet less accurate method to retrieve travel
penalties incrementally is to use a pre-computed penalty
grid. The idea is to partition space using an n × n grid. Each
grid cell c is of equal size and contains all items whose
location falls within the spatial region defined by c. Each
cell c contains a penalty list that stores the pre-computed
penalty values for traveling from anywhere within c to all
other n2 −1 destination cells in the grid; this means all items
within a destination grid cell share the same penalty value.
The penalty list for c is sorted by penalty value and always
stores c (itself) as the first item with a penalty of zero. To
retrieve items incrementally, all items within the cell containing the querying user are returned one-by-one (in any
order) since they have no penalty. After these items are
exhausted, items contained in the next cell in the penalty
list are returned, and so forth until Algorithm 2 terminates
early or processes all items.
To populate the penalty grid, we must calculate the
penalty value for traveling from each cell to every other cell
in the grid. We assume items and users are constrained to
a road network, however, we can also use Euclidean space
without consequence. To calculate the penalty from a single
source cell c to a destination cell d, we first find the average
distance to travel from anywhere within c to all item destinations within d. To do this, we generate an anchor point
p within c that both (1) lies on the road network segment
within c and (2) lies as close as possible to the center of
c. With these criteria, p serves as an approximate average
“starting point" for traveling from c to d. We then calculate
the shortest path distance from p to all items contained in
d on the road network (any shortest path algorithm can be
used). Finally, we average all calculated shortest path distances from c to d. As a final step, we normalize the average
distance from c to d to fall within the rating value range.

Normalization is necessary as the rating domain is usually
small (e.g., zero to five), while distance is measured in miles
or kilometers and can have large values that heavily influence Equation 7. We repeat this entire process for each cell
to all other cells to populate the entire penalty grid.
When new items are added to the system, their presence in a cell d can alter the average distance value used in
penalty calculation for each source cell c. Thus, we recalculate penalty scores in the penalty grid after N new items
enter the system. We assume spatial items are relatively
static, e.g., restaurants do not change location often. Thus,
it is unlikely existing items will change cell locations and in
turn alter penalty scores.

6

S PATIAL U SER R ATINGS FOR S PATIAL I TEMS

This section describes how LARS* produces recommendations using spatial ratings for spatial items represented by
the tuple (user, ulocation, rating, item, ilocation). A salient feature of LARS* is that both the user partitioning and travel
penalty techniques can be used together with very little
change to produce recommendations using spatial user
ratings for spatial items. The data structures and maintenance techniques remain exactly the same as discussed
in Sections 4 and 5; only the query processing framework requires a slight modification. Query processing uses
Algorithm 2 to produce recommendations. However, the
only difference is that the item-based collaborative filtering
prediction score P(u, i) used in the recommendation score
calculation (Line 16 in Algorithm 2) is generated using the
(localized) collaborative filtering model from the partial
pyramid cell that contains the querying user, instead of the
system-wide collaborative filtering model as was used in
Section 5.

7

E XPERIMENTS

This section provides experimental evaluation of LARS*
based on an actual system implementation using C++ and
STL. We compare LARS* with the standard item-based
collaborative filtering technique along with several variations of LARS*. We also compare LARS* to LARS [8].
Experiments are based on three data sets:
Foursquare: a real data set consisting of spatial user
ratings for spatial items derived from Foursquare user histories. We crawled Foursquare and collected data for
1,010,192 users and 642,990 venues across the United States.
Foursquare does not publish each “check-in" for a user,
however, we were able to collect the following pieces of
data: (1) user tips for a venue, (2) the venues for which the
user is the mayor, and (3) the completed to-do list items for
a user. In addition, we extracted each user’s friend list.
Extracting location-based ratings. To extract spatial user
ratings for spatial items from the Foursquare data (i.e.,
the five-tuple (user, ulocation, rating, item, ilocation)), we
map each user visit to a single location-based rating. The
user and item attributes are represented by the unique
Foursquare user and venue identifier, respectively. We
employ the user’s home city in Foursquare as the ulocation attribute. Meanwhile, the ilocation attribute is the item’s
inherent location. We use a numeric rating value range of
[1, 3], translated as follows: (a) 3 represents the user is the

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

“mayor” of the venue, (b) 2 represents that the user left a
“tip” at the venue, and (c) 1 represents the user visited the
venue as a completed “to-do” list item. Using this scheme,
a user may have multiple ratings for a venue, in this case
we use the highest rating value.
Data properties. Our experimental data consisted of 22,390
location-based ratings for 4K users for 2K venues all from
the state of Minnesota, USA. We used this reduced data
set in order to focus our quality experiments on a dense
rating sample. Use of dense ratings data has been shown
to be a very important factor when testing and comparing
recommendation quality [17], since use of sparse data (i.e.,
having users or items with very few ratings) tends to cause
inaccuracies in recommendation techniques.
MovieLens: a real data set consisting of spatial user ratings for non-spatial items taken from the popular MovieLens
recommender system [5]. The Foursquare and MovieLens
data are used to test recommendation quality. The
MovieLens data used in our experiments was real movie
rating data taken from the popular MovieLens recommendation system at the University of Minnesota [5]. This data
consisted of 87,025 ratings for 1,668 movies from 814 users.
Each rating was associated with the zip code of the user
who rated the movie, thus giving us a real data set of spatial
user ratings for non-spatial items.
Synthetic: a synthetically generated data set consisting
of spatial user ratings for spatial items for venues in the
state of Minnesota, USA. The synthetic data set we use
in our experiments is generated to contain 2000 users and
1000 items, and 500,000 ratings. Users and items locations
are randomly generated over the state of Minnesota, USA.
Users’ ratings to items are assigned random values between
zero and five. As this data set contains a number of ratings
that is about twenty five times and five times larger than the
foursquare data set and the Movilens data set, we use such
synthetic data set to test scalability and query efficiency.
Unless mentioned otherwise, the default value of M is
0.3, k is 10, the number of pyramid levels is 8, the influence
level is the lowest pyramid level, and MAX_SLEVELS is set
to two. The rest of this section evaluates LARS* recommendation quality (Section 7.1), trade-offs between storage and
locality (Section 7.4), scalability (Section 7.5), and query processing efficiency (Section 7.6). As the system stores its data
structures in main memory, all reported time measurements
represent the CPU time.

7.1

Recommendation Quality for Varying Pyramid
Levels
These experiments test the recommendation quality
improvement that LARS* achieves over the standard (nonspatial) item-based collaborative filtering method using
both the Foursquare and MovieLens data. To test the
effectiveness of our proposed techniques, we test the
quality improvement of LARS* with only travel penalty
enabled (abbr. LARS*-T), LARS* with only user partitioning enabled and M set to one (abbr. LARS*-U), and
LARS* with both techniques enabled and M set to one
(abbr. LARS*). Notice that LARS*-T represents the traditional item-based collaborative filtering augmented with
the travel penalty technique (section 5) to take the distance

(a)

1395

(b)

Fig. 7. Quality experiments for varying locality. (a) Foursquare data.
(b) MovieLens data.

between the querying user and the recommended items
into account. We do not plot LARS with LARS* as both give
the same result for M=1, and the quality experiments are
meant to show how locality increases the recommendation
quality.
Quality Metric. To measure quality, we build each recommendation method using 80% of the ratings from each
data set. Each rating in the withheld 20% represents a
Foursquare venue or MovieLens movie a user is known
to like (i.e., rated highly). For each rating t in this 20%, we
request a set of k ranked recommendations S by submitting
the user and ulocation associated with t. We first calculate
the quality as the weighted sum of the number of occurrences of the item associated with t (the higher the better)
in S . The weight of an item is a value between zero and one
that determines how close the rank of this item from its real
rank. The quality of each recommendation method is calculated and compared against the baseline, i.e., traditional
item-based collaborative filtering. We finally report the ratio
of improvement in quality each recommendation method
achieves over the baseline. The rationale for this metric is
that since each withheld rating represents a real visit to a
venue (or movie a user liked), the technique that produces
a large number of correctly ranked answers that contain
venues (or movies) a user is known to like is considered of
higher quality.
Fig. 7(a) compares the quality improvement of each technique (over traditional collaborative filtering) for varying
locality (i.e., different levels of the adaptive pyramid) using
the Foursquare data. LARS*-T does not use the adaptive
pyramid, thus has constant quality improvement. However,
LARS*-T shows some quality improvement over traditional
collaborative filtering. This quality boost is due to that fact
that LARS*-T uses a travel penalty technique that recommends items within a feasible distance. Meanwhile, the
quality of LARS* and LARS*-U increases as more localized pyramid cells are used to produce recommendation,
which verifies that user partitioning is indeed beneficial and
necessary for location-based ratings. Ultimately, LARS* has
superior performance due to the additional use of travel
penalty. While travel penalty produces moderate quality gain,
it also enables more efficient query processing, which we
observe later in Section 7.6.
Fig. 7(b) compares the quality improvement of LARS*U over CF (traditional collaborative filtering) for varying
locality using the MovieLens data. Notice that LARS* gives
the same quality improvement as LARS*-U because LARS*T do not apply for this dataset since movies are not spatial.
Compared to CF, the quality improvement achieved by

1396

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

(a)

(b)

Fig. 8. Quality experiments for varying answer sizes. (a) Foursquare

data. (b) MovieLens data.

LARS*-U (and LARS*) increases when it produces movie
recommendations from more localized pyramid cells. This
behavior further verifies that user partitioning is beneficial
in providing quality recommendations localized to a querying user location, even when items are not spatial. Quality
decreases (or levels off for MovieLens) for both LARS*-U
and/or LARS* for lower levels of the adaptive pyramid.
This is due to recommendation starvation, i.e., not having
enough ratings to produce meaningful recommendations.

7.2 Recommendation Quality for Varying k
These experiments test recommendation quality improvement of LARS*, LARS*-U, and LARS*-T for different values
of k (i.e., recommendation answer sizes). We do not plot
LARS with LARS* as both gives the same result for M=1,
and the quality experiments are meant to show how the
degree of locality increases the recommendation quality.
We perform experiments using both the Foursquare and
MovieLens data. Our quality metric is exactly the same as
presented previously in Section 7.1.
Fig. 8(a) depicts the effect of the recommendation list size
k on the quality of each technique using the Foursquare
data set. We report quality numbers using the pyramid
height of four (i.e., the level exhibiting the best quality from
Section 7.1 in Fig. 7(a)). For all sizes of k from one to ten,
LARS* and LARS*-U consistently exhibit better quality. In
fact, LARS* consistently achieves better quality over CF for
all k. LARS*-T exhibits similar quality to CF for smaller k
values, but does better for k values of three and larger.
Fig. 8(b) depicts the effect of the recommendation list
size k on the quality of improvement of LARS*-U (and
LARS*) over CF using the MovieLens data. Notice that
LARS* gives the same quality improvement as LARS*-U
because LARS*-T do not apply for this dataset since movies
are not spatial. This experiment was run using a pyramid
hight of seven (i.e., the level exhibiting the best quality
in Fig. 7(b)). Again, LARS*-U (and LARS*) consistently
exhibits better quality than CF for sizes of K from one to ten.
7.3 Recommendation Quality for Varying M
These experiments compares the quality improvement
achieved by both LARS and LARS* for different values of
M. We perform experiments using both the Foursquare and
MovieLens data. Our quality metric is exactly the same as
presented previously in Section 7.1.
Fig. 9(a) depicts the effect of M on the quality of both
LARS and LARS* using the Foursquare data set. Notice that
we enable both the user partitioning and travel penalty

(a)

(b)

Fig. 9. Quality experiments for varying value of M. (a) Foursquare

data. (b) MovieLens data.

techniques for both LARS and LARS*. We report quality
numbers using the pyramid height of four and the number
of recommended items of ten. When M is equal to zero,
both LARS and LARS* exhibit the same quality improvement as M = 0 represents a traditional collaborative
filtering with the travel penalty technique applied. Also,
when M is set to one, both LARS and LARS* achieve
the same quality improvement as a fully maintained pyramid is maintained in both cases. For M values between
zero and one, the quality improvement of both LARS and
LARS* increases for higher values of M due to the increase
in recommendation locality. LARS* achieves better quality
improvement over LARS because LARS* maintains α-Cells
at lower levels of the pyramid.
Fig. 9(b) depicts the effect of M on the quality of
both LARS and LARS* using the Movilens data set.
We report quality improvement over traditional collaborative filtering using the pyramid height of seven and
the number of recommended items set to ten. Similar
to Foursquare data set, the quality improvement of both
LARS and LARS* increases for higher values of M due
to the increase in recommendation locality. For M values between zero and one, LARS* consistently achieves
higher quality improvement over LARS as LARS* maintains more α-Cells at more granular levels of the pyramid
structure.

7.4 Storage Vs. Locality
Fig. 10 depicts the impact of varying M on both the storage and locality in LARS* using the synthetic data set. We
plot LARS*-M=0 and LARS*-M=1 as constants to delineate
the extreme values of M, i.e., M=0 mirrors traditional collaborative filtering, while M=1 forces LARS* to employ a
complete pyramid. Our metric for locality is locality loss
(defined in Section 4.5.2) when compared to a complete

(a)

(b)

Fig. 10. Effect of M on storage and locality (synthetic data).

(a) Storage. (b) Locality.

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

(a)

(b)

Fig. 11. Scalability of the adaptive pyramid (synthetic data).

(a) Storage. (b) Maintenance.

pyramid (i.e., M=1). LARS*-M=0 requires the lowest storage overhead, but exhibits the highest locality loss, while
LARS*-M=1 exhibits no locality loss but requires the most
storage. For LARS*, increasing M results in increased storage overhead since LARS* favors switching cells to α-Cells,
requiring the maintenance of more pyramid cells each with
its own collaborative filtering model. Each additional αCell incurs a high storage overhead over the original data
size as an additional collaborative filtering model needs to
be maintained. Meanwhile, increasing M results in smaller
locality loss as LARS* merges less and maintains more
localized cells. The most drastic drop in locality loss is
between 0 and 0.3, which is why we chose M=0.3 as a
default. LARS* leads to smaller locality loss (≈26% less)
than LARS because LARS* maintains α-Cells below β-Cells
which result in higher locality gain. On the other hand,
LARS* exhibits slightly higher storage cost (≈5% more storage) than LARS due to the fact that LARS* stores the Item
Ratings Statistics Table per each α-Cell and β-Cell.

7.5 Scalability
Fig. 11 depicts the storage and aggregate maintenance overhead required for an increasing number of ratings using the
synthetic data set. We again plot LARS*-M=0 and LARS*M=1 to indicate the extreme cases for LARS*. Fig. 11(a)
depicts the impact of increasing the number of ratings from
10K to 500K on storage overhead. LARS*-M=0 requires
the lowest amount of storage since it only maintains a
single collaborative filtering model. LARS*-M=1 requires
the highest amount of storage since it requires storage
of a collaborative filtering model for all cells (in all levels) of a complete pyramid. The storage requirement of
LARS* is in between the two extremes since it merges
cells to save storage. Fig. 11(b) depicts the cumulative
computational overhead necessary to maintain the adaptive pyramid initially populated with 100K ratings, then
updated with 200K ratings (increments of 50K reported).
The trend is similar to the storage experiment, where
LARS* exhibits better performance than LARS*-M=1 due
to switching some cells from α-Cells to β-Cells. Though
LARS*-M=0 has the best performance in terms of maintenance and storage overhead, previous experiments show
that it has unacceptable drawbacks in quality/locality.
Compared to LARS, LARS* has less maintenance overhead
(≈38% less) due to the fact that the maintenance algorithm
in LARS* avoids the expensive speculative splitting used
by LARS.

(a)
12. Query processing performance
(a) Snapshot queries. (b) Continuous queries.

Fig.

1397

(b)
(synthetic

data).

7.6 Query Processing Performance
Fig. 12 depicts snapshot and continuous query processing performance of LARS, LARS*, LARS*-U (LARS* with
only user partitioning), LARS*-T (LARS* with only travel
penalty), CF (traditional collaborative filtering), and LARS*M=1 (LARS* with a complete pyramid), using the synthetic
data set.
Snapshot queries. Fig. 12(a) gives the effect of various
number of ratings (10K to 500K) on the average snapshot query performance averaged over 500 queries posed
at random locations. LARS* and LARS*-M=1 consistently
outperform all other techniques; LARS*-M=1 is slightly better due to recommendations always being produced from
the smallest (i.e., most localized) CF models. The performance gap between LARS* and LARS*-U (and CF and
LARS*-T) shows that employing the travel penalty technique
with early termination leads to better query response time.
Similarly, the performance gap between LARS* and LARS*T shows that employing user partitioning technique with
its localized (i.e., smaller) collaborative filtering model also
benefits query processing. LARS* performance is slightly
better than LARS as LARS* sometimes maintains more
localized CF models than LARS which incurs less query
processing time.
Continuous queries. Fig. 12(b) provides the continuous
query processing performance of the LARS* variants by
reporting the aggregate response time of 500 continuous
queries. A continuous query is issued once by a user u
to get an initial answer, then the answer is continuously
updated as u moves. We report the aggregate response
time when varying the travel distance of u from 1 to 30
miles using a random walk over the spatial area covered
by the pyramid. CF has a constant query response time for
all travel distances, as it requires no updates since only
a single cell is present. However, since CF is unaware
of user location change, the consequence is poor recommendation quality (per experiments from Section 7.1).
LARS*-M=1 exhibits the worse performance, as it maintains all cells on all levels and updates the continuous
query whenever the user crosses pyramid cell boundaries.
LARS*-U has a lower response time than LARS*-M=1 due
to switching cells from α-Cells to β-Cells: when a cell is
not present on a given influence level, the query is transferred to its next highest ancestor in the pyramid. Since
cells higher in the pyramid cover larger spatial regions,
query updates occur less often. LARS*-T exhibits slightly
higher query processing overhead compared to LARS*U: even though LARS*-T employs the early termination

1398

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

algorithm, it uses a large (system-wide) collaborative filtering model to (re)generate recommendations once users
cross boundaries in the penalty grid. LARS* exhibits a
better aggregate response time since it employs the early
termination algorithm using a localized (i.e., smaller) collaborative filtering model to produce results while also
switching cells to β-Cells to reduce update frequency. LARS
has a slightly better performance than LARS* as LARS
tends to merge more cells at higher levels in the pyramid
structure.

8

R ELATED W ORK

Location-based services. Current location-based services
employ two main methods to provide interesting destinations to users. (1) KNN techniques [22] and variants (e.g.,
aggregate KNN [24]) simply retrieve the k objects nearest
to a user and are completely removed from any notion of
user personalization. (2) Preference methods such as skylines [25] (and spatial variants [26]) and location-based
top-k methods [27] require users to express explicit preference constraints. Conversely, LARS* is the first locationbased service to consider implicit preferences by using
location-based ratings to help users discover new items.
Recent research has proposed the problem of hyper-local
place ranking [28]. Given a user location and query string
(e.g., “French restaurant"), hyper-local ranking provides a
list of top-k points of interest influenced by previously
logged directional queries (e.g., map direction searches
from point A to point B). While similar in spirit to LARS*,
hyper-local ranking is fundamentally different from our
work as it does not personalize answers to the querying user,
i.e., two users issuing the same search term from the same
location will receive exactly the same ranked answer.
Traditional recommenders. A wide array of techniques
are capable of producing recommendations using nonspatial ratings for non-spatial items represented as the triple
(user, rating, item) (see [4] for a comprehensive survey).
We refer to these as “traditional" recommendation techniques. The closest these approaches come to considering
location is by incorporating contextual attributes into statistical recommendation models (e.g., weather, traffic to
a destination) [29]. However, no traditional approach has
studied explicit location-based ratings as done in LARS*.
Some existing commercial applications make cursory use
of location when proposing interesting items to users. For
instance, Netflix displays a “local favorites” list containing popular movies for a user’s given city. However, these
movies are not personalized to each user (e.g., using recommendation techniques); rather, this list is built using
aggregate rental data for a particular city [30]. LARS*, on
the other hand, produces personalized recommendations
influenced by location-based ratings and a query location.
Location-aware recommenders. The CityVoyager system [31] mines a user’s personal GPS trajectory data to
determine her preferred shopping sites, and provides recommendation based on where the system predicts the user
is likely to go in the future. LARS*, conversely, does not
attempt to predict future user movement, as it produces
recommendations influenced by user and/or item locations
embedded in community ratings.

The spatial activity recommendation system [32] mines
GPS trajectory data with embedded user-provided tags in
order to detect interesting activities located in a city (e.g., art
exhibits and dining near downtown). It uses this data to
answer two query types: (a) given an activity type, return
where in the city this activity is happening, and (b) given
an explicit spatial region, provide the activities available
in this region. This is a vastly different problem than we
study in this paper. LARS* does not mine activities from
GPS data for use as suggestions for a given spatial region.
Rather, we apply LARS* to a more traditional recommendation problem that uses community opinion histories to
produce recommendations.
Geo-measured friend-based collaborative filtering [33]
produces recommendations by using only ratings that are
from a querying user’s social-network friends that live in
the same city. This technique only addresses user location
embedded in ratings. LARS*, on the other hand, addresses
three possible types of location-based ratings. More importantly, LARS* is a complete system (not just a recommendation technique) that employs efficiency and scalability
techniques (e.g., partial pyramid structure, early query termination) necessary for deployment in actual large-scale
applications.

9

C ONCLUSION

LARS*, our proposed location-aware recommender system,
tackles a problem untouched by traditional recommender
systems by dealing with three types of location-based
ratings: spatial ratings for non-spatial items, non-spatial ratings for spatial items, and spatial ratings for spatial items.
LARS* employs user partitioning and travel penalty techniques to support spatial ratings and spatial items, respectively. Both techniques can be applied separately or in
concert to support the various types of location-based ratings. Experimental analysis using real and synthetic data
sets show that LARS* is efficient, scalable, and provides
better quality recommendations than techniques used in
traditional recommender systems.

ACKNOWLEDGMENTS
This work was supported in part by the US National
Science Foundation under Grants IIS-0811998, IIS-0811935,
CNS-0708604, IIS-0952977 and in part by a Microsoft
Research Gift.

R EFERENCES
[1] G. Linden, B. Smith, and J. York, “Amazon.com recommendations: Item-to-item collaborative filtering,” IEEE Internet Comput.,
vol. 7, no. 1, pp. 76–80, Jan./Feb. 2003.
[2] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl,
“GroupLens: An open architecture for collaborative filtering of
netnews,” in Proc. CSWC, Chapel Hill, NC, USA, 1994.
[3] The facebook blog. Facebook Places [Online]. Available:
http://tinyurl.com/3aetfs3
[4] G. Adomavicius and A. Tuzhilin, “Toward the next generation of
recommender systems: A survey of the state-of-the-art and possible extensions,” IEEE Trans. Knowl. Data Eng., vol. 17, no. 6,
pp. 734–749, Jun. 2005.
[5] MovieLens [Online]. Available: http://www.movielens.org/
[6] Foursquare [Online]. Available: http://foursquare.com

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

[7] New York Times - A Peek into Netflix Queues [Online]. Available:
http://www.nytimes.com/interactive/2010/01/10/nyregion/
20100110-netflix-map.html
[8] J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel,
“LARS: A location-aware recommender system,” in Proc. ICDE,
Washington, DC, USA, 2012.
[9] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, “Item-based collaborative filtering recommendation algorithms,” in Proc. Int. Conf.
WWW, Hong Kong, China, 2001.
[10] J. S. Breese, D. Heckerman, and C. Kadie, “Empirical analysis
of predictive algorithms for collaborative filtering,” in Proc. Conf.
UAI, San Francisco, CA, USA, 1998.
[11] W. G. Aref and H. Samet, “Efficient processing of window queries
in the pyramid data structure,” in Proc. ACM Symp. PODS, New
York, NY, USA, 1990.
[12] R. A. Finkel and J. L. Bentley, “Quad trees: A data structure for
retrieval on composite keys,” Acta Inf., vol. 4, no. 1, pp. 1–9, 1974.
[13] A. Guttman, “R-trees: A dynamic index structure for spatial
searching,” in Proc. SIGMOD, New York, NY, USA, 1984.
[14] K. Mouratidis, S. Bakiras, and D. Papadias, “Continuous monitoring of spatial queries in wireless broadcast environments,” IEEE
Trans. Mobile Comput., vol. 8, no. 10, pp. 1297–1311, Oct. 2009.
[15] K. Mouratidis and D. Papadias, “Continuous nearest neighbor
queries over sliding windows,” IEEE Trans. Knowl. Data Eng.,
vol. 19, no. 6, pp. 789–803, Jun. 2007.
[16] M. F. Mokbel, X. Xiong, and W. G. Aref, “SINA: Scalable
incremental processing of continuous queries in spatiotemporal
databases,” in Proc. SIGMOD, Paris, France, 2004.
[17] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl,
“Evaluating collaborative filtering recommender systems,” ACM
TOIS, vol. 22, no. 1, pp. 5–53, 2004.
[18] M. J. Carey and D. Kossmann, “On saying "Enough Already!" in
SQL,” in Proc. SIGMOD, New York, NY, USA, 1997.
[19] S. Chaudhuri and L. Gravano, “Evaluating top-k selection
queries,” in Proc. Int. Conf. VLDB, Edinburgh, U.K., 1999.
[20] R. Fagin, A. Lotem, and M. Naor, “Optimal aggregation algorithms for middleware,” in Proc. ACM Symp. PODS, New York,
NY, USA, 2001.
[21] J. Bao, C.-Y. Chow, M. F. Mokbel, and W.-S. Ku, “Efficient evaluation of k-range nearest neighbor queries in road networks,” in
Proc. Int. Conf. MDM, Kansas City, MO, USA, 2010.
[22] G. R. Hjaltason and H. Samet, “Distance browsing in spatial
databases,” ACM TODS, vol. 24, no. 2, pp. 265–318, 1999.
[23] K. Mouratidis, M. L. Yiu, D. Papadias, and N. Mamoulis,
“Continuous nearest neighbor monitoring in road networks,” in
Proc. Int. Conf. VLDB, Seoul, Korea, 2006.
[24] D. Papadias, Y. Tao, K. Mouratidis, and C. K. Hui, “Aggregate
nearest neighbor queries in spatial databases,” ACM TODS,
vol. 30, no. 2, pp. 529–576, 2005.
[25] S. Börzsönyi, D. Kossmann, and K. Stocker, “The skyline operator,” in Proc. ICDE, Heidelberg, Germany, 2001.
[26] M. Sharifzadeh and C. Shahabi, “The spatial skyline queries,” in
Proc. Int. Conf. VLDB, Seoul, Korea, 2006.
[27] N. Bruno, L. Gravano, and A. Marian, “Evaluating top-k queries
over web-accessible databases,” in Proc. ICDE, San Jose, CA, USA,
2002.
[28] P. Venetis, H. Gonzalez, C. S. Jensen, and A. Y. Halevy, “Hyperlocal, directions-based ranking of places,” PVLDB, vol. 4, no. 5,
pp. 290–301, 2011.
[29] M.-H. Park, J.-H. Hong, and S.-B. Cho, “Location-based recommendation system using Bayesian user’s preference model in
mobile devices,” in Proc. Int. Conf. UIC, Hong Kong, China, 2007.
[30] Netflix News and Info - Local Favorites [Online]. Available:
http://tinyurl.com/4qt8ujo
[31] Y. Takeuchi and M. Sugimoto, “An outdoor recommendation system based on user location history,” in Proc. Int. Conf. UIC, Berlin,
Germany, 2006.
[32] V. W. Zheng, Y. Zheng, X. Xie, and Q. Yang, “Collaborative location and activity recommendations with GPS history data,” in
Proc. Int. Conf. WWW, New York, NY, USA, 2010.
[33] M. Ye, P. Yin, and W.-C. Lee, “Location recommendation for
location-based social networks,” in Proc. ACM GIS, New York,
NY, USA , 2010.

1399

Mohamed Sarwat is a Doctoral candidate
in the Department of Computer Science
and Engineering, University of Minnesota,
Minneapolis, MN, USA. He received the bachelor’s degree in computer engineering from
Cairo University, Egypt, in 2007 and the master’s
degree in computer science from the University
of Minnesota in 2011. His current research interests include a broad area of data management
systems, more specifically, database systems,
database support for recommender systems,
personalized databases, database support for location-based services
and for social networking applications, distributed graph databases,
and large scale data management. He was awarded the University of
Minnesota Doctoral Dissertation Fellowship in 2012. His research was
recognized by the Best Research Paper Award at the 12th International
Symposium on Spatial and Temporal Databases, in 2011.

Justin J. Levandoski is a Researcher with
the Database Group at Microsoft Research,
Redmond, WA, USA. He received the bachelor’s
degree at Carleton College, Northfield, MN, USA
in 2003, and the master’s and Ph.D. degrees
from the University of Minnesota, Minneapolis,
MN, USA, in 2008 and 2011, respectively. His
current research interests include a broad range
of topics dealing with large-scale data management systems, such as cloud computing,
database support for new hardware paradigms,
transaction processing, query processing, and support for new dataintensive applications such as social/recommender systems.

Ahmed Eldawy is a Ph.D. student in
the Department of Computer Science
and Engineering, University of Minnesota,
Minneapolis, MN, USA. His current research
interests include spatial data management,
social networks, and cloud computing, such
as building scalable spatial data management
systems over cloud computing platforms. He
received the bachelor’s and master’s degrees
in computer science from Alexandria University,
Egypt, in 2005 and 2010, respectively.

Mohamed F. Mokbel received the B.Sc. and
M.S. degrees from Alexandria University, Egypt,
in 1996 and 1999, respectively, and the
Ph.D. degree from Purdue University, West
Lafayette, IN, USA, in 2005. Currently, he is
an Assistant Professor in the Department of
Computer Science and Engineering, University
of Minnesota, Minneapolis, MN, USA. His current
research interests include advancing the stateof-the-art in the design and implementation of
database engines to cope with the requirements
of emerging applications (e.g., location-based applications and sensor
networks). His research works has been recognized by three Best Paper
Awards at IEEE MASS 2008, MDM 2009, and SSTD 2011. He is a recipient of the US NSF CAREER Award 2010. He has actively participated
in several program committees for major conferences including ICDE,
SIGMOD, VLDB, SSTD, and ACM GIS. He is/was a program Co-Chair
for ACM SIGSPATIAL GIS 2008, 2009, and 2010. He is an ACM and an
IEEE member and a founding member of ACM SIGSPATIAL.

 For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

GeoSpark: A Cluster Computing Framework for
Processing Large-Scale Spatial Data
Jia Yu

Jinxuan Wu

Mohamed Sarwat

School of Computing, Informatics,
and Decision Systems Engineering,
Arizona State University
699 S. Mill Avenue, Tempe, AZ

School of Computing, Informatics,
and Decision Systems Engineering,
Arizona State University
699 S. Mill Avenue, Tempe, AZ

School of Computing, Informatics,
and Decision Systems Engineering,
Arizona State University
699 S. Mill Avenue, Tempe, AZ

jiayu2@asu.edu

jinxuanw@asu.edu

msarwat@asu.edu

ABSTRACT
This paper introduces GeoSpark an in-memory cluster
computing framework for processing large-scale spatial data.
GeoSpark consists of three layers: Apache Spark Layer,
Spatial RDD Layer and Spatial Query Processing Layer.
Apache Spark Layer provides basic Spark functionalities
that include loading / storing data to disk as well as regular RDD operations. Spatial RDD Layer consists of three
novel Spatial Resilient Distributed Datasets (SRDDs) which
extend regular Apache Spark RDDs to support geometrical
and spatial objects. GeoSpark provides a geometrical operations library that accesses Spatial RDDs to perform basic
geometrical operations (e.g., Overlap, Intersect). System
users can leverage the newly deﬁned SRDDs to eﬀectively
develop spatial data processing programs in Spark. The
Spatial Query Processing Layer eﬃciently executes spatial
query processing algorithms (e.g., Spatial Range, Join, KNN
query) on SRDDs. GeoSpark also allows users to create a
spatial index (e.g., R-tree, Quad-tree ) that boosts spatial
data processing performance in each SRDD partition. Preliminary experiments show that GeoSpark achieves better
run time performance than its Hadoop-based counterparts
(e.g., SpatialHadoop).

Categories and Subject Descriptors
H.2.4 [DATABASE MANAGEMENT]: Systems—Distributed databases; H.2.8 [DATABASE MANAGEMENT]: Database Applications—Spatial databases and
GIS

Keywords
Cluster computing; Large-scale data; Spatial data

1. INTRODUCTION
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from Permissions@acm.org.
SIGSPATIAL’15 November 03-06, 2015, Bellevue, WA, USA
Copyright 2015 ACM ISBN 978-1-4503-3967-4/15/11 $15.00.
http://dx.doi.org/10.1145/2820783.2820860

The volume of available spatial data increased tremendously. Such data includes but not limited to: weather
maps, socioeconomic data, vegetation indices, and more.
Moreover, novel technology allows hundreds of millions of
users to use their mobile devices to access their healthcare
information and bank accounts, interact with friends, buy
stuﬀ online, search interesting places to visit on-the-go, ask
for driving directions, and more. Making sense of such spatial data will be beneﬁcial for several applications that may
transform science and society. Challenges to building such
platform are as follows: Challenge I: System Scalability. The
underlying database system must be able to digest Petabytes
of spatial data, eﬀectively stores it, and allows applications
to eﬃciently retrieve it when necessary. Challenge II: Interactive Performance. The underlying spatial data processing
system must ﬁgure out eﬀective ways to process user’s request in a sub-second response time.
Apache Spark is an in-memory cluster computing system.
Spark provides a novel data abstraction called resilient distributed datasets (RDDs) [9] that are collections of objects
partitioned across a cluster of machines. Each RDD is built
using parallelized transformations (ﬁlter, join or groupBy)
that could be traced back to recover the RDD data. In
memory RDDs allow Spark to outperform existing models
(MapReduce). Unfortunately, Spark does not provide support for spatial data and operations. Hence, users need to
perform the tedious task of programming their own spatial
data processing jobs on top of Spark.
This paper introduces GeoSpark 1 an in-memory cluster computing system for processing large-scale spatial data.
GeoSpark extends the core of Apache Spark to support
spatial data types, indexes, and operations. In other words,
the system extends the resilient distributed datasets (RDDs)
concept to support spatial data. The key contributions of
this paper are as follows: (1) GeoSpark as a full-ﬂedged
cluster computing framework to load, process, and analyze
large-scale spatial data in Apache Spark. (2) A set of out-ofthe-box Spatial Resilient Distributed Dataset (SRDD) types
(e.g., Point RDD and Polygon RDD) that provide in house
support for geometrical and distance operations. SRDDS
provides an Application Programming Interface (API) for
Apache Spark programmers to easily develop their spatial
analysis programs. (3) Spatial data indexing strategies that
partition the input Spatial RDD using a grid structure and
assign grids to machines for parallel execution. GeoSpark
1

GeoSpark website: http://geospark.datasyslab.org























	
	
















Figure 2: SRDD partitioning


	










Figure 1: GeoSpark Overview

also adaptively decides whether a spatial index needs to be
created locally on a Spatial RDD partition to strike a balance between the run time performance and memory/cpu
utilization in the cluster. Experiments show that GeoSpark
achieves better run time performance than its Hadoop-based
counterparts (e.g., SpatialHadoop).
The rest of this paper is organized as follows. Section 2
highlights the related work. GeoSpark architecture is
given in Section 3. Preliminary experiments that evaluate
GeoSpark are given in Section 4. Finally, Section 5 concludes the paper.

2. BACKGROUND AND RELATED WORK
Spatial Database Systems. Spatial database operations are vital for spatial analysis and spatial data mining.
Spatial range queries inquire about certain spatial objects
exist in a certain area (e.g., Return all parks in Phoenix).
Spatial join queries are queries that combine two datasets
or more with a spatial predicate, such as distance relations
(e.g., ﬁnd the parks that have rivers in Phoenix). Spatial
k-Nearest Neighbors queries ﬁnd the k nearest objects to a
given spatial object (e.g., show the 10 nearby restaurants).
Spatial query processing algorithms usually make use of spatial indexes to reduce the query latency. For instance, RTree [3] provides an eﬃcient data partitioning strategy to
eﬃciently index spatial data. Its key idea is that group
nearby objects and put them in the next higher level node
of the tree. Quad-Tree [8] is also a spatial index that recursively divides a two-dimensional space into four quadrants.
Parallel and Distributed Spatial Data Processing.
As the development of distributed data processing system,
more and more people in geospatial area direct their attention to deal with massive geospatial data with distributed
frameworks. Hadoop-GIS [1] utilizes global partition indexing and customizable on demand local spatial indexing
to achieve eﬃcient query processing. SpatialHadoop [2], a
comprehensive extension to Hadoop, has native support for
spatial data by modifying the underlying code of Hadoop.
MD-HBase [6] extends HBase, a non-relational database

runs on top of Hadoop, to support multidimensional indexes
which allows for eﬃcient retrieval of points using range and
kNN queries. Parallel SECONDO [4] combines Hadoop with
SECONDO, a database which can handle non-standard data
types, like spatial data, usually not supported by standard
systems. Although these systems have well-developed functions, all of them are implemented on Hadoop framework.
That means they cannot avoid the disadvantages of Hadoop,
especially a large number of reads and writes on disks.

3.

GEOSPARK ARCHITECTURE

As depicted in Figure 1, GeoSpark consists of three main
layers: (1) Apache Spark Layer: that consists of regular
operations that are natively supported by Apache Spark.
These native functions are responsible for loading / saving
data from / to persistent storage (e.g., stored on local disk or
Hadoop ﬁle system HDFS). (2) Spatial Resilient Distributed
Dataset (SRDD) Layer (Section 3.1). (3) Spatial Query Processing Layer (Section 3.2).

3.1

Spatial RDD (SRDD) Layer

This layer extends Spark with spatial RDDs (SRDDs)
that eﬃciently partition SRDD data elements across machines and introduces novel parallelized spatial transformations and actions (for SRDD) that provide a more intuitive
interface for users to write spatial data analytics programs.
The SRDD layer consists of three new RDDs: PointRDD,
RectangleRDD and PolygonRDD. One useful Geometrical
operations library is also provided for every spatial RDD.
Spatial Objects Support. GeoSpark supports various
spatial data input format (e.g., Comma Separated Value,
Tab Separated Value and Well-Known Text). Each type
of spatial objects is stored in a SRDD, PointRDD, RectangleRDD or PolygonRDD. GeoSpark provides a set of
geometrical operations which is called Geometrical Operations Library. This library natively supports geometrical
operations. For example, Overlap(): Finds all of the internal objects which are intersected with others in geometry;
MinimumBoundingRectangle(): Finds the minimum bounding rectangles for each object in a Spatial RDD or return
a large minimum bounding rectangle which contains all of
the internal objects in a Spatial RDD; Union(): Returns the
union polygon of all polygons in this RDD.
SRDD Partitioning. GeoSpark automatically partitions all loaded Spatial RDDs by creating one global grid
ﬁle for data partitioning. The main idea for assigning each
element in a Spatial RDD to the same 2-Dimensional spatial
grid space is as follows: Firstly, split the spatial space into a





	


 
 








Figure 3: Query execution model

number of equal geographical size grid cells which compose
a global grid ﬁle. Then traverse each element in the SRDD
and assign this element to a grid cell if the element overlaps with this grid cell. If one element intersects with two
or more grid cells, then duplicate this element and assign
diﬀerent grid IDs to the copies of this element. Figure 2 depicts tweets in the U.S. at a particular moment, tweets and
states are assigned to respective grid cells.
SRDD Indexing. Spatial indexes like Quad-Tree and
R-Tree are provided in Spatial IndexRDDs which inherit
from Spatial RDDs. Users are able to initialize a Spatial IndexRDD. Moreover, GeoSpark adaptively decides whether
a local spatial index should be created for a certain Spatial IndexRDD partition based on a tradeoﬀ between the
indexing overhead (memory and time) on one-hand and the
query selectivity as well as the number of spatial objects on
the other hand.

3.2 Spatial Query Processing Layer
This layer supports spatial queries (e.g., Range query and
Join query) for large-scale spatial datasets. After geometrical objects are stored and processed in the Spatial RDD
layer, user may invoke a spatial query provided in Spatial
Query Processing Layer. GeoSpark processes such query
and returns the ﬁnal results to the user. Figure 3 gives the
general execution model followed by GeoSpark . This execution model implements the algorithms proposed by [5]
and [10]. To accelerate a spatial query, GeoSpark leverages the grid partitioned Spatial RDDs, spatial indexing, the
fast in-memory computation and DAG scheduler of Apache
Spark to parallelize the query execution.
Spatial Range Query. GeoSpark executes the spatial
range query algorithm following the execution model: Load
target dataset, partition data, create a spatial index on each
SRDD partition if necessary, broadcast the query window
to each SRDD partition, check the spatial predicate in each
partition, and remove spatial objects duplicates that existed
due to the data partitioning phase.
Spatial Join Query. GeoSpark executes the parallel
spatial join query following the execution model. GeoSpark
ﬁrst partitions the data from the two input SRDDs as well
as creates local spatial indexes (if required) for the SRDD
which is being queried. Then it joins the two datasets by
their keys which are grid IDs. For the spatial objects (from
the two SRDDs) that have the same grid ID, GeoSpark calculates their spatial relations. If two elements from two
SRDDS are overlapped, they are kept in the ﬁnal results.
The algorithm continues to group the results for each rectangle. The grouped results are in the following format: Rect-

angle, Point, Point, ... Finally, the algorithm removes the
duplicated points and returns the result to other operations
or saves the ﬁnal result to disk.
Spatial KNN Query. To process a Spatial KNN query,
GeoSpark uses a heap based top-k algorithm[7], which contains two phases: selection and merge. It takes a partitioned
SRDD, a point P and a number k as inputs. To calculate
the k nearest objects around point P , in the selection phase,
for each SRDD partition GeoSpark calculates the distances
between each object to the given point P , then maintains a
local heap by adding or removing elements based on the distances. This heap contains the nearest k objects around the
given point P . For IndexedSRDD, the system can utilize the
local indexes to reduce the query time. After the selection
phase, GeoSpark merges results from each partition, keeps
the nearest k elements that have the shortest distances to P
and outputs the result.

4.

EXPERIMENTS

This section provides preliminary experimental evaluation that studies the run time performance of the
following large-scale spatial data processing systems:
(1) GeoSpark_NoIndex | QuadTree | RTree: GeoSpark
approach without spatial index, with spatial Quad-Tree
or R-Tree index. In these approaches, data is partitioned according grids. Required spatial indexes are created on each partition after data partitioned. (2) SpatialHadoop_NoIndex | RTree: SpatialHadoop approach without spatial index or with spatial R-Tree index.
Experimental Setup. Our cluster setting on Amazon
EC2 is as follows: (1) CPU per worker: 8 Intel Xeon Processors operating at 2.5 GHz with Turbo up to 3.3 GHz.
(2) Memory per worker: 61 GB in total and 50 GB registered memory in Spark and Hadoop. (3) Storage per worker:
Amazon general purpose SSD. We deploy Ganglia, a scalable
distributed monitoring system for high performance computing systems such as clusters, on our Amazon EC2 experimental cluster.
Datasets. We use three real spatial datasets extracted
from TIGER ﬁles in our experiments: Zcta510 1.5 GB
dataset, Areawater 6.5 GB dataset and Edges 62 GB
dataset. They contain all the cities, all the lakes and all
the meaningful boundaries in the US in rectangle format
correspondingly. All of the datasets are preprocessed by
SpatialHadoop and are open to the public on its website [2].

4.1

Impact of Data Size

This section compares GeoSpark on TIGER Areawater
6.5 GB dataset with TIGER Edges 62 GB dataset as well as
SpatialHadoop. They are tested on 16 nodes cluster. Their
performance are shown in Figure 5. As depicted in Figure 5, GeoSpark and SpatialHadoop cost more run time
on the large dataset than that on the small one. However,
GeoSpark achieves much better run time performance than
SpatialHadoop in both datasets. This superiority is more
obvious on the small dataset. The reason is that GeoSpark
can cache more percentage of the intermediate data in memory on the small scale input than that on the large one. That
accelerates the processing speed.

4.2

Performance of Spatial Iterative Analysis

Spatial co-location pattern recognition is deﬁned as two
or more species are ofter located in a neighborhood rela-

1
3
5
7

9
11
13
15
17

d o u b l e t h r e s h o l d = THRESHOLD;
double baseDistance = 1 . 0 ;
double I n t e r v a l D i s t a n c e = 0 . 5 ;
i n t c o u n t e r =0;
double CoL oc at io n C oef f i cie n t =0.0;
// I n i t i a l i z e IndexedPointRDD
IndexedPointRDD t a r g e t =
new IndexedPointRDD ( SparkContext ,
DatasetLocation ) ;
// I t e r a t i v e Adja cency Matrix C a l c u l a t i o n
w h i l e ( C o L o c a t i o n C o e f f i c i e n t >t h r e s h o l d ) {
PairRDD glbAdjMat =
t a r g e t . S p a t i a l J o i n Q u e r y ( t a r g e t , WITHIN,
baseDistance + counter ∗ IntervalDistance ) ;
C o L o c a t i o n C o e f f i c i e n t=
C a l c u l a t e C o L o c a t i o n ( glbAdjMat ) ;
c o u n t e r ++;
}
r e t u r n b a s e D i s t a n c e + ( c o u n t e r −1) ∗
IntervalDistance ;

Figure 4: Adjacency Matrix (Java code) in GeoSpark

Figure 6: Run Time Performance for Spatial Co-location
Pattern Recognition
indexing and query processing algorithms in Apache Spark
to eﬃciently analyze spatial data at scale. Experiments
on data sizes and spatial analysis show that GeoSpark
achieves better run time performance than its MapReducebased counterparts (e.g., SpatialHadoop). The proposed
ideas are packaged into an open source software artifact.
In the future, we envision GeoSpark to be used by Earth
and Space Scientists, Geographers, Politicians, Commercial
Institutions to analyze spatial data at scale. We also expect
the scientiﬁc community will contribute to GeoSpark and
add new functionalities on top-of it that serve novel spatial
data analysis applications.

6.
Figure 5: Run Time Performance for Spatial Join Over Different Spatial Datasets

tionship. It usually executes multiple times to form a 2dimension curve for observation. This calculation needs the
adjacent matrix between two type of objects which is the
result of a join query. Sample code for ﬁnding adjacent matrix is given in Figure 4. We iteratively query GeoSpark
SRDDs two times with diﬀerent distances which can be
deﬁned as neighborhood relationships in adjacent matrix.
Since SpatialHadoop doesn’t natively support iterative jobs,
we have to run SpatialHadoop_RTree two times for a reasonable comparison. We use the ﬁrst point column in both
of TIGER Zcta 1.5 GB dataset and TIGER Edges 62 GB
dataset and join them.
As shown in Figure 6, GeoSpark outperforms SpatialHadoop in spatial co-location. And their performances are
also improved when we increase the number of machines per
cluster. GeoSpark only costs the quarter time of SpatialHadoop. The main reason behind is that GeoSpark caches
these datasets in memory with SRDDs automatically after
loads from the storage system. The iterative jobs like spatial co-location can invoke these SRDDs multiple times from
memory without any data transformation and data loading. SpatialHadoop has to read and transform the original
datasets again and again.

5. CONCLUSION AND FUTURE WORK
This paper introduced GeoSpark an in-memory cluster
computing framework for processing large-scale spatial data.
GeoSpark provides an API for Apache Spark programmers to easily develop spatial analysis applications. Moreover, GeoSpark provides native support for spatial data

REFERENCES

[1] A. Aji, F. Wang, H. Vo, R. Lee, Q. Liu, X. Zhang, and
J. H. Saltz. Hadoop-GIS: A High Performance Spatial
Data Warehousing System over MapReduce. PVLDB,
6(11):1009–1020, 2013.
[2] A. Eldawy and M. F. Mokbel. A demonstration of
spatialhadoop: An eﬃcient mapreduce framework for
spatial data. PVLDB, 6(12):1230–1233, 2013.
[3] A. Guttman. R-trees: a dynamic index structure for
spatial searching. In SIGMOD, 1984.
[4] J. Lu and R. H. Guting. Parallel Secondo: Boosting
Database Engines with Hadoop. In ICPADS, pages
738 –743, 2012.
[5] G. Luo, J. F. Naughton, and C. J. Ellmann. A
non-blocking parallel spatial join algorithm. In Data
Engineering, 2002. Proceedings. 18th International
Conference on, pages 697–705. IEEE, 2002.
[6] S. Nishimura, S. Das, D. Agrawal, and A. E. Abbadi.
MD-Hbase: A Scalable Multi-dimensional Data
Infrastructure for Location Aware Services. In MDM,
pages 7–16, 2011.
[7] N. Roussopoulos, S. Kelley, and F. Vincent. Nearest
neighbor queries. In ACM SIGMOD record,
volume 24, pages 71–79. ACM, 1995.
[8] H. Samet. The quadtree and related hierarchical data
structures. ACM Computing Surveys (CSUR),
16(2):187–260, 1984.
[9] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,
M. McCauly, M. J. Franklin, S. Shenker, and I. Stoica.
Resilient Distributed Datasets: A Fault-Tolerant
Abstraction for In-Memory Cluster Computing. In
NSDI, pages 15–28, 2012.
[10] X. Zhou, D. J. Abel, and D. Truﬀet. Data partitioning
for parallel spatial join processing. Geoinformatica,
2(2):175–204, 1998.

2012 IEEE 28th International Conference on Data Engineering

LARS: A Location-Aware Recommender System
Justin J. Levandoski 1§ , Mohamed Sarwat 2 , Ahmed Eldawy 3 , Mohamed F. Mokbel 4
1

2−4
1

Microsoft Research, Redmond, WA, USA
Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA

justin.levandoski@microsoft.com,

2

sarwat@cs.umn.edu,

eldawy@cs.umn.edu,

866WDWH
0LQQHVRWD

Abstract—This paper proposes LARS, a location-aware recommender system that uses location-based ratings to produce
recommendations. Traditional recommender systems do not consider spatial properties of users nor items; LARS, on the other
hand, supports a taxonomy of three novel classes of locationbased ratings, namely, spatial ratings for non-spatial items, nonspatial ratings for spatial items, and spatial ratings for spatial items.
LARS exploits user rating locations through user partitioning, a
technique that inﬂuences recommendations with ratings spatially
close to querying users in a manner that maximizes system
scalability while not sacriﬁcing recommendation quality. LARS
exploits item locations using travel penalty, a technique that
favors recommendation candidates closer in travel distance to
querying users in a way that avoids exhaustive access to all spatial
items. LARS can apply these techniques separately, or together,
depending on the type of location-based rating available. Experimental evidence using large-scale real-world data from both the
Foursquare location-based social network and the MovieLens
movie recommendation system reveals that LARS is efﬁcient,
scalable, and capable of producing recommendations twice as
accurate compared to existing recommendation approaches.

:LVFRQVLQ

)ORULGD

7RS0RYLH*HQUHV $YJ5DWLQJ
)LOP1RLU

:DU

'UDPD

'RFXPHQWDU\


:DU

)LOP1RLU

0\VWHU\

5RPDQFH

)DQWDQV\

$QLPDWLRQ

:DU

0XVLFDO

(a) Movielens preference locality
Fig. 1.

4

mokbel@cs.umn.edu

8VHUVIURP

9LVLWHGYHQXHVLQ

9LVLWV

(GLQD01

0LQQHDSROLV01
(GLQD01
(GHQ3UDULH01

5REELQVGDOH01

%URRNO\Q3DUN01











)DOFRQ+HLJKWV
01

5REELQVGDOH01
0LQQHDSROLV01
6W3DXO01
0LQQHDSROLV01
5RVHYLOOH01

(b) Foursquare preference locality

Preference locality in location-based ratings.

In this paper, we propose LARS, a novel location-aware
recommender system built speciﬁcally to produce high-quality
location-based recommendations in an efﬁcient manner. LARS
produces recommendations using a taxonomy of three types
of location-based ratings within a single framework: (1) Spatial ratings for non-spatial items, represented as a four-tuple
(user, ulocation, rating, item), where ulocation represents a
user location, for example, a user located at home rating a
book; (2) non-spatial ratings for spatial items, represented as
a four-tuple (user, rating, item, ilocation), where ilocation
represents an item location, for example, a user with unknown
location rating a restaurant; (3) spatial ratings for spatial
items, represented as a ﬁve-tuple (user, ulocation, rating,
item, ilocation), for example, a user at his/her ofﬁce rating
a restaurant visited for lunch. Traditional rating triples can be
classiﬁed as non-spatial ratings for non-spatial items and do
not ﬁt this taxonomy.

I. I NTRODUCTION
Recommender systems make use of community opinions
to help users identify useful items from a considerably large
search space (e.g., Amazon inventory [1], Netﬂix movies [2]).
The technique used by many of these systems is collaborative
ﬁltering (CF) [3], which analyzes past community opinions
to ﬁnd correlations of similar users and items to suggest
k personalized items (e.g., movies) to a querying user u.
Community opinions are expressed through explicit ratings
represented by the triple (user, rating, item) that represents
a user providing a numeric rating for an item.
Currently, myriad applications can produce location-based
ratings that embed user and/or item locations. For example,
location-based social networks (e.g., Foursquare [4] and Facebook Places [5]) allow users to “check-in” at spatial destinations (e.g., restaurants) and rate their visit, thus are capable of
associating both user and item locations with ratings. Such ratings motivate an interesting new paradigm of location-aware
recommendations, whereby the recommender system exploits
the spatial aspect of ratings when producing recommendations.
Existing recommendation techniques [6] assume ratings are
represented by the (user, rating, item) triple, thus are illequipped to produce location-aware recommendations.

A. Motivation: A Study of Location-Based Ratings
The motivation for our work comes from analysis of two
real-world location-based rating datasets: (1) a subset of the
well-known MovieLens dataset [7] containing approximately
87K movie ratings associated with user zip codes (i.e., spatial ratings for non-spatial items) and (2) data from the
Foursquare [4] location-based social network containing user
visit data for 1M users to 643K venues across the United States
(i.e., spatial ratings for spatial items). Appendix B provides
further details of both datasets. In our analysis we consistently
observed two interesting properties that motivate the need for
location-aware recommendation techniques.
Preference locality. Preference locality suggests users from
a spatial region (e.g., neighborhood) prefer items (e.g., movies,
destinations) that are manifestly different than items preferred
by users from other, even adjacent, regions. Figure 1(a) lists
the top-4 movie genres using average MovieLens ratings of
users from different U.S. states. While each list is different,
the top genres from Florida differ vastly from the others.

This work is supported in part by the National Science Foundation under
Grants IIS-0811998, IIS-0811935, CNS-0708604, IIS-0952977 and by a
Microsoft Research Gift
§Work done while at the University of Minnesota

1084-4627/12 $26.00 © 2012 IEEE
DOI 10.1109/ICDE.2012.54

3

450

LARS produces recommendations using non-spatial ratings
for spatial items, i.e., the tuple (user, rating, item, ilocation),
by using travel penalty, a technique that exploits travel locality. This technique penalizes recommendation candidates
the further they are in travel distance to a querying user. The
challenge here is to avoid computing the travel distance for
all spatial items to produce the list of k recommendations, as
this will greatly consume system resources. LARS addresses
this challenge by employing an efﬁcient query processing
framework capable of terminating early once it discovers that
the list of k answers cannot be altered by processing more
recommendation candidates. To produce recommendations using spatial ratings for spatial items, i.e., the tuple (user,
ulocation, rating, item, ilocation) LARS employs both the user
partitioning and travel penalty techniques to address the user
and item locations associated with the ratings. This is a salient
feature of LARS, as the two techniques can be used separately,
or in concert, depending on the location-based rating type
available in the system.

Florida’s list contains three genres (“Fantasy”, “Animation”,
“Musical”) not in the other lists. This difference implies movie
preferences are unique to speciﬁc spatial regions, and conﬁrms
previous work from the New York Times [8] that analyzed
Netﬂix user queues across U.S. zip codes and found similar
differences. Meanwhile, Figure 1(b) summarizes our observation of preference locality in Foursquare by depicting the visit
destinations for users from three adjacent Minnesota cities.
Each sample exhibits diverse behavior: users from Falcon
Heights, MN favor venues in St. Paul, MN (17% of visits)
Minneapolis (13%), and Roseville, MN (10%), while users
from Robbinsdale, MN prefer venues in Brooklyn Park, MN
(32%) and Robbinsdale (20%). Preference locality suggests
that recommendations should be inﬂuenced by location-based
ratings spatially close to the user. The intuition is that localization inﬂuences recommendation using the unique preferences
found within the spatial region containing the user.
Travel locality. Our second observation is that, when recommended items are spatial, users tend to travel a limited distance
when visiting these venues. We refer to this property as “travel
locality.” In our analysis of Foursquare data, we observed
that 45% of users travel 10 miles or less, while 75% travel
50 miles or less. This observation suggests that spatial items
closer in travel distance to a user should be given precedence
as recommendation candidates. In other words, a recommendation loses efﬁcacy the further a querying user must travel
to visit the destination. Existing recommendation techniques
do not consider travel locality, thus may recommend users
destinations with burdensome travel distances (e.g., a user in
Chicago receiving restaurant recommendations in Seattle).

We experimentally evaluate LARS using real location-based
ratings from Foursquare [4] and MovieLens [7], along with
a generated user workload of both snapshot and continuous
queries. Our experiments show LARS is scalable to real
large-scale recommendation scenarios. Since we have access
to real data, we also evaluate recommendation quality by
building LARS with 80% of the spatial ratings and testing
recommendation accuracy with the remaining 20% of the
(withheld) ratings. We ﬁnd LARS produces recommendations
that are twice as accurate (i.e., able to better predict user
preferences) compared to traditional collaborative ﬁltering. In
summary, the contributions of this paper are as follows:

B. Our Contribution: LARS - A Location-Aware Recommender
Like traditional recommender systems, LARS suggests k
items personalized for a querying user u. However, LARS is
distinct in its ability to produce location-aware recommendations using each of the three types of location-based rating
within a single framework.
LARS produces recommendations using spatial ratings for
non-spatial items, i.e., the tuple (user, ulocation, rating, item),
by employing a user partitioning technique that exploits
preference locality. This technique uses an adaptive pyramid
structure to partition ratings by their user location attribute
into spatial regions of varying sizes at different hierarchies.
For a querying user located in a region R, we apply an
existing collaborative ﬁltering technique that utilizes only the
ratings located in R. The challenge, however, is to determine
whether all regions in the pyramid must be maintained in order
to balance two contradicting factors: scalability and locality .
Maintaining a large number of regions increases locality
(i.e., recommendations unique to smaller spatial regions),
yet adversely affects system scalability because each region
requires storage and maintenance of a collaborative ﬁltering
data structure necessary to produce recommendations (i.e., the
recommender model). The LARS pyramid dynamically adapts
to ﬁnd the right pyramid shape that balances scalability and
recommendation locality.

451

•

•

•

We provide a novel classiﬁcation of three types of
location-based ratings not supported by existing recommender systems: spatial ratings for non-spatial items,
non-spatial ratings for spatial items, and spatial ratings
for spatial items.
We propose LARS, a novel location-aware recommender
system capable of using three classes of location-based
ratings. Within LARS, we propose: (a) a user partitioning technique that exploits user locations in a way
that maximizes system scalability while not sacriﬁcing
recommendation locality and (b) a travel penalty technique that exploits item locations and avoids exhaustively
processing all spatial recommendation candidates.
We provide experimental evidence that LARS scales to
large-scale recommendation scenarios and provides better
quality recommendations than traditional approaches.

This paper is organized as follows: Section II gives an
overview of LARS. Sections III, IV, and V cover LARS
recommendation techniques using spatial ratings for nonspatial items, non-spatial ratings for spatial items, and spatial
ratings for spatial items, respectively. Section VI provides
experimental analysis. Section VII covers related work, while
Section VIII concludes the paper.

8VHUV
X N 

XM







LT







LS

6LPLODULW\/LVW

,WHP



,WHPV

LS



Fig. 2.

XN







XM

L S L T 
VLP

FRUDWHG

LS

LT



LU



LV



LT

LU

L[



L\



L]



Fig. 3.



D5DWLQJVPDWUL[









Item-based similarity calculation.

of ru,l , a user u’s rating for a related item l ∈ L weighted by
sim(i,l), the similarity of l to candidate item i, then normalized
by the sum of similarity scores between i and l. The user
receives as recommendations the top-k items ranked by P(u,i) .
Computing Similarity. To compute sim(ip , iq ), we represent each item as a vector in the user-rating space of the rating
matrix. For instance, Figure 3 depicts vectors for items ip and
iq from the matrix in Figure 2(a). Many similarity functions
have been proposed (e.g., Pearson Correlation, Cosine); we
use the Cosine similarity in LARS due to its popularity:

E,WHPEDVHG&)PRGHO

Item-based CF model generation.

II. LARS OVERVIEW
This section provides an overview of LARS by discussing
the query model and the collaborative ﬁltering method.
A. LARS Query Model
Users (or applications) provide LARS with a user id U ,
numeric limit K, and location L; LARS then returns K
recommended items to the user. LARS supports both snapshot
(i.e., one-time) queries and continuous queries, whereby a user
subscribes to LARS and receives recommendation updates as
her location changes. The technique LARS uses to produce
recommendations depends on the type of location-based rating
available in the system. Query processing support for each type
of location-based rating is discussed in Sections III to V.

sim(ip , iq ) =

ip · iq
ip iq 

(2)

This score is calculated using the vectors’ co-rated dimensions,
e.g., the Cosine similarity between ip and iq in Figure 3
is .7 calculated using the circled co-rated dimensions. Cosine
distance is useful for numeric ratings (e.g., on a scale [1,5]).
For unary ratings, other similarity functions are used (e.g.,
absolute sum [10]).
While we opt to use item-based CF in this paper, no
factors disqualify us from employing other recommendation
techniques. For instance, we could easily employ user-based
CF [6], that uses correlations between users (instead of items).

B. Item-Based Collaborative Filtering
LARS uses item-based collaborative ﬁltering (abbr. CF)
as its primary recommendation technique, chosen due to its
popularity and widespread adoption in commercial systems
(e.g., Amazon [1]). Collaborative ﬁltering (CF) assumes a
set of n users U = {u1 , ..., un } and a set of m items
I = {i1 , ..., im }. Each user uj expresses opinions about a set
of items Iuj ⊆ I. Opinions can be a numeric rating (e.g., the
Netﬂix scale of one to ﬁve stars [2]), or unary (e.g., Facebook
“check-ins” [5]). Conceptually, ratings are represented as a
matrix with users and items as dimensions, as depicted in
Figure 2(a). Given a querying user u, CF produces a set of
k recommended items Ir ⊂ I that u is predicted to like the
most.
Phase I: Model Building. This phase computes a similarity
score sim(ip ,iq ) for each pair of objects ip and iq that have
at least one common rating by the same user (i.e., co-rated
dimensions). Similarity computation is covered below. Using
these scores, a model is built that stores for each item i ∈ I, a
list L of similar items ordered by a similarity score sim(ip ,iq ),
2
as depicted in Figure 2(b). Building this model is an O( RU )
process, where R and U are the number of ratings and users,
respectively. It is common to truncate the model by storing,
for each list L, only the n most similar items with the highest
similarity scores [9]. The value of n is referred to as the model
size and is usually much less than |I|.
Phase II: Recommendation Generation. Given a querying
user u, recommendations are produced by computing u’s
predicted rating P(u,i) for each item i not rated by u [9]:

l∈L sim(i, l) ∗ ru,l
(1)
P(u,i) = 
l∈L |sim(i, l)|

III. S PATIAL U SER R ATINGS FOR
N ON -S PATIAL I TEMS
This section describes how LARS produces recommendations using spatial ratings for non-spatial items represented by
the tuple (user, ulocation, rating, item). The idea is to exploit
preference locality, i.e., the observation that user opinions
are spatially unique (based on analysis in Section I-A). We
identify three requirements for producing recommendations
using spatial ratings for non-spatial items: (1) Locality: recommendations should be inﬂuenced by those ratings with user
locations spatially close to the querying user location (i.e., in
a spatial neighborhood); (2) Scalability: the recommendation
procedure and data structure should scale up to large number
of users; (3) Inﬂuence: system users should have the ability to
control the size of the spatial neighborhood (e.g., city block,
zip code, or county) that inﬂuences their recommendations.
LARS achieves its requirements by employing a user partitioning technique that maintains an adaptive pyramid structure,
where the shape of the adaptive pyramid is driven by the
three goals of locality, scalability, and inﬂuence. The idea
is to adaptively partition the rating tuples (user, ulocation,
rating, item) into spatial regions based on the ulocation
attribute. Then, LARS produces recommendations using any
existing collaborative ﬁltering method (we use item-based
CF) over the remaining three attributes (user, rating, item)
of only the ratings within the spatial region containing the
querying user. We note that ratings can come from users with

Before this computation, we reduce each similarity list L to
contain only items rated by user u. The prediction is the sum

452

0RGHO &,'

earlier, the model in C is built using only the spatial ratings
associated with user locations within C.
In addition to traditional recommendation queries (i.e.,
snapshot queries), LARS also supports continuous queries and
can account for the inﬂuence requirement for each user as
follows.
Continuous queries. LARS evaluates a continuous query
in full once it is issued, and sends recommendations back
to a user U as an initial answer. LARS then monitors the
movement of U using her location updates. As long as U
does not cross the boundary of her current grid cell, LARS
does nothing as the initial answer is still valid. Once U crosses
a cell boundary, LARS reevaluates the recommendation query
for the new cell and only sends incremental updates [13] to the
last reported answer. Like snapshot queries, if a cell at level h
is not maintained, the query is temporarily transferred higher
in the pyramid to the nearest maintained ancestor cell. Note
that since higher-level cells maintain larger spatial regions,
the continuous query will cross spatial boundaries less often,
reducing the amount of required recommendation updates.
Inﬂuence level. LARS addresses the inﬂuence requirement
by allowing querying users to specify an optional inﬂuence
level (in addition to location L and limit K) that controls
the size of the spatial neighborhood used to inﬂuence their
recommendations. An inﬂuence level I maps to a pyramid
level and acts much like a “zoom” level in Google or Bing
maps (e.g., city block, neighborhood, entire city). The level I
instructs LARS to process the recommendation query starting
from the grid cell containing the querying user location at
level I, instead of the lowest maintained grid cell (the default).
An inﬂuence level of zero forces LARS to use the root cell
of the pyramid, and thus act as a traditional (non-spatial)
collaborative ﬁltering recommender system.

(QWLUH6\VWHP$UHDOHYHO
[*ULGOHYHO




[*ULGOHYHO





[*ULGOHYHO




Fig. 4.

Partial pyramid data structure.

varying tastes, and that our method only forces collaborative
ﬁltering to produce personalized user recommendations based
only on ratings restricted to a speciﬁc spatial region. In this
section, we describe the pyramid structure in Section III-A,
query processing in Section III-B, and ﬁnally data structure
maintenance in Section III-C.
A. Data Structure
LARS employs a partial pyramid structure [11] (equivalent
to a partial quad-tree [12]) as depicted in Figure 4. The pyramid decomposes the space into H levels (i.e., pyramid height).
For a given level h, the space is partitioned into 4h equal area
grid cells. For example, at the pyramid root (level 0), one grid
cell represents the entire geographic area, level 1 partitions
space into four equi-area cells, and so forth. We represent
each cell with a unique identiﬁer cid. In each cell, we store
an item-based collaborative ﬁltering model built using only
the spatial ratings with user locations contained in the cell’s
spatial region. A rating may contribute to up to H collaborative
ﬁltering models: one per each pyramid level starting from
the lowest maintained grid cell containing the embedded user
location up to the root level. Note that the root cell (level 0) of
the pyramid represents a “traditional” (i.e., non-spatial) itembased collaborative ﬁltering model. Levels in the pyramid can
be incomplete, as LARS will periodically merge or split cells
based on trade-offs of locality and scalability (discussed in
Section III-C). For example, in Figure 4, the four cells in the
upper right corner of level 3 are not maintained (depicted as
blank white squares).
We chose to employ a pyramid as it is a “space-partitioning”
structure that is guaranteed to completely cover a given space.
For our purposes, “data-partitioning” structures (e.g., R-trees)
are less ideal, as they index data points and are not guaranteed
to completely cover a given space.

C. Data Structure Maintenance
This section describes building and maintaining the pyramid
data structure. Initially, to build the pyramid, all location-based
ratings currently in the system are used to build a complete
pyramid of height H, such that all cells in all H levels are
present and contain a collaborative ﬁltering model. The initial
height H is chosen according to the level of locality desired,
where the cells in the lowest pyramid level represent the most
localized regions. After this initial build, we invoke a merging
step that scans all cells starting from the lowest level h and
merges quadrants (i.e., four cells with a common parent) into
their parent at level h − 1 if it is determined that a tolerated
amount of locality will not be lost (merging is discussed
in Section III-C1). We note that while the original partial
pyramid [11] was concerned with spatial queries over static
data, it did not address pyramid maintenance.
As time goes by, new users, ratings, and items will be added
to the system. This new data will both increase the size of the
collaborative ﬁltering models maintained in the pyramid cells,
as well as alter recommendations produced from each cell.
To account for these changes, LARS performs maintenance
on a cell-by-cell basis. Maintenance is triggered for a cell C

B. Query Processing
Given a recommendation query (as described in Section II-A) with user location L and a limit K, LARS performs
two query processing steps: (1) The user location L is used
to ﬁnd the lowest maintained cell C in the adaptive pyramid
that contains L. This is done by hashing the user location to
retrieve the cell at the lowest level of the pyramid. If this cell
is not maintained, we return the nearest maintained ancestor
cell. (2) The top-k recommended items are generated using
the item-based collaborative ﬁltering technique (covered in
Section II-B) using the model stored at C. As mentioned

453

tenance is performed only after N % new ratings are added to
a pyramid cell, meaning maintenance will be amortized over
many operations.
1) Cell Merging: Merging entails discarding an entire
quadrant of cells at level h with a common parent at level h−1.
Merging improves scalability (i.e., storage and computational
overhead) of LARS, as it reduces storage by discarding the
item-based collaborative ﬁltering (CF) models of the merged
cells. Furthermore, merging improves computational overhead
in two ways: (a) less maintenance computation, since less
CF models are periodically rebuilt, and (b) less continuous
query processing computation, as merged cells represent a
larger spatial region, hence, users will cross cell boundaries
less often triggering less recommendation updates. Merging
hurts locality, since merged cells capture community opinions
from a wider spatial region, causing less unique (i.e., “local”)
recommendations than smaller cells.
To determine whether to merge a quadrant q into its
parent cell CP (i.e., function CheckDoMerge on line 8 in
Algorithm 1), we calculate two percentage values: (1) locality loss, the amount of locality lost by (potentially) merging,
and (2) scalability gain, the amount of scalability gained by
(potentially) merging. Details of calculating these percentages
are covered next. When deciding to merge, we deﬁne a system
parameter M, a real number in the range [0,1] that deﬁnes
a tradeoff between scalability gain and locality loss. LARS
merges (i.e., discards quadrant q) if:

Algorithm 1 Pyramid maintenance algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

/* Called after cell C receives N % new ratings */
Function PyramidMaintenance(Cell C, Level h)
/*Step I: Model Rebuild */
Rebuild item-based collaborative ﬁltering model for cell C
/*Step II: Merge/Split Maintenance */
if (C has children quadrant q maintained at level h + 1) then
if (All cells in q have no maintained children) then
CheckDoMerge(q,C) /* Merge covered in Section III-C1 */
end if
else
CheckDoSplit(C) /* Split covered in Section III-C2 */
end if
return

once it receives N % new ratings; the percentage is computed
from the number of existing ratings in C. We do this because
an appealing quality of collaborative ﬁltering is that as a
model matures (i.e., more data is used to build the model),
more updates are needed to signiﬁcantly change the top-k
recommendations produced from it [14]. Thus, maintenance
is needed less often. Algorithm 1 provides the pseudocode
for the LARS maintenance algorithm. The algorithm takes as
input a pyramid cell C and level h, and includes two main
steps: model rebuild and merge/split maintenance.
Step I: Model Rebuild. The ﬁrst step is to rebuild the
item-based collaborative ﬁltering (CF) model for a cell C, as
described in Section II-B (line 4). Rebuilding the CF model
is necessary to allow the model to “evolve” as new locationbased ratings enter the system (e.g., accounting for new items,
ratings, or users). Given the cost of building the CF model is
2
the cost of the model rebuild for a
O( RU ) (per Section II-B),
h 2
)
R2
cell C at level h is (R/4
h
(U/4 ) = 4h U , assuming ratings and users
are uniformly distributed.
Step II: Merging/Split Maintenance. After rebuilding the
CF model for cell C, LARS invokes a merge/split maintenance
step that may decide to merge or split cells based on tradeoffs in scalability and locality. The algorithm ﬁrst checks if
C has a child quadrant q maintained at level h + 1 (line 6),
and that none of the four cells in q have maintained children
of their own (line 7). If both cases hold, LARS considers
quadrant q as a candidate to merge into its parent cell C
(calling function CheckDoMerge on line 8). We provide details
of merging in Section III-C1. On the other hand, if C does
not have a child quadrant maintained at level h + 1 (line 10),
LARS considers splitting C into four child cells at level h + 1
(calling function CheckDoSplit on line 11). The split operation is covered in Section III-C2. Merging and splitting are
performed completely in quadrants (i.e., four equi-area cells
with the same parent). We made this decision for simplicity in
maintaining the partial pyramid. However, we also discuss (in
Section III-D) relaxing this constraint by merging and splitting
at a ﬁner granularity than a quadrant.
We note the following features of pyramid maintenance:
(1) Maintenance can be performed completely ofﬂine, i.e.,
LARS can continue to produce recommendations using the
”old” pyramid cells while part of the pyramid is being updated;
(2) maintenance does not entail rebuilding the whole pyramid
at once, instead, only one cell is rebuilt at a time; (3) main-

(1 − M) ∗ scalability gain > M ∗ locality loss

(3)

A smaller M value implies gaining scalability is important
and the system is willing to lose a large amount of locality
for small gains in scalability. Conversely, a larger M value
implies scalability is not a concern, and the amount of locality
lost must be small in order to merge. At the extremes, setting
M=0 (i.e., always merge) implies LARS will function as
a traditional CF recommender system, while setting M=1
causes LARS to never merge, i.e., LARS will employ a
complete pyramid structure maintaining all cells at all levels.
Calculating Locality Loss. We calculate locality loss
by observing the loss of recommendation uniqueness when
discarding a cell quadrant q and using its parent cell CP
to produce recommendations in its place. We perform this
calculation in three steps. (1) Sample. We take a sample of
diverse system users U that have at least one rating within CP
(and by deﬁnition one of the more localized cells Cu ∈ q).
Due to space, we do not discuss user sampling in detail,
however, the intuition is to select a set of users with diverse
tastes by comparing each user’s rating history. We measure
diversity using the Cosine distance between users in the same
manner as Equation 2, except we employ user vectors in the
calculation (instead of item vectors). (2) Compare. For each
user u ∈ U, we measure the potential loss of recommendation
uniqueness by comparing the list of top-k recommendations
RP produced from the merged cell CP (i.e., the parent) with
the list of recommendations Ru that the user receives from the
more localized cell Cu ∈ q. Formally, the loss of uniqueness

454





























Fig. 5.

	

	












	


	





	


	






	


	







creates more granular cells causing user locations to cross cell
boundaries more often, triggering recommendation updates.
To determine whether to split a cell CP into four child
cells (i.e., function CheckDoSplit on line 11 of Algorithm 1),
we perform a speculative split that creates a temporary child
quadrant qs for CP . Using CP and qs , two percentages are
calculated: locality gain and scalability loss. These values are
the opposite of those calculated for the merge operation. LARS
splits CP only if the following condition holds:









Merge and split example.

P|
can be computed as the ratio |Ru −R
, which indicates the
k
number of recommended items that appear in Ru but not in the
parent recommendation RP , normalized to the total number of
recommended objects k. (3) Average. We calculate the average
loss of uniqueness over all users in U to produce a single
percentage value, termed locality loss.
Calculating scalability gain. Scalability gain is measured
in storage and computation savings. We measure scalability
gain by summing the model sizes for each of the merged (i.e.,
child) cells (abbr. sizem), and divide this value by the sum
of sizem and the size of the parent cell. We refer to this
percentage as the storage gain. We also quantify computation
savings using storage gain as a surrogate measurement, as
computation is considered a direct function of the amount of
data in the system.
)+k), where
Cost. The cost of CheckDoMerge is |U|(2( n|I|
4h
|U| is the size of the user sample, |I| is the number of
items in the model stored within a cell, n is the model
size (Section II-B), and k is the cost of comparing two
recommendation lists. We note that this cost is less than the
model re-build step.
Example. Figure 5 depicts four merge candidate cells C1
to C4 at level h merging into their parent CP at level h − 1,
along with four sampled users u1 to u4 . Each user location is
shown twice: once within one of the cells Cu and then at the
parent cell CP . The recommendations produced for each user
from cell Cu and CP are provided in the table in Figure 5,
along with the locality loss for each user. For example, for
user u1 , cell C1 produces recommendations Ru1 ={I1 , I2 , I5 ,
I6 }, while CP produces recommendations RP ={I1 , I2 , I5 ,
I7 }. Thus, the loss of locality for u1 is 25% as only one item
out of four (I6 ) will be lost if merging occurs. Given locality
loss for the four users u1 to u4 as 25%, 25%, 0%, and 50%,
the ﬁnal locality loss value is the average 25%. To calculate
scalability gain, assume the sum of the model sizes for cells
C1 to C4 and CP is 4GB, and the sum of the model sizes for
cells C1 to C4 is 2GB. Then, the scalability gain is 24 =50%.
Assuming M=0.7, then (0.3 * 50) < (0.7 * 25), meaning that
LARS will not merge cells C1 , C2 , C3 , C4 into CP .
2) Splitting: Splitting entails creating a new cell quadrant at
pyramid level h under a cell at level h − 1. Splitting improves
locality in LARS, as newly split cells represent more granular
spatial regions capable of producing recommendations unique
to the smaller, more “local”, spatial regions. On the other hand,
splitting hurts scalability by requiring storage and maintenance
of more item-based collaborative ﬁltering models. Splitting
also negatively affects continuous query processing, since it

M ∗ locality gain > (1 − M) ∗ scalability loss

(4)

This equation represents the opposite criteria of that presented
for merging in Equation 3. We will next describe how to
perform speculative splitting, followed by a description of how
to calculate locality gain and scalability loss.
Speculative splitting. In order to evaluate locality gain and
scalability loss, we must build, from scratch, the collaborative
ﬁltering (CF) models of the four cells that potentially result
from the split, as they do not exist in the partial pyramid.
As building CF models is non-trivial due to its high cost
(Section II-B), we perform a cheaper speculative split that
builds each model using a random sample of only 50%
of the ratings from the spatial region of each potentially
split cell. LARS uses these models to measure locality gain
and scalability loss. If LARS decides to split, it builds the
complete model for the newly split cells using all of the ratings.
Speculative splitting is sufﬁcient for calculating locality gain
and scalability loss using the item-based CF technique, as
experiments on real data and workloads have shown that using
50% of the ratings for model-building results in loss of only
3% of recommendation accuracy [9], assuming sufﬁciently
high number of ratings (i.e., order of thousands). Thus, we
only speculatively split if we have more than 1,000 ratings for
the potentially split cell, otherwise, the model for the cell is
built using all of R.
Calculating locality gain. After speculatively splitting a
cell at level h into four child cells at level h + 1, evaluating
locality gain is performed exactly the same as for merging,
where we compute the ratio of recommendations that will appear in Ru but not in RP , where Ru and RP are the list of topk recommendations generated by the speculatively split cells
C1 to C4 and the existing parent cell CP , respectively. Like the
merging case, we average locality gain over all sampled users.
One caveat here is that if any of the speculatively split cells do
not contain ratings for enough unique items (say less than ten
unique items), we immediately set the locality gain to 0, which
disqualiﬁes splitting. We do this to prevent recommendation
starvation, i.e., not having enough diverse items to produce
meaningful recommendations.
Calculating scalability loss. We calculate scalability loss
by estimating the storage necessary to maintain the newly split
cells. Recall from Section II-B that the maximum size of an
item-based CF model is approximately n|I|, where n is the
model size. We can multiply n|I| by the number of bytes
needed to store an item in a CF model to ﬁnd an upper-bound
storage size of each potentially split cell. The sum of these four

455

estimated sizes (abbr. sizes ) divided by the sum of the size
of the existing parent cell and sizes represents the scalability
loss metric.
Cost. The cost of CheckDoSplit is the sum of two operations
(1) the cost of speculatively building four CF models at
2
level h + 1 using 50% of the rating, which is 4 4(0.5R)
(h+1) U (per
Section II-B) and (2) the cost of calculating locality gain and
scalability loss, which is the same cost as CheckDoMerge.
Example. Consider the example used for merging in Figure 5, but now assume we have only a cell CP , and are trying
to determine whether to split CP into four new cells C1 to
C4 . Locality gain will be computed as in the table in Figure 5
to be 25%. Further, assume that we estimate the extra storage
overhead for splitting (i.e., storage loss) to be 50%. Assuming
M=0.7, then (0.7 * 25) > (0.3 * 50), meaning that LARS
will decide to split CP into four cells as locality gain is
signiﬁcantly higher than scalability loss.

IV. N ON -S PATIAL U SER R ATINGS FOR
S PATIAL I TEMS
This section describes how LARS produces recommendations using non-spatial ratings for spatial items represented
by the tuple (user, rating, item, ilocation). The idea is to
exploit travel locality, i.e., the observation that users limit
their choice of spatial venues based on travel distance (based
on analysis in Section I-A). Traditional (non-spatial) recommendation techniques may produces recommendations with
burdensome travel distances (e.g., hundreds of miles away).
LARS produces recommendations within reasonable travel
distances by using travel penalty, a technique that penalizes
the recommendation rank of items the further in travel distance
they are from a querying user. Travel penalty may incur expensive computational overhead by calculating travel distance
to each item. Thus, LARS employs an efﬁcient query processing technique capable of early termination to produce the
recommendations without calculating the travel distance to all
items. Section IV-A describes the query processing framework
while Section IV-B describes travel distance computation.

D. Partial Merging and Splitting
So far, we have assumed cells are merged and split in complete quadrants. We now relax this constraint by discussing
the changes to LARS necessary to support partial merging
and splitting of pyramid cells.
1) Partial Merging: It may be beneﬁcial to partially merge
at a more granular level in order to sacriﬁce less locality while
still gaining scalability. For example, in Figure 5 we may
only want to merge cells C1 and C2 while leaving cells C3
and C4 intact, meaning three child cells would be maintained
under the example parent CP . To support partial merging, all
techniques described in Section III-C1 remain the same, with
two exceptions: (1) The resulting merged candidate cell (e.g.,
C1 merged with C2 , abbreviated C12 ) plays the role of the
“parent” cell in evaluating locality loss; (2) When calculating
storage gain, we must subtract the size of the resulting merge
candidate cell (e.g., C12 ) from the sum of the sizes of cells that
will merge (e.g., C1 and C2 ), since we no longer discard the
merged cells completely, i.e., the resulting merged cell now
replaces the individual cells.
Partial merging involves extra overhead (compared to merging complete quadrants) since we must build, from scratch,
the CF model for the candidate merge result cell (e.g., C12 ) in
order to calculate locality loss. In order to perform the build
efﬁciently, we perform a speculative merge that builds the CF
model using only 50% of the rating data. This is the same
method used in speculative splitting (Section III-C2), except
applied to the case of merging.
2) Partial Splitting: To support partial splitting, all techniques discussed in Section III-C2 remain the same. There are,
however, two distinguishable cases of partial splitting: (1) A
“parent” at level h splitting into less than four cells at level
h + 1. This case requires speculative splitting to be aware of
which “partial” child cells to create. (2) A cell at level h is
split into two or three separate cells that remain at level h,
i.e., cells at level h + 1 are not created. This case requires that
a previous partial merge took place that originally reduced a
cell quadrant to two or three cells.

A. Query Processing
Query processing for spatial items using the travel penalty
technique employs a single system-wide item-based collaborative ﬁltering model to generate the top-k recommendations
by ranking each spatial item i for a querying user u based on
RecScore(u, i), computed as:
RecScore(u, i) = P (u, i) − T ravelP enalty(u, i)

(5)

P (u, i) is the standard item-based CF predicted rating of item
i for user u (see Section II-B). T ravelP enalty(u, i) is the
road network travel distance between u and i normalized to
the same value range as the rating scale (e.g., [0, 5]).
When processing recommendations, we aim to avoid calculating Equation 5 for all candidate items to ﬁnd the top-k
recommendations, which can become quite expensive given
the need to compute travel distances. To avoid such computation, we evaluate items in monotonically increasing order of
travel penalty (i.e., travel distance), enabling us to use early
termination principles from top-k query processing [15], [16],
[17]. We now present the main idea of our query processing
algorithm and in the next section discuss how to compute
travel penalties in an increasing order of travel distance.
Algorithm 2 provides the pseudo code of our query processing algorithm that takes a querying user id U , a location
L, and a limit K as input, and returns the list R of top-k
recommended items. The algorithm starts by running a knearest-neighbor algorithm to populate the list R with k items
with lowest travel penalty; R is sorted by the recommendation
score computed using Equation 5. This initial part is concluded
by setting the lowest recommendation score value (LowestRecScore) as the RecScore of the k th item in R (Lines 3 to 8).
Then, the algorithm starts to retrieve items one by one in
the order of their penalty score. This can be done using an
incremental k-nearest-neighbor algorithm, as will be described
in the next section. For each item i, we calculate the maximum

456

d to the ratings scale to get the travel penalty in Equation 5.
Incremental KNN techniques exist for both Euclidean distance [19] and (road) network distance [18], [20]. The advantage of using Incremental KNN techniques is that they
provide an exact travel distances between a querying user’s
location and each recommendation candidate item. The disadvantage is that distances must be computed online at query
runtime, which can be expensive. For instance, the runtime
complexity of retrieving a single item using incremental KNN
in Euclidean space is [19]: O(k+logN ), where N and k are the
number of total items and items retrieved so far, respectively.
2) Penalty Grid: A Heuristic Ofﬂine Method: A more efﬁcient, yet less accurate method to retrieve travel penalties
incrementally is to use a pre-computed penalty grid. The idea
is to partition space using an n × n grid. Each grid cell c
is of equal size and contains all items whose location falls
within the spatial region deﬁned by c. Each cell c contains
a penalty list that stores the pre-computed penalty values for
traveling from anywhere within c to all other n2 −1 destination
cells in the grid; this means all items within a destination grid
cell share the same penalty value. The penalty list for c is
sorted by penalty value and always stores c (itself) as the ﬁrst
item with a penalty of zero. To retrieve items incrementally, all
items within the cell containing the querying user are returned
one-by-one (in any order) since they have no penalty. After
these items are exhausted, items contained in the next cell in
the penalty list are returned, and so forth until Algorithm 2
terminates early or processes all items.
To populate the penalty grid, we must calculate the penalty
value for traveling from each cell to every other cell in the
grid. We assume items and users are constrained to a road
network, however, we can also use Euclidean space without
consequence. To calculate the penalty from a single source cell
c to a destination cell d, we ﬁrst ﬁnd the average distance to
travel from anywhere within c to all item destinations within d.
To do this, we generate an anchor point p within c that both
(1) lies on the road network segment within c and (2) lies
as close as possible to the center of c. With these criteria, p
serves as an approximate average “starting point” for traveling
from c to d. We then calculate the shortest path distance
from p to all items contained in d on the road network (any
shortest path algorithm can be used). Finally, we average all
calculated shortest path distances from c to d. As a ﬁnal step,
we normalize the average distance from c to d to fall within
the rating value range. Normalization is necessary as the rating
domain is usually small (e.g., zero to ﬁve), while distance is
measured in miles or kilometers and can have large values that
heavily inﬂuence Equation 5. We repeat this entire process for
each cell to all other cells to populate the entire penalty grid.
When new items are added to the system, their presence in
a cell d can alter the average distance value used in penalty
calculation for each source cell c. Thus, we recalculate penalty
scores in the penalty grid after N new items enter the system.
We assume spatial items are relatively static, e.g., restaurants
do not change location often. Thus, it is unlikely existing items
will change cell locations and in turn alter penalty scores.

Algorithm 2 Travel Penalty Algorithm for Spatial Items
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

Function LARS SpatialItems(User U, Location L, Limit K)
/* Populate a list R with a set of K items*/
R←φ
for (K iterations) do
i ← Retrieve the item with the next lowest travel penalty (Section IV-B)
Insert i into R ordered by RecScore(U, i) computed by Equation 5
end for
LowestRecScore ← RecScore of the kth object in R
/*Retrieve items one by one in order of their penalty value */
while there are more items to process do
i ← Retrieve the next item in order of penalty score (Section IV-B)
MaxP ossibleScore ← MAX RAT IN G - i.penalty
if MaxP ossibleScore ≤ LowestRecScore then
return R /* early termination - end query processing */
end if
RecScore(U, i) ← P (U, i) - i.penalty /* Equation 5 */
if RecScore(U, i) > LowestRecScore then
Insert i into R ordered by RecScore(U, i)
LowestRecScore ← RecScore of the kth object in R
end if
end while
return R

possible recommendation score that i can have by subtracting
the travel penalty of i from MAX RATING, the maximum
possible rating value in the system, e.g., 5 (Line 12). If i
cannot make it into the list of top-k recommended items with
this maximum possible score, we immediately terminate the algorithm by returning R as the top-k recommendations without
computing the recommendation score (and travel distance) for
more items (Lines 13 to 15). The rationale here is that since
we are retrieving items in increasing order of their penalty
and calculating the maximum score that any remaining item
can have, then there is no chance that any unprocessed item
can beat the lowest recommendation score in R. If the early
termination case does not arise, we continue to compute the
score for each item i using Equation 5, insert i into R sorted by
its score (removing the k th item if necessary), and adjust the
lowest recommendation value accordingly (Lines 16 to 20).
Travel penalty requires very little maintenance. The only
maintenance necessary is to occasionally rebuild the single
system-wide item-based collaborative ﬁltering model in order
to account for new location-based ratings that enter the system.
Following the reasoning discussed in Section III-C, we rebuild
the model after receiving N % new location-based ratings.
B. Incremental Travel Penalty Computation
This section gives an overview of two methods we implemented in LARS to incrementally retrieve items one by one ordered by their travel penalty. The two methods exhibit a tradeoff between query processing efﬁciency and penalty accuracy:
(1) an online method that provides exact travel penalties but is
expensive to compute, and (2) an ofﬂine heuristic method that
is less exact but efﬁcient in penalty retrieval. Both methods
can be employed interchangeably in Line 11 of Algorithm 2.
1) Incremental KNN: An Exact Online Method: To calculate an exact travel penalty for a user u to item i, we employ an
incremental k-nearest-neighbor (KNN) technique [18], [19],
[20]. Given a user location l, incremental KNN algorithms
return, on each invocation, the next item i nearest to u with
regard to travel distance d. In our case, we normalize distance

457

1

2
3
4
5
6
7
Number of Pyramid Levels

(a) Foursquare data
Fig. 6.

8

300

LARS
LARS-U
LARS-T
CF

250
200

Quality

LARS-U
CF

350
300
250
200
150
100
50
0

Quality

LARS
LARS-U
LARS-T
CF

Quality

Quality

280
260
240
220
200
180
160
140
120

150
100
50
0

1

2

3

4

5

6

7

8

1

2

3

4

5

6

7

k

Number of Pyramid Levels

(b) MovieLens data

(a) Foursquare data

Quality experiments for varying locality

Fig. 7.

V. S PATIAL U SER R ATINGS FOR
S PATIAL I TEMS

8

9 10

LARS-U
CF

350
300
250
200
150
100
50
0
1

2

3

4

5

6

7

8

9 10

k

(b) MovieLens data

Quality experiments for varying answer sizes

and MovieLens data. To test the effectiveness of our proposed techniques, we test the quality of LARS with only
travel penalty enabled (abbr. LARS-T), LARS with only user
partitioning enabled (abbr. LARS-U), and LARS with both
techniques enabled (abbr. LARS). To measure quality, we build
each recommendation method using 80% of the ratings from
each data set. Each rating in the withheld 20% represents a
Foursquare venue or MovieLens movie a user is known to
like (i.e., rated highly). For each rating t in this 20%, we
request a set of k recommendations R by submitting the user
and ulocation associated with t. The quality measure is the
count of how many times R contains the item associated with
t (the higher the better). The rationale for this metric is that
since each withheld rating represents a real visit to a venue
(or movie a user liked), the technique that produces a large
number of answers that contain venues (or movies) a user is
known to like is considered of higher quality.

This section describes how LARS produces recommendations using spatial ratings for spatial items represented by the
tuple (user, ulocation, rating, item, ilocation). A salient feature
of LARS is that both the user partitioning and travel penalty
techniques can be used together with very little change to
produce recommendations using spatial user ratings for spatial
items. The data structures and maintenance techniques remain
exactly the same as discussed in Sections III and IV; only
the query processing framework requires a slight modiﬁcation.
Query processing uses Algorithm 2 to produce recommendations. However, the only difference is that the item-based
collaborative ﬁltering prediction score P (u, i) used in the recommendation score calculation (Line 16 in Algorithm 2) is
generated using the (localized) collaborative ﬁltering model
from the partial pyramid cell that contains the querying user,
instead of the system-wide collaborative ﬁltering model as was
used in Section IV.

Figure 6(a) compares the quality of each technique for varying locality (i.e., different levels of the adaptive pyramid) using
the Foursquare data. Both CF and LARS-T do not use the
adaptive pyramid, thus have constant quality values. The gap
between CF and LARS-T highlights the beneﬁt of using the
travel penalty technique that recommends items within a feasible distance. Meanwhile, the quality of LARS and LARS-U
increases as more localized pyramid cells are used to produce recommendation, which veriﬁes that user partitioning is
indeed beneﬁcial and necessary for location-based ratings. Ultimately, LARS has superior performance due to the additional
use of travel penalty. While travel penalty produces moderate
quality gain, it also enables more efﬁcient query processing,
which we observe later in Section VI-E).

VI. E XPERIMENTS
This section provides experimental evaluation of LARS
based on an actual system implementation. We compare LARS
with the standard item-based collaborative ﬁltering technique
along with several variations of LARS. Experiments are based
on three data sets: (1) Foursquare: a real data set consisting of
spatial user ratings for spatial items derived from Foursquare
user histories. (2) MovieLens: a real data set consisting of
spatial user ratings for non-spatial items taken from the
popular MovieLens recommender system [7]. The Foursquare
and MovieLens data are used to test recommendation quality.
(3) Synthetic: a synthetically generated data set consisting
spatial user ratings for spatial items for venues in the state of
Minnesota, USA; we use this data to test scalability and query
efﬁciency. Details of all data sets are found in Appendix B.
Unless mentioned otherwise, the default value of M is 0.3,
k is 10, the number of pyramid levels is 8, and the inﬂuence
level is the lowest pyramid level. The rest of this section evaluates LARS recommendation quality (Section VI-A), trade-offs
between storage and locality (Section VI-C), scalability (Section VI-D), and query processing efﬁciency (Section VI-E).

Figure 6(b) compares the quality of LARS-U and CF for
varying locality using the MovieLens data (LARS and LARST do not apply since movies are not spatial). While CF quality
is constant, the quality of LARS-U increases when it produces
movie recommendations from more localized pyramid cells.
This behavior further veriﬁes that user partitioning is beneﬁcial in providing quality recommendations localized to a
querying user location, even when items are not spatial. Quality decreases (or levels off for MovieLens) for both LARS-U
and/or LARS for lower levels of the adaptive pyramid. This
is due to recommendation starvation, i.e., not having enough
ratings to produce meaningful recommendations.

A. Recommendation Quality for Varying Pyramid Levels
These experiments test the recommendation quality of
LARS against the standard (non-spatial) item-based collaborative ﬁltering method (denoted as CF) using both the Fourquare

458

4
3

LARS-M=0
LARS-M=1
LARS

2
1
0

0 0.1 0.2 0.3 0.5 0.6 0.7 0.8 0.9 1
M

(a) Storage

60
50
40
30

Storage (GB)

5

Locality Loss (%)

Storage (GB)

6

LARS-M=0
LARS-M=1
LARS

20
10
0

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
M

(b) Locality

8
LARS-M=0
7 LARS-M=1
LARS
6
5
4
3
2
1
0
10
50
100
200
500
Number of Ratings (* 1000)

Aggregate Maint Time (* 1K sec)

70

7

(a) Storage
Fig. 8.

Effect of M on storage and locality

Fig. 9.

B. Recommendation Quality for Varying Values of k

18
LARS-M=0
16 LARS-M=1
14
LARS
12
10
8
6
4
2
0
10
50

100

150

200

Ratings Updates So Far (* 1000)

(b) Maintenance

Scalability of the adaptive pyramid

D. Scalability

These experiments test recommendation quality of LARS,
LARS-U, LARS-T, and CF for different values of k (i.e.,
recommendation answer sizes). We perform experiments using
both the Foursquare and MovieLens data. Our quality metric
is exactly the same as presented previously in Section VI-A.
Figure 7(a) depicts the effect of the recommendation list
size k on the quality of each technique using the Foursquare
data set. We report quality numbers using the pyramid height
of four (i.e., the level exhibiting the best quality from Section VI-A in Figure 6(a)). For all sizes of k from one to ten,
LARS and LARS-U consistently exhibit better quality. In fact,
LARS is consistently twice as accurate as CF for all k. LARST exhibits similar quality to CF for smaller k values, but does
better for k values of three and larger.
Figure 7(b) depicts the effect of the recommendation list
size k on the quality of LARS-U and CF using the MovieLens
data (LARS and LARS-T do not apply in this experiment
since movies are not spatial). This experiment was run using
a pyramid hight of seven (i.e., the level exhibiting the best
quality in Figure 6(b)). Again, LARS-U consistently exhibits
better quality than CF for sizes of K from one to ten. In fact,
the quality of CF increases by just a fraction as k increases.
Meanwhile, the quality of LARS-U increases by a factor of
seven as k increases from one to ten.

Figure 9 depicts the storage and aggregate maintenance overhead required for an increasing number of ratings. We again
plot LARS-M=0 and LARS-M=1 to indicate the extreme cases
for LARS. Figure 9(a) depicts the impact of increasing the
number of ratings from 10K to 500K on storage overhead.
LARS-M=0 requires the lowest amount of storage since it
only maintains a single collaborative ﬁltering model. LARSM=1 requires the highest amount of storage since it requires
storage of a collaborative ﬁltering model for all cells (in all
levels) of a complete pyramid. The storage requirement of
LARS is in between the two extremes since it merges cells to
save storage. Figure 9(b) depicts the cumulative computational
overhead necessary to maintain the adaptive pyramid initially
populated with 100K ratings, then updated with 200K ratings
(increments of 50K reported). The trend is similar to the storage experiment, where LARS exhibits better performance than
LARS-M=1 due to merging. Though LARS-M=0 has the best
performance in terms of maintenance and storage overhead,
previous experiments show that it has unacceptable drawbacks
in quality/locality.
E. Query Processing Performance
Figure 10 depicts snapshot and continuous query processing performance of LARS, LARS-U (LARS with only user
partitioning), LARS-T (LARS with only travel penalty), CF
(traditional collaborative ﬁltering), and LARS-M=1 (LARS
with a complete pyramid).
Snapshot queries. Figure 10(a) gives the effect of various
number of ratings (10K to 500K) on the average snapshot
query performance averaged over 500 queries posed at random
locations. LARS and LARS-M=1 consistently outperform all
other techniques; LARS-M=1 is slightly better due to recommendations always being produced from the smallest (i.e.,
most localized) CF models. The performance gap between
LARS and LARS-U (and CF and LARS-T) shows that employing the travel penalty technique with early termination
leads to better query response time. Similarly, the performance
gap between LARS and LARS-T shows that employing user
partitioning technique with its localized (i.e., smaller) collaborative ﬁltering model also beneﬁts query processing.
Continuous queries. Figure 10(b) provides the continuous
query processing performance of the LARS variants by reporting the aggregate response time of 500 continuous queries. A
continuous query is issued once by a user u to get an initial

C. Storage Vs. Locality
Figure 8 depicts the impact of varying M on both the
storage and locality in LARS. We plot LARS-M=0 and LARSM=1 as constants to delineate the extreme values of M, i.e.,
M=0 mirrors traditional collaborative ﬁltering, while M=1
forces LARS to employ a complete pyramid. Our metric for
locality is locality loss (deﬁned in Section III-C1) when compared to a complete pyramid (i.e., M=1). LARS-M=0 requires
the lowest storage overhead, but exhibits the highest locality
loss, while LARS-M=1 exhibits no locality loss but requires
the most storage. For LARS, increasing M results in increased
storage overhead since LARS favors splitting, requiring the
maintenance of more pyramid cells each with its own collaborative ﬁltering model. Meanwhile, increasing M results
in smaller locality loss as LARS merges less and maintains
more localized cells. The most drastic drop in locality loss
is between 0 and 0.3, which is why we chose M=0.3 as a
default.

459

CF
LARS-M=1
LARS-U
150
LARS-T
LARS
100
50
0

10

50

100

200

Number of Ratings (*1000)

(a) Snapshot Queries
Fig. 10.

500

Aggregate Response Time (sec)

Response Time (ms)

200

Traditional recommenders. A wide array of techniques are
capable of producing recommendations using non-spatial ratings for non-spatial items represented as the triple (user, rating, item) (see [6] for a comprehensive survey). We refer to
these as “traditional” recommendation techniques. The closest
these approaches come to considering location is by incorporating contextual attributes into statistical recommendation
models (e.g., weather, trafﬁc to a destination) [26]. However,
no traditional approach has studied explicit location-based ratings as done in LARS. Some existing commercial applications
make cursory use of location when proposing interesting items
to users. For instance, Netﬂix [2] displays a “local favorites”
list containing popular movies for a user’s given city. However,
these movies are not personalized to each user (e.g., using
recommendation techniques); rather, this list is built using
aggregate rental data for a particular city [27]. LARS, on
the other hand, produces personalized recommendations inﬂuenced by location-based ratings and a querying user location.
Location-aware recommenders. The CityVoyager system [28]
mines a user’s personal GPS trajectory data to determine her
preferred shopping sites, and provides recommendation based
on where the system predicts the user is likely to go in the
future. LARS, conversely, does not attempt to predict future
user movement, as it produces recommendations inﬂuenced by
user and/or item locations embedded in community ratings.
The spatial activity recommendation system [29] mines GPS
trajectory data with embedded user-provided tags in order to
detect interesting activities located in a city (e.g., art exhibits
and dining near downtown). It uses this data to answer two
query types: (a) given an activity type, return where in the city
this activity is happening, and (b) given an explicit spatial
region, provide the activities available in this region. This
is a vastly different problem than we study in this paper.
LARS does not mine activities from GPS data for use as
suggestions for a given spatial region. Rather, we apply LARS
to a more traditional recommendation problem that uses community opinion histories to produce recommendations.
Geo-measured friend-based collaborative ﬁltering [30] produces recommendations by using only ratings that are from
a querying user’s social-network friends that live in the same
city. This technique only addresses user location embedded
in ratings. LARS, on the other hand, addresses three possible
types of location-based ratings. More importantly, LARS is a
complete system (not just a recommendation technique) that
employs efﬁciency and scalability techniques (e.g., merging,
splitting, early query termination) necessary for deployment in
actual large-scale applications.

2

CF
LARS-M=1
LARS-U
1.5
LARS-T
LARS
1
0.5
0
0.6

3

6.2

9.3 12.4 15.5 18.6

Travel Distance (miles)

(b) Continuous Queries

Query Processing Performance.

answer, then the answer is continuously updated as u moves.
We report the aggregate response time when varying the travel
distance of u from 1 to 30 miles using a random walk over
the spatial area covered by the pyramid. CF has a constant
query response time for all travel distances, as it requires no
updates since only a single cell is present. However, since CF
is unaware of user location change, the consequence is poor
recommendation quality (per experiments from Section VI-A).
LARS-M=1 exhibits the worse performance, as it maintains all
cells on all levels and updates the continuous query whenever
the user crosses pyramid cell boundaries. LARS-U has a lower
response time than LARS-M=1 due to merging: when a cell is
not present on a given inﬂuence level, the query is transferred
to its next highest ancestor in the pyramid. Since cells higher
in the pyramid cover larger spatial regions, query updates
occur less often. LARS-T exhibits slightly higher query processing overhead compared to LARS-U: even though LARST employs the early termination algorithm, it uses a large
(system-wide) collaborative ﬁltering model to (re)generate recommendations once users cross boundaries in the penalty grid.
LARS exhibits a better aggregate response time since it employs the early termination algorithm using a localized (i.e.,
smaller) collaborative ﬁltering model to produce results while
also merging cells to reduce update frequency.
VII. R ELATED W ORK
Location-based services. Current location-based services employ two main methods to provide interesting destinations to
users. (1) KNN techniques [19] and variants (e.g., aggregate
KNN [21]) simply retrieve the k objects nearest to a user and
are completely removed from any notion of user personalization. (2) Preference methods such as skylines [22] (and spatial
variants [23]) and location-based top-k methods [24] require
users to express explicit preference constraints. Conversely,
LARS is the ﬁrst location-based service to consider implicit
preferences by using location-based ratings to help users
discover new and interesting items.
Recent research has proposed the problem of hyper-local
place ranking [25]. Given a user location and query string
(e.g., “French restaurant”), hyper-local ranking provides a list
of top-k points of interest inﬂuenced by previously logged
directional queries (e.g., map direction searches from point
A to point B). While similar in spirit to LARS, hyper-local
ranking is fundamentally different from our work as it does
not personalize answers to the querying user, i.e., two users
issuing the same search term from the same location will
receive exactly the same ranked answer set.

VIII. C ONCLUSION
LARS, our proposed location-aware recommender system,
tackles a problem untouched by traditional recommender systems by dealing with three types of location-based ratings:
spatial ratings for non-spatial items, non-spatial ratings for
spatial items, and spatial ratings for spatial items. LARS employs user partitioning and travel penalty techniques to support spatial ratings and spatial items, respectively. Both tech-

460

A PPENDIX
A. Foursquare Description
Foursquare [4] is a mobile location-based social network
application. Users are associated with a home city, and alert
friends when visiting a venue (e.g., restaurant) by “checkingin” on their mobile phones. During a “check-in”, users can
also leave “tips”, which are free text notes describing what
that they liked about the venue. Any other user can add the
“tip” to her “to-do list” if interested in visiting the venue.
Once a user visits a venue in the “to-do list” , she marks it
as “done”. Also, users who check into a venue the most are
considered the “mayor” of that venue.

niques can be applied separately or in concert to support the
various types of location-based ratings. Experimental analysis
using real and synthetic data sets show that LARS is efﬁcient,
scalable, and provides better quality recommendations than
techniques used in traditional recommender systems.
R EFERENCES
[1] G. Linden et al, “Amazon.com Recommendations: Item-to-Item Collaborative Filtering,” IEEE Internet Computing, vol. 7, no. 1, pp. 76–80,
2003.
[2] “Netﬂix: http://www.netﬂix.com.”
[3] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl, “GroupLens: An Open Architecture for Collaborative Filtering of Netnews,”
in CSWC, 1994.
[4] “Foursquare: http://foursquare.com.”
[5] “The Facebook Blog, ”Facebook Places”: http://tinyurl.com/3aetfs3.”
[6] G. Adomavicius and A. Tuzhilin, “Toward the Next Generation of
Recommender Systems: A Survey of the State-of-the-Art and Possible
Extensions,” TKDE, vol. 17, no. 6, pp. 734–749, 2005.
[7] “MovieLens: http://www.movielens.org/.”
[8] “New
York Times - A Peek
Into Netﬂix
Queues:
http://www.nytimes.com/interactive/2010/01/10/nyregion/20100110netﬂix-map.html.”
[9] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, “Item-Based Collaborative Filtering Recommendation Algorithms,” in WWW, 2001.
[10] J. S. Breese, D. Heckerman, and C. Kadie, “Empirical Analysis of
Predictive Algorithms for Collaborative Filtering,” in UAI, 1998.
[11] W. G. Aref and H. Samet, “Efﬁcient Processing of Window Queries in
the Pyramid Data Structure,” in PODS, 1990.
[12] R. A. Finkel and J. L. Bentley, “Quad trees: A data structure for retrieval
on composite keys,” Acta Inf., vol. 4, pp. 1–9, 1974.
[13] M. F. Mokbel et al, “SINA: Scalable Incremental Processing of Continuous Queries in Spatiotemporal Databases,” in SIGMOD, 2008.
[14] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl, “Evaluating
Collaborative Filtering Recommender Systems,” TOIS, vol. 22, no. 1, pp.
5–53, 2004.
[15] M. J. Carey et al, “On saying ”Enough Already!” in SQL,” in SIGMOD,
1997.
[16] S. Chaudhuri et al, “Evaluating Top-K Selection Queries,” in VLDB,
1999.
[17] R. Fagin, A. Lotem, and M. Naor, “Optimal Aggregation Algorithms
for Middleware,” in PODS, 2001.
[18] J. Bao, C.-Y. Chow, M. F. Mokbel, and W.-S. Ku, “Efﬁcient evaluation
of k-range nearest neighbor queries in road networks,” in MDM, 2010.
[19] G. R. Hjaltason and H. Samet, “Distance Browsing in Spatial
Databases,” TODS, vol. 24, no. 2, pp. 265–318, 1999.
[20] K. Mouratidis, M. L. Yiu, D. Papadias, and N. Mamoulis, “Continuous
nearest neighbor monitoring in road networks,” in VLDB, 2006.
[21] D. Papadias, Y. Tao, K. Mouratidis, and C. K. Hui, “Aggregate Nearest
Neighbor Queries in Spatial Databases,” TODS, vol. 30, no. 2, pp. 529–
576, 2005.
[22] S. Börzsönyi et al, “The Skyline Operator,” in ICDE, 2001.
[23] M. Sharifzadeh and C. Shahabi, “The Spatial Skyline Queries,” in VLDB,
2006.
[24] N. Bruno, L. Gravano, and A. Marian, “Evaluating Top-k Queries over
Web-Accessible Databases,” in ICDE, 2002.
[25] P. Venetis, H. Gonzalez, C. S. Jensen, and A. Y. Halevy, “Hyper-Local,
Directions-Based Ranking of Places,” PVLDB, vol. 4, no. 5, pp. 290–
301, 2011.
[26] M.-H. Park et al, “Location-Based Recommendation System Using
Bayesian User’s Preference Model in Mobile Devices,” in UIC, 2007.
[27] “Netﬂix News and Info - Local Favorites: http://tinyurl.com/4qt8ujo.”
[28] Y. Takeuchi and M. Sugimoto, “An Outdoor Recommendation System
based on User Location History,” in UIC, 2006.
[29] V. W. Zheng, Y. Zheng, X. Xie, and Q. Yang, “Collaborative Location
and Activity Recommendations with GPS History Data,” in WWW, 2010.
[30] M. Ye, P. Yin, and W.-C. Lee, “Location Recommendation for Locationbased Social Networks,” in ACM SIGSPATIAL GIS, 2010.

B. Experimental Data Details
1) Foursquare Data: We crawled Foursquare and collected
data for 1,010,192 users and 642,990 venues across the United
States. Foursquare does not publish each “check-in” for a user,
however, we were able to collect the following pieces of data:
(1) user tips for a venue, (2) the venues for which the user is
the mayor, and (3) the completed to-do list items for a user.
In addition, we extracted each user’s friend list.
Extracting location-based ratings. To extract spatial user
ratings for spatial items from the Foursquare data (i.e., the
ﬁve-tuple (user, ulocation, rating, item, ilocation)), we map
each user visit to a single location-based rating. The user and
item attributes are represented by the unique Foursquare user
and venue identiﬁer, respectively. We employ the user’s home
city in Foursquare as the ulocation attribute. Meanwhile, the
ilocation attribute is the item’s inherent location. We use a
numeric rating value range of [1, 3], translated as follows:
(a) 3 represents the user is the “mayor” of the venue, (b) 2
represents that the user left a “tip” at the venue, and (c) 1
represents the user visited the venue as a completed “to-do”
list item. Using this scheme, a user may have multiple ratings
for a venue, in this case we use the highest rating value.
Data properties. Our experimental data consisted of 22,390
location-based ratings for 4K users for 2K venues all from
the state of Minnesota, USA. We used this reduced data set
in order to focus our quality experiments on a dense rating
sample. Use of dense ratings data has been shown to be a very
important factor when testing and comparing recommendation
quality [14], since use of sparse data (i.e., having users or
items with very few ratings) tends to cause inaccuracies in
recommendation techniques.
2) MovieLens Data: The MovieLens data used in our experiments was real movie rating data taken from the popular
MovieLens recommendation system at the University of Minnesota [7]. This data consisted of 87,025 ratings for 1,668
movies from 814 users. Each rating was associated with the
zip code of the user who rated the movie, thus giving us a
real data set of spatial user ratings for non-spatial items.
3) Synthetic Data: The synthetic data set we use in our experiments was generated to contain 2000 users and 1000 items,
and 500,000 ratings. Users and items locations are randomly
generated over the state of Minnesota, USA. Users’ ratings to
items are assigned random values between zero and ﬁve.

461

2012 IEEE 28th International Conference on Data Engineering

Horton: Online Query Execution Engine For Large
Distributed Graphs
Sameh Elnikety

Mohamed Sarwat

Gabriel Kliot

{samehe,yuxhe,gkliot}@microsoft.com

sarwat@cs.umn.edu

Abstract—Large graphs are used in many applications, such
as social networking. The management of these graphs poses
new challenges because such graphs are too large to ﬁt on a
single server. Current distributed techniques such as map-reduce
and Pregel are not well-suited for processing interactive adhoc queries against large graphs. In this paper we demonstrate
Horton, a distributed interactive query execution engine for large
graphs. Horton deﬁnes a query language that allows expressing
regular language reachability queries and provides a query
execution engine with a query optimizer that allows interactive
execution of queries on large distributed graphs in parallel. In
the demo, we show the functionality of Horton managing a large
graph for a social networking application called Codebook, in
which a graph models data on software components, developers,
development artifacts (such as bug reports), and their interactions
in large software projects.

designed to process large datasets over a distributed infrastructure, but it is designed for batch processing rather than online
query processing. Recently introduced systems for processing
large graphs focus on ofﬂine batch processing. Systems like
Pregel [9] and Surfer [5], [4] support batch processing on
graphs with high throughput rather than interactive queries
with low latency. We also point out that systems that manage
a large number of small graphs, as used in bioinformatics and
chemoinformatics, do not meet the requirements for querying
large graphs.
With the increased popularity of interactive services such as
social network applications, it becomes important to manage
large graphs online, supporting querying with small latency.
Online processing also allows answering a rich set of adhoc queries. In an interactive system, a user may request,
for example, to see her friends and their status updates.
The user may search for photos in which she is tagged
with a speciﬁc friend. The database research community has
paid little attention to building distributed systems to manage
and query large graphs interactively, which is an emerging
important application need.
We claim that such challenges require building new systems,
speciﬁcally designed to handle large graphs. In this paper,
we demonstrate Horton: an online distributed query execution
engine for large graphs. Horton provides a query language
that expresses regular language reachability queries on graphs.
Horton consists of a graph query execution engine and a query
optimizer that is able to efﬁciently process online queries on
large graphs. As a key design decision for online processing,
Horton partitions the graph among several servers, and stores
the graph partitions in main memory to offer fast query
response time. This is motivated by the availability of many
machines in data centers allowing horizontal scaling.
The remainder of the paper is structured as follows. Section II show the graph data model and the query language used
in Horton. Section III describes the architecture and design of
Horton and the query execution steps, and Section IV presents
our demonstration scenario.

I. I NTRODUCTION
Graphs are widely used in many application domains,
including social networking, interactive gaming, online knowledge discovery, computer networks, and the world-wide web.
For example, online social networks (OSN) employ large
social graphs as used in popular sites such as Facebook [7],
and Linkedin [8]. In the simplest form, a social graph contains
nodes that represent people and edges that represent friendships. Social graphs today are much richer, maintaining data on
photos, news, and groups. For instance, a node can represent a
person or a photo, and an edge between a person and a photo
means that the person is tagged (appears) in the photo.
The popularity and size of social networks pose new challenges. For example, Facebook [7] reports that the number
of its users increased ﬁve times in a short period, from 100
million in 2008 to 500 million in 2010. Another example
is Codebook [1], [2], which is a social network application
that maintains information about software engineers, software
components, and their interactions in large software projects.
Generating the Codebook graph for a large project, such as the
Windows or Linux operating systems, results in a very large
graph with billions of nodes and edges.
Such a large network cannot be managed on a single server.
In addition, current distributed techniques are not well-suited
for interactive online querying of large graphs. In particular,
the relational model is ill suited for graph query processing [9]
making reachability queries hard to express, and therefore
distributed database clusters becomes a non-viable option
to manage large graphs. The map-reduce [6] framework is
1084-4627/12 $26.00 © 2012 IEEE
DOI 10.1109/ICDE.2012.129

Yuxiong He

Microsoft Research
Redmond, WA

Dept. of Computer Science and Engineering
University of Minnesota, Twin Cities

II. H ORTON DATA M ODEL AND Q UERY L ANGUAGE
In this section we highlight the data model and query
language used by Horton.
1289

Graph Client Library

Graph Partition 1

Results

Photo1
Mo

Asynchronous
Communication

John

Photo7

Query Execution
Engine

Query
Photo8

Graph Coordinator

Tim
Bob

Graph Partition 2

Graph Query Optimizer

Mike

Query to Finite State
Automaton

Photo6

Asynchronous
Communication

Asynchronous
Communication

Photo3

Query Validation/Parsing

Query Execution
Engine

.
.
.

Photo2

Photo4

Graph Manager

Graph Partition N
Graph Partitioner

FriendOf

Asynchronous
Communication

Tagged
Graph Monitor

Fig. 1.

Example of a small social graph with persons and photos.

System
System
Administrator
Administrator

A. Data Model
Horton supports rich graph data. Nodes represent entities
and have types as well as a set of key value pairs representing
the data associated with this entity. Edges represent relationships between entities and have type and data as well. There
is no restriction on the kind or amount of data associated with
nodes and edges.
Horton manages both directed and undirected graphs. If the
graph is directed, each node stores both inbound and outbound
edges, to allow queries to traverse both directions. We show
examples for undirected graphs as they are simpler, but both
are supported in Horton.

Graph Loader

Fig. 2.

Query Execution
Engine

Horton system architecture.

SELECT Photo FROM
Tim−Tagged−Photo−Tagged−Person−FriendOf−Tim

III. H ORTON A RCHITECTURE
Figure 2 shows an overview of the Horton system. The
system comprises four components: the graph client library,
graph coordinator, graph partitions, and graph manager. The
graph client library sends queries to the graph coordinator and
uses an asynchronous messaging system to receive the results.
The graph coordinator prepares an execution plan for the
query, transforms it into a ﬁnite state machine, and initializes
the query processing on the appropriate graph partitions. Each
partition runs the query executor and sends the query results
back to the client. The graph manager transfers the graph
between main memory and persistent storage, and partitions
the graph.

B. Query Language
Queries are in the form of regular language expressions. A
query is a sequence of node predicates and edge predicates.
Each predicate can contain conjunctions and disjunctions on
node and edge attributes (including type) as well as closures
such as regular operators “*” (zero or more), and “+” (one or
more).
Figure 1 shows an example of a social graph that has two
node types, Person and Photo. In the ﬁgure, a solid line edge
represents a friendship relationship (i.e., between two Person
nodes) and a dotted line edge represents a tagging relationship
(i.e., between a Person and a Photo nodes). As a speciﬁc query,
to retrieve the list of all the photos in which John is tagged,
the query is expressed as follows:

A. Graph Client library
The graph client library sends queries to the graph coordinator in the form of regular expressions, and receives the
query results directly from the graph partitions.
B. Graph Coordinator
The graph coordinator provides the query interface for
Horton. The coordinator receives the query from the client
library, validates and parses the query. Then, the coordinator
optimizes the query plan and translates the query into a ﬁnite
state machine, which is sent to the partitions for parallel
execution. The details are illustrated below:

Photo−Tagged−John

To retrieve the photos in which Tim is tagged and at least one
of his friends is tagged too, the query is as follows:
Tim−Tagged−Photo−Tagged−Person−FriendOf−Tim

The query execution engine returns all paths in the graph
that satisfy the regular expression. If the user is interested in
only a speciﬁc part of the path, e.g., a speciﬁc node, a SELECT
statement is used. For instance, in the last query if the user is
interested in photos, the query is expressed as follows:

Query Parsing and Validation. When the graph coordinator
receives a query from the graph client, it ﬁrst parses the query
and checks the syntax and the validity of node and edge
predicates.

1290

Edge_Type =
Modifies

Edge_Type =
Commits

S6

S6

Node_Type =
Person

S1

S5

S1

S2
Node_Type =
FileRevision

Edge_Type = Modifies

S4

S3

(a) FSM for query 1

Fig. 3.

S4

Edge_Type =
Manages

Node_Type =
ChangeSet

S2

pe = ion
is
e_Ty
Nod emRev 3
It
7
Work ID = #6
e_
Nod

Node_Type = Person
Node_ID = Dave

S0

S5

Edge
_
Crea Type =
ted

S0
Node_Type =
Code

Node
_
Work Type =
It
Node emRevis
ion
_ID =
#673

S7

=
Type
Edge_
d
Close

Graph Query Optimizer. The graph query optimizer accepts
the query in a parsed regular expression form, enumerates
various execution plans, and ﬁnds a plan with lowest cost.
Cost includes both expected total query execution time on
graph partitions and communication cost among them. The
Horton graph query optimizer employs a set of optimization
strategies and integrates them using a dynamic programming
framework to quickly estimate the cost of execution plans.
An example optimization strategy is predicate ordering. This
optimization strategy evaluates the cost of executing the query
with different predicate orders and ﬁnds the plan with the
lowest cost. Finding a good predicate order to evaluate a query
is important because different predicate orders give the same
query results but may have orders-of-magnitudes differences
in cost due to the sizes of the intermediate results. We use
dynamic programming to ﬁnd the predicate order with the
minimum expected cost in time complexity of O(n3 ) where
n is the number of predicates in the query.

Node_Type = Person

S3

(b) FSM for query 3

Finite state machine for queries 1 and 3.

D. Graph Manager
The graph manager provides an administrative interface
to manage the graph, to perform tasks such as loading a
partitioned graph, and adding or removing servers. Graph
manager also supports updates to the graph, such as adding
and removing nodes and edges, as well as updating the data
associated with nodes and edges.
Horton supports the output of any partitioning algorithm,
assigning partitions to servers, and placing nodes in the right
partitions. Horton is not equipped with a speciﬁc graph partitioning algorithm because this is an expensive ofﬂine operation. The simplest form of graph partitioning is hashing based
on node or edge attributes. Large graphs are usually scale-free
and partitioning algorithms for large scale-free graphs can be
used to assign nodes to partitions while preserving locality in
graph accesses.

Query to Finite State Machine Translator. After the query
is optimized, the query plan is translated into a ﬁnite state
machine. The ﬁnite state machine expresses the query in a
form that is efﬁciently executed by the query execution engine.
The state machine is sent to the graph partitions and executed
by their local execution engine.
Asynchronous Communication Subsystem. The communication between the graph coordinator and the various graph
partitions and among the partitions themselves is done through
asynchronous communication protocols that have mechanisms
for remote method invocation and for allowing direct streaming of results from a graph partition machine to the client
without involving the graph coordinator.
C. Graph Partition
Every graph partition manages a set of graph nodes and
edges. Partitions are the main scale-out mechanism for Horton.
Each partition resides on a separate server and maintains graph
data in main memory. When a graph partition receives the
ﬁnite state machine of a query from the graph coordinator, it
executes the query using a local execution engine. The graph
partition may need to communicate with other graph partitions
because the execution of a single query may involve distributed
processing among several partitions.

E. Implementation
Horton is written in C# in the .NET framework. Asynchronous communication is implemented using sockets and
.NET TPL (task parallel library). Horton is built on top of
Orleans [3] which is a distributed runtime for cloud applications.
IV. D EMONSTRATION S CENARIO
A. Demonstration Setup

Query Execution Engine. Each partition has an execution
engine that takes the ﬁnite state machine of the query as input
and runs a bulk synchronous [10] breadth ﬁrst graph traversal
constrained by the ﬁnite state machine. The execution engine
checks whether the nodes which are local to the partition
satisfy the ﬁnite state machine. Next, for all the nodes that
satisfy the ﬁnite state machine, the execution engine checks if
their outgoing edges also satisfy the state machine, and decides
whether to continue traversing along the path. When the query
execution engine ﬁnds a graph node that matches an accepting
state in the ﬁnite state machine (and therefore satisﬁes the
original query), the execution engine sends this result to the
client.

In the demonstration we show the processing of ad-hoc
online queries over Codebook graphs, as we choose to use
realistic graphs and queries, rather than synthetic data. Codebook [1], [2] is a social network application that represents
software engineers, software components, and their interactions in a large software project. In particular, Codebook
manages information about source code with its revisions,
documentation, and the organizational structure of developers
to answer queries such as “Who wrote this function?”, “What
libraries depend on this library?”, and “Whom should I
contact to ﬁx this bug?”. An example of a Codebook graph is
presented in Figure 4.

1291

Fig. 4.

Example of a Codebook graph [2].

We also demonstrate other ad-hoc queries, showing the
ﬂexibility of the query language and the effectiveness of the
execution engine.

B. Demonstration Scenario
We demonstrate the query processing phases. We issue adhoc queries using command line interface. Query examples
and their corresponding commands in Horton are given below:

V. ACKNOWLEDGEMENT
We thank Alan Geller and Jim Larus for their feedback. We
also thank Sergey Bykov, Ravi Pandya, Jorgen Thelin, Andrew
Begel, Timothy Cook, and Ron Estrin for their part in building
the infrastructure and for many fruitful discussions.

1) Which pieces of source code are modiﬁed by Dave?
C:/> Horton -query "(Person Dave) Committer ChangeSet
Modifies FileRevision Modifies SoureCode"

R EFERENCES

2) Who wrote the speciﬁcation for the MethodSquare
code?

[1] Andrew Begel and Robert DeLine. Codebook: Social networking over
code. In Proceedings of the International Conference on Software
Engineering, ICSE, 2009.
[2] Andrew Begel, Khoo Yit Phang, and Thomas Zimmermann. Codebook:
Discovering and Exploiting Relationships in Software Repositories. In
Proceedings of the International Conference on Software Engineering,
ICSE, 2010.
[3] Sergey Bykov, Alan Geller, Gabriel Kliot, James Larus, Ravi Pandya,
and Jorgen Thelin. Orleans: Cloud Computing for Everyone. In
Proceedings of the ACM Symposium on Cloud Computing, SOCC, 2011.
[4] Rishan Chen, Xuetian Weng, Bingsheng He, and Mao Yang. Large
Graph Processing in the Cloud. In Proceedings of the ACM International
Conference on Management of Data, SIGMOD, 2010.
[5] Rishan Chen, Xuetian Weng, Bingsheng He, Mao Yang, Byron Choi,
and Xiaoming Li. On the Efﬁciency and Programmability of Large
Graph Processing in the Cloud. Technical Report MSR-TR-2010-44,
Microsoft Research, 2010.
[6] Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simpliﬁed Data
Processing on Large Clusters. In Proceedings of the USENIX Symposium
on Operating System Design and Implementation, OSDI, 2004.
[7] Facebook. http://www.facebook.com/.
[8] Linkedin. http://www.linkedin.com.
[9] Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik, James C.
Dehnert, Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. Pregel: a
System for Large-Scale Graph Processing. In Proceedings of the ACM
International Conference on Management of Data, SIGMOD, 2010.
[10] Leslie G. Valiant. A bridging Model for Parallel Computation. Commun.
ACM, 33(8):103–111, 1990.

C:/> Horton -query "(Code MethodSquare) MentionedBy
WordDocument AuthoredBy Person"

3) Who is the manager of the person who closed or
created work item bug #673?
C:/> Horton -query "Person Manages Person (Closed |
Created) (WorkItemRevision #673)"

The ﬁnite state machine for query 1 is shown in Figure 3(a).
The start state is S0, then the transition from a state to another
is conditioned by a node predicate (e.g., Node Type = ChangeSet) or an edge predicate (e.g., Edge Type = Committer).
The accepting state is S7. Figure 3(b) shows the ﬁnite state
machine for query 3, which contains a disjunction (i.e., Closed
OR Created). Optimizations are enabled by ﬂag, -optimize, as
in the following example:
C:/> Horton -optimize -query "(Person Dave) Commits
ChangeSet Modifies FileRevision Modifies Code"

The system reports the execution time of each query. The
query result is in the form of graph paths (a sequence of graph
nodes). For example, the result for query 1 issued on the graph
shown in Figure 4 is as follows:
Answer Path1:
(Dave) (ChangeSet #45) (FileRevision $Foo/Moo.cs#6)
(SourceCode MethodSquare)

1292

The Second ACM SIGSPATIAL PhD Workshop Report
Seattle, Washington, USA - November 3, 2015
Peter Scheuermann1 , Mohamed Sarwat2
1
Department of Computer Science, Northwestern University, USA
2
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, USA
1
demiryur@usc.edu,2msarwat@asu.edu

1 Summary
The ACM SIGSPATIAL Ph.D. Workshop is a forum where Ph.D. students present, discuss, and receive feedback on their research in a constructive atmosphere. The Workshop will be attended by professors, researchers
and practitioners in the ACM SIGSPATIAL community, who will participate actively and contribute to the discussions. The workshop provided an opportunity for doctoral students to explore and develop their research
interests in the broad areas addressed by the ACM SIGSPATIAL community. We invited PhD students to submit
a summary of their dissertation work to share their work with students in a similar situation as well as senior
researchers in the field. Submissions provided a clear problem definition, explain why it is important, survey
related work, and summarize the new solutions that are pursued. Submissions also focused on describing the
contribution they made in their doctoral dissertation. The strongest candidates were those who have a clear topic
and research approach, and have made some progress, but who are not so far along that they can no longer make
changes.

2 Program
This year, we accepted six papers to the PhD Workshop. The list of papers is as follows:
• Shrutilipi Bhattacharjee (University of Minnesota) “Prediction of Meteorological Parameters: A Semantic
Kriging Approach”
• Zhe Jiang (University of Minnesota) “Learning Spatial Decision Trees for Land Cover Mapping”
• Jiaxin Ding (Stony Brook University) “Trajectory Mining, Representation and Privacy Protection”
• Salles Viana Gomes Magalhães (Rensselaer Polytechnic Institute) “An efficient algorithm for computing
the exact overlay of triangulations”
• Benedict Budig (Universität Wrzburg) “Efficient Algorithms and User Interaction for Metadata Extraction
from Historical Maps”
• Logic Salmon (Naval Academy Research Lab) “A Holistic approach combining real-time and historical
data for maritime traffic monitoring”
Authors got the chance to present their papers and get feedback on their dissertation topic from experienced
researchers (from both academia and industry) in Geographic Information Systems and Spatial Data Analytics.

41

3 Keynote
The workshop featured a keynote speech by Professor Mario Nascimento (University of Alberta) entitled “Using
the Wisdom of the Crowd in Route Planning”. The talk outlined a number of methods for extracting crowdsourced geospatial data in order to compute routes that minimize an objective function that combines qualitative
(i.e., popularity) and quantitative (e.g., distance) metrics. These methods are extracting information from travel
logs and picture repositories and use it to construct an enriched road network. This network can subsequently
be searched to obtain paths between different points that include points-of-interest with higher popularity, while
incurring only a small additional spatial distance.

4 Panel
The workshop ended with a well-attended panel discussion on the topic “How to succeed in the business of
PhD”. The panel included experts from industry (John Krumm, Microsoft Research) and academia (Lars Kulik,
University of Melbourne; Gabriel Ghinita, Northeastern University; Mohamed Sarwat, Arizona State University). The panel addressed a range of issues, such as how to choose a thesis topic, how to disseminate your
research results as well as how to start/manage a research group in academia and industry.

42

Horton+: A Distributed System for Processing Declarative
Reachability Queries over Partitioned Graphs
Mohamed Sarwat1
1

Sameh Elnikety2

Mohamed F. Mokbel1

Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA
2
Microsoft Research, Redmond, WA, USA

ABSTRACT
Horton+ is a graph query processing system that executes declarative reachability queries on a partitioned attributed multi-graph.
It employs a query language, query optimizer, and a distributed
execution engine. The query language expresses declarative reachability queries, and supports closures and predicates on node and
edge attributes to match graph paths. We introduce three algebraic
operators, select, traverse, and join, and a query is compiled into an
execution plan containing these operators. As reachability queries
access the graph elements in a random access pattern, the graph is
therefore maintained in the main memory of a cluster of servers to
reduce query execution time. We develop a distributed execution
engine that processes a query plan in parallel on the graph servers.
Since the query language is declarative, we build a query optimizer
that uses graph statistics to estimate predicate selectivity. We experimentally evaluate the system performance on a cluster of 16
graph servers using synthetic graphs as well as a real graph from
an application that uses reachability queries. The evaluation shows
(1) the efficiency of the optimizer in reducing query execution time,
(2) system scalability with the size of the graph and with the number of servers, and (3) the convenience of using declarative queries.

1.

Yuxiong He2

INTRODUCTION

Graphs are widely used in many application domains, including social networking [31], software collaboration [4], geo-spatial
road networks [37], interactive gaming [44], among others [13, 27].
For example in a social network graph, a node represents a person,
photo, video, location, event, or group. An edge represents a binary
relation between two persons such as friendship, family or work relations such as “advisor of” and “manager of”. An edge shows that
a person is tagged in a photo, is attending an event or is member
of a group. Common queries include “finding Alice’s photos taken
in Singapore”, “finding Bob’s friends of friends”, and “finding all
people advised directly or indirectly by Prof. Carol”.
Several emerging applications, e.g., Facebook Graph
Search [16], allow users to issue interactive queries over a
graph. In such applications, graph nodes and edges have several
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Articles from this volume were invited to present
their results at The 39th International Conference on Very Large Data Bases,
August 26th - 30th 2013, Riva del Garda, Trento, Italy.
Proceedings of the VLDB Endowment, Vol. 6, No. 14
Copyright 2013 VLDB Endowment 2150-8097/13/14... $ 10.00.

attributes, and users query the modelled entities and their relationships. Such queries can be expressed as graph paths, and
we call this query class reachability queries over an attributed
multi-graph. These applications introduce the following requirements: (1) Usability: A query should be declarative, rather than
procedural. (2) Low latency: As applications are interactive, they
require low query execution time. (3) Scalability: The employed
graphs may not fit on a single server, motivating a distributed
system design.
To motivate the need for a new system, we briefly discuss the
existing categories of graph processing systems: (1) Relational
database systems are not efficient in handling graph reachability
queries because these queries are recursive and may contain closures. SQL needs to be extended with recursion to execute such
queries. (2) Semi-structured data management systems, e.g. [36,
45], provide query languages such as SPARQL [43] to query
RDF data, and XML query languages to query XML documents.
SPARQL-based systems target a different class of queries, graph
pattern matching rather than reachability. XML querying techniques [45] manage tree-structured data instead of graphs. (3) Centralized graph platforms, e.g. Grace [34] and GraphChi [20], require the graph to fit on one server. (4) Distributed graph platforms,
such as Pregel [30], Giraph [19], Trinity [39] and PowerGraph [29],
accept procedural programs to be computed over the graph. Such
systems focus on graph computations rather than graph querying,
and they assume the users to be expert programmers.
In this paper, we present the design, implementation and evaluation of Horton+, a distributed system for processing declarative
reachability queries over a partitioned graph. Horton+ employs a
declarative query language that uses regular language reachability
to express reachability queries over an attributed multi-graph. We
introduce three algebraic graph operators, select, traverse, and join,
and use them to execute a query plan.
Executing graph reachability queries generates a random access
pattern to the memory system; Horton+, therefore, manages the
graph in the main memory of a cluster of servers. We build a distributed execution engine that executes the three graph operators in
parallel and batches messages among the graph servers.
Since the query language is declarative, rather than procedural,
there are multiple ways to execute a query. Horton+ is equipped
with a query optimizer that reduces the query execution latency.
The system maintains a set of graph statistics that are used by the
optimizer. Among a rich space of possible query plans, the optimizer employs a cost model and selectivity estimation techniques
to estimate the cost of executing each plan. The optimizer selects
the execution plan that minimizes the expected number of visited
graph nodes and reduces communication among the graph servers.
Horton+ is the first distributed query processor for processing

1918

Query ::=

Partition A
Dan

Carol

Manages

Friend

g
Ta

Ta
g

{Age=25}

Photo1

Alice

g
Ta

Ta
g

{Year=2012}

Friend

Bob

Manages

Graph:
Alice is a friend of both Bob and Dan.
Dan' s age is 25.
Photo1 is taken in year 2012.
Photo1 tags Alice, Bob, Carol, and Dan.
Alice is the manager of Bob.
Bob is the manager of Carol.

NodePred ::=

Query: 'Alice'-Friend-Person
Answer paths:
Alice-Friend-Bob
Alice-Friend-Dan

EdgePred ::=

Query: 'Alice'(-Manages>-Person)*
Answer paths:
Alice-Manages>-Bob
Alice-Manages>-Bob-Manages>-Carol

AttrPred
Operand
NodeType
EdgeType

Partition B

::=
::=
::=
::=

NodePred
Query-EdgePred-Query
(Query OR Query)
Query (-EdgePred-Query)∗
(Query-EdgePred-)∗ Query
Query (-EdgePred-Query)+
(Query-EdgePred-)+ Query
Id | NodeType | NodeType{(AttrPred)+ }
(NOT NodePred)
(NodePred AND NodePred)
(NodePred OR NodePred)
EdgeType | EdgeType{(AttrPred)+}
(NOT EdgePred)
(EdgePred AND EdgePred)
(EdgePred OR EdgePred)
Operand BinaryOperator Operand
AttributeName | AttributeValue
Node | TypeId
Edge | TypeId

Figure 1: Small fragment of a social graph.

Figure 2: Abstract syntax of the graph query language.

reachability queries over an attributed multi-graph. We implement
Horton+ and evaluate it experimentally to show its scalability and
efficiency in executing graph reachability queries using both real
and synthetic graphs on a cluster of 16 graph servers. We also
compare Horton+ with the Giraph [19] system to highlight benefits
of Horton+ and its declarative query language and optimizer. An
early version of the system, called Horton was demonstrated [38].
Horton+ introduces major additions, including the graph operators,
query optimization, and different query plan and execution model
with a formal query language.
In summary, the contributions of this paper are the following:
(1) Horton+ as a full-fledged system for distributed processing of
graph reachability queries. (2) A formal graph query language that
supports reachability queries over an attributed multi-graph (Section 2.2). (3) A distributed query processor that executes query
plans over multiple graph servers (Section 3). (4) A query optimizer, equipped with cost and selectivity estimation techniques
(Section 4), that optimizes the issued query. (5) An experimental evaluation on a cluster of servers using both real and synthetic
graphs and a comparison with Giraph (Section 5).

graph, each node stores both inbound and outbound edges to allow
queries to traverse both directions.

2.

GRAPH MODEL & QUERY LANGUAGE

In this section, we give an overview of the graph model in Section 2.1, and describe the query language in Section 2.2.

2.1 Graph Model
We use a general graph model; an attributed multi-graph G =
{V, E } has a set of nodes V and a set of edges E . A node represents
an entity with a primary key (id), a categorical type (e.g., person,
photo or event), and a set of arbitrary attributes. An edge is a binary
relationship between two nodes, and it has a categorical type (e.g.,
friend-of or tagged-in), and a set of arbitrary attributes (e.g., edge
direction and edge weight). Multiple edges may link two nodes,
representing several relationships.
Figure 1 shows a fragment of a social graph as an example. There
are two node types: Person (Alice, Bob, Carol, Dan) and Photo
(Photo1), and three edge types: Friend, Tag and Manages. A node
may have attributes, e.g., age of Dan is 25. The figure also shows
two queries and their answers. The graph is partitioned: Nodes
Carol and Dan are in partition A, and nodes Alice, Bob, and Photo1
are in partition B.
Horton+ manages both directed and undirected graphs in main
memory with pointer-based representation. In the case of a directed

2.2 Query Language
The objective of the query language is to express graph reachability queries declaratively, rather than in a procedural manner,
making developers more productive and allowing query optimization. The query language specifies relationships between entities
as graph paths. Figure 2 depicts the abstract syntax of the language. The non-terminal Query is the start symbol, and a query
starts with a node predicate and possibly followed by a sequence of
edge and node predicate pairs. Closures specify paths of arbitrary
length, and they are supported using Kleene star “∗ ” (zero or more)
and Kleene plus “+ ” (one or more).
A node predicate specifies a node id, a node type such as Photo to
match nodes of type photo, or Node to match any node type. A node
predicate may contain predicates on node attributes, e.g., photos
that are black and white taken this year (Photo{color=‘B&W’
AND year=2013}). Node predicates can be composed. For example, the predicate (Photo OR Video), which matches nodes
of type photo or video, is composed of two predicates. Similarly, an edge predicate specifies an edge type. For instance, a
Tag, Friend, or Edge matches a tag edge, friend edge, any edge,
respectively. An edge predicate can also specify multiple predicates on edge attributes, e.g., a friendship relation since last year
(Friend{year=2012}). We use the "<" and ">" symbols to
represent edge directions in a directed graph.
Q1
Q2
Q3
Q4

‘Alice’-Tag>-Photo-Tag<-‘Bob’
Photo-Tag<-Person-Friend-‘Alice’
‘Alice’-Tag>-Photo-Tag<-Person-Friend-‘Alice’
‘Alice’(-Advice>-Person)∗-Coauthor-‘Bob’

Table 1: Query examples.
Example. Table 1 shows four example queries. Q1 finds all
photos in which both Alice and Bob are tagged. Q1 has three node
predicates and two edge predicates: The first node predicate specifies the node id (‘Alice’). The second node predicate provides
the node type (Photo), and the third node predicate specifies the
node id. The two edge predicates specify edge type (Tag). Similarly, Q2 retrieves all photos in which a friend of Alice is tagged,
and Q3 finds all photos in which Alice is tagged with one of her
friends. Q4 finds whether Prof. Alice or her academic descendants

1919

Q

Query Plan
Node.id = ‘Bob’

1

2

S0

S1
Node.type
= Photo

S0

Node.id = ‘Bob’

S0

S1

S3

S4

S3
Node.type
= Person

Edge.type
= Friend

S2
Edge.type =
CoAuthor

S3
Node.type
= Person

S2

S5

Node.id = ‘Alice’

S3

S5
Node.type
= Person

Edge.type
= Tag

Node.type
= Photo

S2

Node.id = ‘Alice’

S4
Edge.type
= Friend

Node.type
= Person

Edge.type
= Tag

S1

Edge.type
= Tag

S3

S2

S1

Node.id = ‘Alice’

4

S2

S1

3

S0

Node.type
= Photo

Edge.type
= Tag

Node.id = ‘Alice’

S0

Edge.type
= Tag

S4

S5

JOIN
Node.id = ‘Alice’

S4

Edge.type
= Advise

Figure 3: Query execution plan for Q1 , Q2 , Q3 , and Q4 .
have co-authored a paper with Bob. Q4 is recursive with a closure
((-Advice>-Person)∗) over a pair of predicates.

3.

DISTRIBUTED QUERY PROCESSOR

The query processor receives an input query and returns the
matched results. The input query is compiled into a query plan,
which can be executed directly, or first optimized and subsequently
executed. We introduce three algebraic graph operators, select, traverse, and join. Employing these operators is important: (1) The
query processor becomes a composition of a few basic building
blocks. Each operator has clearly defined functionality and efficient
implementation. (2) The query optimizer builds a cost model for
each operator using graph statistics to explore cost-efficient ways
to combine them to answer a query. For clarity of presentation,
we first describe the operators assuming a centralized environment,
and we next present the distributed implementation.

3.1 Compilation into an Execution Plan
An input query is compiled into a plan containing one or more
deterministic finite state automata (DFA) and graph operators. The
query plan has a recursive tree structure: Each tree node is either a
leaf node containing a DFA, or an intermediate node containing the
join operator and two trees where each is a query plan.
Example. Figure 3 shows four execution plans for queries Q1 ,
Q2 , Q3 , and Q4 . The plan for Q1 is a leaf node containing a DFA
that has six states (S0 to S5 ). S0 is the starting state, and the set
of starting nodes for the query are the nodes which satisfy the first
node predicate (Node.id=‘Bob’) on the transition from S0 to S1 .
The plan of Q3 includes a join between two query plans, and Q4 ’s
plan contains a loop because the query contains a Kleene star.

3.2 Algebraic Graph Operators
To execute the query plan, we introduce the following graph operators: The select operator locates the starting nodes from which
path matching proceeds. The traverse operator traverses a set of
nodes through their edges to a new set of nodes iteratively. The
traversal is conditioned by the DFA transition predicates, and partial graph paths are accumulated. The join operator joins the result
of two query plans to construct longer paths. The query optimizer
introduces the join operator into the query plan, and it rewrites a
DFA into one or several more efficient DFAs. DFA matching is

performed first using the select operator to find the starting nodes,
then a sequence of calls to the traverse operator.
Select Operator. The objective of the select operator is to determine the set of starting nodes efficiently. The operator takes a DFA
and applies the transition predicate from the initial state on nodes
to select the set of starting nodes. The operator employs a primary
key index if the node primary key (id) is specified, or a hash index on the node type if node type is specified, and secondary index
structures are exploited to match attribute predicates. If no index
is available or predicate selectivity is very low, the select operator
applies the predicates on all nodes of the given type.
Traverse Operator. The traverse operator is iterative; it receives
a set of partial paths and the DFA. Initially the set of partial paths
is the set of starting nodes from the select operator. In each iteration, the traverse operator matches each partial path into one or
more longer paths if they satisfy the transition predicates of the
DFA state. For the graph elements that satisfy the predicates, the
traverse operator appends them to the partial paths, and the partial paths that are not extended in the iteration are dropped. Upon
reaching an accepting state, the partial paths are returned as matching results for the DFA. This processing pattern results in traversing
the graph in breadth-first manner from each starting node.
The complexity of the traverse operator iterations is upper
bounded by the product of the number of start nodes, length of the
query, and expected number of edges per node in the graph when
the query has no closures. We discuss a more accurate cost estimation using graph statistics in Section 4.
The termination of the traverse operator iterations is an important
property since the graph may contain cycles, and the DFA may contain loops (corresponding to closures). Each partial path maintains
the DFA state at which each node and edge was matched along the
path. Before a node is visited, the partial path is checked to ensure
that the node is not visited again in the same DFA state. All queries
terminate since each node is visited at most once in each state of
the DFA for each constructed path.
Join Operator. The join operator receives two sets of matching
paths from two query plans, and constructs longer paths by joining paths from the two sets. After each query plan is evaluated
independently to produce its resulting paths, the join operation is
performed based on the ids of the last nodes from the first path set
and the ids of the last nodes of the second path set. For example,
the two DFA’s of Q3 in Figure 3 are joined based on the Person
id matched at S5 of the first DFA and at S3 of the second DFA.

3.3 Distributed Execution Engine
We use multiple partition servers (a) to query graphs that do not
fit in the main memory of a single server, and (b) to evaluate each
query in parallel on the partition servers. In this section, we discuss
the architecture of the distributed execution engine and the implementation of the algebraic graph operators.
Architecture. The graph is partitioned into disjoint partitions,
each managed by a partition server that is responsible for managing its own subset of graph data and associated indexes. Each edge
is represented at both the source and destination nodes. A remote
edge that connects two nodes in two different partitions specifies
both the id of the remote node and the target partition where the remote node exists. A server is designated as the coordinator, and is
responsible for query parsing, compilation and optimization. The
coordinator uses a directory service that maintains two mappings:
a mapping from a partition id to a server network address, and another mapping from node id to a partition id. The coordinator also
maintains the graph statistics used by the optimizer.

1920

Algorithm 1 Distributed query processing
1: Function E VALUATE(QTree)
/* Case 1: The query tree is a Join */
if QTree is JOIN then
L AnswerPaths ← E VALUATE(QTree.LeftSubTree)
R AnswerPaths ← E VALUATE(QTree.RightSubTree)
AnswerPaths ← J OIN O PERATOR (L AnswerPaths, R AnswerPaths)
/* Case 2: The query tree is a DFA */
6: if QTree is Leaf DFA node then
7: for all Partition P in AllPartitions do
8:
StartingNodes[P] ← P. SELECTO PERATOR (QTree.DFA)
9:
PartialPaths[P] = StartingNodes[P]
10:
if StartingNodes[P] 6= ∅ then
11:
CurrentPartitions += P
12: if CurrentPartitions 6= ∅ then
13:
AnswerPaths ← T RAVERSEO PERATOR (QTree.QueryID,
QTree.DFA, CurrentPartitions)
14: return AnswerPaths

2:
3:
4:
5:

An effective graph partitioning algorithm assigns nodes to partitions to preserve locality in graph accesses, and it reduces communication overhead among partitions during query processing.
Graph partitioning is not the focus of our work. Horton+ can, however, incorporate any existing graph partitioning scheme, including
hashing (which is used by default) or more sophisticated partitioning tools [1].

3.3.1 Operators
Select Operator. The select operator determines the set of starting nodes by evaluating the first transition predicate of the DFA.
In the distributed environment, the coordinator invokes the select
operator on all partitions in parallel. A partition replies with a message to the coordinator indicating whether it finds matching nodes
or not. The coordinator registers the partitions with matches as participants in the DFA execution. An important special case is when
the DFA transition predicate is a primary key equality, providing
the node id. The coordinator invokes the select operator at the target partition, determined by the directory service.
Traverse Operator. The traverse operator starts at each partition with one or more starting nodes as determined by the select
operator. It then initiates a bulk synchronous breadth first search
(BFS) [46] among the participant partitions, synchronized by the
coordinator, to transition from one DFA state to the next while
matching graph elements. The traverse operator iterates through a
sequence of BFS levels that represent the DFA states. At each DFA
state (BFS level), the traverse operator performs three main steps:
(1) Local computation is the step in which each participant partition
runs its own local query execution engine to check on the graph elements that satisfy the current DFA state. If an accepting DFA state
is reached, signaling a matching path, the partition communicates
the path to the coordinator. (2) Global communication is the step
where graph partitions send messages to each other to prepare for
the next DFA state (which could point to a node in a different partition). (3) Bulk Synchronization is the coordination/synchronization
step needed to advance to the next DFA state. The coordinator implements a barrier, waiting for synchronization messages from all
participant partition servers in order to advance the DFA to the next
breadth first search level. When a partition cannot advance a DFA
as no match is found, it sends a no-more-matches message to the
coordinator. The distributed evaluation terminates when the coordinator receives all matching paths and no-more-matches messages
from all participating partitions.
Join Operator. The coordinator either runs the join operator
locally or assigns it to one of the partition servers with matching
results. We apply a heuristic that selects the least-loaded partition
server to process the paths matching the two query plans. The join

Algorithm 2 Traverse operator
1: Function T RAVERSE O PERATOR(QueryID, DFA, CurrentPartitions)
2: AnswerPaths←∅
DFACursor←0
3: /* Sequence of Breadth First Search (BFS) levels (DFA states) */
4: while CurrentPartitions 6= ∅ do
5: /* STEPS 1 & 2: Local Computation & Global Communication */
6: for all Partition P in CurrentPartitions do
7:
PARTITION E XECUTION E NGINE(P, QueryID, DFA, DFACursor)
8: CurrentPartitions ← ∅
9: /* STEP 3: Bulk Synchronization/Coordination */
10: PartitionsMsgs = WAITFORPARTITIONS(QueryID, DFA)
11: for all Partition P in PartitionsMsgs do
12:
AnswerPaths += P.FullPaths
13:
CurrentPartitions += P.NextPartitions
14: DFACursor ← ADVANCECURSOR(DFA, DFACursor)
15: return AnswerPaths

is performed using sort-merge join, which takes O(R log(R) +
S log(S) + R + S) time to run at the coordinator where R and
S denote the number of paths from the two plans to join.

3.3.2 Algorithm and Communication Patterns
Algorithm 1 shows the workflow of the distributed query execution using the operators. The algorithm takes the query execution
plan QT ree as input, and returns the set of matching paths. The
algorithm handles two cases:
Case 1: QT ree is a join: The algorithm recursively executes the
left and right trees of QT ree representing two query plans. It then
joins their results using the J OIN O PERATOR.
The communication pattern of a join operator is as follows: (1)
The coordinator starts the execution by sending the query plans to
the partition servers; it also decides where to run the join operator
and informs the partition servers. (2) After the evaluation of the
two trees completes, the partition servers send their matching paths
to the designated server that performs the join operation.
In summary, for a join operator, the coordinator sends P control
messages to start the execution, and the partitions send their results
back to the designated server with at most P data messages, where
P is the number of graph partitions.
Case 2: QT ree is a leaf DFA node: The algorithm runs an initialization step (lines 6 to 11) where the coordinator broadcasts the
query DFA to all partitions to invoke the select operator, S ELEC T O PERATOR , to find the starting nodes at each partition. Next,
T RAVERSE O PERATOR (Algorithm 2) is invoked to find the matching paths. Finally, the matched paths are returned as answer.
Algorithm 2 gives the pseudo code of the T RAVERSE O PERA TOR . The algorithm takes the following inputs: (1) QueryID: represents the ID of issued query, (2) DFA: denotes the query DFA, and
(3) CurrentPartitions represents the set of participant graph partitions. T RAVERSE O PERATOR initializes the DFA cursor to the first
DFA state (DFACursor←0). The algorithm iterates over a sequence
of breadth first search (BFS) levels (lines 4 to 14).
STEP 1 & 2: Local Computation & Global Communication
(lines 4 to 8): At each BFS level, the coordinator signals all partitions P ∈ CurrentPartitions to advance the DFACursor to the next
DFA state. Each partition receives the partial path matches from
other partitions with the ending nodes belonging to the partition,
and it combines them with its local partial matches. Then, it checks
for local graph elements to match the current DFA state. When the
next DFA state points to a remote node (located in a different partition), the partition server buffers all the partial matches towards the
same remote partition into a message, and sends the message to the
remote partition. As each partition server sends at most one message to all other partitions, the total number of these data messages
at each level of BFS is bounded by P (P − 1).

1921

Q
1

2

3

Alternative query execution plans
Plan1
Plan2
Plan3
Plan1
Plan2
Plan3
Plan1
Plan2
Plan3

:
:
:
:
:
:
:
:
:

Plan4 :
4

Plan1 :
Plan2 :

‘Alice’ Tag Photo Tag ‘Bob’
‘Bob’ Tag Photo Tag ‘Alice’
(‘Alice’ Tag Photo)1(‘Bob’ Tag Photo)
Photo Tag Person Friend ‘Alice’
‘Alice’ Friend Person Tag Photo
(‘Alice’ Friend Person)1(Photo Tag Person)
‘Alice’ Tag Photo Tag Person Friend ‘Alice’
‘Alice’ Friend Person Tag Photo Tag ‘Alice’
(‘Alice’ Tag Photo) 1
(‘Alice’ Friend Person Tag Photo)
(‘Alice’ Tag Photo Tag Person) 1
(‘Alice’ Friend Person)
‘Alice’ (Advise Person)∗ Coauthor ‘Bob’
‘Bob’ Coauthor (Person Advise)∗ ‘Alice’

Table 2: Examples of alternative query plans.
STEP 3: Bulk Synchronization (lines 9 to 14): The coordinator waits for messages from participant partitions CurrentPartitions
signalling the end of local computation and global communication steps. The received messages PartitionsMsgs may contain two
pieces of information: (1) FullPaths: represent a set of full matching graph paths if found. (2) NextPartitions: represent the set of
partitions that will participate in the next BFS level. The partial
matches are communicated only among partition servers; the coordinator does not send or receive any intermediate result. The T RA VERSE O PERATOR terminates when there is no participant partition
at the next level (CurrentPartitions=∅).
In summary, each level of the traverse operator incurs (1) at most
2P control messages between the coordinator and the partitions to
signal the start and end of the level, and (2) at most P (P − 1)
data messages among the partitions to exchange intermediate results. The size of the control messages is small, and therefore, the
workload of the coordinator is light without requiring any intensive computation or communication. The size of the data messages
depends on each query, and it depends on the size of intermediate
results that are distributed among the partition servers. The total
levels of BFS is bounded by the length of the DFA if it does not
contain a loop.

4.

QUERY OPTIMIZATION

The declarative language of Horton+ makes query optimization
possible and important. A declarative language only expresses the
logic of a computation without describing its control flow: the
query optimizer of Horton+ can choose to run a query among many
implementations (or execution plans), preferably the one with the
lowest cost. However, finding such an execution plan is not trivial, which requires an accurate cost model that estimates the cost
of execution plans by taking into account of graph statistics and
a computationally-efficient enumeration algorithm that finds the
lowest-cost solution in a short amount of time. This section describes query optimizer of Horton+, which efficiently finds an optimal query execution plan (visiting the fewest number of graph
nodes) for queries without closure operators. Moreover, we develop a heuristic algorithm to perform optimization for queries with
closure operators, and we discuss how the optimizer takes communication cost into consideration for distributed graphs.

4.1 Space of Query Plans
The query optimizer takes a compiled query plan as input and
outputs an efficient query execution plan. The output plan is represented as a tree with DFAs in the leave nodes and join operators as

the intermediate nodes. To produce an efficient plan, the optimizer
enumerates various execution plans, estimates their costs and returns the lowest-cost plan. Here we define the cost of an execution
plan by estimating the total number of nodes it visits. The fewer
the number of visited nodes, the more efficient the execution plan.
For a graph query Q = h N1 , E1 , · · · , Ni , Ei , Ni+1 , · · · , Ek−1 ,
Nk i, with k node predicates and k − 1 edge predicates, Horton+
query optimizer first considers k possible plans as follows: (1) One
plan is to execute the query starting from N1 to Nk . (2) Another
plan is to execute the query in the reverse order, from Nk to N1 .
(3) k − 2 plans as dividing Q at node Ni , 1 < i < k, into two subqueries as follows: (a) a subquery that starts from N1 and ends at
Ni , and (b) a subquery that starts from Ni and ends at Nk . The results from the two subqueries are joined to produce the final answer.
Each of the two subqueries can be recursively optimized by considering its execution in the forward and reverse order, as well as
further splitting into shorter subqueries. However, for simplicity illustration, we first describe a simple version of the query optimizer
where the first subquery is executed in the forward order from N1
to Ni while the second subquery is executed in the reverse order,
from Nk to Ni . We present the complete recursive query optimizer
in Section 4.6. A query can be executed in the forward or reverse
order because each edge (whether directed or not) can be accessed
from the its two nodes.
Example. Table 2 gives all non-recursive query plans that the
query optimizer considers for queries Q1 to Q4 . For example,
Q3 has four node predicates, and hence four possible plans as follows: (1) The forward order from N1 to N4 , which finds the graph
node for Alice, then finds photos in which Alice is tagged.
From these photos, we find all persons tagged in any of these
photos. Among these persons, we find the ones who are friends
with Alice. (2) The reverse order, from N4 to N1 , which finds
the graph node for Alice, then all friends of Alice. For these
friends, we find all photos in which they are tagged. Among these
photos, we find the ones in which Alice is tagged. (3) A join at
N2 , where we have two subqueries. The first subquery finds all
photos in which Alice is tagged (the forward order from N1 to
N2 ), while the second subquery finds all Alice friends, and then
finds all photos in which Alice friends are tagged (the reverse
order, from N4 to N2 ). The outputs of the two subqueries (set of
photos) are joined to get the intersection. (4) A join at N3 , where
we have two subqueries. The first subquery goes from Alice to
all photos she is tagged in, and then all persons who are tagged in
these photos (the forward order from N1 to N3 ), and the second
subquery goes from Alice to all her friends (the reverse order,
from N4 to N3 ). The outputs of the two subqueries are joined.

4.2 Graph Statistics
This section outlines four main statistics functions, S(Ni ),
T (Ni ), F (Ni , Ej , Nh ), and G(Ni , Ej , Nh ) that are used to estimate the cost of each considered query plan.
S(Ni ) and T(Ni ). Given a node predicate Ni , S(Ni ) estimates the
number of nodes that satisfy predicate Ni while T (Ni ) estimates
the number of nodes that need to be visited to find the ones satisfying Ni . Here S(Ni ) represents the selectivity of a node predicate
indicating the number of successful matches while T(Ni ) represents
the cost to find the successful matches. We use rules similar to
those used in the query optimizer of relational database systems. If
the node predicate is an id equality, indexed as a primary key, then
S(Ni ) = T (Ni ) = 1. If the predicate is on a non-indexed field
and one tenth of the nodes in this node type satisfying the predicate,
then S(Ni ) = 0.1m and T (Ni ) = m where m is the total number
of nodes of this node type. If a histogram is maintained, we can get

1922

a better accuracy on estimating S(Ni ), yet T (Ni ) depends on the
index availability.
F(Ni , Ej , Nh ) and G(Ni , Ej , Nh ). Given two node predicates
Ni and Nh and an edge predicate Ej , F (Ni , Ej , Nh ) estimates
the number of nodes that are reachable from Ni through the edge
predicate Ej and satisfy the predicate Nh while G(Ni , Ej , Nh )
estimates the number of nodes that need to be visited to find these
nodes, i.e., the number of reachable nodes from Ni to the node
type of Nh using the edge predicate Ej . Again, F (Ni , Ej , Nh )
is a measure of selectivity indicating the number of successful
matches while G(Ni , Ej , Nh ) measures the cost to find the successful matches. G(Ni , Ej , Nh ) is computed by utilizing few
statistics maintained for the number of edges of each type connected to each node type. This number is then divided by the selectivity of the predicate at node Nh to compute F (Ni , Ej , Nh ). For
example, if Ni is of id Alice, then, we know that there is only one
node satisfying predicate Ni . Then, if Ej is of type Tag, and we
know from our statistics that Alice is tagged in 20 photos, then,
we say that we will visit 20 nodes matching Nh so F (Ni , Ej , Nh )
= G(Ni , Ej , Nh ) = 20. However, if the predicate Nh includes only
black & white photos, and we know that only 10% of the photos
are black & white, then F (Ni , Ej , Nh )=2 while the total number
of visited nodes is G(Ni , Ej , Nh )=20 as we have to visit all of the
20 photos in order to find the black & white ones.
Collecting Statistics. Since the underlying graph is partitioned and
distributed among multiple graph partition servers, collecting the
aforementioned statistics is performed as follows: (1) The graph
directory service (DS) sends a statistics collection request to all
graph partition servers. (2) Each graph partition server, in parallel, calculates the graph statistics S(Ni ), T (Ni ), F (Ni , Ej , Nh ),
and G(Ni , Ej , Nh ) for all graph nodes and edges stored locally
on that partition. (3) Then, each partition sends back a message
to the graph directory service reporting its own local graph statistics. (4) Finally, the graph directory service aggregates the statistics
from the partitions to generate the global graph statistics.

4.3 Objective Function and Cost Model

N3 , we visit a total number of nodes S(N1 ) × F (N1 , E1 , N2 )
× G(N2 , E2 , N3 ), which corresponds to the number of qualified
nodes from N2 , which is S(N1 ) × F (N1 , E1 , N2 ), multiplied by
the number of nodes we need to visit to satisfy the predicate N3 ,
which is G(N2 , E2 , N3 ). The total number of visited nodes for
Q[1, i] is the sum of the number of visited nodes at each predicate
Nj , which is formally presented as:
Cost(Q[1, i]) =

T (N1 ) + S(N

 1 ) × G(N1 , E1 , N2 ) +



j−1
i

X
Y

S(N1 )
F (Nh , Eh , Nh+1 )
G(Nj , Ej , Nj+1 )


j=2
h=1


0

1≤i≤k

where Cost(Q[1, i]) is the cost of executing a subquery of Q in
the forward order from N1 to Ni , Cost(Q[k, i]) is the cost of executing a subquery of Q in the reverse order, from Nk to Ni , and
Join(Q, i) is the cost of joining the results of these two subqueries.
The trivial cases of i = k and i = 1 correspond to the query plans
with forward and reverse orders, respectively, where no join operation is involved, i.e., Join(Q, i) = 0.
Given the functions S(Ni ), T (Ni ), F (Ni , Ej , Nh ), and
G(Ni , Ej , Nh ) (described in Section 4.2), Cost(Q[1, i]),
Cost(Q[k, i]), and Join(Q, i) can be calculated as follows:
Cost(Q[1,i]). For the case when i=1, where Cost(Q[1, i]) is set
to zero, corresponding to the execution of Q in the reverse order, from Nk to N1 . For the case when i > 1, we first need
to visit T (N1 ) nodes to find the S(N1 ) nodes that satisfy the
first node predicate N1 . Then, for the second node N2 , the cost
is S(N1 ) × G(N1 , E1 , N2 ), which corresponds to the number
of qualified nodes from N1 multiplied by the number of nodes
we visit to satisfy the predicate N2 . Then, for the third node

i=1

Cost(Q[k,i]). Similar to Cost(Q[1,i], but Cost(Q[k,i]) estimates
the cost execution in the reverse order. For the non-trivial case
of i < k, we start by getting the number of visited nodes of
type Nk as T (Nk ). Then, we follow the nodes in the reverse order, e.g., for node Nk−1 , we visit S(Nk ) × G(Nk , Ek−1 , Nk−1 )
nodes. For node Nk−2 , we visit S(Nk ) × F (Nk , Ek−1 , Nk−1 ) ×
G(Nk−1 , Ek−2 , Nk−2 ), and so on. Formally:
Cost(Q[k, i]) =

T (Nk ) + S(N


 k ) × G(Nk , Ek−1 , Nk−1 ) +


k−2
k−1

X
Y

F (Nh+1 , Eh , Nh )
S(Nk )
G(Nj+1 , Ej , Nj )


j=i
h=j+1


0

i<k
i=k

Join(Q,i). A trivial case is when i=1 or i=k, where Join(Q, i) is
set to zero, indicating that the query plan corresponds to either the
reverse or forward order, respectively. For the non-trivial case (1 <
i < k), Join(Q, i) is computed as the Cartesian product of the two
sets involved in the join. The first set includes the estimated number
of nodes satisfying all the Q
predicates in the forward order from N1
to Ni , which is: S(N1 ) i−1
j=1 F (Nj , Ej , Nj+1 ). Similarly, the
second set includes the estimated number of nodes satisfying all
the predicates in the reverse order from Nk to Ni , which is S(Nk )
Qk
j=i+1 F (Nj , Ej−1 , Nj−1 ). Formally:
Join(Q, i) =

i−1
Y


 S(N1 )S(Nk )
F (Nj , Ej , Nj+1 )


Given the space of k query plans for any query Q with k node
predicates, it is the objective of the query optimizer to find the plan
with the lowest estimated cost in terms of the number of visited
nodes. Formally, Horton+ aims to minimize the objective function
Cost(Q[1, k]), represented as:
Cost(Q[1, k]) = min (Cost(Q[1, i]) + Cost(Q[k, i]) + Join(Q, i))

i>1

j=1






0

k
Y

F (Nj , Ej−1 , Nj−1 )

j=i+1

1<i<k
i = 1 OR i = k

4.4 Numerical Example
Figure 4-a gives examples of some collected statistics that are
enough for the query optimizer to decide on the best execution plan
for Q1 , Q2 , and Q3 of Table 1. For simplicity and ease of illustration, we assume that T (Ni ) and G(Ni , Ej , Nh ) are equivalent
to their counterparts S(Ni ) and F (Ni , Ej , Nh ). In our example,
S(‘Alice’) is set to one where there is only one node with id Alice.
S(Photo) is set to one million, indicating the number of nodes of
type Photo in the whole graph. F(‘Alice’, Friend, Person) is set
to 10 as Alice has only 10 friends of type Person. Statistics are
bi-directional as F(Person, Friend, ‘Alice’) is set to 150 as the average number of friends for each person. F(Person, Tag, Photo) and
F(Photo, Tag, Person) are set to 20 and 3 as the average number of
“photos per persons” and “persons tagged in a photo”, respectively.
Figure 4-b gives the cost of each query plan for Q1 , Q2 , Q3
based on the statistics of Figure 4-a. As an example, we describe
the optimal plan of Q3 as follows:
Q3 . Plan 4 (Join at the third node N3 ) has the lowest cost,
computed as the sum of three parts: (a) The cost of going

1923

T(Alice) = S(Alice)

1

T(Bob) = S(Bob)

1

T(Photo) = S(Photo)

1M

Q P
1 2 1 + 1×2 + 1×2×50=103

G,F(Alice, Friend, Person) 10
G,F(Alice, Tag, Photo)

50

G,F(Bob, Tag, Photo)

2

Cost

1 1 + 1×50 + 1×50×2=151
3 (1+1×50) +(1+1×2)+1×1×50×2=154
1 1 + 1×10 + 1×10×20=311
2 2 1M + ……..

G,F(Person, Friend, Alice) 150

3 1M + ……..

G,F(Person, Tag, Photo)

20

1 1+1×50+1×50×3+1×50×3×150=22701

G,F(Photo, Tag, Alice)

50

G,F(Photo, Tag, Bob)

2

G,F(Photo, Tag, Person)

3

(a) Statistics

3

2 1+1×10+1×10×20+1×10×20×50=10211
3 (1+1×50)+(1+1×10+1×10×20)+1×1×50×10×20=10262
4 (1+1×50+1×50×3)+(1+1×10)+1×1×50×3×10=1712

(b) Query Plan Cost for Q1, Q2, Q3 (optimal plans are shaded)

Figure 4: Example of statistics and cost of query plans.
in the forward order from N1 to N3 as Cost(1,3) = T (N1 ) +
S(N1 ) G(N1 , E1 , N2 ) + S(N1 ) F (N1 , E1 , N2 ) G(N2 , E2 , N3 ),
which is equivalent to: T(‘Alice’) + S(‘Alice’) G(‘Alice’,Tag,Photo)
+ S(‘Alice’) F(‘Alice’,Tag,Photo) G(Photo,Tag,Person) = 201.
(b) The cost of going in the reverse order, from N4 to N3 , as
Cost(4,3) = T (N4 ) + S(N4 ) G(N4 , E3 , N3 ), which is equivalent
to: T(‘Alice’) + S(‘Alice’) G(‘Alice’,Friend,Person) = 11. (c) The
cost of joining the results from the two previous parts as Join(3)
= S(N1 ) S(N3 ) F (N1 , E1 , N2 ) F (N2 , E2 , N3 ) F (N4 , E3 , N3 ),
which is equivalent to: S(‘Alice’) S(‘Alice’) F(‘Alice’,Tag,Photo)
F(Photo,Tag,Person) F(‘Alice’,Friend,Person) = 1500. Finally,
the total cost of this plan is the sum of these three costs as
201+11+1500 = 1712.

4.5 Query Optimization Algorithm
Algorithm 3 gives the pseudo code of the query optimizer. The
input to the algorithm is a query Q with k node predicates and k −1
edge predicates. The output is a join pointer on where to split the
query to achieve the best performance in terms of the number of
visited nodes. A join pointer value of k or 1 indicates that the best
query plan is the forward or reverse order, respectively, without any
join. A basic algorithm to find the lowest cost would compute the
cost of k different execution plans individually where each plan can
cost up to O(k2 ) number of addition and multiplication operations,
which gives a total cost of O(k3 ), where k is the number of nodes in
the input path query. Here, we present an algorithm with a total cost
of only O(k), which exploits common subcomputations to reduce
computational complexity.
The algorithm has three main parts: The first part (Lines 2 to 8)
incrementally fills four arrays, CostF, JoinF, CostR, and JoinR,
each of size k. An item i > 1 in any of the two arrays, CostF[i],
JoinF[i], maintains the cost of query evaluation in the forward order from N1 to Ni as CostF[i]= T (N1 ) + S(N1 ) G(N1 , E1 , N2 )
Pi
Qj−1
+ S(N1 )
j=2 (G(Nj , Ej , Nj+1 )
h=1 F (Nh , Eh , Nh+1 )),
and if thereQis a join at node i, the additional cost is JoinF[i]
= S(N1 ) ih=1 F (Nh , Eh , Nh+1 ). Similarly, an item i < k
in any of the two arrays, CostR[i], JoinR[i], maintains the cost of
query evaluation in the reverse order, from Nk to Ni , and part of
the join cost should we decide to join at node i. The second part
of the algorithm (Lines 9 to 14) computes the cost of forward and
reverse execution order of Q. As the costs are computed incrementally, the forward and reverse order costs are computed by adding
two terms: (1) The cost encountered to reach to node Nk−1 and
N2 , which is CostF[k-1] and CostR[2], respectively, and (2) The
number of nodes to visit to reach to Nk and N1 , which is computed as the number of paths we have, i.e., join cost, till Nk−1 and
N2 (JoinF[k-1] and JoinR[2]) multiplied by the number of output
nodes of each path to reach Nk and N1 (G(Nk−1 , Ek−1 , Nk ) and
G(N2 , E1 , N1 )), respectively. The minimum of the forward and

Algorithm 3 Query optimizer
1: Function QUERYOPTIMIZER(Q = h N1 , E1 , · · · , Ek−1 , Nk i)
2: JoinF[1] ← S(N1 ); CostF[1] ← T (N1 );
3: JoinR[k] ← S(Nk ); CostR[k] ← T (Nk );
4: for i = 2 to k − 1 do
5: JoinF[i] ← JoinF[i-1] × F (Ni−1 , Ei−1 , Ni )
6: CostF[i] ← CostF[i-1] + JoinF[i-1] × G(Ni−1 , Ei−1 , Ni )
7: JoinR[k-i+1]←JoinR[k-i+2] ×F (Nk−i+2 ,Ek−i+1 ,Nk−i+1 )
8: CostR[k-i+1]
←
CostR[k-i+2]
+
JoinR[k-i+2]
×G(Nk−i+2 ,Ek−i+1 ,Nk−i+1 )

9: ForwardCost ← CostF[k-1] + JoinF[k-1] × G(Nk−1 , Ek−1 , Nk )
10: ReverseCost ← CostR[2] + JoinR[2] × G(N2 , E1 , N1 )
11: if ForwardCost < ReverseCost then
12: MinCost ← ForwardCost ; JoinPointer ← k;
13: else
14: MinCost ← ReverseCost; JoinPointer ← 1;
15: for i = 2 to k − 1 do
16: TotalCost ← CostF[i] + CostR[i] + JoinF[i] × JoinR[i];
17: if TotalCost < MinCost then
18:
MinCost ← TotalCost; JoinPointer ← i;
19: Return JoinPointer;

reverse order costs is set as the current optimal plan with the join
pointer set as k and 1, respectively. The third part of the algorithm
(Lines 15 to 18) iterates over all nodes to compute the total cost of
joining at each node i as CostF[i]+CostR[i]+JoinF[i]×JoinR[i].
The value of i that corresponds to the minimum total cost is returned as a join pointer.
For Q1 in Table 1, JoinF={1,50,-}, CostF={1,51,-}, JoinR={,2,1}, CostR={-,3,1}, which results in ForwardCost = 51 + 50×2
= 151 and ReverseCost=3 + 2×50 = 103. The cost of joining at
node N2 = 51 + 3 + 50×2 = 154. Hence, join pointer is set to 1.

4.6 Query Plans with Recursive Joins
We have described a simple version of Horton+ query optimizer
that only considers splitting a given query into two subqueries.
However, the full version of Horton+ query optimizer considers recursive splits of subqueries and produces an optimal execution plan
based on the graph statistics. More specifically, it takes each subquery and recursively considers it for optimization, i.e., a subquery
can be evaluated in the forward or reverse order, or can be split
again to another two subqueries. The output of the recursive query
optimization is a query execution plan represented as a tree: Each
leaf node is a DFA representing a traversal of a subquery, and each
intermediate node is a join operator that joins the results of two
subqueries. Denoting RCost(Q(p, q)) as the cost of a query (or
subquery) using recursive optimization where p ≤ q, we present
the recursive formulation of the query optimizer:
RCost(Q(p, q)) =
min{SCost(Q(p, q)), SCost(Q(q, p)),
min {RCost(Q(p, i)) + RCost(Q(i, q)) + Join(Q(p, i), Q(i, q))}}

p<i<q

Here, SCost(Q(p, q)) and SCost(Q(q, p)) are the cost of evaluating the query Q(p, q) in the forward and reverse orders respectively, and Join(Q(p, i), Q(i, q)) is the cost of joining the subqueries Q(p, i) and Q(i, q). We present their formulation as follows.
SCost(Q[p, i]) =

T (Np ) + S(N
p ) × G(Np , Ep , Np+1 ) +




j−1
i

X
Y
G(Nj , Ej , Nj+1 )
F (Nh , Eh , Nh+1 )
S(Np )


j=p+1
h=1


0

1924

p<i
p=i

Join(Q(p, i), Q(i, q)) =

i−1
Y



F (Nj , Ej , Nj+1 )
 S(Np )S(Nq )
j=p






4.8 Optimization for Distributed Execution
q
Y

F (Nj , Ej−1 , Nj−1 )

j=i+1

0

1<i<k
i = 1 OR i = k

The cost of solving RCost(Q(1, k)) naively is Ω(k!), where k
is the number of query node predicates. We use a dynamic programming framework to solve the problem efficiently: it computes
and stores the optimal solutions for subqueries and uses those to
construct the optimal solution for a bigger problem. Dynamic programming effectively reduces the computational cost of obtaining
an optimal execution plan to O(k3 ), which is rather affordable as
query length is often not that large (even long queries have length
under 20 in most cases). Recursive splits can be mostly beneficial
when there are many selective nodes within a long query.

4.7 Closure Operators
Estimating the cost of a query with closure operators is complex
because a query optimizer does not know the number of recursive
steps a query would take to complete without actually running the
query. Thus, we develop a heuristic algorithm: (1) it optimizes the
non-closure part of a query by exploring different traverse orders
and join sequences using the techniques we presented earlier, and
(2) it further exploits the closure part of the query with different
number of recursive steps. This algorithm includes three phases.
While illustrating the phases, we use an example query N1 − E2 −
N2 − (E3 − N3 )∗ − E4 − N4 − E5 − N5 , and we assume that we
consider k number of recursive steps where k = 0, 1, 2, 3.
(1) For each recursive level k, we remove the closure operator
and expand the query according to the value of k. For example,
with k = 2, our example query has a form of Q(k = 2) = N1 −
E2 − N 2 − (E3 − N3 − E3′ − N3′ ) − E4 − N4 − E5 − N5 . For
the expanded query instance, we compute a good plan, denoted as
plank . The plan is computed similarly as in Section 4.6, however,
we exclude those plans that would perform a join operation inside
the recursive block, e.g., E3 − N3 − E3′ − N3′ is the recursive
block for Q(k=2). In other words, we treat the recursive block as an
atomic unit: we can execute it in the forward or reverse order but we
do not perform any join inside it. For the other parts of the query,
we still consider the join operator based on the cost estimates.
(2) If all the plans, plank for k = 0, 1, 2, 3, have equivalent execution sequence, we use this sequence to execute the recursive
query. Here we define two plans have equivalent execution sequence if they have join on the same node predicates and have the
same evaluation order for the same subqueries.
(3) However, if for different k values, their optimized plans have
different execution sequences, we estimate the cost of using each
“local optimal” execution sequence in the recursive query, and
we call this cost TotalCost(plank ) for a given plank . Among all
plank where k = 0, 1, 2, 3, we find the plan with the minimum
TotalCost(plank ), and use plank for the recursive query.
Example.
We give an example on how to compute
TotalCost(plank ). Suppose that k = 1 and plan1 is to execute the
query from left to right. The total cost of applying plan1 to the
recursive query is as follows:
T otalCost(plan1 )

=

SCost(Q(k = 0)) + SCost(Q(k = 1))
+SCost(Q(k = 2))
−2SCost(N1 − E2 − N2 ) .

We remove the additional sequential cost of processing the subquery Q(N1 − E2 − N2 ) because this cost is incurred only once in
the recursive execution.

Horton+ takes into account the communication cost when optimizing the input query. It distinguishes between: (1) local edge:
two nodes of a local edge reside on the same graph partition, and
(2) remote edge: two nodes of a remote edge are stored on different graph partitions. Horton+ can assign higher cost for accessing remote edges and lower cost for local edges. We achieve it
by incorporating the local/remote information into graph statistics
thus influencing our cost estimation and final decision. For example, as described in Section 4.2, G(Ni , Ej , Nh ) estimates the
number of nodes that need to be visited through the edge predicate Ej to satisfy the predicate Nh . We can revise its cost to
reflect the cost difference of remote and local edge accesses with
additional statistics P(Ni , Ej , Nh ) that defines the probability of
remote edges for edge predicates Ej . Supposing that the cost of
remote and local access is c : 1, the revised cost of G(Ni , Ej , Nh )
is G′ (Ni , Ej , Nh ) = c × G(Ni , Ej , Nh ) × P (Ni , Ej , Nh ) +
G(Ni , Ej , Nh ) × (1 − P (Ni , Ej , Nh )). By using the new statistics G′ (Ni , Ej , Nh ), we apply the same optimization procedure as
described earlier to decide efficient query plan considering communication costs.

5. EXPERIMENTAL EVALUATION
This section presents an experimental evaluation of Horton+ [38]. Our objective is to assess three aspects: system efficiency (query optimization), scalability (distributed execution),
and usability (declarative querying). We also provide a comparison with Giraph [19]; a graph processing system built on top of
Hadoop. Horton+ is implemented in C# in 30K lines of code.
The implementation includes the client interfaces, query language
parser and compiler, query optimizer, and distributed query processor. We use two graph types:
(1) We use a real graph from a software collaboration system,
called Codebook [4], which models software engineers and their
software artifacts, including source code, bug reports, projects, and
their relationships. The graph has 2,910,535 nodes, 13,612,406
edges, 8 node types, and 11 edge types. It is generated by crawling multiple data sources including source code repositories, and
employee directory and document databases. Each node and edge
is associated with a large number of attributes. The graph data is
represented natively in main memory as C# objects. The memory
footprint of the graph is around 12 GB, including object overheads
and the intermediate results while evaluating queries.
(2) We generate synthetic graphs with different sizes using the
RMAT graph generator [5] that produces scale-free graphs. The
graph schema including the types and attributes of nodes and edges
are set to mimic the real graph schema. The node and edge types
and attributes are generated using the Zipf distribution which models the popularity of attribute values, and we set the number of
edges to five times the number of nodes.
Workload. There is no standard benchmark for reachability
queries. We, therefore, characterize the queries from Codebook,
and classify them into four categories. For each category, we show
one of most frequent queries in Table 3. (1) Short queries have a
small number of predicates as in query Q1 . Since the query length
is short, the query optimizer searches a small space. (2) Selective
queries have one or more selective predicates. For example query
Q2 contains two id predicates ‘Dave’ and ‘Tim’. Due to the
high selectivity, selective queries traverse a small number of paths
to compute the final answer. (3) Report queries return a large result
set, such as query Q3 . Report queries are the most expensive to execute. (4) Closure queries require recursive graph traversal. Query

1925

Query
Short (Q1 )
Selective (Q2 )
Report (Q3 )

Closure (Q4 )

Query in Plain English
Find the person who committed checkin 400 and the WorkItemRevisions it modifies
Find Dave’s checkins that modified a WorkItem create by Tim
For each checkin, find the person (along with his manager) who
committer it as well as all the work items (along with their WebURLs that are modified by that checkin)
Retrieve all checkins that any employee in Dave organizational
chart (working under him) committed.

Query in Horton+
Person-Committer-Checkin{id=400}- Modifies-WorkItemRevision
Person{id=‘Dave’}-Committer-Checkin -Modifies-WorkItem
-CreatedBy-‘Tim’
Person-Manages-Person- Committer-Checkin-Modifies
-WorkItemRevision-Modifies-WorkItem -Links-WebURL
Person{id=‘Dave’} (-Manages-Person)∗-Checkin

0.5

1

2

Graph Size (* 106)

(a) Short Query.

4

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

Horton-opt
Horton-non

0.5

1

2

45
40
35
30
25
20
15
10
5
0

Horton-opt
Horton-non
Time (sec)

Horton-opt
Horton-non

Time (sec)

10
9
8
7
6
5
4
3
2
1
0

Time (sec)

Time (sec)

Table 3: The graph queries used in the experiments.

4

0.5

Graph Size (* 106)

1

2

16
14
12
10
8
6
4
2
0

Horton-non
Horton-opt-norec
Horton-opt

4

0.5

1

Graph Size (* 106)

(b) Selective Query.

2

4

Graph Size (* 106)

(c) Report Query.

(d) Report Query (recursive).

Figure 5: Impact of query optimization on the query execution time (using the synthetic graphs) on a single server.

5.1 Efficiency (Query Optimization)
In this section, we study the benefits of the query optimizer on
reducing query execution time for a graph deployed first on a single server, and then on multiple servers. We run two versions of
Horton+: Horton-opt is the full version of Horton+ including
its query optimizer. Horton-non represents Horton+ with the
optimizer turned off, i.e., queries are executed in the forward order.
The experimental results demonstrate that optimization reduces the
latency of many queries by a factor of 5 — 15 times. Moreover, the
larger the graph size, the higher the optimization benefits.

5.1.1 Deployment on a Single Graph Server
Short query Q1 . Figure 5(a) shows the performance of executing
query Q1 on synthetic graphs. The X-axis is the size of the synthetic graph (0.5, 1, 2, and 4 million nodes), and the Y-axis is execution time. Horton-opt outperforms Horton-non for all graph
sizes because Horton-opt splits Q1 at the middle selective node
predicate Checkin{id=400} and processes two subqueries. The
query execution time in both Horton-non and Horton-opt
becomes higher with increasing graph size because the query execution engine visits more graph nodes when the graph size becomes
larger. The benefits of optimization become more significant with
the increase in graph size.
Selective query Q2 . Figure 5(b) shows the results of running query
Q2 . Horton-non and Horton-opt give the same performance,
as they both execute the forward execution plan. The results also
show that the overhead of running the query optimizer is almost

45
40
35
30
25
20
15
10
5
0

Horton-opt
Horton-non

Time (sec)

Time (sec)

Q4 is a closure query and it retrieves the management hierarchy of
a person using Kleene star.
Performance Metrics. Our main performance metric is the query
execution time. We also examine the computation and communication costs to process the queries.
Experimental Environment. All experiments are run on a cluster
of 16 graph partition servers plus two servers as the coordinator and
client. Each server has an Intel QuadCore 2.9 GHz CPU, 16 GB
RAM, and runs Windows Server 2008. The servers are connected
by a Gigabit Ethernet switch.

1

2

3

4

5

6

7

8

Number of Servers

(a) Short Query.

9 10

1100
1000
900
800
700
600
500
400
300
200
100
0

Horton-opt
Horton-non

1

2

3

4

5

6

7

8

9 10

Number of Servers

(b) Report Query.

Figure 6: Impact of optimization on execution time (using the
real graph), number of servers varies from 1 to 10.

negligible, compared with the total query execution time. The optimized and non-optimized plans for Q4 are the same with equal
execution time. We omit Q4 as it is similar to Figure 5(b).
Report query Q3 . Figure 5(c) shows the performance of query
Q3 . The optimizer provides lower execution time, and the benefits
increase with the graph size.
Recursive split for query Q′3 .
Figure 5(d) shows interesting results:
We change Q3 into Q′3 by adding
two predicates to the third (CheckIn{id=390}) and
fourth (WorkItemRevision{id=610}) node predicates.
Horton-opt exploits these predicates to reduce the execution
time by a factor of 13. Horton-opt chooses a query plan
that includes recursive splits at two nodes. First, Horton-opt
splits the query at the third node (CheckIn{id=390}) into
two subqueries. Next for the second subquery, Horton-opt
performs a recursive split, where this second subquery is split
at the fourth node (WorkItemRevision{id=610}). The
recursive split is the main reason behind the impressive performance of Horton-opt for Q′3 . To measure the benefits
of recursive splits, we run a third version of Horton+, termed
Horton-opt-norec, which uses the same optimizer but
without recursive splits; thus the optimized query plan contains
at most one split. Horton-opt-norec produces a plan for
Q′3 with a single split at the third node. Figure 5(d) shows that

1926

1

2

3

4 5 6 7 8
Number of Severs

(a) Short Query.

9 10

80
70
60
50
40
30
20
10
0

1,200
Computation Cost
Communication Cost

Computation Cost
Communication Cost

800
600
400
200

1

2

3 4 5 7 8
Number of Severs

9

0

10

1

(b) Selective Query.

100
90
80
70
60
50
40
30
20
10
0

Computation Cost
Communication Cost

Time (sec)

1,000
Time (sec)

Computation Cost
Communication Cost

Time (sec)

Time (sec)

45
40
35
30
25
20
15
10
5
0

2

3

4 5 6 7 8
Number of Severs

9 10

(c) Report Query.

1

2

3

4 5 6 7 8
Number of Severs

9 10

(d) Closure Query.

Figure 7: Query execution time (using the real graph) while varying number of partition servers from 1 to 10.
Horton-opt yields up to 7 times better performance than that of
Horton-opt-norec, which shows the benefits of employing
recursive splits in the query optimizer.

5.1.2 Deployment on Multiple Graph Servers
Figures 6(a) and 6(b) depict the query execution time of
Horton-opt and Horton-non for queries Q1 and Q3 executed
over the real graph, while varying the number of servers from 1 to
10 over the X-axis. We omit the results of Q2 and Q4 since the
optimized query plans are the same as the non-optimized plans.
For query Q1 , Horton-opt consistently achieves from 12 to
14 times better performance compared with Horton-non because in Horton-opt the optimizer splits Q1 at the middle node
predicate CheckIn{id=400} and executes the two subqueries.
The optimized plan is substantially more efficient than the forward
execution of Q1 .
For query Q3 , Horton-opt consistently outperforms
Horton-non by a factor of 8 to 10 times as Horton-Opt uses
an optimized plan that splits the query at the middle node predicate
CheckIn. These results show that query optimizer produces good
plans, suitable for deployments both on a single server and on
multiple servers.

5.2 Scalability (Distributed Processing)
We study the performance of distributed query processing in two
cases. First, we use the real graph which fits in a single server,
and study the distributed execution overhead with the number of
servers. Second, we use a large synthetic graph that does not fit in
a single server to show query processing times.
(1) Real Graph. Since the real graph fits in the main memory of single server, this constitutes a challenging environment for
evaluating a distributed system: (1) There is no benefit from the aggregated main memories of the servers, and (2) the communication
and synchronization inefficiencies are emphasized. A single server
could be more efficient as it incurs no messaging overhead.
We vary the number of partition servers from 1 to 10. Figures 7(a) to 7(d) show the execution time of queries Q1 , Q2 , Q3 ,
and Q4 . The query execution time has two components: computation time and communication time. The computation time is time
used for local computations at the servers, and the communication
time is the time spent in message passing among the servers. The
performance of Q1 , Q2 , Q3 , and Q4 improves with increasing the
number of servers. The improvement comes from executing the
query in parallel on more servers, reducing the parallel computation time component as the number of servers increases as depicted
in the figures. More graph partitions lead to a reduction in the number nodes and edges per partition, further reducing the amount of
local computation per server.
However, the performance gain shows diminishing returns because the communication time increases with the number of
servers. The communication time is zero for a single server, and

Query

Total execution

Communication

Computation

Short Query(Q1 )

47.588 sec

0.723 sec

46.865 sec

Selective Query(Q2 )

6.294 sec

0.693 sec

5.601 sec

Table 4: Execution time for 1024 million nodes, 5120 million
edges synthetic graph deployed on 16 partition servers.
it increases as more servers are added because more messages
are exchanged among the graph servers during query execution.
The communication cost is dominated by the messages exchanged
among the graph servers during the global communication step performed by the traverse operator. These results show that the system
is efficient, and query execution time improves with the number of
servers even if the graph fits in the memory of one server.
(2) Synthetic Graph. We use a synthetic graph with 1024 million nodes and 5120 million edges with an aggregate memory footprint of 145 GB, partitioned on a cluster of 16 servers. This experiment shows that Horton+ processes queries over graphs that do not
fit on a single server, and it exploits multiple servers to execute a
single query in parallel.
Table 4 shows the execution time of queries Q1 and Q2 . The execution time of Q1 is approximately 48 seconds. Q1 execution plan
splits the query into two subqueries at the (CheckIn{id=400})
node predicate. Both subqueries are executed separately and the
their outputs are joined to form the final answer. Even though the
graph is partitioned on 16 servers, only 1.5% of the query execution
time is spent in communication and the remaining 98.5% is spent
for local computation. This is in contrast to the findings we observe
in Figures 7(a) and 7(c), where the communication cost is dominant
for only 10 servers. The reason is that with larger graph sizes, partitioning the graph among 16 servers provides enough work for each
server to parallelize query processing.
Query Q2 shows similar benefits to Q1 , as the majority of the
time is spent in computations rather than in communication. The
computation cost comes mainly from traversing the graph elements
at the graph partition servers during the local computation steps
performed by the traverse operator. These results show that Horton+ efficiently parallelizes execution over a cluster of servers.

5.3 Usability (Declarative Queries)
Writing a procedural program takes more effort compared with
expressing an equivalent declarative query, particularly for novice
users. The procedural program is harder to write, debug, and maintain. Moreover, expressing a query directly into a procedural program may not lead to an efficient execution, and it is well-known
that procedural programs are hard to optimize automatically.
We support this argument with anecdotal evidence: Figure 9
depicts query Q3 (from Table 3) in a procedural language (i.e.,
Java) in Giraph. We make two observations: (1) The procedural program is longer and more complex. (2) Comparable graph
systems such as Giraph [19], Pregel [30], and Trinity [39] require

1927

0.5

1

2

Graph Size (* 106)

(a) Short Query.

4

Non-Optimized
Optimized

0.5

1

2

4

350
300
250
200
150
100
50
0

Non-Optimized
Optimized
Time (sec)

50
45
40
35
30
25
20
15
10
5

Time (sec)

Non-Optimized
Optimized
Time (sec)

Time (sec)

11
10
9
8
7
6
5
4
3
2
1

0.5

Graph Size (* 106)

1

2

4

Graph Size (* 106)

(b) Selective Query.

(c) Report Query.

140
130
120
110
100
90
80
70
60
50
40

Non-Optimized
Optimized

0.5

1

2

4

Graph Size (* 106)

(d) Closure Query.

Figure 8: Impact of optimization on execution time in Giraph (using the pseudorandom synthetic graph) on a 10 servers cluster.
public void compute(Iterable<Text> m) throws IOException{
Text message = m.next().get(); int st = getSuperstep();
if (st == 0 && getValue().get() == "Person") {
for (Edge<LongWritable, Text> edge : getEdges())
if (edge.getValue().get() == "Manages")
sendMessage(edge.getTargetVertexId(),formatMsg(m));
} else if (st == 1 && getValue().get() == "Person") {
for (Edge<LongWritable, Text> edge : getEdges())
if (edge.getValue().get() == "Committer")
sendMessage(edge.getTargetVertexId(),formatMsg(m));
} else if (st == 2 && getValue().get() == "Checkin") {
for (Edge<LongWritable, Text> edge : getEdges())
if (edge.getValue().get() == "Modifies")
sendMessage(edge.getTargetVertexId(),formatMsg(m));
} else if (st == 3
&& getValue().get() == "WorkItemRevision") {
for (Edge<LongWritable, Text> edge : getEdges())
if (edge.getValue().get() == "Modifies")
sendMessage(edge.getTargetVertexId(),formatMsg(m));
} else if (st == 4 && getValue().get() == "WorkItem") {
for (Edge<LongWritable, Text> edge : getEdges())
if (edge.getValue().get() == "Links")
sendMessage(edge.getTargetVertexId(),formatMsg(m));
} else if (st == 5 && getValue().get() == "WebURL") {
}
voteToHalt();
}

Figure 9: Giraph Program (Java pseudo code) for Query Q3 .
programmers to write procedural programs with explicit communication messages for queries like Q3 . The procedural program for
Q3 is likely to have a high execution time, similar to the time of the
non-optimized execution plan in Figure 5(c), which is almost an
order of magnitude higher than the optimized plan. Furthermore,
writing an efficient program requires the user to be closely familiar with both the underlying graph and the execution engine; for
example it is challenging for the programmer to split a query into
multiple subqueries at the right node predicates and to write code
to join the outputs in the right order. Horton+, on the other hand,
allows users to declaratively express queries, and optimizes them.

5.4 Comparing Horton+ with Giraph
This section compares Horton+ with Apache Giraph, which is a
large-scale graph processing system built on-top of Hadoop. Both
Horton+ and Giraph store the graph in the main memory of a cluster
of servers. Also, both employ the bulk synchronous parallel execution paradigm to process queries in parallel over the distributed
graph. By default, Giraph loads the graph from the Hadoop HDFS
file system, and then deploys it to the cluster for each submitted
graph processing job. Giraph users write queries as procedural
Java programs which are executed directly without optimization,
whereas Horton+ maintains graph statistics to optimize the queries.
We write a procedural program for each query in Table 3. For example, Figure 9 shows part of the Java code equivalent to Q3 . The
actual code is longer, and it contains additional lines for reading
graph data from and writing the results to Hadoop HDFS. In addi-

tion, we use the Horton+ optimizer to generate an optimized query
plan and we write an optimized program in Giraph emulating the
optimized plan.
We study the execution time of these procedural programs on
Giraph. Our objective is not to compare the performance of Giraph
with Horton+ directly because they use different software stacks of
managed and unmanaged components (such as JVM and CLR/.Net
framework) and different communication primitives and libraries.
Instead, our objective is to (1) put Horton+ performance in perspective, and (2) show that Horton+ optimization strategies can also be
used to guide writing better procedural programs for Giraph.
We deploy graphs of sizes 500K, 1M, 2M, and 4M nodes
(number of edges is five times the number of nodes) generated using the the pseudorandom synthetic graph benchmark provided by Giraph, over 10 servers in a Hadoop cluster running
hadoop-0.20.203.0 with 30 mappers. Figures 8(a) to 8(d)
show the performance of the procedural programs. We compare
the plain Giraph Java programs (labelled Non-Optimized) with
the Java programs written following the Horton+ optimized plans
(labelled Optimized). Horton+ produces optimized plans different from prior plans because the graph statistics of the pseudorandom synthetic graph are quite different from the statistics of
the prior real and synthetic graphs. The Optimized programs
outperforms the Non-Optimized for Q1 , Q2 , Q3 because the
Optimized programs traverse fewer graph nodes and edges, incurring less computation and communication overheads. For Q4 ,
both Optimized and Non-optimized achieve the same performance as the optimized plan is equivalent to the forward plan.
These results show that the Horton+ optimization techniques are
general, and its optimizer can provide guidance in writing more
efficient procedural programs for Giraph.

6. RELATED WORK
Graph Query Languages. Graph query languages are based on
either regular expressions [10, 11, 22], SQL-like languages [3, 36,
40], or a procedural languages [23]. Horton+ uses a formal declarative query language to express reachability queries, and more importantly it provides an efficient distributed execution engine to execute its declarative queries.
Graph Processing Algorithms. In-memory graph processing algorithms include computationally-intensive algorithms, e.g., graph
mining [32, 42], dense subgraphs [18, 35], and pattern matching [14], where the emphasis is on having reasonable latency for
problems that are likely to be NP-complete. Online graph algorithms support simple graph queries, e.g., shortest path queries [17,
47], reachability queries [8, 15, 24], smaller versions of complex
queries, e.g., pattern matching queries [15, 50], or approximate
queries on a streaming environment [2, 49]. Horton+ focuses on
processing reachability queries over a partitioned graph, and provides a query language, optimizer and distributed execution engine.

1928

Distributed Graph Query Processing. Research in distributed
graph query processing has focused on either leveraging the
MapReduce paradigm [12] to support graph operations [7, 9, 25]
or building distributed computation models for graph queries, e.g.,
Pregel [30], Trinity [39], GraphChi [20], and PowerGraph [29].
Horton+ is different because (1) it supports a declarative query language and (2) it optimizes query execution. In contrast, systems
like Pregel provide an API for developers to write procedural programs, which are harder to write, debug, maintain and optimize.
Graph Query Optimization. Existing graph query optimization
techniques focus on either building index structures [48, 50], or on
developing selectivity estimation modules for certain graph queries
[33, 49]. These techniques are complementary to Horton+, and it
can employ such techniques. Several optimization techniques on
tree structures, such as for XML documents, are not applicable to
graphs, which contain cycles.
Graph Libraries. Graph libraries provide various graph algorithms within a single framework [6, 21, 26, 28, 41]. Horton+
provides a query language rather than a set of graph algorithms.
It consists of multiple components including compiler, optimizer,
and distributed query processor.

7.

CONCLUSION

This paper presents the design, implementation and evaluation of
Horton+, a distributed system for processing reachability queries
on a partitioned attributed multi-graph. The system has a declarative query language, distributed query processor, and query optimizer. The query language expresses reachability queries and supports closures and predicates on the attributes of nodes and edges.
The distributed query processor executes a query plan using three
algebraic graph operators, select, traverse and join to find paths that
match the user query. The query optimizer employs a cost model
and selectivity estimation techniques to rewrite the query plan. Experiments on real and synthetic graphs on a cluster of servers show
system the scalability and efficiency.

8.

REFERENCES

[1] A. Abou-Rjeili and G. Karypis. Multilevel algorithms for partitioning
power-law graphs. In IPDPS, 2006.
[2] C. C. Aggarwal, Y. Li, P. S. Yu, and R. Jin. On Dense Pattern Mining in Graph
Streams. PVLDB, 3(1):975–984, 2010.
[3] G. O. Arocena and A. O. Mendelzon. WebOQL: Restructuring Documents,
Databases, and Webs. In ICDE, 1998.
[4] A. Begel, K. Y. Phang, and T. Zimmermann. Codebook: Discovering and
Exploiting Relationships in Software Repositories. In ICSE, 2010.
[5] D. Chakrabarti, Y. Zhan, and C. Faloutsos. R-MAT: A Recursive Model for
Graph Mining. In SDM, Apr. 2004.
[6] A. Chan, F. K. H. A. Dehne, and R. Taylor. CGMGRAPH/CGMLIB:
Implementing and Testing CGM Graph Algorithms on PC Clusters and Shared
Memory Machines. IJHPCA, 19(1):81–97, 2005.
[7] R. Chen, X. Weng, B. He, and M. Yang. Large Graph Processing in the Cloud
(Demo). In SIGMOD, 2010.
[8] Y. Chen and Y. Chen. An Efficient Algorithm for Answering Graph
Reachability Queries. In ICDE, 2008.
[9] J. Cohen. Graph Twiddling in a MapReduce World. Computing in Science and
Engineering, 11(4):29–41, 2009.
[10] M. P. Consens and A. O. Mendelzon. GraphLog: a Visual Formalism for Real
Life Recursion. In PODS, 1990.
[11] I. F. Cruz, A. O. Mendelzon, and P. T. Wood. A Graphical Query Language
Supporting Recursion. In SIGMOD, 1987.
[12] J. Dean and S. Ghemawat. MapReduce: Simplified Data Processing on Large
Clusters. In OSDI, 2004.
[13] M. Faloutsos, P. Faloutsos, and C. Faloutsos. On Power-law Relationships of
the Internet Topology. In ACM SIGCOMM, 1999.
[14] W. Fan, J. Li, J. Luo, Z. Tan, X. Wang, and Y. Wu. Incremental Graph Pattern
Matching. In SIGMOD, 2011.

[15] W. Fan, J. Li, S. Ma, N. Tang, and Y. Wu. Adding Regular Expressions to
Graph Reachability and Pattern Queries. In ICDE, 2011.
[16] Facebook Graph Search.
https://www.facebook.com/about/graphsearch.
[17] J. Gao, R. Jin, J. Zhou, J. X. Yu, X. Jiang, and T. Wang. Relational Approach
for Shortest Path Discovery over Large Graphs. PVLDB, 5(4):358–369, 2011.
[18] D. Gibson, R. Kumar, and A. Tomkins. Discovering Large Dense Subgraphs in
Massive Graphs. In VLDB, 2005.
[19] Giraph. http://incubator.apache.org/giraph/.
[20] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin. GraphChi:
Large-Scale Graph Computation on Just a PC. In OSDI, 2012.
[21] D. Gregor and A. Lumsdaine. The Parallel BGL: A Generic Library for
Distributed Graph Computations. In POOSC, 2005.
[22] R. H. Güting. GraphDB: Modeling and Querying Graphs in Databases. In
VLDB, 1994.
[23] H. He and A. K. Singh. Graphs-at-a-time: Query Language and Access
Methods for Graph Databases. In SIGMOD, 2008.
[24] R. Jin, H. Hong, H. Wang, N. Ruan, and Y. Xiang. Computing Label-constraint
Reachability in Graph Databases. In SIGMOD, 2010.
[25] U. Kang, D. H. Chau, and C. Faloutsos. Mining Large Graphs: Algorithms,
Inference, and Discoveries. In ICDE, 2011.
[26] U. Kang, H. Tong, J. Sun, C.-Y. Lin, and C. Faloutsos. GBASE: A Scalable and
General Graph Management System. In KDD, 2011.
[27] J. M. Kleinberg, R. Kumar, P. Raghavan, S. Rajagopalan, and A. Tomkins. The
Web as a Graph: Measurements, Models, and Methods. In Conference on
Computing and Combinatorics, COCOON, 1999.
[28] D. E. Knuth. The Stanford GraphBase: A Platform for Combinatorial
Computing. Addison-Wesley, 1993.
[29] A. Kyrola, G. Blelloch, and C. Guestrin. PowerGraph: Distributed
Graph-Parallel Computation on Natural Graphs. In OSDI, 2012.
[30] G. Malewicz, M. H. Austern, A. J. C. Bik, J. C. Dehnert, I. Horn, N. Leiser, and
G. Czajkowski. Pregel: A System for Large-Scale Graph Processing. In
SIGMOD, 2010.
[31] M. E. J. Newman, D. J. Watts, and S. H. Strogatz. Random Graph Models of
Social Networks. Proceedings of the National Academy of Sciences of the USA,
99(1):2566–2572, Feb. 2002.
[32] J. Pei, D. Jiang, and A. Zhang. On Mining Cross-graph Quasi-cliques. In KDD,
2005.
[33] Y. Peng, B. Choi, and J. Xu. Selectivity Estimation of Twig Queries on Cyclic
Graphs. In ICDE, 2011.
[34] V. Prabhakaran, M. Wu, X. Weng, F. McSherry, L. Zhou, and M. Haridasan.
Managing large graphs on multi-cores with graph awareness. In USENIX ATC,
2012.
[35] S. Ranu and A. K. Singh. GraphSig: A Scalable Approach to Mining
Significant Subgraphs in Large Graph Databases. In ICDE, 2009.
[36] S. Sakr, S. Elnikety, and Y. He. G-SPARQL: A Hybrid Engine for Querying
Large Attributed Graphs. In CIKM, 2012.
[37] J. Sankaranarayanan and H. Samet. Distance Oracles for Spatial Networks. In
ICDE, 2009.
[38] M. Sarwat, S. Elnikety, Y. He, and G. Kliot. Horton: Online Query Execution
Engine for Large Distributed Graphs (Demo). In ICDE, 2012.
[39] B. Shao, H. Wang, and Y. Li. Trinity: A Distributed Graph Engine on a
Memory Cloud. In SIGMOD, 2013.
[40] L. Sheng, Z. M. Özsoyoglu, and G. Özsoyoglu. A Graph Query Language and
Its Query Processing. In ICDE, 1999.
[41] J. G. Siek, L.-Q. Lee, and A. Lumsdaine. The Boost Graph Library - User
Guide and Reference Manual. C++ in-depth series. Pearson / Prentice Hall,
2002.
[42] A. Silva, W. M. Jr., and M. J. Zaki. Mining Attribute-structure Correlated
Patterns in Large Attributed Graphs. PVLDB, 5(5):466–477, 2012.
[43] SPARQL. http://www.w3.org/TR/rdf-sparql-query/.
[44] G. Szabo and G. Fath. Evolutionary Games on Graphs. Physics Reports,
446(4-6):97–216, July 2007.
[45] I. Tatarinov, S. D. Viglas, K. Beyer, J. Shanmugasundaram, E. Shekita, and
C. Zhang. Storing and querying ordered XML using a relational database
system. In SIGMOD, 2002.
[46] L. G. Valiant. A bridging Model for Parallel Computation. Communincations of
ACM, 33(8):103–111, 1990.
[47] F. Wei. TEDI: Efficient Shortest Path Query Answering on Graphs. In
SIGMOD, 2010.
[48] X. Yan, P. S. Yu, and J. Han. Graph Indexing: A Frequent Structure-based
Approach. In SIGMOD, 2004.
[49] P. Zhao, C. C. Aggarwal, and M. Wang. gSketch: On Query Estimation in
Graph Streams. PVLDB, 5(3):193–204, 2011.
[50] P. Zhao and J. Han. On Graph Query Optimization in Large Networks. PVLDB,
3(1):340–351, 2010.

1929

The First ACM SIGSPATIAL PhD Symposium 2014
Ugur Demiryurek1 , Mohamed Sarwat2
Department of Computer Science, University of Southern California, USA
2
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, USA
1
demiryur@usc.edu,2msarwat@asu.edu
1

1 Summary
The ACM SIGSPATIAL Ph.D. Symposium is a forum where Ph.D. students present, discuss, and receive feedback on their research in a constructive atmosphere. The symposium will be attended by professors, researchers
and practitioners in the ACM SIGSPATIAL community, who will participate actively and contribute to the discussions. The workshop is co-located with ACM SIGSPATIAL GIS 2014 The ACM SIGSPATIAL 2014 PhD
Symposium provides an opportunity for doctoral students to explore and develop their research interests in the
broad areas addressed by the ACM SIGSPATIAL community. We invite PhD students to submit a summary of
their dissertation work to share their work with students in a similar situation as well as senior researchers in
the field. We have two tracks for submission. The Junior PhD Track is for students who are in early stages of
their doctoral studies. The submission should provide a clear problem definition, explain why it is important,
survey related work, and summarize the new solutions that are pursued. The Senior PhD Track is for students
who are close to completion (expected to graduate by 2014/2015). The submissions focused on describing the
contribution they made in their doctoral dissertation. The strongest candidates are those who have a clear topic
and research approach, and have made some progress, but who are not so far along that they can no longer make
changes.

2 Program
This year, we accepted five papers to the PhD Symposium. The list of papers is as follows:
• Partitions to Improve Spatial Reasoning (Author: Matthew P. Dube – Supervised by: Max J. Egenhofer)
• Novel Clustering and Analysis Techniques for Mining Spatio-temporal Data (Author: Yongli Zhang –
Supervised by: Christoph F. Eick)
• Spatial Sensor Data Processing and Analysis for Mobile Media Applications (Author: Guanfeng Wang –
Supervised by: Roger Zimmermann)
• Towards Resource Route Queries with Reappearance (Author: Gregor Joss – Supervised by: Matthias
Schubert)
• SimMatching - Adaptable Road Network Matching for Efficient and Scalable Spatial Data Integration
(Author: Michael Schfers – Supervised by: Udo W. Lipeck)
Authors got the chance to present their papers and get feedback on their dissertation topic from experienced
researchers (from both academia and industry) in Geographic Information Systems and Spatial Data Analytics.

72

3 Keynote
The PhD symposium featured a Keynote speech by professor Ouri Wolfson (Richard and Loan Hill Professor
of Computer Science at the University of Illinois at Chicago) on ”What to Research in Spatial Information and
Hot to Do So”. This talk is divided into two parts the What and the How. The What part describes the research
directions, based on Dr. Wolfson’s perspective, that are most promising in the area of spatial information. These
involve abstractions of dynamic data about space and time to guide users in conducting everyday activities. In the
How part Dr. Wolfoson describes the character and attitude traits that I view as essential to conduct world-class
research. These involve problem selection, collaboration and inspiration.

73

Sindbad: A Location-based Social Networking System
Mohamed Sarwat⋆ , Jie Bao⋆ , Ahmed Eldawy⋆ , Justin J. Levandoski† , Amr Magdy⋆ , and
Mohamed F. Mokbel⋆
⋆

⋆

Dept. of Computer Science and Engineering, University of Minnesota, Minneapolis, MN 55455
†
Microsoft Research, Redmond, WA 98052-6399

{sarwat,baojie,eldawy,amr,mokbel}@cs.umn.edu, † justin.levandoski@microsoft.com

ABSTRACT

Foursquare [7]). These existing location-based social networks are
strictly built for mobile devices, and only allow users to receive
messages about the whereabouts of their friends (e.g., Foursquare
“check-ins” that give an alert that “your friend Alice has checked
in at restaurant A”). Sindbad, on the other hand, takes a broader
approach that marries functionality of traditional social-networks
with location-based social scenarios (e.g., friend news posts with
spatial extents, location-influenced recommendations) [3]. Thus,
Sindbad is appropriate for both traditional social networking scenarios (e.g., desktop-based applications) as well as location-based
scenarios (e.g., mobile-based applications).
Users of Sindbad can select their friend list as well as getting
listed as friends to other users in a same way like traditional social
networking systems. In addition, users can post (spatial) messages
and/or rate (spatial) objects (e.g., restaurants), which will be seen
by their friends. Once a user logs on to Sindbad, the user will see
an incoming location-aware news feed posted by the user friends.
Sindbad has a location-aware news feed module [2], named GeoFeed. For any user, GeoFeed efficiently retrieves the relevant messages from her friends based on the user location and the message
spatial extents. Geofeed minimizes the total system overhead for
delivering the location-aware news feed to the user while guaranteeing a certain response time for each user to obtain the requested
location-aware news feed.
Sindbad users can receive location-aware recommendation
about spatial items, e.g., restaurants, or non-spatial items, e.g.,
movies. To this end, Sindbad is equipped with a location-aware
recommender module, called LARS [8]. For any user, LARS efficiently suggests (spatial) items based on users locations, items locations, and previous ratings by user friends. Sindbad produces recommendations by employing a user partitioning and travel distance
penalty techniques in order to deliver relevant recommendations to
its users. In addition to both GeoFeed and LARS, Sindbad adopts
a location-aware ranking module that efficiently selects the top-k
relevant objects produced from either the location-aware news feed
or the recommender system modules.
A major part of Sindbad is built inside PostgreSQL; an opensource DBMS. Hence, Sindbad: (a) takes advantage of the scalability provided by the DBMS, and (b) is able to employ early pruning techniques inside the DBMS engine, which yields an efficient
performance for the news feed, recommendation, and ranking functionalities. Moreover, Sindbad provides an RESTful [6] web API
so that it would allow a wide variety of applications to easily communicate with Sindbad and make use of its unique features. The
system is demonstrated using both a web and smart phone (i.e., Android) applications that we built on top of Sindbad particularly for
demonstration purpose. Moreover, the system internals are demon-

This demo presents Sindbad; a location-based social networking
system. Sindbad supports three new services beyond traditional
social networking services, namely, location-aware news feed,
location-aware recommender, and location-aware ranking. These
new services not only consider social relevance for its users, but
they also consider spatial relevance. Since location-aware social
networking systems have to deal with large number of users, large
number of messages, and user mobility, efficiency and scalability
are important issues. To this end, Sindbad encapsulates its three
main services inside the query processing engine of PostgreSQL.
Usage and internal functionality of Sindbad, implemented with
PostgreSQL and Google Maps API, are demonstrated through user
(i.e., web/phone) and system analyzer GUI interfaces, respectively.

Categories and Subject Descriptors
H.2.8 [Database Applications]: Spatial databases and GIS

General Terms
Design, Management, Performance, Algorithms

Keywords
Social Networking, Recommender Systems, News Feed, Spatial
Rating, Spatial Message

1.

INTRODUCTION

This demo presents Sindbad: a location-based social networking system built with an open-source database management system. Sindbad distinguishes itself from existing social networking
systems (e.g., Facebook [4] and Twitter [10]) as it injects locationawareness within every aspect of social interaction and functionality in the system. For example, posted messages in Sindbad have
inherent spatial extents (i.e., spatial location and spatial range) and
users receive friend news feed based on their locations and the
spatial extents of messages posted by their friends. The functionality of Sindbad is fundamentally different from current incarnations of location-based social networks (e.g., Facebook Places [5],

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGMOD ’12, May 20–24, 2012, Scottsdale, Arizona, USA.
Copyright 2012 ACM 978-1-4503-1247-9/12/05 ...$10.00.

649

strated through an administrator-like interface that shows the different system parameters as well as general statistics about Sindbad.
The rest of the paper is organized as follows: Section 2 gives the
system architecture for Sindbad. The main three components of
Sindbad, namely, location-aware news feed (GeoFeed), locationaware recommender system (LARS), and location-aware ranking,
are discussed in Sections 3, 4, and 5, respectively. Finally, the
demonstration scenario is described in Section 6.

a book, (2) Non-spatial ratings for spatial items, represented as a
four-tuple (user, rating, item, itemLocation); for example, a user
with unknown location rating a restaurant with an inherent location, and (3) Spatial ratings for spatial items, represented as a fivetuple (user, userLocation, rating, item, itemLocation); for example,
a user at his/her office rating a restaurant with an inherent location.
Location-aware news feed queries. Once a Sindbad user logs
on to the system, a location-aware news feed query is fired to retrieve the relevant news feed, i.e., messages posted by the user’s
friends that have spatial extents covering the location of the requesting user. Details of the execution of the location-aware news feed
query will be discussed in Section 3. The output of the locationaware news feed module (GeoFeed) will be processed further by
the location-aware ranking module to get only the top-k news feed
based on the spatial and social relevance, which will be returned to
the user as the requested news feed. Details of the location-aware
ranking module will be described in Section 5.
Location-aware recommendation queries. Sindbad users can request recommendations of either spatial items (e.g., restaurants,
stores) or non-spatial items (e.g., movies) by explicitly issuing a
location-aware recommendation query. The location-aware recommender module (LARS) suggests a set of items based on: (a) the
user location (if available), (b) the item location (if available), and
(c) ratings previously posted by either the user or the user’s friends.
Details of LARS will be discussed in Section 4. Similar to locationaware news feed queries, the output of LARS goes through the
location-aware ranking module to select only the top-k items based
on both spatial and social relevance.

2.

3. LOCATION-AWARE NEWS FEED

, ,,

!"#$#%&
'())#*()&

, ,-+),9>$%$?,@+..$7+,

!"#$#%&
3(%(4#05(&

!"#$%"&'()$*+,-+).,
/++0,12+"/++03,

-+).,/++0,

!85/#%&
3(%(4#05(&

!"#$%"&
'()$*+,
4$&56&7,

6,(7(,(05()&

+)(,).,/(01)2/"&

;*"C?+,D>0$E+.,

!85/#%&
3(%(4#05(&
4+#"88+&0$%"&,

!"#$%"&'()$*+,
4+#"88+&0+*,1!(493,

!"#$%"&'()$*+,
4+#"88+&0$%"&,F=+*B,

96&0:$0,(;<,/=&#%"&.,

>(?&;""%/5#$80&
!"#$%"&'()$*+,-+).,
/++0,A=+*B,

!"#$#%&
3(%(4#05(&

!"#$#%&
3#$0*)&

,,

!<#,=&6280(&;""%/5#$80&

.#5(?88@&;""%/5#$80&

G,
G,
G,

.89,):9#,(&;""%/5#$80&

, ,-+),9>$%$?,4$%&7,

Figure 1: Sindbad System Architecture.

SINDBAD ARCHITECTURE

Although news feed functionality is widely available in all social
network systems [9], these systems select the relevant messages
either based on the message timestamp or some importance criteria
that ignores the spatial aspect of posted messages. Thus, users may
miss several important messages that are spatially related to them.
For example, when a traveling user logs on to a social network site,
the user would like to get the news feed that match his/her new
location, rather than receiving the most recent (non-spatial) news
feed. The same concept applies for users who continuously log
onto the system from the same location, yet have a large number of
friends. It is of essence for such users to limit their news feed to the
messages related to their location. Examples of the location-aware
news feed returned by Sindbad include a message about local news,
a comment about a local store, or a status message targeting friends
in a certain area.
The main idea of the location-aware news feed module (GeoFeed) is to abstract the location-aware news feed problem into
one that evaluates a set of location-based point queries against each
friend in a user’s friend list that retrieves the set of messages issued
that overlap with the querying user’s location. The location-aware
news feed is equipped with three different approaches for evaluating each location-based query: (1) spatial pull approach, in which
the query is answered through exploiting a spatial index over the
messages posted by the friend, (2) spatial push approach, in which
the query simply retrieves the answer from a pre-computed materialized view maintained by the friend, and (3) shared push approach, in which the pre-computation and materialized view maintenance are shared among multiple users. Then, the main challenge of GeoFeed is to decide on when to use each of these three
approaches for a query.
A better response time calls for using the spatial push approach
for all location-aware news feed queries issued to Sindbad. In this
case, all location-aware news feed are pre-computed. However,

Figure 1 depicts the Sindbad system architecture that consists
of three main modules, namely, location-aware news feed (GeoFeed), location-aware ranking, and location-aware recommender
(LARS), and three types of stored data, namely, spatial messages,
user profiles, and spatial ratings. The communication between
Sindbad and the outside world is held through RESTful web API
interface (named Sindbad API Functions in Figure 1). The API
functions facilitates building a wide variety of applications (e.g.,
web applications, smart phone applications) on top of Sindbad. As
shown in Figure 1, Sindbad API functions can also be used to complement the functionality of existing social networking websites
e.g., Facebook, and turn their news feed and recommendation to
be location-aware. Sindbad can take five different types of input
(i.e., through the API interface): profile updates, a new message, a
new rating, a location-aware news feed query, and a location-aware
recommender query. The actions taken by Sindbad for each input
is described as follows:
Profile updates. As in typical social networking systems, Sindbad users can update their personal information, their friend list, or
accept a friend invitation from others.
A new message. Users can post spatial messages to be seen by their
friends, if relevant. A spatial message is represented by the tuple:
(MessageID, Content, Timestamp, Spatial), where MessageID and
Content represent the message identifier and contents, respectively,
Timestamp is the time the message is generated, while Spatial indicates the spatial range for which the message is effective. The
message is deemed relevant to only those users who are located
within its spatial range.
A new rating. Sindbad users can give location-aware (spatial) ratings to various items in a scale from one to five. Location-aware
(spatial) ratings can take any of these three forms: (1) Spatial ratings for non-spatial items, represented as a four-tuple (user, userLocation, rating, item); for example, a user located at home rating

650

this approach results in tremendous system overhead since a massive number of materialized views must be maintained. On the
other hand, favoring system overhead may result in executing more
queries using the spatial pull approach as no views needs to be
maintained. However, this approach may result in a long query response time for users who have a large number of friends, since
they will suffer a long delay when retrieving their news feed. Sindbad takes these factors into account when deciding on which approach to use to evaluate each query in a way that minimizes the
system overhead and guarantees a certain user response time. Sindbad is equipped with an elegant decision model that decides upon
using these approaches in a way that: (a) minimizes the system
overhead for delivering the location-aware news feed, and (b) guarantees a certain response time for each user to obtain the requested
location-aware news feed. More details about GeoFeed decision
model are provided in [2].

4.

Figure 2: Sindbad Web Application ScreenShot
locations associated with the ratings. More details about LARS are
given in [8].

LOCATION-AWARE RECOMMENDER

In general, recommender systems make use of community opinions to help users identify useful items from a considerably large
search space (e.g., Amazon inventory, Netflix movies). Community
opinions are expressed through explicit ratings represented by the
triple (user, rating, item) that represents a user providing a numeric
rating for an item (e.g., movie). Unfortunately, ratings represented
by this triple ignore the fact that both users and (some) items are
spatial in nature. Unlike traditional recommendation techniques
that assume the (non-spatial) rating triple (user, rating, item), the
location-aware recommender module (LARS) in Sindbad supports
a taxonomy of three types of location-based ratings: (1) Spatial
ratings for non-spatial items, represented as a four-tuple (user, ulocation, rating, item), where ulocation presents the user location,
(2) Non-spatial ratings for spatial items, represented as a four-tuple
(user, rating, item, ilocation), where ilocation presents the item location, and (3) Spatial ratings for spatial items, represented as a
five-tuple (user, ulocation, rating, item, ilocation), where ulocation
and ilocation present the user and item locations, respectively.
Sindbad produces recommendations using spatial user ratings
for non-spatial items by employing a user partitioning technique
that exploits the user location embedded in the ratings. This technique uses an adaptive pyramid structure to partition ratings by
their user location attribute into spatial regions of varying sizes at
different hierarchies. Then, for a querying user located in a region
R, we apply an existing collaborative filtering technique [1] that
utilizes only the ratings located in R. The challenge, however, is to
determine whether all regions in the pyramid must be maintained
in order to balance two contradicting factors: scalability and locality. Maintaining a large number of regions increases locality (i.e.,
recommendations unique to smaller spatial regions), yet adversely
affects system scalability because each region requires storage and
maintenance of a collaborative filtering data structure (i.e., model)
necessary to produce recommendations. The pyramid dynamically
adapts to find the right pyramid shape that minimizes storage overhead, i.e., increases scalability, without sacrificing locality.
Sindbad produces recommendations using non-spatial user ratings for spatial items by employing a travel penalty technique that
accounts for item locations embedded in the ratings. This technique
favors recommendation candidates the closer they are in travel distance to a querying user. Sindbad employs an efficient query processing framework capable of terminating early once it discovers
that the list of recommended items cannot be altered by processing
more candidates. Finally, to produce recommendations using spatial ratings for spatial items, Sindbad employs both the user partitioning and travel penalty techniques to address the user and item

5. LOCATION-AWARE RANKING
Sindbad users may have different preferences over messages
from the location-based news feed or recommender system. For
example, a traveling user may be more interested in messages that
were issued close to her current locations. On the other hand, the
stationary user maybe more interested in the most recently issued
messages. Moreover, due to the large volume of messages submitted to Sindbad and the user’s limited viewing capability (e.g.,
40 messages for the web page and 20 messages for mobile application), Sindbad provides a location-aware ranking module that is responsible for ranking the results coming out of the location-aware
news feed module and location-aware recommender module, based
on the user’s preferences.
Instead of ranking all objects, Sindbad location-aware ranking module encapsulates the user’s ranking preferences within the
query processor to improve the response time for the user. Moreover, if the user is continuously online, Sindbad location-aware
module is responsible for keeping either the news feed or the recommended items correctly ranked (i.e., continuously evaluating the
ranking function) as the user moves or if the user changes her preferences.

6. DEMONSTRATION SCENARIO
We hooked up Sinbad with Foursquare (by means of Foursquare
APIs) so that the demo attendee can use his/her existing Foursquare
account (if they already have one) to interact with Sindbad. In case
the demo attendee is not already registered as a Foursquare user or
does not want to sign up in Sindbad, we have prepared several preset user accounts for demonstration purpose. Moreover, we have
collected data (e.g., restaurants, Foursqure users preferences...) for
the city of Scottsdale in Arizona, where SIGMOD takes place this
year. This will increase the interaction with the demo attendees by
providing location-aware news feed to SIGMOD attendees or recommending them locations (e.g., restaurants, bars...) in Scottsdale.
Web/Phone Application. We demonstrate Sindbad using both
web (see Figure 2) and Android phone (see Figure 3) interfaces
which communicate with the system using Sindbad API Interface.
Both applications are integrated with the Google Maps API, and
can be accessed through standard web browsers. The demo attendee can choose to log onto Sindbad using one of several preset user accounts, or using his own existing Foursquare account, or
even by creating a new user account on the fly for the demonstration
purpose. When the user logs on to Sindbad, the application displays

651

(a) Location-Aware News Feed
(b) Location-Aware Recommender
Figure 3: Sindbad Android Phone Application ScreenShots

Figure 4: Sindbad System Analyzer ScreenShot
the relevant news for the user on the map. Each message is associated with a circle representing the range of each message. The
demo attendee may “change” locations by dragging and dropping
the green arrow on the map, to see how the news feed will change
accordingly. The user may also submit a geo-tagged message to the
system. For instance, the user shares a message "The pizza at ABC
restaurant is awesome" with a range distance of one mile. Moreover, The demo attendee can also ask Sindbad for recommendations (e.g., Restaurants in Scottsdale where SIGMOD takes place)
by clicking on the recommendation link on the left-hand side of the
web interface or by clicking on the location-aware recommendation
button in the mobile app. The user then enters the type of object he
is interested in (e.g., restaurant, theaters, stores). a spatial range in
miles, and also the number of recommended items to be returned to
him and then presses the Recommend button (see Figure 3(b)). The
recommended items are then shown on the map.
Sindbad system analyzer. The Sindbad analyzer shows the system behavior while different operations are applied to Sindbad (see
Figure 4). The GUI shows a map of the city of Minneapolis, in
which users that are currently logged in and are moving in the area,
are shown on the map in blue circles. The demo attendee can select
a user to track by clicking on the corresponding blue circle. At this
point, a drop down menu pops up giving two options: (1) News
feed: when selecting this option, a location-based news feed query
is issued to Sindbad, and the result is shown as a list of messages;
each message is associated with the user friend that posted the message as well as the approach used to retrieve this message (i.e., pull,
push, shared-push) (2) Recommendation: when selecting this option, a location-based recommendation query is issued to Sindbad,

and the result is shown as a list of items recommended to the user.
When the user moves, the news feed and recommendation results
are updated accordingly. The analyzer also shows a log of Sindbad
operations (e.g., user logins, queries, posted messages). The analyzer gives some statistics about GeoFeed (e.g., percentage of push,
pull, or shared-push messages) and LARS modules (e.g., number
of ratings, number of queries, and number of pyramid levels).

7. ACKNOWLEDGEMENT
This work is supported in part by the National Science Foundation under Grants IIS-0811998, IIS-0811935, CNS-0708604, IIS0952977 and by a Microsoft Research Gift.

8. REFERENCES
[1] G. Adomavicius and A. Tuzhilin. Toward the Next Generation of Recommender
Systems: A Survey of the State-of-the-Art and Possible Extensions. TKDE,
17(6):734–749, 2005.
[2] J. Bao, M. F. Mokbel, and C.-Y. Chow. GeoFeed: A Location-Aware News
Feed System. In ICDE, 2012.
[3] C.-Y. Chow, J. Bao, and M. F. Mokbel. Towards Location-based Social
Networking Services. In ACM SIGSPATIAL-LBSN, 2010.
[4] Facebook. http://www.facebook.com/.
[5] The Facebook Blog, "Facebook Places": http://tinyurl.com/3aetfs3.
[6] R. T. Fielding and R. N. Taylo. Principled Design Of The Modern Web
architecture. In ICSE, 2000.
[7] Foursquare: http://foursquare.com.
[8] J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel. LARS: A
Location-Aware Recommender System. In ICDE, 2012.
[9] A. Silberstein, J. Terrace, B. F. Cooper, and R. Ramakrishnan. Feeding Frenzy:
Selectively Materializing User’s Event Feed. In SIGMOD, 2010.
[10] Twitter. http://www.twitter.com/.

652

Yuhan Sun

Mohamed Sarwat

CIDSE
Arizona State University
Tempe, AZ 85287-9309
Email: Yuhan.Sun.1@asu.edu

CIDSE
Arizona State University
Tempe, AZ 85287-9309
Email: msarwat@asu.edu

I. I NTRODUCTION
Graphs are widely used to model data in many application domains, including social networking, citation network
analysis, studying biological function of genes, and brain
simulation. A graph contains a set of vertices and a set of
edges that connect these vertices. Each graph vertex or edge
may possess a set of properties (aka. attributes). Thanks to the
wide spread use of GPS-enabled devices, many applications
assign a spatial attribute to a vertex (e.g., geo-tagged social
media). Figure 1 depicts an example of a social graph that
has two types of vertices: Person and Venue and two types
of edges: Follow and Like. Vertices with type Person
have two properties (i.e., attributes): name and age. Vertices
with type Venue have two properties: name and spatial
location. A spatial location attribute represents the spatial
location of the entity (i.e., Venue) represented by such vertex.
In Figure 1, vertices {e, f, g, h, i} are spatial vertices which
represent venues.

d

w

llo

Fo

Person

c

Follow

b

a
Like

Abstract—Graphs are widely used to model data in many
application domains. Thanks to the wide spread use of GPSenabled devices, many applications assign a spatial attribute
to graph vertices (e.g., geo-tagged social media). Users may
issue a Reachability Query with Spatial Range Predicate (abbr.
RangeReach). RangeReach finds whether an input vertex
can reach any spatial vertex that lies within an input spatial
range. An example of a RangeReach query is: Given a social
graph, find whether Alice can reach any of the venues located
within the geographical area of Arizona State University. The
paper proposes G EO R EACH an approach that adds spatial data
awareness to a graph database management system (GDBMS).
G EO R EACH allows efficient execution of RangeReach queries,
yet without compromising a lot on the overall system scalability
(measured in terms of storage size and initialization/maintenance
time). To achieve that, G EO R EACH is equipped with a lightweight data structure, namely SPA-Graph, that augments the
underlying graph data with spatial indexing directories. When a
RangeReach query is issued, the system employs a prunedgraph traversal approach. Experiments based on real system
implementation inside Neo4j proves that G EO R EACH exhibits
up to two orders of magnitude better query response time and
up to four times less storage than the state-of-the-art spatial and
reachability indexing approaches.

l

Like

arXiv:1603.05355v1 [cs.DB] 17 Mar 2016

GeoReach: An Efficient Approach for Evaluating
Graph Reachability Queries with Spatial Range
Predicates

j

h

k

a: {name: Alice, age: 19}
b: {name: Dan, age: 20}
c: {name: Carol, age: 35}
d: {name: Bob, age: 25}
j: {name: Kate, age: 18}
k: {name: Mat, age: 23}
l: {name: Katharine, age:21}

g
e

i
f

P
R

Venue
e: {name: Pita Jungle}
f :{name: Chipotle}
g: {name: Sushi 101}
h: {name: Subway}
i: {name: McDonald's}

Fig. 1: Location-Aware Social Graph

Graph Database Management Systems (GDBMSs) emerged
as a prominent NoSQL approach to store, query, and analyze
graph data [15], [8], [25], [24], [28]. Using a GDBMS,
users can pose reachability analysis queries like: (i) Find out
whether two vertices in the graph are reachable, e.g., Are Alice
(vertex a) and Katharine (vertex l) reachable in the social
graph given in Figure 1. (ii) Search for graph paths that match
a given regular language expression representing predicates on
graph elements, e.g., Find all venues that Alice’s Followees
and/or her Followees’ Followees also liked. Similarly, users
may issue a Reachability Query with Spatial Range Predicate
(abbr. RangeReach). A RangeReach query takes as input a
graph vertex v and a spatial range R and returns true only if v
can reach any spatial vertex (that possesses a spatial attribute)
which lies within the extent of R (formal definition is given in
Section II). An example of a RangeReach query is: Find out
whether Alice can reach any of the Venues located within the
geographical area of Arizona State University (depicted as a
dotted red rectangle R in Figure 1). As given in Figure 1, The
answer to this query is true since Alice can reach Sushi 101
(vertex g) which is located within R. Another query example
is to find out whether Katharine can reach any of the venues
located within R. The answer to this query is false due to the

fact that the only venue reachable from Katharine, Subway
(vertex h), is not located within R.
There are several straightforward approaches to execute
a RangeReach query: (1) Traversal Approach: The naive
approach traverses the graph, checks whether each visited
vertex is a spatial vertex and returns true as the answer if
the vertex’s spatial attribute lies within the input query range
R. This approach yields no storage/maintenance overhead
since no pre-computed data structure is maintained. However,
the Traversal approach may lead to high query response
time since the algorithm may traverse the whole graph to
answer the query. (2) Transitive Closure (TC) Approach: this
approach leverages the pre-computed transitive closure [27]
of the graph to retrieve all vertices that are reachable from v
and returns true if at least one spatial vertex (located in the
spatial range R) that is reachable from v. The TC approach
achieves the lowest query response time, however it needs
to pre-compute (and maintain) the graph transitive closure
which is deemed notoriously infeasible especially for largescale graphs. (3) Spatial-Reachability Indexing (SpaReach)
Approach: uses a spatial index [3], [22] to locate all spatial
vertices VR that lie within the spatial range R and then uses
a reachability index [35] to find out whether v can reach any
vertex in VR . SpaReach achieves better query response time
than the Traversal approach but it still needs to necessarily
probe the reachability index for spatial vertices that may never
be reached from the v. Moreover, SpaReach has to store and
maintain two index structures which may preclude the system
scalability.
In this paper, we propose G EO R EACH, a scalable and
time-efficient approach that answers graph reachability queries
with spatial range predicates (RangeReach). G EO R EACH is
equipped with a light-weight data structure, namely SPAGraph, that augments the underlying graph data with spatial
indexing directories. When a RangeReach query is issued,
the system employs a pruned-graph traversal approach. As
opposed to the SpaReach approach, G EO R EACH leverages
the Spa-Graph’ s auxiliary spatial indexing information to
alternate between spatial filtering and graph traversal and early
prunes those graph paths that are guaranteed: (a) not to reach
any spatial vertex or (b) to only reach spatial vertices that
outside the input spatial range query. As opposed to the TC
and SpaReach approaches, G EO R EACH decides the amount of
spatial indexing entries (attached to the graph) that strikes a
balance between query processing efficiency on one hand and
scalability (in terms of storage overhead) on the other hand. In
summary, the main contributions of this paper are as follows:
• To the best of the authors’ knowledge, the paper is the
first that formally motivates and defines RangeReach,
a novel graph query that enriches classic graph reachability analysis queries with spatial range predicates.
RangeReach finds out whether an input graph vertex can
reach any spatial vertex that lies within an input spatial
range.
• The paper proposes G EO R EACH a generic approach that
adds spatial data awareness to an existing GDBMS.

Notation
G = {V, E}
Vvout
Vvin
RF (v)
VS
RFS (v)
n
m
v1 ❀ v2
MBR(P )

Description
A graph G with a set of vertices V and set of edges E
The set of vertices that can be reached via a direct edge
from a vertex v
The set of vertices that can reach (via a direct edge) vertex
v
The set of vertices that are reachable from (via any number
of edges) vertex v
The set of spatial vertices in G such that VS ⊆ V
The set of spatial vertices that are reachable from (via any
number of edges) vertex v
The cardinality of V (n = |V |); the number of vertices in
G
The cardinality of E (m = |E|); the number of edges in G
v2 is reachable from v1 via connected path in G (such that
both v1 and v2 ∈ V )
Minimum bounding rectangle of a set of spatial polygons
P (e.g., points, rectangles)

TABLE I: Notations.

G EO R EACH allows efficient execution of RangeReach
queries issued on a GDBMS, yet without compromising
a lot on the overall system scalability (measured in terms
of storage size and initialization/maintenance time).
1
• The paper experimentally evaluates G EO R EACH
using
real graph datasets based on a system implementation
inside Neo4j (an open source graph database system).
The experiments show that G EO R EACH exhibits up to
two orders of magnitude better query response time and
occupies up to four times less storage than the state-ofthe-art spatial and reachability indexing approaches.
The rest of the paper is organized as follows: Section II lays
out the preliminary background and related work. The SPAGraph data structure, G EO R EACH query processing, initialization and maintenance algorithms are explained in Sections III
to V. Section VI experimentally evaluates the performance of
G EO R EACH. Finally, Section VII concludes the paper.
II. P RELIMINARIES

AND

BACKGROUND

This section highlights the necessary background and related research work. Table I summarizes the main notations in
the paper.
A. Preliminaries
Graph Data. G EO R EACH deals with a directed property
graph G = (V, E) where (1) V is a set of vertices such that
each vertex has a set of properties (attributes) and (2) E is a set
of edges in which every edge can be represented as a tuple of
two vertices v1 and v2 (v1 , v2 ∈ V ). The set of spatial vertices
VS ⊆ V such that each v ∈ VS has a spatial attribute (property)
v.spatial. The spatial attribute v.spatial may be a geometrical
point, rectangle, or a polygon. For ease of presentation, we
assume that a spatial attribute of spatial vertex is represented
by a point. Figure 1 depicts an example of a directed property
graph. Spatial Vertices VS are represented by black colored
circles and are located in a two-dimensional planer space while
white colored circles represent regular vertices that do not
1 https://github.com/DataSystemsLab/GeoGraphDB–Neo4j

possess a spatial attribute. Arrows indicate directions of edges
in the graph.
Graph Reachability (v1 ❀ v2 ). Given two vertices v1 and v2
in a graph G, v1 can reach v2 (v1 ❀ v2 ) or in other words v2
is reachable from v1 if and only if there is at least one graph
path from v1 to v2 . For example, in Figure 1, vertex a can
reach vertex f through the graph path a->c->i->f so it can
be represented as a ❀ f . On the other hand, c cannot reach
h.
Reachability with Spatial Range Predicate (RangeReach).
RangeReach queries find whether a graph vertex can reach a
specific spatial region (range) R. Given a vertex v ∈ V in a
Graph G and a spatial range R, RangeReach can be described
as follows:

RangeReach(v, R) =


true













f alse

if ∃ v ′ such that
(1) v ′ ∈ VS
(2) v ′ .spatial lies within R
(3) v ❀ v ′
Otherwise.
(1)

As given in Equation 1, if any spatial vertex v ′ ∈ VS that
lies within the extent of the spatial range R is reachable from
the input vertex v, then RangeReach(v, R) returns true (i.e.,
v ❀ R). For example, in Figure 1, RangeReach(a, R) = true
since a can reach at least one spatial vertex f in R. However,
RangeReach(l, R) = false since l can merely reach a spatial
vertex h which is not located in R. Vertex d cannot reach R
since it cannot reach any vertex.
B. Related Work
This section presents previous work on reachability indexes,
spatial indexes, and straightforward solutions to processing
graph reachability queries with spatial range predicates (RangeReach).
Reachability Index. Existing solutions to processing graph
reachability queries (u ❀ v) can be divided into three
categories [35]: (1) Pruned Graph Traversal [6], [30], [34]:
These approaches pre-compute some auxiliary reachability
information offline. When a query is issued, the query processing algorithm traverses the graph using a classic traversal
algorithm, e.g., Depth First Search (DFS) or Breadth First
Search (BFS), and leverages the pre-computed reachability
information to prune the search space. (2) Transitive closure
retrieval [1], [7], [17], [18], [27], [31], [32]: this approach
pre-computes the transitive closure of a graph offline and compresses it to reduce its storage footprint. When a query u ❀ v
is posed, the transitive closure of the source vertex u is fetched
and decomposed. Then the query processing algorithm checks
whether the terminal vertex v lies in the transitive closure of u.
and (3) Two-Hop label matching [5], [8], [10], [11], [12], [26]:
The two-hop label matching approach assigns each vertex v in
the graph an out-label set Lout (v) and an in-label set Lin (v).
When a reachability query is answered, the algorithm decides
that u ❀ v if and only if Lout (v) ∩ Lin (v) 6= ∅. Since the

two label sets do not contain all in and out vertices, size of
the reachability index reduces.
Spatial Index. A spatial index [21], [23], [29] is used
for efficient retrieval of either multi-dimensional objects (e.g.,
hx,yi coordinates of an object location) or objects with spatial
extents, e.g., polygon areas represented by their minimum
boundary rectangles (MBR). Spatial index structures can be
broadly classified to hierarchical (i.e., tree-based) and nonhierarchical index structures. Hierarchical tree-based spatial
index structures can be classified into another two broad
categories: (a) the class of data-partitioning trees, also known
as the class of Grow-and-Post trees [20], which refers to the
class of hierarchical data structures that basically extend the
B-tree index structure [2], [13] to support multi-dimensional
and spatial objects. The main idea is to recursively partition
the spatial data based on a spatial proximity clustering, which
means that the spatial clusters may overlap. Examples of
spatial index structures in this category include R-tree [16] and
R*-tree [3]. (b) the class of space-partitioning trees that refers
to the class of hierarchical data structures that recursively
decomposes the space into disjoint partitions. Examples of
spatial index structures in this category include the Quadtree [14] and k-d tree [4].
Spatial Data in Graphs. Some existing graph database
systems, e.g., Neo4j, allow users to define spatial properties on
graph elements. However, these systems do not provide native
support for RangeReach queries. Hence, users need to create
both a spatial index and a reachability index to efficiently
answer a RangeReach queries (drawbacks of this approach
are given in the following section). On the other hand, existing
research work [19] extends the RDF data with spatial data to
support RDF queries with spatial predicates (including range
and spatial join). However, such technique is limited to RDF
and not general graph databases. It also does not provide an
efficient solution to handle reachability queries.
C. Straightforward Solutions
There are three main straightforward approaches to process
a RangeReach query, described as follows:
Approach I: Graph Traversal. This approach executes
a spatial reachability query using a classical graph traversal
algorithm like DFS (Depth First Search) or BFS (Breadth
First Search). When RangeReach(v, R) is invoked, the system
traverses the graph from the starting vertex v. For each visited
vertex, the algorithm checks whether it is a spatial vertex and
returns true as the query answer if the vertex’s location lies
within the input query range R because the requirement of
spatial reachability is satisfied and hence v ❀ R. Otherwise,
the algorithm keeps traversing the graph. If all vertices that v
can reach do not lie in R, that means v cannot reach R.
Approach II: Transitive Closure (TC). This approach precomputes the transitive closure of the graph and stores it as an
adjacency matrix in the database. Transitive closure of a graph
stores the connectivity component of the graph which can be
used to answer reachability query in constant time. Since the
final result will be determined by spatial vertices, only spatial

vertices are stored. When RangeReach(v, R) is invoked, the
system retrieves all spatial vertices that are reachable from v
by means of the transitive closure. The system then returns
true if at least one spatial vertex that is reachable from v is
also located in the spatial range R.
Approach III: SpaReach. This approach constructs two
indexes a-priori: (1) A Spatial Index: that indexes all spatial
vertices in the graph and (2) A Reachability Index: that indexes
the reachability information of all vertices in the graph. When
a RangeReach query is issued, the system first takes advantage
of the spatial index to locate all spatial vertices VR that
lie within the spatial range R. For each vertex v ′ ∈ VR ,
a reachability query against the reachability index is issued
to test whether v can reach v ′ . For example, to answer
RangeReach(a, R) in Figure 2, spatial index is exploited first
to retrieve all spatial vertices that are located in R. From the
range query result, it can be known that g, i and f are located
in rectangle R. Then graph reachability index is accessed to
determine whether a can reach any located-in vertex. Hence, it
is obvious RangeReach(a, R) = true by using this approach.
Critique. The Graph Traversal approach yields no storage/maintenance overhead since no pre-computed data structure is maintained. However, the traversal approach may lead
to high query response time (O(m) where m is the number
of edges in the graph) since the algorithm may traverse the
whole graph to answer the query. The TC approach needs
to pre-compute (and maintain) the graph transitive closure
which is deemed notoriously infeasible especially for largescale graphs. The transitive closure computation is O(kn3 ) or
O(nm) and the TC storage overhead is O(kn2 ) where n is
total number of vertices and k is the ratio of spatial vertices
to the total number of vertices in the graph. To answer a
RangeReach query, the TC approach takes O(kn) time since it
checks whether each reachable spatial vertex in the transitive
closure is located within the query rectangle. On the other
hand, SpaReach builds a reachability index, which is a timeconsuming step, in O(n3 ) [32] time. The storage overhead
of a spatial index is O(n) and that of a reachability index
is O(nm1/2 ). To store the two indices, the overall storage
overhead is O(nm1/2 ). Storage cost of this approach is far less
than TC approach but still not small enough to accommodate
large-scale graphs. The query time complexity of a spatial
index is O(kn) while that of reachability index is m1/2 . But
for a graph reachability query, checking is demanded for each
spatial vertex in the result set generated by the range query.
Hence, cost of second step reachability query is O(knm1/2 ).
The total cost should be O(knm1/2 ). Query performance
of Spa-Reach is highly impacted by the size of the query
rectangle since the query rectangle determines how many
spatial vertices are located in the region. In Figure 1, query
rectangle R overlaps with three spatial vertices. For example,
to answer RangeReach(l, R), all three vertices {f, g, i} will
be checked against the reachability index to decide whether
any of them is reachable from l and in fact neither of them
is reachable. In a large graph, a query rectangle will possibly
contain a large number of vertices. That will definitely lead to

d
c
b

a
l

1

2
h

5

4

3

RMBR(j)

6

8

7 g

L0

e

13

Q

10

9
14

f

12

11

15

i

16

17
19

B-vertex:
a: true
d: false
f: false
h: false
k: false

k

j

18

L1

20

21

R-vertex:
j: RMBR(j)
G-vertex:
b: {2, 19}
c: {12, 14}
e: {14}
g: {12, 14}
i: {14}
l: {2}

L2

Fig. 2: SPA-Graph Overview

high unreasonable high query response time.
III. O UR A PPROACH : G EO R EACH
In this section, we give an overview of G EO R EACH an efficient and scalable approach for executing graph reachability
queries with spatial range predicates.
A. Data Structure
In this section, we explain how G EO R EACH augments a
graph structure with spatial indexing entries to form what we
call SPatially-Augmented Graph (SPA-Graph). To be generic,
G EO R EACHstores the newly added spatial indexing entries
the same way other properties are stored in a graph database
system. The structure of a SPA-Graph is similar to that of
the original graph except that each vertex v ∈ V in a SPAGraph G = {V, E} stores spatial reachability information. A
SPA-Graph has three different types of vertices, described as
follows:
•

•

B-Vertex: a B-Vertex v (v ∈ V ) stores an extra bit (i.e.,
boolean), called Spatial Reachability Bit (abbr. GeoB)
that determines whether v can reach any spatial vertex
(u ∈ VS ) in the graph. GeoB of a vertex v is set to 1
(i.e., true) in case v can reach at least one spatial vertex
in the graph and reset to 0 (i.e., false) otherwise.
R-Vertex: an R-Vertex v (v ∈ V ) stores an additional attribute, namely Reachability Minimum Bounding
Rectangle (abbr. RMBR(v)). RMBR(v) represents the
minimum bounding rectangle MBR(S) (represented by
a top-left and a lower-right corner point) that encloses all
spatial polygons which represent all spatial vertices S that
are reachable from vertex v (RMBR(v) = MBR(RFS (v)),
RFS (v) = {u|v ❀ u, u ∈ VS }).

G-Vertex: a G-Vertex v stores a list of spatial grid cells,
called the reachability grid list (abbr. ReachGrid(v)). Each
grid cell C in ReachGrid(v) belongs to a hierarchical grid
data structure that splits the total physical space into n
spatial grid cells. Each spatial vertex u ∈ VS will be
assigned a unique cell ID (k ∈ [1, n]) in case u is located
within the extents of cell k, noted as Grid(u) = k. Each
cell C ∈ ReachGrid(v) contains at least one spatial vertex
that is reachable from v (ReachGrid(v) = ∪ Grid(u),
{u|v ❀ u, u ∈ VS }).
Lemma 3.1: Let v (v ∈ V ) be a vertex in a SPA-Graph
G = {V, E} and Vvout be the set of vertices that can be
reached via a direct edge from a vertex v. The reachability
minimum bounding rectangle of v (RMBR(v)) is equivalent
to the minimum bounding rectangle that encloses all its outedge neighbors Vvout and their reachability minimum bounding
rectangles. RMBR(v) = MBRv′ ∈Vvout (RMBR(v ′ ), v ′ .spatial).
Proof: Based on the reachability definition, the set of
reachable vertices RF (v) from a vertex v is equal to the union
of the set of vertices that is reached from v via a direct edge
′
(Vvout ) and all vertices that are reached from each vertex v ∈
Vvout . Hence, the set (RFS (v)) of reachable spatial vertices
from v is given in Equation 2.
•

RFS (v) =
′

[

′

(v ′ ∪ RFS (v ))

(2)

v ∈Vvout

And since RMBR(v) = MBR(RFS (v)), then the the reachability minimum bounding rectangle of v is as follows:
RM BR(v) = M BR(
′

[

′

(v ′ ∪ RFS (v ))))
(3)

v ∈Vvout
′

′

= M BRv′ ∈Vvout (RM BR(v ), v .spatial)

That concludes the proof.
Lemma 3.2: The set of reachable spatial grid cells from a
given vertex v is equal to the union of all spatial grid cells
reached from its all its out-edge neighbors and grid cells that
contain the spatial neighbors
ReachGrid(v) =

[

(ReachGrid(v ′ ) ∪ Grid(v ′ ))

(4)

v ′ ∈Vvout

Proof: Similar to that of Lemma III-A.
Example. Figure 2 gives an example of a SPA-Graph. GeoB
of vertex b is set to 1 (true) since b can reach three spatial
vertices e, f and h. GeoB for d is 0 since d cannot reach any
spatial vertex in the graph. Figure 2 also gives an example of a
Reachability Minimum Bounding Rectangle RMBR of vertex
j (i.e., RMBR(j)). All reachable spatial vertices from j are g,
i, h and f . Figure 2 also depicts an example of ReachGrid.
There are three layers of grids, denoted as L0 , L1 , L2 from
top to bottom. The uppermost layer L0 is split into 4 × 4
grid cells; each cell is assigned a unique id from 1 to 16.
We denote grid cell with id 1 as G1 for brevity. The middle
layer gird L1 is split into four cells G17 to G20 . Each cell
in L1 covers four times larger space than each cell in L0 .
G17 in L1 covers exactly the same area of G1 , G2 , G5 , G6

Algorithm 1 Reachability Query with Spatial Range Predicate
1: Function R ANGER EACH(v, R)
2: if v is a spatial vertex and v.spatial Lie In R then return true
3: Terminate ← true
4: if v is a B-vertex then
5:
if GeoB(v) = true then Terminate ← false
6: else if v is a R-vertex then
7:
if R full contains RMBR(v) then return true
8:
if R no overlap with RMBR(v) then return false
9:
Terminate ← false
10: else if v is a G-vertex then
11:
for each grid Gi ∈ ReachGrid(v) do
12:
if R fully contains Gi then return true
13:
Gi partially overlaps with R then Terminate ← false
14: if Terminate = false then
15:
for each vertex v ′ ∈ Vvout do
16:
if R ANGER EACH(v ′ , R) = true then return true
17: return false

in L0 . The bottom layer L2 contains only a single grid cell
which covers all four grids in L1 and represents the whole
physical space. All spatial vertices reachable from vertex a
are located in G2 , G7 , G9 , G12 and G14 , respectively. Hence,
ReachGrid(a) can be {2, 7, 9, 12, 14}. Notice that vertex e
and f are both located in G9 and G14 covered by G19 in
ReachGrid(a) can be replaced by G19 . Then, ReachGrid(a)
= {2, 7, 12, 19}. In fact, there exist more options to represent
ReachGrid(a), such as {17, 18, 19, 20} or {21} by merging
into only a single grid cell in L2 . When we look into
ReachGrid of connected vertices, for instance g, ReachGrid(g)
is {12, 14} and ReachGrid(i) is {14}. It is easy to verify that
ReachGrid(g) is ReachGrid(i)∪Grid(i.spatial), which accords
with lemma 3.2.
SPA-Graph Intuition. The main idea behind the SPAGraph is to leverage the spatial reachability bit, reachability
minimum bounding rectangle and reachability grid list stored
in a B-Vertex, R-Vertex or a G-Vertex to prune graph paths
that are guaranteed (or not) to satisfy both the spatial range
predicate and the reachability condition. That way, G EO R E ACH cuts down the number of traversed graph vertices and
edges and hence significantly reduce the overall latency of a
RangeReach query.
B. Query Processing
This section explains the RangeReach query processing
algorithm. The main objective is to visit as less graph vertices
and edges as possible to reduce the overall query latency.
The query processing algorithm accelerates the SPA-Graph
traversal procedure by pruning those graph paths that are
guaranteed (or not) to satisfy the spatial reachability constraint.
Algorithm 1 gives pseudocode for query processing. The
algorithm takes as input a graph vertex v and query rectangle
R. It then starts traversing the graph starting from v. For
each visited vertex v, three cases might happen, explained as
follows:
Case I (B-vertex): In case GeoB is false, a B-vertex
cannot reach any spatial vertex and hence the algorithm stops
traversing all graph paths after this vertex. Otherwise, further
traversal from current B-vertex is required when GeoB value
is true. Line 4 to 5 in algorithm 1 is for processing such case.

RMBR

A

R

I

A
I

RMBR

Query Rectangle

RMBR
R

Query Rectangle

(a) No Overlap

(x1,y1)

(x1,y1)

B

B
e

RMBR

(b) Lie In

(x2,y2)

(x2,y2)

e
f

RMBR

Q

(a)

Query
Rectangle

(b)

A
I

RMBR

f

Q

Query Rectangle

(c) Partially Covered By

R
(x1,y1)

B

RMBR

e
(x2,y2)

Fig. 3: Relationships between RMBR and a query rectangle
f

Q

Case II (R-vertex): For a visited R-vertex u, there are three
conditions that may happen (see figure 3). They are the case
from line 6 to 9 in algorithm 1:
• Case II.A: RMBR(u) lies within the query rectangle (see
Figure 3b). In such case, the algorithm terminates and
returns true as the answer to the query since there must
exist at least a spatial vertex that is reachable from v.
• Case II.B: The spatial query region R does not overlap
with RMBR(u) (see Figure 3a). Since all reachable spatial
vertices of u must lie inside RMBR(u), there is no
reachable vertex can be located in the query rectangle.
As a result, graph paths originating at u can be pruned.
• Case III.C: RMBR(u) is partially covered by the query
rectangle (see Figure 3c). In this case, the algorithm keeps
traversing the graph by fetching the set of vertices Vvout
that can be reached via a direct edge from v.
Case III (G-vertex): For a G-vertex u, it store many
reachable grids from u. Actually, it can be regarded as many
smaller RMBRs. So three cases may also happen. Algorithm 1
line 13 to 18 is for such case. Three cases will happen are
explained as follows:
• Case III.A: The query rectangle R fully contains any
grid cell in ReachGrid(u). In such case, the algorithms
terminates and returns true as the query answer.
• Case III.B: The query rectangle have no overlap with all
grids in ReachGrid(u). This case means that v cannot
reach any grids overlapped with R. Then we never
traverse from v and this search branch is pruned.
• Case III.C: If the query rectangle fully contains none
of the reachable grid and partially overlap with any
reachable grid, it corresponds to Partially Covered By
case for RMBR. So further traversal is performed.
Figure 2 gives an example of RangeReach that finds
whether vertex a can reach query rectangle Q (the shaded one
in figure 2). At the beginning of the traversal, the algorithm
checks the category of a. In case, It is a B-vertex and its GeoB
value is true, the algorithm recursively traverses out-edge
neighbors of a and perform recursive checking. Therefore,
the algorithm retrieves vertices b, c, d and j. For vertex b,

(c)

Fig. 4: R-vertex Pruning Power
it is a G-vertex and its reachable grids are G2 and G19 . G19
cover the range of four grids in L0 . They are G9 , G10 , G13
and G14 . The spatial range is merely partially covered by
Q (Case III.C), hence it is possible for b to reach Q. We
cannot make an assured decision in this step so b is recorded
for future traversal. Another neighbor is c. ReachGrid(c) is
{12, 14} which means that G12 and G14 are reachable from
c. G14 lies in Q (Case III.A). In such case, since a ❀ c, we
can conclude that a ❀ R. The algorithm then halts the graph
traversal at this step and returns true as the query answer.
IV. SPA-G RAPH A NALYSIS
This section analyzes each SPA-Graph vertex type rom two
perspectives: (1) Storage Overhead: the amount of storage
overhead that each vertex type adds to the system (2) Pruning
Power: the probability that the query processing algorithm
terminates when a vertex of such type is visited during the
graph traversal.
B-vertex. When visiting a B-Vertex, in case GeoB is false,
the query processing algorithm prunes all subsequent graph
paths originated at such vertex. That is due to the fact that such
vertex cannot reach any spatial vertex in the graph. Otherwise,
the query processing algorithm continues traversing the graph.
As a result, pruned power of a B-vertex lies in the condition
that GeoB is false. For a given graph, number of vertices that
can reach any space is a certain value. So probability that a
vertex can reach any spatial vertex is denoted as Ptrue . This is
also the probability of a B-vertex whose GeoB value is true.
Probability of a B-vertex whose GeoB value is false, denoted
as Pf alse , will be 1 − Ptrue . To sum up, pruned power of a
B-vertex is 1 − Ptrue or Pf alse
R-vertex. When an R-vertex is visited, the condition
whether the vertex can reach any space still exists. If the
R-vertex cannot reach any space, we assign the R-vertex a
specific value to represent it(e.g. set coordinates of RMBR’s

bottom-left point bigger than that of the top-right point). In
this case, pruned power of a R-vertex will be the same with
a B-vertex, which is Pf alse . Otherwise, when the R-vertex
can reach some space, it will be more complex. Because
information of RMBR and query rectangle R have some
impact on the pruned power of this R-vertex. The algorithm
stops traversing the graph in both the No Overlap and Lie
In cases depicted in Figures 3a and 3b. Figure 4 shows the
two cases that R-vertex will stop the traversal. In Figure 4,
width and height of the total 2D space are denoted as A and
B. Assume that the query rectangle can be located anywhere
in the space with equal probability. We use (x1 , y1 ) and
(x2 , y2 ) to represent the RMBR’s top-left corner and lowerright point coordinates, respectively. Then all possible areas
where top-left vertex of query rectangle Q should be part
of the total space, denoted as I (see the shadowed area in
the figure. Its area is determined by size of query rectangle.
Denote width and height of Q are e and f , then area of I,
AI = (A − e) × (B − f ).
First, we estimate probability of No Overlap case. Figure 4a
shows one case of No Overlap. If the query rectangle Q do
not overlap with RMBR, top-left vertex of Q must lie outside
rectangle R which is forms the overlap region (drawn with
solid line in Figure 4b). Area of R (denoted as AR ) is obviously determined by the RMBR location and size of Q. It can
be easily observed that AR = (x2 −(x1 −e))×(y2 −(y1 −f )).
Another possible case is demonstrated in Figure 4b. In such
case, if we calculate R in the same way, range of R will
exceeds area of I which contains all possible locations. As a
result, AR = AI in this case. As we can see, area of overlap
region is determined by the range of R and I altogether.
Then we can have a general representation of the overlap area
AOverlap = (min(A − e, x2 ) − max(0, x1 − e)) × (min(B −
f, y2 )−max(0, x2 −f ). The No Overlap area is AI −AOverlap
and the probability of having a No Overlap case is calculated
as follows:
PN oOverlap =

AOverlap
AI − AOverlap
=1−
.
AI
AI

(5)

Figure 4c depicts the Lie In case. When top-left vertex of
Q lies in region R, then such Lie In case will happen. To
ensure that R exists, it is necessary that e > (x2 − x1 ) and
f > (y2 −y1 ). If it is not, then probability of such case must be
0. If this requirement is satisfied, then AR = (x1 − (x2 − e))×
(y1 − (y2 − f )). Recall what is met in the above-mentioned
case, R may exceed the area of I. Similarly, more general
area should be AR = (min(A − e, x1 ) − max(0, x1 − (x2 −
e))) × (min(B − f, y1 ) − max(0, y1 − (y2 − f ))). Probability
R
of such case should be A
AI . To sum up, we have
PLieIn =

(

AR
AI

0

e > (x2 − x1 ) and f > (y2 − y1 )
else

(6)

After we sum up all conditional probabilities based on
Ptrue and Pf alse , pruning power of an R-vertex is equal to

Algorithm 2 G EO R EACH Initialization Algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:

Function INITIALIZE(Graph G = {V, E})
/*PHASE I: SPA-Graph Vertex Initialization */
for each Vertex v ∈ V according their sequence in topology do
InitializeVertex(G, v, MAX_REACH_GRIDS, MAX_RMBR)
/* PHASE II: Reachable Grid Cells Merging */
for each G-vertex v do
for each layer Li from L1 to Lbottom do
for each grid cell Gi in Li do
if Number of reachable grids in corresponding region in Li−1 is larger
than MERGE_COUNT then
10:
Add Gi in Li into ReachGrid(v)
11:
Remove reachable grid cells that are covered by Gi in higher layers

(PN oOverlap +PLieIn )×Ptrue +Pf alse . Evidently, the pruning
power of an R-vertex is more powerful than a B-vertex. When
the storage overhead of an R-vertex is considered, coordinates
of RMBR’s top-left and lower-right vertices should be stored.
Thus its storage will be at least four bytes depending on the
spatial data precision. That means the storage overhead of a
G-Vertex is always higher than that of a B-Vertex.
G-vertex. For a high resolution grid, it is of no doubt that a
G-vertex possesses a high pruning power. However, this comes
at the cost of higher storage overhead because more grid cells
occupies more space. When a G-vertex is compared with an
R-vertex, the area of an R-vertex is much larger than a grid. In
this case, an R-vertex can be seen as a a simplified G-vertex
for which the grid cell size is equal to that of RMBR. One
extreme case of R-vertex is that the vertex can reach only one
spatial vertex. In such case, RMBR is location of the reachable
spatial vertex. Such R-vertex can still be counted as a G-vertex
whose grid size x → 0. According the rule, it should be with
higher storage overhead and more accuracy. Actually, storing it
as a G-vertex will cost an integer while any R-vertex requires
storage for four float or even double number.
V. I NITIALIZATION & M AINTENANCE
This section describes the SPA-Graph initialization algorithm. The G EO R EACH initialization algorithm (Pseudocode
is given in Algorithm 2) takes as input a graph Graph
G = {V, E} and runs in two main phases: (1) Phase I:
SPA-Graph Vertex Type Initialization: this phase leverages the
tradeoff between query response time and storage overhead
explained in Section IV to determine the type of each vertex.
(2) Phase II: Reachable Grid Cells Merging: This step further
reduces the storage overhead of each G-Vertex in the SPAGraph by merging a set of grid cells into a single grid cell.
Details of each phase are described in Section V-A and V-B
A. SPA-Graph Vertex Type Initialization
To determine the type of each vertex, the initialization
algorithm takes into account the following system parameters:
• MAX_RMBR: This parameter represents a threshold that
limits space area of each RMBR. If a vertex v is
an R-vertex, area of RMBR(v) cannot be larger than
MAX_RMBR. Otherwise, v will be degraded to a B-vertex.
• MAX_REACH_GRIDS: This parameter sets up the maximum number of grid cells in each ReachGrid. If a vertex
v is a G-vertex, number of grid cells in ReachGrid(v)

Algorithm 3 SPA-Graph Vertex Initialization Algorithm

Algorithm 4 Maintain R-vertex

1: Function INITIALIZE V ERTEX(Graph G = {V, E}, Vertex v)
2: Type ← InitializeType(v)
3: switch (Type)
4: case B-vertex:
5:
Set v B-vertex and GeoB(v) = true
6: case G-vertex:
7:
ReachGrid(v) ← ∅
8:
for each Vertex v ′ ∈ Vvout do
9:
Maintain-GVertex(v, v ′ )
10:
if Number of grids in ReachGrid(v) ¿ MAX_REACH_GRIDS then
11:
Set v R-vertex and break
12:
Type ← R-vertex
13:
if Number of grids in ReachGrid(v) = 0 then
14:
Set v B-vertex, GeoB(v) ← false and break
15: case R-vertex:
16:
RMBR(v) ← ∅
17:
for each Vertex v ′ ∈ Vvout do
18:
Maintain-RVertex(v, v ′ )
19:
if Area(RMBR(v)) ¿ MAX_RMBR then
20:
Set v B-vertex, GeoB(v) ← true and break
21: end switch

1: Function M AINTAIN -RV ERTEX(From-side vertex v, To-side vertex v′ )
2: switch (Type of v′ )
3: case B-vertex:
4:
if GeoB(v ′ ) = true then
5:
Set v ′ B-vertex and GeoB(v) ← true
6:
else if RMBR(v) fully contains MBR(v ′ .spatial) then
7:
return false
8:
else
9:
RMBR(v) ← MBR(RMBR(v), v ′ .spatial)
10: case R-vertex:
11:
if RMBR(v) fully contains MBR(RMBR(v ′ ), v ′ .spatial) then
12:
return false
13:
else
14:
RMBR(v) ← MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial)
15: case G-vertex:
16:
if RMBR(v) fully contains MBR(RMBR(v ′ ), v ′ .spatial) then
17:
return false
18:
else
19:
RMBR(v) ← MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial)
20: end switch
21: return true

cannot exceed MAX_REACH_GRIDS. Otherwise, v will
be degraded to an R-vertex.
Algorithm 3 gives the pseudocode of the vertex initialization
algorithm. Vertices are processed based on their topological
sequence in the graph. For each vertex, the algorithm first
determines the initial vertex type using the InitializeType
function (pseudocode omitted for brevity). For a vertex v,
categories of vertex v ′ ({v ′ | v ′ ∈ Vvout }) will be checked.
If there is any B-vertex v ′ with GeoB(v ′ ) = true, v is directly
initialized to a B-vertex with GeoB(v) = true. Otherwise, if
there is any R-vertex, the function will return an R-vertex type,
which means that v is initialized to R-vertex. If either of the
above happens, the function returns G-vertex type. Based on
the initial vertex type, the algorithm may encounter one of the
following three cases:
Case I (B-vertex): The algorithm directly sets v as a Bvertex and GeoB(v) = true because there must exist one outedge neighbor v ′ of v such that GeoB(v ′ ) = true.
Case III (R-vertex): For each v ′ (v ′ ∈ Vvout ), the algorithm calls the Maintain-RVertex algorithm. Algorithm 4
shows the pseudocode of the Maintain-RVertex algorithm.
Maintain-RVertex aggregates RMBR information. After each
aggregation step, area of RMBR(v) will be compared with
MAX_RMBR: In case the area of RMBR(v) is larger than
MAX_RMBR, the algorithm sets v to be a B-vertex with a true
GeoBvalue and terminates. When v ′ is either a G-vertex or
an R-vertex, the algorithm uses the new bounding rectangle
returned from MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial) to
update the current RMBR(v). The algorithm calculates the
RMBR of a G-vertex in case III. In case v ′ is a B-vertex,
GeoB(v ′ ) must be reset to false. The algorithm then updates
RMBR(v) to MBR(RMBR(v), v.spatial).
Case II (G-vertex): For each vertex v ′ (v ′ ∈ Vvout ),
Maintain-GVertex (pseudocode omitted for the sake of space)
is invoked to calculate the ReachGrid of v ′ . In case v ′
is a B-vertex with GeoB(v ′ ) = false and v ′ is a spatial
vertex, the grid cell that contains the location of v ′ will be
added into ReachGrid(v). If v ′ is a G-vertex, all grid cells

in ReachGrid(v ′ ) and Grid(v ′ .spatial) will be added into
ReachGrid(v). It does not matter whether v ′ is a spatial vertex
or not. If v ′ is not a spatial vertex, Grid(v ′ .spatial) is ∅.
After accumulating information from each neighbor v ′ , the
algorithm changes the type of v to R-vertex immediately in
case the number of reachable grid cells in ReachGrid(v) is
larger than MAX_REACH_GRIDS. Therefore, the algorithm
sets the Type to R-vertex since RMBR(v) should be calculated
for possible future usage, e.g. RMBR of in-edge neighbors of
v(it will be shown in R-vertex case).
Example. Figure 2 depicts a SPA-Graph with MAX_RMBR
= 0.8A and MAX_REACH_GRIDS = 4, where A is area of the
whole space. Each vertex is attached with some information
and affiliated to one category of G EO R EACH index. Their
affiliations are listed in the figure. It is obvious that those
vertices which cannot reach any spatial vertices will be stored
as B-vertex and have a false boolean GeoB value to represent
such condition. Vertices d, f , h, i, j and k are assigned a
false value. Other vertices are G-vertex initially. ReachGrid(a)
= {2, 7, 9, 12, 14}, ReachGrid(b) = {2, 9, 14}, ReachGrid(c)
= {12, 14}, ReachGrid(e) = {14}, ReachGrid(g) = {12, 14},
ReachGrid(i) = {14}, ReachGrid(j) = {2, 7, 12, 14} and
ReachGrid(l) = {2}. Because of MAX_REACH_GRIDS, some
of them will be degraded to an R-vertex. Number of reachable grids in ReachGrid(a) and ReachGrid(j) are 4 and
5, respectively. Both of them are larger than or equal to
MERGE_COUNT. They will be degraded to R-vertex first. Then
area of their RMBR are compared with MAX_RMBR. Area
of RMBR(a) is apparently over 80% of the total space area.
According to MAX_RMBR, a is stored as a B-vertex with a true
value while j is stored as an R-vertex with an RMBR.
B. Reachable Grid Cells Merging
After the type of each vertex is decided, the initialization
algorithm performs the reachable grid cells merging phase
(lines 5 to 11 in Algorithm 2). In this phase, the algorithm
merges adjacent grid cells to reduce the overall storage overhead of each G-Vertex. To achieve that, the algorithm assumes
a system parameter, namely MERGE_COUNT. This parameter

Algorithm 5 Maintain B-vertex
1: Function M AINTAIN -BV ERTEX(From-side vertex v, To-side vertex v′ )
2: if GeoB(v) = true then
3:
return false
4: else
5:
switch (Type of v ′ )
6:
case B-vertex:
7:
if GeoB(v ′ ) = true then
8:
GeoB(v) ← true
9:
else if v ′ .spatial 6= NULL then
10:
ReachGrid(v) ← Grid(v ′ .spatial)
11:
else
12:
return false
13:
case R-vertex:
14:
RMBR(v) ← MBR(RMBR(v ′ ), v ′ .spatial)
15:
case G-vertex:
16:
ReachGrid(v) ← ReachGrid(v ′ )∪Grid(v ′ .spatial)
17:
end switch
18: return true

determines how G EO R EACH merges spatially adjacent grid
cells according to MERGE_COUNT. In each spatial region with
four grid cells, the number of reachable grid cells should not
be less than MERGE_COUNT. Otherwise, we merge the four
grid cells into a single grid cell in the lower layer.
For each G-vertex v, all grid cells in grid cell layers L1 to
Lbottom are checked. When a grid cell Gi in Li is processed,
four grid cells in Li−1 that cover the same space with Gi will
be accessed. If number of reachable grid cells is larger than or
equal to MERGE_COUNT, Gi should be added in ReachGrid(v)
first. Then all grid cells covered by Gi in layers from L0 to
Li−1 should be removed. In order to achieve that, a recursive
approach is implemented as follows. For each grid cell in Li−1
that is reachable from v, the algorithm directly remove it from
ReachGrid(v). The removal stops at this grid in this layer. No
recursive checking is required on grid cells in higher layers for
which the space is covered by the reachable grid cell. Since
all those reachable grid cells have been removed already. For
those grid cells that are not reachable from v, the algorithm
cannot assure that they do not cover some reachable grids in
a higher layer. Hence, the recursive removal is invoked until
the algorithm reaches the highest layer or other reachable grid
cells are visited.
The SPA-Graph in Figure 2 has a MERGE_COUNT set to 2.
There is no merging in e, i and l because their ReachGrids
contain only one grid. The rest are b, c and g. In ReachGrid(b),
for each grid in L1 , we make the MERGE_COUNT checking.
G17 covers four grids G1 , G2 , G5 and G6 in L0 . In such
four-grids region, only G2 is reachable from b. The merging
will not happen in G17 . It is the same case in G18 and G20 .
However, there are two grids, G9 and G14 covered by G19
in L1 . As a result, the two grids in L0 will be removed from
ReachGrid(b) with G19 being added instead. For the grid G21
in L2 , the same checking in L1 will be performed. Since, only
G19 is reachable, no merging happens. Finally, ReachGrid(b)
= {2, 19}. Similarly, we can have ReachGrid(c) = {12, 14}
and ReachGrid(g) = {12, 14} where no merging occurs.
C. SPA-Graph Maintenance
When the structure of a graph is updated, i.e., adding or
deleting edges and/or vertices, G EO R EACHneeds to maintain

the SPA-Graph structure accordingly. Moreover, when the
spatial attribute of a vertex changes, G EO R EACHmay need to
maintain the RMBRand/or ReachGridproperties of that vertex
and other connected vertices as well. As a matter of fact, all
graph updates can be simulated as a combination of adding
and/or deleting a set of edges.
Adding an edge. When an edge is added to the graph,
the directly-influenced vertices are those that are connected
to another vertex by the newly added edge. The spatial
reachability information of the to-side vertex will not be
influenced by the new edge. Based upon Lemmas III-A
and 3.2, the spatial reachability information, i.e., RMBRor
ReachGrid, of the to-side vertex should be modified based
on the the from-side vertex. On the other hand, the fromside vertex may remain the same or change. In the former
case, there is no recursive updates required for the in-edge
neighbors of the from-side vertex. Otherwise, the recursive
updates are performed in the reverse direction until no change
occurs or there is no more in-edge neighbor. A queue Q will
be exploited to track the updated vertices. When Q is not
empty, which means there are still some in-edge neighbors
waiting for updates, the algorithm retrieves the next vertex
in the queue. For such vertex, all its in-edge neighbors are
updated by using the reachability information stored on this
vertex. Updated neighbors will then be pushed into the queue.
The algorithm halts when the queue is empty. Depending on
category of the from-side vertex, corresponding maintenance
functions, including Maintain-BVertex, Maintain-RVertex and
Maintain-GVertex are used to update the newly added spatial
reachability information.
Algorithm 5 is used when the from-side vertex is a B-vertex.
In algorithm 5, if the from-side vertex v is already a B-vertex
with GeoB(v) = true. The added edge will never cause any
change on v. Hence a false value is returned. In case GeoB(v)
= false, the algorithm considers type of the to-side vertex v ′ .
•

•

•

B-vertex. If GeoB(v ′ ) = true, it is no doubt that GeoB(v)
will be set to true and a true value will be returned.
Otherwise, the algorithm checks whether v ′ is spatial.
If it is, ReachGrid(v) is updated with Grid(v ′ spatial).
Otherwise, the algorithm returns false because v is not
changed.
R-vertex. In such case, it is certain that v will be updated
to an R-vertex. The algorithm merely updates RMBR(v)
with MBR(RMBR(v ′ ), v ′ .spatial).
G-vertex. It is similar to the R-vertex case. Type of v ′ can
decide that v should be a G-vertex and the algorithm updates ReachGrid(v) with ReachGrid(v ′)∪Grid(v ′ .spatial)

Maintain-BVertex and Maintain-RVertex are what we use
in the initialization. However, there is a new condition that
should be taken into consideration. When the from-side vertex
v is an R-vertex and the to-side vertex v ′ is a G-vertex, the
algorithm needs to update the RMBR(v) with ReachGrid(v ′ ).
Under such circumstance, first a dummy RMBR(v ′ ) will be
constructed using ReachGrid(v ′ ). Although it is not the exact
RMBR of v ′ , it is still precise. Error of the width and height

will not be greater than size of a grid cell. No matter what
function is invoked to update the from-side vertex, G EO R E ACH takes into account the system parameters MAX_RMBR and
MAX_REACH_GRIDS are checked on RMBR and ReachGrid,
respectively.
Deleting an edge. When an edge is removed, the to-side
vertex will be not impacted by the deleting which is the same
with adding an edge. To maintain the correctness of spatial
reachability information stored on the from-side vertex, the
only way is to reinitialize its spatial reachability information
according to all its current out-edge neighbors. If its structure
is different from the original state due to the deleting, the
structure of all its in-edge neighbors will be rebuilt recursively.
A queue Q is used to keep track of the changed vertices. The
way G EO R EACHmaintains the queue and the operations on
each vertex in the queue are similar to the AddEdge procedure.
Maintenance cost of deleting an edge will be O(kn3 ) because
the whole G EO R EACH index may be reinitialized.
VI. E XPERIMENTAL E VALUATION
In this section, we present a comprehensive experimental evaluation of G EO R EACH performance. We compare the
following approaches: GeoMT0, GeoMT2, GeoMT3, GeoP,
GeoRMBR and SpaReach. GeoMT0, GeoMT2 and GeoMT3 are approaches that store only ReachGrid by setting MAX_REACH_GRIDS to the total number of grids in
the space and MAX_RMBR to A where A represent the
area of the whole 2D space. Their difference lies in the
value of MERGE_COUNT. GeoMT0 is an approach where
MERGE_COUNT is 0. In such approach, no higher layer
grids are merged. MERGE_COUNT is set to 2 and 3 respectively in GeoMT2 and GeoMT3. GeoP is an approach in
which MERGE_COUNT = 0, MAX_REACH_GRIDS = 200 and
MAX_RMBR = A. In such approach, reachable grids in ReachGrid will not be merged. If the number of reachable grids
of ReachGrid(v) is larger than 200 then v will be degraded
to an R-vertex. Since MAX_RMBR = A, there will be no Bvertex. In GeoRMBR, MAX_REACH_GRIDS = 0, MAX_RMBR
= A, hence only RMBR s are stored. In all ReachGrid related
approaches, the total space is split into 128 × 128 pieces in
the highest grid layer. SpaReach approach is implemented with
both spatial index and reachability index. Graph structure is
stored in Neo4j graph database. Reachability index is stored as
attributes of each graph vertex in Neo4j database. Reachability
index we use is proposed in [33]. Spatial index used SpaReach
approaches is implemented by gist index in postgresql. To
integrate Neo4j and postgresql databases, for each vertex in
the graph, we assign it an id to uniquely identify it.
Experimental Environment. The source code for evaluating query response time is implemented in Java and compiled
with java-7-openjdk-amd64. Source codes of index construction are implemented in c++ and complied using g++ 4.8.4.
Gist index is constructed automatically by using command
line in Postgresql shell. All evaluation experiments are run
on a computer with an 3.60GHz CPU, 32GB RAM running
Ubuntu 14.04 Linux OS.

TABLE II: Graph Datasets (K = 103 )
Dataset
citeseerx
go-uniprot
patent
uniprot22m
uniprot100m
uniprot150m

|V |
6540K
6968K
3775K
1595K
16087K
25038K

|E|
15011K
34770K
16519K
1595K
16087K
25038K

davg
2.30
4.99
4.38
1.00
1.00
1.00

l
59
21
32
4
9
10

Datasets. We evaluate the performance of our methods
using six real datasets [9], [33] (see Table II). Number of
vertices and edges are listed in column |V | and |E|. Column
davg and l are average degree of vertices and length of the
longest path in the graph, respectively. Citeseerx and patent
are real life citation graphs extracted from CiteSeerx2 and US
patents3 [33]. Go-uniprot is a graph generated from Gene Ontology and annotation files from Uniprot4 [33]. Uniprot22m,
uniprot100m and uniprot150m are RDF graphs from UniProt
database [33]. The aforementioned datasets represent graphs
that possess no spatial attributes. For each graph, we simulate
spatial data by assigning a spatial location to a subset of
the graph vertices. During the experiments, we change the
ratio of spatial vertices to the total number of vertices from
20% to 80%. During the experiments, we vary the spatial
distribution to be: uniform, zipf, and clustered distributions.
Unless mentioned otherwise, the number of spatial clusters is
set to 4 by default.
A. Query Response Time
In this section, we fist compare the query response time
performance of SpaReach to our GeoP approach. Afterwards,
we change tunable parameters in G EO R EACH to evaluate
influence of these thresholds. For each dataset, we change the
spatial selectivity of the input query rectangle from 0.0001 to
0.1. For each query spatial selectivity, we randomly generate
500 queries by randomly selecting 500 random vertices and
500 random spatial locations of the query rectangle. The
reported query response time is calculated as the average time
taken to answer the 500 queries.
Figure 5 depicts the query response time of GeoP and
SpaReach on four datasets. 80% of vertices in the graph
are spatial and they are randomly-distributed in space. For
brevity, we omit the results of the other two datasets, i.e.,
uniprot22m and uniprot100m, since they have almost the same
graph structure and exihibit the same performance. As it turns
out In Figure 5, GeoP outperforms SpaReach for any query
spatial selectivity in uniprot150m, go-uniprot and citeseerx.
For these datasets, SpaReach approach cost more time when
query selectivity increases. When we increasing the query
range size, the range query step tends to return a larger number
of spatial vertices. Hence, the graph reachability checking
step has to check more spatial vertices. Figure 5c and 5d
show similar experiment results. In conclusion, GeoP is much
2 http://citeseer.ist.psu.edu/
3 http://snap.stanford.edu/data/
4 http://www.uniprot.org/

query time (sec)

GeoP

SpaReach

query time (sec)

GeoP

SpaReach

query time (sec)

GeoP

SpaReach

query time (sec)

10000

100

10

100

100

10

1

10

1

1

0.1

1

0.01
0.0001

0.001

0.01

0.1

0.1
0.0001

0.001

0.01

0.1

0.01
0.0001

0.001

0.01

0.1

0.1
0.0001

0.001

GeoP

0.01

Query range

Query range

Query range

Query range

(a) uniprot150m

(b) patent

(c) go-uniprot

(d) citeseerx

SpaReach

0.1

Fig. 5: Query response time (80% spatial vertex ratio, randomly-distributed spatial data, and spatial selectivity ranging from 0.0001 to 0.1)
more query-efficient in relatively sparse graphs. Patent dataset
is the densest graph with richer reachability information.
Figure 5b indicates that even when spatial selectivity set to
0.0001, GeoP can achieve almost the same performance as
SpaReach. When spatial selectivity increases, GeoP outperforms SpaReach again. In a denser graph, the performance
difference between the two approaches is smaller than in
sparse graphs especially when the spatial selectivity is low.
Table III compares the query response time of all our approaches for the uniprot150m, patent go-uniprot and citeseerx
datasets with randomly distributed spatial vertices and spatial
ratio of 80%. In uniprot150m, all our approaches almost have
the same performance. The same pattern happens with the
uniprot22m, uniprot100m and go-uniprot datasets. So we use
uniprot150m as a representative.
For the patent graph with random-distributed spatial vertices
and spatial ratio of 20%, query efficiency difference can be
easily caught. GeoMT0 keeps information of exact reachable
grids of every vertex which brings us fast query speed, but
also the highest storage overhead. RMBR stores general spatial
boundary of reachable vertices which is the most scalable.
However, such approach spend the most time in answering the
query. Since GeoMT3 is an approach that MERGE_COUNT is
set to 3, just few grids in GeoMT3 are merged. As a result,
its query time is merely little bit longer than GeoMT0. There
are more grids getting merged in GeoMT2 than in GeoMT3.
Inaccuracy caused by more integration lowers efficiency of
GeoMT3 in query. GeoP is combination of ReachGrid and
RMBR. Its query efficiency is lower than GeoMT0 and better
than GeoRMBR. In this case, GeoMT2 outperforms GeoP. But
it is not always the case. By tuning MAX_REACH_GRIDS to
a larger number, GeoP can be more efficient in query.
In citeseerx, GeoMT0 keeps the best performance as expected. Performance of GeoP is in between GeoMT0 and
GeoRMBR as what is shown in patent. But GeoMT2 and
GeoMT3 reveal almost the same efficiency and they are worse
than GeoRMBR. Distinct polarized graph structure accounts
for the abnormal appearance. In citeseerx, all vertices can be
divided into two groups. One group consists of vertices that
cannot reach any vertex. The other group contains a what
we call center vertex. The center vertex has huge number of
out-edge neighbor vertices and is connected by huge number
of vertices as well. Because the center vertex can reach that
many vertices, it can reach nearly all grid cells in space. As

a result, vertices that can reach the center vertex can also
reach all grid cells in space. So no matter what value is
MAX_REACH_GRIDS, reachable grids in ReachGrid of these
vertices will be merged into only one grid in a lower layer
until to the bottom layer which is the whole space. Then such
ReachGrid can merely function as a GeoB which owns poorer
locality than RMBR.
B. Storage Overhead
Figure 6a gives the storage overhead of all approaches
for the uniprot150m dataset. In this experiment, the spatial
vertices are randomly distributed in space. Since uniprot22m
and uniprot100m share the same pattern with uniprot150m
(even spatial distribution of vertices varies), they are not shown
in the figure. The experiments show that G EO R EACH and
all its variants require less storage overhead than SpaReach
because of the additional overhead introduced by the spatial index. When there are less spatial vertices, SpaReach
obviously occupies less space because size of spatial index
lessens. However, SpaReach always requires more storage
than any other approaches. Storage overhead of G EO R EACH
approaches shows a two-stages pattern which means it is either
very high (ratio = 0.8, 0.6 and 0.4) or very low (ratio = 0.2).
The reason is as follows. These graphs are sparse and almost
all vertices reach the same vertex. This vertex cannot reach any
other vertex. Let us call it an end vertex. If the end vertex is
a spatial vertex, then all vertices that can reach the end vertex
will keep their spatial reachability information (no matter what
category they are) in storage. But if it is not, majority of
vertices will store nothing for spatial reachability information.
GeoMT0 and GeoP are of almost the same index size because
of sparsity and end-point phenomenon in these graphs. Such
characteristic causes that almost each vertex can just reach
only one grid which makes MAX_REACH_GRIDS invalid in
approach GeoMT0 (number of reachable grids is always less
than MAX_REACH_GRIDS) which makes GeoMT0 and GeoP
have nearly the same size. For similar reason, MERGE_COUNT
becomes invalid in these datasets which makes GeoMT2 and
GeoMT3 share the same index size with GeoMT0 and GeoP.
We also find out that index size of GeoRMBR is slightly larger
than GeoMT0 approaches. Intuitively, RMBR should be more
scalable than ReachGrid. But most of the vertices in these
three graphs can reach only one grid. In GeoRMBR, for each
vertex that have reachable spatial vertices, we assign an RMBR

TABLE III: Query Response Time in three datasets, 80% spatial vertex ratio, and spatial selectivity ranging from 0.0001 to 0.1
Selectivity
0.0001
0.001
0.01
0.1
Index size

MT0
68
65
66
69

GeoMT0
GeoP

(MB)
2400

uniprot150m
MT3 GeoP
67
66
78
66
65
65
65
75

MT2
68
77
66
65

GeoMT2
GeoRMBR

GeoMT3
SpaReach

Index size

RMBR
66
65
65
66
GeoMT0
GeoP

(MB)
6000

MT0
643
168
87
51

GeoMT2
GeoRMBR

patent
MT3
741
185
98
59

MT2
762
258
143
108

GeoMT3
SpaReach

Index size

GeoP
1570
559
217
155

GeoMT0
GeoP

(MB)
750

RMBR
2991
1965
915
348
GeoMT2
GeoRMBR

MT0
202
34
32
33

GeoMT3
SpaReach

Index size

4000

500

800

800

2000

250

400

0
0.8

0.6

0.4

0.2

0
0.8

0.6

0.4

0.2

GeoMT0
GeoP

(MB)
1200

1600

0

citeseerx
MT3 GeoP
203
210
471
207
410
189
399
160

MT2
212
460
408
399

GeoMT2
GeoRMBR

RMBR
234
215
200
183
GeoMT3
SpaReach

0
0.8

0.6

0.4

0.2

0.8

0.6

0.4

ratio

ratio

ratio

ratio

(a) uniprot150m

(b) patent

(c) go-uniprot

(d) citeseerx

0.2

Fig. 6: Storage Overhead (Randomly distributed, spatial vertex ratio from 0.8 to 0.2)
index size(MB)

Random

Clustered

Zipf

2400
1800

index size(MB)

Random

Clustered

Zipf

index size(MB)

Random

Clustered

Zipf

index size(MB)

1800

7 

6

1200



400

600

2 

200

Random

Clustered

Zipf

1200
600
0

0

0
GeoMT2

GeoP

GeoRMBR SpaReach

(a) uniprot150m

GeoMT2

GeoP

GeoRMBR SpaReach

(b) patent

0
GeoMT2

GeoP

GeoRMBR SpaReach

(c) go-uniprot

GeoMT2

GeoP

GeoRMBR SpaReach

(d) citeseerx

Fig. 7: Storage Overhead for varying spatial data distribution (randomly, cluster and zipf distributed) and 0.8 spatial vertex ratio)
which will be stored as coordinates of RMBR’s top-left and
lower-right points. It is more scalable to store one grid id than
two coordinates. So when a graph is highly sparse, index size
of GeoMT0 is possible to be less than GeoRMBR.
Figure 6c shows that in go-uniprot all G EO R EACH approaches performs better than SpaReach. When we compare
all the G EO R EACH approaches, GeoMT0, GeoMT2 and GeoMT3 lead to almost the same storage overhead. That happens
due to the fact that go-uniprot is a very sparse graph. A
vertex can only reach few grids in the whole space. Grid
cells in ReachGrid can hardly be spatially adjacent to each
other which causes no integration. The graph sparsity makes
the number of reachable grids in ReachGrid always less than
MAX_REACH_GRIDS which leads to less R-vertices and more
G-vertices. In consequence, go-uniprot, GeoMT0, GeoMT2,
GeoMT3 and GeoP lead to the same storage overhead. It
is rational that GeoRMBR requires the least storage because
RMBR occupies less storage than ReachGrid.
When graphs are denser, results become more complex.
Figure 6b shows index size of different approaches in patent
dataset with randomly-distributed spatial vertices. GeoRMBR
and GeoP, take the first and the second least storage and
are far less than other approaches because both of them
use RMBR which is more scalable. GeoMT0 takes the most
storage in all spatial ratios for that ReachGrid takes high
storage overhead. GeoMT2 and GeoMT3 require less storage
than GeoMT0 because spatially-adjacent reachable grids in

GeoMT0 are merged which brings us scalability. GeoMT3
are more scalable than GeoMT2 because MERGE_COUNT in
GeoMT2 is 2 which causes more integration. There are three
approaches, GeoMT3, GeoP and GeoRMBR, that outperform
SpaReach approach. By tuning parameters in G EO R EACH, we
are able achieve different performance in storage overhead and
can also outperform SpaReach.
Figure 6d depicts index size of all approaches in citeseerx
with randomly distributed spatial vertices. Spatial vertices ratio
ranges from 0.8 to 0.2. All G EO R EACH approaches outperform
SpaReach except for one outlier when spatial vertices ratio
is 0.2. GeoMT0 consumes huge storage. This is caused by
the center vertex which is above-mentioned. Recall that large
proportion of ReachGrid contains almost all grids in space.
After bitmap compression, it will cause low storage overhead.
This is why when spatial vertices ratio is 0.8, 0.6 and 0.4,
GeoMT0 consumes small size of index. When the ratio is 0.2,
there are less spatial vertices. Although graph structure does
not change, the center vertex reach less spatial vertices and
less grids. Then the bitmap compression brings no advantage
in storage overhead.
Figure 7 shows the impact of spatial data distribution on
the storage cost. GeoMT0, GeoMT2 and GeoMT3 are all
ReachGrid-based approaches. Spatial data distribution of vertices influences all approaches the same way. For all datasets,
SpaReach is not influenced by the spatial data distribution.
SpaReach consists of two sections: (1) The reachability index

size is determined by graph structure and (2) The spatial
index size is directly determined by number of spatial vertices. Hence, SpaReach exhibits the same storage overhead
for different spatial data distributions. When spatial vertices
distribution varies, GeoRMBR also keeps stable storage overhead. This is due to the fact that the storage overhead for
each RMBR is a constant and the number of stored RMBR
s is determined by the graph structure and spatial vertices
ratio, and not by the spatial vertices distribution. Spatial data
distribution can only influence the shape of each RMBR.
Figure 7a shows that each approach in G EO R EACH keeps
the same storage overhead under different distributions in
uniprot150m. As mentioned before, GeoMT0, GeoMT2, GeoMT3 and GeoP actually represent the same data structure
since there is only a single reachable grid in ReachGrid. When
there is only one grid reachable, varying the spatial distribution
becomes invalid for all approaches which use ReachGrid.
Figure 7b and 7c shows that the storage overhead introduced by ReachGrid-based approaches decreases when spatial
vertices become more congested. Randomly distributed spatial
data is the least congested while zipf distributed is the most.
The number of reachable spatial vertices from each vertex do
not change but these reachable spatial vertices become more
concentrated in space. This leads to less reachable grids in
ReachGrid.
Figure 7d shows that when spatial vertices are more congested, ReachGrid based approaches, i.e., GeoMT0, GeoMT2
and GeoMT3, tend to be less scalable. Recall that citeseerx
dataset is a polarized graph with a center vertex. One group
contains vertices that can reach huge number of vertices (about
200,000) due to the center vertex. When spatial vertices are
more concentrated and that will lead to more storage overhead.
C. Initialization time
In this section, we evaluate the index initialization time
for all considered approaches. For brevity, we only show the
performance results for four datasets, uniprot150m, patent,
go-uniprot and citeseerx, since uniprot22m, uniprot100m and
uniprot150m datasets exhibit the same performance. Figure 8a
shows that SpaReach requires much more construction time
than the other approaches under all spatial ratios. Although
these graphs are sparse, they contain large number of vertices. This characteristic causes huge overhead in constructing
a spatial index which dominates the initialization time in
SpaReach. Hence, SpaReach takes much more time than all
other approaches. However, the SpaReach initialization time
decreases when decreasing the number spatial vertices since
the spatial index building step deals with less spatial vertices
in such case. However, SpaReach remains the worst even when
the spatial vertex ratio is set to 20%.
Figures 8b and 8d gives the initialization time for both the
patent and citeseerx datasets, respectively. GeoRMBR takes
significantly less initialization time compared to all other
approaches. GeoP takes less time than the rest of approaches
because it is ReachGrid of partial vertices whose number of
reachable grids are less than MAX_REACH_GRIDS that are

calculated. In most cases, GeoMT0 can achieve almost equal
or better performance compared to SpaReach while GeoMT2
and GeoMT3 requires more time due to the integration of
adjacent reachable grids. To sum up, GeoRMBR and GeoP
perform much better than SpaReach in initialization even
in very dense graphs. GeoMT0 can keep almost the same
performance with SpaReach approach.
Figure 8c shows the initialization time for all six approaches on the go-uniprot dataset. Both RMBR approaches,
i.e., GeoRMBR and GeoP, still outperform SpaReach. This
is due to the fact that a spatial index constitutes a high
proportion of SpaReach initialization time. As opposed to the
uniprot150m case, the smaller performance gap between initializing GeoRMBR and SpaReach in go-uniprot.is explained
as follows. The size of go-uniprotis far less than uniprot150m
which decreases the spatial index initialization cost. As a
result, the index construction time in SpaReach is less than
that in uniprot150m. Since this graph has more reachability
information, all G EO R EACH approaches require more time
than in uniprot150m. It is conjunction of G EO R EACH and
SpaReach index size changes that causes the smaller gap.
VII. C ONCLUSION
This paper describes G EO R EACH a novel approach that
evaluates graph reachability queries and spatial range predicates side-by-side. G EO R EACH extends the functionality of a
given graph database management system with light-weight
spatial indexing entries to efficiently prune the graph traversal
based on spatial constraints. G EO R EACH allows users to
tune the system performance to achieve both efficiency and
scalability. Based on extensive experiments, we show that
G EO R EACH can be scalable and query-efficient than existing spatial and reachability indexing approaches in relatively
sparse graphs. Even in rather dense graphs, our approach
can outperform existing approaches in storage overhead and
initialization time and still achieves faster query response
time. In the future, we plan to study we plan to study
the extensibility of G EO R EACH to support different spatial
predicates. Furthermore, we aim to extend the framework
to support a distributed system environment. Last but not
least, we also plan to study the applicability of G EO R EACH
to various application domains including: Spatial Influence
Maximization, Location and Social-Aware Recommendation,
and Location-Aware Citation Network Analysis.
R EFERENCES
[1] R. Agrawal, A. Borgida, and H. V. Jagadish. Efficient Management of
Transitive Relationships in Large Data and Knowledge Bases. ACM,
1989.
[2] R. Bayer and E. M. McCreight. Organization and Maintenance of Large
Ordered Indices. Acta Informatica, 1(3):173–89, 1972.
[3] N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger. The R*-Tree:
An Efficient and Robust Access Method for Points and Rectangles. In
SIGMOD, pages 322–331, May 1990.
[4] J. L. Bentley. Multidimensional Binary Search Trees Used for Associative Searching. Communications of the ACM, CACM, 18(9):509–517,
1975.
[5] J. Cai and C. K. Poon. Path-hop: efficiently indexing large graphs for
reachability queries. In CIKM, pages 119–128. ACM, 2010.

Ix 	me
(sec)
480

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT
SpaReach

Index me

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT3
SpaReach

Index me

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT3 x me
(sec)
SpaReach
00

5


(sec)
150

320

00

100

200

1

15


50

100

0
0.8

0.

0.4

0.2

(sec)

0

0
8

0
0.6

0.4

0.2

08

0
0.6

0.4

0.2

08

GeoMT0
GeoP

GeoMT2
GeoRMBR

0.

0

ratio

ratio

ratio

ratio

(a) uniprot150

(b) patent

(c) go-uniprot

(d) citeseerx

GeoMT
SpaReach

02

Fig. 8: Initialization time (Randomly distributed, spatial vertex ratio from 0.8 to 0.2)

[6] L. Chen, A. Gupta, and M. E. Kurul. Stack-based algorithms for pattern
matching on dags. In VLDB, pages 493–504. VLDB Endowment, 2005.
[7] Y. Chen and Y. Chen. An efficient algorithm for answering graph
reachability queries. In ICDE, pages 893–902. IEEE, 2008.
[8] J. Cheng, S. Huang, H. Wu, and A. W.-C. Fu. Tf-label: a topologicalfolding labeling scheme for reachability querying in a large graph. In
SIGMOD, pages 193–204. ACM, 2013.
[9] J. Cheng, S. Huang, H. Wu, and A. W.-C. Fu. Tf-label: a topologicalfolding labeling scheme for reachability querying in a large graph. In
SIGMOD, pages 193–204. ACM, 2013.
[10] J. Cheng, Z. Shang, H. Cheng, H. Wang, and J. X. Yu. K-reach: who
is in your small world. PVLDB, 5(11):1292–1303, 2012.
[11] J. Cheng, J. X. Yu, X. Lin, H. Wang, and P. S. Yu. Fast computing
reachability labelings for large graphs with high compression rate. In
EDBT, pages 193–204. ACM, 2008.
[12] E. Cohen, E. Halperin, H. Kaplan, and U. Zwick. Reachability and
distance queries via 2-hop labels. SIAM Journal on Computing,
32(5):1338–1355, 2003.
[13] D. Comer. The Ubiquitous B-Tree. ACM Computing Surveys, 11(2):121–
137, 1979.
[14] R. A. Finkel and J. L. Bentley. Quad trees: A Data Structure for Retrieval
of Composite Keys. Acta Informatica, 4(1):1–9, 1974.
[15] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin. GraphChi:
Large-Scale Graph Computation on Just a PC. In OSDI, 2012.
[16] A. Guttman. R-Trees: A Dynamic Index Structure For Spatial Searching.
In SIGMOD, 1984.
[17] H. Jagadish. A compression technique to materialize transitive closure.
TODS, 15(4):558–598, 1990.
[18] R. Jin, Y. Xiang, N. Ruan, and H. Wang. Efficiently answering
reachability queries on very large directed graphs. In SIGMOD, pages
595–608. ACM, 2008.
[19] J. Liagouris, N. Mamoulis, P. Bouros, and M. Terrovitis. An effective
encoding scheme for spatial RDF data. PVLDB, 7(12):1271–1282, 2014.
[20] D. B. Lomet. Grow and Post Index Trees: Roles, Techniques and Future
Potential. In SSD, pages 183–206, Aug. 1991.
[21] P. Rigaux, M. Scholl, and A. Voisard. Spatial Databases with Application to GIS. Morgan Kaufmann, 2002.
[22] H. Samet. The Design and Analysis of Spatial Data Structures. AddisonWesley, 1990.
[23] H. Samet. Foundations of Multidimensional and Metric Data Structures.
Morgan Kaufmann, 2006.
[24] M. Sarwat, S. Elnikety, Y. He, and G. Kliot. Horton: Online Query
Execution Engine for Large Distributed Graphs. In ICDE, 2012.
[25] M. Sarwat, S. Elnikety, Y. He, and M. F. Mokbel. Horton+: A Distributed
System for Processing Declarative Reachability Queries over Partitioned
Graphs. PVLDB, 6(14):1918–1929, 2013.
[26] R. Schenkel, A. Theobald, and G. Weikum. Hopi: An efficient connection index for complex xml document collections. In EDBT, pages
237–255. Springer, 2004.
[27] S. Seufert, A. Anand, S. Bedathur, and G. Weikum. Ferrari: Flexible
and efficient reachability range assignment for graph indexing. In ICDE,
pages 1009–1020. IEEE, 2013.
[28] B. Shao, H. Wang, and Y. Li. Trinity: A Distributed Graph Engine on
a Memory Cloud. In SIGMOD, 2013.
[29] S. Shekhar and S. Chawla. Spatial Databases: A Tour. Prentice Hall,
2003.
[30] S. Trißl and U. Leser. Fast and practical indexing and querying of very
large graphs. In SIGMOD, pages 845–856. ACM, 2007.

[31] S. J. van Schaik and O. de Moor. A memory efficient reachability data
structure through bit vector compression. In SIGMOD, pages 913–924.
ACM, 2011.
[32] H. Wang, H. He, J. Yang, P. S. Yu, and J. X. Yu. Dual labeling:
Answering graph reachability queries in constant time. In ICDE, pages
75–75. IEEE, 2006.
[33] Y. Yano, T. Akiba, Y. Iwata, and Y. Yoshida. Fast and scalable
reachability queries on graphs by pruned labeling with landmarks and
paths. In CIKM, pages 1601–1606. ACM, 2013.
[34] H. Yildirim, V. Chaoji, and M. J. Zaki. Grail: Scalable reachability index
for large graphs. PVLDB, 3(1-2):276–284, 2010.
[35] A. D. Zhu, W. Lin, S. Wang, and X. Xiao. Reachability queries on large
dynamic graphs: a total order approach. In SIGMOD, pages 1323–1334.
ACM, 2014.

A Demonstration of MNTG A Web-based Road Network Traffic Generator
Mohamed F. Mokbel *1, Louai Alarabi *2, Jie Bao *3, Ahmed Eldawy *4, Amr Magdy *5, Mohamed Sarwat *6,
Ethan Waytas *7, Steven Yackel #8
*

University of Minnesota, Minneapolis, MN 55455, USA
{ mokbel, louai, 3 baojie, 4 eldawy, 5 amr, 6 sarwat}@cs.umn.edu, 7 wayt0012@umn.edu
#
Microsoft
8
spazard1@live.com
1

2

Abstract—This demo presents Minnesota Traffic Generator
(MNTG); an extensible web-based road network traffic generator.
MNTG enables its users to generate traffic data at any arbitrary
road networks with different traffic generators. Unlike existing
traffic generators that require a lot of time/effort to install,
configure, and run, MNTG is a web service with a user-friendly
interface where users can specify an arbitrary spatial region,
select a traffic generator, and submit their traffic generation
request. Once the traffic data is generated by MNTG, users can
then download and/or visualize the generated data. MNTG can be
extended to support: (1) various traffic generators. It is already
shipped with the two most common traffic generators, Brinkhoff
and BerlinMOD, but other generators can be easily added.
(2) various road network sources. It is shipped with U.S. Tiger
files and OpenStreetMap, but other sources can be also added.
A beta version of MNTG is launched at: http://mntg.cs.umn.edu.

I.

I NTRODUCTION

generate traffic data in arbitrary spatial regions using existing
traffic generators. For example, to be able to use Brinkhoff
or BerlinMOD generators for a different city than the default shipped one (Oldenburg and Berlin for Brinkhoff and
BerlinMOD generators, respectively), the user needs to first
obtain the road network information for the city of interest,
which is a tedious task by itself. For example, to get the road
network information for the city of Chicago, a user may need
to understand the format of OpenStreetMap [9], and then write
a program that extracts the road network of Chicago from
OpenStreetMap. After obtaining the new data, the user then
needs to modify the obtained format to match the required
one by either Brinkhoff or BerlinMOD. Such set of tedious
operations made it hard for casual users to use these traffic
generators for arbitrary spatial areas. As a testimony, one
can observe that almost all the literature that harnessed these
generators, used their default cities.

Having access to road network traffic data is a major
need to validate and evaluate indexing and query processing techniques in moving objects databases, spatio-temporal
databases, and data streams. Unfortunately, such data is not
easily available, and is usually of much smaller scale than
needed. The process of extracting real traffic data requires
installing and configuring many GPS-enabled devices and
continuously monitoring the locations of such devices, which
is a cumbersome task. For instance, GeoLife project [13] took
more than four years to collect 17,621 trajectories dataset with
the involvement of 182 volunteers in Beijing. As a result,
researchers have been using existing traffic generators as a
means of getting synthetic datasets that exhibit similar behavior
to real data. The most two common traffic generators are
Brinkhoff [1] and BerlinMOD [3], which have been widely
adopted by large numbers of papers in the database literature,
e.g., see [2], [5], [6], [8], [10], [12].

This demo presents Minnesota Traffic Generator
(MNTG) [7]; an extensible web-based road network traffic
generator. MNTG is not a new traffic generator. Instead, it
is a framework that encapsulates existing traffic generators
and makes them easily accessible. MNTG overcomes the
hurdles of existing traffic generators with three main features:
(1) MNTG is a web service with a user-friendly map interface.
Behind the scenes, MNTG carries the burden of configuring
and running existing traffic generators. (2) MNTG can be used
for any arbitrary spatial area, where users can just mark their
area of interest on a map interface, and submit their traffic
requests accordingly. (3) MNTG users do not need to worry
about the processing time or computing resources, where
MNTG has its own dedicated server that internally processes
the request in a multi-threaded paradigm, and emails the user
back when the data is generated. The notifying email includes
a link to download and/or visualize the data.

Even though existing traffic generators are quite useful,
nonetheless, most of them suffer from the following: (1) It
may take the user significant amount of effort to install and
configure the traffic generation tool. For example, in order
to run BerlinMOD, the user needs to first install a moving
object database, i.e., SECONDO [4], and then get familiar with
the script commands used to install it. After the installation,
users still need to understand an extensive list of configuration
parameters for each traffic generator. (2) It is not trivial to

MNTG is extensible to support: (1) various traffic generators. Currently, MNTG is shipped with the two most common
traffic generators, Brinkhoff and BerlinMOD, yet, it also has
the interface that can be used to add new traffic generators.
(2) various road network sources. It is currently shipped with
the support for U.S. Tiger files [11] and OpenStreetMap [9],
yet, it also has the interface that can be used to add other
sources for road network data.

∗

This work is supported in part by the National Science Foundation,
USA, under Grants IIS-0952977 and IIS-1218168.

978-1-4799-2555-1/14/$31.00 © 2014 IEEE

1246

A beta version of MNTG is launched as a web service for public use; a prototype can be accessed via
http://mntg.cs.umn.edu. The beta version supports both
Brinkhoff and BerlinMOD traffic generators on U.S Tiger files
and OpenStreetMap data. The extensibility interface for adding

ICDE Conference 2014

Traffic generation request

Download/visualize
traffic results

Traffic Data Users

System
Frontend

Status

Web Interface

Status

System
Backend

Download/
Visualization
Tools

Email Notifier

Road Network
Converter

2)

Notification

Notification

Results

Traffic Processor

Traffic Requests

Brinkhoff
BerlinMOD

Traffic Results

Random
New Generator

Road Network Data Sources

B. Case Study 1: U.S. Tiger Files

Traffic Generator
Developers
New Data Source OpenStreetMaps US Tiger Files

Fig. 1.

MNTG System Overview.

more generators or other road network sources is currently
working internally under our support, and will be shown in the
demo. Since its launch in February 2013, MNTG has received
more than 500 traffic generation requests from researchers
world wide. We envision that MNTG will be the de facto
standard for generating road network data for researchers in
spatial and spatio-temporal databases worldwide.
II.

S YSTEM OVERVIEW

Figure 1 gives an overview of MNTG architecture. A user
interacts with MNTG through its system front-end that includes
a web interface that allows users to submit traffic generation
requests, an email notifier that retrieves the status updates
from the back-end and keeps users posted, and download
and visualization tools that allow users to download and/or
visualize the generated data. Internally, MNTG system backend processes the traffic generation requests, where it includes
two main components: (1) Road Network Converter, which
is responsible for extracting the road network data from
different data sources. It currently uses US tiger files or OpenStreetMaps. Yet, it is extensible to support other road network
sources. (2) Traffic Processor, which takes the road network
data and feeds it to the underlying traffic generator. It currently
support both Brinkhoff and BerlinMOD data generators. Yet,
it is extensible to include other data generators.
III.

ROAD N ETWORK C ONVERTER

MNTG employs a road network converter that (a) receives
the user area of interest, (b) extracts the road network data
for the area of interest, and (c) sends the extracted data to the
traffic processor. We will first explain (and demonstrate) how
to include a road network data source in MNTG, then, we will
discuss two case studies that are already realized in MNTG;
US Tiger files, and OpenStreetMap.
A. Adding Road Network Data to MNTG
To add a new road network data source, MNTG provides
a template that consists of the following two function headers:
1)

as its output. This includes pruning all information
outside the selected area as well as pruning the non
road network information of the selected area.
PrepareStandardOutput.
This
function
takes the road network information from
ExtractRoadNetwork as its input, and produces
two standard text files node.txt and edge.txt,
that contain the final set of nodes and edges in the
selected area, respectively. The text files are in a
standard format to make it portable to various traffic
generators employed by MNTG traffic processor.

ExtractRoadNetwork. This function takes
a rectangular area, defined by two <latitude,
longitude> coordinates, as its input, and produces
the road network information of the selected area

1247

US Topologically Integrated Geographic Encoding and
Referencing (Tiger) Files [11] are published by US census
bureau on a yearly basis to provide the most recent information about US geographical data, which include city
boundaries, road networks, address information, water features,
and much more. A very unique feature of US Tiger Files
is that the files are partitioned and organized based on US
counties. In other words, all roads in a county are packed
as one compressed file with a unique file identifier, e.g.,
tl_2010_01001_roads.zip, where tl means tiger line,
2010 indicates the publishing year of the data, 01001 is
a unique identifier for the county (in this case is Autauga,
Alabama), and roads represents the type of data.
In this case, the function ExtractRoadNetwork identifies all counties that overlap with the rectangle area selected
by the user. Then, it loads the road network files for all
overlapped counties and filters out the road segments outside
the selected region. Then, the PrepareStandardOutput
function converts Tiger file format to the standard output
format for node.txt and edge.txt.
C. Case Study 2: OpenStreetMap
OpenStreetMap [9] (OSM) is a free GIS service, contributed mainly by volunteers to record all spatial landmarks
(including road networks) worldwide. All data in OSM is
encapsulated into a large XML file Planet.osm that includes four primitive data types: (1) Node, that represents a
spatial point by its latitude and longitude coordinates, (2) Way,
that consists of a sequence of nodes to construct a line or a
polygon, (3) Relation, that specifies the relation between ways
and nodes, e.g., two ways are connected together, and (4) Tags,
that provide description for other data types, i.e., node, Way
and Relation, using a key-value pair.
In this case, the function ExtractRoadNetwork first
retrieves all geographical data with an “osm” file, based on
the spatial area specified by the user, from OpenStreetMap
interface. Then, it parses all XML key-value pairs in the
downloaded “osm” file to extract the road network information.
Then, PrepareStandardOutput converts the road network information (extracted from the “osm” file) to the standard output format for the files node.txt and edge.txt.
IV.

T RAFFIC P ROCESSOR

MNTG provides a wrapper around existing traffic generators to ensure their ease of use. The traffic processor
component of MNTG is responsible on: (1) receiving the road

network data from the road network converter and feeds it to
the underlying generator, (2) running the underlying selected
traffic generator, and (3) producing the generating traffic to
be downloaded and/or visualized on a map interface. We first
explain how to include a traffic generator in MNTG. Then, we
discuss two case studies that are already realized in MNTG;
Brinkhoff and BerlinMOD traffic generators.

Traffic Generation
Parameters

A. Adding a Traffic Generator to MNTG
To add a new traffic generator, MNTG provides a template
that consists of the following three function headers:
1)

2)

3)

RoadNetworkConvert. This function takes a road
network in the standard format of two text files
node.txt and edge.txt as its input. The output
would be the same road network, but in a different
format that is required by the traffic generator.
TrafficGeneration. This function first pops up
a dialogue interface to prompt the user to enter various parameters for the generation request, e.g., number of objects and time interval. Then, it takes these
parameters along with the specified road network
extracted from RoadNetworkConvert, and makes
an external execution call to the traffic generator.
TrafficResultConvert. This function takes its
input as the output of the TrafficGeneration
function. Then, it converts the input into a standard
text format as < Object ID, Timestamp, type, latitude,
longitude> that can be easily downloaded as a text
file and/or visualized a map.

B. Case Study 1: Brinkhoff Generator

Traffic Generation
Region

Fig. 2.

MNTG Web GUI: Traffic Generation (Google Maps Interface)

To run the BerlinMOD traffic generator, a user would
need to set up a SECONDO database [4], and uses a
set of scripts to query it. To support BerlinMOD generator inside MNTG, we install and configure a SECONDO
database in our server and implement the three abstract
functions as follows: The RoadNetworkConvert function
reads the standard road network files and transforms it to
the format used in BerlinMOD. Ultimately, it produces a
data file named street.data, where the road segments
are represented by a pair of locations bounded by a set
of brackets. The TrafficGeneration function prepares
a customized query script for the SECONDO database,
i.e., BerlinMOD_DataGenerator_RequestID.SEC. It
does so by preparing a generic script in advance, and updates
the parameters based on the user request. Then, the function
runs the modified script to generate the traffic data. The
TrafficResultConvert function converts the traffic data
produced by BerlinMOD into the standard output format.

Brinkhoff traffic generator is one of most widely used
traffic generators [1] (cited 650+ per Google Scholar),
where its general idea is to simulate the object movements from two random locations using the shortest
path. To support Brinkhoff generator inside MNTG, we
implement its three abstract functions as follows: The
RoadNetworkConvert function converts the output of
the Road Network Converter into two binary files based
on the need of Brinkhoff, i.e., request_ID.node and
request_ID.edge. The TrafficGeneration function
prepares Brinkhoff configuration file, i.e., property.txt,
and assembles the calling command based on the user’s request. The TrafficResultConvert function converts the
traffic data produced by Brinkhoff generator into our standard
output format.

The audience interact with MNTG through the following
steps: (1) traffic request submission, where audience can submit their requests online by selecting the traffic generation
area from either Google Maps or OpenStreetMap interface
and the traffic generator method, (2) traffic data download and
visualization, where audience can either download or visualize
their requested data, (3) adding a new road network data
source, where audience can add a new road network data
source to MNTG, and (4) adding a new traffic generator,
where audience can register a new traffic generator, and then
uses it in their new traffic generation request.

C. Case Study 2: BerlinMOD Generator

A. Traffic Request Submission

BerlinMOD is another very popular traffic generator [3],
where it simulates human movements during the weekdays
and weekends. Users can specify their work and home areas
in the road networks, then the generator simulates the users
movements based on two rules: (1) during the weekdays, a
user leaves Home in the morning (at 8 a.m.+ T1), drives to
Work, stays there until 4 p.m.+ T2 in the afternoon, and then
returns back Home, (2) during the weekends, a user has an
0.4 probability to do an additional trip which may have 1-3
intermediate stops and ends at home.

1248

V.

D EMONSTRATION S CENARIO

Figure 2 gives the interface for MNTG. To generate data,
a user performs the following four steps: (1) either drag/zoom
the map or write an address in the search field to get the
geographical area of interest, (2) draw a rectangle on the map
for the traffic generation area, (3) select the traffic generator
from the drop down menu, e.g., Brinkhoff or BerlinMOD, and
(4) Click on the Generate button, and enter the parameters for
the traffic simulation, including the user email address.
Once the request is submitted, MNTG sends an email
to the user acknowledging the receipt of that request. Upon

Map, which is given to the user as a verification for the new
data to import.
D. Traffic Processor Extension

Fig. 3.

The audience may register a new traffic generator to
MNTG. For demonstration purpose, we prepared a new traffic
generator, termed RandomTraffic, which requires binary format
of road networks, runs with a java file, and outputs the traffic
result in a binary format. The RandomTraffic generator takes
its input as the number of moving objects and average speed.
Then, it selects random points as one per each moving object.
For each random point, we select another random destination,
and calculate the shortest path. Given the average speed,
objects move along the shortest path till their destinations.

MNTG Traffic Visualization (OpenStreetMap Interface)

(a) Extracted Nodes

Fig. 4.

We will show the audience the java template class file for
the RandomTraffic generator, which includes the implementation of its three abstract functions, RoadNetworkConvert,
RandomTraffic, and TrafficGeneration. Then, we will
upload this file to MNTG, which indicates that a new traffic
generator is registered. Audience will then restart MNTG to
see that they can submit a new traffic generator request using
the RandomTraffic generator.

(b) Extracted Edges

Adding New Road Network Data Source

completion of the traffic request, another email is sent to the
user with two hyper links. One for downloading the generated
data, and one for visualizing the data on MNTG web interface.
The time MNTG takes to complete the request mainly depends
on the request size and the underlying traffic generator, e.g.,
number of objects and simulation time.
B. Traffic Data Download & Visualization
Audience can use MNTG to download or visualize their
requested data. Downloaded data will have a standard text
format as follows:
OID TS Type
0
0 newpoint
1
0 newpoint

Lat
44.98636
44.99894

Lng
-93.29820
-93.18128

MNTG also stores the generated traffic data inside a
MySQL database, which can be used for visualization. Traffic
visualization is performed using Google Maps v3 API for
displaying overlays in HTML. The data is loaded via ajax
into the web page which then creates an overlay for each
time stamp of the moving object. Figure 3 gives an example
for traffic visualization, where the colored circles on the map
represent the moving objects. The user can either view the
animated object movements or move the handler on the bottom
scroll to view the snapshot at a specific time.
C. Adding a Road Network Data Source
The demo attendees can add a new road network data
source to MNTG. We have a prepared data set, termed
HighwayRoadNetwork, which includes only the highways in
US. We will show the audience how to register this new
road network with MNTG using two abstract functions. Once
registered, audience will see that they can submit a new request
to MNTG using the new road network. Figure 4 depicts the
nodes and edges of the HighwayRoadNetwork on a Google

1249

R EFERENCES
[1] T. Brinkhoff. A Framework for Generating Network-based Moving
Objects. GeoInformatica, 6(2), 2002.
[2] S. Chen, C. S. Jensen, and D. Lin. A Benchmark for Evaluating Moving
Object Indexes. VLDB Journal, 1(2), 2008.
[3] C. Düntgen, T. Behr, and R. H. Güting. BerlinMOD: a Benchmark for
Moving Object Databases. VLDB Journal, 18(6), 2009.
[4] R. H. Güting, T. Behr, and C. Düntgen. Secondo: A platform for moving
objects database research and for publishing and integrating research
implementations. IEEE Data Engineering Bulletin, 33(2), 2010.
[5] H. Hu, J. Xu, and D. L. Lee. PAM: An Efficient and Privacy-Aware
Monitoring Framework for Continuously Moving Objects. IEEE TKDE,
22(3), 2010.
[6] H. Jeung, Q. Liu, H. T. Shen, and X. Zhou. A Hybrid Prediction Model
for Moving Objects. In ICDE, 2008.
[7] M. F. Mokbel, L. Alarabi, J. Bao, A. Eldawy, A. Magdy, M. Sarwat,
E. Waytas, and S. Yackel. MNTG: An Extensible Web-based Traffic
Generator. In SSTD, 2013.
[8] M. F. Mokbel, X. Xiong, and W. G. Aref. SINA: Scalable Incremental
Processing of Continuous Queries in Spatio-temporal Databases. In
SIGMOD, 2004.
[9] OpenStreetMaps. http://www.openstreetmap.org/.
[10] H.-P. Tsai, D.-N. Yang, and M.-S. Chen. Mining Group Movement
Patterns for Tracking Moving Objects Efficiently. IEEE TKDE, 23(2),
2011.
[11] US TIGER LINES. http://www.census.gov/geo/maps-data/data/tigerline.html.
[12] W. Wu, W. Guo, and K.-L. Tan. Distributed Processing of Moving
K-Nearest-Neighbor Query on Moving Objects. In ICDE, 2007.
[13] Y. Zheng, Y. Chen, X. Xie, and W.-Y. Ma. GeoLife2.0: A LocationBased Social Networking Service. In MDM, 2009.

A Demonstration of GeoSpark: A Cluster Computing
Framework for Processing Big Spatial Data
Jia Yu

Jinxuan Wu

Mohamed Sarwat

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, Arizona 85281
Email: jiayu2@asu.edu

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, Arizona 85281
Email: jinxuanw@asu.edu

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, Arizona 85281
Email: msarwat@asu.edu

Abstract—This paper demonstrates G EO S PARK a cluster
computing framework for developing and processing large-scale
spatial data analytics programs. G EO S PARK consists of three
main layers: Apache Spark Layer, Spatial RDD Layer and Spatial
Query Processing Layer. Apache Spark Layer provides basic
Apache Spark functionalities as regular RDD operations. Spatial
RDD Layer consists of three novel Spatial Resilient Distributed
Datasets (SRDDs) which extend regular Apache Spark RDD to
support geometrical and spatial objects with data partitioning
and indexing. Spatial Query Processing Layer executes spatial
queries (e.g., Spatial Join) on SRDDs. The dynamic status of
SRDDs and spatial operations are visualized by G EO S PARK monitoring map interface. We demonstrate G EO S PARK using three
spatial analytics applications (spatial aggregation, autocorrelation
and co-location) to show how users can easily deﬁne their spatial
analytics tasks and efﬁciently process such tasks on large-scale
spatial data at interactive performance.

I.

I NTRODUCTION

Spatial data includes but is not limited to: weather maps,
geological maps, socioeconomic data, vegetation indices, and
more. Moreover, novel technology allows hundreds of millions
of users to use their mobile devices to access their healthcare information and bank accounts, interact with friends,
buy stuff online, search interesting places to visit on-thego, ask for driving directions, and more. In consequence,
everything we do on the mobile internet leaves breadcrumbs
of spatial digital traces, e.g., geo-tagged tweets, venue checkins. Making sense of such spatial data will be beneﬁcial for
several applications that may transform science and society
– For example: (1) Socio-Economic Analysis: that includes
for example climate change analysis, study of deforestation,
population migration, and variation in sea levels, (2) Urban
Planning: assisting government in city/regional planning, road
network design, and transportation / trafﬁc engineering, and
(3) Commerce and Advertisement: e.g., point-of-interest (POI)
recommendation services. The aforementioned applications
need a powerful data management platform to handle the large
volume of spatial data such applications deal with. Challenges
to building such platform are as follows:
Challenge I: System Scalability. The massive-scale
of available spatial data hinders making sense of it using traditional spatial database management systems.
Moreover, large-scale spatial data, besides its tremendous storage footprint, may be extremely difﬁcult to

978-1-5090-2020-1/16/$31.00 © 2016 IEEE

manage and maintain. The underlying database system
must be able to digest Petabytes of spatial data and
effectively analyze it.
Challenge II: Fast Analytics. In spatial data analytics
applications, users will not tolerate delays introduced
by the underlying spatial database system. Instead, the
user needs to see useful information quickly. Hence,
the underlying spatial data processing system must
ﬁgure out effective ways to execute spatial analytics
in parallel.
Existing spatial database systems extend relational database
systems with new data types, functions, operators, and index
structures to handle spatial operations based on the Open
Geospatial Consortium. Even though such systems sort of
provide full support for spatial data storage and access, they
suffer from a scalability issue. Based upon a relational database
system, such systems are not scalable enough to handle largescale analytics over big spatial data. Recent works (e.g., [1],
[2]) extend the Hadoop ecosystem to perform spatial analytics
at scale. The Hadoop-based approach indeed achieves high
scalability. However, these systems though exhibit excellent
performance in batch-processing jobs, they show poor performance handling applications that require fast data analysis.
Apache Spark [3], on the other hand, is an in-memory cluster
computing system. Spark provides a novel data abstraction
called resilient distributed datasets (RDDs) [4] that are collections of objects partitioned across a cluster of machines. Each
RDD is built using parallelized transformations (ﬁlter, join or
groupBy) that could be traced back to recover the RDD data.
In memory RDDs allow Spark to outperform existing models
(MapReduce) by up to two orders of magnitude. Unfortunately,
Spark does not provide native support for spatial data and
spatial operations. Hence, users need to perform the tedious
task of programming their own spatial data processing jobs on
top of Spark.
This paper demonstrates G EO S PARK 1 [5] an in-memory
cluster computing system for processing large-scale spatial
data. G EO S PARK extends Apache Spark to support spatial data
types and operations. In other words, the system extends the
resilient distributed datasets (RDDs) concept to support spatial
data. This problem is quite challenging due to the fact that
(1) spatial data may be quite complex, e.g., rivers’ and cities’

1410

1 GeoSpark

Github repository: https://github.com/Sarwat/GeoSpark

ICDE 2016 Conference

Fig. 2: SRDD partitioning

users to write spatial data analytics programs. The SRDD layer
consists of three new RDDs: PointRDD, RectangleRDD and
PolygonRDD. One useful Geometrical operations library is
also provided for every spatial RDD.

Fig. 1: GeoSpark architecture

geometrical boundaries, (2) spatial (and geometric) operations
(e.g., Overlap, MinimumBoundingRectangle, Union) cannot be
easily and efﬁciently expressed using regular RDD transformations and actions. G EO S PARK extends RDDs to form Spatial
RDDs (SRDDs) and efﬁciently partitions SRDD data elements
across machines and introduces novel parallelized spatial (geometric operations that follows the Open Geosptial Consortium
(OGC) [6] standard) transformations and actions (for SRDD)
that provide a more intuitive interface for users to write spatial
data analytics programs. Moreover, G EO S PARK extends the
SRDD layer to execute spatial queries (e.g., Range query,
KNN query, and Join query) on large-scale spatial datasets.
The dynamic status of SRDDs and associated queries are visualized by G EO S PARK monitoring interface throughout each
entire spatial analytics process. We demonstrate G EO S PARK
using three applications: (1) Application 1 uses G EO S PARK
to calculate geospatial autocorrelation in a spatial dataset,
(2) Application 2 leverages the system to generate a heat map
of the San-Francisco trees population, and (3) Application 3
executes a spatial co-location pattern mining with the help of
G EO S PARK .
II.

G EO S PARK

ARCHITECTURE

As depicted in Figure 1, G EO S PARK consists of three
main layers: (1) Apache Spark Layer: that consists of regular
operations that are natively supported by Apache Spark. These
native functions are responsible for loading / saving data from
/ to persistent storage (e.g., stored on local disk or Hadoop
ﬁle system HDFS). (2) Spatial Resilient Distributed Dataset
(SRDD) Layer (Section II-A). (3) Spatial Query Processing
Layer (Section II-B).
A. Spatial RDD (SRDD) Layer
This layer extends Spark with spatial RDDs (SRDDs)
that efﬁciently partition SRDD data elements across machines
and introduces novel parallelized spatial transformations and
actions (for SRDD) that provide a more intuitive interface for

Spatial Objects Support. G EO S PARK supports various
spatial data input format (e.g., Comma Separated Value,
Tab Separated Value and Well-Known Text). Each type of
spatial objects is stored in a SRDD, PointRDD, RectangleRDD or PolygonRDD. G EO S PARK provides a set of geometrical operations which is called Geometrical Operations
Library. This library natively supports geometrical operations. For example, Overlap(): Finds all of the internal objects which are intersected with others in geometry;
MinimumBoundingRectangle(): Finds the minimum
bounding rectangles for each object in a Spatial RDD or return
a large minimum bounding rectangle which contains all of
the internal objects in a Spatial RDD; Union(): Returns the
union polygon of all polygons in this RDD.
SRDD Partitioning. G EO S PARK automatically partitions
all loaded Spatial RDDs by creating one global grid ﬁle for
data partitioning. The main idea for assigning each element in
a Spatial RDD to the same 2-Dimensional spatial grid space
is as follows: Firstly, split the spatial space into a number of
non-equal grid cells which compose a global grid ﬁle. This
global grid ﬁle has load balanced grids according to presampling techniques. Then traverse each element in the SRDD
and assign this element to a grid cell if the element overlaps
with this grid cell. If one element intersects with two or more
grid cells, then duplicate this element and assign different grid
IDs to the copies of this element. Figure 2 depicts tweets in
the U.S. at a particular moment, tweets and states are assigned
to respective grid cells.
SRDD Indexing. Spatial indexes like Quad-Tree and RTree are provided in Spatial IndexRDDs which inherit from
Spatial RDDs. Users are able to initialize a Spatial IndexRDD.
Moreover, G EO S PARK adaptively decides whether a local
spatial index should be created for a certain Spatial IndexRDD
partition based on a tradeoff between the indexing overhead
(memory and time) on one-hand and the query selectivity as
well as the number of spatial objects on the other hand.

1411

B. Spatial Query Processing Layer
This layer supports spatial queries (e.g., Range query and
Join query) for large-scale spatial datasets. After geometrical
objects are stored and processed in the Spatial RDD layer, user
may invoke a spatial query provided in Spatial Query Processing Layer. G EO S PARK processes such query and returns the
ﬁnal results to the user. G EO S PARK execution model implements the algorithms proposed by [7] and [8]. To accelerate a
spatial query, G EO S PARK leverages the grid partitioned Spatial
RDDs, spatial indexing, the fast in-memory computation and
DAG scheduler of Apache Spark to parallelize the query
execution.
Spatial Range Query. G EO S PARK executes the spatial
range query algorithm following the execution model: Load
target dataset, partition data, create a spatial index on each
SRDD partition if necessary, broadcast the query window
to each SRDD partition, check the spatial predicate in each
partition, and remove spatial objects duplicates that existed
due to the data partitioning phase.
Spatial Join Query. G EO S PARK executes the parallel
spatial join query following the execution model. GeoSpark
ﬁrst partitions the data from the two input SRDDs as well
as creates local spatial indexes (if required) for the SRDD
which is being queried. Then it joins the two datasets by their
keys which are grid IDs. For the spatial objects (from the
two SRDDs) that have the same grid ID, GeoSpark calculates
their spatial relations. If two elements from two SRDDS are
overlapped, they are kept in the ﬁnal results. The algorithm
continues to group the results for each rectangle. The grouped
results are in the following format: Rectangle, Point, Point,
... Finally, the algorithm removes the duplicated points and
returns the result to other operations or saves the ﬁnal result
to disk.
Spatial KNN Query. To process a Spatial KNN query,
G EO S PARK uses a heap based top-k algorithm[9], which
contains two phases: selection and merge. It takes a partitioned
SRDD, a point
and a number
as inputs. To calculate
the nearest objects around point , in the selection phase,
for each SRDD partition G EO S PARK calculates the distances
between each object to the given point , then maintains
a local heap by adding or removing elements based on the
distances. This heap contains the nearest objects around the
given point . For IndexedSRDD, the system can utilize the
local indexes to reduce the query time. After the selection
phase, G EO S PARK merges results from each partition, keeps
the nearest elements that have the shortest distances to
and outputs the result.
III.

D EMONSTRATION

SCENARIOS

We demonstrate G EO S PARK using three spatial applications which are described below. G EO S PARK provides a
monitoring map interface for system users to visualize and
monitor the spatial program dynamically. A screenshot of this
tool is provided in Figure 3. The interface allows users to
execute Scala code interactively through an integrated Scale
shell. Meanwhile, a map on-top of the shell visualizes the
SRDDs generated by Scala code. Throughout the entire spatial
analytics process, all generated SRDDs are listed on the left
side pane of the user interface. When the user clicks on any

SRDD partition (if it is still alive) on the left pane, she obtains
more detailed information from a nested menu such as the
data size in this partition, physical machine IP address, CPU
and memory utilization. Besides the description of SRDDs,
the tool also provides the status of a running spatial program
in a progress bar format. By browsing G EO S PARK Monitoring
Tool, users can interactively monitor the run time of their entire
spatial analytics program.
A. Application 1: Spatial Autocorrelation
Spatial autocorrelation studies whether neighbor spatial
data points might have correlations in some non-spatial attributes. Moran’s I and Geary’s C are two common coefﬁcients in spatial autocorrelation. Based on them, analysts can
determine whether these objects inﬂuence each other. These
efﬁcients are deﬁned by two speciﬁc formulas correspondingly.
An important part of these formulas is to ﬁnd the spatial
adjacent matrix. In this matrix, each tuples stands for whether
two objects, such as points, rectangles or polygons, are within
a speciﬁed distance.
An application programmer may leverage G EO S PARK
SpatialJoinQuery() to calculate the spatial adjacent
matrix. Assume one dataset is composed of millions of point
objects. The process to ﬁnd the global adjacent matrix in
G EO S PARK is as as follows: (1) Call G EO S PARK PointRDD
initialization method to store the dataset in memory. Data
partitioning and indexing are also completed by G EO S PARK
at this stage. (2) Call G EO S PARK SpatialJoinQuery()
in PointRDD. The ﬁrst parameter is the query point set itself
and the second one is the speciﬁed distance. (3) Use a new
instance of Spatial PairRDD to store the result of Step (2).
Step (2) will return the whole point set which has a new
column specify the neighbors of each tuple within the distance.
The expected schema is like this: Point coordinates (longitude,
latitude), neighbor 1 coordinates (longitude, latitude), neighbor
2 coordinates (longitude, latitude), ... (4) Call persistence
method in G EO S PARK to persist the resulting PointRDD.
B. Application 2: Spatial Aggregation
Assume an environmental scientist – studying the relationship between air quality and trees – would like to explore the
trees population in San Francisco. A query may leverage the
SpatialRangeQuery() provided by G EO S PARK to just
return all trees in San Francisco. Alternatively, a heat map
(spatial aggregate) that shows the distribution of trees in San
Francisco may be also helpful. This spatial aggregate query
(i.e., heat map) needs to count all trees at every single region
over the map.
In the heat map case, in terms of spatial queries, the
heat map is a spatial join in which the target set is the tree
map in San Francisco and the query area set is a set of
San Francisco regions. The region number depends on the
display resolution, or granularity, in the heat map. One proper
G EO S PARK program is as follows: (3) Use a Spatial PairRDD
to store the result of Step (2) which is the count for each
polygon. The Spatial PairRDD follows the schema like this:
(Polygon, count) such that Polygon represents the boundaries
of the spatial region. (4) Call persistence method in Spark to
persist the result PolygonRDD.

1412

Fig. 3: G EO S PARK Monitoring Tool

C. Application 3: Spatial Co-location
Spatial co-location is deﬁned as two or more species are often located in a neighborhood relationship. The determination
of this co-location pattern may beneﬁt many further scientiﬁc
researches. Biologists may ﬁnd symbiotic relationships, mobile
carriers can provide proper plans based users’ co-location, and
advertising agencies are able to place directed advertisements
at the center of co-located populations. For instance, one
existing co-location pattern is that one kind of tigers always
live within a certain distance from one kind of rabbits. Thus
we may infer one possible fact that these tigers feed on these
rabbits.
Some co-efﬁcients are applied to determine the co-location
relationship. Ripley’s K function [10] is the most common one
in real life. It usually executes numerous times iteratively and
ﬁnds the ideal distance. The calculation of K function also
needs the adjacent matrix between two type of objects. As we
mentioned in spatial autocorrelation analysis, seeking adjacent
matrix may leverage G EO S PARK SpatialJoinQuery().
Programmer are able to follow the same procedure depicted in
Spatial Autocorrelation.
Furthermore, spatial co-location, different from the previous basic spatial applications, is able to maximize the in memory computation goodness of G EO S PARK . Under G EO S PARK
framework, users only need to spend time on loading data,
partitioning data, and constructing indexes in the ﬁrst iteration
and then G EO S PARK automatically caches these intermediate
data in memory. In the next numerous iterations, users are

able to directly keep mining the co-location pattern using the
cache in memory instead of loading and pre-processing data
from scratch.
R EFERENCES
[1] A. Aji, F. Wang, H. Vo, R. Lee, Q. Liu, X. Zhang, and J. H. Saltz,
“Hadoop-GIS: A High Performance Spatial Data Warehousing System
over MapReduce,” Proceedings of the VLDB Endowment, PVLDB,
vol. 6, no. 11, pp. 1009–1020, 2013.
[2] A. Eldawy and M. F. Mokbel, “A demonstration of spatialhadoop: An
efﬁcient mapreduce framework for spatial data,” Proceedings of the
VLDB Endowment, PVLDB, vol. 6, no. 12, pp. 1230–1233, 2013.
[3] “Spark,” https://spark.apache.org.
[4] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauly,
M. J. Franklin, S. Shenker, and I. Stoica, “Resilient Distributed Datasets:
A Fault-Tolerant Abstraction for In-Memory Cluster Computing,” in
Proceedings of the USENIX Symposium on Networked Systems Design
and Implementation, NSDI, 2012, pp. 15–28.
[5] J. Yu, J. Wu, and M. Sarwat, “Geosaprk: A cluster computing framework for processing large scale spatial data,” in Proceedings of ACM
SIGSPATIAL GIS, 2015.
[6] “Open Geospatial Consortium,” http://www.opengeospatial.org/.
[7] G. Luo, J. F. Naughton, and C. J. Ellmann, “A non-blocking parallel
spatial join algorithm,” in Data Engineering, 2002. Proceedings. 18th
International Conference on. IEEE, 2002, pp. 697–705.
[8] X. Zhou, D. J. Abel, and D. Truffet, “Data partitioning for parallel
spatial join processing,” Geoinformatica, vol. 2, no. 2, pp. 175–204,
1998.
[9] N. Roussopoulos, S. Kelley, and F. Vincent, “Nearest neighbor queries,”
in ACM sigmod record, vol. 24, no. 2. ACM, 1995, pp. 71–79.
[10] B. D. Ripley, Spatial statistics. John Wiley & Sons, 2005, vol. 575.

1413

Two Birds, One Stone: A Fast, yet Lightweight, Indexing
Scheme for Modern Database Systems
1

Jia Yu

2

Mohamed Sarwat

School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, 699 S. Mill Avenue, Tempe, AZ
1

jiayu2@asu.edu, 2 msarwat@asu.edu

ABSTRACT

Table 1: Index overhead and storage dollar cost

Classic database indexes (e.g., B+ -Tree), though speed up
queries, suﬀer from two main drawbacks: (1) An index usually yields 5% to 15% additional storage overhead which results in non-ignorable dollar cost in big data scenarios especially when deployed on modern storage devices. (2) Maintaining an index incurs high latency because the DBMS has
to locate and update those index pages aﬀected by the underlying table changes. This paper proposes Hippo a fast,
yet scalable, database indexing approach. It significantly
shrinks the index storage and mitigates maintenance overhead without compromising much on the query execution
performance. Hippo stores disk page ranges instead of tuple
pointers in the indexed table to reduce the storage space occupied by the index. It maintains simplified histograms that
represent the data distribution and adopts a page grouping technique that groups contiguous pages into page ranges
based on the similarity of their index key attribute distributions. When a query is issued, Hippo leverages the page
ranges and histogram-based page summaries to recognize
those pages such that their tuples are guaranteed not to satisfy the query predicates and inspects the remaining pages.
Experiments based on real and synthetic datasets show that
Hippo occupies up to two orders of magnitude less storage
space than that of the B+ -Tree while still achieving comparable query execution performance to that of the B+ -Tree
for 0.1% - 1% selectivity factors. Also, the experiments show
that Hippo outperforms BRIN (Block Range Index) in executing queries with various selectivity factors. Furthermore,
Hippo achieves up to three orders of magnitude less maintenance overhead and up to an order of magnitude higher
throughput (for hybrid query/update workloads) than its
counterparts.

1.

(a) B+ -Tree overhead
TPC-H
2 GB
20 GB
200 GB

Index size
0.25 GB
2.51 GB
25 GB

Initialization time
30 sec
500 sec
8000 sec

Insertion time
10 sec
1180 sec
42000 sec

(b) Storage dollar cost
HDD
0.04 $/GB

EnterpriseHDD
0.1 $/GB

SSD
0.5 $/GB

EnterpriseSSD
1.4 $/GB

table. Even though classic database indexes improve the
query response time, they face the following challenges:
• Indexing Overhead: A database index usually
yields 5% to 15% additional storage overhead. Although the overhead may not seem too high in small
databases, it results in non-ignorable dollar cost in big
data scenarios. Table 1a depicts the storage overhead
of a B+ -Tree created on the Lineitem table from the
TPC-H benchmark [6] (database size varies from 2, 20
and 200 GB). Moreover, the dollar cost increases dramatically when the DBMS is deployed on modern storage devices (e.g., Solid State Drives and Non-Volatile
Memory) because they are still more than an order of
magnitude expensive than Hard Disk Drives (HDDs).
Table 1b lists the dollar cost per storage unit collected
from Amazon.com and NewEgg.com. In addition, initializing an index may be a time consuming process
especially when the index is created on a large table
(see Table 1a).
• Maintenance Overhead: A DBMS must update the
index after inserting (deleting) tuples into (from) the
underlying table. Maintaining a database index incurs high latency because the DBMS has to locate and
update those index entries aﬀected by the underlying
table changes. For instance, maintaining a B+ -Tree
searches the tree structure and perhaps performs a set
of tree nodes splitting or merging operations. That
requires plenty of disk I/O operations and hence encumbers the time performance of the entire DBMS in
big data scenarios. Table 1a shows the B+ Tree insertion overhead (insert 0.1% records) for the TPC-H
Lineitem table.

INTRODUCTION

A database system (DBMS) often employs an index structure, e.g., B+ -Tree, to speed up queries issued on the indexed

This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivatives 4.0 International License. To view a copy
of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For
any use beyond those covered by this license, obtain permission by emailing
info@vldb.org.
Proceedings of the VLDB Endowment, Vol. 10, No. 4
Copyright 2016 VLDB Endowment 2150-8097/16/12.

385

Existing approaches that tackle one or more of the aforementioned challenges are classified as follows: (1) Compressed indexes: Compressed B+ -Tree approaches [8, 9, 21]
reduce the storage overhead but compromise on the query
performance due to the additional compression and decompression time. Compressed bitmap indexes also reduce index
storage overhead [10, 12, 16] but they mainly suit low cardinality attributes which are quite rare. For high cardinality
attributes, the storage overhead of compressed bitmap indexes significantly increases [19]. (2) Approximate indexes:
An approximate index [4, 11, 14] trades query accuracy for
storage to produce smaller, yet fast, index structures. Even
though approximate indexes may shrink the storage size,
users cannot rely on their un-guaranteed query accuracy in
many accuracy-sensitive application scenarios like banking
systems or user archive systems. (3) Sparse indexes: A
sparse index [5, 13, 17, 18] only stores pointers which refer to disk pages and value ranges (min and max values)
in each page so that it can save indexing and maintenance
overhead. It is generally built on ordered attributes. For
a posed query, it finds value ranges which cover or overlap
the query predicate and then rapidly inspects the associated
few parent table pages one by one for retrieving truly qualified tuples. However, for unordered attributes which are
much more common, sparse indexes compromise too much
on query performance because they find numerous qualified
value ranges and have to inspect a large number of pages.
This paper proposes Hippo a fast, yet scalable, sparse
database indexing approach. In contrast to existing tree index structures, Hippo stores disk page ranges (each works
as a pointer of one or many pages) instead of tuple pointers
in the indexed table to reduce the storage space occupied by
the index. Unlike existing approximate indexing methods,
Hippo guarantees the query result accuracy by inspecting
possible qualified pages and only emitting those tuples that
satisfy the query predicate. As opposed to existing sparse
indexes, Hippo maintains simplified histograms that represent the data distribution for pages no matter how skew it is,
as the summaries for these pages in each page range. Since
Hippo relies on histograms already created and maintained
by almost every existing DBMS (e.g., PostgreSQL), the system does not exhibit a major additional overhead to create
the index. Hippo also adopts a page grouping technique
that groups contiguous pages into page ranges based on the
similarity of their index key attribute distributions. When a
query is issued on the indexed database table, Hippo leverages the page ranges and histogram-based page summaries
to recognize those pages for which the internal tuples are
guaranteed not to satisfy the query predicates and inspects
the remaining pages. Thus Hippo achieves competitive performance on common range queries without compromising
the accuracy. For data insertion and deletion, Hippo dispenses with the numerous disk operations by rapidly locating the aﬀected index entries. Hippo also adaptively decides
whether to adopt an eager or lazy index maintenance strategy to mitigate the maintenance overhead while ensuring
future queries are answered correctly.
We implemented a prototype of Hippo inside PostgreSQL
9.51 . Experiments based on the TPC-H benchmark as well
as real and synthetic datasets show that Hippo occupies up
to two orders of magnitude less storage space than that of
1

Figure 1: Initialize and search Hippo on age table
the B+ -Tree while still achieving comparable query execution performance to that of the B+ -Tree for 0.1% - 1% selectivity factors. Also, the experiments show that Hippo
outperforms BRIN, though occupies more storage space, in
executing queries with various selectivity factors. Furthermore, Hippo achieves up to three orders of magnitude less
maintenance overhead than its counterparts, i.e., B+ -Tree
and BRIN. Most importantly, Hippo exhibits up to an order of magnitude higher throughput (measured in terms of
the number of tuples processed per second) than both BRIN
and B+ -Tree for hybrid query/update workloads.
The rest of the paper is structured as follows: In Section 2,
we explain the key idea behind Hippo and describe the index structure. We explain how Hippo searches the index,
builds the index from scratch, and maintains it eﬃciently in
Sections 3, 4 and 5. In Section 6, we deduce a cost model
for Hippo. Extensive experimental evaluation is presented
in Section 7. We summarize a variety of related indexing
approaches in Section 8. Finally, Section 9 concludes the
paper and highlights possible future directions.

2. HIPPO OVERVIEW
This section gives an overview of Hippo. Figure 1 depicts
a running example that describes the index initialization
(left part of the figure) and search (right part of the figure)
processes in Hippo. The main challenges of designing an index are to reduce the indexing overhead in terms of storage
and initialization time as well as speed up the index maintenance while still keeping competitive query performance.
To achieve that, an index should possess the following two
main properties: (1) Less Index Entries: For better storage
space utilization, an index should determine and only store
the most representative index entries that summarize the
key attribute. Keeping too many index entries inevitably
results in high storage overhead as well as high initialization
time. (2) Index Entries Independence: The index entries

https://github.com/DataSystemsLab/hippo-postgresql

386

Index Entries Sorted List

Summarized
Page Range
StartPageID EndPageID

Histogram-based
Summary
Bit 1

…

Bit 2

Algorithm 1: Hippo index search

Bit b

1

1
2

2
.
.
.

StartPageID EndPageID

Bit 1

…

Bit 2

3

Bit b

4
5

.
.
.

.
.
.

6
7
8

n
StartPageID EndPageID

Bit 1

Bit 2

…

9

Bit b

10
11

Figure 2: Hippo Index Structure

12
13
14
15

should be independent from each other. In other words, the
range of values that each index entry represents should have
minimal overlap with other index entries. Interdependence
among index entries, like that in a B+ -Tree, results in overlapped tree nodes. That may lead to more I/O operations
during query processing and several cascaded updates during index maintenance.
Data Structure. Figure 2 depicts the index structure.
To create an index, Hippo scans the indexed table and generates histogram-based summaries for a set of disk page based
on the index key attribute. These summaries are then stored
by Hippo along with page ranges they summarize. As shown
in Figure 2, Hippo consists of n index entries such that each
entry consists of the following two components:

16
17
18

Data: A given query predicate Q
Result: A set of qualified tuples R
// Step I: Scanning Index Entries;
Set of Possible Qualified Pages P = φ;
foreach Index Entry in Hippo do
if the partial histogram has joint buckets with Q then
Add the IDs of pages indexed by the entry to P ;
end
end
// Step II: Filtering False Positive Pages;
Set of Qualified Tuples R = φ;
foreach Page ID ∈ P do
Retrieve the corresponding page p;
foreach tuple t ∈ p do
if t satisfies the query predicate then
Add t to R;
end
end
end
return R;

only stores two page IDs and a compressed bitmap.(2) Each
page of the parent table is only summarized by one Hippo
index entry. Hence, any updates that occur in a certain
page only aﬀect a single independent index entry. Finally,
during a query, pages whose partial histograms do not have
desired buckets are guaranteed not to satisfy certain query
predicates and marked as false positives. Thus Hippo only
inspects other pages that probably satisfies the query predicate and achieves competitive query performance.

• Summarized Page Range: represents the IDs of
the first and last pages summarized (i.e., StartPageID
and EndPageID in Figure 2) by the index entry. The
DBMS can load particular pages into buﬀer according to their IDs. Hippo summarizes more than one
physically contiguous pages to reduce the overall index size, e.g., Page 1 - 10, 11 - 25, 26 - 30 in Figure 1.
The number of summarized pages in each index entry
varies. Hippo adopts a page grouping technique that
groups contiguous pages into page ranges based on the
similarity of their index attribute distributions, using
the partial histogram density (explained in Section 4).

3. INDEX SEARCH
The search algorithm takes as input a query predicate
and returns a set of qualified tuples. As explained in Section 2, partial histograms are stored in a bitmap format.
Hence, any query predicates for a particular attribute are
broken down into atomic units: equality query predicate
and range query predicate. Each unit predicate is compared
with the buckets of the complete height balanced histogram
(discussed in Section 4). A bucket is hit by a predicate if
the predicate fully contains, overlaps, or is fully contained
by the bucket. Each unit predicate can hit at least one or
more buckets. Afterwards, the query predicate is converted
to a bitmap. Each bit in this bitmap reflects whether the
bucket that has the corresponding ID is hit (1) or not (0).
Thus, the corresponding bits of all hit buckets are set to 1.
The search algorithm then runs in two main steps (see
pseudo code in Algorithm 1): (1) Step I: Scanning Hippo
index entries and (2) Step II: Filtering false positive pages.
The search process leverages the index structure to avoid
unnecessary page inspection so that Hippo can achieve competitive query performance.

• Histogram-based Summary: A bitmap that represents a subset of the complete height balanced
histogram buckets (maintained by the underlying
DBMS), aka. partial histogram. Each bucket, if exists, indicates that at least one of the tuples of this
bucket exists in the summarized pages. Each partial
histogram represents the distribution of the data in
the summarized contiguous pages. Since each bucket
of a height balanced histogram roughly contains the
same number of tuples, each of them has the same
probability to be hit by a random tuple from the table. Hippo leverages this feature to handle a variety
of data distributions, e.g., uniform, skewed. To reduce
the storage footprint, only bucket IDs are kept in partial histograms and partial histograms are stored in a
compressed bitmap format. For instance, the partial
histogram of the first index entry in Figure 1 is 01110.

3.1 Step I: Scanning index entries
Step I finds possible qualified disk pages, which may contain tuples that satisfy the query predicate. Since it is quite
possible that some pages may not contain any qualified tuple especially for highly selective queries, Hippo prunes these
index entries (that index these unqualified pages) that definitely do not contain any qualified pages.
In this step, the search algorithm scans the Hippo index.
For each index entry, the algorithm retrieves the partial his-

Main idea. Hippo solves the aforementioned challenges
as follows: (1) Each index entry summarizes many pages and

387

Hippo finds that query predicate age = 55 hits bucket 3.
Since the first one of the three partial histograms nicely
contains bucket 3, only the disk pages 1 - 10 are selected as
possible qualified pages and hence sent for further inspection in step II. It is also worth noting that these partial
histograms summarize diﬀerent number of pages.
Figure 3: Scan index entries

4. INDEX INITIALIZATION
To create an index, Hippo takes as input a database table and the key attribute (i.e., column) in this table. Hippo
then performs two main steps (See pseudo code in Algorithm 2) to initialize itself: (1) Generate partial histograms
(Section 4.1), and (2) Group similar pages into page ranges
(Section 4.2), described as follows.

togram which summarizes the data distribution in the pages
indexed by such entry. The algorithm then checks whether
the input query predicate has one or more joint (i.e. overlapped) buckets with the partial histogram. To eﬃciently
process that, Hippo performs a nested loop between each
partial histogram and the input query predicate to find the
joint buckets. Since both the partial histograms and the
query predicate are in a bitmap format, Hippo accelerates
the nested loop by performing a bitwise ’AND’ of the bytes
from both sides, aka. bit-level parallelism. In case bitwise
’AND’ing the two bytes returns 0, that means there exist no
joint buckets between the query predicate and the partial
histogram. Entries with partial histograms that do not contain the hit buckets (i.e., the corresponding bits are 0) are
guaranteed not to contain any qualified disk pages. Hence,
Hippo disqualifies these pages and excludes them from further processing. On the other hand, index entries with partial histograms that contain at least one of the hit buckets,
i.e., the corresponding bits are 1, may or may not have qualified pages. Hippo deems these pages as possible qualified
pages and hence forwards their IDs to the next step.
Figure 3 visualizes the procedure of scanning index entries
according to their partial histograms. In Figure 3, buckets
hit by the query predicates and the partial histogram are
represented in a bitmap format. According to this figure,
the partial histogram misses a query predicate if the highlighted area of the predicate falls into the blank area of the
partial histogram, whereas a partial histogram is selected if
the predicate does not fall completely into the blank area of
the histogram.

4.1 Generate partial histograms
To initialize the index, Hippo leverages a complete height
balanced histogram, maintained by most DBMSs, that represents the data distribution. A histogram consists of a set
of buckets such that each bucket represents the count of tuples with attribute value within the bucket range. A partial
histogram only contains a subset of the buckets that belongs to the height balanced histogram. The resolution of
the complete histogram (H) is defined as the total number
of buckets that belongs to this histogram. A histogram will
obviously have larger physical storage size if it has higher
resolution. The histogram resolution also aﬀects the query
response time (see Section 6 for further details).
Hippo stores a partial histogram for each index entry to
represent the data distribution of tuples in one or many disk
pages summarized by the entry. Partial histograms allow
Hippo to early identify unqualified disk pages and avoid unnecessary page inspection. To generate partial histograms,
Hippo scans all disk pages of the indexed table. For each
page, the algorithm retrieves each tuple, the key attribute
value is extracted from each tuple and then compared to
the complete histogram using binary search. Buckets hit by
tuples are kept for this page and then compose a partial histogram. A partial histogram only contains distinct buckets.
For instance, there is a group of age attribute values like
the first entry of Hippo given in Figure 1: 21, 22, 55, 75,
77. Bucket 2 is hit by 21 and 22, bucket 3 is hit by 55 and
bucket 4 is hit by 77 (see partial histogram 1 in Figure 1).
Hippo shrinks the storage footprint of partial histograms
by dropping all bucket value ranges and only keeping bucket
IDs. Actually, as mentioned in Section 2, dropping value
range information does not have much negative impact on
the index search. To further shrink the storage footprint,
Hippo stores the histogram bucks IDs in bitmap type format instead of using an integer type (4 bytes or more). Each
partial histogram is stored as a bitmap such that each bit
represents a bucket at the same position in a complete histogram. Bit value 1 means the associated bucket is hit and
stored in this partial histogram while 0 means the associated bucket is not included. The partial histogram can also
be compressed by any existing bitmap compression technique. The time for compressing and decompressing partial
histograms is ignorable compared to that of inspecting possible qualified pages.

3.2 Step II: Filtering false positive pages
The previous step identifies many unqualified disk pages
that are guaranteed not to satisfy the query predicate. However, not all unqualified pages can be detected by the previous step. The set of possible qualified pages, retrieved
from Step I, may still contain false positives (defined below). During the search process, Hippo considers a possible
qualified page p a false positive if and only if (1) p lies in
the page range summarized by a qualified index entry from
Step I and (2) p does not contain any tuple that satisfies
the input query predicate. To filter out false positive pages,
Step II inspects every tuple in each possible qualified page,
retrieves those tuples that satisfy the query predicate, and
finally returns those tuples as the answer.
Step II takes as input the set of possible qualified pages
IDs, formatted in a separate bitmap. Each bit in this bitmap
is mapped to the page at the same position in the original
table indexed by Hippo. For each page ID, Hippo retrieves
the corresponding page from disk and checks each tuple in
that page against the query predicate. In case, a tuple satisfies the query predicate, the algorithm adds this tuple to
the final result set. The right part of Figure 1 describes how
to search the index using an input query predicate. First,

4.2 Group pages into page ranges
Generating a partial histogram for each disk page may
lead to very high storage overhead. Grouping contiguous

388

Algorithm 2: Hippo index initialization

1
2
3
4
5
6

7

8
9
10
11
12
13
14

Algorithm 3: Update Hippo for data insertion

Data: Pages of a parent table
Result: Hippo index
Create a working partial histogram (in bitmap format);
Set StartPage = 1 and EndPage = 1;
foreach page do
Find distinct buckets hit by its tuples;
Set associated bits to 1 in the partial histogram;
if the working partial histogram density > threshold
then
Store the partial histogram and the page range
(StartPage and EndPage) as an index entry;
Create a new working partial histogram;
StartPage = EndPage + 1;
EndPage = StartPage;
else
EndPage = EndPage + 1;
end
end

1
2
3
4
5
6
7
8
9
10
11
12
13
14

means of the partial histogram density. The basic idea is
that new pages will not be summarized into a partial histogram if its density is larger than the threshold and a new
partial histogram will be created for the following pages.
The left part of Figure 1 depicts how the initialization
process for an index create on the age attribute. In the
example, the partial histogram density is set to 0.6. All
tuples are compared with the complete histogram and IDs
of distinct buckets hit by all tuples are generated as partial
histograms along with their page range. So far, as Figure 1
and 2 shows, each index entry has the following parameters:
a partial histogram in compressed bitmap format and two
integers that stand for the first and last pages summarized
by this histogram (summarized page range). Each entry is
then serialized and stored on disk.

pages and merging their partial histograms into a larger
partial histogram (in other words, summarizing more pages
within one partial histogram) can tremendously reduce the
storage overhead. However, that does not mean that all
pages should be grouped together and summarized by a single merged partial histogram. The more pages are summarized, the more buckets the partial histogram contains.
If the partial histogram becomes a complete histogram and
covers any possible query predicates, it is unable to filter the
false positives and the disk pages summarized by this partial
histogram will be always treated as possible qualified pages.
One strategy is to group a fixed number of contiguous
pages per partial histogram. Yet, this strategy is not eﬃcient when a set of contiguous pages have much more similar
data distribution than other areas. To remedy that, Hippo
dynamically groups more contiguous pages under the same
index entry when they possess similar data distribution and
less contiguous pages if they do not show similar data distribution. To take the page grouping decision, Hippo leverages
a parameter called partial histogram density. The density of
a partial histogram is defined as the ratio of complete histogram buckets that belongs to the partial histogram. Obviously, the complete histogram has a density value of 1. The
definition can be formalized as follows:
P artial histogram density (D) =

Data: A newly inserted tuple that belongs to Page a
Result: Updated Hippo
Find the bucket hit by the inserted tuple;
Locate a Hippo index entry which summarizes Page a;
if an index entry is located
then
Fetch the located Hippo index entry;
Update the retrieved entry if necessary;
else
Retrieve the entry that summarizes the last page;
if the partial histogram density < threshold then
Summarize Page a into the retrieved index entry;
else
Summarize Page a into a new index entry;
end
end

5. INDEX MAINTENANCE
Inserting (deleting) tuples into (from) the table requires
maintaining the index. That is necessary to ensure that
the DBMS can retrieve the correct set of tuples that match
the query predicate. However, the overhead introduced by
frequently maintaining the index may preclude system scalability. This section explains how Hippo handles updates.

5.1 Data insertion

# Bucketspartial histogram
# Bucketscomplete histogram

The density exhibits an important phenomenon that, for
a set of contiguous disk pages, their merged partial histogram density will be very low if these pages are very similar, and vice versa. Therefore, a partial histogram with
a certain density may summarize more pages if these contiguous pages have similar data, vice versa. Making use of
this phenomenon enables Hippo to dynamically group pages
and merge partial histograms into one. In addition, it is understandable that a lower density partial histogram (summarizes less pages) has the high probability to be excluded
from further processing.
Users can easily set the same density value for all partial histograms as a threshold. Hippo can automatically
decide how many pages each partial histogram should summarize. Algorithm 2 depicts how Hippo initializes the index
and summarizes more pages within a partial histogram by

389

Hippo adopts an eager update strategy when a new tuple
is inserted to the indexed table. An eager strategy instantly
updates or checks the index at least when a new tuple is
inserted. Otherwise, all subsequent queries might miss the
newly inserted tuple. Data insertion may change the physical structure of a table (i.e., heap file). The new tuple may
belong to any pages of the indexed table. The insertion
procedure (See Algorithm 3) performs the following steps:
(I) Locate the aﬀected index entry, and (II) Update the index entry if necessary.
Step I: Locate the aﬀected index entry: After
retrieving the complete histogram, the algorithm checks
whether a newly inserted tuple hits one or more of the histogram buckets. The newly inserted tuple belongs to a disk
page. This page may be a new page has not been summarized by any partial histograms before or an old page
which has been summarized. However, because the numbers of pages summarized by each histogram are diﬀerent,
searching Hippo index entries to find the one contains this

target page is inevitable. From the perspective of disk storage, in a Hippo, all partial histograms are stored on disk in
a serialized format. It will be extremely time-consuming if
every entry is retrieved from disk, de-serialized and checked
against the target page. The algorithm then searches the
index entries by means of the index entries sorted list explained in Section 5.3.
Step II: Update the index entry: In case the inserted
tuple belongs to a new page and the partial histogram density which summarizes the last disk page is smaller than the
density threshold set by the system user, the algorithm summarizes the new page into this partial histogram in the last
index entry. Otherwise, the algorithm creates a new partial
histogram to summarize this page and stores them in a new
index entry. In case a new tuple belongs to a page that is
already summarized by Hippo, the partial histogram in the
associated index entry will be updated if the inserted tuple
hits a new bucket.
It is worth noting that: (1) Since the compressed bitmaps
of partial histograms may have diﬀerent size, the updated
index entry may not fit the space left at the old location.
Thus the updated one may be put at the end of Hippo.
(2) After some changes (replacing old or creating new index
entry) in Hippo, the corresponding position of the sorted list
might need to be updated.
The I/O cost incurred by eagerly updating the index due
to a newly inserted tuple is equal to log(# of index entries)
+ 4. Locating the aﬀected index entry yields log(# of index entries) I/Os, whereas Step II consumes 4 I/Os to update the index entry. Section 6 gives more details on how to
estimate the number of index entries in Hippo.

Updated Hippo

Sorted list
Pointer
Page #
Low
High

Page range Partial histogram Internal data
1 - 10
2,3,4
21,22,55,75,77
Blank space
11,12,25,101,110
26 - 30
1,2,5

…

…
11 – 25

1,2,4,5

Move

13,23,24,62,91,92

Figure 4: Hippo Index Entries Sorted List

index entry and then updates it. Since the index entries are
not guaranteed to be sorted based on the page IDs (noted in
data insertion section), an auxiliary structure for recording
the sorted order is introduced to Hippo.
The sorted list is initialized after all steps in Section 4
with the original order of index entries and put at the first
several index pages of Hippo. During the entire Hippo life
time, the sorted list maintains a list of pointers of Hippo
index entries in the ascending order of page IDs. Actually
each pointer represents the fixed size physical address of
an index entry and these addresses can be used to retrieve
index entries directly. That way, the premise of a binary
search has been satisfied. Figure 4 depicts the Hippo index
entries sorted list. Index entry 2 in Figure 1 has a new
bucket ID 1 due to a newly inserted tuple in its internal
data and hence this entry becomes the last index entry in
Figure 4. The sorted list is still able to record the ascending
order and help Hippo to perform a binary search on the
index entries. In addition, such sorted list leads to slight
additional maintenance overhead: Some index updates need
to modify the aﬀected pointers in the sorted list to reflect
the new physical addresses.

5.2 Data deletion
The eager update strategy is deemed necessary for data
insertion to ensure the correctness of future queries issued
on the indexed table. However, the eager update strategy
is not necessary after deleting data from the table. That is
due to the fact that Hippo inspects possible qualified pages
during the index search process and pages with qualified
deleted tuples might be still marked as possible qualified
page in the first phase of the search algorithm. Even if these
pages contain deleted tuples, such pages will be discarded
during the ”Step II: filtering false positive pages” phase of
the search algorithm. However, not maintaining the index
at all may introduce a lot of false positives during the search
process, which may takes its toll on the query repossess time.
Hippo still ensures the correctness of queries even if it
does not update the index at all after deleting tuples from a
table. To achieve that, Hippo adopts a periodic lazy update
strategy for data deletion. The deletion strategy maintains
the index after a bulk of delete operations are performed to
the indexed table. In such case, Hippo traverses all index
entries. For each index entry, the system inspects the header
of each summarized page for seeking notes made by DBMSs
(e.g., PostgreSQL makes notes in page headers if data is
removed from pages). Hippo re-summarizes the entire index
entry instantly within the original page range if data deletion
on one page is detected. The re-summarization follows the
same steps in Section 4.

6. COST MODEL
This section deduces a cost model for Hippo. Table 2
summarizes the main notations. Given a database table
R with a number of tuples Card and average number of
tuples per disk page pageCard, a user may create a Hippo
index on attribute (i.e., column) ai of R. Let the complete
histogram resolution be H (it has H buckets in total) and
the partial histogram density be D. Assume that each Hippo
index entry on average summarizes P data pages and T
tuples. Queries executed against the index have an average
selectivity factor SF . To calculate the query I/O cost, we
need to consider: (1) I/Oscanning index represents the cost of
scanning the index entries (Phase I in the search algorithm)
and (2) I/Of iltering f alse positives represents the I/O cost of
filtering false positive pages (Phase II).
Estimating the number of index entries. Since all
index entries are scanned in the first phase, the I/O cost
of this phase is equal to the total pages the index spans
index entries
on disk (I/Oscanning index = # of pageCard
). To estimate
the number of Hippo index entries, we have to estimate how
many disk pages (P ) are summarized by a partial histogram
in general, or how many tuples (T ) are checked against the
complete histogram to generate a partial histogram. This
problem is very similar to the Coupon Collector’s Problem[7]. This problem can be described like that: ”A vending machine sells H types of coupons (a complete histogram
with H buckets). Alice is purchasing coupons from this machine. Each time (each tuple) she can get a random type

5.3 Index Entries Sorted List
When a new tuple is inserted, Hippo executes a fast binary
search (according to the page IDs) to locate the aﬀected

390

Table 2: Notations used in Cost Estimation
Term

Definition
Complete histogram resolution which means the
number of buckets in this complete histogram
Average number of histogram buckets hit by a page

H
pageH

P
T
Card
pageCard

Average number of tuples per page
The selectivity factor of the issued query

coupon (a bucket) but she might already have a same one.
Alice keeps purchasing until she gets D ∗ H types of coupons
(distinct buckets). How many times (T ) does she need to
purchase?” Therefore, the expectation of T is determined by
the following equation:
T = H×(
= H×

1
1
1
+
+ ... +
)
H
H −1
H − D×H + 1

D×H−1
!
i=0

1
H−i

index

D×H−1
!
Card
1
×(
)−1
H×pageCard
H
−i
i=0

(5)

P rob = (Average buckets hit by a query predicate)×D
= SF ×H×D

(6)

To be precise, P rob follows a piecewise function as follows:

(1)

P rob =

"

SF ×H×D
1

SF ×H !
SF ×H >

1
D
1
D

Given the aforementioned discussion, we observe that
(1) when SF and H are fixed, the smaller D is, the smaller
P rob is. (2) when H and D are fixed, the smaller SF is, the
smaller P rob is. (3) when SF and D are fixed, the smaller
H is, the smaller P rob is. It is obvious that the probability
in Equation 6 is equivalent to the probability that pages in
an index entry are checked in the second phase, i.e., filtering false positive pages. Since the total pages in Table R
Card
, the mathematical expectation of the number of
is pageCard
pages in R checked by the second part, as known as the I/O
cost of second part, is:

(2)

Note that the partial histogram density D ∈ [ pageH
, 1].
H
That means the global density should be larger than the
ratio of average hit histogram buckets per page to all histogram buckets because page is the minimum unit when
grouping pages based on density. Estimating pageH is also
a variant of Coupon Collector’s Problem: How many types
of coupons (distinct buckets) will Alice get if she purchases
pageCard coupons (tuples)? Given Equation 2, the mathematical expectation of pageH can be easily found as follows:

I/Of iltering

1
pageH = H×(1 − (1 − )pageCard )
H

=

Estimating the number of read data pages. Data
pages summarized by each index entry are likely to be
checked in the second phase of the search algorithm, filtering false positive pages, if their associated partial histogram
has joint buckets with the query predicate. Determining
the probability of having joint buckets contributes to the
query I/O cost estimation. The probability that a partial
histogram in an index entry has joint buckets with a query
predicate depends on how likely a predicate overlaps with
the highlighted area in partial histograms (see Figure 3).
The probability is determined by the equation given below:

Partial histogram density, which is an user supplied
parameter
Average number of pages summarized by a partial
histogram for a certain attribute
Average number of tuples summarized by a partial
histogram for a certain attribute
Total number of tuples of the indexed table

D

SF

I/Oscanning

f alse positives

= (P rob×

(3)

Card
)
pageCard

(7)

By adding up I/Oscanning index (Equation 5) and
I/Of iltering f alse positives (See Equation 7), the total query
I/O cost is as follows:

The number of Hippo index entries is equivalent to the
total number of tuples in the indexed table divided by the
average number of tuples summarized by each index entry,
. Hence, the number of index entries is given in
i.e., Card
T
Equation 4. The index size is equal to the product of the
number of index entries and the size of a single entry. The
size of each index entry is roughly equal to each partial histogram size.

Query I/O =

Card
×((H×
pageCard

D×H−1
!
i=0

1
)−1 + P rob) (8)
H −i

7. EXPERIMENTS
# of Index entries = Card/(H×

D×H−1
!
i=0

1
)
H −i

This section provides a comprehensive experimental evaluation of Hippo. All experiments are run on an Ubuntu
Linux 14.04 64 bit machine with 8 cores CPU (3.5 GHz per
core), 32 GB memory, and 2 TB magnetic hard disk. We
install PostgreSQL 9.5 (128 MB default buﬀer pool) on the
test machine.
Compared Approaches. During the experiments, we
study the performance of the following indexing schemes:
(1) Hippo: A complete prototype of our proposed indexing approach implemented inside the core engine of PostgreSQL 9.5. Unless mentioned otherwise, the default partial

(4)

Given Equation 4, we observe the following: (1) For a
certain H, the higher the value of D, the less Hippo index
entries there exist. (2) For a certain D, the higher H there is,
the less Hippo index entries there are. Meanwhile, the size of
each index entry increases with the growth of the complete
histogram resolution. The final I/O cost of scanning the
index entries is given in Equation 5.

391

Table 3: Tuning Parameters

histogram density is set to 20% and the default histogram
resolution (H) is set to 400. (2) B+ -Tree: The default implementation of the B+ -Tree in PostgreSQL 9.5 (with a default
fill factor of 90), (3) BRIN: A sparse Block Range Index
implemented in PostgreSQL 9.5 with 128 default pages per
range. We also consider other BRIN settings, i.e., BRIN-32
and BRIN-512, with 32 and 512 pages per range respectively.
Datasets. We use the following four datasets:

Parameter

Value

Size

Initial.
time

Query
time

Default

D=20%
R=400
40%

1012 MB

2765 sec

2500 sec

680 MB

2724 sec

3500 sec

80%

145 MB

2695 sec

4500 sec

800

822 MB

2762 sec

3000 sec

1600

710 MB

2760 sec

3500 sec

Density
(D)
Resolution
(R)

• TPC-H : A 207 GB decision support benchmark that
consists of a suite of business oriented ad-hoc queries
and data modifications. Tables populated by TPCH follow a uniform data distribution. For evaluation
purposes, we build indexes on Linitem table PartKey,
SuppKey or OrderKey attribute. PartKey attribute
has 40 million distinct values while SuppKey has 2
million distinct values and the values of OrderKey attribute are sorted. For TPC-H benchmark queries, we
also build indexes on L Shipdate and L Receiptdate
when necessary.

does not really remove data from disk unless a VACUUM command is called automatically or manually. Hippo then updates the index for data deletion when a VACUUM command
is invoked. In addition, it is better to rebuild Hippo index if there is a huge change of the parent attribute’s histogram. Furthermore, a script, running as a background
process, drops the system cache during the experiments.

7.1 Tuning Hippo parameters
This section evaluates the performance of Hippo by tuning two main system parameters: partial histogram density
D (Default value is 20%) and complete histogram resolution H (Default value is 400). For these experiments, we
build Hippo on PartKey attribute in Lineitem table of 200
GB TPC-H benchmarks. We then evaluate the index size,
initialization time, and query response time.

• Exponential distribution synthetic dataset (abbr. Exponential): This 200 GB dataset consists of three
attributes, IncrementalID, RandomNumber, Payload.
RandomNumber attribute follows exponential data
distribution which is highly skewed.
• Wikipedia traﬃc (abbr.
Wikipedia) [2]: A 231
GB Wikipedia article traﬃc statistics covering seven
months period log. The log file consists of 4 attributes:
PageName, PageInfo, PageCategory, PageCount. For
evaluation purposes, we build index on the PageCount
attribute which stands for hourly page views.

7.1.1 Impact of partial histogram density
The following experiment compares the default Hippo
density (20%) with two diﬀerent densities (40% and 80%)
and tests their query time with selectivity factor 0.1%. As
given in Table 3, when we increase the density Hippo exhibits less indexing overhead as expected. That happens
due to the fact that Hippo summarizes more pages per partial histogram and write less index entries on disk. Similarly,
higher density leads to more query time because it is more
likely to overlap with query predicates and result in more
pages are selected as possible qualified pages.

• New York City taxi dataset (abbr. NYC Taxi) [1]:
This dataset contains 197 GB New York City Yellow
and Green Taxi trips published by New York City Taxi
& Limousine Commission website. Each record includes pick-up and drop-oﬀ dates/times, pick-up and
drop-oﬀ locations, trip distances, and itemized fares.
We reduce the dimension of pick-up location from 2D
(longitude, latitude) to 1D (integer) using a spatial dimension reduction method, Hilbert Curve, and build
indexes on pick-up location attribute.

7.1.2 Impact of histogram resolution
This section compares the default Hippo histogram resolution (400) to two diﬀerent histogram resolutions (800 and
1600) and tests their query time with selectivity factor 0.1%.
The density impact on the index size, initialization time and
query time is given in Table 3 .
As given in Table 3, with the growth of histogram resolution, Hippo size decreases moderately. The explanation is that higher histogram resolution leads to less partial histograms and each partial histogram in the index may
summarize more pages. However, the partial histogram (in
bitmap format) has larger physical size because the bitmap
has to store more bits.
As Table 3 shows, the query response time of Hippo varies
with the growth of histogram resolution. This is because for
the large histogram resolution, the query predicate may hit
more buckets so that this Hippo is more likely to overlap
with query predicates and result in more pages are selected
as possible qualified pages.

Implementation details. We have implemented a prototype of Hippo inside PostgreSQL 9.5 as one of the main
index access methods by leveraging the underlying interfaces which include but not limited to ”ambuild”, ”amgetbitmap”, ”aminsert” and ”amvacuumcleanup”. A PostgreSQL 9.5 user creates and queries the index as follows:
CREATE INDEX hippo_idx ON lineitem USING hippo(partkey)
SELECT * FROM lineitem
WHERE partkey > 1000 AND partkey < 2000
DROP INDEX hippo_idx

The final implementation has slight diﬀerences from the
aforementioned details due to platform-dependent features.
For instance, Hippo only records possible qualified page IDs
in a tid bitmap format and returns it to the kernel. PostgreSQL automatically inspects pages and checks each tuples
against the query predicate. PostgreSQL DELETE command

7.2 Indexing overhead
This section studies the indexing overhead (in terms of
index size and index initialization time) of the B+ -Tree,

392

Figure 5: Index size on diﬀerent datasets (log. scale)

(a) TPC-H

(b) Exponential

Figure 6: Index initial. time on diﬀerent datasets

(c) Wikipedia

(d) NYC Taxi

Figure 7: Query response time at diﬀerent selectivity factors
sponding value ranges (min and max values). Among diﬀerent versions of BRIN, BRIN-32 exhibits the largest storage
overhead while BRIN-512 shows the lowest storage overhead
because the latter can summarize more pages per entry.
On the other hand, as Figure 6 depicts, Hippo and BRIN
consume less time to initialize the index as compared to
the B+ -Tree. That is due to the fact that the B+ -Tree has
numerous index entries (tree nodes) stored on disk while
Hippo and BRIN have just a few index entries. Moreover,
since Hippo has to compare each tuple to the complete histogram which is kept in memory temporarily during index
initialization, Hippo may take more time than BRIN to initialize itself. Diﬀerent versions of BRIN spends most of the
initialization time on scanning the data table and hence do
not show much time diﬀerence.

Hippo, BRIN (128 pages per range by default), BRIN-32,
and BRIN-512. The indexes are built on TPC-H Lineitem
table PartKey (TPCH PK), SuppKey (TPCH SK), OrderKey (TPCH OK) attributes, Exponential table RandomNumber attribute, Wikipedia table PageCount attribute,
NYC Taxi table pick-up location attribute.
As given in Figure 5, Hippo occupies 25 to 30 times
smaller storage space than the B+ -Tree on all datasets (except on TPC-H OrderKey attribute). This happens because
Hippo only stores disk page ranges along with page summaries. Furthermore, Hippo on TPC-H PartKey incurs the
same storage space as that of the index built on the SuppKey attribute (which has 20 times less distinct attribute
values). That means the number of distinct values does
not actually impact Hippo index size as long as it is larger
than the number of complete histogram buckets. Each attribute value has the same probability to hit a histogram
bucket no matter how many distinct attribute values there
are. This is because the complete histogram leveraged by
Hippo summarizes the data distribution of the entire table.
Hippo still occupies small storage space on tables with different data distributions, such as Exponential, Wikipedia
and NYC Taxi data. That happens because the complete
histogram, which is height balanced makes sure that each tuple has the same probability to hit a bucket and then avoid
the eﬀect of data skewness. However, it is worth noting
that Hippo has a significant size reduction when the data is
sorted on TPC-H OrderKey attribute. In this case, Hippo
only contains five index entries and each index entry summarizes thousands of pages. When data is totally sorted,
Hippo keeps summarizing pages until the first 20% of the
complete histogram buckets (No.1 - 80) are hit, then the
next 20% (No. 81 - 160), and so forth. Therefore, Hippo
cannot achieve competitive query time in this case.
In addition, BRIN exhibits the smallest index size among
the three indexes since it only stores page ranges and corre-

7.3 Query response time
This section studies the query response time of the three
indexes, B+ -Tree, Hippo and BRIN (128 pages by default).
We first evaluate the query response time of the three indexes when diﬀerent query selectivity factors are applied.
Then, we further explore the performance of each index using the TPC-H benchmark queries which deliver industrywide practical queries for decision support.

7.3.1 Queries with different selectivity factors
This experiment studies the query execution performance
while varying the selectivity factor from 0.001%, 0.01%,
0.1% to 1%. According to the Hippo cost model, the corresponding query time costs in this experiment are 0.2Card,
0.2Card, 0.2Card and 0.8Card. The indexes are built on
TPC-H Lineitem table PartKey attribute (TPC-H), Exponential table RandomNumber attribute, Wikipedia table
PageCount attribute, NYC Taxi table pick-up location attribute. We also compare diﬀerent versions of BRIN (BRIN32 and BRIN-512) on TPC-H PartKey attribute.

393

(b) Exponential

(a) TPC-H

(c) Wikipedia

(d) NYC Taxi

Figure 8: Data update time (logarithmic scale) at diﬀerent update percentage
As Table 4 shows, the cost model exhibits high accuracy.
Furthermore, the cost model accuracy is stable especially
for the first three lower selectivity factors. However, when
SF = 1%, the accuracy is relatively lower especially on Exponential and NYC Taxi table. The reason behind that is
two-fold: (1) The 1% selectivity factor query predicate may
hit more buckets than the other lower SF values. That leads
to quite diﬀerent overlap situations with partial histograms.
(2) The complete height balanced histogram, maintained
by the DBMS, does not perfectly reflect the data distribution since it is created periodically using some statistical
approaches. Exponential and NYC Taxi tables exhibit relatively more clustered/skewed data distribution. That makes
it more diﬃcult to reflect their data distribution accurately.
On the other hand, the histogram of a uniformly distributed
TPC-H table is very accurate so that predicated I/O cost is
more accurate in this case.

As the results shown in Figure 7, all the indexes consume
more time to query data on all datasets with the increasing of query selectivity factors. All versions of BRIN are
two or more times worse than B+ -Tree and Hippo at almost
all selectivity factors. They have to scan almost the entire
tables due to their very insuﬃcient page summaries - value
ranges. Moreover, B+ -Tree is not better than Hippo at 0.1%
query selectivity factor although it is faster than Hippo at
low query selectivity factors like 0.001% and 0.01%. Actually, the performance of Hippo is very stable on all datasets
including highly skewed data and real life data. In addition,
Hippo consumes much more time at the last selectivity factor 1% because it has to scan much more pages as predicted
by the cost model. Compared to the B+ -Tree, Hippo maintains a competitive query response time performance at selectivity factor 0.1% but consumes 25 - 30 times less storage
space. In contrast to BRIN, Hippo achieves less query response time at the small enough index size. Therefore, we
may conclude that Hippo makes a good tradeoﬀ between
query response time and index storage overhead at medium
query selectivity factors, i.e, 0.1%.

7.3.3 TPC-H benchmark queries
This section compares Hippo to the B+ -Tree and BRIN
using the TPC-H benchmark queries. We select all TPC-H
benchmark queries that contain typical range queries and
hence need to access an index. We then adjust their selectivity factors to 0.1% (i.e., one week reports). We build the
three indexes on the L ShipDate (Query 6, 7, 14, 15 and 20)
and L ReceiptDate (Query 12) attributes in the Lineitem table as required by the queries. The qualified queries, Query
6, 7, 12, 14, 15 and 20, perform at least one index search
(Query 15 performs twice) on the evaluated indexes.

7.3.2 Evaluating the cost model accuracy
This section conducts a comparison between the estimated
query I/O cost and the actual I/O cost of running a query
on Hippo. In this experiment, we vary the query selectivity factors to take the values of 0.001%, 0.01%, 0.1%, and
1%. Hence, the average number of buckets hit by the query
predicate (SF ∗ H) should be 0.004, 0.04, 0.4, and 4 respectively. However, in practice, no in-boundary queries can
hit less than 1 bucket. Therefore, the average hit buckets by predicates are 1, 1, 1 and 4. Given H = 400 and
D = 20%, the query I/O cost estimated by Equation 8 is
Card
∗ (0.05% + 20%|20%|20%|80%). We observe that:
pageCard
(1) Queries for the first three SF values yields pretty similar
I/O cost. That matches the experimental results depicted
in Figure 7. (2) The I/O cost of scanning index entries conCard
∗ 0.05% which is at least 40 times less than
sumes pageCard
that of filtering false positive pages.

Table 5: Query response time (Sec) on TPC-H

0.001%
0.02%

0.01%
0.02%

0.1%
0.21%

1%
6.18%

Exponential

0.37%

0.37%

0.35%

12.69%

Wikipedia

0.91%

0.91%

1.19%

9.10%

NYC Taxi

0.87%

0.87%

0.51%

13.39%

Q6
2450

Q7
259000

Q12
2930

Q14
2670

Q15
4900

Q20
3500

Hippo

2700

260400

3200

BRIN

5600

276200

6200

3180

5400

3750

6340

11300

6700

As Table 5 depicts, Hippo achieves similar query response
time to that of the B+ -Tree and runs around two times faster
than BRIN on all selected TPC-H benchmark queries. It is
also worth noting that the time diﬀerence among all three
indexes becomes non-obvious for Query 7. That happens
because the query processor spends most of the time joining
multiple tables, which dominates the execution time for Q7.

Table 4: The estimated query I/O deviation from
the actual query I/O for diﬀerent selectivity factors
SF
TPC-H

Index type
B+ -Tree

7.4 Maintenance Overhead
This experiment investigates the index maintenance time
of three kinds of indexes, B+ -Tree, Hippo and BRIN, on

394

(a) TPC-H

(b) Exponential

(c) Wikipedia

(d) NYC Taxi

Figure 9: Throughput on diﬀerent query / update workloads (logarithmic scale)
all datasets when insertions or deletions. The indexes are
built on TPC-H Lineitem table PartKey attribute, Exponential table RandomNumber attribute, Wikipedia table PageCount attribute, and NYC Taxi table Pick-up location attribute. This experiment uses a fair setting which counts the
batch maintenance time after randomly inserting or deleting a certain amount (0.0001% , 0.001%, 0.01%, and 0.1%)
of tuples. In addition, after inserting tuples into the parent
table, the indexes’ default update operations are executed
because they adopt an eager strategy to keep indexes up
to date. However, after deleting the certain amount of tuples from the parent table, we rebuild BRIN from scratch
because BRIN does not have any proper update strategies
for deletion. We also compare diﬀerent versions of BRIN
(BRIN-32 and BRIN-512) on TPC-H PartKey attribute.
As depicted in Figure 8 (in a logarithmic scale), Hippo
costs up to three orders of magnitude less time to maintain
the index than the B+ -Tree and up to 50 times less time than
all versions of BRIN. This happens because the B+ -Tree
spends more time on searching proper index entry insert /
delete location and adjusting tree nodes. On the other hand,
BRIN’s maintenance is very slow after deletion since it has
to rebuild the index after a batch of delete operations.

Tree for update-intensive workload. Furthermore, for query
intensive workloads, Hippo still can exhibit slightly better
throughput than that of the B+ -Tree at a much small index
storage overhead.

8. RELATED WORK
Tree Indexes B+ -Tree is the most commonly used type
of indexes. The basic idea can be summarized as follows:
For a non-leaf node, the value of its left child node must
be smaller than that of its right child node. Each leaf node
points to the physical address of the original tuple. With the
help of this structure, searching B+ -Tree can be completed
in one binary search time scope. The excellent query performance of B+ -Tree and other tree like indexes is benefited by
their well designed structures which consist of many non-leaf
nodes for quick searching and leaf nodes for fast accessing
parent tuples. This feature incurs two inevitable drawbacks:
(1) Storing plenty of nodes costs a huge chunk of disk storage. (2) Index maintenance is extremely time-consuming.
For any insertions or deletions occur on parent table, tree
like indexes firstly have to traverse themselves for finding
proper update locations and then split, merge or re-order
one or more nodes which are out of date.
Bitmap Indexes A Bitmap index [12, 16, 21] has been
widely applied to low cardinality and read-only datasets. It
uses bitmaps to represent values without trading query performance. However, Bitmap index’s storage overhead significantly increases when indexing high cardinality attributes
because each index entry has to expand its bitmap to accommodate more distinct values. Bitmap index also does
not perform well in update-intensive workloads due to tuplewise index structure.
Compressed Indexes Compressed indexes drop some
repeated index information to save space and recover it as
fast as possible upon queries but they all have guaranteed
query accuracy. These techniques are applied to tree indexes [9, 10]. Though compressed indexes are storage economy, they require additional time for compressing beforehand and decompressing on-the-fly. Compromising on the
time of initialization, query and maintenance is not desirable
in many time-sensitive scenarios.
Approximate Indexes Approximate indexes [4, 11, 14]
give up the query accuracy and only store some representative information of parent tables for saving indexing and
maintenance overhead and improving query speed. They
propose many eﬃcient statistics algorithms to figure out the
most representative information which is worth to be stored.
In addition, some people focus on approximate query processing (AQP)[3, 20] which relies on data sampling and error

7.5 Performance on hybrid workloads
This section studies the performance of Hippo in hybrid query/update workloads. In this experiment, we
build the considered indexes on TPC-H Lineitem table
PartKey attribute, Exponential table RandomNumber attribute, Wikipedia table PageCount attribute, and NYC
Taxi table Pick-up location attribute. We use five diﬀerent hybrid query/update workloads: 10%, 30%, 50%, 70%
and 90%. The percentage here stands for the percentage of
queries in the entire workload. For example, 10% means
10% of the operations that access the index are queries
and 90% are updates. The average selectivity factor is
0.1%. The index performance is measured by throughput
(Tuples/second) defined as the number of qualified tuples
queried or updated per a given period of time. The results
are given in Figure 9 (in logarithmic scale).
As it turns out in Figure 9, Hippo has the highest throughput on all workloads. Hippo and BRIN can have higher
throughput at update-intensive workloads like 10% and
30%. That happens since Hippo and BRIN have less index maintenance time that that of the B+ -Tree. On the
other hand, B+ -Tree achieves higher throughput on queryintensive workloads like 70% and 90%. This is due to the
fact that B+ -Tree costs less or same query response time
compared to Hippo. Therefore, we can conclude that Hippo
performs orders of magnitudes better than BRIN and B+ -

395

bar estimating to accelerate query speed directly. However,
trading query accuracy makes them applicable to limited
scenarios such as loose queries.
Sparse Indexes A spars index, e.g., as Zone Map [5],
Block Range Index [17], Storage Index [18], and Small Materialized Aggregates (SMA) index [13], only stores pointers to
disk pages / column blocks in parent tables and value ranges
(min and max values) in each page / column block so that
it can reduce the storage overhead. For a posed query, it
finds value ranges which cover the query predicate and then
inspects the associated few parent table pages one by one
for retrieving truly qualified tuples. However, for unordered
data, a sparse index has to spend lots of time on page scanning since the stored value ranges (min and max values)
may cover most query predicates. In addition, column imprints [15], a cache-conscious secondary index, significantly
enhances the traditional sparse indexes to speed up queries
at a reasonably low storage overhead in-memory data warehousing systems. It leverages histograms and bitmap compression but does not support dynamic pages / column block
size control to further optimize the storage overhead reduction especially with partially clustered data. The column imprints approach is designed to handle query-intensive
workloads and puts less emphasis on eﬃciently update the
index in row stores.

9.

[3] S. Agarwal, H. Milner, A. Kleiner, A. Talwalkar,
M. Jordan, S. Madden, B. Mozafari, and I. Stoica. Knowing
when you’re wrong: building fast and reliable approximate
query processing systems. In Proceedings of the
International Conference on Management of Data,
SIGMOD, pages 481–492. ACM, 2014.
[4] M. Athanassoulis and A. Ailamaki. Bf-tree: Approximate
tree indexing. In Proceedings of the International
Conference on Very Large Data Bases, VLDB, pages
1881–1892. VLDB Endowment, 2014.
[5] C. Bontempo and G. Zagelow. The ibm data warehouse
architecture. The Communications of the ACM,
41(9):38–48, 1998.
[6] T. P. P. Council. Tpc-h benchmark specification.
http://www. tcp. org/hspec. html, 2008.
[7] P. Flajolet, D. Gardy, and L. Thimonier. Birthday paradox,
coupon collectors, caching algorithms and self-organizing
search. Discrete Applied Mathematics, 39(3):207–229, 1992.
[8] F. Fusco, M. P. Stoecklin, and M. Vlachos. Net-fli:
on-the-fly compression, archiving and indexing of streaming
network traﬃc. VLDB Journal, 3(1-2):1382–1393, 2010.
[9] J. Goldstein, R. Ramakrishnan, and U. Shaft. Compressing
relations and indexes. In Proceedings of the International
Conference on Data Engineering, ICDE, pages 370–379.
IEEE, 1998.
[10] G. Guzun, G. Canahuate, D. Chiu, and J. Sawin. A tunable
compression framework for bitmap indices. In Proceedings
of the International Conference on Data Engineering,
ICDE, pages 484–495. IEEE, 2014.
[11] M. E. Houle and J. Sakuma. Fast approximate similarity
search in extremely high-dimensional data sets. In
Proceedings of the International Conference on Data
Engineering, ICDE, pages 619–630. IEEE, 2005.
[12] D. Lemire, O. Kaser, and K. Aouiche. Sorting improves
word-aligned bitmap indexes. Data & Knowledge
Engineering, 69(1):3–28, 2010.
[13] G. Moerkotte. Small materialized aggregates: A light
weight index structure for data warehousing. In Proceedings
of the International Conference on Very Large Data Bases,
VLDB, pages 476–487. VLDB Endowment, 1998.
[14] Y. Sakurai, M. Yoshikawa, S. Uemura, H. Kojima, et al.
The a-tree: An index structure for high-dimensional spaces
using relative approximation. In Proceedings of the
International Conference on Very Large Data Bases,
VLDB, pages 5–16. VLDB Endowment, 2000.
[15] L. Sidirourgos and M. L. Kersten. Column imprints: a
secondary index structure. In Proceedings of the
International Conference on Management of Data,
SIGMOD, pages 893–904. ACM, 2013.
[16] K. Stockinger and K. Wu. Bitmap indices for data
warehouses. Data Warehouses and OLAP: Concepts,
Architectures and Solutions, page 57, 2006.
[17] M. Stonebraker and L. A. Rowe. The design of postgres. In
Proceedings of the International Conference on
Management of Data, SIGMOD. ACM, 1986.
[18] R. Weiss. A technical overview of the oracle exadata
database machine and exadata storage server. Oracle White
Paper. Oracle Corporation, Redwood Shores, 2012.
[19] K. Wu, E. Otoo, and A. Shoshani. On the performance of
bitmap indices for high cardinality attributes. In
Proceedings of the International Conference on Very Large
Data Bases, VLDB, pages 24–35. VLDB Endowment, 2004.
[20] K. Zeng, S. Gao, B. Mozafari, and C. Zaniolo. The
analytical bootstrap: a new method for fast error
estimation in approximate query processing. In Proceedings
of the International Conference on Management of Data,
SIGMOD, pages 277–288. ACM, 2014.
[21] M. Zukowski, S. Heman, N. Nes, and P. Boncz.
Super-scalar ram-cpu cache compression. In Proceedings of
the International Conference on Data Engineering, ICDE,
pages 59–59. IEEE, 2006.

CONCLUSION AND FUTURE WORK

The paper introduces Hippo a data-aware sparse indexing
approach that eﬃciently and accurately answers database
queries. Hippo occupies up to two orders of magnitude
less storage overhead than de-facto database indexes, i.e.,
B+ -tree while achieving comparable query execution performance. To achieve that, Hippo stores page ranges instead of tuples in the indexed table to reduce the storage
space occupied by the index. Furthermore, Hippo maintains histograms, which represent the data distribution for
one or more pages, as the summaries for these pages. This
structure significantly shrinks index storage footprint without compromising much performance on high and medium
selectivity queries. Moreover, Hippo achieves about three
orders of magnitudes less maintenance overhead compared
to the B+ -tree and BRIN. Such performance benefits make
Hippo a very promising alternative to index high cardinality
attributes in big data application scenarios. Furthermore,
the simplicity of the proposed structure makes it practical
for DBMS vendors to adopt Hippo as an alternative indexing
technique. In the future, we plan to adapt Hippo to support
more complex data types, e.g., spatial data, unstructured
data. We also plan to study more eﬃcient concurrency control mechanisms for Hippo. Furthermore, we also plan to
extend Hippo to function within the context of in-memory
database systems as well as column stores.

10. ACKNOWLEDGEMENT
This work is supported by the National Science Foundation under Grant 1654861.

11. REFERENCES
[1] New york city taxi and limousine commission. http://www.
nyc.gov/html/tlc/html/about/trip\_record\_data.html.
[2] Page view statistics for wikimedia projects.
https://dumps.wikimedia.org/other/pagecounts-raw/.

396

arXiv:1604.03234v1 [cs.DB] 12 Apr 2016

Hippo: A Fast, yet Scalable, Database Indexing Approach
Jia Yu

Mohamed Sarwat

Arizona State University
699 S. Mill Avenue, Tempe, AZ

Arizona State University
699 S. Mill Avenue, Tempe, AZ

jiayu2@asu.edu

msarwat@asu.edu
TPC-H
2 GB
20 GB
200 GB

ABSTRACT
+

Even though existing database indexes (e.g., B -Tree) speed
up the query execution, they suffer from two main drawbacks: (1) A database index usually yields 5% to 15% additional storage overhead which results in non-ignorable dollar cost in big data scenarios especially when deployed on
modern storage devices like Solid State Disk (SSD) or NonVolatile Memory (NVM). (2) Maintaining a database index incurs high latency because the DBMS has to find and
update those index pages affected by the underlying table
changes. This paper proposes Hippo a fast, yet scalable,
database indexing approach. Hippo only stores the pointers
of disk pages along with light weight histogram-based summaries. The proposed structure significantly shrinks index
storage and maintenance overhead without compromising
much on query execution performance. Experiments, based
on real Hippo implementation inside PostgreSQL 9.5, using the TPC-H benchmark show that Hippo achieves up to
two orders of magnitude less storage space and up to three
orders of magnitude less maintenance overhead than traditional database indexes, i.e., B+ -Tree. Furthermore, the experiments also show that Hippo achieves comparable query
execution performance to that of the B+ -Tree for various
selectivity factors.

1.

Index size
0.25 GB
2.51 GB
25 GB

HDD
0.04 $/GB

Initialization time
30 sec
500 sec
8000 sec

(a) B+ -Tree overhead
E-HDD
SSD
0.1 $/GB 0.5 $/GB

Insertion time
10 sec
1180 sec
42000 sec
E-SSD
1.4 $/GB

(b) Storage dollar cost

Table 1: Index overhead and storage dollar cost

storage overhead. Even though the storage overhead
may not seem too high in small databases, it results in non-ignorable dollar cost in big data scenarios. Table 1a depicts the storage overhead of a B+ Tree created on the Lineitem table from the TPCH [5] benchmark (database size varies from 2, 20 and
200 GB). Moreover, the storage dollar cost is dramatically amplified when the DBMS is deployed on modern storage devices (e.g., Solid State Drives and NonVolatile Memory) because they are still more than an
order of magnitude expensive than Hard Disk Drives
(HDDs) per unit of storage. Table 1b lists the dollar
cost per storage unit collected from Amazon.com and
NewEgg.com (Enterprise is abbreviated to E). In addition, initializing an index may be a time consuming
process especially when the index is created on a large
database table. Such high initialization overhead may
delay the analysis process (see Table 1a).

INTRODUCTION

A database system (DBMS) often employs an index structure, e.g., B+ -Tree [4], to speed up query execution at the
cost of additional storage and maintenance overhead. A
DBMS user may create an index on one or more attributes
of a database table. A created index allows the DBMS to
quickly locate tuples without having to scan the whole indexed table. Even though existing database indexes significantly improve the query response time, they suffer from
the following drawbacks:

• Maintenance Overhead: A DBMS must update the
index after inserting (deleting) tuples into (from) the
underlying table. Maintaining a database index incurs high latency because the DBMS has to find and
update those index entries affected by the underlying
table changes. For instance, maintaining a B+ -Tree
searches the tree structure and perhaps performs a set
of tree nodes splitting or merging operations. That
requires plenty of disk I/O operations and hence encumbers the time performance of the entire DBMS in
big data scenarios. Table 1a shows the B+ Tree insertion overhead (insert 0.1% records) for the TPC-H
Lineitem table.

• Indexing Overhead: Indexing overhead consists of
two parts - storage and initialization time overhead.
A database index usually yields 5% to 15% additional

Existing approaches that tackle one or more of the aforementioned drawbacks are classified as follows: (1) Compressed indexes: Compressed B+ -Tree approaches [7, 8, 19]
1

reduce index storage overhead but all these methods compromise on query performance due to the additional compression and decompression time. Compressed bitmap indexes also reduce index storage overhead [9, 11, 14] but they
mainly suit low cardinality attributes which are quite rare.
For high cardinality attributes, the storage overhead of compressed bitmap indexes significantly increases [17]. (2) Approximate indexes: Approximate indexing approaches [2, 10,
12] trade query accuracy for storage to produce smaller, yet
fast, index structures. Even though approximate indexes
may shrink the storage size, users cannot rely on their unguaranteed query accuracy in many accuracy-sensitive application scenarios like banking systems or user archive systems. (3) Sparse indexes: A sparse index [3, 13, 15, 16] only
stores pointers which refer to disk pages and value ranges
(min and max values) in each page so that it can save indexing and maintenance overhead. It is generally built on
ordered attributes. For a posed query, it finds value ranges
which cover or overlap the query predicate and then rapidly
inspects the associated few parent table pages one by one
for retrieving truly qualified tuples. However, for unordered
attributes which are much more common, sparse indexes
compromise too much on query performance because they
find numerous qualified value ranges and have to inspect a
large number of pages.
This paper proposes Hippo1 a fast, yet scalable, sparse
database indexing approach. In contrast to existing tree index structures, Hippo stores disk page ranges (each works
as a pointer of one or many pages) instead of tuple pointers
in the indexed table to reduce the storage space occupied by
the index. Unlike existing approximate indexing methods,
Hippo guarantees the query result accuracy by inspecting
possible qualified pages and only emitting those tuples that
satisfy the query predicate. As opposed to existing sparse
indexes, Hippo maintains simplified histograms that represent the data distribution for pages no matter how skew it is,
as the summaries for these pages in each page range. Since
Hippo relies on histograms already created and maintained
by almost every existing DBMS (e.g., PostgreSQL), the system does not exhibit a major additional overhead to create
the index. Hippo also adopts a page grouping technique
that groups contiguous pages into page ranges based on the
similarity of their index key attribute distributions. When a
query is issued on the indexed database table, Hippo leverages the page ranges and page summaries to recognize those
pages for which the internal tuples are guaranteed not to satisfy the query predicates and inspects the remaining pages.
Thus Hippo achieves competitive performance on common
range queries without compromising the accuracy. For data
insertion and deletion, Hippo dispenses with the numerous
disk operations by rapidly locating the affected index entries. Hippo also adaptively decides whether to adopt an
eager or lazy index maintenance strategy to mitigate the
maintenance overhead while ensuring future queries are answered correctly.
We implemented a prototype of Hippo inside PostgreSQL
9.5. Experiments based on the TPC-H benchmark show
that Hippo achieves up to two orders of magnitude less storage space and up to three orders of magnitude less maintenance overhead than traditional database indexes, i.e.,
B+ -Tree. Furthermore, the experiments show that Hippo
1

Create an index

Execute a query
User

Age table
Disk Page #

Query predicate

Internal data

1,2,3,4,5,… 21,22,55,75,77,…

Compare

Bucket
2,3,4,…

(Return)

Bucket
1
2
3
4
5

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

Age = 55
Compare

(Return)

Bucket
3

Complete height balanced histogram
Filter false positives

Generate partial histograms

Hippo
Bucket
2
3
4

Age
21 - 40
41 - 60
61 - 90

Partial histogram 1
Bucket
2
4
5

Age
21 - 40
61 - 90
91 - 120

Partial histogram 2

Page range Partial histogram Internal data
(Return)
1 - 10
2,3,4
21,22,55,75,77
11 – 25
2,4,5
23,24,62,91,92
26 - 30
1,2,5
11,12,25,101,110
Index Entry 1

…

Bucket
1
2
5

Age
1 - 20
21 - 40
91 - 120

Inspect
Page
1 - 10

Partial histogram 3
Legend: Index Initialization

Database
Index Search

Figure 1: Initialize and search Hippo on age table

achieves comparable query execution performance to that
of the B+ -Tree for various selectivity factors.
The remainder of the paper is structured as follows: In
Section 2, we explain the idea of Hippo and show its structure. We demonstrate how to query Hippo swiftly, build
Hippo from scratch, and maintain Hippo efficiently in Section 3, 4 and 5. In Section 6, we provide useful cost estimation for these three scenarios. Extensive experiments
and related analysis are included in Section 7. We discuss
related work then analyze the drawbacks in existing indexes
in Section 8. Finally, Section 9 concludes the paper.

2. HIPPO OVERVIEW
This section gives an overview of Hippo. A running example that describes a Hippo index built on an age table is
given in Figure 1. The figure’s right part which depicts how
to search Hippo and the left part which shows how to initialize Hippo are explained in Section 3 and 4 respectively.
The main challenges of designing an index are to reduce
the indexing overhead in terms of storage and initialization
time as well as speed up the index maintenance while still
keeping competitive query performance. To achieve that,
an index should possess the following two main properties:
(1) Less Index Entries: For better storage space utilization,
an index should determine and only store the most representative index entries that summarize the key attribute.
Keeping too many index entries inevitably results in high
storage overhead as well as high initialization time. (2) Index Entries Independence: Index entries of a created index
should be independent from each other. In other words, the
range of values that each index entry represents should have
minimal overlap with other index entries. Interdependence
among index entries, like that in a B+ -Tree, may lead to
overlapped tree nodes traverse during query processing and
several cascaded updates during index maintenance.

https://github.com/DataSystemsLab/hippo-postgresql

2

Age = 55

Data Structure. When creating an index, Hippo scans
the indexed table and generates histogram-based summaries
for disk pages based upon the index key attribute. Afterwards, these summaries are stored by Hippo along with
pointers of the pages they summarize. As shown in Figure 1, a Hippo index entry consists of the following two
components (Internal data of pages is given in the figure
only for the ease of understanding):

Bucket
1
2
3
4
5

Age > 55
Bucket
1
2
3
4
5

Age > 55 AND Age < 65

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

Bucket
1
2
3
4
5

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

Figure 2: Convert query predicates

• Summarized Page Range: The page range (works
as a pointer) represents the IDs (i.e., address) of the
first and last pages summarized by a certain histogram
based summary. DBMS can load particular pages into
buffer according to their customized IDs. Hippo is able
to summarize more than one contiguous (in terms of
physical storage) pages to reduce the overall index size
to a great extent (e.g., Page 1 - 10, 11 - 25, 26 - 30).
The number of summarized pages (denoted as pages
per partial histogram) in each index entry varies. For
a certain index attribute, some contiguous pages have
very similar content but some are not. Hence, Hippo
adopts a page grouping technique that groups contiguous pages into page ranges based on the similarity of
their index attribute distributions, using the partial
histogram density (explained in Section 4).

predicate. The search process leverages the index structure
to avoid worthless page inspection so that Hippo can achieve
competitive query performance.
Algorithm 1: Hippo index search
Data: A given query predicate and Hippo index
Result: Qualified tuples
1 Create Bitmap a for the given predicate;
2 foreach bucket of the complete histogram do
3
if it is hit by the query predicate then
4
Set the corresponding bit in Bitmap a to 1;
5
end
6 end
7 Create Bitmap b for recording all pages;
8 foreach partial histogram do
9
if it has joint buckets with Bitmap a then
10
Set the corresponding bits of the summarized
pages in Bitmap b to 1;
11
end
12 end
13 foreach page marked as 1 in Bitmap b do
14
Check each tuple in it against the predicate;
15 end

• Histogram-based Page Summary: The page summary in each index entry is a partial histogram
that represents a subset of the complete height balanced histogram buckets (maintained by the underlying DBMS). Each bucket if exists indicates that at
least one of the tuples of this bucket exists in the summarized pages. Each partial histogram represents the
distribution of the data in the summarized contiguous
pages. Since each bucket of a height balanced histogram roughly contains the same number of tuples,
each of them has the same probability to be hit by a
random tuple from the table. Hippo leverages this feature to handle data which has various or even skewed
distribution. To save storage space, only bucket IDs
are kept in partial histograms and partial histograms
are stored in a compressed bitmap format. For instance, the partial histogram of the first Hippo index
entry in Figure 1 is stored as 01110. Each bit, set to 1
or 0, reflects whether the corresponding bucket exists
or not.

3.1 Convert query predicates
The main idea is to check each partial histogram against
the given query predicate for filtering false positives and
so speeding up the query. However, as explained in Section 2, partial histograms are stored in bitmap formats without recording value ranges of buckets. Therefore, there has
to be an additional step to recover the missing information
for each partial histogram on-the-fly or convert the predicate to the bitmap format per query. Obviously, the later
one is more efficient.
Any query predicates for a particular attribute can be
broken down into atomic units: equality query predicate
and range query predicate. Age = 55 is a typical equality
query predicate while age > 55 is a range query predicate.
These unit predicates can be combined together by AND
operator like age > 55 AND age <65.
Each unit predicate is compared with the buckets of the
complete height balanced histogram (retrieving method is
discussed in Section 4). A bucket is hit by a predicate if
the predicate fully contains, overlaps, or is fully contained
by the bucket. Each unit predicate can hit one, at least,
or more buckets. For instance, according to the complete
histogram in Figure 1, bucket 3 whose description is 41 - 60
is hit by age = 55 while bucket 3, 4, and 5 are hit by age
> 55. This strategy is also applicable for the conjunct query
predicates. For a conjunct predicate like age > 55 and age
< 65, only buckets which are hit by all these unit predicates

Main idea. Hippo solves the aforementioned challenges as follows: (1) Each index entry summarizes many
pages and each only stores two page IDs and a compressed
bitmap.(2) Each page of the parent table is only summarized by one Hippo index entry. Hence, any updates that
occur in a certain page only affect a single independent index entry. Finally, during a query, pages whose partial histograms do not have desired buckets are guaranteed not to
satisfy certain query predicates and marked as false positives. Thus Hippo only inspects other pages that probably
satisfies the query predicate and achieves competitive query
performance.

3.

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

INDEX SEARCH

The search algorithm runs in three main steps: (1) Step 1:
convert query predicates, (2) Step 2: filter false positives and
(3) Step 3: inspect possible qualified pages against the query
3

Age = 55

Partial histogram 1

(bitmap)

(bitmap)

Bucket
0
0
1
0
0

AND
0
0
1
0
0

IDs of possible qualified pages are recorded in a separate
bitmap. Each bit in this bitmap is mapped to the page at
the same position in the parent table. For instance, the bit
at position 1 in the bitmap is mapped to the page ID 1 of the
parent table. The value (1 or 0) of this bit reflects whether
the associate page is a possible qualified page or not.
Hippo has to inspect all of the possible qualified pages
recorded in the bitmap against the query predicate one by
one because every retained page from the previous step is
possible to contain qualified tuples. The only way to inspect
these possible qualified pages is to traverse them and check
each tuple in each page one by one. Qualified tuples are
returned to the DBMSs.
Algorithm 1 shows the three steps of Hippo index search.
The right part of Figure 1 describes how to search Hippo
index using a certain query predicate. Firstly, Hippo finds
query predicate age = 55 hits bucket 3. And the first one of
the three partial histograms nicely contains bucket 3. Thus
only the disk pages 1 - 10 are selected as possible qualified
pages which need further inspection. It is also worth noting
that these partial histograms summarize different number of
pages.

Bucket
0
1
1
1
0

Bitwise AND = 1
Figure 3: Bitwise AND two bitmaps to find joint buckets

simultaneously (the joint bucket 3 and 4) are kept as the final
result and others are directly discarded. Figure 2 shows the
hit buckets of three query predicates. Afterwards, the given
query predicate is converted to a bitmap. Each bit in this
bitmap reflects whether the bucket has the corresponding
ID is hit (1) or not (0). Thus the corresponding bits of all
hit buckets are set to 1.

3.2 Filter false positives
4. INDEX INITIALIZATION

Filtering false positives is the most important step of
Hippo index search. Each Hippo index entry stores a page
range and a summary of several contiguous pages but it is
very possible that none of these pages in the certain index
entry contain the qualified tuples especially for small range
queries. This kind of pages and their associated index entries are false positives. This step is to check each partial
histogram against the converted query predicate, recognize
some false positive pages utmost and finally avoid worthless
page inspection on these pages.
A given query predicate hits one ,at least, or more buckets
of the complete histogram. Pages whose partial histograms
contain the hit buckets (the corresponding bitmap bits are
1) might have qualified tuples, whereas pages whose partial histograms don’t contain these buckets (the corresponding bitmap bits are 0) are guaranteed not to contain qualified tuples. The former kind of pages are possible qualified
pages. In contrast, the later kind of pages are false positives
and excluded from the next step - inspect possible qualified
pages. The straight way to find false positive pages is to
do a nested loop between each partial histogram and the
converted query predicate to find the joint buckets.
Interestingly, because both of partial histograms and the
converted query predicate are in bitmap format, the nested
loop can be accelerated by bitwise ’AND’ing the bytes from
both sides, aka bit-level parallelism. If bitwise ’AND’ing
the two bytes from both sides returns 1, that means there
are joint buckets between the query predicate and the partial histogram. Thus the pages are possible qualified pages.
Figure 3 provides an example of how to perform a bitwise
AND using the same data in Figure 1.

Hippo performs three main steps to initialize itself:
(1) Retrieve a complete histogram, (2) Generate partial histograms, and (3) Group similar pages into page ranges, described as follows.
Algorithm 2: Hippo index initialization
Data: Pages of a parent table
Result: Hippo index
1 Create a working partial histogram (in bitmap format);
2 Set StartPage = 1 and EndPage = 1;
3 foreach page do
4
Find distinct buckets hit by its tuples;
5
Set associated bits to 1 in the partial histogram;
6
if the working partial histogram density > threshold
then
7
Store the partial histogram and the page range
(StartPage and EndPage) as an index entry;
8
Create a new working partial histogram;
9
StartPage = EndPage + 1;
10
EndPage = StartPage;
11
else
12
EndPage = EndPage + 1;
13
end
14 end

4.1 Retrieve a complete histogram
Histograms used in Hippo include a complete height balanced histogram and many partial histograms. A complete
height balanced histogram represents the distribution of all
tuples and already exists in DBMSs. Respectively, a partial
histogram, as a subsection, only contains partial buckets
from the complete histogram. Therefore, for generating any
partial histograms, a complete histogram should be retrieved
at the first priority. Full-fledged functions for retrieving a
complete histogram exist in any DBMSs. Detailed explanation for these functions is omitted in this paper since it is

3.3 Inspect possible qualified pages
The previous step recognizes many false positive pages
and excludes them from possible qualified pages. However,
one fact is that not all false positives can be detected by
the previous step. Possible qualified pages still may contain
false positives and this is why they are called ”possible”.
This step is to inspect the tuples in each possible qualified
pages and retrieve the qualified tuples directly.
4

histograms into one larger partial histogram (in other words,
summarizing more pages within one partial histogram) can
make Hippo more efficient. On the other hand, users may
want to shrink Hippo physically to a greater extent. For
example, if a partial histogram can summarize 10 pages in
one go, the new Hippo size will be much smaller. Grouping
more pages into one page range and summarizing them with
just one partial histogram are expected and practical as well.
Yet, this is not saying that all pages should be grouped
together and summarized by one merged partial histogram.
As more and more pages are summarized, this partial histogram contains more and more buckets until all buckets
from the complete histogram are included. At this moment,
this partial histogram becomes a complete histogram and
covers any possible query predicates. That means this kind
of partial histograms is unable to help Hippo to filter the
false positives and the disk pages summarized by this partial histogram will be always treated as possible qualified
pages.
One strategy is to group a fixed number of contiguous
pages per range/partial histogram. Yet, this strategy is
not suitable if some contiguous pages in a certain area have
much more similar data distribution than other areas. Lacking the awareness of data distribution cannot reduce storage overhead smartly. Under this circumstance, it is better
to let Hippo group more pages together in this area and
group less pages together in other areas dynamically. For
instance, assume original pages per partial histogram is 100.
If there are 1000 out of 10000 disk pages and the tuples in
these 1000 pages are exactly same, a better way to shrink
the index size is to set the P from 100 to 1000 for grouping/summarizing these 1000 pages into one range/partial
histogram and change it back to 100 for other 9000 pages.
A terminology - partial histogram density is introduced
here. The density of a partial histogram is the percentage
of kept buckets in the total buckets of a complete histogram.
The complete histogram has a density value of 1. The definition can be formalized as follows:

not our focus. We also assume that the complete histogram
is not changed at any time because the global distribution
of the parent table will not be affected even if some local
updates are performed.
The resolution of the complete histogram (denoted as histogram resolution) is adjustable. A complete histogram is
considered as higher resolution if it contains more buckets.
The resolution of partial histograms is consistent with their
complete histograms technically. It is apparent that a complete histogram will have larger physical size if it has higher
resolution and, accordingly, the numerous partial histograms
are also physically larger than the low resolution ones. On
the other hand, the histogram resolution also affects Hippo
query time. The cost estimation section will further discuss
this issue.

4.2 Generate partial histograms
A partial histogram only contains some buckets from the
complete histogram. It is used to represent the distribution
of parent tuples in one or many disk pages. In other words,
people can get an approximate overview from the partial
histogram of these pages: What values might lie in these
pages and what do not. These partial histograms are able
to help Hippo to recognize false positives utmost and avoid
worthless page inspection. We explain how to generate a
partial histogram for each disk page in this section.
Generating partial histograms traverses all disk pages of
the parent table from the start to end. For each page, a
nested loop passes through each tuple in this page. The
specified attribute value is extracted from each tuple and
compared with the complete histogram (using a binary
search). Buckets hit by tuples are kept for this page and
then compose a partial histogram. A partial histogram only
contains distinct buckets. For instance, there is a group of
age values like the first entry of Hippo shown in Figure 1:
21, 22, 55, 75, 77. Bucket 2 is hit by 21 and 22, bucket 3 is
hit by 55 and bucket 4 is hit by 77. Therefore, the partial
histogram for these values is just as partial histogram 1 in
Figure 1.
Shrinking the physical size of partial histograms is desirable. The basic idea is to drop all bucket value ranges and
only keep bucket IDs. Hippo in Figure 1 shows the effect.
Actually, as mentioned in Section 2, dropping value range
information does not impact much on the index search. To
further shrink the size, storing bucket IDs in integer type (4
bytes or more) is also considered as an overhead. Bitmap format storage is a better choice to bypass this overhead. Each
partial histogram is stored as a bitmap. Each bit in a bitmap
stands for a bucket at the same position in a complete histogram. Bit value 1 means the associated bucket is hit and
kept in this partial histogram while 0 means the associated
bucket is not included. Bitmap compression is introduced to
Hippo as well. The partial histogram in a bitmap format can
be compressed by any existing compression techniques. The
time of compressing and decompressing partial histograms is
ignorable in contrast to that of inspecting possible qualified
pages.

P artial histogram density =

Bucketspartial histogram
Bucketscomplete histogram

This density has an important phenomenon that, for a group
of contiguous pages, their merged partial histogram density
will be very low if these pages are very similar, vice versa.
Therefore, a partial histogram with a certain density may
summarize more pages if these contiguous pages have similar data, vice versa. Making use of this phenomenon enables Hippo to dynamically group pages and merge partial
histograms into one. In addition, it is understandable that
a lower density partial histogram (summarizes less pages)
has the high probability to be recognized as false positives
so that speed up queries.
User can easily set a same density for all partial histograms as a threshold. Each partial histogram can automatically decide how many pages it should summarize.
Algorithm 2 depicts how to initialize a Hippo and summarize more pages within one partial histogram with the help
of the partial histogram density. The basic idea is that new
pages will not be summarized into a partial histogram if
its density is larger than the threshold and a new partial
histogram will be created for the following pages.
Figure 1’s left part depicts how to initialize a Hippo on
the age table with a partial histogram density 0.6. All of

4.3 Group similar pages into page ranges
Generating a partial histogram per one disk page is as easy
as that in Section 4.2. However, for some contiguous pages
which have similar data, it is a waste of storage. Grouping
them together as many as possible and merging their partial
5

Algorithm 3: Update Hippo for data insertion
Data: A new inserted tuple belongs to Page a
Result: Updated Hippo
1 Find the bucket hit by the inserted tuple;
2 Locate a Hippo index entry which summarizes Page a;
3 if one index entry is located then
4
Retrieve the associated Hippo index entry;
5
Update the retrieved entry if necessary;
6 else
7
Retrieve the Hippo entry summarizes the last page;
8
if the partial histogram density < threshold then
9
Summarize Page a into the retrieved entry;
10
else
11
Summarize Page a into a new entry;
12
end
13 end

Updated Hippo
S	
 
t
e #
L

H



Page range Partial histogram Internal data
1 - 10
2,3,4
21,22,55,75,77
Blank space
2 - 3
12
111221111
11  2

124

Mve

132324 29192

Figure 4: Hippo Index Entries Sorted List

time-consuming if every entry is retrieved from disk, deserialized and checked against the target page. Therefore,
a binary search on Hippo index entries is a good choice.
(This search actually leverages the index entries sorted list
explained in Section 5.3.)
Step 3: Update the index entry: If the new tuple
belongs to a new page not summarized by any Hippo index
entries and the density of Hippo partial histogram which
summarizes the last disk page is smaller than the density
threshold set by users, this new page will be summarized
into this partial histogram in the last index entry otherwise
a new partial histogram will be created to summarize this
page and stored in a new Hippo index entry. For a new tuple
belongs to pages already summarized by Hippo, the partial
histogram in the associated index entry will be updated if
the inserted tuple hits a new bucket.
It is worth noting that: (1) Since the compressed bitmaps
of partial histograms may have different size, the updated
index entry may not fit the space left at the old location.
Thus the updated one may be put at the end of Hippo.
(2) After some changes (replacing old or creating new index
entry) in Hippo, the corresponding position of the sorted list
needs to be updated.

the tuples are compared with the complete histogram and
IDs of distinct buckets hit by tuples are generated as partial
histograms along with page range.
So far, as Figure 1 shows, each entry in a Hippo index has the following content: a partial histogram in compressed bitmap format and two integers stand for the first
and last pages summarized by this histogram (summarized
page range). Each entry is serialized and stored on disk.

5.

inte

INDEX MAINTENANCE

Inserting (deleting) tuples into (from) the indexed table
requires maintaining the index to ensure that the DBMS
can retrieve the correct set of tuples that match the query
predicate. However, the overhead of maintaining the index
quite frequently may preclude system scalability. This section explains how Hippo handles updates.

5.2 Data deletion

5.1 Data insertion

The eager update strategy is not highly desired for data
deletion. Hippo still ensures the correctness of queries even
if it doesn’t update itself at all after deleting tuples from a
table. This benefits by inspecting possible qualified pages
in index search. Pages used to have qualified tuples might
be still marked as possible qualified pages but they are discarded after being inspected against the query predicates. A
periodic update or bulk update will be a good choice here.
For data deletion, Hippo adopts a lazy update strategy
that maintains the index after a bulk of delete operations.
In such case, Hippo traverses each index entry from the start
to end. For each index entry, Hippo inspects the header of
each summarized page for seeking notes made by DBMSs
(e.g., PostgreSQL makes notes in page headers if data is
removed from pages). Hippo re-summarizes the entire index
entry instantly within the original page range if data deletion
on one page is detected. The re-summarization follows the
same steps in Section 4. It is worth noting that this updated
Hippo index entry is not leading to the update on the sorted
list because the updated partial histogram, having same or
less buckets, can obtain same or less compress bitmap size
and the new index entry certainly fits the old space.

Hippo should instantly update or check the index at least
after inserting one record into the indexed table. Otherwise, all subsequent queries might miss the newly inserted
tuple since it is not reflected by the index. Therefore, Hippo
adopts an eager update strategy when a new tuple is inserted. Data insertion may change the physical structure
of a table. The new tuple may belong to any pages of the
indexed table. The insertion procedure (See Algorithm 3)
performs the following steps: (1) Find buckets hit by the
new tuple, (2) Locate the affected index entry, and (3) Update the index entry if necessary.
Step 1: Find buckets hit by the new tuple: Similar
with some steps of generating partial histogram in index
initialization, after retrieving the complete histogram, the
newly inserted tuple is checked against it using a binary
search and a bucket hit by this new tuple is found.
Step 2: Locate the affected index entry: The new
tuple has to belong to one page in this table. This page may
be a new one which has not been summarized by any partial
histograms before or an old one which has been summarized.
However, because the numbers of pages summarized by each
histogram are different, searching Hippo index entry to find
the one contains this target page is inevitable. From the
perspective of disk storage, in a Hippo, all partial histograms
are stored on disk in a serialized format. It will be extremely

5.3 Index Entries Sorted List
When a new tuple is inserted, Hippo executes a fast binary
search (according to the page IDs) to locate the affected
6

Term
H
D
P
T
Card
pageCard
SF

Qy  ate 1

Definition
Complete histogram resolution which means the
number of buckets in this complete histogram
Partial histogram density which is an user supplied threshold
Pages summarized by one partial histogram for a
certain attribute
Tuples summarized by one partial histogram for
a certain attribute
Cardinality (the total number of tuples) of the
indexed table
Number of tuples in each page of the indexed table
Query selectivity factor =

Query output
Query input

Miss…
Partial histogram
O
Qy  ate 2

Figure 5: Visualize how to filter false positives

The first step of Hippo index search is to traverse Hippo
index entries. Pages in each index entry are likely to be
selected for further inspection if their associated partial histogram has joint buckets with the query predicate. Determining the probability of having joint buckets contributes
to the query time cost estimation.
For the ease of presentation, Figure 5 visualizes the procedure of filtering false positives according to their partial
histograms. Partial histogram density (D) of this index is
0.2. The complete histogram constitutes of 10 buckets in
total (H = 10). Assume the indexed table’s tuples follow an
uniform distribution based upon the key attribute. Let the
query selectivity factor (SF ) be 20%. In Figure 5, buckets
hit by the query predicates and the partial histogram are
represented in a bitmap format. According to this figure,
the partial histogram misses a query predicate if the highlighted area of the predicate falls into the blank area of the
partial histogram, whereas a partial histogram is selected if
the predicate does not fall completely into the blank area of
the histogram. In other words, the probability of a partial
histogram having joint buckets with a predicate depends on
how likely a predicate doesn’t fall into the blank area of
a partial histogram. The probability is determined by the
formula given below (The terms are defined in Table 2):

* 100%

Table 2: Notations used in Cost Estimation

index entry and then updates it. Since the index entries are
not guaranteed to be sorted based on the page IDs (noted in
data insertion section), an auxiliary structure for recording
the sorted order is introduced to Hippo.
The sorted list is initialized after all steps in Section 4
with the original order of index entries and put at the first
several index pages of Hippo. During the entire Hippo life
time, the sorted list maintains a list of pointers of Hippo
index entries in the ascending order of page IDs. Actually
each pointer represents the fixed size physical address of
an index entry and these addresses can be used to retrieve
index entries directly. That way, the premise of a binary
search has been satisfied. Figure 4 depicts the Hippo index
entries sorted list. Index entry 2 in Figure 1 has a new
bucket ID 1 due to a newly inserted tuple in its internal
data and hence this entry becomes the last index entry in
Figure 4. The sorted list is still able to record the ascending
order and help Hippo to perform a binary search on the
index entries. In addition, such sorted list leads to slight
additional maintenance overhead: Some index updates need
to modify the affected pointers in the sorted list to reflect
the new physical addresses.

6.

P rob = (Buckets hit by a query predicate) ∗ D
= (SF ∗ H) ∗ D

(1)

To be precise, P rob follows a piecewise function as follows:

COST ESTIMATION

P rob =

This section gives a detailed cost estimation of Hippo.
We first provide an accurate query time cost model which
assists the DBMS query optimizer in picking an efficient
query execution plan. Estimating the storage overhead of
an index can also facilitate better disk space management
and planning. Index initialization certainly consumes a large
chunk of time. Similarly, index maintenance can present a
significant time overhead in any write-intensive application.
Both of them should be carefully estimated.
Table 2 summarizes the main notations we use to derive
the cost model. Given a database table R with Card number
of tuples (i.e., cardinality) and average number of tuples per
disk page equal to pageCard, a user may create a Hippo
index on attribute ai of R. When initializing the index,
Hippo sets the complete histogram resolution to H (it has
H buckets in total) and the partial histogram density to D.
Assume that each Hippo index entry summarizes P indexed
table pages (in terms of pages)/ T tuples (in terms of tuples).
P and T vary for each index entry. Queries executed against
the index have average selectivity factor SF .

(

(SF ∗ H) ∗ D
1

1
S∗H 6 D
1
SF ∗ H > D

SF ∗ H ∈ {1, 2, 3, 4, ...}
SF ∗ H should be no smaller than 1 no matter how small
SF is. Because the query predicate at least hits one bucket
of the complete histogram. Therefore, the probability in
Figure 5 is 20% × 10 × 0.2 = 40%. That means pages summarized by each index entry have 40% probability to be selected as possible qualified pages. Given the aforementioned
discussion, we observe the following:
Observation 1: When SF and H are fixed, the smaller
D is, the smaller P rob is.
Observation 2: When H and D are fixed, the smaller
SF is, the smaller P rob is.
Observation 3: When SF and D are fixed, the smaller
H is, the smaller P rob is.
In fact, the probability given above is equal to the percentage of inspected tuples in all tuples. In addition, considering
that Hippo index entries are much less than the inspected
tuples of the parent table, the total query time cost estimation is mainly decided by the time spent on inspecting
possible qualified pages. Thus, the query time cost estimation (in terms of disk I/O) can be concluded as follows:

6.1 Query time
7

Query time = (P rob ∗ Card)

(2)

Hippo index entries =

If we substitute P rob with its piecewise function, the
query time cost is as follows:

=

Card
T
1
H ∗ (H
+

(5)
Card
+ ... +

1
H−1

1
)
H−D∗H+1

(6)
Query time =

(

(SF ∗ H) ∗ D ∗ Card
Card

SF ∗ H 6
SF ∗ H >

1
D
1
D

D ∈ [ pageCard
, 1]
H
Some observations can be obtained from Formula 6:
Observation 1 For a certain H, the higher D there is,
the less Hippo index entries there are.
Observation 2 For a certain D, the higher H there is,
the less Hippo index entries there are. Meanwhile, the size
of each Hippo index entry is increasing with the growth of
the complete histogram resolution.
Index initialization time hinges on the number of disk I/Os
because it takes much more time than memory I/Os. In
general, the initialization time is composed of two parts:
retrieve parent tuples one by one and write index entry to
disk one by one. Accordingly, Hippo initialization time can
be deduced as follows:

SF ∗ H ∈ {1, 2, 3, 4, ...}

6.2 Indexing overhead
Indexing overhead which consists of storage overhead and
initialization time highly hinges on the number of index entries in an index. The more index entries there are, the more
disk writes and storage space an index costs. B+ -Tree and
other indexes take huge disk space and time for storing their
substantial nodes one by one.
The first problem in estimating the number of Hippo index
entries is that: how many disk pages (P ) are summarized by
one partial histogram in general? Or, how many tuples (T )
are checked against the complete histogram for generating
one partial histogram? Interestingly, this problem is very
similar with Coupon Collector’s Problem[6]. This problem
can be described like that: ”A vending machine sells H types
of coupons (a complete histogram with H buckets). Alice
is purchasing coupons from this machine. Each time (each
tuple) she can get a random type coupon (a bucket) but
she might already have a same one. Alice keeps purchasing
until she gets D ∗ H types of coupons (distinct buckets).
How many times (T ) does she need to purchase?”
Therefore, the expectations of T and P are determined by
the following formulas (The terms are defined in Table 2):
H
H
H
H
+
+
+ ... +
H
H −1
H −2
H −D∗H +1
1
1
1
+ ... +
)
=H ∗( +
H
H −1
H −D∗H +1
T
P =
pageCard

Hippo initialization time = Card + Hippo index entries
(7)
The number of Hippo index entries mentioned in the formula
above can be substituted by its mathematical expectation
in Formula 6.

6.3 Maintenance time
Data insertion. Hippo updates itself eagerly for data
insertion so that this operation is relatively time sensitive.
There are five steps cost disk I/Os in this update: retrieve
the complete histogram, locate associated Hippo index entry, retrieve the associated index entry, update the index
entry (if necessary) and update the mapped sorted list element. It is not hard to conclude that locating the associated index entry completes in log(Hippo index entries) I/O
times, whereas other four steps are able to accomplish their
assignments in constant I/O times. Thus the data insertion
time cost estimation model is summarized as follows under
different conditions:

T =

(3)
(4)

, 1]
D ∈ [ pageCard
H
The product of D ∗ H is the actual number of buckets
in each partial histogram. This value should be no smaller
than the tuples per disk page (pageCard) in case that each
tuple in a certain page hit one unique bucket.
For instance, in a Hippo, the complete histogram has 1000
buckets in total and the partial histogram density is 0.1. The
105.3
expectation of T and P will be 105.3 and pageCard
respectively. That means each partial histogram may summarize
105.3
pages under this circumstance. In another exampageCard
ple, if the total number of buckets is 10000 and the density
2230
is 0.2, T and P will be 2230 and pageCard
correspondingly.
After being aware of the expectation of the number of P ,
it is not hard to deduce the approximate number of index
entries in a Hippo. Thus the estimation of Hippo index
entries number is Formula 5. If we substitute T with their
mathematical expectations in Formula 3 and Formula 5 will
be changed to Formula 6. Hippo index size is equal to the
product of the number of index entries and the size of one
entry which roughly depends on each partial histogram size
(in compressed bitmap format).

Data insert time = log(Hippo index entries) + 4

(8)

Hippo index entries mentioned in Formula 8 can be substituted by its mathematical estimation in Formula 6.
Data deletion. Hippo updates itself lazily for data deletion so that it is hard to finalize a general estimation model.
However, it is recommended that do not update Hippo for
data deletion too frequently because Hippo will re-traverse
and re-summarize all disk pages summarized by one Hippo
index entry once it detects that one disk page has data deletion. This algorithm is more suitable for bulk deletion and
lazy update strategy.

7. EXPERIMENTS
This section provides extensive experiments of Hippo
along with reasonable analysis for supporting insights discussed before. For the ease of testing, Hippo has been implemented into PostgreSQL 9.5 kernel. All the experiments
are completed on PostgreSQL.
8

2
1
0

25x

25x

9

B+-Tree

Hippo

2.8x

6
3
2x

1.5x

0
2
20
200
TPC-H workload size (GB)

x 10000000

25x

Insertion time (ms)

Hippo

x 1000000

B+-Tree

Initialization time (ms)

x 10000

Index Size (MB)

3

5

Hippo

1200x

2
1
0
2
20
200
TPC-H workload size (GB)

2
20
200
TPC-H workload size (GB)

(a) Index size

B+-Tree

(c) Index insertion time

(b) Index initialization time

2
1
0

5

B+-Tree

Hippo

4
3
2
1
0

x 1000000

Hippo

Query time (ms)

B+-Tree

x 100000

5

Query time (ms)

x 10000

Query time (ms)

Figure 6: Index overhead on different TPC-H workload size
5

B+-Tree

Hippo

4
3
2
1
0

0.001%
Query selectivity (%)

0.01%
0.1%
1%
Query selectivity (%)

(a) 2 GB

(b) 20 GB

0.001%

0.01%
0.1%
1%
Query selectivity (%)

(c) 200 GB

Figure 7: Index query time on different TPC-H workload size
It is also worth noting that the final Hippo implementation in PostgreSQL has some slight differences from the
details above caused by some platform-dependent features
as follows:
Automatically inspect pages: Hippo only records possible qualified page IDs in a tid bitmap format and returns it
to the kernel. PostgreSQL will automatically inspect pages
and check tuples against query predicates.
Store the complete histogram on disk: Compared
with other disk operations, retrieving the complete histogram from PostgreSQL system cache is relatively slow so
that Hippo stores it on disk and executes a binary search on
it when query or update for data insertion and deletion. It
is better to rebuild Hippo index if there is a huge change of
the parent attribute’s histogram.
Vacuum tables to physically delete data: PostgreSQL DELETE command does not really remove data
from disk unless a VACUUM command is called automatically or manually. Thus Hippo will update itself for data
deletion when a VACUUM command is called.

Datasets and Workload. We use TPC-H workload in
the experiments with different scale factors (2, 20, 200). The
corresponding dataset sizes are 2 GB, 20 GB and 200 GB.
All TPC-H data follows an uniform distribution. We use
the largest table of TPC-H workload - Lineitem table in
most experiments and it has three corresponding sizes: 1.3
GB, 13.8 GB and 142 GB. We compare the query time of
Hippo with B+ -Tree through different query selectivity factors (0.001%, 0.01%, 0.1% and 1%). In addition, we also
test the two indexes using TPC-H standard queries 6, 15
and 20. We use TPC-H standard refresh operation (insert
0.1% new tuples into the DBMS) to test the maintenance
overhead of B+ -Tree and Hippo.
Experimental Setup. The test machine has 8 CPUs
(3.5 GHz per core), 32 GB memory, and 2 TB magnetic
disk with PostgreSQL 9.5 installed. Unless mentioned otherwise, Hippo sets the default partial histogram density to
20% and the default histogram resolution to 400. The impact of parameters is also discussed.

7.1 Implementation Details

7.2 Pre-tune Hippo parameters

We have implemented a prototype of Hippo inside the core
kernel of PostgreSQL 9.5 as one of the main index access
methods by leveraging the underlying interfaces which include but not limited to ”ambuild”, ”amgetbitmap”, ”aminsert” and ”amvacuumcleanup”. A database user is able to
create and query a Hippo index as follows:

Hippo is a flexible index which can be tuned by the
database user to perfectly fit his specific scenarios. There are
two parameters, partial histogram density D (Default value
is 20%) and complete histogram resolution H (Default value
is 400), discussed in this section. Referring to the estimation
before, both of them have impacts on index size, initialization time, and query time. For these experiments, we build
Hippo and B+ -tree on ”partkey” attribute in Lineitem table
of 200 GB TPC-H workload. As mentioned in Introduction,
B+ -Tree has 25 GB index size at this time.

CREATE INDEX hippo_idx ON lineitem USING hippo(partkey)
SELECT * FROM lineitem
WHERE partkey > 1000 AND partkey < 2000

7.2.1 Impact of partial histogram densities

DROP INDEX hippo_idx

9

3
2
1
0

B+-Tree

Hippo

4
3
2
1
0
400
1600
Hippo histogram resolution

Figure 8: Partial histogram density

Figure 9: Histogram resolution

Density (D)
Resolution (R)

6

B+-Tree

Hippo

4
2
0

20%
40%
Hippo partial histogram density (%)

Parameter
Default

x 1000000

4

5

Query time (ms)

Hippo

x 1000000

B+-Tree

Query time (ms)

x 1000000

Query time (ms)

5

Value
D=20% R=400

Size
1012 MB

Initial. time
2765 sec

40%

680 MB

2724 sec

80%

145 MB

2695 sec

800

822 MB

2762 sec

1600

710 MB

2760 sec

Q6
-

Figure 10: TPC-H standard queries

impact on the index size and initialization time is given in
Table 3 and the impact on query time is depicted in Figure 9.
As Table 3 illustrates, with the growth of histogram resolution, Hippo size reduces moderately. The explanation
is that Hippo which has higher histogram resolution consists of less partial histograms and each partial histogram in
this Hippo may summarize more pages but the partial histogram (in bitmap format) has larger physical size because
the bitmap has to store more bits.
As Figure 9 shows, the query time of three Hippos varies
with the growth of histogram resolution. This is because for
the large histogram resolution, the query predicate may hit
more buckets so that this Hippo is more likely to overlap
with query predicates and result in more pages are selected
as possible qualified pages. At this selectivity factor, Hippo
which has histogram resolution 400 is just a little bit worse
than B+ -Tree in terms of query time.

Table 3: Parameters affect Hippo indexing overhead

Hippo introduces a terminology ”partial histogram density” to dynamically control the number of pages summarized by one partial histogram. Based on the discussion
before, the partial histogram density may affect Hippo size,
initialization time and query time. The following experiment compares the default Hippo density (20%) with two
different densities (40% and 80%) and tests their query time
with selectivity factor 0.1%. According to the discussion in
Section 6.2, partial histograms under the three different density setting may summarize around 2 pages, 5 pages and 17
pages respectively (if one page contains 50 tuples). Thus it
can be estimated that the index size of 20% density Hippo
is around 2 times of 40% density Hippo and 8 times of 80%
density Hippo. The impact of the density on Hippo size and
initialization time is described in Table 3 and the impact on
query time is described in Figure 8.
It can be observed that as we increase the density, Hippo
indexing overhead decreases as expected (up to two orders
of magnitude smaller than B+ -Tree in terms of storage) because Hippo is able to summarize more pages per partial
histogram and write less index entries on disk. Similarly,
Hippo which has higher density costs more query time because it is more likely to overlap with query predicates and
result in more pages are selected as possible qualified pages.
At this selectivity factor, Hippo which has density 20% is
just a little bit worse than B+ -Tree in terms of query time.

7.3 Compare Hippo to B+ -Tree
This section compares Hippo with B+ -Tree in terms of
indexing overhead (index size and initialization time), index
maintenance overhead and index query time. To further illustrate the advantages of Hippo, we also compare these indexes using TPC-H standard queries. Hippo tested in this
section uses the default setting which has histogram resolution 400 and partial histogram density 20%.

7.3.1 Indexing overhead
The following experiment builds B-Tree and Hippo on
attribute ”partkey” in Lineitem table of TPC-H workload
(2 GB, 20 GB and 200 GB) and measures their indexing
overhead including index size and index initialization time.
Hippo only stores disk page pointers along with their summaries so that it may have much less index entries in contrast
with B+ -Tree. Thus it is not difficult to understand that
Hippo remains an index size which is lower than B+ -Tree.
In addition, referring to the discussion in the initialization
time estimation model, Hippo initialization time should be
far less than B+ -Tree because B+ -Tree has numerous nodes
to be written to disk.
As Figure 6a illustrates, the index size increases with the
growth of data size. The index size of Hippo is around
25 times smaller than that of B+ -Tree on all workload
sizes. Thus Hippo significantly reduces the storage overhead. Moreover, as Figure 6b shows, Hippo index initialization is at least 1.5x faster that of B+ -Tree.

7.2.2 Impact of histogram resolutions
Each partial histogram of Hippo is composed of some
buckets from the complete histogram. The number of buckets in this complete histogram represents the histogram resolution. The more buckets there are, the higher resolution
the complete histogram has. According to the discussion
before, the histogram resolution may affect index size, initialization time and query time. The following experiment
compares the default Hippo histogram resolution (400) with
two different histogram resolutions (800 and 1600) and tests
their query time with selectivity factor 0.1%. The density

7.3.2 Index maintenance overhead
Hippo updates itself eagerly after inserting a tuple into the
parent table. This eager update strategy for data insertion
10

is also adopted by B+ -Tree so that the two indexes can be
compared together. In terms of update time complexity, B+ Tree has approximate log(Card) and Hippo has (log(Hippo
index entries) + 4). Thus it can be predicted that, for inserting same percentage of tuples, while the update time of
Hippo and B+ -Tree is increasing with the growth of data
size. Hippo will take much less time to update itself than
B+ -Tree because Card is much larger than the number of
Hippo index entries. And also the difference of update
time between Hippo and B+ -Tree will be larger on larger
workload. The experiment uses TPC-H Lineitem table and
creates B+ -Tree and Hippo on attribute ”partkey”. Afterwards, TPC-H refresh transaction which inserts 0.1% new
tuples into Lineitem table is executed. The insertion time
of the indexes is compared in Figure 6c.
As Figure 6c shows, the two indexes take more time to
update on large workload. And also the difference between B+ -Tree and Hippo is more obvious (1200x) on the
largest workload as expected. This is because B+ -Tree
spends much more time on searching proper tuple insert
location (log(Card)) and its update time is increasing with
the growth of TPC-H workload.
Hippo updates itself lazily after deleting data which means
it updates itself after many data deletions occur. In contrast,
B+ -Tree takes an eager update strategy which has around
log(Card) update time cost. It may not make much sense
to compare the two indexes for data deletion.

Index
type

Fast
Query

Guaranteed
Accuracy

Low
Storage

Fast
Maintenance

B+ -Tree

✓

✓

✗

✗

Compressed

✗

✓

✓

✗

Approximate

✓

✗

✓

✗

Sparse

✗

✓

✓

✓

Hippo

✓

✓

✓

✓

Table 4: Compared Indexing Approaches

tor is 0.1%. Thus we find three TPC-H queries which have
typical range queries on ”l shipdate” attribute (Query 6, 15
and 20) and set the range query selectivity factor to 0.1%
which means one week. The query plans of the three queries
are described as follows:
Query 6 This query has a very simple plan. It firstly
performs an index search on Lineitem table using one of
the candidate indexes, then filters the returned values and
finally aggregates the values to calculate the result.
Query 15 This query builds a sub-view beforehand and
embeds it into the main query twice. The range query which
leverages the candidate indexes is a part of the sub-view.
Query 20 The candidate indexes are invoked in a subquery. Then range query results are sorted and aggregated
for calculation. The result is cached into memory and used
the upper level query.
As Figure 10 depicts, Hippo consumes similar query time
with B+ -Tree on Query 6, 15 and 20. The difference between the two indexes is more obvious on Query 15 because
this query invokes the range query twice. Therefore, we
may conclude that Hippo may achieve almost similar query
performance with B+ -Tree at the 25 times smaller storage
overhead when the query selectivity factor is 0.1%.

7.3.3 Impact of query selectivity factors
In this experiment, the query selectivity factors used for
B+ -Tree and Hippo are 0.001%, 0.01%, 0.1% and 1%. According to the query time cost estimation of Hippo, the corresponding query time costs in this experiment are 0.2Card,
0.2Card, 0.2Card and 0.8Card. Therefore, it can be predicted that there will be a great time gap between the first
three Hippo queries and the last one Hippo query. On the
other hand, B+ -Tree should be faster than Hippo at low
query selectivity factor like 0.001% but the difference between the two indexes should be narrowed with the growth
of query selectivity factors.
The result in Figure 7 perfectly matches our predication:
the last Hippo query consumes much more time than the
first three queries. Among them, query time of 0.1% selectivity factor query is a little higher than the first two because
it returns more query results which costs more to retrieve.
Both indexes cost more time on queries with the decreasing of query selectivity factors. B+ -Tree has almost similar
query time with Hippo at 0.1% query selectivity factor. It
is worth noting that B+ -Tree consumes 25 times more storage than Hippo. Therefore, we may conclude that Hippo
makes a well tradeoff between query time and index storage
overhead on medium query selectivity factors like 0.1% so
that, under this scenario, Hippo is a good substitution for
B+ -Tree if the database user is sensitive to aforementioned
index overhead.

8. RELATED WORK
Table 4 summarizes state-of-the-art database index structures in terms of query time, accuracy, storage overhead and
maintenance overhead.
Tree Index Structures: B+ -Tree is the most commonly
used type of indexes. The basic idea can be summarized as
follows: For a non-leaf node, the value of its left child node
must be smaller than that of its right child node. Each leaf
node points to the physical address of the original tuple.
With the help of this structure, searching B+ -Tree can be
completed in one binary search time scope. The excellent
query performance of B+ -Tree and other tree like indexes
is benefited by their well designed structures which consist
of many non-leaf nodes for quick searching and leaf nodes
for fast accessing parent tuples. This feature incurs two
inevitable drawbacks: (1) Storing plenty of nodes costs a
huge chunk of disk storage. As shown in Section 1, it results
in non-ignorable dollar cost and huge initialization time in
big data scenarios. (2) Index maintenance is extremely timeconsuming. For any insertions or deletions occur on parent
table, tree like indexes firstly have to traverse themselves
for finding proper update locations and then split, merge or
re-order one or more nodes which are out of date.
Compressed Index Structures: Compressed indexes
try to drop some repeated index information as much as
possible beforehand for saving space and recover it as fast
as possible upon queries from users but they all have guaranteed query accuracy. These techniques are applied to tree

7.3.4 TPC-H queries
To further explore the query performance of Hippo in the
real business decision support, we compare Hippo with B+ Tree using TPC-H standard queries. Both of the two indexes are built on ”l shipdate” attribute in Lineitem table
of 200 GB workload. As discussed before, Hippo costs similar query time with B+ -Tree when the query selectivity fac11

10. REFERENCES

indexes [8, 9] and bitmap indexes [7, 11, 14, 19] (low cardinality and read-only datasets). Though compressed indexes
are storage economy, they require additional time for compressing beforehand and decompressing on-the-fly. Compromising on the time of initialization, query and maintenance
is not desirable in many time-sensitive scenarios. Hippo on
the other hand reduces the storage overhead by dropping redundancy tuple pointers and hence still achieves competitive
query response time.
Approximate Index Structures: Approximate indexes [2, 10, 12] give up the query accuracy and only store
some representative information of parent tables for saving
indexing and maintenance overhead and improving query
performance. They propose many efficient statistics algorithms to figure out the most representative information
which is worth to be stored. In addition, some people focus on approximate query processing (AQP)[1, 18] which
relies on data sampling and error bar estimating to accelerate query speed directly. However, trading query accuracy
makes them applicable to limited scenarios. On the other
hand, Hippo, though still reduces the storage overhead, only
returns exact answer that match the query predicate.
Sparse Index Structures: Sparse index (denoted as
Zone Map Index in IBM Data Warehouse[3], Data Pack
structure in Infobright[13], Block Range Index in PostgreSQL[15], and Storage Index in Oracle Exadata[16]) is
a simple index structure implemented by many popular
DBMS in recent years. Sparse index only stores pointers
which point to disk pages of parent tables and value ranges
(min and max values) in each page so that it can save indexing and maintenance overhead. It is generally built on
ordered attributes. For a posed query, it finds value ranges
which cover or overlap the query predicate and then rapidly
inspects the associated few parent table pages one by one
for retrieving truly qualified tuples. However, for most real
life attributes which have unordered data, sparse index has
to spend lots of time on page scanning because the stored
value ranges (min and max values) may cover most query
predicates and encumber the page inspection. Therefore, an
efficient yet concise page summarizing method (i.e., Hippo)
instead of simple value ranges is highly desirable.

9.

[1] S. Agarwal, H. Milner, A. Kleiner, A. Talwalkar,
M. Jordan, S. Madden, B. Mozafari, and I. Stoica.
Knowing when you’re wrong: building fast and
reliable approximate query processing systems. In
SIGMOD, pages 481–492. ACM, 2014.
[2] M. Athanassoulis and A. Ailamaki. Bf-tree:
Approximate tree indexing. In VLDB, pages
1881–1892. VLDB Endowment, 2014.
[3] C. Bontempo and G. Zagelow. The ibm data
warehouse architecture. CACM, 41(9):38–48, 1998.
[4] D. Comer. Ubiquitous b-tree. CSUR, 11(2):121–137,
1979.
[5] T. P. P. Council. Tpc-h benchmark specification.
Published at http://www. tcp. org/hspec. html, 2008.
[6] P. Flajolet, D. Gardy, and L. Thimonier. Birthday
paradox, coupon collectors, caching algorithms and
self-organizing search. Discrete Applied Mathematics,
39(3):207–229, 1992.
[7] F. Fusco, M. P. Stoecklin, and M. Vlachos. Net-fli:
on-the-fly compression, archiving and indexing of
streaming network traffic. VLDB J., 3(1-2):1382–1393,
2010.
[8] J. Goldstein, R. Ramakrishnan, and U. Shaft.
Compressing relations and indexes. In ICDE, pages
370–379. IEEE, 1998.
[9] G. Guzun, G. Canahuate, D. Chiu, and J. Sawin. A
tunable compression framework for bitmap indices. In
ICDE, pages 484–495. IEEE, 2014.
[10] M. E. Houle and J. Sakuma. Fast approximate
similarity search in extremely high-dimensional data
sets. In ICDE, pages 619–630. IEEE, 2005.
[11] D. Lemire, O. Kaser, and K. Aouiche. Sorting
improves word-aligned bitmap indexes. Data &
Knowledge Engineering, 69(1):3–28, 2010.
[12] Y. Sakurai, M. Yoshikawa, S. Uemura, H. Kojima,
et al. The a-tree: An index structure for
high-dimensional spaces using relative approximation.
In VLDB, pages 5–16. VLDB Endowment, 2000.
[13] D. Ślezak and V. Eastwood. Data warehouse
technology by infobright. In SIGMOD, pages 841–846.
ACM, 2009.
[14] K. Stockinger and K. Wu. Bitmap indices for data
warehouses. Data Warehouses and OLAP: Concepts,
Architectures and Solutions, page 57, 2006.
[15] M. Stonebraker and L. A. Rowe. The design of
postgres. In SIGMOD, pages 340–355. ACM, 1986.
[16] R. Weiss. A technical overview of the oracle exadata
database machine and exadata storage server. Oracle
White Paper. Oracle Corporation, Redwood Shores,
2012.
[17] K. Wu, E. Otoo, and A. Shoshani. On the performance
of bitmap indices for high cardinality attributes. In
VLDB, pages 24–35. VLDB Endowment, 2004.
[18] K. Zeng, S. Gao, B. Mozafari, and C. Zaniolo. The
analytical bootstrap: a new method for fast error
estimation in approximate query processing. In
SIGMOD, pages 277–288. ACM, 2014.
[19] M. Zukowski, S. Heman, N. Nes, and P. Boncz.
Super-scalar ram-cpu cache compression. In ICDE,
pages 59–59. IEEE, 2006.

CONCLUSION

The paper introduces Hippo a sparse indexing approach
that efficiently and accurately answers database queries
while occupying up to two orders of magnitude less storage
overhead than de-facto database indexes, i.e., B+ -tree. To
achieve that, Hippo stores pointers of pages instead of tuples
in the indexed table to reduce the storage space occupied
by the index. Furthermore, Hippo maintains histograms,
which represent the data distribution for one or more pages,
as the summaries for these pages. This structure significantly shrinks index storage footprint without compromising much on performance of common analytics queries, i.e.,
TPC-H workload. Moreover, Hippo achieves about three
orders of magnitudes less maintenance overhead compared
to the B+ -tree. Such performance benefits make Hippo a
very promising alternative to index data in big data application scenarios. Furthermore, the simplicity of the proposed
structure makes it practical for database systems vendors to
adopt Hippo as an alternative indexing technique. In the
future, we plan to adapt Hippo to support more complex
data types, e.g., spatial data, unstructured data.
12

SMILE: A Data Sharing Platform
for Mobile Apps in the Cloud
Jagan Sankaranarayanan
Hakan Hacıgümüş
NEC Labs America, Cupertino, CA

∗

Haopeng Zhang

∗

Mohamed Sarwat

University of Massachusetts Amherst

University of Minnesota

haopeng@cs.umass.edu

sarwat@cs.umn.edu

{jagan,hakan}@nec-labs.com
ABSTRACT

get a seamless experience as the three apps now behave as a single
entity.

We identify an opportunity to share data among mobile apps hosted
in the cloud, thus helping users improve their mobile experience,
while resulting in cost savings for the cloud provider. In this work,
we propose a platform for sharing data among mobile apps hosted
in the cloud. A “sharing” is specified by a triple consisting of: (a)
a set of data sources to be shared, (b) a set of specified transformations on the shared data, and (c) a staleness (freshness) requirement
on the shared data. The platform addresses the following two main
challenges: What sharings to admit into the system under a set of
specified constraints, how to implement a sharing at a low cost while
maintaining the desired level of staleness. We show that reductions
in costs are achievable by exploiting the commonalities between the
different sharings in the platform. Experimental evaluation is performed with a cloud platform containing 25 sharings among mobile apps with realistic datasets containing user, social, location and
checkin data. Our platform is able to maintain the sharings with very
few violations, even under a very high update rate. Our results show
that our method results in a cost savings of over 35% for the cloud
provider, while enabling an improved mobile experience for users.

Interestingly, with the increasing use of cloud-based resources,
many of these apps may be hosted in the same cloud infrastructure
(e.g., Amazon EC2). To enable such rich interactions, mobile apps
should make their datasets available for sharing, as a way of encouraging other apps to build complementary features. At the same time,
apps can consume several datasets from other apps in the cloud infrastructure. We identify two key considerations in sharing data that
is important to mobile app developers.
• App developers want reliable access to datasets and do not
want to deal with the complexity of creating and maintaining
mechanisms (e.g., APIs [1, 4] or web services [2]) for sharing
data. Furthermore, they desire a service that is flexible enough
to meet their needs while providing guarantees on the quality
of the service.
• App developers want timely access to datasets. Mobility of
users imposes limits on how much staleness app developers
can tolerate on the datasets. This is because many types of
user-related data get progressively less valuable with time. For
instance, the location of a mobile user that is 50 seconds stale
may be of limited use to a navigation app; however, 10 seconds stale data may be suitable.

1. INTRODUCTION
Mobile applications (apps) compete in an increasingly crowded
marketplace, with possibly thousand of apps performing similar or
identical functions. In this crowded marketplace, developers can differentiate their apps by offering features that make the user’s mobile
experience more personalized. For instance, apps like SpotiSquare
connect with Foursquare venues to determine the current location
(venue) of the user, and then choose a music playlist depending on
users’ current context (e.g., eating dinner, exercising, driving etc.).
To create such an experience requires that the app has access to additional information (i.e., datasets) about its user. The following
example shows possible interactions among three apps, showcasing
the benefits of sharing user information.

In this paper, we propose a data sharing service in the cloud called
SMILE (Sharing MIddLEware). The service provider, who manages
the cloud infrastructure, offers data sharing as service and like any
commercial business makes money by delivering services according
to agreed upon quality of service levels. Data sharing is achieved
by a mobile app developer (henceforth referred to as a consumer)
specifying a sharing. A sharing must identify datasets of interest,
the desired transformations on the data, and a staleness requirement.
The consumer and the provider enter into a Service Level Agreement
(SLA). The SLA is a contract specifying that the provider will ensure reliable access to the consumer on the shared data at the risk of
paying a penalty if it is not maintained at the agreed upon staleness.
To successfully achieve data sharing on a large scale, two practical
problems need to be solved by the provider. First, it is important to
determine if a new sharing can be admitted (i.e., accepted) into the
system and maintained at the appropriate staleness. This may not
always be possible, especially if the datasets are updated at a high
rate and the sharing needs to be maintained at a low staleness. If a
sharing is incorrectly admitted, it will result in significant losses for
the provider since the SLA may specify penalties for the provider in
case the sharing misses the staleness requirement. Second, implementing the sharings is not free in the sense that the provider has to

E XAMPLE 1. Consider the three apps — Opentable (restaurant
reservation), Plango (calendar) and Sonar (friends location monitoring). Appointments of users requiring dinner reservations are shared
by Plango with Opentable, which can then suggest restaurant options
to users. Sonar can suggest a nearby restaurant as a meeting place
by sharing their location information with Opentable. Mobile users
∗Work done while at NEC Labs
(c) 2014, Copyright is with the authors. Published in Proc. of EDBT on
OpenProceedings.org. Distribution of this paper is permitted under the
terms of the Creative Commons license CC-by-nc-nd 4.0

688

10.5441/002/edbt.2014.75

2.

pay for the resources (i.e., CPU, Disk, Network) consumed in the
cloud. Reducing provider cost is another important consideration.
The two practical problems we outlined above pose significant
technical challenges that we address in this paper.

RELATED WORK

While there has been some work on sharing in a mobile environment, they consider sharing either in an adhoc setting, such as between two mobile users [19], or among a group of mobile users [22].
A middleware for connecting mobile users and devices has been proposed [27] for providing various mobile services, such as management, security, and context awareness, but not for sharing.
Sharing using MVs adds interesting dimensions to a well studied problem domain. An MV maintenance process traditionally is
broken into a propagation step, where updates to the MV are computed and an apply step, where updates are applied to the MV. First
of all, the autonomy of the tenants means that synchronous propagation algorithms [10], where all sources are always at a consistent snapshot, are unsuitable for our purposes. Furthermore, to deal
with the autonomy of the tenants, one has to resort to a compensation algorithm [28], where the propagation is computed on asynchronous source relations [5, 24, 29]. In particular, MVs over distributed asynchronous sources have been studied in the context of
a single data warehouse [5, 29] to which all updates are sent. The
key optimization studied in [5, 29] is in terms of reducing the number of queries needed to bring the MVs to a consistent state in the
face of continuous updates on the source relations. [24] shows how
n-way asynchronous propagation queries can be computed in small
asynchronous steps, which are rolled together to bring the MVs to
any consistent state between last refresh and present. Reducing the
cost of maintenance plans of a set of materialized view S is explored
in [20], where common subexpressions [23] are created that are most
beneficial to S. Their optimization is to decide what set of common
subexpressions to create and whether to maintain views in an incremental or recomputation fashion. Staleness of MVs in a data warehouse setup is discussed in [16], where a coarse model to determine
periodicity of refresh operation is developed.
As we will see later in the paper, our setup is different from [5,29]
in the sense that multiple MVs are maintained on multiple machines
in our multitenant cloud database. Moreover, different update mechanisms with different costs and staleness properties can be generated
based on where the updates are shipped as well as where the intermediate relations are placed, making the problem harder than [5,29].
Next, [24] assumes that all the source relations are locally available on the same machine, which makes the application of their approach to our problem infeasible without an expensive distributed
query. We combine propagation queries from [24] with join ordering [11, 18], such that propagation queries involving n source relations are computed in a sequence involving two relations at a time,
requiring no distributed queries. In particular, we first ensure that
the update mechanisms can provide SLA guarantees, after which
common expressions among the various sharing arrangements are
merged to reduce cost for the provider, which is similar in spirit
to [20, 23]. Our work adds several additional dimensions to [20, 23]
in terms of placement of relations, capacity of machines, SLA guarantee, and cost.
In contrast to [16], which determines the periodicity of the refresh operation of MVs maintained in a warehouse, our work is distinguished in the following way. Our work develops refresh cycles
for multiple MVs from distributed sources with different staleness
requirements while simultaneously reducing the total maintenance
cost. This is significantly more complicated than the simple setup
in [16] where they develop a simple model for determining a single
refresh periodicity between a RDBMS and data warehouse without
considering cost.
Our work is related to traditional view selection problems [6] in
the sense that the set of sharing arrangements could have been obtained via the application of a view selection algorithm taking the

1. Maintaining the shared datasets at required staleness: The
shared datasets are maintained as materialized views (MVs)
and are always kept under the staleness specified in the SLAs.
This is challenging because the sharings involve multiple
datasets with varying staleness requirements. Note that there
are many other ways (e.g., APIs, web services) of enabling
sharing in the cloud; a discussion on the various methods and
the pros-and-cons of each is given in [25].
2. Testing for admissibility: As noted above, there is a need
for an effective method to decide whether to accept or decline a sharing agreement under multiple constraints, such as
the given staleness, SLA penalty and platform cost considerations.
3. Cost reduction for the provider: To reduce cost, the platform provider needs to identify commonalities across multiple sharings with different staleness requirements, in order to
save computational effort.
These three problems may not be specific to data sharing for mobile
apps only, but also applicable to other areas. However, we observe
that mobility makes these problems challenging and the solutions
much more relevant in real world settings.
Some of these problems have been previously considered in the
context of MV maintainance [5, 16, 23, 24] and multi-query optimization [11, 26]. However, prior work either considers the mechanics of MV maintainance [24] and refresh rates [16], or cost savings by removing commonalities [23, 26], but not staleness and cost
requirements simultaneously. While important, the feasibility and
economic value of sharing as well as infrastructure and privacy considerations are considered by [9, 13, 25], thus it is not the focus in
this work.
In this work, we focus on the practical and technical challenges
of enabling data sharing for mobile apps in the cloud. Mobility provides a perfect use-case scenario as it aligns with the three elements
of our problem setup: it is cloud-based, requires reliable access to
rich information, and has strict staleness requirements on datasets.
To our knowledge, this work represents the first systematic, cloudbased platform for enabling data sharing with staleness guarantees.
We make the following contributions in this paper.
1. A declarative sharing platform, which is fully implemented as
a part of industrial system, with staleness guarantees on the
sharings (Section 3).
2. A method of determining the admissibility of sharings ensures
that the system only admits those sharings that it can maintain
(Section 6).
3. A method for reducing the cost of maintaining the sharings by
amortizing work across multiple sharings, where each sharing
has its own constraints (Section 7).
4. Experimental evaluation is performed on a cloud platform
with 25 sharings posed on realistic user, location, social, and
checkin datasets. Our results show that the SMILE platform
can maintain a large number of sharings with very low SLA
violations, even under a high rate of updates. By amortizing
work across multiple sharings, SMILE is able to achieve a cost
savings of over 35% for the provider (Section 9).

689

consumer workload as input. Our problem shares common aspects
with the cache investment problem [14] in terms of placement (what
and where to be cached) of intermediate results and the goodness
(another notion of staleness) of cache. Cache refresh in [14] piggy
banks on queries, whereas we establish a dedicated mechanism to
keep the MVs at the desired staleness. Our work shares common
elements with [15] in the sense that merging data flow paths with
common tuple lineage is similar to the way we perform plumbing
operations on a sharing plan.
A related data sharing effort in the cloud is the F LEX S CHEME [8],
where multiple versions of shared schema are maintained in the
cloud, with the focus on enabling evolution of shared schema used
by multiple tenants. Data markets [9] for pricing data in the cloud
looks at the problem from tenant and consumer perspectives, but we
look at the problem from the provider’s perspective. A similar but
not identical problem is reducing the cost for the consumers (i.e., fair
pricing of common queries) [9] and sharing work across concurrent
running queries [26]. Although we only concern ourselves with the
staleness of the data as the only quality measure of the data being
shared, other considerations such as data completeness, accuracy are
also applicable here [7]. In our problem, the challenge is to maintain
the sharing arrangements always maintained on agreed upon terms
(i.e., SLAs), while keeping down the infrastructure costs. Satisfying
these dual goals makes the sharing problem challenging from the
provider’s perspective.

E XAMPLE 2. Plango (i.e., Calendar app) makes the base relation of User_Events of events extracted from users’ calendar
available for sharing. Opentable has its own relation User_Accts
of users that use the app. It specifies a sharing, “I want to know
about dinner events for the users who use my app within 10 seconds
of a new event being recorded so that I could offer recommendations to them.” The base relations are User_Events and User_Accts, and the transformations are specified as the following SPJ
query: EventType=“dinner" from a join of User_Events
and User_Accts. The staleness t is specified as 10 seconds and
the penalty pens is $.001 per late delivery of a tuple.
After the sharing Si is defined as per the above example, it is
given to the provider for deciding admissibility and implementation
in the platform, which is described in Section 6.

4.

SMILE ARCHITECTURE

Figure 1 shows the architecture of the system. There is a set of
machines available to implement the sharings. Each machine runs a
single database instance (Postgresql in our case). The SMILE platform consists of three main components — (a) delta capture, (b)
sharing optimizer, and (c) sharing executor — that perform the following functions, respectively: (a) capture changes (i.e., delta) on
the base relations as updates are applied on them; (b) generates plan
for moving these updates from the base relations to the MVs; and
(c) schedules the movement of these updates by taking system fluctuations into account. We briefly describe the three main system
components below.

Sharing Plan
Machine

Agent

Agent

Machine

¢R
Postgresql

Database

Gateway

R

Machine

DELTA CAPTURE

MV

Push

Workload

Machine

Heartbeat

The provider has to consider a set S of sharings {S1 , S2 · · · Sm },
for inclusion in the sharing platform. Here, we consider the case
where there are no existing sharings in the system, yet the solution
we develop is equally applicable to the case when the platform already has several prior sharings. Our solution that we later develop
will identify which of the sharings in S should be admitted into
the sharing framework, while at the same time minimizing provider
cost and meeting SLA requirements. Each Si specifies the applicable datasets, transformations, staleness requirements and penalties
as described next.
To specify a sharing Si , a consumer starts by identifying datasets
(i.e., base relations) of interest, or subsets of datasets. Next, the consumer must determine a way to combine these datasets by specifying
transformations on the data. In this work, we restrict the transformations on the base relations to include the following three operators.

Agent

Postgresql

3. PRELIMINARIES

Agent

Infrastructure

Pub/Sub
Input
Sharing
Sharing
Sharings S
Executor
Optimizer
Data Sharing Framework

1. Choose a subset of tuples using a selection predicate

Figure 1: Architecture of the sharing platform

2. Choose a subset of the attributes

4.0.1 Delta Capture and Timestamps

3. Combine base relations using a common key

As the base relations are updated, a delta capturing mechanism
(i.e., tuple creation, deletion or updates) records the modified tuples. Our mechanism uses the Streaming Replication facility [3] in
Postgresql to capture the deltas. This module in Postgres allows
the Write Ahead Log (WAL) to be streamed on to another Postgresql instance in recovery mode so that a nearly identical replica
of a database can be maintained. Our module fakes itself as a Postgresql instance and obtains a WAL stream. The modified tuples are
extracted from the stream, unpacked and written to the disk.
Every base relation R is associated with a delta relation, denoted
by ∆R that records the modified tuples as update queries are applied
on R. The tuples in ∆R are populated by the delta capturing module. The MVs in the system also contain corresponding delta tables.
If R is a MV then ∆R contains both prior updates as well as those
that have not yet been applied to R. The tuples in ∆R of a MV is
populated, moved and applied by the sharing executor.

In other words, the transformation can be specified using a SelectProject-Join (SPJ) query that is applied on the base relations. The
consumer next specifies a staleness requirement t expressed in time
units (e.g., 20 seconds) on the shared data as well as any applicable penalty pens . A sharing in SMILE is enabled by the creation
of a materialized view (MV), which describes the transformations
over the base relations. For each sharing Si , the system creates a
MV which is always maintained within a staleness of t time units as
specified by Si . This means even though the base relations are independently updated, the state of the MV is always consistent with the
state of the base relations within t seconds.
As an illustration of defining a sharing, we revisit Example 1 and
provide a more concrete example of a sharing that the Opentable
(i.e., restaurant) app may define using the SMILE platform.

690

Every relation, delta of a relation or MV in our platform records
its last modification timestamp. The timestamps are generated using
a distributed clock [17] that is periodically synchronized. Each tuple
in the delta also records an associated timestamp. Maintaining the
sharings at their appropriate level of staleness is achieved by keeping track of the last modification timestamps of the base relations
and comparing them to the timestamp of the MV. The SMILE system maintains an up-to-date timestamp information on each sharing,
hence is aware of the current staleness of all the sharings. Updates
are moved from the base relations to the MV in a way that ensures
that the sharings do not miss their SLAs.

4.0.2

that the vertices are relations or deltas of relations tied to a particular machine, and the edges apply transformational operators. The
plan is expressed using the following four edge operators, that 1)
apply updates (DeltaToRel) , 2) copy updates between machines
(CopyDelta) , 3) join updates (Join), and 4) union (i.e., merge) updates (Union).
As the plan operates on base relations that are asynchronously
updated, the input vertices to an operator may have different timestamps. An operator takes any mismatch in the timestamps into
account by rolling back all the input vertices to the minimum of
the timestamps among its inputs. This is referred to as compensations [28]. Rolling back the timestamp of a relation or a MV is
possible due to the delta relations associated it. The operators for
applying, copying and merging updates are based on their standard
interpretations, except that they additionally apply compensations to
the inputs as the first step. Our join operator performs a compensation which is an implementation of the algorithm from [28].
We will not provide the implementational details of the operators
but instead show an example of a plan that performs a relational join
on two asynchronous base relations A and B on different machines.
The plan is referred to as “in-place” as it does not involve making
the copies of the base relations. The vertices and the edge operators
in the plan periodically move the updates from the base relations to
the MV to keep it maintained incrementally.

Sharing Optimizer

Given a set S of new sharings, the sharing optimizer generates an
update mechanism for each sharing in S using a three step procedure
described below.
a. A sharing Si in S can be admitted if the system can maintain
Si at the desired level of staleness. This determination is necessary to prevent the system from entering into SLAs that it
cannot satisfy (Section 6).
b. If Si is admissible, we generate its sharing plan such that it
can move updates from the base relations to the MV within
the time specified in the staleness SLA. Moreover, the sharing
plan is also cost effective in terms of its infrastructure resource
consumption (Section 6).

A

ΔA

B

DELTA CAPTURE

ΔB

c. Once the individual sharing plans of all the sharings in S are
determined, commonalities across sharings are identified and
removed to produce a single global sharing plan D that implements all the sharings (Section 7).

4.0.3

DELTA CAPTURE

ΔA

COPY
UPDATES

JOIN

JOIN

Δ(A⋈ΔB)

Δ(ΔA⋈B) Machine m

Machine m1

Sharing Executor

2

COPY
UPDATES

COPY
UPDATES

Δ(A⋈ΔB) UNION

The sharing executor is the execution engine of the system which
maintains the sharings at or below the required staleness level. The
sharing executor is an implementation of an asynchronous view
maintenance algorithm [24].
The sharing executor computes the current staleness of a sharing
by taking the difference between the maximum of the timestamps of
all the base relations to that of the MV. The executor keeps track of
which of the sharings will soon miss their staleness SLA. It schedules the updates to be applied on the MV so that its staleness is
reduced. Each machine in the infrastructure runs an agent that communicates with the sharing executor via a pub/sub system (e.g., ActiveMQ). The agents send periodic messages to the sharing executor
with the last modification timestamps of the base relations and the
MVs.
Our implementation of the executor is lazy by design in the sense
that it does not refresh unless it is absolutely necessary or the sharing
will miss its SLA. This way, the executor bunches as much work as
possible thereby reducing redundant effort. The refresh is neither too
early nor too late, but finishes just before a sharing is about to miss
its staleness SLA. We provide more details on the sharing executor
in Section 8.

ΔB

UPDATES

Δ(A⋈B)
Machine m3

Δ(ΔA⋈B)
A⋈B

APPLY UPDATES

Figure 2: One possible plan involving an in-place join of a base
relation A on machine m1 and B on machine m2 such that the
resulting MV A 1 B is placed on machine m3

E XAMPLE 3. Figure 2 shows the plan of a sharing Si that performs a transformation A 1 B on two base relations, A and B.
The plan is a DAG consisting of 12 vertices and 10 edges. The vertices are either base relations (e.g., A, B or its copies), MVs (e.g.,
A 1 B) or delta relations (e.g., ∆(∆A 1 B)). The edges corresponds to operators that either apply, copy, merge, join updates,
to complete the transformation path from the base relations to the
MV. Note that select and project predicates can be specified in Si ’s
transformation. All the four edge types can apply select and project
predicates to their inputs if one is specified in addition to their usual
functionalities. We handle these predicates by using the pushdown
heuristic [11].

5. SHARING PLAN

Given a sharing that specifies a set of transformations on the base
relations, the plan generation algorithm enumerates all the plans that
implement the sharing. However, not all of the plans satisfy the
constraints we develop in the reminder of this section. In particular,
we concern ourselves with two key properties of a plan, namely its
critical time path and dollar costs, which are described below.

The update mechanism of a sharing is implemented as a sharing
plan, which is analogous to a query execution plan in databases. We
will henceforth refer to it simply as a plan in the rest of the paper. The plan is expressed in terms of four operators that form the
transformational path for the updates from the base relations to the
MV. This is represented using a Directed Acyclic Graph (DAG) such

691

5.1

Critical Time Path

The critical time path of the plan is the longest path in terms of
seconds that represents the most time consuming data transformation path in the plan. Note that the plan is admissible only if the
length of its critical time path is less than the required staleness of
the sharing, or else the system cannot maintain it.
The sharing optimizer estimates the critical time path of a plan
using a time cost model for each operator. The model estimates the
time taken for each operator given the size of the updates. Note that
finding the longest path between two vertices on a general graph
is an NP-hard problem, but the plans are DAGs, on which longest
path calculation is tractable. The system implements the procedure
CP(p) that takes a sharing plan p and outputs its critical time path
in seconds. For example, in the plan p shown in Figure 2, CP(p)
computes the time taken along the longest transformation path from
A or B to the MV A 1 B. Section 9 provides additional details on
how we developed the time cost model for the four operators.

5.2

C OST(p) = resCost(p) · (1 +

CP (p)
) + e(λ−µ)·s · pens
s

(1)

resCost(p) is the cost of resource usage. As discussed before, to avoid SLA violation due to multiple sharings competing for resource, we over-provision the resource by a factor of
CP (p)/s where CP (p) is the length of the critical time path of
p. e(λ·a−µ)·s · pens is the estimated penalty of missing the staleness
SLA due to higher-than-expected tuple arrival rate, where pens is
the penalty of missing the staleness SLA for a single tuple.

6.

SHARING OPTIMIZER

The goal of the sharing optimizer is to produce a low-cost admissible plan. Satisfying the dual constraints of finding an admissible
plan that is provably cheapest amongst all plans is a hard problem.
A sharing Si specifies SPJ transformations on a set of base relations. As the base relations are hosted on different machines, there
are several ways of combining them as well as where to place the
intermediate results. This results in plans with varying dollar cost
and critical time paths. For instance, performing many operations
in parallel on different machines may produce a plan with a small
critical time path. But such a plan may have a high dollar cost due to
high infrastructure costs involved in using many machines. On the
other hand, operations can also be performed sequentially to reduce
the dollar cost but at the expense of a high critical time path.
Among the generated plans those that have a critical time path
greater than the SLA of Si cannot be maintained by the system at the
desired staleness level, and hence are not admissible. The admissibility of plans forms the hard constraint of our problem in the sense
that the system should not admit a sharing that cannot be handled
by the system. At the same time, it also should not deny admitting
sharings that otherwise should have been admissible.
The sharing optimizer is based on the polynomial time heuristic
solution developed for System-R [11] and its analogous distributed
variant R∗ [18]. Our approach relies on generating, using a dynamic
programming approach, the cheapest possible plan in terms of dollar
costs, regardless of its critical time path and another plan with the
smallest critical time path, regardless of its dollar costs. We refer to
these plans as Dynamic Programming Dollar (DPD) and Dynamic
Programming Time (DPT), respectively. The DPD and DPT plans
have the following properties:

Cost Model

The cost of the plan, expressed in dollars per second, is computed by the amount of CPU, network, and disk capacity consumed
to setup the sharing and maintain it at the required staleness. The
provider periodically moves the updates to the MVs and buys CPU,
disk and network capacities from the Infrastructure as a Service
(IaaS). This cost can be further divided into two categories: resource
usage (i.e., CPU, disk capacity, network capacity) and penalty due
to possible SLA violations.
Resource Usage. There are existing analytical models that estimate the usage of various resources for maintaining a MV, based on
update rate, join selectivity, data location, etc. (e.g., [21]). This
analytical model is implemented as a resCost function that computes the cost of the resources consumed by a plan. Furthermore,
the resource usage should also vary with the staleness SLA of the
sharing. When the required staleness is much longer than the critical time path, e.g., the critical time path is 1 second and the staleness
requirement is 30 seconds, the sharing executor has much flexibility in deciding when to update the MV. Specifically, given a new
tuple to the base relations, the service provider can push it to the
MV immediately, or wait for as long as 29 second before pushing it.
On the other hand, when the staleness becomes close to the critical
time path, there is much less flexibility since other sharings in the
infrastructure may compete for resources.
In order to reduce the negative interaction at low staleness values,
the resources allocated to the plan are over-provisioned by a factor
that is inversely proportional to the required staleness. This simple
strategy ensures that the negative interactions are mostly avoided at
low staleness values.
SLA Penalty. At low staleness values the natural fluctuations in
the update rates may cause a plan to miss the SLA. This is because
the plan estimates the critical time path using the average arrival rate,
but in practice this is an over simplification as the updates frequently
vary. So, we have to estimate how much of penalty may be incurred
given the required staleness, which also has to be factored into the
cost. We estimate this by assuming a Possion arrival of updates, and
modeling the plan as a M/M/1 queuing system. Given the arrival rate
of each base relations, we can estimate the arrival rate of tuples in
the MV based on the selectivity of joins. The average service time of
the M/M/1 queue corresponds to the most time consuming operator
in the plan.
For an M/M/1 queue with arrival rate λ and service rate µ, the
percentage of items with sojourn time t larger than the staleness SLA
s is P (t > s) = e(λ−µ)·s . Thus the dynamic cost of a plan p with
staleness s is calculated as:

1. DPT is a plan with a low critical time path that is not optimized
on the operating dollar cost. If DPT is not admissible, then the
sharing can be safely rejected by the provider as there cannot
be a plan with a lower critical time path.
2. DPD is a plan with a low operating dollar cost that is not optimized on the critical time path. If DPD is admissible, then it
is also of the cheapest cost.
We provide a dynamic programming formulation to produce DPT
and DPD in Section 6.1, and provide a plan generation algorithm in
Section 6.2.

6.1 Dynamic Programming Formulation
We cast the problem of generating a plan as a bottom-up dynamic
programming formulation given by J OIN C OST in Algorithm 1. Consider a sharing that specifies a join sequence on the base relations.
For example, Figure 2 shows a join sequence of length two using the
two base relations, A and B.
Let Si be a sharing in S such that S RC(Si ) is the set of source
vertices of Si and MV(Si ) is the vertex corresponding to the MV

692

ΔA
ΔB

B

COPYDELTA
COPYDELTA

JOIN

Δ(A⋈ΔB)

m1

COPY
DELTA Δ(A ΔB)
⋈
UNION

Δ(A⋈B)

COPY
DELTA

ΔB
JOIN

Δ(ΔA⋈B)

Δ(ΔA⋈B)
DELTA
TOREL

m3

A⋈B

ΔA

m2

A

B
JO
IN

(a) (b)

B

ΔB

m2

COPYDELTA

JOIN

Δ(A⋈ΔB)
COPY
DELTA

ΔA
ΔB

Δ(ΔA⋈B)

B
A

m1

UNIO

Δ(ΔA⋈B)

A

A

ΔA
JOIN

DELTA

m1

Δ(A⋈ΔB)

Δ(ΔA⋈B)

COPYDELTA

m3

Δ(ΔA⋈B)
UNION

N

m3 Δ(A⋈B)

Δ(A⋈B) DELTATOREL A⋈B

m1

JOIN

COPYDELTA

COPYDELTA

Δ(A⋈ΔB)

ΔB

ΔA COPY

m2

ΔA

B

COPY
DELTA
DELTATOREL

ΔA

A⋈B

DELTA
TOREL

A

m3

ΔB

B
JOIN

JOIN

Δ(A⋈ΔB)
DELTATOREL

ΔB

m2

COPY
DELTA

A

(c) (d) Δ(ΔA⋈B)

Δ(A⋈ΔB)
UN

ION

Δ(A⋈B)

A⋈B

DELTATOREL

Figure 3: Four ways of joining A and B, (a) in-place (no copies of A or B), (b) copy B (c) copy A, (d) copy A and B.
Algorithm 1 sub J OIN C OST(<R, mi >)
1: for all join sequences R − a do
2: for mj ∈ set of available machines do
3:
(C AP<R−a,mj > , D <R−a,mj > ) = J OIN C OST(<R − a, mj >)
4:
for mk ∈ set of available machines do
5:
C AP0 ← C AP<R−a,mj >
6:
D 0 ← D <R−a,mj >
7:
Choose cheapest case among (a)–(d), update C AP0 and D 0
8:
Case (a): In-place join of <R − a, mj > with <a, mk >
9:
Case (b): Copy <R − a, mj > to <R − a, mk >. In-place join of

of Si . SLA(Si ) is the staleness SLA of Si . M AC(Si ) denoted the
set of dedicated machines available to host the sharing. Below, the
short-hand notation <v, m> denotes any vertex v (i.e., relation, MV
or a delta of a relation) in the plan that is placed on a machine m.
We capture the state of the problem up on creating a join sequence
R on a machine m (i.e., <R, m>) as (D<R,m> , C AP<R,m> ) such that:
1. D<R,m> is the cheapest plan among all those that produce R
on machine m. The cheapest plan is chosen by applying a
cost function C OST C ALC, which takes a plan as input and
produces a cost value. Later, we will specify two implementations of C OST C ALC that will produce the DPT and DPD
plans.

10:
11:
12:

<R,m>

13:
14:
15:
16:

2. C AP
is the remaining capacity in the infrastructure (e.g.,
CPU, disk, network capacities) after discounting the capacity
consumed by D<R,m> .
We generate the join sequence R at machine m bottom-up by
enumerating all the states corresponding to: a) R−a on any machine
in M AC(Si ), b) with any remaining capacity. R − a refers to a
join sequence R without a base relation a. D<R,m> is generated by
adding vertices and edges required to join R − a with a to the plan
from the prior state. Among the plans generated this way, we choose
the plan with the smallest value produced by C OST C ALC.
Such a formulation is used by J OIN C OST to obtain the plan of any
arbitrary join sequence R on mi ∈ M AC(Si ), which is formed by
joining R − a on mj ∈ M AC(Si ) (i.e., <R − a, mj >) with a base
relation a on machine mk ∈ M AC(Si ) (i.e., <a, mk >).
At a high level, given two relations A on machine m1 , and B on
machine m2 , we consider four ways of producing A ./ B on a machine m3 , which are illustrated in Figure 3. In particular, Figure 3a
shows the case where A ./ B is produced without copying any of
the base relations (i.e., in-place join). Figure 3b–c show the cases
when one of the base relations is copied. Figure 3d shows the case
when both A and B are copied to m3 , and then an in-place join is
performed. Note that the four cases (a)–(d) in the J OIN C OST formulation (lines 8–11) correspond to the four ways of joining two
relations A and B in Figure 3.
J OIN C OST uses an in-place join and a copy procedure to update
the plan D<R−a,mj > to produce R on machine mi . We explain below the procedure of creating a copy of a relation and joining two
relations using A and B in place of R − a and a, respectively.
To copy of <A, m1 > to form <A, m2 > requires the addition
of one vertex and two edges – vertex <A, m2 > and CopyDelta
between <∆A, m1 > and <∆A, m2 >, and DeltaToRel (apply updates) between <∆A, m2 >. An in-place join between <A, m1 > and
<B, m2 > to produce <A ./ B, m3 > can add up to 8 vertices and 8
edges, as shown in Figure 3 depending on whether m1 , m2 and m3
are all distinct machines.
After adding the necessary vertices and edges, the capacities of
machines involved are modified using the resCost function defined

<R − a, mk > and <a, mk >
Case (c): Copy <a, mk > to <a, mj >.
In-place join of
<R − a, mj > with <a, mj >.
Case (d): Copy <R − a, mj > to <R − a, mi >. Copy <a, mk >
to <a, mi >. In-place join of <R − a, mi > and <a, mi >
Update C AP<R,mi > , D <R,mi > with D 0 and C AP0 if C OST C ALC(D 0 )
is cheaper.
end for
end for
end for
return C AP<R,mi > , D <R,mi >

previously. If there is no capacity left in m1 or m2 , C OST C ALC
function would cost such a plan at ∞ indicating that the plan is infeasible.

6.2 Plan Generation Algorithm
Finally, we describe an algorithm which takes a sharing Si and
generates two sharing plans depending on the choice of the cost
function, which we had left unspecified earlier. Recall that the DPD
plan has a low dollar cost, whereas the DPT plan has a low critical
time path.
If a DPD plan is needed, C OST C ALC uses C OST function (described in Section 5.2), which computes the cost of a plan in dollars
per second. If a DPT plan is needed, C OST C ALC uses the CP function (described in Section 5.1), which computes the critical time path
of the plan.
The plan generation algorithm computes the cheapest way to build
all join sequences of length 1 < x ≤ |S RC(Si )| in a bottom-up
manner, where S RC(Si ) is set of base relations of Si . The algorithm
obtains the cost to construct longer join sequences of length x, using
the output of prior invocations for join sequences of length x − 1.
The algorithm terminates when it produces the join sequences corresponding to the set of transformations specified in the sharing. The
sharing optimizer generates both DPD and DPT plans. If the DPT
plan is not admissible, then it means that there may not exist a plan
of Si that is admissible and hence, Si is rejected by the provider. If
DPD and DPT are both admissible, the sharing optimizer uses DPD
as it has a lower cost than DPT.

7.

MULTI-SHARING OPTIMIZATION

If two different admitted sharings share similar vertices and edges,
there could be an opportunity to further reduce the cost. The
provider can take advantage of this commonality by amortizing the

693

operating costs across several sharings. The commonality here is
replacing two disjoint sets of vertices and edges belonging to different sharings that perform identical or similar transformation with a
common set for multiple sharings.
Although our idea of merging commonalities in plans is similar as
merging common subexpressions in concurrent running query execution plans [26], there are two main differences. First, our infrastructure contains multiple servers and the cost of moving the data
across the servers has to be considered. Second, as we show below,
unlike [26] we do not restrict to only merging identical subexpressions across plans.
D is a global plan obtained by merging the plan of sharings in
S and then discarding duplicate edges and vertices. Note that D
need not be a single connected component. Each vertex (or edge)
v ∈ D records the identity of all the sharings that it serves, such that
S HR(v) records the sharings to which v belongs. Given a vertex (or
a set of vertices) v, let A NC(v) be the set of vertices and edges that
are ancestors of v in D.
Commonalities in D are reduced by applying a plumbing operator
repeatedly until the D does not perform any redundant work. A
plumbing operator takes two sets of vertices v1 and v2 belonging to
plans as inputs. Then, vertices and edges that supply the vertices in
v2 (or v1 ) are retained but those supplying v1 (or v2 ) are discarded.
We now discuss the mechanics of a plumbing operator as well as an
algorithm to apply them.

7.1

Note that we do not preclude other kinds of plumbing operations
here as long as they do not compromise the correctness of the plan
and result in a cost reduction.
A plumbing operation p is implemented as follows. First, add
the necessary edges and vertices to perform p. For all vertices and
edges v ∈ A NC(D ST(p)), remove S HR(D ST(p)) from the S HR(v).
For all v ∈ A NC(S RC(p)), add S HR(D ST(p)) to S HR(v). Add
S HR(D ST(p)) to S HR(S RC(p)) as well. Remove vertices and edges
v ∈ A NC(D ST(p)) s.t., S HR(v) = ∅.

7.2 Optimization Algorithm
Given the global plan D, we want to identity a set of plumbing operations to perform, that would produce the provably least cost plan
without exceeding the staleness SLA of any of the sharings. The
hardness of this problem comes from the observation that a plumbing can affect the benefit of other plumbings. Figure 4c shows an
example of three plumbings pi , pj and pk that affect one another.
For example, pi cannot be applied if pj has already been applied to
D as applying pj would have removed the vertices in S RC(pi ). For
the same reason pj cannot be applied if pk has already been applied.
Finally, the benefit and the increase to the critical time path due to
pi is affected if pk has already been already applied.
An optimal algorithm that chooses a set of plumbings to perform
resulting in a provably cheapest plan is a hard problem. For our purposes here any plan that is cheaper than D is good enough. Our algorithm is referred to as greedy hill-climbing approach as it applies
one plumbing at a time in a greedy fashion, until no more plumbings can be applied to D. This algorithm has at least the desirable
property that it is fast to execute and intuitive to understand.
Given the global plan D, let P be the set of all possible plumbing
operations that can be performed on D. The greedy hill-climbing
algorithm at each iteration first computes P and then applies the
plumbing operation p ∈ P with the maximum benefit. The set of
plumbings P is obtained using a two step process. First, we examine
all pairs of vertices in D to determine if a copy plumbing can be performed between them. Second, we examine all applicable triples of
vertices to determine if a join plumbing can be performed between
them. The benefit and the critical time path increase due to each
plumbing are computed. If a plumbing has zero or negative benefit,
or causes a sharing to miss its SLA, it is discarded from P . The
algorithm terminates when no more plumbing operation can be applied to D. The resulting global plan forms the input to the sharing
executor. We will show later in Figure 13 that this strategy is quite
effective and results in large savings for the provider.

Plumbing Operations

A plumbing operation p is defined between a set of source vertices S RC(p) and another set of destination vertices D ST(p), such
that after the plumbing operation D ST(p) gets tuples via S RC(p).
Applying p on D results in a potential cost reduction is due to the
removal of vertices from D, although some expenditure in the way
of additional vertices and edges is made to facilitate the plumbing.
The benefit of p is defined as the dollar cost savings due to the removal of vertices and edges in the plan minus the cost of adding the
additional vertices and edges to implement p. Applying p can potentially increase the critical time path of all sharings in S HR(D ST(p)).
This is because they now obtain their tuples via S RC(p), which may
be a longer path in terms of time. We note that p is feasible only if
it has a positive benefit. Moreover, performing p should not cause
the critical time path of any of the sharings in S HR(D ST(p)) to exceed their SLA. We consider the following two kinds of plumbing
operations, which are shown in Figures 4a–b.

COPY
DELTA

ove

DST(pi)

pi

SRC(pi)

(b)

Rem

ove

Rem

(a)

DST(pi)

SRC

pi

(pi )

(c)

pi

8.

pj

SHARING EXECUTOR

The sharing executor takes a global sharing plan D and maintains
the individual sharings at the appropriate level of staleness. In its
very nature, the sharing executor must be robust to any deviations in
the update rate and the behavior of the machines in the infrastructure.
The sharing executor applies its own set of run time optimizations,
some of which are briefly described. We first describe how staleness
is computed and how the push operator reduces the staleness of a
sharing arrangement. Next, we design a model to determine the most
appropriate time to push the sharings.

JOIN

pk

Figure 4: Types of plumbings (a) CopyDelta and (b) Join and (c)
how plumbings can affect one another

8.1 Staleness and Push

1. Copy Plumbing: Takes two delta vertices on different machines and adds a CopyDelta edge between them.

Each machine runs an agent which is responsible for sending periodic timestamps updates to the sharing executor. We refer to these
messages as heartbeats. We implement the following three functions
– TS(.), M IN TS(.), M AX TS(.) – that obtain the current timestamp
of a vertex, and the minimum and maximum timestamps of a set of
vertices in the sharing plan. For instance, M IN TS(S RC(Si )) and

2. Join Plumbing: Takes two vertices – a delta and a relation
– and performs a join to obtain the destination delta vertex.
For example, ∆(A ./ ∆B) is plumbed using A and ∆B by a
Join edge and up to two CopyDelta edges.

694

M AX TS(S RC(Si )) are the minimum and maximum timestamps of
the sources of Si .
The current staleness of a sharing is defined as the difference between the maximum of the timestamps of the base relations of Si to
that of the MV of Si . Note that the staleness should always be maintained to be less than SLA(Si ). This is captured by the following
inequality.

to take into account recent system fluctuations. We record the actual
time to perform each of the operators, compare it against estimated
time and periodically recompute the time model. This feedback loop
allows our system to be reasonably robust to data and machine fluctuations.
An appropriate timestamp t to advance the MV of Si should be
greater than the current timestamp of MV but should be less than
or equal to the minimum of the timestamp of the sources of Si . In
particular,

M AX TS(S RC(Si )) − TS(MV(Si )) ≤ SLA(Si ).
To reduce the staleness of a sharing Si , the executor schedules a
sequence of P USH commands in a topological order starting with
S RC(Si ), until the timestamp of the MV has a more up-to-date
timestamp. A P USH command instructs the agent to advance the
timestamp of a vertex in the sharing plan by applying an operator
denoted by its incoming edge. The incoming edge belongs to one of
the four edge types we described in Section 5.
Suppose an agent receives a P USH command to advance a vertex
v to timestamp t. Suppose that e is an incoming edge of v. The
agent first obtains a write lock on v. It then compares the current
timestamp t0 of v with that of t. If t0 ≥ t then there is no need to
perform any work. If t0 < t, then the agent performs the operation
corresponding to e’s type so that the timestamp of v can be advanced
to t. Once the operation has been performed the agent responds
with a P USH D ONE command, and piggybacks useful statistics such
as time taken to perform the operation and current timestamps. The
executor up on receiving the P USH D ONE proceeds with the outgoing
edges of v.
When it comes to maintaining multiple sharings, the design of a
sharing executor is simplified due to the observation that any sharing in S can be pushed independently of the others in S even though
they may have common edges and vertices. Suppose a vertex is at
a timestamp t and there are two concurrent P USH commands to advance it to t1 and t2 , t ≤ t1 ≤ t2 , respectively. Regardless of the
order in which the two commands are executed, the amount of work
done by e is equal to the work done to advance the timestamp to
t2 . This is why the sharing executor does not have to coordinate
P USH commands between the various sharings that it maintains.
This makes for a simple design of the sharing executor, especially
since the sharings may have different staleness SLAs and may have
to be pushed at different rates.

8.2

TS(MV(Si )) < t ≤ M IN TS(S RC(Si )).
When the push finishes, the MV would be at the timestamp t, while
the timestamp of the sources may all be advanced by CP(Di , t −
TS(MV(Si ))). So, the staleness of the sharing at the time the push
finishes would be: M AX TS(S RC(Si )) + CP(Di , t−TS(MV(Si ))).
We stipulate that the staleness when the push finishes should be less
than the staleness SLA using the following inequality:
M AX TS(S RC(Si )) + CP(Di , t − TS(MV(Si ))) − t ≤ SLA(Si ).
On the other hand, the sharing executor does not want to push too
early as well. In other words, the sharing executor is early if the
push command could have waited a bit longer and still could have
completed before Si became stale. This can be stipulated by adding
the additional constraint that:
l ∗ SLA(Si ) ≤

M AX TS(S RC(Si ))+
CP(Di , t − TS(MV(Si ))) − t

≤ SLA(Si ),

where l > 0 (0.8 in our case) is chosen to account for run-time
deviations, such as a queuing delay if the P USH waits for the capacity on a machine to be available. An appropriate value of t is
obtained by performing a binary search between TS(MV(Si )) and
M IN TS(S RC(Si )) that satisfies the above constraint.

9.

EXPERIMENTS

In this section, we present an experimental study to validate the
SMILE sharing platform. We first describe the experimental setup
in Section 9.1. We then evaluate the performance of our system for
varying rate of updates on the base relation in Section 9.2 and varying SLA in Section 9.3. We examine the effect of varying the number of machines and the sharings in the infrastructure in Section 9.4.
Next, the efficacy of the hill-climbing algorithm applied to DPT and
DPD is shown in Section 9.5. Finally, we highlight the robustness
of the sharing executor in Section 9.6 by varying the update rates on
the base relations and varying read workload on the MV.

Design

Our sharing executor uses a simple model to determine two key
questions: a) Is it time to push Si ? b) By how much to advance
the timestamp of MV of Si ? To determine these two questions, we
develop a model to determine the most appropriate time to schedule
the push and the timestamp to push the MV to such that the sharing
will not miss its SLA.
A simpler design of a sharing executor pushes all the sharings
in S every second so that they do not violate their SLA. Given the
property that the critical time path of an admissible sharing is less
than its staleness SLA, the push will finish before the SLA is violated. However, our sharing executor does not push every second
but rather bunches up sufficient work so that the push is issued as
late as possible. Yet, it is scheduled such that the push would be
completed before Si becomes stale.
To develop the model, we modify the critical time path function
CP(Di , x) to take an additional parameter x, which corresponds to
the amount of timestamp to advance the MV of Si . In Section 5.1
when we described how we compute the critical time path of a sharing plan, x was defaulted to be one but now can take up any arbitrary
value greater than or equal to one. We also added a feedback loop
to the CP function so that it constantly recomputes the time model

9.1 Setup
Our experimental setup creates a mobile cloud ecosystem containing 25 apps where sharings are specified using user, social network, location, checkin, and user-content datasets; the datasets and
the sharings are representative of those one may find in a mobile
cloud. We collected Twitter tweets from a gardenhose stream, which
is a 10% sampling of all the tweets in Twitter, for a six month
period starting from September 2010. The tweets were unpacked
into nine base relations corresponding to the information about the
user (i.e., users relation), tweets (i.e., tweets relation), social
network (socnet relation), checkins (foursq), and user-content
(i.e., urls, hashtags, curloc, photos relations) associated
with the tweets and the location of the user (i.e., loc relation).
This creates our realistic datasets that capture rich information about
users, locations, social contacts, checkins and the various contents
the users are interest in.

695

Table 1: Twitter base relations (left) and the twenty-five sharings (right) used in the evaluations
Base Relations:

Sharings (Apps):
S1
S2
S3
S4
S5
S6
S7

users ./ socnet (twitaholic)
users ./ tweets ./ curloc (twellow)
users ./ tweets ./ urls (tweetmeme)
users ./ tweets ./ urls ./ curloc (twitdom)
users ./ tweets (tweetstats)
tweets ./ curloc (nearbytweets)
urls ./ curloc (nearbyurls)

S13
S14
S15
S16
S17
S18
S19

S8

tweets ./ photos (twitpic)

S20

S9

foursq ./ tweets (checkoutcheckins)

S21

S10
S11

hashtags ./ tweets (monitter)
foursq ./ users ./ tweets
./ curloc (arrivaltracker)
foursq ./ users ./ tweets (route)

S22
S23
S24
S25

S12

9.1.1

Total Push Time (seconds)

We specify 25 sharings by combining these 9 base relations in
different ways. A description of the base relations and the 25 sharings used in the evaluation are shown in Table 1. For each of the
25 sharings, we also mention an existing real app that may benefit from such a sharing. For instance, the application twitter-360,
which displays nearby photos may be interested in S18 which corresponds to users ./ tweets ./ photos ./ curloc. By building
an ecosystem around twitter data and choosing sharing that match
the functionalities of existing apps, we are ensuring that our evaluation is as realistic as possible. Our setup consisted of 6 machines,
such that the 25 sharings were arbitrarily assigned to the available
machines. All the machines in our setup ran identical versions of
Postgresql 9.1 database. Our system starts with 7 million tweets
prepopulated into our databases.
As we vary the rate of arrival of tweets into our system, the rate
of update on the base relations (other than tweets) depends on the
number of tweets seen so far by the system. For instance, at the
beginning any incoming tweet most likely will contain the identity
of a user not previously seen by the system, which would result in the
insertion of a tuple to the users relation. However, after the system
has ingested a sufficient number of tweets, the update rate on the
users relation will decrease as some of the users already would be
present in the users relation. The dependence between the number
of tweets ingested by the system and the chance that an incoming
tweet will result in an insertion to a base relation is expressed in
terms of an update ratio. After 7 million tweets have been ingested
by our system, the update ratio of encountering a previously unseen
user in the next tweet is around 0.3. The update ratio values for some
of the remaining relations were 0.25, 0.02, 0.1, 0.2, for socnet,
loc, curloc and urls, respectively. Using the update ratios, we
can estimate the rate of updates on all the base relations by varying
the rate of incoming tweets in the system.

foursq ./ users ./ tweets ./loc (locc.us)
tweets ./ loc (locafollow)
users ./ loc ./ tweets ./ curloc (twittervision)
foursq ./ users ./ tweets ./ socnet (yelp)
users ./ loc (twittermap)
users ./ tweets ./ photos ./ curloc (twitter-360)
users ./ tweets
./ hashtags ./ curloc (hashtags.org)
users ./ tweets ./ hashtags
./ photos ./ curloc (nearbytweets)
users ./ tweets ./ foursq
./ photos ./ curloc (nearbytweets)
foursq ./ curloc (nearbytweets)
photos ./ curloc (twitxr)
hashtags ./ curloc (nearbytweets)
hashtags ./ users ./ tweets (twistroi)

5

Total Push Time (seconds)

User info
Tweets info
Social network
User address
User current loc
Tweet links
Tweet entities
Photo links
Rest. checkins

4
3
2
1
0
0

3000

6000

0.25
0.2
0.15
0.1
0.05
0

9000

0

No. of Delta Tuples

4000

Total Push Time (seconds)

(b)

5
4
3
2
1
0
0

3000

8000

No. of Delta Tuples

(a)

Total Push Time (seconds)

users(uid, ...)
tweets(uid, uid, ...)
socnet(uid, uid2, ...)
loc(uid, place, ...)
curloc(tid, lat, lng, ...)
urls(tid, url, ...)
hashtags(tid, tags, ...)
photos(tid, urls, ...)
foursq(tid, rid, ...)

6000

9000

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

No. of Output Tuples
(c)

3000

6000

9000

No. of Output Tuples
(d)

Figure 5: Time cost model of the four edge types, namely a)
DeltaToRel, b) CopyDelta, c) Join, and d) Union

9.1.2

Dollar and Time Cost Models

The infrastructure costs for our cost model was obtained from
Amazon EC2 pricing available on the web1 . Our machines are
equivalent to large Linux instances, which cost $0.34/hour. For the
network cost, we assumed that the instances were in different availability zone but in the same region, which had a transfer cost of $0.01
per GB. For storage, we used EBS storage at $0.11 GB/month.
We developed a time model for each edge type to estimate the
time taken to process tuples, as function of the number of input tuples. Our setup to compute a time model consisted of two machines
with 15 base relations of varying sizes between 200k and 50 million tuples, number of attributes from 1 to 7 as well as different
attribute types forming tens of sharings between the base relations.
We pushed a varying number of tuples between 1 and 10k through
each edge in the setup and then measured the time taken to perform
each P USH operation, which is recorded in Figure 5. It can be seen
that the time taken to push tuples through different edge types is
linear in the number of tuples for all the edge types, although with
different slopes. These plots form the basis of our time cost model.

Snapshot Module

To determine the efficacy of our system, an independent auditing
module in our sharing executor, called snapshot, records the staleness of all the sharings once every 5 seconds. Suppose that a sharing
Sj was found to have a staleness less than the SLA staleness at snapshot i. If Sj satisfies the SLA staleness in snapshot i + 1, then the
system is assumed to have maintained Sj at the appropriate level of
staleness for all intermediate time periods between i and i + 1. The
converse is true if Sj is found to have violated the SLA at snapshot
i+1. The snapshot module also keeps track of the cost, and the number of tuples moved between snapshots. Additionally, we record the
staleness of all sharings before and after a P USH operation as well
as the cost incurred and time taken for each P USH operation.

9.2 Varying Rate
We used 6 machines and 25 sharings as shown in Table 1 with a
1

696

http://aws.amazon.com/ec2/pricing/

0

100

200

300

0

Snapshot

100 200 300
Snapshot

(S3)

0

100 200 300
Snapshot

0

100 200 300
Snapshot

(S4)

0

100 200 300
Snapshot

(S10)

0

100 200 300
Snapshot

(S5)

0

100 200 300
Snapshot

60
50
40
30
20
10
0

60
50
40
30
20
10
0

No. Tuples Moved

100000
0
0

400

300000

S16

100000
0
0

100 200 300
Snapshot

S1

200000
100000
0
0

200

400

Snapshot

300000

200
400
Snapshot

S3

200000
100000
0
0

200
400
Snapshot

1.2e+06

(S11)

0

200

200000

Snapshot

(S18)

0

S11

200000

No. Tuples Moved

100 200 300
Snapshot

60
50
40
30
20
10
0

100 200 300
Snapshot

300000

No. Tuples Moved

60
50
40
30
20
10
0

0

0

No. Tuples Moved

60
50
40
30
20
10
0

Staleness (seconds)

Staleness (seconds)

Staleness (seconds)

60
50
40
30
20
10
0

(S9)

(S17)

(S25)

ALL, SLA=45s

1.1e+06

No. Tuples Moved

100 200 300
Snapshot

100 200 300
Snapshot

60
50
40
30
20
10
0

100 200 300
Snapshot

Staleness (seconds)

0

60
50
40
30
20
10
0

0

60
50
40
30
20
10
0

0

0

60
50
40
30
20
10
0

Staleness (seconds)

10

(S2)

(S8)

(S16)

(S24)

Staleness (seconds)

20

100 200 300
Snapshot

100 200 300
Snapshot

60
50
40
30
20
10
0

100 200 300
Snapshot

Staleness (seconds)

60
50
40
30
20
10
0

0

30

60
50
40
30
20
10
0

0

0

60
50
40
30
20
10
0

Staleness (seconds)

40

(S7)

(S15)

(S23)

Staleness (seconds)

(S1), SLA=45s

50

60
50
40
30
20
10
0

100 200 300
Snapshot

60
50
40
30
20
10
0

100 200 300
Snapshot

Staleness (seconds)

Staleness (seconds)

60

0

Staleness (seconds)

Staleness (seconds)

100 200 300
Snapshot

(S14)

0

60
50
40
30
20
10
0

Staleness (seconds)

0

60
50
40
30
20
10
0

(S22)

Staleness (seconds)

100 200 300
Snapshot

(S13)

100 200 300
Snapshot

Staleness (seconds)

0

60
50
40
30
20
10
0

0

60
50
40
30
20
10
0

Staleness (seconds)

(S12)

100 200 300
Snapshot

(S21)

Staleness (seconds)

60
50
40
30
20
10
0

0

60
50
40
30
20
10
0

Staleness (seconds)

100 200 300
Snapshot

(S20)

Staleness (seconds)

0

60
50
40
30
20
10
0

Staleness (seconds)

Staleness (seconds)

(S19)

Staleness (seconds)

Staleness (seconds)
Staleness (seconds)

60
50
40
30
20
10
0

100 200 300
Snapshot

(S6)

1e+06
900000
800000
700000
600000
500000
400000
300000
200000
100000
0

0

100 200 300
Snapshot

0

100

200

300

400

Snapshot

Figure 6: Staleness of twenty-five sharings across snapshots for a rate of 6k tweets/second (left) and the number of tuples moved in
each snapshot (right). Due to space constraints, only two zoomed-in graphs are provided for readability, while the remaining figures
are rendered small to show trends.
Cost vs. Data Rate

15
10

30
20

0

S1

100

200
300
Push Iteration
(a)

400

1000

6
4
2

500
200
100

0

50 G.1k.5k1k F 2k3k5k6k

50 G.1k.5k1k F 2k3k5k6k

Tweets / second

Tweets / second

0

2000

4000

6000

Time Elapsed (seconds)

(b)

(c)

40
35

streams, and about 3 violations per sharing-hour (i.e., per hour per
sharings) for 6k tweets per second. Note that the zero violations for
both the gardenhose and firehose streams were in spite of their unpredictable arrival rate, which is shown in Figure 8c. At 6k tweets
per second, the cost was about $25 per sharing-hour, although note
that some of the sharings were much more expensive than the others.
In contrast, the average cost for the firehose (F) stream was about $6
per sharing-hour. Secondly, the number of tuples moved per snapshot (i.e., 5 seconds) across all the sharings was between 600k and
1.1 million tuples as shown in Figure 6 (right).

30

15
0

8

(a)

20

0

2000

Figure 8: (a) Cost and (b) violations per sharing-hour for gardenhose (G), and firehose (F) streams and rates from 50 to 6k.
Variations in Gardenhose (G) rate shown in (c)

25
10

5

Gardenhose Rate

10

Tweets / second

20

45

40

No.of violations per hour

25

50
Time Pushed

100

200

300

400

Push Iteration
(b)

Figure 7: a) Staleness before and after a P USH on S1 , while b)
shows how much is pushed
Figure 6 (left) shows the staleness of a sharing S1 across different
snapshots. It can be seen that the staleness of each of the sharings increases until it comes close to the SLA (i.e., 45 seconds), after which
the staleness sharply reduces to a low value due to a P USH from the
sharing executor. The staleness before and after a push operation are
shown in Figure 7a, where it can be seen that the P USH operation
reduces the staleness of S1 to less than 10 seconds, just before the
staleness of S1 was about to exceed the SLA. Figure 7b shows that
every push operation advanced the timestamp of S1 by 25 to 40 seconds, which shows the lazy behavior of the sharing executor. One
thing to note here is that there were only 31 violations for all the 25
sharings for the entire duration of the experiment lasting about 40
minutes. We summarize some of our observations below.
The number of violations with varying incoming rate of tweets as
well as the cost to maintain the sharings in sharing-hour are shown in
Figures 8a–b. The results did not show a well defined trend between
the number of violations and the rate of incoming tweets, except that
the violations were very low, even for 6k tweets/second. First of all,
there were zero violations for the firehose (F) and gardenhose (G)

% change in tuples moved

Stalenes (seconds)

55

BEFORE (S1)
AFTER (S1)

50

Violations vs. Data Rate

30

Dollars per hour

SLA of 45 seconds, while varying the rate of tweets from 50 to 6k
tweets/second. At 6k tweets/second (i.e., 3.6 billion tweets/week),
the update rate matches the current known volume of tweets in Twitter [12]. We also replayed gardenhose stream, which roughly corresponds to an average of 100 tweets/second. The rate of arrival for
tweets for a two hour window is shown in Figure 8c. As the gardenhose is a 1 out of 10 sampling of tweets from the firehose, which is
a stream containing all the tweets in Twitter, we recreated a stream
similar (although by no means equivalent) to firehose by replaying
gardenhose at 10X speed.

Tuples moved with/without sharing commonality
0
-500
-1000
-1500
-2000

Small Gap

Large Gap

-2500
-3000
S1

S3

S4 S20

S7

S8

S9 S10 S23

Sharing Arrangements

Figure 9: Number of tuples moved in the setup in Figure 6 expressed as percent reduction from the case when individual sharings are run in isolation
Next, notice in Figure 6 (left) that some sharings, such as S7, S8,
S9, S10, and S23 have a larger gap between the peak staleness value

697

9.3

7000
6000
5000
4000
3000
2000

35

8000

30

7000
Tweets/second

8000
Tweets/second

Rate vs. Sharings

Tuples/machine vs. Machines

Thousands of Tuples/second

Rate vs. Machines

and the SLA, whereas others such as S1, S3, S4, and S20 have relatively smaller gaps. The reason for this is that those sharings with
larger gaps benefit from the commonality with other sharings but not
so for those with smaller gaps. To test this hypothesis, we compared
the number of tuples moved for each sharing in the above experimental setup with the number of tuples moved when the sharings
are run in isolation. The number of tuples moved in the former case
is shown as a percentage reduction from the latter case in Figure 9.
It can be seen that sharings with small gaps only benefit modestly
from the presence of other sharings, whereas those with larger gaps
benefit immensely from the presence of other sharings.

25
20
15
10

4000

2000

0
2

No. of Machines

5000

3000

5

2 3 4 5

6000

3

4

5

20 25 30 40 50

No. of Sharings

No. of Machines

(a)

(b)

(c)

Figure 11: Maximum tweet rate for varying (a) machines, (b)
sharings on a setup with SLA = 45 seconds

Varying SLA

Table 2: Number of violations per sharing-hour (rounded-up)
for varying SLA between 10 and 60 secs
Staleness SLA
10 20
30
40
50
60
Mix
Violations
4
1
2
1
0
0
0

support without losing the stability of the system. We have built an
appropriate mechanism to monitor the stability of our system, which
is not discussed here due to lack of space. It can be seen from Figure 11a that increasing the number of machines increases the maximum rate that can be handled by our system. Moreover, adding an
extra machine increases the processing capacity of our system by at
least 25–30k tuples/sec as can be seen from Figure 11b. Next, we
varied the number of sharings from 20 to 50 keeping the number
of machines fixed at 6 and SLA of 45 seconds, as shown in Figure 11c. We increased the number of sharings beyond 25 by placing
the same sharing on more than one machine. With increasing number of sharings, the maximum rate decreases as database and other
system bottlenecks start manifesting due to the increased number of
vertices and edges that the system has to manage.

Our setup consisted of 6 machines, 25 sharings and an incoming
rate of 1000 tweets/second. We varied the SLA between 10 and 60
seconds. Table 2 shows the effect of varying the SLA in terms of
the number of violations. The number of violations is maximum
for SLA = 10 seconds at 4 violations per sharing-hour. In general,
the number of violations for staleness SLA values greater than 10
seconds is extremely low at either 1 or 2. The higher number of
violations for 30 seconds (at 2 per sharing-hour) compared to those
for 20 and 40 seconds (at 1 per sharing-hour) was due to temporary
fluctuations in system resources.

9.5 Algorithm Comparisons

Cost Change of Mix vs. Uniform SLA

-100
-200
-300
-400

SLA=40
S9 S11 S13 S15

100

S17 S19 S21 S23 S25

Sharing Arrangements

Figure 10: Cost change for mix SLA compared to uniform SLA

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

200

50

100
Snapshots

100

150

200

Snapshots

0.0025

DPT+HC

50

In the above experimental setup, we also examined a case where
we assigned non-uniform SLA (see mix in Table 2) to the 25 sharings. In particular, S1–S7 were assigned a SLA of 10 seconds, S8–
S15 a SLA of 40 seconds, and S16–S25 a SLA of 60 seconds. From
Table 2, we can see that the mix case resulted in zero violations, although having comparable dollar costs to the uniform SLA cases.
Then in Figure 10 we expressed the cost of an individual sharing in
the mix case as a percentage change to the corresponding cost from
the uniform case (i.e., compare costs of S1 from mix with uniform
when SLA was 10 seconds). It is interesting to note that although
the costs of S1–S7 have become marginally more expensive, the cost
of the other sharings (i.e., S8–S15, S16–S25) is now significantly
cheaper. Hence, we can conclude that few sharings with small SLAs
subsidize the operating cost of other (related) sharings.

9.4

150

0.0033

DPD

Snapshots

SLA=60

-500
S1 S3 S5 S7

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

0.0042

DPT

50

SLA=10

Cost (Dollars)

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

0

Cost (Dollars)

Cost (Dollars)

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

Cost (Dollars)

% change in cost

100

150

200

0.0023

DPD+HC

50

100

150

200

Snapshots

Figure 12: Cost of DPT and DPD reduced by applying hillclimbing algorithm to produce DPT+HC and DPD+HC
We examined the efficacy of the hill-climbing algorithm that we
apply to the DPD and DPT algorithms to reduce the cost for the
provider. For this experimental setup, we used 6 machines, 25 sharings and a rate of 1000 tweets/second. The cost model we considered
was same as before, except that we changed the networking pricing to be within the same availability region in EC2 (i.e., no cost).
We generated DPD and DPT sharing plans for this setup, and then
applied the hill-climbing algorithm to both these sharing plans to
produce DPD+HC, and DPT+HC, respectively. The average cost in
dollars per sharing-second for the four sharing plans in sharing-hour
were as follows — DPT 0.0042, DPD 0.0033, DPT+HC 0.0025,
and DPD+HC 0.0023 as shown in Figure 12. It can be noticed
that DPD+HC has the cheapest cost but is comparable to DPT+HC.
When we compared DPD with DPD+HC, and DPT with DPT+HC,
the difference is quite significant representing a 35% reduction in
cost, thus making a case for our hill-climbing approach.

Varying Machines and Sharings

In this experimental setup, we varied the machines from 2 to 5,
while keeping the number of sharings fixed at 25 and a SLA of 45
seconds. For every setup, the capacity of the machine was determined to be the highest rate of tweets that the set of machines can

698

Edge
Vertex

DPD

10.

DPT

240
220
200
180
160
0

2

4

6

8

10

12

14

0

2

4

6

8

10

12

14

16

Iterations

Figure 13: Reduction in vertices and edges as plumbing operations are sequentially applied to DPD and DPT

11.

Figure 13 shows the number of vertices and edges as the hillclimbing algorithm takes DPD or DPT sharing plan as input and
performs plumbing operations in a sequential fashion. As can be
seen from the figure, the sharing plan is reduced by more than 80
vertices and edges for both DPD and DPT, which represents significant savings in terms of cost.

9.6

Staleness (seconds)

8Users,
75 tweets/s

60

16Users,
75 tweets/s

32Users,
100 tweets/s

50Users,
150 tweets/s

50
40
30
20
10
0
0

50

100

150
Snapshot

200

250

REFERENCES

[1] Infochimps. http://www.infochimps.com/, 2012.
[2] Microsoft azure market. http://datamarket.azure.com/, 2012.
[3] Postgres streaming replication.
http://wiki.postgresql.org/wiki/Streaming_Replication,
2012.
[4] Xignite. http://www.xignite.com/, 2012.
[5] D. Agrawal, A. E. Abbadi, A. Singh, and T. Yurek. Efficient view maintenance at
data warehouses. In SIGMOD, 1997.
[6] S. Agrawal, S. Chaudhuri, and V. R. Narasayya. Automated selection of
materialized views and indexes in SQL databases. In VLDB, 2000.
[7] S. Al-Kiswany, H. Hacigümüs, Z. Liu, and J. Sankaranarayanan. Cost
exploration of data sharings in the cloud. In EDBT, 2013.
[8] S. Aulbach, M. Seibold, D. Jacobs, and A. Kemper. Extensibility and data
sharing in evolving multi-tenant databases. In ICDE, 2011.
[9] M. Balazinska, B. Howe, and D. Suciu. Data markets in the cloud: An
opportunity for the database community. PVLDB, 4(12):1482–1485, 2011.
[10] J. A. Blakeley, P.-A. Larson, and F. W. Tompa. Efficiently updating materialized
views. In SIGMOD, 1986.
[11] S. Chaudhuri. An overview of query optimization in relational systems. In
PODS, 1998.
[12] D. Costolo. The power of Twitter as a communication tool. http://www.
fordschool.umich.edu/video/newest/1975704207001/, 2012.
[13] V. Kantere, D. Dash, G. Gratsias, and A. Ailamaki. Predicting cost amortization
for query services. In SIGMOD, 2011.
[14] D. Kossmann, M. J. Franklin, and G. Drasch. Cache investment: integrating
query optimization and distributed data placement. TODS, 25:517–558, 2000.
[15] S. Krishnamurthy, M. J. Franklin, J. M. Hellerstein, and G. Jacobson. The case
for precision sharing. In VLDB, 2004.
[16] A. Labrinidis and N. Roussopoulos. Reduction of materialized view staleness
using online updates. CS-TR-3878, UMD CS, 1998.
[17] L. Lamport. Time, clocks, and the ordering of events in a distributed system.
CACM, 21(7):558–565, 1978.
[18] G. M. Lohman, C. Mohan, L. M. Haas, D. Daniels, B. G. Lindsay, P. G. Selinger,
and P. F. Wilms. Query processing in R∗ . In Query Processing in Database
Systems, pages 31–47. Springer, 1985.
[19] C. Mascolo, L. Capra, S. Zachariadis, and W. Emmerich. XMIDDLE: A
data-sharing middleware for mobile computing. Wireless Personal
Communications, 21(1):77–103, 2002.
[20] H. Mistry, P. Roy, S. Sudarshan, and K. Ramamritham. Materialized view
selection and maintenance using multi-query optimization. In SIGMOD, 2001.
[21] T.-V.-A. Nguyen, S. Bimonte, L. d’Orazio, and J. Darmont. Cost models for view
materialization in the cloud. In EDBT, 2012.
[22] T. Plagemann, J. Andersson, O. Drugan, V. Goebel, C. Griwodz, P. Halvorsen,
E. Munthe-Kaas, M. Puzar, N. Sanderson, and K. Skjelsvik. Middleware services
for information sharing in mobile ad-hoc networks. Broadband Sat. Comm. Sys.
and Challenges of Mob., 169:225–236, 2005.
[23] K. A. Ross, D. Srivastava, and S. Sudarshan. Materialized view maintenance and
integrity constraint checking: Trading space for time. In SIGMOD, 1996.
[24] K. Salem, K. Beyer, B. Lindsay, and R. Cochrane. How to roll a join:
Asynchronous incremental view maintenance. In SIGMOD, 2000.
[25] J. Sankaranarayanan, H. Hacigumus, and J. Tatemura. COSMOS: A platform for
seamless mobile services in the cloud. In MDM, 2011.
[26] T. Sellis. Multiple-query optimization. TODS, 13(1):23–52, 1988.
[27] M. M. Wang, J. N. Cao, J. Li, and S. K. Dasi. Middleware for wireless sensor
networks: A survey. JCST, 23(3):305–326, 2008.
[28] Y. Zhuge, H. García-Molina, J. Hammer, and J. Widom. View maintenance in a
warehousing environment. In SIGMOD, 1995.
[29] Y. Zhuge, H. García-Molina, and J. Wiener. The strobe algorithms for
multi-source warehouse consistency. In PDIS, 1997.

Robustness of Sharing Executor
70

CONCLUDING REMARKS

In this paper we presented a platform that can maintain sharings at
the appropriate level of staleness. Experimental results showed the
effectiveness of our platform in maintaining several sharings with
low violations even under a high update rate. We will examine the
following possible extensions in a future work. The platform can
be extended to support aggregate operators by developing additional
operators. Next, easy addition or removal of sharings on the fly as
the system is running can be provided. Finally, before markets for
sharing data be envisioned, issues related to the pricing of data [9]
have to be addressed.

DPT+HC

260

DPD+HC

Number of Vertices/Edges

280

300

Figure 14: Staleness of S4 across snapshots as the rate and the
workload on the MV abruptly change
This experiment shows the robustness of the sharing executor as
the machine capacities change during run-time. The setup consists
of 4 machines hosting the sharings S1 ...S4 . Figure 14 shows the
staleness of S4 with an SLA of 50 seconds (marked) as recorded
by the snapshot module. The SLA of the remaining 3 sharings was
between 20 and 70 seconds. We applied a read workload on each
of the four MVs corresponding to the four sharings by the way of a
simulated user generating a single query template in a closed loop.
Initially, the incoming rate is set at 50 tweets/second and each of the
MVs is subjected to two users.
As the system is running we abruptly vary the number of users on
each MV as well as the rate of incoming tweets. The number of users
per MV is varied in four phases from 8 to 50, while simultaneously
changing the rate of incoming tweets from 50 to 150 tweets/second.
After the first phase when the number of users was increased from 8
to 16 users, all the machines are heavily loaded. The average staleness value for each phase is also marked in Figure 14.
As the number of users and the rate of incoming tweets increase,
the machines get progressively more loaded. However, it can be seen
from the boundaries of the phase changes in the figure, the sharing
executor quickly adapts to the changing data rate and the increased
workload on the databases. The model in spite of the infrastructure
being loaded never allows the staleness of the sharing to exceed beyond 40 seconds. The sharing executor is able to do this by taking
advantage of the slack between the critical time path of the sharing
plan, which is a few seconds and that of the staleness SLA, which
is 50 seconds. Even if the time taken to push the sharing becomes
progressively slower due to the system load, the executor is able to
schedule the updates in a way that the SLAs are not violated.

699

RecDB: Towards DBMS Support for Online Recommender
Systems
Mohamed Sarwat
Supervised by: Mohamed F. Mokbel
Dept. of Computer Science and Engineering, University of Minnesota, Minneapolis, MN 55455

{sarwat,mokbel}@cs.umn.edu
ABSTRACT

Movielens [21]). The technique used by many of these systems is
collaborative filtering (CF) [24], which analyzes past community
opinions to find correlations of similar users and items to suggest k
personalized items (e.g., movies) to a querying user u. Community
opinions are expressed through explicit ratings represented by the
triple (user, rating, item) that represents a user providing a numeric
rating for an item.
Traditional recommender systems do not take into account system issues (i.e., scalability and query efficiency) for two main reasons: (1) Due to the nature of the recommended items (e.g., books,
movies, cloths), traditional systems were built with the implicit assumption that the recommendation model changes slowly, which
tolerates using an offline process that builds a fresh model daily or
weekly in order to adapt to changes in the underlying content [14,
22, 26]. (2) The recommender system community put more focus
on recommendation result quality in order to increase user satisfaction.
Such traditional practices are no longer valid in an increasingly
dynamic online world. In an age of staggering web use growth and
ever-popular social media applications (e.g., Facebook [9], Google
Reader [13]), users are expressing their opinions over a diverse set
of data (e.g., news stories, Facebook posts, retail purchases) faster
than ever. In such an environment, the system must adapt quickly
to its diverse and ever-changing content. Recommender systems
cannot wait weeks, days, or even hours to rebuild their models [6].
The rate that new items or users enter the system (e.g., Facebook
updates, news posts), and the rate at which users express opinions
over items (e.g., Diggs [7], Facebook “likes" [10]), requires recommender models to change in minutes or seconds, implying models
be updated online (i.e., in real time).
In this paper, we propose RecDB; an efficient and scalable system that provides online recommendation to users. Unlike existing
implementation approaches, RecDB pushes the recommender system functionality inside the database engine in order to provide online recommendations for users. In the rest of the paper, we show
the different components of RecDB and how it modifies different
layers in the database stack.

Recommender systems have become popular in both commercial
and academic settings. The main purpose of recommender systems
is to suggest to users useful and interesting items or content (data)
from a considerably large set of items. Traditional recommender
systems do not take into account system issues (i.e., scalability and
query efficiency). In an age of staggering web use growth and everpopular social media applications (e.g., Facebook, Google Reader),
users are expressing their opinions over a diverse set of data (e.g.,
news stories, Facebook posts, retail purchases) faster than ever. In
this paper, we propose RecDB; a fully fledged database system that
provides online recommendation to users. We implement RecDB
using existing open source database system Apache Derby, and
we use showcase the effectiveness of RecDB by adopting inside
Sindbad; a Location-Based Social Networking system developed
at University of Minnesota.

Categories and Subject Descriptors
H.2.8 [Database Applications]: Spatial databases and GIS; Data
mining; Statistical databases

General Terms
Design, Experimentation, Human Factors, Performance

Keywords
Social Networking, Recommender Systems, Query Processing,
Model Maintenance, Filtered Recommendation

1.

INTRODUCTION

Recommender systems have become popular in both commercial [6, 20] and academic settings [1, 5, 21]. The main purpose
of recommender systems is to suggest to users useful and interesting items or content (data) from a considerably large set of items.
For instance, recommender systems have successfully been leveraged to help users find interesting books and media from a massive inventory base (Amazon [20]), news items from the Internet (Google News [6]), and movies from a large catalog (Netflix,

2. RECOMMENDER SYSTEMS
The basic functionality of a Recommender System is to take a set
of ratings (i.e., triplet user, item, rating) as input, build a recommendation model, and then use the generated model to retrieve a
set of recommended items for each user [15, 25]. Nowadays, the
high rate that new items or users enter the system, and the high
rate that users express opinions over items requires the set of recommended items for each user to change in minutes or even seconds. The process consists of two phases (Figure 1): (1) Model
Building Phase: in which the recommendation model is generated

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
SIGMOD/PODS’12 PhD Symposium, May 20, 2012, Scottsdale, AZ, USA.
Copyright 2012 ACM 978-1-4503-1326-1/12/05...$10.00.

33

sim( i p , i q) = .7
Items i
q
...

ip

Users
...

3

uk

5

...

uj

1

co-rated

co-rated

...

4

Item

4

iq

.7

ir

.6

is

.4

2

ir

ix

.9

iy

.5

iz

.2

...

uk

1

5

...

uj

iq

2

Model

3
sim( i p , i q) = .7

co-rated

4

4

Figure 1: Item-based CF model generation and item/item similarity calculation.
using the user/item ratings data, and (2) Recommendation Generation Phase: in which the recommendation model is used to create
a set of recommended items for each user. Conceptually, ratings
are represented as a matrix with users and items as dimensions, as
depicted in Figure 1. Given a querying user u, CF produces a set of
k recommended items Ir ⊂ I that u is predicted to like the most.
Phase I: Model Building. This phase computes a similarity score
sim(ip ,iq ) for each pair of objects ip and iq that have at least one
common rating by the same user (i.e., co-rated dimensions). Similarity computation is covered below. Using these scores, a model
is built that stores for each item i ∈ I, a list L of similar items
ordered by a similarity score sim(ip ,iq ), as depicted in Figure 1).
2
Building this model is an O( RU ) process, where R and U are the
number of ratings and users, respectively. It is common to truncate
the model by storing, for each list L, only the n most similar items
with the highest similarity scores [26]. The value of n is referred
to as the model size and is usually much less than |I|.
Phase II: Recommendation Generation. Given a querying user
u, recommendations are produced by computing u’s predicted rating P(u,i) for each item i not rated by u [26]:

l∈L sim(i, l) ∗ ru,l
P(u,i) = 
(1)
l∈L |sim(i, l)|

4. RECDB OVERVIEW
We propose RecDB; a system that implements the recommender
system functionality inside the DBMS engine. RecDB is built on
three main system design pillars:

Before this computation, we reduce each similarity list L to contain only items rated by user u. The prediction is the sum of ru,l ,
the user’s rating for a related item l ∈ L weighted by sim(i,l), the
similarity of l to candidate item i, then normalized by the sum of
similarity scores between i and l. The user receives as recommendations the top-k items ranked by P(u,i) .
Computing Similarity. To compute sim(ip , iq ), we represent each
item as a vector in the user-rating space of the rating matrix. For
instance, Figure 1 depicts vectors for items ip and iq from the matrix in Figure 1. Many similarity functions have been proposed
(e.g., Pearson Correlation, Cosine); we use the Cosine similarity in
LARS due to its popularity:
sim(ip , iq ) =

ip · iq

ip iq 

IMPLE-

Recommender system functionality has been always taken care
of in the application layer as depicted in Figure 2. In other words,
the application developer implements all the logic behind the recommendation functionality, and uses the DBMS only for storage;
we call that the traditional approach. The good news about the
traditional approach is that it gives high freedom to the application
developer to write its own recommendation techniques that fits the
end-user of the application Nonetheless, the traditional approach
suffers from the following drawbacks: (1) Implementation Complexity: As the application developer is responsible for the whole
recommender system logic, the application development process
might end up being tedious. (2) Lack of System Expertise: All the
application developer cares about is the application functionality,
hence s/he might not be able to handle the system performance and
scalability issues.
On the other hand, the Built-in approach pushes both steps (i.e.,
model building and recommendation generation) of the recommender system inside the DBMS. Hence, the application developer
just focuses on the application logic and relies on the DBMS to
take care of the system performance and scalability issues. However, the Built-in approach is sort of rigid as it mandates the usage of specific recommendation techniques that are implemented
a-priori inside the DBMS. In case the application developer wants
to employ a different recommendation technique, s/he might either
implement the new recommendation technique inside the DBMS
or alternatively use the traditional approach.
The Extensible approach is similar to the Built-in approach, with
the exception that the DBMS is extensible to new recommendation
techniques, which could be declared by the application developer.
The Extensible approach combines the advantages of both the traditional approach and the Built-in approach in such a way that it
isolates the application developer from the system issues and at
the same time allow her/him to define new recommendation techniques. For the aforementioned reasons, we set the Extensible approach as our system design goal when building RecDB.

Similarity List

ip

Rating data

ip

3. RECOMMENDER SYSTEM
MENTATION APPROACHES

• Low Latency: That is necessary in order to feed recommendations to users in an online/real time manner.
• High Extensibility: That means that the system is extensible
to as many recommendation techniques as possible.
• High Flexibility: That means the system provides flexible
recommendation based-upon application requirements.
To this end, RecDB adopts an extensible approach (see Figure 4),
as described in section 3. RecDB pushes both recommender system
phases inside the core engine of an DBMS To achieve that, RecDB
needs to modify almost all layers of the database stack; ranging
from the query parser to the storage engine. In the rest of the paper,
we will highlight the three main RecDB components, namely: (1)
RecStore: It is a module built inside the database storage engine
that incrementally maintains the recommendation model aiming at
increasing the system efficiency. (2) Rec-tree: it is an efficient tree
structure that provides flexible recommendation based upon the indexed users/items attributes. (3) RecQuery: It is a module built
inside the database query processor, whose role is to minimize the

(2)

This score is calculated using the vectors’ co-rated dimensions,
e.g., the Cosine similarity between ip and iq in Figure 1 is .7 calculated using the circled co-rated dimensions. Cosine distance is
useful for numeric ratings (e.g., on a scale [1,5]). For unary ratings, other similarity functions are used (e.g., absolute sum [4]).

34



 




 


$



	!




#



#


	!



$





#


Figure 3: Built-In Approach

Figure 4: Extensible Approach

CREATE REC-TREE INDEX
ON users_Table (UserAge, UserCity)
USERS FROM users_Table KEY userID -- (userID, name, . . . )
ITEMS FROM items_Table KEY itemID -- (itemID, itemDetails)
RATINGS FROM ratings_Table KEY (userID,itemID) --(userID,itemID,rating)

recommendation query latency, in order to recommend a high number of items to a huge number of system users in an online manner.
In the rest of this section, we will give a little bit of details of the
aforementioned modules.

4.1 RecStore:
nance



 



 





 




$




Figure 2: Traditional Approach

	



	




 




	



	!



Figure 5: SQL Example 1 to Create Rec-tree Index

Incremental Model Mainte-

To be effective, recommender systems must evolve with their
content. For example, new users enter the system changing the collective opinions of items, the system adds new items widening the
recommendation pool, or user tastes change. These actions affect
the recommender model, that in turn affect the system’s recommendation quality. Traditionally, most systems have been able to
tolerate using an offline process that builds a fresh model daily or
weekly in order to adapt to changes in the underlying content [14,
22, 26]. However, the rate that new items or users enter the system
in nowadays application (e.g., Facebook updates, news posts), and
the rate which users express opinions over items (e.g., Diggs [7],
Facebook “likes" [10]), requires recommender models to change in
minutes or seconds, implying models be updated online.
Recent work from the data management community has shown
that many popular recommendation methods (including collaborative filtering) can be expressed with conventional SQL, effectively pushing the core logic of recommender systems within the
DBMS [16]. However, the approach does nothing to address the
pressing problem of online model maintenance, as collaborative
filtering still requires a computationally intense offline model generation phase when implemented with a DBMS.
RecStore [19] is a module built to complement the storage engine
of an DBMS, which enhances the recommendation model generation step by proposing a method that incrementally maintains the
recommendation model when new user/item ratings enter the system. The basic idea behind RecStore is to separate the logical and
internal representations of the recommender model. RecStore receives updates to the user/item ratings data (i.e., the base data for a
collaborative filtering models) and maintains its internal representation based on these updates. As RecStore is built into the DBMS
storage engine, it outputs tuples to the query processor through access methods that transform data from the internal representation

CREATE REC-TREE INDEX
ON items_Table (ItemType)
USERS FROM users_Table KEY userID -- (userID, name, . . . )
ITEMS FROM items_Table KEY itemID -- (itemID, itemDetails)
RATINGS FROM ratings_Table KEY (userID,itemID) --(userID,itemID,rating)

Figure 6: SQL Example 2 to Create Rec-tree Index

into the logical representation expected by the query processor.
RecStore is designed with extensibility in mind. RecStore’s architecture is generic, and thus the logic for a number of different recommendation methods can easily be “plugged into" the RecStore
framework, making it a one-stop solution to support a number of
popular recommender models within the DBMS. RecStore is also
adaptive to system workloads, tunable to realize a trade-off that
makes query processing more efficient at the cost of update overhead, and vice versa.

4.2 Rec-tree: An Efficient Index Structure for
Processing Online Recommender Queries
Recommender systems need to provide flexible recommendation
to the end-user. For instance, Amazon.com has a huge number
of users, items (i.e., products), and user/item ratings. Among all
Amazon.com items, a user might be interested to get recommended
"Books" only. Also, a user might want to get recommended items
that are bought only by users living in Minnesota. Moreover, a user
might need to get recommended only "Books" that were bought
by users who lives in Minnesota and their age is between 21 and
35. The main challenge is that we need to maintain a recommendation model for all attributes ranges defining the index structure.
Notice that existing database index structures (e.g., B + -tree and

35

(a) Storage

Aggregate Maint Time (* 1K sec)

Storage (GB)

8
LARS-M=0
7 LARS-M=1
LARS
6
5
4
3
2
1
0
10
50
100
200
500
Number of Ratings (* 1000)

18
LARS-M=0
16 LARS-M=1
14
LARS
12
10
8
6
4
2
0
10
50

CREATE RECOMMENDATION VIEW
Rec_View KEY (userID,itemID) -- (userID,itemID)
USERS FROM users_Table KEY userID -- (userID, name, . . . )
ITEMS FROM items_Table KEY itemID -- (itemID, itemDetails)
RATINGS FROM ratings_Table KEY (userID,itemID) -- (userID,itemID,rating)
USING ItemBasedModel

Figure 8: RECOMMENDATION VIEW
100

150

200

Ratings Updates So Far (* 1000)

(b) Maintenance
uation, RecDB pushes the second step (i.e., Recommendation Generation) inside the DBMS, by creating a RECOMMENDATION
VIEW as in Figure 8. In Figure 8, the application developer declares a view called Rec_View which has two fields userID and
itemID (representing the recommended items for each user). All
users are defined in users_Table that has userID as a primary key.
All items are stored in items_Table that has itemID as a primary
key. The ratings are represented by ratings_Table which is defined
by the triplet (userID,itemID,rating). The USING keyword determines the recommendation model (e.g., Item-Based Collaborative
filtering) used to generate recommendation for users. Once having
the recommendation view, the application developer can issue regular SQL queries on that view to select appropriate recommended
items for end-users. RecQuery modifies the query processor by
making use of the recommendation view; mentioned before. Recall that the main goal is to provide recommendations for users
in real time and online manner while the recommendation model
is updated frequently. To this end, RecQuery has to answer two
main research questions: (1) What to materialize ? – What items
(of which users) should be stored in the recommendation view and
which not, and (2) How to materialize ? – For materialized items
(users), how to incrementally maintain the view.

Figure 7: Scalability of LARS
R-tree) can be naively employed to solve the problem. However,
that approach suffers from scalability issues as it needs to maintain
a recommendation model for all values indexed by the tree. Moreover, as we maintain a recommendation model for all values in each
level of the tree, the query and update (e.g., insertion and deletion)
performance becomes a severe system bottleneck.
To solve the problem, we propose Rec-tree, a multi-dimensional
tree index structure that is built specifically to index recommendation models and provides flexible and online recommendation to
users. As in traditional database index structures, the user can define which users/items attributes (i.e., dimensions) needed to be indexed by Rec-tree (see Examples in Figures 5 and 6 ). Rec-tree partitions the user/items ratings space based upon the index attribute
and maintain a recommendation model for each partition. Rec-tree
is an adaptive structure that decides whether to merge or split a tree
node by employing a tradeoff between recommendation quality and
system scalability.
As a proof of concept, we implemented LARS [18]; a LocationAware Recommender System that provides recommendation for
users base on their locations as well as the items locations. LARS
employs a partial pyramid structure [3] (equivalent to a partial
quad-tree [11]). In each pyramid cell, we store an item-based collaborative filtering model built using only the spatial ratings with
user locations contained in the cell’s spatial region. rating may
contribute to up to at most a single cell at each pyramid level: starting from the lowest maintained grid cell containing the embedded
user location up to the root level. Levels in the pyramid can be incomplete, as LARS will periodically merge or split cells based on
trade-offs of locality and scalability. When deciding to merge, we
define a system parameter M, a real number in the range [0,1] that
defines a tradeoff between scalability gain and locality loss. LARS
merges (i.e., discards quadrant q) if:
(1 − M) ∗ scalability_gain > M ∗ locality_loss

5. DATA SETS
In order to evaluate RecDB system quality and performance, we
make use three main data sets as follows:
• MovieLens data: The MovieLens data used in is a real movie
rating data taken from the popular MovieLens recommendation system at the University of Minnesota [23]. This data
consists of 10 million ratings for 10,000 movies from 72,000
users. Users’ ratings to items takes values between zero and
five.
• Foursquare data: Foursquare [12] is a mobile location-based
social network application. Users are associated with a home
city, and alert friends when visiting a venue (e.g., restaurant)
by “checking-in" on their mobile phones. During a “checkin", users can also leave “tips", which are free text notes
describing what that they liked about the venue. Any other
user can add the “tip" to her “to-do list" if interested in visiting the venue. Once a user visits a venue in the “to-do list"
, she marks it as “done". Also, users who check into a venue
the most are considered the “mayor" of that venue. We
crawled Foursquare and collected data for 1,010,192 users
and 642,990 venues across the United States. Foursquare
does not publish each “check-in" for a user, however, we
were able to collect the following pieces of data: (1) user tips
for a venue, (2) the venues for which the user is the mayor,
and (3) the completed to-do list items for a user. In addition,
we extracted each user’s friend list. To extract user/items
ratings from foursquare data, we use a numeric rating value
range of [1, 3], translated as follows: (a) 3 represents the user
is the “mayor” of the venue, (b) 2 represents that the user left
a “tip” at the venue, and (c) 1 represents the user visited the

(3)

Initial Experiments. Figure 7 depicts the storage and aggregate maintenance overhead required for an increasing number of
ratings. We again LARS-M=0 and LARS-M=1 to indicate the extreme cases for LARS. Figure 7(a) depicts the impact of increasing the number of ratings from 10K to 500K on storage overhead.
LARS-M=0 requires the lowest amount of storage since it only
maintains a single collaborative filtering model. LARS-M=1 requires the highest amount of storage since it requires storage of a
collaborative filtering model for all cells (in all levels) of a complete pyramid. The storage requirement of LARS is in between the
two extremes since it merges cells to save storage.

4.3 RecQuery: Low-Latency Recommendation Generation
In traditional recommender systems, when a user logs in, a complex SQL query is issued at the application layer, while the user
only cares about the set of recommended items. To remedy this sit-

36

pushes the recommender systems phases inside the database engine. RecDB has three main component: (1) RecStore that efficiently maintains the recommendation model in order to serve online recommendations for end-users, (2) Rec-tree is an index structure that provides flexible recommendation functionality filtered by
user/item attributes, and (3) RecQuery that efficiently maintains a
recommendation view in order to serve low-latency recommendation to end-users. RecDB is implemented as part of existing DBMS
(i.e., Apache Derby) and is demonstrated by Sindbad; a Locationbased Social Networking system.

8. REFERENCES

[1] G. Adomavicius and A. Tuzhilin. Toward the Next Generation of
Recommender Systems: A Survey of the State-of-the-Art and
Possible Extensions. TKDE, 17(6), 2005.
[2] Apache Derby: http://db.apache.org/derby.
[3] W. G. Aref and H. Samet. Efficient Processing of Window Queries in
the Pyramid Data Structure. In PODS, 1990.
[4] J. S. Breese, D. Heckerman, and C. Kadie. Epirical Analysis of
Predictive Algorithms for Collaborative Filtering. In UAI, 1998.
[5] CoFE Recommender System: http://eecs.oregonstate.edu/iis/CoFE.
[6] A. Das et al. Google News Personalization: Scalable Online
Collaborative Filtering. In WWW, 2007.
[7] Digg: http://digg.com.
[8] M. D. Ekstrand, M. Ludwig, J. A. Konstan, and J. T. Riedl.
Rethinking the Recommender Research Ecosystem: Reproducibility,
Openness, and LensKit. In RecSys, 2011.
[9] Facebook: http://www.facebook.com.
[10] Facebook turns on its ’Like’ button:
http://news.cnet.com/8301-1023_3-10160112-93.html.
[11] R. A. Finkel and J. L. Bentley. Quad trees: A data structure for
retrieval on composite keys. Acta Inf., 4:1–9, 1974.
[12] Foursquare: http://foursquare.com.
[13] Google Reader: www.google.com/reader.
[14] G. Karypis. Evaluation of Item-Based Top-N Recommendation
Algorithms. In CIKM, 2001.
[15] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon,
and J. Riedl. GroupLens: Applying Collaborative Filtering to Usenet
News. Commun. ACM, 40(3), 1997.
[16] G. Koutrika, B. Bercovitz, and H. Garcia-Molina. FlexRecs:
Expressing and Combining Flexible Recommendations. In SIGMOD,
2009.
[17] LensKit: http://lenskit.grouplens.org/.
[18] J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel. LARS: A
Location-Aware Social Networking System. In ICDE, 2012.
[19] J. J. Levandoski, M. Sarwat, M. F. Mokbel, and M. D. Ekstrand.
RecStore: An Extensible and Adaptive Framework for Online
Recommender Queries inside the Database Engine. In EDBT, 2012.
[20] G. Linden, B. Smith, and J. York. Amazon.com Recommendations:
Item-to-Item Collaborative Filtering. IEEE Internet Computing, 7(1),
2003.
[21] B. N. Miller, I. Alber, S. K. Lam, J. A. Konstan, and J. Riedl.
MovieLens Unplugged: Experiences with an Occasionally
Connected Recommender System. In IUI, 2002.
[22] B. N. Miller, J. A. Konstan, and J. Riedl. PocketLens: Toward a
Personal Recommender System. TOIS, 22(3), 2004.
[23] MovieLens: http://www.movielens.org/.
[24] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl.
GroupLens: An Open Architecture for Collaborative Filtering of
Netnews. In CSWC, 1994.
[25] P. Resnick and H. R. Varian. Recommender Systems. Commun.
ACM, 40(3), 1997.
[26] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-Based
Collaborative Filtering Recommendation Algorithms. In WWW,
2001.
[27] M. Sarwat, J. Bao, A. Eldawy, J. J. Levandoski, and M. F. Mokbel.
Sindbad: A Location-based Social Networking System. In SIGMOD,
2012.

Figure 9: Sindbad Recommendation Service

venue as a completed “to-do” list item. Using this scheme,
a user may have multiple ratings for a venue, in this case we
use the highest rating value.
• Synthetic data: The synthetic data set is generated using a
synthetic data generator that takes the number of users, number of items, and number of ratings as input and randomly
generates a set of user/item ratings. Users’ ratings to items
are assigned random values between zero and five. Notice
that the synthetic data is only used to test the system performance and is never user to test the recommendation quality.

6.

SYSTEM PROTOTYPE

To implement the recommendation functionality RecDB employs the Lenskit recommender framework built at the University
of Minnesota. LensKit [8, 17] is an open source toolkit for building, researching, and studying recommender systems. To support
RecDB system functionalities, we modify the different layers of
Apache Derby [2], an open source relational database system built
in Java and available under Apache License.
We plan to build a RecDB prototype inside Sindbad [18, 27];
a location-based social networking system built at the University
of Minnesota. Sindbad users can request recommendations of either spatial items (e.g., restaurants, stores) or non-spatial items
(e.g., movies) by explicitly issuing a location-aware recommendation query. The location-aware recommender module (LARS)
suggests a set of items based on: (a) the user location (if available),
(b) the item location (if available), and (c) ratings previously posted
by either the user or the user’s friends. Figure 9 depicts an Android
Phone Application that is built on top of Sindbad. As shown in
the figure, End-users may ask Sindbad for recommendations (e.g.,
Restaurants) by clicking on the recommendation link on the lefthand side of the web interface or by clicking on the location-aware
recommendation button in the mobile app. The user then enters the
type of object he is interested in (e.g., restaurant, theaters, stores). a
spatial range in miles, and also the number of recommended items
to be returned to him and then presses the Recommend button. The
recommended items are then shown on the map. Sindbad is used
to demonstrate the effectiveness of RecDB in a real application setting; which gives high credibility to the system.

7.

CONCLUSION

In this paper, we propose RecDB; a fully fledged database system
that provides recommendation functionality in an efficient and scalable way. To this end, RecDB uses an extensible approach which

37

The Anatomy of Sindbad: A Location-Aware Social
Networking System
Mohamed Sarwat⋆ , Jie Bao⋆ , Ahmed Eldawy⋆ , Justin J. Levandoski† , Amr Magdy⋆ , and
Mohamed F. Mokbel⋆
⋆

⋆

Dept. of Computer Science and Engineering, University of Minnesota, Minneapolis, MN 55455
†
Microsoft Research, Redmond, WA 98052-6399

{sarwat,baojie,eldawy,amr,mokbel}@cs.umn.edu, † justin.levandoski@microsoft.com

ABSTRACT

Places [5], Foursquare [7]). These existing location-based social
networks are strictly built for mobile devices, and only allow users
to receive messages about the whereabouts of their friends (e.g.,
Foursquare “check-ins” that give an alert that “your friend Alice has checked in at restaurant A”). Sindbad, on the other hand,
takes a broader approach that marries functionality of traditional
social-networks with location-based social scenarios (e.g., friend
news posts with spatial extents, location-influenced recommendations) [3]. Thus, Sindbad is appropriate for both traditional social
networking scenarios (e.g., desktop-based applications) as well as
location-based scenarios (e.g., mobile-based applications).
Users of Sindbad can entertain one or more of the following
functionalities: (a) select their friend list as well as getting listed as
friends to other users in a same way like traditional social network
systems, (b) post (spatial) messages and/or rate (spatial) objects
(e.g., restaurant or movies), which will be seen by their friends,
(c) once a user logs on to Sindbad, the user will see an incoming
location-aware news feed from the user friends. The news feed
is selected based on both the user location and the spatial extents
of the posted messages, and (d) get a location-aware recommendation about spatial items, e.g., restaurants, or non-spatial items,
e.g., movies. The recommendation is based on the user location,
the item location, and what are the items that the friends of the
user have liked. In summary, Sindbad distinguishes itself from all
previous systems in one or more of the following aspects:

This paper features Sindbad; a location-based social networking
system. Sindbad supports three new services beyond traditional
social networking services, namely, location-aware news feed,
location-aware recommender, and location-aware ranking. These
new services not only consider social relevance for its users, but
they also consider spatial relevance. Since location-aware social
networking systems have to deal with large number of users, large
number of messages, and user mobility, efficiency and scalability
are important issues. To this end, Sindbad encapsulates its three
main services inside the query processing engine of PostgreSQL.
Usage and internal functionality of Sindbad are implemented with
PostgreSQL and Google Maps API. Both a web and android phone
applications are built on top of Sindbad for better interaction with
the system users.

Categories and Subject Descriptors
H.2.8 [Database Applications]: Spatial databases and GIS

General Terms
Design, Management, Performance, Algorithms

Keywords
Social Networking, Recommender Systems, News Feed, Spatial
Rating, Spatial Message

1.

1. Posted messages in Sindbad have spatial extents in which
they are deemed relevant to only those friends who are located within the message spatial extent.
2. Sindbad allows its users to express their opinions by rating
different items, e.g., restaurants, movies, or stores.
3. Sindbad is equipped with a location-aware news feed module
that, for any user, efficiently retrieves the relevant messages
from her friends based on the user location and the message
spatial extents.
4. Sindbad is equipped with a location-aware recommender
module that, for any user, efficiently suggests (spatial) items
based on users locations, items locations, and previous ratings by user friends.
5. Sindbad is equipped with a location-aware ranking module that efficiently selects the top-k relevant objects produced from either the location-aware news feed or the recommender system modules. The ranking is based on both
the spatial and social relevance.
6. A major part of Sindbad is built inside PostgreSQL; an opensource DBMS. Hence, Sindbad: (a) takes advantage of the
scalability provided by the DBMS, and (b) is able to employ
early pruning techniques inside the DBMS engine, which

INTRODUCTION

In this paper we feature Sindbad [10]: a location-based social
networking system. Sindbad distinguishes itself from existing social networking systems (e.g., Facebook [4] and Twitter [12]) as
it injects location-awareness within every aspect of social interaction and functionality in the system. For example, posted messages
in Sindbad have inherent spatial extents (i.e., spatial location and
spatial range) and users receive friend news feed based on their locations and the spatial extents of messages posted by their friends.
The functionality of Sindbad is fundamentally different from current incarnations of location-based social networks (e.g., Facebook
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
ACM SIGSPATIAL LBSN ’12, November 6, 2012. Redondo Beach, CA,
USA
Copyright 2012 ACM 978-1-4503-1698-9/12/11 ...$15.00.

1

The rest of the paper is organized as follows: Section 2 gives
the system architecture of Sindbad. The three main components of
Sindbad, namely, location-aware news feed (GeoFeed), locationaware recommender system (LARS), and location-aware ranking,
are discussed in Sections 3, 4, and 5, respectively. Finally, section 6
concludes the paper.

dicates the spatial range for which the message is effective. The
message is deemed relevant to only those users who are located
within its spatial range.
A new rating. Sindbad users can give location-aware (spatial) ratings to various items in a scale from one to five. Location-aware
(spatial) ratings can take any of these three forms: (1) Spatial ratings for non-spatial items, represented as a four-tuple (user, userLocation, rating, item); for example, a user located at home rating
a book, (2) Non-spatial ratings for spatial items, represented as a
four-tuple (user, rating, item, itemLocation); for example, a user
with unknown location rating a restaurant with an inherent location, and (3) Spatial ratings for spatial items, represented as a fivetuple (user, userLocation, rating, item, itemLocation); for example,
a user at his/her office rating a restaurant with an inherent location.
Location-aware news feed queries. Once a Sindbad user logs
on to the system, a location-aware news feed query is fired to retrieve the relevant news feed, i.e., messages posted by the user’s
friends that have spatial extents covering the location of the requesting user. Details of the execution of the location-aware news feed
query will be discussed in Section 3. The output of the locationaware news feed module (GeoFeed) will be processed further by
the location-aware ranking module to get only the top-k news feed
based on the spatial and social relevance, which will be returned to
the user as the requested news feed. Details of the location-aware
ranking module will be described in Section 5.
Location-aware recommendation queries. Sindbad users can request recommendations of either spatial items (e.g., restaurants,
stores) or non-spatial items (e.g., movies) by explicitly issuing a
location-aware recommendation query. The location-aware recommender module (LARS) suggests a set of items based on: (a) the
user location (if available), (b) the item location (if available), and
(c) ratings previously posted by either the user or the user’s friends.
Details of LARS will be discussed in Section 4. Similar to locationaware news feed queries, the output of LARS goes through the
location-aware ranking module to select only the top-k items based
on both spatial and social relevance.

2.

3. LOCATION-AWARE NEWS FEED

, ,,

!"#$#%&
'())#*()&

, ,-+),9>$%$?,@+..$7+,

!"#$#%&
3(%(4#05(&

!"#$%"&'()$*+,-+).,
/++0,12+"/++03,

-+).,/++0,

!85/#%&
3(%(4#05(&

!"#$%"&
'()$*+,
4$&56&7,

6,(7(,(05()&

+)(,).,/(01)2/"&

;*"C?+,D>0$E+.,

!85/#%&
3(%(4#05(&
4+#"88+&0$%"&,

!"#$%"&'()$*+,
4+#"88+&0+*,1!(493,

!"#$%"&'()$*+,
4+#"88+&0$%"&,F=+*B,

96&0:$0,(;<,/=&#%"&.,

>(?&;""%/5#$80&
!"#$%"&'()$*+,-+).,
/++0,A=+*B,

!"#$#%&
3(%(4#05(&

!"#$#%&
3#$0*)&

,,

!<#,=&6280(&;""%/5#$80&

.#5(?88@&;""%/5#$80&

G,
G,
G,

.89,):9#,(&;""%/5#$80&

, ,-+),9>$%$?,4$%&7,

Figure 1: Sindbad System Architecture.
yields an efficient performance for the news feed, recommendation, and ranking functionalities.
7. Sindbad provides an RESTful [6] web API so that it would
allow a wide variety of applications to easily communicate
with Sindbad and make use of its unique features. Both a
web and smart phone (i.e., Android) applications are implemented on top of Sindbad to enrich the system user experience.

SINDBAD ARCHITECTURE

Motivation. Although news feed functionality is widely available
in all social network systems [11], these systems select the relevant
messages either based on the message timestamp or some importance criteria that ignores the spatial aspect of posted messages.
Thus, users may miss several important messages that are spatially
related to them. For example, when a traveling user logs on to a
social network site, the user would like to get the news feed that
match his/her new location, rather than receiving the most recent
(non-spatial) news feed. The same concept applies for users who
continuously log onto the system from the same location, yet have
a large number of friends. It is of essence for such users to limit
their news feed to the messages related to their location. Examples of the location-aware news feed returned by Sindbad include
a message about local news, a comment about a local store, or a
status message targeting friends in a certain area.
Contribution. The main idea of the location-aware news feed
module (GeoFeed) is to abstract the location-aware news feed problem into one that evaluates a set of location-based point queries
against each friend in a user’s friend list that retrieves the set of
messages issued that overlap with the querying user’s location.
The location-aware news feed is equipped with three different approaches for evaluating each location-based query: (1) spatial pull
approach, in which the query is answered through exploiting a spatial index over the messages posted by the friend, (2) spatial push
approach, in which the query simply retrieves the answer from

Figure 1 depicts the Sindbad system architecture that consists
of three main modules, namely, location-aware news feed (GeoFeed), location-aware ranking, and location-aware recommender
(LARS), and three types of stored data, namely, spatial messages,
user profiles, and spatial ratings. The communication between
Sindbad and the outside world is held through RESTful web API
interface (named Sindbad API Functions in Figure 1). The API
functions facilitates building a wide variety of applications (e.g.,
web applications, smart phone applications) on top of Sindbad. As
shown in Figure 1, Sindbad API functions can also be used to complement the functionality of existing social networking websites
e.g., Facebook, and turn their news feed and recommendation to
be location-aware. Sindbad can take five different types of input
(i.e., through the API interface): profile updates, a new message, a
new rating, a location-aware news feed query, and a location-aware
recommender query. The actions taken by Sindbad for each input
is described as follows:
Profile updates. As in typical social networking systems, Sindbad users can update their personal information, their friend list, or
accept a friend invitation from others.
A new message. Users can post spatial messages to be seen by their
friends, if relevant. A spatial message is represented by the tuple:
(MessageID, Content, Timestamp, Spatial), where MessageID and
Content represent the message identifier and contents, respectively,
Timestamp is the time the message is generated, while Spatial in-

2

(a) Sindbad News Feed Service
(b) Sindbabd Recommendation Service
Figure 2: Sindbad Web Application ScreenShots

(a) Location-Aware News Feed
(b) Location-Aware Recommender
Figure 3: Sindbad Android Phone Application ScreenShots
a pre-computed materialized view maintained by the friend, and
(3) shared push approach, in which the pre-computation and materialized view maintenance are shared among multiple users. Then,
the main challenge of GeoFeed is to decide on when to use each of
these three approaches for a query.
A better response time calls for using the spatial push approach
for all location-aware news feed queries issued to Sindbad. In this
case, all location-aware news feed are pre-computed. However,
this approach results in tremendous system overhead since a massive number of materialized views must be maintained. On the
other hand, favoring system overhead may result in executing more
queries using the spatial pull approach as no views needs to be
maintained. However, this approach may result in a long query response time for users who have a large number of friends, since
they will suffer a long delay when retrieving their news feed. Sindbad takes these factors into account when deciding on which approach to use to evaluate each query in a way that minimizes the
system overhead and guarantees a certain user response time. Sindbad is equipped with an elegant decision model that decides upon
using these approaches in a way that: (a) minimizes the system
overhead for delivering the location-aware news feed, and (b) guarantees a certain response time for each user to obtain the requested
location-aware news feed. More details about GeoFeed decision
model are provided in [2].
Application Dynamics. Figures 2(a) and 3(a) give an example
of how the user receives the location-aware news feed service via
both the Sindbad web application and phone application. When the
user logs on to Sindbad, the application displays the relevant news
for the user on the map. Each message is associated with a circle
representing the range of each message. The user may “change”
locations by dragging and dropping the green arrow on the map, to
see how the news feed will change accordingly. The user may also
submit a geo-tagged message to the system. For instance, the user

shares a message "The pizza at ABC restaurant is awesome" with a
range distance of one mile.

4. LOCATION-AWARE RECOMMENDER
Motivation. This section describes the location-aware recommender module [8, 9] of Sindbad. In general, recommender systems make use of community opinions to help users identify useful
items from a considerably large search space (e.g., Amazon inventory, Netflix movies). The technique used by many of these widely
deployed systems is collaborative filtering [1], which analyzes past
community opinions to find correlations of similar users and items
to suggest a set of personalized items to the querying user. Community opinions are expressed through explicit ratings represented by
the triple (user, rating, item) that represents a user providing a numeric rating for an item (e.g., movie). Unfortunately, ratings represented by this triple ignore the fact that both users and (some) items
are spatial in nature. Unlike traditional recommendation techniques
that assume the (non-spatial) rating triple (user, rating, item), the
location-aware recommender module (LARS) in Sindbad supports
a taxonomy of three types of location-based ratings: (1) Spatial
ratings for non-spatial items, represented as a four-tuple (user, ulocation, rating, item), where ulocation presents the user location,
(2) Non-spatial ratings for spatial items, represented as a four-tuple
(user, rating, item, ilocation), where ilocation presents the item location, and (3) Spatial ratings for spatial items, represented as a
five-tuple (user, ulocation, rating, item, ilocation), where ulocation
and ilocation present the user and item locations, respectively.
Contribution. Sindbad produces recommendations using spatial
user ratings for non-spatial items by employing a user partitioning
technique that exploits the user location embedded in the ratings.
This technique uses an adaptive pyramid structure to partition ratings by their user location attribute into spatial regions of varying
sizes at different hierarchies. Then, for a querying user located in a
region R, we apply an existing collaborative filtering technique [1]

3

that utilizes only the ratings located in R. The challenge, however,
is to determine whether all regions in the pyramid must be maintained in order to balance two contradicting factors: scalability and
locality. Maintaining a large number of regions increases locality (i.e., recommendations unique to smaller spatial regions), yet
adversely affects system scalability because each region requires
storage and maintenance of a collaborative filtering data structure
(i.e., model) necessary to produce recommendations. The pyramid
dynamically adapts to find the right pyramid shape that minimizes
storage overhead, i.e., increases scalability, without sacrificing locality. When deciding to merge or split a pyramid cell, we define a
system parameter M, a real number in the range [0,1] that defines
a tradeoff between scalability gain and locality loss. LARS merges
the child cells into the parent cell if:
(1 − M) ∗ scalability_gain > M ∗ locality_loss

or spatial domain (e.g., ω =1 indicates the user cares only for the
close messages, and ω = 0 indicates the user cares only for the
recent messages). The final rank score will be calculated based on
the following equation:
Rank Score = ω × Spatial Score + (1 − ω) × Temporal Score
Instead of ranking all objects, Sindbad location-aware ranking module encapsulates the user’s ranking preferences within the
query processor to improve the response time for the user. Moreover, if the user is continuously online, Sindbad location-aware
ranking module is responsible for keeping either the news feed or
the recommended items correctly ranked (i.e., continuously evaluating the ranking function) as the user moves or if the user changes
her preferences. The main idea is to reduce the number of friends
the system querying for the relevant messages. Because we notice
that for a top-k preferred location-based news feed query, it needs
to retrieve the relevant messages from at most k friends. Thus, we
track the highest ranked message scores from all the user friends to
select a candidate set of friends to retrieve the relevant messages.
Then, GeoRank queries friends by their highest ranked message
scores and keeps updating the ranking boundary for top-k candidate messages. The query processing terminates early once the
highest ranking score of the next querying friend is less then the
ranking boundary to save the significant amount of redundant computations.

(1)

Sindbad produces recommendations using non-spatial user ratings for spatial items by employing a travel penalty technique that
accounts for item locations embedded in the ratings. This technique
favors recommendation candidates the closer they are in travel distance to a querying user. The challenge here is to avoid computing
the travel distance for all spatial items to produce the list of recommended items, as this will greatly consume system resources. Sindbad addresses this challenge by employing an efficient query processing framework capable of terminating early once it discovers
that the list of recommended items cannot be altered by processing
more candidates. Sindbad employs an efficient query processing
framework capable of terminating early once it discovers that the
list of recommended items cannot be altered by processing more
candidates. Finally, to produce recommendations using spatial ratings for spatial items, Sindbad employs both the user partitioning
and travel penalty techniques to address the user and item locations
associated with the ratings. More details about LARS are given
in [8].
Application Dynamics. Figures 2(b) and 3(a) gives an example
of how the user interacts with the location-aware recommendation
service through both the Sindbad web and phone applications. The
user can ask Sindbad for recommendations (e.g., Restaurants in
Scottsdale where SIGMOD takes place) by clicking on the recommendation tab of the web interface or by clicking on the locationaware recommendation button in the mobile app. The user then
enters the type of object he is interested in (e.g., restaurant, theaters, stores) a spatial range in miles, and also the number of recommended items to be returned to him and then presses the Recommend button. The recommended items are then shown on the
map.

5.

6. CONCLUSION AND FUTURE WORK
In the paper, we gave an overview of Sindbad system architecture. Sindbad is a location-aware social networking system that
considers the spatial location as first class citizen within every aspects of social networking services. We introduced the three main
modules that constitute Sindbad: (1) Location-Aware News: that
delivers location-aware messages to the user, (2) Location-Aware
Recommendation: that generates recommendations based on both
the users and items spatial locations, and (3) Location-Aware Ranking: that incorporates the spatial location in ranking both the user
news and recommendations. In the future, we plan to plug-in more
social services inside Sindbad to enrich the user experience.

7. ACKNOWLEDGEMENT
This work is supported in part by the National Science Foundation under Grants IIS-0811998, IIS-0811935, CNS-0708604, IIS0952977 and by a Microsoft Research Gift.

8.[1] G.REFERENCES
Adomavicius and A. Tuzhilin. Toward the Next Generation of Recommender

LOCATION-AWARE RANKING

Sindbad users may have different preferences over messages
from the location-based news feed or recommender system. For
example, a traveling user may be more interested in messages that
were issued close to her current locations. On the other hand, the
stationary user maybe more interested in the most recently issued
messages. Moreover, due to the large volume of messages submitted to Sindbad and the user’s limited viewing capability (e.g.,
40 messages for the web page and 20 messages for mobile application), Sindbad provides a location-aware ranking module that is responsible for ranking the results coming out of the location-aware
news feed module and location-aware recommender module, based
on the user’s preferences.
GeoRank ranks the messages based on multiple domains, i.e.,
temporal domain and spatial domain. Each user can also specify a
preference parameter ω to indicate her preference over the temporal

[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]

4

Systems: A Survey of the State-of-the-Art and Possible Extensions. TKDE,
17(6):734–749, 2005.
J. Bao, M. F. Mokbel, and C.-Y. Chow. GeoFeed: A Location-Aware News
Feed System. In ICDE, 2012.
C.-Y. Chow, J. Bao, and M. F. Mokbel. Towards Location-based Social
Networking Services. In ACM SIGSPATIAL-LBSN, 2010.
Facebook. http://www.facebook.com/.
The Facebook Blog, "Facebook Places": http://tinyurl.com/3aetfs3.
R. T. Fielding and R. N. Taylo. Principled Design Of The Modern Web
architecture. In ICSE, 2000.
Foursquare: http://foursquare.com.
J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel. LARS: A
Location-Aware Recommender System. In ICDE, 2012.
M. Sarwat. Recdb: Towards dbms support for online recommender systems. In
SIGMOD/PODS PhD Symposium, pages 33–38, 2012.
M. Sarwat, J. Bao, A. Eldawy, J. J. Levandoski, A. Magdy, and M. F. Mokbel.
Sindbad: a location-based social networking system. In SIGMOD, 2012.
A. Silberstein, J. Terrace, B. F. Cooper, and R. Ramakrishnan. Feeding Frenzy:
Selectively Materializing User’s Event Feed. In SIGMOD, 2010.
Twitter. http://www.twitter.com/.

RecDB in Action: Recommendation Made Easy in
∗
Relational Databases
Mohamed Sarwat

James Avery

Mohamed F. Mokbel

University of Minnesota, Twin Cities
Minneapolis, MN 55455

University of Minnesota, Twin Cities
Minneapolis, MN 55455

University of Minnesota, Twin Cities
Minneapolis, MN 55455

sarwat@cs.umn.edu

javery@cs.umn.edu

mokbel@cs.umn.edu

ABSTRACT
In this paper, we demonstrate RecDB; a full-fledged database system that provides personalized recommendation to users. We implemented RecDB using an existing open source database system
PostgreSQL, and we demonstrate the effectiveness of RecDB using two existing recommendation applications (1) Restaurant Recommendation, (2) Movie Recommendation. To make the demo
even more interactive, we showcase a novel application that recommends research papers presented at VLDB 2013 to the conference
attendees based on their publication history in DBLP.

1.

INTRODUCTION

Recommender systems have grabbed significant attention in both
commercial [3, 4, 7] and academic [1, 2, 5, 6, 9] settings. The main
objective of a recommender system is to suggest new interesting
items to users from a large pool of items. Recommender systems
are implicitly employed on a daily basis to recommend movies
(e.g., NetFlix), friends (e.g., Facebook), news articles (e.g., Google
News) [4], books/products (e.g., Amazon) [7], and Microblog posts
(e.g., Twitter). For instance, Netflix reported that 75% of Movies
users watch on Netflix is from recommendation.
Recommendation techniques exploit the history of events performed by the system users in order to extract a set of interesting
items for each user. These events might be users clicks (i.e., website links visited), users opinions (e.g., movie ratings), or users purchases (e.g., buying a product on Amazon). In technical terms, a
recommender system takes as input a set of users U , items I, and
user/item events R and estimates a utility function F(u, i) that predicts how much a certain user u ∈ U will like an item i ∈ I such
that i has not been already seen (or watched, consumed...) by u [2].
Currently, to support the recommendation functionality in any
application, a developer must implement the recommendation utility estimation as well as the recommendation query execution algorithms at the application layer. That is considered a hassle for a
novice application developer who might not be quite familiar with
efficient recommender system implementations. An application
∗This work is supported in part by the National Science Foundation
under Grants IIS-0952977 and IIS-1218168.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Articles from this volume were invited to present
their results at The 39th International Conference on Very Large Data Bases,
August 26th - 30th 2013, Riva del Garda, Trento, Italy.
Proceedings of the VLDB Endowment, Vol. 6, No. 12
Copyright 2013 VLDB Endowment 2150-8097/13/10... $ 10.00.

developer would prefer to declaratively create and query recommenders and save the effort to focus on the main application logic.
Moreover, optimizing recommendation queries might be tedious
especially that most applications integrate (e.g., JOIN) generated
recommendation with other data to enrich the end-user experience.
In this paper, we demonstrate RecDB a full-fledged database
system that produces personalized recommendations to the system users. Integrated with an open source relational DBMS (i.e.,
PostgreSQL), RecDB uses SQL to seamlessly integrate the recommendation functionality with traditional relational operators, i.e.,
SELECT, PROJECT, JOIN. To this end, RecDB introduces a new
SQL statement, CREATE RECOMMENDER, that takes the input
user/item events data, internally runs the recommendation algorithm, and creates the data structures necessary to generate recommendations. RecDB therefore employs a new SQL operator,
RECOMMEND, that leverages the initialized data structures to generate recommendations to the querying user. We summarize the main
features of RecDB, as follows.
• Usability: The system is easily used and configured so that
a novice application developer can define a variety of recommenders that fits the application needs in few lines of code.
RecDB helps the community building an out-of-the-box tool
to implement a myriad of recommendation applications.
• Flexibility: RecDB is flexible in terms of defining a recommender using a wide range of popular recommendation algorithms (e.g., item-based/user-based collaborative filtering,
singular value decomposition), presented in the literature and
implemented inside RecDB.
• Seamless Integration: The system is able to seamlessly integrate the recommendation functionality in the traditional
SPJ, i.e., SELECT, PROJECT, JOIN, query pipeline to execute rich recommendation queries.
• Efficiency: RecDB provides near real-time personalized recommendation to a high number of users over a large pool of
items, and enormous user/item events matrix.
To prove RecDB effectiveness, we demonstrate the system using
two (existing) real life applications: (1) MovieLens [8]: a system
developed at University of Minnesota that delivers movie recommendation to ≈ 72K end-users world wide and (2) Sindbad [10]:
A location-aware social networking system developed at University of Minnesota and provides a restaurant recommendation service [11] to its users. We replace the underlying recommender system, already-deployed for both applications, with RecDB to show
the effectiveness of our system and its applicability to real life recommendation scenarios. Moreover, we build a new application
that leverages the publication history (i.e., retrieved from DBLP) of
VLDB 2013 conference attendees and recommends papers to them

1242

Application Developer

System Client

Create / Drop
Recommender

Recommendation
Query / Answer

Layer I

Recommender Creation SQL

Recommendation Query SQL

Layer II

Recommender Initialization

Query Processing

Layer III

Recommendation Models, Data
Structures, and Maintained Views

Recommender Statistics
& Histograms

RecDB

2.2 Recommendation Algorithms
Users

Items

Events

Recommender
Input Data

Figure 1: RecDB System Overview.

accordingly. For the paper recommendation application, we furthermore allow the demo attendee to issue ad-hoc recommendation
queries using psql, i.e., PostgreSQL client, to show the simplicity
of integrating recommendation functionality with other relational
operators in PostgreSQL.

2.

The application developer specifies the following parameters:
(1) Recommender name: A unique name assigned to the newly
declared Recommender. (2) Users Table, Items Table,
and Events Table: names of SQL tables that contains the
users, items, and user/item events information. The users, items,
and events data tables are specified in the USERS FROM, ITEMS
FROM, and EVENTS FROM SQL clauses. (4) Recommendation
Algorithm: the application developer may choose to build the
recommender using several recommendation algorithms supported
by RecDB (e.g., Item-Item collaborative filtering, User-User collaborative filtering, singular value decomposition), by specifying
the recommendation algorithm in the USING clause.

Most recommendation algorithms perform two main steps:
Step I: Model Building: That step is performed by the recommender initialization component when the application developer
issues a CREATE RECOMMENDER statement to RecDB. That step
consists of building a recommendation model RecModel using the
recommender input data. For instance, for the Item-Item Collaborative Filtering algorithm, we generate an items similarity list. To
compute the similarity SimScore(ip , iq ), we represent each item as
a vector in the user-events space of the user/item events matrix.
Many similarity functions have been proposed (e.g., Cosine); the
Cosine similarity is calculated as given in equation 1.

SYSTEM OVERVIEW

Figure 1 highlights the layered architecture of RecDB.
Input Data. RecDB assumes the following data, as input:
(1) Users: a set of users U = {u1 , ..., un }. (2) Items: a set of
items I = {i1 , ..., im }. (3) Events: Each user uj performs actions
or expresses opinions about a set of items Iuj ⊆ I. Events can
be a numeric rating (e.g., the Netflix scale of one to five stars), or
unary (e.g., Facebook “likes”, Foursquare “check-ins”, or Amazon
purchases). RecDB consists of three layers, as follows:
Layer I: SQL Layer: This layer supports two new SQL clauses:
(1) Recommender Creation SQL, and (2) Recommender Query
SQL. These new SQL clauses are leveraged by the application developer and the clients in declaring and querying personalized recommenders. The parsed SQL statements are then passed to the
relevant component in the processing layer.
Layer II: Processing Layer: As given in Figure 1, this layer
consists of two main components, namely (1) Recommender Initialization: This component creates the necessary recommendation models, data structures, and views for a created recommender.
(2) Query processing: This component efficiently executes recommendation queries over a created recommender and returns the recommendation answer back to the user.
Layer III: Indexing and Storage Layer: This layer stores a set
of data structures and recommendation models necessary to produce recommendations. For efficient query execution, RecDB also
stores a set of views that contains the final recommendation scores
generated using the recommendation model. The system also saves
statistics about created recommenders and query/update workloads
that are harnessed by the query processing component.

2.1 Recommender Creation
The application developer creates a new recommender using the
CREATE RECOMMENDER SQL statement, as follows:
CREATE RECOMMENDER [Recommender Name]
USERS FROM [Users Table] KEY Users.uid
ITEMS FROM [Items Table] KEY Items.iid
EVENTS FROM [Events Table] KEY Events.eid
USING [Recommendation Algorithm]

SimScore(ip , iq ) =

i~p · i~q
~
kip kki~q k

(1)

Step II: Recommendation Generation: This step is performed by
the query processing component when a user issues a recommendation query to RecDB. This step utilizes the RecModel (e.g.,
items similarity list) created in Step I to predict a recommendation score, RecScore(u, i) (equation 2), for each user/item pair.
RecScore(u, i) reflects how much each user u likes item i.
P
l∈L sim(i, l) ∗ ru,l
RecScore(u, i) = P
(2)
l∈L |sim(i, l)|
RecDB users may select an algorithm among a variety of recommendation algorithms that fits their application needs. Examples
are as follows: (1) Item-Item Collaborative Filtering with Cosine
Similarity Function (abbr. ItemCosCF), and its variants (2) UserUser Collaborative filtering (abbr. UserCosCF), and its variants
(3) Regularized Gradient Descent Singular Value Decomposition
(abbr. SVD). (4) Content-based Filtering (abbr. ContentFilter).

2.3 Recommendation Query
Once a recommender is initialized, users can issue recommendation queries over that initialized recommender. A recommender is
exposed to the querying user as a virtual SQL table that has a virtual
schema, (uid,iid,RecScore), explained as follows: (1) uid:
ID of a user who exists in the users table, (2) iid: ID of an item in
the items table, (3) RecScore: a recommendation score (values
between 0 and 1) that predicts, i.e., based on the underlying recommendation algorithm, how much the user would like the item.
In RecDB, we define a new clause named RECOMMEND that is
integrated with traditional SQL clauses, e.g., SELECT, FROM, and
WHERE clauses, as follows:
SELECT [Select Clause]
FROM [Recommender], [Tables]
WHERE [Where Clause]
RECOMMEND(k) User_ID

The RECOMMEND clause is responsible for generating k recommendations using an initialized recommender. In the FROM clause,

1243

Recommender

Recommender Declaration SQL

Recommendation Query SQL

MovieRec
Movie recommender built using the item-item CF (ItemCF)
recommendation algorithm

Q1 : SELECT A.mid FROM MovieRec A RECOMMEND(5) A.uid = 1

RestaurantRec
Restaurant recommender built
using the singular value decomposition (SVD) recommendation algorithm

CREATE RECOMMENDER MovieRec
USERS FROM Users KEY uid
Items FROM Movies KEY mid
EVENTS FROM Ratings KEY uid,mid
USING ItemCosCF
CREATE RECOMMENDER RestaurantRec
USERS FROM Users KEY uid
Items FROM Restaurants KEY rid
EVENTS FROM CheckIns KEY uid,rid
USING SVD

PapersRec
VLDB 2013 paper recommender built with contentbased filtering recommendation algorithm

CREATE RECOMMENDER PapersRec
USERS FROM Authors KEY aid
Items FROM Papers KEY pid
EVENTS FROM Citations KEY aid,pid
USING ContentFilter

Q2 : SELECT E.name FROM MovieRec A, Movies E
WHERE A.mid = E.mid AND E.genre = ’Comedy’
RECOMMEND(5) A.uid = 10
Q3 : SELECT C.name FROM RestaurantRec1 B, Restaurants C
WHERE B.rid = C.rid AND C.location = ’New York City’
RECOMMEND(10) B.uid = 1
Q4 : SELECT C.name FROM RestaurantRec1 B, Restaurants C
WHERE B.rid = C.rid AND C.location = ’Riva Del Garda’
RECOMMEND(10) B.uid = 10
Q5 : SELECT F.title FROM PapersRec D, Papers F
WHERE F.pid = D.pid AND F.venue=’VLDB2013’
RECOMMEND(10) D.aid = 100
Q6 : SELECT F.title, G.session, G.time
FROM PapersRec D, Papers F, VLDB2013Program G
WHERE D.pid = F.pid AND G.pid = F.pid AND
F.venue=’VLDB2013’ AND G.Day = 2
RECOMMEND(10) D.aid = 100

Table 1: RecDB Applications SQL.

Figure 2: MovieLens: Movie recommendation website.
the user specifies a recommender [Recommender] that is harnessed by the system to produce k recommended items for user
USER ID. To execute a recommendation query, RecDB invokes an
operator, named Recommend, that is responsible for evaluating the
user/item recommendation scores for all items unseen by the querying user. When a user asks for recommendation, the Recommend
operator calculates the recommendation score RecScore , based
on the selected recommendation algorithm (see Equation 2), for
all candidates items and selects the top-k items with the highest
RecScore value and returns them to the user. Recommend is integrated with other relational operators (e.g., Select, Project,
Join) in the query pipeline.

Figure 3: Sindbad: Restaurant recommendation website.

In this section, we present two existing real life applications for
which we employ RecDB as the underlying system for demonstration purpose: (1) Movilens: Movie Recommendation Application,
and (2) Sindbad: Restaurant Recommendation Application. Furthermore, we developed an application that recommends papers
presented in VLDB 2013 to the conference attendees based on their
publication history. Table 1 shows how to create and query recommenders, in RecDB, for the three aforementioned applications.

the set of users that contains information about all users registered with MyRest. Each user tuple consists of a user ID
and name. (2) Movies (mid, title, genre): the set of
movies saved in the database; each movie has a unique ID and
name. (3) Ratings (uid, mid, rating, timestamp):
The history of ratings such that each rating represents how much a
user liked a movie she/he watched.
The first row in Table 1 gives the details of the CREATE
RECOMMENDER SQL statement used to declare MovieRec, a recommender that is created on top of the Users, Movies, and Ratings database tables. We specify the item-item collaborative filtering method to be applied to the declared recommender. Query
Q1 retrieves five movie recommendations using MovieRec.
MovieRec is placed in the FROM statement of the issued query.
The user, for whom the recommendation needs to be generated
(A.uid = 1), is passed in RECOMMEND(5) clause. Q2 recommends five Comedy movies to user (A.uid = 10) and returns the
title (title) of each movie.

3.1 MovieLens: Movie Recommendation

3.2 Sindbad: Restaurant Recommendation

Figure 3 depicts a screenshot from MovieLens—movie recommendation application. The data set leveraged by this application consists of three tables: (1) Users (uid, name):

Figure 3 shows a screenshot of Sindbad restaurant recommendation service. The data set leveraged by this application consists of three tables: (1) Users (uid): that con-

3.

DEMONSTRATION SCENARIOS

1244

Figure 4: VLDB 2013 Papers Recommendation Application.
tains IDs of all registered users, (2) Restaurants (rid,
name, location): the set of restaurants saved in the database
such that each restaurant has a name and a spatial location (i.e., city), and (3) CheckIns (uid, rid, visited,
timestamp): The history of check-ins that represents whether a
user has visited a restaurants before. In such case, the visited
field is set one if the user visited the restaurant, and zero otherwise.
The application generates restaurant recommendation to users
based upon their spatial locations. We create RestaurantRec;
a recommender that builds a singular value decomposition (SVD)
recommendation model using CheckIns table as the user/item
events matrix. A user visiting ’New York City’ asks for
restaurant recommendation by issuing query Q3 . For Q3 , the
user states the current user location using traditional SQL operators (WHERE B.iid = C.iid AND C.location = ’New
York City’). RecDB therefore produces a set of ten restaurants
by passing the user ID (B.uid = 1) to the RECOMMEND(10)
clause. Similarly, Q4 recommends ten restaurants in ’Riva Del
Garda’ to the user (uid = 10).

along with the session name, the day/time in which the paper is
presented, as well as the presentation location (e.g., hall name).
The last row in Table 1 gives the SQL used for building the paper
recommendation application. We create a content-filtering recommender (ContentFilter [2]) that leverages the papers abstracts
content and the Citations table to recommend users new papers
(in VLDB 2013) that are similar (in content) to other papers they
cited before. Using this application, the demo attendee may ask
for papers recommendation by issuing queries similar to Q5 and
Q6 in table 1. For instance, Q5 recommends VLDB 2013 papers
that correspond to the top-k papers for which the content is similar to the papers cited by the querying user before. Q6 performs
the same functionality with the extra feature of recommending only
papers that are scheduled to be presented in the second day of the
conference. The idea is to get real time paper recommendation
for the conference attendees. We also allow the user to choose a
specific conference day to get paper recommendation accordingly.
For more interactivity, we allow the demo attendee to issue ad-hoc
queries using psql.

3.3 VLDB 2013 Papers Recommendation

4. REFERENCES

Figure 3 exhibits a screenshot of the paper recommendation
application. We leverage the DBLP citation database to build
an application that recommends papers to VLDB 2013 attendees
such that the recommended papers are presented in the conference. The database schema is as follows: (1) Authors (aid,
dblp name): a table that contains a set of 500 authors that
publish papers in database venues (i.e., VLDB, SIGMOD, ICDE,
EDBT). Each user tuple consists of an author identifier (aid),
and the author name as it appears in DBLP. (2) Papers (pid,
title, abstract, venue): the set of papers published by
any author in the Authors table in database venues (including
VLDB 2013). Each tuple contains a paper identifier (pid), title of
the paper (title), the abstract content, and the venue in which
the paper is published (venue). (3) Citations (aid, pid,
cited, timestamp): The history of citations such that each
citation represents whether an author has cited a paper. cited
is a boolean field; it is set to one if the author aid has cited paper pid, and zero otherwise. (4) VLDB2013Program (pid,
aid, session,Day,time,location): that contains the
VLDB 2013 conference schedule. Each entry represents a paper

[1] Z. Abbassi and L. V. S. Lakshmanan. On Efficient Recommendations for
Online Exchange Markets. In ICDE, 2009.
[2] G. Adomavicius and A. Tuzhilin. Toward the Next Generation of Recommender
Systems: A Survey of the State-of-the-Art and Possible Extensions. TKDE,
17(6), 2005.
[3] S. Amer-Yahia, A. Galland, J. Stoyanovich, and C. Yu. From del.icio.us to
x.qui.site: recommendations in social tagging sites. In SIGMOD, 2008.
[4] A. Das et al. Google News Personalization: Scalable Online Collaborative
Filtering. In WWW, 2007.
[5] B. Kanagal, A. Ahmed, S. Pandey, V. Josifovski, J. Yuan, and L. G. Pueyo.
Supercharging Recommender Systems using Taxonomies for Learning User
Purchase Behavior. PVLDB, 5(10):956–967, 2012.
[6] G. Koutrika, B. Bercovitz, and H. Garcia-Molina. FlexRecs: Expressing and
Combining Flexible Recommendations. In SIGMOD, 2009.
[7] G. Linden, B. Smith, and J. York. Amazon.com Recommendations:
Item-to-Item Collaborative Filtering. IEEE Internet Computing, 7(1), 2003.
[8] MovieLens: http://www.movielens.org/.
[9] S. B. Roy, S. Amer-Yahia, A. Chawla, G. Das, and C. Yu. Space efficiency in
group recommendation. VLDB Journal, 19(6), 2010.
[10] M. Sarwat, J. Bao, A. Eldawy, J. J. Levandoski, A. Magdy, and M. F. Mokbel.
Sindbad: A Location-based Social Networking System. In SIGMOD, 2012.
[11] M. Sarwat, J. J. Levandoski, A. Eldawy, and M. F. Mokbel. LARS*: A Scalable
and Efficient Location-Aware Recommender System. In TKDE, 2013.

1245

2013 IEEE 14th International Conference on Mobile Data Management

P LUTUS: Leveraging Location-based Social
Networks to Recommend Potential Customers to
Venues
Mohamed Sarwat
Ahmed Eldawy
Mohamed F. Mokbel
Department of Computer Science and Engineering
University of Minnesota
200 SE Union Street, Minneapolis, MN 55455, USA

John Riedl

sarwat@cs.umn.edu, eldawy@cs.umn.edu, mokbel@cs.umn.edu, riedl@cs.umn.edu
services (e.g., BrightKite 1 , Foursquare 2 , Gowalla 3 , and
Facebook Places) are getting more and more popular. For
instance, as of September, 2012, Foursquare claims to have
over 25 million people worldwide, and over billions of checkins with millions more every day [5]. Users, in a location-based
social networking website, can select their friend list as well
as getting listed as friends to other users in a same way like
traditional social networking systems. In addition, users are
associated with a geo-location, and might alert friends when
visiting a venue (e.g., restaurant, bar) by checking-in on their
mobile phones (e.g., iPhone, Android).

Abstract—In a business setting, the customer value is crucial
as it determines how much it is worth spending to acquire
a particular customer. Viral marketing techniques leverages
social ties among users to help advertising a particular product.
Recently, as mobile devices (e.g., smart phones, GPS devices) became ubiquitous, location-based social networking websites (e.g.,
Gowalla, BrightKite, Foursquare) are getting more and more
popular. Along with location-based social networking services
being prominent, new kind of data came into play besides the
traditional social networking data: (1) Spatial data: represents
the users geo-locations, venues geo-locations and information
about users visiting different venues. (2) Users Opinions data:
represents how much a user likes the venues she visits (e.g.,
Alice visited restaurant A and gave it a rating of ﬁve over ﬁve).
In this paper, we present P LUTUS; a framework that assists
venues (e.g., restaurant, gym, shopping mall) owners in growing
their business. To recommend the best set of customers, P LUTUS
takes three main aspects into consideration: (1) Social aspect,
(2) Spatial aspect, and (3) Users opinions aspect. To this end,
P LUTUS proposes two main algorithms: (1) Proﬁt Calculation: It
is responsible of calculating the total proﬁt that a user u may
add to a venue v taking into account the social, spatial, and
user opinions aspects. (2) Proﬁt Maximization: This algorithm is
used to maximize the total proﬁt of a given venue. We evaluated
P LUTUS using real data set extracted from an existing Locationbased Social Networking website, Foursquare. The results show
that Plutus achieves higher estimated proﬁt and more efﬁcient
proﬁt calculation than naive marketing algorithms.

I.

Along with location-based social networking services being
ubiquitous, new kind of data came into play besides the
traditional social networking (i.e., friendship) data: (1) Spatial data: represents the users geo-locations, venues (e.g.,
restaurant, gym, shopping mall) geo-locations and information
about users visiting different venues. (2) Users Opinions data:
represents how much a user likes the venues she visits (e.g.,
Alice visited restaurant A and gave it a rating of ﬁve over
ﬁve). The combination of social, spatial, and opinions data
extracted from location-based social networking services can
be leveraged to help venues grow their business. This can be
achieved by recommending a set of users U to a venue v who
are expected to inﬂuence other users that are: (a) socially close
to users in U , (b) nearby venue v geo-location, and (c) are
expected to like venue v.
In this paper, we present P LUTUS; a framework that assists
venues owners in growing their business. The framework helps
marketers efﬁciently spending their budget in a way that
maximizes the expected venue’s proﬁt. For instance, if the
business owner has to distribute K coupons, P LUTUS helps
her selecting the best K customers that maximizes the expected
proﬁt. For simplicity, we measure the expected proﬁt in terms
of the number of people visiting the venue per a period of
time. For example, If venue A is visited by 1000 users in 10
days and venue B is visited by 600 users in the same 10 days,
implying that venue A has higher proﬁt than B.

I NTRODUCTION

In a business setting, the customer value is crucial as
it determines how much it is worth spending to acquire a
particular customer [1]. Past work [2], [3], [4] has proposed
using social ties to help advertising a particular product; a
technique also known as viral marketing. By using social
ties, the customer value is not only determined by how much
dollars this particular customer is willing to spend to buy
the advertised product. However, the customer value is also
determined by how much inﬂuence s/he has on his/her friends
(i.e., network value), as a more inﬂuential customer might
inﬂuence his/her friends to buy the product, as well.

To recommend the set of users that leads to achieving
higher proﬁt, P LUTUS takes into account three main aspects:
1 BrightKite:

http://brightkite.com
http://foursquare.com
3 Gowalla: http://gowalla.com.

Recently, as mobile devices (e.g., smart phones, GPS
devices) became ubiquitous, location-based social networking
978-0-7695-4973-6/13 $26.00 © 2013 IEEE
DOI 10.1109/MDM.2013.13

2 Foursquare:

26

Mean

Stdev

Would you consider visiting a restaurant, if you know that
a friend of yours likes and visits this restaurant

0.96

0.2

If you know that your favorite celebrity likes and visits a
particular restaurant, will this encourage you to visit this
restaurant

0.81

0.43

If you know that a professional food critique gave a high
rating to some restaurant, will this encourage you to visit
this restaurant

0.74

0.39

Do you use the ”check-in” (location sharing) option in any
social networking website

0.62

0.48

TABLE I.



Survey Item

M ECHANICAL T URK S URVEY











	












Fig. 1.

(1) Social Network: The framework leverages the social ties
among users drawn from the social graph, and hence acquires
the most inﬂuential users in the social graph. In other words,
P LUTUS keeps track of how each user inﬂuences each of his
friends in the social graph and uses that in recommending the
best users. (2) Spatial: P LUTUS considers the venue location
(i.e., address), the user location, and user’s check-ins (i.e.,
visits) at different venues to retrieve users that are spatially
relevant to the business. (3) Users Opinions: The framework
takes into account the user ratings to different venues to acquire
the users that are expected to like the venue.

Foursquare Users Travel Distance

measure whether users are inﬂuenced by their social ties (i.e.,
friends, favorite celebrities, and experts) when it comes to
visiting venues (e.g., restaurants). We have asked the users
the questions shown in table I. In the survey, 62 participants
claims that they use the check-in option provided by some
social networking services (e.g., Facebook, Foursquare). More
importantly, 96 participants admit they would go to a restaurant
if they know their friends likes and visits this restaurant. On
the other hand, 81 of participants said they would consider
visiting a restaurant if they knew their favorite celebrity prefers
and visits that restaurant. Finally, 74 mentioned they would
visit a restaurant if they knew that an expert food critique
gave a high rating to that restaurant. The take-away from
the study is that users are substantially inﬂuenced by the
opinions of their acquaintances, celebrities, and experts when
visiting restaurants. Moreover, the numbers in Table I show
that users are inﬂuenced by their friends more than experts
and celebrities. A more profound user study is performed by
Lindqvist et.al in [7].

To this end, P LUTUS proposes two main algorithms:
(1) Proﬁt Calculation: It is responsible for calculating the
total proﬁt that a user u may add to a venue v taking into
account the social, spatial, and user opinions aspects. (2) Proﬁt
Maximization: This algorithm has two versions (Celebritybased and Coupon-based proﬁt maximization) and each of
them is used to maximize the total proﬁt of a given venue.
Each version, more or less, serves the same purpose, i.e.,
maximizing the expected venue proﬁt (i.e., increase the number
of people visiting the place). However, they differ based upon
the user requirements and expectations; A P LUTUS user may
select one of the following versions of the proﬁt maximization
algorithm: (a) Celebrity-based Proﬁt Maximization: Given a
total marketing budget (e.g., total amount of dollars dedicated
for the marketing campaign), P LUTUS recommends any number of users that maximizes the total proﬁt of the designated
venue. (b) Coupon-based Proﬁt Maximization: Given a maximum marketing budget per user (e.g., coupon price), P LUTUS
recommends K users that maximizes the expected total proﬁt
of the designated venue.

Foursquare Data Analysis. We analyzed data from the
Foursquare location-based social network containing user visits
data for 1M users to 643K venues across the United States.
Figure 1 gives the travel distance histogram of the examined
Foursquare users. We ﬁgured out that users tend to travel a
limited distance when visiting these venues. We refer to this
property as travel locality. In our analysis of Foursquare data,
we observed that 45% of users travel 10 miles or less, while
75% travel 50 miles or less. The results are similar to the
results of location-based social network data analysis given
in [8]. This observation suggests that customers closer in travel
distance to a venue are expected to bring higher proﬁt to that
venue. In other words, the farther a customer is from a venue,
the more that customer loses her added value to that venue.

In the rest of the paper, we ﬁrst present an initial study
in section II to motivate the problem. we then give an
overview of P LUTUS in section III. We then describe the
proﬁt calculation algorithm in section IV. Both versions of the
Proﬁt maximization algorithm are then explained in section V.
Section VI evaluate the performance of P LUTUS. Related
works are highlighted in section VII. Finally, Section VIII
summarizes P LUTUS contributions and concludes the paper.
II.













III.

P LUTUS OVERVIEW

Figure 2 gives an overview of P LUTUS. As presented in the
ﬁgure, P LUTUS takes three kinds of data, as input: (1) Social
Network Data, (2) Spatial Data, and (3) Users Opinions data,
described as follows.

I NITIAL S TUDY

A. Social Networking Data
Mechanical Turk Study. To motivate our work, we ran a
user study on Amazon Mechanical Turk [6]. We conducted
a survey on a hundred users through Amazon Mechanical
Turk. The main aim of the survey was to quantitatively

The social networking data model is a typical directed
social graph in which nodes represent the set of users U = {u1 ,
u2 , ..., un }, and a set of edges E = {e1 , e2 , ..., em } represents
the friendship relationship between users. In the rest of the

27

	
 

(



	

"!


 



"

"


 !

"




$



Fig. 2.





!"

$*



P LUTUS framework overview

paper, we use the notions user and node interchangeably to
represent a social graph node. Each user ui is associated with
a static proﬁt S ui ,vj which represents the expected static proﬁt
gained when Customer (i.e., user) ui visits the venue vj . For
example, when Alice visits restaurant R, she may have a 12
dollars meal, so S Alice,R is equal to $12. Each user has a
cost C ui ,vj deﬁned as the amount of dollars needed to attract
a Customer (i.e., user) ui to visit the venue vj . For instance,
the cost to make Justin Bieber visit Gym X, C JustinBieber,X
is one million dollars. Each directed edge e ∈ E between two
users u1 and u2 is annotated with the inﬂuence that u1 has on
u2 , denoted as I ui ,ul , and formally deﬁned as follows:

	

	



(
(

"
$*




(

"!



"
$*

(
(





$*
"

"
$*




"

"!



"!
"!
"!

"
(
"










Fig. 3.

	



(
$*




"
"!
"
"

 !


 















 
 



)"$
)"$




	

	










)"$
&'
!!#" %
!!#" %
!"
!!#" %
!$!%"

P LUTUS Data Model

ﬁltering assumes a set of n users U = {u1 , ..., un } and a
set of m venues V = {v1 , ..., vm }. Each user uj expresses
opinions about a set of venues Vuj ⊆ V. For a venue v,
P LUTUS calculates the predicted rating that u would give to
venue v using item-based collaborative ﬁltering technique (see
section IV-A for details).

Deﬁnition 1. Given a user u1 and another user u2 , I u1 ,u2 is
deﬁned as the inﬂuence of user u1 on u2 . It is the probability
that user u2 visits a place given that user u1 already visited
the same place. Notice that I u1 ,u2 is not a symmetric relation.
In other words, the value of I u1 ,u2 may be different from the
value of I u2 ,u1 .

D. Plutus Functionality
Input and Output Parameters. P LUTUS takes up to three
input parameters from its users, as shown in Figure 2. These
parameter are as follows:

For instance in Figure 3, I Eva,Joe is equal to 0.3 and
I Joe,Eva is equal to 0.7. That means that Joe has higher
inﬂuence on Eva than Eva has on him.

1)

B. Spatial Data

2)

The spatial data gives information on the user location,
venue location and the visits of different users to different
venues. Each User ui ∈ U is associated with a geo-location
(i.e., Longitude and Latitude) Lui . For instance, if the address
of user ui is x1 ,y1 , then Lui =x1 ,y1 . Similarly, each venue
vi ∈ V has a geo-location Lvi referring to its spatial address.
For instance, if the address of restaurant R is x1 ,y1 , then
LR =x1 ,y1 . Each user might have visited different venues at
different times, by means of the check-in function available
in location-based social networks. We have a set of checkins C = {u1 ,v1 ,t1 , u2 ,v2 ,t2 , ..., ui ,vj ,tl }. A check-in c
= ui ,vj ,tl , which means that user ui has visited venue vj at
time tl .

3)

Venue V : It is the venue for which P LUTUS will
recommend a set of potential customers that are
expected to increase the proﬁt of that venue.
Budget/Coupon price: It is a mandatory parameter
used to determine the amount of dollars dedicated to
the marketing campaign. This amount may represent
a full budget or a single coupon price; P LUTUS deals
with both scenarios.
Number of customers (i.e.,) K: It is an optional parameter used to determine the numbers of customers
to be recommended to the designated venue V to
increase its expected proﬁt.

The output of P LUTUS is a set of customers (i.e., social
network users) that are expected to increase the proﬁt of the
input venue V . The framework may return a set of customers
with aribitrary size, unless K is given as input. In this case,
P LUTUS returns a number of customers m such that m ≤ K.
P LUTUS Algorithms. P LUTUS has two main algorithms:
(1) Proﬁt Calculation Algorithm: It is used to calculate the
proﬁt of recommending a user u to a venue v, and (2) Proﬁt
Maximization algorithm: Given the calculated proﬁt, this algorithm determines what customers (i.e, users) should be
recommended to venue V to increase its proﬁt. The proﬁt
maximization algorithm has two versions: (a) Celebrity-based
Proﬁt Maximization: Given a total marketing budget (e.g.,
total amount of dollars dedicated for the marketing campaign)

C. Users Opinions Data
The opinions data deﬁne how much a user u likes a
venue v. The users opinions can be extracted from the ratings
they gave to different venues they visited before. However,
in case the user did not visit the venue before, we employ a
rating prediction technique to predict how the user might have
rated that particular venue. A well-know method for rating
prediction is collaborative ﬁltering [9], [10]. Collaborative

28

A. Proﬁt Calculation with Users Opinions

Algorithm 1 Network Proﬁt Calculation
1: Function CalculateNetworkProﬁt(Customer ui , Venue v)
2: N etworkP rof it ← 0.0
j
3: for each User Fui
in User ui Friends List do
4:
N etworkP rof it ← N etworkP rof it +
CalculateTotalProﬁt(P

F

5: end for
6: return N etworkP rof it

j ,v)
ui

×I

ui ,F

Notice that the total proﬁt presented in equation 1 does not
take the user rating (i.e., opinion) into account. The user rating
to venue Rui ,v is necessary to be included in the equation, as
the user interest in the venue v incurs a high effect on the total
proﬁt. In other words, the total proﬁt should be higher in the
case of high user rating than that of low rating. The user ui
rating to venue v is incorporated in the equation, as follows:

j
ui



Pui ,v = Rui ,v × (Nui ,v + Sui ,v − Cui ,v )

provided by venue V , recommend a set of users that maximize
the total proﬁt. In this case, the Budget/Coupon price input
parameter represents the whole budget. (b) Coupon-based
Proﬁt Maximization: Given a maximum marketing budget per
user (e.g., coupon cost) provided by venue V , recommend K
users that maximize the expected total proﬁt. In such case, the
Budget/Coupon price input parameter represents the coupon
price. Moreover, the Number of customers parameter must be
speciﬁed in the input. In the rest of the paper, we will describe
each algorithm in details.
IV.

As Rui ,v takes a value from 0 to 1, it acts as a damping factor
for the calculated proﬁt. In other words, if Rui ,v is equal to
zero that means the user ui does not like the venue v, and hence
the proﬁt that user ui brings to venue v is equal to zero. On
the hand, in case Rui ,v is equal to one which means the user
ui likes the venue v, which affects the total proﬁt positively.
If a user u did not visit (rate) venue v before, P LUTUS thus
predicts the value of Rui ,v based on the history of available
user opinions.

P ROFIT C ALCULATION

To this end, we build an item-based collaborative ﬁltering
model by analyzing the entire user/venue rating space, and
using statistical techniques to ﬁnd correlated venues. There
are several methods to perform collaborative ﬁltering including
item-based [11], user-based [12], regression-based [11], or approaches that use more sophisticated probabilistic models (e.g.,
Bayesian Networks [13]). These correlations are measured by
a score, or weight, that deﬁnes the strength of the relation.
The item-item model builds, for each of the m venues V in
the database, a list L of similar venues. Given two venues
vp and vq , we can derive their similarity score sim(vp , vq )
by representing each as a vector in the user-rating space, and
then use a similarity function over the two vectors to compute
a numeric value representing the strength of their relationship.

The proﬁt calculation algorithm estimates the proﬁt of
recommending customer ui to venue v. ∀ ui ∈ U , ∃ P ui ,v
(P ui ,v ≥ 0) such that P ui ,v represents the total proﬁt user ui
brings to venue v:
Pui ,v = Nui ,v + Sui ,v − Cui ,v

(1)

Both static proﬁt Sui ,v and Cost Cui ,v has been explained
before in section III. Nui ,v represents the network proﬁt which
is formally deﬁned as follows:
Deﬁnition 2. The Network Proﬁt Nui ,v ≥ 0 is deﬁned as
the expected network proﬁt gained when user ui visits the
venue v. It is dynamic in the sense that it depends on the
social ties of user ui . Nui ,v is dependent on how ui inﬂuences
his/her friends, and how that inﬂuence spreads through the
entire social graph. The network proﬁt that venue v gains by
acquiring user ui is calculated as follows:

Nui ,v =

f ui

j=0

PF j

ui ,v

× Iui ,F j

ui

(3)

The similarity function, sim(vp , vq ), computes the similarity of vectors vp and vq using only their co-rated dimensions.
Finally, we store vp , vq , and sim(ip , iq ) in the model. The
similarity measure need not be symmetric, i.e., it is possible
that sim(ip , iq ) 
= sim(vq , vp ). Many similarity measures
have been proposed in the literature [14], [11], among which
we have selected Pearson Correlation similarity measure as
it is one of the most widely used in literature. it measures
the similarity between venues using their Pearson correlation
coefﬁcient as follows:

(ru,vp − rvp )(ru,vq − rvq )
sim(vp , vq ) = u∈Uc
(4)
σ vp σ vq

(2)

Such that fui represents the total number of friends of user ui .
F jui denotes the j th friend of user ui and P F j ,v represents
ui
the total proﬁt user F jui brings to venue v.
Algorithm 1 gives the pseudocode of the network proﬁt
calculation algorithm that takes a customer ui and a venue v as
input, and returns the total expected network proﬁt that venue
v gains by acquiring customer ui . The network proﬁt Nui ,v is
calculated by running a breadth ﬁrst search traversal over the
social graph starting from user (node) ui (lines 2 to 5). For
j
j
each friend Fui
of user ui , we calculate the total proﬁt of Fui
by invoking the CalculateTotalProﬁt algorithm (explained
later in section IV-C) and then multiplying it by Iui ,F j ; the
ui
j
. The algorithm goes
inﬂuence of user ui on his friend Fui
for all social graph descendants of ui , recursively calculate
the network proﬁt of each descendant, and adds its share
in the network proﬁt to the ﬁnal network proﬁt of user ui ;
N etworkP rof it.

Uc represents users who co-rated items vp and vq , ru,vp and
ru,vq represent a user’s ratings, and rvp and rvq represent the
average rating for venues vp and vq , respectively. σvp and σvq
are the standard deviations for vp and vq
We then use the model to predict ratings for venues that
a user ua has not rated. Rating predictions are produced
by performing aggregation over the generated collaborative
ﬁltering model. Using the item-based collaborative ﬁltering
technique, we compute the predicted rating Rui ,v for venue v
and user ui as a weighted sum [11]:

sim(v, l) ∗ rui ,l

(5)
Rui ,v = l∈L
l∈L sim(v, l)

29

The prediction is the sum of the user’s rating for a related
venue l, rui ,l , weighted by the similarity to the candidate item
v. The prediction is normalized by the sum of scores between
v and l.

Algorithm 2 Plutus Proﬁt Calculation
Function CalculateTotalProﬁt(Customer ui , Venue v)
CurrentN ode ← Retrieve the graph node that represents ui
Sui ,v ← GetStaticProﬁt(ui ,v)
Cui ,v ← GetStaticCost(ui ,v)
Rui ,v ← GetRating(ui ,v) /* using equation 5 */
Tui ,v ← GetTravelPenaly(ui ,v)
if CurrentN ode.isVisited equals true then
T otalP rof it ← 0.0
else
CurrentN ode.isVisited ← true
T otalP rof it ← Tui ,v ×Rui ,v ×(Sui ,v −Cui ,v +
CalculateNetworkProﬁt(ui ,v))
12: end if
13: return T otalP rof it

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

B. Proﬁt Calculation with Travel Penalty
The idea is to exploit the observation that users limit their
choice of spatial venues based on travel distance (mentioned
in section II). Travel penalty is a value that penalizes the total
proﬁt of a user based on her distance from the designated
venue. Travel penalty may incur expensive computational
overhead by calculating travel distance to each user. Thus,
we employ an efﬁcient query processing technique capable
of early termination to the total proﬁt without calculating the
travel distance to all users. The total proﬁt, after incorporating
the spatial effect, is expressed as follows:


Pui ,v = Tui ,v × Rui ,v × (Nui ,v + Sui ,v − Cui ,v )

and (2) Coupon-based proﬁt maximization (section V-B). In
the rest of this section, we explain both versions in details.
A. Celebrity-based Proﬁt Maximization

(6)

Tui ,v is the travel penalty applied due to the euclidean distance
between user ui and venue v. Notive that Tui ,v takes a value
between zero and one. When (Tui ,v = 0), this means that the
distance between ui and v is too high, and (Tui ,v = 1) means
that ui is too close to v. To add the geo-spatial effect, we
multiply the travel penalty Tui ,v by the total proﬁt value in
equation 3. The ﬁnal expected total proﬁt that user ui brings
to venue v is given in equation 6.

The celebrity-based proﬁt maximization algorithm allows
the venue (e.g., restaurant) owners to specify a total budget
they are planning to spend on the marketing campaign. In
this case, P LUTUS retrieves a set of customers (i.e., users in
the social graph) that are expected to increase the proﬁt for
the designated venue. The celebrity-based proﬁt maximization
problem can be formulated as an integer linear program, as
follows:
n

max i=0 xi × Pui ,v
n
s.t.
i=0 xi × Cui ,v ≤ Bv
xi ∈ {0, 1}.

C. Proﬁt Calculation Algorithm
From equations 1 to 6, the total proﬁt in P LUTUS is
calculated as follows:


Pui ,v = Tui ,v × Rui ,v × (

f ui

j=0

PF j

ui ,v

Assume the social graph described in section III such that
n is the
total number of nodes (i.e., customers) in the graph.

Let Pui ,v denote the total proﬁt that results from incorporating
customer ui in the result set of venue v (calculated using
Algorithm 2) and let B v represent the total budget speciﬁed
by venue v owner. Let xi be set to 1 if user ui is selected and
reset to 0 otherwise. We need to maximize the total proﬁt, such
that the total cost of all selected customers does not exceed
the total budget B v .

× Iui ,F j + Sui ,v − Cui ,v )
ui

Algorithm 2 provides the pseudocode for the total proﬁt
calculation algorithm. The algorithm takes a customer ui and
a venue v as input, and returns the total expected proﬁt that
venue v gains by acquiring customer ui . First, the algorithm
retrieves the user static proﬁt Sui ,v and Static cost Cui ,v
necessary for user ui to visit venue v. Notice that both Sui ,v
and Cui ,v are saved for each user in the social graph (see
section III-A). The algorithm then retrieves the rating score
that user ui gave to venue v. If user ui did not rate venue
v before, then equation 5 is applied to predict the value of
Rui ,v . The algorithm also calculates the travel penalty Tui ,v
based on the distance of ui from v. Then, we calculate the
total proﬁt by subtracting Cui ,v from Sui ,v and then adding the
network proﬁt calculated using the CalculateNetworkProﬁt
algorithm. Finally, the returned value is multiplied by both
Tui ,v and Rui ,v to incorporate both the spatial effect and the
user opinions effect in the proﬁt calculation. The
ﬁnal returned

value T otalP rof it is the expected proﬁt Pui ,v that user ui
brings to venue v.
V.

The celebrity-based proﬁt maximization problem is NPComplete; 0/1 knapsack problem can be reduced to our problem (proof omitted for brevity). The problem is even
harder

than the 0/1 knapsack problem; as the value Pui ,v is not
constant. The order in which
we pick customers leads to

different proﬁt values, Pui ,v . Moreover, the problem at hand
may be so hard that it is likely that a c-approximation solution
does not exist. Hence, we resort to a simple strategy such as
greedy, which at least has the desirable property that it is fast
to execute and intuitive to understand.
Algorithm. The greedy hill climbing algorithm at each
iteration ﬁrst computes a set of m candidate Customers A
⊆ U , such that the cost of adding each customer ai , Cai ,v ,
to the result set does not exceed the total venue budget Bv .
Therefore, we will pick the customer that leads to the highest
proﬁt value Pui ,v , exclude it from being picked in the future,
and update the capacity usage of the total venue budget Bv .
The algorithm terminates when the remaining budget in the
total venue budget Bv is less than or equal zero or there is
no customers in the candidate set A. The pseudocode for the
celebrity-based hill climbing algorithm is omitted for brevity.

P ROFIT M AXIMIZATION

The proﬁt maximization algorithm retrieves the set of
customers that are expected to maximize the total venue proﬁt.
P LUTUS supports two versions of the proﬁt maximization algorithm: (1) Celebrity-based proﬁt maximization (section V-A),

30

Limited Customers. An extension of the celebrity-based
proﬁt maximization problem is to limit the number of customers to allow the framework users to specify a total budget
and a ﬁxed number of required customers K. In this case,
P LUTUS retrieves a set of K customers (i.e., users in the social
graph) that are expected to increase the venue proﬁt. In such
case, the problem can be formulated similarly to the celebritybased proﬁt maximization problem, with an extra constraint as
follows:
n

max i=0 xi × Pui ,v
n
s.t. i=0 xi × Cui ,v ≤ Bv
n
i=0 xi <= K
xi ∈ {0, 1}.

Algorithm 3 Coupon-based Proﬁt Maximization
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:

The additional constraint is necessary so that we ensure that
only K Customers are returned in the result set. The algorithm
to solve that problem is similar to the celebrity-based hill
climbing algorithm, except that the termination condition is
extended to incorporate the the number of customers limit.
B. Coupon-based Proﬁt Maximization
The coupon-based proﬁt maximization version allows
venues owners to specify a set of K coupons and a ﬁxed price
Bv for each coupon. The framework will then fetch the venue
a set of customers (i.e., users in the social graph) that are
expected to increase the proﬁt. The Coupon scheme can be
formulated as follows:
n

max
i=0 xi × Pui ,v
s.t. 
xi × Cui ,v ≤ Bv
n
i=0 xi <= K
xi ∈ {0, 1}.

Function Geo Coupons(v, L, Bv , K)
/* Populate a list R with a set of K customers*/
R←φ
while R size ≤ K do
ui ← Retrieve the customer with the next lowest travel penalty
/*Customer Cost must be ≤ Coupon Price */
if Cui ,v ≤ Bv then

Compute Pu ,v by Equation 3
i



Insert ui into R ordered by Pu ,v
i
end if
end while
LowestP rof it ← P rof it of the kth object in R
/*Get customers one by one in order of their penalty value */
while there are more customers to process do
ui ← Retrieve the next customer in order of penalty score

M axP ossibleP rof it ← PM AX × Tui ,v
if M axP ossibleP rof it ≤ LowestP rof it then
return R /* early termination - end query processing */
end
if


Pu ,v ← Pu ,v × Tui ,v /* Equation 6 */
i



if Pu

i ,v

i

> LowestP rof it then


Insert i into R ordered by Pu ,v
i
LowestP rof it ← P rof it of the kth object in R
end if
end while
return R

to maximize the total proﬁt. The algorithm starts by running
a k-nearest-neighbor algorithm to populate the list R with k
customers with lowest travel penalty while the cost to acquire
each of these customer Cui ,v is less than or equal to the
coupon price
Bv . The list R is sorted by the the total computed

proﬁt Pui ,v using Equation 3. This initial part is concluded by
setting the lowest proﬁt value (LowestProﬁt) as the Proﬁt of
the k th customer in R (Lines 3 to 12). Then, the algorithm
starts to retrieve customers one by one in the order of their
travel penalty score. This can be done using an incremental knearest-neighbor algorithm. For each customer ui , we calculate
the maximum possible proﬁt that i can have by multiplying the
travel penalty of ui Tui ,v by PM AX , the maximum possible
proﬁt value in the system. If ui cannot make it into the list of
top-k customers with this maximum possible proﬁt value, we
immediately terminate the algorithm by returning R as the topk customers without computing the proﬁt (and travel distance)
for more customers in the social graph (Lines 17 to 19). The
rationale here is that since we are retrieving customers in
increasing order of their penalty and calculating the maximum
proﬁt that any remaining customer can have, then there is no
way that any unprocessed customer can beat the lowest proﬁt
value in R. If the early termination case does not arise, the
algorithm continues to compute the proﬁt for each customer
ui using Equation 3, insert ui into R sorted by its proﬁt value
(removing the k th customer if necessary), and adjust the lowest
proﬁt value accordingly (Lines 20 to 24).

Example. A restaurant owner is willing to distribute 10
coupons, each worth a 100 dollars value of meals purchases.
Hence, the restaurant owner speciﬁes K=10 and Bv =100.
P LUTUS therefore selects a set of m (i.e., m ≤ K) users that
are expected to maximize the total restaurant proﬁt.
Similar to the celebrity-based hill proﬁt maximization,
we use a greedy strategy to select m customers, such that
m ≤ K. The algorithm follows the same steps as in the
celebrity scheme algorithm, except that at each iteration it
picks
a customer ai that leads to the highest proﬁt value

Pui ,v for venue v whereas the cost Cai ,v does not exceed the
coupon price Bv . The algorithm terminates when the remaining
number of coupons reaches zero, which means all coupons are
taken.
Algorithm. In the algorithm, we
P LUTUS Coupon-based

aim to avoid calculating Pui ,v in Equation 6 for all customers
in the social graph to ﬁnd the k customers to offer them
coupons, which can become quite expensive given the need
to compute the network proﬁt Nui ,v for each customer. To
avoid such computation, we evaluate customers in monotonically increasing order of travel penalty (mentioned earlier in
section IV-B), enabling us to use early termination principles
from top-k query processing [15], [16], [17].

To calculate an exact travel penalty for a customer ui to
venue v, we employ an incremental k-nearest-neighbor (KNN)
technique [18], [19]. Given a venue v location Lv , incremental
KNN algorithms return, on each invocation, the next customer
ui nearest to v with regard to travel distance d. In our case,
we normalize distance d to take a value from zero and one to
get the travel penalty Tui ,v in Equation 6. Incremental KNN
techniques exist for both Euclidean distance [18] and (road)
network distance [19]. The advantage of using Incremental
KNN techniques is that they provide an exact travel distance

Algorithm 3 provides the pseudocode of the P LUTUS
coupon-based proﬁt maximization algorithm that takes a venue
v, a coupon price Bv , and coupons count K as input, and
returns the list R of m (m ≤ K) customers that are expected

31

according to the activity of the user (total number of checkins
in the system). The intuition is that a user with higher activity
is likely to visit to the designated venue that another user with
lower activity. An edge is added to the social graph between
two friends or between a brand and a follower. For each pair
of friends (ui , ul ), we add a pair of directed edges, (ui , ul )
and (ul , ui ). The inﬂuence of a user ui on his friend ul is
calculated by counting the number of checkins by user ui to
any venue vj followed by a checkin of ul to the same venue
vj . We assume here that the ui visited this place ﬁrst and tells
ul about this place who was convinced (inﬂuenced) to visit the
same place later.
Fig. 4.

In the experiments, we measure quality, processing time
and expected proﬁt for the following algorithms:

Foursquare mobile application

value between a venue’s location and each customer in the
social graph. The runtime complexity of retrieving a single
customer using incremental KNN in Euclidean space is [18]:
O(k+logN ), where N and k are the number of total customers
and customers retrieved so far, respectively.

•
•
•

VI.

E XPERIMENTAL E VALUATION

This section provides experimental evaluation of P LUTUS
based on an actual implementation using JAVA 6.

•

Data Set. All of our experiments are based on a real
dataset obtained from Foursquare. Foursquare [20] is a mobile
location-based social network application (see ﬁgure 4 4 ). We
use the publicly available APIs provided by Foursquare to run
a crawler that collected results for 4,392 users and 36,963
venues. Users are associated with a home city, and alert friends
when visiting a venue (e.g., restaurant) by “checking-in” on
their mobile phones. During a “check-in”, users can also leave
“tips”, which are free text notes describing what they liked
about the venue. Any other user can add the “tip” to her “todo list” if interested in visiting the venue. Once a user visits a
venue in the “to-do list” , she marks it as “done”. Also, users
who check into a venue the most are considered the “mayor”
of that venue.

All experiments were performed on an Intel Core2 8400 at
3Ghz with 4GB of RAM machine running Ubuntu Linux 10.04
operating system.
Quality Metric. To measure the quality of our algorithm,
we choose a venue and retrieve all checkins on this venue.
Then, we ﬁnd a time point such that 80% of the checkins are
before this time (training set) and only 20% are beyond this
time (test set). We build the social graph around this venue
using the training set. We then pass this social graph to the
algorithm and let it choose the best users to attract to the venue.
We increment our quality measure by one for each chosen user
who actually visited the venue (according to the test set). We
further check the friends of each of those users who actually
visited the venue, we increment the quality measure by one
for each friend who visited the venue after user visit. In other
words, we start a breadth ﬁrst search from the chosen users
who visited the venue. For each user in the search, we only
traverse his friends who went to the same venue after her visit.
The total number of users traversed in this search is the quality
measure we use.

Extracting user opinions. Foursquare does not give the
functionality to a user to give an explicit rating for a place. To
extract user opinions for different venues from the Foursquare
data, we map each user visit to a single rating. The user and
venue attributes are represented by the unique Foursquare user
and venue identiﬁer, respectively. We use a numeric rating
value range of [1, 4], translated as follows: (a) 4 represents
the user is the “mayor” of the venue, (b) 3 represents that the
user left a “tip” at the venue, and (c) 2 represents the user
marked a tip left in the venue as “done”. (d) 1 represents the
user only visited (check-in) the venue.

A. Plutus-Celeb versus Celeb
In this section, we compare the quality, gained proﬁt, and
processing of both the Celeb and Plutus-Celeb algorithms.

Building the social graph. Using Foursquare data, we
build a social graph of these users and annotate it with
inﬂuence, cost and proﬁt to allow our algorithm to work. In
the generated social graph, each user is represented by a node.
Location of each node is assigned to the location of the home
city of the associated user. A static cost is assigned to each
user according to the total number of friends of this user and
her distance to the chosen venue. The cost increases with the
user popularity (number of friends). Proﬁt is assigned to a user
4 The

Celeb: a basic celebrity-based algorithm that selects
the most popular users in the social graph that covers
the budget.
Plutus-Celeb: represents the celebrity-based proﬁt
maximization algorithm presented in section V-A.
Coup: a basic coupon-based algorithm that selects the
top-k most popular users in the social graph and assign
a coupon to each of them.
Plutus-Coup: represents the coupon-based proﬁt maximization algorithm presented in section V-B (Algorithm 3).

1) Effect of budget on Quality: Figure 5 shows the quality
measure for both Celeb and Plutus-Celeb. We measure the
quality while varying the budget to take the values $5000,
$10000, $20000 , $50000, and $100000. As it turns out from
the ﬁgure, Plutus-Celeb consistently achieves higher quality
than Celeb. That is explained by the fact that Plutus-Celeb
takes into account the social ties, the spatial distance, and the
user ratings as opposed to Celeb. More speciﬁcally, the gap
between Plutus-Celeb and Celeb is higher for smaller budget

screenshot in Figure 4 taken from: http://foursquare.tumblr.com/

32



























	

	

	

	












	

	

	

Fig. 7.

Effect of budget on quality


	
















	
























Fig. 6.

	

Effect of budget on time




Fig. 5.

	








	










Effect of budget on proﬁt

Fig. 8.

values. These results manifest that Plutus-Celeb is efﬁcient in
spending the budget even with tight budget values.

Effect of coupon price on quality

for coupon price values $1, $2, $3, $4, $5, and $10. As
shown in Figure 8, increasing the coupon price while keeping
total number of coupons ﬁxed, increases total quality of the
algorithm for both approaches. The intuition behind this is that
as the coupon price increases, both algorithms has a wider
choice when selecting people to give coupons and hence there
is a high possibility that both algorithms to return high quality
answer.

2) Effect of budget on proﬁt: Figure 6 compares the processing time for running both Celeb and Plutus-Celeb. The
processing time is measured in terms of CPU time taken by
each algorithm to return the set of recommended customers.
We measure the processing for budget values $5000, $10000,
$20000 , $50000, and $100000. As it turns out from ﬁgure 7,
Plutus-Celeb takes more time to run thatn celeb for all budget
values. That behavior is explained by the fact that Celeb incurs
no processing overhead as it uniformly selects customers that
match the budget. On the other hand, Plutus-Celeb needs to
calculate the total proﬁt Pui ,v of all users while running the
hill climbing algorithm. However, Plutus-Celeb takes order of
seconds to run which is acceptable especially that the algorithm
is expected to be performed ofﬂine.

However, Plutus-Coup shows consistently higher quality
than Coup for all coupon price values. This phenomenon
is explained by the fact that Plutus-Coup efﬁciently assigns
coupons to users based on their social, spatial, and their interest
in the designated venue. For coupon price value , the quality
of Coup jumped closer to Plutus-Coup as in this case many
users in the system including low cost users accept higher value
coupons. That increased the possibility that Coup achieves a
high quality.

3) Effect of budget on processing time: Figure 7 measures
the total expected proﬁt gained using both Celeb and PlutusCeleb. We measure the total proﬁt in dollars for the whole
budget values $5000, $10000, $20000 , $50000, and $100000.
As presented in the ﬁgure, Plutus-Celeb incurs 40× to 180×
higher estimated proﬁt than Celeb for all budget values. That
behavior is explained by the fact that Celeb might select users
that are not relevant to the designated venue. On the other hand,
Plutus-Celeb recommends those user that are socially, spatially
relevant to the designated venue while taking the users interest
in that venue into account.

Figure 11 presents the quality of both algorithms while
setting the number of coupons (K) to 1, 5, 10, 50, and 100.
The ﬁgure shows that for small number of coupons PlutusCoup achieves higher quality than Coup because Plutus-Coup
efﬁciently distributes the small number of coupons on the user
with the highest potential. For larger number of coupons, Coup
quality comes close to Plutus-Coup. This happens because
with higher number of coupons, the possibility for Coup to
get high potential users increases.
2) Effect of coupon price / number of coupons on proﬁt:
Figure 9 calculates the total estimated proﬁt for both PlutusCoup and Coup algorithms. We measure the estimated proﬁt
for coupon price values $1, $2, $3, $4, $5, and $10. As it turns
out from the ﬁgure, Plutus-Coup achieves up to 9× higher
estimated proﬁt than Coup. That happened due to the fact
that Plutus-Coup picked the most relevant customers to the
designated venue.

B. Plutus-Coup versus Coup
In this section, we compare the quality, gained proﬁt, and
processing of both the Coup and Plutus-Coup algorithms.
1) Effect of coupon price / number of coupons on quality:
In ﬁgure 8, we measure the quality of both the Plutus-Coup and
Coup algorithms. The quality of each algorithm is measured

33











































	












	





Fig. 11.


 !






























Fig. 10.



Effect of number of coupons on quality








	








	



Effect of coupon price on proﬁt



Fig. 9.



	



	





Effect of coupon price on processing time

Fig. 12.

Figure 12 measures the proﬁt for both algorithms while
setting the number of coupons (K) to 1, 5, 10, 50, and 100.
As given in the ﬁgure, Plutus-Coup consistently outperforms
Coup for the estimated proﬁt as Plutus-Coup is expected to
recommend customers with higher value than that of Coup.

Effect of number of coupons on proﬁt

Viral Marketing [3], [4], [27], [25], [26] use social
networks as a word of mouth tool that helps achieving
marketing objectives through self-replicating viral processes.
For instance, a publisher in the phase of new book release,
might pre-select a set of Twitter users and give them free
copies of the book. These Twitter users, if well selected,
might substantially help in advertising that new book, by word
of mouth, and hence increasing the book sales. Domingos
in [27] discusses the importance of viral marketing in nonspatial products (e.g., movies, books) marketing. He argued
that using traditional direct marketing may lead to suboptimal
marketing decisions. Domingos proposed the idea that online
social networks can be leveraged to calculate the network
value of customers, and hence making more precise marketing
decisions. Richardson and Domingos in [3] views the market
as a social network instead of independent customers. They
mined a recommender system database to retrieve the social
inﬂuence among users. The authors in [25], [26] also leverages
the social inﬂuence to increase proﬁt and maximize a product
adoption in online social networks. P LUTUS extends the viral
marketing techniques to incorporate both the spatial and users
opinions effects.

3) Effect of coupon price / number of coupons on processing time: Figure 10 measures the total processing time of both
Plutus-Coup and Coup algorithms while setting the coupon
price values to $1, $2, $3, $4, $5, and $10 . As presented in
the ﬁgure, Plutus-Coup incurs more processing overhead than
Coup. That is explained by the fact that Plutus-Coup needs
to calculate the total proﬁt calculation for each distantly close
user which leads to more processing time. However, PlutusCoup still terminates only in order of seconds as the algorithm
employs an early termination strategy using the travel penalty.
Figure 13 compares both processing time of both PlutusCoup and Coup algorithms. As it turns out from the ﬁgure,
increasing the number of coupons (K) leads an increase in
the total processing time of Plutus-Coup. This is explained by
the fact that Plutus-Coup needs to calculate the total proﬁt for
more users in the social graph for higher value of K.
VII.



R ELATED W ORK

Location-based Social Networks [28], [29], [30], [31]
have been exploited to study the spatial behavior of users and
how that behavior is associated with the users social ties [7],
[32]. Cho, Myers and Leskovec in [32] have applied extensive
data analysis on two data sets from real location-based social
networking websites (i.e., Gowalla and BrightKite). They also
derived a user mobility model taking into account three submodels: (a) Model for spatial locations that a user regularly
visits, (b) A model for temporal movement between these
locations, and (c) A model for movement that is inﬂuenced
by the social network ties. The authors in [31] proposed a
novel method to recommend items to users in location-based
social networks. They proposed a taxonomy of location-based

Informational social inﬂuence [2], [21], [22], [23], [24]
is the psychological phenomenon that describes the positive
inﬂuence created when someone ﬁnds out that others are doing
something, also known as Social Proof. Kempe, Kleinberg
and Tardos in [2] build the theoretical foundation behind the
optimization problem of selecting the most inuential nodes in
a social network. Their work aims at maximizing the spread
of inﬂuence in the social graph. They have shown the problem
to be NP-hard, but can be approximated within 63% of the
optimal using a a natural greedy search strategy. Zhang et. al
studies the geo-social inﬂuence of spatial events in locationbased social networks [24].

34






 
 !
!
	









[7]

[8]
[9]







[10]





Fig. 13.

[11]

Effect of number of coupons on processing time

[12]

ratings that aims at incorporating the geo-location of venues
and users in building the recommendation model, resulting
into more relevant recommendation results. P LUTUS leverages
data generated from existing location-based social networking
systems to retrieve the set of users that are expected to
maximize the total venue proﬁt.

[13]
[14]
[15]

Recommender Systems. A Recommender systems speculates
how much a user would like an item she has never seen
(bought, watched, ...) before. Collaborative Filtering [12] is
considered the most popular technique among several recommendation techniques proposed in the literature [12], [9],
[33], [34]. There are several methods to perform collaborative
ﬁltering including item-item [11], user-user [12], regressionbased [11], or approaches that use more sophisticated probabilistic models (e.g., Bayesian Networks [13]). Collaborative
ﬁltering techniques analyze past community opinions to ﬁnd
correlations of similar users (or items) to suggest k personalized items (e.g., movies) to a querying user u. Community
opinions are usually expressed through explicit ratings represented by the triple (user, item, rating) that represents a
user providing a numeric rating for an item. P LUTUS employs
the item-based collaborative ﬁltering technique to predict the
rating that a user would give to an unseen item.
VIII.

[16]

[17] R. Fagin, A. Lotem, and M. Naor, “Optimal Aggregation Algorithms
for Middleware,” in PODS, 2001.
[18] G. R. Hjaltason and H. Samet, “Distance Browsing in Spatial
Databases,” TODS, vol. 24, no. 2, pp. 265–318, 1999.
[19] K. Mouratidis, M. L. Yiu, D. Papadias, and N. Mamoulis, “Continuous
nearest neighbor monitoring in road networks,” in VLDB, 2006.
[20] “Foursquare: http://foursquare.com.”
[21] M. Deutsch and H. B. Gerard, “A study of normative and informational
social inﬂuences upon individual judgment.,” The Journal of Abnormal
and Social Psychology, vol. 51, no. 3, pp. 629–636, 1955.
[22] H. C. Kelman, “Compliance, identiﬁcation, and internalization: Three
processes of attitude change,” Journal of Conﬂict Resolution, vol. 2,
no. 1, pp. 51–60, 1958.
[23] R. B. Cialdini, Inﬂuence: Science and Practice, vol. 3rd. Allyn and
Bacon, 2001.
[24] C. Zhang, L. Shou, K. Chen, G. Chen, and Y. Bei, “Evaluating geosocial inﬂuence in location-based social networks,” in CIKM, 2012.
[25] W. Lu and L. V. S. Lakshmanan, “Proﬁt maximization over social
networks,” in ICDM, 2012.
[26] S. Bhagat, A. Goyal, and L. V. S. Lakshmanan, “Maximizing product
adoption in social networks,” in WSDM, 2012.
[27] P. Domingos, “Mining social networks for viral marketing,” IEEE
Intelligent Systems, vol. 20, no. 1, pp. 80–82, 2005.
[28] M. Sarwat, J. Bao, A. Eldawy, J. J. Levandoski, A. Magdy, and M. F.
Mokbel, “Sindbad: A Location-based Social Networking System,” in
SIGMOD, 2012.
[29] M. Sarwat, J. Bao, A. Eldawy, J. J. Levandoski, A. Magdy, and
M. F. Mokbel, “The Anatomy of Sindbad: a Location-Aware Social
Networking System,” in Proceedings of the 5th International Workshop
on Location-Based Social Networks, 2012.
[30] J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel, “LARS: A
Location-Aware Recommender System,” in ICDE, 2012.
[31] M. Sarwat, J. J. Levandoski, A. Eldawy, and M. F. Mokbel, “LARS*:
A Scalable and Efﬁcient Location-Aware Recommender System,” in
TKDE, 2013.
[32] E. Cho, S. A. Myers, and J. Leskovec, “Friendship and mobility: user
movement in location-based social networks,” in KDD, 2011.
[33] M. D. Ekstrand, M. Ludwig, J. A. Konstan, and J. T. Riedl, “Rethinking
the recommender research ecosystem: reproducibility, openness, and
lenskit,” in RecSys, 2011.
[34] J. J. Levandoski, M. Sarwat, M. D. Ekstrand, and M. F. Mokbel,
“RecStore: An Extensible and Adaptive Framework for Online Recommender Queries inside the Database Engine,” in EDBT, 2012.

C ONCLUSION AND F UTURE W ORK

In this paper, we presented P LUTUS, a marketing framework that aims at recommending customers to grow the
business of a particular venue. To this end, P LUTUS leverages
social data, spatial data, and user opinions data provided by
location-based social networks, and uses them in concert to
retrieve those customers that are expected to maximize the
total venue proﬁt. We tested the performance of P LUTUS using
real data from Frousquare location-based social network. In the
future, we plan to deploy our framework in a real life setup to
test P LUTUS with real venues (e.g., restaurants).
R EFERENCES
[1]

[2]
[3]
[4]
[5]
[6]

J. Lindqvist, J. Cranshaw, J. Wiese, J. Hong, and J. Zimmerman, “I’m
the Mayor of My House: Examining Why People Use Foursquare - A
Social-Driven Location Sharing Application,” in CHI, 2011.
M. Ye, P. Yin, and W.-C. Lee, “Location Recommendation for Locationbased Social Networks,” in GIS, 2010.
G. Adomavicius and A. Tuzhilin, “Toward the Next Generation of
Recommender Systems: A Survey of the State-of-the-Art and Possible
Extensions,” TKDE, vol. 17, no. 6, 2005.
J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl, “Evaluating Collaborative Filtering Recommender Systems,” TOIS, vol. 22,
no. 1, 2004.
B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, “Item-Based Collaborative Filtering Recommendation Algorithms,” in WWW, 2001.
P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl, “GroupLens: An Open Architecture for Collaborative Filtering of Netnews,”
in CSWC, 1994.
J. S. Breese, D. Heckerman, and C. Kadie, “Epirical Analysis of
Predictive Algorithms for Collaborative Filtering,” in UAI, 1998.
G. Karypis, “Evaluation of Item-Based Top-N Recommendation Algorithms,” in CIKM, 2001.
M. J. Carey et al, “On saying ”Enough Already!” in SQL,” in SIGMOD, 1997.
S. Chaudhuri et al, “Evaluating Top-K Selection Queries,” in VLDB,
1999.

T. Y. Chan, C. Wu, and Y. Xie, “Measuring the lifetime value of
customers acquired from google search advertising,” Marketing Science,
vol. 30, pp. 837–850, Sept. 2011.
D. Kempe, J. M. Kleinberg, and . Tardos, “Maximizing the spread of
inﬂuence through a social network,” in KDD, 2003.
M. Richardson and P. Domingos, “Mining Knowledge-Sharing Sites for
Viral Marketing,” in KDD, 2002.
P. Domingos and M. Richardson, “Mining the network value of customers,” in KDD, 2002.
“About Foursquare: https://foursquare.com/about/.”
A. Kittur, E. H. Chi, and B. Suh, “Crowdsourcing User Studies with
Mechanical Turk,” in CHI, 2008.

35

ACM SIGSPATIAL GIS Cup 2013
Geo-fencing
Siva Ravada1

Mohamed Ali2

Jie Bao3

Mohamed Sarwat4

Oracle, One Oracle Drive, Nashua, NH 03062, USA
Microsoft, One Microsoft Way, Redmond, WA, USA
4
Department of Computer Science & Engineering, University of Minnesota, Minneapolis, USA
1

2

3
1

siva.ravada@oracle.com, 2 mali@microsoft.com, {3 baojie,4 sarwat}@cs.umn.edu,

ABSTRACT
st

The 21 ACM SIGSPATIAL Conference on Advances in Geographic Information Systems (GIS) was held in November of 2013
in Orlando, Florida. Following the success of last year’s event, we
organized the second programming contest associated with the conference, called the SIGSPATIAL GIS Cup 2013. The subject of the
competition was Geo-fencing, which identifies the qualified point
and area pairs using a virtual perimeter for a real-world geographic
area. We describe the contest details, and the results, as well as the
lessons learned during the process.

Categories and Subject Descriptors
H.2.8 [Database Applications]: Spatial databases and GIS; D.2.8
[Metrics]:

General Terms
Algorithms, Performance, Theory

Keywords

authors of this paper represent the organizing committee members
of this year’s competition. This paper discusses the SIGSPATIAL
GIS Cup 2013 [3], serving as a record of the contest rules, data,
winners, and the lessons we learned during the process.
In term of the contest topic this year, we chose Geo-fencing,
which identifies the qualified point and area pairs using a virtual
perimeter for a real-world geographic area. Geo-fencing technique
is widely used in many location-based services, e.g., location-based
advertisements (which send the targeted ads to the users when they
are close to the shopping mall) and child location services (which
notify parents when a child leaves a designated area).
The problem to compute the qualified points and polygon pairs
based on some spatial predicates seems easy at first glance. However, with a large number of points, the system efficiency becomes a
vital issue. On the other hand, with the arbitrary shapes of the polygons (especially for the case of polygons with inner rings), guaranteeing the correctness of the algorithm output is a non-trivial task.
Geo-fencing is still an open research field, e.g.,[8, 9], without any
dominating solutions.

2.

location-based services, spatial query processing, Geo-fencing.

1. INTRODUCTION
ACM SIGSPATIAL [1] addresses issues related to the acquisition, management, and processing of spatially-related information
with a focus on algorithmic, geometric, and visual considerations.
The scope includes, but is not limited to, geographic information
systems (GIS). ACM SIGSPATIAL GIS is the annual conference
sponsored by SIGSPATIAL group. Along with the conference,
SIGSPATIAL GIS Cup, which is a algorithmic programming contest focusing on GIS related problem, is also held since 2012 (i.e.,
SIGSPATIAL GIS CUP 2012 [2]).
We continue to hold the competition as SIGSPATIAL GIS CUP
2013 [3]. Based on the suggestions we got in the last year, the competition was launched much earlier in February and ended in July,
which gives the competitors more time to work on the problem. The
Permission to make digital or hard copies of all or part of this work for personal or class room use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org
ACM SIGSPATIAL GIS’13 November 5-8, 2013. Orlando, Florida, USA
Copyright (c) 2013 ACM ISBN 978-1-4503-2521-9/13/11 ...$15.00.

574

SIMILAR COMPETITIONS

Netflix Prize. Netflix Prize [7] is one of the most known recent
competitions, which was focused on the area of recommendation
systems and provided a $1,000,000 prize to the winner. Competitors were provided with a training dataset containing over 100 million ratings from over 480 thousand users. They were also provided
a qualifying test set of over 2.8 million customer/movie id pairs
with the ratings withheld, divided into two disjoint subsets
KDD CUP. The KDD Cup [5] is a yearly competition in data mining that started in 1997. It targets researchers from both industry and academia, particularly students, interested in KDD. For the
most recent competition, i.e., KDD Cup 2013, the task was to determine the true relations between the research papers and the authors,
within a large amount of papers and the ambiguous author names.
SIGMOD Programming Contest. SIGMOD programming contest [10] is a competition to develop an efficient data management
systems, since 2009. Students take several months to work on
their implementation for a particular querying task, which is judged
base on the overall performance on a variety of workloads. For
the most recent competition, i.e., SIGMOD programming contest
2013, the topic focuses on the real-time keywords matching in massive tweets.
As a summary, SIGSPATIAL GIS CUP, similar to the above
competitions, is an open competition to the entire undergraduate/graduate
student over the world. However, in GIS Cup, the topic of the competition is focussed more on the GIS-related problems.

3. GEO-FENCING

distances calculated are not the actual distances on the globe,
but are sufficient for the purpose of the competition.

3.1 Background
Geo-fencing is a virtual perimeter for a real-world geographic
area. A geo-fence could be a radius around a store or point location,
or a predefined set of boundaries, like zip code boundaries. When
the participating user enters or exits a geo-fence, that user receives
a notification. As a consequence, Geo-fencing is used widely in
many location-based services, e.g., location-based advertisements
(which send the targeted ads to the users when they are close to
the shopping mall) and child location services (which notify parents when a child leaves a designated area). It is still an ongoing
research [8, 9], which involves a lot of relevant techniques in GIS
(e.g., spatial indexing and spatial query processing).
For the contest, each team is given access to a large amount of
point locations and spatial polygons along with some spatial predicates. The goal for each team is to find every (point, polygon) pair
in the dataset that satisfies the spatial predicate.

3.2 Problem Definition
In the contest, participants are given two sets of input files: 1) Points
file, which contains a series of point locations to simulate the user
movements, and 2) Polygon file, which can be in any shape (with
or without interior rings) that is made up of straight line segments.
Both of the points and polygons are associated with a sequence
number/timestamp. As a result, the objective of the program is to
match each of these points with one or more of the polygons from
the input based on one of the following spatial predicates:
INSIDE which evaluates to TRUE if a point is inside the polygon. Notice that the point can be associated with one or
more polygons (i.e., two overlapping polygons can contain
the same point). A point can be INSIDE any polygon with
a sequence number (i.e., timestamp) less than the sequence
number of the point where only the latest position of each
polygon (up to that sequence number) is considered.
WITHIN n (e.g., WITHIN a distance of 1000 units): which evaluates to true if the point is at less than 1000 units distance
from the polygon. Notice that each point may be associated
with one or more of the polygons, as the same point can be
within 1000 units distance from several polygons.
The input points are associated with an ID and the same point
might appear multiple times in the input. So these input points can
be thought of as moving points. Each instance of the input point
may appear zero or more times in the output. If a point is not associated with any polygon, it does not appear in the output. Different
instances of the input point may be associated with different polygons in the output. The polygons are also associated with a unique
ID. And the location of a polygon can change over time: think of
them as moving polygons. In the input sequence, the points should
always look at the latest position of the polygon to find the right
association. The shape of the polygon might change along with the
location.

3.3 Assumptions
1. Data is assumed to be in the spherical Mercator Projection
format 1 . That means, the calculations will be done in Cartesian space. We are limiting the scope to simple 2D Cartesian
based system to keep the distance calculations simple. The
1

2. Polygons are valid and are ordered CCW for outer-rings, and
CW for inner rings. Polygons are valid according to the rules
defined by OpenGIS Simple Features Specification for SQL,
Revision1.1 2 .
3. Number of polygons will vary for each test run. But the number will never be more than 500 polygons. Number of points
will vary for each test run. But the number will never be
more than 1 Million.
4. The distance of a point to a polygon is the distance of the
point to the closest point on the polygon. The resolution of
the distance is two digits after decimal.If the point is inside
or on the boundary of the polygon then the distance is zero.
5. There is no positional uncertainty in the data.

3.4
3.4.1

575

Input Files

Data input to the program is provided as two files. The first file
contains the input points and the second file contains polygons used
to describe the regions. Both the points and the polygons are in
GML 2.1.1 format:
Points Format: POINT:ID:Sequence:<gml Point>
Polygons Format: POLYGON:ID:Sequence:<gml Polygon>
As the training data, we provide two point files generated by the
Minnesota Traffic Generator [6] using Brinkoff traffic model [4]:
points500.txt and point1000.txt. The files contain a
set of 500 and 1000 points, respectively. Each point in the file
may have a different spatial location at different sequence number,
which essentially are the simulated vehicles moving on the road
networks in Minneapolis. On the other hand, we also provide two
polygon files: polys10.txt and polys15.txt, which contains a set of 10 or 15 distinct polygons respectively. Each polygon
in the file may have a different spatial locations and shapes at different sequence numbers.
The sequence numbers are generated so that each instance of the
data in a file gets a unique number. In the dataset, the sequence
numbers are the logical timestamps as recorded by a system. So
no two points has the same sequence number and no two polygons
have the same sequence number.

3.4.2

Output Files

The output of the program from the competitors should contain
the matching combinations of the point and polygon pairs with their
sequence numbers. An example output should be as follows:
Output Format Point ID:Sequence:Polygon ID:Sequence
One important thing to note here is the sequence number. When
considering the association of points and polygons, the sequence
number values should be taken into account. A point can never
be associated with a polygon that has a higher (or equal) sequence
number. When a polygon gets a new sequence number, the old
instance of that polygon ceases to exist.
To help the competitors to verify the correctness of the program,
we also provide the sample output files. For each point and polygon file combinations, we give the sample output for two spatial
2

http://docs.openlayers.org/library/spherical_mercator.html

Training Data Set

http://portal.opengeospatial.org/files/?artifact_id=829

predicates: “INSIDE” and “WITHIN_1000”. Each of the file contains the correct combinations of the point/polygon pairs with the
matching sequence numbers.

4. SUBMISSION & EVALUATION
4.1 Submission
The submission is done via the online conference management
tool 3 . Each team is asked to submit one single .zip file that contains
three main components:
1. An executable program named “Geofence.exe”, as well
as any additional libraries.
2. All the source files as well as any dependent files, which are
used to verify the originality of the submitted work.
3. A a readme.txt file, which includes any special instructions on how to compile the submitted code.
For a qualified executable file, i.e., “Geofence.exe”, it should
accept four command line parameters, as follows:
1. Spatial Predicates. Specifies the type of spatial predicates
is used in this test run, e.g., “INSIDE”|“WITHIN n”.
2. PointInputFilePath. Specifies the path to read the point data
file. Each point data file is a single test case and contains a
lot of point locations.
3. PolygonInputFilePath. Specifies the path to read the polygon file. Each polygon data file is a single test case and contains a series of polygons.

In plain English, the final grade is weighted by the total execution time of the program. Also, we have a new criterion for this
year’s competition, “if the Grade is less than 90%, then we will
not consider the program for the award”. This is the suggestions
taken from last year’s competition, where winners of the SIGSPTIAL GIS CUP competition pay too much attention in optimizing
the file reading speed of the program and the correctness is not
equally considered.

4.3

5.

Grade
Run-time
3
https://cmt.research.microsoft.com/GISCUP2013/

(1)

576

Statistics

SIGSPTATIAL GIS CUP this year received a lot of attention. We
have over 120 subscribers in our mailing list, where over hundreds
of emails have been exchanged during the competition. Finally, we
received 29 qualified submissions, on August 1st, from different
research groups all over the world.

5.2

Testing Dataset

In the evaluation process, we use the following dataset
1. Polygon10, which includes 10 different moving polygons,
published on the website.

4.2 Evaluation Metric

Score =

RESULTS & INSIGHTS

5.1

4. OutputFilePath. Specifies the path to result file, where the
program is expected to store as the output.

The evaluation process is conducted using an automatic grader
that tests the submitted programs using a collection of test cases in
a batch mode. The automatic grader measures both the accuracy of
the generated output and the execution time of the program. Both
the algorithm efficiency and correctness are very important in our
final evaluation.
Efficiency. The time to process all the input data is the main criterion for the evaluation of the solution. So the solution with the best
rate (points processed per minute) is the winner. We also measure
the total run-time for each program to complete all of our test
cases.
Correctness. The accuracy of the result is also considered. If you
process all the input points in 5 minutes but only 1000 points are
accurately associated with the right polygon, then the rate will be
1000 points per 5 minutes. We will calculate a Grade for the
correctness in the way that: if the produced (point/polygon) pair
matches the spatial predicate, the participant earns one point. If
the (point/polygon) pair identified by the program does not match
the spatial predicate or is missed by the program, then one point is
deducted from the participant.
Final Score. The final score is calculated as follows:

Evaluation Environment

As the competitors are able to use any programming language
to build their executable, we need to ensure that we are able to run
the program in our evaluation environment. As a result, we need
to install several different frameworks on the testing machine. The
most common requirements were Java and .NET framework.
The final evaluation is performed on a server machine with an
Intel Xeon 4-core Processor E5-2609 and 64 GB memory, running Microsoft Windows 7. During the testing process, we turn
off all the unrelated applications, and multi-core processing based
solutions are encouraged. However, we limited the total amount of
memory that can be consumed by each program to 1 GB.

2. Polygon15, which includes 15 different moving polygons,
published on the website.
3. Polygon2, which includes only 2 different moving polygons.
However, these two polygons are much more complicated
compared to the previous 2 polygon sets. 4
4. Point500, which includes 500 different moving objects, generated by MNTG [6].
5. Point1000, which includes 10000 different moving objects,
generated by MNTG [6].

5.3

Evaluation

Testing Cases. For each program, we test with the two spatial
predicates:
1. Inside. There are four different test cases: 1) inside_500_10,
2) inside_500_15, 3) inside_1000_10, and 4) inside_1000_15,
where the first number indicates the point file, and the second
number is the polygon file used in the testing case.
2. Within. There are also four test cases: 1) within1000_500_10,
2) within1000_500_15, 3) within1000_1000_10, and
4) within1000_1000_15, where we set the distance as 1000
units, and use the same set of point and polygon files.
4

When running this dataset, none of the competitors’ program
could achieve 90% or more in correctness metric. As a result, we
did not consider this dataset in our final ranking.

Solution
"Point-Polygon
Topological
Relationship Query using
Hierarchical Indices

Authors
Tianyu Zhou; Hong
Wei; Heng Zhang; Yin
Wang; Yanmin Zhu;
Haibing Guan"

Efficiency Score

Correctness Score

Final Score

70.5 milliseconds

18998.875

261.8232859

"Edge-Based Locality Sensitive Hashing for Efficient GeoFencing Application"

Yi Yu; Suhua Tang;
Roger Zimmermann

77.75 milliseconds

18998.875

238.0181325

"Quick Geo-Fencing Using
Trajectory Partitioning and
Boundary Simplification"

Suikai Li;
Weiwei
Sun; Renchu Song;
Zhangqing
Shan;
Zheyong Chen; Xinyu
Zhang

141 milliseconds

18676.625

130.3732294

Table 1: Final Results
Scoring Metrics. For a test run, one of the predicates is tested,
where a correctness score (as the correct point-polygon pairs) and
an efficiency score (in milliseconds) are recorded. After that, we
calculate the final score by dividing the correctness score with the
efficiency score. We run the same test 5 times and report the average score. As a result, each of the submitted programs ran 8
different tests against different input points, polygons and spatial
predicates. Finally, the final score is averaged over all scores resulting from the 8 test runs.
During the actual evaluation process, there are several cases where
the submitted programs crashed running one or more test cases. We
did not consider these failed programs in our final ranking.

for correctness and efficiency. The top ranked competitors did very
well in most of the testing datasets. However, it still leaves some
room for improvement and further research based on their performance over the complicated dataset. We envision that, this competition will serve as a new starting point for the research not only
in GeoFencing but also in other related topics. We also learnt a lot
of lessons organizing the competition. And, it is our hope that this
competition will help foster the growth of the GIS community and
that future SIGSPATIAL GIS Cups will continue this tradition and
grow in scope and participation.

5.4 Final Results

We would like to thank nVIDIA and Microsoft for their generous support and acknowledge the support of ACM SIGSPATIAL
Executive Committee and the 21st SIGSPATIAL Conference Organizing Committee.

Based on the testing scores, we selected the top-3 highest ranked
competitors as the contest winners. The winners and their final
scores are listed in Table 1. All of the three winning teams show
significant edge (in terms of the processing speed) over the rest of
the qualified submissions.

6. LESSONS LEARNED
Overall, organizing such a programming competition is a very
enjoyable process. However, we did learn several lessons during
the competition and the evaluation, that we would like to share with
the masters of the future SIGSPATIAL GIS CUP competitions.
SIGSPATIAL GIS CUP requires the participants to submit executables rather than the result files (as the other competition have
done). We faced with several challenges during the evaluation
phrases: (1) More detailed examples maybe needed for the participants, as many of them did not follow the exact command line
examples, and (2) some of the submitted programs were missing
dependent files/libraries. As a result, we would suggest to the future contest organizers to build an online judging program to minimize the overhead.

7. CONCLUSION
In this paper, we described the details of the 2nd SIGSPATIAL
GIS CUP. Following the tremendous success of the first SIGSPATIAL GIS CUP last year and the need for developing effective solutions to interesting geospatial challenges, we continue the programming competition with the focus on a very basic, yet widely
used GIS function, i.e., Geo-fencing. We followed the successful
competition infrastructure, and improved it based on the suggestions we got from the last year. In this year’s contest, we got 29
qualified submissions, and tested them based on different datasets

577

8.

9.

ACKNOWLEDGMENT

REFERENCES

[1] ACM SIGSPATIAL. http://www.sigspatial.org/.
[2] ACM SIGSPATIAL GIS CUP 2012.
http://depts.washington.edu/giscup/home.
[3] ACM SIGSPATIAL GIS CUP 2013.
http://dmlab.cs.umn.edu/GISCUP2013/.
[4] Thomas Brinkhoff. A framework for generating
network-based moving objects. GeoInformatica,
6(2):153–180, 2002.
[5] KDD CUP. http://www.kdd.org/.
[6] Mohamed F. Mokbel, Louai Alarabi, Jie Bao, Ahmed
Eldawy, Amr Magdy, Mohamed Sarwat, Ethan Waytas, and
Steven Yackel. Mntg: An extensible web-based traffic
generator. In Proceedings of the 13th International
Symposium on Spatial and Temporal Databases, 2013.
[7] Netflix Prize. http://www.netflixprize.com/.
[8] Jihoon Ryoo, Hwangnam Kim, and Samir R Das.
Geo-fencing: geographical-fencing based energy-aware
proactive framework for mobile devices. In Proceedings of
the 2012 IEEE 20th International Workshop on Quality of
Service, page 26. IEEE Press, 2012.
[9] Anmol Sheth, Srinivasan Seshan, and David Wetherall.
Geo-fencing: Confining wi-fi coverage to physical
boundaries. pages 274–290, 2009.
[10] SIGMOD Programming Contest. http://www.sigmod.org/.

Mobility and Social Networking: A Data Management
∗
Perspective
Mohamed F. Mokbel

Mohamed Sarwat

Dept. of Computer Science and Engineering
University of Minnesota, Twin Cities
Minneapolis, MN 55455

Dept. Computer Science and Engineering
University of Minnesota, Twin Cities
Minneapolis, MN 55455

mokbel@cs.umn.edu

sarwat@cs.umn.edu

ABSTRACT
This tutorial presents the state-of-the-art research that lies at the
intersection of two hot topics in the data management community:
(1) social networking and (2) mobility. In this tutorial, we give
an overview of existing research work, systems, and applications
related to both social networking and mobility. In addition, we
introduce several resources (i.e., datasets, software tools) as well as
a list of promising research directions.

1.

TUTORIAL OUTLINE

Online social networks, such as Facebook and Twitter have become very popular in the past decade. Users register to online social networks in order to keep in touch with their friends and family,
learn about their news, get recommendations from them, and engage in online social events. As mobile devices (e.g., smart phones,
GPS devices) became ubiquitous, location-based social networking services (e.g., Foursquare and Facebook Places) are getting
more and more popular. For instance, as of September 2012,
Foursquare claims to have over 25 million people worldwide, and
over billions of check-ins with millions more every day. Users, in a
location-based social network, are associated with a geo-location,
and might alert friends when visiting a venue (e.g., restaurant, bar)
by checking-in on their mobile phones (e.g., iPhone, Android). The
rise of location-based social networking applications has led to the
emergence of both social networking and mobility side by side,
which led to the rise of new research challenges and opportunities.
This tutorial presents the state-of-the-art research that lies at the
intersection of both: Social Networking and Mobility.
The merger between social networking and mobility [33, 4, 8,
46] brought together the following data types: (1) Social Networking data: represents the friendship between different users (usually
represented by a social graph) as well as all sorts of social interactions between users. (2) Spatial/Spatio-temporal data: represents
the users geo-locations, venues (e.g., restaurant, gym, shopping
mall) geo-locations and information about users visiting different

places at different times. (3) Users Opinions data: represents how
much a user likes the places she visits by expressing (e.g., Alice
visited restaurant A and gave it a rating of five over five). We then
illustrate how different mixes of this data trilogy has been leveraged
to explore new trends and by developers to build novel mobile applications [4, 8, 16, 42].
Social networking data management research is mainly concerned with managing users social interactions and collaboration,
storing / retrieving social media (e.g., Microblogs, News Feed),
and analyzing users behavior. Mobile data management research
focuses on handling user GeoSpatial location and contextual information. This tutorial takes an overarching approach by surveying the research that combines both social networking and mobility from the following perspectives: (1) Microblog search and social news feed queries, (2) Recommendation Services, (3) Analytics, (4) Crowdsourcing, (5) System and Media Visualization, and
(6) Risks and Threats. In summary, the tutorial consists of the following parts:
1. PART I: Microblog Search and Social News Feed Queries:
In this part, we first describe how both geo-location and social awareness work in concert to answer queries [3, 1, 7,
6, 5, 13, 21, 30, 35, 38, 41]. We present recent studies that
show how both spatial and social aspects can be combined
to enhance Microblog search and social news feed quality.
We also analyze existing GeoSocial Microblog search and
Location-aware New feed querying system from a scalability and efficiency perspective.
2. PART II: Recommendation Services: We present recent studies which show that geo-location matters in recommender
systems, and we manifest several techniques to incorporate
the spatial/spatio-temporal information [17, 34, 37, 39, 43]
side-by-side with users opinions data in traditional recommender systems [16, 28, 36, 31]. We also highlight the research works that leverage GeoSocial data points and trajectories for travel and itinerary recommendations [47, 45].
3. PART III: Analytics: We give an overview of computational
techniques that are harnessed to analyze GeoSocial data [9,
23, 20, 44] to learn more about human behavior, the societal
and economical consequences of such analysis [19, 2].
4. PART IV: Crowdsourcing: We summarize research work performed in the Volunteered Geographic Information (VGI)
and participatory sensing area. We then survey recent papers that address the crowdsourcing topic [15, 11, 24, 25, 27,
26] from a mobility perspective [10, 14].
5. PART V: Systems and Media Visualization: we highlight systems that aim at visualizing GeoSocial Media [22, 33, 48]
(e.g., Geo-Tagged Tweets, Geo-Tagged Videos) and social

∗This work is supported in part by the National Science Foundation
under Grants IIS-0952977 and IIS-1218168.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Articles from this volume were invited to present
their results at The 39th International Conference on Very Large Data Bases,
August 26th - 30th 2013, Riva del Garda, Trento, Italy.
Proceedings of the VLDB Endowment, Vol. 6, No. 11
Copyright 2013 VLDB Endowment 2150-8097/13/09... $ 10.00.

1196

interactions between mobile users. We present the state-ofthe-art GeoSocial visualization techniques [32, 18].
6. PART VI: Risks and Threats: In this part, we throw spotlight
on the main risks that may arise from combining social networking and mobility. For instance, we highlight the possible
threats of revealing user location in online social networking
websites. We then highlight recent research work that aims at
preserving the user location and absence privacy in locationaware social networking services (e.g., privacy [29, 40, 12]).
For all aforementioned topics, we present results from recent research work, case studies featuring hot mobile applications, and
the anatomy of built systems. Finally, we conclude the tutorial by
summarizing and presenting open research directions.

2.[1] J.REFERENCES
Bao, M. F. Mokbel, and C.-Y. Chow. GeoFeed: A Location-Aware
News Feed System. In ICDE, 2012.
[2] S. Bhagat, A. Goyal, and L. V. S. Lakshmanan. Maximizing product
adoption in social networks. In WSDM, 2012.
[3] M. Busch, K. Gade, B. Larson, P. Lok, S. Luckenbill, and J. Lin.
Earlybird: Real-Time Search at Twitter. In ICDE, 2012.
[4] Y. Cai and T. Xu. Design, analysis, and implementation of a
large-scale real-time location-based information sharing system. In
MobiSys, 2008.
[5] C. C. CAO, J. She, Y. Tong, and L. Chen. Whom to ask? jury
selection for decision making tasks on micro-blog services. In VLDB,
2012.
[6] C. Chen, F. Li, B. C. Ooi, and S. Wu. Ti: an efficient indexing
mechanism for real-time search on tweets. In SIGMOD, 2011.
[7] L. Chen, G. Cong, C. S. Jensen, and D. Wu. Spatial Keyword Query
Processing: An Experimental Evaluation. In VLDB, 2013.
[8] Y. Chen, K. Jiang, Y. Zheng, C. Li, and N. Yu. Trajectory
simplification method for location-based social networking services.
In GIS-LBSN, 2009.
[9] E. Cho, S. A. Myers, and J. Leskovec. Friendship and mobility: user
movement in location-based social networks. In KDD, 2011.
[10] A. Efentakis, D. Theodorakis, and D. Pfoser. Crowdsourcing
computing resources for shortest-path computation. In GIS, 2012.
[11] A. Feng, M. J. Franklin, D. Kossmann, T. Kraska, S. Madden,
S. Ramesh, A. Wang, and R. Xin. CrowdDB: Query Processing with
the VLDB Crowd. PVLDB, 2011.
[12] D. Freni, C. R. Vicente, S. Mascetti, C. Bettini, and C. S. Jensen.
Preserving location and absence privacy in geo-social networks. In
CIKM, 2010.
[13] N. Gupta, L. Kot, S. Roy, G. Bender, J. Gehrke, and C. Koch.
Entangled queries: Enabling declarative data-driven coordination.
TODS, 37(3):21, 2012.
[14] L. Kazemi and C. Shahabi. Geocrowd: Enabling query answering
with spatial crowdsourcing. In GIS, 2012.
[15] A. Kittur, E. H. Chi, and B. Suh. Crowdsourcing user studies with
mechanical turk. In CHI, 2008.
[16] K. Kodama, Y. Iijima, X. Guo, and Y. Ishikawa. Skyline queries
based on user locations and preferences for making location-based
recommendations. In GIS-LBSN, 2009.
[17] J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel. LARS: A
Location-Aware Recommender System. In ICDE, 2012.
[18] M. D. Lieberman and H. Samet. Supporting rapid processing and
interactive map-based exploration of streaming news. In GIS, 2012.
[19] W. Lu and L. V. S. Lakshmanan. Profit maximization over social
networks. In ICDM, 2012.
[20] W. Lu, Y. Shen, S. Chen, and B. C. Ooi. Efficient processing of k
nearest neighbor joins using mapreduce. In VLDB, 2012.
[21] K. Mamouras, S. Oren, L. Seeman, L. Kot, and J. Gehrke. The
Complexity of Social Coordination. PVLDB, 5(11):1172–1183, 2012.
[22] A. Marcus, M. S. Bernstein, O. Badar, D. R. Karger, S. Madden, and
R. C. Miller. Tweets as data: demonstration of TweeQL and Twitinfo.
In SIGMOD, 2011.

[23] G. D. F. Morales, A. Gionis, and M. Sozio. Social content matching
in mapreduce. In VLDB, 2011.
[24] A. G. Parameswaran, H. Garcia-Molina, H. Park, N. Polyzotis,
A. Ramesh, and J. Widom. CrowdScreen: algorithms for filtering
data with humans. In SIGMOD, pages 361–372, 2012.
[25] A. G. Parameswaran, H. Park, H. Garcia-Molina, N. Polyzotis, and
J. Widom. Deco: declarative crowdsourcing. In CIKM, 2012.
[26] H. Park, R. Pang, A. G. Parameswaran, H. Garcia-Molina,
N. Polyzotis, and J. Widom. An overview of the deco system: data
model and query language; query processing and optimization.
SIGMOD Record, 41(4):22–27, 2012.
[27] H. Park, R. Pang, A. G. Parameswaran, H. Garcia-Molina,
N. Polyzotis, and J. Widom. Deco: A System for Declarative
Crowdsourcing. PVLDB, 5(12):1990–1993, 2012.
[28] M.-H. Park et al. Location-based recommendation system using
bayesian user’s preference model in mobile devices. In UIC, 2007.
[29] K. P. N. Puttaswamy and B. Y. Zhao. Preserving privacy in
location-based mobile social applications. In HotMobile, 2010.
[30] S. B. Roy and K. Chakrabarti. Location-aware type ahead search on
spatial databases: semantics and efficiency. In SIGMOD, 2011.
[31] S. B. Roy et al. Space efficiency in group recommendation. VLDB
J., 19(6):877–900, 2010.
[32] A. D. Sarma, H. Lee, H. Gonzalez, J. Madhavan, and A. Y. Halevy.
Efficient spatial sampling of large geographical tables. In SIGMOD,
2012.
[33] M. Sarwat, J. Bao, A. Eldawy, J. J. Levandoski, A. Magdy, and M. F.
Mokbel. Sindbad: A Location-based Social Networking System. In
SIGMOD, 2012.
[34] M. Sarwat, J. J. Levandoski, A. Eldawy, and M. F. Mokbel. LARS*:
A Scalable and Efficient Location-Aware Recommender System. In
TKDE, 2013.
[35] A. Silberstein, J. Terrace, B. F. Cooper, and R. Ramakrishnan.
Feeding frenzy: Selectively materializing user’s event feed. In
SIGMOD, 2010.
[36] Y. Takeuchi and M. Sugimoto. An outdoor recommendation system
based on user location history. In UIC, 2006.
[37] S. Thirumuruganathan, M. Das, S. Desai, S. Amer-Yahia, G. Das,
and C. Yu. MapRat: Meaningful Explanation, Interactive Exploration
and Geo-Visualization of Collaborative Ratings. PVLDB,
5(12):1986–1989, 2012.
[38] J. R. Thomsen, M. L. Yiu, and C. S. Jensen. Effective caching of
shortest paths for location-based services. In SIGMOD, 2012.
[39] P. Venetis, H. Gonzalez, C. S. Jensen, and A. Y. Halevy. Hyper-local,
directions-based ranking of places. VLDB, 4(5):290–301, 2011.
[40] W. Wei, F. Xu, and Q. Li. MobiShare: Flexible privacy-preserving
location sharing in mobile online social networks. In INFOCOM,
2012.
[41] W. Xu, C.-Y. Chow, M. L. Yiu, Q. Li, and C. K. Poon. MobiFeed: a
location-aware news feed system for mobile users. In GIS, 2012.
[42] M. Ye, P. Yin, and W.-C. Lee. Location recommendation for
location-based social networks. In GIS, 2010.
[43] H. Yin, B. Cui, J. Li, J. Yao, and C. Chen. Challenging the Long Tail
Recommendation. PVLDB, 5(9):896–907, 2012.
[44] C. Zhang, L. Shou, K. Chen, G. Chen, and Y. Bei. Evaluating
geo-social influence in location-based social networks. In CIKM,
2012.
[45] V. Zheng, Y. Zheng, X. Xie, and Q. Yang. Collaborative Location and
Activity Recommendations with GPS History Data. In WWW, 2010.
[46] Y. Zheng, Y. Chen, X. Xie, and W.-Y. Ma. Geolife2.0: A
location-based social networking service. In MDM, 2009.
[47] Y. Zheng and X. Xie. Learning travel recommendations from
user-generated GPS traces. ACM Transactions on Intelligent Systems
and Technology (TIST), 2(1):2, 2011.
[48] Y. Zheng, X. Xie, and W.-Y. Ma. GeoLife: A Collaborative Social
Networking Service among User, Location and Trajectory. IEEE
Data Eng. Bull., 33(2):32–39, 2010.

1197

2015 IEEE International Conference on Cloud Engineering

MobiSocial (Mobile and Social) Data Management: A Tutorial
Mohamed Sarwat
Arizona State University
Tempe, USA
msarwat@asu.edu

Mohamed F. Mokbel
University of Minnesota
Minneapolis, USA
mokbel@cs.umn.edu

Abstract—The rise of the Social Internet, in the past decade,
stimulated the invention of human-centered technologies that
study and serve humans as individuals and in groups. For
instance, social networking services provide ways for individuals to connect and interact with their friends. Also,
personalized recommender systems leverage the collaborative
social intelligence of all users’ opinions to recommend: books,
news, movies, or products in general. These social technologies
have been enhancing the quality of Internet services and
enriching the end-user experience. Furthermore, the Mobile
Internet allows hundreds of millions of users to frequently
use their mobile devices to access their healthcare information
and bank accounts, interact with friends, buy stuff online,
search interesting places to visit on-the-go, ask for driving
directions, and more. In consequence, everything we do on the
MobiSocial Internet leaves breadcrumbs of digital traces that,
when managed and analyzed well, could deﬁnitely be leveraged
to improve life. Services that leverage Mobile and/or Social
data have become killer applications in the cloud. Nonetheless,
a major challenge that Cloud Service providers face is how
to manage (store, index, query) MobiSocial data hosted in the
cloud. Unfortunately, classic data management systems are not
well adapted to handle data-intensive MobiSocial applications.
The tutorial surveys state-of-the-art MobiSocial data management systems and research prototypes from the following
perspectives: (1) Geo-tagged Microblog search, location-aware
and mobile social news feed queries, and GeoSocial Graph
search, (2) Mobile Recommendation Services, and (3) GeoCrowdsourcing. We ﬁnally highlight the risks and threats
(e.g., privacy) that result from combining mobility and social
networking. We conclude the tutorial by summarizing and
presenting open research directions.

interactions between users. (2) Spatial/Spatio-temporal data:
represents the users geo-locations, venues (e.g., restaurant,
gym, and shopping mall) geo-locations and information
about users visiting different places at different times. (3)
Users Opinions data: represents how much a user likes the
places she visits by expressing (e.g., Alice visited restaurant
A and gave it a rating of ﬁve over ﬁve). We then illustrate
how different mixes of this data trilogy has been leveraged to
explore new trends and by developers to build novel mobile
applications.
II. S ESSION II (120 MINS )
In the second part, we present the state-of-the-art research
in managing MobiSocial data, from the following perspectives: (1) GeoSocial Search and Query Processing: That
incorporates both the geo-location and the social awareness
in answering queries. We then present several indexing
and query processing algorithms that efﬁciently access geotagged social media (e.g., tweets, news feed, social graph
entities). (2) Mobile Recommendation Services: We present
recent studies which show that geo-location matters in
recommender systems, and we manifest several techniques
to incorporate the spatial/spatio-temporal information and
users opinions data side by side in traditional recommender
systems. We also highlight the research works that leverage
GeoSocial data points and trajectories for travel and itinerary
recommendations. (3) Location-aware Crowdsourcing: We
give an overview of the Volunteered Geographic Information
(VGI) area and we survey the recent papers that address
the Crowdsourcing topic from a geographic perspective. For
all aforementioned topics, we present results from recent
research work, case studies from hot mobile applications,
and the anatomy of built systems. Then, we highlight the
main risks that result from combining social networking
and mobility (e.g., privacy). Finally, we introduce possible
research directions to the audience and we also manifest
several resources, e.g., data, software tools, that assist the
audience in starting their own research in social networking
and mobility.

I. S ESSION I (60 MINS )
In the ﬁrst part, we start by giving a quick overview of
social networking services (e.g., Facebook, Twitter), their
evolution, and how they impact the society. Similarly, we explain through examples and case studies how the widespread
of mobile devices changes the computing paradigm in a
way that impacted our daily life. We then illustrate how the
marriage of both social networking and mobility technologies has led to the rise of location-based social networking
systems. We give a brief history of several attempts to
combine social networking and mobility. Therefore, we
explain the richness of data generated by merging both
social networking and mobility: (1) Social Networking data:
represents the friendship between different users (usually
represented by a social graph) as well as all sorts of social
978-1-4799-8218-9/15 $31.00 © 2015 IEEE
DOI 10.1109/IC2E.2015.34

4

RecStore: An Extensible and Adaptive Framework for
Online Recommender Queries inside the Database Engine
Justin J. Levandoski1§

Mohamed Sarwat2

Mohamed F. Mokbel2

Michael D. Ekstrand2

1

2

Microsoft Research, Redmond, WA, justin.levandoski@microsoft.com
University of Minnesota, Minneapolis, MN, {sarwat,mokbel,ekstrand}@cs.umn.edu

ABSTRACT

and movies from a large catalog (Netflix, Movielens [23]). By far,
the most popular recommendation method used is collaborative filtering [18, 30], which consists of two phases: (1) A computationally expensive offline model generation phase that uses community opinions (e.g., user ratings) of data items in order to derive
meaningful correlations between users and/or items. (2) An online
recommendation generation phase that uses the model to produce
recommendations. From a database perspective, the recommendation process is simply a set of SQL-based recommender queries
built to provide answers according to a particular recommendation
method [19]. Examples of recommendation methods include userbased [29] or item-based [31] collaborative filtering.

Most recommendation methods (e.g., collaborative filtering) consist of (1) a computationally intense offline phase that computes a
recommender model based on users’ opinions of items, and (2) an
online phase consisting of SQL-based queries that use the model
(generated offline) to derive user preferences and provide recommendations for interesting items. Current application usage trends
require a completely online recommender process, meaning the
recommender model must update in real time as new opinions enter the system. To tackle this problem, we propose RecStore, a
DBMS storage engine module capable of efficient online model
maintenance. Externally, models managed by RecStore behave as
relational tables, thus existing SQL-based recommendation queries
remain unchanged while gaining online model support. RecStore
maintains internal statistics and data structures aimed at providing efficient incremental updates to the recommender model, while
employing an adaptive strategy for internal maintenance and load
shedding to realize a balance between efficiency in updates or query
processing based on system workloads. RecStore is also extensible,
supporting a declarative syntax for defining recommender models. The efficacy of RecStore is demonstrated by providing the implementation details of three state-of-the-art collaborative filtering
models. We provide an extensive experimental evaluation of a prototype of RecStore, built inside the storage engine of PostgreSQL,
using a real-life recommender system workload.

1.

To be effective, recommender systems must evolve with their content. For example, new users enter the system changing the collective opinions of items, the system adds new items widening the
recommendation pool, or user tastes change. These actions affect
the recommender model, that in turn affect the system’s recommendation quality. Traditionally, most systems have been able to
tolerate using an offline process that builds a fresh model daily or
weekly in order to adapt to changes in the underlying content [17,
24, 31]. However, these traditional practices are no longer valid in
an increasingly dynamic online world. In an age of staggering web
use growth and ever-popular social media applications (e.g., Facebook [9], Google Reader [12]), users are expressing their opinions
over a diverse set of data (e.g., news stories, Facebook posts, retail
purchases) faster than ever. In such an environment, forcing recommender systems to use an offline model building phase is unacceptable, as the system must adapt quickly to its diverse and everchanging content. Recommender systems cannot wait weeks, days,
or even hours to rebuild their models [6]. The rate that new items or
users enter the system (e.g., Facebook updates, news posts), and the
rate that which users express opinions over items (e.g., Diggs [8],
Facebook “likes" [10]), requires recommender models to change in
minutes or seconds, implying models be updated online.

INTRODUCTION

Recommender systems have grown popular in both commercial [6,
21] and academic settings [1, 5, 23]. The purpose of recommender
systems is to help users identify useful, interesting items or content (data) from a considerably large search space. For example,
recommender systems have successfully been used to help users
find interesting books and media from a massive inventory base
(Amazon [21]), news items from the Internet (Google News [6]),
This work is supported in part by the National Science Foundation under Grants IIS-0811998, IIS-0811935, CNS-0708604,
IIS-0952977 and by a Microsoft Research gift

Recent work from the data management community has shown
that many popular recommendation methods (including collaborative filtering) can be expressed with conventional SQL, effectively pushing the core logic of recommender systems within the
DBMS [19]. However, the approach does nothing to address the
pressing problem of online model maintenance, as collaborative
filtering still requires a computationally intense offline model generation phase when implemented with a DBMS.

§Work done while at the University of Minnesota

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
EDBT 2012, March 26–30, 2012, Berlin, Germany.
Copyright 2012 ACM 978-1-4503-0790-1/12/03 ...$10.00

In this paper, we address the problem of providing online recommender model maintenance for DBMS-based recommender systems. We present RecStore, a module built inside the storage engine

86

of a relational database system. RecStore enables online model support for DBMS-based recommender systems (e.g., [19]) through
efficient incremental updates to only parts of the model affected
by a rating update. Thus, updating the recommender model does
not involve significant overhead, nor regeneration of the model
from scratch. RecStore exposes the model to the query processor
as a standard relational table, meaning that existing recommender
queries can remain unchanged.

B E

DEBFE

DEF

A

DEF

B

C F

B

E

A

!

B

C
A BC

The basic idea behind RecStore is to separate the logical and internal representations of the recommender model. RecStore receives
updates to the user/item rating data (i.e., the base data for a collaborative filtering models) and maintains its internal representation
based on these updates. As RecStore is built into the DBMS storage engine, it outputs tuples to the query processor though access
methods that transform data from the internal representation into
the logical representation expected by the query processor.

Figure 1: Item-based Model Generation
support for other methods is discussed later in Section 5. Collaborative filtering assumes a set of n users U = {u1 , ..., un } and a
set of m items I = {i1 , ..., im }. Each user uj expresses opinions
about a set of items Iuj ⊆ I. In this paper, we assume opinions
are expressed through an explicit numeric rating (e.g., one through
five stars), but other methods are possible (e.g., hyperlink clicks,
Facebook “likes" [10], Diggs [8]). An active user ua is given a set
of recommendations Ir such that Iua ∩ Ir = ∅, i.e., the user has
not rated the recommended items. The recommendation process
is usually broken into two phases: (1) an offline model generation phase that creates a model storing correlations between items
and/or users, and (2) an online recommendation generation phase
that uses the model to generate recommended items. There are
several methods to perform collaborative filtering including itembased [31], user-based [29], regression-based [31], or approaches
that use more sophisticated probabilistic models (e.g., Bayesian
Networks [3]).

RecStore is designed with extensibility in mind. RecStore’s architecture is generic, and thus the logic for a number of different
recommendation methods can easily be “plugged into" the RecStore framework, making it a one-stop solution to support a number of popular recommender models within the DBMS. We provide
a generic definition syntax for RecStore, and provide implementation case studies for various memory-based [1, 3] collaborative
filtering methods (e.g., item-based [31] and user-based [29]). We
also discuss support for other non-trivial recommendation methods
(e.g., [3, 20]).
RecStore is also adaptive to system workloads, tunable to realize a
trade-off that makes query processing more efficient at the cost of
update overhead, and vice versa. At one extreme, RecStore has lowest query latency by making update costs more expensive; appropriate for query-intense workloads. At the other extreme, RecStore
minimizes update costs by pushing computation into query processing; appropriate for update-intense workloads. For particularly
update-intense workloads, RecStore also performs load-shedding to
process only important updates that significantly alter the recommender model and change the answers to recommender queries.

Below we describe the details of item-based [31] and userbased [29] collaborative filtering, by far two of the most popular recommendation methods in use today (e.g., Amazon [21]).
These methods are classified as “memory-based” recommendation
approaches [3, 1], so called because they “remember" the opinion
history of the entire user base in order to provide recommendations.
Details of other methods are discussed in Section 5.

2.1 Offline Model Generation
RecStore requires a small code footprint, which is advantageous to
implementation in existing database engines. Our prototype of RecStore, built inside PostgreSQL [28], between the storage engine and
query processor, requires approximately 600 lines of either modified or new code. Rigorous experimental study of our RecStore
prototype using a real workload from the popular MovieLens [26]
recommender system shows that RecStore exhibits desirable performance in both updates and query processing compared to existing DBMS approaches that support online recommender models
using regular and materialized views.

The offline model generation phase analyzes the entire user/item
rating space, and uses statistical techniques to find correlated items
and/or users. These correlations are measured by a score, or weight,
that defines the strength of the relation.

2.1.1 Item-based collaborative filtering

COLLABORATIVE FILTERING AND
THE DBMS

The item-item model builds, for each of the m items I in the
database, a list L of similar items. Given two items ip and iq , we
can derive their similarity score sim(ip , iq ) by representing each
as a vector in the user-rating space, and then use a similarity function over the two vectors to compute a numeric value representing
the strength of their relationship. Figure 1 depicts this item-item
model-building process. Conceptually, we can represent the ratings data as a matrix, with users and items each representing a
dimension, as depicted on the left side of Figure 1. The similarity function, sim(ip , iq ), computes the similarity of vectors ip and
iq using only their co-rated dimensions. In our example uj and
uk represent the co-rated dimensions. Finally, we store ip , iq , and
sim(ip , iq ) in our model, as depicted on the right side of Figure 1.
The similarity measure need not be symmetric, i.e., it is possible
that sim(ip , iq ) 6= sim(iq , ip ).

This section provides an overview of collaborative filtering, the primary recommendation method we are concerned with in this paper;

Many similarity measures have been proposed in the literature [17,

The rest of this paper is organized as follows. Section 2 provides
a preliminary background. Related work is covered in Section 7.
Section 3 introduces the RecStore architecture. Section 4 describes
the functionality of RecStore. Section 6 provides an experimental
evaluation of RecStore. Finally, Section 8 concludes the paper.

2.

87

31]. On of the most popular measures used is the cosine distance,
calculated as:

sim(ip , iq ) = k

i~p · i~q
ki~p kki~q k

A BC DEFB
FB
FE B
!"
CD
#
EC FB $
DEF %C
#
)

CF DB
"
&
'(

(1)

+B BDEFB DB
"
# F EC
/ #C
&
B
'(
#DB
F
# F 1&
+ &
!3 # F &

Here, items ip and iq are represented as vectors in the user-rating
space, and k represents a dampening factor that discounts the influence of item pairs having high scores, but only a few common ratings [14]; given the co-rating count between two items as
corate(ip , iq ), k is defined as:

1
corate(ip , iq ) ≥ 50
k=
(2)
corate(iq , iq )/50 otherwise

CD
A BC
A BC
#DEF % EC DEF

%

*

,F
E

C C % -B %.FB C
EFB $FB
#DEF %0
/ #C 0 EC
CD
A BC
) # F $
12
$1 /CB B,F F $
&
CD
2
!3 DB ,F
2
*

DB

,F

A BC0

Figure 2: Item-based recommender query

2.1.2 User-based collaborative filtering
P(ua ,i) = r ua +

The user-user model is similar in nature to the item-item paradigm,
except that the model calculates similarity between users (instead
of items). This calculation is performed by comparing user vectors
in the item-rating space. For example, in Figure 1, focusing on the
user/item matrix, users uj and uk can be represented as vectors in
item space, and compared based on the items they have co-rated
(i.e., ip and iq ). The user-user model primarily uses cosine distance and Pearson correlation as similarity measures [3], much like
that of the item-item paradigm with the exception that similarity is
measured in item space rather than user space.

P

− r ul ) ∗ sim(ua , ul )
l∈L |sim(ua , ul )|

l∈L (rul ,i

P

(4)

This value is the weighted average of deviations from a related user
ul ’s mean. In this equation, rul ,i represents a user ul ’s (non-zero)
rating for item i, while r ua and r ul represent the average rating
values for users ua and ul , respectively.

2.3 DBMS-based Collaborative Filtering
A DBMS can be used to implement the recommendation process just described. Ratings data can be stored in a relation
Ratings(userId,itemId,rating), where userId and itemId represent
unique ids of users and items, respectively.

2.2 Online recommendation generation
The online recommendation generation phase employs the ability
to predict ratings for items that a user ua has not yet rated. Rating
predictions are produced by performing aggregation over the recommender models. These predictions can be used to (1) give the
user their predicted score for a specific item on request, or (2) produce a set of top-N recommended items based on highest predicted
scores.

2.3.1 Model representation
The model can be represented by a three-column table Model(item,rel_itm,score) for the item-item model, or
Model(user,rel_user,score) for the user-user model (different
schemas may be necessary for other methods).

2.3.2 Recommender queries

2.2.1 Item-based collaborative filtering
Recommendation generation for the item-based cosine method produces the top-n items based on predicted score using two steps.
(1) Reduction: cut down the model such that each item i left in the
model is an item not rated by user ua , while i’s similarity list L
contains only items l already rated by ua . (2) Compute: the predicted rating P(ua ,i) for an item i and user ua is calculated as a
weighted sum [31]:
P
l∈L sim(i, l) ∗ rua ,l
P
P(ua ,i) =
(3)
l∈L sim(i, l)

A DBMS-based recommenders will use SQL to produce recommendations. Figure 2 provides an SQL example of the process discussed in Section 2.2 (listed in two parts for readability). The first
query finds all movies rated by a user X. The second query uses
these results to produce recommendations for user X using Equation 3. The WHERE clause represents the reduction step, while
the SELECT clause represents the computation step. The query assumes the model relation M(itm,rel_itm,sim) is already generated
offline.

The prediction is the sum of the user’s rating for a related item
l, rua ,l , weighted by the similarity to the candidate item i. The
prediction is normalized by the sum of scores between i and l.

Figure 3 depicts the high-level architecture of RecStore, built inside
the storage engine of a DBMS. RecStore consists of the following
main components:

3. RecStore ARCHITECTURE

2.2.2 User-based Collaborative Filtering

• Intermediate store and filter. The intermediate store contains a set of statistics, functions, and/or data structures that
are efficient to update, and can be used to quickly generate
part of the recommender model. The data maintained in the
intermediate store is specific to the recommendation method.
Whenever RecStore receives ratings updates (i.e., insertions,

Rating prediction in the user-based recommender paradigm is similar in spirit to the item-based method. Recall that the similarity
list L in the user-user paradigm is a list of similar users to a particular user u. A prediction P(ua ,i) for an item i given user ua is
calculated as [18]:

88

AA

DC
A

B

DC E
DC A

BC

DE F E

AABC DEF A

BC

ABBC
AB

DEF
B

BC

C
F C E
DE A

DC BC
DEF

AB C
DE A

F

B
DEF
B

Figure 4: Query plan for 2nd query in Figure 2

Figure 3: RecStore Architecture

deletions, or changes to the ratings table), it applies an intermediate filter that determines whether the update will affect
the contents of the intermediate store (Section 4.1.1).
• Model store and filter. The model store represents the materialized model that matches the exact storage schema needed
by the recommender method (e.g., (itm, rel_itm, sim) for the
item-based model covered in Section 2). Any changes to
the intermediate store goes through a model filter that determines whether it affects the contents of the model store
(Section 4.1.2).

filtering, share commonalities in model structure. We defer such
discussion until later in Section 5. For now, we use the example
of the item-based cosine model to illustrate RecStore’s approach to
providing online model maintenance, consisting of two steps.

4.1.1 Step 1: Intermediate Filter
We describe the functionality of the intermediate filter with an example using the item-based cosine method described in Section 2.
For this method, the intermediate store contains a “deconstructed”
cosine score (Equation 3), where we store for each item pair (ip ,iq )
that share at least one co-rated dimension (1) pdot(ip , iq ), their partial dot product, (2) lenp (ip , iq ) and lenq (iq , ip ), the partial length
of each vector for only the co-rated dimensions, and (3) co(ip , iq ),
the number of users who have co-rated items ip and iq . This data is
stored as a six-column relation, where the first two columns store
the item id pairs, while the last four columns store the four statistics
just described.

The DBMS query processor requests data from RecStore while
executing recommender queries. RecStore employs two standard
DBMS access methods to interface with the query processor: scan,
i.e., return all model data, and index, i.e., return only model data
satisfying a given condition (e.g., item id = x). The access methods can produce tuples (i.e., model values) either directly from the
model store, or on demand from the intermediate store, or the base
ratings data; these query processing details are covered in Section 4.2. As an example, consider query plan given in Figure 4
that retrieves all tuples in the Model relation with item ids equal to
those rated by user X (this operation is performed by query two
in Figure 2). This plan performs an index scan over the model
to perform the join between usrXMovies. In our experience, most
access to RecStore will be index-based, as recommendation generation queries require only a portion of the model (similar to Figure 4).

4.

RecStore employs an intermediate filter upon receiving a rating update R. The intermediate filter performs three tasks in the following order. (1) Filter. This task determines whether R will
be used to update entries in the intermediate store. If not, R is
immediately dropped (but still stored in the ratings data). This
step is required by the adaptive maintenance and load shedding
techniques discussed later Section 4.2. In the general case this
step will not drop any updates. (2) Enumeration. This task determines all intermediate store entries E that will change due to
R. For our item-based cosine example with a new rating for item
ip , E would contain all entries (ip ,iq ) for which items ip and iq
are co-rated by the user u. (3) Updates. Finally, all statistics,
functions, or data structures in the intermediate store associated
with an entry e ∈ E are updated. These updates are then forwarded to the model filter. For our item-based cosine example,
the stored statistics are updated as follows, assuming a new rating
for item ip with value sp : pdot(ip , iq ) = pdot(ip , iq ) + sp × sq ,
lenp (ip , iq ) = lenp (ip , iq ) + sp , lenq (iq , ip ) = lenq (iq , ip ) + sq ,
and co(ip , iq ) = co(ip , iq ) + 1.

RecStore: BUILT-IN ONLINE
DBMS-BASED RECOMMENDERS

The main objective of RecStore is to bring online model support
to existing recommender queries for various workloads and recommendation methods. This objective presents three main challenges that we address in the rest of this paper: (1) Efficient online
incremental maintenance of the recommender model, i.e., avoiding expensive model regeneration with each update (Section 4.1).
(2) The ability to adapt the system to various workloads, e.g., query
or update-intensive workloads (Section 4.2). (3) The ability to support various existing recommender methods (Section 5).

Together, the intermediate filter and store are the keys to efficient online model maintenance in RecStore. The filter reduces
update processing overhead by allowing RecStore to only process
the updates necessary to maintain an accurate intermediate representation. The contents of the intermediate store keep computational overhead low for online maintenance by allowing RecStore
to quickly update the intermediate store and, once updated, quickly
derive a final model score from the intermediate representation.

4.1 Online Model Maintenance
This section describes the framework for online model maintenance within RecStore. The framework is extensible, and its specific functionality is determined by the underlying recommendation
method. While this approach may seem overly-tailored to each specific method, we note that many methods, especially collaborative

89

4.1.2 Step 2: Model Filter

(e.g., from the intermediate statistics covered in Section 4.1.1 for
the item-based cosine method). (3) If the entry is not maintained
in the intermediate nor the model store, the model value must be
produced on-demand using the base ratings data (e.g., using Equation 1 for the item-based cosine method). Thus, as α and β decrease, query processing latency increases as more model values
must be produced on demand. Larger values of α and β have a
reverse effect on update and query processing efficiency.

Upon receiving updates from the intermediate filter, the model filter executes the same three tasks as the intermediate filter (i.e.,
filter, enumeration, and updates), except applied to the model
store instead of the intermediate store. Continuing our itembased cosine example, its model store contains entries of the form
(ip ,iq ,sim(ip ,iq )), i.e., the item-based model schema discussed in
Section 2. The model filter uses the statistical updates from the intermediate store for item pairs (ip ,iq ) to update the similarity score
in the model store entry (ip ,iq , sim(ip ,iq )) as follows per Equation 2: (1) If statistic co(ip , iq ) < 50, then sim(ip , iq ) is updated
as:

sim(ip , iq ) =

50 ∗

Using the maintenance parameters α and β allows RecStore to be
tuned for a wide range of workloads. More update-intense workloads can lower values of α and β at the cost of increasing recommender query latency. Meanwhile, query-intense workloads can
use larger values of α and β at the cost of increasing update overhead. We now explore several strategies for α and β settings; experimental analysis for these strategies is given in Section 6.

co(ip , iq ) ∗ pdot(ip , iq )
p
p
lenp (ip , iq ) lenq (ip , iq )

(2) If statistic co(ip , iq ) ≥ 50, we update sim(ip , iq ) as:

• Extreme Approaches. Two extreme approaches can be
taken by RecStore: (1) Materialize all. In this approach
α = β = M, meaning RecStore’s intermediate and model
stores maintain all required model information. RecStore filters just apply the conditions imposed by the specific similarity functions upon receiving a rating update. Recommendation generation, i.e., the query processing functionality that
generates recommended items, is most efficient at this extreme. However, storage and maintenance costs are at their
highest with this approach. (2) Materialize none. In this approach α = β = 0, and basically mimics the use of regular
DBMS views that we recompute model values on demand.
In this approach there is no need for the intermediate store,
model store, nor filters. Recommendation generation for this
approach is very expensive, but incurs no storage and maintenance costs as nothing is maintained.

pdot(ip , iq )
sim(ip , iq ) = p
p
lenp (ip , iq ) lenq (ip , iq )

Updating the similarity score is the final step in the RecStore online
maintenance process.

4.2 Adaptive Strategies for System Workloads
This section discusses how RecStore adapts to different workload
characteristics. We first discuss generic maintenance strategies that
help realize an update and query efficiency trade-off. We then discuss load-shedding for update-intensive workloads.

4.2.1 Update vs. Query Efficiency Trade-off

• Intermediate Store Only. In this approach α = M and
β = 0. This approach (abbr. Intermediate Only) represents a middle ground between Materialize All and Materialize None, where we materialize the intermediate store in
full for all required model information, while not maintaining the model store. This means that the initial filter will be
applied on all incoming updates as described in Section 4.1,
while there is no filter for the model store. The recommendation generation process for a requested object o (e.g., item
or user) needs to rebuild part of the model store that includes
o using the fully maintained intermediate store. This rebuilding process makes this approach incur higher query processing cost compared to the Materialize All approach, but much
lower query processing cost than the Materialize None approach. On the other hand, storage and maintenance costs are
lower than the materialize all approach, as the model store is
nonexistent.

While the intermediate and model store are beneficial to RecStore,
their sizes may lead to non-trivial maintenance costs. For instance,
in item-item or user-user collaborative filtering, the size of the
model can reach O(n2 ), where n is the number of items (or users).
In this case, RecStore could be responsible for updating and maintaining data for O(n2 ) items (or users) in its intermediate and model
store, leading to burdensome maintenance costs. In this section, we
explore a trade-off: reducing the storage and maintenance of data
in the intermediate store and model store (i.e., the internal maintenance approach) in return for sacrificing query processing (i.e.,
recommendation generation) efficiency.
RecStore can be tuned to realize an efficiency trade-off between
updates and query processing. The basic idea is to maintain α entries in the intermediate store, β entries in the model store, and
require the invariant that α ≥ β, i.e., all entries in the model store
are also maintained in the intermediate store. Both values cannot
be greater than M: the total possible number of entries, a modelspecific value (e.g., for item-based models M = I 2 ).

• Full Intermediate Store and Partial Model Store. This
approach (abbr. Partial Model) sets α = M and β = N ,
and represents a middle ground between the Materialize all
and Intermediate Only approaches. This approach materializes only a portion of the model store, i.e., only N objects
(e.g., items or users), while materializing the intermediate
store in full. We employ hotspot detection (described in Section 4.2.2) to select the N items in the model store. This
approach directs the initial filter will be applied to all incoming updates. All updates made to the intermediate store are
still forwarded to the model filter as described in Section 4.1,

Low values of α and β imply low incremental update latency as the
filters update fewer entries in the intermediate and model stores. On
the other hand, during query processing, the access methods must
service requests from the query processor by producing model values in the following order of efficiency: (1) directly from the model
store if the entry is maintained there. (2) If the entry is not maintained in the model store but maintained in the intermediate store,
the model value is produced on-demand from the intermediate store

90

AB

however, the model filter only accepts updates for the qualifying N objects, and their related objects, that are maintained in the model store. The query processing and storage/maintenance overhead for this approach lies between the
Materialize all and Intermediate Only approaches.

CB D
A

EF EF

B
C DEF
BCF DEF
C D F
BC D F
A
A C
A
AB
C DEF
DEA BC DEF
D
EA D
D A
D
D A
A
D!
A
A C
A
DA
DDB
A
A C
E
E D
!"#
A
D
D
"
C
# C
A
A
D
D
"
BC
# BC
A
A
C
# BC
A
A
D
A CB D AB
C DEF
DEA BC DEF
D
EA $%&'()*
E!
A CB D
DA
DDB
A
A C C $
E D
!"#
A
E
+
D
,-!
D#
.,-#
D
D !#
D
D B!/
D D
D.
D
D !#
D
D B!/
BC

• Partial Intermediate Store and Partial Model Store. This
approach sets α = K and β = N , and is similar to the Partial Model approach, except that we also partially materialize
the intermediate store. The model store still maintains data
for N objects, while the intermediate store maintains data
for K objects. These K and N objects are derived using
hotspot detection (described next in Section 4.2.2). This approach directs the initial filter only accepts incoming updates
for the K objects (items of users), and their related objects,
that qualify for storage in the intermediate store. The model
filter remains unchanged from the Partial Model approach.
The query processing and storage/maintenance overhead for
this approach lies between the Partial Model and Intermediate Only approaches.

Figure 5: Registering a recommendation method

Finally, we discuss how RecStore supports recommendation methods beyond “memory-based” collaborative filtering.

4.2.2 HotSpot Detection
For the approaches that use partial materialization, α and β should
ideally be set to ensure the maintenance of model hotspots, i.e.,
popular or frequently accessed entries. This setting assures efficient query processing over popular model entries, while sacrificing higher query latency for less popular model entries. We use two
methods to detect hotspots. (1) Most accessed. Keep the α and β
most accessed entries from the model determined by simple usage
statistics from the access methods. (2) Most rated. Keep the α and
β most popular entries in the model determined by association with
the most ratings (e.g., most-rated movies, users who rate the most
movies).

5.1 Registering a Recommender Method
We provide a syntax for registering a new recommender method
model within RecStore. Figure 5 gives an example for registering
the item-based cosine method. Registration begins by first defining the model name, and then providing a from and optional where
clause to specify the base data used in the model. For the itembased cosine model, the base data comes from the Ratings relation,
and the where-clause defines a relational constraint (in the form of
a self-join) declaring that model entries are (non-equal) items that
are co-rated by the same user. The major clauses are:

4.2.3 Load Shedding
For the special case of update-intense workloads where the system
is incapable of processing all ratings updates, RecStore is capable
of load-shedding. The goal of load-shedding is to process only updates that significantly alter the recommender model, thus changing
the answer to recommender queries. Load-shedding techniques are
model-specific, and RecStore executes these techniques in a special
filter before the intermediate filter.

• WITH INTERMEDIATE STORE: defines the data in the
intermediate store; in this case the intermediate statistics for
the item-based cosine method.
• WITH INTERMEDIATE FILTER: defines the intermediate filter in two parts. (1) Allow Updates With defines the
logic for filtering incoming updates (task 1 discussed in Section 4.1.1), currently contained in a user-defined function.
(2) Update defines how to compute data in the intermediate
store when given a rating update that is not filtered; the logic
can be given directly or contained in a user-defined function.

As an example, consider the item-based cosine method, where an
update should only be processed if it changes the order in the model
similarity lists. In this case, altered order in any similarity list can
potentially change the answer to a recommender query per Equation 3. An effective heuristic approach to achieve this goal is to process updates that change intermediate store entries with a co-rating
count (i.e., the statistic co(ip , iq )) below a pre-set threshold T . The
intuition here is that low co-rated items have less terms defining
their cosine distance (Equation (1)), thus an update will likely alter
the score significantly compared to more highly co-rated items. Of
course, more sophisticated statistical techniques can apply. However, any load-shedding approach should remain simple to evaluate
and maintain due to its mission-critical purpose.

5.

• WITH MODEL STORE: defines the name and schema of
the model store, this schema is exposed to the rest of the
DBMS and used by the recommender queries. Any attributes
computed from data in the intermediate store are given the
COMPUTED prefix. Our example item-based cosine follows
the schema discussed in Section 4.1.2, where the value sim is
a computed attribute.
• WITH MODEL FILTER: follows the same syntax as the
intermediate filter, with the exception that the compute clause
defines how to update the model store values using data from
the intermediate store.

RECSTORE EXTENSIBILITY

RecStore provides a generic extensible architecture capable of
supporting different recommendation methods. This section first
demonstrates how to register a preference method with RecStore.
We then provide various case studies demonstrating how RecStore
accommodates other item-based collaborative filtering methods.

5.2 Item-Based Collaborative Filtering
We now discuss RecStore registration for two other item-based collaborative filtering methods [31], namely probabilistic and Pearson

91

Intermediate
Store

Intermediate
Filter

Model Store
Model Filter

Probabilistic
len(ip ): partial vector length of for ip
freq(ip ): no. ratings for ip
sum(ip ,iq ): sum of scores for ip given co-rated item iq

Update sumq (ip ,iq ) only where user u co-rated ip and
iq , always update other statistics.
Update logic
sum(ip ,iq )=sum(ip ,iq )+sp ;
len(ip )=len(ip )+s2p ;
freq(ip )=freq(ip )+1

(ip ,iq ,sim(ip , iq ))
Update entry (ip ,iq ,sim(ip , iq )) for each statistical
update for pair (ip ,iq )
Update Logic
sim(ip , iq )= √

sumq (ip ,iq )
len(iq )∗f req(ip )∗(f req(iq ))α

Pearson
mean(ip ): mean rating score for ip
stddev(ip ): standard dev. for ip
freq(ip ): no. ratings for ip
sum(ip ): sum of ratings for ip
sumsq(ip ): sum or ratings squared for ip
coprodsum(ip ,iq ): sum of product deviation from mean for ip given co-rated dimension iq
Always update mean(ip ), stddev(ip ), freq(ip ), sum(ip ), sumsq(ip ).Only update coprodsum(ip ,iq )
if user u co-rated ip and iq , andmean(ip ) has not changed greater than ∆ since last
coprodsum(ip ,iq )recalculation.
Update logic
s

(f req(ip )−1)mean(ip )
;
f req(ip )
sum(ip )=sum(ip )+sp ; sumsq(ip )=sumsq(ip )+s2p ;
q
f req(ip )∗sumsq(ip )−sum(ip )2
;
stddev(ip )=
f req(ip )
p
freq(ip )=freq(ip )+1; mean(ip )= f req(i
+
p)

coprodsum(ip ,iq )=coprodsum(ip ,iq )+(sp − mean(ip ))(sq − mean(ip ))
(ip ,iq ,sim(ip , iq ))
Update entry for each (ip ,iq ,sim(ip , iq )) for each statistical update affecting pair (ip ,iq ). Completely recalculate coprodsum(ip ,iq ) if mean(ip ) has changed greater than threshold ∆.
Update Logic
coprodsum(i ,i )

p q
If mean(ip ) has changed less than ∆, sim(ip , iq )= stddev(ip )stddev(i
, otherwise
q)
P
coprodsum(ip ,iq )= u∈U (sp −mean(ip ))(sq −mean(ip ))
c
sim(ip , iq )=
stddev(ip )stddev(iq )

Table 1: Realizing probabilistic and Pearson item-based collaborative filtering methods in RecStore, summary of implementation
approach assuming new rating by user u for item ip with score sp
The third column of Table 1 provides an approach to implementing
the Pearson method in RecStore. The intermediate store maintains
for an item ip its mean rating value for an item (mean(ip )), its standard deviation of rating values (stddev(ip )), the total number of
ratings for ip (freq(ip )), the sum of ratings for ip (sum(ip )), and the
sum of the squared rating values for ip (sumsq(ip )). The intermediate store also maintains coprodsum(ip ,iq ): the sum of the product
of deviations from the mean (i.e., the numerator in Equation 6) for
an item pair (ip ,iq ) given that they share at least one co-rated dimension. The intermediate filter updates all single-item statistics
(those maintained for ip only). The statistic coprodsum(ip ,iq ) is
incremented by the product of the deviation of user u’s score for
ip (i.e., sp ) from the newly calculated mean(ip ), and the deviation
of iq (i.e., sq ) from the stored mean for item iq (mean(iq )). Note
that previous rating scores for ip in the sum deviated from different
means, since mean(ip ) changed with this update. In essence, we
are willing to forgo this difference in accuracy as long as mean(ip )
has not changed by at least a value ∆ since the last calculation of
coprodsum(ip ,iq ). What we gain in this trade-off is efficiency, since
updating coprodsum(ip ,iq ) is more efficient than recalculating the
sum from scratch.

item-based recommenders. We demonstrate each use case assuming a user u has provided a new rating value sp for an item ip .
Item-based probabilistic recommender. This method is similar
to our running example of the item-based cosine recommender, except the similarity score sim(ip , iq ) is measured as the conditional
probability between two items ip and iq as follows.
P
u∈Uc ru,iq
(5)
sim(ip , iq ) =
F req(ip ) × (F req(iq ))α
Here, ru,iq represents a rating for item iq normalized to unit-length,
F req(i) represents the number of non-zero ratings for item i, and
α is a scaling factor [17].
The second column of Table 1 provides an approach to implementing the item-based probabilistic method in RecStore. The intermediate store contains (1) the partial vector length for item iq (len(iq )),
(2) the total number of ratings for ip (freq(ip )), and (3) the itempair statistic maintains the running sum ratings for item ip given
that it is co-rated with an item iq (sum(ip ,iq )). The intermediate
filter updates all single-item statistics, while only updating the pair
statistic for which items ip and iq are both rated by user u. Each
statistic update requires constant time. The model filter, upon receiving changes to the intermediate statistics, updates the similarity
score sim(ip ,iq ) for pairs ip ,iq in constant time using the intermediate statistics (equation given in the last row, second column of
Table 1).

The model filter updates the similarity score sim(ip ,iq ) for pairs
ip ,iq in the model store using one of two methods (both given in the
last row, third column of Table 1). (1) If the value mean(ip ) had not
changed by ∆ since the last recalculation of coprodsum(ip ,iq ), then
we can update sim(ip ,iq ) efficiently by dividing coprodsum(ip ,iq )
by the product of the standard deviations. Otherwise, we must recalculate the value of coprodsum(ip ,iq ) from scratch for each entry
using the current value of mean(ip ).

Item-based Pearson recommender. This method is similar to the
item-based cosine method, except it measures the similarity between objects using their Pearson correlation coefficient as follows.
P
u∈Uc (Ru,ip − Rip )(Ru,iq − Riq )
(6)
sim(ip , iq ) =
σip σiq

5.3 User-based Collaborative Filtering
The model for user-based collaborative filtering [29] is similar to
the item-based approach, except that the model stores groups of
similar users (as described in Section 2). Thus, the use cases previously discussed for the item-based approach can apply directly to
the user-based approach, with the exception that similarity is measured over user vectors in the item rating space.

Uc represents users who co-rated items ip and iq , Ru,ip and Ru,iq
represent a user’s ratings, and Rip and Riq represent the average
rating for items ip and iq , respectively. σip and σiq are the standard
deviations for ip and iq

92

accessed

5
4
3
2
1
0

Time (sec)

Time (sec)

Number of Updates

(a) Cosine

2.5K

4.5K

7K

Number of Updates

Figure 7: Update Efficiency

5.4 Non-“Memory-Based” Collaborative
Filtering within RecStore

(b) Update

β = M , intermediate only (ionly) where α=M and β = 0 , partial model hotspot maintenance where α=M and β is set to 20%
of all movies (pm-m), and partial intermediate and model hotspot
maintenance (pm-mi) where α and β are set to 40% and 20% of
all movies. We also compare against regular (viewreg) and materialized DBMS views (viewmat). The viewreg approach is implemented using a regular PostgreSQL view, but since PostgreSQL
does not support materialized views, we provide a fair simulation
of viewmat within RecStore by maintaining a materialized Model
store without the use of an intermediate store.

Many other recommendation methods use models that are not
similarity-based lists, as is the case with the “memory-based" collaborative filtering techniques we have explored. In general, RecStore can support these different recommendation techniques as
long as their models can be represented by sufficient statistics to
update the model incrementally. For instance, recommendation
methods that use sophisticated probabilistic models (e.g., Bayesian
Networks [3], Markov decision processes [32]) do not lend themselves well to incremental updates, due to the computationally intense optimization process used to learn their parameters. On the
other hand, methods that use linear regression to learn a rating prediction model [31] can fit easily within RecStore. In this case,
the intermediate store can maintain the general linear model statistics: X (the regression design matrix), X T (X transposed) and
f (the regressand). It is known that these statistics are incrementally updatable and sufficient to learn unknown regression coefficients by solving the system of equations [11]: X T Xβ = X T f ,
where β represents the learned regression coefficients. The source
for these statistics depends on the recommendation method. Examples include ratings vectors [31], a multi-dimensional ratings
base (e.g., multi-dimensional recommenders [2]), or item attributes
(e.g., content-based recommenders [7]).

6.

viewreg
viewmat
pm-mi
pm-m
ionly
matall

2

0
500

7K

)

4.5K

4

b
ro
i(p
-m
pm
s)

)

6

viewreg
viewmat
pm-mi
pm-m
ionly
matall

2.5K

(b) Update

Figure 6: Hotspot Detection

2

0
500

o
i(c
-m
pm

b
ro
(p
-m
pm
s)

4

o
(c
-m
pm

b)
ro
i(p
-m
pm
)
os
i(c
-m
pm
b)
ro
(p
-m
pm
s)

o
(c
-m
pm

(a) Query
6

accessed

rated
6
Time (sec)

Time (sec)

rated
50
45
40
35
30
25
20
15
10
5
0

We provide experiments for: (1) Partial maintenance strategies
(Section 6.1), (2) update efficiency (Section 6.2), (3) query efficiency using the query given in Figure 2 (Section 6.3), and (4) a
real recommender system workload trace consisting of interleaved
queries and updates (Section 6.4). Each experiment is run for both
the cosine and probabilistic item-based recommendation method
(details of both methods given in Section 5).
The experiment machine is an Intel Core2 8400 at 3Ghz with 4GB
of RAM running Ubuntu Linux 8.04. Our performance metric is
the elapsed time over an average of five runs reported by the PostgreSQL EXPLAIN ANALYZE command.

6.1 Hotspot Detection Strategies

EXPERIMENTAL EVALUATION

This experiment studies the effectiveness of our two hotspot detection strategies covered in Section 4.2: most-rated (abbr. rated)
and most-accessed (abbr. accessed). We use a real workload trace
consisting of the continuous arrival of both ratings updates and recommender queries against the MovieLens system [23, 26]. We
start with a Ratings table that already contains 950K ratings, and
report the total time necessary to process 1K ratings updates in-

This section experimentally evaluates the performance of a prototype of RecStore implemented in between the storage engine and
query processor of the PostgreSQL 8.4 database system [28] using
the real-world Movielens 10M rating data set [27]. We test various
RecStore adaptive maintenance strategies based on α and β proposed in Section 4.2.1: materialize all (abbr. matall) where α =

93

2

matall
ionly
pm-m

Time (sec)

Time (sec)

20
matall
18
ionly
16
pm-m
14
pm-mi
12 viewreg
10
8
6
4
2
0
5K
25K

45K

0
5K

70K

Number of Ratings

25K

45K

70K

Number of Ratings

(a) Cosine

(b) Cosine Zoomed

20
18
16
14
12
10
8
6
4
2
0
5K

2

matall
ionly
pm-m
pm-mi
viewreg

25K

matall
ionly
pm-m

Time (sec)

Time (sec)

Figure 8: Query Efficiency

45K

0
5K

70K

Number of Ratings

25K

45K

70K

Number of Ratings

(a) Probabilistic

(b) Probabilistic Zoomed
Figure 9: Query Efficiency

6.3 Query Efficiency

terleaved with 40 recommendation generation queries for different
users. Figures 6(a) and 6(b) report performance for rated and accessed using both the pm-mi and pm-m approaches implementing
the cosine and probabilistic methods. The update performance is
relatively similar between the rated and accessed strategies for all
cases. However, the query performance of rated over accessed exhibits a 50% speedup, as rated is able to keep model data in the intermediate and model store requested by the recommendation generation queries. Thus, in the rest of this section, we employ the
rated strategy for both pm-mi and pm-m.

This experiment studies query efficiency and scalability. We measure the time to perform the recommender query given in Figure 2
for a user X as the number of tuples in the Ratings table increases
from 5K to 70K. We choose user X as the user that has rated the
most movies. Figures 8(a) and 9(a) give the results for the cosine and probabilistic recommendation methods, respectively. The
viewreg approach (a regular DBMS view) performs very poorly,
as it must calculate all requested model scores from scratch from
the ratings relation. The pm-mi approach exhibits performance between matnone and the rest of the approaches, as it must service
a fraction of its requests from the ratings data, similar to viewreg.
Figures 8(b) and 9(b) zoom in on the matall, ionly, and pm-m approaches for the cosine and probabilistic models, respectively. As
expected, the matall approach exhibits the best query processing
performance as it must only retrieve values from the model store.
Both ionly and pm-m exhibit close performance to matall. We
do not plot the viewmat, since it exhibits the same performance
as matall, as the query operates over a completely materialized
model relation for both approach. Due to both query and update
performance, we can conclude that RecStore provides better support for online recommender systems compared to existing DBMS
approaches (viewmat and viewreg).

6.2 Update Efficiency
This experiment studies update efficiency and scalability. We start
with a Ratings table already containing 950K rating tuples, and
measure the total time it takes to process 500, 2.5K, 4.5K, and 7K
updates, respectively. Figures 7(a) and 7(b) give the results for the
cosine and probabilistic methods, respectively. For both methods,
all approaches exhibit the same relative performance. The materialized view (viewmat) incurs the most overhead of all approaches.
This performance is due to the need, on every update, to recalculate the model score from scratch using the ratings data. The RecStore matall strategy, on the other hand, incurs less update overhead compared to viewmat due to its intermediate store, that helps
it to efficiently update the model store. This experiment confirms
that RecStore overcomes the update efficiency drawback of materialized views. Both ionly and pm-mi exhibit better performance,
with ionly doing slightly better due to not having to maintain a
partial model store. Both matnone and pm-mi exhibit the best performance due to the low storage and maintenance costs.

6.4 Update + Query Workload
This experiment uses our real recommender system workload trace
(described in Section 6.1) to test comprehensive update and query
processing performance. Figures 10(a) and 11(a) give the results
of both query and updates for the cosine and probabilistic methods, respectively. Both viewreg and pm-mi exhibit poor query pro-

94

ionly
pm-m

pm-mi
viewreg

90
80
70
60
50
40
30
20
10
0
query

matall
viewmat

ionly
pm-m

query

Figure 10: Real Workload
pm-mi
viewreg

Time (sec)
query

(a) Probabilistic

ionly
pm-m

8
7
6
5
4
3
2
1
0

updates

query

Figure 11: Real Workload

updates

(b) Probabilistic Zoomed

provide recommendations [1, 3]. The scope of most work within
collaborative filtering systems has been from a user-centric perspective, e.g., providing users with quality [15, 22] and trustworthy recommendations [25]. Other work has explored high-level
approaches to memory-based collaborative filtering (e.g., userbased [18, 29], item-based [17, 31], hybrid [4]) and their effect on
recommendation quality. There is a scarcity of work that studies
recommenders from a systems perspective, i.e., measuring query
processing efficiency of different architectures. Herlocker et al. in
their 2004 detailed evaluation of recommender systems state [15]:

cessing performance for the workload, with viewreg performing
almost an order magnitude worse than other approaches. While
the viewreg performance is expected, the pm-mi performance is
more surprising. Both viewreg and pm-mi exhibit the best update
performance out of all approaches, as confirmed by our previous
experiments (Section 6.2). However, the query processing performance of viewreg makes it an unattractive alternative, while the
update/query processing tradeoff for pm-mi is a borderline choice
due to its high query processing penalty. Figures 10(b) and 11(b)
remove the viewreg and pm-mi numbers to zoom in on the other
approaches for the cosine and probabilistic models, respectively.
Both the matall and viewmat approaches exhibit the same query
processing performance that is superior to ionly and pm-m. As
for updates, we note again that matall (RecStore) provides more
efficient update performance over viewmat (materialized views).
Meanwhile, pm-m and ionly show superior update performance to
matall and viewmat, with ionly providing the best performance.

“We have chosen not to discuss computation performance of recommender algorithms. Such performance is certainly important,
and in the future we expect there to be work on the quality of timelimited and memory-limited recommendations.”
To date, the research community is still lacking such important
work on recommender system performance. Further, very little
work has suggested a systems solution to online recommender systems. An exception is the Google News recommender [6]. However, this work is specific to the recommendation methods created
specifically for Google’s click logs. In contrast, our work takes a
more generic approach to online recommender systems by offering
a generic and extensible framework within the DBMS engine that
accommodates various recommendation methods.

In this experiment, we can observe the update/query processing
trade-off discussed in Section 4.2.1 for high values of α and β
(matall) compared to lower values of α and β (ionly and pmmi). Thus, for slightly more update-heavy recommender systems,
the ionly or pm-mi is preferable due to efficient updates with little query processing penalty. Meanwhile, for more query-heavy
systems, the matall approach is preferable with tolerable update
penalty.

7.

updates

(b) Cosine Zoomed
matall
viewmat

70
60
50
40
30
20
10
0

ionly
pm-m

7
6
5
4
3
2
1
0

updates

(a) Cosine

Time (sec)

matall
viewmat

Time (sec)

Time (sec)

matall
viewmat

DBMS and Recommender Systems. Little systems research has
addressed the intersection of database and recommender systems
(as asserted by [16]). The AWESOME system [33] suggests recommendation methods to use based on the characteristics of the
data stored in a database. Closer to our work is FlexRecs [19],

RELATED WORK

Collaborative Filtering. The term collaborative filtering has a
broad definition [1, 3]. We mainly focus on the original memorybased collaborative filtering approach [18, 29], so called because it
“remembers" the ratings history of the entire user/item spectrum to

95

that studies a flexible model and workflow for expressing a number
of recommendation methods. FlexRecs compiles its workflow into
a series of conventional SQL queries to execute the recommendation process. This work shows that implementation of many different recommendations methods is possible within a DBMS. While
FlexRecs addresses the implementation of recommendation logic
using a DBMS, it still assumes the model building phase for many
recommendation methods is performed offline. Our work addresses
online model support for DBMS-based recommender queries.

Network Analysis. In WWW, 2008.
[8] Digg: http://digg.com.
[9] Facebook: http://www.facebook.com.
[10] Facebook turns on its ’Like’ button:
http://news.cnet.com/8301-1023_3-10160112-93.html.
[11] G. H. Golub and C. F. V. Loan. Matrix Computations. Johns
Hopkins, 1989.
[12] Google Reader: www.google.com/reader.
[13] A. Gupta and I. S. Mumick. Materialized Views: Techniques,
Implementations, and Applications. MIT Press, 1999.
[14] J. L. Herlocker, J. A. Konstan, A. Borchers, and J. Riedl. An
Algorithmic Framework for Performing Collaborative
Filtering. In SIGIR, 1999.
[15] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl.
Evaluating Collaborative Filtering Recommender Systems.
TOIS, 22(1), 2004.
[16] Y. E. Ioannidis and G. Koutrika. Personalized Systems:
Models and Methods from an IR and DB Perspective. In
VLDB, 2005.
[17] G. Karypis. Evaluation of Item-Based Top-N
Recommendation Algorithms. In CIKM, 2001.
[18] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R.
Gordon, and J. Riedl. GroupLens: Applying Collaborative
Filtering to Usenet News. Commun. ACM, 40(3), 1997.
[19] G. Koutrika, B. Bercovitz, and H. Garcia-Molina. FlexRecs:
Expressing and Combining Flexible Recommendations. In
SIGMOD, 2009.
[20] B. Krulwich. Lifestyle Finder: Intelligent User Profiling
Using Large-Scale Demographic Data. Artificial Intelligence
Magazing, 18(2), 1997.
[21] G. Linden, B. Smith, and J. York. Amazon.com
Recommendations: Item-to-Item Collaborative Filtering.
IEEE Internet Computing, 7(1), 2003.
[22] S. M. McNee, J. Riedl, and J. A. Konstan. Making
recommendations better: an analytic model for
human-recommender interaction. In CHI, 2006.
[23] B. N. Miller, I. Alber, S. K. Lam, J. A. Konstan, and J. Riedl.
MovieLens Unplugged: Experiences with an Occasionally
Connected Recommender System. In IUI, 2002.
[24] B. N. Miller, J. A. Konstan, and J. Riedl. PocketLens:
Toward a Personal Recommender System. TOIS, 22(3),
2004.
[25] B. Mobasher et al. Toward trustworthy recommender
systems: An analysis of attack models and algorithm
robustness. ACM TOIT, 7(4), 2007.
[26] MovieLens: http://www.movielens.org.
[27] Movielens Datasets: http://www.grouplens.org/node/73.
[28] PostgreSQL: http://www.postgresql.org.
[29] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and
J. Riedl. GroupLens: An Open Architecture for
Collaborative Filtering of Netnews. In CSWC, 1994.
[30] P. Resnick and H. R. Varian. Recommender Systems.
Commun. ACM, 40(3), 1997.
[31] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-Based
Collaborative Filtering Recommendation Algorithms. In
WWW, 2001.
[32] G. Shani, R. I. Brafman, and D. Heckerman. An MDP-Based
Recommender System. In UAI, 2002.
[33] A. Thor and E. Rahm. AWESOME - A Data
Warehouse-based System for Adaptive Website
Recommendations. In VLDB, 2004.

Database Views. We can employ DBMS views as a solution to
online collaborative filtering. Views are a fundamental topic within
the data management research community, with a rich volume of
research addressing various view aspects, including, but not limited
to, view composition, materialized view maintenance, and query
processing using views [13]. Views provide a general solution to a
wide range of data management problems, including security (i.e.,
data access restriction), transparency from a physical schema, and
ease-of-use. In this paper, we study the specific data management
problem of online maintenance of recommender models, for which
DBMS views incur serious efficiency drawbacks.

8.

CONCLUSION

This paper presented RecStore, an extensible and adaptive DBMS
storage engine module that provides online support for recommender queries. We fist presented the generic architecture of RecStore, and then described how RecStore supports online recommender model maintenance by enabling fast incremental updates
to the model by implementing an intermediate and model store.
We described how RecStore adapts to various system workloads,
and provides load-shedding support for update-intense workloads.
We then demonstrated the extensibility of RecStore by presenting a declarative model registration language, and provided casestudies showing how RecStore accommodates various recommendation methods. Using a real recommender system workload and a
system prototype of RecStore inside PostgreSQL, our experimental
results show that RecStore provides superior performance to existing DBMS view approaches to support online recommender systems. Further, the experiments also confirm that RecStore is indeed adaptive to update-heavy or query-heavy recommender system workloads.

9.

REFERENCES

[1] G. Adomavicius and A. Tuzhilin. Toward the Next
Generation of Recommender Systems: A Survey of the
State-of-the-Art and Possible Extensions. TKDE, 17(6),
2005.
[2] G. Adomavicius et al. Incorporating Contextual Information
in Recommender Systems Using a Multidimensional
Approach. TOIS, 23(1), 2005.
[3] J. S. Breese, D. Heckerman, and C. Kadie. Epirical Analysis
of Predictive Algorithms for Collaborative Filtering. In UAI,
1998.
[4] R. Burke. Hybrid Recommender Systems: Survey and
Experiments. User Modeling and User-Adapted Interaction,
12(4), 1997.
[5] CoFE Recommender System:
http://eecs.oregonstate.edu/iis/CoFE.
[6] A. Das et al. Google News Personalization: Scalable Online
Collaborative Filtering. In WWW, 2007.
[7] S. Debnath, N. Ganguly, and P. Mitra. Feature Weighting in
Content Based Recommendation System Using Social

96

Geoinformatica (2013) 17:417–448
DOI 10.1007/s10707-012-0164-9

Generic and efficient framework for search trees
on flash memory storage systems
Mohamed Sarwat · Mohamed F. Mokbel ·
Xun Zhou · Suman Nath

Received: 16 February 2012 / Revised: 27 June 2012 /
Accepted: 12 July 2012 / Published online: 30 August 2012
© Springer Science+Business Media, LLC 2012

Abstract Tree index structures are crucial components in data management systems.
Existing tree index structure are designed with the implicit assumption that the
underlying external memory storage is the conventional magnetic hard disk drives.
This assumption is going to be invalid soon, as flash memory storage is increasingly
adopted as the main storage media in mobile devices, digital cameras, embedded
sensors, and notebooks. Though it is direct and simple to port existing tree index
structures on the flash memory storage, that direct approach does not consider the
unique characteristics of flash memory, i.e., slow write operations, and erase-beforeupdate property, which would result in a sub optimal performance. In this paper, we
introduce FAST (i.e., Flash-Aware Search Trees) as a generic framework for flashaware tree index structures. FAST distinguishes itself from all previous attempts of
flash memory indexing in two aspects: (1) FAST is a generic framework that can
be applied to a wide class of data partitioning tree structures including R-tree and
its variants, and (2) FAST achieves both ef f iciency and durability of read and write
flash operations through memory flushing and crash recovery techniques. Extensive
experimental results, based on an actual implementation of FAST inside the GiST

The research of M. Sarwat and M. F. Mokbel is supported in part by the National Science
Foundation under Grants IIS-0811998, IIS-0811935, CNS-0708604, IIS-0952977,
by a Microsoft Research Gift, and by a seed grant from UMN DTC.
M. Sarwat ( ) · M. F. Mokbel · X. Zhou
Department of Computer Science and Engineering, University of Minnesota - Twin Cities,
200 SE Union Street, Minneapolis, MN 55455, USA
e-mail: sarwat@cs.umn.edu
M. F. Mokbel
e-mail: mokbel@cs.umn.edu
X. Zhou
e-mail: xun@cs.umn.edu
S. Nath
Microsoft Research, One Microsoft Way - Redmond, Redmond, WA 98052, USA
e-mail: sumann@microsoft.com

418

Geoinformatica (2013) 17:417–448

index structure in PostgreSQL, show that FAST achieves better performance than
its competitors.
Keywords Flash memory · Tree · Spatial · Index structure · Storage ·
Multi-dimensional · Data · System

1 Introduction
Data partitioning tree index structures are crucial components in spatial data
management systems, as they are mainly used for efficient spatial data retrieval,
hence boosting up query performance. The most common examples of such index
structures include B-tree [4], with its variants [10, 27], for one-dimensional indexing,
and R-tree [14], with its variants [5, 17, 32, 34], for multi-dimensional indexing. Data
partitioning tree index structures are designed with the implicit assumption that the
underlying external memory storage is the conventional magnetic hard disk drives,
and thus has to account for the mechanical disk movement and its seek and rotational
delay costs. This assumption is going to be invalid soon, as flash memory storage is
expected to soon prevail in the storage market replacing the magnetic hard disks
for many applications [11, 12, 31]. Flash memory storage is increasingly adopted as
the main storage media in mobile devices and as a storage alternative in laptops,
desktops, and enterprise class servers (e.g., in forms of SSDs) [3, 21, 23, 28, 33].
Recently, several data-intensive applications have started using custom flash cards
(e.g., ReMix [19]) with large capacity and access to underlying raw flash chips. Such
a popularity of flash is mainly due to its superior characteristics that include smaller
size, lighter weight, lower power consumption, shock resistance, lower noise, and
faster read performance [16, 18, 20, 22, 29].
Flash memory is block-oriented, i.e., pages are clustered into a set of blocks. Thus,
it has fundamentally different characteristics, compared to the conventional pageoriented magnetic disks, especially for the write operations. First, write operations in
flash are slower than read operations. Second, random writes are substantially slower
than sequential writes. In devices that allow direct access to flash chips (e.g., ReMix
[19]), a random write operation updates the contents of an already written part of the
block, which requires an expensive block erase operation,1 followed by a sequential
write operation on the erased block; an operation termed as erase-before-update [7,
20]. SSDs, which emulate a disk-like interface with a Flash Translation Layer (FTL),
also need to internally address flash’s erase-before-update property with logging and
garbage collection, and hence random writes, especially small random writes, are
significantly slower than sequential writes in almost all SSDs [7].
Though it is direct and simple to port existing tree index structures (e.g., R-tree
and B-tree) on FTL-equipped flash devices (e.g., SSDs), that direct approach does
not consider the unique characteristics of flash memory and therefore would result
in a sub-optimal performance due to the random writes encountered by these index
structures. To remedy this situation, several approaches have been proposed for

a typical flash memory, the cost of read, write, and erase operations are 25, 200 and 1,500 µs,
respectively [3].

1 In

Geoinformatica (2013) 17:417–448

419

flash-aware index structures that either focus on a specific index structure, and make
it a flash-aware, e.g., flash-aware B-tree [30, 36] and R-tree [35], or design brand new
index structures specific to the flash storage [2, 24, 25].
Unfortunately, previous works on flash-aware search trees suffer from two
major limitations. First, these trees are specialized—they are not flexible enough
to support new data types or new ways of partitioning and searching data. For
example, FlashDB [30], which is designed to use a B-Tree, does not support R-Tree
functionalities. RFTL [35] is designed to work with R-tree, and does not support
B-tree functionalities. Thus, if a system needs to support many applications with
diverse data partitioning and searching requirements, it needs to have multiple tree
data structures. The effort required to implement and maintain multiple such data
structures is high.
Second, existing flash-aware designs often show trade-offs between efficiency and
durability. Many designs sacrifice strict durability guarantee to achieve efficiency
[24, 25, 30, 35, 36]. They buffer updates in memory and flush them in batches to
amortize the cost of random writes. Such buffering poses the risk that in-memory
updates may be lost if the system crashes. On the other hand, several designs achieve
strict durability by writing (in a sequential log) all updates to flash [2]. However, this
increases the cost of search for many log entries that need to be read from flash
in order to access each tree node [30]. In summary, no existing flash-aware tree
structure achieves both strict durability and efficiency.
In this paper, we address the above two limitations by introducing FAST; a
framework for Flash-Aware Search Tree index structures. FAST distinguishes itself
from all previous flash-aware approaches in two main aspects: (1) Rather than
focusing on a specific index structure or building a new index structure, FAST is
a generic framework that can be applied to a wide variety of tree index structures,
including B-tree, R-tree along with their variants. Such an important property makes
FAST a very attractive solution to database industry as it is practical to port it inside
the database engine with minimal disturbance to the engine code. (2) FAST achieves
both efficiency and durability in the same design. For efficiency, FAST buffers all
the incoming updates in memory while employing an intelligent f lushing policy that
evicts selected updates from memory to minimize the cost of writing to the flash
storage. In the mean time, FAST guarantees durability by sequentially logging each
in-memory update and by employing an efficient crash recovery technique.
FAST mainly has four modules, update, search, f lushing, and recovery. The update
module is responsible on buffering incoming tree updates in an in-memory data
structure, while writing small entries sequentially in a designated flash-resident log
file. The search module retrieves requested data from the flash storage and updates
it with recent updates stored in memory, if any. The f lushing module is triggered
once the memory is full and is responsible on evicting flash blocks from memory to
the flash storage to give space for incoming updates. Finally, the recovery module
ensures the durability of in-memory updates in case of a system crash.
FAST is a generic system approach that neither changes the structure of tree
indexes it is applied to, nor changes the search, insert, delete, or update algorithms
of these indexes. FAST only changes the way these algorithms reads, or updates the
tree nodes in order to make the index structure flash-aware. We have implemented
FAST within the GiST framework [15] inside PostgrSQL. As GiST is a generalized
index structure, FAST can support any tree index structure that GiST is supporting,

420

Geoinformatica (2013) 17:417–448

including one-dimensional tree index structures (e.g., B-tree [4]) and including but
not restricted to R-tree [14], R*-tree [5], SS-tree [34], and SR-tree [17], as well as Btree and its variants. In summary, the contributions of this paper can be summarized
as follows:
–
–

–

We introduce FAST; a general framework that adapts existing tree index structures to consider and exploit the unique properties of the flash memory storage.
We show how to achieve efficiency and durability in the same design. For
efficiency, we introduce two f lushing policies that smartly select parts of the main
memory buffer to be flushed into the flash storage in a way that amortizes expensive random write operations. We also introduce a crash recovery technique that
ensures the durability of update transactions in case of system crash.
We give experimental evidence for generality, efficiency, and durability of FAST
framework when applied to different data partitioning tree index structures.

The rest of the paper is organized as follows: An overview of Flash Memory
storage is given in Section 2. Section 3 highlights related work to FAST. Section 4
gives an overview of FAST along with its data structure. The four modules of
FAST, namely, update, search, f lushing, and recovery are discussed in Sections 5–
8, respectively. Section 9 gives experimental results. Finally, Section 10 concludes
the paper.

2 Flash memory storage overview
Figure 1 gives an overview of a typical flash memory storage device. In flash memory,
data is stored in an array of flash blocks. Each block contains ≈64–128 pages,
where a page is the smallest unit of access. Flash memory supports three types of
operations: read, write, and erase. The Erase operation is the most expensive one
where it can only be done at the block level and results in setting of all bits within
a block to ones. Read is a low latency page level operation and can occur randomly
anywhere in the flash memory without incurring any additional cost. Write is also a
page level operation and can be performed only once a page has been previously
erased since it sets the required bits to zeros. In that sense, writing on a previously
erased block is a low latency operation, and termed as a sequential write, while
writing on an already written block will result in a block erase operation before the
actual write operation, and thus, would incur much higher cost. Current generation
flash memory-based storage devices have varying access latencies for each of these
operations. On average, compared to read operations, write operations are eight
times slower, while erase operations are 60 times slower [6]. The typical access
latencies for read, write, and erase operations in flash memory devices are 25, 200
and 1,500 µs, respectively [3].
The Flash Translation layer (FTL) [26] is a layer on top of NAND flash memory
that makes the flash memory device acts like a virtual disk. The FTL layer receives
read and write commands for logical pages addresses from the application layer and
converts them to the internal flash memory commands (i.e., read page, write page,
erase block) on physical pages/blocks addresses. To emulate disk like in-place update
operation for a logical page Plogical , the FTL writes data into a new physical page
Pphysical, maintains a mapping between logical pages and physical pages, and marks

Geoinformatica (2013) 17:417–448
Fig. 1 Flash memory storage.
Grey rectangles represent
pages that are contained in
blocks represented by dotted
rectangles

421
Logical Page Read

Logical Page Write

Flash Translation Layer (FTL)
Page Read

Block Erase

Page Write

NAND Flash Memory

the previous physical location of Pphysical as invalid for future garbage collection.
Even though FTL allows existing disk based applications to use flash memory
without any modiÞcations, it needs to internally deal with flash physical constraint
of erasing a block before updating a page in that block. Besides this asymmetric read
and write latency issue, a flash memory block can only be erased for a limited number
of times (e.g., 105 –106 ), after which it acts like a read-only device [3]. FTL employs
various wear-leveling techniques to even out the erase counts of different blocks in
the flash memory to increase its longevity [8]. However, still early wear-out of flash
memory is one of the big concerns in widely deploying flash memory storage devices
[31], and thus, it is of essence that flash memory avoids block erases as much as
possible. Recent studies show that current FTL schemes are very effective for the
workloads with sequential access write patterns. However, for the workloads with
random access patterns, these schemes show very poor performance [9]

3 Related work
Previous approaches for flash-aware index structures can be classified into two categories: (1) Making an existing specif ic index structure flash-aware, which includes
flash-aware B-tree (e.g., FlashDB [30] and BFTL [36]) and flash-aware R-tree (e.g.,
RFTL [35]). The main idea of these index structures is to save the B-tree (R-tree)
operations in a reservation buffer residing on main memory. When the reservation
buffer is full, its content is totally flushed to flash memory. For instance, BFTL and
RFTL are adding a buffering layer on top of the flash translation layer in order
to make B-trees work efficiently on flash devices. (2) Designing brand new onedimensional index structures specific to the flash storage, e.g., the LA-tree [2] and the
FD-tree [24, 25]. LA-tree is flash friendly index structure that is intended to replace
the B-tree. LA-tree stores the updates in cascaded buffers residing on flash memory
and, then empties these buffers dynamically based on the operations workload. FDtree is also a one-dimensional index structure that allows small random writes to

422

Geoinformatica (2013) 17:417–448

occur only in a small portion of the tree called the head tree which exists at the
top level of the tree. When the capacity of the head tree is exceeded, its entries are
merged in batches to subsequent tree levels.
In terms of the performance-durability trade-off, previous approaches either:
(a) achieve efficiency, yet sacrifice durability, by buffering updates in main memory
and flush them in batches to flash memory to amortize the cost of random writes
[24, 25, 30, 35, 36]. However, storing updates in memory without taking into account
system failures, which leads to durability issue, where in-memory updates may be
lost if the system crashes, or (b) achieve durability, yet sacrifice efficiency, by writing
all the recent updates in a sequential log file [2], hence retrieving the updates from
the log file in case of a system crash. However, doing this increases the cost of search
for many log entries that need to be read from flash in order to access each tree node
with search and update operations [30].
FAST distinguishes itself from all previous techniques in three main aspects:
(1) FAST is a general framework for data-partitioning tree index structures built
inside GiST [15]. As GiST is a generalized index structure that can instantiate a wide
set of data-partitioning trees that include B-tree [4], R-tree [14], R*-tree [5], SS-tree
[34], and SR-tree [17]), FAST can support any tree that GiST is supporting. (2) FAST
ensures both the ef f iciency and durability of system transactions where updates are
buffered in memory, yet, an efficient crash recovery technique is triggered in case of
a system crash to ensure the durability. (3) FAST is not a brand new index structure,
hence does not need to replace existing tree indexes. However, it complements the
existing tree index structures in database management systems to make them work
efficiently on flash storage devices, with much less implementation cost.

4 Fast system overview
Figure 2 gives an overview of FAST. The original tree is stored on persistent flash
memory storage while recent updates are stored in an in-memory buffer. Both parts
need to be combined together to get the most recent version of the tree structure.
FAST has four main modules, depicted in bold rectangles, namely, update, search,
flushing, and crash recovery. FAST is optimized for both SSDs and raw flash devices.
SSDs are the dominant flash device for large database applications. On the other
hand, raw flash chips, which are dominant in embedded systems and custom flash
cards (e.g., ReMix [19]), are getting popular for data-intensive applications.
4.1 FAST modules
In this section, we explain FAST system architecture, along with its four main
modules; (1) Update, (2) Search, (3) Flushing, and (4) Crash recovery. The actions of
these four modules are triggered through three main events, namely, search queries,
data updates, and system restart.
Update module Similar to some of the previous research for indexing in flash
memory, FAST buffers its recent updates in memory, and flushes them later, in
bulk, to the persistent flash storage. However, FAST update module distinguishes
itself from previous research in two main aspects: (1) FAST does not store the

Geoinformatica (2013) 17:417–448

423

Fig. 2 FAST system architecture

update operations in memory, instead, it stores the results of the update operations in
memory, and (2) FAST ensures the durability of update operations by writing small
log entries to the persistent storage. These log entries are written sequentially to the
flash storage, i.e., very small overhead. Details of the update module will be discussed
in Section 5.
Search module The search module in FAST answers point and range queries that
can be imposed to the underlying tree structure. The main challenge in the search
module is that the actual tree structure is split between the flash storage and the
memory. Thus, the main responsibility of the search module is to construct the recent
image of the tree by integrating the stored tree in flash with the tree updates in
memory that did not make it to the flash storage yet. Details of the search module
will be discussed in Section 6.
Flushing module As the memory resource is limited, it will be filled up with the
recent tree updates. In this case, FAST triggers its flushing module that employs a
f lushing policy to select some of the in-memory updates and write them, in bulk,
into the flash storage. Previous research in flash indexing flush their in-memory
updates or log file entries by writing all the memory or log updates once to the flash
storage. In contrast, the flushing module in FAST distinguishes itself from previous
techniques in two main aspects: (1) FAST employs f lushing policies that smartly
selects some of the updates from memory to be flushed to the flash storage in a way
that amortizes the expensive cost of the block erase operation over a large set of
random write operations, and (2) FAST logs the flushing process using a single log
entry written sequentially on the flash storage. Details of the flushing module will be
discussed in Section 7.
Crash recovery module FAST employs a crash recovery module to ensure the
durability of update operations. This is a crucial module in FAST, as only because
of this module, we are able to have our updates in memory, and not to worry about
any data losses. This is in contrast to previous research in flash indexing that may
encounter data losses in case of system crash, e.g., [24, 25, 35, 36]. The crash recovery

424

Geoinformatica (2013) 17:417–448

module is mainly responsible on two operations: (1) Once the system restarts after
crash, the crash recovery module utilizes the log file entries, written by both the
update and flushing modules, to reconstruct the state of the flash storage and inmemory updates just before the crash took place, and (2) maintaining the size of
the log file within the allowed limit. As the log space is limited, FAST needs to
periodically compact the log entries. Details of this module will be discussed in
Section 8.
4.2 FAST design goals
FAST avoids the tradeoff of durability and efficiency by using a combination of
buffering and logging. Unlike existing efficient-but-not-durable designs [24, 25, 30,
35, 36], FAST uses write-ahead-logging and crash recovery to ensure strict system
durability. FAST makes tree updates efficient by buffering write operations in main
memory and by employing an intelligent flushing policy that optimizes I/O costs for
both SSDs and raw flash devices. Unlike existing durable-but-inefficient solutions
[2], FAST does not require reading in-flash log entries for each search/update
operation, which makes reading FAST trees efficient.
4.3 FAST data structure
Other than the underlying index tree structure stored in the flash memory storage,
FAST maintains two main data structures, namely, the Tree Modif ications Table,
and Log File, described below.
Tree modif ications table This is an in-memory hash table (depicted in Fig. 3)
that keeps track of recent tree updates that did not make it to the flash storage
yet. Assuming no hashing collisions, each entry in the hash table represents the
modification applied to a unique node identifier, and has the form (status, list)
where status is either NEW, DEL, or MOD to indicate if this node is newly created,
deleted, or just modified, respectively, while list is a pointer to a new node, null,
or a list of node modifications based on whether the status is NEW, DEL, or
MOD, respectively. For MOD case, each modification in the list is presented by the
quadruple (TimeStamp, type, index, value) where TimeStamp represents the time at

Fig. 3 Tree modifications table

Geoinformatica (2013) 17:417–448

425

which the update happened, type is either K, P F , or P M , to indicate if the modified
entry is the key, a pointer to a flash node, or a pointer to an in-memory node,
respectively, while index and value determines the index and the new value for the
modified node entry, respectively. In Fig. 3, there are two modifications in nodes A
and D, one modification in nodes B and F, while node G is newly created and node
H is deleted.
Log f ile This is a set of flash memory blocks, reserved for recovery purposes. A log
file includes short logs, written sequentially, about insert, delete, update, and flushing
operations. Each log entry includes the triple (operation, node_list, modif ication)
where operation indicates the type of this log entry as either insert, delete, update, or
flush, node_list includes the list of affected nodes by this operation in case of a flush
operation, or the only affected node, otherwise, modif ication is similar to the triple
(type, index, value), used in the tree modif ications table. All log entries are written
sequentially to the flash storage, which has a much lower cost than random writes
that call for the erase operation.
4.4 Running example
Throughout the rest of this paper, we will use Fig. 4 as a running example where six
objects O1 to O6 , depicted by small black circles, are indexed by an R-tree. Then, two
objects O7 and O8 , depicted by small white circles, are to be inserted in the same Rtree. Figure 4a depicts the eight objects in the two-dimensional space domain, while
Fig. 4b gives the flash-resident R-tree with only the six objects that made it to the

O1

10

O8

O2

8
O5

O3

6
O7

O4

4

O6

2

0

2

4

6

8

10

12

14

(a) 2D Space
A

1, K, 2, (12,4,14,2)

B

C

D
O1

O2

E

F

O3

O4

G
O5

(b) R-tree Index

C

Mod

G

Mod

B

Mod

D

Mod

O6

2, K, 2, O7

3, K, 2, (5,10,8,7)

4, K, 2, O8

(c) Tree Modifications Table

Fig. 4 Illustrating example for search and update operations in FAST

426

Geoinformatica (2013) 17:417–448

Table 1 Cost analysis parameters
Parameter

Definition

T
RM
WM
RF

The underlying tree index structure to which FAST has been applied
The average time to read a node update entry from the tree modifications table
The average time to write a node update entry to the tree modifications table
The average time to read a tree node from the underlying tree T residing
on flash memory
The average time to write a tree node to the underlying tree T residing
on flash memory
The average time to erase a whole block on the flash memory device

WF
EF

flash memory. Finally, Fig. 4c gives the in-memory buffer (tree modif ications table)
upon the insertion of O7 and O8 in the tree.
4.5 Operations cost parameters
For each FAST module, we analyze the cost model of its main operations, including
search, update, flushing, crash recovery and log compaction. To this end, we define
the parameters given in Table 1.

5 Tree updates in FAST
This section discusses the update operations in FAST, which include inserting a new
entry and deleting/updating an existing entry. An update operation to any tree in
FAST may result in creating new tree nodes as in the case of splitting operations (i.e.,
when inserting an element in the tree leads to node overflow), deleting existing tree
nodes as in the case of merging operations (i.e., when deleting an element from the
tree leads to node underflow), or just modifying existing node keys and/or pointers.
Main idea For any update operation (i.e., insert, delete, update) that needs to
be applied to the index tree, FAST does not change the underlying insert, delete,
or update algorithm for the tree structure it represents. Instead, FAST runs the
underlying update algorithm for the tree it represents, with the only exception
of writing any changes caused by the update operation in memory instead of the
external storage, to be flushed later to the flash storage, and logging the result of
the update operation. A main distinguishing characteristic of FAST is that what
is buffered in memory, and also written in the log file, is the result of the update
operation, not a log of this operation.
Algorithm Algorithm 1 gives the pseudo code of inserting an object Obj in FAST.
The algorithms for deleting and updating objects are similar in spirit to the insertion
algorithm, and thus are omitted from the paper. The algorithm mainly has two steps:
(1) Executing the insertion in memory (Line 2 in Algorithm 1). This is basically
done by calling the insertion procedure of the underlying tree, e.g., R-tree insertion,
with two main differences. First, the insertion operation calls the search operation,
discussed later in Section 6, to find where we need to insert our data based on the
most recent version of the tree, constructed from main memory recent updates and

Geoinformatica (2013) 17:417–448

427

Algorithm 1 Insert an Object in the Tree
1: Function Insert(Obj)
/* STEP 1: Executing the Insertion in Memory only */
2: L ← List of modified nodes from the in-memory execution of inserting Obj in
the underlying tree
/* STEP 2: Buf fering and Logging the Updates */
3: for each Node N in L do
4:
HashEntry ← N entry in the Tree Modif ications Table
5:
if HashEntry is not NULL then
6:
Add the triple (MOD, N , updates in N ) to the log file
7:
if the status of HashEntry is MOD then
8:
Add the changes in N to the list of changes of HashEntry
9:
else
10:
Apply the changes in N to the new node of HashEntry
11:
end if
12:
else
13:
HashEntry ← Create a new entry for N in the Tree Modif ications Table
14:
if N is a newly created node then
15:
Add the triple (NEW, N , updates in N ) to the log file
16:
Set HashEntry status to NEW, and its pointer to N
17:
else
18:
Add the triple (MOD, N , updates in N ) to the log file
19:
Set HashEntry status to MOD, and its pointer to the list of changes that
took place in N
20:
end if
21:
end if
22: end for

the in-flash tree index structure. Second, the modified or newly created nodes that
result back from the insertion operation are not written back to the flash storage,
instead, they will be returned to the algorithm in a list L. Notice that the insertion
procedure may result in creating new nodes if it encounters a split operation.
(2) Buf fering and logging the tree updates (Lines 3–22 in Algorithm 1). For each
modified node N in the list L, we check if there is an entry for N in our in-memory
buffer, tree modif ications table. If this is the case, we first add a corresponding log
entry that records the changes that took place in N . Then, we either add the changes
in N to the list of changes in its entry in the tree modif ications table if this entry status
is MOD, or update N entry in the tree modif ications table, if the entry status is NEW.
On the other hand, if there is no entry for N in the tree modif ications table, we create
such entry, add it to the log file, and fill it according to whether N is a newly created
node or a modified one.
Example In our running example of Fig. 4, inserting O7 results in modifying two
nodes, G and C. Node G needs to have an extra key to hold O7 while node C needs
to modify its minimum bounding rectangle that points to G to accommodate its size
change. The changes in both nodes are stored in the tree modif ications table depicted

428
Fig. 5 FAST logging and
recovery example

Geoinformatica (2013) 17:417–448

Log#

Operation

Node

Modification

1

MOD

C

1, K, 2, (12,4,14,2)

2

MOD

G

2, K, 2, O7

3

MOD

B

3, K, 2, (5,10,8,7)

4

MOD

D

4, K,2, O8

5

FLUSH

B, C, D

*

(a) FAST Log File
Log#
1

Operation

Node

Modification

MOD

G

2, K, 2, O7

(a) FAST Log File after Crash Recovery

in Fig. 4c. The log entries for this operation are depicted in the first two entries of the
log file of Fig. 5a. Similarly, inserting O8 results in modifying nodes, D and B
Cost analysis For a given update operation U applied to a tree index structure T,
let yi,U ∈ {0, 1} represent whether or not node i of T has been modified by U. Let N
be the total number of nodes in T at the time U is applied, then the total cost CU of
update operation U applied on T is as follows:
CU = C Q +

N


yi,U ∗ [W F + W M + L]

(1)

i=0

The update operation (e.g., insert, delete, modify) requires first a search query Q
for a proper leaf node in T. This also takes the same search time C Q as illustrated
above. For each updated node i due to applying U, yi,U = 1, and for each of these
updates we write a sequential log entry to the log file
each takes W F time. Hence,
that
N
the total time to write all log entries is equal to i=0
yi,U ∗ W F . For each updated
node i, we also perform a lookup on the tree modifications table to get the entry for
node i, which is performed in constant time L. In addition, the total time to write
the modifications to all nodes (for which yi,U = 1) in the tree modifications table is
N
i=0 yi,U ∗ W M . All of the above sums up to give the update cost given in Eq. 1
6 Searching in FAST
Given a query Q, the search operation returns those objects indexed by FAST and
satisfy Q. The search query Q could be a point query that searches for objects with
a specific (point) value, or a range query that searches for objects within a specific
range. An important promise of FAST is that it does not change the main search
algorithm for any tree it represents. Instead, FAST complements the underlying
searching algorithm to consider the latest tree updates stored in memory.
Main idea As it is the case for any index tree, the search algorithm starts by fetching
the root node from the secondary storage, unless it is already buffered in memory.
Then, based on the entries in the root, we find out which tree pointer to follow to

Geoinformatica (2013) 17:417–448

429

fetch another node from the next level. The algorithm goes on recursively by fetching
nodes from the secondary storage and traversing the tree structure till we either find
a node that includes the objects we are searching for or conclude that there are no
objects that satisfy the search query. The challenging part here is that the retrieved
nodes from the flash storage do not include the recent in-memory stored updates.
FAST complements this search algorithm to apply the recent tree updates to each
retrieved node from the flash storage. In particular, for each visited node, FAST
constructs the latest version of the node by merging the retrieved version from the
flash storage with the recent in-memory updates for that node.
Algorithm Algorithm 2 gives the pseudo code of the search operation in FAST.
The algorithm takes two input parameters, the query Q, which might be a point or
range query, and a pointer to the root node R of the tree we want to search in. The
output of the algorithm is the list of objects that satisfy the input query Q. Starting
from the root node and for each visited node R in the tree, the algorithm mainly
goes through two main steps: (1) Constructing the most recent version of R (Line 2 in
Algorithm 2). This is mainly to integrate the latest flash-residant version of R with
its in-memory stored updates. Algorithm 3 gives the detailed pseudo code for this

Algorithm 2 Searching for an Object indexed by the Tree
1: Function Search(Query Q, Tree Node R)
/* STEP 1: Constructing the most recent version of R */
2: N ← RetrieveNode(R)
/* STEP 2: Recursive search calls */
3: if N is non-leaf node then
4:
Check each entry E in N . If E satisfies the query Q, invoke Search(Q,
E.NodePointer) for the subtree below E
5: else
6:
Check each entry E in N . If E satisfies the search query Q, return the object
to which E is pointing
7: end if

Algorithm 3 Retrieving a tree node
1: Function RetrieveNode(Tree Node R)
2: FlashNode ← Retrieve node R from the flash-resident index tree
3: HashEntry ← R’s entry in the Tree Modif ications Table
4: if HashEntry is NULL then
5:
return FlashNode
6: end if
7: if the status of HashEntry is MOD then
8:
FlashNode ← FlashNode ∪ All the updates in HashEntry list
9:
return FlashNode
10: end if
/* We are trying to retrieve either a new or a deleted node */
11: return the node that HashEntry is pointing to

430

Geoinformatica (2013) 17:417–448

step, where initially, we read R from the flash storage. Then, we check if there is
an entry for R in the tree modif ications table. If this is not the case, then we know
that the version we have read from the flash storage is up-to-date, and we just return
it back as the most recent version. On the other hand, if R has an entry in the tree
modif ications table, we either apply the changes stored in this entry to R in case the
entry status is MOD, or just return the node that this entry is pointing to instead of R.
This return value could be null in case the entry status is DEL. (2) Recursive search
calls (Lines 3–7 in Algorithm 2). This step is typical in any tree search algorithm, and
it is basically inherited from the underlying tree that FAST is representing. The idea
is to check if R is a leaf node or not. If R is a non-leaf node, we will check each entry
E in the node. If E satisfies the search query Q, we recursively search in the subtree
below E. On the other hand, if R is a leaf node, we will also check each entry E in
the node, yet if E satisfies the search query Q, we will return the object to which E is
pointing to as an answer to the query.
Example Given the range query Q in Fig. 4a, FAST search algorithm will first
fetch the root node A stored in flash memory. As there is no entry for A in the tree
modif ications table (Fig. 4c), then the version of A stored in flash memory is the most
recent one. Then, node C is the next node to be fetched from flash memory by the
searching algorithm. As the tree modif ications table has an entry for C with status
MOD, the modifications listed in the tree modif ications table for C will be applied
to the version of C read from the flash storage. Similarly, the search algorithm will
construct the leaf nodes F and G by first fetching them from flash memory, and then
reading their recent updates from the tree modif ications table. Finally, the result of
this query is {O4 , O5 , O6 , O7 }.
Cost analysis For a given search query Q applied to a tree index structure T, let
xi,Q ∈ {0, 1} represent whether node i of T is visited or not when issuing query Q.
Let Mi,Q be the number of modifications applied to node i and buffered in the tree
modifications table at the time Q is issued. Let N be the total number of nodes in T
at the time Q is issued, then the total search cost C Q on T is as follows:
CQ =

N


xi,Q ∗ [R F + (Mi,Q × R M ) + L]

(2)

i=0

Assuming a range query, the search operation returns a number of objects within
the query range. In FAST, when reading a node i from the flash-resident R-tree, we
also need to accommodate all the corresponding modifications on i that have been
recorded in the tree modifications table. Then, the total cost of reading a node i
would thus be (R F + Tm ) where Tm is the in-memory processing time for each node.
For the in-memory processing part, it first takes constant time L to locate the node
in the tree modification table , and then takes a linear scan of the list to apply all
the modifications. Given that the number of modifications associated with each node
is Mi,Q , then Tm = (Mi,Q × R M ) + L, where Mi,Q is upper bounded by the memory
size.

Geoinformatica (2013) 17:417–448

431

7 Memory flushing in FAST
As discussed in Section 5, the effect of all incoming updates in FAST has to be
buffered in memory. As memory is a scarce resource, it will eventually be filled up
with incoming updates. In that case, FAST triggers its flushing module, equipped
with a f lushing policy, to free some memory space by evicting a selected part of the
memory, termed a f lushing unit, to the flash storage. Such flushing is done in a way
that amortizes the cost of expensive random write operations over a high number of
update operations. In this section, we first define the flushing unit. Then, we discuss
the flushing policy used in FAST. Finally, we explain the FAST flushing algorithm.
The motivation of having a f lushing policy that flushes only part of the memory
is twofold: (1) Clearing the whole memory at once will cause a significant pause to
the system due to the need of erasing all the flash blocks that include at least one
update record in memory. As a result, it is better to consider clearing only part of the
memory in a way that does not really pause the system. In this paper, we present two
main flushing policies employed by the system, and we empirically evaluate both of
them, (2) Considering that we need to flush only part of the memory, it is crucial to
select that part in a way that reduces the overhead of the block erase operation.
7.1 Flushing unit
An important design parameter, in FAST, is the size of a f lushing unit, the granularity
of consecutive memory space written in the flash storage during each flush operation.
Our goal is to find a suitable f lushing unit size that minimizes the average cost of
flushing an update operation to the flash storage, denoted as C. The value of C
average writing cost
depends on two factors: C1 = numb
; the average cost per bytes written,
er of written b ytes

er of written b ytes
; the number of bytes written per update. This gives C =
and C2 = numb
numb er of updates
C1 × C2 .
Interestingly, the values of C1 and C2 show opposite behaviors with the increase
of the f lushing unit size. First consider C1 . On raw flash devices (e.g., ReMix [19]),
for a f lushing unit smaller than a flash block, C1 decreases with the increase of
the flushing unit size (see [29] for more detail experiments). This is intuitive, since
with a larger f lushing unit, the cost of erasing a block is amortized over more bytes
in the flushing unit. The same is also true for SSDs since small random writes
introduce large garbage collection overheads, while large random writes approach
the performance of sequential writes. Previous work has shown that, on several SSDs
including the ones from Samsung, MTron, and Transcend, random write latency per
byte increases by ≈32× when the write size is reduced from 16 KB to 0.5 KB [7]. Even
on newer generation SSDs from Intel, we observed an increase of ≈4× in a similar
experimental setup. This suggests that a flushing unit should not be very small, as that
would result in a large value of C1 . On the other hand, the value of C2 increases with
increasing the size of the f lushing unit. Due to non-uniform updates of tree nodes, a
large flushing unit is unlikely to have as dense updates as a small flushing unit. Thus,
the larger a f lushing unit is, the less the number of updates per byte is (i.e., the higher
the value of C2 is). Another disadvantage of large f lushing unit is that it may cause a
significant pause to the system. All these suggest that the f lushing unit should not be
very large.

432

Geoinformatica (2013) 17:417–448

Deciding the optimal size of a f lushing unit requires finding a sweet spot between
the competing costs of C1 and C2 . Our experiments show that for raw flash devices, a
f lushing unit of one flash block minimizes the overall cost. For SSDs, a f lushing unit
of size 16 KB is a good choice, as it gives a good balance between the values of C1
and C2 . Note that a flushing unit size of 16 KB also matches the optimal size of a tree
node, as suggested by Gray et al. [13]. Thus, with a tree of this optimal node size of
16 KB, we can simply flush one node at a time from the memory.
7.2 Flushing policies
FAST is designed so that different flushing policies can be plugged in to the system.
In the rest of this section, we discuss two main flushing policies adopted by FAST:
(1) FAST Flushing Policy, and (2) FAST* Flushing Policy.
7.2.1 FAST f lushing policy
The main idea of FAST f lushing policy is to minimize the average cost of writing
each update to the underlying flash storage. To that end, FAST flushing policy aims
to flush the in-memory tree updates that belong to the f lushing unit that has the
highest number of in-memory updates. In that case, the cost of writing the f lushing
unit will be amortized among the highest possible number of updates. Moreover,
since the maximum number of updates are being flushed out, this frees up the
maximum amount of memory used by buffered updates. Finally, as done in the
update operations, the flushing operation is logged in the log file to ensure the
durability of system transactions.
Data structure The flushing policy maintains an in-memory max heap structure,
termed FlushHeap, of all f lushing units that have at least one in-memory tree update.
The max heap is ordered on the number of in-memory updates for each f lushing unit,
and is updated with each incoming tree update. Updates in max heap is O(n), where
n is the number of flash blocks with in-memory updates. In the mean time, retrieving
the flushing unit with maximum number of updates is an O(1) operation.
7.2.2 FAST* f lushing policy
The FAST* f lushing policy is an enhancement over the FAST flushing policy
described in Section 7.2.1. FAST* flushing policy takes into account two parameters
that helps in deciding which unit must be flushed: (1) Number of updates per
flushing unit: It is the same parameter used by the FAST flushing policy explained in
Section 7.2.1; which favors the flash unit that has the highest number of updates, and
(2) Time stamp of the flushing unit: which represents the last time a flash block has
been updated. When deciding which unit needs to be flushed, that parameter gives
higher priority to the flushing unit that has the lowest time stamp (i.e., least recently
updated).
FAST* Flushing Policy employs a Top-1 selection algorithm to select a flushing
unit to be evicted to flash memory with the objective of maximizing the number of
updates per flushing unit and minimizing the time stamp of the flushing unit. The
intuition behind such a policy is that it is sometimes better to keep the block that
has the highest number of updates in the tree modifications table (i.e., in memory)

Geoinformatica (2013) 17:417–448

433

and not to flush it, especially if that block is expected to receive more updates
(i.e., recently updated block). On the other hand, it might be better to flush a flash
block that has a bit less number of updates, but it is not expected to be updated
frequently (i.e., least recently updated block). Hence, FAST* flushing policy makes
that tradeoff between the two parameters in order to amortize the total number of
erase operations on flash memory storage systems.
7.3 Flushing algorithm
Algorithm 4 gives the pseudo code for flushing tree updates. The algorithm has two
main steps: (1) Finding out the list of f lushed tree nodes (Lines 2–9 in Algorithm 4).
This step starts by finding out the victim f lushing unit, MaxUnit, using the flushing
policy passed to the algorithm. Then, we scan the tree modif ications table to find
all updated tree nodes that belong to MaxUnit. For each such node, we construct
the most recent version of the node by retrieving the tree node from the flash
storage, and updating it with the in-memory updates. This is done by calling the
RetrieveNode(N ) function, given in Algorithm 3. The list of these updated nodes
constitute the list of to be flushed nodes, FlushList. (2) Flushing, logging, and cleaning
selected tree nodes (Lines 10–15 in Algorithm 4). In this step, all nodes in the
FlushList are written once to the flash storage. As all these nodes reside in one
f lushing unit, this operation would have a minimal cost due to our careful selection
of the f lushing unit size. Then, similar to update operations, we log the flushing
operation to ensure durability. Finally, all flushed nodes are removed from the tree
modif ications table to free memory space for new updates.
Algorithm 4 Flushing Tree Updates
1: Function FlushTreeUpdates(FlushPolicy)
/* STEP 1: Finding out the list of flushed tree nodes */
2: FlushList ← {φ}
3: MaxUnit ← Retrieve Unit to be Flushed uisng FlushPolicy
4: for each Node N in tree modif ications table do
5:
if N ∈ MaxUnit then
6:
F ← RetrieveNode(N )
7:
FlushList ← FlushList ∪ F
8:
end if
9: end for
/* STEP 2: Flushing, logging, and cleaning selected nodes */
10: Flush all tree updates ∈ FlushList to a clean flash memory block
11: Add (Flush, All Nodes in FlushList) to the log file
12: Erase the old flash memory block and update the index pointer to refer the new
block
13: for each Node F in FlushList do
14:
Delete F from the Tree Modif ications Table
15: end for
Example In our running example given in Fig. 4, assume that the memory is full,
hence FAST triggers its flushing module. Assume also that nodes B, C, and D reside

434

Geoinformatica (2013) 17:417–448

in the same f lushing unit B1 , while nodes E, F, and G reside in another f lushing
unit B2 . The number of updates in B1 is three as each of nodes B, C and D has
been updated once. On the other hand, the number of updates in B2 is one because
nodes E and F has no updates at all, and node G has only a single update. Hence,
as per FAST flushing policy, MaxUnit is set to B1 , and we will invoke RetrieveNode
algorithm for all nodes belonging to B1 (i.e., nodes B, C, and D) to get the most
recent version of these nodes and flush them to flash memory. Then, the log entry
(Flush; Nodes B, C, D) is added to the log file (depicted as the last log entry
in Fig. 5a). Finally, the entries for nodes B, C, and D are removed from the tree
modif ications table.
Cost analysis For a given flushing operation F applied to a tree index structure T,
Let Pflush be the set of tree nodes that belongs to the block selected to be flushed.
Let M p be the number of modifications applied to node p and buffered in the tree
modifications table at the time F is applied. Hence, the total cost C F of flushing
operation F applied on T is as follows:

CF = EF + H +
[R F + (M p ∗ R M ) + W F + L]
(3)
p ∈ Pflush

We decide which unit to flush by employing the flushing policy passed to the
algorithm. The cost of this in memory operation H varies based on which flushing
policy is activated. For each node p ∈ Pflush
, we first need to retrieve the node current
value saved in flash memory which
costs
p ∈ Pflush R F , and then lookup the node in

the tree modifications table in
L.
For each node p ∈ Pflush , we read all
p ∈ Pflush
M p modifications
of
p
that
are
buffered
in
the
tree modifications table, which sum

up to p ∈ Pflush (M p ∗ R M ). Before we write the new nodes values, we first erase the
whole flash block which costs
 E F time. For each node p ∈ Pflush , we write the flushed
node new value, that costs p ∈ Pflush W F . All of the above sum up to give the flushing
operation cost given by Eq. 3.

8 Crash recovery and log compaction in FAST
As discussed before, FAST heavily relies on storing recent updates in memory, to
be flushed later to the flash storage. Although such design efficiently amortizes the
expensive random write operations over a large number of updates, it poses another
challenge where memory contents may be lost in case of system crash. To avoid such
loss of data, FAST employs a crash recovery module that ensures the durability of
in-memory updates even if the system crashed. The crash recovery module in FAST
mainly relies on the log file entries, written sequentially upon the update and flush
operations.
In this section, we will first describe the crash recovery module and logging
mechanism in FAST. Then, we will follow by discussing the log compaction operation
in FAST, which is mainly done to ensure that the log file is within a certain size limit.
Log compaction has a very similar operation to the recovery module, and it is crucial
to keep up the efficiency of FAST. For simplicity, we will not consider the case of
having a system crash during the recovery process, as this can be handled in a similar
way to traditional recovery modules in database systems.

Geoinformatica (2013) 17:417–448

435

8.1 Recovery
The recovery module in FAST is triggered when the system restarts from a crash,
with the goal of restoring the state of the system just before the crash took place.
The state of the system includes the contents of the in-memory data structure, tree
modif ications table, and the flash-resident tree index structure. By doing so, FAST
ensures the durability of all non-flushed updates that were stored in memory before
crash.
Main idea The main idea of the recovery operation is to scan the log file bottomup to be aware of the flushed nodes, i.e., nodes that made their way to the flash
storage. During this bottom-up scanning, we also find out the set of operations that
need to be replayed to restore the tree modif ications table. Then, the recovery module
cleans all the flash blocks, and starts to replay the non-flushed operations in the order
of their insertion, i.e., top-down. The replay process includes insertion in the tree
modif ications table as well as a new log entry. It is important here to reiterate our
assumption that there will be no crash during the recovery process, so, it is safe to
keep the list of operations to be replayed in memory. If we will consider a system
crash during the recovery process, we might just leave the operations to be replayed
in the log, and scan the whole log file again in a top-down manner. In this top-down
scan, we will only replay the operations for non-flushed nodes, while writing the new
log entries into a clean flash block. The result of the crash recovery module is that
the state of the memory will be stored as it was before the system crashes, and the
log file will be an exact image of the tree modif ications table.
Algorithm Algorithm 5 gives the pseudo code for crash recovery in FAST, which
has two main steps: (1) Bottom-Up scan (Lines 2–11 in Algorithm 5). In this step,
FAST scans the log file bottom-up, i.e., in the reverse order of the insertion of log
entries. For each log entry L in the log file, if the operation of L is Flush, then
we know that all the nodes listed in this entry have already made their way to the
flash storage. Thus, we keep track of these nodes in a list, termed FlushedNodes,
so that we avoid redoing any updates over any of these nodes later. On the other
side, if the operation of L is not Flush, we check if the node in L entry is in the list
FlushedNodes. If this is the case, we just ignore this entry as we know that it has
made its way to the flash storage. Otherwise, we push this log entry into a stack of
operations, termed RedoStack, as it indicates a non-flushed entry at the crash time.
At the end of this step, we pass the RedoStack to the second step. (2) Top-Down
processing (Lines 13–19 in Algorithm 5). At the beginning, we first create a new log
file Fnew . Then, this step basically goes through all the entries in the RedoStack in a
top-down way, i.e., the order of insertion in the log file. As all these operations were
not flushed by the crash time, we just add each operation to the tree modif ications
table and add a corresponding log entry in the new Log File Fnew . The reason of
doing these operations in a top-down way is to ensure that we have the same order
of updates, which is essential in case one node has multiple non-flushed updates. At
the end of this step, the tree modif ications table will be exactly the same as it was just
before the crash time, while the new log file Fnew will be exactly an image of the tree
modif ications table stored in the flash storage. Finally, we change the log file pointer
to refer to the new log file Fnew and we finally erase the old log file flash blocks.

436

Geoinformatica (2013) 17:417–448

Algorithm 5 Crash Recovery
1: Function RecoverFromCrash()
/* STEP 1: Bottom-Up Cleaning */
2: FlushedNodes ← φ
3: for each Log Entry L in the log file in a reverse order do
4:
if the operation of L is Flush then
5:
FlushedNodes ← FlushedNodes ∪ the list of nodes in L
6:
else
7:
if the node in entry L ∈
/ FlushedNodes then
8:
Push L into the stack of updates RedoStack
9:
end if
10:
end if
11: end for
/* Phase 2: Top-Down Processing */
12: Create a new Log File Fnew
13: while RedoStack is not Empty do
14:
Op ← Pop an update operation from the top of RedoStack
15:
Insert the operation Op into the tree modif ications table
16:
Add a log entry for Op in the new log file Fnew
17: end while
18: Change the Log File pointer to refer to the new Log File Fnew
19: Clean all the old log entries by erasing the old log file flash blocks

Example In our running example, the log entries of inserting Objects O7 and O8
in Fig. 4 are given as the first four log entries in Fig. 5a. Then, the last log entry
in Fig. 5a corresponds to flushing nodes B, C, and D. We assume that the system
is crashed just after inserting this flushing operation. Upon restarting the system,
the recovery module will be invoked. First, the bottom-up scanning process will be
started with the last entry of the log file, where nodes B, C, and D are added to the
list FlushedNodes. Then, for the next log entry, i.e., the fourth entry, as the node
affected by this entry D is already in the FlushedNodes list, we just ignore this entry,
since we are sure that it has made its way to disk. Similarly, we ignore the third log
entry for node B. For the second log entry, as the affected node G is not in the
FlushedNodes list, we know that this operation did not make it to the storage yet,
and we add it to the RedoStack to be redone later. The bottom-up scanning step is
concluded by ignoring the first log entry as its affected node C is already flushed, and
by wiping out all log entries. Then, the top-down processing step starts with only one
entry in the RedoStack that corresponds to node G. This entry will be added to the
tree modif ications table and log file. Figure 5b gives the log file after the end of the
recovery module which also corresponds to the entries of the tree modif ications table
after recovering from failure.
Cost analysis For a given crash recovery operation R applied to a tree index
structure T, let Z be the set of operations recorded in the log file. Let α (0 ≤ α ≤ 1)
be the fraction of operations in Z that had been flushed to T before the system fails.
Let Spage , Sblock , and Slog be the byte size of the flash page, flash block and flash log

Geoinformatica (2013) 17:417–448

437

file, respectively. Hence, the total cost C R of a crash recovery operation R applied
on T is as follows:


RF
RF
+ Z × α × (W M + W F )
+
(4)
C R = Slog ×
Spage
Sblock
As all the Z entries in the log file have to be scanned, then the total cost to scan
Slog
them is R F × Spage
. In addition, only Z × α log file operations need to be redone
(i.e., written back to the tree modifications table), which results to an additional cost
of Z × α × W M . As all redone operations are written back to memory, an additional
cost of logging them is Z × α × W F . The old log file blocks needs to be erased, which
Slog
incurs a cost of E F × Sblock
. All of the above sums up to give the recovery cost given
in Eq. 4.
8.2 Log compaction
As FAST log file is a limited resource, it may eventually become full. In this case,
FAST triggers a log compaction module that organizes the log file entries for better
space utilization. This can be achieved by two space saving techniques: (a) Removing
all the log entries of flushed nodes. As these nodes have already made their way to
the flash storage, we do not need to keep their log entries anymore, and (b) Packing
small log entries in a larger writing unit. Whenever a new log entry is inserted, it
mostly has a small size that may occupy a flash page as the smallest writing unit
to the flash storage. At the time of compaction, these small entries can be packed
together to achieve the maximum possible space utilization.
The main idea and algorithm for the log compaction module are almost the same
as the ones used for the recovery module, with the exception that the entries in the
RedoStack will not be added to the tree modif ications table, yet they will just be
written back to the log file, in a more compact way. As in the recovery module,
Fig. 5a and b give the log file before and after log compaction, respectively. The log
compaction have similar expensive cost as the recovery process. Fortunately, with
an appropriate size of log file and memory, it will not be common to call the log
compaction module.
It is unlikely that the log compaction module will not really compact the log file
much. This may take place only for a very small log size and a very large memory size,
as there will be a lot of non-flushed operations in memory with their corresponding
log entries. Notice that if the memory size is small, there will be a lot of flushing
operations, which means that log compaction can always find log entries to be
removed. If this unlikely case takes place, we call an emergency f lushing operation
where we force flushing all main memory contents to the flash memory persistent
storage, and hence clean all the log file contents leaving space for more log entries to
be added.
Cost analysis The log compaction is almost the same as the crash recovery procedure. The only difference is that records are not redone (written to the tree
modifications table). Similar to recovery cost, the log compaction cost CC is as
follows:


RF
RF
+ Z × α × WF
+
(5)
CC = Slog ×
Spage
Sblock

438

Geoinformatica (2013) 17:417–448

9 Experimental evaluation
This section experimentally evaluates the performance of FAST, compared to the
state-of-the-art algorithms for one-dimensional and multi-dimensional flash index
structures: (1) Lazy Adaptive Tree (LA-tree) [2]: LA-tree is a flash friendly one
dimensional index structure that is intended to replace the B-tree. LA-tree stores
the updates in cascaded buffers residing on flash memory and, then empties these
buffers dynamically based on the operations workload. (2) FD-tree [24, 25]: FD-tree
is a one-dimensional index structure that allows small random writes to occur only
in a small portion of the tree called the head tree which exists at the top level of
the tree. When the capacity of the head tree is exceeded, its entries are merged in
batches to subsequent tree levels. (3) RFTL [35]: RFTL is a mutli-dimensional tree
index structure that adds a buffering layer on top of the flash translation layer (FTL)
in order to make R-trees work efficiently on flash devices.
We instantiate B-tree and R-tree instances of FAST using both flushing policies
(i.e., FAST flushing policy and FAST* flushing policy), termed FAST-Btree, FAST*Btree, FAST-Rtree, and FAST*-Rtree , respectively, by implementing FAST inside
the GiST generalized index structure [15], which is already built inside PostgreSQL
[1]. In our experiments, we use two synthetic workloads: (1) Lookup intensive
workload (W L ): that includes 80 % search operations and 20 % update operations
(i.e., insert, delete, or update). (2) Update intensive workload, (WU ): that includes
20 % search operations and 80 % update operations.
Unless mentioned otherwise, we set the number of workload operations to
10 million operations, main memory size to 256 KB (i.e., the amount of memory
dedicated to main memory buffer used by FAST), tree index size to 512 MB, and log
file size to 10 MB, which means that the default log size is ≈2 % of the index size.
The experiments in this section mainly discuss the effect of varying the memory
size, log file size, index size, and number of updates on the performance of FASTBtree, FAST-Rtree, LA-tree, FD-tree, and RFTL. Also, we study the performance of
flushing, log compaction, and recovery operations in FAST. In addition, we compare
the implementation cost between FAST and its counterparts. Our performance
metrics are mainly the number of flash memory erase operations and the average
response time. However, in almost all of our experiments, we got a similar trend for
both performance measures. Thus, for brevity, we only show the experiments for the
number of flash memory erase operations, which is the most expensive operation in
flash storage. Although we compare FAST to its counterparts from a performance
point of view, however we believe the main contribution of FAST is not in the
performance gain. The generic structure and low implementation cost are the main
advantages of FAST over specific flash-aware tree index structures.
All experiments were run on both raw flash memory storage, and solid state drives
(SSDs). For raw flash, we used the raw NAND flash emulator described in [2].
The emulator was populated with exhaustive measurements from a custom-designed
Mica2 sensor board with a Toshiba1Gb NAND TC58DVG02A1FT00 flash chip. For
SSDs, we used a 32GB MSP-SATA7525032 SSD device. All the experiments were
run on a machine with Intel Core2 8400 at 3Ghz with 4GB of RAM running Ubuntu
Linux 8.04.

Geoinformatica (2013) 17:417–448

439

9.1 Effect of memory size
Figure 6 and b give the effect of varying the memory size from 128 KB to 1,024 KB
(in a log scale) on the number of erase operations, encountered in FAST-Btree, LAtree, and FD-tree, for workloads W L and WU , respectively. For both workloads and
for all memory sizes, FAST-Btree consistently has much lower erase operations than
that of the LA-tree. More specifically, Fast-Btree results in having only from half
to one third of the erase operations encountered by LA-tree. This is mainly due to
the choice of f lushing unit and f lushing policy used in FAST that amortize the block
erase operations over a large number of updates. Also, for both experiments, the
number of erase operations decreases with the increase of the memory size, which is
intuitive as more memory means less frequent need for flushing, and hence less need
for block erase operations.
The performance of FAST-Btree is slightly better than that of FD-tree, because
FD-tree does not employ a crash recovery technique (i.e., no logging overhead).
FAST still performs better than FD-tree due to FAST flushing policy that selects
the best block to be flushed to flash memory. Although the performance of FD-tree
is close to FAST-Btree, however FAST has the edge of being a generic framework
which is applied to many tree index structures and needs less work and overhead
(in terms of lines of code) to be incorporated in the database engine. Comparing
the two workloads against each other, we can see that the workload WU encounters
much more erase operations than that of workload W L . This is mainly because WU
is an update intensive workload which results in many in-memory updates that need
to flushed. FAST*-Btree gives a slightly better performance than FAST-Btree as
FAST*-Btree employs a flushing policy that does not only rely on the number of
updates per flash block, but also takes into account the last time a flash block has
been updated. Hence, FAST*-tree gives a chance for those flash blocks that has
higher number of updates to stay in memory if more updates are expected to be
applied to these blocks.
Figures 7a and b give similar experiments to that of Fig. 6 and b, with the exception
that we run the experiments for two-dimensional search and update operations
for both the Fast-Rtree and RFTL. To be able to do so, we have adjusted our
600

# of erase operations *(103)

# of erase operations *(103)

90
80
70
60
50
40
30

FAST*–Btree
FAST–Btree
LA–tree
10
FD–tree

20

0
128

256

512

1024

FAST*–Btree
FAST–Btree
LA–tree
FD–tree

500
400
300
200
100
0
128

256

512

Memory Size (KB)

Memory Size (KB)

(a) WL

(b) WU

Fig. 6 Effect of memory size on one-dimensional index structure

1024

440

Geoinformatica (2013) 17:417–448
900

FAST*–Rtree
FAST–Rtree
RFTL

80

# of erase operations *(103)

# of erase operations *(103)

100

60
40
20

128

256

512

1024

FAST*-Rtree
FAST-Rtree
RFTL

800
700
600
500
400
300
200
100
0
128

256

512

Memory Size (KB)

Memory Size (KB)

(a) Spatial-WL

(b) Spatial-WU

1024

Fig. 7 Effect of memory size on multi-dimensional index structure

workload W L and WU to Spatial-W L and Spatial-WU , respectively, which have twodimensional operations instead of the one-dimensional operations used in W L and
WU . The result of these experiments have the same trend as the ones done for onedimensional tree structures, where FAST-Rtree has consistently better performance
than RFTL in all cases, with around one half to one third of the number of erase
operations encountered in RFTL. Similar to the one-dimesnional case, FAST*-Rtree
slightly outperforms FAST-Rtree. Comparing the multi-dimensional workload to the
one dimensional one shows that the multi-dimensional workload encounters more
erase operations which is mainly due to the facts that the update operation may span
more nodes. However, even with this, FAST still keeps its performance ratio over its
counterparts.
The experiments in Figs. 6 and 7 not only shows that FAST has better performance
than its counterparts LA-tree, FD-tree and RFTL, but it also shows the power of
the FAST framework where it can be applied to both one-dimensional and multidimensional index structures with the same efficiency. In other words, it is not only
that FAST is better than LA-tree and FD-tree, but it is also the fact that FAST has
the ability to efficiently support multi-dimensional search and update operations in
which LA-tree or FD-tree cannot even support.
9.2 Effect of log file size
Figure 8 gives the effect of varying the log file size from 10 MB (i.e., 2 % of the
index size) to 25 MB (i.e., 5 % of the index size) on the number of erase operations,
encountered in FAST-Btree, LA-tree, and FD-tree for workload W L (Fig. 8a) and
FAST-Rtree and RFTL for workload Spatial-WU (Fig. 8b). For brevity, we do not
show the experiments of FAST-Btree, LA-tree, and FD-tree for workload WU nor
the experiment of FAST-Rtree and RFTL for workload Spatial-W L . As can be seen
from the figures, the performance of both LA-tee, FD-tree, and RFTL is not affected
by the change of the log file size. This is mainly because these three approaches
rely on buffering incoming updates, and hence does not make use of any log file. It
is interesting, however, to see that the number of erase operations in FAST-Btree
and FAST-Rtree significantly decreases with the increase of the log file size, given
that the memory size is set to its default value of 256 KB in all experiments. The

Geoinformatica (2013) 17:417–448
800

FAST*–Btree
FAST–Btree
LA–tree
FD–tree

90
80
70

# of erase operations *(103)

# of erase operations *(103)

100

441

60
50
40
30
20
10
0

10

20

15

25

700
600
500

FAST*–Rtree
FAST–Rtree
RFTL

400
300
200
100
0

10

15

20

Maximum Log File Size (MB)

Maximum Log File Size (MB)

(a) WL

(b) Spatial-WU

25

Fig. 8 Effect of FAST log file size

justification for this is that with the increase of the log file size, there will be less
need for FAST to do log compaction. FAST*-Btree and FAST*-Rtree shows the
same trend as FAST-Btree and FAST-Rtree except that they slightly give better
performance due to the fact that they apply the FAST* Flushing policy.
Revisiting Figs. 6 and 7 in Section 9.1, the number of erase operations encountered
in both LA-tree, FD-tree, and RFTL were only coming from flushing buffered
updates, while the number of erase operations in FAST were coming from two
sources, flushing in-memory updates, and log compaction. Then, the experiment in
this section (Fig. 8) shows that a large fraction of the erase operations in FAST is
coming from the log compaction operation, which can be significantly reduced with
the slight increase of the log file. With this, we can see that FAST achieves close
to an order of magnitude less erase operations than its counterparts for both onedimensional and multi-dimensional index structures when having the log file as small
as 5 % of the index size, i.e., 25 MB.
9.3 Effect of index size
Figure 9 gives the effect of varying the index size from 128 MB to 4 GB (in a log
scale) on the number of erase operations, encountered in FAST-Btree, LA-tree,
and FD-tree for workload W L (Fig. 9a) and FAST-Rtree and RFTL for workload
Spatial-WU (Fig. 9b). Same as in Section 9.2, we omit other workloads for brevity. In
all cases, FAST consistently gives much better performance than its counterparts.
Both FAST and other index structures have similar trend of a linear increase of
the number of erase operations with the increase of the index size. This is mainly
because with a larger index, an update operation may end up modifying more nodes
in the index hierarchy, or more overlapped nodes in case of multi-dimensional index
structures. Moreover, FAST*-Btree and FAST*-Rtree give a bit better performance
than FAST-Btree and FAST-Rtree, respectively. This is basically due to the fact that
FAST* flushing policy handles the flash memory updates better than the original
FAST flushing policy, hence when the index size increase the possibility that more
blocks are updated increases leading to such performance gain for both FAST*-Btree
and FAST*-Rtree. The take home message from this experiment is that FAST still
maintains its performance gain over its counterparts even with the large increase of
the index size.

Geoinformatica (2013) 17:417–448

140

# of Erase Operations *(103)

# of Erase Operations *(103)

442
FAST*–Btree
FAST–Btree
LA–tree
FD–tree

120
100
80
60
40
20
0

1200

FAST*–Rtree
FAST–Rtree
RFTL

1000
800
600
400
200
0

B
4G

B
2G

B

2M

B

6M

B

8M

B
1G

51

25

12

B
4G

B
2G

B

2M

B

6M

B

8M

B
1G

51

25

12

Index Size

Index Size

(a) WL

(b) Spatial-WU

Fig. 9 Effect of tree index size

9.4 Effect of number of updates

10000
9000
8000
7000
6000
5000
4000
3000
2000
1000
0

# of erase operations *(103)

# of erase operations *(103)

Figure 9 gives the effect of varying the number of update operations from one
million to 100 millions (in a log scale) on the number of erase operations for both
one-dimensional (i.e., FAST-Btree, LA-tree, and FD-tree in Fig. 10a) and multidimensional index structures (i.e., FAST-Rtree and RFTL in Fig. 10b). As we are
only interested in update operations, the workload for the experiments in this section
is just a stream of incoming update operations, up to 100 million operations. As
can be seen from the figure, FAST scales well with the number of updates and still
maintains its superior performance over its counterparts from both one-dimensional
(LA-tree) and multi-dimensional index structures (RFTL). FAST performs slightly
better than FD-tree; this is because FD-tree (one dimensional index structure) is
buffering some of the tree updates in memory and flushes them when needed, but
FAST applies a flushing policy, which flushes only the block with the highest number
of updates. In addition, FAST* slightly outperforms FAST because FAST* flushing
policy employs a Top-1 algorithm that maximizes the number of updates per block
and minimize the timestamp at which the block has been updated, hence the total
amortized update cost in FAST* is less than FAST.

FAST*–Btree
FAST–Btree
LA–tree
FD–tree

1

10

100

FAST*–Rtree
FAST–Rtree
RFTL

10000

1000

1

10

# of Updates *(106)

# of Updates *(106)

(a) FAST-Btree

(b) FAST-Rtree

Fig. 10 Effect of number of updates

100

Geoinformatica (2013) 17:417–448

443

9.5 Flushing performance

550
500
450
400
350
300
250
200
150
100
50
0
128

600

FAST*–Flush
FAST–Flush
Flush–All
Rand–Flush

256

512

# of erase operations *(103)

# of erase operations *(103)

Figure 11 illustrates the performance of the f lushing policy employed by FAST
compared to a naive flushing policy, termed f lush-all that just flushes all the memory
contents to the flash storage once. We also compare FAST to a random flushing
policy, termed Rand-Flush that chooses a block at random and flushed its contents
to the flash storage The performance is given with respect to various memory
sizes (Fig. 11a) and log file sizes (Fig. 11a). Both experiments were run for FASTBtree under workload WU . Running these experiments for FAST-Rtree and other
workloads give similar performance, and thus omitted for brevity.
Figure 11a gives the effect of varying the memory size from 128 KB to 1,024 KB
on the number of erase operations for flush-all policy, Rand-Flush policy and FAST
flushing policy. In all cases, FAST has much lower erase operations than the flushall and Rand-Flush policies, which is about one fourth of the erase operations for a
memory size of 512 KB. The main reason behind this gain in FAST is that it amortizes
the cost of the block erase operation over a large number of updates, and hence, will
free more memory with each flushing operation. On the other side, in the flush-all or
Rand-Flush policy, a block may be erased just because it has only one single update in
the memory. In this case, although a block is erased, it does not free much memory
space. The Rand-Flush policy performance is slightly better than that of flush-all
policy because the Rand-Flush flushes only one block and hence keeping all other
blocks in memory, which decrease the cost of random writes on these blocks.
FAST* flushing policy is better than FAST flushing policy as it better amortizes
the update cost. FAST* policy may still keep a block that has the highest number of
updates in memory if this block has higher potential to be updated soon, and hence
the decreasing the number of erase operations applied to that block.
Figure 11b gives a similar experiment to that of Fig. 11a with the exception that
we study the effect of changing the log size from 10 MB to 25 MB on the number
of erase operations. In all cases, FAST flushing policy is superior, which is intuitive
given the above explanation for Fig. 11a. However, an interesting observation from
Fig. 11b is that the gain from FAST flushing policy over the flush-all and RandFlush policies increases with the increase of the log file size. This means that FAST
flushing policy makes better use of the log file than the flush-all and Rand-Flush
policies. A justification for this is as follows: As FAST flushing policy evicts a block

1024

FAST*–Flush
FAST–Flush
Flush–All
Rand–Flush

500
400
300
200
100
0

10

15

20

Memory Size (KB)

Log Size (MB)

(a) Memory size

(b) Log size

Fig. 11 Flushing performance

25

444

Geoinformatica (2013) 17:417–448

to the storage only if it has high number of updates, the log entry for this flushing
operation will include many updated nodes. Then, in the log compaction process,
there will be a lot of space for compaction. This would not be the case for the flushall and Rand-Flush policies where a log entry for a flush operation may include only
one flushed node. Then, at the time of log compaction, there will be nothing much
to compact, which means that the log compaction will be called again. As discussed
in Section 9.2 and Fig. 9, log compaction is a major factor in the number of erase
operations. Reducing the frequency of log compaction makes FAST flushing policy
more superior than the flush-all policy. Moreover, FAST* flushing policy slightly
outperforms FAST flushing policy because of the fact that FAST* may prefer to
keep the block that has the highest number of updates in memory leading to less
erase operations on the flash memory storage.
9.6 Log compaction
Figure 12a gives the behavior and frequency of log compaction operations in FAST
when running a sequence of 200 thousands update operations for a log file size of
10 MB. The Y axis in this figure gives the size of the filled part of the log file, started
as empty. The size is monotonically increasing with having more update operations
till it reaches its maximum limit of 10 MB. Then, the log compaction operation is
triggered to compact the log file. As can be seen from the figure, the log compaction
operation may compact the log file from 20 to 60 % of its capacity, which is very
efficient compaction. Another take from this experiment is that we have made only
seven log compaction operations for 200 thousands update operations, which means
that the log compaction process is not very common, making FAST more efficient
even with a large amount of update operations.
9.7 Recovery performance
Figure 12b gives the overhead of the recovery process in FAST, which serves also
as the overhead of the log compaction process. The overhead of recovery increases
linearly with the size increase of the log file contents at the time of crash. This is
intuitive as with more log entries in the log file, it will take more time from the FAST

100

Recovery Time (millisec)

Log File Size (MB)

10
8
6
4
2
0

0

50

100

150

200

FAST

90
80
70
60
50
40
30
20
10

1

2

3

4

5

6

Number of Updates So Far *(103)

Log Size (MB)

(a) Log Compaction

(b) Recovery

Fig. 12 Log compaction and recovery

7

8

9

Geoinformatica (2013) 17:417–448

445

recovery module to scan this log file, and replay some of its operations to recover the
lost main memory contents. However, what we really want to emphasize on in this
experiment is that the overhead of recovery is only about 100 ms for a log file that
includes 9 MB of log entries. This shows that the recovery overhead is a low price to
pay to ensure transaction durability.

10 Conclusion
This paper presented FAST; a generic framework for flash-aware data-partitioning
tree index structures. FAST distinguishes itself from all previous attempts of flash
memory indexing in two aspects: (1) FAST is a generic framework that can be applied
to a wide class of tree index structures, and (2) FAST achieves both ef f iciency
and durability of read and write flash operations. FAST has four main modules,
namely, update, search, f lushing, and recovery. The update module is responsible
on buffering incoming tree updates in an in-memory data structure, while writing
small entries sequentially in a designated flash-resident log file. The search module
retrieves requested data from the flash storage and updates it with recent updates
stored in memory, if any. The f lushing module is responsible on evicting flash blocks
from memory to the flash storage to give space for incoming updates. Finally, the
recovery module ensures the durability of in-memory updates in case of a system
crash.

References
1. PostgreSQL. http://www.postgresql.org
2. Agrawal D, Ganesan D, Sitaraman RK, Diao Y, Singh S (2009) Lazy-adaptive tree: an optimized
index structure for flash devices. PVLDB
3. Agrawal N, Prabhakaran V, Wobber T, Davis J, Manasse M, Panigrahy R (2008) Design
tradeoffs for SSD performance. In: Usenix annual technical conference, USENIX
4. Bayer R, McCreight EM (1972) Organization and maintenance of large ordered indices. Acta
Inform 1:173–189
5. Beckmann N, Kriegel H-P, Schneider R, Seeger B (1990) The R*-tree: an efficient and robust
access method for points and rectangles. In: SIGMOD
6. Birrell A, Isard M, Thacker C, Wobber T (2007) A design for high-performance flash disks. ACM
SIGOPS Oper Syst Rev 41(2):88–93
7. Bouganim L, Jónsson B, Bonnet P (2009) uFLIP: understanding flash IO patterns. In: CIDR
8. Chang Y-H, Hsieh J-W, Kuo T-W (2007) Endurance enhancement of flash-memory storage
systems: an efficient static wear leveling design. In: Proceedings of the annual ACM IEEE Design
Automation Conference, DAC, pp 212–217
9. Chen S (2009) FlashLogging: exploiting flash devices for synchronous logging performance. In:
SIGMOD. New York, NY
10. Comer D (1979) The ubiquitous B-tree. ACM Comput Surv 11(2):121–137
11. Gray J (2006) Tape is dead, disk is tape, flash is disk, RAM locality is king. http://research.
microsoft.com/∼gray/talks/Flash_is_Good.ppt. Accessed Dec 2006
12. Gray J, Fitzgerald B (2008) Flash disk opportunity for server applications. ACM Queue 6(4):18–
23
13. Gray J, Graefe G (1997) The five-minute rule ten years later, and other computer storage rules
of thumb. SIGMOD Rec 26(4):63–68
14. Guttman A (1984) R-trees: a dynamic index structure for spatial searching. In: SIGMOD
15. Hellerstein JM, Naughton JF, Pfeffer A (1995) Generalized search trees for database systems.
In: VLDB
16. Hutsell W (2007) Solid state storage for the enterprise. Storage Networking Industry Association
(SNIA) Tutorial, Fall

446

Geoinformatica (2013) 17:417–448

17. Katayama N, Satoh, S (1997) The sr-tree: an index structure for high-dimensional nearest neighbor queries. In: SIGMOD
18. Kim H, Ahn S (2008) BPLRU: a buffer management scheme for improving random writes in
flash storage. In: FAST
19. Lavenier D, Xinchun X, Georges G (2006) seed-based genomic sequence comparison using a
FPGA/FLASH accelerator. In: ICFPT
20. Lee S, Moon B (2007) Design of flash-based DBMS: an in-page logging approach. In: SIGMOD
21. Lee S-W, Moon B, Park C, Kim J-M, Kim S-W (2008) A case for flash memory SSD in enterprise
database applications. In: SIGMOD
22. Lee S-W, Park D-J, sum Chung T, Lee D-H, Park S, Song H-J (2007) A log buffer-based flash
translation layer using fully-associate sector translation. TECS
23. Leventhal A (2008) Flash storage today. ACM Queue 6(4):24–30
24. Li Y, He B, Luo Q, Yi K (2009) Tree indexing on flash disks. In: ICDE
25. Li Y, He B, Yang RJ, Luo Q, Yi K (2010) Tree indexing on solid state drives. Proceedings of the
VLDB Endowment 3(1–2):1195–1206
26. Ma D, Feng J, Li G (2011) LazyFTL: A page-level flash translation layer optimized for NAND
flash memory. In: SIGMOD
27. McCreight EM (1977) Pagination of B*-trees with variable-length records. Commun ACM
20(9):670–674
28. Moshayedi M, Wilkison P (2008) Enterprise SSDs. ACM Queue 6(4):32–39
29. Nath S, Gibbons PB (2008) Online maintenance of very large random samples on flash storage.
In: VLDB
30. Nath S, Kansal A (2007) Flashdb: dynamic self-tuning database for NAND flash. In: IPSN
31. Reinsel D, Janukowicz J (2008) Datacenter SSDs: solid footing for growth. http://www.samsung.
com/us/business/semiconductor/news/downloads/210290.pdf. Accessed Jan 2008
32. Sellis TK, Roussopoulos N, Faloutsos C (1987) The R+-tree: a dynamic index for multidimensional objects. In: VLDB
33. Shah MA, Harizopoulos S, Wiener JL, Graefe G (2008) Fast scans and joins using flash drives.
In: International Workshop of Data Managment on New Hardware, DaMoN
34. White DA, Jain R (1996) Similarity indexing with the SS-tree. In: ICDE
35. Wu C, Chang L, Kuo T (2003) An efficient R-tree implementation over flash-memory storage
systems. In: GIS
36. Wu C, Kuo T, Chang L (2007) An efficient B-tree layer implementation for flash-memory storage
systems. TECS

Mohamed Sarwat is a PhD candidate at the Computer Science and Engineering department,
University of Minnesota, where he also received his master’s degree in computer science in 2011. His
research interest lies in the broad area of Database systems, spatio-temporal databases, distributed
graph databases, social networking, cloud computing, large-scale data management, data indexing
and storage systems. He has been awarded the University of Minnesota Doctoral Dissertation
Fellowship in 2012/2013. He has been a recipient of Best Research Paper Award in the 12th
international symposium on spatial and temporal databases 2011.

Geoinformatica (2013) 17:417–448

447

Mohamed F. Mokbel is an associate professor in the Department of Computer Science and Engineering, University of Minnesota. His current main research interests focus on providing database
and platform support for spatial data, moving objects, and location-based services. Mohamed is the
main architect for the PLACE, Casper, and CareDB systems that provide a database support for
location-based services, location privacy, and personalization, respectively. His research work has
been recognized by two best paper awards at IEEE MASS 2008 and MDM 2009 and by the NSF
CAREER award 2010. Mohamed is currently the general co-chair of SSTD 2011 and program cochair for MDM 2011, DMSN 2011, and LBSN 2011. Mohamed was also the proceeding chair of ACM
SIGMOD 2010, and the program co-chair for ACM SIGSPATIAL GIS 2008, 2009, and 2010. He
serves in the editorial board of IEEE Data Engineering Bulletin, Distributed and Parallel Databases
Journal, and Journal of Spatial Information Science. Mohamed is an ACM and IEEE member and a
founding member of ACM SIGSPATIAL.

Xun Zhou received his B.Eng., and M.Eng., in Computer Science and Technology from Harbin
Institute of Technology, Harbin, China in 2007 and 2009 respectively. He is currently a Ph.D. student
in Computer Science at the University of Minnesota, Twin Cities. His research interests include
spatiotemporal data mining, spatial databases and Geographical Information Systems (GIS). His
current application focus is understanding climate change from data.

448

Geoinformatica (2013) 17:417–448

Suman Nath is a researcher in the Sensing and Energy Research Group at Microsoft Research
Redmond. He works on various data management problems in mobile and sensing systems. He
received his PhD from Carnegie Mellon University in 2005. He has authored 20+ patents (granted or
pending), 70+ papers in various computer science conferences and journals, and received Best Paper
Awards at BaseNets 2004, USENIX NSDI 2006, IEEE ICDE 2008, and SSTD 2011. At Microsoft,
he received the Gold Star Award, which recognizes excellence in leadership and contributions for
Microsoft’s long-term success.

Matrix Factorization with Explicit Trust and Distrust Side Information
for Improved Social Recommendation
RANA FORSATI, Shahid Beheshti University and University of Minnesota
MEHRDAD MAHDAVI, Michigan State University
MEHRNOUSH SHAMSFARD, Shahid Beheshti University
MOHAMED SARWAT, University of Minnesota

With the advent of online social networks, recommender systems have became crucial for the success of
many online applications/services due to their significance role in tailoring these applications to user-specific
needs or preferences. Despite their increasing popularity, in general, recommender systems suffer from data
sparsity and cold-start problems. To alleviate these issues, in recent years, there has been an upsurge of
interest in exploiting social information such as trust relations among users along with the rating data to
improve the performance of recommender systems. The main motivation for exploiting trust information in
the recommendation process stems from the observation that the ideas we are exposed to and the choices
we make are significantly influenced by our social context. However, in large user communities, in addition
to trust relations, distrust relations also exist between users. For instance, in Epinions, the concepts of
personal “web of trust” and personal “block list” allow users to categorize their friends based on the quality
of reviews into trusted and distrusted friends, respectively. Hence, it will be interesting to incorporate this
new source of information in recommendation as well. In contrast to the incorporation of trust information
in recommendation which is thriving, the potential of explicitly incorporating distrust relations is almost
unexplored. In this article, we propose a matrix factorization-based model for recommendation in social rating
networks that properly incorporates both trust and distrust relationships aiming to improve the quality of
recommendations and mitigate the data sparsity and cold-start users issues. Through experiments on the
Epinions dataset, we show that our new algorithm outperforms its standard trust-enhanced or distrustenhanced counterparts with respect to accuracy, thereby demonstrating the positive effect that incorporation
of explicit distrust information can have on recommender systems.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and
Retrieval—Information filtering; I.2 [Computing Methodologies]: Artificial Intelligence; I.2.6 [Artificial
Intelligence]: Learning; J.4 [Computer Applications]: Social and Behavioral Sciences
General Terms: Design, Algorithms
Additional Key Words and Phrases: Matrix factorization, recommender systems, social relationships
ACM Reference Format:
Rana Forsati, Mehrdad Mahdavi, Mehrnoush Shamsfard, and Mohamed Sarwat. 2014. Matrix factorization
with explicit trust and distrust side information for improved social recommendation. ACM Trans. Inf. Syst.
32, 4, Article 17 (October 2014), 38 pages.
DOI: http://dx.doi.org/10.1145/2641564

Author’s addresses: R. Forsati (corresponding author) and M. Shamsfard, Natural Language Processing
(NLP) Research Lab, Faculty of Electrical and Computer Engineering, Shahid Beheshti University, G. C.,
Tehran, Iran; M. Mahdavi, Department of Computer Science and Engineering, Michigan State University,
East Lansing, MI; M. Sarwat, Computer Science and Engineering Department, University of Minnesota,
Minneapolis, MN; corresponding author’s email: rana.forsati@gmail.com.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by
others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
c 2014 ACM 1046-8188/2014/10-ART17 $15.00

DOI: http://dx.doi.org/10.1145/2641564
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17

17:2

R. Forsati et al.

1. INTRODUCTION

The huge amount of information available on the Web has made it increasingly challenging to cope with this information overload and find the most relevant information
one is really interested in. Recommender systems intend to provide users with recommendations of products they might appreciate, taking into account their past ratings,
purchase history, or interest. The recent proliferation of online social networks has further enhanced the need for such systems. Therefore, it is obvious why such systems are
indispensable for the success of many online applications such as Amazon, iTunes, and
Netflix to guide the search process and help users to effectively find the information or
products they are looking for [Miller et al. 2004]. Roughly speaking, the overarching
goal of recommender systems is to identify a subset of items (e.g., products, movies,
books, music, news, and webpages) that are likely to be more interesting to users based
on their interests [Deshpande and Karypis 2004; Wu et al. 2009; Forsati and Meybodi
2010; Bobadilla et al. 2013].
In general, most widely used recommender systems (RS) can be broadly classified
into content-based (CB), collaborative filtering (CF), or hybrid methods [Adomavicius
and Tuzhilin 2005]. In CB recommendation, one tries to recommend items similar
to those a given user preferred in the past. These methods usually rely on external
information, such as explicit item descriptions, user profiles, and/or the appropriate
features extracted from items to analyze item similarity or user preference to provide
recommendation. In contrast, CF recommendation, the most popular method adopted
by contemporary recommender systems, is based on the core assumption that similar
users on similar items express similar interest, and it usually relies on the rating information to build a model out of the rating information in the past without having
access to external information required in CB methods. The hybrid approaches proposed combine both CB- and CF-based recommenders to gain advantages and avoid
certain limitations of each type of systems [Good et al. 1999; Soboroff and Nicholas
1999; Pazzani 1999; Melville et al. 2002; Pavlov and Pennock 2002; Talabeigi et al.
2010; Forsati et al. 2013].
The essence of CF lies in analyzing the neighborhood information of past users and
items’ interactions in the user-item rating matrix to generate personalized recommendations based on the preferences of other users with similar behavior. CF has been
shown to be an effective approach to recommender systems. The advantage of these
types of recommender systems over content-based RS is that the CF-based methods
do not require an explicit representation of the items in terms of features, but is based
only on the judgments/ratings of the users. These CF algorithms are mainly divided
into two main categories [Gu et al. 2010]: memory-based methods (also known as
neighborhood-based methods) [Wang et al. 2006b; Chen et al. 2009] and model-based
methods [Hofmann 2004; Si and Jin 2003; Srebro and Jaakkola 2003; Zhang et al.
2006]. Recently, another direction in CF considers how to combine memory-based and
model-based approaches to take advantage of both types of methods, thereby building
a more accurate hybrid recommender system [Pennock et al. 2000; Xue et al. 2005;
Koren 2008].
The heart of memory-based CF methods is the measurement of similarity based
on ratings of items given by users: either the similarity of users (user-oriented CF)
[Herlocker et al. 1999], the similarity of items (items-oriented CF) [Sarwar et al. 2001],
or combined user-oriented and item-oriented collaborative filtering approaches to overcome the limitations specific to either of them [Wang et al. 2006a]. The user-oriented CF
computes the similarity among users, usually based on user profiles or past behavior,
and seeks consistency in the predictions among similar users [Yu et al. 2004; Hofmann
2004]. The item-oriented CF, on the other hand, allows input of additional item-wise

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:3

information and is also capable of capturing the interactions among them. If the rating
of an item by a user is unavailable, collaborative-filtering methods estimate it by computing a weighted average of known ratings of the items from the most similar users.
Memory-based collaborative filtering is most effective when users have expressed
enough ratings to have common ratings with other users, but it performs poorly for
so-called cold-start users. Cold-start users are new users who have expressed only a few
ratings. Thus, for memory-based CF methods to be effective, large amounts of userrating data are required. Unfortunately, due to the sparsity of the user-item rating
matrix, memory-based methods may fail to correctly identify the most similar users
or items, which in turn decreases the recommender accuracy. Another major issue
that memory-based methods suffer from is the scalability problem. The reason being
essentially the fact that when the number of users and items is very large, which
is common in many real-world applications, the search to identify the k most similar
neighbors of the active user is computationally burdensome. In summary, data sparsity
and non-scalability issues are two main issues current memory-based methods suffer
from.
To overcome the limitations of memory-based methods, model-based approaches have
been proposed, which establish a model using the observed ratings that can interpret
the given data and predict the unknown ratings [Adomavicius and Tuzhilin 2005]. In
contrast to memory-based algorithms, model-based algorithms try to model the users
based on their past ratings and use these models to predict the ratings on unseen items.
In model-based CF, the goal is to employ statistical and machine learning techniques
to learn models from the data and make recommendations based on the learned model.
Methods in this category include aspect model [Hofmann 2004; Si and Jin 2003], clustering methods [Kohrs and Merialdo 1999], Bayesian model [Zhang and Koren 2007],
and low-dimensional linear factor models such as matrix factorization (MF) [Srebro
et al. 2005; Srebro and Jaakkola 2003; Zhang et al. 2006; Salakhutdinov and Mnih
2008b]. Due to its efficiency in handling very huge datasets, matrix factorization-based
methods have become one of the most popular models among the model-based methods, for example, weighted low-rank matrix factorization [Srebro and Jaakkola 2003],
weighted nonnegative matrix factorization (WNMF) [Zhang et al. 2006], maximum
margin matrix factorization (MMMF) [Srebro et al. 2005], and probabilistic matrix factorization (PMF) [Salakhutdinov and Mnih 2008b]. These methods assume that user
preferences can be modeled by only a small number of latent factors [Dasgupta et al.
2002] and all focus on fitting the user-item rating matrix using low-rank approximations only based on the observed ratings. The recommender system we propose in this
article adheres to the model-based factorization paradigm.
Although latent factor models and in particular matrix factorization are able to
generate high-quality recommendations, these techniques also suffer from the data
sparsity problem in real-world scenarios and fail to address users who rated only a
few items. For instance, according to Sarwar et al. [2001], the density of non-missing
ratings in most commercial recommender systems is less than one or even much less.
Therefore, it is unsatisfactory to rely predictions on such small amounts of data, which
becomes more challenging in the presence of large number of users or items. This
observation necessitates tackling the data sparsity problem in an affirmative manner
to be able to generate more accurate recommendations.
One of the most prominent approaches to tackling the data sparsity problem is to
compensate for the lack of information in the rating matrix with other sources of
side information which are available to the recommender system. For example, social
media applications allow users to connect with each other and to interact with items
of interest such as songs, videos, pages, news, and groups. In such networks, the ideas
we are exposed to and the choices we make are significantly influenced by our social
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:4

R. Forsati et al.

context. More specifically, users generally tend to connect with other users due to some
commonalities they share, often reflected in similar interests. Moreover, in many reallife applications it may be the case that only social information about certain users
is available while interaction data between the items and those users has not yet
been observed. Therefore, the social data accumulated in social networks would be a
rich source of information for the recommender system to utilize as side information
to alleviate the data sparsity problem. To accomplish this goal, in recent years, the
trust-based recommender systems became an emerging field to provide users with
personalized item recommendations based on the historical ratings given by users and
the trust relationships among users (e.g., social friends).
Social-enhanced recommendation systems are becoming of greater significance and
practicality with the increased availability of online reviews, ratings, friendship links,
and follower relationships. Moreover, many e-commerce and consumer review websites
provide both reviews of products and a social network structure among the reviewers.
As an example, the e-commerce site Epinions [Guha et al. 2004] asks its users to indicate which reviews/users they trust and use this trust information to rank the reviews
of products. Similar patterns can be found in online communities such as Slashdot in
which millions of users post news and comment daily and are capable of tagging other
users as friends/foes or fans/freaks. Another example is the ski mountaineering site
Moleskiing [Avesani et al. 2005] which enables users to share their opinions about
the snow conditions of the different ski routes and also express how much they trust
the other users. Another well-known example is the FilmTrsut system [Golbeck and
Hendler 2006], an online social network that provides movie rating and review features
to its users. The social networking component of the website requires users to provide
a trust rating for each person they add as a friend. Also users on Wikipedia can vote
for or against the nomination of others to adminship [Burke and Kraut 2008]. These
websites have come to play an important role in guiding users’ opinions on products
and, in many cases, also influence their decisions in buying or not buying the product
or service. The results of experiments in Crandall et al. [2008] and of similar works
confirm that a social network can be exploited to improve the quality of recommendations. From this point of view, traditional recommender systems that ignore the social
structure between users may no longer be suitable.
A fundamental assumption in social-based recommender systems which has been
adopted by almost all of the relevant literature is that if two users have a friendship relation, then the recommendation from his or her friends probably has higher
trustworthiness than strangers. Therefore, the goal becomes how to combine the useritem rating matrix with the social/trust network of a user to boost the accuracy of the
recommendation system and alleviate the sparsity problem. Over the years, several
studies have addressed the issue of the transfer of trust among users in online social
networks. These studies exploit the fact that trust can be passed from one member
to another in a social network, creating trust chains, based on its propagative and
transitive nature.1 Therefore, some recommendation methods fusing social relations
by regularization [Jamali and Ester 2011; Li and Yeung 2009; Ma et al. 2011a; Zhu
et al. 2011] or factorization [Ma et al. 2008, 2011b; Salakhutdinov and Mnih 2008a,
2008b; Srebro and Jaakkola 2003; Salakhutdinov et al. 2007; Rennie and Srebro 2005]
were proposed that exploit trust relations in a social network.

1 We note that while the concept of trust has been studied in many disciplines, including sociology, psychology,
economics, and computer science from different perspectives, the issue of propagation and transitivity have
often been debated in literature, and different authors have reached different conclusions (see e.g., [Sherchan
et al. 2013] for a thorough discussion).

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:5

Also, the results of incorporating trust information in recommender systems is appealing and has been the focus of much researcher in the last few years, but in large
user communities, besides the trust relationship between users, the distrust relationships are also unavoidable. For example, Epinions provided the feature that enables
users to categorize other users in a personal web of trust list based on their quality
as a reviewer. Later on, this feature integrated with the concept of personal block list,
which reflects the members that are distrusted by a particular user. In other words, if
a user encounters a member whose reviews are consistently offensive, inaccurate, or
otherwise low quality, she can add that member to her block list. Therefore, it would be
tempting to investigate whether or not distrust information could be effectively utilized
to boost the accuracy of recommender systems as well.
In contrast to trust information for which there has been a great deal of research,
the potential advantage/disadvantage of explicitly utilizing distrust information is
almost unexplored. Recently, few attempts have been made to explicitly incorporate
the distrust relations in recommendation process [Guha et al. 2004; Ma et al. 2009b;
Victor et al. 2011b, 2013], which demonstrated that the recommender systems can
benefit from the proper incorporation of distrust relations in social networks. However,
despite these positive results, there are some unique challenges involved in distrustenhanced recommender systems. In particular, it has proven challenging to model
distrust propagation in a manner which is both logically consistent and psychologically
plausible. Furthermore, the naive modeling of distrust as negative trust raises a
number of challenges—both algorithmic and philosophical. Finally, it is an open
challenge how to incorporate trust and distrust relations in model-based methods
simultaneously. This article is concerned with these questions and gives an affirmative
solution to challenges involved with distrust-enhanced recommendation. In particular,
the proposed method makes it possible to simultaneously incorporate both trust and
distrust relationships in recommender systems to increase the prediction accuracy. To
the best of our knowledge, this is the first work that models distrust relations into the
matrix factorization problem along with trust relations at the same time.
The main intuition behind the proposed algorithm is that one can interpret the distrust relations between users as dissimilarity in their preferences. In particular, when
a user u distrusts another user v, it indicates that user u disagrees with most of the
opinions issued, or ratings made by user v. Therefore, the latent features of user u
obtained by matrix factorization must be as dissimilar as possible to v’s latent features. In other words, this intuition suggests directly incorporating the distrust into
recommendation by considering distrust as reversing the deviation of latent features.
However, when combined with the trust relations between users, due to the contradictory role of trust and distrust relations in propagating social information in the matrix
factorization process, this idea fails to effectively capture both relations simultaneously. This statement also follows from the preliminary experimental results in Victor
et al. [2011b] for memory-based CF methods that demonstrated regarding distrust as
an indication to reverse deviations in not the right way to incorporate distrust.
To remedy this problem, we settle for a less ambitious goal and propose another
method to facilitate the learning from both types of relations. In particular, we try
to learn latent features in a manner such that the latent features of users who are
distrusted by the user u have a guaranteed minimum dissimilarity gap from the worst
dissimilarity of users who are trusted by user u. By this formulation, we ensure that
when user u agrees on an item with one of his trusted friends, he will disagree on
the same item with his distrusted friends with a minimum predefined margin. We
note that this idea significantly departs from the existing works in distrust-enhanced
memory-based recommender systems [Victor et al. 2011b, 2013] that employ the distrust relations to either filter out or debug the trust relations to reduce the prediction
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:6

R. Forsati et al.

task to a trust-enhanced recommendation. In particular, the proposed method ranks
the latent features of trusted and distrusted friends of each user to reflect the effect of
relation in factorization.
Summary of Contributions. This work makes the following key contributions.
—A matrix factorization-based algorithm for simultaneous incorporation of trust and
distrust relationships in recommender systems. To the best of our knowledge, this is
the first model-based recommender algorithm that is able to leverage both types of
relationships in recommendation.
—An efficient stochastic optimization algorithm to solve the optimization problem
which makes the proposed method scalable to large social networks.
—An empirical investigation of the consistency of the social relationships with rating
information. In particular, we examine to what extent trust and distrust relations
between users are aligned with the ratings they issued on items.
—An exhaustive set of experiments on the Epinions dataset to empirically evaluate the
performance of the proposed algorithm and demonstrate its merits and advantages.
—A detailed comparison of the proposed algorithm to state-of-the-art trust/distrustenhanced memory/model-based recommender systems.
Outline. The rest of this article is organized as follows. In Section 2, we draw connections to and put our work in context of some of the most recent work on social
recommender systems. Section 3 formally introduces the matrix factorization problem,
an optimization-based framework to solve it, and its extension to incorporate the trust
relations between users. The proposed algorithm along with optimization methods are
discussed in Section 4. Section 5 includes our experimental result on the Epinions
dataset which demonstrates the merits of the proposed algorithm in alleviating the
data sparsity problem in rating matrix and generating more accurate recommendations. Finally, Section 6 concludes and discusses a few directions as future work.
2. RELATED WORK ON SOCIAL RECOMMENDATION

Earlier in the introduction, we discussed some of the main lines of research on recommender systems; here, we survey further lines of study that are most directly-related to
our work on social-enhanced recommendation. Many successful algorithms have been
developed over the past few years to incorporate social information in recommender
systems. After reviewing trust-enhanced memory-based approaches, we discuss some
model-based approaches for recommendation in social networks with trust relations.
Finally, we review major approaches in distrust modeling and distrust-enhanced recommender systems.
2.1. Trust-Enhanced Memory-Based Recommendation

Social network data has been widely investigated in the memory-based approaches.
These methods typically explore the social network and find a neighborhood of users
trusted (directly or indirectly) by a user and perform the recommendation by aggregating their ratings. These methods use the transitivity of trust and propagate trust
to indirect neighbors in the social network [Massa and Avesani 2004, 2009; Konstas
et al. 2009; Jamali and Ester 2009, 2010, 2011; Koren et al. 2009].
In Massa and Avesani [2004], a trust-aware collaborative filtering method for recommender systems is proposed. In this work, the collaborative filtering process is informed
by the reputation of users, which is computed by propagating trust. Konstas et al. [2009]
proposed a method based on the random walk algorithm to utilize social connection and
other social annotations to improve recommendation accuracy. However, this method
does not utilize the rating information and is not applicable to constructing a random
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:7

walk graph in real datasets. TidalTrust [Golbeck 2006] performs a modied breadthfirst search in the trust network to compute a prediction. To compute the trust value
between user u and v who are not directly connected, TidalTrust aggregates the trust
value between u’s direct neighbors and v weighted by the direct trust values of u and
its direct neighbors.
MoleTrust [Massa and Avesani 2004, 2005; Zhang and Koren 2007] applies the same
idea as TidalTrust, but MoleTrust considers all the raters up to a fixed maximumdepth given as an input, independent of any specific user and item. The trust metric
in MoleTrust consists of two major steps. First, cycles in trust networks are removed.
Therefore, removing trust cycles beforehand from trust networks can significantly
speed up the proposed algorithm because every user only needs to be visited once to
infer trust values. Second, trust values are calculated based on the obtained directed
acyclic graph by performing a simple graph random walk.
TrustWalker [Jamali and Ester 2009] combines trust-based and item-based recommendation to consider enough ratings without suffering from noisy data. Their experiments show that TrustWalker outperforms other existing memory-based approaches.
Each random walk on the user trust graph returns a predicted rating for user u on
target item i. The probability of stopping is directly proportional to the similarity between the target item and the most similar item j, weighted by the sigmoid function of
step size k. The more the similarity, the greater the probability of stopping and using
the rating on item j as the predicted rating for item i. As the step size increases, the
probability of stopping decreases. Thus ratings by closer friends on similar items are
considered more reliable than ratings on the target item by friends further away.
We note that all these methods are neighborhood-based methods which employ only
heuristic algorithms to generate recommendations. There are several problems with
this approach. The relationship between the trust network and the user-item matrix
has not been studied systematically. Moreover, these methods are not scalable to very
large datasets, since they may need to calculate the pairwise user similarities and
pairwise user trust scores.
2.2. Trust-Enhanced Model-Based Recommendation

Recently, researchers have exploited matrix factorization techniques to learn latent
features for users and items from the observed ratings, and fusing social relations
among users with rating data as will be detailed in Section 3. These methods can be
divided into two types: regularization-based methods and factorization-based methods.
Here we review some existing matrix factorization algorithms that incorporate trust
information in the factorization process.
2.2.1. Regularization-Based Social Recommendation. Regularization-based methods typically add a regularization term to the loss function and minimizes it. Most recently, Ma
et al. [2011a] proposed an idea based on social-regularized matrix factorization to make
recommendation based on social network information. In this approach, the social regularization term is added to the loss function, which measures the difference between
the latent feature vector of a user and those of his friends. A probability model similar
to the model in Ma et al. [2011a] is proposed by Jamali and Ester [2011]. The graph
Laplacian regularization term of social relations is added into the loss function in
Li and Yeung [2009] and minimizes the loss function by alternative projection algorithm. Zhu et al. [2011] used the same model in Li and Yeung [2009] and built
graph Laplacian of social relations using three kinds of kernel functions. In Liu et al.
[2013], the minimization problem is formulated as a low-rank semidefinite optimization
problem.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:8

R. Forsati et al.

2.2.2. Factorization-Based Social Recommendation. In factorization-based methods, social
relationships between users are represented as a social relation matrix, which is factored as well as the rating matrix. The loss function is the weighted sum of the social relation matrix factorization error and the rating matrix factorization error. For instance,
SoRec [Ma et al. 2008] incorporates the social network graph into the probabilistic matrix factorization model by simultaneously factorizing the user-item rating matrix and
the social trust networks by sharing a common latent low-dimensional user feature
matrix [Liu et al. 2013]. The experimental analysis shows that this method generates
better recommendations than the non-social filtering algorithms [Jamali and Ester
2010]. However, the disadvantage of this work is that although users’ social networks
are integrated into the recommender systems by factorizing the social trust graph,
the real-world recommendation processes are not reflected in the model. Two sets of
different feature vectors are assumed for users, which makes the interpretability of
the model very hard [Jamali and Ester 2010; Ma et al. 2009a]. This drawback not only
causes lack of interpretability in the model, but also affects the recommendation qualities. A better model named Social Trust Ensemble (STE) [Ma et al. 2009a] is proposed,
by making the latent features of a user’s direct neighbors affect the rating of the user.
Their method is a linear combination of a basic matrix factorization approach and a social network-based approach. Experiments show that their model outperforms the basic
matrix factorization-based approach and existing trust-based approaches. However, in
their model, the feature vectors of direct neighbors of u affect the ratings of u instead of
affecting the feature vector of u. This model does not handle trust propagation. Another
method for recommendation in social networks has been proposed in Ma et al. [2009b].
This method is not a generative model and defines a loss function to be minimized.
The main disadvantage of this method is that it punishes the users with lots of social
relations more than other users. Finally, SocialMF [Jamali and Ester 2010] is a matrix
factorization-based model which incorporates social influence by making the features
of every user depend on the features of his/her direct neighbors in the social network.
2.3. Distrust-Enhanced Social Recommendation

In contrast to incorporation of trust relations, unfortunately most of the literature on
social recommendation totally ignores the potential of distrust information in boosting
the accuracy of recommendations. In particular, only recently a few work have started
to investigate the rule of distrust information in the recommendation process, both
from theoretical and empirical viewpoints [Guha et al. 2004; Ziegler and Lausen 2005;
Nalluri 2008; Ziegler 2009; Ma et al. 2009b; Wierzowiecki and Wierzbicki 2010; Victor
et al. 2011b, 2011c, 2013; Verbiest et al. 2012]. Although these studies have shown
that distrust information can be plentiful, but there is a significant gap in clear understanding of distrust in recommender systems. The most important reasons for this
shortage are the lack of datasets that contain distrust information and dearth of a
unified consensus on modeling and propagation of distrust.
A formal framework of trust propagation schemes, introducing the formal and computational treatment of distrust propagation, has been developed in Guha et al. [2004].
In an extension of this work, Ziegler [2009] proposed clever adaptations in order to
handle distrust and sinks such as trust decay and normalization. In Wierzowiecki and
Wierzbicki [2010], a trust/distrust propagation algorithm called CloseLook is proposed,
which is capable of using the same kinds of trust propagation as the algorithm proposed
by Guha et al. [2004]. Leskovec et al. [2010a] extended the results of Guha et al. [2004]
using a machine-learning framework (instead of the propagation algorithms based
on an adjacency matrix) to enable the evaluation of the most informative structural
features for the prediction task of positive/negative links in online social networks. A
comprehensive framework that computes trust/distrust estimations for user pairs in
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:9

the network using trust metrics is built in Victor et al. [2011c]: given two users in the
trust network, we can search for a path between them and propagate the trust scores
along this path to obtain an estimation. When more than one path is available, we
may single out the most relevant ones (selection), and aggregation operators can then
be used to combine the propagated trust scores into one final trust score, according to
different trust score propagation operators.
Ma et al. [2009b] was the first seminal work to demonstrate that the incorporation
of distrust information could be beneficial based on a model-based recommender system. In Victor et al. [2011c, 2013], the same question is addressed in memory-based
approaches. In particular, Victor et al. [2013] embarked upon the distrust-enhanced recommendation and showed that with careful incorporation of distrust metric, distrustenhanced recommender systems are able to outperform their trust-only counterparts.
The main rational behind the algorithm proposed in Victor et al. [2013] is to employ
the distrust information to debug or filter out the users’ propagated web of trust. It is
also has been realized that the debugging methods must exhibit a moderate behavior
in order to be effective. Verbiest et al. [2012] addressed the problem of considering the
length of the paths that connect two users for computing trust-distrust between them,
according to the concept of trust decay. This work also introduced several aggregation
strategies for trust scores with variable path lengths.
Finally we note that the aforementioned works try to either model or utilize
trust/distrust information. In recent years, there has been an upsurge of interest in
predicting trust and distrust relations in a social network [Leskovec et al. 2010a;
DuBois et al. 2011; Bachi et al. 2012; Patil et al. 2013]. For instance, Leskovec et al.
[2010a] casts the problem as a sign prediction problem (i.e., +1 for friendship and −1
for opposition) and utilizes machine learning methods to predict the sign of links in the
social network. In DuBois et al. [2011] a new method is presented for computing both
trust and distrust by combining an inference algorithm that relies on a probabilistic
interpretation of trust based on random graphs with a modified spring-embedding algorithm to classify an edge. Another direction of research is to examine the consistency
of social relations with theories in social psychology [Cartwright and Harary 1956;
Leskovec et al. 2010b]. Our work significantly departs from these works on prediction
or consistency analysis of social relations, and aims to effectively incorporate distrust
information in matrix factorization for effective recommendation.
3. MATRIX FACTORIZATION-BASED RECOMMENDER SYSTEMS

This section provides a formal definition of collaborative filtering, the primary recommendation method we are concerned with in this article, followed by solution methods
for low-rank factorization that are proposed in the literature to address the problem.
(See Table I for Common notations and their meanings.)
3.1. Matrix Factorization for Recommendation

In collaborative filtering, we assume that there is a set of n users U = {u1 , . . . , un} and
a set of m items I = {i1 , . . . , im}, where each user ui expresses opinions about a set of
items. In this article, we assume opinions are expressed through an explicit numeric
rating (e.g., scale from one to five), but other rating methods such as hyperlink clicks are
possible as well. We are mainly interested in recommending a set of items for an active
user such that the user has not rated these items before. To this end, we are aimed
at learning a model from the existing ratings, that is, offline phase, and then use the
learned model to generate recommendations for active users, that is, online phase. The
rating information is summarized in an n × m matrix R ∈ Rn×m, 1 ≤ i ≤ n, 1 ≤ j ≤ m,
where the rows correspond to the users and the columns correspond to the items, and
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:10

R. Forsati et al.
Table I. Summary of Notations Consistently Used in the Article and Their Meaning
Symbol

Meaning

U = {u1 , . . . , un}, n
I = {i1 , . . . , im}, m
k
R ∈ Rn×m
R , |R |
U ∈ Rn×k
V ∈ Rm×k
S ∈ {−1, +1}n×n
S , |S |
W ∈ Rn×n
+
N (u) ⊆ [n]
N+ (u) ⊆ [n]
N− (u) ⊆ [n]
D : Rk × Rk → R+

The set of users in system and the number of users
The set of items and the number of items
The dimension of latent features in factorization
The partially observed rating matrix
The set of observed entires in rating matrix and its size
The matrix of latent features for users
The matrix of latent features for items
The social network between n users
The set of extracted triplets from the social relations and its size
The pairwise similarity matrix between users
Neighbors of user u in the social graph
The set of trusted neighbors by user u in the social graph
The set of distrusted neighbors by user u in the social graph
The measurement function used to assess the similarly of latent features

the ( p, q)th entry is the rate given by user up to the item iq . We note that the rating
matrix is partially observed, and it is sparse in most cases.
An efficient and effective approach to recommender systems is to factorize the useritem rating matrix R by a multiplicative of k-rank matrices R ≈ UV , where U ∈ Rn×k
and V ∈ Rm×k utilize the factorized user-specific and item-specific matrices, respectively,
to make further missing data prediction. The main intuition behind a low-dimensional
factor model is that there is only a small number of factors influencing the preferences,
and that a user’s preference vector is determined by how each factor applies to that
user. This low rank assumption makes it possible to effectively recover the missing
entires in the rating matrix from the observed entries. We note that the celebrated
Singular Value Decomposition (SVD) method for factorizing the rating matrix R is
not applicable here due to the fact that the rating matrix is partially available and
we are only allowed to utilize the observed entries in factorization process. There are
two basic formulations to solve this problem: optimization based (see e.g., [Rennie and
Srebro 2005; Liu et al. 2013; Ma et al. 2008; Koren et al. 2009]) and probabilistic [Mnih
and Salakhutdinov 2007]. In the following sections, we first review the optimizationbased framework for matrix factorization and then discuss how it can be extended to
incorporate trust information.
3.2. Optimization-Based Matrix Factorization

Let R be the set of observed ratings in the user-item matrix R ∈ Rn×m, that is,
R = {(i, j) ∈ [n] × [m] : Ri j has been observed},
where n is the number of users and mis the number of items to be rated. In optimizationbased matrix factorization, the goal is to learn the latent matrices U and V by solving
the following optimization problem:
⎡
⎤
 

1
λ
λ
2
U
V

min ⎣L(U, V) =
UF +
VF ⎦ ,
Ri j − Ui,:
(1)
V j,: +
U,V
2
2
2
(i, j)∈R

where  · F is the Frobenius norm of a matrix, that is, AF =

	
 

n
m
i=1

j=1

|Ai j |2 .

The optimization problem in Eq. (1) constitutes of three terms: the first term aims
to minimize the inconsistency between the observed entries and their corresponding
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:11

value obtained by the factorized matrices. The last two terms regularize the latent
matrices for users and items, respectively. The parameters λU and λV are regularization
parameters that are introduced to control the regularization of latent matrices U and
V, respectively. We would like to emphasize that the problem in Eq. (1) is non-convex
jointly in both U and V. However, despite its non-convexity, the formulation in Eq. (1)
is widely used in practical collaborative filtering applications, as the performance is
competitive, or better as compared to trace-norm minimization, while scalability is
much better. For example, as indicated in Koren et al. [2009], to address the Netflix
problem, Eq. (1) has been applied with a fair amount of success to factorize datasets
with 100 million ratings.
3.3. Matrix Factorization with Trust Side Information

Recently it has been shown that just relying on the rating matrix to build a recommender system is not as accurate as expected. The main reason for this claim is the
known cold-start users problem and the sparsity of the rating matrix. Cold-start users
are one of the most important challenges in recommender systems. Since cold-start
users are more dependent on the social network compared to users with more ratings,
the effect of using trust propagation gets more important for cold-start users. Moreover,
in many real-life systems, a very large portion of users do not express any ratings, and
they only participate in the social network. Hence, using only the observed ratings does
not allow us to learn the user features.
One of the most prominent approaches to tacking the data sparsity problem in matrix factorization is to compensate for the lack of information in rating matrix with
other sources of side information which are available to the recommender system. It
has been recently shown that social information, such as trust relationship between
users, is a rich source of side information to compensate for the sparsity. The already
mentioned traditional recommendation techniques are all based on working on the
user-item rating matrix, and ignore the abundant relationships among users. Trustbased recommendation usually involves constructing a trust network where nodes are
users and edges represent the trust placed on them. The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the
opinions of other users in the trust network. The intuition is that users tend to adopt
items recommended by trusted friends rather than strangers, and that trust is positively and strongly correlated with user preferences. Recommendation techniques that
analyze trust networks were found to provide very accurate and highly personalized
results.
To incorporate the social relations in the optimization problem formulated in Eq. (1),
a few papers [Ma et al. 2009b, 2011a; Jamali and Ester 2011; Liu et al. 2013; Zhu et al.
2011] proposed the social regularization method which aims at keeping the latent
vector of each user similar to his/her neighbors in the social network. The proposed
models force the user feature vectors to be close to those of their neighbors to be able to
learn the latent user features for users with no or very few ratings [Jamali and Ester
2011]. More specifically, the optimization problem becomes
L(U, V) =

2 λU
1  
λV

UF +
VF
V j,: +
Ri j − Ui,:
2
2
2
(i, j)∈R



n 


λS  
1

+
U j,: 
Ui,: − |N (i)|
,
2

i=1 
j∈N (i)

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

(2)

17:12

R. Forsati et al.

where λ S is the social regularization parameter and N (i) is the subset of users who has
relationship with the ith user in the social graph.
The rationale behind this social regularization idea is that every user’s taste is
relatively similar to the average taste of his friends in the social network. We note that
in using this idea, latent features of users indirectly connected in the social network
will be dependent, and hence the trust gets propagated. A more reasonable and realistic
model should treat all friends differently based on how similar they are. Let us assume
the weight of a relationship between two users i and j is captured by Wi j , where
W ∈ Rn×n denotes the social weight matrix. It is easy to extend the model in Eq. (2) to
treat friends differently based on the weight matrix W as
2 λU
1  
λV

UF +
VF
Ri j − Ui,:
L(U, V) =
V j,: +
(3)
2
2
2
(i, j)∈R




n

λS  
j∈N (i) Wi j U j,: 

+
Ui,: − 

.

2
j∈N (i) Wi j 
i=1

An alternative formulation is to regularize each user’s friends individually, resulting
in the following objective function [Ma et al. 2011a]:
2 λU
1  
λV

L(U, V) =
UF +
VF
Ri j − Ui,:
V j,: +
2
2
2
(i, j)∈R

n

2
λS 
+
Wi j Ui,: − U j,:  ,
2
i, j=1

where we simply assumed that for any j ∈
/ N (i), Wi j = 0.
As mentioned earlier, the objective function in L(U, V) is not jointly convex in both
U and V, but it is convex in each of them fixing the other one. Therefore, to find a local
solution, one can stick to the standard gradient descent method to find a solution in an
iterative manner as follows:
Ut+1 ← Ut − ηt ∇U L(U, V)|U=Ut ,V=Vt ,
Vt+1 ← Vt − ηt ∇V L(U, V)|U=Ut ,V=Vt .
4. MATRIX FACTORIZATION WITH TRUST AND DISTRUST SIDE INFORMATION

In this section, we describe the proposed algorithm for social recommendation which
is able to incorporate both trust and distrust relationships in the social network along
with the partially observed rating matrix. We then present two strategies to solve the
derived optimization problem, one based on the gradient descent optimization algorithm which generates more accurate solutions but is computationally cumbersome,
and another based on the stochastic gradient descent method which is computationally
more efficient for large rating and social matrices but suffers from slow convergence
rate.
4.1. Algorithm Description

As already discussed, the vast majority of related work in the field of matrix factorization for recommendation has primarily focused on trust propagation and has
simply ignored the distrust information between users or, intrinsically, is not capable
of exploiting it. Now, we aim at developing a matrix factorization-based model for recommendation in social rating networks to utilize both trust and distrust relationships.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:13

We incorporate the trust/distrust relationship between users in our model to improve
the quality of recommendations. While intuition and experimental evidence indicate
that trust is somewhat transitive, distrust is certainly not transitive. Thus, when we
intend to propagate distrust through a network, questions about transitivity and how
to deal with conflicting information abound.
To inject social influence in our model, the basic idea is to find appropriate latent
features for users such that each user is brought closer to the users she/he trusts and
separated from the users that she/he distrusts and who have different interests. We
note that simply incorporating this idea in matrix factorization by naively penalizing
the similarity of each user’s latent features to his distrusted friends’ latent features
fails to reach the desired goal. The main reason being that distrust is not as transitive
as trust, that is, distrust can not directly replace trust in trust propagation approaches,
and utilizing distrust requires careful consideration (trust is transitive, i.e., if user u
trusts user v and v trusts w, there is a good chance that u will trust w, but distrust is
certainly not transitive, i.e., if u distrusts v and v distrusts w, then w may be closer to
u than v or maybe even farther away). It is noticeable that this statement is consistent
with the preliminary experimental results in Victor et al. [2011b] for memory-based
CF methods that indicate regarding distrust as an indication to reverse deviations in
not the right way to incorporate distrust. Therefore, we pursue another approach to
model the distrust in the recommendation process.
The main intuition behind the proposed framework stems from the observation that
trust relations between users can be treated as agreement on items and distrust relations can be considered as disagreement on items. Then, the question becomes how can
we guarantee that when a user agrees on an item with one of his/her friends, he/she will
disagree on the same item with his/her distrusted friends with a reasonable margin.
We note that this margin should be large enough to make it possible to distinguish
between two types of friends. In terms of latent features, this observation translates to
having a margin between the similarity and dissimilarity of users’ latent features to
his/her trusted and distrusted friends.
Alternatively, one can view the proposed method from the viewpoint of connectivity of
latent features in a properly designated graph. Intuitively, certain features or groups of
features should influence how users connect in the social network, and thus it should
be possible to learn a mapping from features to connectivity in the social network
such that the mapping respects the underlying structure of the social network. In the
basic matrix factorization algorithm for recommendation, we can consider the latent
features as isolated vertices of a graph where there is no connection between nodes.
This can be generalized to the social-enhanced setting by considering the social graph
as the underlying graph between latent features with two types of edges (i.e., trust
and distrust relations correspond to positive and negative edges, respectively). Now
the problem reduces to learning the latent features for each user u such that users
trusted by u in the social network (with positive edges) are close and users which are
distrusted by u (with negative edges) are more distant. Learning latent features in this
manner respects the inherent topology of the social network.
Figure 1 shows an example to illustrate the intuition behind this idea. For ease of
exposition, we only consider the latent features for the user u1 . From the trust network
in Figure 1(a), we can see that user u1 trusts the list of users N+ = {u2 , u4 , u6 , u7 },
and from the distrust network in Figure 1(b), we see that user u1 distrusts the list
of users N− = {u3 , u5 }. The goal is to learn the latent features that obeys two goals:
(i) it minimizes the prediction error on observed entries in the rating matrix, (ii) it
respects the underlying structure of the trust and distrust networks between users.
In Figure 1(d), the latent features are depicted in the Euclidean space from the viewpoint of user u1 . As shown in Figure 1(d), for user u1 , the latent features of his/her
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:14

R. Forsati et al.

Fig. 1. A simple example with seven users {u1 , u2 , . . . , u7 } and six items {i1 , i2 , . . . , i6 } to illustrate the main
intuition behind the proposed algorithm. The inputs of the algorithm are (a) trust network, (b) distrust
network, and (c) partially observed rating matrix R, respectively. As shown in (d) for user u1 , the learned
latent features for all his trusted friends {u2 , u4 , u6 , u7 } are closer to u1 ’s latent features than his distrusted
friends {u3 , u5 } with a margin of 1.

trusted friends N+ lie inside the solid circle centered at u1 , and the latent features of
his/her distrusted friends N− lie outside the dashed circle. The gap between two circles
guarantees that there always exists a safe margin between u1 ’s agreements with his
trusted and distrusted friends. One simple way to impose these constraints on the latent features of users is to generate a set of triplets for any combination of trusted and
distrusted friends (e.g., one such triplet for user u1 can be constructed as (u1 , u2 , u5 ))
and force the margin constraint to hold for all extracted triplets. This ensures that the
minimum margin gap will definitely exist between the latent features of all the trusted
and distrusted friends as desired and makes it possible to incorporate both types of
relationships between users in the matrix factorization.
It is worth mentioning that similar to the social-enhanced recommender systems
previously discussed, the proposed algorithm is also based on hypotheses about the
existence and the correlation of trust/distrust relations and ratings in the data. The
empirical investigation of correlation between social relations and rating information
has been the focus of a bulk of recent research including [Ziegler and Golbeck 2007;
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:15

Patil et al. 2013; Ma 2013], where the results reinforce the hypothesis that ratings
from trusted people count more than those from others and in particular distrusted
neighbors. We have also conducted experiments, as will be detailed in Section 5.5, to empirically investigate the correlation/alignment between social relations and the rating
information issued by users which supports our strategy in exploiting the trust/distrust
relations in matrix factorization.
We now formalize the proposed solution. As the first ingredient, we need a measure
to evaluate the consistency between the latent features of users, that is, the matrix U,
and the trust and distrust constraints existing between users in the social network. To
this end, we introduce a monotonically-increasing convex loss function (z) to measure
the discrepancy between the latent features of different users. Let ui , u j , and uk be
three users in the model such that ui trusts u j but distrusts uk. The main intuition
behind the proposed framework is that the latent features of ui , that is, Ui,: must be
more similar to u j ’s latent features than latent features for user uk. For each such
a triplet, we penalize the objective function by (D(Ui,: , U j,: ) − D(Ui,: , Uk,: )), where the
function D : Rk ×Rk → R+ measures the similarity between two latent vectors assigned
to two different users, and  : R → R+ is a penalty function that is utilized to assess
the violation of latent vectors of trusted and distrusted users. Example loss functions
include hinge loss (z) = max(0, 1 − z) and logistic loss (z) = log(1 + e−z ), which are
widely used convex surrogates of 0-1 loss function in learning community.
Let S denote the set of extracted triplets from the social relations, that is,


S = (i, j, k) ∈ [n] × [n] × [n] : Si j = 1 & Sik = −1 .
Here, a positive relationship means friends or a trusted relationship and a negative
relationship means foes or a distrust relationship. Then, our goal becomes to find a
factorization of matrix R such that the learned latent features of users are consistent
with the constraints in S , where the consistency is reflected in the loss function. This
results in the following optimization problem:
2 λU
1  
λV

L(U, V) =
UF +
VF
Ri j − Ui,:
V j,: +
2
2
2
(i, j)∈R

+

λS
|S |



(D(Ui,: , U j,: ) − D(Ui,: , Uk,: )).

(4)

(i, j,k)∈S

Let us make this general formulation more specific by setting (·) and D(·, ·) to be the
hinge loss and the Euclidian distance, respectively. Under these two assumptions, the
objective can be formulated as
2 λU
1  
λV

L(U, V) =
VF
Ri j − Ui,:
V j,: + UF +
2
2
2
(i, j)∈R



R(U,V)

λS
+
|S |





max 0, 1 − Ui,: − U j,: 2 + Ui,: − Uk,: 2 .

(5)

(i, j,k)∈S

Here the constraints have been written in terms of hinge-losses over triplets, each
consisting of a user, his/her trusted friend, and his/her distrusted friend. Solving the
optimization problem in Eq. (5) outputs the latent features for users and items that
can be utilized to estimate the missing values in the user-item matrix. Comparing the
formulation in Eq. (5) to the existing factorization-based methods discussed earlier
reveals two main features of the proposed formulation. First, it aims to minimize
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:16

R. Forsati et al.

the error on the observed ratings and to respect the inherent structure of the social
network among the users. The trade-off between these two objectives is captured by
the regularization parameter λ S which is required to be tuned effectively.
In a similar way, applying the logistic loss to the general formulation in Eq. (4) yields
the following objective:
2 λU
1  
λV

L(U, V) =
UF +
VF
V j,: +
Ri j − Ui,:
2
2
2
(i, j)∈R

+

λS
|S |






log 1 + exp Ui,: − Uk,: 2 − Ui,: − U j,: )2 .

(6)

(i, j,k)∈S

Remark 4.1. We note that in several applications of recommender systems, besides
the observed ratings, a description of the users and/or the objects through attributes
(e.g., gender, age) or measures of similarity is available that could potentially benefit the
process of recommendation (see, e.g., [Agarwal and Chen 2010] for a few interesting
applications). In that case, it is tempting to take advantage of both known ratings
and descriptions to model the preferences of users. A natural way to incorporate the
available metadata is to kernalize the similarity measure between latent features based
on a positive definite kernel between pairs that can be deduced from the metadata.
More specifically, instead of simply using Euclidian distance as the similarity measure
between latent features in Eq. (5), we can use the kernel matrix K obtained from the
Laplacian of the graph obtained from the metadata to measure the similarity
 


D(Ui,: , U j,: ) = Ui,: − U j,: K Ui,: − U j,: ,


where K = (D − W)−1 , with D as a diagonal matrix with Di,i = nj=1 Wi j . Here, W
captures the pairwise weight between users in the similarity graph between users that
is computed based on the available metadata about users.
Remark 4.2. We would like to emphasize that it is straightforward to generalize the
proposed framework to incorporate similarity and dissimilarity information between
items. What we need is to extract the triplets from the trust/distrust links between
items and repeat the same process we did for users. This will add another term to the
objective in terms of latent features of items V, as shown in the following generalized
formulation:
2 λU
1  
λV

L(U, V) =
UF +
VF
Ri j − Ui,:
V j,: +
2
2
2
(i, j)∈R

+
+

λS
|S |
λI
|I |





max 0, 1 − Ui,: − U j,: 2 + Ui,: − Uk,: 2

(i, j,k)∈S





max 0, 1 − Vi,: − V j,: 2 + Vi,: − Vk,: 2 ,

(i, j,k)∈I

where λ I is the regularization parameter and I is the set of triplets extracted from
the similar/dissimilar links between items. The similarity/dissimilarity links between
items can be constructed according to tags issued by users or associated with items,
and categories. For example, if two items are attached with a same tag, there is a trust
link between them, and otherwise a distrust link. Alternatively, trust/distrust links
can be extracted by measuring similarity/dissimilarity based on the item properties or
profile if provided. This could further improve the accuracy of recommendations.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:17

ALGORITHM 1: GD-based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, S
2: Output: U and V
3: for t = 1, . . . , T do
4:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
5:
Compute ∇U by Eq. 7
6:
Compute ∇V by Eq. 8
7:
Update:
Ut+1 = Ut − ηt ∇U |U=Ut ,V=Vt
Vt+1 = Vt − ηt ∇V |U=Ut ,V=Vt
8: end for
9: return UT +1 and VT +1 .

4.2. Batch Gradient Descent-Based Optimization

In optimization for supervised machine learning, there exist two regimes in which
popular algorithms tend to operate: the stochastic approximation regime, which
samples a small dataset per iteration, typically a single data point, and the batch or
sample average approximation regime, in which larger samples are used to compute an
approximate gradient. The choice between these two extremes outlines the well-known
trade-off between inexpensive noisy steps and expensive but more reliable steps.
Two preliminary examples of these regimes are the Gradient Descent (GD) and the
Stochastic Gradient Descent (SGD) methods, respectively. Both GD and SGD methods
start with some initial point and iteratively update the solution using the gradient
information at intermediate solutions. The main difference is that GD requires a full
gradient information at each iteration, while SGD only requires an unbiased estimate
of the full gradient which can be done by sampling.
We now discuss the application of the GD algorithm for solving the optimization
problem in Eq. (5), as detailed in Algorithm 1. Recall that the objective function is
not jointly convex in both U and V. On the other hand, the objective is convex in one
parameter by fixing the other one. Therefore, we follow an iterative method to minimize
the objective. At each iteration, first by fixing V, we take a step in the direction of the
negative gradient for U and repeat the same process for V by fixing U.
For ease of exposition, we introduce further notation. For any triplet (i, j, k) ∈ S , we
note that Ui,: − U j,: 2 − Ui,: − Uk,: 2 can be written as Tr(CU U), where Tr(·) denotes
the trace of the input matrix and C is a sparse auxiliary matrix defined for each triplet
with all entries equal to zero except: Cik = Cki = C j j = 1 and Ckk = Ci j = C ji = −1.
Having defined this notation, we can write the objective in Eq. (5) as
L(U, V) = R(U, V) +

λV
λS
λU
UF +
VF +
2
2
|S |






max 0, 1 − Tr Cikj U U .

(i, j,k)∈S

where Cikj is the C matrix previously defined which is associated with triplet (i, j, k).
To apply the GD method, we need to compute the gradient of L(U, V) with respect to
U and V, which we denote by ∇U = ∇U L(U, V) and ∇V = ∇V L(U, V), respectively. We
have
∇U = ∇U R(U, V) + λU U −

λS
|S |





k
1[Tr(Cikj U U)<1] UCik
j + UCi j ,

(i, j,k)∈S

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

(7)

17:18

R. Forsati et al.

where 1[·] is the indicator function which takes a value of one if its argument is true,
and zero otherwise. Similarly for ∇V , we have
∇V = ∇V R(U, V) + λV V.

(8)

The main shortcoming of the GD method is its high computational cost per iteration
due to the gradient computation (i.e., step (7)) which is expensive when the size of
social constraints S is large. We note that the size of S can be as large as O(n3 )
by considering all triplets in the social graph. In the next section, we provide an
alternative solution to resolving this issue using the stochastic gradient descent and
mini-batch SGD methods which are more efficient than the GD method in terms of the
computational cost per iteration but with a slow convergence rate in terms of target
approximation error.
4.3. Stochastic and Mini-Batch Optimization

As discussed, when the size of the social network is very large, the size of S may cause
computational problems in solving the optimization problem in Eq. (5) using the GD
method. The reason is essentially the fact that computing the gradient at each iteration
requires going through all the triplets in S , which is infeasible for large networks.
To alleviate this problem, we propose a stochastic gradient-based [Nemirovski et al.
2009] method for solving the optimization problem. The main idea is to choose a
fixed subset of triplets for gradient computation instead of all |S | triplets at each
iteration [Cotter et al. 2011]. More specifically, at each iteration, we sample B triplets
uniformly at random from S to compute the next solution. We note that this strategy
generates unbiased estimates of the true gradient and makes each iteration of the
algorithm computationally more efficient compared to the full gradient counterpart.
In the simplest case, the SGD algorithm, only one triplet is chosen at each iteration
to generate an unbiased estimate of the full gradient. We note that in practice, SGD
is usually implemented based on data shuffling, that is, making the sequence of the
training samples random and then training the model by going through the training
samples one by one. An intermediate solution, known as mini-batch SGD, chooses
a subset of triplets to compute the gradient. The promise is that by selecting more
triplets at each iteration, on one hand the variance of stochastic gradients decreases
promotional to the number of sampled triplets, and on the other hand the algorithm
enjoys the light computational cost of basic SGD method.
The detailed steps of the algorithm are shown in Algorithm 2. The mini-batch SGD
method improves the computational efficiency by grouping multiple constraints into a
mini-batch and only updating the U and V once for each mini-batch. For brevity, we
will refer to this algorithm as Mini-SGD. More specifically, the Mini-SGD algorithm,
instead of computing the full gradient over all triplets, samples B triplets uniformly at
random from S , where 1 ≤ B ≤ |S | is a parameter that needs to be provided to the
algorithm, and computes the stochastic gradient as


λS 
k
1[Tr(Cikj Ut Ut )<1] UCik
∇t =
j + UCi j ,
B
(i, j,k)∈ B

where  B is the set of B sampled triplets from S . We note that



λS
k
1[Tr(Cikj Ut Ut )<1] UCik
E[∇t ] =
j + UCi j ,
|S |
(i, j,k)∈S

that is, ∇t is an unbiased estimate of the full gradient in the right-hand side. When
B = |S |, each iteration handles the original objective function, and Mini-SGD reduces
to the batch GD algorithm. We note that both GD and SGD share the same convergence
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:19

ALGORITHM 2: Mini-SGD-Based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, S , min batch size B
2: Output: U and V
3: for t = 1, . . . , T do
4:
∇t ← 0
5:
for b = 1, . . . , B do
6:
(i, j, k) ← Sample random triplet from S
7:
if (1 − Ui,: − U j,: )2 + Ui,: − Uk,: 2 > 0) then
8:
∇t ← Ut Cikj U
t
9:
end if
10:
end for
11:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
12:
Update:


λS
Ut+1 = Ut − ηt ∇U R(Ut , Vt ) + λU Ut +
∇t
B|S |
13:

Update:



Vt+1 = Vt − ηt ∇V R(Ut , Vt ) + λV Vt

14: end for
15: return UT +1 and VT +1 .

rate in terms of the√number of iterations in expectation for non-smooth optimization
problems (i.e., O(1/ T ) after T iterations), but the SGD method requires much less
running time to convergence compared to the GD method due to the efficiency of its
individual iterations.
5. EXPERIMENTAL RESULTS

In this section, we conduct exhaustive experiments to demonstrate the merits and
advantages of the proposed algorithm. We conduct the experiments on the wellknown Epinions2 dataset, aiming to accomplish and answer the following fundamental
questions.
(1) Prediction accuracy. How does the proposed algorithm perform in comparison to
the state-of- the-art algorithms with/without incorporating trust and distrust relationships between users. Whether or not the trust/distrust social network could
help in making more accurate recommendations?
(2) Correlation of social relations with rating information. To what extent are the
trusted and distrusted friends of a user u aligned with the ratings user u issued
for the reviews written by his friends? A positive answer to this question indicates
that two users will issue similar (dissimilar) ratings if they are connected by a trust
(distrust) relation and prefer to behave similarly.
(3) Model selection. What role do the regularization parameters λ S , λU , and λV play in
the accuracy of the proposed recommender system and what is the best strategy to
tune these parameters?
(4) Handling cold-start users. How does exploiting social relationships in the prediction
process affect the performance of recommendation for cold-start users?
(5) Trading trust for distrust. To what extent can the distrust relations compensate for
the lack of trust relations?
2 http://www.trustlet.org/wiki/Epinions

datasets.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:20

R. Forsati et al.

(6) Efficiency of optimization. What is the trade-off between accuracy and efficiency by
moving from the gradient descent to the stochastic gradient descent with different
batch sizes?
In the following sections, we intend to answer these questions. We begin by introducing
the dataset we use in our experiments and the metrics we employ to evaluate the
results, followed by the detailed experimental results.
5.1. Dataset Description and Experimental Setup

The Epinions Dataset. We begin by discussing the dataset we have chosen for our
experiments. To evaluate the proposed algorithm on trust and distrust-aware recommendations, we use the Epinions dataset [Guha et al. 2004], a popular e-commerce site
and customer review website where users share opinions on various types of items such
as electronic products, companies, and movies, through writing reviews about them or
assigning a rating to the reviews written by other users. The rating values in Epinions
are discrete values ranging from not helpful (1/5) to most helpful (5/5). These ratings
and reviews could potentially influence future customers when they are about to decide
whether a product is worth buying or a movie is worth watching.
Epinions allows users to evaluate other users based on the quality of their reviews
and to make trust and distrust relations with other users in addition to the ratings.
Every member of Epinions can maintain a “trust” list of people he/she trusts that
is referred to as web of trust (social network with trust relationships) based on the
reviewers with consistent ratings or “distrust” list known as block list (social network
with distrust relationships) for reviewers whose reviews were consistently found to
be inaccurate or low quality. The fact that the dataset contains explicit positive and
negative relations between users makes it very appropriate for studying issues in
trust- and distrust-enhanced recommender systems. Epinions is thus an ideal source
for experiments on social recommendation. We remark that the Epinions dataset only
contains bivalent relations (i.e., contains only full trust and full distrust, and no gradual
statements).
To conduct the coming experiments, we sampled a subset of the Epinions dataset
with n = 121, 240 users and m = 685, 621 different items. The total number of observed
ratings in the sampled dataset is 12,721,437, which approximately includes 0.02% of
all entries in the rating matrix R which demonstrates the sparsity of the rating matrix.
We note that the selected items are the most frequently rated overall. The statistics
of the dataset are given in Table II. The social statistics of the this data source are
summarized in Table III. The frequencies of ratings for users are shown are Table IV.
In the user distrust network, the total number of issued distrust statements is 96,823.
As to the user trust network, the total number of issued trust statements is 481,799.
Experimental Setup. To better evaluate the effect of utilizing the social side information in recommendation accuracy, we employ different amount of training data 90%,
80% , 70%, and 60% to create four different training sets that are increasingly sparse,
but the social network remains the same in all of them. Training data 90%, for example, means we randomly select 90% of the ratings from the sampled Epinions dataset
as the training data to predict the remaining 10% of ratings. The random selection
was carried out five times independently to have a fair comparison. Also, since our
preliminary results on a smaller dataset revealed that hinge loss performs better than
exponential loss, in the rest of experiments, we stick to this loss function. However, we
note that exponential loss is slightly faster in optimizing the corresponding objective
function thanks to its smoothness, but it was negligible considering its worse accuracy
compared to hinge loss. All implementations are in Matlab, and all experiments were
performed on a 4-core 2.0GHZ of a load-free machine with a 12G of RAM.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:21

Table II. Statistics of Sample Data from Epinions Dataset
Used in Our Experiments
Statistic
Number of users
Number of items
Number of ratings
Number of trust relations
Number of distrust relations
Minimum number of ratings by users
Minimum number of ratings for items
Maximum number of ratings by users
Maximum number of ratings for items
Average number of ratings by users
Average number of ratings for items

Quantity
121,240
685,621
12,721,437
481,799
96,823
1
1
148735
945
85.08
15.26

Table III. Maximum and Average Trust and Distrust Relations
for Users in the Sampled Dataset
Statistics

Trust per user

Be Trusted per user

Max
Min
Average

1983
1
4.76

2941
0
4.76

Max
Min
Average

Distrust per user
1188
1
0.91

Be Distrusted per user
429
0
0.91

Table IV. Frequencies of User’s Rating
# of Ratings
# of Users

0–10
4,198,074
(≈33%)

11–20
3,053,144
(≈24%)

21–30
2,289,858
(≈18%)

31–40
1,526,572
(≈12%)

41–50
534,300
(≈4.2%)

51–60
267,150
(≈2.1%)

# of Ratings
# of Users

61–70
157,745
(≈1.24%)

71–80
143,752
(≈1.13%)

81–90
104,315
(≈0.82%)

91–100
43,252
(≈0.34%)

101–200
21,626
(≈0.17%)

201–300
10,686
(≈0.084%)

5.2. Metrics

5.2.1. Metrics for Rating Prediction. We employ two well-known measures, mean absolute
error (MAE) and root mean squared error (RMSE) [Herlocker et al. 2004] to measure the prediction accuracy of the proposed approach in comparison with other basic
collaborative filtering and trust/distrust-enhanced recommendation methods.
MAE is a very appropriate and useful measure for evaluating prediction accuracy in
offline tests [Herlocker et al. 2004; Massa and Avesani 2004]. To calculate MAE, the
predicted rating is compared with the real rating and the difference (in absolute value)
considered as the prediction error. Then, these individual errors are averaged over all
predictions to obtain the overall MAE value. More precisely, let T denote the set of
ratings to be predicted, that is, T = {(i, j) ∈ [n] × [m], Ri j needs to be predicted} and let
R̂ denote the prediction matrix obtained by algorithm after factorization. Then,


(i, j)∈T |Ri j − R̂i j |
,
MAE =
|T |
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:22

R. Forsati et al.

where Ri j is the real rating assigned by user i to item j, and R̂i j is the rating user i
would assign to item j that is predicted by the algorithm.
The RMSE metric is defined as


2
(i, j)∈T (Ri j − R̂i j )
RMSE =
.
|T |
The first measure (MAE) considers every error of equal value, while the second one
(RMSE) emphasizes larger errors. We would like to emphasize that even small improvements in RMSE are considered valuable in the context of recommender systems.
For example, the Netflix prize competition offered $ 1,000,000 reward for a reduction
of the RMSE by 10% [Victor et al. 2013].
5.2.2. Metrics for Evaluating the Correlation of Ratings with Trust/Distrust Relations. As part of
our experiments, we investigate how the explicit trust/distrust relations between users
in the social network are aligned with the implicit trust/distrust relations between
users conveyed from the rating information. We use recall, mean average precision
(MAP) [Manning et al. 2008], and normalized discount cumulative gain (NDCG) to
evaluate the ranking results. Recall is defined as the number of relevant friends divided
by the total number of friends in the social network. Precision is defined as the number
of relevant friends (trusted or distrusted) divided by the number of friends in the social
network. Given a user u, let ri be the relevance score of the friend ranked at position i,
where ri = 1 if the user is relevant to the u and ri = 0 otherwise. Then we can compute
the average precision (AP) as


i ri × Precision@i
.
AP =
# of relevant friends
MAP is the average of AP over all the users in the network.
NDCG is a normalization of the discounted cumulative gain (DCG) measure. DCG
is a weighted sum of the degree of relevancy of the ranked users. The weight is a
decreasing function of the rank (position) of the user, and therefore called discount.
NDCG normalizes DCG by the ideal DCG (IDCG), which is simply the DCG measure
of the best-ranking result. Thus NDCG measure is always a number in [0, 1]. NDCG
at position k is defined as
k

2ri − 1
NDCG@k = Zk
,
log(i + 1)
i=1

where k is also called the scope, which means the number of top-ranked users presented
to the user and Zk is chosen such that the perfect ranking has an NDCG value of 1.
We note that the base of the logarithm does not matter for NDCG, since constant
scaling will cancel out due to normalization. We will assume it is the natural logarithm
throughout this article.
5.3. Model Selection

Tuning of parameters (a.k.a model selection in learning community) is a critical problem in most of the learning problems. In some situations, the learning performance
may drastically vary with different choices of parameters. There are three parameters
in Eq. (5) that play very important roles in the effectivity of the proposed algorithm.
These are λU , λV , and λ S . Between these, λ S controls how much the proposed algorithm
should incorporate the information of the social network in completing the partially
observed rating matrix. In the extreme case, a very small value for λ S , the algorithm
almost forgets that social information exists between the users and only utilizes the
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:23

Fig. 2. Grid search to find the best values for λU and λC on the dataset with 90% of rating information.

observed user-item rating matrix for factorization. On the other hand, if we employ
a very large value for λ S , the social network information will dominate the learning
process, leading to poorer performance. Therefore, in order to not hurt the recommendation performance, we need to find a reasonable value for a social regularization
parameter. To this end, we analyze how the combination of these parameters affect the
recommendation performance.
We conduct a grid search on the potential values of two parameters λ S and λV to find
the combination with the best performance. Figure 2 shows the grid search results for
these parameters on a dataset with 90% of training data, where the optimal prediction
accuracy is achieved at point (14.8, 11) with the optimal RMSE = 1.12. We would
like to emphasize that we have done the cross-validation only for pairs of (λ S , λV ) and
(λ S , λU ), considering (i) the grid search for the triplet (λ S , λU , λV ) is computationally
burdensome, and (ii) our preliminary experiments showed that λV and λU behave
similarly with respect to λ S . Based on the results reported in Figure 2, in the remaining
experiments, we set λ S = 14.8, λV = 11, and λU = 13 when training is performed on
the dataset with 90% rating information. We repeat the same process to find out the
best setting of regularization parameters for other datasets with 80%, 70%, and 60%
rating data as well.
5.4. Baseline Methods

Here we briefly discuss the baseline algorithms against which we intend to compare
the proposed algorithm. The baseline algorithms are chosen from both types of
memory-based and model-based recommender systems with different types of trust
and distrust relations. In particular, we consider the following basic algorithms.
—MF (Matrix Factorization-based Recommender). This is the basic matrix
factorization-based recommender formulated in the optimization problem in Eq. (1),
which does not take social data into account.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:24

R. Forsati et al.

—MF+T (Matrix Factorization with Trust Information). To exploit the trust relations
between users in matrix factorization, Ma et al. [2009b] relied on the fact that the
distance between latent features of users who trust each other must be minimized
and can be formulated as the following objective:
n
1 
min
D(Ui,: , U j,: ),
U 2
i=1 j∈N+ (i)

where N+ (i) is the set of users the ith user trusts in the social network (i.e., Si j = +1).
By employing this intuition in the basic formulation in Eq. (1), Ma et al. [2009b]
solved the following optimization problem:
⎡
⎤
n


 

1
α
λ
λ
2
U
V

min ⎣
Ri j − Ui,:
V j,: +
D(Ui,: , U j,: ) +
UF +
VF ⎦ .
U,V
2
2
2
2
i=1 j∈N+ (i)

(i, j)∈R

—MF+D (Matrix Factorization with Distrust Information). The basic intuition behind
the algorithm proposed in Ma et al. [2009b] to exploit the distrust relations is as
follows: if user ui distrusts user u j , then we can assume that their corresponding
latent features Ui,: and U j,: would have a large distance. As a result, we aim to
maximize the following quantity for all users:
max
U

n
1 
D(Ui,: , U j,: ),
2
i=1 j∈N− (i)

where N− (i) denotes the set of users the ith users distrusts (i.e., Si j = −1). Adding this
term to the basic optimization problem in Eq. (1), we obtain the following optimization
problem:
⎡
⎤
n





1
β
λU
λV
2

min ⎣
Ri j − Ui,:
V j,: −
D(Ui,: , U j,: ) +
UF +
VF ⎦ .
U,V
2
2
2
2
(i, j)∈R

i=1 j∈N− (i)

—MF+TD (Matrix Factorization with Trust and Distrust Information). This algorithm
stands for the algorithm proposed in the present work. We note that there is no algorithm in the literature that exploits both trust and distrust relations in factorization
process simultaneously.
—NB (Neighborhood-Based Recommender). This algorithm is the basic memory-based
recommender algorithm that predicts a rating of a target item i for user u using a
combination of the ratings of neighbors of u (similar users) that already issued a
rating for item i. Formally,


u
 ∈N (u),Wuu
 >0 Wuu
 (Rui − R̄u)


R̂ui = R̄u +
,
(9)
u
 ∈N (u),Wuu
 Wuu
where the pairwise weight Wuu
 between pair of users (u, u
 ) is calculated by the
Pearson’s correlation coefficient [Herlocker et al. 2004]
—NB+T (Neighborhood with Trust Information) [Massa and Avesani 2004, 2009;
Golbeck 2005]. The basic idea behind the trust-based recommender systems proposed in TidalTrsut [Golbeck 2005] and MoleTrsut [Massa and Avesani 2004] is to
limit the set of neighbors in Eq. (9) to the users who are trusted by user u. The
distinguishing feature of these algorithms is the mechanism of trust propagation to
estimate the trust transitively for all the users. By adapting Eq. (9) to only consider
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

trustworthy neighbors in predicting the new ratings, we obtain


u
 ∈N+∗ (u),Wuu
 >0 Wuu
 (Rui − R̄u)


R̂ui = R̄u +
,
u
 ∈N+∗ (u),Wuu
 >0 Wuu

17:25

(10)

where N+∗ (u) is the set of trusted neighbors of u in the social network with propagated
trust relations (when there is no propagation, we have N+∗ (u) = N+ (u)). We note that
instead of Pearson’s correlation coefficient as the weighting schema, we can infer
the weights exploiting the social relation between the users. Since for the dataset
we consider in our experiments, the trust/distrust relations are binary values, the
social-based pairwise distance would be simply the hamming distance between the
binary vector representation of social relations of users. For implementation details,
we refer to Victor et al. [2011a, Chapter 6].
—NB+TD-F (Neighborhood with Trust Information and Distrust Information as Filtration) [Victor et al. 2011b, 2013]. A simple strategy for using distrust relations
in the recommendation is to filter out distrusted users from the list of neighbors in
predicting the ratings. As a result, we adapt Eq. (9) to exclude distrusted users from
the users’ propagated web of trust.
—NB+TD-D (Neighborhood-Based with Trust Information and Integrated Distrust Information) [Victor et al. 2011b, 2013]. In the same spirit as the filtration strategy,
we can use distrust relations to debug the trust relations. More specifically, if user u
trusts user v, v trusts w, and u distrusts w, then the latter distrust relation contradicts the propagation of the trust from u to w and can be excluded from the prediction.
In this method, distrust is used to debug the trust relations.
5.5. On the Consistency of Social Relations and Rating Information

As already mentioned, the Epinions website allows users to write reviews about products and services and to rate reviews written by other users. Epinions also allows
users to define their web of trust, that is, “reviewers whose reviews and ratings have
been consistently found to be valuable” and their block list, that is, “reviewers whose
reviews are found to be consistently inaccurate or not valuable”. Different intuitions
on interpreting these social information will result in different models. The main rationale behind incorporating trust and distrust relations in recommendation process
is to take the trust/distrust relations between users in the social network as the level
of agreement between ratings assigned to reviews by users.3 Therefore, investigating
the consistency or alignment between user ratings (implicit trust) and trust/distrust
relations in the social network (explicit trsut) become an important issue.
Here, we aim to empirically investigate whether or not there is a correlation between
a user’s current trustees/friends or distrusted friends and the ratings that user would
assign to reviews issued by his neighbors. Obviously, if there is no correlation between
the social context of a user and his/her ratings to reviews written by his neighbors,
then the social structure does not provide any advantage to the rating information.
On the other hand, if there exists such a correlation, then the social context could be
supplementary information to compensate for the lack of rating information to boost
the accuracy of recommendations.
The consistency of trust relations and rating information issued by users on the
reviews written by his trustees has been analyzed [Ziegler and Golbeck 2007; Guo et al.
2014]. However, Ziegler and Golbeck [2007] also claimed that social trust (i.e., explicit
trust) and similarity between users based on their issued ratings (i.e., implicit trust) are
3 In

the literature, the similarity between users conveyed from the rating information issued by users and
the direct relation in the social network are usually referred to as implicit and explicit trust, respectively.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:26

R. Forsati et al.
Table V. Consistency of Implicit and Explicit Trust Relations in the Dataset for Different Ranges of
Ratings Measured in Terms of NDCG, Recall, and MAP
# of Ratings
0–20
21–40
41–60
61–80
≥81

NDCG@10

NDCG@20

Recall@10

Recall@20

Recall@40

MAP

0.083
0.108
0.117
0.120
0.135

0.078
0.103
0.112
0.117
0.126

0.054
0.080
0.083
0.088
0.091

0.092
0.125
0.128
0.132
0.151

0.156
0.198
0.225
0.230
0.253

0.140
0.190
0.208
0.230
0.244

Table VI. Consistency of Implicit and Explicit Distrust Relations in the Dataset for Different Ranges
of Ratings Measured in Terms of NDCG, Recall, and MAP
# of Ratings
0–20
21–40
41–60
61–80
≥81

NDCG@10

NDCG@20

Recall@10

Recall@20

Recall@40

MAP

0.065
0.071
0.082
0.089
0.104

0.057
0.068
0.072
0.078
0.096

0.045
0.060
0.075
0.081
0.087

0.071
0.077
0.085
0.105
0.125

0.132
0.140
0.158
0.164
0.191

0.130
0.134
0.152
0.160
0.183

not the same, and can be used complementary. According to Ma [2013], when comparing
implicit social information with explicit social information, the performance of using
implicit information is slightly worse. We further investigate the same question about
the consistency of distrust relations and ratings issued by users to their distrusted
neighbors. The positive answer to this question can be interpreted as follows. Given
that user u is interested in item i, the chances that v, trusted (distrusted) by u, also likes
this item i is much higher (lower) than for user w not explicitly trusted (distrusted)
by u.
To measure the similarity between users, there are several methods we can borrow
in the literature. In this article, we adopt the most popular approach that is referred to
as the Pearson correlation coefficient (PCC) P : U × U → [−1, +1] [Breese et al. 1998;
Massa and Avesani 2009], which is defined as

m
i=1 (Rui − R̄u)(Rvi − R̄v )
P(u, v) = 	

, ∀u, v ∈ U,

m
m
2×
2
(R
−
R̄
)
(R
−
R̄
)
ui
u
vi
v
j=1
j=1
where R̄u and R̄v are the average of ratings issued by users u and v, respectively. The
PCC measures the extent to which there is a linear relationship between the rating
behaviors of the two users, the extreme values being −1 and 1. The similarity of two
users becomes negative when users have completely diverging ratings. We note that
this quantity can be considered as the implicit trust between users that is conveyed
via ratings given by users.
To conduct this set of experiments, we first group all the users in the training data
set based on the number of ratings, and then measure the prediction accuracies of
different user groups. Users are grouped into five classes: [1, 20), [20, 40), [40, 60),
[60, 80), and >81. In order to have a comprehensive view of the ranking performance,
we present the NDCG, recall, and MAP scores of trust and distrust alignments on the
Epinions dataset in Table V and Table VI, respectively. We note that the dataset we
use in our experiments only contains bivalent trust values, that is, −1 and +1, and it
is not possible to have an ordering on the list of friends (time stamp of relations would
be an option to order the friends, but unfortunately it is not available in our dataset).
To compute the NDCG, we use the ordering of trusted/distrusted friends which yields
the best value.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:27

Table VII. Alignment Rate of Users in Establishing Trust/Distrust Relationships with Future Users in the
Social Network Based on the Majority Vote of Their Current Trusted/Distrusted Friends
Setting

Type of Relation (u ; w)

% of Relations

Alignment Rate (%)

n+ > n−

+
−
+
−
+

48.80
2.54
1.15
8.02
39.49

92.09
8.15
17.88
83.42
−

n+ < n−
n+ = n− > 0 or n+ = n− = 0

Note: The number of trusted friends (+) and distrusted friends (−) are denoted by n+ and n− ,
respectively. Here u denotes the current user and w stands for a future user in the network.

On the positive side, we observe a clear trend of alignment between ratings assigned
by a user and the type of relation he has made in the social network. This observation
coincides with our intuition. Overall, when more ratings are observed for a user, the
similarity calculation process will find more accurate similar or dissimilar neighbors
for this user, since we have more information to represent or interpret this user. Hence,
by increasing the number of ratings, it is conceivable from the results in Tables V
and VI that the alignment between implicit and explicit neighbors becomes better. By
comparing the results in Tables V and VI we can see that trust relations are slightly
better aligned than the distrust relations.
On the negative side, the results show that the NDCG on both types of relations is
small. One explanation for this phenomenon is that the Epinions dataset is not tightly
bound to a specific application. For example, a user may trust or distrust anther user
based on his/her comments on a specific product, but they might have similar taste
on other products. Furthermore, compared to other datasets such as FilmTrusts, the
Epinions dataset is a very sparse dataset, and consequently it is relatively inaccurate
to rely on the rating information to compute the implicit trust relations. Finally, our
approach to distinguishing trust/distrust lists from the rating information is limited
by the PCC trust metric we have utilized. We conjecture that better trust metrics
able to exploit other side information, such as time and interactional information,
would be helpful in distinguishing implicit trusted/distrusted friends, leading to better
alignment between implicit and explicit trust relations.
We also conduct experiments to evaluate the consistency of social network only
based on the trust/distrust relations between users. In particular, we investigate to
what extent a user’s relations are aligned with the opinion of his/her neighbors in
the social network. More specifically, let u be a user who is about to make a trust or
distrust relation to another user v. We assume that n+ number of u’s neighbors trust v
and n− number of u’s neighbors distrust v. We note that in the real dataset, the distrust
relations are hidden. To conduct this set of experiments, we randomly sample 30% of
the relations from the social network and use the remaining 70% to predict the type of
sampled relations4 by majority voting.
Table VII shows the results on the consistency of social relations. We observe that in
all cases there is an alignment between the opinions of a user’s friends and his/her own
relation (92.09% and 83.42% when the majority of friends trust and distrust the target
user, respectively). This might be due to the social influence of people on the social
network; however, it is hard to justify the existence of such a correlation in the Epinions
dataset which includes reviews for a diverse set of products and taste of users. One
interesting observation from the results reported in Table VII is the case where the
number of distrusted users dominates the number of trusted users (i.e., n− > n+ ). While
4A

more realistic way would be to use the time stamp of relations to create the training and test sets.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:28

R. Forsati et al.

Table VIII. Accuracy of Prediction of Matrix Factorization with Three Different Methods Measured in Terms of
MAE and RMSE Errors
k

% of Training

10

60%
70%
80%
90%

20

60%
70%
80%
90%

MF

MF+T

MF+D

MF+TD

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

Measure

0.9813 ± 0.042
1.6050 ± 0.032
0.9462 ± 0.083
1.5327 ± 0.032
0.9150± 0.022
1.3824 ± 0.032
0.8921 ± 0.025
1.2166 ± 0.017

0.8561 ± 0.032
1.4125 ± 0.022
0.8332 ± 0.092
1.2407 ± 0.063
0.8206 ± 0.041
1.1906 ± 0.042
0.8158 ± 0.016
1.1403 ± 0.027

0.9720 ± 0.038
1.5036 ± 0.040
0.9241 ± 0.012
1.4405 ± 0.023
0.8722 ± 0.034
1.3155 ± 0.026
0.8736 ± 0.053
1.1869 ± 0.049

0.8310 ± 0.016
1.2294 ± 0.086
0.8206 ± 0.023
1.1562 ± 0.043
0.8113 ± 0.032
1.1061 ± 0.021
0.8025 ± 0.014
1.0872 ± 0.020

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9972 ± 0.016
1.6248 ± 0.014
0.9688 ± 0.019
1.5162 ± 0.016
0.9365 ± 0.025
1.4081 ± 0.015
0.9224 ± 0.016
1.2207 ± 0.0 18

0.8431 ± 0.018
1.3904 ± 0.042
0.8342 ± 0.062
1.2722 ± 0.027
0.8172 ± 0.011
1.1853 ± 0.023
0.8128 ± 0.021
1.1402 ± 0.026

0.9746 ± 0.060
1.5423 ± 0.046
0.9350 ± 0.022
1.4540 ± 0.075
0.8705 ± 0.016
1.3591 ± 0.073
0.8805 ± 0.032
1.1933 ± 0.028

0.8475 ± 0.012
1.1837 ± 0.023
0.8290 ± 0.034
1.1452 ± 0.016
0.8129 ± 0.025
1.1049 ± 0.082
0.8096 ± 0.010
1.0851 ± 0.011

Note: The parameter k represents the number of latent features in factorization.

the distrust relations are private to other users, we can see that there is a significant
alignment between a users relation type and his distrusted friends.
5.6. On the Power of Utilizing Social Relationships

We now turn to investigate the effect of utilizing social relationships between users
on the accuracy of recommendations in factorization-based methods. In other words,
we would like to experimentally evaluate whether incorporating distrust can indeed
enhance the trust-based recommendation process. To this end, we run four different
MF (i.e., pure matrix factorization-based algorithm), MF+T (i.e., matrix factorization
with only trust relationships), MF+D (i.e., matrix factorization with only distrust relationships), and MF+TD (i.e., the algorithm proposed here) algorithms on the dataset.
We run the algorithms with k = 10 and k = 20 latent vector dimensions. As mentioned
earlier, different amounts of training data 90%, 80% , 70%, and 60% have been used to
create four different training sets that are increasingly sparse, but the social network
remains the same in all of them. We evaluate all algorithms by both MAE and RMSE
measures.
Table VIII shows the MAE and RMSE errors for the four sampled datasets. First, as
we expected, the performance of all learning algorithms improves with an increasing
number of training data. It is also not surprising to see that the MF+T, MF+D, and
MF+TD algorithms which exploit social side information perform better than the pure
matrix factorization-based MF algorithm. Second, the proposed algorithm outperforms
all other baseline algorithms for all the cases, indicating that it is effective for incorporating both types of social side information in the recommendation. This result by itself
indicates that besides trust relationships in the social network, distrust information is
also a rich source of information and can be utilized in recommendation algorithms. We
note that distrust information needs to be incorporated carefully, as its nature is totally
different from trust information. Finally, it is noticeable that MF+T outperforms the
MF+D algorithm due to a huge number of trust relations to distrust relations in our
dataset. It is also remarkable that users are more likely to be influenced by their friends
to make trust relations than the distrust relations due to the private nature of distrust
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:29

Table IX. Comparison with Other Popular Methods
Method

Parameter (s)

MAE

RMSE

MF
MF+T
MF+D
MF+TD

k = 10 and λU = λV = 5
k = 10, λU = λV = 5 , and α = 1
k = 10, λU = λV = 5 , and β = 10
k = 10, λU = 13, λV = 11 , and λ S = 14.8

0.8921
0.8158
0.8736
0.8025

1.2166
1.1403
1.1852
1.0872

NB
NB+T
NB+TD-F
NB+TD-D

p=1
p = 1 and q = 3
p = 1 and q = 3

0.9381
0.8904
0.8692
0.8728

1.5275
1.3455
1.2455
1.2604

Note: The reported values are the MAE and RMSE on the dataset with 90%
rating information. The values of parameters for each specific algorithm are
included in the second column.

relations in the Epinions dataset. This might lead us to believe that distrust relations
have better quality than trust relations, which requires a deeper investigation to be
verified.
5.7. Comparison to Baseline Algorithms

Another question that is worth investigating is how state-of-the-art approaches perform compared to the method proposed here. To this end, we compare the performance of the MF-TD algorithm with the baseline algorithms introduced in Section 5.4.
Table IX contains the results of our experiments with eight different algorithms on the
dataset with 90% rating data. The second column in the table represents the configuration of parameters used by each algorithm.
When we utilize trust/distrust relations in neighborhood-based algorithms, a crucial
decision we need to make is to which level the propagation must be performed (no propagation corresponds to single-level propagation which only includes direct neighbors).
Let p and q denote the level of propagation for trust and distrust relations, respectively.
Let us first consider the trust propagation to decide the value of p. We note that there
is a trade-off between accuracy and the level of trust propagation: longer propagation
levels results in less accurate trust predictions. This is due to the fact that when we
use longer propagation levels, the further away we are heading from each user, and
consequently decrease the confidence on the predictions. Obviously, this affects the accuracy of the recommendations significantly. As a result, for the trust propagation we
only consider single-level propagation by choosing p = 1 (i.e., N+∗ = N+ ). We also note
that since in the Epinions dataset, a user can not simultaneously trust and distrust
another user, in the neighborhood-based method with distrust relations, the debugging
only makes sense for propagated information. Therefore, we perform a three-level distrust propagation (q = 3) to constitute the set of distrusted users for each users. We
note that the longer the propagation levels, the more often distrust evidence can be
found for a particular user, and hence the less neighbors will be left to participate in
the recommendation process. For factorization-based methods, the value of regularization parameters, that is, λU , λV , and λ S , are determined by the procedure discussed in
Section 5.3.
The results of Table IX reveal some interesting conclusions as summarized here.
—From Table IX, we can observe that for factorization-based methods, incorporating
trust or distrust information boosts the performance of recommendation in terms of
both accuracy measures. This demonstrates the advantages of trust and distrustaware recommendation algorithms. We also can see that both MF+T and MF+D
perform better than the non-social MF, but the performance of MF+T is significantly
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:30

R. Forsati et al.

better than MF+D. As discussed, this observation does not indicate that trust relations are more beneficial than distrust relations, as in our dataset, only 16.7% of
relations are distrust relations. The MF+TD algorithm that is able to employ both
types of relations is significantly better than other algorithms, which demonstrates
the advantages of our proposed method in utilizing trust and distrust relations.
—Looking at the results reported in Table IX, it can immediately be noticed that
the incorporation of trust and distrust information in neighborhood-based methods
decreases the prediction error, but the improvement is not as significant as the
factorization-based methods. We note that for the NB+T method with longer levels
of propagation ( p = 2, 3), our experiments revealed that the accuracy remains almost
the same or has gotten worse on both MAE and RMSE measures, and this is why we
only report the results only for p = 1. In contrast, for distrust propagation, we found
out that q = 3 has a visible impact on the performance of both filtering and debugging
methods. We would like to emphasize that for longer levels of distrust propagation
in the Epinions dataset, that is, q > 4, we found that the size of the set of distrusted
users N−∗ (·) becomes large for most users, which degrades the prediction accuracy. We
also observe another interesting result about the performance of the NB+TD method
with filtering and debugging strategies. We found that although filtering generates
slightly better predictions, NB+TD-F performs almost as well as the NB+TD-D
method. Although this observation does not suggest any of these methods as the
method of choice in incorporating distrust, we believe that the accuracy might differ
from dataset to dataset, and it strongly depends on the propagation/aggregation
strategy.
—Considering the results for both model-based and memory-based methods in
Table IX, we can conclude a few interesting observations. First, we notice that
factorization-based methods with trust/distrust information perform better than the
neighborhood-based methods. Second, the incorporation of trust and distrust relations in matrix factorization has significant improvement compared to improvement
achieved by memory-based methods. Although the type of filtration or debugging
strategy could significantly affect the accuracy of incorporating distrust in memorybased methods, the main shortcoming of these methods comes from the fact that
these algorithms somehow exclude the influence of distrusted users from the rating prediction. This stands in stark contrast to the model proposed in this article
that ranks the neighbors based on the type of relation. This observation necessitates devising better algorithms for propagation and aggregation of trust/distrust
information in memory-based methods.
5.8. Handling Cold-Start Users by Social Side Information

In this section, we demonstrate the use of the social network to further illustrate the
potential of the proposed framework and the relevance of incorporating side information. To do so, as another set of our experiments, we intend to examine the performance
of proposed algorithm on cold-start users. Addressing cold-start users (i.e., users with
few ratings or new users) is very important for the success of recommender systems
due to the huge numbers of this type of users in many real-world systems. As a result, handling cold-start users is one of the main challenges in the existing systems.
To evaluate different algorithms, we randomly select 30%, 20%, 10%, and 5% as the
cold-start users. For cold-start users, we do not include any rating in the training data
and consider all the ratings made by cold-start users as testing data.
Table X shows the performance of the previously mentioned algorithms. As it is
clear from Table X, when the number of cold-start users is low with respect to the
total number of users, say 5% of total users, the affect of the distrust relationships
is negligible in prediction accuracy. But, when the number of cold-start users is high,
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:31

Table X. Accuracy of Handling Cold-Start Users and the Effect of Social Relations
% of Cold-start Users
30%
20%
10%
5%

Measure
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

MF

MF+T

MF+D

MF+TD

0.9923
1.7211
0.9812
1.7088
0.9334
1.4222
0.9134
1.3852

0.8824
1.5562
0.8805
1.4339
0.8477
1.3782
0.8292
1.2921

0.9721
1.6433
0.9505
1.6250
0.9182
1.4006
0.8633
1.3255

0.8533
1.4802
0.8472
1.2630
0.8322
1.2655
0.8280
1.2888

Note: The number of leant features in this experiments is set to k = 10. The first
column shows the number of cold-start users sampled randomly from all users
in the dataset. For the cold-starts users, all the ratings have been excluded from
the training data and used in the evaluation of three different algorithms.

exploiting the trust and distrust relationships significantly improves the performance
of the recommendation. This result is interesting, as it reveals that the lack of rating
information for cold-start and new users can be alleviated by incorporating the social
relations of users, and in particular, both trust and distrust relationships.
5.9. Trading Trust for Distrust Relationships

We also compare the potential benefit of trust relations to distrust relations in the
proposed algorithm. More specifically, we would like to see to what extent the distrust
relations can compensate for the lack of trust relations. We run the proposed algorithm
with the subset of trust and distrust relations and compare it to the algorithm which
only utilizes all of the trust relations. To set up this set of experiments, we randomly
sample a subset of trust relations and gradually increase the amount of distrust relations to see when the effect of distrust information compensates for the effect of missed
trust relations.
We sample 433,619 (approximately 90%) trust relations from the total 481,799 trust
relations and vary the number of distrust relations fed to the proposed algorithm.
Table XI reports the accuracy of the proposed algorithm for different numbers of distrust relations in the datasets. All these samplings have been done uniformly at random. We use 90% of all ratings for training and the remaining 10% for evaluation,
and set the dimension of the latent features to k = 10. As can be concluded from
Table XI, when we feed the proposed algorithm MF+TD with 90% of trust and 50%
of the distrust relations, it reveals very similar behavior to the trust-enhanced matrix factorization-based method MF+T, which only utilizes all the trust relations in
factorization. This result is interesting in the sense that the distrust information between users is as important as the trust information (we note that in this scenario,
the number trust relations excluded from the training is almost same as the number
of distrust relations included). By increasing the number of distrust relations, we can
observe that the accuracy of recommendations increases as expected. In summary, this
set of experiments validates that incorporating distrust relations could indeed enhance
the trust-based recommendation process and could be considered as a rich source of
information to be exploited.
5.10. On the Impact of Batch Size in Stochastic Optimization

As mentioned earlier, directly solving the optimization problem in Eq. (5) using full
gradient descent method requires going through all the triplets in the constraint set
S , which could be computationally expensive due to the huge number of triplets in
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:32

R. Forsati et al.
Table XI. Accuracy of Proposed Algorithm on a Dataset with 39,0257 (≈ 90%) Trust Relations
Sampled Uniformly at Random from All Trust Relations with Varied Number of Distrust Relations
Method

# of Trust Relations

MF+TD

433,619 (≈90%)

# of Distrust Relations
9,682 (≈10%)
19,364 (≈20%)
29,047 (≈30%)
38,729 (≈40%)
48,411 (≈ 50%)
58,093 (≈60%)
67,776 (≈70%)
77,458 (≈80%)
87,140 (≈90%)
96,823 (= 100%)

MF+T

481,799 (=100%)

0

Measure

Accuracy

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.8803 ± 0.051
1.2166 ± 0.028
0.8755 ± 0.033
1.1944 ± 0.042
0.8604 ± 0.036
1.1822 ± 0.081
0.8431 ± 0.047
1.1706 ± 0.055
0.8165 ± 0.056
1.1425 ± 0.091
0.8130 ± 0.035
1.1380 ± 0.046
0.8122 ± 0.041
1.1306 ± 0.042
0.8095 ± 0.036
1.1290 ± 0.085
0.8061 ± 0.044
1.1176 ± 0.067
0.8050 ± 0.052
1.1092 ± 0.063

MAE
RMSE

0.8158 ± 0.016
1.1403 ± 0.027

Note: The learning is performed based on 90% of all ratings with k = 10 as the dimension of
latent features.

S . To overcome this efficiency problem, one can turn to the stochastic gradient scent
method which tries to generate unbiased estimates of the gradient at each iteration in
a much cheaper way by sampling a subset of triplets from S .
To accomplish this goal, we perform gradient descent and stochastic gradient descent
to solve the optimization problem in Eq. (5) to find the matrices U and V following the
updating equations derived in Eqs. (7) and (8). At each iteration t, the currently learned
matrices Ut and Vt are used to predict the ratings in the testset. In particular, at each
iteration, we evaluate the RMSE and MAE on the testset and terminate training once
the RMSE and MAE starts increasing or once the maximum number of iterations is
reached. We run the algorithm with latent vectors of dimension k = 10.
We compare the computational efficiency between the proposed algorithm with GD
and mini-batch SGD with different batch sizes. We note that the GD updating rule
can be considered a mini-batch SGD, where the batch size B is deterministically set
to be B = |S |, and simple SGD can be considered a mini-batch SGD with B = 1. We
remark that in contrast to GD method which uses all the triplets in S for gradient
computation at each iteration, for the SGD method—due to uniform sampling over all
tuples in S —some of the tuples may be used more than once and some of the tuples
might never been used for gradient computation.
Figures 3 and 4 show the convergence rate of four different updating rules in terms
of the number of iterations t for two different measures RMSE and RME, respectively. The first algorithm denoted by GD runs the simple full gradient descent iteratively to optimize the objective. The other three algorithms SGD1, SGD2, and SGD3
in the figures use the batch sizes B = 0.1 ∗ |S |, B = 0.2 ∗ | S |, and B = 0.3 ∗ |S |,
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:33

Fig. 3. Comparison of accuracy of prediction in terms of RMSE with GD and SGD with three varied batch
sizes.

respectively. In our experiments, due to very slow convergence of the basic SGD method
with B = 1 in comparison to other fours methods, we simply exclude its result from the
discussion.
In terms of accuracy of predictions, from both Figures 3 and 4, we can conclude that
the GD has the best convergence and SGD3 has the worst convergence in all settings.
This is because, although all four of the algorithms use an unbiased estimate of the
true gradient to update the solution at each iteration, the variance of each stochastic
gradient is proportional to the size of the batch size B. Therefore, for larger values of B,
the variance of stochastic gradients is smaller, and the algorithm convergences faster,
but for smaller values of B, the algorithm suffers from high variance in stochastic
gradients and convergences slowly. We emphasize that this comparison holds for iteration complexity which is different from the computational complexity (running time)
of individual iterations. More specifically, each iteration of GD requires |S | gradient
computations, while for SGD, we only need to perform B  |S | gradient computations.
In summary, SGD has lightweight iteration but requires more iterations to converge.
In contrast, GD takes expensive steps in a much fewer number of iterations. From Figures 3 and 4, it is noticeable that although a large number of iterations is usually needed
to obtain a solution of desirable accuracy using SGD, the lightweight computation per
iteration makes SGD attractive for the optimization problem in Eq. (5) for large number
of users. We also not that for the GD method, the error is a monotonically-decreasing
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:34

R. Forsati et al.

Fig. 4. Comparison of accuracy of prediction in terms of MAE with GD and SGD with three varied batch
sizes.

function it terms of the number of iterations t, but for the SGD-based methods, this
does not hold. This is because although the SGD algorithm is guaranteed to converge to
an optimal solution (at least in expectation), there is no guarantee that the stochastic
gradients provide a descent direction for the objective at each iteration due to the noise
in computing gradients. As a result, for a few iterations, we can see that the objective
increases but finally it convergences as expected.
6. CONCLUSIONS AND FUTURE WORKS

In this article, we have made progress towards making distrust information beneficial
in the social recommendation problem. In particular, we have proposed a framework
based on matrix factorization which is able to incorporate both trust and distrust relationships between users in a factorization algorithm. We experimentally investigated
the potential of distrust as side information to overcome data sparsity and cold-start
problems in traditional recommender systems. In summary, our results showed that
more accurate recommendations can be obtained by incorporating distrust relations,
indicating that distrust information can indeed be beneficial for the recommendation
process.
This work leaves few directions, both theoretically and empirically, as future work.
From an empirical point of view, it would be interesting to extend our model for
weighted social trust and distrust relations. One challenge in this direction is that,
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:35

as far as we know, there is no publicly available dataset that includes weighted (gradual) trust and distrust information. Also, the experimental results we have conducted
on the consistency of social relations with rating information hint at a number of potential enhancements in future work. In particular, it would be interesting to further
examine the correlation between implicit and explicit distrust information. An important challenge in this direction is to develop better metrics to measure the implicit
trust between users, as the simple metrics such as the Pearson correlation coefficient
seem insufficient. Furthermore, since we only consider distrust between users, it would
be easy to generalize our model in the same way to incorporate dissimilarity between
items and investigate how it works in practice. Also, our preliminary results indicated
that hinge loss almost performs better than exponential loss, but from the optimization
viewpoint, exponential loss is more attractive due to its smoothness. So, an interesting
direction would be to use a smoothed version of hinge loss to gain from both optimization efficiency and algorithmic accuracy.
ACKNOWLEDGMENTS
The authors would like to thank the Associate Editor and three anonymous reviewers for their immensely
insightful comments and helpful suggestions on the original version of this article. R. Forsati would also
like to thank Professor Mohamed Mokbel, Department of Computer Science and Engineering, University of
Minnesota, for the opportunity to visit his research group while doing this work.

REFERENCES
Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the next generation of recommender systems:
A survey of the state-of-the-art and possible extensions. IEEE Trans. Knowl. Data Eng. 17, 6 (2005),
734–749.
Deepak Agarwal and Bee-Chung Chen. 2010. fLDA: Matrix factorization through latent dirichlet allocation.
In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining. ACM, 91–100.
Paolo Avesani, Paolo Massa, and Roberto Tiella. 2005. A trust-enhanced recommender system application:
Moleskiing. In Proceedings of the ACM Symposium on Applied Computing. 1589–1593.
Giacomo Bachi, Michele Coscia, Anna Monreale, and Fosca Giannotti. 2012. Classifying trust/distrust relationships in online social networks. In International Conference on Privacy, Security, Risk and Trust
(PASSAT) and International Confernece on Social Computing (SocialCom). IEEE, 552–557.
Jesús Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Gutiérrez. 2013. Recommender systems
survey. Knowl. Based Syst. 46 (2013), 109–132.
John S. Breese, David Heckerman, and Carl Kadie. 1998. Empirical analysis of predictive algorithms for
collaborative filtering. In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence.
Morgan Kaufmann Publishers Inc., 43–52.
Moira Burke and Robert Kraut. 2008. Mopping up: Modeling Wikipedia promotion decisions. In Proceedings
of the ACM Conference on Computer Supported Cooperative Work. ACM, 27–36.
Dorwin Cartwright and Frank Harary. 1956. Structural balance: A generalization of Heider’s theory. Psychol.
Rev. 63, 5 (1956), 277.
Gang Chen, Fei Wang, and Changshui Zhang. 2009. Collaborative filtering using orthogonal nonnegative
matrix tri-factorization. Inf. Process. Manag. 45, 3 (2009), 368–379.
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. 2011. Better mini-batch algorithms via
accelerated gradient methods. In Conference on Neural Information Processing Systems (NIPS). Vol. 24
1647–1655.
David Crandall, Dan Cosley, Daniel Huttenlocher, Jon Kleinberg, and Siddharth Suri. 2008. Feedback effects
between similarity and social influence in online communities. In Proceedings of the 14th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. ACM, 160–168.
Sanjoy Dasgupta, Michael L. Littman, and David McAllester. 2002. PAC generalization bounds for cotraining. In Proceedings of the Conference on Neural Information Processing Systems. 375–382.
Mukund Deshpande and George Karypis. 2004. Item-based top-n recommendation algorithms. ACM Trans.
Inf. Syst. 22, 1 (2004), 143–177.
Thomas DuBois, Jennifer Golbeck, and Aravind Srinivasan. 2011. Predicting trust and distrust in social
networks. In Proceedings of the IEEE 3rd International Conference on Privacy, Security, Risk and Trust
(PASSAT), and IEEE 3rd International Conference on Social Computing (SocialCom). IEEE, 418–424.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:36

R. Forsati et al.

Rana Forsati, Hanieh Mohammadi Doustdar, Mehrnoush Shamsfard, Andisheh Keikha, and Mohammad
Reza Meybodi. 2013. A fuzzy co-clustering approach for hybrid recommender systems. Int. J. Hybrid
Intell. Syst. 10, 2 (2013), 71–81.
Rana Forsati and Mohammad Reza Meybodi. 2010. Effective page recommendation algorithms based on
distributed learning automata and weighted association rules. Expert Syst. Appl. 37, 2 (2010), 1316–
1330.
Jennifer Golbeck. 2005. Computing and applying trust in web-based social networks. Ph.D. Dissertation,
University of Maryland at College Park.
Jennifer Golbeck. 2006. Generating predictive movie recommendations from trust in social networks. In Proceedings of the 4th International Conference on Trust Management (iTrust). Lecture Notes in Computer
Science, Vol. 3986, Springer, Berlin, 93–104.
Jennifer Golbeck and James Hendler. 2006. Filmtrust: Movie recommendations using trust in web-based
social networks. In Proceedings of the IEEE Consumer Communications and Networking Conference,
Vol. 96. Citeseer.
Nathaniel Good, J. Ben Schafer, Joseph A. Konstan, Al Borchers, Badrul Sarwar, Jon Herlocker, and John
Riedl. 1999. Combining collaborative filtering with personal agents for better recommendations. In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications
of Artificial Intelligence Conference (AAAI/IAAI). 439–446.
Quanquan Gu, Jie Zhou, and Chris Ding. 2010. Collaborative filtering: Weighted nonnegative matrix factorization incorporating user and item graphs. In Proceedings of the SIAM International Conference on
Data Mining (SDM). 199–210.
R. Guha, Ravi Kumar, Prabhakar Raghavan, and Andrew Tomkins. 2004. Propagation of trust and distrust.
In Proceedings of the 13th International Conference on World Wide Web. ACM, 403–412.
Guibing Guo, Jie Zhang, Daniel Thalmann, Anirban Basu, and Neil Yorke-Smith. 2014. From ratings to
trust: An empirical study of implicit trust in recommender systems. In Proceedings of the 29th ACM
Symposiam on Applied Computing (SAC).
Jonathan L. Herlocker, Joseph A. Konstan, Al Borchers, and John Riedl. 1999. An algorithmic framework
for performing collaborative filtering. In Proceedings of the 22nd Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval. ACM, 230–237.
Jonathan L. Herlocker, Joseph A. Konstan, Loren G. Terveen, and John T. Riedl. 2004. Evaluating collaborative filtering recommender systems. ACM Trans. Inf. Syst. 22, 1 (2004), 5–53.
Thomas Hofmann. 2004. Latent semantic models for collaborative filtering. ACM Trans. Inf. Syst. 22, 1
(2004), 89–115.
Mohsen Jamali and Martin Ester. 2009. TrustWalker: A random walk model for combining trust-based
and item-based recommendation. In Proceedings of the 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, 397–406.
Mohsen Jamali and Martin Ester. 2010. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the 4th ACM Conference on Recommender Systems.
ACM, 135–142.
Mohsen Jamali and Martin Ester. 2011. A transitivity aware matrix factorization model for recommendation
in social networks. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence,
Vol. 3. AAAI Press, 2644–2649.
Arnd Kohrs and Bernard Merialdo. 1999. Clustering for collaborative filtering applications. In Computational
Intelligence for Modelling, Control & Automation. IOS Press.
Ioannis Konstas, Vassilios Stathopoulos, and Joemon M. Jose. 2009. On social networks and collaborative
recommendation. In Proceedings of the 32nd International ACM SIGIR Conference on Research and
Development in Information Retrieval. ACM, 195–202.
Yehuda Koren. 2008. Factorization meets the neighborhood: A multifaceted collaborative filtering model.
In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. ACM, 426–434.
Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender
systems. Computer 42, 8 (2009), 30–37.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010a. Predicting positive and negative links in
online social networks. In Proceedings of the 19th International Conference on World Wide Web. ACM,
641–650.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010b. Signed networks in social media. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1361–1370.
Wu-Jun Li and Dit-Yan Yeung. 2009. Relation regularized matrix factorization. In Proceedings of the 21st
International Conference on Artificial Intelligence (IJCAI’09).
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:37

Juntao Liu, Caihua Wu, and Wenyu Liu. 2013. Bayesian probabilistic matrix factorization with social relations and item contents for recommendation. Decision Support Syst. 55, 3 (June 2013), 838–850.
Hao Ma. 2013. An experimental study on implicit social recommendation. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 73–82.
Hao Ma, Irwin King, and Michael R. Lyu. 2009a. Learning to recommend with social trust ensemble.
In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in
Information Retrieval. ACM, 203–210.
Hao Ma, Michael R. Lyu, and Irwin King. 2009b. Learning to recommend with trust and distrust relationships. In Proceedings of the 3rd ACM Conference on Recommender Systems. ACM, 189–196.
Hao Ma, Haixuan Yang, Michael R. Lyu, and Irwin King. 2008. SoRec: Social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM Conference on Information and Knowledge
Management. ACM, 931–940.
Hao Ma, Dengyong Zhou, Chao Liu, Michael R. Lyu, and Irwin King. 2011a. Recommender systems with
social regularization. In Proceedings of the 4th ACM International Conference on Web Search and Data
Mining. ACM, 287–296.
Hao Ma, Tom Chao Zhou, Michael R. Lyu, and Irwin King. 2011b. Improving recommender systems by
incorporating social contextual information. ACM Trans. Inf. Syst. 29, 2 (2011), 9.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information
Retrieval. Vol. 1. Cambridge University Press, Cambridge.
Paolo Massa and Paolo Avesani. 2004. Trust-aware collaborative filtering for recommender systems. In On
the Move to Meaningful Internet Systems 2004: CoopIS, DOA, and ODBASE. Lecture Notes in Computer
Science, Vol. 3290. Springer, 492–508.
Paolo Massa and Paolo Avesani. 2005. Controversial users demand local trust metrics: An experimental study
on Epinions.com community. In Proceedings of the 20th National Conference on Artificial Intelligence
(AAAI’05). 121–126.
Paolo Massa and Paolo Avesani. 2009. Trust metrics in recommender systems. In Computing with Social
Trust. Human-Computer Intercation Series, Springer, 259–285.
Prem Melville, Raymond J. Mooney, and Ramadass Nagarajan. 2002. Content-boosted collaborative filtering
for improved recommendations. In Proceedings of the 18th National Conference on Artificial Intelligence
(AAAI). 187–192.
Bradley N. Miller, Joseph A. Konstan, and John Riedl. 2004. PocketLens: Toward a personal recommender
system. ACM Trans. Inf. Syst. (TOIS) 22, 3 (2004), 437–476.
Andriy Mnih and Ruslan Salakhutdinov. 2007. Probabilistic matrix factorization. In Proceedings of the 21st
Annual Conference on Neural Information Processing Systems (NIPS). 1257–1264.
Uma Nalluri. 2008. Utility of distrust in online recommender systems. Technical Report, Coopstone.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. 2009. Robust stochastic approximation approach to stochastic programming. SIAM J. Optim. 19, 4 (2009), 1574–1609.
Akshay Patil, Golnaz Ghasemiesfeh, Roozbeh Ebrahimi, and Jie Gao. 2013. Quantifying social influence in
epinions. Human 2, 2 (2013).
Dmitry Pavlov and David M. Pennock. 2002. A maximum entropy approach to collaborative filtering in
dynamic, sparse, high-dimensional domains. In Proceedings of the Annual Conference on Neural Information Processing System (NIPS), Vol. 2. 1441–1448.
Michael J. Pazzani. 1999. A framework for collaborative, content-based and demographic filtering. Artif.
Intell. Rev. 13, 5–6 (1999), 393–408.
David M. Pennock, Eric Horvitz, Steve Lawrence, and C. Lee Giles. 2000. Collaborative filtering by personality diagnosis: A hybrid memory-and model-based approach. In Proceedings of the 16th Conference on
Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc., 473–480.
Jasson D. M. Rennie and Nathan Srebro. 2005. Fast maximum margin matrix factorization for collaborative
prediction. In Proceedings of the 22nd International Conference on Machine Learning. ACM, 713–719.
Ruslan Salakhutdinov and Andriy Mnih. 2008a. Bayesian probabilistic matrix factorization using Markov
chain Monte Carlo. In Proceedings of the 25th International Conference on Machine Learning. ACM,
880–887.
Ruslan Salakhutdinov and Andriy Mnih. 2008b. Probabilistic matrix factorization. In Proceedings of the
22nd Annual Conference on Neural Information Processing Systems. 1257–1264.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted Boltzmann machines for collaborative filtering. In Proceedings of the 24th International Conference on Machine Learning. ACM,
791–798.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:38

R. Forsati et al.

Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based collaborative filtering
recommendation algorithms. In Proceedings of the 10th International Conference on World Wide Web.
ACM, 285–295.
Wanita Sherchan, Surya Nepal, and Cecile Paris. 2013. A survey of trust in social networks. ACM Comput.
Surv. 45, 4 (2013), 47.
Luo Si and Rong Jin. 2003. Flexible mixture model for collaborative filtering. In Proceedings of the 20th
International Conference on Machine Learning (ICML), Vol. 3. 704–711.
Ian Soboroff and Charles Nicholas. 1999. Combining content and collaboration in text filtering. In Proceedings
of the IJCAI Workshop on Machine Learning for Information Filtering, Vol. 99. 86–91.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted low-rank approximations. In Proceedings of the 20th
International Conference on Machine Learning (ICML), Vol. 3. 720–727.
Nathan Srebro, Jason D. M. Rennie, and Tommi Jaakkola. 2005. Maximum-margin matrix factorization. In
Proceedings of the Conference on Neural Information Processing Systems (NIPS). 1329–1336.
Mojdeh Talabeigi, Rana Forsati, and Mohammad Reza Meybodi. 2010. A hybrid web recommender system
based on cellular learning automata. In Proceedings of the IEEE International Conference on Granular
Computing (GrC). IEEE, 453–458.
Nele Verbiest, Chris Cornelis, Patricia Victor, and Enrique Herrera-Viedma. 2012. Trust and distrust aggregation enhanced with path length incorporation. Fuzzy Sets Syst. 202 (2012), 61–74.
Patricia Victor, Chris Cornelis, Martine De Cock, and Ankur Teredesai. 2011b. Trust- and distrust-based
recommendations for controversial reviews. IEEE Intell. Syst. 26, 1 (2011), 48–55.
Patricia Victor, Chris Cornelis, and Martine De Cock. 2011a. Trust Networks for Recommender Systems.
Atlantis-Computational Intelligence Series, Vol. 4. Springer, Berlin.
Patricia Victor, Chris Cornelis, Martine De Cock, and Enrique Herrera-Viedma. 2011c. Practical aggregation
operators for gradual trust and distrust. Fuzzy Sets Syst. 184, 1 (2011), 126–147.
Patricia Victor, Nele Verbiest, Chris Cornelis, and Martine De Cock. 2013. Enhancing the trust-based recommendation process with explicit distrust. ACM Trans. Web 7, 2 (2013), 6.
Fei Wang, Sheng Ma, Liuzhong Yang, and Tao Li. 2006b. Recommendation on item graphs. In Proceedings
of the 6th International Conference on Data Mining (ICDM’06). IEEE, 1119–1123.
Jun Wang, Arjen P. De Vries, and Marcel J. T. Reinders. 2006a. Unifying user-based and item-based collaborative filtering approaches by similarity fusion. In Proceedings of the 29th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval. ACM, 501–508.
Grzegorz Wierzowiecki and Adam Wierzbicki. 2010. Efficient and correct trust propagation using closelook.
In Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent
Agent Technology (WI-IAT). Vol. 1. IEEE, 676–681.
Lei Wu, Steven C. H. Hoi, Rong Jin, Jianke Zhu, and Nenghai Yu. 2009. Distance metric learning from
uncertain side information with application to automated photo tagging. In Proceedings of the 17th
ACM International Conference on Multimedia. ACM, 135–144.
Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, Hua-Jun Zeng, Yong Yu, and Zheng Chen. 2005. Scalable
collaborative filtering using cluster-based smoothing. In Proceedings of the 28th Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 114–121.
Kai Yu, Anton Schwaighofer, Volker Tresp, Xiaowei Xu, and H.-P. Kriegel. 2004. Probabilistic memory-based
collaborative filtering. IEEE Trans. Knowl. Data Eng. 16, 1 (2004), 56–69.
Sheng Zhang, Weihong Wang, James Ford, and Fillia Makedon. 2006. Learning from incomplete ratings
using non-negative matrix factorization. In Proceedings of the 6th SIAM Conference on Data Mining
(SDM).
Yi Zhang and Jonathan Koren. 2007. Efficient bayesian hierarchical user modeling for recommendation
system. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval. ACM, 47–54.
Jianke Zhu, Hao Ma, Chun Chen, and Jiajun Bu. 2011. Social recommendation using low-rank semidefinite
program. In Proceedings of the 25th AAAI Conference on Artificial Intelligence.
Cai-Nicolas Ziegler. 2009. On propagating interpersonal trust in social networks. In Computing with Social
Trust Human-Computer Interaction Series, Springer, Berlin, 133–168.
Cai-Nicolas Ziegler and Jennifer Golbeck. 2007. Investigating interactions of trust and interest similarity.
Decision Support Syst. 43, 2 (2007), 460–475.
Cai-Nicolas Ziegler and Georg Lausen. 2005. Propagation models for trust and distrust in social networks.
Inf. Syst. Frontiers 7, 4–5 (2005), 337–358.
Received November 2013; revised April, June 2014; accepted June 2014
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

2015 16th IEEE International Conference on Mobile Data Management

R ECATHON: A Middleware for Context-Aware
Recommendation in Database Systems
1 Mohamed
1

Sarwat, 2 James L. Avery, 3 Mohamed F. Mokbel

School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ
2
IBM Mobile Innovation Lab, Austin, 11501 Burnet Road Austin, TX
3
Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN
1

msarwat@asu.edu,

2

jlavery@us.ibm.com,

mokbel@cs.umn.edu

tion based on the whole users’ opinions history and hence
generates recommendation to the querying user regardless of
the user attributes (e.g., user age, job, and gender). On the
other side, a multidimensional recommender would be able
to estimate the recommendation utility function based on a
subset of the users’ opinions data that corresponds to speciﬁc
attributes’ range and hence would be able to support contextaware, preference-aware, or user demographic-aware recommendations queries such as: Recommend me 10 movies that
people in my age would like, Recommend me 10 restaurants
that are located in Tempe, Arizona, or Recommend me 5
movies that people of my gender would like. From a modeling
perspective, it has been shown that such queries would exhibit
higher accuracy than context-free recommendations [3], [6].
Building a multidimensional (context-aware) recommender
poses the following challenges: (1) Challenge I: Deciding
the contextual attributes to build a recommender on and the
recommendation algorithm to estimate the utility function. For
example, if a user has two attributes, age and gender, then
there is a possibility of four recommenders, a context-free,
an age-aware, a gender-aware, and an (age, gender)-aware
recommender. Also, each recommender may have different
versions based on the employed recommendation algorithm.
(2) Challenge II: For a multidimensional recommender, how
to efﬁciently store and maintain the underlying recommendation models. A basic solution would be to materialize
all recommendation models that correspond to all attributes
combinations. Nonetheless, this approach incurs tremendous
storage and maintenance overhead. (3) Challenge III: How
to execute context-aware recommendation queries expressed
by users over a recommender. A straightforward solution
implements the recommendation functionality on-top of a
database system. However, this approach does not harness the
full power of the database engine.
In this paper, we introduce R ECATHON : a middleware
for building context-aware recommendation applications in a
database management system. R ECATHON tackles Challenge I
by providing a declarative interface for users to build custommade multidimensional recommenders. The main idea is to
deal with building recommenders inside the database engine
in an analogous way to creating indexes (or views). Database
users can build indexes on tables, based on the query workload.
Similarly, if there is a signiﬁcant number of recommendation

Abstract—This paper presents R ECATHON; a context-aware
recommender system built entirely inside a database system.
Unlike traditional recommender systems that are context-free
where they support the general query of Recommend movies for
a certain user, R ECATHON users can request recommendations
based on their age, location, gender, or any other contextual/demographical/preferential user attribute. A main challenge
of supporting such kind of recommenders is the difﬁculty of
deciding what attributes to build recommenders on. R ECATHON
addresses this challenge as it supports building recommenders
in database systems in an analogous way to building index
structures. Users can decide to create recommenders on selected
attributes, e.g., age and/or gender, and then entertain efﬁcient
support of multidimensional recommenders on the selected attributes. R ECATHON employs a multi-dimensional index structure for each built recommender that can be accessed using
novel query execution algorithms to support efﬁcient retrieval
for recommender queries. Experimental results based on an
actual prototype of R ECATHON, built inside PostgreSQL, using
real MovieLens and Foursquare data show that R ECATHON
exhibits real time performance for large-scale multidimensional
recommendation.

I. I NTRODUCTION
Recently, recommender systems have grabbed researchers’
attention in both industry [2], [7], [9], [19] and academia [14],
[13], [24], [28], [29]. The main goal of a recommender system
is to suggest new and interesting items to users from a
large pool of items. Recommender systems are implicitly employed on a daily basis to recommend movies (e.g., Netﬂix),
books/products (e.g., Amazon), friends (e.g., Facebook), and
news articles (e.g., Google News). A recommender system
exploits the users’ opinions (e.g., movie ratings) and/or purchasing (e.g., watching, reading) history in order to extract
a set of interesting items for each user. In technical terms,
a recommendation algorithm takes as input a set of users
U , items I, and ratings (history of users’ opinions) R and
estimates a utility function RecScore(u, i) that predicts how
much a certain user u ∈ U would like an item i ∈ I such that
i has not been already seen (or watched, consumed, etc) by
u [4]. To estimate such a utility function, many recommendation algorithms have been proposed in the literature [4], [11]
(e.g., Collaborative Filtering).
Classical recommender systems answer the traditional
context-free recommendation query like, Recommend me 10
movies, where we estimate the recommendation utility func978-1-4799-9972-9/15 $31.00 © 2015 IEEE
DOI 10.1109/MDM.2015.63

3

54

queries that use age and salary, a R ECATHON designer might
decide to build a recommender over the ﬁelds age, salary.
To achieve that, R ECATHON extends SQL with new statements to create/drop multidimensional recommenders, namely
CREATE/DROP RECOMMENDER. When creating a context-aware
recommender, the user speciﬁes the recommender name, the
sets of users, items, and ratings that will be used to build the
recommender. In addition, the user speciﬁes the set of contextual attributes that the recommender will support and selects
one of R ECATHON built-in recommendation algorithms.
To address Challenge II, we initialize and maintain a multidimensional grid data structure for each recommender created
using the CREATE RECOMMENDER statement, where each dimension corresponds to one of the attributes. Each grid cell
includes a recommendation model that is used to efﬁciently
generate recommendations to users. Materializing a large
number of grid cells incurs more recommendation models
being stored and maintained, which may preclude system scalability. To remedy this issue, R ECATHON adaptively decides,
based upon the query/update workload, which recommender
cells to maintain in order to reduce the overall recommender
storage and maintenance overheard without largely compromising the query execution performance. To query an existing
multidimensional recommender (Challenge III), R ECATHON
users specify the recommender in the FROM clause and the
user identiﬁer as a predicate to the WHERE clause of a SQL
query. R ECATHON executes the query and produces the set
of recommended items, along with their scores with respect
to the querying user. To reduce query latency, R ECATHON
optimizes incoming recommendation queries through a set of
query execution algorithms that embed the recommendation
functionality within other database operations, namely, select,
join, and ranking.
Experiments, based on actual system implementation inside
PostgreSQL, using real data extracted from MovieLens [20]
and Foursquare [27], show that R ECATHON exhibits high performance for large-scale multidimensional recommendation.
The rest of the paper is organized as follows: Preliminaries
are given in Section II. Challenges I to III are addressed in
Sections III to V. Experiments are presented in Section VI.
Related work is outlined in Section VII. Finally, Section VIII
concludes the paper.












	



-.
















/#




 









(




 









#




 









01













	

2
















#,

	










	






	
 	




	













	







 !"#$!%&








'"()*+#,








	

		

Fig. 1.

	



Recommender Input Data.

input data. The format of the model depends on the underlying recommendation algorithm. For example, a recommendation model for the item-item cosine-similarity model (ItemCosCF) [4] is a similarity list of the tuples ip , iq , SimScore,
where SimScore is the similarity score between items ip and
iq . To compute SimScore(ip, iq ), we represent each item as a
vector in the user-rating space of the user/item ratings matrix.
The Cosine similarity is then calculated as follows:
SimScore(ip , iq ) =

ip · iq

ip iq 

(1)

Step II: Recommendation Generation: This step utilizes
the RecModel (e.g., items similarity list) created in Step I
to predict a recommendation score, RecScore(u, i), for each
user/item pair. RecScore(u, i) reﬂects how much each user u
likes the unseen item i. The recommendation score RecScore
depends on the recommendation algorithm deﬁned for the
underlying recommender. For the ItemCosCF recommendation
algorithm, RecScore(u, i) for each item i not rated by u is
calculated as follows:

l∈L sim(i, l) ∗ ru,l
RecScore(u, i) = 
(2)
l∈L |sim(i, l)|
Before this computation, we reduce each similarity list L to
contain only items rated by user u. The recommendation score
is the sum of ru,l , a user u’s rating for a related item l ∈ L
weighted by sim(i,l), the similarity of l to candidate item i,
then normalized by the sum of similarity between i and l.

II. P RELIMINARIES

III. M ULTI - DIMENSIONAL R ECOMMENDER

Data Model. R ECATHON assumes the following input data:
(1) Users: a set of users and their attributes. (2) Items: a set of
items and their attributes. (3) Ratings: users expressing their
opinions over items. Opinions can be a numeric rating (e.g.,
one to ﬁve stars), or unary (e.g., Facebook “check-ins”). Also,
ratings may represent purchasing behavior (e.g., Amazon).
Figure 1 gives an example of movie recommendation data.
Recommendation Algorithms. Most recommendation algorithms produce recommendations in two steps, as follows:
Step I: Recommendation Model Building: This step consists
of building a recommendation model RecModel using the

Figure 2 depicts R ECATHON architecture. To support multidimensional recommenders, the ﬁrst challenge (Challenge I)
is to provide a tool to the system users to freely decide which
attributes and recommendation algorithm to be used in building the recommender. R ECATHON addresses this challenge by
allowing users to use a SQL-like clause to deﬁne new multidimensional recommenders by specifying the recommender
data and attributes (i.e., dimensions). R ECATHON exposes the
newly created recommender to its users as a virtual schema,
where users can issue SQL queries to obtain a set of recommended items (e.g., movies) based on the speciﬁed attributes

55

	
	
	
	

		




	
	

		









	




	


	



	
		


		
	

	
		
"		!#	
$
%%	&'



	



 






	

!
	



	



	
		

	

	



	
	

		
	

Fig. 2.

Recathon Architecture.

With this SQL, a new recommender, named AgeRec, is
added to the database system, and can be queried later to
return a set of recommended movies based on the user age,
e.g., recommend me ﬁve movies that people in my age like.
Example 2: AgeCityGenderRec: an (age, city, gender)aware recommender created on the input in Figure 1, using
the SVD recommendation algorithm:

(e.g., age and city). This section focuses on how users interact
with the system. In particular, Section III-A explains the SQL
clause for creating a new recommender, while Section III-B
explains the SQL for querying a recommender.
A. Creating a New Recommender
To allow creating a new recommender-aware system, R E CATHON employs a new SQL statement, called CREATE
RECOMMENDER, as follows:

CREATE RECOMMENDER AgeCityGenderRec
USERS FROM Users U ITEMS FROM Movies
RATINGS FROM Ratings
ATTRIBUTES U.age, U.city, U.gender USING SVD

CREATE RECOMMENDER [Recommender Name]
USERS FROM [Users Table]
ITEMS FROM [Items Table]
RATINGS FROM [Ratings Table]
ATTRIBUTES [Attributes Set]
USING [Recommendation algorithm]

With this SQL clause, a new recommender, named AgeCityGenderRec, is added to the database system, and can be
queried later to return to a querying users a set of recommended movies based on the user age, city, and gender, e.g.,
recommend me ﬁve movies that people in my age, living in my
city, and of my gender, would like.
Notice that in the above two examples, if we had no
ATTRIBUTES clause, we would create a traditional recommender that can be queried to recommend a set of movies for a
certain user, regardless of the user attributes, e.g., recommend
me ﬁve movies.

The recommender creation SQL, presented above, has the following parameters: (1) Recommender name is a unique name
assigned to the created Recommender. (2) Users Table,
Items Table, and Ratings Table are the names of relational tables containing the users, items, and ratings input data
(e.g., the tables in Figure 1). (3) Attributes Set is a set
of dimensions that the recommender will be built on (e.g.,
age). (4) Recommendation algorithm is the algorithm used
to build the recommender. Currently, R ECATHON supports
three main recommendation algorithms (with their variants):
(a) Item-Item Collaborative Filtering with Cosine (abbr. ItemCosCF) or Pearson Correlation (abbr. ItemPearCF) similarity
functions, (b) User-User Collaborative ﬁltering (abbr. UserCosCF / UserPearCF), and (c) Regularized Gradient Descent
Singular Value Decomposition (abbr. SVD). If no recommendation algorithm is speciﬁed, R ECATHON employs by default
the ItemCosCF algorithm. Examples are given below:
Example 1: AgeRec: an age-aware recommender created on
the input data stored in the Users, Movies, and Ratings tables
of Figure 1, using the ItemCosCF recommendation algorithm:

B. Querying a Recommender
Once a recommender is created using the CREATE
RECOMMENDER statement, it is exposed to querying users as
a virtual table with the schema: (uid,iid,RecScore), where
uid is a user identiﬁer, iid is an item identiﬁer, and RecScore
is the recommendation score that predicts how much the user
uid would like the item iid according to the underlying
recommender function, speciﬁed in the USING clause of the
CREATE RECOMMENDER statement. Then, R ECATHON users
can issue SQL queries over the created multidimensional
recommender schema as follows:

CREATE RECOMMENDER AgeRec USERS FROM Users U
ITEMS FROM Movies RATINGS FROM Ratings
ATTRIBUTES U.age USING ItemCosCF

SELECT
FROM
WHERE

56

[Select Clause]
[Recommender], [Tables]
[Where Clause]

Query 4
Query 5

	
















	


	








	

%&&








 !

)!!&








 ! "#

'(

	


	





TABLE I
R ECOMMENDATION Q UERY E XAMPLES





	







	

	











The SELECT and WHERE clauses are typical as in any SQL
query. The FROM clause may accept, in addition to relational
tables, a [Recommender] schema, which is created by a
CREATE RECOMMENDER statement. In the WHERE clause, the
querying user may specify the UserID, the user identiﬁer for
whom the recommendation needs to be generated. R ECATHON
then returns a set of tuples ItemID, RecScore that represents the predicted recommendation score RecScore for each
item ItemID based on the recommender speciﬁed in the FROM
clause. Examples are given in table I. For instance, Query 2
returns the top-10 recommended items to user with ID 1, based
on the user age, city, and gender. In this case, the query uses
the AgeCityGenderRec, which was created before using a
CREATE RECOMMENDER. Since this recommender was created
based on the age, city, and gender attributes, it will return an
answer to the user based on the values of these attributes in
the USERS table for user ID 1.

	


%!&

$






Query 3




Select * From AgeCityGenderRec R Where R.uid=2
Select R.iid From AgeCityGenderRec R Where R.uid=1
Order By R.RecScore Desc Limit 10
Select R.iid, R.RecScore From AgeRec R
Where R.uid=1
AND R.iid IN (1,2,3,4,5)
Select M.name, R.RecScore From AgeRec R, Movies M
Where R.uid=1 AND M.iid = R.iid AND M.genre=’Action’
Select R.iid, R.RecScore From AgeRec R, Movies M Where
R.uid AND R.iid = M.iid AND M.genre = ’Action’ Order By
R.RecScore Desc Limit 5



Query 1
Query 2




	



	










Fig. 3.

	

	

Recathon data structure.

Figure 3 gives an example of RecCatalog, where it has
four entries for four recommenders, AgeRec, AgeGenderRec,
AgeCityGenderRec, and GeneralRec. With each recommender,
the corresponding attributes are listed. Notice that in the case
of GeneralRec, Attributes is empty, which corresponds to a
general recommender system regardless of any attributes.
B. Multi-dimensional Grid
For each created recommender, R ECATHON maintains a
Multi-dimensional Grid G, where each dimension corresponds
to one of the recommender attributes. A grid cell C in G
represents a subdomain of the space created by the multiple
attributes. The subdomain could be a certain value for categorical attributes or range of values for continuous domain
attributes. For example, as AgeRec recommender in Figure 3
is deﬁned based on only one attribute (age), its index is a onedimensional grid based on the age attribute. As this is a continuous domain attribute, each cell represents a range of age
values, i.e., [18-24], [25-34], and [35-60]. In the meantime, the
AgeCityGenderRec recommender index is a three-dimensional
grid based on three attributes (age, city, and gender). The age
dimension is divided into three categories based on range of
values. The city attribute has three values as {Minneapolis, St.
Paul, Edina}, while the gender attribute is divided into two
categorical values as {Male, Female}. The top left outer cell
(check marked in Figure 3) in AgeCityGenderRec represents
the values 18-24, Minneapolis, Female that correspond to its
values of the age, city, gender dimensions.
Each cell in the multi-dimensional grid points to a table,
RecModel, that maintains auxiliary precomputed information
to speed up the generation of the recommendation query result.
The precomputed information may have different schema
based on the underlying recommendation algorithm. For example, for the Item-Item collaborative ﬁltering algorithm,
RecModel represents an items similarity list with the schema
(ItemID1, ItemID2, SimScore), where SimScore is computed
per equation 1 (Section II).
Initialization. The multi-dimensional grid G is initialized
upon issuing a CREATE RECOMMENDER statement, through two
main steps: (1) Grid Construction, where we allocate the memory space for the grid, and decide on each cell size in terms of

IV. R ECATHON I NDEXING
After creating a recommender, a main challenge (challenge II) is how to internally represent, store, and maintain the
underlying recommendation models in a scalable manner. To
address this issue, we introduce the following data structures
to represent user-created multidimensional recommenders:
(1) We maintain one global structure, namely RecCatalog,
for all created recommenders (section IV-A). (2) For each
recommender, we maintain one multidimensional grid and we
explain how we reduce both storage and maintenance overhead
to achieve scalability (section IV-B).
A. Recommender Catalog
R ECATHON maintains a relational table, termed RecCatalog, that includes metadata about all created recommenders,
and is stored as part of the main database catalog that
includes information about tables, index structures, etc. A
row in RecCatalog has seven attributes: (1) RecName; the
recommender name, (2) Users; the input users table, (3) Items;
the input items table, (4) Ratings; the input ratings table,
(5) Attributes; a vector where each element corresponds to
an attribute in the users table that contributes to the recommender model, (6) Algorithm; the algorithm used to generate
predicted scores, and (7) RecIndex; a pointer to the multidimensional grid index for this particular recommender. A new
row is added/deleted to/from RecCatalog with each successful
CREATE RECOMMENDER / DROP RECOMMENDER SQL statement.

57

the values it represents. In case of categorical attributes (e.g.,
Gender, Job, and City), we allocate one cell per attribute. For
continuous domain attributes (e.g., age and salary), we divide
the space into N parts, where parts have almost equal number
of ratings. More sophisticated techniques can be used to divide
the space. Yet, we opt for a simple division here as a proof of
concept for R ECATHON functionality. (2) RecModel Building,
where the RecModel table for each cell C in G is built by
running the speciﬁed recommender algorithm in the CREATE
RECOMMENDER statement on the set of users U whose attributes
correspond to the subdomain covered by C. For instance, in
case of ItemCosCF recommendation algorithm, we scan the
ratings table and run a nested loop algorithm over all items to
calculate the cosine similarity score between every item pair in
each cell C using equation 1. After the initialization procedure
terminates, a pointer to the newly created grid structure G is
added to the RecIndex ﬁeld corresponding to the appropriate
recommender entry in RecCatalog.
Maintenance. To get the most accurate result, the RecModel
at each cell C should be updated with every single new rating
for a user u whose attributes correspond to the subdomain
covered by C. However, this is infeasible as most recommendation algorithms employ complex computational techniques
that are very costly to update. The update maintenance procedure differs based on the underlying recommender algorithm,
speciﬁed in the CREATE RECOMMENDER statement. Yet, most
of the algorithms may call for a complete model rebuilding to
incorporate any new update. To avoid such prohibitive cost,
we decide to update the RecModel in a cell C only if the
number of new updates in C reaches to a certain percentage
ratio α (a system parameter) from the number of entries used
to build the current model in C. We do so because an appealing
quality of most supported recommendation algorithms is that
as RecModel matures (i.e., more data is used to build it),
more updates are needed to signiﬁcantly change the recommendations produced from it. The smaller the value of α, the
more up-to-date is the RecModel, yet, the larger the cost in
frequently updating RecModel. A typical value of α is 0.5.
To realize this maintenance procedure, we maintain two
counters in each cell C: (1) Ctotal as the number of entries
used to build the current RecModel in C, and (2) Cnew as
the number of updates received in C since the last build of
RecModel. Any update for a user u located in C will increase
new
Cnew by one, yet it would not affect Ctotal . Once CCtotal
> α,
we rebuild the model in C, reset Cnew to 0 and update Ctotal
to be the current number of entries located in C.
Storage and Maintenance Optimization. Since storing
and maintaining a recommendation model for every cell is
quite expensive and may preclude system scalability, R E CATHON automatically determines which recommender cells
to materialize based on (per-cell) statistics. R ECATHON only
materializes RecModel only for hot cells to mitigate the
recommendation query latency as well as reduce both the
overall maintenance cost and the storage overhead occupied
by the multidimensional grid. A hot cell is deﬁned as the cell
that receives more recommendation queries than data updates.

Algorithm 1 Cell Maintenance
1: Function Maintenance (Cell C)
2: if Cnew /Ctotal > α then
3:
UpdateRate ← U C/(T SU − T Sinit )
4:
QueryRate ← QC/(T SQ − T Sinit )
5:
if (1 − β)×QueryRate ≥ ( β× UpdateRate ) then
6:
Create RecModel on disk if not already maintained
7:
Train RecModel using data lying within C
8:
else
9:
Delete RecModel from disk if already maintained
10:
Cnew ← 0
11:
Ctotal ← the current number of entries located in C

To take the materialization decision, R ECATHON maintains the
following statistics for each grid cell C: (1) Queries Count
QC: represents the number of issued recommendation queries
over the recommendation model RecModel in cell C since
the cell was created. (2) Updates Count U C: keeps track of
the number of updates performed over the user/item/ratings
data lying within cell C since the cell was created. (3) Query
TimeStamp T SQ : time stamp of the last query issued over cell
C. (4) Update TimeStamp T SU : time stamp of the last update
transaction performed over cell C.
Algorithm 1 gives the pseudocode of the maintenance
process. Using cell C statistics, the algorithm (pseudocode
omitted for brevity) determines whether to keep and train
the underlying (already existing) RecModel or drop it to
save storage and maintenance overhead. The algorithm ﬁrst
leverages the maintained statistics to calculate the update rate
UpdateRate and query rate QueryRate in cell C. If (1 − β)×
QueryRate is larger than or equal β× UpdateRate, that means
cell C is hot, such that β denotes a system parameter speciﬁed
by the user. In such case, the algorithm creates a recommendation model entry RecModel for cell C on disk and train
RecModel using up-to-date data lying within C. Otherwise,
we delete RecModel from disk and all incoming queries over
C will have to ﬁrst train RecModel on-the-ﬂy.
Note that the value of β exhibits a tradeoff between (1) scalability measured in terms of storage and maintenance overhead
on one hand and (2) query processing performance on the
other hand. The larger the value of β the less the number of
recommendation models maintained in the multidimensional
grid and hence the higher the system scalability and the lower
the query execution performance. On the contrary, a low β
leads to more models being maintained and hence higher query
execution performance for the price of less scalability.
V. Q UERY P ROCESSING
Giving an initialized multidimensional recommender, a
main challenge (Challenge III) is how to efﬁciently execute
recommendation queries over such recommender. The recommendation query takes two input parameters, a created
recommender R as a virtual table and a querying user id uid.
The algorithm then returns a set of tuples S such that each
tuple s ∈ S; s = i, RecScore represents for each item i
(unseen by uid), a recommendation score RecScore using R.
Query 1 in Table I is a very simple example of a recommendation SQL query. Query 1 lists all items that are not rated

58

index. Then, for each tuple t in the rating table that includes
the user u, we retrieve the similarity score between the item i
we want to score and the rated item t.iid, based on the table
RecModel. We accumulate such scores, each weighted by the
user rating t.rating. The ﬁnal score ends up to be the average
total weighted score divided by the total similarity score, as
was described earlier in Section II.

Algorithm 2 R ECOMMEND (uid, R)
Cat ← RecCatalog entry that corresponds to recommender R
G ← Cat.RecIndex (The Multi-dimensional Grid Index)
Attr ← The set of attributes in Cat.Attributes
AttrV ← Values of attributes Attr in table Cat.U sers for uid
C ← The cell in G that corresponds to AttrV
I ← Set of items in table Cat.Ratings that are not rated by user uid
AnswerSet ← φ
for each item i ∈ I do
RecScore ← Cat.Algorithm.GetRecScore(uid, i,
Cat.Ratings,C.RecM odel)
10:
AnswerSet ← AnswerSet ∪ i.iid, RecScore
1:
2:
3:
4:
5:
6:
7:
8:
9:

A. Selection
In many cases, a user may want to only know the recommendation score for a speciﬁc item (e.g., a movie in Netﬂix)
or a set of few items (e.g., a set of few books in Amazon). A
straightforward execution of such queries would perform the
recommendation generation algorithm ﬁrst and then ﬁlter out
the unneeded items. This plan performs well only if the predictive selectivity is very low. For highly selective predicates,
the R ECOMMEND algorithm performs lots of unnecessary
work fetching all items data from disk and calculating their
recommendation scores, while only few items are needed.
Since in many cases, the predicate selectivity is very high
(it is common to select only one item), R ECATHON employs
a variant of R ECOMMEND, called F ILTER R ECOMMEND. Instead of calculating the recommendation scores for all items
lying within cell C, F ILTER R ECOMMEND takes the ﬁltering
predicate as input and prunes the predicted recommendation
score calculation for those items that do not satisfy the ﬁltering
predicate. To achieve that, we modify the loop (lines 8 to 10)
in Algorithm 2 to iterate and calculate the recommendation
only for items that satisfy the iid selection predicate.

11: return AnswerSet

by users uid = 2 along with their predicted recommendation
score, based on the user age, city, and gender. By specifying
the AgeCityGenderRec recommender in the FROM clause, the
R ECOMMEND algorithm will look at the recommender catalog
RecCatalog to ﬁnd out that this recommender needs to know
the age, city, and gender of user u with uid=2, and is built
for the users table, depicted in Figure 1. Retrieving data from
this table, the R ECOMMEND algorithm ﬁnds that this query is
for user Carol; a 22 year old female, living in ’Minneapolis’.
With this data, we follow the index pointer of the RecCatalog
entry to the three-dimensional grid index, and retrieve the
RecModel stored in the grid cell that corresponds to the entry:
22, Minneapolis, Female. As the recommender functionality
aims to provide a predicted score for those items that are not
rated by the querying user, we need to scan the ratings table
(obtained from the RecCatalog) to ﬁnd those items I that are
not rated by uid, but have some ratings from other users.
For each of these items, we compute its predicted score using
the underlying recommendation algorithm, also obtained from
RecCatalog. Finally, the R ECOMMEND algorithm returns all
items in I along with their computed scores as the result.
Algorithm 2 gives the pseudocode of R ECOMMEND. Given
a user ID uid and a recommender R, the algorithm starts
by retrieving the catalog entry Cat that corresponds to the
recommender R. Following the information on the Cat entry,
we get pointers to the corresponding multi-dimensional grid
index G. Then, we retrieve the values for the attributes
speciﬁed in R for the querying user uid. Based on these
values, we locate the cell C in G that represents the user
attributes. Since we need to compute the recommender score
for only those items that are not rated by the user uid, yet
are rated by others, we scan the table of ratings to get the set
of such items I. For each of these items, we call the underlying recommendation algorithm Cat.Algorithm.GetRecScore,
which is speciﬁc to R, as speciﬁed in the USING clause in the
CREATE RECOMMENDER statement. Finally, the ﬁnal answer set
is composed of all the items in T , with the score of each item
computed from the GetRecScore function.
For example, the GetRecScore function (Pseudocode omitted for space constraints) for the Item-Item Collaborative
Filtering takes four parameters: the user u, item i, the table
of ratings, and the RecModel table that is maintained at the
corresponding grid cell of user u in the multi-dimensional grid

B. Join
Query 4 in Table I gives an example of a recommendation
query, where the output of the recommender generation algorithm needs to be joined with another table to get the movie
names instead of their identiﬁers. This is a very common query
in any recommender system. For example, Netﬂix and Amazon
always return the item information, not the item identiﬁers. As
the R ECOMMEND algorithm only returns the item identiﬁer,
the result has to be joined with the item table to return the
item information details, e.g., name, price, specs, etc. The
straightforward plan for executing such join queries may be
acceptable only if there is no ﬁlter over the items table, or
the ﬁlter has very low selectivity. Otherwise, if the ﬁlter is
highly selective, R ECOMMEND ends up doing redundant work
computing the scores for all items, while only few of them are
needed. It is very common to have a very selective ﬁlter over
the items table, e.g., only Action movies.
To efﬁciently support such queries, R ECATHON employs
the J OIN R ECOMMEND algorithm. Besides the user u and a
recommender R, J OIN R ECOMMEND takes a joined database
relation rel (e.g., Movies) as input, combines their tuples,
and returns the joined result. Analogous to index nested loop
join, J OIN R ECOMMEND employs the input relation rel as
the outer relation. For each retrieved tuple tup ∈ rel, the
algorithm calculates the score for item i with iid equal to

59

C. Ranking (Top-k)

450
400
350
300
250
200
150
100
50
0

7
ItemCosCF
ItemPearCF
SVD
UserCosCF
UserPearCF

Storage (Gbytes)

Init. Time (sec)

tup.iid in the same way it was calculated in the R ECOM MEND algorithm (Algorithm 2). The algorithm then concatenates (iid,RecScore) and tup and the resulting joined tuple
(tup,iid,Recscore) is ﬁnally added to the join answer S. The
algorithm terminates when there are no more tuples left in rel.

5
4
3
2
1

1

2

7

14
21
# of Grid Cells

42

(a) Initialization Time

Query 2 in Table I gives an example of a ranking query
where we need to return only the top-10 recommended items.
This is a very important query as it is very common to return
to the user a limited number of recommended items rather than
all items with their scores. The straightforward query plan for
this query would incur too much overhead in computing the
score for all items, and then return only the top 10 ones.
Since top-k recommendation is by far the most commonly
used query in existing commercial applications (e.g., Amazon and Netﬂix), we further optimize its query execution
performance by pre-computing a set of top-|L| items for
each user u and caching these items in a sorted list, named
Lu . When a querying user u issues a top-k recommendation
query, R ECATHON just needs to fetch the ﬁrst k items in Lu
(k << |L|). Note that this approach functions correctly only
for pure top-k recommendation. If the recommendation query
consists of a top-k operation accompanied with selection or
join, using the pre-computed list will not necessarily return a
correct answer. For example, consider Query 5 in Table I. This
query recommends the top 5 Action movies to user uid = 1,
based on the user age (i.e., use the AgeRec recommender).
Query 5 needs to join the AgeRec recommender with the
Movies table to only select Action movies and then return the
top-5 Action movies to user 1. In that case, Lu may not contain
any Action movie whereas the Movies table might contain
some. Hence, R ECATHON would apply J OIN R ECOMMEND
ﬁrst on recommender AgeRec and table Movies and then
perform a traditional top-k operation on the join result.

ItemCosCF
ItemPearCF
SVD
UserCosCF
UserPearCF

6

Fig. 4.

147

0

1

2

7

14
21
# of Grid Cells

42

147

(b) Storage Overhead

Initialization Time and Storage Overhead.

where that user lives. Each item is also assigned a spatial
location that represents where this item is located.
Section VI-A studies the storage and initialization overhead
while Section VI-B studies queries performance. All experiments were performed on a machine with 3.6 Ghz Quad-Core
processor, 16 GB RAM, 500 GB storage, and running Ubuntu
Linux 12.04. The default dataset is MovieLens.
A. Recommender Creation
We evaluate the context-aware recommender creation and
initialization process performance using the following two
metrics: (1) Recommender Initialization Time: the total runtime (in seconds) taken by the system to process a CREATE
RECOMMENDER statement, and (2) Recommender Storage Overhead: the amount of storage (in Gbytes) occupied by the
multidimensional grid and recommendation models created
upon recommender creation. We run our experiments for
the following ﬁve popular recommendation algorithms, all
supported and built into R ECATHON:
1) ItemCosCF: Item-Item collaborative ﬁltering with cosine distance used to measure similarity among items
(explained before in section II).
2) ItemPearCF: Item-Item Collaborative ﬁltering with
Pearson correlation used to measure similarity among
items.
3) UserCosCF: User-User Collaborative Filtering with cosine distance used to measure similarity among users.
4) UserPearCF: User-User Collaborative Filtering with
Pearson correlation used to measure similarity among
users.
5) SVD: Regularized Gradient Descent Singular Value Decomposition.
Figure 4 gives the effect of varying the number of grid cells
for the multi-dimensional grid structure on the context-aware
recommender initialization time and storage overhead. Since
we have three contextual attributes, age, gender, and job, we
have the possibility of eight context-aware recommenders with
1, 2, 7, 14, 21, 42, 147, and 294 three-dimensional grid cells.
A context-aware recommender with one grid cell corresponds
to context-free recommenders that do not take care of any
attributes. On the other side, a context-aware recommender of
294 cells corresponds to the combination of three attributes:
age, gender, and job. The number 294 is the product of 21,
7, and 2, as the number of categories of job, age, and gender

VI. E XPERIMENTAL E VALUATION
This section presents a comprehensive experimental evaluation of R ECATHON based on an actual system implementation inside RecDB [26] integrated with PostgreSQL 9.2.
All proposed techniques are implemented using the iterator
model adopted by PostgreSQL for operator implementations.
We evaluate R ECATHON using two real datasets described
as follows: (1) MovieLens: a real movie recommendation
dataset [20] that consists of 6040 users, 3883 movies, and
one million (1M) ratings. Each user has gender, age, and job
attributes. The user gender attribute consists of two values
{Female, Male}, the age attribute is partitioned into seven
ranges, and the job attribute has 21 job categories. Each
movie has name and genre attributes. The ratings data contains
historical ratings (in a scale from 1 to 5) that users have
assigned to movies. (2) Foursquare: a real venue (e.g., restaurant) recommendation dataset extracted [27] from Foursquare
website, and consists of 150K users, 90K venues, and 1M
ratings. Each user has a spatial location attribute that represents

60

5

Data SIze (* 10 )

1.2
1
0.8
0.6
0.2

5

Data Size (* 10 )

(a) ItemCosCF
(b) SVD
iid Selection Predicate: Varying Data Size (MovieLens)

Rec
FiltRec

0.4

.1 .2 .3 .4 .5 .6 .7 .8 .9 1
% of Results

Fig. 6.

attributes, respectively. Similarly, 147 grid cells correspond to
an age, job-aware recommender, and so on.
Figure 4 shows that the higher the number of grid cells,
the higher the initialization time and storage overhead for
the ItemCosCF, ItemPearCF, and SVD recommendation algorithms. The main reason is that more grid cells lead to
building more recommendation models, as we build one
recommendation model per grid cell. This consumes both
storage and computation time. However, the opposite scenario happens for UserCosCF and UserPearCF algorithms.
This counter intuitive behavior is mainly because these two
recommender algorithms partition the user ratings based on
the user attributes and hence, the number of users is small in
each cell and building a user similarity list for several small
cells is faster (and requires less storage) than building the user
similarity list for one fat cell. Overall, for all recommender
algorithms and number of grid cells, R ECATHON is able to
provide a reasonable computational and storage overhead.
Comparing various recommender algorithms to each other
shows that UserCosCF and UserPearCF are mostly faster and
occupy less storage on disk than ItemCosCF and ItemPearCF.
That happens due to the fact that the number of ratings per user
in the dataset is less than the number of ratings per item. For
all created recommenders, SVD consistently takes more time
than other algorithms since SVD is an iterative algorithm that
takes several iterations to build the recommendation model.

1.4
1.2
1
0.8
0.6
0.4
0.2
0

Response Time (sec)

1 2 3 4 5 6 7 8 9 10

Response Time (sec)

Fig. 5.

Rec
FiltRec

1.4

0.35
0.3
0.25
0.2
0.15
0.1
0.05
0

Rec
FiltRec
.1 .2 .3 .4 .5 .6 .7 .8 .9 1
% of Results

(a) ItemCosCF
(b) SVD
iid Selection Predicate: Varying Selectivity (MovieLens)

Rec
IndRec

1 2 3 4 5 6 7 8 9 10
Data Size (*105)

Response Time (sec)

1 2 3 4 5 6 7 8 9 10

0.3
0.25
0.2
0.15
0.1
0.05
0

Response Time (sec)

Rec
FiltRec

Response Time (sec)

Response Time (sec)

1.2
1
0.8
0.6
0.4
0.2
0

0.3
0.25
0.2
0.15
0.1
0.05
0

Rec
IndRec

1 2 3 4 5 6 7 8 9 10
Data Size (* 105)

(a) ItemCosCF
(b) SVD
Fig. 7. Ranking (Top-k): Varying Data Size (MovieLens)

Varying data size. Figure 5 studies the impact of data size
on the query execution performance. We vary the data size
from 100K to 1M ratings, executing a set of 100 synthetically
generated queries using ItemCosCF and SVD recommendation
algorithms. For all recommender algorithms, FiltRec outperforms Rec by at least an order of magnitude. This is obvious
since FiltRec applies the iid ﬁltering predicate before calculating the predicted recommendation score for an item, which
saves a huge amount of effort wasted by Rec in applying the
R ECOMMEND algorithm ﬁrst on all items and then performing
the predicate ﬁltering step. In the meantime, the response time
increases as the data size gets bigger since we need to retrieve
more recommendation models in ItemCosCF. In the SVD case,
the response time remains unchanged.
Varying Selectivity. Figure 6 gives the impact of iid predicate
selectivity on the average query response time. As it turns out
from the ﬁgure, as we increase the percentage of reported
items (decrease selectivity), the average query response time
in Rec remains constant since the R ECOMMEND algorithm
predicts the recommendation score for all items anyway before
applying the iid predicate. On the other hand, FiltRec query response time increases linearly with the percentage of reported
items until it reaches the same performance as Rec when 100%
of items are reported. That happens because FiltRec, when
100% of items are returned, performs the same amount of
recommendation score calculation as Rec. However, for higher
selectivity (i.e., lower % of results), FiltRec outperforms Rec
by more than an order of magnitude. With iid selection, a
typical selectivity would be very high, i.e., less than 1%, which
shows better performance for FiltRec over Rec.
2) Ranking (Top-k): This section studies the performance
of top-k recommendation queries. Unless mentioned otherwise, k is set to 1000 and the data size is 1M ratings.
Varying data size. Figure 7 depicts the effect of data size
on top-k recommendation query performance. As given in
the ﬁgure, IndRec exhibits more than an order of magnitude
performance over Rec. That is justiﬁed by the fact that IndRec

B. Recommender Queries
In this section, we evaluate the query execution performance
in terms of the query response time. We consider the following
four different approaches for query execution: (1) Rec: a
query plan that only relies on the R ECOMMEND algorithm
to execute recommendation queries. (2) FiltRec: a query plan
that leverages the F ILTER R ECOMMEND algorithm to optimize
recommendation queries with a predicate over iid. (3) IndRec:
a plan that exploits the pre-computed sorted list of recommended items to execute a ranking (Top-k) query. (4) JoinRec:
a plan that employs the J OIN R ECOMMEND algorithm to join a
recommendation with a database table. For space constraints,
we plot only the results of three recommendation algorithms,
namely, ItemCosCF, UserPearCF, and SVD. Experiments run
for the age, gender-aware recommender.
1) Selection Predicate: In this section, we study the performance of recommendation queries with selection predicate
applied the item. Unless mentioned otherwise, the query
selectivity is set to 25% and the data size is 1M ratings.

61

0.01

Rec
IndRec

1 2 3 4 5 6 7 8 9 10

1
0.1
0.01
0.001

2

Rec
IndRec
1 2 3 4 5 6 7 8 9 10
2

Limit (* 10 )

Limit (* 10 )

Join
JoinRec

1 2 3 4 5 6 7 8 9 10
Data Size (*105)

Fig. 9.

Response Time (sec)

Response Time (sec)

(a) ItemCosCF
(b) SVD
Fig. 8. Ranking (Top-k): Varying Limit (k) Size (MovieLens)
4
3.5
3
2.5
2
1.5
1
0.5

40
35
30
25
20
15
10
5
0

6.5
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5

Fig. 10.

Response Time (sec)

0.1

Response Time (sec)

1

Response Time (sec)

Response Time (sec)

10

Join
JoinRec

.1 .2 .3 .4 .5 .6 .7 .8 .9 1
% of Results

70
60
50
40
30
20
10
0

Join
JoinRec

.1 .2 .3 .4 .5 .6 .7 .8 .9 1
% of Results

(a) ItemCosCF
(b) SVD
Join: Varying Joined Table (rel) Selectivity (Foursquare).

and JoinRec, since more data are joined. That happens for
all recommendation algorithms, except for SVD.
Varying Selectivity. In these experiments, we vary the selectivity of the joined table with the input recommender. We
generate a workload of 100 random join queries of the same
selectivity, and execute such queries for each recommendation
algorithm. We simulate the selectivity change in terms of the
ratio of output tuples (ﬁltered by a selection predicate) over
the original number of tuples in the joined table rel. Figure 10
shows that JoinRec exhibits more than an order of magnitude
better performance than Join for high selectivity (small %
of rel). However, when the selectivity decreases (% of rel
increases), JoinRec performance becomes closer to Join since
JoinRec has to compute the predicted score for more items.

Join
JoinRec

1 2 3 4 5 6 7 8 9 10
Data Size (*105)

(a) ItemCosCF
(b) SVD
Join: Varying Joined Recommender Data Size (Foursquare).

leverages the pre-computed recommendation scores, created
for active users, in the RecScore Index to retrieve items in
sorted order. Hence, the limit operator terminates early when
the required number of items is retrieved. On the other hand,
Rec has to ﬁrst calculate recommendation scores, then sort
items based on their score, and ﬁnally pick the top-k items.
Note that the query response time in Rec increases as the
data gets bigger (except for SVD) because Rec takes more
time accessing bigger models. However, the response time in
IndRec slightly increases since IndRec retrieve pre-computed
recommendation scored in sorted order.
Varying k. Figure 8 gives the impact of k on top-k recommendation query performance. We vary k from 100 to 1000,
generate a workload of 100 top-k recommendation queries,
and measure the response time for ItemCosCF and SVD
algorithms. In all algorithms, IndRec achieves more than an
order of magnitude better performance than Rec for all values
of k. Moreover, Rec performance is constant for different k
values, which is explained by the fact that sorting in Rec is
dominating the query execution performance. On the other
side, the response time in IndRec slightly increases for larger
k values as more items are accessed in RecScore Index.
3) Join: This section studies the performance of recommendation queries that involve joining the recommendation answer
with other database tables in the Foursquare dataset. The
joined table has a selection predicate that ﬁlters out unwanted
items. Unless mentioned otherwise, the predicate selectivity is
set to 25% and the data size is 1M ratings.
Varying data size. In this experiment, we build recommenders
with different data sizes (100K to 1M ratings), and we
generate a workload of 100 join queries that join the created
recommender and the movies table. Figure 9 shows that
JoinRec scales about an order of magnitude better than Join
for all recommendation algorithms. The main reason is that
J OIN R ECOMMEND efﬁciently calculates the predicted score
only for ﬁltered items. In the meantime, the bigger the data
size, the worse the query execution performance for both Join

VII. R ELATED W ORK
Recommendation Algorithms. A Recommendation algorithm speculates how much a user would like an item she
has never seen (bought, watched) before. Collaborative Filtering [23] is considered the most popular amongst several recommendation algorithms proposed in the literature [4], [23].
There are several methods to perform collaborative ﬁltering
including item-item [25], user-user [23], regression-based [25],
or approaches that use more sophisticated probabilistic models
(e.g., Bayesian Networks [8]). Collaborative ﬁltering techniques analyze past community opinions to ﬁnd similar users
(or items) to suggest k personalized items (e.g., movies) to
a querying user u. R ECATHON does not come up with a
novel recommendation algorithm. However, it adapts existing
algorithms to generate multidimensional recommendation.
Recommender systems in databases. Few, and recent,
works have studied the problem of integrating the recommender system functionality with database systems. This includes a framework for expressing ﬂexible recommendation
by separating the logical representation of a recommender
system from its physical execution [15], [16], algorithms
for answering recommendation requests with complex constraints [21], [22], a query language for recommendation [5],
and extensible frameworks to deﬁne new recommendation
algorithms [11], [18], leveraging recommendation for database
exploration [10]. Unlike R ECATHON, the aforementioned work
lacks one or more of the following features: (1) Producing
multidimensional recommendation to users, (2) Executing
online recommendation queries in a near real time manner,
(3) Efﬁciently initializing and maintaining multiple recommendation algorithms.

62

Context-Aware Recommendation. Existing context-aware
recommendation algorithms [6] focus on leveraging contextual information to improve recommendation accuracy over
classical recommendation techniques. Conceptual models for
context-aware recommendation have also been proposed for
better representation of multidimensional attributes in recommender systems [6]. Several frameworks have proposed
deﬁning context-aware recommendation services over the
web using either client/server architecture [1], or by mimicking successful web development paradigms [12]. Such
techniques, though they provide support for context-aware
recommendation, do not consider system performance issues
(e.g., efﬁciency and scalability). Location-aware recommender
systems [17], [27] present a special case of context-aware
recommender systems, where efﬁciency and scalability are
main concerns. However, the proposed techniques for locationaware recommender systems are strongly geared towards the
spatial attribute, with no direct applicability to other attributes.

[6] G. Adomavicius et al. Incorporating Contextual Information in Recommender Systems Using a Multidimensional Approach. ACM Transactions on Information Systems, TOIS, 23(1), 2005.
[7] S. Amer-Yahia, A. Galland, J. Stoyanovich, and C. Yu. From del.icio.us
to x.qui.site: recommendations in social tagging sites. In Proceedings of
the ACM International Conference on Management of Data, SIGMOD,
2008.
[8] J. S. Breese, D. Heckerman, and C. Kadie. Empirical Analysis of
Predictive Algorithms for Collaborative Filtering. In Proceedings of
the Conference on Uncertainty in Artiﬁcial Intelligence, UAI, 1998.
[9] A. Das et al. Google News Personalization: Scalable Online Collaborative Filtering. In Proceedings of the International World Wide Web
Conference, WWW, 2007.
[10] M. Drosou and E. Pitoura. YMALDB: A Result-Driven Recommendation System for Databases. In Proceedings of the International
Conference on Extending Database Technology, EDBT, 2013.
[11] M. D. Ekstrand, M. Ludwig, J. A. Konstan, and J. T. Riedl. Rethinking
the recommender research ecosystem: reproducibility, openness, and
lenskit. In RecSys, 2011.
[12] T. Hussein, T. Linder, W. Gaulke, and J. Ziegler. Context-aware
Recommendations on Rails. In International Workshop on Contextaware Recommender Systems, CARS, 2009.
[13] H. Kailun, W. Hsu, and M. L. Lee. Utilizing Social Pressure in
Recommender Systems. In Proceedings of the IEEE International
Conference on Data Engineering, ICDE, 2013.
[14] B. Kanagal, A. Ahmed, S. Pandey, V. Josifovski, J. Yuan, and L. G.
Pueyo. Supercharging Recommender Systems using Taxonomies for
Learning User Purchase Behavior. PVLDB, 5(10):956–967, 2012.
[15] G. Koutrika, B. Bercovitz, and H. Garcia-Molina. FlexRecs: Expressing
and Combining Flexible Recommendations. In Proceedings of the ACM
International Conference on Management of Data, SIGMOD, 2009.
[16] J. J. Levandoski, M. Sarwat, M. D. Ekstrand, and M. F. Mokbel. RecStore: An Extensible and Adaptive Framework for Online Recommender
Queries inside the Database Engine. In Proceedings of the International
Conference on Extending Database Technology, EDBT, 2012.
[17] J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel. LARS:
A Location-Aware Recommender System. In Proceedings of the IEEE
International Conference on Data Engineering, ICDE, 2012.
[18] J. J. Levandoski, M. Sarwat, M. F. Mokbel, and M. D. Ekstrand. RecStore: An Extensible and Adaptive Framework for Online Recommender
Queries inside the Database Engine. In Proceedings of the International
Conference on Extending Database Technology, EDBT, 2012.
[19] G. Linden, B. Smith, and J. York. Amazon.com Recommendations:
Item-to-Item Collaborative Filtering. IEEE Internet Computing, 7(1),
2003.
[20] Movielens Datasets: http://www.grouplens.org/node/73.
[21] A. G. Parameswaran, H. Garcia-Molina, and J. D. Ullman. Evaluating,
combining and generalizing recommendations with prerequisites. In Proceedings of the International Conference on Information and Knowledge
Managemen, CIKM, 2010.
[22] A. G. Parameswaran, P. Venetis, and H. Garcia-Molina. Recommendation systems with complex constraints: A course recommendation
perspective. ACM Transactions on Information Systems, TOIS, 29(4):20,
2011.
[23] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl. GroupLens: An Open Architecture for Collaborative Filtering of Netnews. In
CSWC, 1994.
[24] S. B. Roy, S. Amer-Yahia, A. Chawla, G. Das, and C. Yu. Space
efﬁciency in group recommendation. VLDB Journal, 19(6), 2010.
[25] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-Based Collaborative Filtering Recommendation Algorithms. In Proceedings of the
International World Wide Web Conference, WWW, 2001.
[26] M. Sarwat, J. Avery, and M. F. Mokbel. RecDB in Action: Recommendation Made Easy in Relational Databases. Proceedings of the Very
Large DataBases Endowment, PVLDB, 6(12):1242–1245, 2013.
[27] M. Sarwat, J. J. Levandoski, A. Eldawy, and M. F. Mokbel. LARS*: A
Scalable and Efﬁcient Location-Aware Recommender System. In IEEE
Transactions on Knowledge and Data Engineering, TKDE, 2014.
[28] M. Vartak and S. Madden. CHIC: a combination-based recommendation
system. In Proceedings of the ACM International Conference on
Management of Data, SIGMOD, 2013.
[29] H. Yin, B. Cui, J. Li, J. Yao, and C. Chen. Challenging the Long Tail
Recommendation. PVLDB, 5(9):896–907, 2012.

VIII. C ONCLUSION
We have presented R ECATHON; a multidimensional recommender system. In an analogous way to creating indexes on
table attributes that are most likely to appear in incoming
queries, R ECATHON users can create a recommender over a
certain set of attributes. The created recommender can be then
queried to provide recommended items based on a certain
set of attributes. Examples of recommender queries include
“Recommend me 10 movies that people in my age and gender
would like”. In this case, we issue an age, gender-aware
recommender. If such recommender is created a-priori, this
query will entertain a real-time response. R ECATHON employs
a multi-dimensional index for each built recommender to
support efﬁcient recommender queries. The system is able
to seamlessly integrate the recommendation functionality with
traditional database operations to execute a variety of contextaware recommendation queries. Extensive experiments show
that R ECATHON exhibits real time performance for large-scale
context-aware recommendation scenarios.
IX. ACKNOWLEDGEMENT
This work is supported by the National Science Foundation,
USA, under Grants IIS-0952977 and IIS-1218168.
R EFERENCES
[1] S. Abbar, M. Bouzeghoub, and S. Lopez. Context-aware recommender
systems: A service-oriented approach. In PersDB, 2009.
[2] F. Abel, Q. Gao, G.-J. Houben, and K. Tao. Analyzing user modeling
on twitter for personalized news recommendations. In Proceedings of
the International Conference on User Modeling, Adaption and Personalization, UMAP, 2011.
[3] G. Adomavicius, B. Mobasher, F. Ricci, and A. Tuzhilin. Context-aware
recommender systems. AI Magazine, 32(3), 2011.
[4] G. Adomavicius and A. Tuzhilin. Toward the Next Generation of
Recommender Systems: A Survey of the State-of-the-Art and Possible
Extensions. IEEE Transactions on Knowledge and Data Engineering,
TKDE, 17(6), 2005.
[5] G. Adomavicius, A. Tuzhilin, and R. Zheng. Request: A query language
for customizing recommendations. Information Systems Research,
22(1):99–117, 2011.

63

2015 16th IEEE International Conference on Mobile Data Management

Interactive and Scalable Exploration of Big Spatial
Data - A Data Management Perspective
(Invited Industrial Paper)
Mohamed Sarwat
School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University, Tempe, AZ
699 South Mill Ave, Tempe, AZ 85281, USA
msarwat@asu.edu
Abstract—Recently, the volume of available spatial data increased tremendously. For instance, in November 2013 NASA
announced the release of hundreds of Terabytes of its earth
remote sensing dataset. Such data includes but not limited to:
weather maps, socioeconomic data, vegetation indices, geological
maps, and more. Making sense of such spatial data will be
beneﬁcial for several applications that may transform science and
society – For example: (1) Space Science: that allows astronomers
to study and probably discover new features of both the earth
and the outer space, (2) Socio-Economic Analysis: that includes
for example climate change analysis, study of deforestation,
population migration, and variation in sea levels, (3) Urban
Planning: assisting government in city planning, road network
design, and transportation engineering, (4) Disaster Planning:
that helps in assessing the impact of natural disasters. The
main aim of this paper is to investigate novel data management
techniques that enable interactive and scalable exploration of big
spatial data.
The paper envisions novel system architectures that provide
support for interactive and spatial data exploration, as follows:
(1) The paper suggests extending data analytics frameworks,
e.g., Apache Spark, to support spatial data types and operations
at scale. The resulting framework will serve as a scalable
backbone for processing spatial data exploration tasks. (2) It
also sketches novel structures and algorithms that leverage
modern hardware, e.g., SSDs, and in-memory data processing
techniques to efﬁciently store and access spatial data. Second,
the paper proposes extending spatial database systems to support an exploration-aware spatial query evaluation paradigm
through three novel components: (1) Spatial Query Steering:
that allows the user to slightly modify the query conditions
online (zooming in/out) and retrieve the new results in very
low latency. (2) Recommendation-Aware Spatial Querying: that
injects the recommendation functionality inside classical spatial
query executors to support spatial data recommendation. It
leverages recommendation algorithms to predict what spatial
objects/areas the user would like based on her past interactions
with the system. (3) Spatial Query Approximation: That aims
at achieving interactive performance by studying the tradeoff
between approximate spatial data exploration and query response
time.

2013 NASA announced the release of a 500 TeraBytes of
its earth dataset generated by remote sensing and satellite
imagery technologies [17]. Such data includes but not limited
to: weather maps, socioeconomic data, vegetation indices,
geological maps, and more. Furthermore, NASA mentions
that the released data represents just an initial collection and
that they plan to publish more datasets through the Open
NASA Earth Exchange (OpenNEX) program. On the other
hand, novel technology allows hundreds of millions of users
to frequently use their mobile devices to access their healthcare
information and bank accounts, interact with friends, buy stuff
online, search interesting places to visit on-the-go, ask for
driving directions, and more. In consequence, everything we
do on the mobile internet leaves breadcrumbs of spatial digital
traces, e.g., geo-tagged tweets, venue check-ins.
Making sense of such spatial data will be beneﬁcial for
several applications that may transform science and society
– For example: (1) Space Science: that allows astronomers
to study and probably discover new features of both the
earth and the outer space, (2) Socio-Economic Analysis: that
includes for example climate change analysis [13], study of
deforestation [64], population migration [12], and variation in
sea levels [59], (3) Urban Planning: assisting government in
city/regional planning, road network design, and transportation
/ trafﬁc engineering [30], (4) Disaster Planning: That helps
in assessing the impact of natural (e.g., Hurricanes) and
man-made disasters, (5) Commerce and Advertisement [8],
[15], [61], [67]: that includes for instance point-of-interest
(POI) recommendation services in which we analyze the user
spatial behavior and recommends POIs accordingly. Many
of the aforementioned applications needs a powerful data
management platforms to handle the large volume of spatial
data such applications deal with.

I. I NTRODUCTION
Spatial data represent objects that possess spatial attributes
which denote the spatial locations and/or geometrical shape
of these objects. In the past decade, the volume of available
spatial data increased tremendously. For instance, in November

A. Challenges for Big Spatial Data Exploration

978-1-4799-9972-9/15 $31.00 © 2015 IEEE
DOI 10.1109/MDM.2015.67

Data scientists are in deep need of a system that allows
them to make sense of big spatial data. Despite the abundance
of spatial data, most of it is not well-leveraged yet mostly due
to the following system challenges:
263

•

•

•

Challenge I: Exploratory Interface. In many cases, the
user (data scientist/analyst) does not know exactly what
kind of information he needs to extract from the spatial
data at hand. A user would need a data management
system that allows her to explore, and not only to query
spatial data using classical query processing techniques.
Challenge II: System Scalability. The massive-scale of
available spatial data hinders making sense of it using
traditional spatial query processing techniques. Moreover,
big spatial data, besides its tremendous storage footprint,
may be extremely difﬁcult to manage and maintain.
The underlying data management system must be able
to digest Petabytes of spatial data, effectively stores it,
and allows applications to efﬁciently retrieve it when
necessary.
Challenge III: Interactive Performance. In a spatial
data exploration session, the user will not tolerate delays
introduced by the underlying spatial data management
system to execute queries efﬁciently. Instead, the user
needs to see useful information quickly and interactively
change her query if necessary. Hence, the underlying
spatial database system must ﬁgure out effective ways
to process user’s request in a sub-second response time.

To address the problem of injecting exploration-awareness
in the classical spatial querying paradigm (Challenge I), the
paper envisions an exploration-aware spatial query execution
component would provide the following main functionalities:
(1) Spatial Query Steering and Speculation: Speculate what
kind of spatial data and/or spatial operations the user might
be interested in based on her query context and history of
interactions with the system. This task investigates how the
spatial query executor could help the user to slightly modify
the query qualiﬁcations and retrieve the new results in subsecond response. A slight modiﬁcation might be a small
change to the designated spatial range or the proximity condition, and zooming in/out the initial spatial area of interest.
(2) Recommendation-Aware Spatial queries: This research task
studies the integration of recommender systems functionalities
with state-of-the-art spatial query execution engines. In such
case, the user might provide feedback by specifying whether
she liked (or not) the spatial exploration results retrieved so
far. The system then quickly learns the user’s preferences and
retrieve more relevant results during the same or future spatial
data exploration session. (3) Spatial Query Approximation:
Existing spatial query executors utilize a lot of the system
resources (and still take too much time) to generate an exact
spatial query answer whereas an approximate answer might
sometimes be good enough. This paper studies the tradeoff
between result accuracy and performance in spatial join and
spatial aggregate analytics and proposes a framework for realtime approximate query execution over large-scale geospatial
data. The main goal is to generate approximate spatial query
answers in sub-seconds response time (Challenge III) while
minimizing the accuracy loss.

B. GeoExpo - Vision
This paper investigates novel data management techniques
for interactive and scalable exploration of big spatial data.
To tackle the aforementioned challenges, this paper proposes
GeoExpo a system that consists of the following two main
components:
The ﬁrst component investigates novel system architectures
that provide support for interactive and spatial data exploration
(Challenges I & II). To build this component, we propose
the following two main research tasks: (1) The ﬁrst task
addresses the problem of extending Apache Spark (a scalable
data analytics framework) to support spatial data types and
operations. The resulting framework will serve as a scalable backbone for processing spatial data exploration tasks.
(2) Existing spatial database structures [28], [41], [36], [33]
and algorithms (e.g., spatial range, spatial join [10], [48],
[51]) deal with the implicit assumption that the underlying
secondary storage is the mechanical hard disk (HDD). HDD
notoriously exhibits poor data access performance, especially
when interactive performance is deemed a priority. On the
other hand, modern non-volatile memory devices (i.e., SolidState-Drives) exhibits orders of magnitude better I/O speed
and throughput than HDD. This paper proposes tailored spatial
data structures and algorithms that leverage modern nonvolatile memory devices to efﬁciently store and access spatial
data residing on each spatial data node. Furthermore, this paper
aims at leveraging state-of-the-art in-Memory data processing
techniques to achieve sub-second response time necessary
for spatial data exploration workload. The ultimate goal is
to harness the maximum capacity of the memory hierarchy
for storing and accessing spatial data to achieve interactive
performance (Challenge III).

II. G EO E XPO OVERVIEW
GeoExpo has two main components (see Figure 1): (1) novel
system architectures to support scalable spatial computing
operations (Section III). (2) investigates extending state-ofthe-art spatial database systems to support exploration-aware
spatial query processing techniques (Section IV).
III. S CALABLE S PATIAL DATA M ANAGEMENT P LATFORM
The goal of this component is two-fold: (1) Extending statethe-art scalable cluster computing paradigms to support spatial
computations (Section III-A) and (2) Crafting novel spatial
data management techniques on modern hardware devices
(Section III-B).
A. Extending Apache Spark to support Spatial Data
Existing spatial database management systems, e.g, PostGIS [49], extends classical relational database systems with
new data types, functions, operators, and index structures
to handle spatial operations based on the Open Geospatial
Consortium. Even though such systems sort of provide full
support for spatial data storage and access, they suffer from
a scalability issue. Based upon a relational database system,
such systems are not scalable enough to handle more than
Gigabytes or at most few terabytes of spatial data, and hence

264

Fig. 1.

expressed using regular RDD transformations and actions.
GeoExpo investigates the problem of how spatial data could
represented using RDDs to form spatial RDDs (SRDDs).
More speciﬁcally, this task addresses the problem of efﬁciently partitioning SRDD data elements across machines and
introduces novel parallelized spatial (geometric operations that
follows the Open Geosptial Consortium (OGC) [46] standard)
transformations and actions (for SRDD) that provide a more
intuitive interface for users to write spatial data exploration
and analytics programs on-top of Spark. This task also aims
at extending Spark SQL engine (Shark / Spark SQL) [20], [60]
to support spatial query evaluation. SharkGIS would consists
of a set of SQL User-Deﬁned-Functions (UDFs) that maps to
spatial data types and proximity constraints. The input/output
of these UDFs would be quite similar to UFDs deﬁned in
PostGIS, an extension to PostgreSQL [1] that provides a SQL
interface for users to express spatial operations on geographical data. SharkGIS (the proposed Shark Spatial Extension)
will build on the SRDDs concept (mentioned earlier) to decide
how spatial object-relational tuples could be stored, indexed,
and accessed using SRDDs. Furthermore, One of the goals of
SharkGIS is to craft efﬁcient methods to process, spatial range,
KNN (K-Nearest Neighbors), and spatial join queries [35],
[34], [48] inside Apache Spark.

GeoExpo Overview.

are not able to perform large-scale analytics over big spatial
data. Recent works (e.g., [5], [6], [18], [19], [45], [65])
extend the Hadoop [29] ecosystem, e.g., MapReduce [14] and
Hive [58], to perform spatial analytics at scale. The Hadoopbased approach indeed achieves high scalability. However,
these systems though exhibits excellent performance in batchprocessing jobs, they show poor performance handling applications that require interactive performance. For instance,
consider the case when a data scientist wants to analyze spatial
data with exploration intent and hence cannot tolerate hours
to get the analysis results.
Apache Spark [57], [63], on the other hand, is a in-memory
cluster computing system. Spark provides a novel data abstraction called resilient distributed datasets (RDDs) [62] that are
collections of objects partitioned across a cluster of machines.
Each RDD is built using parallelized transformations (ﬁlter,
join or groupBy) that could be traced back to recover the
RDD data. For fault tolerance, Spark rebuild lost data on
failure using lineage: each RDD remembers how it was built
from other datasets (through transformations) to recover itself.
RDDs allow Spark to outperform existing models (Hadoop
MapReduce) by up to two orders of magnitude in multi-pass
analytics. Spark is easy to use and program as it provides an
intuitive API for users to write their data analytics tasks on
RDDs. Unfortunately, Spark does not provide native support
for spatial data and spatial operations. Hence, users need to
perform the tedious task of programming their own spatial
data exploration jobs on top of Spark.
This module addresses the problem of extending Apache
Spark to support spatial data types and operations. The main
aim is to extend the resilient distributed datasets (RDDs) concept to support spatial data. This problem is quite challenging
due to the fact that (1) spatial data may be quite complex,
e.g., rivers’ and cities’ geometrical boundaries, (2) spatial (and
geometric) operations (e.g., Overlap, Intersect, Convex Hull,
Cartographic Distances) [50] cannot be easily and efﬁciently

B. Spatial Data Management on Modern Hardware
Existing spatial data management algorithms deal with the
implicit assumption that the underlying secondary storage is
the mechanical hard disk (HDD). HDD notoriously exhibits
poor data access performance, especially when interactive
performance is deemed a priority [27]. GeoExpo investigates
novel spatial data structures and algorithms that leverage
modern non-volatile memory devices (e.g., Flash Memory
Solid-State-Drives (SSD), Phase Change Memory (PCM)) to
efﬁciently store and access spatial data. For instance, an SSD
device is block-oriented, i.e., pages are clustered into a set
of blocks. Moreover, SSDs are also able to parallelize I/O
operations and hence increase the overall I/O throughput.
Thus, it has fundamentally different characteristics, compared
to the conventional magnetic disks, especially for the write
operations.Such a popularity of SSDs is mainly due to its
superior characteristics that include smaller size, lower power
consumption, and faster read performance [9], [26], [40], [66].
This module presents novel spatial data structures and
algorithms that leverage modern Non-Volatile Memory devices
to efﬁciently store and access spatial data. Spatial tree indexes,
e.g., R-Tree and its variants [36], [37], [56], are crucial for
efﬁcient spatial data retrieval. Existing solutions are build
for one-dimensional B+ -tree like indexes and hence are not
well adapted to handle spatial data workloads [4], [44], [43],
[39]. GeoExpo distinguishes itself from previous approaches
in two main aspects: (1) Rather than focusing on a speciﬁc
index structure or building a new index, We aim to build
a generic framework, similar to the GIST framework [31],
that can be applied to a wide variety of tree index structures,
including R-tree along with its variants. (2) GeoExpo aims at

265

achieving both efﬁciency and durability in the same design.
For efﬁciency, the proposed approach buffers all the incoming
updates in memory while employing a ﬂushing policy that
evicts selected updates from memory to minimize the cost of
writing to the ﬂash storage. In the mean time, we need guarantee durability by sequentially logging each in-memory update
and by employing an efﬁcient crash recovery technique. In our
preliminary study, we designed FAST [54], [55]; a framework
for Flash-Aware Spatial Tree index structures. Algebraic cost
models and experiments show that the proposed approach provided 80 % boost in performance for range queries over roadnetwork spatial datasets. We plan to extend this idea by investigating the use of a richer memory hierarchy that includes:
RAM, SSD, and HDD [16]. Our initial study shows that we
could classify spatial regions as hot or cold based on the spatial
operations performed on these regions. Therefore, the system
caches hot spatial regions on SSDs and cold spatial regions on
HDD to decrease the overall I/O latency. Another goal of this
task is extending spatial database systems to harness the I/O
parallelism in SSDs. To our knowledge, no previous works
investigated leveraging internal SSD parallelism to increase
spatial database I/O throughput. There exist multiple levels of
parallelism in ﬂash memory SSDs. For instance, ﬂash memory
packages are connected to an SSD controller through multiple
channels. We can control the spatial operations’ I/O workload
such that each channel can be operated independently and
simultaneously. Moreover, since each ﬂash memory package
can be operated independently, spatial I/O operations on a
package attached to the same channel can be interleaved so the
bus utilization can be optimized. Moreover, we propose storing
col-located spatial data on the same die of a ﬂash memory
package such that each die can execute independent spatial
commands which increases the overall spatial I/O throughput.
This task also investigates spatial data requests re-scheduling
to exploit internal parallelism of SSDs to further boost the
overall performance of spatial data exploration tasks.

speculation algorithm that is able to speculate what kind of
queries the user might issue in her spatial data exploration
session. This task is challenging for the following reasons:
(1) The number of spatial query variations might be endless
and hence speculatively computing the answer to all possible
variations leads to huge system overhead. (2) Even if the
number of speculative queries is reasonable, the user might
wind up not using any of the speculatively calculated answers
and hence the amount of work spent on speculation and data
pre-fetching would be a waste.
This module investigates the problem of predicting the
user’s future interactions (modiﬁcation to original query) with
the system during her current exploration session. When the
user issues an ad-hoc spatial query (e.g., Q1 : Return Air
Quality data within Tempe, Arizona), the system might for
instance determine a set of speculative queries that returns air
quality in geospatial regions that are of close proximity to
Tempe (e.g., Q2 : Return Air Quality Data within Scottsdale,
AZ), bigger regions that contain Tempe (e.g., Q3 : Return Air
Quality Data within the Phoenix Metropolitan area), or smaller
neighborhoods within Tempe (e.g., Q4 : Return Air Quality
Data around the Arizona State University Campus). The
system processes these speculative queries in the background,
and materializes the returned spatial objects in a data structure
called the SQ Cache. The query speculation algorithm relies
on two main components to take its decision, brieﬂy described
as follows: (1) Ofﬂine Modeling Component: that takes as
input: (a) User,Object Set: this represents the set of spatial
objects (polygons or points) that appeared in the answer of
past spatial queries issued by each system user. (b) Spatial
Exploration Constraints: the possible set of exploration actions
available for the system users to change the qualiﬁcations
of the spatial queries. Therefore, the system builds a query
speculation model using a Hidden Markov Model in which
the hidden state is the successive spatial queries in the exploration session. (2) Spatial Query Steering: this component
implements a deterministic ﬁnite state automaton (DFA) that
represents the spatial exploration session: each DFA state
denotes a spatial exploration action (Panning, Zooming In/Out)
as well as user feedback states. This component also performs
progressive processing of spatial queries where the system
produces the result for speculative queries based on partially
processed spatial regions. When the user submits a new adhoc spatial query, the system calculates the likelihood that the
user would perform a speciﬁc action, generates speculative
spatial queries accordingly, and returns the top-k queries that
are likely to be posed by the user during the exploration
session. The system pre-computes and materializes the relevant
spatial data in the SQ Cache which will be fetched later when
necessary.

IV. E XPLORATION -AWARE S PATIAL Q UERY E VALUATION
Existing spatial databases exhibit ill-Support for an exploration workloads – They do not provide a querying interface to
its users that facilitates spatial data exploration. They instead
necessitates that users must know (in advance) what spatial
data they need and allows them to issue queries accordingly.
GeoExpo aims at crafting an exploration-aware spatial querying paradigm through the following research tasks.
A. Spatial Query Steering and Speculation
This task investigates query speculation for spatial data
exploration. For example, assume the user issues a spatial
query [47] that retrieves spatial objects that are within a
speciﬁc rectangular range R. Assume the user, in exploration
mode, then decided to slightly expand, shrink, or move the

original rectangular range query R to R . If the system could


predict R , it might speculatively pre-fetch the answer to R

so that the user gets the answer to R very fast when needed.
To achieve that, the system needs to employ a smart query

B. Spatial Query Approximation
Achieving real-time performance for queries issued over
big spatial data may sometimes be quite challenging even
when employing high performance computing and modern
hardware infrastructure. Assume an environmental scientist

266

 
 

	


	



!!!
!!!

	



!!!
!!!
!!!

	




!!!
!!!

Fig. 4.
Fig. 2.

Fig. 3.

spatial aggregation models (i.e., Count, Average, Sum) using
only the spatial objects with user locations contained in the
cell’s spatial region. A spatial object may contribute to up to
H aggregation models; one per each pyramid level. Note that
the root cell (level 0) of the pyramid represents the aggregate
values built using all spatial data in the system. Levels in the
pyramid can be incomplete, as we will periodically merge or
split cells. For example, in Fig 4, the four cells in the upper
right corner of level 3 are not maintained (depicted as blank
white squares).
This pyramid structure is advantageous over a simple grid
structure for two main reasons: (1) We will be maintaining
much less number of cells than the gird structure as not every
cell in every level is maintained, and (2) Users can freely
explore different levels (Zoom in/out) of the pyramid by setting
a preferred zoom level indicating how localized they would
like to receive their spatial aggregate measure. A zoom level
of zero means that the user would like to get her aggregate
measure from the root level. The higher zoom level, the more
localized is the answer. We aim to build on our initial ideas to:
(1) build accurate cost models and quality of service measures
for the shape of the spatial exploration pyramid to study the
trade-off of merging four neighbor grid cells to only one
maintained at a higher level. Merging children cells will reduce
the system overhead as less cells will need to be maintained,
however, this may affect how the accuracy of more localized
spatial aggregation, and (2) design an online decision model
that decides on merging and splitting pyramid cells online.
A merging decision can be taken if an good approximate
(with reasonable accuracy) answer at an individual (child) cell
could be easily inferred using the spatial aggregate measure
stored at a merged (parent) cell. Also, a splitting decision
can be taken if the system ﬁnds that more localized answer
can be obtained, and that is worth the extra overhead. Since
existing approximate query processing techniques do not take
into account the characteristics of complex spatial [3], [2], this
task also studies spatial data sampling techniques to process
spatial aggregation with guaranteed error bounds.

San Francisco Heat Map

– studying the relationship between air quality and trees –
would like to explore the trees population in San Francisco
(See Figure 21 . As given in Fig 3, a query that just returns
all trees in San Francisco would not help the user much.
Alternatively, a heat map (spatial aggregate) that shows the
distribution of trees in San Francisco (See Fig 32 ) would
be more helpful to the data scientist to explore the data.
This spatial aggregate query (i.e., heat map) needs to count
all trees at every single latitude,longitude position over the
map. Now assume the user zooms-in the San Francisco trees
heat map. Given that spatial data size (e.g., weather) and
the U.S. geographical locations might be huge, traditional
SQL aggregation techniques [24], [23], [25] may take so
long to execute such interactive exploration queries. This task
addresses the problem of trading interactive performance for
accuracy in evaluating spatial queries. The main aim of this
task is to come up with a good enough approximation to
answer spatial queries in real time.
The idea of our ﬁrst cut solution is to craft a novel
data structure, called the spatial exploration pyramid structure
instead of a simple grid structure [7], [21], as depicted in
Figure 4. The pyramid decomposes the space into H levels
(i.e., pyramid height). For a given level h, the space is
partitioned into 4h equal area grid cells. In each cell, we store
1 Figure
2 Figure

Exploration Pyramid

San Francisco Trees

C. Recommendation-Aware Spatial Query Processing
Many spatial operations, e.g., Spatial Overlap, Spatial Join,
relies on spatial data indexes (e.g., R-tree, Grid) as a back-

created using ArcGIS JavaScript API
created using ArcGIS JavaScript Heatmap Overlay API

267

bone to efﬁciently store and access spatial data. However,
existing spatial data index structures allows users to lookup
spatial data that matches exactly the designated query. For
example, Query 2 (given below) returns all hotels (store in
the Hotels Table in PostGIS) that lie within the spatial extents
of the Phoenix metropolitan area. Such classical spatial queries
clearly do not provide support for spatial data recommendation.

tial data using both their spatial attributes and their recommendation scores. This research also extends existing spatial query
execution engines with an additional Spatial-Recommend SQL
operator. Consider the following scenario: Alice plans to visit
‘Phoenix’ for business and she looks for Hotels in the area.
Query 2 predicts how much user 1 (i.e., Hotel) would like
unexplored hotels using the RECOMMEND operator (based on
the singular value decomposition algorithm [22]). The query
also leverages the ST Contains() function (provided by
PostGIS) to recommend only hotels that lie within the extent
of the ‘Phoenix’ area. In this scenario, the system ﬁrst needs
to retrieve hotels that lie within the ‘Phoenix’ area. Therefore,
the system could predict what hotels Alice would be interested
in based on her past interactions as well as other users’
interactions with the system [53], [42].

Query 1: Retrieve all Hotels that exist in the ‘Phoenix’
urban area.
Select H.name From Hotels as H, City as C
Where C.name = ‘Phoenix’
AND ST Contains(C.geom, H.geom)

Recommendation [11], [32], [38], [52] is the process of
suggesting useful data to the user with based on a large pool of
historical data. GeoExpo aims to extend existing spatial data
index structures to support spatial data exploration through
recommendation. In order to support spatial data exploration,
we propose extending existing spatial data indexes and query
execution engines to support spatial data recommendation. The
problem is to come up with an efﬁcient way to store spatial
data, ﬁlter it, predict its relevancy and recommend it.
The straightforward approach considers a set of data points
of interest in space and then constructing a spatial index,
e.g., R-tree, on all the spatial data objects. Similarity between
different items/data points in the database can be calculated
by using the cosine distance. When the user asks for a set
of spatial objects in a certain boundary region, the system
ﬁrst searches for all the points lying within the region. It
is not feasible to go through all the data point outcomes
from the search. For this purpose, the system ﬁlters huge
data ignoring all those points that may not interest him based
on his previous interactions with the system. Then, only the
items with top-k recommendation scores are displayed to the
user. The disadvantages of this method is as follows: (1) If
the spatial search gives returns many results, recommendation
score calculation has to be done for all those data points.
Hence, this approach is advisable only for highly selective
queries. (2) In case the total number of spatial objects exceeds
the number of objects explored by the user, this method may
not be adequate. (3) If a user did not rate many spatial data
points in the given region, then recommendations may not be
appropriate.
Some of these disadvantages can be overcome by calculating recommendation score of items rated prior to searching
them on the R-tree. The data points with recommendation
scores are looked up in the spatial index to check whether they
belong to the designated location. All those objects belonging
to the desired region will be sorted and the top-k will be
eventually displayed to the user. This problem can be further
optimized to give faster recommendations by improving the
storage of spatial data. GeoExpo investigates the problem of
augmenting existing spatial indexes, e.g., R-Tree, to ﬁlter spa-

Query 2: Predict How much user 1 like Hotels that exist in
the ‘Phoenix’ urban area.
Select H.name, R.ratingva
From HotelRatings as R, Hotels as H, City as C
Recommend R.iid To R.uid On R.ratingVal Using SVD
Where R.uid=1 AND R.iid=H.vid
AND C.name = ‘Phoenix’
AND ST Contains(C.geom, H.geom)

As presented above, GeoExpo aims at seamlessly integrating
the recommendation functionality within the core of a spatial
database engine.
V. C ONCLUSION
The paper proposes GeoExpo; a system that tackles data
management challenges that lie ahead of enabling scalable
and interactive spatial data exploration. More speciﬁcally, the
paper lists three main system challenges that faces spatial
data exploration applications: (1) An Exploratory Interface,
(2) System Scalability, and (3) Interactive Performance. To
address these challenges, GeoExpo envisions two main components: (A) High-Performance Spatial Computing Platform: this
component aims at leveraging state-of-the-art cluster/parallel
computing paradigms (i.e., Apache Spark) to support complex
geospatial operations at scale. It also extends existing spatial
data stores to harness the uniques characteristics of NonVolatile Memory devices to boost spatial data access performance. (B) Exploration-Aware Spatial Querying Paradigm:
This component re-thinks existing spatial querying paradigms
to incorporate user exploration intent. More speciﬁcally, this
component investigates speculative query execution and query
steering strategies to provide interactive performance during
the user’s spatial data exploration section. This component also
studies the tradeoff between the accuracy of the spatial query
result and the system responsiveness and proposed an efﬁcient
approximate spatial query execution approach. Moreover, this
component integrates state-of-the-art recommendation systems
to allow users to explore spatial data based on their personal
preferences and their past interactions with the system.

268

R EFERENCES
[1] PostgreSQL. http://PostgreSQL:http://www.postgresql.org.
[2] S. Agarwal, H. Milner, A. Kleiner, A. Talwalkar, M. I. Jordan, S. Madden, B. Mozafari, and I. Stoica. Knowing when you’re wrong: building
fast and reliable approximate query processing systems. In Proceedings
of the ACM International Conference on Management of Data, SIGMOD, pages 481–492, 2014.
[3] S. Agarwal, B. Mozafari, A. Panda, H. Milner, S. Madden, and I. Stoica.
BlinkDB: queries with bounded errors and bounded response times on
very large data. In EuroSys, pages 29–42, 2013.
[4] D. Agrawal, D. Ganesan, R. K. Sitaraman, Y. Diao, and S. Singh.
Lazy-Adaptive Tree: An Optimized Index Structure for Flash Devices.
PVLDB, 2009.
[5] A. Aji, X. Sun, H. Vo, Q. Liu, R. Lee, X. Zhang, J. H. Saltz, and F. Wang.
Demonstration of hadoop-gis: a spatial data warehousing system over
mapreduce. In Proceedings of the ACM Symposium on Advances in
Geographic Information Systems, ACM GIS, 2013.
[6] A. Aji, F. Wang, H. Vo, R. Lee, Q. Liu, X. Zhang, and J. H. Saltz.
Hadoop-GIS: A High Performance Spatial Data Warehousing System
over MapReduce. Proceedings of the VLDB Endowment, PVLDB,
6(11):1009–1020, 2013.
[7] W. G. Aref and H. Samet. Efﬁcient Processing of Window Queries in
the Pyramid Data Structure. In Proceedings of the ACM Symposium on
Principles of Database Systems, PODS, pages 265–272, Nashville, TN,
Apr. 1990.
[8] J. Bao, Y. Zheng, and M. F. Mokbel. Location-based and Preferenceaware Recommendation Using Sparse Geo-social Networking Data.
In Proceedings of the 20th International Conference on Advances in
Geographic Information Systems, Proceedings of the ACM Symposium
on Advances in Geographic Information Systems, ACM GIS, 2012.
[9] L. Bouganim, B. Jónsson, and P. Bonnet. uFLIP: Understanding Flash IO
Patterns. In Proceedings of the International Conference on Innovative
Data Systems Research, CIDR, 2009.
[10] T. Brinkhoff, H.-P. Kriegel, and B. Seeger. Efﬁcient Processing of
Spatial Joins Using R-Trees. In Proceedings of the ACM International
Conference on Management of Data, SIGMOD, pages 237–246, 1993.
[11] R. Burke. Hybrid Recommender Systems: Survey and Experiments.
User Modeling and User-Adapted Interaction, 12(4):331–370, 1997.
[12] C. Chen, M. Burton, E. Greenberger, and J. Dmitrieva. Population
migration and the variation of dopamine D4 receptor (DRD4) allele frequencies around the globe. Evolution and Human Behavior, 20(5):309–
324, 1999.
[13] N. R. C. Committee on the Science of Climate Change. Climate Change
Science: An Analysis of Some Key Questions. The National Academies
Press, 2001.
[14] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed Data Processing on
Large Clusters. Communications of ACM, 51:107–113, 2008.
[15] S. Dhar and U. Varshney. Challenges and business models for mobile
location-based services and advertising. Communications of the ACM,
54(5):121–128, 2011.
[16] J. Do, D. Zhang, J. M. Patel, and D. J. DeWitt. Fast peak-to-peak
behavior with SSD buffer pool. In Proceedings of the IEEE International
Conference on Data Engineering, ICDE, 2013.
[17] Earth
Science
Data on
AWS With
NASA / NEX
Public
Data
Sets.
http://aws.typepad.com/aws/2013/11/
process-earth-science-data-on-aws-with-nasa-nex.html.
[18] A. Eldawy, Y. Li, M. F. Mokbel, and R. Janardan. CG Hadoop:
computational geometry in MapReduce. In Proceedings of the ACM
Symposium on Advances in Geographic Information Systems, ACM GIS,
2013.
[19] A. Eldawy and M. F. Mokbel. A demonstration of spatialhadoop: An
efﬁcient mapreduce framework for spatial data. Proceedings of the
VLDB Endowment, PVLDB, 6(12):1230–1233, 2013.
[20] C. Engle, A. Lupher, R. Xin, M. Zaharia, M. J. Franklin, S. Shenker,
and I. Stoica. Shark: fast data analysis using coarse-grained distributed
memory. In Proceedings of the ACM International Conference on
Management of Data, SIGMOD, pages 689–692, 2012.
[21] R. A. Finkel and J. L. Bentley. Quad trees: A data structure for retrieval
on composite keys. Acta Informatica, 4:1–9, 1974.
[22] G. H. Golub and C. Reinsch. Singular value decomposition and least
squares solutions. Numerische Mathematik, 14(5):403–420, 1970.
[23] L. I. Gómez, S. A. Gómez, and A. A. Vaisman. A generic data model and
query language for spatiotemporal OLAP cube analysis. In Proceedings

[24]
[25]
[26]
[27]
[28]
[29]
[30]
[31]
[32]
[33]
[34]
[35]
[36]
[37]
[38]

[39]

[40]

[41]
[42]
[43]
[44]
[45]
[46]
[47]
[48]
[49]
[50]
[51]

269

of the International Conference on Extending Database Technology,
EDBT, pages 300–311, 2012.
L. I. Gómez, S. Haesevoets, B. Kuijpers, and A. A. Vaisman. Spatial
aggregation: Data model and implementation. Information Systems,
34(6):551–576, 2009.
L. I. Gómez, B. Kuijpers, and A. A. Vaisman. A data model and query
language for spatio-temporal decision support. GeoInformatica Journal,
15(3):455–496, 2011.
J. Gray and B. Fitzgerald. Flash Disk Opportunity for Server Applications. ACM Queue, 2008.
J. Gray and G. Graefe. The ﬁve-minute rule ten years later, and other
computer storage rules of thumb. SIGMOD Record, 26(4):63–68, 1997.
A. Guttman. R-Trees: A Dynamic Index Structure for Spatial Searching.
In Proceedings of the ACM International Conference on Management
of Data, SIGMOD, pages 47–57, 1984.
Apache. Hadoop. http://hadoop.apache.org/.
P. Hall. Cities of Tomorrow: An Intellectual History of Urban Planning
and Design Since 1880. John Wiley & Sons, 2014.
J. M. Hellerstein, J. F. Naughton, and A. Pfeffer. Generalized search trees
for database systems. In Proceedings of the International Conference
on Very Large Data Bases, VLDB, 1995.
J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl. Evaluating
Collaborative Filtering Recommender Systems. ACM Transactions on
Information Systems, TOIS, 22(1):5–53, 2004.
E. G. Hoel and H. Samet. Data-Parallel R-Tree Algorithms. In
International Conference on Parallel Processing, ICPP, pages 47–50,
1993.
E. G. Hoel and H. Samet. Benchmarking spatial join operations with
spatial output. In VLDB, pages 606–618, 1995.
E. H. Jacox and H. Samet. Spatial join techniques. ACM Transactions
on Database Systems, TODS, 32(1):7, 2007.
I. Kamel and C. Faloutsos. Hilbert R-tree: An Improved R-tree using
Fractals. In Proceedings of the International Conference on Very Large
Data Bases, VLDB, pages 500–509, 1994.
N. Katayama and S. Satoh. The sr-tree: An index structure for highdimensional nearest neighbor queries. In Proceedings of the ACM
International Conference on Management of Data, SIGMOD, 1997.
G. Koutrika, B. Bercovitz, and H. Garcia-Molina. FlexRecs: Expressing
and Combining Flexible Recommendations. In Proceedings of the ACM
International Conference on Management of Data, SIGMOD, pages
745–758, Providence, RI, July 2009.
S. Lee, B. Moon, and C. Park. Advances in Flash Memory SSD
Technology for Enterprise Database Applications. In Proceedings of
the ACM International Conference on Management of Data, SIGMOD,
2009.
S.-W. Lee, B. Moon, C. Park, J.-M. Kim, and S.-W. Kim. A case for
Flash memory SSD in Enterprise Database Applications. In Proceedings of the ACM International Conference on Management of Data,
SIGMOD, 2008.
S. Leutenegger, M. Lopez, and J. Edgington. STR: A Simple and Efﬁcient Algorithm for R-Tree Packing. In Proceedings of the International
Conference on Data Engineering, ICDE, pages 497–506, 1997.
J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel. LARS:
A Location-Aware Recommender System. In Proceedings of the
International Conference on Data Engineering, ICDE, 2012.
Y. Li, B. He, Q. Luo, and K. Yi. Tree indexing on Flash Disks. In
Proceedings of the IEEE International Conference on Data Engineering,
ICDE, 2009.
Y. Li, B. He, R. J. Yang, Q. Luo, and K. Yi. Tree Indexing on Solid
State Drives. PVLDB, 3(1), 2010.
J. Lu and R. H. Guting. Parallel Secondo: Boosting Database Engines
with Hadoop. In International Conference on Parallel and Distributed
Systems, pages 738 –743, 2012.
Open Geospatial Consortium. http://www.opengeospatial.org/.
D. Papadias, J. Zhang, N. Mamoulis, and Y. Tao. Query processing
in spatial network databases. In Proceedings of the International
Conference on Very Large Data Bases, VLDB, pages 802–813, 2003.
J. M. Patel and D. J. DeWitt. Partition based spatial-merge join. In
ACM SIGMOD Record, volume 25, pages 259–270. ACM, 1996.
PostGIS. http://postgis.net.
F. Preparata and M. I. Shamos. Computational Geometry: An Introduction. Springer-Verlag, 1985.
D. Rotem. Spatial Join Indices. In Proceedings of the International
Conference on Data Engineering, ICDE, pages 500 –509, 1991.

[52] M. Sarwat, J. Avery, and M. F. Mokbel. RecDB in Action: Recommendation Made Easy in Relational Databases. PVLDB, 6(12):1242–1245,
2013.
[53] M. Sarwat, J. J. Levandoski, A. Eldawy, and M. F. Mokbel. LARS*:
An Efﬁcient and Scalable Location-Aware Recommender System. IEEE
Transactions on Knowledge and Data Engineering, TKDE, 26(6):1384–
1399, 2014.
[54] M. Sarwat, M. F. Mokbel, X. Zhou, and S. Nath. Fast: A generic framework for ﬂash-aware spatial trees. In Proceedings of the International
Symposium on Advances in Spatial and Temporal Databases, SSTD,
2011.
[55] M. Sarwat, M. F. Mokbel, X. Zhou, and S. Nath. Generic and
efﬁcient framework for search trees on ﬂash memory storage systems.
GeoInformatica, 17(3):417–448, 2013.
[56] T. K. Sellis, N. Roussopoulos, and C. Faloutsos. The R+-Tree: A
Dynamic Index for Multi-Dimensional Objects. In Proceedings of the
International Conference on Very Large Data Bases, VLDB, 1987.
[57] Spark. https://spark.apache.org.
[58] A. Thusoo, J. S. Sen, N. Jain, Z. Shao, P. Chakka, S. Anthony, H. Liu,
P. Wyckoff, and R. Murthy. Hive: A Warehousing Solution over a MapReduce Framework. Proceedings of the VLDB Endowment, PVLDB,
pages 1626–1629, 2009.
[59] P. L. Woodworth, M. Menéndez, and W. R. Gehrels. Evidence for
century-timescale acceleration in mean sea levels and for recent changes
in extreme sea levels. Surveys in geophysics, 32(4-5):603–618, 2011.
[60] R. S. Xin, J. Rosen, M. Zaharia, M. J. Franklin, S. Shenker, and I. Stoica.
Shark: SQL and rich analytics at scale. In Proceedings of the ACM
International Conference on Management of Data, SIGMOD, pages 13–
24, 2013.
[61] M. Ye, P. Yin, and W.-C. Lee. Location Recommendation for Locationbased Social Networks. In Proceedings of the ACM Symposium on
Advances in Geographic Information Systems, ACM GIS, pages 458–
461, San Jose, CA, Nov. 2010.
[62] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauly,
M. J. Franklin, S. Shenker, and I. Stoica. Resilient Distributed Datasets:
A Fault-Tolerant Abstraction for In-Memory Cluster Computing. In
Proceedings of the USENIX Symposium on Networked Systems Design
and Implementation, NSDI, pages 15–28, 2012.
[63] M. Zaharia, T. Das, H. Li, T. Hunter, S. Shenker, and I. Stoica.
Discretized streams: fault-tolerant streaming computation at scale. In
Proceedings of the ACM SIGOPS Symposium on Operating Systems
Principles, SOSP, pages 423–438, 2013.
[64] N. Zeng, R. E. Dickinson, and X. Zeng. Climatic Impact of Amazon
Deforestation?A Mechanistic Model Study. Journal of Climate, 9, 1996.
[65] C. Zhang, F. Li, and J. Jestes. Efﬁcient Parallel kNN Joins for Large
Data in MapReduce. In Proceedings of the International Conference on
Extending Database Technology, EDBT, pages 38–49, 2012.
[66] N. Zhang, J. Tatemura, J. M. Patel, and H. Hacigümüs. Re-evaluating
designs for multi-tenant OLTP workloads on SSD-based I/O subsystems.
In Proceedings of the ACM International Conference on Management
of Data, SIGMOD, 2014.
[67] V. W. Zheng, Y. Zheng, X. Xie, and Q. Yang. Collaborative Location
and Activity Recommendations with GPS History Data. In Proceedings
of the International Conference on World Wide Web, WWW, pages 1029–
1038, Raleigh, NC, Apr. 2010.

270

