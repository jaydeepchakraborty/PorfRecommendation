Rural Livelihoods and Income Enhancement
in the “New” Economy
Alakananda Rao

Partha Dasgupta

Anudip Foundation for Social Welfare
Kolkata, India

Arizona State University
Tempe, AZ, USA

alka.rao@alvarisys.com

partha@asu.edu

ABSTRACT
Technology alone, however innovative or even “cheap” is not
the solution to poverty, illiteracy, infrastructural inadequacies
and the myriads of ills of those below the poverty line in the
third world. The ability to utilize technological advances to meet
developmental goals is critical This paper brings an in-depth
case study that illustrates the success of a program for poverty
reduction that judiciously integrated some basic computing and
communication
facilities
with
education,
training,
entrepreneurship and lifelong goals to bring higher levels of
prosperity to villages of one of the poorest segments of the
world.

1. Introduction
Technological advances of the past century have completely
changed the way people live and work. Information Technology
and ubiquitous Internet have made it possible for people to cross
barriers of geography and work cohesively in different locations.
But all of this has impacted only what is known as the
“developed” world, and even in the developing economies, the
“cream of the crop”, the top 10% of the country’s population
who have the education levels and resources to utilize their
skills and technological prowess.
Technology has been much touted as one of the effective levers
that promise to bring more prosperity via information exchange
for the world’s impoverished. Can it serve the needs of those for
whom earning the bare minimum of a dollar a day is a daily
challenge, perhaps a billion people on this planet?
This paper presents a different and counter-intuitive perspective
on a successful deployment of a large-scale effort to enhance the
lives via income escalation, of a large segment of an
impoverished population. For them the end run is, not to
participate in social networks, watch Internet videos or blogs but
to enhance income. This is an example of how technology can
be used for social gains where there is a merger of various
streams like social sciences, business management and
information technology, while placing the needs of the
community at the center of interest.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
NDSR’10, June 15, 2010, San Francisco, CA, USA.
Copyright 2010 ISBN 978-1-4503-0193-0…$10.00.

The effort described in this paper was undertaken by an NGO
(or non-profit organization) “Anudip Foundation for Social
Welfare” headquartered in Kolkata, India, with funding mainly
from American India Foundation and other donors.

2. The Context Setting
The mention of India to-day conjures up the picture of an ITskilled country with large back-office support hubs in cities like
Bangalore, Delhi, and Hyderabad. Considered the home of
computer talent and the bastion of the outsourcing craze - it is
much hyped as a country on a major economic upswing - a
third-world country which has transformed population into a
hip-urban community.
The reality is quite different, as the census figures of 2001 [1]
show. Only about 13% of the billion residents of the country live
in large urban areas and even fewer enjoy western style
education, technology and amenities. The rest of them live in the
mid-tier cities as well as in the 600,000 villages, the real India,
with incomes barely above the “dollar a day” definition.
In terms of technology the national government focused on
increasing rural connectivity both through Internet and Cellular
bandwidth expansions. In India, cell users in urban areas are
taxed by the federal government to help build a corpus fund
called “Universal Services Obligation Fund” [12, 13] which is
then invested in infrastructure for cellular coverage in rural
areas. At one time this fund accumulated to almost $1 billion,
due to lack of viable investment opportunities
In parallel, another scheme under the E-governance initiative is
the collaboration of national and state governments and private
corporations resulted in the deployment of “Information Kiosks”
to each of the 600,000 villages in a Public-Private-Partnership
model, providing citizen services, at an estimated investment of
$400 million surely that should go a long way to mitigate the
poverty levels .
“Information technology is a key mechanism for addressing the
knowledge asymmetry between the governments and governed”
says Nandan Nilekani in his book “Imagining India”, [3] but can
that happen if citizens have neither the ability to read nor to
operate a computer? Only the operator driven information kiosk
can then perhaps fulfill this gap.
Note the computer usage model. An information kiosk is
essentially an Internet-connected desktop computer, shared by a
large number of people. This reality is significantly different
from the concept of “personal computer”, a single individual
with resources to own a computer. A computer here is a scarce
resource that is operated by a few that can grapple its linguistic
and technological intricacies often an intermediary, or an

cellular service (voice and data) is available in some zones
(data is unused).

operator who can translate the benefits of access to information
for the rural communities.
Not surprisingly, the cellular usage (based on voice
communication),
which
provide
human-to-human
communication abilities and keeps families and friends in touch
has grown exponentially, overtaking the revenues from urban
usage to those in the rural sector. But usage of cellular data for
internet usage has shown little rural penetration. And while this
is the case for India, most underdeveloped regions show similar
patterns. But are these statistics a reflection of the true impact on
the rural communities – be it economic or social?
How can we assess the real economic benefits?

3. The Specific Geography
The setting of this case study is one of the poorest sections of
the world called the “Sunderbans” (see Wikipedia for location)
part of the world's largest delta (80,000 sq km or 30,000 sq
miles –slightly smaller than the state of Indiana). Sunderbans is
a World Heritage site of UNESCO, which straddles the
countries of India and Bangladesh. This case study was limited
to the Indian part (due to political considerations) of the
Sunderbans which has a size of about 2,500 sq. miles (size of
Delaware) and home to about 5.4 million people making the
population density very high for a rural area.
The Sunderbans have no cities, just scattered villages and
islands where there are settlements. The area produces fish,
timber, agricultural produce, honey. There are hardly any
industries in the area and even the local harvest produce and
fishing is shipped to the urban areas for further processing. The
per capita income of the inhabitants is largely unknown; the
(untrustable) reported figures show 37% living below the
poverty line quoted at the figure of $1/day.
Many young men have not completed their education as they
cannot attend schools regularly due to long sailing trips on
fishing trawlers where they work as hired labor as sole breadwinners in their family. Older male members are most likely to
have lost their lives in these precarious means for earning a
livelihood. For those who are lucky to have the means to an
education, there are schools and even many government run
“colleges”. But the quality of education is poor and the medium
of instruction is only the local language. At the end of the
course, even the so-called college graduates find it hard to find
a suitable job, since they have no particular skills and no
exposure to computers, considered a basic tool at the workplace
of to-day.
In the developed world it is almost incredible that someone with
a college degree, does not have a basic familiarity with
computers, but those familiar with rural India, and perhaps other
developing economies will know this to be a reality leading to
the coinage of the term “digital divide”
Making logistics harder, this particular area is characterized by a
network of waterways (natural artifact of swamps) which makes
travel and communication infrastructure difficult and this has
been the reason for lack of industrial development in the area.
The area is also prone to devastating cyclones, like the Aila that
hit the region last year. The bottom line being:
•

Extreme poverty and lack of basic infrastructure.

•

Almost complete lack of civilization basics such as
electricity, telephones. But, due to government investments,

•

Travel is tedious, time consuming and dependent of
availability of local boats and suitability of weather.

4. Our Approach
Our aim was to deploy technology to benefit the people of this
region, but before embarking on any project it was decided to
commission an ethnographic study in 2005 to study the impact
and needs of the community and the viability of the projectmuch like the Requirements Analysis or Feasibility assessment
used for a Systems Project.
This information needs analysis was carried out by Reuters
Foundation/Stanford University in collaboration with a team
from Actionaid International (an International development
agency) [2]. The study attempted to understand the impact of the
problems on the local community, with relevance to the various
issues facing them – like food, health, education, governance,
livelihood, disaster preparedness and gender. A participatory
evaluation was also carried out to ascertain their priorities on
how Information and Communication Technologies could be
used to address these
The outcome of the study revealed some startling facts! The
major area of concern of the people was the livelihood
insecurity, not any of the other issues which we had assumed
would have a greater impact. The people we interviewed felt
that if we address their income enhancement need they would
find ways to address all other issues.
Their primary concern was the lack of access to opportunity in
the economic growth due to lack of skills and capacity among
the youth in this region. The rural youth, even with degrees did
not have access to jobs as these degrees were academic and did
not give them skills for the workplace.
This helped define the goals for the project later undertaken by
Anudip Foundation
•

Create “new” economy livelihoods for the people in rural
areas.

•

Reduce migration to urban centers and create economic
growth for rural areas.

•

Utilize the investments made by the government
organizations for wealth creation opportunities.

•

Explore the IT usage model of many people accessing
information through a single information access point (Info
Kiosk) via skilled intermediaries.

Our target is to create livelihoods for 100,000 unemployed
youth in rural India through our initiatives in the next 5 years.

4.1 “What if?”: An Approach to Failure
It is interesting to note that initially we had embarked on a
technology inspired plan. The focus was on Communication
Network with integrated Disaster Warning Systems – for the
fishermen.
Our approach was to start by laying the infrastructure of
communications to this swampy, disconnected land. The idea
was to use marine radios, overlaid with digital communication
hardware to create a mesh network. This network would be
gateways into the Internet at appropriate semi-urban access

points into the telephone-DSL system. Internet Kiosks and
shared computers would be placed in villages and possibly even
on deep-sea fishing boats to provide a whole slew of life saving
information access functions (weather and hurricane warnings,
human communications and so on).
The idea sounded innovative and attractive the mesh network
using radio communications would enable digital transmissions
where there are no communication wires. Internet and
communication access (VoIP for voice) would be enabled in
really backward lands.
Some startling facts emerged - the deployment of such a system
would have been a failure:
•

•

•

The cost of this system would be prohibitive and even
higher would be the cost of maintaining the system. There
is no local talent – people who know about wireless
communication and internet configurations would be
expensive to transport and house in these locations.
Government regulations controlling security of wireless
networks in this region which is a “border area” would be
hard to overcome, and hence the deep-sea fisherfolks
would not be able to access such a system.
At the end, there would be Internet access, but what use
would that be to people whose focus is attaining income?

4.2 The Education and Economic Approach
Technology is viewed as cheap, but that is from the perspective
on $100/day income earners. In the bottom end of third world
society is it expensive stuff. Even more expensive is the cost of
running, maintaining and repairing the infrastructures. On the
other hand, unskilled labor is dirt cheap. Even semi-skilled or
even skilled labor is very cheap.
The idea is to leverage the low cost human capital. Using a few
well educated people (not cheap) we can educate literate people
to become trainers. Then we use these trainers to instill basic
technical skills to people in order to turn the unskilled into
cheap but more skilled human capital. The process of course
consumes little money but enhances the income potential of the
$1/day strata of society into almost a $5/day productive people.
Of course, entrepreneurship breeds further employment
prospects that are then propagated economically.
Even this approach has its pitfalls. All the barriers or logistics,
lack of infrastructure, travel problems, legal barriers and so on,
have to be overcome. We were able to do that.
The training delivery cost is nominal as each training program is
delivered by 2-3 trainers/ hardware support personnel with a
monthly salary of about $300 (indeed, personnel costs are low
but this figure amounts to $10/day, ten-fold higher than $1/day).
The computers have been donated and the students (a batch of
20) contribute about $6 towards the running costs of the
program. Thus even with administrative overheads of travel to
rural locations and stay costs of $250 per person per program
and the cost of course preparation and other management costs;
the cost per program works out to $2,000. This averages to
about $100 per student, a far cry from the millions of dollars
invested in technology led so called innovative projects!
As we show later, we did achieve the goal of developing
enhancing income potential worked at various levels. In addition
to placing graduates in jobs in cities, our entrepreneurs are
running businesses in the hinterlands and making about $5/day.

4.3 Basic Skills Training
We made the obvious decision to focus on basic computer
education first. Unlike regular vocational training programs
however, this plan would focus on the total development of the
youth from the time of enrolment to the phase where they would
be in a position to have a sustainable livelihood. We undertook
creating core computer curriculum for youth with basic highschool education and delivering this in the rural areas.
The courses cover computer usage, computer hardware, basic
applications, internet access and browsing, email and searching,
software configuration and maintenance and such other skills.
The courses were first developed in English with textual
material, slides and exercises and assignments from industry. We
use CDMA data connections where available, judiciously – to
avoid increasing costs.
The students, local youth with elementary education can read
the English alphabet but have a poor understanding of the
language. Hence all the material was then translated to Bengali,
the local language, and this improved the understanding of the
conceptual background and aids faster learning.
Geared at dropouts with no employment, who have never seen a
computer before, the courses start with how to switch on a
Computer and take the students through the basic operations in
MS-Paint, MS-Office and so on.
The courses are trainer-led, though supported with slide
projection allowing an audio-visual approach to the training, and
ensuring that there is some standardization and quality of
delivery is consistent. These are delivered in an intensive handson manner which allows the trainees to attain a level of skill in
the period of 30 days. The first 15 days of lecture-cum-hands-on
session is guided by our trainers. These are followed by 15 days
of practice sessions where the trainers provide part-time
supervision, thus allowing the students to acquire knowledge,
perform independent troubleshooting and develop operational
skills - leading to a high level of confidence. A system of
evaluation is used to provide feedback to the students and along
with certificate of course completion,

4.4 Job Placement
Due to the lack of communication infrastructure in this region,
most of the time even those willing to work in the cities are
excluded from the information network and do not receive the
information on job openings in time for them to send the
applications. We populate a database of the trained students and
make their profiles available to prospective employers.
The database thus helps the employers to locate them quickly
and inexpensively. There is also a two-way communication, for
those students who are registered. The job openings and
matching is then handled via a “two-way” communication
system using text messages. This reduces the lead time for
information to reach applicants, allowing those in the far remote
locales to respond to opportunities where they would not have
been able to.

4.5 The Entrepreneurship Module
But not all the trainees are able to find employment at the end of
the Basic Skills training courses. Since local employment
opportunities are minimal, only those who are able to migrate to
the nearby urban areas can actually stay employed. For young

women, where society does not allow them to travel outside the
local geography, this forces them to be underemployed.

Their platter of services provide a low-cost option to the
community

Hence to develop economic progress in the rural areas, we
followed the basic module with advanced modules on
entrepreneurship development for setting up a rural microenterprise using the computer as basic tool. This training covers
the basics of starting a business, Accounting, Financial
Management (including where to get access to funds, how to
open a bank account), marketing and similar areas. This helps
the students to set up shop on their own and offers an income
generation option, preferred over traditional livelihood which is
agriculture based. Further skill training is also provided for the
type of business that the entrepreneur would like to do. These
courses have been developed with inputs from the needs for the
services in the rural geography, based mainly on the feedback
received from the rural community members themselves.

•

Digital photography and passport photo printing

•

Old photo touch up

•

Typing both English and Bengali

•

Printing of business card, brochures, school question
papers, publications for the local administrative authority
and so on.

Advanced training is provided in the following lines of business
which are popular in the rural areas: (1) Desk-top Publishing (2)
Digital Photography and Video editing (3) Cyber Café
Management and (4) Accounting.

5. Real Life Case Studies
Since Anudip’s establishment in 2006 it has grown to employ 41
trainers and graduate about 2500 students. The plan is to
increase the number of trainers and students significantly in the
coming years, subject to availability of funding.
The following are real life entrepreneurship examples that have
emerged. Many of our trainees have migrated to cities, but some
especially women are pioneering rural commerce. At the
moment the number of successful small-business operations is
limited, but we are anticipating growth as time progresses.

5.1 “Missed Call” Communication
One of the interesting features of these entrepreneurs is that
once empowered with the skills they are quick to find innovative
use of the voice technology of the cellphones to acquire and
retain rural customers at little or no cost to themselves, through
the customary use of “missed calls”. What is a missed call? In
India especially among the low income communities who have a
cellphone but do not wish to pay charges, a novel concept is the
use of a call given to another number where the caller hangs up
before the called party can answer. This indicates a predetermined message and sometimes even the message can vary
depending on the number of rings the caller allows. Much to the
bane of cellphone service providers a large number (about 40%)
of all calls in India are missed calls proving that given the
backbone, people are able to find innovative use of this without
any additional cost. But quick to understand the opportunity and
adapt the cellphone operators have moved to a business model
where the increased numbers of connections for cellphone users
bring in larger revenues.

5.2 Digital Graphics Studio
Five young women started this unit at the local market at
‘Raspunja village in May 2007 after completing IT Basic and
Advanced training in desk-top Publishing. They started with two
computers, one printer and one scanner and one digital camera
under an incubation financing model. Their initial struggle was
against the social barriers of unmarried young girls entering into
the uncertainties of a business -- but their confidence in their
own skill and capacity makes them a profitable venture to-day.

Design and printing of greeting cards, marriage invitation
cards, banners.
The local community has benefited from the services provided
as they would have no option other than to lose a work day and
pay for the travel to the city to obtain the same services. Now all
they have to do is drop in to the shop at the local market to take
a photograph and when it is ready a “missed call” tells them that
their order is ready for picking up.
•

5.3 “Power House” - Internet Ticketing
Power House is been set up as a Cyber Café a group of youths in
a in a marketplace at South Barasat, a remote village using one
CDMA connection. Most people in the region are of course not
aware of the types of services that Internet can offer. Quick to
understand the potential after the training course, the group has
set up the services to book railway tickets for the local villagers.
The service charges are about $1 to $2 for train ticket booking
and ticket printing. In this remote village, people would earlier
need to go to urban locations for this service. Many of the
villagers are not literate and this meant not only loss of work
days, transportation costs to the city but also left them open to
exploitation by unscrupulous touts in the urban areas (illiterate
people are easily swindled).
Now they simply walk into the local cybercafé or perhaps even
call the operators and request that bookings be made. A “missed
call” informs them the ticket is ready for pickup. The villager
pays in cash; and Power House uses a prepaid wallet system (ItZ
card) to pay online. This is a different model than an individual
user at a PC with a single credit card and making own travel
plans! Essentially this model works more like a hub and spoke
and the reach of a single computer and Internet connection can
be multiplied to cater to a whole village. The two owners now
make about $5/day each.

5.4 “Sadhana” Cyber Café
“Sadhana” is another entrepreneurship unit set up as a Cyber
Café, by a couple of our trainees, the first in the area. The
students of the nearby college have been quick to seize the
facilities, giving rise to further systemic spread of knowledge.
After preliminary coaching by the owners of this Café (for
which the college students pay a fee of $0.50 per hour, they
have started creating e-mail accounts and can also apply jobs
online, and enter for competitive examination for government
jobs. Job seekers, unemployed youth and students are now
constantly surfing the net (cost $0.40-$0.60 per hour),
The local elder community has also realized the benefits of an
email over the ubiquitous postcard especially when there are
family emergencies (death, sickness, financial support) and
bread winners located in urban conglomerates like Delhi or
Mumbai have to be informed quickly. Old lady “Moni”
communicates with her son, a chauffer in Delhi on email, and

insists on paying the $0.50 to posses her own email account,
rather than use the common one at Sadhana Computer.
Sadhana Computers has invested in just 2 computers and is
linked to the Internet through a CDMA data connection. The
capital outlay for Sadhana was $1,000, obtained as a loan that is
serviced though a revenue of $20 per day. The revenue minus
loan and operation costs, i.e. the profit, amounts to over $5/day
per owner, and is a five-fold increase over the $1/day income
that would be considered normal.

5.5 Friendly Neighborhood InfoCenter
The government offered to support, under the E-governance
funds [14], facilities such that entrepreneurs could set up centers
where local communities could access web-based government
services.
In the village of Urelchandipur, it was almost impossible to find
someone met the mandated eligibility criteria of basic
familiarity with computers to become an “Entrepreneur”, under
this program. One female graduate (named Sukanya) of our
program finally came forward to take this on.
The scheme helps her earn commissions from the Government
information services under the Right to Information act. She
also provides services like payment of electricity bills, Insurance
premiums and so on.
Since the utilization of such services is low, the commissions do
not provide enough income. Sukanya has means to supplement
her income by teaching some of the skills she had learnt to the
children in the local community. While many other Kioskoperators closed down due to insufficient funds and training,
Sukanya has found a novel way to leverage the government
infrastructure for the benefit of the local community as well as
earn a livelihood.

5.6 Observations
While the case studies are limited in numbers, we can see that
$5 a day is an easy target in rural grassroots technological
startups providing services to the community. We also now
believe that infrastructure development is not sustainable and
can be a waste without grassroots education with income
enhancement and entrepreneurship goals. These examples
illustrate the importance of “capacity-building” in any model of
inclusive growth that can help eradicate poverty. We started
making a dent in about 2007 and have just started to gather
momentum. We are optimistic about our integrated model and
are looking forward to enhancing economic growth in the
Sunderbans.

6. What worked
When technology is the hammer, every problem looks like a
nail. Technological progress is not the goal; the goal is making
life better. We focused on the nail and then built a hammer
around it -- a malleable one.
The ethnographic study identified the problem (or the nail) –
Livelihood.
Focusing on the problem, it was important to choose appropriate
use of technology rather than the technology itself, and we
developed a full-cycle approach that led the person to reach
from entry to a viable livelihood option.

The project took an inter-disciplinary approach where we
worked with development practitioners and the community to
arrive at a holistic solution, not just a technological one. The
steps we followed were more in line with a Systems
development project – though more on the lines of a traditional
SDLC approach.
We kept our costs low by leveraging on the community
participation. We made use of underutilized governmental
premises, we relied on word-of-mouth advertising and we used
low-cost cellular and data communications which were part of
the Universal Services Obligation Fund generated infrastructure.
Thus we could focus on our core function area and scale up
quickly.
Community participation was taken at each step, bringing the
needs into focus and involving them in the use of technology.
Pilot implementation was done and scaled up after incorporating
the learning. Continuous feedback and monitoring and
community participation was a key feature.

7. The Politics of Technology
United Nations set up the Millennium Development Goals
(MDG) to create a world free of poverty by 2015. Goal 8 calls
for a global partnership for technological development.

Summarizing:
In cooperation with the developing countries develop
decent and productive work for youth.

•

In cooperation with private sector, make available the
benefits of new technologies especially information and
communication technologies (ICTs)1.
The second bullet has led most governments in developing
countries towards what is now often termed universal access to
technology. This approach draws on the individualistic model of
Personal Computer and Internet access use in the developed
world; and clones the idea for underdeveloped regions. This a
model completely unsuitable for the rural areas where most of
the poor live. The result is high cost investments in computer
technology for the rural areas with little or no return on
investment.
•

Policy debates have raged where investments by governments in
technology infrastructure have been questioned by development
practitioners. Often it is said that these investments are futile
and a waste of meager resources which needs to be deployed in
food, health and education.
This is the philosophy which has led to the concept of the
"digital divide", where the rural and the economically weaker
sections of society, in the developing nations are not only
hampered by their lack of skills and resources, but further
marginalized due to their lack of competence in the use of
technology.
We were successful in deploying education in conjunction with
technology for income enhancement and the ability to leverage
(hitherto unused) government developed infrastructure to
enhance livelihood opportunities.

1

Extracted from United Nations Millennium Declaration at
www.un.org/milleniumgoals

8. Future Directions
While we have scaled up rapidly on this project over the past
four years, to achieve substantial impact of this project it is
important for us to be able to use technology and existing
available networks, where investments have already been made
to the maximum.
The trainer led model of deployment has been effective even in
this harsh geography, we now have plans to extend our model
and reach larger sections and our plans are to deploy the
distance learning and self-learning methodology. We are
exploring collaborations with researchers to use the mobile
handsets as teaching tools through games and through selflearning techniques, including interactive voice techniques. This
can help those who are on a computer to access the lessons on
the voice options of the cell and practice on simple standalone
desktop computer.
With the recent advent of 3G networks in India there is perhaps
a probability of using more of the functionality of the mobile
hand sets as a pre-learning device for first time users. We
envision skill development leading to the ability to handle work
outsources to rural centers from the overcrowded cities.
Also in terms of content, we plan to devise applications like
matrimonial services where portals can be developed
particularly suited to the rural context where we work.
Collecting and uploading relevant content could lead to a whole
new set of livelihood options for youth in the region.

[3] Nandan Nilekani, Imagining India, book published by
Penguin Press HC (March 19, 2009).
[4] “Invention-led Development”, Innovations, MIT Press
Journals - Winter 2010, Vol 5 no. 1.
[5] B. Bowonder, Vinay Gupta and Amit Singh, Developing a
Rural Market E-hub - The case study of e-Choupal
experience of ITC, Planning Commission Report, 2003.
[6] Sasi Kumar, Vijay Pratap Singh Aditya, Tapan S. Parikh,
Enabling Banking for Rural Communities, Special Issue on
Information and Communication Technology for Rural
Financial Services, CAB Calling April-June 2008, Vol 2.
[7] Carlos Miranda Levy, Social Impact from Technology blog
posted on 14 November 2005 at Digital Visions
Fellowship.
[8] Annual Administrative Report 2007-2008 - Office of the
District Magistrate & Collector , South 24 Parganas,
Kolkata – 700 027.
[9] Demography of Districts – S. 24 Parganas
[10] Building Blocks for a Rural Service Delivery Channel,
Project Development Document, Volume 1, October 2006,
Published by Infrastructure Leasing & Financial Services
Limited.
[11] Alok Kumar Sanjay and Vivek Gupta, Gyandoot: Trying to
Improve Government Services for Rural Citizens in India,
eTransparency Case Study No.11.

9. References

[12] Policy Document – Universal Services Obligation Fund.

[1] Government of India, Census of India 2001.

[13] Arunima Patel, Bhavya Sharma, Ritu Khandelia, Roshan
PF, Sandhya Chandrasekhar, Universal Service Obligation
- A Critique of the Consultation August 2001.

[2] Actionaid and Reuters Digital Vision, The Need For
Information and Communication Technology in the
Sunderbans: A Sunderbans Perspective: Views From the
Grassroots (report)
http://www.anudip.org/files/Study_Final_Draft-1.pdf

[14] Srei Sahaj E-Village Limited,
http://www.sahajcorporate.com/index.php

A Probe-Based Monitoring S c h e m e for an Object-Oriented,
Distributed Operating System

Partha Dasgupta t
Georgia Tech

1. Introduction
This paper presents an integration of three topics,
namely distributed systems, object-oriented design and
system monitoring. System monitoring Is a functionality
that enables a large system to keep track of the health of
its individual components, especially If It has multiple
instances of each type. This is particularly interesting in
distributed systems that implement fault tolerance. Given
the ability to detect falling, flaky or failed components
(software modules end hardware units) the system has the
ability to reconfigure the healthy units, on the fly, to work
around the faulty ones.
The basic mechanism we propose, for monitoring, Is
the usage of probes. Probes are a powerful tool In many
envlronments and has been proposed for deadlock detection, debugging, backup processing and so on. [ChMi82]
Although the usage of probe-based algorithms can be
generalized for many appllcatlons, the usage is very
dependent on the exact semantics of the probe Implementatlon and the environment in whlch they function.
In the following pages we explain the use of probes In
a particular distributed system framework. The system

under consideration is a prototype of a distributed operating system and programming environment we call Clouds.
Clouds is a object-oriented system building concept, that is
wholly structured on the object concept. This forms a solid
base layer for further research into the programming and
environment aspects of object-oriented design.
We first present a short summary of the design criteria, goals and architecture of the Clouds system. This
will illustrate the framework of fault tolerant distributed system that we have currently. We then present a design for
the probe system that will fit elegantly into the existing
design. We show how the probes (in this context) can be
effectively used for system monitoring, reconflguration and
fault tolerance. The probe system also has soma other
payoffs like the easy Implementations of interactive debugglng support.
2. T h e Clouds O p e r a t i n g S y s t e m
The Clouds operating system is a distributed, objectoriented operating system that supports objects, nested
actions, reconfiguration, fault tolerance, orphan and
deadlock detection and integration (location transparency).
[Mo81, AI83, Mc84a, DaLeSp85]

tAulh~'a Acklrm:
School of Info.mltlon w)d ComputerScience
Georg~ InsUluleof Technology
AUanIL GA 30332
ph: +I (404) 884 2572
Eie~ronk:Addrm.
CSNeI: imrlha@GlTech.OSNet
Arpa: imrlhe%GaTech.CSNel@ CSNeI-R#ay.ARPA

The basic building blocks in Clouds are objects,
actions end processes. Processes are carriers of the
thread of control, on behalf of actions. The actions are
atomic units of activity, consisting of a partial order of invocations of operations defined in objects (see sections 2.2
and 2.3).

1 ~ mamrmhu Ixm ouRxmdIni ~ by NASAun~ 0ramNAG-I~.

2.1. The Clouds A r c h i t e c t u r e

uucP: ...I{~¢k,ira~..d,hp~ba,ahn~)k~l~h¢alha

Permission to copy without fee all or pan or this material is granted provided
that the copies are not made or distributed for direct commercial advantalLe,
the ACM copyright notice and the title of the publication and its date appear.
and notice is given that copying is by permission of the Association for
Computing Machinery. To copy otherwise, or to republish, requires a fee and/
or specificpermiss/on.
P~ 1986 A C M

0-89791-204-7/86/~57 75¢

September1986

The architecture of a distributed system can be partitioned into two main areas: the hardware configuration and
the operating system structure. The operating system
structure is loosely described above, as the set of objects,
actions and processes. The hardware organization of
Clouds is a set of processors, loosely coupled over a
medium-to-high-speed network. The prototype configuration is shown in Fig. 1.

OOPSLA '86 Proceedings

57

I

I
I

I=,
I ..I-7"~--~I~
It---'I

I L2~._J

...........

~ C ~o M b ~ - -,

wcondaryconnecaon

......
".

(dual- ported dirks)

/I

ITI1Trl
Ethemet

WorkstaUon=.

Fig. 1 : The ~
The prototype consists of three VAX/750 computar8
with 3 Meg memory each, connected by an Ethernat. The
Ethernat serves both as a back-end network that links the
machines of the distributed system, se well as a front end
network that the user8 use to gain access to the system.
The front end network i8 the gateway to the Clouds 8y8tam, and the user access is through workstations (diskless
SUN's or IBM-PC/AT's). Since the users are not hard
wired to any site, in case of tallures, users can be vlrtually
transparently floated to another serviceable processor. The
desktop computers used by the user run a special interfacing 8oftware system whlch can communicate to Clouds
and can Intelligently lake part In system monitoring and can
decide to change hosts if necessary, in cooperation with
the surviving part of Clouds. [Mc84a]
The disk drives used for secondary permanent
storage are dual ported. This allows reassignment of disk
drives in case of processor failure and helps in rsconflguretion, and leads to high availability of data.
2.2.

Objects

The operating Rystem structure of Clouds is based on
the antion/objsct paradigm. All permanent eystem components in Clouds are objects. Objects form a clean conceptual encapsulation of data and programs. They are useful in providing synchronization and recovery as will be
described later. The objects are acceded by processes
on behalf of actions.

58

Prototype Configuration.
In a simplistic view, an object is an inetanos of an
abstract data type (cosmetically similar to modular in
Modula-2 or classes in Simuis). The object encapsulates
permanent clam and 8 set of routines that can aocese (reed
or update) the data. The only s e . ~ s path to the data contained In the object is through the routines (or operations)
defined in the object. To the external world, the object is
thus an entity providing a set of entry points. [Jo79]
Object instances in Clouds are permanent, that is they
exist after they are crested and are visible until they are
explicitly deleted (like files). Any update caused on the data
in an object by an operation Inv(x~tlon is ~
permane~.
Thus all programs, data and files in the Clouds system are
obJscts. For example, a file objRot is an object containing
the file data, along with the reed and write operation
defined on the data. Thus each object can be tailor rnada
to the application that requires the use of the object, and
also tailored to suit the 800se8 methods and other update
oriteris applicable to the data oontainad in the objsct.
Clouds obJeot are more powerful than Just 8 black
box containing Rome procedurse and static data. It also
oontalna a stack (temporary data), heap (permanent
dynamically allocated data), and powerful support for concurrency control and recovery.
Objects in Clouds are pasalve, that is they conaiet of
Just 8 memory image, without any proceeses/server8 aesociatad with them. This 8110w8 virtually unlimitfKI object
inetanoea with little overhead. Also invoking an objRot doer
not involve scheduling server prooesses. Neither is the
conourrcmoy limited by the degree of multi-threading faesible in the server.

OOPSLA '86 Proceedings

September1986

permanent
datasegment

volatile,
per- process
data segment

errey
points
code segment

Fig. 2: Clouds Object Structure.
When • pr _o,~___Invokes an operation on an object, is
~u'rles Its thread of execution Into the procedure defined In
the object. Concurrently running processes thus are able to
Invoke concurrent operations In an object. This may or may
not be allowable, depending Upon the nature of the data In
an object. The synchronization rules can be effectively built
Into the object (see section 2.4).

The action management system is a part of the
Clouds kernel that ensures the etomiclty of the nested
action scheme. The Implementation of the action management system 18an Integral part of the kernel and 18fast, efficient, and runs with very low overhead [Ks85]. Details
about the management scheme and action semantics are
omitted here.

After • process updates the object, the updates may
need to be rolled back if the transection fails, or rather, the
updates should be made after the transaction commits.
The recovery features that ensure proper •tomic updates
can •iso be built into the objects.

2 . 4 . S y n c h r o n i z a t i o n a n d R e c o v e r y i n Clouds

The implementation techniques for synchronization
recovery are briefly explained in esction 2.4. An outline
of • Clouds object 18shown in Fig. 2.
2.S. A c t i o n s

A user on the Clouds system can start up a transection. A user tranesctlon 18 a top-level action. A top level
action is an atomic unit of work, that either effectively terminltes and causes permanent updates to recoverable
objects, or does not leave a trace.
A top level action can spawn more actions or sub•cfiche. These ere nested lictlons. They can run concurrently
with other sul:~mtion8 and the top level action, or they can
be spawned esquentially. The subactlona inherit the locks,
and the views of the parent action, but execute IndependenW. The details of the nested action romantics ere omit°
tad here.

September1986

Concurrency control (synchronization) and action
• tomiclty are the responsibility of the objects touched by
an action. In Clouds the concurrency control and recovery
can be provided by default, by the Clouds ~
(autoeync and •uto-recovery) or can be tailored for specific
objects by the •pplication programmer, or can be omitted
altogether for certain objects.
When an object is defined as auto-sync, the object
compiler Includes default code In the object entry and exit
points to adhere to the 2-phase locking protocol. Each
operation Is classified •s reed or update operation depending upon the semantics of the operation. Invoking • reed
operation causes the invoking action to acquire a reed lock
on the object (if possible, or the process waits until the
lock Is obtained.) Slmllerly invoking • write operation
causes the acquisition of a write lock, or the upgrade of •
prHxleting reed lock to • write lock.
Locke ere not released when the operation returns or
terminates, but ere released by the commit phase, which is
also handled by the object as described below.

OOPSLA '86 Proceedings

59

When an action invokes an operation on a recoverable
object, the action management system makes a bookkeeping entry to that effect. Each object, by default has three
well known entry points, labeled pre-commlt, commit and
abort. When an action terminates, the action management
system invokes these entry points to inform each object
that participated in the computation, the result of the
action. (The two commit entry points are needed by the 2phase commit protocol, that is used to terminate successful actions). Activation of the commit or abort entry points
in an object releases all the locks.

causes an Implicit rollback, and the ~-tlon may not be able
to execute until the fault has been rectified. This degree of
fault tolerance can be Improved by the usage of better
techniques that allow the action to continue using alternate
paths of execution.

When a process invokes an object, the object is
mapped to virtual memory, and all updates occur In
memory. The abort operation simply frees the object, the
pre-commit and commit pair flushes all the updates to the
permanent storage. [McAI82, PiSp85, Sp84]

4. S y s t e m M o n i t o r i n g end Probes

2.8. Pelluro R o s l l l o n c e

Clouds also provides support for single faults, and
crashes. A fault generally causes all affected action to
abort, providing a guard against inconsistent executions
caused by faults. Failure or network partitions can give rise
to orphans that hold locks and cause loss of throughput.
Clouds supports a time driven orphan elimination scheme
that effectively hunts down and aborts all orphaned
processes within a short interval after the orphanation
[McHe86]. The orphan detection routine makes usa of a
clever timeout scheme that Is rarely affected by system
sluggishness. The same scheme with minor modification
doubles as a deadlock detector that breaks deadlocks.
The reconfiguration and monitoring system achieves
high system resource availability. Since all the users
interact with Clouds through an Ethernet, no user is hard
wired to a site. A user has a primary site and a backup site
associated with the user session. In case of a failure Of the
primary site, the backup site provides the user with system
response. Thus site failures do not cause users to be left
without access to the system. Since disks in the system
are dual ported, they are accessible from two sites. Only
one of the ports is active at any time. If the primary site
controlling the disk fails, the other site activates the second
port, and effectively all objects located at the faulty site get
transferred to the working site. This also enhances the
availability of the permanent data in the face of site failures.
The details of the algorithms that control the reconfiguration system are omitted for the sake of brevity.
3. E n h a n c i n g F a u l t T o l c r n n c s
The basic fault tolerance mechanism supported by

Clouds is the action paradigm implemented by the action
management system. The action paradigm ensures consistency of the computing environment In the face of
failures. It is a backvmrd recovery scheme. A felled actlon

60

The key to improved fault tolerance lies in the impk)mentatlon of a mechanism for the system to monitor itself.
The monitoring can be at several levels, discussed later,
but the basic components of the monitoring system m'e

probes.

A distributed system supporting fault tolerance needs
a system monitoring subsystem that effectively keeps tmok
of the status of all the hardware components and software
components (both active and passive). In this section we
present a description of a probe-based monitoring system
that can be coupled with the reconfiguratlon system to
enhance the failure resilience of the system.
4.1. I m p l e m e n t i n g P r o b e s In Clouds
Probes in Clouds are a form of emergency status
enquiries, that can be sent from a process to an object or
to another process. When a probe is sent to an object, the
probe causes the invocation of a probe-procedure deflrmd
by default In the object. The probe procedure returns to
the caller a status report of the object. This includes the
status of the synchronization mechanisms, the actions
currently executing in the object and other relevant information (Fig. 3).
Probes can also be sent to processes or actions. A
process does not have to explicitly receive a probe. The
probe causes a process thread of control to Jump to the
probe handler, Irrespective of the current status of the pro.
caes. (If the process is currently blocked, it executes the
probe handler and returns to the blocked condition.) This
IS somewhat similar to the way an interrupt causes the
CPU of a computer to cell an interrupt handler. Thus
probes to processes are conceptually similar to interrupfa
(or signals in the Unix operating system). The probe
handler sands a message back to the originator of the
probe, reporting the status condition of the process.
The probe handler in the process and the probe procadure in the object are pieces of code that are generated
by default by the object compiler or the process compiler,
or may be supplied by the programmer of the object or
process.
The probe handler/procedures are scheduled and
executed at higher priorities than the regular p r e c i s
scheduling prioritisa. Since the probes cause an immediate, asynchronous reply, and the probes are not
suspended by syr~hronizaUon mechanisms, the time I J a n

OOPSLA '86 Proceedings

September1986

Process

obiec=

status Info

~ndler

•

Probe

Probe En
Point
Operatic
Entry
Points

Causes an Interrupt
for the control flow

i

:Control
- ' Flow

..

Fig. 3: Probe Handlers in Objects and Processes
by the probe to return to the sender 18 not very dependant
on unpredlotabfa oondltiono, or heavy processing loads.
Thus timeoute can be quite effectively used for receiving
replies from probes. For example, if the system suspects a
faulty network connection from system A to system B, a
probe can be sent from 8ymm A to the network controller
of watem B. If the ¢ortnecUon is Indeed faulty, the probe
will not return. Syltmn A can effectively determine this condition by timing out, retrying and timing out again. Thus
using this scheme we can detect the difference between
a dead machine end • slow one, with • high degree of
a~urasy.
4.2. The P r l m n r y / B o c k u p P o r l d l g m
The primary-backup paradigm can be used to
enhance the fault tolerance of any crucial system process
or action. For instance e critical action that needs to be
executed to completion regardless of falluras in hardware
or communications can be handled as follows.
For each crucial action or process, associate a
backup action (or process). The back up action has the
lame capabilities as the primary, but is dormant most of
time. Periodically the backup process sends • probe to the
primary. The primary probe handler esnds beck an I am
OK status report. If the status report Is not received in •
certain amount of time the backup retries. If the retry fells,
this implies the primary is dead, and the backup takes over
as the primary and creates a new backup action.
The primary action also periodically checks on the
backup. If the backup has failed, then the primary creates a
new backup. For obvious reasons the primary and backup
Mtould be located at dMarent eltN. (Fig. 4.)

September1986

The drawback of this schema is that if network partitioning occurs, we may get two primaries and two backups, as both the primary and the backup decides that the
other process is deed. In Clouds however this produces
no undesirable effects. Since the objects the two primaries
have to access are the same, one (or both) of the actions
will fail because of the unacce8sability of some object due
to the partition. So only one (if any) will run to completion.
This scheme handles single stopping failures. That is
if any one process fails the system recovers. After the system recovers, another failure can be handled. But two
failures occurring faster than the recovery time cannot be
handled. The scheme can be modified to handle a multiple
number of failures, but the complexity and overhead
Increases by a large factor. In Clouds it was deemed very
unlikely that two failures will occur in such • short time and
not worth the extra expense [McKe84a]
Clouds failure tolerance does not consider corruption
or mass failure of storage media. Automatic recovery of
storage media can be achieved by mirrored media, stable
storage and as on. These methods are well understood,
generally expensive and can be integrated into Clouds. We
do not consider these techniques at this point.
4.3.

Syotem Hoolth Monitoring using Probes

The primary/backup strategy is just one of the applications of the probe system. This schenm only utilizes the
ability to esnd probes to processes. Since probes can be
ssnt to passive objects as well they can be used in more
situations. The more powerful application of probes is to
build a system monitoring subsystem. System monitoring
Involves keeping track of the state of each system com-

OOPSLA '86 Proceedings

61

e

............

Probe

× ,~

r-,-^.-^,--^_^_-.,-_^..-,'^^.-,-:--:^,.:
.....

Y

',

',,

Probe
I m OK . . ~ "

I

Failed

I
'

',',,

-~";";";":^:":"::~-':~.'?:-:'~I

Probe

\

I

~::.~.~-~. :'.~-'~

=1

timeoutIx;. . . . . . . . . . . . .

I
I

....

•~ ^

:~.C, . . . .

"

x^.IB
"2"

Backup.l:~¢omesPrimary
... ~ I e n e w ~ a c K u p
~

~.~.~~ . ~

Probe

I

.1-.--"

i

Failed
Backup

I

,
i

1

I

create new backup
Fig. 4. The Primary portent that has any role to play in the overall function of
the system. P m i v a components such es device intarfaces, device control objecle and so on, can be monitored
by special monitoring procaeses that invoke diagnostic
operations on these entities (or objects encapsulating the
device drivers).
The system monitoring subsystem consists of a procees (daemon) that runs at each site (monitor). The monitor has a list of components that it needs to keep track of.
The list is has • static part and • dynamic part. The static
part contains capabilities to various critical system comportents (network drivers, disk drivers, schedulers, action
management system and so on). The dynamic part oonsista of capabilities to user defined objects and ectione that
the user expressly records with the monitor, for tasks that
require a high degree of fault tolerance.
The monitor at one site him a lOglesl backup, that is •
monitor at another site. The various monitors act as
primariae for the site it runs On and doubles es a backup
for a remote monitor. This allows the distributed syMem to
detect site failures end network partitions. (Fig. 5.)
The monitor periodically probes all the components in
its list. The status of these components are stored in a fully
replicated database. This database has the same structure
and properties as the database used to locate Clouds
objects, i.e. it is highly available, but may not be conei~mt
at all sites, or may contain out of data data. The inconsistancy of the database does not cause major disruptions
in Imrvice. The data in the database are used by various
system esrvic~ end the reconfiguratlon system.
62

BackupParadigm.
Blmically two kinds of failure informaUon is evaJiable in
the database. The first type Is an envy that says 8 cornportent X at site A Is faulty or Inaccessible. This is • Io¢81
fault. The second type denotes that site A is unreschable
or dead. This Is 8 fault having global effect. (We are using
an Etharnet as a local area network. For point to point
¢ommunlcations, more data may be available, e.g. site A is
unreeeheble from site B, but accessible via site C.)
The database updated by the monitoring system ties It
to the Clouds reconflgurefion system. The reconfiguration
system and the interface to the monitor is discussed in the
next section.
g. The

Reeonflguratlon System

The reconfiguretion system is responsible for maintaining normal system operations In the event of a detectable failure of one or more system ¢omponents. Clouds
recognizes two forms of reconflguratione, namely upwarda
and downwards. Downward reconflguration occurs when
some component falls, and upward occurs when soma
new or repaired component is brought on line. We will
mainly discuss downward reconflguration hare, as failure is
our major concern.
The reconflguration system in Clouds handles downward reconfiguration by aborting all actions associated with
the failed component. It then terminates all orphans

OOPSLA '86 Proceedings

September1986

I

""'/Q

object

(weaklyconsistent)

B

I_. ..........

J

Rg 5: The structure of ,'he monitoring system.
associated with the failed actions. The users who were
affected by the failure are reassigned to functional sites and
site search tables are updated to reflect the nonavailability
of the malfuncUoning site. The failure of a site also makes
the objects stored at the site inaccessible. Access to these
objects are restored by switching disk ports (if possible).
We divide the task of downward reconfiguratlon into
two parts, namely short term reconflgurat/on sad long
term reconflgurat/on. Short term reconfigurstion is used in
the case of emergencies or large scale failures. This allows
the system to substitute criticst components end functionality and keep on running. Long term recovery uses monitorIng systems to detect failure of non critlcst components,
intermittent failures or bugs, anticipated failures and such,
and tries to isolate faults.
Short term reconflguration is handled as described
above. When a failure is detected, all actions associated
with this site and their orphans are aborted. If some of
these actions were being run as fault tolerant actions, their
backup counterparts would automatically lake over.
Aocfm to objects that may become unacceseible due to
the failure are restored, if possible, through alternate
nlee~.

September1986

Long term reconflguration is more rermed and adds
stability to the system. This incorporates oocasior~l
analysis of the distributed (and replicated) directory mainmined by the health monitoring system. Any faults or probable faults that may cause disruption are pinpointed and
evasive measures taken. The evasive measures include
changing the configuration of the system to avoid the
usage of suspected hardware end cresting alternate paths
or alternate services to-¢eplaca lost components.
Upward reconflguration is a feature of Clouds that will
be useful in most growing environments. This will allow
additions of newer hardware, sites and network connectiona transparently. Upward reconfiguration is handled in
two ways. If the new hardware is a device, then its drivers
are added es objects, and any application that needs them
can sccmm them right away. If a site is added, it simply has
to go on line. The search mechanisms will be able to find
objects at the new site by its broadcast search mechanism,
and the site will automatically enter the search hint files.
The whole process Is automatic and natural. No special
method Is needed for upward reconfiguration.

OOPSLA '86 Proceedings

63

~,~e~onflgurator

~

~

* transfer users
* transfer d i s k s
* reinitlalize
devices

3d
databases

Fcj.6: Interfacing the Monitoring and ReconfigurationSystems
8.1. Interfacing Monitoring and Rooonflgura.
tlon
The key to a successful reconflgumtlon system Is the
acquisition of the failure data. The monltoring system
forms a useful source of data for the reconflgureUon syetom.
The reconfiguration system is fully distributed, that Is
each site runs • reconfiguretion daemon. The reconfiguraUon daemon periodically checks the failure database. (This
can be somewhat improved. Details later). When a failure
Is detected the reconfiguration system works as follows:
If the failure is local, that is the reconfiguration process at site A finds that a component at site A is faulty, It
updates the name service databases to this effect. That Is
this component is marked as unavailable for the system.
The reconfiguretion system then attempts to get the fault
rectified. For example if the component is • device driver, It
may delete and reload it from the template and relnltialize It.
If the component ia • disk, it will attempt to find an alternate
path to the disk via • different site, end then trer~er all the
objects on the disk to the other site. Similar actions can be
taken for an unresponsive user workstation.
If the failure is global, e.g. site A is unreachable, it will
be detected at another site say B, where the backup was
located. At this point the reconfiguration system at site B
broadcasts update information to all sites so that no Invocation request is sent to site A. It then attempts to transfer
the disk connected to A to other sites having ports to A. All
users on A are then fioated off A to other functional sites.
The conceptual interfaces between the monitor, reconfiguration system, and the object lecator database is

64

shown on Fig. 6. The handling of global failures (or site
failures) Is depicted In Fig. 7.
The periodic scanning of the fault database by the
reconflguretion system may not get crash help soon
enough. Quick notiflcaUon of faults to the reconflguretlon
system by the monitoring system can be achieved by
probes. After a critical fault is detected by the monitor, and
added to the database, the monitor sends a probe to the
reconfigurator, which Immediately wakes up and does the
needed repair.
The reconfigurstlon system Is of course another fault
tolerant system and should be under the supervision of the
monitor. However failure of the recortfiguratlon system cannot be handled by the reconfigurstlon system and should
be handled by the monitor. However failed monitors can be
handled by the reconfigurator.
ll.2. L o n g T e r m R e e o n f l g u r a t l o n
Most of the techniques described above ere applicable for short term reconfiguretions. Long term reconfigumtlon techniques present an Interesting field of reesarch that
has not been adequately studied. The deelaiorm that the
long term reconflguring system has to take are based on
many factors such as Importance of the system components, likelihood of failure of questionable components,
the hardware system configuration and the availability and
usefulness of alternate methods.
For example a lot of system components give rise to
Intermittent failures. Some of these problems disappear on
their own, others work on retries, and then appear again.
Especially on device interfaces, these problems can be
often solved by reinitializlng the device, or the driver.

OOPSLA '86 Proceedings

September1986

site A

site B

site C

.

.

.

.

.

.

~--~T--"-- ~ "
...............

.........

...............

~f~ur~..

.............

'~

ok

Fig. 7: Reconfigurati¢)n on Site Failure
The monitoring system can be used to keep track of
the errors that oocur on these components and mairdai,,
statiellcW information on the frequency of occurrences and
severity. When errore des above "normal" levels, the
offending component can be rebootsd if possible, or
marked Inecce~ible. This can result in early warnings
and help In the prevention of lystem cruhea due to flaky
components.
6. D e b u g g i n g l a p p e d

As we have stated, the two basic software componun--ts of the system are procaine and objects.
Processes are active entitles, executing on behalf of one
tmlk (or action). An object is a passive entity, that supports
the execution of multiple tasks in its domain.
Several debugging techniques are under study, especlelly as debugging dletrlbuted systems ulm complex t ~ h nlques to handle timing and cortourrency aspecte. Probee
can be used to implement s simplistic yet highly effective
debugging tool.
We treat processes and objects separately. A special
debug probe sent to an object, causes the invocation of a
IntsraotM) debugging routine, that reaides in the object.

September1986

This routine allows the programmer to check the Insides of
an obJ~t even when it I$ In motion. This aUowl on the fly
Inepectlon of system objects without shutting the m/stem
down. Depending upon the object, repair on a running
engine may be allowable.
A debug probe on a process causes the process to
trap to a debug procedure. Similarly, the Insides of the
procees can be checked. However, in this case, the procees 18 not executing user code, and thus only asynchronous snapshots are poesible. Some wnchronlty can then
be achieved by setting breskpoints, but the possibilities ere
not as powerful as the debugging of objects.
7. C o n c l u e l o n o

Clouds is a fault tolerant distributed system. The fault
tolerance of Clouds is currently under implementation as
an action-be~d backward recovery wstem. This type of
fault tolerance leaves • lot to be desired. In this paper we
have shown an easily implementable subsystem that effectively monitors the status of the distributed wstem, and
actively helps the fault tolerant mechanisms to achieve

OOPSLA '86 Proceedings

65

forward progress. The paper uses the useful probe
mechanism to achieve this and proposes a 81mple implementation scheme for probes in the Clouds kernel, that can
monitor both passive and active syl~em components.
Lastly, we present aome techniques that uses probe based
mechanisms for debugging objects and procesese.

Dlstr ~u'~l Programs, ACM TOPLAS, VoL 5,
J,). 3, July 1983

[McA,.'..] McKendry M. 8. and AIIchln J. E. Object.Based
Synchronization and Recovery Technical
Report GIT-ICS-82/15, Georgia Ir,-tt. of Tech.
IMce4A]

8. Illeferencea
[AI83]

AIIchin, J. E., An Architecture for Reliable
Decentralized Systems, Ph.D. Thesis, School
of Information and Computer Science, Georgia
Institute of Technology, 1983 (818o released as
technical report GIT-iCS-83/23)

[AIBI83] Alines, G.T., Black, A.P., Lazowske, E.D., and
NOS, J.D., The Eden System: A Technical
Review, Technical Report 83-10-05, Departmerit of Computer Science, Washington University, Oct 1983.
[AIMc83] AIIchln, J. E., and M. S. McKandry, Synchronlzallon and Recovery of Actions, Proceedings
of the 2nd AGM SiGACT-SIGOPS Symposium
on Principles of Distributed Computing
(PODC), Montreal, August 1983
[ChMI82] Chandy, K. M. and MIshra, J. A. A Distributed
Algorithm for Detecting Resource Deadlocks
In Distributed Systems. Proceedings of the 2nd
ACM SIGACT/SIGOPS Symposium on Principles of Distributed Computing.

[McHe851 McKandry, M. S. and H~'lihy, M. Time-Driven
Orphan Detection. Techlcal Report, CarnegieMellon University, CMU-CS-85-138, July 1985.

[MoSl]

Jones,A. K., The Object Model: A Conceptual
Tool for Structuring Software, Operating Syatan~: An Advanced Course, Springer-Verlag,
NY, 1979, pp. 7-16

[Ke85]

Kanley,G. Design of an Action Management
System for a Distributed Operating System.
Masters' Thesis, Georgia Institute of Technology, November 1985.

[LlSc83]

Liskov, B., and R. Scheifler, Guardians and
Actions: Ungulstlc Support for Robust,

66

Mou, J. E. Nested Transactions: An
Approach to Reliable Computing. M.I.T.
Technical Report MIT/LCR/TR-260.

tPiSp85] Pitts, D. and Spafford E., Notes on a Storage
Manager for the Clouds Kernel. Georgia Institute of Technology Technical Report, GIT-ICS85/07.

[sp85!

Spector, A.Z., Butcher, J, Danniels, D.S.,
Duchamp, D.J., Eppinger, J.L., Fineman, C.E.,
Heddays, A., and Schwarz, P.M., Support for
Distributed Transaction In the TABS Prototype, IEEE Transa~on on Software Engineering Vol 11,6 (June 1985).

Isps6]

Spafford E. Kernel Structures for a Distributed
Operating System. Ph.D. Thesis, Georgia Institute of Tech., May 1988.

[DaLe85] Desgupta P., LeBianc R. J. mid Spafford E. The
Clouds Project: Design and Implementation of
a Fault Tolerant Distributed Operating Systefn.
Technical Report, GIT-ICS-85/29. Georgia
Tech, October 1985.
[Jo79]

McKandry, M.S., Clouds: A I-'.~ui,.Tolerant Distributed Operating System, Technical Report
GIT-ICS-84/, Georgia Institute of Technology,
1984.

OOPSLA'86 Proceedings

September1986

A Role-Based Trust Model for Peer-to-Peer Communities and Dynamic
Coalitions
Mujtaba Khambatti
Microsoft Corporation
mujtabak@microsoft.com

Partha Dasgupta
Arizona State University
partha@asu.edu

Abstract
Although P2P systems are usually used for information
exchange between peers, they have either protected
peers’ anonymity, or required transacting peers to trust
each other implicitly. Both these approaches are
vulnerable to attacks by malicious peers who could abuse
the P2P system to spread viruses, incorrect, or damaging
information.
In this paper, we propose an approach for trust
management in P2P systems. We introduce an optimistic
role-based model for trust amongst peers and show that it
is scalable, dynamic, revocable, secure and transitive.
Our proposed solution permits asymmetric trust
relationships that can be verified by any peer in the
system through a simple, low-cost algorithm. This paper
introduces a metric known as iComplex that combines a
peer’s trust value for each of its roles into a single,
relative, probabilistic guarantee of trust. Finally, we
discuss how our trust model allows peers to revoke
relationships with malicious peers, and the nonrepudiation of peer relations.
We use simulations to illustrate the trust value
distribution amongst peers in the network. Our analysis
and experiments demonstrates the low-cost involved to
verify and validate trust values. Lastly, we establish the
effectiveness of using sum as the aggregation function to
combine trust values of a peer.

Keywords: Communities, Dynamic Coalitions, Peer-toPeer, Trust.

1. Introduction
The emergence of decentralized and dynamic filesharing applications, such as Napster [1] and Gnutella [2],
provided the catalyst that drew a lot of attention to a new
breed of distributed systems called peer-to-peer (P2P)
systems. Current peer-to-peer systems are often targeted
for global information sharing, replicated file storage, and
searching by using an end-to-end overlay network.
Although these systems usually involve information
exchange between peers, they have either protected peers’

Kyung Dong Ryu
Arizona State University
kdryu@asu.edu

anonymity [3, 4], or required transacting peers to trust
each other implicitly [2].
Both these approaches are vulnerable to attacks by
malicious peers who could abuse the P2P system to spread
viruses, incorrect, or damaging information. Therefore in
order to enable practical information sharing in such
decentralized and dynamic systems, a viable trust model
needs to be incorporated that will allow peers to have
varying amounts of dynamically changeable trust amongst
each other. The main challenges that need to be addressed
are: how to describe if a peer is trustworthy, what low-cost
verification algorithm can be executed by a peer to
determine the trust value of some other peer, how are trust
values about peers exchanged within the system, how can
dishonest peers be punished.
In this paper, we propose an approach for trust
management in P2P systems. We introduce a role-based
model for trust amongst peers and show that it is scalable,
dynamic, revocable, secure and transitive. The trust model
assigns role-based trust values to peers proportional to
their status in the system. The status of a peer depends on
its relationships with other peers. Our proposed solution
permits asymmetric trust relationships that can be verified
by any peer in the system through a simple, low-cost
algorithm. Since trust values are proportional to the status
of a peer, it is essential to ensure that relationships
between any two peers will be legally binding and have
non-repudiation; that is peers cannot falsely deny their
relationship with another peer. However it is equally
essential that peers have the ability to revoke their
relationships with malicious peers to punish them for false
or damaging information. Finally, this paper introduces a
metric that combines a peer’s trust value for each of its
roles. The combined trust value is a single, relative,
probabilistic guarantee that offers peers with a simple,
verifiable trust metric about other peers in the P2P system.
Traditionally, many of these functionalities were
implemented through exhaustive policy lists that needed
to be created at system design time or through a complex
predetermined role framework, such as role-based access
control (RBAC) [5]. In contrast, our trust management
system is dynamic and requires minimal global
knowledge. Further, the decentralized nature of our
algorithms makes it suitable for P2P systems.

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

1.1. Dynamic Coalitions
The research described in this paper is part of a larger
project known as Dynamic Coalitions [6]. Dynamic
Coalitions enables a set of partners to work together while
sharing information, resources, and capabilities in a
controlled and accountable fashion. The partners
themselves are organizations composed of people,
departments, computational entities, and agents who
perform tasks consistent with the internal rules of their
organization.
Coalitions are supported by several innovative
techniques such as transitive delegation, cryptographic file
systems, capacity sandboxing, reverse sandboxing, and
fine-grained access control. These techniques facilitate
scalable authentication and revocable authorization of
agent computations even when they span resources of
different organizations. In addition, they improve overall
efficiency by permitting migration of computations to, and
a caching of services in, partly trusted environments of
another organization.

communities are implicitly formed (made up of peers with
the same or similar interests). Note that communities are
formed implicitly, i.e. they are self organizing. If a peer in
New York declares an interest in wombats, and a peer in
China also declares the same interest, then the two peers
become part of an implicit, undiscovered community. A
peer may belong to many different communities and
communities may overlap.

1.3. Canonical Application
The canonical application that we consider for our
algorithms is a digital library built out of a collection of
peers in which each peer owns a set of books that it is
willing to share with other peers (assume these are noncopyrighted works). The subjects of the books owned by a
peer form its set of interests. Peers are implicitly grouped
into communities based on the common interests they
share. Because a peer could own books from a variety of
subjects, we can imagine that a peer could be a member of
multiple communities.

1.2. Communities of Peers

1.4. Limitations of Our Approach

In an electronically connected world, people use
network-addressable1 computing elements (such as a
desktop personal computer, a laptop computer, a personal
digital assistant, and so on), which we call peers. Peers
have comparable roles and responsibilities and are used
by their owners to communicate information, share or
consume services and resources with other peers whom
they know. Every peer belongs to at least one predetermined group corresponding to the department or
organization of its human user. For home users, the
domain name of an Internet connection is used to identify
the pre-determined group of the peer. Thus the basic
construct of P2P systems can be used to implement a
practical Dynamic Coalition environment where coalitions
are created between peers in different groups.
In our research, we investigate a generalization of the
notion of peer group to a multiplicity of groups (possibly
overlapping) called peer communities. While a group is a
physical collection of objects, a community is a set of
active members, who are involved in sharing,
communicating and promoting a common interest.
Our concept of peer communities is loosely based on
the idea of “interest groups”, such as Yahoo Groups [7],
Usenet Newsgroups, or web communities. The user of a
peer in the system claims to have some interests and
depending upon the claims of all the peers’ users,

In terms of limitations, the techniques that we
developed can only be applied to specific situations, such
as the digital library application, where the set of interests
is constrained, well defined and understood by almost all
the peer members. Our proposed algorithms would place
individual users into peer communities based on the
common interests that they share with other peers. A
generalized peer-to-peer system, where the set of interests
includes the universe of all possible interests, might not
contain a single peer that shares common interests with
other peers. Therefore no communities would form.
Additionally, in real life, links between peers are not
always bidirectional. We make this assumption to simplify
the process of network formation.
Some of the other problems that are outside the scope
of this dissertation are: specifying how static IP addresses
can be used as peer identities; providing more than just
probabilistic guarantees of trust, therefore the trust
paradigm is vulnerable to peers that lie infrequently and
with due measure; describing the channel or protocol of
communication used by peers; defining the format for the
interests of a peer, obtaining the interests from a peer; and
fragmentation of the network after targeted denial-ofservice attacks on certain peers.

1

We assume that each peer has a static IP address that serves as the
peer identity. While this assumption is not universally true, it can be
facilitated through various techniques (dynamic DNS, IPv6, or firewall
penetrating mechanisms) that are outside the scope of this paper.

2. Terms and Definitions
We argue that peer communities provide an implicit
and natural organization of peers in structures that can be
efficiently uncovered or cultivated. In our previous work,

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

we provided a motivation for the study of peer-to-peer
communities and illustrated some scenarios to define and
discover peer communities [8]. Using simulated models of
communities, we have gained an insight to the architecture
of randomly created communities. Our algorithm for the
discovery of communities allows for the computation of
Link Weights, a very important value that enables the
working of all our subsequent algorithms. Link weights
help determine the membership of a peer in a community.
They are also used to rank peers in a community for the
purposes of information dissemination and trust. For a
detailed understanding of our algorithms for community
formation, discovery, information dissemination and
search, we refer readers to [8, 9, 10].
In this section, we briefly explain some of the terms
from our earlier work. First we describe what we mean by
interests. We then provide a definition of P2P
communities, followed by a thorough treatment of the
subject of peer links. Finally at the end of this section we
define Link Weights.

P
C
I

Figure 1. Venn diagram of interest attribute
sets for a peer. I is the universe of all
attributes; P is the set of personal
attributes; and C is the set of claimed
attributes.

2.1. Interest Attributes
In our model, interests are represented by attributes,
which are used to determine the peer communities in
which a particular peer would participate. There are of
course privacy and security concerns in using such
information, so we divide interests into two classes –
personal and claimed.
The full set of attributes for a peer is called personal
attributes. However, for privacy and/or security reasons,
all these attributes may not be used to determine
community membership. A peer may also not want to
reveal some of her personal attributes because she might
not consider them relevant amongst the peers that she
knows. Hence, a peer explicitly makes only a subset of
these attributes public, which are called claimed attributes
(see Fig. 1).

2.2. P2P Communities
Below, we formally define a peer-to-peer community
based on the attributes of each peer.
PEER-TO-PEER COMMUNITY: The non-empty set N of
nodes is a peer-to-peer community iff N has a non-null
signature.
SIGNATURE: Let i be a node and Ci be a set that
contains attributes claimed by i. Consider a non-empty set
N of nodes. Then the set resulting from the intersection of
Ck, for all k ∈ N is called a signature of the set N.
With this definition, given any collection of peers, we
would be able to tell whether the collection is a peer-topeer community or not.

2.3. Peer Links
We observe in projects like HITS [11] and Web
Communities [12] the concept of self-organized
communities that form implicitly based on hypertext links
between web pages. The human creators of the web page
explicitly place these links typically in order to point
towards web pages with similar content. This is one of the
factors that Internet search engines have exploited to
enhance their search operation.
We draw an analogy from the above research to
understand the behavior of peer-to-peer systems. We find
that peers also regularly link to other peers, in the form of
relationships (being present in their address book), or
direct connections (being on the same network), when
their human owners share something in common. We
assume that these links are bi-directional communication
channels that can be established on an as-needed basis. In
a social network, this is similar to getting in touch with
people you know when you need something. We refer to
these end-to-end overlay communication channels as peer
links.
2.3.1. The Need for Peer Links. Links are not necessary
to form and manage peer-to-peer communities. However,
they are needed to to feasibly run low-cost algorithms
such as community formation and discovery. We therefore
introduce the notion of a set of neighbors, which are
directly (1-hop) linked peers. The neighbors of a peer help
when a peer needs to communicate with other peers that
are not directly linked to it.
When node X is born, it needs to have one or more
logical neighbors. If it has three neighbors, A, B, and C,
then we say that it has three links, XÆA, XÆB, and XÆC.
Unlike overlay networks used by Distributed Hash Table
(DHT) based peer-to-peer systems, our link based

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

1016

V

Interest
Magazine
Fiction
Technical
Biography

Link Weight
4
18
6
23

1014
Number of Peers

V

1012
1010
108
106
104
102

Figure 2. (left) Peer V compares its claimed
interest attributes with all 1-hop and 2-hop
neighbors. (right) As a result of the
comparison, V calculates Link Weights for
each claimed interest attribute.
network is not contingent to node names, but to user
selected neighbors. Like in social networks, the more links
a node acquires the more successful it will be in receiving
information and searching the peer-to-peer network. It is
the responsibility of each peer to acquire as many
neighbors as possible.
First, let us explain a case where links are essential.
Suppose a node, belonging to domain abc.com claims the
attribute “baseball”. This node is essentially isolated,
unless it a priori knows about the other members of the
baseball community or the other members of the abc.com
community. There is a need for a “seed” to start the
community formation and information search needs.
2.3.2. Creating Peer Links. Flooding and querying a
central server are two solutions to the isolation problem
described above; however, the first is expensive and the
second violates the self-configuring tenet of the peer-topeer structure.
A new node X has the following options that solve the
isolation problem:
1. Connect to a special bootstrapping node present
within each network domain
2. Connect to a peer known to X that it knows – a
friend / colleague.
For a novice/new node, the first option may be the
most appropriate. As X ages, it finds other nodes and adds
these links to improve search speed and information
access. The linkages are bi-directional and similar to
friendships in real life, or to http links in the Web. They
are directed by humans.

2.4. Link Weights
Peer links are used to compute Link Weights at each
peer in the network. As mentioned earlier, this value is
very important to help determine the membership of a
peer in a community and rank peers in a community for
the purposes of information dissemination and trust.

1
1

2

3

4

Hop Level of Neighbors

Figure 3. The graph above was plotted
using a log Y-axis. It shows how the number
of peers that can be reached increases with
greater depth. The experiment involved a
10,000 node P2P network. (See Appendix for
technique used to form P2P networks)

Below, we provide a definition for Link Weights followed
by an illustrative example in Fig. 2. In subsequent sections
we explain how this value is used in our algorithms.
LINK WEIGHTS: This is the weight calculated for each
claimed attribute of a peer V based on the number of links
from V that can reach, after at most one indirection, other
peers that claim the same attribute.
The constraint of at most one indirection is necessary
to restrict the maximum depth up to which peers will be
examined since more than two levels deep resulted in an
unacceptably high number of communication messages.
See Fig. 3 for the average number of peers that are
reachable from a peer.

3. P2P Community Trust Model
We propose an optimistic trust model that provides
probabilistic guarantees based on the status / popularity of
the peers. Peers have the ability to revoke their
relationships with malicious peers and thus cause the trust
values of wrong-doers to be reduced. The probabilistic
guarantee provides a web-of-trust style estimate based on
a peer’s past transactions. The accuracy of the guarantee
depends on the thoroughness of the peer in discovering
and validating the trust values of other peers. Therefore,
non-critical transactions need not consume the resources
of the P2P system.
In this section we describe our model for trust using
P2P Communities. We explain how trust can be assigned
and discovered. The following sections discuss how trust
can be revoked, and protected against non-repudiation.

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

3.1. Peer Roles and Involvement
We previously pointed out that P2P communities are
implicitly formed, self-organizing structures that depend
on the declared (claimed) interests of peers. As a result,
peers may belong to more than one, possibly overlapping
community. In the case of a constrained application, such
as a digital library, community structures will span across
departmental or organizational boundaries. For instance, if
the digital library were implemented by government
departments to share documents and resources, a
conceivable community might include peers from both,
the Department of Commerce (Maritime Administration)
and the Department of Environmental Resources that are
concurrently interested in pollution in US ocean waterways. This is an example of a cross-departmental
community. Peers might also be part of intra-departmental
communities, such as the community of Maritime
Administration, or the community of Transportation
within the Department of Commerce.
The different communities within which a peer can
participate due to its claimed interest attributes constitute
the roles of the peer. Every peer will have at least one role
corresponding to its pre-determined group. Link Weights,
by definition, indicate the number of peers known directly
(1-hop neighbors), or indirectly (2-hop neighbors) to a
peer within each of its roles (communities). Below we
provide a definition for involvement, which, like Link
Weights, is associated with each role Ȍ of a peer V and is
proportional to the number of peers within the
neighborhood (1-hop and 2-hop neighbors) of V that are
also part of Ȍ. We call peers with high values of
involvement, seers (See [10] for definition).
INVOLVEMENT: The average of link weights for
elements of the intersection set Ci ŀ S is directly
proportional to the involvement of node i which has the
claimed attribute set Ci in a peer-to-peer community with
signature S.
If peer V from Fig. 2 is a member of community of
Science Fiction enthusiasts that has a community signature
of {“Technical”, “Fiction”}, then the involvement value of
V in this community will be directly proportional 12,
which is the average of the individual Link Weights for
the claimed attributes, “Technical” and “Fiction”.
For the purposes of simplicity, the examples discussed
in this paper consider P2P communities each formed due
to single shared interest attributes. This means that the
signature S of every community will be a single attribute
set. Therefore, the intersection set Ci ŀ S can only contain
one claimed attribute which has an associated Link
Weight that is also the Involvement value of the peer in
the community S. Nevertheless, our definition for
Involvement provides a way to extract values in more

complex scenarios where communities of peers share
more than just a single interest attribute in common.

3.2. Trust, Links and Link Weights
3.2.1. First Attempt: Trust and Links. We initially
associated trust values with peer links due to the following
reasons: (1) Peers create and maintain links to other peers
whom they know and therefore trust (optimistically); (2)
Since links are bi-directional, information provided by
peers that have more links might be more trustworthy. The
association
of
trustworthiness
of
information
(authoritativeness) with links is used by Google in its
PageRank metric [13]. The PageRank of a web page
measures the authoritativeness of its content; (3) Peer
links offer a simple, natural trust model that can easily be
revoked. If after some transaction, a peer loses trust in its
neighbor, it can break (remove) that link, thereby reducing
the number of links at its neighbor.
There are strong arguments that can be made against
the use of popularity as a measure of trust, quality or
reliability. However, there are specific applications in
which this mapping would not be unusual. Examples of
analogous systems with a similar association between trust
and links include: citation graphs in scientific
publications, where experts who are well-known and
highly regarded by most other authors tend to be highly
connected nodes [11, 14, 15]; and eBay points, where the
rank of users is proportional to the number of transactions
(purchase / sale) that they have completed with other eBay
users.
Despite its wide-spread use, the association of trust
with peer links does not provide an elegant solution to
trust management. Often, peers that are highly linked-to
(hubs) make mistakes, provide incorrect information, or
assist in spreading damaging information unintentionally.
[16] argues that viruses or damaging information from
hubs can epidemically spread and persist within a scalefree network, such as P2P network.
We believe that by making a slight modification, links
can provide practical and accurate trust guarantees in
decentralized systems. The most important detail that has
been lacking in previous trust models is the consideration
that peers participate in many different communities
(roles). Therefore, in the citation graph, although an
author of papers in Biology is highly cited, it is
conceivable that the author’s explanations of Electrical
Engineering concepts are incorrect. Likewise, on eBay, a
popular antique seller is not necessarily a trusted expert on
electronic equipment.
3.2.2. Second Attempt: Trust and Links Weights. It is
necessary to consider the roles of a peer when deriving its
trust value. We thus propose the use of Link Weights as

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

an indication of role-based trust. With reference to Fig. 2,
peer V knows more peers within the community of peers
interested in “Biography”, than it knows within its other
communities. As a result, information provided by V and
classified as “Biography” is more likely to be accurate
than information provided by V and classified as
“Magazine”.
Let us imagine another peer named W that has a Link
Weight of 10 associated with the interest “Biography.”
Using our model, V would have a trust value of 10 for W,
but W would have a higher trust value of 23 for V. The
association of Link Weights with trust values allows for

Trust Value

1000

100

10

1
0

200

400

600

800

1000

Peer ID

(a) Trust Values of all peers (see median band)

Trust Value

1000

100

10

1
1

10

100

Figure 4. Trust Value Distribution when
trust is associated with Link Weights. (1000
peers)

Trust Value

1000
100
10
1
100

3.3. Trust Value Distribution
We plot the distribution of trust values in a simulated
P2P network containing 1000 nodes. The graph in Fig.
4(b) shows a non-scale-free distribution of trust values,
i.e. a large number of peers had trust values around 100.
On the contrary, Fig. 5 shows a power law decay,
indicating that almost all peers have low trust values
except for a small group of peers that have exceptionally
high trust values This is important because it highlights
the dissimilarity in the distribution of trust values when it
was obtained from links (Fig. 5) and our model (Fig. 4),
where trust values are obtained from Link Weights.
In order to correctly model the P2P network, we
ensured that the peer link topology was scale-free (See
Appendix for technique used to form P2P networks).
Fig. 4(a) illustrates that the majority of peers start out
with a median trust value (around the center of a range of
values), while a small group of peers have either higher or
lower trust values. In the network considered, the average
trust value was 100, maximum was 628, minimum was 7,
and mode and median were 93. This distribution of trust
values is most suitable for an optimistic trust model such
as ours because a peer can enter into transactions with
other peers whose trust values are median and most likely
comparable to its own. Malicious peers will find their trust
values dropping unlike in a scale-free distribution where
trust values usually cannot be lowered because most peers
start out with low trust values. For critical transactions,
information can be sought from peers with higher trust
values.

3.3. Verification and Validation of Trust Values

(b) Frequency of peers with different trust values

10

trust

1000

Number of Peers

1

asymmetric trust relationships that imitates
relationships amongst humans in a social network.

1000

Number of Peers

Figure 5. Trust Value Distribution when
trust is associated with links. (1000 peers)

In [8] we proposed an Attribute Escalation algorithm
that uncovered implicit communities and enabled the
formation of new communities. The algorithm is an
autonomous procedure that is asynchronously executed by
each peer. To execute the algorithm, peer V sends all
peers within its neighborhood (through message
forwarding by its neighbors) a list of its claimed attributes.
In turn, V receives the lists of claimed attributes from its
neighborhood peers. Through this simple exchange, we
demonstrated how communities were formed. The
calculation of Link Weights corresponding to each
claimed attribute follows the algorithm.
We propose a simple modification to the Attribute
Escalation algorithm that will allow trust values of a peer
to be guaranteed. Instead of simply sending out the list of
claimed attributes, each peer V will construct the

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

following message M and send it individually to each of
its neighborhood peers {V1, V2, … , Vn}.

{

}

M = IPdest , IPsource , CAsource , E source (M )
where,
M is the constructed message,
IPdest is the destination identity (a neighborhood peer),
IPsource is the sender identity (i.e. V),

CAsource is the claimed attributes list of the sender,

100%
95%
90%
85%
80%
75%
70%
65%
60%

20% false

80
%
90
%
10
0%

70
%

60
%

50
%

40
%

30
%

30% false

%(m /N )

Figure 6. The graph illustrates the
relationship
between
percentage
of
messages chosen to validate (%m/N) and
probability of uncovering false messages
(ȡ')
%False Messages
Uncovered

N − k N − k −1 N − k − 2
N −k −m
×
×
×Λ ×
N
N −1
N −2
N −m
So the probability that W will discover a false message

ρ=

10% false

20
%

10
%

ȡ'

E source (M ) is the M’s signature by the sender’s private
key.
Every peer is responsible for storing messages received
from its neighborhood peers in a publicly accessible
blackboard [10]. Blackboards are like websites and the
content on a peer’s blackboard can be viewed by any peer
within the system.

verify this value by visiting V’s blackboard and recalculating the Link Weight from the posted messages.
This calculation is a simple counting operation with a
complexity of O(n) (See [8] for detailed algorithm). Prior
to verifying the Link Weight however, W might chose to
validate the signatures of the messages posted on V’s
blackboard in an attempt to uncover fabricated messages
that were used to artificially increase V’s Link Weight
value. We call these fabricated messages false messages.
It might seem intuitive that before entering into a
critical transaction with peer V, an exhaustive process
needs to be employed where every one of V’s messages
has its signature validated. However, we show that
contrary to intuition, peers need only validate a small
percentage of messages to uncover one or more false
messages (if they exist) with a high degree of probability.
We begin by providing a brief theoretical analysis and
back it with results obtained from experiments.
Let N be the number of messages on peer V’s
blackboard. Assume k messages are false. Therefore, N-k
messages are not false. Also assume that peer W randomly
selects m messages to validate. Now the probability that W
will not discover any false messages is given by:

100%
80%

N=100

60%

N=700

40%
20%
0%
0%

20%

40%

60%

80%

100%

%False Messages

Figure 7. Plot of percentage false messages
uncovered (Y-axis) as percentage of false
messages increased (X-axis). %(m/N) = 10%

Let us return to the example in Fig. 2. When V claims a
Link Weight (and therefore trust value) of 23 for
“Biography,” any peer W in the P2P system will be able to

is:

ρ ′ = 1− ρ
Fig. 6 plots the relation between percentage messages
verified and ȡ'. The curves vary for k =10% of N, k =20%
of N, and k =30% of N. N was chosen to be 100. An
increasing percentage of m/N forms the X-axis, while ȡ'
forms the Y-axis. The graph shows that selecting just 1020% of the messages to validate will uncover false
messages with probability of 70-95%. If a peer fabricates
more messages, then the validation of messages will
quickly uncover false messages.
The theoretical analysis indicates that peers will
uncover false messages even when a small, randomly
selected set of messages is validated. We also observed
this behavior in our experiments. Fig. 7 presents two
cases: N = 100 messages, and N = 700 messages, when the
percentage of messages randomly selected for validation
(%m/N) was set at only 10%. The graph shows that even
when only 10% of the messages are false, 10% of those
false messages were uncovered by a peer that randomly
selected 10% of the original number of messages to
validate.

4. Using the Trust Model in Dynamic Coalitions
Dynamic Coalitions are temporarily formed between
peers belonging to different communities that each
represents a separate organization / department. The trust
model we proposed can be used to provide probabilistic
trust guarantees to each peer in the coalition.

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

bool CommonCommunities(int);
int[] ListTrusts(int);
bool AskNeighbors(int);
int[] Lower(int[]);
bool Ask2HopNeighbors(int);
bool VerifiedTrusts(int[]);
int[] FindTrust(int PeerID)
begin
int[] list_Trusts; // trust values
if (CommonCommunities(PeerID))
begin-if
list_Trusts=ListTrusts(PeerID);
else-if (AskNeighbors(PeerID))
list_Trusts=ListTrusts(PeerID);
list_Trusts=Lower(list_Trusts);
else-if (Ask2HopNeighbors(PeerID))
list_Trusts=listTrusts(PeerID);
list_Trusts=Lower(list_Trusts);
list_Trusts=Lower(list_Trusts);
else-if
WARNING “No trust values!”;
return NULL;
end-if
if (VerifiedTrusts(list_Trusts))
begin-if
return list_Trusts;
else
WARNING “Found False Messages”;
return list_Trusts;
end-if
end-proc

Figure 8. Pseudo-code for finding trust values of
a peer in a coalition
Fig. 8 lists the algorithm that we use to obtain trust
values of a peer in a Dynamic Coalition. Since peers can
belong to more than one community, the FindTrust
method finds all the trust values (Link Weights) of a peer
V. The method can be invoked by any peer W (not
necessarily part of the Coalition). Initially, the
CommonCommunities method checks V’s claimed
attributes (posted as messages on its blackboard) for any
common attributes between W and V, indicating possibly
shared communities. This is a linear search operation with
complexity of O(n). If there are no common attributes, W
asks all its immediate neighbors if any of them share
communities with V. In the worst case scenario (neighbors
need to execute CommonCommunities), the operation has
O(n2) complexity. The best case scenario (neighbors
already know common communities from past
transactions) is an O(n) operation. As a final attempt, if
still no common attributes exist, W asks its 2-hop
neighbors the same question (worst case: O(n2), best case:
O(n)) before giving up trying to find trust values for V.

Remember that after the attribute escalation algorithm, a
peer knows the identities of all its 2-hop neighbors and
therefore does not have to find their identities at this stage.
In order to reduce bandwidth utilization and processing
time, a peer might decide to forego finding trust values
from its 2-hop neighbors. Our experiments revealed that
2-hop neighbors need to be consulted 32% of the time
when 10% of randomly selected peers invoked FindTrust.
For each attribute found in common with V, the
corresponding Link Weight is stored in list_Trusts. Link
Weights provided by 1-hop neighbors will be multiplied
by 0.5 and values provided by 2-hop neighbors get
multiplied by 0.25. All trust values are validated using the
process described in the earlier section.
The list of trust values provides a peer in a coalition
with a probabilistic trust guarantee about another peer.
Tampered values do not go undetected (due to verification
and validation), making these values secure. Additionally,
trust values can be transitively obtained from other peers
and scaled down depending on the peer providing the
values.
As an alternative to our current scaling down process,
trust values transitively obtained from another peer could
be multiplied by the trust value of that peer. However, we
have not yet explained how a peer can have a single trust
value. In the next section, we will present our idea for a
collective trust value of a peer that can be used for a more
realistic scaling process amongst other benefits.

4.1 Aggregating Trust Values into an iComplex
The list of trust values associated with each peer can be
used to provide probabilistic guarantees to other peers.
However in a practical implementation of P2P
communities, a single shared interest attribute will not
always be the signature of a community. At the end of
section 3.1 we assumed that every community signature
contained only a single attribute. Let us see what happens
when this simplification assumption were temporarily
removed.
Firstly this means that |S| could be greater than one.
Imagine a digital library with several communities of
peers. Suppose there exists a community (of sciencefiction enthusiasts)2 with signature S1 = {“Fiction”,
“Technical”}, and another community (of fiction
enthusiasts) with signature S2 = {“Fiction”}. Finally
assume that Peer V (from Fig. 2) is a member of both
these overlapping communities. Based on the Link Weight
values from Fig. 2 and the definition of Involvement
(Section 3.1), V is more involved in the community of
fiction enthusiasts than in the former community. If there

2

Assigning community names can be done through various consensus
and election protocols that are outside the scope of this paper.

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

exists another peer W with Link Weight values,
“Fiction”=18 and “Technical”=18 (therefore W’s
Involvement in community S1 is 18), then information
provided by W and classified as “science-fiction” is more
likely to be accurate than information provided by V
having the same classification.
Involvement values, which are associated with each

Seer Roles

20
15
10
5
0
0

2000

4000

6000

8000

10000

iComplex

Community Memberships
Held

(a) iComplex Vs. Number of communities in
which peer is a seer
20
15
10
5
0
0

2000

4000

6000

8000

10000

iComplex

Avg. Community Size

(b) iComplex Vs. Community memberships held
660
640
620
600

involvement values corresponding to a peer into an
iComplex. An iComplex value is calculated by each peer
individually and stored on their own blackboards. Since
iComplex values are, in essence, aggregations of trust
values, the verification and validation process described
earlier will still apply. As a system design, all peers need
to agree upon the aggregation function to calculate their
iComplex values. Examples include but are not restricted
to: sum of all trust values, or average of all relative trust
values. A relative trust value rt1 = t1 divided by the
approximate size of community S1={C1}, where t1 is a
trust value for attribute C1, and the approximate size of the
community is obtained through Distributed Discovery [9,
10].
In Fig. 8 we show the behavior when sum is the
aggregate function used to compute iComplex values. The
first graph illustrates that higher iComplex values implies
that the peers are seers (highly-involved) in more
communities and therefore these seers have higher trust
values than other peers in each of their roles. The next
graph shows the relationship between iComplex and
number of memberships held by a peer. This graph is
significant because it demonstrates the effectiveness of an
iComplex value aggregated using a simple sum function.
The graph shows that peers cannot increase their
iComplex values by simply joining many communities. In
fact peers that are members of many communities are
most likely to have low iComplex values (notice the
clustering of points close to the Y-axis). The final graph
shows that peers cannot obtain high iComplex values by
joining very large communities. The peers with the
highest iComplex values were members of average sized
communities.
Therefore, iComplex values calculated by adding all
trust values of a peer can provide a reliable, collective
trust value. Moreover, peers will not be able to
synthetically increase their iComplex value by simply
joining more communities or joining larger communities.
A higher iComplex usually indicates that the peer is a seer
of many communities and therefore trusted by the peers of
those communities.

4.2. Using iComplex for Information Assurance

580
0

2000

4000

6000

8000

10000

iComplex

(c) iComplex Vs. Average Community Size

Figure 8. The above graphs show the
behavior of iComplex when calculated as a
sum of all trust values.
community within which a peer is a member, play an
important role in determining accurate role-based trust
values of a peer. We now propose aggregating all

The community-based organization of peers enables a
more efficient searching mechanism [10] that works by
targeting one or more communities, irrespective of the
current membership of the searching peer. Any peer that
needs to search the P2P network constructs a three-part
search query containing: (1) the identity of the peer
creating the query, (2) the actual query for an item, and
(3) a list of meta-information that describe the item.
In an interest-based P2P network, such as our digital
library, a peer might use interest attributes as metainformation to a query. For instance, if the query is for

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

“books about Vampires,” the list of meta-information
might include attributes such as, “Twentieth century,”
“Bram Stoker,” and “European authors.” Responses to a
query are received asynchronously by the searching peer.
A peer will respond to a query if it either owns the
requested books or can provide information about the peer
that owns the requested books.
The response to a query contains: (1) the identity of the
responding peer, (2) the query for which the response is
being sent, and (3) a list of Link Weight values
corresponding to as many meta-information attributes that
match the claimed attributes of the responding peer.
An incremental change to the format of the responses
can be made by requiring the responding peer to send its
iComplex value as well. This provides the querying peer
with information on the probabilistic trust values of the
responding peers. The iComplex values received will
allow a querying peer to rank responses based on the
probabilistic trust values of the responding peers.

4.3. Attacks and Threat Assessment
Without the iComplex value, malicious peers could
misinform a querying peer about the peer that owns a
particular book / resource. A misinformed querying peer
will then obtain incorrect / damaging data from the peer
identified by malicious peers. In a business-world
implementation of digital libraries, malicious peers might
dishonestly divert traffic away from certain other peers.
Since iComplex values are ultimately calculated from
Link Weights which are dependent on the number of peers
in one’s neighborhood that share a certain attribute, one
way of fraudulently increasing the iComplex value would
be to create dummy neighbors with real peer identities and
interests. This is a difficult problem to solve. There have
been a few attempts to solve this by: using reputationbased systems or making it difficult to create a new peer
identity [17] (by computationally expensive key
generation, or associating it with a government issued
identity number, such as social security number, voter
identification number, and so on).
Finally, because our trust model provides probabilistic
guarantees, a peer with a high iComplex value can still
provide (with low probability of doing so) incorrect /
damaging information as a result of a query. We therefore
propose a revocation mechanism (described in the next
section) as a means to punish wrong-doers.

5. Revocation and Non-Repudiation of Trust
In this section we discuss how our trust model allows
peers to revoke relationships with malicious peers, and the
non-repudiation of peer relations. Malicious peers are not
only peers that provide incorrect/damaging information,

but also are peers that use unfair methods to lower the
trust values of their neighbors.

5.1. Revocation
We propose a distributed revocation mechanism, where
each peer maintains its own revocation list. Therefore a
disgruntled peer W that has been affected by previous
transactions with a malicious peer V can simply maintain
this information in a revocation list posted on its
blackboard.
The validation procedure described earlier involves
validating the signatures of the messages posted on a peer
V’s blackboard in an attempt to uncover false messages. In
order to allow for revocation of these messages, we
propose an additional action that peers entering into a
transaction with V can execute after the validation
procedure. We call this action Revocation Check. The
action entails: (1) randomly selecting a few validated
messages from V’s blackboard; (2) determining the peers
that authored those messages; and (3) visiting the
blackboards of the message authors to check for possible
revocations. If the Revocation Check finds that the
message authors have placed V in their revocation lists,
then those messages are called revoked messages.
Section 3.3 explains the relationship between the
number of messages validated and the number of false
messages uncovered. We therefore know that if 10% of
the messages of a peer V were selected for Revocation
Check and 10% of V’s neighbors had placed V in their
revocation lists, then the Revocation Check will find that
10% of the selected messages are revoked messages.
As a result, if V has maintained a good record over a
large number of transactions, except for a few
incorrect/damaging transactions, then its trust value will
remain high. Also, a malicious neighbor of V would not be
able to independently bring down the V’s trust value.
Finally, the Revocation Check procedure can ascertain
if a malicious neighbor W of V has unfairly revoked its
relationship with V. This means that W continues to
account V’s signed messages to calculate its trust values
and iComplex value even after placing V in its revocation
list.

5.2. Non-Repudiation
We have shown how peers can revoke their
relationships with malicious peers to punish them for false
or damaging information. However, since trust values are
derived from peer links, it is essential to ensure that peers
cannot falsely deny their relationship with another peer.
A malicious neighbor W of V cannot lie about the fact
that it is a neighbor of V. This is because the signed
message (section 3.3) addressed to V and created by W

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

will be publicly accessible from V’s blackboard.
Therefore trust values calculated as a result of peer links
have non-repudiation.

6. Related Work
A considerable amount of research has focused on the
analysis of link structures in collections of objects.
Through these analyses, researchers had hoped to discover
a process that could be implemented to effectively identify
and discover specific patterns in the collection. Early
attempts to analyze the collective properties of interacting
agents have been found in social networks [18], where
link structures like cliques, centroids and diameters were
studied. The field of citation analysis [19] and
bibliometrics [20] seek to identify patterns in collections
of literature documents by using citation links.
Patterns in several complex systems have been found
to be self-organizing [21], often because of the
autonomous creation of links by participating nodes (with
some influence of a partial system view). Previously, such
types of systems had been described by Erdös and Rényi
[22] who modeled complex systems as random networks
and studied their properties. Watts and Strogatz [23] later
studied the properties of large regularly connected graphs
of nodes that contain a few random long-distance edges
between nodes. They modeled this structure and
demonstrated that the path-length between any two nodes
of the graph is in fact surprisingly small. As a result, they
called these semi-random structures small-world
networks. More recently, Strogatz [24] and Amaral et al.
[25] observed that many networks demonstrated
topological properties that were different from the
predictions made by random network theory. Specifically
in these networks, called "scale-free networks", the degree
distribution of participating nodes was found to decay as a
power law.
Perhaps one of the earliest formalizations of trust in
computing systems was done by Marsh [26]. He attempted
to integrate the various facets of trust from the disciplines
of economics, psychology, philosophy and sociology.
Rahman and Hailes [27] proposed a trust model based on
the work done by Marsh but specifically for online virtual
communities where every agent maintained a large data
structure representing a version of global knowledge
about the entire network. Gil and Ratnakar [28] describe a
feedback mechanism that assigns credibility and reliability
values to sources based on averages of feedback received
from individual users.
More along the lines of trust and social networks,
Golbeck, Hendler and Parsia [29] presented an approach
to integrate social network analysis and the semantic web.
Yu and Singh [30] introduced a referral graph comprising
agents as weighted nodes and referrals as weighted edges

between participating agents. The graph topology can be
changed over time, for instance after bad experiences
agents can change their list of neighbors and also
propagate information about the "bad" agent within the
network. Yolum and Singh [31] propose a similar
approach that enables the study of the emergence of
authorities in self-organizing referral networks. Pujol et al.
[32] associate reputation of an agent with its degree in a
social network graph. Similar to PageRank in Google, an
agent gets a high reputation if it is pointed to by other
agents that also have high reputation. Aberer and
Despotovic [33] analyze earlier transactions of agents and
derive from that the reputation of an agent. Reputation
provides a value that indicates the probability that the
agent will cheat. They also presented a design for trust
management using their proprietary decentralized storage
method.
Our work introduces a novel role-based trust model
and discusses its use within dynamic coalitions of peers.
We associate trust values with Link Weights instead of
links and finally propose an aggregation of different trust
values of a peer into a single probabilistic trust value. Our
algorithms are completely decentralized and the trust
values are secure and can be thoroughly validated and
verified without a high communication overhead.

7. Conclusion
Without a viable trust model, information sharing in
P2P systems will be susceptible to the spreading of
viruses, and incorrect or damaging information. In this
paper, we consider a P2P system containing selforganizing, overlapping, interest-based communities that
can be uncovered using decentralized techniques. We
relate peer communities to Dynamic Coalitions where
coalitions are created between peers in different
communities. The communities within which a peer can
participate due to its claimed interests constitute the roles
of the peer. Every peer will have at least one role
corresponding to its pre-determined group. Each claimed
interest of a peer is associated with a Link Weight that
indicates the number of neighboring peers (1-hop or 2hop) sharing the same interest. The computation of Link
Weights has been described in an earlier work and
involves simple message exchanges amongst peers in a
process known as Attribute Escalation. In this paper, we
discussed how these messages can be protected against
tampering and counterfeiting.
Our proposed trust model is optimistic in that the
majority of peers start with a median trust value, while a
small group of peers have either higher or lower trust
values. We proposed the use of Link Weights as an
indication of role-based trust and provided the definition
of Involvement to extract values in complex scenarios

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

where communities of peers shared more than just a single
interest in common. Trust values are a probabilistic
guarantee similar to web-of-trust style estimates and is
based on a peer’s past transactions. We proposed a simple
modification to the Attribute Escalation algorithm that
allowed trust values of a peer to be verified and validated
by any other peer in the network. The theoretical analysis
of the validation process indicated that selecting just 1020% of messages to validate uncovered false messages
with probability of 70-95%. If a peer fabricated more
messages, then the validation of messages quickly
uncovered false messages. Similar results were also
obtained experimentally.
The trust model we proposed can also provide
probabilistic trust guarantees to each peer in the coalition.
We provided an algorithm to obtain trust values of a peer
in a Dynamic Coalition transitively. Involvement values,
which are associated with each community within which a
peer is a member, play an important role in determining
accurate role-based trust values of a peer. We proposed
aggregating all involvement values corresponding to a
peer into an iComplex. An iComplex value is calculated
by each peer individually and can be guaranteed by using
the verification and validation process. We demonstrated
experimentally that iComplex values calculated by adding
all trust values of a peer can provide a reliable, collective
trust value. We discussed the use of iComplex values in
information assurance and also delved into the subject of
attacks and known threats of our trust model.
Finally we explained how our trust model allows peers
to revoke relationships with malicious peers, and the nonrepudiation of peer relations. Malicious peers are not only
peers that provide incorrect/damaging information, but
also are peers that use unfair methods to lower the trust
values of their neighbors.

8. References
[1] Napster. http://www.napster.com
[2] Gnutella. http://www.gnutelliums.com
[3] I. Clark, O. Sandberg, B. Wiley, and T. Hong,
“Freenet: A distributed anonymous information
storage and retrieval system”, Proc. of the Work. on
Design Issues in Anonymity and Unobservability,
Berkeley, CA, 2000.
[4] R. Dingledine, M. Freedman, and D. Molnar, “The
freehaven project: Distributed anonymous storage
service”, Proc. of the Work. on Design Issues in
Anonymity and Unobservability, 2000.
[5] D.F. Ferraiolo and D.R. Kuhn, “Role Based Access
Control”, 15th National Computer Security
Conference, 1992.
[6] P. Dasgupta, V. Karamcheti, and Z. Kedem,
“E cient and secure information sharing in

Ĝ

distributed, collaborative environments”, Proc. of 3rd
Intl. Work. on Communication-based Systems, April
2000.
[7] Yahoo Groups. http://groups.yahoo.com
[8] M. Khambatti, K. Ryu, P. Dasgupta, “Efficient
Discovery of Implicitly formed Peer-to-Peer
Communities”, Int’l Jour. of Parallel and Distributed
Systems and Networks, vol. 5, no 4. 2002, pp. 155164.
[9] M.S. Khambatti, K.D. Ryu and P. Dasgupta, “PushPull Gossiping for Information Sharing in Peer-toPeer Communities”, Int’l Conf. on Parallel and
Distributed Processing Techniques and Applications,
Las Vegas, NV, June 2003.
[10] M. Khambatti, K. Ryu and P. Dasgupta, “Structuring
Peer-to-Peer
Networks
using
Interest-Based
Communities”, Int’l Work. On Databases,
Information Systems and Peer-to-Peer Computing,
Berlin, Germany, September 2003.
[11] J. Kleinberg, “Authoritative sources in a hyper-linked
environment”, Proc. of the 9th Annual AVM-SIAM
Symp. on Discrete Algorithms, 1998.
[12] G.W. Flake, S. Lawrence, and C.L. Giles, “Efficient
Identification of Web Communities,” Proc. 6th Int’l
Conf. Knowledge Discovery and Data Mining, ACM
Press, New York, 2000.
[13] S. Brin and L. Page, “The anatomy of a large-scale
hypertextual Web search engine”, Computer
Networks and ISDN Systems, 30(1–7), 1998, pp.
107–117.
[14] L. Adamic and E. Adar, "Friends and neighbors on
the web", (unpublished), 2000.
[15] L. Page, S. Brin, R. Motwani and T. Winograd, “The
PageRank citation ranking: Bringing order to the
Web”, Stanford Digital Libraries Working Paper,
1998.
[16] R. Pastor-Satorras and A. Vespignani, “Epidemic
Spreading in Scale-Free Networks”, Phy. Rev.
Letters, vol. 86, no. 14, April 2001, pp. 3200-3203.
[17] T. Murphy VII and A.K. Manjhi, “Anonymous
Identity and Trust for Peer-to-Peer Networks”,
(unpublished), 2002.
[18] J. Scott, Social network analysis: a handbook, SAGE
Publications, 1991.
[19] E. Garfield, Citation Indexing: Its Theory and
Application in Science, Wiley, New York, 1979.
[20] H.D. White and K.W. McCain, “Bibliometrics”, Ann.
Rev. Information Science and Technology, Elsevier,
1989, pp. 119-186.
[21] R. Axelrod and M.D. Cohen, “Harnessing
Complexity: Organizational Implications of a
Scientific Frontier”, Basic Books, New York, NY,
2000.

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

[22] P. Erdös and A. Rényi, “On the Strentgh of
Connectedness of a Random Graph”, Acta Math.
Acad. Sci., Hungary, 12, 1961, pp. 261-267.
[23] D. Watts and S. Strogatz, “Collective Dynamics of
“Small-World” Networks”, Canadian Jour. Math,
vol. 8, no. 3, 1956, pp. 399-404.
[24] S.H. Strogatz. “Exploring complex networks,”
Nature, London, 410, 268, 2001.
[25] L.A.N. Amaral, A. Scala, M. Barthélémy, and H.E.
Stanley, “Classes of small-world networks,” Proc.
Natl. Acad. Sci. U.S.A. 97, 2000, pp. 11149-11152.
[26] S.Marsh, “Formalising Trust as a Computational
Concept”, Ph.D. Thesis, University of Stirling, 1994.
[27] A. Abdul-Rahman and S. Hailes, “Supporting Trust in
Virtual Communities”, Proc. of the 33rd Hawaii Int’l
Conf. on System Sciences, 2000.
[28] Y. Gil and V. Ratnakar, “Trusting Information
Sources One Citizen at a Time”, Proc. of the 1st Int’l
Semantic Web Conf., Sardinia, Italy, June 2002.
[29] J. Golbeck, B. Parsia, and J. Hendler, “Trust
networks on the semantic web”, Proc. of Cooperative
Intelligent Agents 2003, Helsinki, Finland, August
2003.
[30] B. Yu and M.P. Singh, “A Social Mechanism of
Reputation Management in Electronic Communities”,
Proc. of the 4th Int’l Work. on Cooperative
Information Agents, M. Klusch, L. Kerschberg (Eds.),
Lecture Notes in Computer Science, vol. 1860,
Springer, 2000.
[31] P. Yolum and M.P. Singh, “Emergent properties of
referral systems”, Proc. of the 2nd Int’l Joint Conf.
on Autonomous Agents and MultiAgent Systems,
ACM Press, July 2003.
[32] J.M. Pujol, R. Sangüesa, and J. Delgado, “Extracting
reputation in multi agent systems by means of social
network topology”, Proc. of the 1st Int’l Joint Conf.
on Autonomous Agents and Multiagent Systems, pp.
467–474, ACM Press, July 2002.
[33] K. Aberer and Z. Despotovic, “Managing trust in a
peer-2-peer information system”, Proc. of the 10th
Int’l Conf. on Information and Knowledge
Management, H. Paques and L. Liu and D. Grossman
(Eds.), ACM Press, 2001, pp. 310-317
[34] T. Bu and D. Towsley, “On distinguishing between
internet power law topology generators,” Proc. of
INFOCOM, 2002.
[35] R. Albert, H. Jeong, and A. Barabasi, “Diameter of
world-wide web,” Nature, vol. 410, Sept. 1999, pp.
130-131.
[36] M. Granovetter. “Strength of weak ties,” American
Jour. of Sociology, vol. 78, 1973, pp. 1360-1380.
[37] H. Jeong, Z. Néda and A.-L. Barabási, “Measuring
preferential attachment for evolving networks”, Euro.
Phys. Lett. 61, 567, 2003.

[38] M. E. J. Newman. “Clustering and preferential
attachment in growing networks,” Phys. Rev. E 64,
025102, 2001.
[39] A. Barabási and R. Albert, “Emergence of scaling in
random networks,” Science, vol. 286, 1999, pp. 509512.
[40] M. Steyvers and J. B. Tenenbaum, “The large-scale
structure of semantic networks: statistical analyses
and a model of semantic growth” (submitted to 25th
Annual Meeting of the Cognitive Science Society)

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

Appendix
In this section, we describe the properties of the
resulting P2P network, such as small-world networks, and
scale free networks. Finally, we propose a new technique
for generating a P2P network for our simulations.

Small-World Networks
Watts and Strogatz [23] have described a special type
of semi-random network, called “Small-World Networks,”
which contain a few randomly re-wired long-distance
edges within a regularly connected network of nodes.
These networks have low characteristic path lengths (as in
random networks) and high clustering coefficients (as in
regular networks). Subsequently, a number of papers have
acknowledged the existence of small-world networks in
the Internet topology [34, 35]; the power grid of the
western United States; various social networks [23], such
as the collaboration graph of film actors; Erdös numbered
research scientists; and even in the neural network of the
worm C. elegans.
Further, Granovetter [36] discusses the existence and
shows the importance of weak social ties (links) between
highly connected clusters of friends.
The similarity of P2P networks to social networks and
the fact that humans direct peer links led us to believe that
P2P networks would also exhibit small-world behavior. In
fact, this has already been observed in existing P2P
networks, such as Gnutella [2].

In order to correctly model the P2P network, it is
important that we also incorporate the scale-free property
into the network topology.

Creating Our Own P2P Network
We needed to provide a mechanism to ensure that our
P2P network topology would exhibit the properties of a
small-world network and would also show a power-law
distribution for frequency vs. degree.
Our next approach [9] involved enforcing certain rules
on new peers that wanted to join the P2P system. We were
inspired by the work of M. Steyvers and J. Tenenbaum
[40] on semantic networks, and extended the domain of
their model to a P2P network that involved peers and
links.
The new peer, X, has to follow a two-step procedure
(described below) in order to join a P2P system.

Let N = {set of known peers} (See section 2.3.2)

℘

→ = “connects (links) to” with probability ℘
℘ A ∝ degree of node A (kA)
X

℘

A


→ A,

A∈ N

1
→ B
∀ B = m ′A ∈ M ′A , X 

Scale-Free Networks

where,

Scale-free networks are characterized by the uneven
distribution of connections (links) in the nodes of the
network. Unlike a random network that exhibits a Poisson
distribution of node degrees, a scale-free network
demonstrates a degree distribution that decays as a power
law. Hence, scale-free networks have sometimes been
described as power-law networks.
During the study on complex networks, [24, 25]
observed that many networks demonstrated topological
properties that were different from the predictions made
by random network theory. In particular, the existence of
some very well connected “hub” nodes dramatically
influenced the behavior of these scale-free networks
during random node failures and spreading of information.
It has been shown that various networks, such as the
collaboration graphs of actors and scientists, were
developed due the feature of preferential attachment [37,
38, 39]. This feature describes the probability of a node
acquiring new links as an increasing function of the links
that it currently has.

M A = {set of neighbors of A}, M A = k A

Step (1)
Step (2)

M ′A ⊆ M A such that M ′A = d (globally defined)
i ∈ M ′A with probability ℘i ∝ k i iff i ∈ M A

Proceedings of the Second IEEE International Information Assurance Workshop (IWIA’04)
0-7695-2117-7/04 $ 20.00 © 2004 IEEE

CLIDE: A Distributed, Symbolic Programming System
based on Large-Grained Persistent Objects *
Mark P. Pearson

Partha Dasgupta

Distributed Systems Laboratory
College of Computing
Georgia Institute of Technology
Atlanta, GA 30332-0280

Department of Computer Science
and Engineering
Arizona State University
Tempe, AZ 85287-5406

mpearson@cc.gatech. edu

partha@enuxua.eas.asu.edu

Abstract

design of CLIDE is presented in Section 4 and is followed
by a discussion of the usage of CLIDE in Section 5 . The
megaprogrammingparadigm as it relates to CLIDE and the
implementation of distributed applications is presented in
Section 6. Finally, the CLIDE system’s current implementation status and future research directions are presented
in Section 7.

CLOUDS LISP DISTRmUTED ENVIRONMENTS
(CLIDE) is a
distributed, persistent object-based symbolic programming
system being implemented on the CLOUDSdistributed operating system. LISP environment instances are stored as
large-grained persistent objects, enabling users on many
machines to share the contents of these environments
through inter-environment evaluations. CLIDE provides
a comprehensive research environment for distributed symbolic system language, invocation and consistency semantics and an implementation vehicle for the construction
of the symbolic processing portions of complex megaprogrammed systems.

1

2

Distributed problem solving through the use of multiple cooperating subsystems, experts, or agents has been a focus
of recent research in the computing sciences. Examples include autonomous mobile robot navigation [2], distributed
decision support systems [6] [ll],multiple heuristic systems
and Distributed Smalltalk applications involving heterogeneous, non-Smalltalk processors [3] [4].
Recently, research in Distributed Artificial Intelligence
(DAI) has focused on the construction of software architectures which support the interaction of multiple intelligent systems. Multiprocessor oriented DAI architecture
work has primarily focused on system performance and
parallel execution. The CAGE system allows single users
to specify parallelism for multiple computing agents on
a shared memory multiprocessor [l], while the POLIGON
system supports concurrent, blackboard-based AI applications on distributed memory multiprocessor machines with
high-bandwidth interprocessor communication [21]. MACE
(Multi-Agent Computing Environment) allows each instantiated agent to run on a specific processor of an Intel SYM-1
large-memory hypercube [Ill. In all of these systems, the
distribution of cooperating agents is simulated on a single
multiprocessing system.
SIMULACTsimulates a multiprocessor system with a one
to one agent/processor mapping on a network of LISP machines [19]. In SIMULACT,
each agent runs synchronously
with all the other agents in the system via a central coordinator. Despite the distributed nature of their applications,
CAGE,POLIGON,
MACE, and SIMULACTsupport cooperating agents in a single-user, global environment.
Distributed Smalltalk provides a framework for cooperation among geographically separate Smalltalk users by
allowing direct access to remote, small-grained Smalltalk

Introduction

In the past, distributed system research has primarily focused on the theory and implementation of basic mechanisms and policies involved in distributed system construction, but has largely failed to address the actual programming paradigms, administration, features and use of
such systems. Shortfalls of current “networked” computing systems, a greater understanding of fine-grained objectoriented program construction and the existence of experimental distributed systems has motivated research in distributed systems programming techniques and programming in the large.
This paper presents the design of CLIDE, a distributed
symbolic programming system. CLIDE is built on top of
the CLOUDSdistributed operating system which is currently
in use at Georgia Tech. CLIDE allows users to construct
large distributed programs via the specification and instantiation of distinct, but sharable, symbolic processing environments. These environments, which encapsulate particular behaviors or services, are stored as large-grained,
CLOUDSobjects which persist until explicitly deleted. The
motivations for constructing distributed symbolic programming systems and the current state of such systems are
presented in Section 2. We describe design goals of the
CLIDE system in Section 2.1. In Section 3, an overview of
the CLOUDSdistributed operating system is presented. The
‘This work funded by NSF grant CCR-86-19886.

CH2996-7/91/0000/0626$01.OO 0 1991 IEEE

Motivations and Related Work

626

P r o v i d e s mechanisms f o r maintenance of environmental consistency and security. These qualities are

objects and, to some extent, environment sharing. Each
user environment exists in a logically distinct address space
on a specific machine. The complexity of the Smalltalk class
hierarchy limits object mobility between machines.
Avalon/Common Lisp extends Common Lisp [22] to
support fine-grained, recoverable atomic objects and remote evaluation [15]. This implementation allows remote
evaluation of LISP expressions among a group of distributed evaluators and supports limited transparent object
transmission between evaluators. Remote evaluations involving complex data types require the user to define sending and receiving object transmission functions. Each evaluator process is single-threaded and has a private recowerable store managed by a separate process for recoverable,
atomic objects.
Persistent CLOS (PCLOS) is an extension of CLOS
[5] that supports database independent persistence of finegrained CLOS objects beyond the LISP session that created
them [20]. Through PCLOS language constructs, users access a virtual database that contains CLOS object information. This virtual database is physically implemented by
several database servers which communicate over an Ethernet.
Distributed Smalltalk, Avalon/Common Lisp, and PCLOS are distributed implementations based on communicating processes that may not migrate. As a result, the
host operating system for these implementations is unable
to perform load balancing operations on processing activity pertaining to their execution. PCLOS provides distribution of CLOS objects and methods but does not s u p
port remote evaluation. Problems with environment security in Distributed Smalltalk allow “users to shoot others”
[3]. It is not apparent that Avalon/Common Lisp addresses
these security issues either; instead they focus on recoverable atomic objects.

2.1

necessary for any system which must maintain consistency
of shared environments for a community of users.

Supports the architecture of future megaprogrammed information systems.
Some of the above features can be achieved by using the
large-grained persistent objects and threads provided by the
CLOUDSoperating system. Portions of the design of CLIDE
were inspired by the availability of these features. Since the
design of CLIDE depends upon the semantics of CLOUDS
objects and threads, we first present a brief overview of the
CLOUDSdistributed operating system.

3

CLOUDS
Overview

CLOUDSis a general purpose, experimental, distributed
operating system which supports two simple primitives:
threads and persistent object memory. With these primitives, CLOUDSintegrates a number of loosely-coupled machines to produce a system that behaves like a centralized,
time-sharing system [lo].

3.1

Object and Threads

A CLOUDSobject is a passive, persistent virtual address
space that is not associated with any process or thread.
In terms of conventional operating systems, CLOUDSobjects are persistent like files; differ from active, processassociated virtual address spaces; and are accessed like addressable memory.
Like fine-grained objects supported by object-oriented
languages such as Smalltalk [la] and C++ [23], CLOUDS
large-grained objects contain data and methods that operate on the data. The large size of CLOUDSobjects and the
overhead associated with object invocation favor them for
the persistent encapsulation of behaviors normally modeled by entire programs (which may be fine-grain objectoriented). CLOUDSobjects send messages to each other via
inter-object invocation. One CLOUDSobject may invoke a
public operationof another CLOUDSobject, with or without
par ame t ers.
Threads are active entities which invoke public operations of CLOUDSobjects. Threads, via object invocation,
can execute code and manipulate data in multiple virtual
address spaces. During each object invocation, a thread
executes code and manipulates data within the invoked object. Parameters passed in an object invocation and returned at that invocation’s termination are data, not addresses, since addresses in the context of one CLOUDSobject are meaningless in the context of another. Multiple
threads executing in the same object share the contents of
that object’s virtual address space. Of course, visibility of
addressable data and code is limited to the current address
space.

CLIDE Design Goals

The distributed symbolic processing system we envision is
one that:
P r o v i d e s m e c h a n i s m s for cooperation among multiple users and seamless s h a r i n g of d i s t i n c t environments. Seamless sharing of environments allows straightforward construction of distribu t8ed decision support systems.

Supports e n v i r o n m e n t persistence. Activities pertaining to the saving, loading, a.nd initialization of a particular user’s symbolic processing environment are timeconsuming. If environments can persist, then the time devoted to these activities can be eliminated.
P r o v i d e s mechanisms for parallel execution w i t h i n
a d i s t i n c t e n v i r o n m e n t . Intra-environment parallelism
allows previous work on efficient parallel LISP evaluation
to be extended to multiple site distributed systems.

627

3.2

Distributed Shared Memory

......................................

:
.

The CLOUDSdistributed system is comprised of network
connected data servers, computation servers, and user
workstations. In order to distribute CLOUDSobjects and
threads over these machines, CLOUDSprovides user transparent support for distributing computation and storage.
Distributed shared memory (DSM) supports the notion
of shared memory on a non-shared distributed memory architecture and enforces a coherence protocol among the
data servers that preserves single-copy semantics for all objects [17]. Through DSM, the contents of any CLOUDSobject may be accessed from any computation node. Further,
an object may be shared among many computation nodes
with the DSM system maintaining the consistency of its
contents [9]. CLOUDSassigns a computation node to a computation thread based on system load or user-preference, in
cases of user-directed load balancing. If both objects reside
at the same computation node, the CLOUDSinvocation is
local, otherwise it is remote.
Once thread execution commences, the required object
is brought in to the selected compute server via demand
paging. In addition, synchronization support allows threads
to synchronize their actions regardless of where they execute.

4

.

i

Compute and Data Servers

I1

CLOUDS Persistent Object
I

I .

CLiDE Interpreter
multi-threaded
evaluator

evaluate()

:

(public)

Heap Symbol Table

............................

I. . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .I . . . . . . .
I

I

Location
Independent,
User 1 / 0
I

in ow6

I
_

_

_

_

_

_

d

UNIX Workstation

......................................

:

Figure 1: CLIDE Environment Structure

CLIDE Design

CLOUDSLISP DISTRBUTED ENVIRONMENTS(CLIDE) is
a distributed, persistent object-based, symbolic programming environment built on top of the CLOUDSdistributed
operating system. The features of CLOUDSprovide us a
substrate on which we can build a system that exhibits the
characteristics presented in Section 2.1. Further, the ability to realize these characteristics a t the operating system
level validates the utility of the persistent, large-grained,
object/thread computing system model.
CLOUDS objects are the repositories for persistent
CLIDE symbolic processing environments. The CLOUDS
invocation mechanism provides a means for sharing environments. CLOUDSobjects with object-like interfaces allow
construction of encapsulated, protected CLIDE environments. These interfaces, along with CLOUDSconsistency
mechanisms, provide a framework for maintenance of environmental consistency and security. Computation and storage distribution in CLOUDSsupports construction of faulttolerant decision support systems. And, as we shall see later
(Section 6), CLOUDSlarge-grained persistent objects, coupled with CLIDE environments, support the architecture
of future megaprogrammed information systems.

4.1

CLOUDS Distributed Operating System

Eiivironment Storage Model

A typical CLIDE system is composed of a group of cooperating CLOUDSpersistent objects of the CLIDE class. Each
of these persistent objects emulates a single-user, symbolic
processing environment contained in a distinct, persistent,
virtual memory address space. We call an individual symbolic processing environment a CLIDE environment. Mul-

628

tiple users can create, maintain, and modify their own user
environments without the explicit loading and saving of environmental data because unlike typical symbolic processing environments, CLIDE environments persist until explicitly deleted.

4.2

CLIDE Environment Public Operations

At the core of a CLIDE environment (Figure 1) is a LISP
interpreter, whose dialect is based partly on Common LISP
[22] and Kernel [14], a portable LISP interpreter.
The functionality of a CLIDE environment is made
available to users, other CLIDE environments, and
CLOUDS objects representing classes programmed in
CC++ (CLOUDS C++) or DISTRIBUTED EIFFEL [9]
through an object-like interface that has five publicly available operations: c o n s t r u c t , clone, evaluate, l o g i n , and
logout.
The c o n s t r u c t operation creates a new instance of
a persistent CLIDE environment and assigns it a unique
name.
The clone operation creates a copy of a current CLIDE
environment instance by first instantiating a new CLIDE
environment and copying all the CLIDE environment information from the cloned environment to the new one. Different types of CLIDE environments with certain characteristics, such as a Common Lisp Object System (CLOS) implementation and predefined CLOS object hierarchy, could
be specified and cloned enabling the construction of a hierarchy of CLIDE environment sub-classes.
The evaluate operation enables other CLOUDSobjects

cific administrative sites in order to prevent performance
degradation at compute servers involved in ongoing evaluation activity.

to perform LISP evaluations in a CLIDE environment. This
extremely powerful operation is the conduit for environment sharing among a group of CLIDE environments. The
CLIDE evaluation model is discussed in Section 5.2.
The l o g i n operation allows a user to attach a CLIDE
environment to an X Windows server window. If the attach
is successful, a reader thread is spawned which will read
input directly from the user interface window and evaluate
it in the CLIDE environment attached to it.
The logout operation allows the system operator to
detach an X window from a CLIDE environment in case of
a system mishap.

4.3

Using CLIDE

5

As previously noted, the functionality of a CLIDE environment is made available through an object-like interface
utilized by specific users, other CLIDE environments, and
CLOUDSobjects of other classes. In the subsequent sections,
we discuss the CLIDE user interface, the CLIDE evaluation
model, and CLIDE environment security and consistency
issues. Following this discussion, we give an example.

The CLIDE Interpreter

The interpreter in a CLIDE environment has a multithreaded evaluator. The evaluator of CLIDE environment A is responsible for evaluating LISP expressions in
the context of environment A. Any portions of a LISP
expression that should be evaluated in the context of another CLIDE environment B are sent to its multi-threaded
evaluator via a CLOUDSinvocation. The public evaluate
operation of B is responsible for receiving requests for evaluation from other CLIDE environments, validating evaluation permissions, evaluating the expression, and finally
returning the results.
A CLIDE environment reader thread is spawned when
a user invokes an environment’s l o g i n operation. The environment reader passes along LISP expressions to the evaluator for interpretation.
Since CLIDE environments persist, reclamation of
space taken up by unreferenced heap objects is of primary importance. Our system design calls for a two-fold,
g a r b a g e collection s t r a t e g y similar to Wilson’s Opportunistic Garbage Collector [26]. While evaluations are active within a CLIDE environment, generational garbage
collection takes place when a user is not likely to notice.
Further, as CLIDE remote evaluations complete, certain
dereferenced symbols pertaining to it are automatically collected. When no evaluations are active in a CLIDE environment, the state of the environment heap is checkpointed
and the tenured object generation’ is garbage collected. If
a remote evaluation is received by the CLIDE environment
during this potentially lengthy garbage collection operation,
the collection operation is aborted, and the environment
state is rolled back. Attempts to invoke a CLIDE environment’s l o g i n entry point during a tenured generation
garbage collection operation are blocked until the operation completes. Reader and remote evaluation threads of
control are used to spawn garbage collection threads. Utilization of object/thread migration mechanisms in CLOUDS
allows garbage collection operations to be relegated to spe-

5.1

User Interface

The system configuration for CLIDE is a hybrid one consisting of multiple machines, each running either UNIX2
or the CLOUDSdistributed operating system. The CLOUDS
and UNIX machines cooperatively interact to provide users
with the benefits of both operating systems.
A UNIX-based CLIDE interpreter serves as the user
interface conduit to CLOUDS-based persistent CLIDE environments. With this user interface setup, users can enter, test, and debug CLIDE functions in the UNIX-based
CLIDE environment before moving them to a CLIDE environment running under CLOUDS.In addition, the CLOUDS
operating system interface, instead of being a collection of
cryptic “guru-mnemonic” commands (like UNIX), is a set of
CLIDE commands which can be quickly assimilated by any
LISP programmer. CLIDE persistent environments, combined with a UNIX-based CLIDE interpreter user interface
strategy, allows the construction of a multi-user symbolic
processing environment on a UNIX-CLOUDSsystem platform.

5.2

Evaluation Model

The CLOUDSinvocation model supports CLIDE environment sharing through inter-environment synchronous or
asynchronous evaluations. In order to see the effects of
the CLIDE evaluation model on a group CLIDE environments, we will look at local, remot.e, synchronous and asynchronous evaluations.

5.2.1

Synchronous Remote Evaluations

In general, CLIDE evaluations are assumed to be local
and synchronous unless explicitly specified otherwise. The
CLIDE remote function is used to specify remote environment evaluations. The syntax of this LISP form is:
(remote <remote-environment>

Research has shown that there is a high mortality rate among
recently created objects. A generational garbage collector scavenges this young group of objects most often. Objects that survive several garbage collection scavenges are considered “longlived“ and are tenured to a memory space that is garbage collected less often.

<expr>)

2UNIX is a trademark of UNIX System Laboratories, Inc.

629

For example, the LISP expression evaluated in CLIDE environment envo:

as a string t o the receiving object. CLOUDSobjects implemented in either DISTRmUTED EIFFEL or cc++must
provide buffer space to capture and parse the contents of
the string passed to it. The development of systems with
modules that are programmed in different languages is facilitated by the ability of CLOUDSobjects to invoke each
other through standard interfaces. Thus, a system which
has a calculation and real-time control module, a rule-base
module, and a storage module based on fine-grained objects could be implemented in CC++, CLIDE, and DISTRIBUTED EIFFELrespectively, as a set of three persistent
CLOUDSobjects.
The claim function allows the user to claim results of
one or more future sets. This function blocks (waits) until
each return expression specified is true. The return expression is a logic expression that specifies which (if any) of a
future set’s evaluations must complete for the future set to
be considered claimable. An example showing the construction of the return expression is shown in Section 5.5.
The claim-status function enables the user to check
the completion status of one or more future sets. The function returns a list of lists, each containing the requested set
name and the identifiers of the evaluations in that set that
have completed.
Care must be taken to prevent the accumulation of stale
or orphaned CLIDE future set structures since they are
stored in the environment heap. Hence, the d i s c a r d function, which is used t o discard individual evaluation results
or pending request(s) for results of a particular evaluation
from a named future set. In addition, the discard function
can remove the entire future set from the heap.

(remote envl
(lookup-function
(remote env2 data-item)))
implies two synchronous, remote evaluations. First, an
object invocation is made to the e v a l u a t e operation of
CLIDE environment enol. The expression passed to envl
is:
(1ookup-f unct i o n

(remote env2 data-item))
Now, the CLIDE environment env1 makes an object invocation t o the e v a l u a t e operation of CLIDE environment
env2. Thus, the expression data-item is passed to envz.
CLIDE environment env2 evaluates data-item and returns the results of this evaluation to env1 which subsequently uses these results as an argument to its function
lookup-function. Upon completion of this evaluation, the
final results are returned to the original caller.

5.2.2 Asynchronous Evaluations
Parallel processing research in the late 70’s introduced the
notion of using asynchronous evaluations to exploit parallelism [16]. Later, the semantic implications of futures in
LISP-like systems were addressed in Multilisp [13]. Since
then, the future abstraction has been extended by others in order to ease programming of distributed applications with asynchronous execution entities. Examples include Argus [18] and Cronus [24]. The future abstraction in CLIDE is supported in part by CLOUDS, which
provides asynchronous invocation and claiming of single
threads, and a suite of CLIDE functions which take care
of the identification, monitoring, and grouping of sets of
asynchronous evaluations. Representative functions of this
suite include: create-future-set, clouds-invoke, claim,
claim-status, and discard.
The create-future-set function creates a future set
structure and names it. Future sets group together one or
more asynchronous evaluations into one abstraction that
can be referred to at a future time.
Asynchronous object evaluations in CLIDE are triggered by the appearance of an asynchronous evaluation directive (‘!’) followed by the set name of a future set.
! <set-name> ( [remote <remote-env>l<expr>

...

5.3

Environmental Security

Any system which provides support for a set of shared objects must provide a means t o guarantee their consistency
and integrity.
Since the contents of CLIDE environments are easily
shared via CLIDE remote evaluation, the design of environmental security measures which guarantee CLIDE environment integrity are particularly important.
The CLOUDSdistributed operating system provides security for CLOUDSobjects by treating them as protected
distinct virtual spaces. CLOUDSobjects are semantically
similar to fine-grained objects supported by object-oriented
languages like Eiffel or C++ and may be accessed only
through programmer defined object-like interfaces. Users
must go through the CLOUDSoperating system via these
object-interfaces in order to access the contents of a particular CLOUDSobject. CLOUDSobjects are thus protected
the same way as user spaces are protected from the effects
of other users in conventional operating systems.
Although a CLOUDSobject is protected, the contents
of a CLIDE environment, encapsulated in a CLOUDSobject, is still vulnerable due GLIDE'S Lisp-like nature. Since
remote evaluation within an environment is possible, that
environment’s contents can be modified maliciously unless
additional security measures are taken. Each symbol in

)

CLIDE evaluations of this form return a positive integer
identifying the evaluation within the future set. Nil is returned if the supplied future set structure does not exist.
Evaluations may be directed to a non-CLIDE Clouds
object via the clouds-invoke function. This function generates a CLOUDSobject invocation. Non-optional parameters include an ezpcpression to be evaluated, the object name
of a CLOUDS object of any class, and the operation of
the CLOUDSobject to invoke. The expression is passed

630

a CLIDE environment has an access list which states the
permissions of access for that symbol. The access list for a
symbol contains access rights for local and remote evaluations.
Through symbol level security, CLIDE environments
with object-based interfaces are able to be constructed. All
symbols in a CLIDE environment are assumed to be private. A programmer can define CLIDE functions within an
environment that will serve as that environment's public interface and make their access level public. Thus, a CLIDE
environment can encapsulate "code" and data as effectively
as any Eiffel or C++ fine-grained object.

5.4

I

Environment Consistency

CLIDE environment consistency is provided through
CLOUDS'invocation-based consistency control mechanisms
[8]. Each public operation of a CLOUDSobject has a consistency label which states the consistency requirements for
the program code accessible from that entry point. This approach defines three consistency classes, global, local, and
standard. Properly used, global consistency (GCP) guarantees that CLOUDSwill automatically control consistency
and recovery across a group of CLOUDSobjects of any class.
Local consistency (LCP), on the other hand, is used when
the desire to maintain consistency is localized to one particular object. This consistency class guarantees that the
system will control consistency and recovery so that one
particular CLOUDSobject stays internally consistent. Standard consistency (SCP) guarantees nothing; if a particular
execution suffers no failures, the object data will be consistent, otherwise consistency is not guaranteed. The Lisp-like
nature of the CLIDE environment requires that these consistency labels be extended to apply to any CLIDE symbol.

5.5

High-Speed
Vision System

depth,fir

1 7 . e Base
Sear hAccess Functions

1

recv-c d

ICC++ Object

\

~1

U
Cartographer updates World-Map

I

--....

"locale" asynchronous remote invocations
Claim returns when visual processing
completes and heuristic search returns

I

Figure 2: Robot Navigation System Agents

answer this question, the navigator must update its worldmap and find out where it is in that map.
The code fragment that accomplishes these tasks looks
like this:

An Example

The following example demonstrates the power of the
CLIDE evaluation model and system structuring paradigm.
Suppose we have implemented the following agents for
the navigation system of an autonomous mobile robot.
Each agent is encapsulated in a CLOUDSpersistent object.
Figure 2 shows the interrelationships between each of the
agents.

1))
2>>

3>>

4>>

Navigator. This agent is responsible for supervising the
navigation of the robot. It is a CLIDE environment.

5>>

Cartographer. This agent collects sensory data from a
high-speed vision system, makes inferences about the sensory data, and updates the world-map of the robot. It is a
CLOUDSCC++ object.

6>>

7>>

World-Map. This CLIDE environment contains the
robot's world map. This object is shared by the navigator and cartographer agents.

[GCP (prog (locale sl 92 ul)
(create-future-set locale)
(SCP setq sl
!locale(remote world-map
(heuristic-search kb)))
(SCP setq s2
!locale(remote world-map
(depth-first-search kb)))
(setq ul
!locale(clouds-invoke
l1do-visual-processingo1
"cartographer" "recv-cmd"))
(claim (locale '(and (or sl s2) ul)))
(clouds-invoke
"update-world-map1'
"cartographer" "recv-cmd") ) ]

First, the navigator begins a globally consistent program evaluation (line 1). Within this GCP evaluation,
the navigator agent creates a future set named locale
(line 2). The world-map CLIDE environment, via CLIDE

A portion of the navigator agent's code deals with answering the question "Where am I in the world?". In order to

631

cation and migration issues are taken care of automatically.
Location specific objects which perform particular tasks
may be anchored to a particular hardware platform, but as
far as the user is concerned, that location is one, global machine. Thus, all CLOUDSobjects are referenced in the same
manner, whether they are physically distributed or not. As
shown in Section 5.5, CLIDE environments which encapsulate particular behaviors and knowledge representations can
be constructed. Through CLIDE’S straightforward evaluation semantics, CLIDE environments can be programmed
to cooperate among themselves to find the solution to a
complex problem. Further, the integrity and consistency
of these intelligent agents is maintained through CLOUDS
system consistency mechanisms and CLIDE security mechanisms.
Persistent environments in CLIDE are an appropriate
storage medium for the symbolic processing megamodules
of a megaprogram. Transparent storage and computation
distribution in CLOUDSallows thread-based megaprograms
to run on loosely coupled, distributed hardware. In addition, megamodules can be specified in the language most
appropriate for the task. The CLIDE system on CLOUDS
provides DAI researchers a means to physically distribute
DAI software architectures and integrate them with heterogeneous megamodules based on different non-symbolic
software paradigms to create distributed artificial intelligence megaprograms made up of CLIDE environments and
CLOUDSobjects programmed in languages like CC++ and
DISTRI~UTED
EIFFEL.

security mechanisms, provides two publicly available methods for searching its knowledge base. The first method
is a h e u r i s t i c - s e a r c h that works very well at times (depending on the data) but can run forever at other times
(line 3). The second method is a depth-first-search
that runs very predictably. When the h e u r i s t i c - s e a r c h
runs well, it can be many times faster than the standard
depth-f i r s t - s e a r c h (line 4). So, the navigator makes
two asynchronous, remote evaluations to the world-map
CLIDE environment, which effectively begins the simultaneous search of the world-map knowledge base, kb, using
the two different public methods. Note that both of these
searches are conducted with standard consistency evaluations. This is allowable because the searches are read only
and necessary because a GCP evaluation would not allow simultaneous searches in a CLIDE environment (world-map)
it was enforcing consistency upon [7].
In addition, the navigator must update the world-map.
Only the cartographer knows how to do this, so the navigator makes a final asynchronous, remote evaluation to the
Cartographer object (line 5). Now, the navigator waits until
the visuaZprocessing(line 5) and either search (line 3 and 4)
completes. When this condition becomes true, the claim
call unblocks (line 6). The cartographer is then directed
by the navigator to update the world-map with its newly
gathered information (line 7).
The GCP evaluation makes sure that all three CLIDE
environments stay consistent with respect to each other. If
the GCP evaluation commits, then the changes made in the
navigator, cartographer, and world-map environments will
be flushed to stable storage and will be visible to subsequent
evaluation threads. If the GCP evaluation aborts, the state
of these environments would be the same as they were just
prior to the initiation of the GCP evaluation, as if the GCP
evaluatioii had never occurred.

6

7

The Megaprogramming Paradigm

Programming in the very large, or megaprogramming, deals
with the development, construction, and maintenance of
very large applications made up of large programs or megamodules with object-like interfaces [25]. A megaprogram
is composed of a group of these modules which interact
with each other through well-defined object-like interfaces.
Generally, constructing the object-like interfaces for these
megamodules requires programming semantics support beyond that of typical fine-grained object systems and support for these semantics must be provided by the operating
system on which the megaprogram resides. This programming paradigm is particularly well suited for object/thread
distributed computing systems like CLOUDSsince the components of megaprogram construction and execution are the
operating system’s primary units of distribution.
CLOUDSsimplifies large-grained object semantics somewhat by allowing programming to proceed as if all CLOUDS
objects reside on one huge computer system. There is no
need to support object location programming semantics for
CLOUDSuser object specification because, in most cases, lo-

632

Implementation and Future Work

Implementation of a single threaded version of CLIDE for
UNIX (in C++) and CLOUDS(in CC++ [9]) is complete.
Current implementation work is focused on multi-threading
the CLIDE environment. This task has required design
work on an evaluator that can handle multiple evaluation
contexts while sending and receiving native heap data to
and from other CLIDE environments. In addition, design
work has begun on truly encapsulated fine-grained intraheap objects which will be used to simplify the specification of inter-environment evaluation security, and provide
functionality similar to Common Lisp packages and CLOS
0bject.s.
An implementation of CLOUDSconsistency mechanisms
which supports consistency labeling of CLOUDSobject entry points is complete [7]. An intra-environment version of
these mechanisms will need to be implemented in order to
support consistency for individual CLIDE evaluations and
symbols.

References
[l] N. Aiello. User Directed Control of Parallelism; The
CAGE System. In Proceedings of the Expert Systems
Workshop, pages 146-151, Pacific Grove, CA., April
1986. Asilomar.

[2] R.C. Arkin. Motor schema-based mobile robot navigation. International Journal of Robotics Research,
8(4):92-112, August 1989.

[16] Henry Baker Jr. and Carl Hewitt. The Incremental
Garbage Collection of Processes. SIGPLAN Notices,
12(8):55-59, August 1977. Proceedings of the Symposium on AI and Programming Languages.

[3] John K. Bennett. The Design and Implementation of
Distributed Smalltalk. SIGPLAN Notices, 22(12):318330, December 1987.

[17] K. Li and P. Hudak. Memory Coherence in Shared
Virtual Memory Systems. ACM Transactions on Cornputer Systems, 7(4):321-359, November 1989.

[4] John K. Bennett.
Experience With Distributed
Smalltalk.
Software - Practice and Experience,
20(2):157-180, February 1990.

[18] B. Liskov and L. Shrira. Promises: Linguistic Support for Efficient Asynchronous Procedure Calls in Distributed Systems. In SIGPLAN Notices, pages 260267, Vol 23, No. 7, July 1988.

[5] D. Bobrow, L. DeMichiel, R. Gabriel, G. Kiczales,
D. Moon, and S. Keene. The Common Lisp Object
System specification. Technical Report T R 88-002R,
X3J13 standards committee document, 1988.

[19] D.J. MacIntosh and S.E. Conry. A Distributed Development Environment for Distributed Expert Systems.
Third Annual Expert Systems in Government Conference, pages 72-79, 1987.

[6] D.A. Carlson and S. Ram. An Object-Oriented Design for Distributed Knowledge-Based Systems. Proceedings of the Twenty-Second Annual Hawaii International Conference on Systems Sciences, III:55-63,
January 1989.

[20] Andreas Paepcke. PCLOS: A Critical Review. In
Proceedings of the Conference on Object-Oriented
Programming Systems, Languages, and Applications,
1989.

[7] Ray Chen and Partha Dasgupta. Implementing Con-

sistency Control Mechanisms in the CLOUDS Distributed Operating System. In Proceedings of the
11th International Conference on Distributed Computing Systems, May 1991.
[8] Raymond C. Chen and Partha Dasgupta. Linking Consistency with Object/Thread Semantics: An Approach
to Robust Computation. In Proceedings of the 9th International Conference on Distributed Computing Systems, June 1989.

[21] J. Rice. Poligon, A System for Parallel Problem Solving. In Proceedings of the Expert Systems Workshop,
pages 152-159, Pacific Grove, CA., April 1986. Asilomar.
[22] G. Steele. Common LISP. Digital Press, Burlington,
MA., 1984.
[23] Bjarne Stroustrup. The C++ Programming Language.
Addison-Wesley Publishing Company, Reading, MA,
1986.

[9] P. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohindra, M. Pearson, R. Chen, and C. Wilkenloh. Language and Operating System Support for Distributed
Programming in CLOUDS. In Proceedings of the 2nd
Symposium on Experiences with Distributed and Multiprocessor Systems, 1991.

[24] R. Walker, E. Floyd and P. Neves. Asychronous Remote Operation Execution in Distributed Systems. In
Proceedings of the 10th International Conference on
Distributed Computing Systems, pages 253-259, May
1990.

[IO] P. Dasgupta, R. C. Chen, S. Menon, M. P. Pearson,
R. Anantlianarayanan, et al. The Design and Implementation of the Clouds Distributed Operating System. Usenix Computing Systems, 3(1), 1990.

[25] Peter Wegner. Concepts and Paradigms of ObjectOriented Programming. OOPS Messenger, 1(1):8-85,
August 1990.
[26] Paul Wilson and Thomas Moher.
Design of the
Opportunistic Garbage Collector. In Proceedings of
the Conference on Object-Oriented Programming, Systems, Languages, and Applications, 1989.

[I13 L. Gasser, C. Braganza, and N. Herman. Implementing Distributed AI Systems Using MACE. Proceedings
of the Third Conference on Artificial Intelligence Applications, pages 315-320, 1987.
[I21 Adele Goldberg and David Robson. Smalltalk-80: The
Language and its Implementation. Addison-Wesley,
Menlo Park, CA., 1983.
[13] R. Halstead. Multilisp: A Language for Concurrent
Symbolic Computation. A CM Transactions on Programming Languages and Systems, 7(4):501-538, October 1985.
[14] Sharam Hekmatpour. LISP: A Portable Implementation. Prentice Hall International (U10 Ltd., Hemel
Hempstead, United Kingdom, 1989.
[15] J. Stamos and D. Gifford. Remote Evaluation. ACM
Transactions on Programming Languages and Systems, 12(4):537-65, October 1990.

633

Proceedings of 2016 IEEE 13th International Conference on Networking, Sensing, and Control
Mexico City, Mexico, April 28-30, 2016

Intelligent and Adaptive Temperature Control for
Large-Scale Buildings and Homes
Yuan Wang

Partha Dasgupta

Arizona State University
Tempe, AZ, USA
Email: Yuan.Wang.4@asu.edu

Arizona State University
Tempe, AZ, USA
Email: partha@asu.edu

Abstract—Temperature control in smart buildings and homes
can be automated by having computer controlled air-conditioning
systems along with temperature sensors that are distributed in
the controlled area. However, programming actuators in largescale buildings and homes can be time consuming and expensive.
We present an approach that algorithmically sets up the control
system that can generate optimal actuator settings for large-scale
environments.
This paper clearly describes how the temperature control
problem is modeled using convex quadratic programming. The
impact of every air conditioner(AC) on each sensor at a particular
time is learnt using linear regression model. The resulting system
controls air-conditioning equipments to ensure the maintenance
of user comforts and low cost of energy consumptions. Our
method works as generic control algorithms and are not preprogrammed for a particular place. The system can be deployed in
large scale environments. It can accept multiple target setpoints
at a time, which improves the flexibility and efficiency for
temperature control. The feasibility, adaptivity and scalability
features of the system have been validated through various actual
and simulated experiments.

I. I NTRODUCTION
About 70% of the electricity load is consumed by commercial and residential buildings in the US. Studies show by the
year 2025, building electricity energy costs will be over 430
billion dollars. With the rapid increase of energy costs in the
building sector, carbon dioxide(CO2 ) emissions are growing
faster than before. The sector contributes around 39% CO2
emissions in the US per year, more than any other sector such
as transportation, which results in threat of climate change.
Heating, ventilation and air conditioning(HVAC) system is
the largest energy consuming sector in buildings and homes.
In the year 2009, about 48% energy costs are contributed
by space heating and cooling. Most of traditional HVAC
systems in buildings and homes are controlled by thermostats
that are manually configured either by centralized control
technicians or individual users, which requires a lot of human
efforts. For maintaining user comfort levels in work places,
these systems usually keep AC settings all the day, even
during off hours when the places are not occupied, which
causes a lot of unnecessary energy consumptions. Some of
thermostats in office areas do not provide friendly interfaces
and flexible functionalities for users to control. In a centralized
air-conditioning control area, users can not customize the

c
978-1-4673-9975-3/16/$31.00 
2016
IEEE

temperature settings for a particular place, which causes a lot
of inconvenience.
Saving energy and satisfying user comforts are two main
aims for HVAC control systems design. For saving energy,
amount of total energy costs should be minimized during the
control. For maintaining user comforts, inside room temperature(sensed by temperature motes) should be uniform and stay
in a satisfied range. There always exist tradeoffs between these
two goals. One of the popular products in current market for
smart thermostat technology is the Nest Learning Thermostat
[1]. It learns user preferences such as temperature levels during
a day for about a week and then it can automatically generate
AC control plans based on user input data. The drawback of
this approach is it relies too much on user selected data. If
users randomly provide some incorrect data to the thermostat
or they disable it for a particular time, the effectiveness of
the thermostat may drop down. [2] proposed an approach
called smart thermostat that can automatically turn on/off air
conditioning systems by sensing occupancy and sleep patterns.
It can also generate an energy efficient plan for the preheating stage by looking at system configurations and analyzing
historical occupancy patterns. One limitation of this work is
it relies on a lot of information from the equipment itself and
this paper only evaluates a single type. The scalability and
adaptivity of the system need to be improved.
Although there are a lot of great improvements for smart
building technologies nowadays, current control plans for
HVAC systems still have some drawbacks and limitations.
First, in most of the commercial buildings, one thermostat is
used for controlling multiple vents at same time. People in
different working areas can not customize their preferences
when they have conflicts on temperature settings. Second,
current HVAC control approaches are not good for large scale
environments where there are many standalone ACs in an
area. Each AC is independently controlled by a single switch.
A subset of ACs need to be turned on according to users’
preference settings. The control plan should optimize user
comfort levels without sacrificing too much energy. Last, in the
current HVAC control system, temperature sensors are usually
built in thermostats that are put on the wall. They should be
placed at sitting areas in order to better capture surrounding
temperature for users.
In this paper, we design an air-conditioning control system

for tackling the above challenges. Our system can be used for
controlling large commercial and residential buildings where
there exist multiple ACs, control switches and temperature
sensors. The goal of the system is to optimize the user comfort
levels and minimize energy consumptions. There are two main
stages in this system: predictive stage and adjustive stage. In
the predictive stage, mathematical models are used to formalize the problem and optimization algorithms are used to get a
predicted solution. In the adjustive stage, feedback system are
designed to reduce potential errors. Our system is good for
providing fine-grained settings during precooling/preheating
stage. Our approach is adaptive, which means it can be easily
applied in any type of building environments.
II. R ELATED W ORK
Thermostats technologies have been widely used in HVAC
systems for automatically controlling HVAC equipments in
buildings and homes. The basic logic behind is HVAC equipments are turned on when its controlled area is occupied and
turned off when occupants leave the area. The thermostats
are preprogrammed and temperature setpoints are predefined
according to the local environments such as static occupancy
patterns. It relies on too much static information therefore
it is hard to adapt to environmental changes. An alternative
method called reactive thermostat is proposed for tackling
the problem. It uses various sensors such as motion sensors
or door sensors to detect user activities in real time so that
the control system can adjust system settings when pattern
changes. However, some studies found that this method didn’t
improve the efficiency as people expected since there usually
exists long delay for the system reaction due to the hardware
limitations which even save less energy than the previous
programmable thermostat approach.
[2] proposed an approach called smart thermostat that can
automatically turn on/off air conditioning systems by sensing
occupancy and sleep patterns in real time. It can also generate
an energy efficient plan for the preheating stage by looking
at system configurations and analyzing historical occupancy
patterns. This approach dynamically controls HVAC systems
based on occupancy status in a place, which can effectively
adapt to environmental changes. One limitation of this work
is it relies on a lot of information from the equipment itself
and this paper only evaluates a single type. The scalability and
adaptivity part of the system need to be improved.
Nest Learning Thermostat [1] is a popular user-centric tool
for HVAC system control, which is very easy to use compare
to the traditional programmable thermostat. It automatically
learns a user’s preferences and behaviors based on some precollected data from the user. The drawback of this approach
is it relies too much information on the user-input data. If
users randomly provide some incorrect data to the thermostat
or they disable it for a particular time, the effectiveness of
the thermostat may drop down. Another drawback of this
approach is it is usually set up in a single AC environment
such as residential homes where only one target temperature
value at a time is accepted. When two people have conflicts

on target temperature values in a place or the thermostat is set
up in a large scale environment where multiple ACs exist, the
thermostat can not give effective solutions.
A fuzzy inference system for adaptively doing heating
control is proposed in [3]. The main idea is it takes power
profile of the previous day, adjusts the profile based on current
conditions and then applies the latest profile to the current day.
It uses Artificial Neural Network model to predict the future
comfort levels and a fuzzy rule is designed for the setting
adjustment step. The drawback of the system is it mainly
focuses on maintaining user comforts during the control. The
energy saving part is only considered when a place is not
occupied. The fuzzy rule should be improved so that some
fine-grained settings can be provided instead of using the
words “large”, “medium” and “small”.
Our approach is proposed for tackling the above problems.
It can be deployed in a large scale environment without any
static or customized configurations. If needed, users can set
their own preferences on temperature setpoints in a workplace.
The system can efficiently and effectively control HVAC
equipments even when there exist multiple target values on
sensors. It provides fine-grained pre-cooling plans based on
occupancy schedules. When environment changes, the system
can automatically update calibration data and adjust the settings accordingly.
WSN technologies have been applied into various areas such
as [4] [5][6]. It consists of portable wireless sensor motes such
as Crossbow’s TelosB or MICAz to monitor the values of
physical conditions, such as temperature, light, humidity, and
so on. WSN data collections use several specific protocols
such as Collection Tree Protocol [7].
III. F ORMAL M ODEL
This section describes how we use mathematical model to
formulate the problem. For simplicity, we build the model for
cooling strategy of air-conditioning control. Heating strategy
can be modeled in a similar way.
In this model we have a set of on/off switches that control
arbitrary n ACs (one per switch) and a set of m temperature
sensors. The sensors are connected to the control system
via a wireless sensor network and the switches are activated
via actuators connected to the system. Physical locations and
correlations between them are initial unknown to the control
system.
The ultimate task is to compute the positions of switches
over time, i.e. durations for each switch to be kept on. We
assume an AC will keep running until its control switch is
turned off. Let t = (t1 , ..., tn )| denote the assignment for AC
switches where ti ≥ 0 and it denotes switch i is kept on for
ti time units.
The goal is to optimize the energy E(t) and the comfort
C(t). E(t) is the total energy consumptions in the control
period. We assume the power of each AC is constant during
the control period. Let w = (w1 , ..., wn )| denote the electric
power for each AC. Then E(t) = w| × t. For C(t), it is
satisfied when all sensor values approach their target values

and stay in a satisfied range. Let e = he1 , ..., em i denote
all sensor readings. Each sensor has a target value to be
reached, for example, sensor j’s target value is tari . Let
tar = htar1 , ..., tarm i denote all target values. They can be set
either by users or default standard values. Hence the problem
can be stated as:
minimize
t

hw| × t, ke − tark22 i

subject to min ≤ ej ≤ max, j = 1, ..., m

(1)

0 ≤ ti ≤ tmax , i = 1, ..., n
By applying the -constraint method [8] designed for solving multi-objective optimization problems, the new objective
function can be defined as:
minimize
t

ke − tark22

subject to min ≤ ej ≤ max, j = 1, ..., m
w| × t ≤ 

(2)

0 ≤ ti ≤ tmax , i = 1, ..., n
We assume that the impact of ACs on sensors(number of
sensor values decrease) over a short time period is additive,
i.e., the impact of two ACs over a short time period on
one sensor(total number of sensor values decrease during the
period) is the sum of the individual impact for each AC. Since
the time period is considered to be small for solving the above
equation, the relationship between time and temperature can be
learnt using linear regression model, which will be discussed
in the next section.
IV. C ONTROL A PPROACHES
There are two main stages for solving the air-conditioning
problem. The first stage is called predictive stage that happens
before the room is occupied, which is used for predicting an
approximate setting for air-conditioning control. The second
stage is called adjustive stage that happens after room is
occupied. It is used for eliminating the potential offsets from
the predictive stage and providing fine-grained air-conditioning
control for maintaining user comfort levels. In order to achieve
max electricity savings, we decide to turn off all ACs when
the controlled area is not occupied, and pre-cool the area in
advance before users enter the space. By default we assume
we have a list of occupancy schedules. The schedule can be
updated by users in real time, for example, a user can send a
notification at any time telling the system when he expects to
get back.
A. Predicting AC Impacts on Sensors Over time
In the cooling stage, an AC continuously provides cooling
air to reduce the inside temperature levels until it is turned
off. Since the relationship between time and temperature in
each cycle is investigated over a short time period, up to an
upperbound tmax , a linear regression model can be identified
with a high coefficient of determination [9]. It is defined in
the form of:

T =k∗t+b
where t stands for interval time and T stands for temperature
level. In order to learn k and b, some experimental data
points should be pre-collected. These coefficients should be
updated when environment changes such as building patterns
or weather conditions. There are multiple ACs and multiple
sensors in the system. For each sensor, every AC has an
impact factor on it, i.e. a pair of (k, b). Since sensor readings
are additive, to a particular sensor j, its reading ej after an
operation of n ACs on a vector t time is:
ej = sj − m|j ∗ t

(3)

where sj stands for the initial sensor j’s value and
 
 
t1
k1j
 
 
 t2 
 k2j 
 

mj = 
 ..  t =  .. 
.
 . 
tn
knj
sj can be collected from the calibration step and mj can
be calculated based on linear regression model.
B. Calibrating and Calculating Impact Factor
In the last section, we discussed how we use a linear
regression model to learn the relationship between time and
temperature. Note that this model is effective when time vector
is small. Assume all ACs are off at the beginning. Calibration
steps involve the following:
• Turn on one AC at a time for a period of time t, record
the temperature changes on every sensor. Selection of
t should be reasonable, so that a range from initial
temperature to the target temperature can be covered for
each sensor, if applicable.
• Analyze the collected data using linear regression model.
Perform linear regression analysis for different period of
time, if necessary. Thus, a impact factor kij might have
different values for different periods of time.
• Repeat step 1 n times until all ACs are counted. For
each sensor j, a vector mj is created. If more than one
kij exist, then for each period of time, a mj is recorded.
In each AC operation cycle, either the predictive stage or
the adjustive stage, only one mj can be used for sensor j.
The selection of mj is decided by the initial sensor value on
j. Each AC is assigned an upperbound value for its operational
time during every cycle, formulated as 0 ≤ ti ≤ tmax . Value
of tmax is associated with value of kij . For each AC i, from
time 0 to time tmax , its impact factor on every sensor j kij
should remain the same.
Ideally, impact factors and room initial temperatures should
be updated frequently to ensure the accuracy of the computation. However, in some cases, these values remain the same
in a period of time. Thus in order to increase the efficiency,
these values don’t have to be re-calibrated or updated if the

change doesn’t exceed a threshold. The threshold value can
be set based on local environments.
C. Solving the Equations to Compute Approximate AC Settings
The result of equation 2 is a time vector t that indicates
the working time interval for every AC in the room. The
upperbound value tmax for each ti in vector t can be different.
The largest value among all values of tmax is assigned to q.
If users return at time p, then the system will start calculating
t from time p − q in order to cool the room at time p. After
time p, the system will go to the adjustive stage until the room
is unoccupied again.
Applying equation 3 to equation 2, we get:

minimize
t

m
X

(m|j ∗ t − xj )2

j=1

subject to sj − max ≤ m|j ∗ t ≤ sj − min, j = 1, ..., m
w| × t ≤ 

The above transformation satisfies the format of quadratic
m
X
programming model [10] where H = 2 ∗
mj ∗ m|j ,
j=1

f | = −2 ∗

m
X

xj ∗ m|j ,

of all tmax values. Number of elements in lb and ub is both n.
Thus our problem belongs to quadratic programming problem,
which can be solved by some existing algorithms such as
interior point method. In addition, the objective function
appearing in equation 4 is a standard form of least-squares
[11] that is convex, therefore our problem is a convex quadratic
programming problem. The result set will be a set of global
minimum.
The weight factor  (which appeared in equation 4) is used
to balance the tradeoff between saving energy and maintaining
user comforts. The assignment of  will significantly affect
the final result, therefore, it should stay in a reasonable range.
In order to rationally set , we first study the approximate
minimum value of , i.e.,  ≥ min , by solving the following
equation:

0 ≤ ti ≤ tmax , i = 1, ..., n
(4)
where xj = sj − tarj . The objective function of equation 4
is a quadratic function that can be re-written in the following
form:

t| ∗ (

m
X

m
X

mj ∗ m|j ) ∗ t − 2 ∗ (

j=1

xj ∗ m|j ) ∗ t +

j=1

The constraints of equation 4 can
where

k11
k21
 k
k22
 12
 .
..
 .
.
 .

k2m
 k1m

Am,n =  −k11 −k21

 −k12 −k22
 .
..
 .
 .
.

−k1m −k2m
w1
w2

x2j (5)

j=1

be re-written as A ∗ t ≤ b

···
···
..
.

kn1
kn2
..
.









· · · knm 

· · · −kn1 

· · · −kn2 
.. 
..

.
. 

· · · −knm 
···
wn

and


m
X


s1 − min
 s − min 
 2



..


.




 sm − min 


b =  max − s1 


 max − s2 


..




.


max − sm 


lb is a vector of zeros, ub is a vector

j=1

minimize
t

w| ∗ t

subject to A0 ∗ t ≤ b0

(6)

0 ≤ ti ≤ tmax , i = 1, ..., n
where A0 and b0 can be obtained from A and b by removing their last rows(w| and ) respectively. This equation is
simpler than the previous one since we remove an optimized
variable(approaching all sensor values to the targets) from the
original equation. In other words, compared to the original
problem, in this equation, we focus on minimizing the energy
usage only, with all sensor readings requiring to stay within an
accepted range. This equation satisfies the form of linear programming model, which can be solved by existing algorithms
such as interior point or simplex algorithm. The value of 
can be set as min + a reasonable threshold that is assigned
according to the real environments.
D. Adjusting AC Settings for Maintaining User Comforts and
Adapting Environmental Changes
There might exist some errors in the predictive stage. In
order to eliminate them, we add an adjustive stage in the
system that can adjust the settings accordingly so that the user
comfort levels can be maintained. Compared to the predictive
stage, the adjustive stage has some similarities and some
differences. The similarity in both stages rely on the proposed
computational model to generate AC settings. Differences are:
• Unlike the predictive stage, the adjustive stage can be
done repeatedly until the controlled area is unoccupied.
In other words, predictive stage is used for precooling the
area before the room is occupied. Adjustive stage needs
to be activated whenever users are in the area and comfort
levels are not satisfied.
• During the first adjustive cycle, impact factor mj is
selected based on initial value on sensor j, as predictive

cycle does. Afterwards, in every adjustive cycle, the
vector mj should be checked using the partial real data
points collected from sensors. If there is any change, the
vector should be updated accordingly. This update reflects
self-calibrating and self-learning features of the system,
which increases the accuracy of the computation.
When environment changes such as location changes or
season changes, impact factor mj need to be re-updated.
Data for learning the factor need to be re-collected from the
environments, but the general approaches remain the same. For
the AC control at a particular day when calibration data for
previous (similar) days have been recorded, impact vectors
should be first chosen from similar categories and updated
accordingly in the real time, similar to the update step in
the adjustive stage. For example, we wish to predict the AC
settings for today at 5pm. We first use yesterday’s mj in the
category of 5pm. After getting the results from the predictive
stage, we turn on the ACs based on the predictive settings.
If in the first 2 minutes, we detect the impact factor varies
a lot compared with yesterday’s one, we will update the mj
accordingly based on today’s data and recompute the settings.
Thus a new dataset of mj for today’s 5pm is obtained. It will
be inserted into database for future computations. If allowed,
the interval time for recalibration can be set longer for better
accuracy.
V. E XPERIMENTAL W ORK AND S IMULATION R ESULTS
A. Verification of Linear Regression Model
We performed two calibration experiments in the test room
with one AC(540 watt) and one sensor in a same day at 3pm
and 4pm respectively. For each experiment, the AC is operated
continuously for about 40 minutes and temperature changes
are recorded. The calibration follows the steps described in
section IV-B. Figure 1 shows the AC impacts on the sensor
at different time points, and their corresponding fitted lines
generated by MATLAB. From the figures, following things
are observed:
•

•
•

Both two figure can be divided into 2 parts. The first
part covers the temperature points from initial point to
24◦ C. The second part covers the temperature points from
23.5◦ C to 19◦ C. Both parts are fitted by linear regression
models with different impact factors. The regressed line
for the first part has a larger scope than the second one’s.
Figure 1(b) has similar slopes compared with Figure 1(a),
for both lines.
Two figures have similar initial sensor values. Temperature’s range is approximately from 19◦ C to 33◦ C.

From the above findings, we verify the accuracy of linear
regression model for learning AC’s impacts on sensors during
selected time intervals. The experiment also shows the calibration data can be reused for a period of time to increase
efficiency. In this experiment, during the 40-minute interval,
the AC has two impact factors on sensors. Every AC operation
cycle should select appropriate impact value for computation.

B. Simulation Experiments and Results
In section IV, the computational model is proved to be
convex, which means the result set is guaranteed be global
minimum. In this section, we use MATLAB to simulate
a multi-AC environment and compute the optimal solution.
The optimization toolbox is called Quadratic Programming.
Simulated set-up is given based on some real experimental
data, described as follows:
Assume there are 3 ACs and 2 sensors in a room. Sensors
are placed at user-sitting areas to capture inside temperature
levels. Initial values(s1 and s2 ) are both 30. Target values
tar1 and tar2 are both 26. Therefore, x1 and x2 are both
4. min is 25 and max is 28. w = (0.45, 0.46, 0.47)| , m1 =
(0.05, 0.06, 0.07)| , m2 = (0.05, 0.06, 0.07)| . Based on these
settings, we can get H, f | , Am,n , b defined in section IV-C.
These factors are needed in the Quadratic Programming toolbox.
In equation 4, weight factor  affects the final result.
It represents the tradeoff between energy savings and user
satisfactions. In this simulation experiment, we set different
values to  and hope to learn the effect of  on the final
result set. In order to better assign the values, the approximate
lowest  is learnt by solving the equation 6 using interior-point
method. For the particular example stated above, lowest  is
14.
From the graph, we can see that when  is becoming
larger, minimum value of the objective function is becoming
smaller(sensor values are more converged to the targets) while
the energy consumption is becoming larger. This matches
theoretical analysis as well: when  is larger(more flexibilities
on energy consumption), user comforts can be better optimized(more candidates are counted), therefore sensor readings
are more converged to the target values.
We also scale our simulations and do simulated experiments
on a larger number of ACs and sensors. We set n = 100
and m = 100. Min, target and max values of sensors do
not change. Assume all ACs have same operational powers(0.46kw). Impact vectors are randomly given on a 0.005base increase from 0.05, for example, if k11 = 0.05, then
k21 = 0.055 and so on. To a particular AC i, kip = kiq . We
run the above set-up in MATLAB. It quickly gives us the result
of M inV alue = 0(global minimum) and Energy = 10.72.
It verifies the scalability of our approach.
VI. C ONCLUSION AND F UTURE W ORK
To enable automated temperature control in a multi-AC
environment, under varying conditions of occupancy, weather,
seasons and other influences it is essential to have a robust
air-conditioning control system that is effective and adaptive.
Such system must be deployable in a simple, cost effective
way without the need for customizations and reprogramming
as conditions change. It also needs to create a comfortable
environments with energy savings as a goal. This paper
presents such a complete core system that can pre-cool the
room before users come back and maintain the comfort levels
with a relative small energy cost. It also clearly shows how

34

34
Collected Data
y=-0.0286t+32.6453
Collected Data
y=-0.0023t+23.8684

32

30

Temperature (celsius)

30

Temperature (celsius)

Collected Data
y=-0.0276t+32.5954
Collected Data
y=-0.0023t+23.8801

32

28
26
24

28
26
24

22

22

20

20

18

18
0

500

1000

1500

2000

2500

0

500

1000

1500

Time (seconds)

Time (seconds)

(a) 3pm

(b) 4pm

2000

2500

Fig. 1. AC Impacts on Sensors over Time

ε's Impact on MinValue
8
7
6

MinValue

Energy Cost (kw s)

ε's Impact on Energy
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14

5
4
3
2
1
0

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

14

15

16

Value of ε

17

18

19

20

21

22

23

24

25

26

27

28

29

30

Value of ε

(a) Energy

(b) MinValue
Fig. 2. Impact of  on Energy and MinValue

the control problem can be written in convex quadratic programming form, which is guaranteed to get global minimum
values. Impact factors of AC on sensors are learnt using linear
regression model. Results have been validated in both experimental and simulated scenarios, which shows the feasibility
and effectiveness of our system.
Our current air-conditioning control system only considers
temperature value as the main target for maintaining user
comfort levels. Other factors such as humidity, air flow, etc
might affect the user comforts as well. These factors should be
added into the system for computation in future. Appropriate
models need to be selected for solving the new problem. In
this paper, we assume we have a list of occupancy schedules,
which is used for the pre-cooling stage. This information needs
to be further studied in future to increase the accuracy of our
system control. With the use of the system, more data will
be collected in the database. These data can be used to better
predict the impact factors. The results will be applied into the
current system control plan. When dataset is made larger, the
system will become more robust and precise.
R EFERENCES
[1] Nest thermostat. [Online]. Available: https://nest.com/thermostat/lifewith-nest-thermostat/

[2] J. Lu, T. Sookoor, V. Srinivasan, G. Gao, B. Holben, J. Stankovic,
E. Field, and K. Whitehouse, “The smart thermostat: using occupancy
sensors to save energy in homes,” in Proceedings of the 8th ACM
Conference on Embedded Networked Sensor Systems. ACM, 2010,
pp. 211–224.
[3] A. Guillemin and N. Morel, “An innovative lighting controller integrated
in a self-adaptive building control system,” Energy and Buildings,
vol. 33, no. 5, pp. 477–487, 2001.
[4] Y. Wang and P. Dasgupta, “Designing an adaptive lighting control system
for smart buildings and homes,” in Networking, Sensing and Control
(ICNSC), 2015 IEEE 12th International Conference on. IEEE, 2015,
pp. 450–455.
[5] A. Mainwaring, D. Culler, J. Polastre, R. Szewczyk, and J. Anderson,
“Wireless sensor networks for habitat monitoring,” in Proceedings of
the 1st ACM international workshop on Wireless sensor networks and
applications. ACM, 2002, pp. 88–97.
[6] Y. Wang and P. Dasgupta, “Designing adaptive lighting control algorithms for smart buildings and homes,” in Networking, Sensing and
Control (ICNSC), 2014 IEEE 11th International Conference on. IEEE,
2014, pp. 279–284.
[7] O. Gnawali, R. Fonseca, K. Jamieson, D. Moss, and P. Levis, “Collection
tree protocol,” in Proceedings of the 7th ACM Conference on Embedded
Networked Sensor Systems. ACM, 2009, pp. 1–14.
[8] G. Mavrotas, “Effective implementation of the ε-constraint method in
multi-objective mathematical programming problems,” Applied Mathematics and Computation, vol. 213, no. 2, pp. 455–465, 2009.
[9] Wikipedia. Coefficient of determination. [Online]. Available:
http://en.wikipedia.org/wiki/Coefficient of determination
[10] MathWorks.
Quadratic
programming.
[Online].
Available:
http://www.mathworks.com/help/optim/ug/quadprog.html
[11] S. Boyd and L. Vandenberghe, Convex optimization.
Cambridge
university press, 2004.

Asynchronous Event Handling in Distributed Object-Based Systems *
Sathis Menon
College of Computing
Georgia Institute of Technology
snm@cc.gatech.edu

ParthaDasgupta
Dept. of Computer Science
Arizona State University
partha@cs.eas.asu.edu

Due to the autonomous nature of distributed computations,

Abstract

asynchronous event notificationis even more important in distributed programming than in centralized programming. Also,
since the activity in a distributed program spans a large domain
of machines, the “unexpected” occurrences are far more probable than in centralized systems. Consider the problem of unlocking shared data items in the case of the abnormaltermination
of a distributed computation. Often, it is not even possible to
know of all the locks the computation has acquired and therefore
cleaning may be impossible, from a central vantage point. Facilities thatraiseand deliver asynchronousevents to cooperating
entities greatly simplify these tasks, in distributed applications.
From an application’sperformance perspective, an important
distributed programming technique involves starting up multiple processes (or threads) to perform a task (concurrently) and
then asynchronouslynotify each other of partial results obtained
(unexpected discoveries, quicker heuristic searches, etc.) A generalized notification scheme is useful in implementing such
algorithms.
Thus we feel a complete, well designed event notification
system is of great importance when consideringthe programmability of a distributed environment. However, distributed notification is an obviouslydifficultproblem. Distributed applications
are composed of many different abstract threads of execution.
Each abstract thread of execution may span many different
machines and its present location may be indeterminable. In
addition, in DSM based systems, the current location of each
data item may be at several different sites, concurrently.Hence
the simple concept of “postingan event’raises serious questions
of ‘post it to which thread, at which site, in which object,” and
so on.
In this paper, we propose a basic set of mechanisms based on
distributed, asynchronous events. Events in our system can be
handled on a per-thread basis or per-object basis. The event
generation and notification mechanism provided by the base
kemel can be used to build system constructs such as hardware
exception notifications and user level software signals. In addition, the generality of the event mechanisms can be used to
implement a host of user services such as extemal virtual memory managers, debuggers, monitors and synchronization services.
After discussing the system model in detail, we present the
design of an event handling system, along with the semantics of
the event mechanisms. We discuss the usefulness of the design

This paper discusses the design and the operating system support necessary for providing asynchronous event handling in
distributed, passive object-basedprogramming environments,
where objects are potentially shared by disparate applications.
We discuss thenecessityof thread-based as well as object-based
event notificationand how a variety of hard to solve distributed
programming issues can be tackled by using the approach
outlined in the design.
Keywords: [Distributed Programming Environments, Passive
Objects, Threads, Concurrency,Events]

1. Introduction
Distributed programming has always been recognized as an
order of magnitude more complex than Centralized programming, and novel system structuring paradigms hold significant
promise in making this problem tractable. Much of the current
research on distributed operating systems and the associated
programming environments are concemed with simplifying the
task of distributed programming by providing constructs that
make these systems look and feel like centralized systems. To
achieve this goal, various system structuring techniques have
been tried. One such structuring involves distributed programming environments based on computation using objects (Eden
[ A h = 851, k g u s [Liskov 871, clouds v.1. [Spaf 861 and SO On).
Structuring such object-based systems using Distributed Shared
Memory (DSM) is becoming a viable paradigm paspupa 911. This
paper focuses on programming environments based on distributed objects and distributed shared memory.
Despite the advances in the design of these DSM based
programming environments, programming these systems remain overly complex. Since the complexities of the distributed
environment are masked by the DSM layer, mechanisms such
as signals that work in a distributed setting are not easily
provided. Consider the UNIX operating system where the signal
mechanism is often used to control processes (nom, terminate,
etc.). In a distributed setting, where individual objects and
threads span machine boundaries, a facility that mimics the
signal mechanism is complex to achieve. Moreover, the interaction between multiple threads of control in an object and signals
posted for the object have not been defied precisely.

*

lhis work is funded m part by NSF grant CCR-8619886.

383

0-8186-3770-6/93$3.000 1993 IEEE

Richard J. LeBlanc, Jr.
College of Computing
Georgia Institute of Technology
rich@ cc.gatech.edu

using some examples. Then we outline our implementation
strategy and finally discuss the related work in this area.

Supplement the features of the notification mechanism so that
higher level functionality such as exception handling, distributed lock management, distributed monitoring, debugging, and
so on, can be easily implemented.

2. TheModel

3. Events in a Distributed System

The model of the system that we chose is one where objects are
passive and persistent. Applications are typically composed of
several such objects, possibly spanning the network. Objects
may allow concurrent execution by multiple threads. The
threads' active inside an object may all belong to the same
application or to different applications. In the latter case, the
object is being shared by different applications.
Objects in shared memory based systems exchange control
via invocations. The calling thread invokes the desired entry
point in the called object. Invocations are similar to procedure
calls, except that they cross object boundaries (address spaces).
In the passive-object paradigm, when an object invokes anotha,
the same logical thread is used to execute the code in the called
object. The thread's attributes (such as parameters of the invocation, state of the YO connections, etc.) become visible to the
called object and are used to exchange information between the
caller and the callee. Thread attributes are a key to our design
and will be discussed in detail in subsequent sections. In the rest
of this paper we refer to this programming environment as the
"Distributed-Object/Concurrent-Thread" (DO/CT) programming environment. The DO/CT environment discussed in this
paper is based on the Clouds Distributed Operating System

In this section, we characterize synchronous and asynchronous
events and their notification and handling in the DOET environment. First, the terminology used throughout this document
is described in more detail.
An event is the occurrence of an observable (in a program
sense) activity in the system. Messages, page faults, hardware
traps, etc. are events that influence the execution of a user
program. For this discussion, we consider events as activities
explicitly generated by the user program or implicitly generated
by the system.
The act of triggering the event is known as raising (or
signaling) the event. Predefined events, which are raised by the
operating system, are termed system events. For example a
division by zero in a user program leads to the raising of a system
event by the operating system. Other system events are page
faults, alarms, hardware exceptions, etc. Application programs
may name events and raise them explicitly. Such events are
termed user evenrs. Naming an event involves registering the
name with the operating system. For example, names such as
COMMR, ABORT, SYNCHRONIZE, can be registered by an
application and raised later to communicate with its group

[Dasgupta 91, Ananth 911.

While the DO/CT environment has been well covered in
literature [chin911,the event handling problems and facilities are
not well documented. Some known problems are:
Programming environmentsthat allow concurrent execution by
several threads in one address space do not clearly specify the
semantics of signal handling. For example, the programming
environmentprovided by.OSF/l uses ad hoc solutions to figure
out which thread should be notified when a signal is posted to
the process [Doeppner 921.
Consider the signaling of an application composed of multiple
processes on a centralized system. Despite the complexity of
the application, it can be controlled (stopped, terminated, restarted, etc.) using the information contained in centralized
kemel data structures. In the DO/CT environment, the state of
an application is distributed across the local kemels running on
the individual nodes.
Conventional process or task-based signaling is no longer adequate in DO/CT environments since an object may be potentially shared by threads belonging to unrelated applications.
From the above observations, we derive the following set of
design goals:
0 Provide a generalized notification mechanism suitable for use
in a DO/CT environment.
0 Ensure @at the mechanism works identically regardless of
whether the objects are invoked using RPC or DSM.
Provide for events that are logically related to any particular
object as well as logically related to a particular computation or
sub-computation.

384

members.
Raising an event results in a notice being sent to a set of
interested recipients. Delivery of the notice eventuallyresults in
the execution of some code by its recipient (or an entity designated by the recipient), usually called the event hander. The act
of selecting the set of recipients and posting the event to the
recipients is known as evenr notiBcation.
Event notificationcan be broadly categorized as synchronous
or asynchronous,with respect to the raiser of the event. Ifraising
the event causes the signaling thread to block until it is explicitly
resumed by a handler, it is termed a synchronous notification.If
the thread raises the event but does not block, it is termed an

asynchronous notification.
Most system events use synchronous notification. For example, a page fault causes the current thread that raised the event
(implicitly) to be blocked until it is resumed after the page is
available. Not all system events need to be notified synchronously: a timer notifcation is an event that does not cause the
sender of the event (a kemel thread) to be blocked. From the user
program, it is possible to raise an event synchronously or asynchronously. Event raising may be synchronousor asynchronous,
but event delivery is always asynchronous and event handling
is always synchronous. That is, if an event is delivered to an
executing thread/process, the process is stopped at the point of
delivery (unexpected, hence asynchronous). After the handler
fiishes executing, the suspended thread is resumed or terminated (hence synchronous).

3.1. Attributes of the DOLT Environment
The design of the event handling facility is influenced by the
structural features of the target DOKT environment. These
characteristics are:
Sharabiliry:
As mentioned previously, one object may be shared by threads
belonging to unrelated applications. Events posted to a thread
should not affect the behavior of the unrelated threads inside
the object address space.
Persistence:
Objects in our model are persistent by nature and may exist
passively (i.e. without any thread associated with it). These
objects should be able to handle events posted to them, even if
there is no thread active inside them.
Dominance:
In an application composed of multiple objects, the object in
which the current thread is active may not be able to make all
decisions conceming events arising due to the thread’s execution [ L C V ~771.For example, when a thread T in object o receives
aTERMINATEevent, the cleanup activity that the event entails
may not be definable or serviceable within 0. In such cases,
other objects higher up in the hierarchy of the invocation chain
or even unrelated objects (typically, central servers) might have
to be given dominating influence over the current object’s
action.
Thread Contexts:
As mentioned previously, in a passive object paradigm, object
invocation is usually carried out by mapping the same (logical)
thread into different objects. This is unlike the RPC style of
communication where threads are bound to processes and a
remote procedure call is a sugar coating on top of message
passing. Consequently, the state of the client is not visible to
the server and any state informationrequired by the server must
be explicitly passed as parameters to the RPC call. In our view,
a procedure call, whether local or remote, must have access to
the global state of the computation. As an example, consider a
local procedure, within a process, named foo. Assume that the
process is connected to an VO channel (such as an X terminal
window). If control is transferred from foo to a procedure bar,
any output from bar also goes to the same terminal window,
without the programmer explicitly performing any redirections.
Thus, the state of the control mechanism (the thread) is visible
across all the procedures. Our environment based on passive
objects merely extends this view across distributed objects and
invocations menon. 931 by encoding threads with attributes.
Thread attributes contain information such as the connections
to the I/O channel that the thread is using, creator of the thread,
consistencylabels for the thread [chm 891, etc. Event information
is a natural addition to the attributes.

3.2. Design Choices
The above characterization of sharability of an object leads to
the obvious design choice: most events need to be handled on a
per-thread basis. Handling events on a per-thread basis allows
the applications to install customized handlers for events raised

while the applications threads’ are active inside a shared object.
These handlers remain active for the thread regardless of where
the thread is currently executing. We call these classes of handlers as thread-based handlers.
The persistent and passive nature of an object implies that
events may be posted to an object, even when there is no thread
active inside it. Thread-based handlers are inappropriate to
handle these cases and what we need are simple, conventional,
objecr-based handlers. Object-based handlers are installed for
an object and remain active while the object persists.

4. Handling Events by Threads and Objects
In this section, we describe the two classes of handlers in more
detail and provide further justification of why the two classes
are necessq.

4.1. Thread-based handlers
Thread-based handlers are used for customized handling of
events arising from a shared object. The object interface lists the
events it wishes the application to handle (thus allowing customization by the application) and the invoker of the object
attaches handler information to the thread. Once a handler has
been attached to handle an event, it remains active as long as the
thread is alive. When the event is generated, it is delivered to the
handler specified by the thread. The design allows for the
handler to be executed within the context of the object in which
the event was raised or within the context of the object in which
the handler was attached. In the former case, the handler code,
located in the thread’s private memory (calledper-threadmemory, [Dasgupta 901) is mapped in to the object’s address space and
is executed.
Allowing the programmer to decidethe context of the handler
is a powerful technique that can be used to implement a variety
of diverse mechanisms such as exception handling and monitoring. Consider exception handling where conventional wisdom
dictates that the exception be repaired (if possible) from a safe
vantage point outside the context of the signaler (Kevin 771). In
the DOET paradigm, when an object invokes another, the
invoker supplies a handler for exceptional events that the invoked object cannot handle. The handler performs any corrective action (ifpossible) and resumes (or terminates)the signaling
thread. On the other hand, consider a monitor for a distributed
application that samples a thread‘s program counter value periodically and sends the information to a monitor system. Such an
application requires a timer event to be generated and handled
within the context of the current object where the thread is
currently active. This is accomplishedby attaching a handler for
the timer event and executing the handler in the current object’s
context. Similar mechanism can be used to implement debuagers that need to have access to the intemals ofthe application
that is being debugged. Details of the monitor application are
discussed in Section 6.
In summW‘, a
Can be one of *e following:
8 An entry point defied within the scope of the thread at thepoint
the thread enables the handler. A thread can attach handler H

385

in object o for event E while it is executing in object 0. As long
as the thread is active, events of type E will be handled by H in
the context of 0 , regardless of when and where the thread is
located,when E is delivered. An extension to this scheme is one
where the handler is an entry point defined in another object.
These kinds of handlers are known as “buddy handlers” [ousterhout8i]. Buddy handlers are handlers known to the current object
in which the handler attachment is done. This is quite useful in
implementing monitors, debuggers, etc. where an application
can specify a central server as the event handler for events
posted to its threads.
A procedure defied in the per-thread area of the thread. The
compiled procedure traverses with the thread and will be made
visible within the current object in which the thread is executing. Executing within the context of the current object enables
the handler to examine and if desired, modify the state of the
objecvthread.
Information necessary to handle the event is encapsulated in a
structure called an event block and is passed to the handler. The
event block contains generic system information such as state of
the registers, etc., for exception handling and space for user
defied data structures for user events.

initialization (or later during an invocation). If any such event is
delivered to the object, the handler attached to the object is
executed. In this regard object-based event handling can be
viewed as another method of invoking operations on objects.
However, there are some differences between object invocation
and raising an asynchronousevent for an object.
First, raising the event causes a kemel thread to perform an
implicit invocation which is not semanticallyvisible as a normal
object invocation. Second, all objects have a set of predefined
system events that have defied handlers. Since these are available in all objects, it is possible to raise a system event on any
object - this is not possible with object invocations as there is
no enforced set of standard routines available in all objects.
Third, object-based events can be raised implicitly by the operating system (for example, exceptions) or explicitly by the user
program itself. Finally, the mechanism with which the invocation is carried out may have much less overhead than object-invocations. For example, a handler thread can be associated with
the object to handle all events on its behalf, thus eliminating
thread-creation costs.

4.2. Chaining of Handlers

Programming with events in the DO/CT environment requires
that it should be possible to direct events at threads as well as
passive objects. Programmers can choose the correct semantics
by using the various addressing schemes provided by the event
handling facility. In addition, synchronous and asynchronous
raising of events is necessary to use the event mechanism as an
exception handling facility as well as a general purpose communication mechanism.
In this section, we discuss the programming interface for the
specification of events and handlers for object-based and threadbased event-handling.Next, we discuss the selection of delivery
options, followedby options for synchronous and asynchronous
delively.

Each object the thread visits is free to attach its own handler to
the thread. When a new handler is attached to a thread that
already has a handler attached for the event, the new handler can
be attached in a LIFO fashion. This is known as chaining of
handlers.
A consequence of possible chaining of handlers to a thread
is that there may be multiple hierarchiesof thread-basedhandlers
eligible to handle the same event. Each object the thread traverses may attach its own handler to potential events arising from
the object it is going to invoke next. In this case, the event
information is passed along the calling chain. This scheme is
similar to the dynamic propagation of exceptions in Ada Fames
821.

5. Programming with Events and Handlers

5.1. Specifying Object-based handlers

Chaining of handlers is very useful in distributed lock management. Every time a thread locks data in an object, the unlock
routine for that data is chained to the thread‘s TERMINATE
handler. If the threads receive a TERMINATE signal, all locked
dura are unlocked, regardless of their location and scope.
In general, chaining of handlers is necessary to ensure the
proper filtering of events between neighboring objects. For
example, if an application uses three objects o b 0 2 and 03, events
arising in 03 may only be known to 03, and maybe to 02 ifretum
parameters signify that. If the event needs to be propagated back
to 01, it must be transformed to a form understandable to 01.
Using chaining, 03 can notify the handler attached in 02, which
in tum can notify the handler attached in 01.

The operating system specifies the default behavior, when
events are delivered to objects. Programmers can explicitly
ovemde the default behavior by placing handlers for events, as
part of the object specification.
The object-based handlers are registered when the object is
initialized and is part of the initialization code of the object. The
template, as applied to a sample object called my-object is
shown below:
class my-object (
private:
handler void my-del et e-handler ( even t-bl ock&) on
(

public:

4.3. Object-based handlers

I f invocable entry points
entry void init ( 1 ;

Object-based event delivery is quite orthogonal to thread based
handlers because events are delivered to passive entities (objects) instead of active entities (threads). An object can attach
event handlers to various system and user defined events upon

entry void work(int id);

386

DELETE

);

grams may use the raise system call to raise an event. Once an
event is raised by the user (explicitly) or the system (implicitly),
it needs to be routed to the appropriate location(s) where the
event can be handled. Routing the event needs a destination
address: either the name of an object or the name of a thread.
The set of valid recipients of events are:
Passive objects: persistent objects without any thread active in
them are potential targets for events. Events delivered to these
objects are handled by object-based handlers.
Current thread most system events are posted to the current
thread.
Thread groups: threads belonging to an application can form a
thread group and event posted to a thread group will be sent to
all the members of the group. This is based on the notion of
process groups [Cheriton 851.
Unrelated thread To support buddy handlers, it should be
possible to send an event to any thread.
In addition, the sender can send the event synchronously, or
asynchronously.Synchronous send will block, until it is explicitly resumed by a handler. Asynchronous send of an event does
not block the sender. Synchronous send is achieved by a variant
of the raise call, called raise-and-wait.
Addressing and blocking options are summarizedin the table
below:

The entry points added to the interface specification and their
semantic meaning are described below:
The keyword handler prefixing the names of the method
w-delete-handler specifies an object-based handler that the
object has installed to handle the event named DELETE Note
that the visibility of this handler is private, implying that this
method cannot be invoked directly. When the DELETE event is
posted to the object, the handler named in the interface will be
automatically invoked. The handler will be passed an event
block containing a kernel defined data structure. If the event
named is a user event, an optional user defined structurewill be
appended to the event block.

5.2. Specifying thread-based handlers
Handlers for thread-based event handling are attached to threads
using a system call attach-handler. The system call parameters specify the type of event and the location of the handler.
The following example shows attaching a thread-based handler, to a thread, to handle exceptions. The initialization code in
the object shown in the above example will be executed by
threads entering the object. This causes the registry of the events
and the name of the handlers, with the operating system.
void my-object : : init (
(

attach-handler(INTERRUPT, my-interrupt-handler);
attach-handler(VM-FAULT, my-server.fault-handler);
attach-handler(TIMER,

monitor-thread,

OWN-CONTEXT);

)

In the above example, the thread executing the code attaches the
handler w-interrupt-handler, which is a private method in
w-object to handle the event INTERRUPT. In the next statement, the thread attaches a handler in object instance named
w-server to handle the VM-FAULT event. Finally, the thread
attaches a compiled procedure named monitor-thread to capture the system event named TIMER. The procedure m n i tor-thread will be executed in the context of the current object
where the thread received the TIMER event.
Various linguistic mechanisms can be used to enforce restraints on the generality of the mechanisms described here. For
example, simple exception handling only requires that the invoker place handlers for handling exceptions that arise when an
object invokes another. This can be accomplishedby the following means:
Entry point signatures in the object interface specifies exceptional events raised by the entry points.
Calling object attaches handlers to these exceptional events at
the point of invocation.
Scope of the handler is restricted to its immediate caller.
Detailed description of the language interface is beyond the
scope of this paper and is available as a technical report wenon
931.

5.3. Routing of events to handlers
To use the event mechanism, the semantics of raising an event
and its subsequent delivery must be clearly defined. User pro-

387

CaU

Recipient of event e

raise ( e ,t i d )
raise ( e , g t i d )
raise ( e ,o i d )
raise-and-wai t ( e ,t i d )
raise-and-wait ( e , g t i d )

Thread tid
Threads in group gtid
Object oid
Thread tid, synchronously
Threads of group gtid,
s nchronously
dbject oid, synchronously

raise-and-wait (e,o i d )

6. Applications of Event Handling
In this section, we will illustrate the use of object-based and
thread-based handlers for applications running in DO/CT environments. First,we will outline how thread-based handlers can
be used to implement simple exception handling. Next, we will
show how thread-based handlers can be used to implement the
monitoring of a distributed computationfor liveliness. An application that requires a combination of object-based and threadbased handlers is outlined in the “distributed 42 problem.”
Finally, we will illustrate how applications can control virtual
memory operations, by setting thread-based handlers for
VM-FAULT events.

6.1. Exception Handling
Exceptions are system events that arise due to the execution of
code in an object, by a thread. In most cases, exceptions arising
while a thread is active inside an object can be handled by a
handler in the object itself.
An object may wish to take some generic corrective action
on an exception before it is propagated to the user (invoker) of
the object. To achieve this, an object can define handlers for
system events of interest, as part of the object interface. When

an exception is raised for any thread, the object’s handler gets
called and if necessary, a further exception may be raised by the
object handler, to be handled by the thread handler. The object
handler can be run using a surrogate fhreud (a thread that takes
on the attributes of the suspended thread that received the
notification) so that the context of the original thread can be
examined and modified.

Assuming that the IIC typed by the user generates an event
named TERMINATE, the termination of all the threads and
notification for all the objects requires the following:
All objects should register an object-based handler for the
predefined event ABORT. When triggered, the handler must
abort the invocation in progress for the thread named in the
event block. This causes the system to send an ABORT event
to the object at the other end of the invocation.
The root object must attach a handler for the event TERMINATE and a handler for the defied event QUIT, to the root
thread. Any subsequent thread spawned from the root thread
inherits the thread attributes (including the event registry and
the handler information). When the event TERMINATE is
triggered anywhere, the handler attached to the root object gets
notified. This handler aborts the top level invocation (causing
all objects to be notified) and raises the event QUIT to the thread
group. The handler for the event QUIT simply terminates the
thread.
The “chasing” of distributed threads necessary for termination
is done by the operating system implementationresponsible for
event delivery, as discussed in Section 7.
To reduce the complexities of the operations outlined here,
the handlers can be made part of the object’s default interface.
Details of these operations can be found in wenon 931.

6.2. Distributed Monitoring
Monitoring applications for liveliness is a difficult task to
achieve in adistributed setting. In this example, we will consider
a distributed application that is composed of a collection of
objects and a thread of computation that spans machine boundaries. We wish to monitor the application by sending periodic
information about the state of the thread (such as the current
object the thread is executing in, current program counter value,
etc.) to a central server. The central server may use the symbol
table information from the compiled objects to display the state
of the application.
To monitor the thread, two facilities are required: a periodic
timer delivered to the thread and a handler to execute when the
timer event is received. The former is achieved by using thread
attributes: a timer event is added to the thread‘s athibute list
(from within another object such as asimple monitor that simply
attaches a handler for the event TIMER for the thread). When
the thread visits another node, the thread attribute list is examined and the event registation information is recreated. This
ensures that a timer event will be delivered to the thread regardless of where it is currently executing.
The handler for the event is aprocedure that gets mapped into
the thread‘s per-thread memory area. Since this is in the same
address space as the object that the thread is executing, the
handler simply gets the suspended thread‘s state, restarts the
thread and sends the information to a central monitor.

6.4. User-level Virtual Memory Managers

6.3. Distributed ”C Problem
In this example, we outline how a distributed application can be
terminated cleanly (upon a user typing a /\c at the controlling
terminal), by using a combination of object-based and threadbased handlers.
Though the problem may appear trivial, it isn’t. Remember
that the objects being used by our application may be sharing
them with other, unrelated, applications. The list of objects
include those that the application’s threads are currently active
in, as well as objects that lie along the threads calling chain (but
may be currently passive). This is necessary so that all of the
objects get a chance to perform appropriate cleanup operations
(such as closing U 0 channels, releasing resources held, etc.).
Also, the list of threads to hunt down and terminate (lest they
tum into orphans) may include threads spawned during asynchronous invocations. In short, the list of candidates to be
notified are:
All threads belonging to the application’s thread-group.
All objects that lie in the path between the “root object” (where
the application was started) and the objects where the threads
are currently active.

388

Building user-level virtual memory managers (extemal pagers)
allows applications to bypass the strict consistency imposed by
the underlying sequentially consistent distributed shared memory. Implementing extemal pagers require extensive support
from the operating system, in terms of allowing virtual memory
operations to be performed on user level segments. Minimally,
the ability to handle VM-FAULT operations at user level, operations to install a user supplied page to back a virtual address
and specifying a segment as pageable are required operations
that should be supported by the operating system.
Thread-based handlers, along with the above mentioned
facilities provided by the operating system, can be used to
implementuser level virtual memory managers. The basic strategy is that the applications will tag regions of memory as
pageable, request VM-FAULT events and designate a server as
the handler for VM-FAULT events (buddy handler). When any
thread faults at an address, the thread is suspended and the
handler attached to the server is notified. The handler code then
supplies a page to satisfy the fault. If another thread faults on the
same memory, the server can supply a copy of the page, and later
merge the pages.
A detailed description of the application is beyond the scope
of this paper.

7. OS Support for Event Notification
In this section, we will outline the primitives that needs to be
supported by the distributed operating system, to implement
object-based and thread-based event handling. The important

The first option requires an operation similar to remote object
invocation except that the thread makes an “unscheduled”invocation to wherever the handler is located. Ifthe handler is located
in the current object, the thread does a local procedure call.
The per-thread handler requires sophisticated OS and Compiler support. The handler code has to be position independent.
Operating system must support the mapping of the handler code
into a well known address in the per-thread area of the thread.
This well known address must match the compiled virtual address of the handler.
Important issues related to fault-toleranceare not addressed
in this paper. When a notification is posted to a thread and the
thread has been destroyed, the sender of the event (if it is an
asynchronous event) needs to be notified. Leaving trails of
information regarding the death of threads create garbage collection problems (similar to the creation of “zombie” processes
on mWr).

issues to tackle are the delivery of the events to threads and
execution of the thread-based handlers.
Object-based event handling requires the operating system
to define the default action for predefined system events. Provisions to overload the default action by objects must be provided.
In addition,to supportposting events to passive objects, asystem
thread needs to be employed.To reduce thread-creation costs, it
is preferable to employ a master handler thread on behalf of a
passive object.

7.1. Delivering Events to Threads
Thread-based event handling introduces interesting problems
regarding the notification of events to threads. When an event is
posted to a thread, the system must track down the thread and
use the current informationin the thread attributetable to run the
handler. Locating a thread’s current location is similar to the
general problem of finding moving resources in the distributed
system [Ahamad 871.However, finding a thread is harder, as threads
move around much faster than other resources (such as objects)
in the distributed system.
A simple solution to finding threads is to broadcast the event
request. When the machine that has the thread active gets the
request, it can block the thread, run the handler on its behalf and
then resume or terminate the thread. However, this is communication intensive and is wasteful.
Another solution is to follow the path of the thread staring
kom its “root node” (i.e. the node on which the thread was
created). We assume that given the unique name of a thread, it
is possible to find the root node. Starting with the root node, one
can traverse the path of the thread, using information in the
system’s thread-control blocks. On a distributed system comprising of n nodes, it is possible to fiid the thread in n steps.
However, this may not find all threads if non-claimable asynchronous invocations are spawned, as the system may not keep
track of asynchronous invocations, the results of which are not
claimed.
At the expense of additional system complexity, a sophisticated thread-management system can be employed to track
down the current location of threads. On systems supporting
multicast communication, application’s threads can create a
multicast group. When a thread leaves the current node and starts
executing in another, the thread-management system can join
the multicast group. Thus, it should be possible to address each
thread by sending a message to its multi-cast group.

8. Discussion
We have presented a general purpose event handling facility for
the DO/CT environment and discussed an implementationstrategy for attaining the semantics and the mechanisms discussed.
The above facility allows:
0 The attachment of event handlers to individual threads even if
the threads cross machine boundaries.
0 The attachment of event handlers to passive objects.
0 Raising of events for a thread. The thread is located in the
system and then the thread is made to execute a handler,
irrespective of the current location of the thread and the current
location of the handler. Chaining of handlers are allowed to
provide for the propagation of events through the calling chain.
0 The implementation strategy is relatively straightforward,
given the facilities of thread creation, kemel threads, DSM and
RPC invocations and thread location facilities.
A prototype implementation is currently in progress.

9. Related Work

7.2. Executing thread-based handlers
As explained earlier, when an event is delivered to athread, there
are two types of handlers that may need to be executed.
Handler installed in the context of the object where the attachment of the event handler occurred. Altematively, the handler
could be in the context of another designated object (buddy
handler).
Handler installed in the per-thread memory of the current
thread, and within the context of the object where the event was
raised.

389

In this section, we survey the relevant work done on event
handling, in distributed and multiprocessor systems.
Most modern operating systems provide primitives for exception handling. The UNIX system provides the signal mechanism to support the delivery of system events to user programs
andraising of asynchronousevents (using the k i l l system call).
UNIX also provides a small number of user definable signals.
The entire design of the UNM signal facility is suitable for single
threaded applications only. Distributed programming by using
the RPC mechanisms do not handle signals directly.
Exception handling in a “shared abstraction” setting was
discussed by Roy Levin, in his PhD thesis [Levin 771. Levin
discussed structural conditiom and flow conditions, which are
vaguely similar to object-based handlers and thread-based handlers in our case. The distinction between these two notions was
not clearly identified in his case.

Medusa, a multiprocessor OS developed at CMU for the Cm*
[OUS~~IIIOU~, 19801, supported the notification of exceptions as internal events to the process that caused it and extemal events to any
other process that has an interest in the object in which the event
arose. Interest is indicated by either being active in the object or
by possessing the capability to it (similar to Levin’s proposal).
Candidates for receiving extemal events included other processes that possesses the capability to the object, as well as a
%Wed buddy.”
’
Due to the large number of potential handlers to extemal
events, Medusa’s (as well as Levin’s) exception reporting has
the potential to cause a tight coupling within the system. This
coupling is undesirable in a distributed system. Also, a lot of
extra work needs to be done to maintain a “current interest list,”
etc. and the event reporting hierarchy tree could grow out of
bounds, if it is not properly controlled.
The exception handling facility in Mach Plack 891 and PLATINUM po~icr]
was designed to support exception handling in a
message-based system that supports concurrency within an address space. Mach supports the posting of exceptions to tasks,
as well <asthreads. The design is based on the separation on
functionality between error handlers and debuggers. Error handlers operate within the context of the task that reported the error
event anddebuggers operate outside of this context, as aseparate
task. Mach kemel statically partitions all exceptions to fall
within one class or the other whereas PLATINUM extends this
by allowing this partitioning to be determined dynamically.
One of the key differences between our design and the work
done in systems such as Mach and PLATJNUM is due to the
nature of active object vs. passive object systems. In the former,
all threads in an object belong to the object (task) whereas in the
latter, the threads may belong to one application or many unrelated applications. Since a logical thread in a passive objectbased system spans many objects, possibly at many nodes,
customized event handling is possible by attaching events and
handlers to threads. In active object systems, application wide
event handling requires a lot of explicit coding by the programmer.

11. References
[Ahamad 871 AhamadM., A”ar,M., Bernabeau J., andKhalidi
M. Y.: Locating Resources in a Distributed System Using Multicast
Communication. Georgia Tech Technical Report GIT-ICS-87/44,
Georgia Institute of Technology, 1987.
[Almes 851 Almes, G. T., Black, A. P., Lazowska, E. D., andNoe,
J., D.: The Eden system: A technical review. In IEEE Trans. on
Software Engineering,,Jan 1985.
[Ananth 911 Ananthanarayanan, R., Dasgupta, P., Menon, S.,
Mohindra, A., and Chen R. C.: Distributed Programming With
Objects and Threads in the Clouds System. In Journal of Distributed and Multiprocessor Operating Systems,, USENIX, 1991.
[Barnes821 Barnes, J. G. P Programming in Ada. AddisonWesley Publishing Company, 1982.
[Black 891 Black, D. L., Golub, D. B., Hauth, K., Tevanian, A.,
and Sanzi,R.,: The Mach Exception Handling Facility.In SIGPLAN
Notices, Vol. 24, 1989.
[Chen 891 Chen, R., Dasgupta, P.: Linking Consistency with Obj e c m e a d Semantics: An approach to Robust Computations: In
9th International Conference on Distributed Computing Systems,
June 1989.
[Cheriton 851 Cheriton D., and Zwaenepoel, W.,: Distributed
Process Groups in the Vkernel,In ACM Transactionson Computer
Systems,Vol. 3, No. 2, May 1985.
[Chin911 Roger S. Chin and Samuel T. Chanson: Distributed
Object-Based Programming Systems. In ACM Computing Surveys,
Vol. 23, Mar. 1991.
[Dasgupta90] Dasgupta, P, Chen, R.: Memory Semantics in
Large Grained Persistent Objects. In Fourth International Workshop on Persistent Object Systems, Sep. 1990.
[Dasgupta911 Dasgupta,P., LeBlanc Jr., R. J., Ahamad, M. and
Ramachandran, U.: The Clouds Distributed Operating System. In
IEEE Computer,Nov. 1991.
[Doeppner 921 Thomas W. Doeppner, Jr. OSF/l Internals. Seminar Notes, USENIX Summer Conference,June 1992.
Fowler 921 Robert Fowler and Leonidas Kontothanassis: Supporting User-Level Exception Handling on aMultiprocessorMicroKernel: Experiences with PLATINUM. In proceedings of
SEDMS-Ill,Newport Beach, CA, Mar. 1992.
[Levin 771 Roy Levin, Program Structures for Exceptional Condition Handling, PhD Thesis, Camegie-Mellon University, 1977.
Liskov 871 Barbara Lskov, Dorothy Curtis, Paul Johnson and
Robert Scheifler: Implementation of Argus, In Proceedings of the
11th ACM Symposium on Operating System Principles, 1987.
menon 931 Menon, S: Event Handling in Passive Object-Based
Distributed Systems. TechnicalReport, GIT-CC-93/09.
[Ousterhout 811 J. K. Ousterhout, Medusa: a distributed operating system. UMI Research Press, 1981.
[Spaf 861 Eugene Spafford Kernel Structures for a Distributed
Operating System. PhD Thesis, School of Information and Computer Science, Georgia Institute of Technology, 1986.

10. Conclusions
This paper presents a generalpurpose event handling facilitythat
can be used for distributed programming. The design is based
on the notion that in distributed object-based systems where
objects are shared by different applications, event handling
needs to be based on positing events to objects and threads. The
nature of the passive object-based systempresents the view that
logical threads span objects across various machines and hence,
any customization required can be achieved by attaching attributes to such threads.
The event handling facility can be used at a “system call”
level without language modifications (though some modifications for elegance are suggested). A complete description of
defaults and other possible bells and whistles have been omitted
due to space considerations.

390

Kernel and Application Integrity Assurance: Ensuring Freedom from Rootkits
and Malware in a Computer System
Lifu Wang

Partha Dasgupta

Department of Computer Science and Engineering
Arizona State University
Tempe AZ, 85281

Department of Computer Science and Engineering
Arizona State University
Tempe AZ, 85281

Abstract
Malware and rootkits are serious security vulnerabilities, and they can be designed to be resistant to anti-viral
software, or even totally undetectable. This paper described
a hierarchical trust management scheme, where the root of
trust is in a non-tamperable hardware co-processor on a
PCI bus. The hardware checks a part of the OS kernel for
integrity which in turn checks other parts till we ensure the
entire system is free of rootkits and viruses. Our system can
detect illegal modifications to kernel, loadable kernel modules and user applications. It also provides a secure communication line for user interaction to enable legal software
updates.

1 Introduction
Malware attacks have become a serious threat over past
few years. A survey by Cybertrust’s ICSA Labs [2] found
that from January 2004 through December 2004, a rate of
virus infection increased 116% month-by-month. In Jun
2006, F-Secure Inc. reported that over 185,000 viruses exist [3]. Malware authors usually do not write malicious
code for fun, it is estimated that 85% of malware are written
purely for profit. The malware is used to steal information
or to use the victims machine for spamming.
Amongst malware, rootkits are the most dangerous threat
as they can disable all detection methods and become totally
hidden. By exploiting software vulnerability such as buffer
overflow, intruders can gain control over the host computer
and install rootkits. In the past three years, the use of rootkits technologies have grown by more than 600% [7]. Unfortunately, there is no real solution to rootkit detection.
In this paper, we present an integrated hardware-software
co-design solution. It is designed to monitor the integrity of
running software in memory from outside the machine. It
also allows legal software updates. At the lowest level is a
new PKI-based (Public Key Infrastructure) hardware device

21st International Conference on
Advanced Information Networking and Applications Workshops (AINAW'07)
0-7695-2847-3/07 $20.00 © 2007

added into host computer. This device contains two components - the “security core” called SecCore and “secure I/O”
called SecIO.
The SecCore consists of hardware connected to the host
computer via PCI bus and software running on this hardware. The SecCore forms our root of trust. The SecIO ensures that the SecCore can communicate with a user without any malware disrupting or spoofing the SecCore outputs
message or user input. We will describe the SecIO and SecCore in more detail in section 4.

2

Threat Model and Goal

In Internet Threat Model, an attacker can get complete
control of the communication media between two ends
(client and server), but the hosts running applications and
protocols are not compromisable. It assumes adversaries
living on the Internet can sniff, forge, spoof or relay the
data during transmission. Current defense schemes including IPsec (IP Security), TLS (Transport Layer Security),
SSL (Secure Sockets Layer) and S-HTTP (Secure HTTP)
prevent the threats in the Internet Threat Model. We define our threat model as Viral Threat Model. In Viral Threat
Model, malware can penetrate into host computer such that
it can gain control over the system as well as infect other
software.
Our goal is to provide a mechanism to attest the integrities of running software in the Viral Threat Model. Thus we
present a scheme to stop even the most powerful forms of
malware from running on computers, even if software vulnerabilities make is possible to inject the malware.

3

Related Work

Making kernels immune to security attacks is infeasible
due to the size and complexity of operating systems. It is
now believed that hardware methods would be needed to
handle the security threats [16]. Analysts from IDC have

forecasted by 2007, 80% of computers will be equipped
with hardware-based security devices rather than security
software. Hardware-based security [1] has been around
since mid-1990. There are hardware security devices such
as IBM’s PCI crypto card [4], Intel’s Processor ID [5], Intel’s encrypted CPU instruction sets [11], UltraSPARC’s serial numbers [12] and so on. A secure and reliable bootstrap architecture [15] uses the AGES modified security
platform to monitor the bootstrap with a trust chains. The
scheme is successful, but it only limits to bootstrap and
does not handle running system. TCG (Trusted Computer
Group) [13] finally merged major vendors and their work
into an alliance and TPM (Trusted Platform Module) [14]
is its security hardware specification. Following the specification, Intel introduced LT (LaGrande Technology) [6]
platform with TPM chip integrated. NGSCB (Next Generation Secure Computing Base) [8] is a software architecture
that runs on the TPM. The future of TPM is not clear due
to its complexity, and critics who points its primary usage
may be to enforce DRM (Digital Right Management) [10].
Our work is different from above technology in different
ways. First, we do not re-design a new VMM-like trusted
OS like NGSCB. Second, we do not separate applications
into trusted and untrusted land. Third, our design is much
simpler. The Copilot [9] is another hardware coprocessorbased solution that supports a kernel integrity monitor for
commodity systems. It can detect malicious modification
even a host kernel is thoroughly compromised. The Copilot is a more hardware-oriented approach than ours. Thus it
has no knowledge about dynamic loadable kernel modules
and applications at run time. Beside, the Copilot cannot
distinguish an update is a user-driven or rootkit-driven, so
it cannot securely perform an legal update. Our work is a
more hardware-software balanced approach, that can check
the running modules and applications as well as allow legal
updates to be performed securely.

4

Our Approach

Our approach consists of a SecCore, implemented in
hardware and a communication system called SecIO. The
SecCore stores PKI keys and run software for computing
crypto functions such as MD5 and RSA. It can access the
system memory and interrupt the host CPU. The SecIO is
physically attached to SecCore and it is not accessible by
any host software.
The SecCore is an embedded system that runs an OS and
software, and is invoked by a timer. Figure 1 shows the SecCore is a PCI device that can be plugged into PCI expansion
slot. The SecCore shares common the basic features that
are found in other crypto chips such as TPM. Unlike other
crypto chips, the SecCore requires the following three conditions. 1) Its internal resources are not accessible by the

21st International Conference on
Advanced Information Networking and Applications Workshops (AINAW'07)
0-7695-2847-3/07 $20.00 © 2007

host chipset or CPU. 2) It must be able to access a part of
host system memory. 3) It can halt or suspend the system
whenever necessary.

Figure 1: The security component

From the host OS side, when the SecCore is plugged in
and discovered, the OS allocates a non-maskable interrupt
vector, configures its software-mapped address, requests an
I/O region, enables DMA line and creates a device special
file. Communications between the host and SecCore are bidirectional. For host-to-SecCore communication, the host
requests a service by writing to SecCore’s command register and data are passed via its PCI memory. The SecCore
will update its status register when an operation is completed so the host can read. These operations typically done
by opening the SecCore device file followed by the ioctl
system call. For SecCore-to-host communication, the SecCore asserts its interrupt pin to deliver a signal to the host
CPU when it needs attention. The data transfer is the same
as in host-to-SecCore request.
In the figure 1, we show the SecIO is connected to the
SecCore. It is a small input/output device that can actually
be any kind of inexpensive I/O hardware ranging from a tiny
mono-color display and calculator-style keypad to higher
end TFT touch-screen. These devices are very common. Its
keypad contains a set of few special buttons - an “ok”, “no”
and “set”. We assume all I/O to and from SecIO is “secure”
that is not visible and tamperable by the host software.

4.1

Checking Kernel Integrity

A software integrity verification method is the computation of a hash upon initialization, which is then regularly
re-checked to ensure there has been no modification of the
software. Software integrity is a distinct identity presented
by different ways such as checksum, SHA or MD5 algorithm. In this paper, we assume software vendors may optionally provide a certificate and integrity of the .text
area for their software and this initial integrity can be di-

rectly used by checkers. If vendors do not provide anything,
the owner can still compute an initial integrity on the host
computer.
The SecCore must periodically execute its checking
function against kernel .text to ensure it is not been altered or destroyed in an unauthorized manner. Overall, every software, including a kernel and applications, are ELF
files. It is made up of one program header followed by a
number of section headers, such as .text, .data, .bss
and .symtab. These headers hold all information such
as its name, type, size, file offset, memory image starting
address and so on. In our case, we are mainly concerned
about the .text section because it is loadable and it contains the static instructions. By default, kernel .text starts
from virtual address 0xc0100000 on x86 platform. It
means bootstrap loads the kernel image at virtual address
0xc0100000, that is the physical address 0x00100000.
As stated earlier, the SecCore is able to access the host
memory. That is done by mapping PCI-shared host memory into SecCore’s space. From the SecCore’s point of view,
the host memory is like another peripheral device’s memory
buffer that can be directly mapped into its own I/O space.
Thus, using the PCI standard, the SecCore can assign a base
address to the host address decoder. This is the way to enable the SecCore can access the kernel .text.
Using SecCore to monitor the running kernel has two
limitations. It cannot verify modules and it cannot verify user processes. Modern OSes are designed to keep the
base kernel as small as possible while other services are put
in modules. A module is not an executable, it cannot be
run standalone, and it does not have an initial integrity. To
ensure the whole system is checkable, we use hierarchical
checking.

4.2

Hierarchical Integrity Checking

We extend the checking mechanism into a hierarchy such
that every running software can be covered. The concept is
straightforward and shown in Figure 2. Instead of attesting the entire host kernel .text, the SecCore only verifies a small but critical block in kernel .text. This part,
which we call SecISR, is an interrupt service routine that
will be executed by host CPU as it receives a signal from
SecCore. At this point, the SecISR becomes “root-of-trust
software”. The SecISR is actually a starter routine - it
validates and then executes a kernel checker and application checker, which are another kernel functions. The kernel checker and application checker then become the next
trusted software in the hierarchy. The kernel checker monitors the integrity of the entire kernel .text and modules,
while the application checker verifies the integrity of some
running processes, such as anti-virus software. Building up
a trusted hierarchy always requires two operations - validation, if passed, followed by execution. In the figure, a solid

21st International Conference on
Advanced Information Networking and Applications Workshops (AINAW'07)
0-7695-2847-3/07 $20.00 © 2007

arrow represents an execution flow and a dotted arrow represents a validation flow.

Figure 2: Hierarchical checking

There are several advantages in our hierarchical checking approach. First, the SecCore only knows the SecISR
and nothing else. Since the SecISR is inside kernel .text,
its address, size and integrity can be pre-determined and
kept in SecCore. Second, all checkers are software and they
are performed by host CPU. But the hierarchical checking
ensures that the checkers in the kernel cannot be modified
or disabled. In addition, it is better to offload the work from
SecCore to host CPU since a SecCore is much less powerful. Third, even though the SecCore shares host system
memory, it does not necessary mean the SecCore is allowed
to address any location out there. For example, a common
allowance in PCI-mapped memory is usually no more than
64KB.
Above scheme has one drawback. Because the checkers
run on host, the integrity data must be accessible by host
CPU, rather than SecCore. These data, both in file and in
memory, are vulnerable. As long as the checkers can access
them, so can rootkits. To protect the data from being tampered, the SecCore must digitally sign them using its private
key. Then the corresponding public key is exported to host
so that the signature can be validated at later time. However,
it leads another vulnerability that the public key can be replaced by a fake one and signature can be forged. To solve
the problem, we need to better embed the public key inside
the kernel and let the SecCore directly verify it. In figure 2,
it shows the public key of the SecCore and CA are stored
in kernel’s .bss segment in lower memory. The kernel declares it an un-initialized global variable and during boot,
the kernel reads the public keys from a file into this variable. Applying the same technique in checking SecISR, we
can have the SecCore locate the address of this variable and
compare with its own copy. Because all checkers are programmed to use this kernel variable for public key which
is protected by the SecCore, therefore the checkers can se-

curely validate the signature accordingly.

4.3

Software updates

Software updates are very common today. Every security scheme must be flexible enough to adapt any legal update, but unfortunately, there is no easy solution. For example, an owner patched anti-virus software and the installer computed the new integrity and requested a signature. At next moment, a rootkit might infect the software,
re-compute its integrity and issue another request to the SecCore. If the second case is accepted, the viral software is
totally legitimate and the application checker will never report anything. The problem is, how the SecCore can possibly distinguish between a rootkit-initiated and user-initiated
request. The SecIO is used to ensure the SecCore can distinguish between a human input and a software generated
input. Since the SecIO is directly connected to SecCore, it
provides an out-of-band secure communication channel.
Using SecIO and human-in-loop action seem to cause
extra workload, since the owner must interact with the SecCore via the SecIO at every update. However, it is not so.
We do not use human IO to verify every arbitrary process
like a.out, but only for updates to the kernel or critical applications such as anti-virus. Other software can be scanned
by anti-virus or anti-malware tools. Our approach does not
replace the current anti-malware techniques but co-exists
with them.

4.4

Attacks and Analysis

We now discuss three different types of viral attacks. The
first type is an attack or attacks to application binary. If
a virus infects an application, both the virus and infected
application will be detected by anti-virus tool. If a virus
rewrites the binary of an anti-virus tool, once the tool is
loaded into memory, it will be detected by the application
checker. If a rootkit replaces any kernel routine in order to
hide the presence of viruses, this activity will be discovered
by kernel checker. If a rootkit disables the checking by removing the SecISR, it will be immediately identified by the
SecCore. Even if an attacker launches multiple attacks at
same time and tries to break the chain, it cannot be successful as long as the SecCore is functional.
The second type of attacks compromises the initial integrity data. A rootkit cannot alter the public key of the
SecCore, because it is in kernel .bss area and directly
checked by the SecCore. A rootkit cannot tamper with the
integrity data since they are protected by SecCore’s digital
signature. It is impossible to forge a signature and replace
the public key in memory without be detected by SecCore.
It is also impossible to trick the software checker reading
another fake public key elsewhere without modifying its binary.

21st International Conference on
Advanced Information Networking and Applications Workshops (AINAW'07)
0-7695-2847-3/07 $20.00 © 2007

The third type of attacks makes the system accept some
illegal updates. A rootkit cannot trick the SecCore sign anything unexpected, since the owner’s confirmation is always
requires. A rootkit cannot forge an output message to host
display and trick the owner to sign a different integrity, because the message shown on host display and SecIO are
different. Finally, a rootkit cannot forge an owner’s input,
because it has no way of controlling the SecIO.

5

System Implementation

Our host computer is a widely used PC with Pentium
4. The security subsystem is simulated by SlotServer 3000,
from OmniCluster Inc. This particular model is based on
Intel Pentium III CPU and VIA Apollo Pro266T chipset.
The north bridge chip connects 2xAGP video (16MB video
RAM), and 256 MB system memory, while the south bridge
chip supports USB ports, IDE controller, super I/O chip,
10/100 Ethernet chip and audio. The SlotServer is a single
board computer and mainly used for web server, firewall or
distributed systems. Thus we have two computers, one running the OS and the other running the SecCore and SecIO.

5.1

PCI System Architecture

In general, most of the PCI devices can at least supports
three types of resource sharing - PCI-shared memory, interrupt pin and bus mastering. The PCI bus is the most
common architecture in consumer computers that allow different peripheral devices attaching to a platform. The PCI
specification covers the electrical characteristics, bus timing and protocols. One of its important features is that it
has a slot based configuration space so that each PCI device has different address decoder. This PCI-shared memory is physically located inside the physical device but the
host can access it just as if it accesses the main memory.
For example, when the CPU issues a read from PCI-shared
memory region, the data are actually located inside the PCI
device. In our case, the chipset sees the SecCore as a regular
PCI device. The host CPU will assign PCI-shared memory
space to it. Similarly, The SecCore also sees the host computer as a PCI device and therefore it can access the host
memory. Using the PCI-shared memory in this matter is
not a new technology. Several of commercial products have
merged the PCI-shared memory from each PCI SBCs (Single Board Computer) into a tightly coupled cluster. In their
cluster, the inter-node communications are going through
PCI-shared memory across PCI bus.
The SecCore needs a mechanism to communicate with
host CPU. Using hardware interrupt is the most common
way to notify the host CPU to execute the corresponding interrupt handler. To share the interrupt line and number, each
device including the SecCore must be assigned a mutual exclusive interrupt number to avoid conflict. Finally, the bus

mastering feature is optional. This technique enables the
bus controller to communicate directly with other devices
without CPU’s attention. It allows a device to be a master,
so it can drive data bus and directly read from/write to memory bank and control signals. DMA is a simple form of bus
mastering where the I/O device is set up by CPU to access
memory block and then signal CPU when I/O is completed.

5.2

Software Integrity Builder

The ELF (Executable and Linkable Format) file is the
most common executable format in Linux systems. There
are three types of ELF object files - executable file, relocatable file and shared library. Each ELF file is composed
of one ELF header holding the roadmap of file structure, followed by program and section headers. The section headers focus on where various parts of the program are within
the file, while the program headers describe where and how
these parts are to be located in memory.
If no integrity is provided from vendor, an initial hash
needs to be built right after the executable is generated or
installed. We developed a routine, called software integrity
builder (or simply builder) to scan the ELF headers of a
given software, locate its code segment, and then compute
the MD5 hash. The checksum function is taken from Linux
utility md5sum.c and specialized for ELF format. The basic logic of building user-level software and kernel are almost identical.
At execution, the sys exec routine is called which
loads the code into memory frames and the linker
script linux/arch/i386/vmlinux.lds sets the kernel code start from 0xc0100000 by default and sets the
values such as start stack, brk, start code of the
process’s memory descriptor, and invokes start thread
macro to modify registers so that eip will point to the entry point of the program interpreter and esp point to the
new user stack, respectively. These data are essential for the
checkers to locate the software image in virtual memory.
Figure 3 shows a static memory instance of an executable
and in-memory image. The left side shows an ELF executable processed by builder while generating an initial integrity. The right side shows a simplified running image in
memory that will be attested by software checker. During
the attestation, the checker verifies the signature, uses the
data to locate its .text and compare its integrity.

5.3

SecISR Routine

Rather than inventing a new interrupt vector, we embed the SecISR inside the system timer ISR (Interrupt Service Routine). The modified timer ISR and its routines
are shown in Figure 4. An IDT (Interrupt Descriptor Table) is an array of descriptor that is used to associate interrupt and exceptions. Each IDT entry stores an inter-

21st International Conference on
Advanced Information Networking and Applications Workshops (AINAW'07)
0-7695-2847-3/07 $20.00 © 2007

Figure 3: Integrity building and checking

rupt handler’s address. The IDT is allowed to store consecutively anywhere in memory and the kernel uses a special register idtr to keep the base address. When an interrupt occurs, the CPU loads idtr and uses the interrupt vector to locate the entry point, and in this case, it is
IRQ0x00 interrupt. Each interrupt vector has its own
ISR, but they all share some common design and routines.
Linux separated an ISR into top and bottom half. The top
half is for the most time critical portion and bottom half is
for non time critical portion that can be deferred. The kernel
uses the generic facility do IRQ, handle IRQ event,
tasklet hi schedule and do softirq to execute
top half and schedule the bottom half to run. In the Figure,
the last function in timer ISR is our wake up checker,
that starts a number of software checkers.

Figure 4: Modified timer interrupt handler

There are few interesting properties. First, the modified timer ISR can work with or without SecCore. If
a system does not have the SecCore plugged in, it still
runs as timer runs, and the SecISR becomes the rootof-trust. However, since the SecISR can be tampered

by rootkits, not having SecCore is not secure. Second,
wake up checker needs to verify the checkers before
invoking them. The integrity data are stored in local file.
It means wake up checker must perform additional file
I/O operation. Third, wake up checker does not run the
checkers in the timer interrupt context. Instead, we created
a new thread for software checkers. Thus, the checking activity will not impact the performance in timer ISR and the
checker can block on their own events.

5.4

Software Checkers

We implemented four software checkers based on their
particularity and specialization. They are 1) system call
checker, 2) module checker, 3) kernel checker, and 4) application checker. The syscall checker is responsible for
verifying all system calls and entries. System calls are the
most popular target to attack by today’s rootkits. The syscall
checker first attests the sys call table array where address of system calls are kept. Next, it validates the integrity
of the .text for each system service routine. Although
these data are part of kernel, the advantage of separating
it from other checking makes it easy to identify the exact
compromised syscall entry.
The module checker is a bit different from others. Kernel modules are linked into the kernel by executing the
insmod user program. Linking a module requires a user
process to interact with the kernel service back and forth
few times. The fundamental challenge is that modules do
not have an initial integrity like others. Therefore, in addition to attestations, the module checker needs to perform
more work. First, when a module is inserted, the checker
needs to compute its integrity and save it in a list. We modified the sys init module and sys delete module,
so they insert and remove an integrity as a module is loaded
and unloaded. Second, in case of deletion, the checker
is called to remove an entry from the list. It inspects the
caller’s return address in the stack frame to make sure only
the sys delete module can do so. This prevents some
rootkits from directly modifying the kernel module list
data to hide themselves.
The kernel checker will verify kernel code segment in
.text and .text.init. We skip about first five pages
in .text.init section where it contains data such as
idt, gdt, swapper pg dir and so on.
The application checker runs in a kernel thread. It walks
thru the kernel process table, and if a registered application
is found to be checked, it indexes the process’s VMAs from
task struct->mm->mmap followed by another round
of translation from VMA to kernel virtual address using the
page table entry pgd. The rest of checking operations are
identical to other checkers.

21st International Conference on
Advanced Information Networking and Applications Workshops (AINAW'07)
0-7695-2847-3/07 $20.00 © 2007

6

Evaluation and Performance

To evaluate our model can detect the rootkits attacks, we
took a few well known real-world kernel rootkits. These
rootkits are obtained from public source domain and all are
not modified. Adore and Adore-ng come by kernel module that they intercept the execution flow by altering system
call table and virtual file system. SuckIT presents a different attack, it is loaded through /dev/kmem by writing to
the memory special file. Our checkers successfully detected
the attacks when Adore replaced system call table and when
SuckIT rewrote the raw memory. The checker also detected
an attack when Adore-ng removed itself from module list,
however, the checker did not recognize Adore-NG illegally
altered the VFS function pointers. We will discuss this limitation in section 7. Beyond the known rootkits, we developed our own synthetic rootkit that can arbitrarily replace
the memory contents in any location. Because our detection
scheme is not known to real-world rootkits, thus we instrumented our rootkit utility to randomly modify the .text
area of given user processes, kernel routines, checkers and
even SecISR. The binary rewrite actions were all successfully detected by different checkers. A summary of above
rootkits attacks and outcomes are depicted in Table 1.
To verify legal software updates, we modified the SecISR and a kernel routine. After the new integrity is legally
signed, the SecCore and SecISR could successfully adapt
the updates and start using these new values. Finally, in
performance tests, we configured our system to execute the
checking about every five seconds. No measurement was
taken on SecCore side because its operations do not interfere with the host CPU. we measured that the performance
impact on host computer is small. During several measurements, the checkers completed in one to five jiffies. In case
of large applications such as database, there would be some
extra cost for kernel to page-fault all missing pages and
bring them in due to user programs are page-on-demand
based. In a system where memory usage is high, additional swap-in and swap-out are required, that could certainly worsen the system performance. However, it does not
happen to kernel checking because kernel itself will never
be swapped out. Running the checking every five seconds is
simply a random choice because there are no known rootkits or viruses that repeat loading and unloading themselves
within every five seconds.

7

Open Issues and Limitations

There are two limitations in our model. First, our integrity check does not include the data section (.bss and
.data). Some programs store pointers to function calls in
data variables. Therefore, it is possible to alter data pointers
without being detected; an example is the Linux VFS (Vir-

Name
Adore 0.34
Adore-NG 1.31
SuckIT 1.3b
Our Utility

Target
Type
Source
Syscall and table Kernel
Loadable kernel module
VFS
Kernel
Loadable kernel module
Syscall handler
Kernel
Raw memory access
Any functions
Kernel/User Raw memory access
Table 1: Common Rootkits and Detection Results

tual File System) which binds a data pointer to proper operations associated with that file system at runtime by storing
the pointer as data. We currently do not have a solution
for this open problem. From the SecCore’s point of view,
it cannot attest the data section, since the data are dynamic
and they are constantly changing. The OS should have a
mechanism to validate the VFS data pointer and execution
flow, because an external device could never know how to
properly handle them. Second, Our prototype implementation is tightly bound to Linux kernel and ELF format. Other
OSes and execution types are not supported, but we believe
it is feasible to migrate across platforms.
We also have two open issues to discuss. The first concern is about the social engineering. Despite of whatever
robust security scheme is deployed, computers can still be
very vulnerable if the owners voluntarily install malware
or permit malicious update. In software tamper resistance
world, solutions must take computer owners, software and
hardware vendor all together alone with security mechanism. The second concern is the ease-of-use concern. This
is especially true in consumer domain. Our security mechanism automatically runs in background and is unobtrusive.
Also the overhead cost is minimal. However, user interaction is really unavoidable in some cases such as updates.
Computer owners should be aware of the importance of this
defense line and accept the extra human-in-loop inconvenience.

8

Conclusion

This paper has presented our hardware-software codesign for software integrity assurance. Attesting software
integrity is a difficult but critical task to build a secure computing environment. Running on separate hardware, the
SecCore and its verification functionality remains in place
even when the host kernel is thoroughly compromised. The
SecCore is time-driven, independent, and self-sufficient,
and it activates other software checkers on a regular basis
to build up a hierarchical trusted chain. The SecIO is exploited to interact with the owner for an authentic message.
This out-of-band I/O device along with the SecCore provide
a flexible and robust security solution on software integrity
assurance. We also demonstrate a prototype implementation of our design approach. The security hardware is simulated by PCI SBC device, and the software on SecCore
and host computer are a patched Linux kernel. We added

21st International Conference on
Advanced Information Networking and Applications Workshops (AINAW'07)
0-7695-2847-3/07 $20.00 © 2007

Result
Detected
Detected
Detected
Detected

functionalities to support PCI interconnection, and we also
developed routines to build an initial software integrity and
compare it with the running software in memory. Our prototype demonstrates both feasibility and efficiency with the
hierarchical checking.

References
[1] B. Yee. Using Secure Coprocessors. PhD thesis, Carnegie
Mellon University, 1994.
[2] Computer virus prevalence survey, icsa labs 10th annual.
http://www.icsa.net/icsa/docs/html/library/whitepapers
/VPS2004.pdf.
[3] F-secure’s data security wrap-up for january to june 2006.
http://www.f-secure.com/2006/1.
[4] IBM PCI Cryptographic Coprocessor.
http://www03.ibm.com/security/cryptocards/overproduct.shtml.
[5] Intel
Pentium
III
Processor
ID.
http://www.intel.com/support/processors/pentiumIII.
[6] Lagrande
technology
for
safer
computing.
http://www.intel.com/technology/security.
[7] McAfee Inc. Rootkits, part 1 of 3: The growing threat. White
paper, 2006.
[8] Microsoft Next-Generation Secure Computing Base.
http://www.microsoft.com/technet/archive/security/news/
ngscb.mspx.
[9] N. Petroni, T. Fraser, J. Molina and W. Arbaugh. Copilot: A
coprocessor-based kernel runtime integrity monitor. In 13th
USENIX Security Symposium, Aug. 2004.
[10] Not tcg debate. http://www.notcpa.org/.
[11] S. Hedberg. Hp’s international cryptography framework:
Compromise or threat. IEEE Computer 30, 1997.
[12] Sun microsystem. http://sun.com.
[13] Trusted
Computing
Group.
http://www.trustedcomputinggroup.org/.
[14] Processors get hardened - security concerns are mandating the enhancement of on-chip protection, 2004.
http://www.trustedcomputinggroup.org/home/409153.pdf.
[15] W. Arbaugh, D. Farber and J. Smith. Secure and reliable
bootstrap architecture. In IEEE Symposium on Security and
Privacy, May 1997.
[16] Y. Wang, C. Verbowski, H. Wang and J. Lorch. Subvirt: abc
implementing malware with virtual machines. In 2006 IEEE
Symposium on Security and Privacy, 2006.

Tackling Congestion to Address Distributed Denial of Service: A Push-Forward
Mechanism
Srinivasan Krishnamoorthy and Partha Dasgupta
Computer Science and Engineering Department
Arizona State University
Tempe AZ 85287
Abstract—Distributed Denial of Service attacks prevent
legitimate users from accessing a target machine or the service a
target machine provides. One common method of attack is
overwhelming the target machine with a large volume of traffic.
Thus, handling congestion indirectly leads to detection and
recovery from Distributed Denial of Service attacks.
The Internet is an interconnected collection of Autonomous
Systems. Every host on an Autonomous System connects to the
Internet through an Access Router. Monitoring the rate of
packets to and from a host, at the Access Router, helps in
identifying Distributed Denial of Service attacks initiated at the
host. Monitoring every Access Router leads to an effective
Distributed Denial of Service prevention, but is infeasible. An
alternative is a combination of Access Router monitoring and
Intermediate Router monitoring with a novel Push-Forward
mechanism that provides good defense within manageable
deployment requirements. Push-Forward messages reduce the
amount of traffic to monitor at the Intermediate Routers.
Prototype testing and simulations of such a combination reveal
good congestion detection and recovery time with very little
performance overhead.
Index Terms—Network Security, Distributed Denial of Service.

I. INTRODUCTION

D

enial of Service (DoS) attack is defined as an “explicit
attempt by an attacker to prevent legitimate users of a
service from using that service” [1]. Distributed Denial of
Service (DDoS) attack is an extension to DoS where the attack
originates from distributed sources. In DDoS attacks, one
target machine is attacked from a large number of
collaborating attack machines. DDoS attacks have become a
prevalent networking issue that requires a distributed,
coordinated detection and recovery mechanism. Detecting or
preventing a DDoS attack is hard because the traffic looks
normal; the attacking packets are no different from the normal
packets.
A. Background
DDoS attacks can be classified as logic or brute-force
attacks. Logic attacks exploit protocol vulnerabilities, and can
be easily prevented by fixing the exploited bugs. Brute-force
attacks use normal looking packets to overwhelm the target
machine. Defending against brute-force attacks is tougher.
There are many solutions to defend against these attacks.

IEEE Communications Society
Globecom 2004

Based on their deployment location, defense mechanisms can
be classified as Target-end, Intermediate network and Sourceend solutions [2]. Target-end mechanism deployed near the
victim can identify DDoS attacks easily, because they can
view the aggregate attack traffic, but tracing back to source of
the attacks is hard and the amount of traffic to analyze is large.
Intermediate network solutions, deployed at the core network
infrastructure, have trace back and deployment problems.
Source-end solutions, deployed at the edges of the Internet
(access networks), too have deployment problems; but tracing
back to attack sources is easy and amount of traffic to analyze
is minimal. We propose a solution to detect and recover from
brute-force attacks that deploys at both the Source-end and
Intermediate network.
B. Motivation
The Internet is a collection of Autonomous systems. Every
host in an Autonomous System has to connect to the Internet
through an Access Router, thus a natural location to deploy a
Source-end solution. Since the traffic to be analyzed by the
Access Router is less, a sophisticated mechanism can be
deployed for Source-end solutions.
A survey conducted by Moore identifies 94% of the attack
traffic as TCP based [3]. Thus traffic measurement between
source and destination machines at the Access router provides
a good indication of disproportionate flows. These flows can
be congestion causing flows at the target machine and in turn
can be attack traffic.
C. Related Work
There has been a lot of research in the area of detection,
recovery, prevention and traceback of DDoS attacks. The
mechanism in this paper describes a detection and recovery
technique with the added advantage of easy traceback to the
sources of the attacks.
The classic way to prevent a network based attack is
configuring a firewall [4] to filter incoming and outgoing
traffic. Ingress / Egress filtering falls under the above
mentioned methodology [5, 6]. In [7], Park and Lee propose a
solution to DoS prevention using route-based filtering. Routebased distributed filtering uses routing information to
determine if a packet is valid with respect to its
source/destination addresses. Such a technique requires the
routers to know the topology of the network and they fail to

2055

0-7803-8794-5/04/$20.00 © 2004 IEEE

protect against more sophisticated attacks while restricting the
network activities.
Flooding Detection System (FDS) [8], installed on leafrouters for DDoS detection can classify the TCP control
packets (SYN/ACK/FIN/RST) and the TCP data packets. By
design, SYN and FIN packets are normally paired. But during
a SYN attack, the number of SYN packets is very high
compared to FIN packets in the leaf-routers to which the attack
machine is connected. This approach can only detect SYN
flooding.
WADeS [9] detects a DDoS attack using Wavelet methods.
It captures the high bandwidth flows followed by computation
of wavelet variance on the traffic, because the attack traffic
would produce changes in the wavelet variance. It also
combines the thresholds to enable attack detection.
ICMP Traceback [10] relies on ICMP traceback messages
that are router-generated. These messages are used by the
victim to reconstruct the attack path. Since the messages are
generated with a low probability, messages could be dropped
due to attack traffic congestion. This makes it hard for the
victim to reconstruct the attack path.
There are additional IP based traceback mechanisms [11, 12,
13] that are used for DDoS attack traceback.
CenterTrack [14] is an overlay network, which tracks certain
packets through IP tunnels and uses that traffic information to
decide if there is an ongoing attack. They require special
tracking routers and they are deployed within an ISP’s network
which limits their traceback to just the local network.
“Pushback” [15] introduces additional functionalities at
routers to monitor the traffic at the routers. They are deployed
near the target machines. Once an attack is detected, the attack
traffic is identified and rate limited. This information is then
sent upstream and upgraded routers act upon the pushback
message. The disadvantage stems from the fact that the
deployment is near the target and by the time the attack is
detected the network is already flooded with attack traffic.
D-WARD is a Source-end solution to detect and recover
from DDoS attacks. D-WARD proposes a pure Source-end
monitoring system that detects and throttles outgoing attack
traffic from a network that has deployed this system.
D-WARD selectively imposes rate limiting on offending
traffic with a self-regulating reverse feedback system [16].
This solution requires heavy user involvement. It suffers from
the deployment issue because it is a pure Source-end solution
and it doesn’t handle legitimate congestion problems.
MULTOPS is another Source-end solution that uses a data
structure to keep track of the traffic pattern at a source router
[17]. This data structure, Multi-Level Tree for Online Packet
Statistics (MULTOPS), can be used with other detection
mechanisms to provide improved results. MULTOPS is not a
complete defense mechanism in itself; it can be used to
complement our solution.

II. REQUIREMENTS AND GOALS
A. Requirements
Our research problem can be stated as “to address DDoS
using congestion control and recovery mechanisms.” Some of
the features that we wanted our solution to exhibit are: a
lightweight router based solution; a solution that does not
require changes to end-user machines; a solution that is
deployed incrementally at the Source end and Intermediate
network and a solution that will contribute very little overhead
at the Intermediate routers.
B. Goals
Our goals to provide an effective DDoS solution can be
summarized as: a) Identify a victim’s unavailability due to a
congestion causing attack traffic, near the source of the
congestion using Access Routers; b) Rate limit such
congestion causing traffic at the Access Routers; c) Alert
downstream intermediate routers about the congestion causing
traffic using the Push-Forward mechanism and d) Rate-limit
such traffic at the intermediate downstream routers to handle
distributed congestion causing sources.
III. DESIGN
The above listed goals are realized using three main
mechanisms in our solution. They are the a) Access and
Intermediate Router Monitoring; b) Rate Limiting and c) PushForward mechanism.
A. Access and Intermediate Router Monitoring
For the Access Router monitoring, the router on a source
(access) network is upgraded with some additional
functionality to determine congestion at the victim. The idea is
to measure the traffic to and from various destination machines
and the source machines. We use this measurement to identify
the unavailability of the destination machines and classify such
machines as suspected victims. This classification is fairly
simple if the traffic is bi-directional, since there is a defined
ratio between the to and fro traffic between the source and the
destination. If the ratio is not maintained then we can classify
the destination as a victim and the flow as congestion causing
traffic. If the traffic is unidirectional, then we use a history
mechanism to classify the flow as congestion causing traffic.
The Intermediate Routers handle traffic from a set of Access
Routers. Thus the volume of traffic handled by Intermediate
Routers is huge. Analyzing each and every packet that flows
through the router puts a lot of overhead on the Intermediate
Routers. Hence, Intermediate Routers analyze only traffic
flows that are suspect. Information about suspected flows is
received from Access Routers that have identified congestion
causing traffic at their access networks. This information is
propagated to the Intermediate Routers using Push-Forward
messages.
B. Rate Limiting
The congestion causing traffic that is identified by the

IEEE Communications Society
Globecom 2004

2056

0-7803-8794-5/04/$20.00 © 2004 IEEE

monitoring mechanism is rate limited. Rate limiting basically
drops packets based on a predefined formula. The rate limiting
is similar at both the Access and the Intermediate routers.
Information about both dropped and forwarded packets are
logged so that this information can be used as a feedback
mechanism to decide if a flow is to be rate limited in the next
interval.
C. Push-Forward mechanism
We have introduced a novel mechanism to handle
distributed congestion causing sources. This mechanism
addresses the deployment issues of a Source-end solution and
reduces the overhead at the Intermediate routers.
Once an Access Router identifies potential attack traffic, it
alerts the downstream routers using a Push-Forward message.
This message alerts the downstream routers of specific suspect
flows that have to be monitored. This takes care of flows that
might originate from an access network that did not deploy our
mechanism. Thus distributed attack sources or flows are
caught at the Intermediate Router, which has upgraded.
Only Access Routers generate the Push-Forward message.
The Push-Forward message is addressed to the suspected
victim. Route prevalence in a network path studied by Paxon
in [18], gives us the confidence that the Push-Forward message
will follow the same path as that of attack traffic.
The Push-Forward mechanism helps keep our design simple
because the Access Routers need not know which downstream
Intermediate Routers have upgraded. Push-Forward messages
initiate monitoring only on upgraded Intermediate Router.

Fig. 1. shows the system visualization. The circles at the
bottom level represent the Access Routers. Other routers are
the Intermediate Routers. Darkened circles represent the
upgraded routers. A1, A2, and A3 are the distributed attack
sources. A1 is caught at the Access Router R1 because R1 has
our upgraded mechanism. A1 generates the Push-Forward
message to alert and hence initiate monitoring at the
Intermediate Routers R2 and R3. The Push-Forward message
is represented by the arrow marks. As a consequence of the
Push-Forward message, A2 is caught at R2 and A3 is caught at
R3.
IV. ARCHITECTURE
The four main modules to handle the various functionalities
of the solution are detailed below.
A. Collection Module
This module logs traffic measurement statistics. The logged
details are the destination IP, source IP, the protocol, the size
of the packet, etc. Since the packet headers are already used for
forwarding look-up, gleaning and logging the same
information from the packets do not add substantial overhead.
B. Statistics Module
The Statistics module periodically parses through the log
generated by the Collection module. The Statistics module
classifies flows that are disproportionate as congestion causing
flows. This decision uses two different methods based on the
traffic type. If the traffic is bi-directional, the module looks for
the ratio between the requests and responses between the
source and the destination. If this ratio is not maintained, then
it classifies the flow as congestion causing. If the traffic is
unidirectional, then the module compares the flow statistics
with the history. This history is generated periodically and is
used as a benchmark that unidirectional flows have to adhere
to.
C. Drop Module
Drop module rate limits the flows that are identified as
congestion causing by the Statistics module. The Drop module
checks and matches each packet with its set of non-conforming
flows. If the packet belongs to one of the flows then the
module decides whether to drop it based on a predefined
formula for rate limiting.

Fig. 1. System visualization.

IEEE Communications Society
Globecom 2004

D. Push-Forward Message Generation Module
This module is responsible for generating the Push-Forward
messages from the Access Routers. Access Routers alert the
Intermediate Routers with the information contained in the
Push-Forward message. The message contains information
about suspect flows, like the destination IP, source IP and the
Protocol of the suspected flow. This message is a simple UDP
packet addressed to the target machine with the IP Router
Alert option set. The Router Alert option forces every
downstream router to deep inspect the packet. Upgraded
Intermediate Routers take note of the information contained in

2057

0-7803-8794-5/04/$20.00 © 2004 IEEE

the packet and other normal Intermediate Routers just forward
the packet without any effect. Thus the Push-Forward
messages initiate the monitoring and rate limiting of suspected
flows at the upgraded Intermediate Routers.
V. IMPLEMENTATION
Our design prototype was implemented on a Linux router
and few main implementation aspects are discussed below.
A. Access Router Implementation
The Collection module implemented as a Linux Kernel
module, residing in the IP layer, logs various information
about every packet that flows through the router. The module
collects packets in intervals of 5 seconds and stops every cycle
to refresh its Drop table. Thus the amount of traffic monitored
is nearly 95%. The Statistics module is a user level process
that periodically parses through the log and generates statistics
of every flow. The parsing happens in O(n) time, ‘n’ being the
number of packets logged in the previous interval. This
statistics is stored in a hash and contain information like the
number of requests seen in the previous monitoring interval,
the number of replies, the number of packets dropped due to
rate limiting, cumulative size of packets, etc. If the flow is bidirectional, the following logarithmic formula is used to decide
if the flow is a congestion causing flow.
Allow_Requests = Replies * (1 + 1 / log (Replies) )
Any flow whose (Requests + Drops) > Allow_Requests is
classified for rate limiting. Here Requests, Replies and Drops
denote the corresponding number of requests, replies, and
drops seen for that flow in the previous monitoring interval.
Information about the identified flows is updated to a Drop
table, which is hashed for easy lookup and is used by the Drop
module for rate limiting. The flows are identified using the
destination IP, source IP and the protocol. The Drop module,
which is a LKM, decides to either accept or drop a packet
belonging to an identified congestion causing flow, using the
following policy:
Packet Count < Min – Forwarded.
Min < Packet Count < Max – Dropped randomly.
Packet Count > Max – Dropped.
where, Packet Count – Current No. of requests in this cycle;
Min – No. of replies seen in previous cycle; Max –
Allow_requests computed for this cycle.
Push-Forward message generation module sends the
information about rate-limited flows to downstream routers
using a simple UDP packet, whose Router Alert (RA) option is
set and is addressed to the target machine. The reserved field
of the RA option contains the protocol information of the
suspected flow.
B. Intermediate Router Implementation
As per our design, under normal circumstances, the
Intermediate Router does not do any monitoring. This reduces
the overhead of monitoring normal traffic at the router. The
Intermediate Router starts its own monitoring and rate limiting

IEEE Communications Society
Globecom 2004

activities only after it receives a Push-Forward message from
one of the Access Routers upstream. For this purpose, the
Intermediate Router maintains a Monitor table that contains
information of flows that have to be monitored, which is
updated by the Push-Forward messages. Only flows that match
one of the entries in the Monitor table are monitored. Then the
Intermediate Router works autonomously similar to that of an
Access Router but do not generate Push-Forward messages.
VI. RESULTS
Our prototype was tested in Intel Pentium 4 desktops
running Linux kernel 2.4.20 at 2GHz. We use five machines,
one as the Access Router, one as the Intermediate Router and
the other three as host machines. We have four functional tests,
three to test the functionalities of the Access Router and one to
test the Intermediate Router and the Push-Forward mechanism.
A. Target Unavailability Test
This test establishes the ability of an Access Router to
identify victims whose services are down. Though the
connection to the victim is available, the victim does not
respond to the traffic from the source machines. This test also
shows that the Access Router rate limits only the deviating
flows. Fig. 2. shows that the flows from machine A to victim V
is rate limited once the Access Router identifies a problem at V
at interval 4. Thus we see a drop in traffic to a predefined
threshold. The other traffic from A to another normal
destination D remains unaffected. We also see that once V
comes out of the congestion, the traffic is brought back to the
normal level in a controlled fashion.

Fig. 2. Target Unavailability Test.

2058

0-7803-8794-5/04/$20.00 © 2004 IEEE

Fig. 3. Service Unavailability Test.

Fig. 5. Intermediate Router and Push-Forward Test.

Fig 6. Simulation.

from A to a victim V. But one flow produces four times the
traffic of another flow. Once the Access Router detects a
problem at V, rate limiting begins and the heavy traffic flow is
rate limited more than the normal flow, hence exhibiting a
feedback property between the victim and the Access Router.

Fig. 4. Fair-share Rate Limit Test.

B. Service Unavailability Test
Here we test the ability of the Access Router to identify a
specific service that is unavailable at the victim. Thus the
Access Router rate limits only the flow whose service is
unavailable and does not affect a flow using another service at
the same machine. Fig. 3. shows that there are two flows from
A to victim V and one flow from another user U to V. Since
only services using TCP goes unavailable at V, only the TCP
traffic is rate limited by the Access Router.
C. Fair-share Rate Limit Test
This test is used to establish the ability of the Access Router
to identify flows that carry heavy traffic and rate limit flows
proportionally, when there are multiple similar flows to the
victim. The chart in Fig. 4 shows two traffic flows. Both are

IEEE Communications Society
Globecom 2004

D. Intermediate Router and Push-Forward Test
This test proves that a suspect flow not caught at its own
non-upgraded Access Router, is identified and rate limited at
the Intermediate Router, because some other upgraded Access
Router has alerted the Intermediate Router with a PushForward message. The chart in Fig 5 shows the traffic being
monitored and rate limited at the Access and Intermediate
Routers. Once the Access Router identified a deviating traffic,
it rate limits such a traffic and alerts the Intermediate Router.
Now the Intermediate Router begins its own monitoring and
identifies similar deviating flows flowing through it and rate
limits those flows. The monitoring, rate limiting and recovery
at the Access Routers and Intermediate Routers are
independent and autonomous.

2059

0-7803-8794-5/04/$20.00 © 2004 IEEE

Simulation
We have conducted large-scale tests and cost-benefit
analysis using Network Simulator 2. We construct an Internet
like network with the basic characteristics of the Internet using
Inet software [19]. In one of our tests, we have an 800-node
network, 695 of them Access Routers and 105 of them
Intermediate Routers. We randomly select 100 Access Routers
as having attackers under them. The attack machines generate
variable number of packets to a selected target machine,
maximum being 2000 packets per second. As a result we
monitored 83,161 packets per second at the selected target
machine. The chart in Fig. 6 shows the amount of traffic
monitored by the Access and Intermediate Routers for varying
degrees of deployment of our solution. The percentage of
traffic monitored gives us an idea of the effectiveness of this
solution to analyze and rate-limit those traffic. The simulation
chart shows the amount of traffic covered in the case of a pure
Source-End solution (the bottom line in the chart) and a
combined deployment. This shows that our solution of using
Intermediate Routers along with a Source-End solution
provides good traffic coverage.

E.

ACKNOWLEDGMENT
We sincerely thank Dr. Amiya Bhattacharya, Assistant
Professor in the Department of Computer Science at New
Mexico State University, for his valuable inputs.
This work was supported in part by DARPA, under
agreement numbers F30602-99-1-0517 and N66001-00-18920.
REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]

VII. DISCUSSION
To keep our design lightweight, we have made some design
decisions that introduce some problems, but which are
relatively easy to handle. Some of the problems that we are
aware of are Push-Forward message spoofing, granularity of
flow monitoring and the history mechanism. The PushForward messages can be authenticated using certificates
between routers. Our design is flexible enough to handle
different granularities of flow monitoring; we can monitor at
the connection port level. The history mechanism has to be
updated regularly to reflect current traffic conditions. Attackers
can misuse this update stage. This misuse can be avoided by
sampling the traffic pattern at random intervals. We also
assume that the access networks have already deployed
defenses against IP address spoofing.

[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]

VIII. CONCLUSION
We have presented a viable lightweight router based DDoS
detection and recovery mechanism. This solution can be
incrementally deployed at the Access and Intermediate
Routers. The Push-Forward mechanism in a novel way of
reducing the overhead incurred at the Intermediate Router.
Simulations show that this solution is better than pure Sourceend solutions.

IEEE Communications Society
Globecom 2004

[17]
[18]
[19]

2060

CERT Coordination Center. Denial of service attacks.
http://www.cert.org/tech_tips/denial_of_service.html.
Mirkovic. J., J. Martin, and P. Reiher. A taxonomy of DDoS attacks and
DDoS defense mechanisms. University of California at Los Angeles.
http://lever.cs.ucla.edu/ddos/uc la_tech_report_020018.pdf.
Moore, D., G. M. Voelker, and S. Savage. 2001. Inferring internet
denial-of-service activity. Proceedings of 10th USENIX Security
Symposium.
Lyu, M., L. Lau. 2000. Firewall security: Policies, testing and
performance evaluation. COMPSAC 2000.
Ferguson, P. 2000. Network ingress filtering: Defeating denial of service
attacks which employ IP source address spoofing. RFC 2827.
http://sunsite.cnlab-switch.ch/ftp/doc /standard/rfc/28xx/2827.
Sans Institute. Egress filtering v 0.2.
http://www.sans.org/y2k/egress.htm.
Park, K., and H. Lee. 2001. On the effectiveness of route-based packet
filtering for distributed DoS attack prevention in power-law internets.
Proceedings of ACM SIGCOMM 2001.
Wang, H., D. Zhang, and K. G. Shin. 2002. Detecting SYN flooding
attacks. Proceedings of INFOCOM.
Ramanathan, A. 2002. WADeS: A tool for distributed denial of service
attack detection. TAMU-ECE-2002-02, Master of Science Thesis.
Bellovin, S.M. 2000. ICMP traceback messages. Internet Draft: Draftbellovin-itrace-00.txt.
Kent, and W. T. Strayer. 2001. Hash-based IP traceback. Proceedings of
ACM SIGCOMM 2001.
Song, D. X., and A. Perring. 2001. Advanced and authenticated marking
schemes for IP traceback. Proceedings of the 2001 IEEE Infocom
Conference.
Dean, D., M. Franklin, and A. Stubblefield. 2001. An algebraic approach
to IP traceback. Proceedings of NDSS '01.
Stone, R. 2000. CenterTrack: An IP overlay network for tracking DoS
floods. 9th USENIX Security Symposium.
Ioannidis, J., and S. M. Bellovin. 2002. Implementing pushback: Routerbased defense against DDoS attacks. Proceedings of NDSS'02.
Mirkovic, J., G. Prier, and P. Reiher. 2003. Source-End DDoS defense.
Second IEEE International Symposium on Network Computing and
Applications.
Gil, T.M., and M. Poletto. 2001. MULTOPS: a data-structure for
bandwidth attack detection. 10th USENIX Security Symposium.
Paxson, V. 1997. End-to-End routing behavior in the internet.
IEEE/ACM Transactions on Networking 5, vol. 5 (5).
Inet Topology Generator. http://topology.eecs.umich.edu/inet/

0-7803-8794-5/04/$20.00 © 2004 IEEE

2010
2010
IEEE
21st21st
International
International
Symposium
Symposium
on on
Software
Software
Reliability
Reliability
Engineering
Engineering

Preventing overflow attacks by memory randomization
Vivek Iyer, Amit Kanitkar
Microsoft Corp.

Partha Dasgupta,
Raghunathan Srinivasan
Arizona State University
Tempe, USA
{partha, raghus}@asu.edu
attacks. The approaches can be used independently or along
with each other. This work focuses on providing “software
diversity” for legacy software on end-user machines. Currently
a malware designer can perform offline analysis of an
application to discover vulnerabilities in it.
These
vulnerabilities can be exploited to launch various kinds of
attacks on multiple systems. Every copy of an application that
is shipped to consumers is exactly the same and contains the
same weaknesses in the same binary locations. Software
diversity breaks up the uniformity making each instance of the
application different; consequently attacks that work on one
instance do not work on another. Address Space Layout
Randomization (ASLR) [5] is an example but this idea is taken
to finer degrees of granularity on the stack frames and heap
memory chunks in this paper.

Abstract—Buffer overflow is known to be a common memory
vulnerability affecting software. It is exploited to gain various
kinds of privilege escalation. C and C++ are very commonly used
to develop applications; due to the efficient “unmanaged”
executions these languages are not safe. These attacks are highly
successful as every executing copy of a shipped binary is the
same. This work presents two approaches to randomizing the
memory layout which does not require modifications at the
developer end. Both techniques are implemented at the user-end
machines and have no requirement for source code. The
feasibility of the two techniques is shown by randomizing
complex applications and demonstrating that the run-time
penalty for the randomization schemes is very less.
Keywords: Buffer overflow,
randomization, software diversity

I.

stack

randomization,

heap

The first approach focuses on stack-smashing class of
buffer overflow attacks. A call stack is the dynamic data
structure that keeps track of the active sub-routines of a
program.
Call stack is composed of stack frames
corresponding to each active function call at any instant of
program execution. A typical stack frame consists of:
arguments passed to the function, return address to the caller,
saved registers, and local variables of the function. It is easy to
overwrite the return address on the stack frame by overflowing
the buffer and storing data beyond its boundary. Carefully
placed values can lead to the process executing code intended
by the attacker. A post-compilation stack frame randomization
technique is implemented in this work that minimizes the
probability of stack smashing attacks and decreases the “return
of investment” for attackers. The stack frame is randomized by
re-writing specific binary instructions that access locations on
the stack. The existing instructions are modified inside the
binary without adding any new instructions or extra bytes of
op-code.

INTRODUCTION

The magnitude of the threat from malware especially on
“consumer computing” platforms is well known and well
understood. Attacks on a computing platform are possible due
to the loopholes and bugs that exist in software. Buffer
overflow attacks form a major genre of memory-errors related
exploits in which a malicious user of a software program feeds
data to a fixed length buffer that extends beyond the size of the
buffer. An overflow can be used to overwrite the values in the
stack; this can cause the program to return to an attacker
specified location to execute attacker intended code. Buffer
overflow has been popular for two decades [1]. It has been
documented that buffer overflows constitute more than 50% of
major security bugs [2]. C and C++ are very commonly used
to develop applications; due to the efficient “unmanaged”
executions these languages are not safe. A vast majority of
vulnerabilities occur in programs developed with these
languages [3]. Every memory buffer used in these languages is
vulnerable to the overflow attack. The responsibility to
perform run-time bounds checking is entirely placed on the
developer. If proper bounds checking on a memory buffer are
not in place, buffer overflow vulnerability is induced in the
code. There are several variants of the buffer overflow attacks
like stack overflows, heap corruption, format string attacks,
integer overflow and so on [4]. When input data exceeds the
amount of memory allocated to a memory buffer it overflows
in the memory region adjacent to this buffer. This can
potentially corrupt pointers in the region and cause run-time
memory exceptions, program termination, and malicious code
execution if cleverly crafted by an attacker.

The second approach presented in this work is heap
randomization. Without proper bounds checking user data
written into a heap memory block can overwrite the
management data of next memory block. Under normal
circumstances this would raise a memory exception and
terminate the process, however, carefully placed data written
into the heap management region of the next block can allow
code to be written into a specific area in memory. This can
allow an attacker to execute arbitrary code. Implemented in
this work is a fine-grained heap randomization technique based
on run-time modification of the heap memory allocations. For
every block of memory requested a memory chunk with a size
larger than requested is returned. The returned memory chunk
contains random pads of memory appended before and after
the actual memory to be used by the process. This is achieved

In this paper two orthogonal approaches are presented; each
of which provides a level of assurance against buffer overflow
This material is based upon work supported in part by the National
Science Foundation under Grant No. CNS - 1011931. Any opinions, findings,
and conclusions or recommendations expressed in this material are those of
the author(s) and do not necessarily reflect the views of the NSF.

1071-9458/10 $26.00 © 2010 IEEE
DOI 10.1109/ISSRE.2010.22

339

require the modification of individual binaries but necessitate
that these binaries be compiled with a feature called Position
Independent Executables (PIE). ASLR has been incorporated
into Windows systems starting with Vista. It has been shown
that PaX ASLR only marginally slows down the time taken to
attack a system. Techniques have been demonstrated to bypass
ASLR protection and to perform a de-randomization attack on
known buffer overflow vulnerabilities [14, 15]. Two important
differences between the stack frame randomization technique
presented in this work and ASLR are that, firstly ASLR does
not change the executables as it is a run-time randomization
scheme, and secondly ASLR provides coarse randomization
where the starting base address of the stack is randomized
every time a binary is executed by the operating system, the
relative addresses within the stack segment do not change. In
this paper the stack frame randomization technique randomizes
every stack frame by re-writing the instructions inside the
binary. This provides fine grained randomization that requires
no run time support. Importantly it also randomizes the relative
addresses such that the distance between a local variable and
the return address is not the same for different installations of
the binary.

by static or run-time patching of heap memory management
libraries in the operating system. This approach does not
require changes to be made to compilers, linker, loaders or
even existing application binaries.
Both methods have been implemented completely in
software without the need for source code from software
vendors. In the remainder of the paper, work related to these
approaches and implementation details of the randomization
approaches are presented.
II.

RELATED WORK

A number of different strategies have been proposed to
mitigate stack smashing attacks. Storing a duplicate copy of
the return address for checking prior to return from a function
has been proposed [6]. StackGuard [7] places a canary value
between the local buffer(s) and the return address. It detects
the occurrence of a buffer overflow based on whether or not
the canary value is corrupted during the function execution.
An advanced version of StackGuard known as ProPolice [8] is
an extension developed for the GNU C Compiler. A static
analysis of source code for identifying potential buffer
overflow vulnerabilities has also been proposed [1, 9].
PointGuard [10] is another popular technique for protection
against buffer overflows intending to corrupt pointers.
PointGuard obfuscates pointer values by encrypting through an
exclusive-or operation with a random value, the pointers are
decrypted only when brought to CPU for dereferencing.
Dynamic loadable libraries have also been implemented that do
sanity checking on the return address of vulnerable library
functions [11].

An approach to heap randomization [16] modifies the heap
chunk structure to ensure the integrity of the allocated heap
chunk. Extra bytes of data in the heap chunk structure are
introduced which contains the header checksum seeded with a
random value. The randomization seed is stored as a static
variable. The checksum value is also used as a canary. Under
this scheme allocation of a memory chunk to a process causes
the computation of its header checksum, hence the checksum is
placed as the canary. When this memory chunk is to be freed,
re-computation of its header checksum occurs which is then
compared with the existing checksum. If the two values do not
match, an exception is raised and the process is terminated.

Randomization is a technique to inject diversity into
computer software. The first known randomization of stack
frame was proposed by placing a pad of random bytes between
return address and local buffers [12]. Random pads make it
difficult to predict the distance between buffers and the return
address on the stack. An attacker has to launch custom attacks
for every copy of the randomized binary. Address obfuscation
has extended the above idea by randomizing the base address
of memory regions, permuting the order of elements in a
binary, and introducing random gaps within memory regions
[13]. Most techniques used for stack randomization require
compiler support. However the current software distribution
model is such that only executable files are provided to the
consumer. These files are already compiled and optimized;
compiler techniques to randomization would not work on
executable files. A wide deployment of compiler supported
randomization would result in many consumers having the
same ‘randomness’ in their binaries. The approach in this
paper randomizes executable code by re-writing the
instructions that allocate the stack frame in every routine in a
binary file. This provides every consumer the opportunity to
obtain a unique copy of the binary where the probability of the
run time stack frame of two versions of the same binary being
the same is low.

Heap randomization for Windows has been implemented
by hooking the RtlHeapCreate(), RtlAllocateHeap(),
RtlReAllocateHeap(), and RtlFreeHeap() functions in ntdll.dll
to provide 19 bits of randomness to base address of a newly
created heap [19]. While this solution provides a high level of
heap protection the authors themselves state that brute force
attacks might succeed in evading their solution.

ASLR [5] modifies an operating system so that the base
address of various segments of a program is randomly
relocated during the program execution. Two such popular
ASLR implementations named PaX and ExecShield are
available for Linux as kernel patches. These approaches do not

Address Space Layout Permutation (ASLP) [20] permutes,
re-orders individual sub-routines contained within the code
segments in addition to random relocation of code and data
segments. It provides sub-page randomization for the stack
and heap region by introducing random gap pages in between

Service Pack 2 for Windows XP provides two major
enhancements to the RtlHeap security not found in the
predecessor versions of Windows. The first enhancement is
the introduction of an 8-bit security cookie in the allocated
chunk boundary tag to improve heap security, but as the
security cookies is just 8-bits long it can hold only 256 unique
values and can be brute-forced in applications which have a
deterministic behavior [3, 17]. The second improvement
provided by XP service pack 2 is the introduction of pointer
sanity checking while performing the unlink operation. If the
sanity checks are not satisfied, a heap corruption is indicated.
However, even these techniques have been evaded in practice
[17, 18].

340

distribution of randomization overhead to the users makes it
critical that the randomization procedure is simple, low cost,
and efficient. From the perspective of an attacker it can be
assumed that access to the binary would be required to reverse
engineer the randomization. This work is targeted for
scenarios where attackers remotely target users using either a
discovered or unknown/un-patched vulnerability in software,
and do not already reside on the machine.

used memory pages. ASLP changes the cross-references of the
relocated routines for which it has to rely on the relocation
information from the compiler.
III.

STACK FRAME RANDOMIZATION

This section describes the stack frame randomization
technique implemented in this work. The size of the run-time
stack frame of every routine is randomized to ensure every
version of a binary has a unique memory layout. The binaries
are instrumented by analyzing the disassembly of the code
segment in it. Additional bytes of code are not injected in the
binary in this approach; instead existing bytes in the code
segment are rewritten to achieve stack randomization. The
randomization process is carried out after the application
binary is installed at the end-user machine.
During
randomization only those instructions that are relevant to the
run-time stack have to be rewritten. By shifting the vulnerable
character buffer down by a random amount, the distance
between the return address and the buffer becomes different for
every copy of the binary. This makes it difficult to use the
same attack string against different copies of the binary.

Consider the code shown in Figure 1. The routine foo takes
in one integer argument. It has two local elements for which
the compiler would need to allocate space on the stack. The
compiler needs to allocate a buffer of 1024 bytes and 4
additional bytes for the integer variable. The C library function
gets (known to be vulnerable to write beyond buffer bounds) is
used to take input from the user to store in the character buffer.
This makes the character array vulnerable to a buffer overflow
attacks. Figure 2 shows the disassembly for the x86
architecture based binary of the function foo. The virtual
address of each instruction is shown on the left, followed by
the hex dump of the instruction op-code and operands. The
rest of the disassembly shows the mnemonic instruction
interpretation of the disassembler.

Randomization can be implemented at various levels. It can
be done at the source code of the program. This would involve
the addition of additional dummy variables in subroutines
during compilation, making each copy of the binary different in
terms of the stack frame. Another implementation may involve
randomizing the code at assembly instruction level before
using the assembler to build the binary. Randomization can
also be achieved after the assembler has produced the object
code or the executable binary. One of the basic assumptions
made in this work is that access to source code would not be
available. It is also assumed that the binaries themselves do
not have any more information than available in a commercial
grade fully optimized executable binary. Compiler support is
not required for this work. A compiler patch makes it
necessary to recompile source code to achieve randomization,
whereas distribution binaries optimized for performance are
randomized in this work. Similar techniques have been
attempted in research for stack frame randomization [12, 13].
The difference between this work and existing research is the
approach taken to randomize stack. Only the disassembly of a
binary (executable) is utilized to randomize the stack frame.
The output of the disassembler is parsed to separate the subroutines within the code segment. This allows filtering of
instructions that are related to the run time stack. The filtered
machine-level instructions are located in the executable binary
and instrumented to introduce a pad of random size into each
function. This randomizes binaries without compromising its
expected behavior or execution semantics. It is very important
not to drastically alter the existing software distribution model.
Eliminating the constraint of source code access provides the
opportunity for randomizing legacy software.
The
randomization process is carried out at the user end. The

Instructions of the following type need to be rewritten in the
binary if a random pad is added:
•

Instructions that create space on the stack frame for local
variables and buffers.
The instruction at address
0x80483c7 in fig. 2 allocates space of 418 Hex bytes on
the stack. If a pad of 32 bytes is injected into the frame,
this instruction will have to allocate 438 Hex bytes of
stack.

•

Instructions that de-allocate space used by the locals of the
function on the stack frame. These instructions are
executed right before the function returns. In this case the
stack de-allocation is done implicitly by the leave
instruction that restores the stack pointer to the frame
pointer; hence explicit modification of any instruction for
correct de-allocation of the random pad is not required. In
certain functions, a constant value is added to the stack
pointer, to de-allocate space on the stack. Such functions
require processing of de-allocation instructions.

080483c4
80483c4:
80483c5:
80483c7:
80483cd:
80483d3:
80483d6:
80483d8:
80483de:
80483e1:
80483e6:
80483e9:
80483ef:
80483f2:
80483f9:
80483fb:

int foo(int dummy_arg)
{
char mesg_string[1024];
int local_variable;
gets(mesg_string);
local_variable = dummy_arg
}
Figure 1. Sample C routine

<foo>:
55
89 e5
81 ec 18
65 a1 14
89 45 fc
31 c0
8d 85 fc
89 04 24
e8 2a ff
8b 45 08
89 85 f8
8b 45 fc
65 33 05
74 05
e8 30 ff

8048400: c9
8048401: c3

04 00 00
00 00 00

fb ff ff
ff ff
fb ff ff
14 00 00
ff ff

push %ebp
mov %esp,%ebp
sub $0x418,%esp
mov %gs:0x14,%eax
mov %eax,-0x4(%ebp)
xor %eax,%eax
lea -0x404(%ebp),%eax
mov %eax,(%esp)
call 8048310 <gets@plt>
mov 0x8(%ebp),%eax
mov %eax,-0x408(%ebp)
mov -0x4(%ebp),%eax
00 xor %gs:0x14,%eax
je 8048400 <foo+0x3c>
call 8048330
<__stack_chk_fail@plt>
leave
ret

Figure 2. Disassembly of sample C routine

341

•

Instructions that access local variables and buffers on the
stack frame. All local variables and buffers are accessed
with the help of the frame pointer EBP. All stack locals
are located below the frame buffer at lower addresses in
the Intel x86 architecture. Due of the introduction of a
random pad the local buffers shift further down from the
frame pointer. The instruction at 0x80483e9 in figure 2
accesses a location 1032 bytes below EBP, this instruction
will need modification.

8057520: 55
8057521: 89 e5
8057523: 57
8057524: 56
8057525: 53
8057526: 81 ec bc 03 00 00
...............
...............
80578cb: 8d 65 f4
80578ce: 5b
80578cf: 5e
80578d0: 5f
80578d1: 5d
80578d2: c3

A naive coarse-grained randomization technique would be to
modify every relevant instruction with the same random pad
constant. This would alter the size of the stack frame of every
function or sub-routine by the same random amount. A coarsegrained technique makes it simpler for an attacker to launch a
brute force attack by trial and error technique. In order to make
it difficult to launch such brute force attacks, a fine-grained
randomization of stack frames is essential.
Uniform
randomization also poses another problem where a constraint
that prevents randomization of one function will make it
impossible to randomize other unrelated portions of the code
segment.

push %ebp
mov %esp,%ebp
push %edi
push %esi
push %ebx
sub $0x3bc,%esp

lea-0xc(%ebp),%esp
pop %ebx
pop %esi
pop %edi
pop %ebp
ret

Figure 3. Disassembly of stack restoration of EBP offset

alignment check exception. Any stack frame randomization
technique should be careful while dealing with such sensitive
instructions. A conservative approach is to not randomize subroutines that have these instructions.
Adding a constant to the stack pointer ESP is one of the
methods to restore its value at the end of function execution.
Such instructions should be rewritten during randomization as
instructions that create space on the stack are randomized.
There is another scenario in which a constant is added to the
stack pointer to restore the ESP after a subroutine calls other
functions during the course of its execution. Arguments to
sub-routines are typically pushed onto the stack before the call
instruction is executed. When the called function returns it is
the responsibility of the caller to clean the arguments off the
stack frame and reset the stack pointer. In order to accomplish
this, the caller adds a constant to the ESP that represents the
number of bytes pushed onto the stack as arguments to the
function that just returned. Such instructions are capable of
misleading a randomizer into acting on it as an instruction that
restores the ESP at the end of function execution. This may
lead to corruption of the stack pointer, resulting in
segmentation faults or run-time memory exceptions.

Constraints: Integer constants are represented in the form of
two’s compliment in a binary file. An N-bit two’s compliment
can represent integers in -2N-1 to + 2N-1. The amount of
padding that a routine can be provided is dependent on the
instruction that can support the least padding. Consider the
instruction mov %edx,-0x80(%ebp). In this instruction, the
negative constant offset from the frame pointer is -0x80. With
a single byte signed integer -0x80 is the most negative number
that can be represented. This instruction would have to be
modified when a random pad is provided to the function. The
local variable would move further away from the EBP making
it necessary to make the operand more negative. An extra byte
is required to represent the new integer even if a pad of one
byte is provided to this function. Hence the function in which
this type of instruction is present cannot be provided even one
byte of pad. However such constraints apply only to a specific
sub-routine as every routine is processed separately.
Certain functions require usage of several general purpose
registers for internal computation. Such functions may
preserve the values of the registers that get clobbered as they
are restored before the function returns to the caller. Such
functions have a specialized function prologue and epilogue to
accomplish this. Figure 3 shows the function prologue and
epilogue of one such function. Such specialized function
epilogues use negative offsets along with the EBP to calculate
the effective address. The instruction at 80578cb deceivingly
looks like an access to a local buffer; such instructions in
function epilogues should never be randomized.
Any
modification to this instruction would potentially corrupt the
stack frame pointer causing either a segmentation fault or some
other run-time memory exception leading to a system crash.

The frame pointer (EBP) is used for access to local variables
on the stack. The random padding injected at the beginning of
the frame makes it necessary to modify instructions that use
EBP to access local variables. The frame pointer is also used
to access the arguments passed to a function. The difference
between these two kinds of instructions is based on whether a
positive or a negative offset is used in the instruction. In the
x86 architecture, the local variables are located at lower
addresses below the frame pointer so negative offsets are used
with EBP to access them. The arguments are located above the
frame pointer at a higher address and positive offsets are used
to access them.
Only the local variables shift during
randomization; hence the random pad does not affect the
arguments on the stack. EBP references using positive offsets
that access arguments to a function should not be randomized.

Certain instructions cause general-protection exceptions
when the memory operand they operate on is not aligned as per
the architecture specification.
One such instruction is
FXSAVE which saves the floating point context to a 512-byte
memory location specified in the 16-byte aligned operand. The
addition of a random pad can violate this constraint, causing
the instruction to result in a general-protection exception or an

Implementation: The prototype randomizer was developed
in C and compiled using gcc, the objdump utility was used to
perform disassembly. Figure 4 shows the flow of the
implemented randomizer. The binary file is fed to the
disassembler; its output is parsed for identification of
instruction operands that need to be modified in the binary.
The parser separates out and analyzes each sub-routine in order

342

Extract relevant
instructions

Binary
file

Disassembler

Sub routine separation and
instruction analysis

Parser

Randomizer

Randomized
binary

Figure 4. Workflow of randomizer

executable binary file. While tracing every instruction the
randomizer keeps track of the subroutine in which the
instruction is present. With the help of the data structure built
for every subroutine during the first pass the randomizer
statically rewrites and instruments the corresponding
instruction in the binary executable. It can be argued that this
solution is vulnerable to brute force techniques where an
attacker could attempt to overflow the stack by providing one
specific return address at every stack location.

to accomplish fine-grained randomization such that every
function is padded separately with a random pad conforming to
the constraints of that specific function. These instructions are
then rewritten directly into the binary.
The prototype works as a 2-pass randomizer. In the first
pass each sub-routine is analyzed to determine the maximum
padding that can be provided to that routine. The randomizer
walks through every instruction within a sub-routine and
updates a data structure that is associated with that routine.
This data structure has the following information: the virtual
address of the routine, the maximum pad that can be provided
to the routine, and the random pad assigned to the routine. The
randomizer looks for instructions that are sensitive to
alignment of memory operands and takes a conservative
approach of not randomizing the subroutine containing them.
Due to the way the Intel architecture binary accesses stack
memory, the amount of randomization that can be performed is
limited. Every instruction in a function is processed to check
the maximum possible random pad allowed for that instruction.
The least of these values becomes the available max pad
(3
3Max)for the function. The actual random pad is (3
3Rand)
selected by generating a random number between 0 and 3Max.
3Rand is then clipped to the nearest factor of 32 to resolve many
alignment requirements of several instructions. It is necessary
to place an upper-limit on 3Max provided to each subroutine.
Providing extremely large pads to all the subroutines increases
the chances of a stack overflow which can cause the process to
crash. An upper-limit of 640 bytes on the pad for every
subroutine is maintained to prevent this scenario.

Using this technique an attacker can successfully jump into
a library call or a different function instead of returning to the
calling routine. However, passing arguments to such an
overflow attack can be considered very difficult. This is due to
the fact that many library calls read arguments off the stack and
these arguments have to be correctly placed on the stack. As a
result of this even if an attacker executes a brute force attack
where the execution returns to a library function, it would be
impossible to pass arguments. In addition if this technique is
used in addition to using canaries the runtime system can
quickly determine that an attack occurred. The prototype
randomizer was tested on a variety of binaries. All the binaries
used in testing were release quality optimized utilities that are
part of Linux distributions. Binaries of different kinds ranging
from moderate to complex were used in testing. Copies of the
following applications were randomized in this work: Open
Office, pidgin, pico, ls, gcc, netstat, ifconfig, route, xcalc, tail,
date, nslookup, sum, head, wc, md5sum, pwd, users, cat,
cksum, hostid, logname, echo, size, firefox, viewres, xwininfo,
oclock, ipcs, pdfinfo, pdftotext, eject, lsmod, clear, vlc, and gij.
A small list of binaries that were successfully randomized is
shown in Table 1. The binaries were thoroughly tested for
segmentation faults and other runtime memory errors that may

In the second pass the randomizer goes through the
instructions in the disassembly and locates them in the

TABLE I.

Binary
Open Office
pidgin
gcc
route
xcalc
echo
firefox
vlc
gij
lsmod

Time taken to
randomize (sec)
20.220
54.718
1.836
0.505
0.109
0.137
1.200
0.057
0.062
0.397

PERFORMANCE TEST RESULTS FOR STACK R ANDOMIZATION

% of subroutines
randomized
88.33
91.48
91.77
93.75
100
100
100
100
100
100

Original stack
(bytes)
21256
16093
1829
2183
11,372
1018
27890
6366
2467
4414

343

Instrumented stack
(bytes)
23170
16407
2009
2614
11592
1079
28216
6688
2633
4473

Overhead
(bytes)
1,914
314
180
431
220
61
326
322
166
59

Virtual
Address

Disassembly of
Original Routine

Disassembly of
Instrumented routine

8049d80:
8049d81:
8049d83:
8049d86:
8049d89:
8049d8c:
8049d8f:
8049d92:
8049d95:
8049d98:
8049d9b:
8049d9e:
8049da1:
8049da3:
8049da5:
8049da7:
8049da9:
8049db0:
8049db2:
8049db5:
8049db8:
8049dbb:
8049dbe:
8049dc0:
8049dc1:

push %ebp
mov %esp,%ebp
sub $0x10,%esp
mov %edi,-0x4(%ebp)
mov 0x8(%ebp),%eax
mov 0xc(%ebp),%edi
mov %ebx,-0xc(%ebp)
mov %esi,-0x8(%ebp)
mov 0x38(%eax),%esi
mov 0x38(%edi),%ecx
mov 0x34(%eax),%ebx
mov 0x34(%edi),%edx
cmp %ecx,%esi
jl 8049dc6 <exit@plt+0x356>
jle 8049dc2 <exit@plt+0x352>
cmp %ecx,%esi
movl $0x1,-0x10(%ebp)
jle 8049dd0 <exit@plt+0x360>
mov -0x10(%ebp),%eax
mov -0xc(%ebp),%ebx
mov -0x8(%ebp),%esi
mov -0x4(%ebp),%edi
mov %ebp,%esp
pop %ebp
ret

push %ebp
mov %esp,%ebp
sub $0x30,%esp
mov %edi,-0x24(%ebp)
mov 0x8(%ebp),%eax
mov 0xc(%ebp),%edi
mov %ebx,-0x2c(%ebp)
mov %esi,-0x28(%ebp)
mov 0x38(%eax),%esi
mov 0x38(%edi),%ecx
mov 0x34(%eax),%ebx
mov 0x34(%edi),%edx
cmp %ecx,%esi
jl 8049dc6 <exit@plt+0x356>
jle 8049dc2 <exit@plt+0x352>
cmp %ecx,%esi
movl $0x1,-0x30(%ebp)
jle 8049dd0 <exit@plt+0x360>
mov -0x30(%ebp),%eax
mov -0x2c(%ebp),%ebx
mov -0x28(%ebp),%esi
mov -0x24(%ebp),%edi
mov %ebp,%esp
pop %ebp
ret

Figure 5. Comparison of Original and Instrumented Disassembly

allocation. This was done by appending a random pad below
and above the pointer to the heap memory chunk returned by
the allocation routine. Figure 6 gives an overview of the
randomization technique. Randomization ensures that each
heap allocation request originating from a program receives a
different buffer during different instances of execution. The
internal layout of the process heap looks different on every run.
This approach mitigates heap overflow attacks in deployed
software. The randomization implemented in this work does
not require changes to be made to the system loader, linker, and
compiler.
The randomization process does not modify

occur due to corruption of the runtime stack randomization.
None of the binaries exhibited any deviation from their
expected behavior and no run-time exceptions were generated.
On an average the randomizer modified the run-time stack of
more than 75% of the sub-routines in every application. This
was due to the conservative approach taken by not randomizing
functions containing certain instructions like FXSAVE. Other
constraints mentioned also contribute to this number being less
than 100%.
Figure 5 shows a sample randomization instance where the
original routine contained an instruction to generate 16 bytes of
stack. After passing the routine through this prototype
randomizer it can be seen that the stack allocated is 48 bytes.
As can be seen in the figure, no net instruction is added or
removed; only the instructions that access the local variables on
the stack are modified to point to the correct location on the
randomized stack frame which in this instance has a shift of 32
bytes. The experiments show that the randomization cost itself
is reasonable and low. Only the size of the run-time stack was
manipulated, hence this approach is not expected to have any
runtime penalty.
IV.

Previous chunk size
Size/Status (In use)
Chunk
pointer

Random Pad 1

Randomized
Allocated
memory chunk

USER DATA
Random Pad 2
Previous chunk size

HEAP MEMORY RANDOMIZATION PER ALLOCATION

Size/Status (In use)

Heap memory randomization was achieved by overallocating the requested chunk and then placing the returned
chunk within the over-allocated chunk. As a first step the
library functions that play a vital role in heap memory
management (functions that perform the free, allocate, and
resize operations) were isolated.
These functions were
wrapped with randomization code. Access to the source code
of an application was not used for this solution as the solution
patches the underlying heap memory management mechanism.
A dual random padding strategy was used for every memory

Forward pointer

Free memory
chunk

Backward pointer

Unused memory

Figure 6. Allocated heap memory chunk with dual random padding

344

original request. A successful malloc() operation returns the
pointer to a newly allocated memory chunk. The value of the
pointer returned to the calling function (user application) is
shifted by i bytes. This integer value i is stored just above the
returned pointer so that memory management functions like
free() and realloc() can determine the random pad value to
calculate the actual starting address of the memory chunk and
the boundary tag information stored above it.

application binaries and does not impose any noticeable
execution overhead on a process. Randomizing the starting
location of every allocated buffer to obfuscate the boundary
pointers is unique to the Linux solution presented.
Linux approach: The random pads placed on the heap
memory are non-uniform and generated on-the-fly during
runtime. The dual random padding strategy is implemented by
appending a random pad below and above the pointer to the
heap memory chunk returned by the allocation algorithm. This
is achieved by identifying and patching the heap management
functions of the GNU C library. The functions identified were:
malloc(), free(), realloc() and memalign(). Other functions
identified were calloc(), valloc() and pvalloc(), however these
functions are based on malloc() and were not modified.

A call to free() made by the requestor function, results in
execution entering the free() public wrapper. The actual
starting address of the allocated chunk is calculated in the
wrapper and passed to the internal free() routine. If these
values are not determined, it would lead to errors in the system.
The value of random numbers set by malloc() is extracted by
reading the location directly above the chunk address passed as
argument to free. Using the integer values stored above the
passed chunk address the original size of the block and the
starting address of the block is determined.
Similar
calculations are made for realloc() and memalign() library
calls.

Two random integers i and j (both multiples of 8 to respect
the internal memory alignment rules) are generated during an
allocation request. The upper limit of the random numbers
generated can be selected heuristically. These two random
integers are added to the size parameter contained in the
original request resulting in an allocation call for i + j +
TABLE II.

RUN-TIME PERFORMANCE TESTS ON LINUX UBUNTU SYSTEM

Test
Dhrystone 2 with register variables

Execution time without
the library patch (sec)
10

Execution time with the
library patch (sec)
10.1

Whetstone Double Precision

10.2

10

Execl Throughput

29.3

29.8

File Read (buffer size = 1024
Max. number of blocks = 2000)

30

30

File Write (buffer size = 1024
Max. number of blocks = 2000)

30

30

File Copy (buffer size = 1024
Max. number of blocks = 2000)

30

30

File Read (buffer size = 256
Max. number of blocks = 500)

30

30.1

File Write (buffer size = 256
Max. number of blocks = 500)

30

30

File Copy (buffer size = 256
Max. number of blocks = 500)

30

30

File Read (buffer size = 4096
Max. number of blocks = 8000)

30

30

File Write (buffer size = 4096
Max. number of blocks = 8000)

30

30

File Copy (buffer size = 4096
Max. number of blocks = 8000)

30

30

Pipe-base Context Switching
Process Creation

10
30

10
30

System Call Overhead

10

10

Shell Script (8 concurrent)

60

60.2

345

TABLE III.

RUN-TIME PERFORMANCE TESTS ON WINDOWS XP SYSTEM

Test

CPU – Integer Math (Mops/s)
CPU – Floating Point Math (Mops/s)
CPU – Prime Number Calculation (1000 primes/s)
CPU – Multimedia Instructions (Millimatrices/s)
CPU – Compression (Kbytes/s)
CPU – Encryption (Mbytes/s)
Physics (Mbytes/s)
String Sort (1000 strings/s)
Memory – Allocate Small Block (Mbytes/s)
Memory – Read Cached (Mbytes/s)
Memory – Read Uncached (Mbytes/s)
Memory – Write (Mbytes/s)
Large RAM

Execution time
without the library
patch (sec)
54
220.8
245.3
1.36
744.9
2.95
40.7
591.2
933.7
1340.4
1272
812.4
120.1

Execution time
with the library
patch (sec)
69.5
218.9
241.9
1.32
732.5
2.96
38.9
592
1434.5
1334.2
1271.2
814.1
118.5

Linux caused segmentation faults. To achieve randomization a
separate heap request A2 is made in addition to the original
request A1 where A2 provides the random padding of i to the
allocated chunk request. To ensure that A2 is released during a
free operation the address of A2 is stored in the last but one
word of A1. The last word of A1 is placed with a “cookie” that
indicates that the heap is randomized. HeapFree() detour
function checks for the four byte cookie during every free
operation request and only if the cookie is detected it frees the
chunk A2. A double free request is prevented by the
implemented HeapFree() detour function as it erases the
cookie during the first free operation. The HeapReAlloac()
detour function does not have any responsibilities as opposed
to the corresponding one in Linux. In the absence of pointer
location randomization all the randomization changes related to
reallocation are implicitly handled by HeapAlloc() and
HeapFree() detours.

Windows approach: The solution for Windows RtlHeap
security also involves generating non-uniform random pads onthe-fly (during runtime) and adding them to a heap memory
chunk to be allocated. The end results provided by this
solution appear similar to the Linux solution; however
Windows poses technical limitations to the implementation of
the solution. The source code for Windows or its RTL (Runtime Library) is not available. Hence the internal workings of
the required heap memory management RTL functions cannot
be studied and it is not possible to edit them to recompile the
library as done on a Linux system. This necessitates a different
patching technique. The Detours [21] research package
provided by Microsoft was utilized to implement heap memory
randomization on Windows. Detours is a library which allows
intercepting arbitrary Win32 binary functions on x86 machines.
The Detours library achieves API hooking on the target
function by overwriting few bytes in the beginning of the
function’s in-core binary image. First the Detours library
injects the user-supplied library containing the ‘hook’ or the
‘Detour’ function into the target process’ memory. Once this is
done it copies over the first few bytes of the target function (to
be overwritten later) into a newly allocated region known as a
trampoline function. As its last instruction this trampoline
contains a jump to the remainder of the target function. The
first few bytes of the target function are overwritten with a
jump instruction to the user provided ‘Detour’ function. This
finishes the insertion of the ‘Detour’ function and the target
Win32 function is hooked. All future calls made to the target
function land in the ‘Detour’ function instead of the original
function.

Performance: Several release-quality applications were
tested with and without the implemented library patch on
Linux Ubuntu 8.04 and Windows XP operating systems.
Applications such as web browsers, office suites, games, and
graphics processing utilities were executed without any
exceptions. These applications did not exhibit any unexpected
behavior and no run-time exceptions were detected during the
test runs.
The Linux solution was evaluated on a
benchmarking tool known as Unixbench (version 4.1); the
results of these tests can be seen in table 2. On the Windows
system, the solution was evaluated on a benchmarking tool
known as Performance Test (evaluation version 7.0); the
observations of the tests are reported in table 3. From the
results of the benchmarking tools it is apparent that the heap
randomization solutions do not induce any noticeable run-time
overhead on the applications executed with the modified
libraries.

Using this run-time library patching or hooking technique
the memory management functions are overridden like the
Linux implementation. The kernel APIs in kernel32.dll
(HeapAlloc(), HeapFree() and HeapReAlloc()) were hooked to
achieve this. To implement location randomization, the ability
to write into the user area of the returned heap chunk is
required. In Windows attempts to save the values of the
random integer i above the shifted pointer as implemented on

V.

CONCLUSION

Techniques in randomization of software have been
researched heavily in the recent years. Randomization reduces

346

[6]

the return of investment for an attacker and makes it difficult to
construct attacks that can be replicated. Buffer overflow
attacks have been a security problem for nearly two decades.
The stack frame randomization approach in this paper mitigates
the prevalence of stack smashing attacks. The disassembly of
executable binaries was utilized to separate, analyze, and
rewrite instructions relevant to the run-time stack of a process.
Fine grained randomization was performed for a number of
binaries where each routine was randomized independently of
other routines in the binary. Tests conducted using the
randomized binaries showed that the process was a low cost
approach that lends itself to the existing software distribution
model without making large scale system wide modifications
like changes to the compiler/linker/loader. Although this
technique makes it difficult to launch a buffer overflow attack
aimed at hijacking the control flow it does not protect against
attacks that intend to corrupt data adjacent to vulnerable
binaries. A better randomization technique can be developed
based on this work that sprinkles pads of random sizes across a
stack frame and within the data variables or locals of a
function.

[7]

[8]

[9]

[10]

[11]

[12]

This paper also presented a fine-grained technique to
randomize the heap section of a program. The library patching
technique used in this solution introduces random pads above
and below every chunk of memory allocated off the heap. The
values for these random pads are generated at run-time due to
which each instantiation of a program allocates different
random pads to the same heap memory buffer. In Linux the
GNU C library was statically patched to introduce random pads
during heap allocation. Under Windows a patch library is
injected into system which contains detours or patch functions
for the target memory management functions. Hooking into
these detours allows randomization of allocated heap buffers.
Due to the technological limitations such as absence of source
code, pointer location randomization is not a viable technique
for Windows, however developing a workaround technique for
the can be pursued as future work.

[13]

[14]
[15]

[16]

[17]

[18]

REFERENCES
[1]

[2]
[3]
[4]
[5]

[19]

D. Wagner , J.S. Foster, E.A. Brewer, and A. Aiken, “A first step
towards automated detection of buffer overrun vulnerabilities,”
Networks and Distributed Security Symposium, pp. 3 – 17, 2000.
J. Viega and G. McGraw, “Building Secure Software,” Addison-Wesley:
2002.
R. Seacord, “Secure Coding in C and C++,” Addison-Wesley: 2005.
J. Foster, V. Osipov, N. Bhalla, and N. Heinen, “Buffer Overflow
Attacks: Detect, Exploit, Prevent,” Syngress Publishing: 2005.
Web link, ASLR: “Address space layout randomization,” retrieved on
April 25 2010, http://pax.grsecurity.net/docs/aslr.txt

[20]

[21]

347

T. Chiueh and F. Hsu. Rad: "A compile-time solution to buffer overflow
attacks," 21st International Conference on Distributed Computing
Systems, pp. 409–417, April 2001.
C. Cowan, C. Pu, D. Maier, H. Hintony, J. Walpole, P. Bakke, S.
Beattie, A. Grier, P. Wagle, and Q. Zhang. "Stackguard: automatic
adaptive detection and prevention of buffer-overflow attacks,"
SSYM’98: Proceedings of the 7th conference on USENIX Security
Symposium, pp. 63–78, 1998.
H. Etoh. "Propolice: GCC extension for protection against stacksmashing attacks," Available online at:
http://www.trl.ibm.com/projects/security/ssp/.
D. Larochelle and D. Evans. "Statically detecting likely buffer overflow
vulnerabilities," SSYM’01: Proceedings of the 10th conference on
USENIX Security Symposium, pp. 177–190, August 2001.
C. Cowan, S. Beattie, J. Johansen, and P. Wagle. "Pointguard: protecting
pointers from buffer overflow vulnerabilities," SSYM’03: Proceedings
of the 12th conference on USENIX Security Symposium, pp. 91–104,
2003.
A. Baratloo, N. Singh, and T. Tsai. "Transparent run-time defense
against stack smashing attacks," ATEC ’00: Proceedings of the annual
conference on USENIX Annual Technical Conference, pp. 21–21, 2000.
S. Forrest, A. Somayaji, and D.H. Ackley, “Building diverse computer
systems,” HOTOS ’97: Proceedings of the 6th Workshop on Hot Topics
in Operating Systems (HotOS-VI), IEEE Computer Society, pages 67–
72, May 1997.
S. Bhatkar, D. C. DuVarney, and R. Sekar, “Address obfuscation: an
efficient approach to combat a broad range of memory error exploits,”
12th USENIX Security Symposium, vol 120, August 2003.
T. Durden. "Bypassing PaX ASLR Protection," Phrack Inc. Available
from URL http://www.phrack.org/issues.html?issue=59&id=9#article.
H. Shacham, M. Page, B. Pfaff, E. Goh, N. Modadugu, and D. Boneh.
"On the effectiveness of address-space randomization," CCS '04:
Proceedings of the 11th ACM conference on Computer and
communications security, pp. 298-307, 2004.
W. Robertson, C. Kruegel, D. Mutz, and F. Valeur. "Run-time Detection
of Heap-based Overflows," Proceedings of the 17th Large Installation
Systems Administration (LISA XVII), 2003.
M. Conover and O. Horovitz. "Windows Heap Exploitation (Win2KSP0
through WINXPSP2)," Available online:
www.cybertech.net/~sh0ksh0k/heap/XPSP2%20Heap%20Exploitation.p
pt.
A. Anisimov. "Defeating Microsoft Windows XP SP2 Heap Protection
and DEP Bypass," Available online:
http://www.aelphaeis.milw0rm.com/papers/121.
L. Li, J.E. Just, and R. Sekar. "Address-Space Randomization for
Windows Systems," Proceedings of the 22nd Annual Computer Security
Applications Conference 2006 (ACSAC'06).
C. Kil, J. Jun, C. Bookholt, J. Xu, P. Ning. "Address Space Layout
Permutation (ASLP): Towards Fine-Grained Randomization of
Commodity Software," Proceedings of the 22nd Annual Computer
Security Applications Conference (ACSAC’06).
G. Hunt and D. Brubacher. "Detours: Binary Interception of Win32
Functions," In 3rd USENIX Windows NT Symposium, USENIX, July
1999.

The Design qnd

Implementation of the Clouds
Distributed Operating System
P. Dasgupta, R. C. Chen, S. Menon,
M. P. Pearson, R. Ananthanarayanan,
U. Ramachandran, M. Ahamad,
R. J. LeBlanc, W. F. Appelbe,
J. M. Bernabéu-Aubán, P. W. Hutto,
M. Y. A. Khalidi, and C. J. Wilkenloh
Georgia Institute of Technology

ABSTRACT: Clouds is a native operating system for
a distributed environment. The Clouds operating
system is built on top of a kernel called Ra. Ra ís a
second generation kernel derived from our experience with the first version of the Clouds operating
system. Rø is a minimal, flexible kernel that provides a framework for implementing a variety of
distributed operating systems.

This paper presents the Clouds paradigm and a
brief overview of its first implementation. We then
present the details of the Rø kernel, the rationale
for its design, and the system services that constitute the Clouds operating system.

This work was funded by NSF grant CCR-8619886.

@

Computing Systems, Vol. 3 . No.

I . Winter 1990

11

I. Introduction
Clouds is a distributed operating system built on top of a minimal
kernel called Rø. The paradigm supported by Clouds provides an
abstraction of storage called objects and an abstraction of execution called threads.

1.1 Basic Philosophies
A primary research topic of the Clouds project is the development
of a set of techniques that can be used to construct a simple,
usable distributed operating system. A distributed operating system should integrate a set of loosely-coupled machines into a centalized,work environment. The following are the requirements of
such a distributed system:
. The operating system must integrate a number of computers, both compute servers and data servers, into one operating environment.

.
.

The system structuring paradigm is important in deciding
the appeal of the system. This should be clean, elegant, simple to use and feasible.
A simple, efficient, yet effective implementation.

To attain this end, we proposed the following basic
philosophies:

.

1,2

We use the much advocated minimalíst philosophy towards
operating system design. The operating system is divided
into several clean, well defined modules and layers. The
kernel is one such layer ofthe operating system and supports only features that cannot be supported elsewhere.

Dasgupta et al.

Modularity in these layers limits the amount of interference
(side-efects) and gives rise to easier upgrading and
debugging.

.

Three basic mechanisms common to all general-purpose systems are computation, storage and I/O. We use simple
primitives to support these: namely, light-weight processes
and persistent object memory. Light-weight processes are
involved in computation. Persistent object memory serves
for all storage needs. The need for user-level disk I/O in our
model is eliminated. Terminal I/O is provided as a special
case

ofobject

access.

1.2 Clouds Design Objectives
Clouds is designed to run on a set of general purpose computers
(uniprocessors or multiprocessors) that are connected via a localarea network. The major design objectives lor Clouds are:

.

Integration of resources through cooperation and location
transparency, leading to simple and uniform interfaces for
distributed processing.

.

Support for various forms of atomicity and data consistency, including transaction processing, and the ability to
tolerate failures.

.

Portability, extensibility and efficient implementation.
Clouds coalesces a distributed network of computers into an
integrated computing environment with the look and feel of a centralized timesharing system. In addition to the integration, the
paradigm used for defrning and implementing the system structure
of the Clouds system is a persistent object/thread model. This
model provides threads to support computation and objects to
support an abstraction of storage. The model has been augmented
to to provide support for reliable programs [Chen & Dasgupta
1989; Chen 19901. The consistency techniques have been
designed, but not implemented, and are outside the scope of this
paper.
The rest of this paper is organized as follows. An overview of
the Clouds project is provided in Section 2, followed by an overview of the Clouds paradigm in Section 3. The paradigm is the
The Design ønd Implementation of the Clouds Distributed Operating System

r3

common link between the first implementation of Cbuds (Clouds
u.1) and the current version (Clouds v.2). We present the implementation of Clouds v.1 in Section 4,the structure and implementation of Clouds v.2 in more detail in Section 5, and the reasons
behind the Clouds redesign in Section 6. Section 7 presents some
of details of the Ra implementation and Section 8 describes the
current state of the implementation of Cbuds on Ra.

2. Project Overview
The first version of the Clouds kernel was implemented in 1986.
This version is referred to as Clouds v.l and was used as an experimental testbed by the implementors. This implementation was
successful in demonstrating the feasibility of a native operating
system supporting the object model. Our experience with Clouds
v./ provided insights into developing better implementations of
the object/thread paradigm.
The lessons learned from the first implementation have been
used to redesign the kernel and build a new version of the operating system called Clouds v.2. Clouds v.2 uses an object/thread
paradigm which is derived from the object/process/action system
[Allchin 1983; Spafford 1986; Wilkes 1987] used by Clouds v.I.
However, most of the design and implementation of the system
are substantially different. Clouds v.1 was targeted to be a testbed
for distributed operating system research. Clouds v.2 is tatgeted to
be a distributed computing platform for research in a wide variety
of areas in computer science.
The present implementation of Cbuds supports the persistent
object/thread paradigm, a networking protocol, distributed virtual
memory support, user-level I/O with virtual terminals on UNIX, a
Cr+ based programming language, and some system management
software.

Work in progress includes user interfaces, consistency support,
fault tolerance, language environments, object programming conventions and better programming language support for object typing, instantiation, and inheritance.

14

Dasgupta et al.

3. The Clouds Paradigm
All data, programs, devices, and resources in Clouds are encapsulated in objects. Objects represent the passive entities in the system. Activity is provided by threads, which execute within
objects.

3.1 Objects
A Clouds object, at the conceptual level, is a virtual address space.
Unlike virtual address spaces in conventional operating systems, a
Clouds object is persistent and is not tied to any thread. A Clouds
object exists forever and survives system crashes and shutdowns
(as does a frle) unless explicitly deleted. As will be seen in the following description of objects, Clouds objects are somewhat
"heavyweight" and are better suited for storage and execution of
large-grained data and programs because invocation and storage of
objects bear some non-trivial overhead.
An object consists of a named address space and the contents
of the address space. Since it does not contain a process, it is
completely passive. Hence, unlike objects in some object based
systems, a Clouds object is not associated with any server process.
(The frrst system to use passive objects was Hydra [Wulf et al.
1974; Wulf et al. 19811.) The contents of each virtual address
space are protected from outside access so that memory (data) in
an object is accessible only by the code in that object and the
operating system.
Each object is an encapsulated address space with entry points
at which threads may commence execution. The code that is
accessible through an entry point is known as an object operation.
Data cannot be transmitted in or out of the object freely, but can
be passed as parameters (see the discussion on threads in Sec-

tion

3.2).

Each Clouds object has a global system-level name called a
sysname, which is a bit-string that is unique over the entire distributed system. Sysnames do not include the current location of the
object (objects may migrate). Therefore, the sysname-based naming scheme in Clouds creates a uniform, flat system name space

The Design and Implementation of the Clouds Distributed Operating System

15

for objects, and allows the object mobility needed for load balancing and reconfiguration. User-level names are translated to
sysnames using a nameserver.

Clouds objects are programmed by the user. System utilities
can be implemented using Clouds objects. A complete Clouds
object can contain user-deflned code and data, and system-defrned
code and data that handles synchronization and recovery. In
addition, it contains a volatile heap for temporary memory allocation, and a permanent heap for allocating memory that becomes a
part of the data structures in the object. Locks and sysnames of
other objects are also a part of the object data space.

3.2 Threads
The only form of user activity in the Clouds system is the user
thread. A thread can be viewed as a thread of control that runs
code in objects, traversing objects and machines as it executes. A
thread executes in an object by entering it through one of several
entry points; after the execution is complete the thread leaves the
object. The code in the object can contain a call to an operation
in another object with arguments. When the thread executes this
call, it temporarily leaves the calling object, enters the called
object and commences execution there. The thread returns to the
calling object after the execution in the called object terminates
with returned results. These argurnents/results are strictly data;
they may not be addresses. (Note that sysnames are data.) This
restriction is necessary as addresses which are meaningful in the
context of one object are meaningless in the context of another
object. In addition, object invocations can be nested.
Several threads can simultaneously enter an object and execute
concurrently (or in parallel, if the host machine is a multiprocessor). Multiple threads executing in the same object share the contents ofthe object's address space.
Unlike processes in conventional operating systems, a thread
can execute code in multiple address spaces. Visibility within an
address space is limited to that address space. Therefore, a thread
cannot access any data outside its current address space. Control
transfer between address spaces occurs through object invocation,

T6

Dasgupta et al.

and data transfer between address spaces occurs through
parameters.

3.3 Object/Thread Paradigm
The structure created by a system composed of objects and
threads has several interesting properties. First, all inter-object
interfaces are procedural. Object invocations are equivalent to
procedure calls on long-lived modules which do not share global
data. Machine boundaries are transparent to inter-object procedure calls or invocations. Local invocations and remote invocations are differentiated only by the operating system.
The storage mechanism used in Clouds differs from those
found in conventional operating systems. Conventionally, files are
used to store persistent data. Memory is associated with processes
and is volatile. The contents of memory associated with a process
are lost when the process terminates. Objects in Clouds unify the
concepts of persistent storage and memory to create the concept
of a persistent address space. This unifrcation makes programming paradigms simpler.
Although files can be implemented using objects (a frle is an
object with operations such as read, write, seek, and so on), the
need for having files disappears in most situations. Programs do
not need to store data in file-like entities, since they can keep the
data in the data spaces of objects, structured appropriately. The
need for user-level naming of files transforms to the need for
user-level naming of objects. Also, Clouds does not provide userlevel support for disk t/O, since frles are not supported by the
operating system. The system creates the illusion of a large
memory space that is persistent (non-volatile). Since memory is
persistent, the need for files to store persistent data is eliminated.
In the object/thread paradigm, the need for messages is eliminated. Like files, messages and ports can be easily simulated by
an object consisting of a bounded buffer that implements the send
and receive operations on the buffer. An arbitrary number of
threads may execute concurrently within an object. Thus,
memory in objects is inherently sharable memory.
The system therefore looks like a set of persístent address
spaces, comprising what we call object memory, which allow
The Design and Implementation of the Clouds Distributed Operating System

t7

control to flow through them. Activity is provided by threads
moving among the population of objects through invocation (Figures 1 and 2). The flow of data between objects is supported by
parameter passing.
dlstributed object space (non-volatile, virtual memory)

entry

\

points

i\
I

Figure

l:

Object Memory in Clouds

3.4 Objects and Types
At the operating system level there is only one type of object:
Clouds obiect. A Clouds object is an address space which may
contain one (or more) data segment(s) and one (or more) code
segment(s). At the operating system level, the Clouds object has
exactly one entry-point. User objects are built using Clouds
objects via an appropriate language and compiler. User objects
have multiple user-defined entry points and are implemented
using the single entry point, operation numbers and a jump table.
User objects and their entry points are typed. Static type
checking is performed on the object and entry point types. No

18

Dasgupta et al.

code segment
user-defined
operat¡ons

Figure

2:

Structure of a Clouds Object

runtime type checking is done by Clouds. Thus, type checking is
implemented completely at the language level and enforced by
compilers. A user object may contain several language defrned
object classes and object instances. These are completely contained within the user object and are not visible to the operating
system. Using this objectlthread paradigm, programmers can
define a set ofobjects that encapsulate the application at hand.
Currently, we defrne user objects in an extended C++ language.
The language supports single inheritance on Clouds user objects
using the Cr+ object structuring paradigm.

4. Clouds v.I
The first implementation of a kernel for Clouds was finished during 1986 and is described in Spafford [ 1986] and Pitts [1986]. The
kernel was broken up into four subsystems: object management,
The Design and Implementation of the Clouds Distributed Operating System

r9

storage management, communications, and action management.

A kernel-supported extension of the nested action model of Moss
[Moss 1981; Allchin 1983; Kenley 1986] made it possible for the
programmer to customize synchronization and recovery mechanisms with a set of locking and commit tools.
As one of the goals of Clouds was to produce an efficient and
usable system, a direct implementation on a bare machine
(VAX-I1) was preferred to an implementation on top of an existing operating system such as UNIX. Also most of the functionality needed by Clouds did not exist on UNIX (such as persistent
shared memory, threads) and most of the features provided by
UNIX were not needed for Clouds. The main goal of the implementation effort was to provide a proof of feasibility of implementing the object/thread model. The kernel, notwithstanding
some of its drawbacks, was a successful demonstration of the
feasibility of the Clouds approach, both in terms of implementation and use.

4.1 Objects
The basic primitives provided by the Clouds v./ kernel are
processes and passive objects. Indeed, one kernel design objective
was to support passive objects at the lowest possible level in the
kernel [Spafford 1986]. The main mechanism provided by the
kernel was object invocation. The system relied heavily on the
VAX virtual memory system to provide object support.
Passive objects ín Clouds v.l ate implemented as follows: the
VAX virtual address space is divided into three segments by the
system architecture, namely the P0, Pl and System segments. The
System segment is used to map the kernel code. The Pl segment
is used to map the process stack and the P0 segment is used to
map the object space. The object space resides on secondary
storage and page tables are used to map the object under invocation into the P0 segment directly from disk. Each process has its
own pageable process stack.
The above scheme allows processes to traverse several object
spaces, while executing, by simpty remapping the P0 segment of a
process to contain the appropriate object space upon each invocation (see the next section for further details). Also, several

20

Dasgupta et al.

processes are allowed to concurrently execute

within the same

object.

4.2 Object Invocation
The basic mechanism used in the Clouds kernel to process an
object invocation from a thread r executing in object 01 is the following: / constructs two argument lists, one for transferring arguments to the object being invoked, 02, înd the other to receive the
output parameters (results) from the invocation of object 02 (see
Spafford [ 1986] for more details). After the construction of the
argument lists, thread ¿ enters the kernel through a protected system call or trap. The kernel searches for the object locally and, if
found, uses the information in the object descriptor to construct
the page mappings for the P0 segment. Then, the kernel saves the
state of the thread, copies the arguments into the process space
(Pl segment) and sets up the new mappings for the P0 segment.
At this point, the contents of 02 are accessible through the mappings of the P0 segment, and t can proceed with the invocation of
O2t method.
On return from the invocation, the thread t also builds an
argument list with the return parameters, and then enters the kernel by means of a protected system call. The kernel now saves the
parameter in a temporary atea, sets up the P0 segment mappings
fot O¡ restores the saved state of t, and copies the return parameters wherever specifred by the second argument list constructed by
the thread at invocation time.
If upon invocation, the kernel cannot frnd object O2locally, it
tries to frnd it remotely. To do so, it broadcasts an RPC request.
The RPC server in the node that has 02 acknowledges the invocation request, and creates a local slave process to invoke 02 on
behalf of ¡. The slave process then proceeds to invoke o2locally,
and when the invocation completes, it sends the return arguments
back to the invoking node. Then the kernel goes on to process the
return parameters as described above.

The Design and Implementøtion of the Clouds Distributed Operating

System

2l

4.3 Experiences with Clouds v.I
Our experiences with Clouds v.1 were gained over a period of 6-8
months while attempting to build a user environment on top of
the kernel. During this time, we encountered a number of problems in the kernel that had to be addressed. Unfortunately, while
doing so, we discovered that complex module interdependencies
within the kernel made it extremely difficult to add necessary
functionality or fix non-trivial bugs without introducing new bugs
in the process. One reason for this was due to poor code structure. Also, direct concurrent access and update of data structures
by routines in different subsystems within the kernel resulted in
synchronization and re-entrancy problems.
The final problem involved a change of platform. For
economic reasons, we wanted to move to Sun workstations. However rrye found that VAX dependencies that appeared throughout
the kernel (such as programming the VAX bus) made the kernel
virtually impossible to port to a new architecture without a complete re-write.
In hindsight, these problems are not entirely surprising. While
portable kernels are desirable, portability was not emphasized during the design and implementation of Clouds v.1. We were more
interested in producing a working prototype. Also, Clouds v./ was
implemented in C. Given the lack of support in C for software
engineering techniques, it should not have come as a surprise that
our first attempt at structuring the internals of a kernel supporting
persistent objects resulted in a less than optimal design.
The basic design and philosophy behind Clouds v.2 is a direct
result of the problems we encountered in working with the first
kernel. The second kernel is a minimalketnel, designed with
flexibility, maintenance, and portability in mind. A complete discussion of the similarities and differences between Clouds v.I and
R¿ is contained in Section 6.

22

Dasgupta et al.

5. Clouds

v.2

The structure of Clouds v.2 is different from Clouds v.1. The
operating system consists of a minimal kernel called Ra, and a set
of systemJevel objects providing the operating system services.
.Rø [Bernabéu-Aubán et al. 1988; 1989] supports a set of basic system functions: virtual memory management, light-weight
processes, low-level scheduling, and support for extensibility.

5.1 The Minimal Kernel Approach
A minimal kernel provides a small set of operations and abstractions that can be effectively used to implement portable operating
systems independenl of the underlying hardware. The kernel
virtual machine that can be used to build operating systems. The minimal kernel idea is similar to the RISC approach
used by computer architects and has been effectively used to build
message-based operating systems such as V [Cheriton & Zwaenpoel 19831, Accent IRashid & Robertson 1981], and Amoeba
[Tanenbaum & Mullender l98l]. The rule we attempted to follow
in our design was: Any service that can be provided outside the
kernel wíthout adversely effecting perþrmance should not be
included in the kernel.
As a result, Rø is primarily a sophisticated memory manager
and a low-level scheduler. R¿ creates a view of memory in terms
of segments, windows and virtual spaces. Unlike the Clouds v.l
kernel, R¿ does not support objects, invocations, storage management, thread management, device management, network protocols, or any user services. All of these services are built on top of
the Ra kernel as modules called system objects. System objects
provide other systems services (user object management, synchronization, naming, networking, device handling, atomicity and
so on) and create the operating system environment. Currently
the Ra kernel and all of the essential system objects are operational; the project is now focusing on higher level services.
There are several advantages to the use of a minimal kernel.
The kernel is small, hence easier to build, debug, and maintain.
Minimal kernels assist in the separation of mechanisms from
creates a

The Design and Implementation of the Clouds Distributed Operating System

23

policy which is critical in achieving operating systems flexibility
and modularity [V/ulf et al. 19741. A minimal kernel provides the
mechanisms and the services above the kernel implement policy.
The services can often be added, removed or replaced without the
need for recompiling the kernel or rebooting the system.

5.2 The Ra Kernel
The principal objectives in Ra's design were:

. Ra should be a small kernel that creates a logical view of the
underlying machine.

. .Rø should be easily extensible.
. One of the possible extensions of Ra should be Clouds.
. It should be possible to efect an efficient implementation on
a variety of architectures.

In addition to the above, the implementation of Ra should
clearly identify and separate the parts that depend on the architecture for which the implementation is being targeted. This should
reduce the effort required to port the kernel to different
architectures.
As was previously stated, object invocation in the frrst version
of Clouds was implemented by manipulating the virtual memory
mappings. The Clouds v./ kernel was targeted to support object
invocations. R¿ is targeted to support the memory management
needs of objects. From our experience with the frrst Clouds ker
nel, we identifred generalizations in the virtual memory management mechanisms that we felt would provide alarget degree of
flexibility in the the design and implementation of new systems.
Memory is the primary abstraction in the Clouds paradigm.
Since objects are implemented as autonomous address spaces,
they need to be built out of sharable, pageable segments. These
segments should be easily migratable to support distributed
memory. These requirements led to the design of the virtual
memory architecture of R¿.
.Rø supports a set of primitive abstractions and an extension
facility. The three major abstractions are:

24

Dasgupta et al.

IsiBas. An IsiBat is an abstraction of activity, and is basically
a very lightweight process. The IsiBa is simply a schedulable
entity consisting of a PCB. An IsiBa has to be provided with a
stack segment and a code segment before it is scheduled. Therefore, IsiBas can be used to create user-level processes, threads, kernel processes, daemons and can be used for a host of tasks need-

ing activity.
Segments. Segments conceptualize persistent memory. A segment is a contiguous block of uninterpreted memory. Segments
are explicitly created and persist until destroyed. Each segment
has a unique system-wide sysname, and a collection of storage
attributes. Segments are stored in a facility called the partition.
Rø does not support partitions, but assumes the existence of at
least one. Partitions are described later. Ãa provides a set of
functions to manipulate segments (create, extend, install, pagein/out and so on).

Virtual Spaces. A virtual space abstracts a frxed-size contiguous region of an IsiBa's virtual address space. Ranges of memory
in a virtual space can be associated with (or mapped to) an arbitrary range of memory in a segment. Each such mapping (called a
window) also defrnes the protections (userJevel read, read-write,
kernel-level read, etc.) on the defrned range of memory. Virtual
spaces are defrned and controlled by a Virtual Space Descriptor or
VSD îor short (see Figure 3). Virtual spaces can be associated with
an IsiBa. This is called installing a virtual space. R¿ is responsible for ensuring that the state of the IsiBa's hardware address
space corresponds to that defrned by its installed virtual spaces
(the virtual memory architecture is described in more detail in
Section 5.3). Virtual spaces can be used by higher level routines
to implement such things as objects. Rø supplies functions that
assemble, install and manipulate virtual spaces.
Ra is implemented using the C+r language and heavily uses
the object oriented programming paradigm provided by C++. The

l.

The term IsiBa comes from early Egyptian. Isi = light, Ba = soul and was coined by
the early designers ofthe R¿ kernel. It is now felt that the term is confusing and we
periodically agree to change it but have not been able to agree on a replacement term
that is both informative and non-boring. The choice ofa replacement term is a
recurring topic of animated discussion.

The Design and Implementation of the Clouds Distributed Operating System

25

Vinual Space defined by descriptor

Figure

3:

VSD Defrning a Virtual Space

functional units in Ra arc encapsulated in C++ objects and Cr+
classes are used to provide structure to the implementation. Cr+
classes are used, for example, to defrne base classes (and hence

minimal interfaces) for system objects, devices, partitions, virtual
memory managers, etc.
J?ø is related to the Mach kernel [Accetta et al. 1986] by its
some of its virtual memory mechanisms and its approach to portable design [Rashid et al. 1987]. The Choices kernel [Campbell et
al. 19871 is built using object-oriented programming techniques
and is thus related to the implementation structure of Rø. During
the design of Ra, we were also influenced by ideas from Multics
[Organick 19721, Hydra [Wulf et aI. 1974], and Accent [Rachid &
Robertson 1981l, as have other distributed operating systems
designed in the tradition of Clouds [Nett et al. 1986; Northcutt
le87l.

26

Dasgupta et al.

5.3 Virtual Memory Architecture
Ra provides a two-level virtual memory architecture similar to
Clouds v./ (similarities and differences are discussed in Section 6).
Rø breaks down the machine's virtual address space, hereafter
referred to as the hardware address space, into a number of frxedsized, contiguous, non-overlapping regions. These regions are
used to map different types of virtual spaces.
5.3.1 Virtual Spaces

Ra supports three types of virtual spaces: O, P, and K (see Figure
a). The contents of each virtual space (of any type, O, P, or K) is
described by a set of windows in the virtual space descriptor.
Each window lr associates protection with a range (x,x +n) of virtual memory controlled by the virtual space and associates a range
(y,y +n) of bytes in a segment s into the virtual memory range
(x,x+n). An access of byte x+a in window w will reference byte
y+a in segment s. A virtual space V may contain an arbitrary
Virtual Space

O-Space

Partition (storage)

I

Segments

I

P-Space

K-Space

Figure

4:

Hardware Address Space

The Design and Implementation of the Clouds Distributed Operating System

27

number of windows mapping non-overlapping ranges within the
bounds of the region controlled by the virtual space.
Instances of virtual spaces of type O (O space) are intended to
map shared entities (e.g. objects), and spaces of type P (P space)
are intended to map process private entities (e.g. stacks). In a
single-processor system there can be only one instance of a virtual
space of type K (K space). This is used to map the Ra kernel and
the system objects. There are additional operations that K space
descriptors must support in addition to the standard operations
defined by the virtual space class. This is handled by making the
Kernel Virtual Space Descriptors (KVSD) a derived class (subclass)
from the base class VSD.
Each IsiBa can have one O space, one P space, and one
K space currently installed. In the current uniprocessor implementation, every IsiBa has the same K space installed, effectively
mapping the kernel into the address space of every IsiBa.
5.3.2 Shared Memory

Sharing memory occurs when two virtual addresses refer to the
same piece of physical memory. In the Rø kernel, sharing can
take place at two levels. Two IsiBas may share memory at the virtual space level by installing the same P or O (or both) virtual
space. When they reference memory within the bounds of the
shared virtual space, they will automatically reference the same
piece of physical memory or backing store.
Memory may also be shared at window level. If virtual space
Vl has a window wI (a,a+n) that maps to (x,x+n) within the segment s and a virtual space V2 has a mapped window w2 (b,b +m)
that maps to $t,y +m) within the same segment s and the two
ranges overlap, then the overlapping area of memory will be
shared by the two windows w1 and w2. To take a simple case, if
wI and w2 both map to the same range of addresses in the segment s (that is, m = n), then a reference to address a+2,(z<n) and
a reference to address b+z will both reference the same memory.
Furthermore, it does not matter to the R¿ kernel if W and V2 are
the same virtual space (in which case memory aliasing will occur)
or different virtual spaces, nor if VI and V2 are different types of
virtual spaces (which will also cause memory aliasing if both Vl

28

Dasgupta et al.

installed in the same IsiBa). Thus, two IsiBas may
share memory by installing the same virtual space. Also they may
share memory if they have diferent virtual spaces installed but
the virtual spaces map common segments.

aîd

V2 are

5.4 Ra Synchronization Primitives
The kernel defines a set of primitives to be used for synchronization and concurrency control inside the kernel and system objects.
At the lowest level, Ra defines (machine-dependent) intemrpt level
control facilities. Since R¿ is intended to be portable to multiprocessors, spin locks are provided. These low-level facilities are
used to construct kernel-level semaphores, memoryless events, and
read/write locks.

5.5 Extensibility
The design of Ra abstracts a logical machine that provides virtual
memory mechanisms and low level scheduling. Hence, it is unusable by itself, and must be extended using system objects into an
operating system such as Clouds. System objects reside in kernel
space (K space), not vseÍ space (O and P space), have direct access
to data and code in the kernel, and share the same protection and
privileges. Hence, system objects are not the same as Clouds
objects. Some system objects are called essential system objects
because they are essential to run Ra. For example, the Partition is
an essential system object. .Rø assumes the existence of at least
one partition.
The system objects in Clouds are organized in a hierarchy of
classes, ultimately deriving from the SysObi class. Rø defrnes a
system object interface and each system object must adhere to this
interface. The system object must have initialize and shutdown
methods, can have private memory and can access kernel data
structures though kernel classes.
Kernel classes are collections of kernel data and procedures to
access and manipulate that data. These include the VSD class,
sysname class, and cpu class. As viewed from a system object, the
kernel is a collection ofkernel classes and instances ofthese
classes. Each cpu, for example, is a specific instance of the cpu
The Design and Implementation of the Clouds Distributed Operating System

29

class and can be manipulated using methods defined by the cpu
class.

The system object interface enforces the strict adherence to
modularity in the operating system. Clouds has been built by
attaching all the relevant system objects to Ra. The system
objects can be thought of as plug-in software modules that can be
linked in with the kernel or loaded dynamically into a running
system.

5.6 Partitions
Partitions are repositories for segments. Rø requests segments
from a partition when needed and releases them to the partition
when their usage is over. The partition is responsible for managing segments on secondary storage. A partition must support at
least the following operations: ActivateSegment, DeactivateSegmen¡ ReadPage, and WritePage. A segment must be activated
before being used, by calling the ActivateSegment operation. This
allows the partition to set up any necessary in-memory data structures. DeactivateSegment is then called after the kernel has
frnished using the segment. ReadPage and WrítePage are selfexplanatory.
A partition is responsible for maintaining and manipulating
segments. Each segment is maintained by exactly one partition,
and the segment is said to reside in that partition. The partition
in which the segment resides is called the controllíng paftition.
Creation and deletion of segments is performed through their controlling partitions. The partition is responsible for maintaining
the information which describes the segment in secondary storage,
similar to file-system code in a conventional operating system. To
read or write segment pages on secondary storage, the controlling
partition of the segment is invoked. The partition is notifred that
one of its segments will be subject to further activity by activating
the segment. Similarly, the partition is told that a segment will
not be used in the near future by deactivating the segment.
In our implementation, a network disk partition stores segments on a UNIX file-server. Each segment is stored as a UNIX
frle. The partition provides pages of segments to Rø over a network. The local lR¿ kernel is unaware of this mechanism. The

30

Dasgupta et al.

network disk partition provides an easy way to create segments on
UNIX which are then available to Clouds machines. This partition is also used for paging and swapping activity.

5.7 Distributed Shared Memory
Segments residing

in a network disk partition cannot be shared by

multiple machines. Thus each network disk partition creates a
private storage space for each Clouds machine.
However, to allow distribution, all objects should be accessible
by all Clouds machines. This means the segments should be
stored by a set of data-servers and should be accessible by all
compute-servers. This is allowed by Distributed Shared Memory
Partitions (DSM Partitions).
Sharing segments among machines may lead to multiple
cached copies of the segments. To ensure one-copy semantics
used by the Clouds paradigm, a coherence controller is necessary.
The DSM partition implements a set of protocols which enforces
the coherence of shared segments (Figure 5). Each DSM partition
knows about a set of segment managers called DSM controllers.
Currently, DSM controllers run on UNIX machines and store segments as files. When Clouds accesses a segment (due to the invocation of an object that uses that segment), the DSM partition on
the local machine gets the segment from the DSM controller. The
DSM controller runs coherence algorithms which ensure that there
is only one copy of the segment in the Clouds system [Ramachandran et al. 1989; Khalidi 1989]. This makes memory appear to be
one-copy shared and creates the view that all objects reside on all
machines.

6. Ra and Clouds v.l
The design of Ra draws heavily on our experiences with Clouds
v.1. The Clouds y.1 was essentially a monolithic kernel. Expandability and flexibility were not primary design goals.
As every academic/commercial operating systems evolves, new
sets of researchers and implementors add to the design and functionality in ways not anticipated by the original designers. While
The Design and Implementation of the Clouds Distributed Operating

System 3l

Clouds

Clouds

Clouds

Ra

Ra

Ra

Partition

Part¡tion

Partition

Client.

Cllent.

Client.

Network

Partition Server

/;n'";

Partition Server

--

/;nr";

.-

Figure 5: DSM Partitions

attempting to build a user environment for Clouds v.l, we realized
that it was difficult to extend the system. A number of features in
Ra attempt to address this problem.
Rø supports a more primitive set of abstractions than Clouds
v..1, however, the system object class allows the addition of operating system services not supported by the kernel. System objects
allow flexibility and enforce modularity. This structuring tech'
nique is is worthy of note in that while many systems allow extensions in the form of device drivers, far more fundamental operating system services have been implemented as.Rø system objects.
To support testing of higherJevel algorithms, the Ra design
also strives to separate mechanisms that implement actions from
policies that decide when actions should be taken. Furthermore,
the object-oriented structure of the R¿ kernel lends itself to easier
maintenance than the Clouds v.1 kernel.

32

Dasgupta et al.

6.1 Multi-threading
In the Clouds v.I design, processes were somewhat heavyweight
and the kernel was basically passive. This meant that virtually all
operating system services were provided through intemrpt and
fault handlers. Therefore, the entire kernel had to be multithreaded. This necessitated very delicate handling of interrupt
priority levels and synchronization primitives throughout the
entire kernel. The problem was compounded by the fact that
concurrency-related mistakes usually result in non-reproducible
timing-related errors which are very difficult to debug. Unfortunately, in a 22,000line multi-threaded kernel written from
scratch by a handful of people, it is virtually guaranteed that
concurrency-related errors will creep in.
In Ra,IsiBas can be used to implement kernel daemons. Kernel daemons run in kernel space with kernel privileges. These
daemons can be used to implement single-threaded servers that
are dispatched using interrupts. This reduces the amount of code
that has to be multi-threaded and run with intemrpts masked.
Kernel daemons have been used in a number of places, most notably in the implementation of remote procedure calls, timer services, network protocols and DSM.

6.2 Virtual Memory Architecture
The design and implementation of Rø's virtual memory architecture incorporates a generalization of the virtual memory architecture of Clouds v.1 and portability-oriented structuring ideas found
in Mach [Rashid et al. 1987]. Rather than embed support deeply
in the kernel for one notion of how objects could be structured,
the decision was made to support a flexible, sophisticated virtual
memory architecture that could be adapted to more than one
object implementation. This would allow future developers the
freedom to modify the object implementation without having to
re-write kernel internals.
R¿ thus supports the notion of an O space, P space, and
K space. By virtue of using the VAX P0, Pl, and System segments
to contain objects, the process stack, and the kernel, respectively,
Clouds v./ also supported the equivalent of an O, P, and K space.
The Design and Implementation of the Clouds Distributed Operating

System 33

However, since support for objects, processes, and actions was
designed into the kernel at the lowest possible level, the structuring of each type of space was hard-coded into the kernel. The P0
segment was broken up into windows but the Pl and System segments were not. In Ra, the structure of the Clouds v.1 P0 segment
is generalized into the notion of a virtual space and applied in an
orthogonal fashion to all three types of virtual spaces. This generalization leads to a number of advantages.
By structuring the hardware address space as virtual spaces, it
is possible in R¿ to break the address space up into more than
three virtual spaces. V/hile the original designers foresaw only the
need for an O, P, and K space, our recent work in memory semantics has convinced us of the need to add a fourth virtual space
[Dasgupta & Chen 1990; Chen 1990]. Due to the uniform handling of virtual spaces in the kernel, adding another virtual space
will be straightforward.
The fact that P space can be broken up into windows allows
more flexibility in designing and implementing user processes and
object invocation. The original Clouds v./ desþ placed all nonstack per-process data in P0 space. The current user object implementation could have been implemented similarly. However, it
was decided to have all per-process data (stack, heap, parameter
passing area) reside in P space instead, which means the entire
object space can be shared. This allows all threads executing in
the same object to share the same O space virtual memory tables.
The Rø kernel is capable of supporting either implementation.
The K space windows allow the kernel and system objects to
use the window class to map portions of segments into welldefined address ranges, operate on the data there, flush the
changes out and unmap the window (freeing it up for further use).
This facility is used by the user process controller to set up a new
processes and freeze/restore them (see Section 8.2).
Also, the -R¿ virtual memory system allows for the existence of
more than one virtual memory manager. Virtual memory
managers handle page faults. Each Rø segment can have a segment type and each segment type can have its own virtual
memory manager. V/hen a page fault occurs, the kernel determines the segment being accessed by the fault. If the segment has
its own virtual memory manager, the kernel calls that manager.
34

Dasgupta et al.

Otherwise, the kernel calls a default virtual memory manager to
service the fault. This'design allows the complexity of services
such as shadow-based recovery to be isolated in its own virtual
memory manager.
Finally, unlike tllre Clouds v.1 virtual memory system, the Ra
virtual memory system is designed with machine independent and
machine dependent classes (inspired by Mach). Each machine
independent class that may have to rely extensively on machinedependent details has a corresponding machine-dependent class
that presents an abstraction of the underlying hardware to the rest
of the kernel. This isolates the machine dependencies from the
rest ofthe kernel.

7. Implementation of Ra
R¿ is implemented in Cr+ [Stroustrup 1986] on the Sun-3/60
architecture. C++ was chosen over C due to the extra support for
data abstraction, and object-oriented design.
C++ facilities such as private data and methods, derived
classes, and virtual functions make it easier to write modular, layered code and hide the implementation details of one part of the
kernel from the rest. This, in turn, reduces hidden interdependencies which makes it easer to change parts of the system without
introducing unforeseen side-effects. While all good software
designers strive to do this, the language support (and enforcement)
provided by C+r has made this a much easier task. Furthermore,
the C++ inline function facility makes it possible to write highlylayered/modular code without incurring extra function-call overhead.
Thus, while the kernel internals are quite intricate, welldefined interfaces exist for requesting kernel services. C++ typechecking enforces adherence to the defined interfaces and hence
prevents system implementors from by-passing those interfaces,
but the ability to deflne optional parameters in class methods
enables interfaces to be easily extended while retaining compatibility with existing code.
The design and implementation of Ra is designed to identify
and isolate machine dependencies. Like the virtual memory
The Design and Implementation of the Clouds Distributed Operating System

35

system, Ra is divided into two sets of frles containing machine
dependent and machine independent classes and definitions.
The implementation of the R¿ kernel consists of about 1,000
lines of assembly code and 12,000 lines of Cr+ code. Approximately 6,000 lines are machine dependent code while the rest are
machine independent. In addition, 17,000 lines of C++ code have
been added to the system in the form of system objects.

8. Clouds v.2 and Rq
Clouds v.2 consists of the Rø kernel plus a collection of system
objects implementing Clouds semantics (see Figures 6 and 7).
The system objects contain more code than the R¿ kernel
itself. A complete description of these are beyond the scope of the
paper. The system objects currently in operation include: buffer
manager, user I/O manager, tty manager, Ethernet driver, Ra
Transport Protocol, network disk partition, DSM partition, object
manager, thread manager, process controller, RPC controller, and

I

ì

The Ra Kernel

L::::J G;-f;h..g I
-Fh.'p
ctassA

I

The System Area

@
@
@
Figure

36

Dasgupta et al.

6:

System Objects and System Space

system monitor. Many of the system objects are implemented
using kernel daemons. Some of the key functions of the system
objects are described below.

8.1 Object Management
An object is implemented in Raby using a virtual space (O space).
The storage of the object is ultimately accessed via the segments
mapped by the windows of the virtual space. The sysname of the
object is the sysname of a segment containing the windowing
information (VSD). Objects are always mapped into the O space
when being executed. A Clouds object is a special case of a Ra
virtual space.
The object invocation mechanism is implemented by means of
two system objects: the Process Controller and Object Manager.
Processes are implemented using segments (to back the perprocess stack for example) and are managed by the process
User Objects

Scheduling
Segments
lnterrupts
Virtual Space

Figure

7: The Clouds/Ra

Environment

The Design and Implementation of the Ctouds Distributed Operating

System 37

controller. Objects are invoked by threads (implemented using
processes). When a thread invokes an object, the process controller is responsible for saving/protecting the state of the current
invocation and installing any state required by the new invocation. During this process, the process controller notifies the object
manager that an object is about to be invoked (referenced) and
that object manager is responsible for ensuring that the control
information (virtual space descriptor) for the object exists. The
object is installed in the O space of the thread and the thread is
placed in the entry point. The physical location of the object is of
no consequence as the DSM partition will page in the segments
from the appropriate DSM controller (or server). Remote object
invocation is performed through an RPC handler which communicates to an RPC server on a remote machine and asks it to invoke
the requested object in a manner similar to the Clouds v.1 object
invocation mechanism.
Invoking the object locally and relying on DSM to page in the
object across the network is not always more efficient than performing a remote procedure call. If processes on different nodes
have the same locality of reference in an object and they invoke
the same object using DSM to page the object, DSM will thrash,
paging the common pages back and forth between the different
nodes. In that case, it might be better to move all the computation to the same node by performing an RPC where the processes
can physically share the memory.
In addition, if the load on the nodes of the system is not balanced, it may be a better idea to send the computation to a
remote node. Notice that the object being invoked need not
reside on the chosen node as DSM can be used to access the
object's segments. This provides another mechanism besides process migration that can be used to balance the load on the system.
However, using local invocations and DSM is superior to RPC in
cases where there is a high degree of locality.
The policy that decides whether to execute a local invocation
or a remote invocation has not been implemented. Currently the
user can decide on the invocation mechanism by using appropriate system calls.

38

Dasgupta et al.

8.2 Thread Management
Processes are implemented by associating an IsiBa with a virtual
space (installed in the P space) which controls per-process memory
such as the process stack and the parameter passing areas. A
thread is a process if the thread only uses local invocations, and is
a collection of processes if the thread has performed a remote
invocation. The thread manager keeps track of a thread's RPC

calls (if any) and controls creation and termination of threads.
However, thread managers do not directly manipulate processes or
per-process memory. The process controller defrnes an abstraction
of a process. The thread manager makes requests on the process
controller to manipulate processes and to perform object invocations and returns.
Although the P space of a process may contain more than one
segment, the state of a process may be saved into one controlling
segment and then later restored. The ability to freeze a process
into one segment and remotely activate that segment (and all
necessary segments after that) using DSM provides Clouds v.2 with
a simple, easy way of performing process migration.

8.3 Ra Transport Protocol
The R¿ Transport Protocol (RaTP) provides reliable message transactions over the Ethernet [Wilkenloh 1989]. The protocol is
designed to be connectionless and is efficient for providing the
request-reply form of communication that is common in clientserver interactions. Since Clouds supports object invocations
using RPC or DSM, this is the type of communication that is
encountered in the system.
RaTP has been implemented both on Ra as well as on UNIX.
In addition to message transaction, RaTP provides interfaces to
the RPC and DSM mechanisms. We are currently using RaTP to
run the DSM clients on Ra and DSM servers on UNIX file servers.

The Design ønd Implemenlation of the Clouds Distributed Operating

System 39

8.4 Clouds I/O System
The Clouds I/O system allows user objects to perform user-level
terminal I/O. Note that Clouds does not support nor does it need
user-level disk or network I/O. The I/O system is handled by a
system object. Each user object is provided with two special
sysnames of I¡o objects (called stdin and stdout, inspired by
UNIX). These objects do not actually exist. They are operating
system level pseudo-objects that support read and write calls. If a
user object calls the write routine, the output reaches the (logical)
terminal associated with the thread. Each logical terminal is a
"text window" on a UNIX workstation.
When a thread is created, a logical terminal is associated with
the thread. The thread carries with it the sysname associated with
this text window. Thus, all I/O calls made from object programs
reach the user, regardless ofwhere the thread is executing. I/O
redirection can be done by simply changing the stdin/stdout
sysnames to those of a user object that supports read and write
calls. Further I/O will cause that user object to be invoked instead
of the Clouds I/O system object.

8.5 Other Services
The other system objects used in the current implementation of
Clouds include a Virtual Memory Manager, System Monitor,
Buffer Manager, Ethernet driver, network disk partition, and DSM
partition.
Devices in Ra conform to a standard interface: a class
definition called RaDevice which is used to define device drivers.
All device drivers must provide at least the methods defined in
the base class RaDevíce. They are open), close), read), write),
getmsg), putmsg), poll), and ioctl).
The system monitor provides a low-level monitor/shell capability. The monitor can be used to read and alter values of variables in the kernel or system objects, as well as execute arbitrary
methods in the kernel or any system object. The system monitor
can also be used for invoking user objects, thus providing a rudimentary shell for Clouds.

40

Dasgupta et al.

The network disk partition is implemented as a system object.
Segments managed by this partition seem to reside on the Clouds
node but actually reside on a UNIX frle server. Reads, writes, and
control messages are shipped to the UNIX system where a server
operates on the UNIX files that correspond to the indicated segments (see Figure 8).
Many services are provided by UNIX programs. These include
terminal emulators, which run under SunWindows and on top of
RaTP interfacing through the user I/O system on rR¿. The compiler is a modifred Gnu C++ compiler that generates Clouds segments. The Clouds shell is a UNIX program that is used to invoke
Clouds objects using the RPC facility.

Figure

8: Network Disk

and DSM Services

The Design and Implementation of the Clouds Distributed Operating

System 4l

8.6 Status and Work in Progress
The facilities described above are operational. Project work is
currently focusing on better system-level and user-level support
tools for effective use of the Clouds operating system. This
includes a user programming environment, a user-level naming
system, and development of application programming techniques
in the persistent object framework.
In addition, work is being done in the area of reliability and
persistent-object programming, the thrust of the original Clouds
system. We have explored memory semantics for programming
persistent objects and have developed a set of flexible mechanisms
that support customized data consistency [Chen & Dasgupta 1989;
Dasgupta & Chen 1990; Chen 1990]. In other fault tolerance
research, we have developed a scheme that replicates data as well
as computation to guarantee forward progress of computations
[Ahamad et al. 1990]. Work is underway to design schemes that
exploit multicast communication to make a variety of services
(e.g. object location, group communication, commit protocols,
replication management, and so on) more efficient [Ahamad &
Belkeir 1989; Belkeir & Ahamad 19891.

9. Concluding Remarks
Clouds is intended to serve as a base for research in distributed
computing at Georgia Tech. The new Clouds kernel, Rø, coupled
with system objects, provides an elegant environment for operating systems development. The design and implementation of Ra
benefited from the experience gained from the design and implementation of the frrst Clouds kernel.
The design and implementation of Cbuds v.2 are geared more
towards flexibility, portability, and maintenance than the Clouds
v.1 kernel. This is reflected in the design of the Rø kernel and its
virtual memory architecture, support for lightweight kernel daemons, and system object facility. Furthermore, the additional
freedom allowed by Ra facilitates more easy testing of alternative
system designs (both mechanisms and algorithms) using R¿ as the

implementation base.
42

Dasgupta et al.

V/e would like to acknowledge all those who have worked on
the Clouds project, past and present, for making this design and
implementation possible. This includes Jim Allchin, Martin
McKendry, Gene Spaford, Dave Pitts, Tom Wilkes and Henry
Strickland for their contributionsto Clouds u..1, and Nasr Be-lkeir,
M. Chelliah, Vibby Gottemut¡kala, Ranjit John, Ajay Mohindr4
Gautam Shah, and Monica Skidmore for their contributions to
Clouds v.2.

The Design and Implementation of the Clouds Distibuted Operating

Systeru 43

References
M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevanian,
and M. Young, Mach: A New Kernel Foundation for UNIX
Development, Proceedings of the Summer USENIX Conference,
July 1986.
M. Ahamad and N. E. Belkeir, Using Multicast Communication for
Dynamic Load Balancing in Local Area Networks, ln 14th Annual
Conf. on Local Computer networlcs, October, 1989.
M. Ahamad, P. Dasgupta, and R. J. LeBlanc, Fault-Tolerant Atomic
Computations in an Object-based Distributed System, Dístributed
Computing Journal, April, 1990.
J. E. Allchin, An Architecture for Reliable Decentralized Systems, Ph.D.
Thesis, School of Information and Computer Science, Georgia
Institute of Technology, 1983. (Available as Technical Report

crr-rcs-83/23.)

N. Belkeir and M. Ahamad, Low Cost Algorithms for Message Delivery
in Dynamic Multicast Groups, ln Proceedings of the 9th International Conference on Distributed Computing Systems, June, 1989.
J. M. Bernabéu-Aubán, P. W. Hutto, M.Y.Khalidi, M. Ahamad,
W. F. Appelbe, P. Dasgupta, R. J. LeBlanc, and
U. Ramachandran, Clouds - A Distributed, Object-Based Operating System: Architecture and Kernel Implementation, European
UNIX systems User Group Autumn Conference, October 1988.
J. M. Bernabéu-Aubán, P. W. Hutto, M.Y.Khalidi, M. Ahamad,
W. F. Appelbe, P. Dasgupta, R. J. LeBlanc, and
U. Ramachandran, The Architecture of Ra: A Kernel for Clouds,
Proceedings of the Twenty-Second Annual Hawaii International
Conference on System Sciences, January 1989.

R. Campbell, G. Johnston, and V. Russo, Choices (Class Hierarchichal
Open Interface for Custom Embedded Systems), Operating System
Review, 2l(3), July 1987.
Raymond C. Chen, Consistency Mechanisms and Memory Semantics for
Persistent Object-Based Distributed Systems, Ph.D. Thesis, School
of Information and Computer Science, Georgia Institute of Technology, 1990. (In Progress.)

R. C. Chen and P. Dasgupta, Linking Consistency with Object/Thread
Semantics: An Approach to Robust Computation,ln Proceedings
of the 9th Internatíonal Conference on Distributed Computing Systems, Jlune 1989.

44

Dasgupta et al.

D. Cheriton and W. Zwaenpoel, The Distributed V Kernel and its Performance for Diskless Workstations, Proceedings Of the gth ACM
Symposium on Operating System Principles, pages 129-139,

October l0-13,

1983.

P. Dasgupta and R. C. Chen, Memory Semantics for Programming Persistent Objects, 1990. (In Progress.)
Greg Kenley, An Action Management System for a Distributed Operating System, Master's Thesis, School of Information and Computer
Science, Georgia Institute of Technology, 1986.

M. Yousef A. Khalidi, Hardware Support for Distributed Object-based
Systems, Ph.D. Thesis, School of Information and Computer Science, Georyia Institute of Technology, 1989. (Available as Technical Report GIT-ICS-89/I 9.)
J. Moss, Nested Transactions: An Approach to Reliable Distributed
Computing, Technical report MIT/LCS/TR-260, MIT Laboratory for

Computer Science, 1981.
E. Nett, J. Kaiser, and R. Kroger, Providing Recoverability in a Transaction Oriented Distributed Operating System, 6th International
Conference on Distributed Computing Systems, pages 590-597,
1986.

J. D. Northcutt, Mechanisms for Reliable Distributed Real-Time Operat
ing Systems, Volume 16 of Perspectives in Computing, Academic
Press, 1987. Editors: W. Rheinboldt and D. Siewiorek.
E. E. Organick, The Multics System, MIT Press, 1972.

David V. Pitts, A Storage Management System for a Reliable Distributed
Operating System, Ph.D. Thesis, School of Information and Computer Science, Georgia Institute of Technology, 1986. (Available
as Technical Report GIT-ICS-86/21.)
Umakishore Ramachandran, Mustaque Ahamad, and M. Yousef
A. Khalidi, Coherence of Distributed Shared Memory: Unifying
Synchronization and Data Transfer, In Eighteenth Annual International Conference on Parallel Processing, August 1989.
R. Rashid and G. Robertson, Accent: A Communication Oriented Network Operating System Kernel, Proceedings of the 9th Symposíum
on Operating Systems Principles , pages 64-7 5, December I 98 I .
R. Rashid, A. Tevanian, M. Young, D. Golub, R. Baron, D. Black,
W. Bolosþ, and J. Chew, Machine-Independent Virtual Memory
Management for Paged Uniprocessor and Multiprocessor Architectures, In Proc. 2nd International Conference on Architectural

The Design and Implementation of the Clouds Distributed Operating System

45

Support

for Programming Languages and Operatíng Systems

(ASPLOS

il),

pages 3l-39, October 1987.

Eugene. H. Spafford, Kernel Structures for a Distributed Operating Sys-

tem, Ph.D. Thesis, School of Information and Computer Science,
Georgia Tech, 1986. Available as Technical Report GIT-ICS86/l 6.)

B. Stroustrup, The C++ Programming Language, Addison-rWesley Publishing Company, 1986.

A. S. Tanenbaum and S. J. Mullender, An Overview of the Amoeba Distributed Operating System, Operating System Review, I 3(3):5 l-64,
July 1981.
Christopher J. Wilkenloh, Design of a Reliable Message Transaction Protocol, Master's Thesis, School of Information and Computer Science, Georgia Institute of Technology, 1989.
C. Thomas Wilkes, Programming Methodologies for Resilience and
Availability, Ph.D. Thesis, School of Information and Computer
Science, Georgia Institute of Technology, 1987. (Available as
Technical Report GIT-ICS-87/32.)

W. Wulf, E. Cohen, W. Corwin, A. Jones, R. Levin, C. Pierson, and
F. Pollack, Hydra: The Kernel of a Multiprocessor Operating System, Communications of the ACM,17(6):337-345, Jlune 1974.
W. A. Wull R. Levin, and S. P. Harbison, HYDRA/C.mmp, An Experi'
mental Computer System, McGraw-Hill, Inc., 1981.

46

Dasgupta et al.

CALYPSO:A Novel Software System for
Fault-Tolerant Parallel Processing on Distributed Platforms4
Arash Baratloot
New York University

Partha Dasguptas
Arizona State University

Abstract
The importance of adapting networks of workstations
for use as parallel processing platfomzs is well established.
Howevel; current solutions do not always address important issues that exist in real networks. External factors like
the sharing of resources, unpredictable behavior of the network, and failures, are present in multiuser networks and
must be addressed.
CALYPSO
is a prototype sofnvare system for writing and
executing parallel programs on non-dedicated platforms,
based on COTS networked workstations, operating systems, and compilers. Among notable properties of the
system are: ( I ) simple programming paradigm incorporating shared memory constructs and separating the programming and the execution parallelism, ( 2 ) transparent
utilization of unreliable shared resources by providing dynamic load balancing and fault tolerance, and (3)effective
perj-ormancefor large classes of coarse-grained computations.
Wepresent the system and report our initial experiments
and perj-omnce results in settings that closely resemble
the dynamic behavior of a “real” network. Under varying work-load conditions, resource availability and process
failures, the eficiency of the test program we present ranged
from 84% to 94% bench-marked against a sequential program.

1 Goals and Properties of CALYPSO
Networks of workstations exist in many organizations,
and their number is growing rapidly. These machines are
mostly idle. Thus, it is very attractive and cost-effective to
*This research was partially supported by the National Science Foundation under grant numbers CCR-91-03953, CCR-94.11590, and CCR95-05519.
t For additional publications and information about the CALYPSO
project, see the URLs: ftp://cs.eas.asu.edu/pub/calypso/
and ftp://cs.nyu.edu/pub/calypso/
$251 Mercer St., New York, NY 10012-1185, (212) 998-3350,
baratloo@cs.nyu.edu.
5 ASU,
Tempe,
AZ
85287-5406,
(602)
965-5583,
partha@cs.eas.asu.edu.
8251 Mercer St., New York, NY 10012-1185, (212) 998-3101,
kedem@cs.nyu.edu.

122
1082-8907/95 $4.00 0 1995 IEEE

Zvi M. KedemT
New York University

utilize this frequently wasted resource. Given the previous
reasons, then why is it that programs that utilize networks of
workstations have not proliferated? A major reason is that
the cost to harness this power is too high. That is, although
networks of workstations are a good value in terms of raw
computing power (meaning hardware), the cost to harness
this power (meaning software development) still remains
high and unattractive.
There are systems to utilize this hidden power, but they
often do not address some important issues. For example
they require extensive changes to the programming model,
or they are unable to handle the separation of programming
and execution parallelism, or they cannot deal with failures,
or they lack adequate load distribution and balancing to
account for slow and fast machines.
The challenging problems in providing a satisfactory environment for parallel processing on networks of workstations are well known. Such problems include programmability, high-performance, scalability, load balancing, and
fault-masking. We have addressed most of these in the
design and the implementation of CALYPSO.
Furthermore, as the commercial and the administrative
realities prohibit the vast majority of users from buying
special purpose hardware and running “private” operating
systems, CALYPSO
utilizes standard hardware and standard
software. The current prototype runs under SunOS, and the
system has been designed and implemented to be portable.
We expect ports to run on most Unix-based operating systems as well as Windows NT.
CALYPSO
is a prototype system for writing and executing parallel programs on multiuser networks. It it unique
among other systems in that it provides the following features through a unified set of solutions.
0

Ease of Programming: The programs are written in
CALYPSO
Source Language (CSL). CSL is C++ with

added constructs to express parallelism. It is based on
the shared memory model and it is very simple to learn
and use. The programmer does not have to partition
the data, or to specify how to synchronize or move it
among the workstations.
0

Separation of Programming Parallelism from Ex-

ecution Parallelism: The parallelism inherent to a
problem is independent of the number machines it
will run on. So, why should a parallel program be tied
to the number of available workstations-a number
that in most cases is unpredictable and transient? This
is a common weakness in many systems, and adds
complexity to an already difficult program development. The mapping between program parallelism and
execution parallelism is transparent in CALYPSO.

not stall while dealing with system’s asynchrony and faults.
Thirdly, newly available maichines can transparently be integrated into an executing computation. And finally, any
of the machines that are “helping out” the parallel computation can fail or slow down at any time.
The mechanism of collating differential memory provides logical coherence andl synchronization while avoiding false sharing. It is an aidaption and refinement of the
two-phase idempotent execution strategy [26] among others. Memory updates are collated to assure exactly-once
logical execution, and they are transmitted as bitwise differences, preventing false sharing. This supports efficient
implementation of idempotence in addition to other performance benefits that we shall see later.

Dynamic Load Balancing and Fault Tolerance:
CALYPSO
automatically distributes the work-load depending on the dynamics of the participating machines. Faster machines are not blocked waiting for
slower machines to finish their “work assignments”-faster machines overtake the slower ones. In addition,
CALYPSO
executions are resilient to failures. All processes, other than a specific designated “manager”
process, can fail at any point without affecting the
correctness of the computation. In contrast with other
fault-tolerant systems, there is effectively no additional cost associated with this feature in the absence
of failures. This is confirmed by our initial observation that the performance of CALYPSO
is comparable
to other non-fault-tolerant systems.

2 Previous and Related Work

High Performance: While providing the features
listed above, our initial experiments indicate that the
overhead is small, and a large class of coarse-grained
computations can benefit from CALYPSO.
The paradigm that CALYPSOembodies in a working
system, was first described in [26]. That paper details a
methodology for instrumenting general parallel programs
to automatically obtain their fault-tolerant counterparts.
While the solutions in [26] were formulated in the context of
synchronous faults, they were later applied in [24] to a certain variant of asynchronous behavior (as does CALYPSO).
A unified set of mechanisms, eager scheduling and collating differential memory, is used to provide the functionality of CALYPSO.
The idempotence property is fundamental in CALYPSO:a code segment can be executed multiple
times (with possibly some partial executions), with exactlyonce semantics.
The importance of idempotence, and the utilization of
the eager scheduling to take advantage of it, was discovered
in [26] in an abstract context. (The term “eager scheduling” itself was coined recently.) Eager scheduling is a
mechanism for assigning concurrently executable tasks to
the available machines. Any machine can execute any
“enabled” task independent of whether this task is already
under execution by another machine. As a consequence,
free machines end up doing more work than loaded ones,
leading to a balanced system. Secondly, computations do

123

CALYPSO
has its roots in results by us and by our colleagues addressing fault tolerance, parallel program execution on fault-prone and asynchronous abstract machines,
and distributed systems [30, 31, 11, 26, 12, 24, 14, 21, 23,
22, 4, 25, 3, 15, 131. The research leading to CALYPSO
started as formal work which developed provable methods
for executing parallel computations, initially on abstract
machines with crash-failing processors, and later on abstract machines with asynchronous processors. An outline of a network of workstations-based system for parallel computing based on earlier formal work was presented
in [13]. CALYPSOis an evolution of this design, and is
the result of considerable redesign and extensive experimentation on progressively more and more sophisticated
implementations. Major new developments in CALYPSO
include: (1) a programming strategy that allows dynamic
thread segment declarations, thus providing programming
scalability; (2) dynamically evaluated termination condition to increase efficiency; (3) addition of sequential steps
to handle external interactions and side effects; (4) a global
management mechanism which uses buffering, collating of
updates and transmits updates as differences thus allowing
arbitrary logical data granularity; (5) memory locality management. However, it does not implement a fault-tolerant
manager as described in [13], which relied on dispersal and
evasion.
Among the many systems that directly address parallel computing on workstation networks, some focus on
providing message passing mechanisms. Message passing systems closely resemble the underlying hardware in a
portable environment. Popular systems include PVM [ 161,
P4 [9] and MPI [18]. The remote procedure call mechanism adds structure to message passing systems and makes
programming a little simpler. ConcerVC [2] from IBM and
DCE-RPC [28] are examples of mature and portable packages. CALYPSO
provides a high-level programming model

Step 1

Sequential Step

Step 3

Step 2

Sequential Step

Parallel Step

Step 4

Parallel Step

Figure 1: A fragment of an evolving execution,

that relieves the programmer from handling the underlying
communication layer.
Another class of systems for parallel computing focuses
on providing DSM (Distributed Shared Memory) across
loosely-coupled workstations. IVY [29] was one of the
first implementations of DSM. Midway [8], Munin [7],
and now TreadMarks [27] and Quarks provide a weaker,
and sometimes multiple consistency semantics in order to
improve performance. In contrast, CALYPSO
provides a
simple, and a unified programming model that separates
logical and execution parallelism. In addition, load balancing is not left for the programmer, rather provided by the
system.
Linda [ 101 is a variant of DSM that provides a common
global space. Piranha [17] is built on top of Linda and
allows workstations to join an ongoing computation as they
become idle, and retreat when reclaimed by their owners.
In fact, our daemons are modeled after Piranha. But unlike
all of the previous systems, CALYPSO
can mask process
crashes.
There have been several proposals to provide fault tolerance, mostly by augmenting an existing system. They
include FT-PVM [32], FT-Linda [5], PLinda 1191, and
Orca [20]. A notable exception is DOME [l] that incorporated fault tolerance and load balancing form the onset.
These systems provide fault tolerance by using well known
mechanisms: checkpointing the data, logging messages,
and using reliable atomic broadcasts. In contrast, CALYPSO
uses a unified set of techniques to tolerate failures and
slowdowns. Furthermore, in the absence of failures, there
is virtually no overhead.

3 Syntax and Semantics of CALYPSO
Programs that run on the CALYPSO
system are written
in CSL. Programs are structured by inserting parallel tasks
into sequential programs. We refer to an execution of such
aparallel task as aparallel step, and we refer to the sequential execution fragment between two consecutive parallel

124

steps as a sequential step. The execution of a parallel step
consists of several concurrent thread segments. Figure 1
illustrates a small fragment of an evolving execution.
CSL is C++ augmented with 4 keywords: shared,
parbegin, parend, and routine. Informally, the
major features and restrictions of CSL are as follows.
The address space of CSL programs is partitioned into
two disjoint areas: private and shared. Shared variables are
declared by:
shared { member-list }
A parallel step is a new compound statement to express
parallelism. A typical form of one such statement is:
parbegin
routine [int-exp ] (int width , int id )
{ routine-body }
parend
(There could be several routine statements.)
During the execution of a parallel step, for each
routine statement several concurrent jobs are logically
“spawned.” We call these thread segments. The number
of thread segments is specified by int-exp, and this number
is passed to the first parameter of the routine-body, i.e. the
width field. The second argument i.e. the id field, contains its unique number which ranges from 0 to int-exp - 1.
Thus, each thread segment is able to adapt and distinguish
its behavior from that of its siblings by these two parameters. Once all thread segments are completed, the parallel
step ends. (Dynamic termination conditions are allowed
too.)
A routine-body is a sequential C++ program fragment.
It can access the shared data, the two parameters passed to
it (width and id), and its local data. It cannot have external
effects (such as VO).
Within a parallel step CR&EW (concurrent read and
exclusive write, or multiple readers and a single writer) semantics are supported. In other words, a data item can be
read by any number of thread segments and written by at
most one. (In fact we have implemented the stronger Com-

One out of "many"workers

The manager

\

\

Return Diffs for Dirty Pages

Figure 2: The software architecture of the current CALYPSO
protot:ype

mon CR&CW model.) Semantically, all thread segments
read the value of shared variables at the beginning of the
parallel step, and write atomically at the end of a parallel
step.
The following is the complete source code of a CSL program that initializes two 500 x 500 matrices with pseudorandom numbers, multiplies them in parallel, and stores the
result in the third matrix.

22
23

24
25
26
27
28 }

parend;
/ / calypso-main

Lines 3-5 define 3 shared arrays. Lines 14-15 initialize
the input arrays A and B. L h e 16 starts a parallel step.
This step consists of only one routine, where NUM thread
segments concurrently perform the multiplication and store
the result in C.

0 1 #include <calypso.H>
02 const int N = 500;
03 shared {
04
float A[Nl [NI, B[Nl [NI, ClNI [NI ;

05

1

1

C[il [jl := 0;
for (int k=O; kcN; k++)
C[il [ . j l += A[il [kl * B[kl [jl;
/ / for
/ / routine

1;

06
0 7 void rFill(f1oat m[N] [NI, int size) {
08
for (int i=O; i<size; i++)
09
for (int j=O; jxsize; j + + )
10
m[i][j] = rand();
11 1
12
1 3 void calypso-main(int ac, char *av[l) {
14
rFill(A, N, N);
15
rFill(B, N, N);
parbegin
16
17
routine[NUM] (int width, int id) {
18
int from = id * (N/width);
19
int to = from + (N/width);
for (int i=from; i<to; i++)
20
21
for (int j=O; jiN; j + + ) {

4 From a Program to1 its Execution
In general the execution of a CSL program is distributed
over a dynamically changing set of interconnected host
machines. We do not need a specific network protocol or
a shared file system for CALYPSO.In fact, we ran a CSL
program on a set of machines.,some in New York and some
in Phoenix, with the machines at each location connected
by an Ethernet, and the two1 locations connected by the
Internet.
A CALYPSOcomputation is executed by exactly one
manager, and a dynamically changing set of worker processes. In the current prototype the manager must be a
completely reliable process. The workers may come and
go, speed up and slow down in an unpredictable manner,

125

Assume that at some point in the execution the first
thread segment has been assigned to two workers, the second thread segment has been assigned to one worker, and
the third thread segment has been given out to a worker,
which has subsequently completed its execution. This is
reflected by the following table:

depending on the transient availability of resources and not
on the properties of the computation. In Figure 2, we sketch
the software architecture of CALYPSO.
Once a CSL program, say program.csl is written, it
is preprocessed, compiled, linked, and then executed.
Our preprocessor reads a CSL program (e.g.
program.csl) and generates a standard C++ program
(e.g. program.c). Basically, routines are stripped away
and wrapped in functions. As a very schematic example
consider the pseudo-CSL program fragment:
parbegin
routine[m] (int wid, int id)
{ SequenceOfStatements }
parend;
The preprocessor replaces the parbegin-parend construct
with calls to our library functions that will at run-time: (1)
create m instances of functions each executing SequenceOfStatements, (2) assign executions of these functions to
available workers, and ( 3 ) manage and monitor the execution.
Compilation of program.c using standard C++ compiler produces program.0 , which is then linked with our
library to produce a.o u t .
As mentioned before, the manager executes sequential
steps. Once it reaches a parallel step, it suspends execution
and “manages” the execution. Again for simplicity assume
the previous CSL program fragment.
Say that SequenceOfStatements has been stripped and
wrapped in a function called f 0 0 1 2 3.At the beginning of
the parallel step the manager prepares a progress table and
initializes it as follows:

Step

I

Function

1

Width

I

Id

I

Started

1

Finished
NO

We now turn to the description of a worker. The worker
knows the shared pages-pages on which all and only
shared variables are located. The worker access-protects
the shared pages (using the system call mprotect ( ) ),
and then contacts the manager for work. The manager
then sends it an assignment specified by the 3 parameters foo123,width,and id. The worker now executes
this assignment: it runs function f0 0 1 2 3 with parameters
width and id. During this execution, the first time a
worker accesses a protected shared variable a SIGSEGV
signal is raised. The signal handler fetches the appropriate
page from the manager, installs it in the worker’s process
space, and unprotects the page for future use. Then the computation proceeds. When the task terminates, the worker
identifies all of its dirty memory and sends the differences
(XORS) between the original page and the updated page to
the manager. Then it protects its shared pages again and
contacts the manager for another job to do.
The manager accepts the first completed execution of
each thread segment and discards subsequent ones, including the updates sent. The manager buffers the updates until
the end of a parallel step, at which time all updates are
performed. Different parts of a page can be updated by
different workers, as long as the CR&EW condition is met.
In a parallel step values read by a worker are those existing at the beginning of the step, and the updated values
are readable only at the beginning of the next step. As a
consequence, correctness is assured in spite of multiplicity
of execution. Yet there is no need for expensive mechanisms such as distributed locking, and page shuttling is
completely avoided.
The simplicity of CSL program lends itself to certain
optimizations. For example, paged-in shared memory segments can be kept valid as long as possible, without paying
the overhead associated with an invalidation protocol. So
for instance, if a page was last modified in step 4, it was
read by some worker in step 6, and that worker is working
on a thread in step 8, then the worker does not fetch the
page but accesses its cached copy. This is a low cost almost free strategy: read-only shared pages are fetched by
a worker at most once; write-only shared pages are never

fool23
NO

The last row of the table, for instance, indicates that
the current step is 2; that a thread segment defined by the
function named f00123 needs to computed; that there are
3 such thread segments defined by this function; that this
rows describes the third out of 3 sibling thread segments
(numbered 0, 1,2); that 0 workers have started working on
this thread segment; and that its computation has not yet
finished.
The system utilizes a simple version o f eager scheduling
as follows. The manager listens to workers requesting
work. It assigns to each free worker a thread segment that
has not been finished-among all such thread segments
it assigns one that has been assigned the least number of
times. Notice that the same thread segment can be assigned
out multiple times.

126

1.o

I.o

0.5
time

1.o

0.5

0.5

- -+
60

120

180

- -+
dime

60

Profile A

120

time

- - -+

180

60

120

Profile B

180

Profile C

Profiles of the machines used in the performance experiments

1

1200
1000
U)

U

3

0.13

800
600

0.6

*
e

.-a,

:.
5
0.2
0.4

400

200

0

0
Seq

1A

2A

3A

4A

5A

6A

Experiment 1: Variable Number of Fast Machines

1000
U)

D

E

4

0.8

800

*

600
400

.0.41

200

OS!

0

5

0
Seq

1A+5B

2A+4B

3A+3B

4A+2B

5A+IB

6A

Experiment Set 2: Fast and Slow Machines

1

1200
1000
U)

U

8

Time

0.6 2

0.8

800

0.6

5.

.-a,

400
6oo

0.4 .%

200

0.2

E

0

0

Seq

1A

IA+IC

IA+2C

1A+3C

1A+4C

1A+5C

Experiment 3: One Fast Machine and Variable "Temporary" Machines

Figure 3: Performance results for the matrix multiplication CSL program

127

1

0
Work
+Efficiency

machine. Hence, Tbase = 828 seconds and Wbase = 828
machine-seconds.
Our performance metric is eficiency, defined by:
efFiciency = wb,,$/w. Efficiency is the important metric,
since it measures how well we use the resources provided
to us, bench-marked against the purely sequential case.
For our experiments we used Sun SLC workstations
connected by a lOMBit Ethernet. Here we report three
sets of experiments, where each experiment is labeled with
the profiles of the machines used, so 2A+4B indicates that
there were 2 machines with profile A and 4 machines with
profile B. “Seq,” labels the execution time of the sequential
matrix multiplication program.
We have accounted for the overhead of fault masking,
load balancing, networking, swapping, updating memory,
etc. The reported times were “wall clock,” or elapsed times,
not CPU or virtual times. As the pseudo-random filling of
the input matrices is “phony,” the times were measured for
the execution starting with line 16. The efficiency ranged
from 84% to 94%. (The latter for a CALYPSOexecution
with a single machine. Thus if we were to utilize this as
the base case, the efficiency would be more than 89% for
all cases.)
CALYPSOworked efficiently and transparently in the
case of dynamic and unpredictable (to the programmer)
availability patterns. It utilized slow machines, integrated
newly arrived machines, and bypassed machines that disappeared.

fetched; modified shared pages are re-fetched only if necessary; and invalidation requests are piggybacked on the
work assignments. The programmer does not declare the
type of coherence or caching technique to use, rathel; the
system dynamically adapts.
Of course, the workers (and their locations) and thread
segments are not related in any fundamental way. In fact,
the current syntax of CSL does not even allow the programmer to specify the workers or how to distribute the
data.

5 Experiments
Several applications have been implemented in CSL.
These include both toy examples and real applications, such
as several sorting algorithms, DFT, computation of eigenvalues and eigenvectors, Option-Adjusted-Spread bond indices [6], and modules of the Automatic Target Recognition
software. We have tested the performance of some of these
applications.
For a clear, simple, and complete illustration, we present
the exact performance results for the specific program listed
in Section 3. The program does not contain any explicit
codefor load balancing orfault tolerance, but is run under
CALYPSO.
To model machines with different capabilities, machines
joining and dropping from the computation, we used machines of three profiles: A, B, and c.
Machine A is “perfect,” and available at 100% capacity
throughout the computation. Machine B is “slow,” perhaps
timeshared with another independent sequential or parallel
computation and therefore available only at 50% capacity.
Machine C becomes available 60 seconds into the computation, and then disappears 120 seconds later. (See Figure 3.)
Each machine profile P , is defined by the function
availability, mapping time (in seconds) into the interval
[0,1]. Then if the computation lasted for time T , the work
T
that was made available to us is
availability, d t . This
is the area of the shaded region (for time interval [0,TI) in
the top graphs of Figure 3. For a computation of time T, the
T
total work is defined by W =
availabilityA dt.
The work W is the “charge” we incur for having the
machines availabIe to us during the computation, whether
we use it effectively or not. Thus the overhead includes the
network time, the time wasted by redoing computations, the
time taken to move data between workers and the manager,
the time spent by the operating system and other system
activities.
We used a purely sequential C++ matrix multiplication
program as our base case, and compared the performance of
the CSLprogram given earlier to it. The sequential program
took 828 seconds, while running on one 100% available

Acknowledgments
We thank the following for their contributions to
the CALYPSOproject: Robert Buff, Churngwei Chu,
Deepak Goyal, Shi-Chen Huang, Mehmet Karaul, Dimitri
Krakovsky, Fabian Monrose, Naftali Schwartz, and David
Stark.

References

st=,

[l] J. Arabe, A. Beguelin, B. Lowekamp, E. Seligman,
M. Starkey, and P. Stephan. DOME: Parallel programming
in a heterogeneous multi-user environment. Submitted to
Supercomputing,1995.

st=,

[2] J. Auerbach, A. Goldberg, G. Goldszmidt, A. Gopal,
M. Kennedy, J. Rao, and J. Russell. ConcerVC: A lan-

guage for distributed programming. In Proc. of the Winter
1994 USENIX Cont , 1994.
[3] Y. Aumann, Z. Kedem, K. Palem, and M. Rabin. Highly
efficient asynchronous execution of large-grained parallel
programs. In Proc. 34th IEEE Ann. Symp. on Foundations
of Computer Science, 1993.
[4] Y. Aumann and M. Rabin. Clock constructionin fully asyn-

chronous parallel systems and PRAM simulation. In Proc.

128

33rd IEEE Ann. Symp. on Foundations of Computer Science, 1992.

[I91 K. Jeong and D. Shasha.
Plinda 2.0: A transactionalkheckpointing appmach to fault tolerant linda. In
Proc. of the 13th Symp. on Reliable Distributed Systems,
1994.

D. Bakken and R. Schlichting. Supporting fault-tolerant
parallel programming in Linda. Technical Report TR9318, The University of Arizona, 1993.

[20] M. Kaashoek, R. Michiells, H. Bal, and A. Tanenbaum.
Transparent fault-tolerance in parallel Orca programs.
Symp. on Experiences wii'h Distributed and Multiprocessor Systems, 1992.

A. Baratloo, P. Dasgupta, Z. Kedem, and D. Krakovsky.
CALYPSOgoes to Wall Street: A case study. In Proc. of
Third Intl. Con$ on ArtiJcial Intelligence Applications on
Wall Street, 1995.

[21] Z . Kedem. Methods for handling faults and asynchrony in
parallel computations. In ,Proc. DARPA Software Technology Con$, 1992.

J. Bennett, J. Carter, and W. Zwaenepoel. Munin: Distributed shared memory based on type-specific memory
coherence. In Proc. 2nd Ann. Symp. on Principles and
Practice of Parallel Programming, 1990,

1221 Z . Kedem and K. Palem. Transformations for the automatic derivation of resilient parallel programs. In Proc.
IEEE Workshopon Fault-Tolerant Parallel and Distributed
Systems, 1992.

B. Bershad, M. Zekauskas, and W. Sawdon. The Midway
distributed shared memory system. In Proc. COMPCON,
1993.

[23] Z . Kedem, K. Palem, M. Rabin, and A. Raghunathan. Efficient program transformations for resilient parallel computation via randomization. In Proc. 24th ACM symp. on
Theory of Computing, 199:!.

J. Boyle, R. Butler, T. Disz, B. Glickfeld,E. Lusk, R. Overbeek, J. Patterson, and R. Stevens. Portable Programs
for Parallel Processors. Holt, Rinehart and Winston, Inc.,
1987.

[%I -Z. Kedem, K. Palem, A. Raghunathan,
and P. Spirakis.
-

.

Combining tentative and dlefinite algorithms for very fast
dependable parallel computing. In Proc. 23rd ACM Symp.
on Theory of Computing, 1'991.

[lo] N. Carrier0 and D. Gelernter. Linda in context. C. ACM,
32, 1989.
[ 111 P. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohindra,

[25] 2.Kedem, K, Palem, A. Ra.ghunathan,and P. Spirakis. Resilient parallel computing on unreliable parallel machines.
In A. Gibbons and P. Spirak:is, editors, Lectures on Parallel
Computation. Cambridge University Press, 1993.

and R. Chen. Distributed programming with objects and
threads in the Clouds system. Computing Sytems, 4, 1991.
[ 121 P. Dasgupta and R. C. Chen. Memory semantics for large

grained persistent objects. In A. Dearle, G. Shaw, a d
S , Zdonick, editors, Implementation of Persistent Object
Systems. Morgan Kaufman, 1990.

[261 Z. Ked"
K. Palem, and F! Spirakis. Efficient robust parallel computations. In Proc. 22nd ACMSymp. on Theory of
Computing, 1990.

1131 P. Dasgupta, Z. Kedem, and M. Rabin. Parallel processing
on networks of workstations: A fault-tolerant, high performance approach. In Proc. of the 15th Intl. Conf. on
Distributed Computing Systems, 1995.

[271

Keleher, S . Dwarkadas, A. cox, and W. Zwaenepoel'
TreadMarks: Distributed hared memory on standard workstations anperating systemsd. In Proc. of the Winter 94
Usenix Cor$, 1994.

[14] P. Dasgupta, R. J. LeBlanc, M. Ahamad, and U. Ramachanclran. The Clouds distributed operating system. IEEE Cornputer, 24, 1991.

[281 N. h e r . The Distributed Computing Environ"2nt naming
architecture. OpenForum, 1992.

[15] M. Fu and P. Dasgupta. Programming support for memory
mapped persistent objects. In Proc. COMPSAC, 1993.

1291 K. Li and p. Hudak. Memory coherence in shared virtual
memory systems. A CM Transactions on Computer Systems,
7 , 1989.

[ 161 G. Geist and V. Sunderam. Network-based concurrent com-

puting on the PVM system. Concurrency: Practice and
experience, 4, 1992.

P O I M. Rabin. Fingerprinting by random polynomials. Techni-

[17] D. Gelernter, M. Jourdenais, and D. Kaminsky. Piranha
scheduling: Strategies and their implementation. Technical
Report TR-983, Yale University Department of Computer
Science, 1993.

[31] M. Rabin. Efficient dispersal Of information for Security,
load balancing and fault tolerance. J. ACM, 36, 1989.

cal report, Harvard University, 1981.

[32] V. Sunderam, G. Geist, J. Dongarra, and R. Manchek.
The PVM concurrent computing system: Evolution, experiences, and trends. Parallel Computing, 20, 1994.

[18] W. Gropp, E. Lusk, and A. Skjellum. Using MPI:
Portable Purallel Programming with the Message-PassingInte$ace. MIT Press, 1994.

129

Parallel Processing on Networks of Workstations:
A Fault-Tolerant, High Performance Approach"
Partha Dasgupta'
Department of Computer Science
and Engineering.
Arizona State Universit
Tempe, AZ 85287-54Ot
Email: partha@asu.edu

Zvi M . Kedem2
Department of Computer Science,
Courant Institute
New York University,
New York, NY 10012.
Email: kedem @ cs.nyu .edu

(with shared memory), to execute large grained parallel
programs efficiently (see section 3). Interestingly, these
algorithms,with appropriatemodifications,are also suitable
for running parallel computations on networks of workstations. This paper presents the latter results.
In addition to supporting parallel computations, we do
so in a fault-tolerant manner. This is an important consideration, as not only are workstations inherently unreliable,
but the transient loading of workstations renders some of
them slow (unpredictably). A fault-tolerance mechanism
that treats slow computers as failed ones can actually speed
up computations that would otherwise get bogged down by
wrong choices during the initial scheduling process. In
addition, in our approach we do not impose significant
additional overhead for fault-tolerance or dynamic load
balancing.

Abstract
One of the mostsoughtaftersoftware innovationof this decade
is the construction of systems using off-the-shelf workstations that
actually deliver, and even surpass, the power and reliability of
supercomputers. Using completely novel techniques: eager scheduling, evasive memory layouts and dispersed data management, it
is possible to build a execution environmentfor parallel programs
on workstation networks. These techniques were originally developed in a theoreticalframework for an abstract machine which
models a shared memory asynchronous multiprocessor.
The network of workstations platform presents an inherently
asynchronous environmentfor the execution of our parallel program. This gives rise to substantial problems of correctness of the
computation and of proper automatic load balancing of the work
amongst the processors, so that a slow processor will not hold up
the total computation. A limiting case of asynchrony is when a
processor becomes infinitely slow, i.e fails, Our methodology
copes with all these problems, as well as with memoryfailures. An
interestingfeature of this system is that it is neither afault-tolerant
system extendedfor parallel processing nor is it parallel processing system extended for fault tolerance. The same novel mechanisms ensure both properties.

1.1. The Technology Setting
While there has been considerable pragmatic research
on the subject of running parallel computations on anetwork
of workstations, our approach is quite different from the
current ones. Systems that support distributed computations generally rely on conventional techniques such as (1)
low-level mechanisms like Distributed Shared Memory
(DSM), Remote Procedure Calls (RPC), and MessageBased programming to implement applications that run on
distributed nodes; (2) language support and runtime packages to write parallel programs and (3) system-level services
like specialized servers, distributed operating systems and
microkernels to support distributed computations.
In consequence, the following fundamental difficulties
are frequently encountered: (1) The programming system
necessitates major re-writing and/or restructuring of application programs to fit the specific model of the system.
(2) Fault tolerance is either not supported or not well integrated with the rest of the system. (3) The overheads are
high due to distributed control and synchronization strategies, or due to extensive replication for fault tolerance.
Our work starts with the premise that two conditions are
critical for the utilization of networks of multiprogrammed
workstations as virtual parallel computers. We would like
programs to run on the network of workstations with mini-

1. Introduction
The basic tenet of high-performance computing is to
allow multiple concurrent threads of control to perform
multiple parts of a computation, while minimizing overheads of operating systems and other softwarehardware
artifacts. Such computationsare typically run on expensive,
dedicated hardware, which is often custom-designed.However, in recent years there has been a concerted push to use
low-cost distributed hardware to execute parallel computations.
This paper describes a novel approach to hamess the
power of workstations for parallel computations requiring
high performance platforms. Recently, new theoretical results have provided precise techniques that allow a formal
machine, modelled as an asynchronous multiprocessor

* This research has been partially supported by the following grants:

'

NSF CCR-95-05519,
2NSF: CCR-91-03953and NSF CCR-94-11590 and
NSF: CCR-93-13775, ONR NW014-91-J-198 1.

467
1063-6927195$4.000 1995 IEEE

Michael 0.Rabin3
Aiken Com utation Lab.
Harvard bniversity
33 Oxford Street
Cambridge, MA 02 138
Email: rabin@das.harvard.edu

mal rewriting. Also parallelism and fault-tolerance should
be two integrated features of the system.

points of the formal work in brief. A lot of detail is lost in
the brevity, and the interested reader is referred to the
original papers.
The formal research uses two abstract machine models:
the ideal machine 34 as presented to the programmer, and
the more realistic machine 9% which is used to execute the
programs written for 34. The formal techniques are used to
automatically transformprograms written for 31.1,to execute
on 5%.

2. Related Research
This research is related to research in parallel processing
in distributed environments as well as fault-tolerant processing. Pedagogically, these two areas are considered separate.
Parallel processing systems are built on top of a message
passing (or RPC) layer as in the very popular PVM [Sun901
system.Other systems along similar lines are include ORCA
[BT90], GLU [JA91],Amber [CAL+89], Concert/C, and so
on. Most of these systems are hard to program and they do
not easily support fault-tolerance. Systems based on global
address spaces, or distributed shared memory include IVY
[LH89], Munin [BCZ90], Clouds [DAM+91, DCM+90,
DLAR901, Mether-NFS [MF89]. They allow networked
workstations to be treated as a multiprocessor system with
the the underlying software providing coherent memory.
Page shuttling, false sharing, need for distributed locking
and lack of fault-tolerance are the disadvantages.The tuplespace concept for sharing and synchronization has been
effectively used in the Linda [CG89] system.
Fault Tolerance has been implemented using replication
of data and computation or checkpointing. THese systems
include CIRCUS I COO^^], LOCUS [PWC+8 11 and Clouds
[DLAR90]. Recently encoded techniques such as RAID
[CLG+94] is gaining popularity. The process group approach and causal communications have been popularized
by ISIS [Bir93].
Combining the parallel processing approaches with
fault-tolerance has met with limited success. Mainly due to
the fact that it compounds the overheads and these high
overheads have to be paid even when there is no failure.

The Ideal Machine 34 The ideal machine N presented to
the programmer is a synchronous shared memory machine
with an unbounded number of perfect virtual processors.
(See Fig. 1.) The programmer writes an ideal parallel program whose execution on 34 consists of a sequence of
parallel steps. In each parallel step, some number of thread
segments execute in parallel, each on a dedicated processor.
A parallel step ends when all the thread segments in it have
completed their execution.Then the next parallel step starts.
(The execution model is shown in Fig 2.)
The Realistic Machine %: The realistic machine 9% has
a finite number of completely asynchronous processors with
unspecified memory organizationbut with all the processors
able to access it. Any instruction can take potentially unbounded amounts of physical time to complete. (Processor
faults are a special case -an instruction never completes.)
The memory is fault-pronetoo. The only atomic instructions
are reads and writes, to the main memory. The model %is
a stable abstraction of some of the problems of a realistic
hardware.
The formal research results provide a method, which
given an ideal program 23 written for 34,produces an

3. The Formal Foundations
This section presents some of the formal results which
form the basis of our design. These results have been
published in [AKPR93, Ked92, KP92, KPRR92, KPRS9 1,
KPRS93, KPS90, Rab83, Rab891. The formal results are
developed in the context of abstract machines modeling
some key properties of realistic highly-parallel machines.
These results lead to precise provably correct and efficient
techniques for execution of parallel computations in a fault-

Synchronization
Point

tolerant manner on these abstract machines. While this

work is not directly implementable on a workstation network, we have been able to adapt the ideas to make such
implementation feasible. In this section we discuss the key
Processors

UnilormlyAccessible Memory

Fig. 2.: The Ideal Program Execution Model

Fig. 1.: The Abstract Machine
468

execution of P on N,such that the execution in correct
and efficient.
The execution is a sequence of parallel steps, logically
numbered 1, 2, ..., etc. The logical clock maintains the
current step number. A “free” processor of 94 eagerly
schedules itself by “grabbing” a copy of a thread segment
whose execution has not been completed in the current
parallel step. It then executes the thread segment: reads
variables, computes, and writes variables. A parallel step
ends when each of its thread segments has been completed.
Then the clock is advanced.
To ensure correct execution, each thread has to be executed logically once (idempotence) and multiple asynchronous executions of the threads should not update variables
out of order (clobberprotection). The execution ordering is
assured by the Certified Touch All technique is used: During
each parallel step, a data structure is maintained for keeping
track of which thread segments have been executed. When
all thread segments have been executed (touched),the clock
is advanced. An efficient algorithm (essentially, O(1ogn))is
used.
A critical difficulty is that an asynchronous processor
can “clobber” a value. Consider a slow processor P I executing a thread segment of some step T, which will include a
write of variable x. Since it is slow, a faster processor P2
executes this thread segment, and the computation moves
further. Then x is updated again in step T’. Subsequently,the
now obsolete write of x by P I is executed, destroying the
current value of x. Clearly the current values need to be
protected.
We sketch here how this is done in the more interesting
case of large-grained variables, say several hundred bytes
or more. First, each variable is dispersed, that is, stored as a
number pieces in an error-correcting manner with the property that the variable can be reconstructed from a fraction of
the pieces. Second, evasion is used. When aprocessor writes
the new values of the pieces of some variable x,it does not
write them in the locations in which they have been stored
so far, but in a random location dependent on the step
number. To return to the previous example, pieces of x
produced in step T’ are written in locations different from
the locations of pieces of x produced in step T. Note that a
late write’s pieces can clobber pieces of current variables.
However, with overwhelming probability, the rate of clobber will be one piece or fewer per variable. Because of
dispersal, the correct value of a variable can be still recovered. The Variable Handle Array is a data structure that
maintains the location of the current values of the variable.
Because of the reuse of the storage, evasion can be done with
a small space overhead. It is worth commentingthat though
evasion may appear to be similar to multi-version storage,
it is in fact very different.

469

Evasion also assures idempotence of a thread segment
execution, as in fact the input and the output locations are
different, even though the names may be the same. (In
x :=Ax),the new value of x is stored in a location different
from where the old vallue of x was kept.) Note that the
memory layout used to cope with asynchrony also provides
fault tolerance. The system can tolerate loss of pieces due to
reasons that have nothing to do with asynchrony, for example, a disk shutdown. To reiterate, dispersal and evasion
together mask out clobbers, provide fault tolerance, and
assure idempotence. (We remind the reader that we do not
describehere how small-grainedvariables andcontrol structures, such as the current addresses of the large-grained
variables, and the clock are stored. In fact, the system design
utilizes techniques different from those developed in the
formal research.)
Sources: The first Idempotent Execution strategy (not
relying on evasion), the Certified Touch All technique, and
the related Eager Scheduling for it, were presented by
Kedem, Palem, and Spirakis [KPS90]. Additional improvements were presented by Kedem, Palem, Raghunathan, and
Spirakis [KPRS91]. (See also [Ked92,KP92,KPRS93].)
The first asynchronous parallel execution (including the
underlying asynchronous clock construction)was presented
by Kedem, Palem, Rabin, and Raghunathan [KPRR92]. An
improved constructionwas presented by Aumann andRabin
[AR93]. Dispersed variables and fingerprinting were presented by Rabin [Rab89,Rab81]. The evasive memory layout was presented by Aumann, Kedem, Palem, and Rabin
[AKPR93].

4. System Design
In this section we present a design that allows us to use
the ideas developed in the formal research to produce a
system that runs on regular networked workstations and
provides support for high-performance,fault-tolerant parallel computations.
Some of the techniques used in the formal design are
either not implementable or not necessary in a real environment. For example, the Certified Touch All algorithm is an
O(1og n) implementation of a scanning algorithm, that is
best done by a linear scan in a real system. Similarly, the
evasion and dispersal algorithms, while initially looking
esoteric, are actually quite useful after modifying the manner in which they are used. However the implementation of
evasion and dispersal, as developedin theory are too expensive for practical systems.For instance, evasionrequires that
each write be preceded by a read that checks for existing
data before overwriting.This overhead is too much for data
access over a network. Similarly, in the theoretical case, the
dispersal is done on top of evasion, while we prefer to use
(modified) evasion over dispersal. Also, since there is no
real global memory, the processor scheduling and variable

program EXAMPLE;
v a r I, J : i n t e g e r ;
procedure d e c l a r a t i o n s ;
begin
cobegin
begin S111; S112; S113; . . .
begin S121; S122; S123; . . .
begin S131; S132; S133; . . .
coend;
cobegin
begin S211; S212; S213; . . .
begin S221; S222; S223; . . .
coend;
end.

program EXAMPLE;
var I , J : i n t e g e r ;
procedure d e c l a r a t i o n s ;
procedure P11;
begin 5111; S112; S113;
procedure P12;
begin S121; S122; S123;
procedure P13;
begin S131; S132; S133;
procedure P21;
begin S211; S212; S213;
procedure P22;
begin 5221; S222; S223;
begin
end.

end;
end;
end;
end;
end;

. . . end;
. . . end;
. . . end;
.

. end;

. . . end;

Fig. 3: The source program and its pre-processedversion
handling needs to be be done differently. The good news is
that when the formal algorithms are modified the resulting
system architecture is feasible, innovative and quite appealing.
Some features of the design are:
0
The execution environment uses a set of regular workstations running a conventional operating system. Some
of these workstations are designated as memory servers
and progress managers, the rest are compute servers.
e After a computation is started, any number of compute
servers may fail, become inaccessible, or become heavily loaded by other computations. As long as there is one
responsive compute server, the computation will progress (albeit, possibly slowly). The failures of memory
servers and progress managers are also tolerated.
e A parallel program is written using barrier synchronization points, expressed by a cobegin-coend or
equivalent structure.
In our presentation we make the following assumptions:
0
The parallel program is a straightline sequence of parallel steps without nesting.
0
If a thread segment writes a variable, then no other thread
segment of the same parallel step writes the variable.
0
Each variable is page aligned.

4.1. Compilation
Aparallel program is written in a language that supports
parallelism. This program is “pre-processed” into a sequential program in a standard language. The resulting program
is compiled using a standard compiler.
To produce executable object code, we start with a
parallel program. Apurullel step is specified by a cobecoend block, and a threud segment (or thread) by
gin
a begin - end block. A sample program (written in
pseudo-Pascal) is given on the left side of Fig. 3. There are
some global declarations and the main body. Within each
begin - end block, the statements are labeled Sij1,
Sij2,

-

....

This parallel program needs to be compiled into executable object code. The pre-processing stage transforms
the source program by stripping away the cobegin coend structure, replacing each begin - end block by
a global procedure, and removing all statements from the
main body. In fact, our program was turned into a set of
procedures, each corresponding to a thread (segment). See
the right part of Fig. 3. The pre-processor assigned names
to these global procedures (e.g. Pij) as shown in the
figure.
Then the transformed program is compiled into object
code by a standard “seq~ential”compiler. In addition, we
extract the starting point of each procedure from the compiler-generated symbol table and create a data structure
called the Progress Table. The structure of this table is
shown below:

/

PI2
PI3

I

oxoqqq
OxOrrr

1
F
F

oxosss
...

...

I

..~

...

This table maintains information about where each procedure P i j starts in the code. The object code and the
Progress Table are placed on a memory server, which is a
system component that stores the program and its data and
sends it over the network to a requesting workstation. For
the first-cut design, we assume the memory server is stable,
that is, it never fails. We will remove this assumption later.

4.2. Memory Service and Execution
As stated before, the stable memory server has the
program, data and the Progress Table. In addition to the
thread-index (or thread-identifier) and starting location of
that thread, the Progress Table also indicates whether a
thread has been started and/or possibly completed. All the
compute servers have a worker daemon, which obtains
information from the Progress Table of a computation if the

workstation is under-utilized. Again, for simplicity let us
assume there is only one parallel computation and hence
only one Progress Table. Although in fact, the Progress
Tables are maintained and accessed by progress managers,
for the time being we proceed as if any compute server can
access the Progress Table in a mutually exclusive fashion.
(Later in the presentation, we modify this.)
When a compute server examines the Progress Table, it
follows the algorithm:
1. Find a thread segment Pj,jsuch that: all the threads Pi-l,x
(for all x ) have been completed, and Pij has not been
started; else if no such thread is found, then find a thread
P,jsuch that it has been started but not completed; else
stop (as the computation has completed).
2. Mark the “started?” value corresponding to Pi,jas true.
3 . Get the page containing the memory location Startij
from the memory server
4. Start executing Pij.
While the thread segment is executing, it needs code and
data pages. As the computation touches each page, this page
is demand-paged into the compute server. The computation
works on the downloaded copy of the page for as long as
the thread segment runs. No attempt is made to keep the
pages coherent as is done in a DSM system, After the
execution of the thread segment completes, the compute
server returns all the modified (“dirty”) pages. It does this
by first identifying the dirty pages, using the modified bits
in the paging system, and then writing the pages to the
memory server. Finally, the compute server sets the variable
“done” corresponding to Pij to true in the Progress Table.
Thus, every under-utilized workstation picks up a thread
segment that can be executed (i.e., this thread segment’s
execution has not been completed, but all the thread segments in thc previous parallel step have bcen complcred).
This is the crux of providing eager scheduling of a somewhat
wait-free flavor, which also provides fault tolerance. Notable properties on the system are:
0
No active compute server waits for any other compute
server to finish. If there is a thread that can be executed,
a compute server may work on it, even if other compute
servers are executing it.
If a compute server becomes slow or fails, other compute servers pick up the slack, without any global coordination. Thus load balancing is automatic and dynamic.
However; the design needs several additional enhancements. The major problem with this design is that memory
gets “clobbered.” To reiterate the situation described in sec.
3, suppose acompute server Wuis slow, and Wb picks up the
thread executing on Wu and finishes executing it. The
parallel step then completes and the computation now progresses to the next parallel step. Much later, Wu completes
and writes back the results. This would destroy the consistency of the computation. Finally, we cannot assume the

existence of a reliable mlemory server. The reliable memory
server can be constructed from a set of replicated memory
servers, but the overhead in synchronization (or atomic
broadcasting) at every read and write is very high.

4.3. Dispersal and ]Evasion
A possible solution to this problem is as follows. Use a
centralized memory server, with built-in intelligence (that
is, a transaction system) that makes every set of writes from
a thread atomic as well as rejecting writes from late threads.
Then we can replicate this facility to provide fault tolerance.
This conventional solution is too expensive, complicated to
implement, and infeasible for regular programming systems. Motivated by the techniques described in sec. 3, we
do it differently, employing variants of evasion and dispersal.
The basic idea behind evasion is the separation of variable name from variable address. While in most programming environments, a variable x is allocated an address An
and this address remains constant throughout the execution.
In our case, a variable’s address changes every time it is
written.
We explain the evasion scheme first. Assume the memory server is reliable, but incapable of determining which
write is a valid write and which is too late. This incapacity
is necessary as we do not want to build global control-state
information (i.e. step number) into the memory service. The
address space of a program occupies some N pages, Pi to
PN. Initially, these Npages are stored in blocks 1 to N of the
memory server. Similarly to the Progress Table, we keep a
Page Allocation Table. Again, we assume all compute servers have efficient and exclusive access to the page allocation
table. We will describe how this is actually done in a later
section. The Daee allocation table has the format:
‘

U

I

2
3
4
I

...

...

...

...

When a thread segment Pjj running on compute server

W reads a page p , and p has not been modified by any other
thread segment, Pi,, reads from the initial location (or block
number)p. If Pjj updates the page, instead of writing it back
to the block p of the memory server, it writes it to any free
block on the memory server storage, say block q. After all
the pages are written back, the processor then updates the
Page Allocation Table by adding entries containing the Page
Numberp, the Writer-ID W, the Step Number i and the new
location of this page q. It does this for all the pages written.
In order to ensure that a thread reads the correct data
when it executes, the following algorithm is used. When a
thread Pi,jreads a page p :

471

7

memory servers

One out of “manf compute servers

\

Primary progress manager/

/

Badtup progress manager

Fig. 4: The Overall Runtime System Structure
a single thread segment is atomic. The atomicity is established when the table entries are added. ( 3 ) A thread segment’s execution, regardless of its structure, is idempotent.
That is, it can be re-executed without adverse effects. Thus,
multiple processors can run the same thread as well as allow
slow processors to continue without the need for tracking
them down and cleaning up after them.

It gets a copy of the Page Allocation Table.
It finds the entry with p in the first column, such that the
value in the third column is the highest value that is less
than i.
0
It retrieves the location q from the last column of the
table and reads in block q from the memory server.
To ensure that the free space is not used up, a simple
garbage collector discards pages that are too old by using
the information in the Progress Table.
This implementation of evasion ensures that the execution of the threads are idempotent, slow threads do not
clobber fast threads and the memory is utilized efficiently.
To make the memory server reliable, we use the dispersal
technique, as described in sec. 3. To re-iterate: each block
ofdisk storage is split into several blocks using aparticularly
efficient method for bulk error correction called the Information Dispersal Algorithm (IDA) [Rab89]. This method
allows us to disperse a variable into n pieces so that every
m pieces suffice for complete reconstruction of its value. The
size of the storage required increases by d m in the process.
Thus when n=5 and m=3, we can lose any two pieces and
reconstruct the value from the remaining 3; the storage and
subsequent communication overhead is just 66% compared
to the 200% overhead needed to achieve such resilience
through standard replication.
Then these pieces are written on n different memory
servers. When a variable is read, we need to retrieve m
blocks from the memory servers and hence n-m memory
servers can be faulty at this point. Since n and m are
parameterizable (depending upon the degree of fault tolerance desired) we can fine tune the system accordingly. Since
the dispersed disk stores evasive memory there is no need
for maintaining inter-update temporal order.
The above realization of evasion and dispersal keeps the
original properties of these two schemes intact. These are:
(1) Memory does not get clobbered. (2) A set of writes from
e
e

4.4. Tables and Progress Managers
The key to efficient working of the system depends upon
the storage and management of the Progress Table and the
Page Allocation Table. The tables are kept in a designated
machine called the progress manager. The progress manager is one central site that both stores and updates the tables
and the free list.
A compute server wanting to run a thread sends a request
to the progress manager, and the progress manager returns
the thread-id and the start-address of the procedure to run,
along with a complete copy of the page allocation table, as
well as a list of free blocks for the output. When the thread
completes execution, it disperses the output, and writes them
to some of the free blocks. It then informs the progress
manager of which pages were updated. The progress manager then marks the thread as done, and updates the page
allocation table and free list as appropriate. As a part of the
reply to this request, the progress manager sends the compute server another thread and the current version of the page
allocation table Thus about two messages per thread execution is required.
So the progress manager is the only single point of
fuilure on our system. To migitate that, we keep a backup
progress manager. At certain intervals, the primary manager
checkpoints the tables to the backup manager. If the primary
manager fails, the backup manager can take over even ifthe
backup manager data is somewhat out of date. Thus the
backup dces not have to be kept totally in sync with the
472

primary, and this is due to the availability of the evasive
memory scheme.
The overall system structure is shown in Fig. 4.
As mentioned earlier, most systems start out either as
parallel processing systems or as fault-tolerant systems and
then add on the remaining half. The major conceptual advantage of this approach is that ours is neither a parallel
processing environment augmented to support fault tolerance, nor is it a fault-tolerant system augmented to do
parallel processing. The same set of mechanisms provides
both.
A negative aspect of systems that use replicated data or
replicated computations for fault tolerance is that they bear
significant runtime overhead even when there are nofaults.
Since faults are rare, the system pays a high price for running
any fault-tolerant computation. Our system does not use
replication. The data integrity is provided by dispersal, and
when compared to replication, dispersal is cheaper. Due to
parameterizability of dispersal, the possibilities of fine tuning the system is larger. Of course, these properties arise
from the theoretical foundations of the system.
Our systemdoes not replicate computations in aconventional way. Only when the execution of some thread segment
seems to be delayed, and there are free computing resources,
another execution of such thread segment is started at another compute server. Such eager scheduling used by our
restart execution strategy, provides automatic load balancing, without the need for process migration.
The table management seems like a bottleneck, but in
reality, it is not. The table management system can be made
efficient by reducing the number of messages. Indeed each
process can update all the tables and allocate the next thread
to run using one request to the progress manager.

to yield a fully functional implementation.The current implementation runs annotated C++ programs, that is the parallel program is written in C++ with embedded constructs
to express parallelism, along the lines described above.
The prototype consists of several (one or more) workstations designated as “compute servers”. At present there
is one machine dedicated to run as a combination of the
“memory servers” and the “progress managers” and it is
called the “manager”. Failures of compute servers are tolerated but the failure of the “manager” is not currently
tolerated. Thus the evasioddispersal algorithms are not
necessary (the central site discards late writes) and have not
been incorporated.
We present the results of one experiment that perfoms
a parallel sort on a large (128KByte) array [Ba194]. There
were a total of 6 execution: (1) A truly, sequential execution
on a single machine--the parallel program was modified into
a sequential program that there is no overhead due to parallelism; and (2-6) Five parallel execution using the manager,
and respectively between 1 and 5 compute servers.. The
manager and the compute servers were 6 identical diskless
Sun Sparcstation SLC’s.
The results are listed in the table in Fig. 5. Each execution has 3 blocks, block 11 random fills the array sequentially,
block 2 sorts different segments of the array in parallel and
block 3 merges the sorted segments sequentially. The row
marked 0 compute servers is the sequential execution. The
efficiency and speedup computation is for the parallel step
(block 2). Efficiency is calculated as the sequential time
divided by the total parallel time multiplied by the number
of compute servers.
It is not surprising that the efficiency is not monotonic.
Executions in which the number of compute servers does
not divide the number of threads (15), are less efficient, as
the unit of assignment is a thread.

5. Experimental Results

6. Conclusions

In order to experimentally evaluate the techniques described in this paper, a prototype implementation of the
system has been built. This effort is a part of a larger
comprehensive system development effort that is expected

We present a design of a runtime environment that
provides a parallel processing environment supporting fault
tolerance and automatic load balancing and runs on a network of machines. The overhead of the system is low due to

4.5. Comparative Discussion

6

4

Time
=Work

2
--SW&P

0
o

-

-

c

v

o

u

Compute S ewers

Fig. 5: The table of sort timings and the corresponding graph (all times in seconds)
473

m

the use of some novel ideas that allow us to avoid replication, synchronization and costly management schemes. The
system is in the prototyping phase and preliminary results
look promising.
We are aware that there is considerable more work to be
done, especially in the area of optimizing the scheduling,
managing the memory architecture better and handling of
the data structures. Also, w e are working on extending the
scheme to support computations that d o U 0 and have nondeterministic executions. Preliminary results exist but are
outside the scope of this paper.

W931
M. Fu and P. Dasgupta. A Concurrent Programming Environment for Memory-Mapped Persistent Objects. the 17th Intl.
Computer Sofrware and Application Conference (COMPSAC93).
[JA91]
R. Jagannathan and E. A. Ashcroft. Fault Tolerance in Parallel
Implementations of Functional Languages, In The Twenty First
International Symposium on Fault-Tolerant Computing. 1991.
[Ked921
Z. Kedem. Methods for Handling Faults and Asynchrony in
Parallel Computations. In 1992 DARPA SofnYare Technology
Conference,pages 189-193, May 1992.
[KP921
Z. Kedem and K. Palem. Transformations for the Automatic
Derivation of Resilient Parallel Programs. In IEEE Workshop
on Fault-TolerantParallel and Distributed Systems, pages 1525, 1992.
[KPRR92]
Z. Kedem, K. Palem, M. Rabin, and A. Raghunathan. Efficient Program Transformations for Resilient Parallel Computation via Randomization. In 24th ACM Symp. on Theory of
Computing, pages 306-317, May 1992.
[KPRS91]
Z. Kedem, K. Palem, A. Raghunathan, and P. Spirakis. Combining Tentative and Definite Algorithms For Very Fast Dependable Parallel Computing. In 23rd ACM Symp. on Theory of
Computing, pages 381-390, May 1991.
[KPRS9 3]
Z. Kedem, K. Palem, A. Raghunathan, and P. Spirakis. Resilient Parallel Computing on Unreliable Parallel Machines. In A.
Gibbons and P. Spirakis, editors, Lectures on Parallel Computation. Cambridge University Press, 1993.
[KPS90]
2. Kedem, K. Palem, and P. Spirakis. Efficient Robust Parallel
Computations. In 22nd ACM Symp. on Theory of Computing,
pages 138-148, May 1990.
[LH89]
K. Li and P. Hudak. Memory Coherence in Shared Virtual
Memory Systems. ACM Transactions on Computer Systems,
7(4):321-359, November 1989.
[MDRK921
B. Millard, P. Dasgupta, S. Rao, R. Kuramkote. Run-Time Support and Storage Management for Memory mapped Persistent
Stores. In Proceedings of the 13th International Conference on
Distributed Computing Systems, 1992.
[MF89]
R. Minnich and D. Farber. The Mether System: Distributed
Shared Memory for SunOS 4.0. In USENIX-Summer, pages
5 1-60, Baltimore, Maryland (USA), 1989.
[PWC+81]
G. Popek, B. Walker, J. Chow, D. Edwards, C. Kline, G.
Rudisin, and G. Thiel. Locus: A Network Transparent, High
Reliability Distributed System. In Proceedings of the 8th ACM
Symposium on Operating Systems Principles, pages 169-177.
ACM, 1981.
[Ra8 I]
M. Rabin. Fingerprinting by Random Polynomials, Technical
Report, Harvard University, 1981.
[Rab89]
M. Rabin. Efficient Dispersal of Information for Security, Load
Balancing and Fault Tolerance. J. ACM, 36:335-348, 1989.
[Sun901
V.S. Sunderam. PVM: A Framework for Parallel Distributed
Computing. Concurrency: Practice and Experience, 2(4):3 15339, 1990.

7. Bibliography
[AKPR93]
Y. Aumann, Z. Kedem, K. Palem, and M. Rabin. Highly Efficient Asynchronous Execution of Large-Grained Parallel Programs. In 34th IEEE Ann. Symp. on Foundations of Computer
Science, pages 271-280, 1993.
[AR93]
Y. Aumann and M. Rabin. Clock Construction in Fully Asynchronous Parallel Systems and PRAM simulation. In 33rdlEEE
Ann. Symp. on Foundations of Computer Science, 1993, pages
147-156.
[Bar941
A. Baratloo. Performance Experiments with a Fault-Tolerant
Parallel Processing System. unpublished manuscript, 1994,
[Bir93]
K. Birman. The Process Group Approach To Reliable Distributed Computing. Technical report, Come11 University, 1993.
[BT90]
H. E. Bal and A. S. Tanenbaum. Orca: A Language for Distributed Object-Based Programming. SIGPLAN Notices, 25(5):1724, may 1990.
[CAL+89]
J. S. Chase, F. G. Amador, E. D. Lazowska, H. M. Levy and R.
J. Littlefield. The Amber System: Parallel Programming on a
Network of Multiprocessors. In Proceedings of the 12th ACM
Symposium on Operating Systems Principles, pages 147-158.
ACM, 1989.
[CG89]
N. Carrier0 and D. Gelemter. Linda in Context. Communication ofACM, 32(4):444-458, 1989.
[CLG+94]
P. Chen, E. Lee, G. Gibson, R. Katz, and D. Patterson: RAID:
High-Performance, Reliable Secondary Storage. ACM Computing Surveys, Volume 26, Number 2, June 1994, pp. 145-186.
[COO~S]
E. Cooper. Replicated distributed programs. Operating Systems
Review, 19(5):63-78, December 1985.
[DAM+9 11
P. Dasgupta, R. Ananthanarayanan, S . Menon, A. Mohindra,
and R. Chen. Distributed Programming with Objects and
Threads in the Clouds System. Computing Systems, 4(3):243276, 1991.

[DCM+90]
P. Dasgupta,R. C.Chen,S. Menon,M. P.Pearson,R. Ananthanarayanan, U. Ramachandran, M. Ahamad, R. J. LeBlanc,
W. F.Appelbe,J. M. Bemabeu-Auban,P. W.Hutto,M. Y. A.
Khalidi, and C. J. Wilkenloh. The Design and ImDlementation
of the Clouds Distributed Operating System. Cobmputing Systems, 3, 1990.
[DLAR90]
P. Dasgupta, R. LeBlanc Jr., M. Ahamad, and U. Ramachandran. The Clouds Distributed Operating System. IEEE Computer, 1990.

474

d P ro grammin g
with Objects and Threads
in the Clouds System*

D i st r ibut

e

Partha Dasgupta and R. Ananthanarayanan

Arizona State University
Sathis Menon and Ajay Mohindra
Georgia Institute of Technology
Raymond Chen
Siemens-Nixdorf Information Systems

ABSTRACT The CLOUDS operating system supports a
distributed environment consisting of compute servers,
data servers and user workstations. The resulting
environment logically simulates an integrated,
centralized computing system. In addition, CLOUDS
supports a programming paradigm that makes
distributed programming simpler. Distributed programs
can be written in a centralized fashion and yet they can
exploit parallelism and distribution at runtime.
The system paradigm is based on an object/thread
model. The basic building blocks for applications are
persistent memory (called objects) and computation
(called threads). Unlike most systems, CLOUDS
separates the notion of memory from computation.
Programming environments based on these
abstractions, though unconventional, provide powerful
tools for composing applications that exploit
concurrency and distribution.

*

This work was supported in part by NSF contract CCR-86- 19886.

@

Computing Systems, Vol.

4 . No. 3 . Summer 1991 243

This paper discusses programming techniques that
use persistence and distribution of memory. The
examples show how separation of computation from
memory can be used to the programmer's advantage.
We also present a distributed programming technique
called implicit distributed programming. The
implementation details of the programming support
subsystems are presented. The system performance
measurements demonstrate the usability of CLOUDS
as a distributed programming platform.

l.

Inftoduction

The CLOUDS operating system supports a distributed environment
consisting of compute servers, data servers and user workstations. The
resulting environment logically simulates an integrated, centralized
computing system.
The basic building blocks of a CLOUDS application are persistent
memory and computation. Persistent memory is encapsulated in an object specified through a program. Computations are encapsulated by
threads, which are created at runtime, under user control or under
program control. Thus, memory and computation are treated aS orthogonal entities. The separation of storage and execution allows concurrent threads in the same address Space and allows a thread to execute in multiple address spaces. In addition, persistent memory unifies
several levels of the storage hierarchy into a single level store. We call
this system model the object/thread paradigm.
In addition, location of an object is orthogonal to its use. That is,
the site which provides persistent store for the object need not be the
same as the site where the object is being used. This property makes
applications completely independent of sites. If several computations
share the object and execute on different sites, CLOUDS preserves
single-copy semantics of the object.
Programming environments exploit these features to support an application development platform. Applications built using these environments indicate that the programming paradigm based on object/threads

244

P. Dasgupta, R. Ananthanarayanaî' S. Menon,

A' Mohindra,

and

R' Chen

is more natural to program than comparable applications built using
other paradigms. This property is mainly due to the absence of code
necessary to deal with location, distribution, secondary storage, and
message passing.

1.1 Objectives
In this paper we explore the programming paradigm and techniques
that allow implementation of complex distributed applications in a simple manner. To this end, we:

.
.

.

.
.

Present the system architecfure, the programming paradigm and
the mechanisms that support transparent distribution, namely,
Distributed Shared Memory and Remote Object Invocation.

Provide details on the Distributed C# (DC+) programming
environment (Section 3) to provide insight into the actual program specification of the features of CLOUDS.
Show how distributed programs are structured and how
"transparent" distribution facilities are made available to the user
(Section 5). In our paradigm, distributed applications can be
written in a centralized fashion and yet exploit the parallelism
provided by distribution at runtim¿. The novelty of the
CLOUDS programming paradigm allows the above prograqtming strategy.
Provide implementation details on the system environment. We
will present implementation details of the key operating system
features that support the distributed programming environments.
Discuss why the separation of processing from memory leads to
the building of versatile programming environments. Many
other systems offer comparable programming environments. A
comparison with these systems is included in section 9.

In addition, we present performance measurements that demonstrate the feasibility of our approach (Section 8).

2. An Overview of Clouds
The user view of CLOUDS consists of a system architecture and a
programming model. The system architecture comprises of a set of
servers as discussed in section 2.1. The programming model consists

Distributed Programming with Objects and Threads in the Clouds System

245

of objects, threads and invocations (sections 2.2,2.3). While such a
programming model could be used for structuring applications on a
single machine, in CLOUDS, distributed shared memory and remote
object invocations are used to achieve tfansparent distribution as discussed in section 2.4.

2.1 The Clouds

System Architecture

The CLOUDS system integrates a set of machines into one seam-less
environment that behaves like one, large computer. The system
configuration is composed of three logical categories of machines, each
supporting a different logical function. These machine categories are
compute servers, d.ata servers and user worlcstations (see Figure 1).
The core of the system consists of a set of homogeneous machines
of the compute Server category. Compute Servers do not have any
secondary storage. These machines provide an execution service for

Compute Server

Compute Server

User Workstation

Clouds

Clouds

Network

Data Server

Data Server

Objects

Figure 1: CLOUDS Logical System Architecture

246

P. Dasgupta, R. Ananthanarayanan' S. Menon,

A' Mohindra'

and

R'

Chen

threads. The compute servers run the Clouds operating system in native mode.
Secondary storage is provided by data servers. Data servers are
used to store CLOUDS objects. The data servers can be machines of
any type and can run any operating system, as long as processes can
run services necessary by Clouds. In our system the data servers run
Unix as well as the RaIP (the Clouds communication protocol) and
some data service processes.
The third machine category is the user workstation, which is expected to support high-power graphics and other interface capabilities.
In our implementation the user workstation is an X-Server running under Unix.
The logical machine categories do not have to be mapped to physical machines using a one-to-one scheme. Although a disk-less machine
can function only as a compute server, a machine with a disk can
simultaneously be a compute and data server. This configuration enhances computing performance, since data access via local disk is
faster than data access over a network. However, in our system, we
use a one-to-one mapping, in order to keep the system implementation
and configuration simpler.

2.2

Objects and Threads

Objects and threads are the artifacts of programming in CLOUDS. An
object is a persistent virtual address space consisting of code, data and
entry points. Each object is a named instance of a specification (or a
class) programmed by the programmer in any language supported by
CLOUDS. Once the object is instantiated, it exists "forever", that is
until explicitly deleted.
While objects are passive abstractions of memory, a thread is an
active abstraction of a CPU. A thread starts executing inside an initial
object and traverses between objects through invocations. On an invocation, the thread leaves the address space of the invoking object and
enters the address space of the invoked object. Parameters, if any, are
transferred between the invoker and the invoked object on invocation
startup and results are returned on termination.
The mapping between threads and objects is defined at runtime by
the invocation mechanism. The invocation causes a thread to enter an
object. If several invocations occur on an object concurrently, multiple
Distributed Programming with Objects and Threads in the Clouds System 247

threads will be executing within the object. The invocation mechanisms also allows a thread to execute in multiple address spaces.
Persistent objects provide storage as well as sharing. Files, which
are the units of permanent storage in other systems, are not necessary
in CLOUDS, since objects are persistent. In a way, objects are similar
to files in that both files and objects are structured containers of persistent data. However, objects provide a means of combining the data
with the code that is used to manþlate the data. Thus, objects are

more structured than files.
Further, the shared memory provided by objects makes message
passing unnecessary when the shared memory is used for purposes
other than synchronization. Synchronization using shared memory imposes a heavy overhead in the form of network traffic and hence is realizedby separate mechanisms, such as semaphores, supported by
CLOUDS, as discussed later.
The separation of computation from the address space has far
reaching effects. It allows concgrrency to be pervasive, that is, all
code is potentially concurrent. Programs need not explicitly specify
concurrency requirements. Also, the thread is not tied to its current
environment. Using the invocation mechanisms it can reach out to environments that it shared with other threads and applications. This feature is especially appealing when shared and distributed repositories of
data are managed and accessed by distributed computations'
Threads execute on compute servers' The permanent storage
repository for object content storage is on data servers. Since threads
execute within objects, the object being invoked is brought to the
compute server running the thread at the time of invocation. The separation of object storage sites and object execution sites allow any
thread executing on any compute server C to invoke any object regardless of its storage site. The object will be executed on site c. If
two thread Tr and Zz executing on compute servers Ct and Cz use the
same object O, O will be available at both C1 and C2 concurrentl].
The coherence of the object contents will be maintained by the system.

2.3 Invocations
As mentioned earlier, object invocation is the mechanism that makes
threads and objects interact. A new thread starts its lifetime when it

248

P. Dasgupta, R. Ananthanarayanan' S. Menon,

A' Mohindra,

and

R'

Chen

Distributed object space (persistent, virtual memory)

_

-Entry points

Stack

Figure 2: Objects, Threads and Invocations

invokes an object and it terminates when this invocation is over. Thus,
one thread performs exactly one top-level invocation.
In the course of this invocation, it may perform nested invocations.
Each nested invocation is done by a thread upon executing an invoke
directive in the program it is executing. The invoke directive is an external procedure call (similar to a remote procedure call).
Each nested invocation can be of two basic types:

.

.

A synchronous invocation causes thread Z to leave the address
space it is currently executing and enter an entry point in another address space (or object). After this invocation terminates,
Z returns to the original object as if it has completed a procedure call.
An asynchronous invocation causes T to create a new thread Z ,.
Z' performs a qynchronous invocation on the target address
space (or object). T continues to execute in the invoking object,
in parallel with Z'. When the asynchronous invocation exits, Z,
terminates. T can check for the termination of Z,or wait and
collect the result of the invocation that T ' computed.

Distributed Programming with Objects and Threads in the Clouds System 249

As noted earlier, a thread can invoke any object at any site. This
makes cLouDS appear to be a centralized system where all objects
are available at any compute server. To allow distributed programming
we allow invocations to have two additional properties.

.

A Local Invocation causes the target object to be invoked at the

.

same compute server as the thread making the invocation.
A Remote Invocation causes the target object to be invoked at
some other compute server. The target compute server may be

provided explicitly as an argument to the invocation request, or
can be implicitly assigned by the system.
The combination of synchronicity of invocations with the location
properties of invocations give rise to four invocation types. These four
types of invocations, combined with the concepts of separate persistent
memory and threads makes CLOUDS a powerful distributed programming environment. We shall discuss later the use of these forms of invocations for writing distributed programs. As an example, we will
show how the remote asynchronous invocation is a simple but powerful mechanism that can be used to start up distributed, parallel computations.

2.4 DSM and ROI
The compute and data servers interact to provide a distributed operating system environment. These interactions occur through the following CLOUDS operating qystem mechanisms:

. Distributed Shared Memory @SM)
. Remote Object Invocation (ROD
DSM is used to store and share objects in the system. For example, let uS assume that a thread is to be run on a particular compute
server. The object in which the thread has to execute must be paged
from the data server to the compute server. This facility requires a remote paging facility, which is provided by DSM. DSM supports the
notion of shared memory on a non-shared memory, distributed architecture IAMMR9O].
In CLOUDS, there is potential for concurrent invocation of the

25O P. Dasgupta, R. Ananthanarayanan, S. Menon, A' Mohindra, and R' Chen

by threads at different compute servers, resulting in multiple copies of the same object being used at several compute servers.
Hence, DSM has to be cognizantof the need to provide the coherence
of shared pages.
The coherence specification of an object O being used at two
nodes A and B, requires thatA andB see the same contents of O. This
is called one-copy semantics. The maintenance of one-copy semantics
is achieved by coherence protocols that are an integral part of the
DSM access strategy tLH86l [RAK89].
Suppose a thread is created on compute serverA to invoke object
O¡. The compute server retrieves a header for the object from the apsame object

propriate data serverl, sets up the object space, and starts the execution of the thread in that space. As the thread executes in that object
space, the code and data of 01, accessed by the thread, are demand
paged from the data server (possibly over the network) to A.
The implication of the CLOUDS DSM mechanism is that every
object in the system logically resides at every node. This powerful
concept separates object storage from its usage, effectively exploiting
the physical nature of distributed systems composed of compute servers
and data servers.
The second method of interaction between servers in the system is
based on the ROI facility. In order to start a user level computation, a
compute server must be selected to execute the thread. The selection is
controlled explicitly by the programmer, or implicitly by the system
based on scheduling policies. The thread is started on the selected
compute server by sending an ROI request to the server. The compute
server completes an ROI request by obtaining a copy of the object (via
DSM) and executing the thread. Thus, ROI can be used by threads to
distribute computations by initiating further processing on different
system compute servers.
If the thread executing in O1 generates an invocation to object O2,
the invocation may happen on A or B on depending on whether a local
or remote invocation was requested by the programmer. In the former
case, if the required pages of object O2 are at other nodes, they have
to be brought to node.4 using DSM. Once the object has been brought

l.

The data is retrieved from the data server that contains the object segments. The systemlevel name of the object contains the identity of the data server.

Distributed Programming wíth Objects and Threads in the Clouds System 251

A, the invocation proceeds. In the latter case, the thread sends an
invocation request to B, which invokes the object Oz and returns the
results to the thread at A. More details on object sharing is provided
in Section 6.2.
to

CLOUDS ROI is similar to remote procedure call (RPC) mechanisms supported by other distributed systems such as the V system
lChe88l. However, it is more general because a ROI can be sent to
any machine and the target does not have to store the called object. A
similar effect can be obtained in other RPC systems, when used in
conjunction with a distributed file system. However, additional mechanisms for maintaining coherence of replicated file data will be necessary. Such issues are handled in a uniform manner by the DSM system
IAMMRgOI.
To summarize:

.

.

The DSM coherence protocol ensures that objects are globally
accessible and data in an object is seen by concurrent threads in
a consistent fashion even if they are executing on different compute servers.
The ROI facility allows for distribution of computation'

The system structure discussed above allows us to support different
kinds of structuring of applications, as described in Section CLOUDS.
In the next section, we give_a brief overview of the specific programming environments supported in CLOUDS.

3.

The Clouds Programming
Environments

Currently CLOUDS supports three varied programming environments,
namely Distributed C+ (DC#), Distributed Eiffel and CLiDE.
DC# provides a system programming environment based on the
C# language. Distributed Eiffel is an application programming environment based on Eiffel tGLgOl. CLiDE is a lisp-based distributed
symbolic processing qystem [PD90]. In this paper we will present programming paradigms for using the DC# language.

252

p. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohindra,

and R. Chen

3.1 Basic Programming Environment
The basic environment supports an object-oriented paradigm. The programmer is provided with two kinds of structuring tools: classes (templates) and instances (objects). CLOUDS objects encapsulate particular
application behavior and are large grained. A class is the template that
is used to generate instances. Object instances may be invoked by user
threads. In order to write application programs for CLOUDS, a programmer specifies one or more CLOUDS classes that define the code
and data of the application. The programmer then creates the requisite
number of instances of these classes. The application is executed by
creating a thread to execute the top-level invocation that runs the application.
A user develops CLOUDS programs using the DC+ language
and then compiles them on a user workstation. Once compiled, generated objects are automatically loaded onto a data server, making them
available to all compute servers. Any compute node (with initiation
from a user) can create instances of these classes. Once a class is instantiated, the resulting object becomes part of the persistent object
memory and can be invoked until explicitly deleted.
Threads are started in objects either interactively or by explicit
thread creation under program control. A user invokes an object by
specifying the object, the entry point and the arguments in a
CLOUDS så¿l/ session running on a user workstation. This shell sends
an invocation request to a compute server and the invocation commences. Users may communicate with the created thread via an Xterminal window on a user workstation.

3.2

The

DC#

Environment

DC+ is a programming

language and environment that provides support for CLOUDS classes, objects, instantiation, inheritance and naming [Ana91]. DC# is an extension of the
language [Str86]. To
give the reader a flavor of programming CLOUDS objects in DC#,
we present a simple example.

C#

In this example, we program a CLOUDS class called rectangle
represented by using the state variables length and width. The

Distributed Programming with Objects and Threads in the Clouds System

253

object has two entry points, one for setting the size of the rectangle
and the other for computing its area. The class rectangle is defined
as follows:

clouds_class rectangle
int
length, width;
// persistent data for rect.
entry size (int length, width);
/ / set size of rect.

entry int area O;
/ / re1"urn area of rect.

end class

Once the class is compiled, instances may be created. Suppose the
rectangle class is instantiated, and the instance is called Rect01.
Now Rect01. size can be used to set the size and Rect01. area can
be invoked to return the area of this rectangle. The entry point in the
object may be invoked by using a command in the CLOUDS shell
command interpreter. Entry points may also be invoked in a user program, allowing one object to call another.
Objects are idenitified by the CLOUDS system using unique system names. Users associate a user level mnemonic name with an object upon instantiation. The DC# runtime system includes a name
server which manages the mapping of user level names to system
names.

CLOUDS objects are referenced from other objects through a special class defined by the language. This class is called a clouds object
appended to the
reþrence class.It is represented by the suffix
-ref
CLOUDS class. The reference class has methods to instantiate and
bind the object, and methods which act as stubs to invoke the user
defined entry points of the CLOUDS class.
User-level names are bound to the system name of an object before
invocations can be performed. This binding is achieved by the bind
operation in the reference class. The following code fragment details
the steps in gaining access to a CLOUDS object Rectol and invoking
operations on it:
rectangle_ref rect;
/ / tecL is a local program
/

/

handle that refers
to an object of
type rectangle
caII to name server

rect.bind("RectOl")
//
rect.size(5, 10)
// invocation of RectO1
printf (n%odn, rect. areaO ) ;
/ / wíII print 50
254

P. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohindra, and R. Chen

The execution of rect. size and rect. area results in the processing of a local synchronous invocation to the object instance

RectOl.
In addition to the local synchronous invocations depicted above,
the the operations can be invoked in a local asynchronous manner by
using the qyntax rect!area or rect!size. Remote object invocations (ROI), both synchronous and asynchronous can be programmed
via the virtual node facility (see Section 5.1).
The above example demonstrates programming one CLOUDS object. Since DC# is an extension of C++, CLOUDS objects can also

contain C# classes and instances. These CH language entities
stored in the address spaces of CLOUDS objects share the properties
of CLOUDS objects: they are persistent and can be accessed concurrently via multiple threads that invoke a particular CLOUDS object.
Because an object invocation on a CLOUDS object is at least an
orcler of magnitude more expensive than a simple procedure call, a
CLOUDS object is appropriate for use as a module that contains several fi ne-grained entities.
DC+ + also provides a variety of other mechanisms to support object programming. These include synchronization, static type checking, builrin data types, memory support services, user I/O support
and facilities to define user interfaces. Some of these facilities are outlined in later sections.
User objects and their entry points are typed by the language
definition. Static type checking is performed on the object and entry
point at compile time. No runtime type checking is done by
CLOUDS.
Modification of classes and instances is discussed briefly in the next
section.

4. Programming A Dictionary
To present the power of combining the building blocks provided by
CLOUDS, we present a simplistic implementation of a dictionary. The
example dictionary is an object supporting the functionalities of insertion, look up and deletion of entries.
The following class defines the dictionary in DC+*. Vy'e have
used descriptive names for some internal procedures, the code for
which is not shown.
Distributed Programming with Objects and Threads in the Clouds System 255

clouds_class dictionary
RecordType data tIaAXl

;

/ storage for the

/

database

entry insert (RecordType itern)
index : hash(iten);
I

ock_data_record ( index)

{

;

insert_item (index, iten)

;

unl ock_data_record ( index)

;

Ì

entry

RecordT5pe lookup (KeyType key)

index : hash(item);
return(dataliteml ) ;

Ì

entry delete (KeyType item)
index : hash(item);
delete_itern (index)

{

;

Ì

end class

This implementation of the dictionary stores data as an affay in
the object. This array is persistent by definition and never needs to be
explicitly written out to a file. Also, the usual conversion overhead between internal memory representation and external data format is
completely eliminated. Not only does this feature save a lot of code in
the implementation of objects, but also is more natural as the programmer deals with only one type of storage-memory.
An instance of the dictionary may be used by computations by invoking entry points in it. Since these computations can be executing
concurrently in the system, the dictionary may be operated upon concurrently by several threads, even if the threads are executing on different compute servers. Note that there is no special program
speciûcation necessary on the part of the programmer to achieve concurrency. That is, though the dictionary is a concurrent program, no
concurrent programming library or routines are necessary to implement it. However, since all code is potentially concurrent, the programmer needs to use locks to ensure re-entrancy of appropriate por-

tions of the code. On the other hand, the need for locking does not
prove to be a problem either. If a programmer does not want concurrency, the compiler can be instructed to protect all entry points with a
lock, like in a monitor.
The tookup entry point does not modify the data contained in the
dictionary. Consequently, when threads at several sites invoke the
lookup entry point, the dictionary gets replicated, automatically.
There is no replication code specified in the program itself. The DSM
mechanism provides replicated copies to all users of the dictionary, if
the lookup entry point is the only one invoked. The replication is
done at runtime with no hints from the programmer (that is the
lookup function was not defined to be a read-only function). Hence,
operations on an object that do not modify the data in it can be executed completely in parallel, even though the computation is performed
on different sites. However, note that this form of replication enhances
the performance of the system, but not the fault-tolerance.
When a thread at any site invokes an entry point that modifies the
dictionary (for example, the delete entry point), the page containing
the data being modified is automatically yanked (invalidated) from all
replicated copies of the data. Later, if any thread reads that page, the
new (updated) copy is automatically provided. Note that this automatic
replication is due to DSM and is not programmed by the user.
The example shows some of the ease of programming in
CLOUDS. Concurrency, read-replication, transparent distribution and
persistence are all used in this simple program. The programmer does
not have to explicitly program these features into the objects, but they
are provided by CLOUDS.

4.1 Modifying the Dictionary
The above dictionary can be modified only by the insert and
delete entry points. This may not be suitable when the dictionary
may need to be re-initialized or its contents need to be extracted.
CLOUDS does not (yet) provide any consistent method of doing these
operations. The programmer can provide initialize and list entry points
for such operations. Also if the directory gets corrupted, the object becomes unusable. At present there is no facility for correcting such situations.

Distributed Programming with Objects and Threads in the Clouds System 257

Both the above problems can be corrected by editing the data segments directly using another program or interactive data editor. Another alternative is to program a class that is inherited from the directory class with appropriate methods. An instance of this class can be
merged with the data segments of the directory instance that needs
fixing. In a persistent programming system repair and interactive
modification tools are necessary, and our research in this area is still
open.

In addition, note that after an instance of the dictionary is created,
modifying the parent class does not modify the type of the instance.
The instance inherits the properties of the class as it existed at the
point of instantiation. Thus, in a sense, CLOUDS classes are immutable.

5. Distributed

Programming
in the Clouds System

While CLOUDS programming environments provide a means to
specify application programs, the structure of these application can
vary widely. As noted earlier, in CLOUDS, objects can be written to
run on a centralized computing environment without regard to concurrency, replication, etc. In contrast, consider a message-based distributed system. One of the popular means of structuring a distributed
program is as a set of processes consisting of a master and a number
of slaves. The master allocates work to the slaves, which perform the
necessary computation and send back the result. All communication is
done through messages. In CLOUDS, the same effect can be achieved
in a different manner that is easy to understand and program. This section discusses how programmers can organize such an application.
An application program, consisting of a set of classes and objects,
can be structured in three different ways:

.

Tleat the CLOUDS qystem as one integrated, centralized system.
Each application is programmed as one or more CLOUDS
classes, each with one or more instances. Existing classes can be
reused. Each instance is an object that can contain a complete
object oriented environment. This is centralized program-

C#

ming.

258

P. Dasgupta, R. Ananthanarayatan, S. Menon, A. Mohindra, and R. Chen

.

.

The programmer can also decide to use the CLOUDS system as
a distributed system in which each object is a pseudo-node.
Computations are executed in as many nodes as there are objects. This is explicit distributed programming.
The third alternative is to structure the application as one (or
more) object(s) as in the centralized scheme, but execute the
computation in a distributed manner by starting the computation
at several nodes, regardless of where the objects are located.
This is called implicit distributed programming.

In addition to the above, the programmer can exploit the persistent
nature of the objects as well as utilize the concurrency within each object. Centralized programming is the same as traditional sequential object oriented programming and will not be dealt with here. We also do
not present persistent and concurrent programming paradigms in this
paper. In the rest of this section, we discuss how explicit and implicit
distributed programming can be achieved.

5.1 Virtual

Nodes

The CLOUDS virtual node facllity is designed to let users target computations to particular virtual nodes. The system is treated as a set of
virtual nodes, each having a node number within a sequential range of
integers. The prograÍrner, while coding CLOUDS objects, is unaware
of the actual physical configuration of the system. Programs request
the desired number of nodes from the run-time system associated with
the virtual nodes facility. If the system can satisfy this request, it returns the actual number of virtual nodes available to the programmer.
The program then partitions its computation based on the number of
nodes granted. The number of virtual nodes in the system need not
correspond to the number of physical nodes.
To run an invocation on a particular node, the user provides the
invocation request with a virtual node number. For example, to synchronously invoke the operation op on object o on a virtual node
identified by node_num (exact syntax not shown for brevity):

O.op (parans)

at

node-num;

Similarly, an asynchronous invocation to op on object o:

Olop (params)

at

node_num;

Distributed Programming with Objects and Threads in the Clouds System 259

5.2 Explicit Distributed Programming
The CLOUDS system can be used as a traditional distributed programming system. The unit of distribution is the object. The prograÍlmer
decides on the number of objects by analyzing the characteristics of
the application. For example, consider distributed sorting. One possible algorithm creates n "sorter" objects, one on each virtual node.
Then, the data is partitioned into n parts and sent to each of the sorter
objects, which sort the data and return the results to the main computation. The main computation then merges the n sorted pieces. This is
what we call explicit distributed programming.
Programming an arbitrary algorithm in this fashion is similar to
programming clients and servers in a distributed message system. It
involves explicit programming of the distribution and protocol
definition to be used for client-server communications and intricate algorithm development. The degree of distribution is also statícaþ
defined by the program, since the number of objects is fixed.

5.3 Implicit Dístributed Programming
In CLOUDS, using DSM and the different types of invocations2 it is
possible to program distributed applications without using the client
server model. This technique allows distribution to be expressed implicitly, and provides the ability to make decisions on the degree of
distribution at runtime.
In implicit programming, the application is structured as one centralized application, typically lusing one CLOUDS object. The unit of
distribution is the thread. Implicit distributed programming structures
the program as a concurrent program and not a distributed one. Each
thread in the concurrent execution runs on a different virtual node, but
uses the same object(s). Since DSM provides one-copy coherent memory across machines, the computation will actually work like a concurrent program. However, if the concurrent threads do not heavily share
the same pages of memory, the performance will be similar to an er
plicitly distributed program.
We present a distributed sorting algorithm based on the above
idea. The following program implements one object called sorter,
Synchronous and Asynchronous combined with Local and Remote.

260

P. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohindra, and R. Chen

which contains an integer array, that is to be sorted. The object has an
operation to sort the array (the entry point sort) which is invoked
when the data needs to be sorted. The code implementing the sort
operation partitions the computation using virtual nodes.
The operation subsort is another entry point in sorter that sorts
part of the array based on parameters that specify starting and ending
points in the array. subsort is executed at different virtual nodes depending on the number of virtual nodes available at runtime. V/hen all
the subsorts terminate, the array is merged.

clouds_class sorter
entry sort O ; / / sort the entire array

private
int arraY tltilAxl ;
entry subsort(int i, int j);
end_class
sorter::subsort(int i, int j) {
sort_injlace(i, j); // sorL arraylil
to array[j], in place.
:

sorter::sortO

{

: getvirnodesO
/ / get free nodes
segsize : MAX/nun_nodes;
/ / size of partitioned data
for (node : 0; node nun_nodes; node**) {
this! subsort (node x seg size,
/ / sel'f invocation
( ( (node + 1) x segsize) - 1) ) at
/ / \laiL for invocations to terminate;

nunnodes

;

node;

merge sorted segments

wait_f or_asynch_invocations
nerge_dataO;

(

)

;

The sub-sorts are concurrently executed using asynchronous invocations. Thus, the sort is executed by multiple threads that execute at a
different (logical) compute servers, and perform computation on different parts of the data in parallel. Note that the data itself is encapsulated in a single object. The data actually required by each thread
migrates to that node automatically, via DSM, as discussed in Section 6.2,

Distributed Progrømming with Obiects and Threads in the Clouds

System 261

Therefore, programming of this sorter object is achieved without
explicit distribution of data, or any knowledge of the actual distribution of the algorithm. Decisions concerning the degree of distribution
of the algorithm are made at runtime.

6.

The Implementation of the System
Environment

CLOUDS is implemented as a native operating system on Sun-3 computers. The compute servers run CLOUDS. The data servers and the
user workstations are implemented by server processes on UNIX
workstations.
CLOUDS is hosted by a minimal kernel called Rø. Ra provides the
basic memory management and scheduling mechanisms. CLOUDS is
built on top of Ra by using pluggable system service modules called
system objects. In this section we discuss the system objects that
provide support for distributed programming: the invocation system,
the synchronization system and the DSM system. In addition, we discuss user-level utilities that provide compilation support for user objects. A more comprehensive description of the implementation of
CLOUDS is available in [DCM+90] [DJAR9l].

6.1 The Invocation System
Objects in CLOUDS are implemented as shared virtual address
spaces. Each object has an object header that defines the layout of the
object address space. Threads are implemented using local processes.
If a thread executes on only one node, then it will be associated with
only one process. However, if the thread performs remote object invocations then the thread will have multiple processes executing on behalf of the thread; one on each machine touched by the distributed
thread.
A thread executing in one object invokes another object through a
system call. The Invocation System then determines, from the system
call parameters, whether the invocation is to be asynchronous or synchronous, and whether it is a local or remote invocation.

262

P. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohindra, and R. Chen

In the case of a synchronous local invocation, the state of the current object invocation is saved. Next, using information in the header
of the invoked object the new object is installed into the address space
of the executing thread. V/hen the thread resumes execution, it will be
executing in the address space of the new object. Asynchronous local
invocations are implemented by creating a new thread to perform the
object invocation.
The invocation bears some overhead due to the fact that the thread
actually changes its data address space from one persistent space to another. For this reason, objects are considered to be large-grained and
invocation is to be used sparingly.
Synchronous remote object invocations are implemented using
slave processes on the remote site and is similar to conventional RPC
implementations [8N83]. In the case of an asynchronous remote invocation, the invoking thread does not block.

6.2 Paging and Sharing of Object Code
and Data
The DSM system is responsible for making all objects available to all
compute servers. It is the software layer between the demand paging
system of the RA kernel and the storage daemons running on data
servers. The DSM system has two subsystems, namely: DSM Server
and DSM Client. Each compute server includes a DSM client and a
DSM server. The data servers each run a DSM server as a Unix process. The communication transport protocol used to communicate between the corresponding system components in different machines is
called RdIP (Ra Tiansport Protocol) [WIL89].
Suppose a compute server A running a computation faults on page
p of data. This fault activates the DSM Client by generating a call to a
method in the system object. The DSM Client locates the DSM server
containing page p.The server, called the owner for any particular
page is fixed, systemwide.
Let site D be the owner of page p. The DSM Client on site A
sends a request to the DSM server on D. If p is currently not being
used by any other compute server, D sends p to A and the computation progresses. Site A now becomes the keeper of p.

Distributed Programming with Objects and Threads in the Clouds System

263

At this point, suppose another computation on another site B page
faults on the same page p.B sends a request to the owner, D. D forwards the request to the DSM server on A, since A is the keeper of p.
In response to the forwarded request, the DSM server at A unmaps p
from the address space of the thread using the page and sends it directly to B. This is called yanking the page. If both A and B use a
page concurrently, this page wlll shuttle between A and B guaranteeing one-copy semantics [LH86] [RAK89].
In the above scheme, each page has one owner (the data server)
and at most one keeper (the compute server using it). For read-only
pages the constraints are relaxed, and a page can have multiple keepers. Read-write pages can be acquired in read-only mode (via readmode page faults) allowing better performance when pages are readshared by several compute servers.

6.3

Support

for Synchronization

The data space of an object is shared by all computations that execute
in the object. Since different computations can run in the same object
concurrently, there is need for mechanisms that provide mutual exclusion and thread synchronization. Since the data in an object is accessible only by threads executing within the object, synchronization is a
local property. That is, the support and programming of thread qynchronization is local to each object.
However, the same object may be used by concurrent threads running on different compute servers. Thus, the synchronization, though
local to each object, is non-local to the machine using the object. This
section discusses the implementation of semaphores and locks that
provide intra-object, yet, distributed synchronization.
Synchronization support can be provided at the language level using constructs such as semaphores, locks and monitors. The implementation of such constructs, however, needs operating system level
support. CLOUDS provides support for synchronization in the form of
semaphores and read-write locks [Ana]. Each semaphore or lock is
identified by the CLOUDS operating system by a name that is composed of two parts: a system name and a local-id. The system name is
the same as the system name of the object where the semaphore/lock
is defined, and each semaphore/lock within the object has a local-id.
This scheme eases management of these lock names by imposing a

264

P. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohindra, and R. Chen

logical hierarchy, based on their intended use. All state information
associated with semaphores and read-write locks is maintained by the
operating system.
Semaphores support create, P and Voperations. Read-write locks
support locking in read mode or write mode, and unlocking. In addition, a get operation is provided with both semaphores and read-write
locks. The get operation is a directive to cache the state information
corresponding to a particular synchronization primitive at the node executing the operation. This operation can be used to improve performance by making use of locality of access to the semaphore or the
read-write lock.

6.4 From Programs to Objects
In this section, we briefly describe how objects are created from a program specification. In particular, we discuss the implementation of

DC#.
DC+

programs are developed on user workstations and are
stored as UNIX text files. A DC# program module consists of a
class definition file and an implementation file. These programs are
converted to C#, using a preprocessor. The converted programs
define a CLOUDS class. In addition, the preprocessor generates interface stubs to access this class. These stubs include the CLOUDS object
reference class (see Section 3.2) and the information needed to support
inheritance of CLOUDS classes. All this information completely
defines a CLOUDS class and is stored as part of the environment of
the programmer. This environment serves as a library when that
CLOUDS class is used or inherited by other CLOUDS classes. C#
programs are compiled with a standard compiler along with the library
that defines, among other things, the CLOUDS system call stubs.
After the compilation of the program(s) to UNIX.o files, the programs are linked with the library using the UNIX link editor (ld). The
link editing phase creates a UNIX executable with the a. out format.
The a.out file is then post-processed into segments that adhere to the
object format3. The program is stored as two files containing the data
segment and the code segment.

3.

An object may contain multiple data segments. The layout and number of segments are
under the control of the programmer.

Distributed Programming with Objects and Threads in the Clouds

System 265

The segment files are then loaded on the data server. The loading
accomplished by adding the segments and the object descriptor (another segment) to the list of segments managed by the data server. At
this point, the segments are accessible on the system. Objects represented by these segments can then be invoked or instantiated.

7. More Programming Support
In addition to the programming support mentioned in earlier sections,
the CLOUDS system supports various types of persistent memory and
provides consistency support for persistent objects. These mechanisms
allow CLOUDS programs to use advanced memory structures and
define consistency requirements of applications.

7.1 Memory Semantics
Persistent memory needs a structured way of specifying attributes such
as longevity and accessibility for the language-level objects contained
in CLOUDS objects. To this end we provide several types of memory
in objects. The sharable, persistent memory is called per-object memory. We also provide per-invocation memory that is not-shared, but is
global to the routines in the object and lasts for the length of each invocation. Similarly, per-thread memory is global to the routines in the
object but specific to a particular thread and lasts until the thread terminates. This variety of memory structures provides a powerful programming support in the CLOUDS system [DC90].

7.2 Consistency Support
The CLOUDS consistency-preservation mechanisms present a uniform
object-thread abstraction that allows prograÍrmers to specify a wide
range of atomicity semantics. This scheme performs automatic locking
and recovery of persistent data. Locking and recovery are performed
at the segment-level and not at the object level. Since segments are
user defined, the segment-level locking allows the user to control the
granularity of locking. Custom recovery and synchronization are still
possible, but will not be necessary in many cases.

Threads are categorized into two kinds, namely s-threads (or standard threads) and cp-threads (or consistency-preserving threads). The
s-threads are not provided with any system-level locking or recovery.
The system supports well defined automatic locking and recovery features for cp-threads. When a cp-thread executes, all segments it reads
are read-locked and the segments it updates are write-locked. On
completion, the segments are committed and locks released. Further,
cp-threads are classified to support global consistency across objects
and local consistency within an object. Since s-threads do not automatically acquire locks, nor are they blocked by any system acquired
locks, they can freely interleave with other s-threads and cp-threads.
The complete discussion of the semantics, behavior and implementation of this scheme is beyond the scope of this paper, and the reader
is referred to [CD89].

8. Perþrmance
This section presents performance measurements for the invocation
subsystem and other related subsystems in CLOUDS. In our environment, compute servers run on diskless Sun-3/60 machines; data servers and user workstations are Sun SPARCstation 1 machines running
UNIX.
Kernel Operation

Page Fault Service (Local) without Z.ero Fill
Page Fault Service (Local) with Zero Fill
Page Fault service from data server (Remote)

629 p.s

1.5 ms
16.1 ms

Täble 1: Basic Timings

Object invocation involves the paging in of the object header from
the data server and the installation of an address space that contains
the object text and data, from the information contained in the object
header. When a thread starts executing in the newly installed address
space, the text and data are fetched on demand by the page-fault handler, in co-operation with DSM. The basing timings for page-fault
handling, when the page is resident on the same node costs 1.5 ms for

Distributed Programming with Objects and Threads in the Clouds System 267

a zero-ûlled 8K page and costs 629 p,s for a non zero-filled page. Such
faults do not require network messages.
The time taken to service a page fault (that requires the page to be
fetched from a remote data server) costs 16.3 ms. The page fetch over
the network uses the R¿iIP reliable transport protocol.
Table 2 summarizes the costs for local object invocation. Invoking
an object for the first time involves at least two page-fault operations
for bringing the object header (an 8K page) and one page of code, to
the local compute server. Such an invocation takes 93 ms, while an invocation that also accesses a data page takes 119 ms. Roughly, half of
these invocation times is spent in remote page fault servicing. The rest
of the time is spent on installing object and thread contexts, protecting
invocation stack etc.
Time

Invocation Operations

Synchronous Local Object Invocation

- l"'time
- 1"' time, I

93 ms
data page

- 2ú time
Asynchronous Local Object Invocation
- l"t time, retum from call

- 2ü time
Table

2: Invocation

119 ms
8.9 ms
17.8 ms

66 ms
17.8 ms

Performance

The next time the same object is invoked, its pages are cached in
memory and invocation time (8.9 ms) drops sharply. The reduction in
time is due to the fact that no page fault occurs and no network network access is needed. Overhead in this case involves switching the
address spaces of processes. In general, object invocation costs should
be amortized over the lifetime of the object at a particular compute
site.

A local asynchronous invocation is measured from the time the invoking thread issues the invocation request to the point the request returns. Such an invocation involves setting up the object header (paging
in one page, on the first invocation) and creating a new thread. The
total time of 66 ms does not involve bringing in code or data pages or
waiting for the nev.rþ created thread to run. The new thread waits for

its time slice before it executes and may wait a long time before it actually executes. Costs for subsequent invocations is less since the object header mapping does not involve network access. However, its
cost is larger than a synchronous invocation due to the thread creation
overhead.

Remote invocations are almost identical to the local invocations,
except that an invocation request is sent to another compute server.
The performance measurements for the CLOUDS distributed operating system show that it is quite competitive with any system that
works over a network without local disks. While initial operations are
slower, subsequent operations are considerably faster. Thus, the
speedup of subsequent operations due to caching provides fast overall
execution characteristics when network costs are properþ amortized.

9.

Related Work

Distributed programming has been around ever since networking was
made possible. Some of the first major distributed applications such as
uucp and USENET used handshaking over communication lines without operating systems support. Bal, et. al. [BST89] presents a comprehensive survey of programming languages and systems developed for
distributed systems, classifying them by functionality and intent. A
complete discussion of all the environments is beyond the scope of this
paper, and we shall compare CLOUDS to some of the closely related
systems. Other related systems can be found in references [CJR87] and
ls+891.
Orca is a progranìming language and runtime system to program
distributed applications [BKT90]. It extends the abstract data type
model to distributed systems through shared data obiects. The runtime
system of Orca provides support for sharing and location of objects.
The programming support is heavily dependent on the Orca runtime
mechanisms and not the underþing operating system. In contrast,
CLOUDS provides most of the support necessary for DC* +. This
support allows multiple language implementations, without reimplementing the runtime support for each language. As mentioned
before, Distributed Eiffel and CLiDE are two other environments that
run on top of CLOUDS.

Distributed Programming with Objects and Threads in the Clouds

System 269

Distributed Eiffel provides a more structured programming environment and is intended for casual programmers while DC# is intended for systems programmers. CLiDE is an environment implemented using DC#, and caters to symbolic programming needs such
as those of AI applications. Multiplicity of languages and programming environments allows the user to choose an appropriate vehicle of
expression depending on the application. Using the Clouds approach,
effort is not duplicated in implementation of multiple runtime systems
that perform similar tasks. Further, object sharing in Orca is restricted
to processes that are related. Such a restriction does not arise in
CLOUDS as a direct result of persistence of objects and orthogonality
of computation and objects.
V/hile Orca hides the location of objects from the prograrnmer,
Emerald [JLHB88] and Amber [CALL89], a descendent of Emerald,
provide mechanisms to move objects when necessary. This feature is
similar to CLOUDS, where objects move to a node on invocation and
remain there if no other nodes invoke the object. However, automatic
replication due to immutable invocations are handled differently.
Replication is controlled in Emerald by the programmer by claiming
the object to be immutable: the system does not check the validity of
the claim. While this property is potentially dangerous due to possible
programming errors, it is also a static property. Amber allows the mutability of an object to be dynamically specified. In CLOUDS, on the
other hand, replication is controlled dynamicalþ based on actual usage
as illustrated in the dictionary example.
Some operating systems implement their own versions of objects at
the kernel level. These include Argus [Lis84], Cronus [5TB86] and
Eden [ABLN85]. The objects in these systems are modules that run as
UNIX processes and respond to invocations or messages. The objects
can be checkpointed to files on demand. Lightweight threads are used
to provide intra-object concurrency and the threads are handled by
built in libraries. Unlike CLOUDS these systems do not provide the
orthogonality of computations. Vy'e feel this orthogonality is not only
natural, but contributes to the elegance of the paradigm used by
CLOUDS.
The Commandos operating system tMG89l provides support for all
types of objects (fine and large grained, persistent and volatile) in a
uniform fashion. The operating system provides management of object

270

P. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohindra, and R. Chen

types, location, and sharing. Among other things, since object typing
is directly supported by the operating system, Commandos is closely
tied to the programming environment and thus is a special-purpose,
single-paradigm system.
In Mach [A+86], multiple threads share a task, which is the unit of
sharing and protection. All resources of a thread are accessible to
other threads associated with the task. At a programmer's level, concurrency is explicitly programmed by creating threads using a library
package. Sharing is also possible through memory objects, which have
to be explicitly mapped in into a task's address space. In CLOUDS,
this is automatically done on invoking the desired object.
The memory in most of the above systems is private to the process
or application using the objects. The objects exist in the global address
space of the process executing the application and are copied in or out
of the space, as and when necessary. When the process terminates, the
memory is lost and the persistent objects have to be saved on secondary storage. Thus the objects, when shared, exist in the memory space
of multiple processes. Our approach is the opposite. The object does
not appear in multiple address spaces. The threads visit the objects address space. We feel that this approach is cleaner, easier to program,
comprehend and also easier to implement.

10. Conclusions
The support for programming distributed objects in a variety of programming languages and environments is one of the strong points of
the CLOUDS distributed operating system. The system provides persistent objects that can be used for programming applications. Since the
objects are persistent, there is no need for explicitly saving state. In
fact, the operating system does not provide for file systems or disk I/O
routines available from the user environments. In addition, CLOUDS
distribution mechanisms allow the programmer to implement applications using implicit distribution techniques. Coupled with the orthogonality of compute and data servers, the system design is elegant, easy
to use and intuitive. This design enhances its usability and represents
the novel aspect of the CLOUDS system environment.

Distributed Programming wíth Objects and Threads in the Clouds System

27I

The performance of the system is more than adequate. The compute performance is dependent on the machines used to run the applications; the only bottleneck being the paging of the objects from the
data servers. This latency can be improved with high-speed networks
or by placing the data servers on the same machines as the compute
servers. However, keeping the data servers physically separate has
some distinct advantages: orthogonality, uniform access costs and
symmetry. Thus it is tradeoff between structure and cost.
Merging some of the data servers with some of the compute servers does not necessarily improve global system performance predictably. To prevent network traffic the objects must be executed at the
machine they are located (using RPC). This pinning would not only
cause increased RPC traffic but would cause higher loads at the compute servers that have high traffic objects. Instead, a solution involving
a high-speed network appears more favorable. Thus, in most cases (except if the host is a high-power multiprocessor) the data servers should
be kept separate and linked via a high-speed network.

II

. Acknowledgments

We thank Mark Pearson for working on the earlier version of this paper. \ffe also thank Chris V/ilkenloh, Gautam Shah, Vibby Gottemukkala and M. Chelliah for implementing some features of the qystem.

272

P. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohindra, and R. Chen

References

[A*86]

M. Accetta et al. Mach: A New Kernel Foundation for Unix
Development. In Proc. Summer Usenix, July, 1986.

[ABLN85] G. Almes, A. Black, E. Laswoska, and J. Noe. The Eden

Sys-

tem: A Technical Review. IEEE Trans. on Software Engg., SE11, January 1985.

IAMMR9OI R. Ananthanarayanan, Sathis Menon, Ajay Mohindra,

and

Umakishore Ramachandran. Integrating Distributed Shared
Memory with Virtual Memory Management. Technical Report
GIT-CC-90/40, Georgia Institute of Technology, 1990.

[Ana]

R. Ananthan¿uayanan. An Implementation Architecture for
Synchronization in a Distributed System. Tþchnical Report (in
progress).

[Ana91]

R. Ananthanarayanan. CC++ Reference Manual. Tþchnical
Report GIT-CC-9l/07, Georgia Institute of Technology, College of Computing, Distributed Systems Laboratory, 1991.

tBKT90l

H. E. Bal, M. F. Kaashoek, and A. S. Tinnenbaum. Experience with distributed programming in Orca. In In Intl. Conf .
on Computer Languages, 1990.

[8N83]

A. D. Binell and B. J. Nelson. Implementing Remote Procedure Calls. ACM Trans. on Computer Systems, October 1983.

[BST89]

H. E. Bal, J. G. Steiner, and A. S. Tänenbaum. Programming
Languages for Distributed Computing Systems. ACM Comuting
Surveys, September 1989.

[CALL89]

J. S. Chase, F. G. Amador, E. D. Lazowska, and H. M.
Levy. The Amber System: Parallel Programming on a Network
of Multiprocessors. Operating Systems Review, 23(5), 1989.

tCD89l

Raymond C. Chen and Partha Dasgupta. Linking Consistency
with Object/Thread Semantics: An Approach to Robust Com-

putation. In Proceedings of the 9th Internntional Conference on
Distributed Computing Systems, June 1989.

[Che88]

D. R. Cheriton. The V Distributed System. Communications of
the ACM,3I(3):314-33, March 1988.

ICJRSTI

R. Campbell, G. Johnston, and V. Russo. Choices (Class Hierarchical Open Interface for Custom Embedded Systems). Operating Systems Review, July 1987.

Distributed Programming with Objects and Threads in the Clouds System 273

tDCgOl

Partha Dasgupta and Raymond C. Chen. Memory Semantics in
Large Grained Persistent Objects. In Proceedings of the 4th International Workshop on Persistent Object Systems (POS). Morgan-Kaufmann, September 1990.

[DCM+90]

P. Dasgupta, R. C. Chen, S. Menon, M. P. Pearson, R. Ananthanarayanan, U. Ramachandran, M. Ahamad, R. J. LeBlanc,
W. F. Appelbe, J. M. Bernabéu-Aubán, P. W. Hutto,
M. Y. A. Khalidi, and C. J. Wilkenloh. The Design and Implementation of the Clnuds Distributed Operating System.
Usenix Computing Systems, 3(1), 1990.

[DJAR9l]

P. Dasgupta, Richard J. LeBlanc Jr., Mustaque Ahamad, and
Umakishore Ramachandran. The CLOUDS Distributed Operating System. IEEE Computer, April1991. To appear.

tGLgOl

L.

Gunaseelan and R. J. LeBlanc. Distributed Eiffel: A language for programming multi-granular, distributed objects.
Georgia Tþch Distributed Systems Laboratory, Submitted for

publication, October 1990.

[JLHB88]

Eric Jul, Henry Levy, Norman Hutchinson, and Andrew Black.
Fine-grained mobility in the Emerald system. ACM Tiansactions on Computer Systems,6(1):109-133, Feb 1988.

tLH86l

Kai Li and Paul Hudak. Memory Coherence in Shared Virtual
Memory Systems. In Proc. 5th ACM Symp. Principles of Distributed Computing, pages 229-239. ACM, August 1986.

[Lis84]

B. Liskov. Overview of the Argus Language and System.
Tþchnical Report Programming Methodology Group Memo 40,
M.I.T., Laboratory for computer Science, February 1984.

tMG89l

J. A. Marques and P. Guedes. Extending the Operating System
Support for an Object Oriented Environment.ln In Proc.
OOPSLA-99 Conference, October 1989.

tPDgOl

M. Pearson and P. Dasgupta. CLIDE: A Distributed, Symbolic
Programming System based on Large-Grained Persistent Objects. Technical Report GIT-CC-90/62, Georgia Institute of
Technology, College of Computing, Atlanta, GA., November
1990.

tRAKSgl

Umakishore Ramachandran, Mustaque Ahamad, and M.
Yousef A. Khalidi. Coherence of Distributed Shared Memory:
Unifying Synchronization and Data Tiansfer.ln Eighteenth Annual International Conference on Parallel Processing, August
1989.

ls+891

M. Shapiro et al.

SOS:

An Object-oriented Operating

tem-Assessment and Perspectiv es. Computing

Sys-

Sy stems,

2(4),

1989.

lsTBs6l

lStrS6l

twilsgl

R. E. Schantz, R. H. Thomas, and G. Bono. The architecture
of the Cronus distributed operating system. In Proc. of the 6th
Int'1. Conf . on Distr. Cornputing Sys., May 1986.
Bjarne Stroustrup. The C# Prograrnrning Langunge.
Addison-Wesley Publishing Company, Reading, MA, 1986.
Christopher J. Wilkenloh. Design of a Reliable Message Ti.ansaction Protocol. Master's thesis, Georgia Institute of Technology, College of Computing, 1989.

Permission to copy without fee all or part ofthis maûerial is granted provided that the copies
are not made or distributed for direct comrnerical advantage, frre Computing Systems copyright
notice and its daæ appear, and notice is given that copying is by permission ofthe Regents of
the University of California. To copy otherwise, or ûo republish, requires a fee and/or specific
permission. See inside front cover for details.

Distributed Progrønming with Objeas andThreads in the Clouds

System 275

A Distributed Lisp Programming System:
Implementation and Usage *
Mark P. Pearson

Partha Dasgupta

Department of Computer Science
and Engineering
Arizona State University
Tempe, AZ 85287-5406
partha @enuxva.eas.asu. edu

Distributed Systems Laboratory
College of Computing
Georgia Institute of Technology
Atlanta, GA 30332-0280
mpearson @cc.gatech.edu

Abstract

on top of the CLOUDSdistributed operating system provides payoffs that are not easily achieved in conventional
computing system environments.

Lisp environments are used for symbolic computations by
a wide variety of large applications. While traditional Lisp
environments do not support persistence, distribution, or
concurrency; several research systems have been developed
that support one or more of these concepts. However, Lisp
environments based on heavy-weight processes tied to perprocess memory spaces do not handle all three of these
concepts well. Instead, one or more of these concepts is
sacrificed to achieve the others.
In this paper, we present the implementation details of
a Lisp environment built on top of a distributed operating system. Our system provides transparent distribution,
protected large-grain persistent heaps, concurrency within
each environment and seamless sharing of Lisp data structures between separate environments.

1

.................................................................................
HEAP

S

Evaluation

Introduction
1

................................................................................

Figure 1: Traditional Lisp Environment

1.1

A Lisp Primer

Unlike conventional programming languages, which have
distinct specification, compilation, and execution phases,
Lisp systems combine all three phases to produce a
programmable programming system. This behavior is
achieved through interpretive evaluation of Lisp object
structures in a common heap. In addition, Lisp does not
differentiate between code and data. Thus, a conventional
Lisp environment contains data structures and functionality common to editors, compilers and runtime programs
(See Figure 1).
An atom is a named symbol which refers to a Lisp data
structure. These data types may be integers, reds, strings,
or Lisp list structures which are made up of these basic
types and/or other Lisp lists. For example, the Lisp list
(fn a (b)) contains the atom fn, followed by the atom a,
and the list (b). List (b) contains one atom, b.
An s-expression may be a single atom, or a Lisp list.
Users of Lisp systems enter s-expressions which are read
and then parsed by an s-ezpression reader. During the
parsing phase, the reader resolves the symbol names in
the s-expression into actual pointers to Lisp data structures through the use of a symbol table. Symbols which do
not yet exist are created during this parsing phase. The

*This work funded by NSF grant CCR-86-19886.

m $3.00 0 1992 IBEB

* r>7;a
cashew

Large application systems often require the use of symbolic
computation for decision support, planning, knowledge
base access, or general rule-based reasoning. Symbolic
computations are often implemented in the Lisp language,
which requires a runtime environment that places large demands on the system upon which it is implemented. Lisp
systems and the programs that run on them typically use
large amounts of memory and compute power. Large distributed applications require features such as persistence,
concurrency, inter-process sharing, synchronization, and
large-grain distribution of computation. Most Lisp-based
systems, however, are unable to supply most of these features because they tend to be implemented in a closed environment on an individual Lisp-based workstation, or as
a segregated and fixed set of processes on stock hardware.
The same features that make Lisp systems so productive and attractive to application programmers make Lisp
systems hard to distribute and Lisp data structures difficult to share. As a result, truly concurrent and distributed executions in Lisp systems have almost never been
tried. Several distributed/persistent/object-oriented Lisp
systems exist, but they suffer from a myriad of problems,
often traceable to a lack of proper system-level support.
In the following section, we discuss the unique needs and
restrictions that Lisp places on systems, and define some
terminology used by Lisp implementations. Then, we will
show how elegant structuring of a distributed Lisp system

0-818&28654

:

690

our system and specify guidelines for large distributed artificial intelligence application development on top of our
system.

result of the parsing phase is a Lisp list structure which
is passed to an evaluator for evaluation. If a single atom
is evaluated, its binding is returned. If a Lisp list is evaluated, then it is assumed that the first atom of the list
represents a Lisp function and the remaining items of the
list are parameters to that function. Users may prevent
evaluation with the quote operator. An example of the
in-memory representation that results from parsing (fn a
(b) ) is shown in Figure 1, referenced from the evaluation
stack.
All Lisp memory structures are created on a common
heap. In Lisp systems, many Lisp data structures are created automatically. During the course of execution, many
Lisp structures are dereferenced and become in-memory
garbage. Thus, Lisp systems utilize a separate garbage collector, which removes these dereferenced structures from
memory.

1.2

CLOUDSLISP DISTRIBUTEDENVIRONMENTS
(CLIDE) is
a distributed, persistent object-based symbolic programming system that implements transparent distribution,
controlled sharing of environments and environmental persistence with assistance from the CLOUDSoperating system [15]. In CLIDE,each instance of a Lisp environment
is stored as a large-grained persistent (LGP) object. An
LGP object is a complete virtual address space which is
mapped to disk storage, making it persistent. In addition,
an LGP object allows concurrent execution threads within
it, providing elegant support for concurrent executions in
a shared address space.
Invocations between LGP objects enable users on many
machines to share the contents of these environments
through inter-environment evaluations. The support for
persistence, distribution, and basic invocation of LGP objects is provided by the CLOUDSoperating system.
Two high level features of CLIDE,which make it unique
with respect to other distributed symbolic processing systems, are support for persistent Lisp environment spaces,
and mechanisms for controlled sharing of code and data
between these environment spaces.
Seamless (yet protected) sharing of environments allows
straightforward construction of large distributed applications, while persistence enables time devoted to the saving,
loading, and initialization of these symbolic processing environment spaces, and fine-grained objects within them,
to be eliminated.

The Problem

A traditional symbolic programming environment consists
of a “read and evaluate” process which accesses and modifies a memory heap in a single user address space. In the
reading phase, Lisp s-expressions are entered by the user,
parsed, and then passed to an evaluator which evaluates
the resulting Lisp list object, recursively (Figure 1).
These operations occur in an address space associated
with a specific process. The environment is single threaded
and segregated from other environments running on the
same system. These traditional systems have addressed
sharing of environmental data by having a single global
heap shared among multiple users, one at a time, or by
using a shared repository, called a blackboard, accessible
by two or more user processes [9] [13].
Conventional Lisp workstations utilize a shared global
heap for all users. Individual users may utilize their own
set of name bindings with the Common Lisp packagemechanism. Each package of name bindings is associated with
a unique symbol table which is used by the s-expression
reader to resolve the locations of Lisp objects at run time.
Evaluations are single threaded and run one at a time in
the global heap. In blackboard systems, several evaluations may be executing in parallel, but the per-process
heaps are separate. The memory structures in the blackboard may be shared by concurrent evaluators; but the
blackboard is treated as a critical section, causing concurrent evaluations involving the blackboard to proceed
lock-step, in a serial order.
Symbolic evaluations in distributed environments operate similarly to blackboard systems. An evaluation thread
is associated with a single process that may run concurrently with other evaluation processes, but access to shared
objects is mutually exclusive. Evaluations may not run
multi-threaded through the same per-process heap.

1.3

Symbolic Processing Environments

2

2.1

Persistence and Controlled Sharing

Several distributed symbolic processing systems provide
persistent, fine-grained objects, notably PCLOS’ and
Avalon/Common Lisp. Persistent CLOS (PCLOS) [14],
an extension of CLOS [i’], supports database independent
persistence of fine-grained CLOS objects beyond the session that created them. Avalon/Common Lisp [4] supports
fine-grained recoverable atomic objects with CAMELOT
[16] recoverable stores. Distributed Smalltalk [l]provides
a framework for cooperation among geographically separate Smalltalk users by allowing direct access to remote,
small-grained Smalltalk objects and, to some extent, environment sharing.
Providing persistence of fine-grain entities such as lists
and objects is accepted as a worthwhile goal. It makes
reuse of state between successive executions of a program
somewhat straightforward and reduces programmer overhead by automating storage and loading of the persistent
state. In addition, some form of sharing can be provided
when one application generates state and saves it into data
structures which are subsequently retrieved by other applications.
However, providing persistence of fine-grain entities is
quite complicated. Loading and storing of a large number
of individual small persistent elements, for each application, bears a large overhead on the underlying system. In

Paper Overview

This paper discusses the implementation and usage of
CLIDE,a symbolic processing environment that uses a suite
of straightforward mechanisms to provide concurrent, controlled sharing of environments in a distributed system.
First, we discuss the various limitations of other systems with similar functionality and contrast them with
our approach. Then, we present implementation details of

‘Persistent Common Lisp Object System

691

.- .

anism to prevent users from inappropriate object class
sharing, users may adversely effect other users. In addition, references to objects across user spaces is permitted,
which allows the creation of cyclic structures and cyclic
garbage across user spaces. Thus, a global garbage detection process is required to collect cyclic garbage across user
spaces.
In CLJDE,sharing between environments is controlled
through an object-like interface. Interaction with a particular CLIDEenvironment may occur only through the public
functions of this interface. Since each CLIDEenvironment
is Lispbased, its interface may change or evolve. It is intended that each CLIDE environment have a manageable
number of public entry points, although many public entry
points may be defined if a user so desires. Fine-grained objects in a particular environment are assumed to be private
unless explicitly declared public. Additionally, objects in
one CLJDEenvironment may not directly refer to objects in
another CLIDE environment. This eliminates the need for
a global garbage collector which operates across multiple
CLIDE environments.

addition, there is overhead in converting the data from an
in-core representation to the storage representation and
vice-versa. An object (or data structure) may have pointers t o other objects. Resolving these pointers while loading, storing, and using these objects is quite difficult since
an object may reference both persistent and non-persistent
objects. The naming of the very large number of objects
generated by many large applications is an arduous task,
especially if the objects are to be shared by many applications. Concurrent sharing of objects between applications
is often not possible. Most schemes force an application to
stop using the object and save it to the store before another
application can use it. In systems which allow concurrent
sharing, application address spaces are not protected from
one another. Schema evolution2 is a problem if the shared
objects are abstract data types with public and private
data and methods. Finally, symbolic processing environments pose the problem of distributed garbage collection.
Examples of one or more of these problems can be seen
in existing distributed symbolic processing systems. PCLOS, Avalon/Common Lisp, and Distributed Smalltalk
provide some means for fine-grained object sharing. In PCLOS, objects are stored in a virtual database managed by
a fixed-set of processes running on one or more database
servers. Sharing is accomplished in much the same way
records in a common database would be shared between
multiple users of a database system. PCLOS does not support remote evaluation. In Avalon/Common Lisp, a persistent fine-grained object instance is stored in a CAMELOT
recoverable store. In this system, there is a one-to-one
correspondence between evaluators and recoverable stores,
and remote evaluations are allowed between evaluators.
If a fine-grained object’s type can be created through an
evaluator’s s-expression reader, then that object is able
to migrate between evaluators. These readable types are
simple data types such as arrays, lists, and Lisp structs.
In order to migrate an object of this type, its print (or
ASCII) representation must be created, transmitted, and
then evaluated by the receiving evaluator. More complex
types, such as object classes, require marshaling and unmarshaling functions. The sending evaluator’s marshaling
function takes an object class and converts it into a textual
representation that is precisely understood by a matching
unmarshaling function at the receiving evaluator.
In CLIDE, fine-grained objects are stored, used, and
shared in their native format. Each fine-grain object is
part of a larger environment which is stored in a persistent
address space. There is no conversion between in-core and
storage formats. In addition, there is no need for moving
objects from special storage repositories to memory and resolving pointers within an environment as all the contents
of the environment memory containing the objects are demand paged from disk. The fine-grained objects within an
environment can be shared by multiple evaluations with
separate evaluation contexts and stacks. Since each environment (consisting of a set of fine-grained objects) is in
a separate address space, the environments are protected
from each other. In addition, inter-environmental evaluation allows usage of objects in one environment by another
environment.
Object mobility in Distributed Smalltalk is limited,
from a practical standpoint, due to the complexity of the
Smalltalk class hierarchy. However, since there is no mech-

2.2

Concurrency and Consistency

In addition to persistence and controlled sharing, other
important aspects of CLIDEinclude support for concurrent execution within a single environment space, transparent ezecution across multiple environment spaces, and
environment space consistency and security.
Intra-environment concurrency is necessary to support
data and code sharing among environment spaces. Further, intra-environment concurrency allows previous work
on parallel evaluation of distinct s-expressions [8] to be extended to multiple-site distributed systems. Transparent
execution across environment spaces allows users to construct programs which act in a distributed fashion without strict knowledge of the location of the environments
involved. Security and consistency are qualities necessary
for a system which must maintain the consistency and integrity of shared environments for a community of users.
Since Avalon/Common Lisp is modeled similarly to
Common Lisp, only one thread of control at a time is allowed in any given evaluator. In addition, no concurrency
is allowed within a transaction. PCLOS does not allow
remote evaluations, nor does it allow concurrent execution
within a user space. Although Distributed Smalltalk allows access to remote objects, thread executions may not
traverse address spaces.
CLIDE supports concurrent execution within a single
environment space and any number of evaluation threads
may be active in a particular environment at any given
time. Each active evaluation thread has a separate evaluation context. A CLLDEenvironment has a multi-threaded
evaluator which controls access to the fine-grained objects
defined within it.
Avalon/Common Lisp operates in the standard
client/server paradigm, and uses remote procedure calls
for evaluations that involve more than one evaluator.
Thus, the system provides hidden distribution from the
point of view of the client. Client calls to particular services may be made without knowledge of how that particular service is distributed. However, implementation of
distributed server code requires structures describing the
location of all the involved evaluators to be set up properly
by the programmer. Thus, developers of services must be

2changingobject-interfaces

692

object. Inconsistencies are prevented by keeping all data
within copies of the object coherent by using a Distributed
Shared Memory (DSM)system. The DSM system enforces
one-copy semantics for all object pages with different coherence protocols [ll] [12].

aware of the strict location of the evaluators. In addition,
evaluators, and their respective data stores, may not migrate to different nodes. In CLIDE,evaluations may be sent
to remote CLIDEenvironments without strict knowledge of
that environment’s location. In addition, these evaluations
may be sent synchronously or asynchronously.
PCLOS provides standard database consistency for
the fine-grained objects in its virtual database. Distributed Smalltalk provides no guarantee of an object’s consistency nor does it support persistent objects.
Avalon/Common Lisp supports recoverable atomic objects
through CAMELOT recoverable stores. CLIDEenvironment consistency is guaranteed at the environment level
through CLOUDSconsistency preserving threads facility [a]
[3]. Security in CLIDEis enforced at the environment level
through a user-defined object-like interface. Remote evaluations to a particular CLIDEenvironment are allowed only
through the defined public functions of that environment’s
interface.

3.1

Remote Evaluation Support

In specifying the design of an efficient remote evaluation
mechanism for CLIDE,we sought to provide a means for
fine-grained object sharing between CLOUDSLGP objects
that did not incur the overhead associated with CLOUDS
invocation parameter passing. If the basic CLOUDSinvocation mechanism is used, the sharing of anything but the
most rudimentary Lisp objects between environments potentially involves the copying and passing of large amounts
of information as parameter data. Further, the CLOUDS
parameter passing area is limited (8KB). The nature of
Lisp raises the possibility that large portions of these
copied Lisp data structures would never be referenced by
the remote environment. Therefore, we added a useraccessible memory mapping facility to the CLOUDSsystem
which allows virtual address ranges of one CLOUDSLGP
object instance to be mapped into an unused virtual address space of another LGP object instance.
Mapping of transient memory areas into an object instance’s persistent object memory space presents some logistical problems since any changes to an instance’s persistent object space must be reflected throughout the distributed system. Instead of allowing a memory range from
one LGP object’s persistent object space to be directly
mapped into another instance’s persistent object space,
we instead elected to map the address range from the calling object’s persistent object space into the called object’s
non-persistent, per-thread memory space, which was previously used for per-process data entities like the process
stack and parameter passing area. Using this strategy, we
are able to avoid the overhead associated with a system
wide update of the representation of an instance’s persistent object space. In addition, we enjoy the safety of
parameter passing without the overhead incurred from the
wholesale copying of fine-grained objects into a parameter
passing area, since the mapped areas in per-thread memory are visible only to the active thread associated with
it. The mechanics of this strategy become more apparent
when we discuss remote evaluation in Section 4.3.

3 System Support for CLIDE
CLIDEenvironments, as described above, need support for
persistence, protected address spaces, concurrent threads,
locking and consistency at the operating system level. This
functionality is provided, to some extent, by the CLOUDS
distributed operating system.
CLOUDSis a general purpose distributed operating system that supports persistent objects, light-weight threads,
inter-object invocations and transparent distribution of applications among a group of compute servers and data
servers. With these primitives, CLOUDSintegrates a number of loosely-coupled machines to produce a system that
behaves like a centralized, time-sharing system [6].
The CLOUDSlarge-grainedpersistent (LGP) object combines the notions of persistent storage and distinct virtual
memory address spaces to create a persistent, distinct, virtual memory address space. These CLOUDSobjects are
passive entities that neither contain a process nor have
their life span tied to any process activity. Active threads
are the “means of execution” in the CLOUDSsystem and
may invoke CLOUDS
objects at user defined entry points. A
thread begins execution when it invokes an entry point in
an object and terminates when it exits the invoked routine.
During the execution of this routine the thread may make
further invocations on routines defined in other objects.
In addition, several threads may execute inside a CLOUDS
LGP object simultaneously. Therefore, the CLOUDS
object
programmer must program publicly accessible routines in
a CLOUDSobject to be re-entrant. This type of programming is assisted by mechanisms such as semaphores and
locks. In addition to location transparent object invocation, asynchronous invocation and claiming mechanisms
are supported [SI.
Physically distributed CLOUDSLGP objects appear logically centralized because the CLOUDSsystem ensures that
any thread can invoke any object on any compute server,
regardless of where the thread originated or where the object is stored. This transparent distribution of objects requires coherent replication. That is, if the same object is
invoked by two threads on two different compute servers,
two copies of this object will exist, one at each of the compute servers. This can cause data inconsistencies in the

4

CLIDEImplementation

A typical CLIDEsystem is composed of a group of cooperating CLOUDSpersistent objects of the CLIDEclass. Each
of these persistent objects emulates a single-user, symbolic
processing environment contained in a distinct, persistent,
virtual memory address space. We call an individual symbolic processing environment a CLIDE environment. The
functionality of a CLIDEenvironment is made available to
users, other CLIDEenvironments, and CLOUDS
objects r e p
resenting classes programmed in CC++ (CLOUDSC++)
or DISTRIBUTED EIFFEL[5] through its object hterface.
The Lisp dialect of CLIDEis based on Kernel [lo], a
small, C-based Lisp interpreter and has been extended to
support a subset of Common Lisp [17] and remote evaluations in the CLOUDSdistributed computing environment.
Details on the motivations for and the design of CLIDE,the

693

evaluation context is implemented as a fine-grained CC++
class that contains four push-down stacks, for evaluations,
arguments, variables and cells; and the methods to manipulate these stacks. When an evaluation thread is created
in an environment, an instance of the EC class is created
in that environment’s heap. The EC instance is created in
persistent object memory because the contents of all active
EC instances must be available to the garbage collection
thread when garbage collection takes place. When an expression is to be evaluated, a reference to that expression
is first pushed onto the evaluation stack of a specific EC,
by calling that EC’s eval-stack-push method. The EC is
then passed to the multi-threaded evaluator. The argument stack is used to build argument lists for Lisp function calls. When a Lisp function F is called, its arguments
are first pushed onto the argument stack; then a CC++
function G , which implements F, is called. G uses the
argument stack to retrieve its arguments. Once the call
returns, the arguments are popped off the EC’s argument
stack via a call to the EC’s arg-stack-pop method.
Since Lisp is an interpreted language, dynamic scoping is used to resolve the bindings of each named variable.
The variable stack is used to preserve the state of a global
variable before it is used as the local variable of a Lisp
form. Examples of these local variables include parameters of a defined Lisp function, and the local variables of
a Lisp prog function. Prior to the evaluation of a function with parameters, the global bindings of the variable
names on the parameter list are pushed onto the variable
stack. These variables are then free to be bound to their
local values provided by the caller of the function, which
are stored on the argument stack. Once the function evaluation is complete, the original bindings of these variables
are restored from the variable stack. The placement of
these stacks into a private, per-evaluation area allows multiple evaluations to make progress at the same time, while
sharing the same copy of the evaluation code.

object interface for CLOUDSobjects of the CLIDEclass, and
how this distributed Lisp system fits within the CLOUDS
framework can be found in [15]. In the following sections,
we discuss the design and implementation of some of the
advanced CLIDE mechanisms which implement the properties of the CLIDEenvironment presented in Section 2.
:LIDE Environment

-publio login()
-

privale remote() (
<<set up remote emlustion>>
target.removsevaluate(...);
<<prowee imparl->

%~:sevslus~)

1;
~

,

Symbol
Table

0

.............................................................................

i

7 lochble cells

Figure 2: CLIDEEnvironment and Lockable Cell:
4.1

The Multi-Threaded Evaluator

Each CLIDE environment has a multi-threaded evaluator,
and two s-expression readers; one for direct user interface,
and another to service remote evaluation requests.
The CLLDE evaluator is a CC++ function inside a
CLOUDSobject. The evaluator, as written, does not schedule or spawn multiple threads. The evaluator is “multithreaded” in the sense that several threads can potentially
invoke and run the evaluator function in the context of
the same environment, at the same time, much like reentrant system calls in a conventional operating system. This
multi-threaded situation occurs when several evaluation
requests for environment A are made by several independent environments at the same time.
Threads executing in the same object in CLOUDSare
truly concurrent, kernel scheduled threads, executing in
the same address space. They have separate stacks and
per-thread data areas but share object code and object
data. Thus, concurrent evaluations in a particular CLIDE
environment will have shared access to the same heap and
Lisp symbol table information, similar to shared memory
programs. Thus, there is a need for per-evaluation private
data areas, and mutual exclusion on shared data areas, in
order to ensure that an evaluation does not adversely affect
the other evaluations active within the confines of a single
CLIDEenvironment.

4.2

4.3

Remote Evaluation

A remote evaluation is the evaluation of an s-expression
or symbol generated by one environment in the context of
another environment. Remote evaluations between CLIDE
environments is effected through the extended CLOUDSobject invocation mechanism discussed in Section 3.1. The
originating environment of a remote evaluation first makes
an object invocation to the remote-evaluation entry point
of another CLIDEenvironment. The receiving environment
validates permissions of the remote call, parses the expression to be evaluated with an s-expression reader, evaluates
the parsed s-expression in the context of its environment
and the remote calling environment, and then returns the
results. Remote evaluations in CLIDEare of the form:
(remote <environment-name> (<import-list>) (<exportlist>) “<expression>” )

As an example, let’s consider the following remote evaluation, which originates in envl and is made to env2.
(remote env2 (key) (key) “(db-access key)” )
The semantics of this statement read: Evaluate the expression (db-access key) in the context of env2. Export
the Lisp structures referenced by the symbol key from
envl to env2 for this evaluation. When the evaluation

Evaluation Contexts

Each evaluation thread that visits an individual CLIDEenvironment utilizes a separate evaluation context (EC). This

694

m c u l bind@

Figure 3:

During the parsing operation, names are first resolved
through the exported symbol table from envl and then
through the main symbol table for env2. Thus, if there
is another definition for key in env2, it is not seen by the
parsing operation (See Figure 3). Individual Lisp symbols
and cells, the building blocks of Lisp structures, that are
outside the boundary of the heap of env2 are copied into
the env2 heap via a memory to memory copy operation as
they are referenced during parsing and evaluation operations (See Section 4.4). Thus, the evaluator never directly
reads or writes the contents of the heap of the culling environment.
The parsed s-expression is pushed onto the evaluation
stack of the EC. Then, the EC is passed to the evaluator.
Any new symbol references created during the evaluation
are entered in the symbol table exported from envl.
At the termination of the evaluation, a reference to the
results of the evaluation is stored in the parameter passing
area. Any symbols in the envl exported symbol table,
which are not on the export-list are garbage collected. The
heap of the calling environment is unmapped. The remoteinvocation then returns to the calling environment.
Now, we are now back in the context of the calling environment, envi. If any symbols are on the import-list,
the heap of the called environment is mapped into an unused virtual address area in the per-thread memory of the
thread now executing in the calling CLIDEenvironment.
In our example, the heap of env2 is mapped into a perthread virtual address space of the thread executing in
envl. The structures referenced by the symbols on its
import-list (key) are then deep copied into the heap of the
calling environment. The heap of the called environment
is then unmapped.

9‘lay

CLIDE
Remote Evaluation

completes, copy back to envl the new Lisp structures referenced by key created during the remote evaluation in
env2 (See Figure 3).
The remote sexpression is first parsed by the reader
in envl which creates a Lisplist structure representing it
on the heap of envl. A pointer to this structure is then
pushed onto the evaluation stack of the evaluation’s local
evaluation contest. Then, the evaluation context is passed
to the multi-threaded evaluator. A private CC++ function
which implements remote is then called as a result of the
evaluation.
First, this remote function creates a parameter passing
area. Next, a symbol table containing references to all the
items on the export-list is created. In our example, a symbol table is created which contains a symbol table reference
for the Lisp symbol key. This symbol table is then stored
in the parameter passing area. Then, a reference to the
expression to be evaluated, in the form of a Lisp string,
and the location and extent of the heap area of envl are
placed in the parameter passing area. Finally, a CLOUDS
remote invocation is made to the remote-evaluation entry
point of the receiving CLIDEenvironment, env2.
A remote invocation request is then received by the
CLJDEenvironment, env2. At this point, the evaluation is
operating in the context of env2. The code at the remoteevaluation entry point then maps the heap area of CLIDE
environment envl into an unused virtual address range in
the per-thread memory associated with the thread now executing in CLIDEenvironment env2. An evaluation context
for this remote evaluation is instantiated. The Lisp string,
representing the expression to be evaluated, is copied into
the heap of env2 from the mapped-in heap of envl via
a memory to memory copy operation. The symbol table
containing references to the symbols exported from envl
is pushed onto the symbol table stack of the EC.
-The remote-evaluation reader then parses the string expression. First, permissions are checked to see that the
called function is a public function. In our example, the
access-listof db-accessis checked to see if it allows public
access. If it is a private function, the evaluation process
terminates, and a permissions failure is returned to the
calling environment.

4.4

Lazy copying

In order to avoid the poor performance characteristics exhibited by deep copying of complex Lisp data structures,
we implemented a lazy copying strategy. For protection
purposes, the heap of the calling environment is mapped
into the called environment in a read-only manner. The
called environment now has access to its own heap and the
heap of the calling environment which gives it access to the
Lisp structures it needs from the calling environment.
In the case of our example, a copy of the symbol structure of key is made in the new environment, but the pointers from this symbol structure are marked “invalid”. If
the evaluator attempts to dereference any invalid pointer
out of a Lisp symbol or cell, the corresponding invalid references out of this structure are resolved in the mapped,
calling heap; copied from this read-only heap into the heap
of the called environment; and then these references are
marked “valid.” These newly copied structures, however,
may still point to structures in the read-only mapped area.
These pointer references are marked “invalid.” When these
“invalid” structures are referenced, the discipline is repeated. Using this lazy copying strategy, only the referenced portions of shared Lisp structures are copied between environments. Locking rules must be followed while
the heaps are mapped.

4.5

Providing Mutual Exclusion

There are two approaches to handling mutual exclusion:
programmed and automatic. In the programmed a p

695

ligible, since it is highly unlikely that these public function would allow code and data structure modification via
remote evaluation. Therefore, the chances of deadlock are
minimized. Deadlocks which do occur indicate inappropriate data updates. Once deadlock is detected, the system
could either elect to resolve the deadlock or rollback the
state of the effected CLIDEenvironments to a pre-deadlock
state and return appropriate run-time error notification to
the user program.

proach, all lists generated by the programmer (or the program) must encode locking strategies. This locking strategy should be such that concurrent evaluations of these
lists will not cause race conditions. Including this in all
CLIDElists would completely change the language and put
the burden of proper execution and sharing on the Lisp
programmer.
Since programmed locking is useful in many situations,
especially for the knowledgeable programmer, we plan to
provide functions for user defined synchronization. However for most programmers, we must provide automatic
synchronization. This strategy is described below.
Automatic synchronization can be easily achieved by requiring each evaluation to obtain locks on the entire shared
heap and the symbol tables it requires before proceeding. However, if we enforce such strict locking, evaluations
would not be able to proceed concurrently in an individual CLIDEenvironment. Instead, they would proceed in a
lock-step, serial order.
In a Lisp heap, Lisp list structures are represented as
binary trees, and symbol table entries point to the roots
of these trees. Symbol table references may also point to
internal nodes of these binary trees, since a Lisp list structure may point to other Lisp list structures, as in the case
of a Lisp function defined in terms of one or more Lisp
functions (See Figure 2). Cyclic structures may occur if a
Lisp function is recursive. Since a recursive function contains a call to itself, there is a pointer to the root of the tree
defining the function from one of the nodes in its function
definition. The strategy involving the locking of entries in
a symbol table is not a viable option, because symbol references are resolved to pointers by the s-expression reader
prior to evaluation, and because multiple symbols could
point to the same structure.
Instead, we have the notion of “lockable” cons-cells (See
Figure 2). A cons-cell is lockable if it is directly referred
t o by any symbol table entry. When a cons-cell is referenced, its type definition discloses whether or not a lock is
required to access it. Every evaluation locks each lockable
cons-cell it visits. The locking is read-write and nestable.
Thus, an evaluation places a read-lock on all the lockable
cons-cells it visits, and write-locks on lockable cons-cells
it updates or creates via Lisp statements such as setq or
defun3. The nestable locks are such that if an evaluation
locks a cell, it can lock it again without blocking. This
is necessary for recursive functions which have a cyclical
structure definition. As evaluations unwind, they unlock
the cells. Reference counts on read locks are maintained to
properly determine when a read-lock for a lockable conscell should be released.
On the surface, this scheme might appear to be expensive and deadlock prone. However, most references made
during functional evaluations are read references. Further,
new memory allocations on the heap are done atomically,
so concurrent evaluations build new structures in unconflicting areas of the heap. These areas are not lockable
until actual symbol references point to them. Lisp defun
and setq calls could cause deadlock if multiple evaluations
try to update the same lockable structure. This could be
alleviated by support for programmer defined mutual exclusion primitives. However, the number of nested defun
or setq calls in a public CLIDELisp function is almost neg-

Garbage Collection

4.6

Since a Lisp object in a particular CLIDEenvironment may
not directly reference Lisp objects in another CLIDE environment, garbage collection of Lisp objects takes place
within the scope of individual CLIDEenvironments. The
strategy for garbage collection in CLIDEis to collect when
no evaluations are active within a particular CLIDEenvironment. If garbage collection is necessary to complete
one or more active evaluations in an individual environment, all evaluations must be blocked while collection takes
place. Currently, we utilize a simple mark and sweep collector for garbage collection. However, use of a generational collection scheme would temper the effects of such
a collection. Further, the remote evaluation discipline in
the CLIDE system provides a unique opportunity to collect known garbage at the termination of a remote evaluation. Tempering the effects of garbage collection is one
determinant of how one should structure distributed Lisp
applications built on CLIDE. In the subsequent section, we
discuss CLIDEapplication structuring.

5

Large Application Development

In laying out the design of any large distributed application
(LDA), the developer must be aware of that application’s:
0

distribution

0

concurrency

0

and consistency needs.

These needs should determine the ultimate structure of
the LDA. Additional structure determinants for large distributed CLIDEapplications include:
0

behavior encapsulation

0

data access and sharing behavior

0

and garbage collection behavior.

Structuring LDA’s by behavior precisely matches the
intent of object-oriented programming. In this case, behaviors are relegated to individual CLIDE environments.
This type of structuring would be useful in the building of a cooperative agent system, where many different
agents interact to solve a particular problem. In this scenario, each agent class would obtain, retain, and divulge
knowledge using specific reasoning, storage, and retrieval
methods. Interaction between agents would occur through
inter-agent evaluations to public functions defined for each
agent class.
In order to maximize concurrency, however, it might be
appropriate to partition an agent’s knowledge. For example, consider an agent that maintains a large global map

3setq binds a Lisp object to a symbol, dejun binds a Lisp
function definition to a symbol.

696

tributed Smalltalk. SIGPLAN Notices, 22(12):318330, December 1987.
[2] Ray Chen and Partha Dasgupta. Linking Consistency
with Object/Thread Semantics: An Approach to Robust Computation. In Proceedings of the 9th International Conference on Distributed Computing Systems,
June 1989.
[3] Ray Chen and Partha Dasgupta. Implementing Consistency Control Mechanisms in the CLOUDSDistributed Operating System. In Proceedings of the 11th
International Conference on Distributed Computing
Systems, May 1991.
[4] S. Clamen, L. D. Leibengood, S. Nettles, and
J. Wing.
Reliable Distributed Computing with
Avalon/Common Lisp. In 1990 International Conference on Computer Languages, pages 169-79, 1990.
[5] P. Dasgupta, R. Ananthanarayanan, S. Menon,
A. Mohindra, M. Pearson, R. Chen, and C. Willcenloh. Language and Operating System Support for Distributed Programming in CLOUDS.In Proceedings of
the 2nd Symposium on Experiences with Distributed
and Multiprocessor Systems, 1991.
[6] Partha Dasgupta, Richard LeBlanc, Jr., Mustaque
Ahamad, and Umakishore Ramachandran.
The
Clouds Distributed Operating System. IEEE Computer, 24(11):2-12, November 1991.
[7] Richard Gabriel, Jon L White, and Daniel Bobrow. CLOS: Integrating Object-Oriented and Functional Programming. Communications of the A CM,
34(9):29-38, September 1991.
[8] R. Halstead. Parallel Symbolic Computing. IEEE
Computer, 19(8):3543, October 1986.
[9] B. Hayes-Roth. A Blackboard Architecture for Control. Journal of Artificial Intelligence, 26:251-321,
1985.
[lo] Sharam Hekmatpour. LISP: A Portable Implementation. Prentice Hall International (UK) Ltd., Hemel
Hempstead, United Kingdom, 1989.
[ll] K. Li and P. Hudak. Memory Coherence in Shared
Virtual Memory Systems. ACM Transactions on
Computer Systems, 7(4):321-359, November 1989.
[12] A. Mohindra and U. Ramachandran. A Survey of Distributed Shared Memory in Loosely-coupled Systems.
Technical Report GIT-CC-91/01, Georgia Institute of
Technology, College of Computing, January 1991.
[13] H. Penny Nii. Blackboard Systems, Part I and Part
11. A I Magazine, 34, July and August 1986.
[14] A. Paepcke. PCLOS: A Critical Review. In Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, 1989.
[15] M. Pearson and P. Dasgupta.
CLIDE: A Distributed, Symbolic Programming System based on
Large-Grained Persistent Objects. In Proceedings
of the 11th International Conference on Distributed
Computing Systems, pages 626-33, May 1991.
[16] A. Z. Spector, J. J. Bloch, D. S. Daniels, Richard P.
Draves, D. Duchamp, J. L. Eppinger, S. G. Menees,
and D. S. Thompson. The Camelot Project. Database
Engineering, December 1986.
[17] G. Steele. Common LISP. Digital Press, Burlington,
MA., 1984.

that is utilized by other agents in a cooperative agent system. If the map was partitioned by neighborhood, concurrent access to it would be enhanced because different remote evaluations accessing different neighborhoods would
not conflict and could therefore run concurrently. In addition, partitioning of the map data into separate CLIDEenvironments would alleviate excessive paging and the effects
of garbage collection. No system page would contain data
from differing neighborhoods, so these pages could migrate
to individbal machines and be accessed without the problem of page thrashing between compute nodes. Concurrent
threads operating on the map as a whole would not be penalized for garbage collection localized in one particular
neighborhood and portions of the global map which were
unused could be collected without affecting any user’s performance.
Through CLOUDSinvocation-based consistency control,
which extends consistency to entire CLIDEenvironments,
updates to a partitioned shared map could be guaranteed
at the neighborhood level, without penalizing read access
to unchanging neighborhoods.
Of course, it is quite possible to write bad applications,
especially if these LDA structure determinants are not considered properly. Concurrency could become severely limited if one CLIDEenvironment contained an entire shared
knowledge base available to many users. Garbage collection in this shared environment could affect all the users
sharing it. Although Lisp objects shared during a CLIDE
remote evaluation involve in-memory copying of data in
native format, unnecessary sharing of large amounts of
Lisp objects would slow performance. Sharing of large
amounts of data between remote-evaluations indicates a
possible problem with the LDA’s structure. In addition,
Lisp implementations tend to hide the costs of function
execution. Further, the amount of garbage a particular
function creates while it executes is not always apparent
to the developer. Thus, the user must carefully consider
how to structure and specify Lisp code in order to avoid unnecessary execution and/or garbage collection costs. Many
Lisp systems have a set of sophisticated tools to monitor
performance. It is apparent to us that industrial use of
the CLIDEenvironment would require similar tools to help
detect function execution and garbage collection performance hits. The nature of the CLOUDSdistributed system
would allow the integration of interesting distributed performance monitoring tools with CLIDEenvironments.

6

Acknowledgments

The authors wish to thank the principles of the CLOUDS
development effort. Sathis Menon, Ajay Mohindra, and
Chris Wilkenloh have made Clouds Version I1 a robust
system through their comprehensive efforts. R. Ananthanarayanan has given us the language we use to express ourselves in CLOUDS,CC++. Vibby Gottemukkala,
Ranjit John, and Gautam Shah implemented the memory
mapping scheme and several M e r e n t distributed shared
memory schemes. Finally, thanks should go to Sambasiva
Bhatta and Eleni Stroulia for the development of interesting applications on top of the CLIDEsystem.

References
[I] J. K. Bennett. The Design and Implementation of Dis-

697

Process Migration: A Generalized Approach
1
Using a Virtualizing Operating System
Tom Boyd and Partha Dasgupta
Department of Computer Science and Engineering
Arizona State University
Tempe AZ, U.S.A.
{tboyd, partha}@asu.edu

Abstract
Process migration has been used to perform specialized tasks, such as load sharing and checkpoint/restarting
long running applications. Implementation typically consists of modifications to existing applications and the
creation of specialized support systems, which limit the
applicability of the methodology. Off the shelf applications have not benefited from process migration technologies, mainly due to the lack of an effective generalized
methodology and facility. The benefits of process migration include mobility, checkpointing, relocation, scheduling and on the fly maintenance. This paper shows how
regular, shrink-wrapped applications can be migrated.
The approach to migration is to virtualize the application by injecting functionality into running applications
and operating systems. Using this scheme, we separate
the physical resource bindings of the application and replace it with virtual bindings. This technique is referred
to as virtualization. We have developed a virtualizing
Operating System (vOS), residing on top of Windows
2000 that injects stock applications with the virtualizing
software. It coordinates activities across multiple platforms providing new functionality to the existing applications. The vOS makes it possible to build communities of
systems that cooperate to run applications and share resources non-intrusively while retaining application binary
compatibility.

1. Introduction
Conventional approaches to process migration depend
on specially written migratable applications, which utilize
distinct migration libraries and sometimes require a restricted set of operating system functions. Other approaches require modifications to the operating system

kernel in order to facilitate migration. Such migrations
are targeted to applications that are involved with parallel
computations or long running numeric algorithms.
General purpose off the shelf (binary only) applications cannot benefit from these specialized migration facilities. Traditionally, it has not been considered necessary to endow applications, such as word processors,
spreadsheets, browsers with migration capabilities. However, having the ability to migrate provides immense advantages for all applications, especially in mobile and distributed environments (see section 1.1).
In our research of virtualizing operating systems, we
have established the need to be able to migrate any running application, without access to its source code. This
paper describes the process which we adopted to build
migration facilities, by decoupling the application from
its environment (virtualization). Our approach allows
legacy applications to participate in newer, “seamlessly
distributed” computing environments, transcending process migration.
The virtualization mechanism depends upon API interception methodology. The APIs of an application are
typically serviced by library routines. Library routines
are connected to the application using a layer of indirection. This layer of indirection presents an opportunity to
capture and modulate the API call through the modification of the API call parameters. These modifications include the ability to capture, modify and restore the state
information, inherently available within the API call,
through the implementation of a system which captures
and restores state information.
This research discusses both the implementation and
the core work that enables existing applications to assume
new and novel characteristics and behaviors. Specifically; we examine process migration using the virtualiz-

1

This research is partially supported by grants from NSF (CCR-9988204), DARPA/Rome Labs (F30602-99-1-0517), Intel, and Microsoft, and is part of
the “Computing Communities” project, a joint effort between Arizona State University and New York University.

Proceedings of the 22 nd International Conference on Distributed Computing Systems (ICDCS’02)
1063-6927/02 $17.00 © 2002 IEEE

ing Operating System, developed to capture and manipulate the application API state data.

1.1 Computing Communities
Our research is part of a larger project called “Computing Communities” (or CC) [1]. The goal of the CC project is to enable a group of computers to behave like a
large community of systems. The community grows or
shrinks based on dynamic resource requirements through
the scheduling and movement of processes, applications
and resource allocations between systems—all transparently.
The computers participating in the CC utilize a standard operating system and run stock applications. The
key technique to achieve such a system is the creation of
a virtualizing Operating System or vOS [2,3]. The main
theme in the vOS is “virtualization”, which is the decoupling of the application process from its physical environment. That is, a process runs on a virtual processor with
connections to a virtual screen and a virtual keyboard, using virtual files, virtual network connections, and other
virtual resources. The vOS has the ability to change the
connections of the virtual resources to real resources at
any point in time, without support from the application.
The vOS implements the functionality to virtualize the
resources by controlling the mapping between the physical resources (seen by the operating system) and virtual
handles (seen by the application). The virtualization and
process migration provided by the vOS can provide a
plethora of advantages:
•

The users can move their virtual “home machines” at
will, even for applications that are currently executing.

•

A critical service running on machine M1 can be
moved to machine M2, if M1 has to be cast away.

•

Schedulers can control a set of CPUs in a preemptive
manner.

•

Applications can use resources, transparently aggregated from several machines. For example, processor-intensive applications can use the CPUs in remote machines.

1.2 Useful Innovations
Utilizing the vOS as the basis for further study, we are
examining some of the more important aspects of a Distributed Operating System, including transparent distribution of resources, fault tolerance, application adaptation,
synchronization, global scheduling and event synchronization; all of which are accommodated within the framework of the vOS architecture.
Process migration, one of the fundamental technologies required for moving applications between systems is

described in this paper. Our research has shown that the
processes and methods used to migrate applications can
be structured into three fundamental approaches. One of
these approaches is fully integrated into the vOS and
through our work, we prove that the remaining work is
feasible.
Although the current work is based on the Windows
2000 (Win2K) operating system, it can be applied any
other stock system. Win2K, like other operating systems,
is structured such that the applications and the operating
system contain a clearly delineated point of indirection,
which is easily exploitable to add or interpose a layer of
middleware.
The structure of the remainder of this paper is as follows: Section 2 provides the general description and an
architectural overview of the virtualizing Operating System. Section 3 describes the process migration methodologies developed in this research. Section 4 focuses on
the observed performance of the migration using the vOS,
while section 5 describes related work. Section 6 summarizes this research.

2. Virtualizing Operating System
The virtualizing Operating System (vOS) is the central
mechanism of our research. The key vOS technology is
“virtualization,” which enables the decoupling of the application process from its physical environment.

2.1 Architecture
The vOS is a hierarchically structured and distributed
application-management system that provides key global
coordination and control services for the CC environment. Each vOS is responsible for a group of machines
reachable through the network which becomes its domain
of control.
The vOS unobtrusively integrates its components into
the existing Win2K system. It provides services that intercept and virtualize the existing applications on the
member machines, without altering the application’s code
in any way.
The vOS system consists of three main components:
the virtalizing System Manage (vSM), the virtualizing
EXecutive (vEX) and the virtualizing INterceptor (vIN).
Each of these components is placed at a key control point
and provides the overall functionality of the vOS. The
control points are located either at the machine or application level.

2.2 Components
The virtualizing System Manager (vSM) is the central
control program for the vOS. It is a process, which resides on a single workstation located anywhere within the
network containing the vOS domain. The vSM manages

Proceedings of the 22 nd International Conference on Distributed Computing Systems (ICDCS’02)
1063-6927/02 $17.00 © 2002 IEEE

individual workstations by interacting with a system service (a process) residing on each workstation named the
virtualizing EXecutive (vEX). It displays the vOS system
status and provides a command and control interface to
each of the vEXs.
The vEX is located on each workstation that is a member of the vOS domain. It uses the application virtualizer,
known as the virtualizing INterceptor (vIN), to capture
and manage the individual workstation processes. The
vEX consists of an application with a user interface that
displays local system state and provides for local command and control activities.
The vEX uses the vIN to initiate and capture process
data from the workstation applications. The vIN is a
software module that is injected into a newly launched
application. Once injected, it monitors the host application by intercepting its API calls to the underlying libraries. The injected code resides in the same address space
as the application under its control. A simple command
interface is integrated into the application through the insertion of a menu item into the existing application menu;
otherwise, it is completely unseen by the user of the interactive application.
The structure for the vOS architecture is a tree with a
single controlling element at the base (vSM), communicating with multiple individual systems through a module
residing on each system (vEX) that in turn communicates
with a module assigned to manage an individual application (vIN).

2.3 Technologies
Two key technologies enable the operation of the vOS.
The first is the API/DLL Interception based on Richter’s
[4] description (also see section 5). APIs are intercepted
by changing the addresses within the Import Address Table (IAT). The IAT contains a set of pointers filled in at
runtime that resolve to the API fulfillment address in the
appropriate DLL. This approach allows new code to be
inserted between the application and the DLL. Once the
API is intercepted, we have full access and control of the
procedure call information.
The second technology is the creation of an abstraction
layer between a running application and the underlying
system through the creation and management of virtual
handles. Virtual handles are values that are created to replace the original handles returned by the API from the
system. The application saves and uses the virtual handles the same it would real handles. The virtual handles
are connected to the real handles when the application
makes requests to the system. This approach allows an
application to be moved between computers, transparently, by remapping the unchanging virtual handles to real
handles when required. The application remains un-

changed and is not it aware of the underlying relationships.
State and handle data are collected, saved and managed within the vIN, reserving it in migration tables. The
tables become a portion of the data copied and restored
during the process migration operation described below.

3. Process Migration
A core activity for the vOS is the capturing of process
API states, which enables the migration of processes between systems (see section 2 above). A prodromal condition for migration is the virtualization of the application
handles and the capture of requisite state information.
Once acquired, the vOS provides the mechanism for the
migration. Collecting and managing the data for this migration requires that a variety of considerations and activities be coordinated between the vOS components.
Specifically, the complexity of the program forms the
guideline for the migration approach that can be employed. The following sections provide a review of the
various data elements and activities managed by the migration system.

3.1 Migration Factors
It is at least theoretically possible to migrate or move
any application between one or more systems. However,
the complexity of the migration is dictated by a combination of several implementation factors associated with
every process. Although these factors interact with each
other, they represent unique and isolated activities in the
migration algorithm. The following is a list of the key
migration factors:
Threading: Each application consists of one or more
threads. If each thread remains independent of any of the
other threads in the process, then the handling of the state
information has the same degree of complexity as a singular state process. Threads that dependent upon each
other are considered intertwined. At the lower level, intertwining can take the form of shared global variables2.
At the higher level it can take the form of complex signaling interrelations.
User Interface (UI): In general, the simpler the interface
type, the less complex the task to migrate the state of the
interface. For example, a text-based interface requires
very little work to gather and restore as compared with a
bit mapped interface.
Input/Output (IO): If file IO is present, the complexity is
represented by both the quantity and type. A single sequential file requires a file pointer, file name and physical
2

Although Global Variables seem of little consequence to complexity
within the same process address space, they introduce complexity when
threads are placed onto separate machines.

Proceedings of the 22 nd International Conference on Distributed Computing Systems (ICDCS’02)
1063-6927/02 $17.00 © 2002 IEEE

address for a migration operation. A more complex example, such as one or more database transactions, requires that sufficient state knowledge be available to, either reconstruct an in-process transaction or to cause a
transaction back-out and a subsequent restart after the
migration.
Network: Network components are not considered movable or migratable, without the aid of some form of middleware or higher-level communication state management. Even using the approach pioneered in [5], more
work is required and thus more complexity is introduced.
Operating System: The degree of homogeneity between
the environments is crucial. Even though there is a general compatibility between the various versions of Microsoft Windows, migration between two versions, such as
Win95 and Win2K, may not be possible for several reasons. Firstly, the content and implementation of the supported system APIs is different. Some of the APIs implemented in Win2K do not have direct correspondence
with Win9x. In addition, syntactically equivalent APIs,
can semantically differ. Even a migration between two
more closely related systems, such as WinNT and Win2K,
provokes implementation issues, when the semantic of an
API varies subtly. Complexity increases between two
systems, such as Linux and Windows, since there is little
direct correspondence between their APIs and implementation. Complexity of this type is not directly solvable

In

a
c re

g
s in

C

om

x
p le

without the addition of either an API translation subsystem or a Windows/Linux API library.
Role: Today’s systems usually consist of some form of
either client/server or peer relationship with other systems. In either case, request and response messages are
generated that affect the current state of the process, as
well as the process’ intermeshing with its environment.
Migrating a process involved with commitments to or
from adjacent systems creates additional complexity. The
issues can involve, at the very least, the network relationships to one or more systems. They can also involve one
or more stateful relationships where the state information
is stored as a series of system table entries, such as are
found in a Windows registry.
Distribution: Distribution is concerned with the aggregate state of a set of distributed applications. In a distributed system, complexity is gauged by the ability to identify and capture the state of the various application components of the process. Spatial arrangement comes into
consideration here if the application must be co-local to
some resource for operational or performance reasons. In
this case, migrating the process may consist of more than
just moving the state information.
Platform: Platform hardware homogeneity is a principle
requirement for movement between platforms. In general, movement between two dissimilar hardware archi-

ity
Distributed Object systems
Distributed Database Applications

vOS
MS Office

Multithreaded windowed communication
and/or file based applications

wordpad
notepad
winmine
calc

Full State

most non windowed, network and file
based applications

Minimal State

Figure 1: Migration Method Mapping

Proceedings of the 22 nd International Conference on Distributed Computing Systems (ICDCS’02)
1063-6927/02 $17.00 © 2002 IEEE

Full Distributed State

tectures is not possible, without some form of emulation.
Complexity is measured here by the degree with which
corrections must be employed to assuage any system
mismatches.
Security: Movement of any process requires that the security environs either be the same or modifications be
made to adjust the settings to the new environment. If the
process is using only the default security descriptors or
the same descriptors in its operation, then complexity is
eased considerably.

3.2 Migration Models
We discovered that the task of migrating a process is
divisible into three methodologies based on the type of
state information required (see figure 1). Simple processes require only a small or minimal amount of state information to be collected and restored; this approach is
referred to as the Minimal State Migration Model. More
complex processes involving network, file and threading
require that a complete collection and restoration of process state be undertaken. This method is defined as the
Full State Migration Model. Processes distributed over
several machines and locations and effectively describing
a distributed state model, must use the more complex Distributed State Migration Model which is not discussed in
this paper.

pointers into the heap where the actual data is placed.
Thus, the heap must be included as part of the migration.
For the “.data” area to be consistent with respect to the
handles, heap locations and values, two actions must take
place. Firstly, the handles must be allocated such that the
same virtual values are returned in the same order each
time the application is restarted. Secondly, the memory
allocations must be at the same locations and contain the
same data for each program execution. These two constraints create a requirement for heap management to be
performed as part of the migration system in order to ensure that both location and content are identical. This can
be seen in the case of two dissimilarly configured systems
where heap allocations of the native Windows routines
are not guaranteed to generate the same addresses. Replacement heap functions provide this guarantee by using
a fixed location in memory for the memory allocations,
thus ensuring that memory remains identical for a given
allocation ordering.
The algorithm that performs the migration consists of
the following steps, which create the clone or copy:
1.
2.
3.

Suspend the source process thread(s)
Write the handle, memory and .data
Terminate the thread

To restore the process the following steps are used:
3.2.1
Minimal State Migration
At the lowest level of migrational complexity is the
partial-state migration method. Nasika and Heballalu
[5,6] provided the pioneering work and Zhang, Khambatti
and Dasgupta [7] extended their findings. Given an application of relatively low complexity, a certain minimal
set of state elements combined with a restart/suspend
technique is all that is required to correctly migrate the
specific set of processes. This minimal set of elements
consists of the “.data” area, heap allocation and handles.
Tests show that these three components correctly recreate
process state for applications containing one or more unrelated windows, one or more non-intertwined threads, no
IO, no networking, same security, same platform, same
OS, no role, no distribution and simple objects.
The .data, heap and handles are combined using a restart/suspend technique listed below. This technique succeeds by setting up the three migration components in
such a way that they create a reproducible memory structure. This means that the address and content of the
global data area, “.data” must be the same for each instantiation of the migrated process. This is in turn achieved
by noting that in the Win2K environment, the “.data” area
generally contains the uninitialized and initialized global
and static variables for the application. However, these
variables are not physically present in this region. The
Windows implementation uses this area as storage for

1.
2.
3.
4.
5.
6.
7.

Launch a new, suspended instance of the process
Inject the vIN and rebuild the handle tables
Resume the thread(s)
After thread(s) quiesces, suspend the thread(s)
Reapply, unapplied handles and memory
Restore the “.data” section
Resume the thread

3.2.2
Full State Migration
To migrate any non-distributed process requires the
capture and restoration of the full process state. A benefit
of this approach is that it provides support for a richer set
of migration scenarios, containing one or more unrelated
windows, one or more non-intertwined threads, IO, networking, differing security, same platform, same OS, differing role, no distribution and multiple objects. The state
elements involved with this technique consist of the
stacks, processor state, virtual handles, “.data” area, heaps
and kernel objects.
In earlier work on process migration [2,3], it was believed that the processor state and heap data combined
with handle virtualizing at the API/System DLL boundary
layer was sufficient for reconstructing the process state.
Further examination shows that crucial portions of the
system state are missing, thus preventing effective reconstruction of the original environment. The missing in-

Proceedings of the 22 nd International Conference on Distributed Computing Systems (ICDCS’02)
1063-6927/02 $17.00 © 2002 IEEE

formation resides within the kernel and represents the
process, thread, and windowing states. Reanimating a
migrated application requires that kernel object states be
recreated and reattached to their application level
manifestations.
We determined that just restoring the stacks and
threads is not sufficient to restart the windows engine.
We examined an alternative approach to restoring the
window state using window “sub classing” and “super
classing” [8], which involves redirecting the window
class procedure to an alternative user procedure. Although a limited success was achieved, it was not possible to fully reconstruct the pre-migration environment.
For example, there is no method to reassociate the current
queues with the premigration application state, so the subclassed window is stillborn on creation.
User mode

Kernel Object API

NTDLL.DLL
Int 2E Interface

Executive API

Win32 USER
and GDI

Kernel mode

Figure 2: User Kernel Call Interface

To capture full process state requires that a virtualization occur between the user and kernel transition levels
(see figure 2). Capturing the handle and state information
at the kernel transition layer allows for virtualization of
the object handles returned by the kernel, providing for
later reconstruction of the underlying system state. The
risk of using this approach is that the kernel API calls are
not publicly documented and are subject to changes without notice. Therefore, changes to the operating system,
including version and maintenance, may require alteration
to the API capture routines
We observed that in many cases at the point of migration, the threads are within a system library which has in
turn made a call to the kernel. Reestablishing these specific states is possible by using one of two approaches.
The first approach is to reissue the kernel call after each
of the kernel objects is recreated. The second is to fail the

kernel call by allowing the call to return with a failure
code, causing the system error handling routines to deal
with the resulting failure. As a variation on this last
theme, a special return value is used to alert the virtualizing code of the need for a restart, and then the call with
the original captured data is reissued.
As in the Minimal State Model, heap management is
also crucial. The heaps and .data relationships and locations must match between migrations. This means that
the target heap memory must be reallocated at the same
addresses without colliding with any preexistent heaps.

4. System Performance
The process migration mechanism has been implemented and tested as a part of a prototype vOS [10]. To
perform process migration using the vOS as the control
system requires that there be a minimum of two systems.
The first system contains a vEX and the vSM. The second
system contains a vEX. The operation of each of these
components is described individually in section 2.
Testing was performed on a simple windows application known as calc.exe which is the stock Windows
calculator accessory. The process of installing the
interception APIs consists of injecting the target
application with the vIN Dynamic Link Library code.
The code, during its initialization, loads amongst other
modules the hook library, which virtualizes the target
APIs upon its instantiation. The time to inject the process
is 526.2 ms and the time required to perform the complete
vIN setup, including API interception, is 1108.7 ms.
The procedural logic to migrate a process is shared between the vEX and the vIN, which use overlapping execution with system signal synchronization. To develop a
performance picture for the process migration, it is best to
measure the lower level procedures that actually perform
the data collection and restoration activities.
To migrate a process, the vOS library routines perform
the complete handle, heap and .data collection, formatting
and migration file build function. The overall clone build
process time is 2,385.8 ms, which includes the file write
time. Contained in this time are the following elements:
•

Handle/Memory collect and format

68.7 ms

•

.data collect and format

69.9 ms

•

Heap compaction

192.6 ms

•

Heap collect and format

264.5 ms

The process is restored as part of the vIN initialization
routine. Since this is also a complete restart of the process, the process is injected with the vIN, taking 362.6 ms,
which is less than the previously measured time due to
the vEX’s direct involvement with the process restore.

Proceedings of the 22 nd International Conference on Distributed Computing Systems (ICDCS’02)
1063-6927/02 $17.00 © 2002 IEEE

The vEX acts to directly control the restore process, requiring less overhead than the normal system injection
processing. The overall restore time is 2,172.0 ms. This
time is less than the original migration build time, in part
due to the use of the Minimal State migration model (section 3.2.1) which does not require the heaps to be restored. After the state information is restored and the application is allowed to initialize to the point of displaying
the window, any handle table and memory table values
that remain are replayed and new real values are recreated3. This procedure brings the application back to the
same state that existed at the time of migration.
The timings for the restore elements are:
•

Handle/Memory restore

•

.data restore

34.1 ms

•

Handle table replay

17.0 ms

•

Memory table replay

35.4 ms

337.3 ms

The total vIN setup time required to support the restoration is 1616.1 ms. This time is slightly higher than the
non-migration time, but is to be expected due to the additional procedure time associated with restoring a process.

5. Related Work
There are two fundamental sets of technology related
to this work. The first is the API injection and interception methods. The second is the system structure supporting the injection and interception.
Mediating Connectors [11] was developed by the USC
Information Sciences Institute and consists of a library
and run time applications. It wraps the DLL API with
code developed by the user and ensures that the new code
is invoked when the wrapped API is referenced in the application. Mediating Connectors is also known as NT
Wrappers. We originally considered incorporating this
technology into the vOS. However, its main focus is on
relatively standard applications which rendered it programmatically heavy and inflexible. The vOS required
more system level control and access to the source modules and thus a facility that was more lightweight.
InjectLib [4] is one of several prescribed methods used
for injecting4 application code into an active process from
a separate process on the same machine. Process A injects a stub routine S into process B, using standard Windows system calls. S is a separate thread that in turn
loads a DLL module D, specified by process A, as part of

the injection. D’s initialization code is executed after it is
loaded, thus providing the base for further interaction
with process B. This approach is incorporated into the
vOS.
Detours [12] from Microsoft Research is an API interception application library using DLL delayed binding in
conjunction with API preamble rewriting to intercept and
redirect application calls. The redirected Win32 function
call is routed through the Detour code where access to the
original function is provided using a trampoline function.
Rewriting the application code creates several potential
security, portability and compatibility issues.
COP [13] is a collaboration between Microsoft Research
and the University of Rochester using Detours. It is Microsoft Foundation Class (MFC) and Common Object
Model (COM) oriented, building and wrapping components around the Win32 API. Applications that do not use
COM are not candidates for inclusion in this technology.
NT-SwiFT [14] also known in its first production release
from Lucent Technologies as SwiFT for Windows NT.
The system is designed to support client/server architectures and provide high availability and reliability. In the
role of fault tolerance and high availability, it is capable
of migrating applications between systems and restarting
them in the event of a failure. The facilities provided can
be used standalone or can be integrated into a complex set
of system components. It is assumed that server application can and should be modified to incorporate the SwiFT
capabilities and that client applications do not necessarily
need to be modified, although SwiFT provides client level
API support. SwiFT performs API interception of a limited number of API’s and DLL’s.
Transparent Checkpoint Facility On NT [15] was specifically designed to checkpoint long running engineering
applications. It performs API interception the same way
as the vOS by capturing system and application state
while the application executes then replaying the captured
state data when restarting an application. Automatic
check pointing is installed on the application by using an
alternate loader and does not require application alteration, however, APIs are available for an application to explicitly control the system behavior. The system has several acknowledged limitations, including the requirement
that temporary file state be retained for a restart. Applications bypassing the IAT may not work correctly and multiple interacting processes are considered too complex to
handle. It does not appear to handle windows or non-file
oriented networking.

6. Summary
3

Values remain after the replay. They are predominantly handles created by keystroke activites not duplicated during the restore procedure.
4
When code is placed into an existing process address space from an
adjacent address space, the new code is termed as “injected”.

In this paper, we have presented experimental process
migration work using the virtualizing Operating System
or vOS. The vOS is the main platform implementation of

Proceedings of the 22 nd International Conference on Distributed Computing Systems (ICDCS’02)
1063-6927/02 $17.00 © 2002 IEEE

the Computing Communities project and forms the basis
for implementing the core technologies. It is distributed
in nature and is integrated into the existing Windows
2000 environment without altering either the applications
or the operating system.
The main focus of this research is the identification
and implementation of migration technologies using the
vOS as a base. We divided the applications into three
fundamental categories based on degree of complexity.
Three methods were defined for use depending on the application complexity. The first method is the Minimal
State and handles simple or non-complex applications.
The second method, Full State, handles more complex
non-distributed applications. The third method is the Distributed State method, which handles applications spread
over a network.
This work has described the process and performance
of the Minimal State method. After conducting further
research, we find that the Full State method it is both feasible and relatively straightforward to implement into the
vOS, using the technologies and techniques we have perfected. The bulk of the work of implementing the Full
State process migration model is not the virtualization
and state capture issues, since we have implemented those
technologies with the Minimal State model. The difficulty in implementing the Full State model is in integrating a reasonable subset of API hooks from the over 1700
published APIs implemented in the system Dynamic-Link
Libraries5.

7. References
[1]

[2]

[3]

Partha Dasgupta, Vijay Karamcheti and Zvi Kedem,
Transparent Distribution Middleware for General Purpose Computations, International Conference on Parallel
and Distributed Processing Techniques and Applications
(PDPTA’99), June 1999.
Tom Boyd and Partha Dasgupta, Virtualizing Operating
Systems for Seamless Distributed Environments, in Proceedings of the IASTED International Conference on Parallel and Distributed Computing and Systems, vol. 2,
November 2000, pp. 735-740.
Tom Boyd and Partha Dasgupta, Injecting Distributed
Capabilities into Legacy Applications Through Cloning
and Virtualization, in the 2000 International Conference
on Parallel and Distributed Processing Techniques and
Applications, vol. 3, June 2000, pp. 1431-1437.

[4]

Jeffrey Richter, Applications for Windows, Fourth Edition, Microsoft Press, 1999.

[5]

Ravikanth Nasika and Partha Dasgupta, Transparent Migration of Distributed Communicating Processes, to ap-

pear in the 13th International Conference on Parallel and
Distributed Computing Systems, August 2000.
[6]

Raghavendra Hebbalalu, File Input/Output and Graphical
Input/Output Handling for Nomadic Process on Windows
NT, Master's Thesis, Arizona State University, August
1999.

[7]

Shu Zhang, Mujtaba Khambatti and Partha Dasgupta,
Process Migration through Virtualization in a Computing
Community, 13th IASTED International Conference on
Parallel and Distributed Computing Systems, August
2001.

[8]

MSDN Library – January 2001, Microsoft Corporation,
2001.

[9]

K. Mani Chandy and Leslie Lamport, Distributed Snapshots: Determining Global States of Distributed Systems,
ACM Transactions on Computing Systems, vol. 3, no. 1,
pp. 63-75, February 1985.

[10] Tom Boyd, Virtualizing Operating Systems for Distributed Services on Networked Workstations, PhD thesis,
Arizona State University, Tempe, AZ, December 2001.
[11] Robert Balzer, Mediating Connectors, Proceedings of the
19th IEEE International Conference on Distributed Computing Systems Workshop, 1994, ISBN 0-262-57104-8.
[12] Galen Hunt and Doug Brubacher, Detours: Binary Interception of Win32 Functions, Proceedings of the 3rd
USENIX Windows NT Symposium, July 1999.
[13] Robert J. Stets, Galen C. Hunt, Michael L. Scott, Component-based APIs: A Versioning and Distributed Resource
Solution”, IEEE Computer, vol. 32, no. 7, July 1999.
[14] Yennun Huang, P. Emerald Chung, Chandra Kintala,
Chung-Yih Wang De-Ron Liang, and De-Ron Liang, NTSwiFT: Software Implemented Fault Tolerance for Windows NT, 2nd USENIX Windows NT Symposium, July
1998.
[15] Johny Srouji, Paul Schuster, Maury Bach, Yulik Kudmin,
A Transparent Checkpoint Facility on NT, Proceedings of
the 2nd USENIX Windows NT Symposium, August 1998.

Sponsor Acknowledgment: Effort sponsored by the Defense Advanced Research Projects Agency, under agreement numbers F30602-99-1-0517 and
N66001-00-1-8920. The U.S. Government is authorized to reproduce and
distribute reprints for governmental purposes notwithstanding any copyright
annotation thereon.
Sponsor Disclaimer: The views and conclusions contained herein are those
of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Defense
Advanced Research Projects Agency, or the U.S. Government.

5

The versions of NTDLL.DLL and GDI.DLL current to this writing
contain 1187 and 543 API entries respectively for a total of 1730.

Proceedings of the 22 nd International Conference on Distributed Computing Systems (ICDCS’02)
1063-6927/02 $17.00 © 2002 IEEE

Poster Abstract: Community Sensor Grids
Deployment and Usage
Partha Dasgupta

Amiya Bhattacharya

Meddage S. Fernando

School of Computing and Informatics
Arizona State University
Tempe, AZ 85287

Department of Computer Science
New Mexico State University
Las Cruces, NM 88003

Department of Computer Science
New Mexico State University
Las Cruces, NM 88003

partha@asu.edu

amiya@nmsu.edu

saliya@nmsu.edu

monitoring and home security may remain to be the primary
reasons behind these deployments, their ubiquitous span
offers a great potential for community use.

ABSTRACT
Design and prototyping are underway for realization of a
concept called community sensor grids. The idea is that
individuals or small community agencies may deploy
sensors to cover their own property, but also make them
available for private as well as public monitoring through
Internet connectivity, leveraging the mass deployment and
use of sensors to the mainstream. Such transient sensor
grids are to be composed of virtual sensor nodes
contributed by more than one physical sensornets while
offering a common processing view and sharing semantics,
in spite of the inherent heterogeneity in ownership,
hardware and operating systems for the hosting nodes.

We explore the possibilities of forming scalable
community-based transient sensornets based on a nonexclusive total access model, so that ordinary citizens are
empowered to program networked sensing applications
expanding the reach beyond their ownership. Sensor nodes
can advertize their capabilities/service over a peer-to-peer
(P2P) network formed by their respective Internet
gateways; and a user with proper credentials can discover
such nodes within the intended area of coverage, thereby
forming a new virtual sensornet to inject a sensing task
without disturbing the ongoing primary tasks at any node.

Categories and Subject Descriptors

Thus a typical user belonging to the community often
borrows sensor node capabilities from parts of the
community sensornet infrastructure to form a transient
virtual sensornet over a desired area of coverage. The rules
of sharing are dictated by the owner of the resource and at
no time the sensing task of a local or remote user should
violate the authority, privacy and security of the provider.

C.2.1 [Computer-Communication Networks]: Network
Architecture and Design—distributed networks, network
communications, wireless communication.
C.3 [Special-purpose and Application-based System]: Realtime and embedded systems.

General Terms
Design, Management, Experimentation, Security.

2. APPLICATIONS
Consider a scenario in urban sensing, where a city is
interested in vigilance on brush fire to actuate an early
response system. Harnessing city owned sensors as well as
enticing citizens to deploy mote-based sensornets within
their property at their own cost can efficiently solve the
sensing problem. Contour detection needs adaptive
sampling of temperature based on frequent single hop
exchange of readings among neighbors. Forming a
community sensor grid would support seamless direct data
exchange possibilities between sensor nodes owned by the
city and the citizens, leading to an energy-efficient
implementation of contour detection leveraging in-network
aggregation and processing.

Keywords
Wireless PAN, virtualization, Internet integration, peer-topeer network.

1. INTRODUCTION
In not too distant future, sensor nodes are likely to be
deployed in various capabilities by private citizens, social
groups, non-profit and commercial organizations and
governments alike. While private use such as asset/health
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
Copyright
is held
bypermission
author/owner
requires prior
specific
and/or a fee.
IPSN’09,
April
13–16,
2009,San
San
Francisco,
USA.
IPSN’09, April 13–16, 2009,
Francisco,
CA, California,
USA.
ACM
978-1-60558-371-6
Copyright
2009 ACM 978-1-60558-371-6/09/04…$5.00.

Scalable design of a large micro-monitoring infrastructure
such as the one needed by NEON (National Ecological
Observatory Network) to supplement its remote-sensing
mechanisms would also be benefited by the community

389

sensor grid based design. Even for a single organization, it
is more logical to divide large infrastructure into
constituents under local maintenance and administration.
While this instills artificial boundaries, a technique for
authenticating a remote user to form a guest sensornet and
inject a sensing application thread can easily overcome the
limitation imposed by administrative boundaries.

infrastructure that can be human-driven, similar to the Web
2.0 systems.
We are investigating several mechanisms, centralized as
well as decentralized, to allow expert users form
customized community sensor grids by gluing together
multiple sensornets (administrative domains), as well as
casual users to use pre-configured grids for data access and
in-situ aggregation. Prevalence of human factors in
discovering sensor coverage and services calls for handling
transience and flexible query semantics, making DHTbased structured P2P search strategies inappropriate for the
search and configure operations [2].

Community sensor grid of virtual nodes

The first-level sensornet advertizing medium would be a
site such as Geocaching (http://www.geocaching.com)
which allows location-based lookups, a map-based mashup
and the ability for anyone to post details about a sensor
network. Such a scheme would be centralized and provide
geographical attributes like Microsoft SensorMap [3]. The
second-level method would be to advertize virtual sensor
grids that are configured and available for use using a web
page that is similar to group web sites, such as Meetup
(http://www.meetup.com). This method can allow not only
geographical but attribute based searches.

Constituent sensornets (administrative domains)

Figure 1: Forming a community sensor grid

Fine grain attribute-based lookups can be done via several
methods involving personal or community web pages using
established terms and attributes that are searchable via
search engines. Databases can be built from such
information that provides more automated searching with
attributes (e.g. http://www.realtor.com). Finally, some sites
are expected to provide access to aggregated, real time,
sensor data. Such sites would in turn use a sensor-grid to
obtain raw data, and provide a user interface (visual or
data-streaming) for generating as well as visualizing useful
aggregated data, such as those provided by live financial
data sources.

3. ARCHITECTURE
A sensornet today has a commonly accepted structure of
being a cloud of sensor nodes hanging off of a gateway at
the edge of the Internet. The idea of grid integration of
sensornets is relatively new and is indeed in a nascent stage.
In brief, the community sensor grid architecture consists of:
(1) virtualized sensor nodes, where each virtual node is
hosted on a physical node but may run a sensing task
isolated from other instances of virtualized views of the
same node, (2) network connectivity for each virtual node
despite their existence on different physical wireless PANs
that may operate on different channels, (3) a single
sensornet view offered across diverse administrative
domains as in Figure 1, (4) a transient underlay that allows
sensing tasks running on a node cluster to bypass IP
gateways if the participating virtual nodes can find a
reliable path with less hops through a neighbor across
hosing domains, (5) a P2P overlay network that ties
together the IP gateways to the constituent underlays of the
sensor grid, (6) and a robust security and access control
management system that uses certificates, user accounts and
role for authentication and access privilege granting [1].

5. ACKNOWLEDGMENTS
This material is based upon work supported by NSF under
Grants no. CNS-0551734 and CNS-0617671, and a GREG
award from the New Mexico State University.

6. REFERENCES
[1] A. Bhattacharya, M. S. Fernando and P. Dasgupta,

2008. Community Sensor Grids: Virtualization for
sharing across domains. In Proc. ACM MobiVirt’08
(Breckenridge, CO, June 2008).
[2] Y. Chawathe et al., 2003. Making Gnutella-like P2P

4. DEPLOYMENT: SEARCH/FORMATION

systems scalable. In Proc. ACM SIGCOMM’03
(Karlsruhe, Germany, August 2003), 407-418.

The deployment of the sensor grids are expected to be viral
along with incentives offered by governmental agencies and
special interest groups. However, the viral growth would be
limited or impossible without a search and configure

[3] S. Nath et al., 2006. SenosrMap: A web site for sensors

world-wide. In Proc. ACM SenSys’06 (Boulder, CO,
November 2006), 273-274.

390

Object Memory and Storage Management in the Clouds Kernel
David V . Pitts

Partha Dasgupta

Department of Computer Science
University of Lowell
Lowell, MAO1854

School of Information and Computer Science
Georgia Institute of Technology
Atlanta, GA 30332

Segmented Address Space
Each object resides in its own segment and has a private
virtual address space. Irrespective of whether the object is
stored on secondary storage or in physical memory, the object
always appears to be in this virtual address space.

Abstract
The Cbuds kemel is a native layer distributed kemel supporting the
Clouds operating system. Cloudr is a distributed object-based
system, designed to support fault tolerance, location independence,
and an actiowobject programming environment. Some of the key
issues in supporting Cloudr are the availability of Object Memory,
Object Location and Object Recovery. Object Memory provides a set
of global, persistent, named address spaces for storing objects. The
address spaces resemble conventional segmentation schemes, but are
persistent and thus replace both the computational and storage
systems used in conventional schemes by a more powerful paradigm.
The Object Location system provides transparent object operation
invocation mechanisms throughout the distributed environment. The
Object Recovery system supports recoverable objects through
shadowing and two-phase commit techniques to allow atomicity of
actions. This paper describes, in brief, the key issues in the design
and implementation of the Object Memory and Storage Management
system. The implementation is operational and in use by the Clouds
Project at Georgia Tech.

Passive
Objects are passive; i.e., they do not have any active entities
(processes) permanently residing in them. As described, an
object is simply a named, vimal address space.
Persistent
Objects are persistent. An object can be created by users (or by
the system) and it exists until explicitly deleted. AU
modifications to the data in an object during its lifetime are
retained. Normal (non-recoverable) objects are as persistent as
conventional files, and are subject to similar problems of
consistency with respect to failure.
Object Invocation by processes
A process invokes an object by entering the address space of
an object at an entry point, and executing inside the object.
Any number of processes can invoke an object concurrently;
they all execute inside the virtual space of the object. Object
operation invocations can be nested.

1. Introduction

Recoverability (if necessary)
The data in the object is recoverable if the object is defined to
be recoverable. Any action touching a recoverable object can
be rolled back if, for any reason, the action fails.

The Clouds kemel is a distributed kemel intended to support the
construction of a fault-tolerant distributed system using the
object/action paradigm. The kernel provides three primitives for use
by higher level software (operating system and application level
software): passive objects, atomic actions, and processes. Objects
provide storage (actually persistent virtual memory) for data and
code; processes provide the thread of control that executes within
objects; and actions provide atomicity necessary for failure or error
containment.

1.1.2 The Clouds P a r a d i g m Figure 1 illustrates a typical Clouds
object. In the Clouds system, every entity is either an object, a
process, or an action. AU long-lived components are objects. Since
each object lives in a persistent virtual address space, the object can
Serve as a vehicle for not only computation, but also for storage. Any
information placed in an object remains there. Thus, given an object

The philosophy and implementation of these primitives and the
kemel are described in detail e l s e ~ h e r e [ ~ * *and
* ~ .are
~ 1 mentioned
only in passing in this paper. The paper focuses on the use of the
virtual memory systems of general purpose computers to support the
object/action paradigm.

permanent

data segment

1.1 Clouds a n d Object S u p p o r t

Volatile
data =sP=t

This section provides a brief overview of the Clouds system and of
the support provided for objects.
1.1.1 Objects in Clouds The central theme in the structure of

code

Clouds is the concept of an object. Objects encapsulate code and
data inm one logical unit. A set of defined entry points into the

segment --

objea’s code (a set of operations) provides the only means of

modifying and accessing the object’s data. An object in Clouds has
de following characteristics:

Figure 1- A-Object

IO

~1-1/88/0000/0010$01.000 1988 IEEE

Enlry
Points

system, there is no need for a file system, no need for program and
data files, and no need for conversion schemes to convert memory
structures to file structures. In fact, there is no need for VO.Even
terminal U 0 can be viewed as a side effect of invocations on a
terminal driver.

distributed object space
(non -volatile, virtual memory)

The key mechanism that supports this view of a computational
environment is the object memory manager. The object memory
manager creates the view of the Clouds world as populated by a set
of passive, persistent objects, which reside in virtual memory. The
object memory manager is supplemented by the object location
system, which integrates the objects into a single, location
independent domain, and by the recovery manager, which provides
support for keeping object data consistent. The focus of this paper is
to provide details about these subsystems, which are the building
blocks for supporting the Clouds paradigm.
There are several other projects and systems which have goals
similar to those of the Clouds operating system: providing objects
and actions on a distributed system. Some representative ones are
Arg~sJ5.61 IsISJ7.8.91
aen,[lO.l1.l21
TABS,(I3.'41
and
COSMOS.[15~16]
All of these systems are built on top of conventionally structured

operating systems such as UNIX.TM(TABS is built on Accent.) Thus
these operating systems use the memory management of the host
system and do not have custom management for object support.
Having custom management has several payoffs, the most notable
one being the conceptual simplicity of the underlying support,
leading to better integration of the implementation. This has several
premeditated side effects. such as efficient support for objects, true
concurrency in objects, true segmentation allowing processes to
traverse object spaces, and support for atomic actions at the native
layer.

1.2 Object M e m o r y and Storage Management
This section provides an overview of object memory and storage
management. We divide the task of managing storage and memory
for objects into three functions.
Object Location:
Since the storage system is a repository for objects, it must
provide the services for locating named objects.
Object Memory Management:
Though objects are really stored on secondary storage, the user
is made to believe the objects are always resident in physical
memory as a set of virtual address spaces. The object memory
manager creates this view through demand paging and
handling of obiect invocation.

-

-

Recovery Management:
As processes execute in an object, the data of the object is
updated. If the process is executing on behalf of an atomic
action, these updates should not become permanent until the
action commits. The recovery manager provides the commit
support and is able to roll back updates made on behalf of
failed actions.
The Object Location system uses sysnames to name objects.
Sysnames are guaranteed to be unique throughout the distributed
Clouds system. Sysnames are combined with access rights to form
capabilities. The C l o d object invocation uses the information to
N

LJNIX is a trademark of AT&T Bell Laboratories.

Figure 2. Object Memory
validate the invocation and then calls the storage manager to locate
the object.
The primary goal of the virtual memory system is to support the
abstraction of object memory. Object memory is the persistent,
constantly available state of an object.
Each virtual address space representing an object is kept on
secondary storage as a core image which is paged into physical
memory on demand. Each process on the system has a stack space,
which is also a virtual address space. When a process invokes an
object, the object virtual address space is mapped onto the process
virtual space, thus merging the object and the process stack into the
same address space. (Almost all paging systems allow processes to
have two segments, namely the code and the data segments. We use
the data segment to store the process stack and map the object into
the code segment.)
In conventional operating systems, the code segment of a process is
static and contains the code the process executes. In Clouds the code
segment of the process contains the object (code and data). As the
process traverses from one object space to another, ~s segment is
switched allowing the process to enter the target object. Figure 2
illustrates the use of object memory within this computational
model.

The calling process is not required to locate or otherwise ready the
object for processing. The object manager maps the object into the
process's space on demand. Inside the object, the programmer
manipulates object data directly; there is no explicit input/output to
perfom, because object data is always present in object memory.
The kemel finds and prepares objects transparently, using a local
invocation as described above, or issuing a remote procedure call
(RPC) if the object is not available
Remote invocations are handled at the remote site, where a slave
process is used to invoke the object. Thus, RPCs are handled like
local invocations at the remote site.
The recovery management system supports the consistency of object
data. Operations performed in object memory must be made
recoverable if performed by an action. Objects are considered by the
kemel to be either non-recoverable,meaning that the consistency of
persistent object data is not guaranteed, or recoverable, meaning that
part or all of object memory is subject to recovery processing and
system crashes will leave this portion of object memory in a
consistent state. The programmer of an object does not specify how
data is made recoverable, although he may specify what data is
recoverable, using the features of the Aeolus programming

1ang~age.I~~191 The virtual memory system provides much of the
information necessary to ensure that object data is recoverable. This
information is used by a set of recovery protocols provided by the
storage manager, which are described in section 4.
In the next few sections we discuss the object location mechanisms
(briefly), the object memory management design (in detail) and the
recovery scheme.

2. Object Location
Every object in the Clouds system has a unique capability,composed
of a unique sysname and some access information. The capability
contains no information as to the location of the object; it simply
identifies one object in the entire distributed system. Processes
access objects (invoke one of the object’s operations) by using the
capability. When a process requests access to an object having a
capability C, the kernel uses the local storage management system to
to search for the object. If the object is found locally, the request
proceeds. If the object is not found, then a global (broadcast)
message is sent, and all of the participating sites try to find the
object. The site which finds the object uses a slave process to access
the object on behalf of the calling process. The kernel search
mechanism is a very simple one, and was chosen primarily for the
simplicity of its implementation in light of the fact that our nodes are
connected via an Ethernet. We do not expect that this will be the
ultimate search strategy and, in fact, we do not believe it will scale to
larger configerations than our current one. A good search strategy is
the topic of on-going research.[201

invokif operation
Figure 3. The Maybe Table
3. Object Memory Support
Object memory relies primarily on the storage management and the
object management subcomponents of the Clouds kernel. Object
management is responsible for handling object operation
inv0cations.[~1The operation invocation mechanism has three
functions: validating the capabilities, argument parameters. and
operation code used in the invocation; initiating the invocation and
handling the return from an operation; and management of an
object’s memory. The storage manager is responsible for the
maintenance of structures representing object data, recovery of
object data (as directed by action management), and performing the
actual mappings necessary for object
An important part
of the sarage manager is the segment system.

Storage management is ultimately responsible for finding an object
at any site. The kernel storage manager attempts to reduce the
search time at each node, particularly in those instances where the
object is not present on the node. This does not remove the need for
a good object search strategy, but is certainly a good optimization for
any search strategy, provided the overhead is not too high. To
determine whether an object is present locally, the segment system
examines in sequence three structures: the Active Object Table
(AOT), the maybe table and finally the partition directories.[211 If an
object is currently in use, it is found in the AOT, and the search
succeeds quickly. Otherwise, the maybe table is consulted. Given a
capability, the maybe table indicates either that the object is not
present locally or that it mighr be. Thus. the maybe table is an
approximate membership tester. The maybe table trades accuracy of
its membership test for speed of the test. The purpose of the maybe
table is to provide a very efficient means of eliminating sites from a
search for an object. Hence, the maybe table is particularly useful in
remote operation invocations. The queries to the AOT and the
maybe table are quick enough that they may be done before a slave
process is started for an incoming search request. If the search is
short-circuited at this site (if the maybe table returns a negative
response), then no process cleanup is needed. Short-circuiting of
local site searches can affect the performance of remote invocations
dramatically as otherwise a search of the local storage may be
necessary.

3.1 Segment System
The segment system forms the interface of the kernel storage
manager and is the mechanism underlying the abstraction of object
memory. The segment system enables the kernel to map a collection
of segments into object memory, forming the object memory space
for a Clouds object.

3.1.1 Clouds Segments. Segments represent an alternative view
of object data and code. This view is reserved for the kernel and
facilitates the kernel’s treatment of objects. Whereas regular users of
an object can manipulate data only through operations defined for the
object (via a system call to operation invocation), the segment
system allows the kernel to operate directly on object data and code
through a set of primitive operations. The segment system can be
viewed as a collection of objects of a single, primitive type.
Segment data is an uninterpreted sequence of bytes which can be
manipulated by segment operations such as: mapping or m a p p i n g
a segment into virtual memory; reading or writing a page of a
segment; and performing recovery procedures on segment data.
Clouds segments serve a similar purpose to the segments found in
Multics.[221 Many of the internal structures used to implement
segments are similar. However, our view of segments is at a lower
level than that presented by Multics and segments find use both in
the support of object mapping and in object recovery.

If it is determined through the maybe table that the object may be
local, the search continues. The disk partition directoriesprovide the
final say as to the existence of the object. If the search is on behalf
of a remote request. a slave process is started by the process manager
for searching the partitions and invoking the object, if it is
Figure 3 illustrates the use of the maybe table during an W C .

3.1.2 Segment Representations. The above abstraction of a
segment is supported by two distinct representations of segment data.
One is the segment as it is mapped into virtual memory. The other is
the representation residing on secondary storage (referred to simply
as storage henceforth). This representation is a tree of storage
pages. The root is a segment header, which contains the identity of
the segment and other descriptive information. The tree leaves are

12

The invoker of an object operation makes a call on the kemel
operation invocation mechanism in the object manager. After
formatting the parameters to the invocation and validating the object
capability and operation code, the object manager determines
whether the object resides on the local site or whether an RPC is
necessary. If the object is not activated already, object management
makes a call on the storage manager to perform a search for the
object as described earlier. The result of this call indicates that the
object is either local and invocation can proceed, or that the object is
on a remote site, and an RPC is necessary. In either case the object
is activated and an object descriptor for the object is added to the
AOT. For a local object, the information describing the segments
necessary for object memory mapping is present; in the case of a
remote object, the object descriptor indicates simply that the object
is not local and no mapping information is available.

the segment data. Intemal nodes of the tree are mapping blocks,
providing paths to data blocks.
The segment header must reside entirely within a storage page, in
order to satisfy the correctness assumptions made for recovery
management. Currently, the design provides for page-sized segment
headers, simplifying the management of the headers. The
information contained in the segment header, with a few exceptions,
is not dependent on the object system residing on top of the storage
manager. The segment header has two components: the segment
descriptor and the segment map. The segment map contains pointers
to the segment data, either through mapping blocks or directly to
data blocks. The segment descriptor contains information describing
the segment, such as the segment’s sysname, its size, whether it
represents data from a recoverable object, and which portions of the
segment data are recoverable. It also contains information
describing the segment’s recovery state. Important information
includes a pointer to a shadow copy of the segment and a field
holding a sysname for the action that is committing in the segment.
These fields are only valid when the segment’s state is
“precommitted”.

When a site receives an RPC, storage management performs a local
search for the object’s data segment. If the search is successful, then
the object manager at this site is called to invoke the appropriate
operation. An object descriptor representing the object memory is
added to the AOT and the invocation is performed.

3.2.2 Segment Mapping. This section discusses the support that
the segment system provides for the operation invocation mechanism
and the abstraction of object memory. Similar to the AOT, the
active segment table (AST) and the associated active segment
descriptors (ASDs) are used to manage activated segments. Each
ASD contains information used to map the segment in virtual
memory. Important to virtual memory management are: the storage
segment header, information describing the windows into which the
segment is mapped; the virtual page table; the storage page table;
and caches of storage data for efficiency. The storage segment
header provides information as to where the segment resides. The
virtual and storage page tables describe the mapping of the virtual
memory image of the segment to the physical memory image and to
the storage image, respectively.

Storage management preserves the consistency of an object’s
persistent state through the use of the recovery protocols described in
a later section. Thus, the storage image for a segment always reflects
the latest committed image of the object’s memory space.

3.2 Object Operation Invocation
Clouds objects are typed. The type of an object is described by its
object template, which is used to create instances of the object.
Information found in the template includes the code, the data format,
and information describing how object memory is to be constructed.
Objects are actually a collection of segments; typically, an object
will havc a code segment and at least one data segment. Segments
may have different attributes depending upon their use: volatile
(non-persistent) heap segments; read-only code segments; and
persistent object data segments. Segments, particularly code
segments, may be shared among several objects. By the use of the
mapping mechanism on segments, the abstraction of a constantly
available object memory space is presented. The object template
contains the description of how the segments must be mapped by
storage management to build object memory. The stmctures used
are called windows. Each window specifies where in object memory
the segment is to be mapped and the attributes of that segment. In
addition, windows allow the mapping of only portions of a segment.
A segment may appear as a window in any number of object
memories, facilitating sharing.

The AST and ASDs serve two purposes. First, they indicate
segments which are currently being accessed by a process or action
or which were recently accessed. This makes the location of active
segments faster, since searches through partition directories are not
necessary. Second, the descriptors act as caches for the management
of the segment. This makes services such as page-fault handling
more efficient.
The page tables provide the actual mapping for the scgment. The
implementation of the tables depends heavily on the hardware and is
not discussed in this paper. The interested reader is referred to [21].
[4], and [23]. An important use of the mapping information is in the
support of action processing. Special mechanisms for providing
action versions of object data using the page tables are described in
[23] and [21]. These mechanisms ensure that each action perceives
its own view of object memory. The recovery protocols used in the
Clouds systems rely in part on the information held in the page table,
particularly the information as to what pages have been modified.

3.2.1 Operation Invocation. Underneath the abstraction of
object memory is the reality that an object’s state may reside on
storage and is not immediately accessible. Objects are activated on
demand by an invocation request. A description of the general
framework of operation invocation is presented, discussing how the
mapping of object memory is initiated. For further details on the
invocation mechanism itself, see [4]. The active object table (AOT)
and its associated active object descriptors (AODs) represent objects
whose operations have been recently invoked. Activated objects are
those objects which have descriptors in the AOT. AODs are created
(and the object activated) when an operation invocation is first made
on the object and remains until aged from the table or replaced by
other AODs that are used frequently. In addition to information
identifying the object and important to the invocation mechanism,
the AODs also contain descriptors reflecting the mapping of
segments into object memory, which are obtained from the object
template.

A segment is activated if it has a descriptor in the AST, and
deactivated otherwise. The activation of a segment is caused by the
operation invocation’s attempt to satisfy an invocation
At the end of a successful search, a segment is activated, but might
not be mapped into virtual memory. Before the invocation can
continue, portions of the segments must be mapped into object
memory windows and the virtual and storage page tables must be
created and initialized to reflect the mapping. The page tables arc
created with only information describing the attributes of the

13

segment pages; the mapping of segment page to physical memory or
storage is done via demand paging. In fact, all activation-object,
segment, and page “activation” (mapping a faulting segment page
into virtual m e m o r y t i s by demand. Volatile segments for objects,
which have no persistent image, may be created at this time.

Otherwise. the page is mapped. The descriptions allocated and
unallocated refer to whether a storage image exists for the page. A
volatile page initially has no storage image. In the case of nonrecoverable and recoverable segments, not all of the pages may be
initially allocated on storage. This speeds object creation, but
requires that storage be allocated when the pages are referenced
subsequently.

Segments may be deactivated when no longer needed by any object;
i.e., when the segment is not mapped into any object memory
window. In the case of volatile segments, there is no persistent state
associated with the segment and deactivation is really the destruction
of the segment. The deactivated segment’s descriptor is removed
from the AST, and the pages used in its mapping are freed. For
segments which represent persistent pations of objects, the segment
might be left active for a time, depending on the fraction of AST
entries that are full. When it is actually decided to deactivate the
segment, due to lack of references or being bumped by another
segment, its persistent state must be Rushed to storage.

The altered description indicates that a page has more than one
storage image associated with it, resulting from a modification by an
action being written to local storage. Note that this description is
appropriate for recoverable segment pages only. In this case, the
action versions of these pages are not part of the persistent segment
image. Unaltered pages have only one storage image contained in
the persistent state of the segment.
To describe the actions taken in the cases represented by these page
states, the following descriptions of local storage are used:

The segment system also provides maintenance of segment
mappings. There are two parts to this mapping: the mapping between
object memory and storage; and the mapping between object
memory and physical memory.

Paging storage:
This is local storage used as a temporary image for data
mapped into object memory. This type of storage is most
commonly used for volatile pages. Paging storage does not
survive between activations of a segment and is not persistent.

The segment system is responsible for locating and transfemng
segment data into object memory. Segment pages will be mapped
into object memory (due to page faults) and out of object memory
(due to page replacement) as in conventional systems. However,
there some differences, since this maintenance by the segment
system is especially critical to recovery management under Clouds.

Segment storage:
This is storage that is part of a non-volatile segment. As such,
this storage survives segment activations, and represents part
of the persistent state of a segment. Such storage is used for
both non-recoverable and recoverable pages.

The recovery algorithms rely on the mapping information to
determine what data has been modified. Also incorporated into
object memory management is the management of action versions.
Since an arbitrary number of actions may be executing inside a given
object, an important task is finding the version of the data that is
visible to a given action. Thus, mapping pages into a segment and
unmapping pages from a segment (part of page replacement) must be
handled differently in Clouds than in a conventional system.

Shadow storage:
Shadow storage is used to represent the uncommitted
modifications of an action which were written to local storage
(as may happen during virtual memory page reclamation).
This storage may become part of a segment after a successful
commit and is only used for recoverable pages. Hence, on
action commit it becomes segment storage. Shadow storage is
not persistent.

To understand how the segment system selects local storage to map
to an object memory page, it is necessary to understand the
underlying semantics of different classes of segments. There are
three important classes of segment to consider: volatile segments;
non-recoverable segments; and recoverable segments. The basic use
of volatile segments is for the data (stack and heap) portion of
processes. These segments have no persistent image; when a local
site crashes, the processes and their segments are gone. Nonrecoverable segments support non-recoverable objects. The
information contained in these segments is persistent, but
consistency is not maintained. Lastly, recoverable segments support
recoverable objects, and therefore the data in these segments must
be both persistent and must be maintained in a consistent state. The
differing semantics of these three segment classes requires differing
treatments of requests to map a page into the segment or remove a
page from the segment. The state of the particular page within a
segment also affects the treatment of the page.

Table 1 illustrates the use of each storage class as a function of page
state and segment class. In the case of unmapped pages, storage is
being allocated. In the case of mapped pages, storage already exists.
In the unmappedlunallocated state, no page has a storage image and
a reference to such a page results in the mapping of a zero-filled page
into the segment. For non-recoverable segments, pages are mapped
from segment storage. Either the segment storage is cached in
memory or the segment system must query on-storage tables. For an
unallocated page, segment storage must be allocated as part of the
mapping. Recoverable segment pages initially are mapped from
segment storage, but after they are altered the mapping is from
shadow storage. The exception to this is when the recoverable page
is initially unallocated. In this case, storage is obtained from shadow
storage, so that an abort or commit may be done properly later.

The page states reflect the status of the storage page table entry for
the page. The state can be determined by examining the storage
page table entry. The states are formed from a cross product of
vectors (shown in Table l), one of which reflects the mapping
between segment page and storage image and the other of which
rrflects the existence of a storage image for the page and whether
there is an action version of the page.
A page is unmapped when it has not been referenced before the time

of the page-fault and has not been mapped into object memory; i.e.,
it does not appear in the underlying virtual memory mapping tables.

14

Table 2. Modifying references to segment pages
Segment Class
Page state
I
Non
I
Volatile
Recoverable Recoverable
Paging
Unmapped/
Segment
Shadow
unallocated
Unmapped/
N/A
Segment
Shadow
. allocated
Paging
Segment
Shadow
Mapped/
unaltered
Mapped/
N/A
N/A
Shadow
I altered

I

I/

I

The discussion so far has concentrated on the mapping between
object memory and storage. The segment system is also responsible
for the mapping between object memory pages and physical page
frames. For the most part this mapping is similar to that found in
conventional systems. However, recoverable segments require
special treatment. Whenever a recoverable page is modified by an
action, conceptually a new version of the page is created for that
action. The collection of pages modified by the action collectively
make up the action’s version of the segment. The creation of
versions for modified recoverable pages is handled transparently by
the segment system. For more details, see [21] and [23].
Table 2 presents a summary of the storage classes used when an
object memory page must be unmapped. Additionally, modified
pages must be flushed to storage. In any state, unmapping a
recoverable page requires shadow storage. When the page is
unmapped, the segment system must allocate shadow storage for the
page. This is so that its contents do not overwrite the current state of
the page in segment storage. as the current state may be in use by
other actions. Similarly, when the page is unaltered, shadow storage
must be allocated before the page is written to storage. When the
page is altered, however, there may already be shadow storage
reserved for the action’s version of this page. If so, it is used;
otherwise new shadow storage is allocated.

In the case of volatile and non-recoverable segments, the appropriate
storage is allocated when the state of the page is
unmappedunallocated. Otherwise, storage has already been
allocated.
With these mechanisms, object memory is made available to
invoking processes and the multiple images associated with the
actions concurrently updating the object are maintained correctly.
The next section examines how the segment system uses this
information in recovery processing for action events.
4. Recovery a n d Segment Mappings
The virtual memory system has a great deal of influence on recovery
ir h e Clouds system. All of the modification history for segment
data by actions is obtained through examination of the page tables
for the segment. Much of this information is necessary for the
proper treatment of both recoverable and non-recoverable segments
and must be maintained in any event. This, together with the
shadowing[24]technique used in recovery processing, yields a natural
and potentially very effective recovery mechanism for the Clouds
system.
Recovery in the Clouds system is performed by the segment system
through three storage management protocols.[21] The storage
manager simply needs to know the sysname of the committing
action, and the list of objects that the action touched, and it can
determine what parts of the corresponding segments’ data must be

shadowed. C l o d s recovery processing is entirely separated from the
concurrency control done by action management.

C l o u h recovery is pessimistic; no part of the recoverable state of a
segment is modified until the commit is complete. Until that time,
all modifications to the segment’s recoverable state exist as shadow
copies. Shadowing is done at the page level. Because Clouds
actions may span several sites, action management uses a two-phase
commit protocol to commit an action. The recovery protocols used
by the storage manager fit into this model.
During the first phase of commit, the preparation phase, action
management calls upon the storage manager to precommir action
modifications. The storage management protocol for the prepare
phase takes the action and object sysnames and builds a shadow
version of each object data segment on local storage. The identity of
modified segment pages is found in the page table for the committing
action. Modified pages must be flushed to local storage as shadow
pages. Note that shadow images may have already been allocated
for some pages that have been written to storage prior to precommit.
Indeed, if there was no modification after this initial write, no write
is necessary during precommit for the modified page; it is already
shadowed correctly. As noted in [ 2 5 ] ,pre-flushing of segment pages
modified by actions as a background activity could produce
considerable savings in terms of writes to storage during the
preparation phase. Besides modified data pages, mapping pages
affected by the action and the segment header must be shadowed as
well. In fact, the shadow version of the segment header acts as a sort
of commit log for action management, as the action sysname is
stored in the shadow. Only at the coordinating site is an additional
log entry needed.[23]The result of precommit is a shadow version of
the segments touched by the committing action. The original
segment header points to the shddow version.
During the second phase of the commit, the action managcr asks the
participating sites if they can commit; that is, if the necessary
shadows have been built. If all agree, commit occurs. Otherwise,
the action aborts. Storage management has a protocol for each case.
Action management invokes each as necessary. If a commit is
indicated, the storage manager makes directory adjustments so that
the shadow versions of the touched segments become the working
versions. Allocations of the shadow pages become permanent and
previous page versions are deallocated. In the case of an abort, the
current segment header is rewritten so that the shadow version is not
referenced and the shadow pages are deallocated.
The recovery protocols assume that single page writes to storage are
atomic and that only one action is committing (although more than
one action may be active) inside a segment at a given time. Given
these assumptions, the recovery protocols leave a segment’s
recoverable data in a consistent state, even in the event of a site
crash. After such a crash, information as to what segments are in a
precommitted state is reconstructed by examination of the segments.
This entails reconstructing the storage allocation state and checking
the structural consistency of all segments as well. Segments in a
precommitted state are then subject to either the phase two commit
or abort protocol, depending on the final outcome of the action (the
action sysname is found in the shadow segment header).

5. Current Status and Performance
This section briefly describes the current status of the kernel
prototype and gives some preliminary performance figures for
recovery processing.

For each test the average time in milliseconds to perform the
protocol is presented along with the number of read and writes
necessary to perform the necessary protocol. The reads and writes
include those necessary to shadow segment structures such as
mapping pages and the segment header.

5.1 Status
The structures and mechanisms described in this paper have been
implemented. Clouds is in operation and uses the object memory and
storage management system to support objects. Operation
invocations (and the attendant mapping of segments into object
memory) is one of the most frequent operations in the CloudF
system. The invocations are location independent and remote
invocation of object operations is currently supported. Storage
management provides the necessary persistent storage for object
data.

Note that these results represent only timings for the shadow paging
scheme and do not represent the efficiency of the segment
inputloutput used to provide paging of object data. The timings for
these operations would on average be closer to those for the disk
reads and writes. Also, these results represent a worst case for
recovery; none of the modified pages was flushed to disk prior to
commit. For larger objects a mechanism to flush modifications into
shadow storage prior to precommit should improve commit times
greatly.

The implementation environment for the first prototype consists of
three VAX 1l/750s connected via a 10Mbivsec Ethernet. Currently,
the only secondary storage devices are three ten megabyte removable
media RL02s. Software for larger storage devices (RA8ls) is under
development, as is a new kernel prototype on a network of Sun 3/60
workstations.

6. Summary
The Clouds kernel supports the object/action paradigm, providing
long-lived objects both for computation and for storage. In addition,
provision for the recovery of object data is made through a set of
protocols provided by the storage management system. The three
major functions which support this paradigm, Object Memory,
Object Location, and Object Recovery, are provided transparently by
the storage management and object management subcomponents of
the kernel. Through these abstractions and because of the
transparent nature in which they are provided, Clouds users and
programmers are provided with a unified distributed environment in
which to work. There is no need for programmers to be concerned
with the underlying mechanisms.

The recovery protocols necessary to support action management
have been implemented. They have been tested and work correctly.
An action management system that provides user level transaction
and recovery support is under implementation. The action
management system uses the commit and recovery mechanisms of
the storage manager to support the user level services.

5.2 Performance
This section presents some timing results for two of the recovery
protocols provided by the storage manager: the preparation phase
protocol; and the final commit protocol for phase two. The recovery
protocols are the slowest of the mechanisms discussed in this paper,
and thus the following performance numbers represent an upper
bound on timing figures for the storage management system.

As the kernel is built directly on the bare machine, the design and
implementation decisions have been made on the basis of what best
supports the objectlaction paradigm. The result is a set of
subcomponents, object management and storage management,
customized for the support of objects. These components are well
integrated and provide efficient support for the objects.

Also presented are some timing figureson the underlying VO support
required by recovery management. This information. along with an
analysis of the number of VO requests required to perform the tests
described, is used.to factor out the hardware impact on the
performance of recovery management. As mentioned previously, the
prototype uses a RL02 removable pack disk drive. This device is
both slow and has limited storage capacity (10 megabytes), but has
such a simple interface that a device object was easily implemented.
This provided the ability to test the storage management and virtual
memory management aspects of the prototype. The RL02 has a one
cylinder seek time of 15 milliseconds and a data transfer rate of 512
kilobytes per second.IZ61

This is particularly evident in the support provided for object
recovery. A large part of the information required to support the
recovery protocols is obtained from the support necessary for object
memory. Improvements in the performance of recovery processing
can be obtained through simple changes to the cumnt mapping
mechanism for object memory (namely the pre-flushing of modified
recoverable pages).

7. Acknowledgements

In timing tests presented in [21], the average time required for a
random read or write over a two megabyte partition was 51
milliseconds. Average seek times recorded for short, five cylinder
seeks and long, 512 cylinder (across disk) seeks, were 21
milliseconds and 66 milliseconds, respectively.

The authors would like to recognize the many people who have put a
great deal of effort into getting Clouds off the ground. The initial
Clouds design was due in part to Martin McKendry and Jim Allchin.
Since then Clouds has been the result of a team of dedicated
researchers that includes Mustaque Ahamad, William Appelbe,
Glenn Benson, Jose Bernagu, Phil Hutto, Greg Kenley, Yousef
Khalidi, Richard LeBlanc, Gene Spafford, Henry Strickland, Win
Strickland, Scott Vorthmann, Peter Wan, and Tom Wikes.

Table 3 shows results for the recovery protocols performed on
segments of various sizes. AU times are in milliseconds. Precommit
is the preparation phase protocol that builds the shadow version of
the segment. Commit is the second phase protocol used in the case
of a successful commit. For each segment, varying numbers of
pages were modified and the recovery protocols used on the segment.

References

Table 3. Recovery times for a 41 page segment

[l]

132
1

42

16

Allchin, J. E. “An Architecture for Reliable Decentralized
Systems.” ml.D. DISS., School of Information and Computer
Science, Georgia Institute of Technology, Atlanta, GA. 1983. (Also
released as technical report GIT-ICS-83/23.)

Allchin, J. E.. and M. S. McKendry. “Synchronization and Recovery
of Actions.” PROCEEDINGSOF THE SECOND SYMPOSruM ON
PRINCIPESOF D I S T R I B ~ T E D C O M P ~ T I N(ACM
G
SIGACT/SIGOPS),
Montreal (August 1983).
Dasgupta, P., R. LeBlanc, and E. Spafford. “The Clouds Project:
Design and Implementation of a Fault-Tolerant Distributed Operating
System.” TECHNICALREPORTGrr-Ics-85/29, School of Information
and Computer Science, Georgia Institute of Technology, Atlanta,
GA, 1985.
Spafford, E. H. “Kemel Scructures for a Distributed Operating
System.” PH.D. DISS., School of Information and Computer Science,
Georgia Institute of Technology, Atlanta, GA, 1986. (Also released
as technical report GIT-ICS-86/16.)
Liskov, B., and R. Scheifler. “Guardians and Actions: Linguistic
Support for Robust Disuibuted Programs.” TRANSACTIONS
ON
PROGRAUMING L4NGUAGESAA’D SYSTEMS (ACM) 5, no. 3 (July 1983).

171 Wilkes, C. T., and R. J. LeBlanc. “Rationale for the Design of

Aeolus: A Systems Programming Language for an Action/Object
System.” PROCEEDINGS OF THE 1986 INTERNATIONAL CONFERFACE
ON CoMPurER LANGUAGES (IEEE Computer Society), Miami, FL
(October 1986): 107-122. (Also available as Technical Report GITICS-86/12.)
181 LeBlanc, R. J.. and C. T. Wilkes. “Systems Programming with
Objects and Actions.” PROCEEDINGSOF TIIE FIFI’II I m E R N A T I O N A L

Denver (July
1985). (Also released, in expanded form, as technical report GITICs 45/03,)
CONFERENCE O N DISTRIBLITED COMPUTING SYSTEMS,

1191 Wilkes, C. T. “Preliminary Aeolus Reference hlanual.” TECIUI~CAL
REPORT Grr-Ics-85/07, School of Information and Computer
Science, Georgia Institute of Technology, Atlanta, GA, 1985. @st
Revision: 17 March 1986.)

Weihl, W., and B. Liskov. “Specification and Implementation of
Resilient Atomic Data Types.” SYUPOVUM ON .DROCRAAfMINC
LANGIJAGE
ISSULV IN S017v’it~c
SYSTEMS (June 1983).

[20] Ahamad, M., M. H. Ammar, J. Bemabeu, and M. Y. A. Khalidi. “A
Multicast Scheme for Locating Objects in a Distributed Operating
System.’’ TECHNICALREPORTCrr-Ics-87/01, School of Information
and Computer Science, Georgia Institute of Technology, Allanla,
CA, January 1987.

Buman, K. p “Replication and Fault-Tolerance in the ISlS
System.” PROCEEDINGS
OF T l i E TWTH S r u p o s I u M ON O P E R A m v G
SYSTEMS f R l N C l P L I 3 (AChI SIGOPS), Orcas Island, Washington
(December 1985). (Also released as technical report TR 85-668.)

[21] Pitts, D. V. “Storage Management for a Reliable Decentralizw!
Operating System.” P1r.D. DES., School of Information and
Computer Science, Georgia Institute of Technology. Atlanta, CA,
1986. (Also released as Technical Report GIT-ICS-86Dl.)

Joseph, T. A. “Low-Cost Management of Replicated Data.” W.D.
DISS..Department of Computer Science, Comell University, Ithaca,
NY, November 1985. (.Also released as Technic& Report TR 85-

[22] Organick, E. I. The Multics System: An Exurnination oflts Sfructure.
Cambridge, Massachuset:s: The MIT Press, 1972.

712.)
Josepii, T. A., and K . P. Birman. “ t o w Cost Management of

Rep!icated

Data

TRANSACTIONS
ON
1966): 54-7C.

iri

Fault-Tolerant

COA4PuTFX SYSTEMS

Distributed Systems.”
{ACM) 4, no. 1 (Febuary

h’oe. J. D., A. B. Proudfool, and C. Pu. “Replization in Distributed
Systems: The Eden Experience.” TECHSTCAL REPORT TR-85-08-06,
Department of Computcr Science, University of Washington, Scattlc,
WA, September 1985.

[ll] Jeasop, W. H., J. D. Noc, D. M. Jacobson, J. Raer, and C. Pu. “The
Eden Transaction-Based File System.” PROCEELIINGS OF T ~ I E
SECOND SYMPOSIUM O N R U J A B I U T Y IN DISTRIBUTFD SOFTWARE A N D
DATABASE
SvsrEnrS (IEEE), Pittsburgh, PA (July 1982): 163-169.

[I21 Pu. C., and J. D. Noe. “Nested Transactions for General Objects:
The Eden Implementation.” TECEIKICAL
REPORT TR-85-12-03,
Department of Computer Science, University of Washington, Seatlle,
WA. December 1985.
1131 Spector, A. Z., D. Daniels, D. Duchamp. J. L Eppinger, and R.
Pausih. “Distributed Transactions for Reliable Systems.”
PROCEEDINGS O F T H E TFNI’ll SYMPOSIUM O N OPERATING SYSTEMS

(ACM SIGOPS), Orcas IsIanQ, WA (December 1985):
127-146. (Alsoreleased as draft technical report CMU-CS-85-117.)

PRINCIPLES

[I41 Spector, A. 2. “The TARS Project.” DATABASEENGINEERING
8, no.
2 (June 1985).
[IS] Blair, G. S . , J. A Mariani, and W. D. Shepherd. “A KnowledgcREPORT,Department of
Based Operating System.’’ TECHNICAL
Computing, University of Lancaster, Lancaster, England, 1985.

1161 Nicol, J. R., G. S. Blair, and W. D. Shepherd. “A Tailored Kemel
Design for a Distributed Operating System.” TECCHP~TCAL
EPORT,
Department of Computing, University of Lancaster, Lancaster,
England, 1985.

[231 Kenley, G. G. “An Action Management System for a Distributed
Operating System.” M.S. THESIS, School of lnformation and
Computer Science, Georgia Institute of Technology, Atlanta, CA,
1986. (Also released as technical report GIT-ICS-86/01.)
[24] Gray, J. N., and others. “The Recovery Manager of the System R
Database hlmager.” COMPUTING
SURVEYS (ACM) 13, no. 2 (June
1981).

1251 Traiger. I. L. ‘‘Virtual Memory Managcment for Data Base
Systems.” OPERACING SYSTEMS REVIEW (ACM SIGOPS) 16, no. 4
(October 1982): 26-48.
[26] DEC.. RMIIRLE User Guide. Maynard, MA: Digilal Equipment
Corporation, April 1982.

1000

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

VOL. 22,

NO. 7, JULY 2010

P2P Reputation Management Using
Distributed Identities and Decentralized
Recommendation Chains
Prashant Dewan and Partha Dasgupta
Abstract—Peer-to-peer (P2P) networks are vulnerable to peers who cheat, propagate malicious code, leech on the network, or simply
do not cooperate. The traditional security techniques developed for the centralized distributed systems like client-server networks are
insufficient for P2P networks by the virtue of their centralized nature. The absence of a central authority in a P2P network poses unique
challenges for reputation management in the network. These challenges include identity management of the peers, secure reputation
data management, Sybil attacks, and above all, availability of reputation data. In this paper, we present a cryptographic protocol for
ensuring secure and timely availability of the reputation data of a peer to other peers at extremely low costs. The past behavior of the
peer is encapsulated in its digital reputation, and is subsequently used to predict its future actions. As a result, a peer’s reputation
motivates it to cooperate and desist from malicious activities. The cryptographic protocol is coupled with self-certification and
cryptographic mechanisms for identity management and countering Sybil attack. We illustrate the security and the efficiency of the
system analytically and by means of simulations in a completely decentralized Gnutella-like P2P network.
Index Terms—Peer-to-peer networks, distributed systems, security, reputations, identity management.

Ç
1

INTRODUCTION

P

EER-TO-PEER

(P2P) networks are self-configuring networks with minimal or no central control. P2P networks
are more vulnerable to dissemination of malicious or
spurious content, malicious code, viruses, worms, and
trojans than the traditional client-server networks, due to
their unregulated and unmanaged nature. For example, the
infamous VBS.Gnutella worm that infected the Gnutella
network, stored trojans in the host machine.
The peers in the P2P network have to be discouraged from
leeching on the network. It has been shown in Tragedy of
Commons [1] that a system where peers work only for selfish
interests while breaking the rules decays to death. Policing
these networks is extremely difficult due to the decentralized
and ad hoc nature of these networks. Besides, P2P networks,
like the Internet, are physically spread across geographic
boundaries and hence are subject to variable laws.
The traditional mechanisms for generating trust and
protecting client-server networks cannot be used for pure1
P2P networks. This is because the trusted central
authority used in the traditional client-server networks
is absent in P2P networks. Introduction of a central
1. The network does not have any central server, global repository, or
global information.

. P. Dewan is with Intel Corporation, 119 NE Atlantic Pl., Hillsboro,
OR 97124. E-mail: prashant.dewan@gmail.com.
. P. Dasgupta is with the Department of Computer Science, Arizona State
University, Tempe, AZ 85287-8809.
Manuscript received 28 Nov. 2007; revised 31 Aug. 2008; accepted 23 Dec.
2008; published online 6 Feb. 2009.
Recommended for acceptance by S. Wang.
For information on obtaining reprints of this article, please send e-mail to:
tkde@computer.org, and reference IEEECS Log Number
TKDE-2007-04-0162.
Digital Object Identifier no. 10.1109/TKDE.2009.45.
1041-4347/10/$26.00 ß 2010 IEEE

trusted authority like a Certificate Authority (CA) can
reduce the difficulty of securing P2P networks. The major
disadvantage of the centralized approach is, if the central
authority turns malicious, the network will become
vulnerable. In the absence of any central authority,
repository, or global information, there is no silver bullet
for securing P2P networks.
In this paper, we investigate Reputation Systems for P2P
networks—a more ambitious approach to protect the P2P
network without using any central component, and thereby
harnessing the full benefits of the P2P network. The
reputations of the peers are used to determine whether a
peer is a malicious peer or a good peer. Once detected, the
malicious peers are ostracized from the network as the good
peers do not perform any transactions with the malicious
peers. Expulsion of malicious peers from the network
significantly reduces the volume of malicious activities.
All peers in the P2P network are identified by identity
certificates (aka identity). The reputation of a given peer is
attached to its identity. The identity certificates are
generated using self-certification, and all peers maintain
their own (and hence trusted) certificate authority which
issues the identity certificate(s) to the peer. Each peer owns
the reputation information pertaining to all its past
transactions2 with other peers in the network, and stores
it locally. A two-party cryptographic protocol not only
protects the reputation information from its owner, but also
facilitates secure exchange of reputation information between the two peers participating in a transaction.
The experiments show that the proposed reputation
infrastructure not only reduces the percentage of malicious
2. A transaction can be as simple as a transfer of a file from the source
(peer) to the destination (peer) or as complex as a securities transaction.
Published by the IEEE Computer Society

DEWAN AND DASGUPTA: P2P REPUTATION MANAGEMENT USING DISTRIBUTED IDENTITIES AND DECENTRALIZED RECOMMENDATION...

transactions in the network, but also generates significantly
less network traffic as compared to other reputation-based
security solutions for P2P networks.
The main contributions of this paper are:
1.
2.
3.

2

A self-certification-based identity system protected
by cryptographically blind identity mechanisms.
A light weight and simple reputation model.
An attack resistant cryptographic protocol for generation of authentic global reputation information of
a peer.

RELATED WORK

2.1 Structured and Unstructured P2P Networks
P2P networks can be categorized into structured and
unstructured P2P networks. The proposed system can be
used on top of both structured and unstructured P2P
networks. In structured networks, the location of the data is
a function of data itself or its metadata. As a result, the search
space is constrained by the metadata. The overlay networks
like Chord [2], CAN [3], and PASTRY [4] are structured
networks and as a result the search in these networks (Chord
is O (log N) where N is the number of nodes in the network) is
much more efficient than in purely unstructured (without
any super nodes) P2P networks. Moreover, in structured
networks, all the nodes know the fundamental structure of
the network and hence can prune their search to the relevant
nodes. The unstructured P2P networks do not have a wellknown architecture. In unstructured networks, there is no
relationship between the data or metadata and its location.
As a result search is of the order of O(N) in these networks,
where N is the number of nodes (each node will receive a
query message at least once).
The reputation schemes proposed in this paper are
independent of the structure of the network. It assumes that
a search function is available and does not put any
constraint on the implementation of the search function.
As a result, the proposed scheme is equally useful in both
the unstructured and the structured networks. The knowledge of the structure of the network can be used for
optimizing the algorithm. We do not assume any such
knowledge in this paper.
2.2 Distributed Systems Security
In this section, we review current work done for protecting
the users of distributed systems using distributed CAs. This
section is focused on distributed systems with one or more
central components.
2.2.1 Publius
Publius [4]3 is a monolithic system comprised of a set of
independently managed servers. It is censorship resistant
and allows a publisher to publish anonymously. It uses
cryptographic secret splitting techniques and divides the
secret among a set of servers. It provides anonymity to the
author of the content, as the servers hosting the content are
not aware of the content or the author.
3. Publius was named after the pen name used by a group of writers for
writing federalist papers in 1787-1788, and had an impact on the US
constitution.

1001

2.2.2 Groove
A commercial application, Groove [5], builds self-administering, context-sensitive, and synchronized share spaces for
exchanging files in fine granularity (small chunks). It
ensures the secrecy of shared spaces and authentication of
the members of a group. A good description of Groove is
provided in [5]. Groove uses shared spaces and assumes
that the members holding rights to a given share space
would (more often than not) trust each other.
2.2.3 SDSI
SDSI [6] is a Simple Distributed Security Infrastructure,
simplifies the X.509 certificates design and provides the
means for self-certification, local name spaces, secure
formation of groups, and simple access control mechanisms. It also provides methods to incorporate global name
spaces and globally trusted identities within the SDSI
infrastructure. In SDSI, the “authority” is distributed among
all members of the networks, which concurs with the
underlying principles of P2P networks.
2.2.4 Dynamic Trust Management
Dynamic Trust Management encapsulates trust management in dynamic distributed environments, where the
members of the system assume frequently changing multiple roles. In addition, the members themselves are
transitory. Agile Management of Dynamic Collaborations
[7], a project of the Stanford University, has developed
methods for identification of components, their authentication, secure group communication protocols, and dynamic
trust management.
2.2.5 RBAC
Role-Based Access Control was introduced in 1992 by
Ferraiolo and Kuhn [8]. RBAC associates permissions
with roles and not with users. As a result, a given user
needs to have the credentials of the role to be able to
obtain the authorization associated with the role. The
users are assigned roles in a many-to-many relationship,
i.e., one user can have many roles and vice versa.
Currently, there are four models of RBAC: core, hierarchical, constrained, and unified.
2.2.6 Cryptographic blinding
Cryptographic blinding was introduced by Chaum in 1983
[9]. Cryptographic blinding enables an authority to digitally
sign a document without seeing the content of the
document.
2.2.7 COCA
COCA [10] uses a set of distributed CA servers and
provides fault tolerance and mitigation against denial of
service attacks. COCA puts lose constraints on the communication channels among the servers and between the client
and the servers in order to improve deployability. It uses
byzantine quorum system to achieve availability.
2.3 Reputation Systems
Reputation systems have been used both in client-server
and P2P networks. The current state of art in reputation
systems for P2P networks can be classified into three main

1002

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

categories. The first two categories consist of the reputation
models and systems developed for the P2P networks. These
reputation systems exemplify the usefulness of a reputation
system and other related reputation metrics [11], for
mitigation of the impact of malicious nodes on P2P
networks. The third category consists of the reputation
systems developed for client-server systems.

2.3.1 Reputation Models
Resnick et al. [12] defines the reputation system as “a
system that collects, distributes, and aggregates feedback
about consumer’s past behavior.” The authors outline the
problems in eliciting, distributing, and aggregating feedback. Resnick et al. explain the problem of pseudospoofing in
[13]. Pseudospoofing is the use of multiple pseudonyms in a
system by the same real-life entity. The disadvantage is that
any entity can discard a handle or a pseudonym with which
a bad reputation is associated and join the system as a new
user, under a new pseudonym. This can possibly nullify the
usefulness of a reputation system, which assigns reputations to handles. The authors also advocate that the
newcomers should pay their dues in order to mitigate the
effect of pseudospoofing. In other words, the newcomers
should not only use the services of the system but should
also contribute to the system as per the system guidelines.
PeerTrust [14] allocates the reputation information to a
certain node on the network for storage, by using hash
functions. Any peer looking for the reputation of another
peer uses the search mechanism of the underlying network
to search for the information. The authors of PeerTrust
argue that trust models based solely on feedback from other
peers in the community are ineffective and inaccurate. The
authors recommend the “degree of satisfaction” of the peer
from previous transactions and the number of transactions
a peer performs in the system should be accounted for
before calculating the reputation of the recommended peer.
Abdul-Rahman and Hailes [15] have proposed another
trust model with corresponding metrics. They argue that
Bayesian probability may not be the best metric for
representing degree of trust, because probability is inherently transitive while trust is not. In addition, the authors
provide methods for combining recommendations and use
the context of recommendations and recommender weights
to evaluate the reputations from recommendations.
Aberer and Despotovic [16] have proposed completely
distributed solution for trust management over the P-Grid
peer-to-peer network. They store reputation data in the
form of a binary search tree, over the network. Any agent
looking for the recommendation data of another agent
searches the P2P network and computes the reputation
from the recommendations received. Chen and Singh [11]
and Schein et al. [17] also provide trust models, similar to
those mentioned above.
Dellarocas [18] has enumerated the design challenges in
the online reporting systems. Dellarocas surveys online
reputation, reporting mechanisms, and the corresponding
issues. In addition, the author provides a good overview of
recommendation repositories, professional rating sites,
collaborative filtering systems [19], and regression approaches. Dellarocas also enumerates the attacks on reputation systems and techniques for foiling those attacks. The

VOL. 22,

NO. 7, JULY 2010

attacks that can be inflicted on the reputation systems are
ballot stuffing and bad mouthing. In ballot stuffing, a peer
receives a large number of (false) positive recommendations
from its friends, to raise its own reputation. Bad mouthing
implies issuing a large number of negative recommendations for a specific peer. The author advocates that the
problems of negative and positive discrimination can be
solved by maintaining anonymity of requesters.

2.3.2 Reputation Infrastructure
P2PRep [20] developed by the security group of Università di Milano, is a reputation-based system developed
on top of Gnutella. In P2PRep, a requester finds a list of
possible providers, polls the current neighbors of the
tentative providers and takes votes from them on the
goodness of the provider. P2PRep is a highly communication intensive system due to its statelessness, i.e.,
peers do not store state (reputation info) of other peers.
P2PRep makes an implicit assumption that the neighbors
of a node will have the reputation information of the
node. This may or may not be true.
In RCert [21] reported by Ooi et al. and RCertPX reported
by Liau et al., the reputation data are stored with the
provider. The provider chains the recommendations together and signs it. A requester contacts the last requester of
the provider in order to obtain the time stamp of the chain.
The time stamp is used to validate the last recommendation
in the chain. The time stamp of the last recommendation is
stored by the last rater (requester). Having verified the last
time stamp, the new requester revokes the time stamp of
the last requester. RCertPX prevents the node from using
the older copies of RCert. This system is very near to the
proposed system but it is hinged on trusting the provider
and assumes unique identities for peers. Liu et al. [22] have
further developed the solution proposed in this paper. They
also use recommendation chains as suggested in this paper
but use third party witnesses who are trustworthy to both
the requester and the provider.
In [23], Zhou et al. propose a bloom filter approach for
fast reputation aggregation. They use gossip algorithms to
disseminate reputation information in a network. The
Gossip-based protocol is designed to tolerate dynamic
peer joining and departure, as well as to avoid possible
peer collusions.
2.3.3 Client-Server (Centralized) Reputation Systems
In the reputation systems based on the client-server model,
the server provides pseudonyms (identities) to users and
inducts them into the system. Once logged into the system,
a requester (client) selects a service provider (server) (from
other users) for a given service, based on the reputation of
the service provider. The requester then receives a service
from the provider. Once the transaction is complete, the
requester gives recommendation to the server based on its
satisfaction level from the transaction. Amazon, eBay, and
Monster follow the client-server-based reputation system.
Although the server forms a single point of failure, the users
(clients) trust the server to ensure the security and integrity
of the reputation data of users. Some of the other websites,
which use various kinds of reputation mechanisms, are
moviefinder.com, reel.com, and CDNOW.com.

DEWAN AND DASGUPTA: P2P REPUTATION MANAGEMENT USING DISTRIBUTED IDENTITIES AND DECENTRALIZED RECOMMENDATION...

Xu et al. have proposed a multilevel reputation scheme
[24] which is also dependent on a central reputation
computation engine. The authors attack the hard problem
of load balancing in a p2p system since peers with good
reputation get overloaded by requests. Their scheme
assigns trust values to contents and reputation values to
peers and allows peers at a certain reputation value to only
access content at same or less trust value thereby diminishing the load balancing problem in P2P networks. Gupta
et al. [25] talk about credit-only and credit-debit scheme.
Their system contains a Reputation Computation Agent for
computing and storing reputations. The reputation computation agent does form a single point of failure or attack.
Piatek et al. [26] showed that even one hop bidirectional
reputation propagation can improve the download times by
75 percent in a set of 100 peers where each peer follows a titfor-tat strategy.

2.4 Identity Management
We included this section in order to summarize the identity
management methods used by the current P2P networks,
reputation systems, and client-server networks. In P2P
networks, there is no way to ascertain the distinctness of a
peer in the absence of a central agency or without using
external means. This thesis has been named as the Sybil
attack and proved in [27] and has been reiterated in [6].
2.4.1 P2P Networks
In Gnutella, a peer is identified by its servent id that is a
function of its IP address. In the DHT-based systems, like
Chord, CAN, and Pastry [28], the peers are assigned
identifiers on the basis of their location in the network.
All the above systems predominantly use the peer
identifiers for locating content and not for evaluating
reputation or for enforcing security. The identifier allocation strategies in these networks are able to meet their
objectives. However, these allocation strategies are not apt
for a reputation-based system (on a P2P network) because
they do not restrict the number of identities a peer can
possibly generate.
2.4.2 Trusted-Third-Party Systems
Identity management for systems that have at least one
trusted third party has been widely researched [29], [30].
Most identity mechanisms for P2P networks depend on the
trusted third party for identity management. Not much
literature is available for identity management of systems
where the trusted third party is absent. Aberer and
coworkers have proposed an identity management system
[31] developed on the top of PGrid. In [31], the peers select
their own PKI-based identifiers and the network is used as a
repository for mapping the identifier to the current location
(IP address) of the peer.
2.4.3 PGP
PGP [32] is based on PKI and enables users to construct their
own web of trust without using any central trusted server.4 It is
based on a P2P model, where each certificate represents a
4. Although PGP servers are available now but they are more for
convenience than necessity.

1003

peer. It cannot be used for identity management in a
reputation system as it does not restrict its users from
generating large number of certificates (multiple identities).

2.4.4 IDEMIX
IBMs project Idemix [29] satisfies some of the needs of
anonymity and discretionary disclosure of credentials. It is
based on the principle that only the information that is
absolutely necessary for the execution of a transaction
should be disclosed to the other party. For example, if Alice
needs a rental car and has a driver’s license, then the rental
company only needs to verify that Alice possesses a driver’s
license. The rental company does not need to know the
name and address of Alice. This is true except in case that
Alice gets involved in an accident with a rental car or if she
tries to run away with the car. In that situation, the rental
company will need additional information about Alice.
Hence, a safety procedure is implemented by which the
rental company acquires the name and address of Alice in
certain untoward scenarios. Here, it is interesting to note
that in order to impose constraints on subjects enjoying
anonymous multiple identities, a backup procedure is
needed in certain situations to trace those identities to the
real-life subjects.

3

REPUTATION SYSTEM

3.1 Threat Model
A Gnutella-like network, has a power-law topology and
supports Insert and Search methods. The peers follow
predefined Join & Leave protocols. The peers are connected
with insecure communication channels. As the peers are
likely to have conflicting interests, a source of motivation is
needed to reduce the number of leechers. Leechers are peers
who derive benefit from the system without contributing to
the system. The rogue peers can also spread malware in the
network (when other peers download content from them).
Finally, peers need a mechanism to judge the quality of the
content before making Go/No-Go decision in transactions
and thereby develop trust relationships with other peers.
A perfect reputation system can provide the means to
achieve the above goals. Any reputation system is vulnerable to ballot stuffing and bad mouthing as described in
[18]. An imperfect reputation system by itself generates
vulnerabilities that can be exploited by attackers. Peers need
to have a unique handle to which their reputations can be
attached. In the absence of any trusted central agency, an
attacker can gather infinite identities and start issuing
recommendations to itself. A peer might modify the
reputation data stored in the network to maliciously raise
its own reputation. Finally, there are other vulnerabilities
that come in the picture depending on how a given
reputation system is designed. We explain those vulnerabilities and their corresponding mitigation in the sections
where we enumerate the design decisions.
3.2 Self-Certification
In order to participate in the reputation system, a peer
needs to have a handle. The reputation of a peer is
associated with its handle. This handle is commonly termed
as the “identity” of the peer even though it may not

1004

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

“identify” a peer, i.e., it may not lead to the real-life identity
of the peer. A peer receives a recommendation for each
transaction performed by it, and all of its recommendations
are accumulated together for calculation of the reputation of
a given peer.
In a centralized system, a trusted authority would have
issued these identity certificates. In a decentralized reputation system, self-certification [33] splits the trusted entity
among the peers and enables them to generate their own
identities. Each peer runs its own CA that issues the identity
certificate(s) to the peer. All the certificates used in selfcertification are similar to SDSI certificates [6]. The reputation of a peer is associated with its identity and the reputation
of a CA is the accumulated reputation of the identities.
Self-certification obviates the centralized trusted entity
needed for issuing identities in a centralized system. Peers
using self-certified identities remain pseudononymous in
the system as there is no way to map the identity of a peer
in the system to its real-life identity. Although anonymity or
at least pseudonymity is extremely desirable in P2P networks, in a reputation system it is a double edge sword. In
the absence of any mapping between multiple identities
and their owner (peer), the system is vulnerable to Sybil
attack or Liar farms.
A malicious peer can use self-certification to generate a
large number of identities and thereby raise the reputation
of one of its identities by performing false transactions with
other identities. The malicious peer does not even need to
collude with other distinct peers to raise its reputation, but
only needs to generate a set of identities for itself. Such a
large set of identities managed by one peer is called an
identity farm. The set of identities that issue false recommendations is called a liar farm. This attack belongs to the
class of attacks termed Sybil attacks. In simple words, a peer
having an identity farm is equally capable of subverting a
reputation system as a peer that has colluded with a large
number of other peers.
An identity farm can be countered if, either a peer is
restricted to one identity or all the identities of a peer can be
mapped back to the peer. A peer can be restricted to one
identity by mapping its identity to its real-life identity and
thereby sacrificing anonymity, or by making the identity
generation extremely resource intensive such that the peer
cannot afford to generate multiple identities. Identity
generation can be made resource intensive by using
traditional micropayment methods, although the resource
restrictions are likely to have a varied impact depending on
each peer’s resourcefulness.
In self-certification, we use a combination of both
approaches. Each peer’s CA can generate multiple identities.
The recommendations received for a peer’s identity from
different identities of other peers, signed by the other peer’s
CA(s), are identified as signed by the same CA, and are
averaged to counter the liar farms. In a transaction, the
requester averages all the recommendations of the provider
by CAs of the provider’s past recommenders. Hence, all the
past recommendations owned by the provider carry equal
weight but they get averaged. Finally, it adds the averages of
each CA to calculate the reputation of the provider identity.

VOL. 22,

NO. 7, JULY 2010

Hence, a peer cannot use its own identities (all generated by
the same CA) to recommend its other identities.
A more determined malicious peer might start multiple
CAs and generate multiple groups of identities. In order to
counter a rogue peer having multiple CAs, the peers are
divided into groups based on different criteria such that a
peer cannot become a part of multiple groups. For example,
a P2P network installed in a school classifies the peers by
different departments, a P2P network installed in a city
classifies the peers by their zip codes. Each peer obtains its
group certificate from the appropriate authority and
attaches it to its CA. The certificate of a group authority
are publicly accessible by any node inside or outside the
group. The peer sends its blinded credentials to the group
authority and the authority verifies the credentials and
signs the group certificate. The authority remains stateless,
i.e., it does not maintain any information to correlate a
certificate with the peer.
Unlike the traditional CA or distributed CA-based
approaches, grouping of peers preserves the anonymity of
the peers; when combined with self-certification it curtails
the possibility of a Sybil attack. In contrast to the traditional
CA-based approach, neither the group authority nor the
transacting peers can establish the identity of the peer. In
addition, certificate revocations are not needed in the
group-based approach as the group authority only vouches
for the real-life existence of the peer, unlike the traditional
certificate-based approaches where various certificate attributes are attested by the authority and necessitate revocation if any of those attributes mutate in time. If a highly
reputed identity is compromised, its misuse would be selfdestructive as its reputation will go down if misused.
The peer is denoted by P while the authority is denoted
by A. Here P !A : X denotes that the peer (P) sends a
message X to the authority (A). The symbol PK2 represents
the private key of the peer P and PK1 represents the public
key of the peer P. EK ðÞ represents encryption of the phrase
ðÞ with key K, while EBK ðXÞ represents blinding phrase X
with key K.
P!A: B1 ¼ fEBKa ðIAlicer Þg; IAlice
The peer Alice generates a BLINDING KEY, Ka and
another identity for herself (IAlicer ). Alice cannot be
identified from her identity (IAlicer ). Subsequently,
she blinds her identity (IAlicer ) with the blinding key
Ka. B1 represents the blinded identity. Alice sends
B1 to the authority with her real identity that proves
her membership to a group.
2. A! P: B2 ¼ EPAuthority K2 fB1 ¼ EBKa ðIAlicer Þg
The authority signs the blinded identity, B1 and
sends it (B2) back to the peer.
3. P: EPAuthority K2 fIAlicer g ¼ fEBKa fB2gg
The peer unblinds the signed identity and extracts
the identity authorized by the authority
EPAuthority K2 fIAlicer g.
The fundamental assumption in the group-based approach is that in a P2P network, peers would be more
interested in the ranks of the prospective providers than in
the absolute value of the reputations. The simulations show
that this approach changes the absolute reputations of peers
considerably but it has only a minimal impact on the
1.

DEWAN AND DASGUPTA: P2P REPUTATION MANAGEMENT USING DISTRIBUTED IDENTITIES AND DECENTRALIZED RECOMMENDATION...

relative ranks of the peers. This approach is inspired from
the Google page rank concept in which the pages in
proximity of each other do not contribute as much to the
page rank of the target page as compared to pages at a
distance [34]. The relative ranks do not stop the peers from
setting thresholds. The thresholds can be based on ranks.
Setting thresholds based on absolute values has very
limited utility. Google uses ranks rather than the absolute
numbers of links pointing to/from pages. It is well evident
from the Google example that rank-based mechanisms are
scalable. It can be argued that there might be some systems
where absolute values are needed. This paper does not
consider that case as use of absolute values needs more
system context specific information that is outside the focus
of this paper.
It can be argued that this approach is unfair to peers
whose genuine recommendations come from peers that are
a part of a large group. We agree with the argument and our
experiments show that the relative ranks of the providers
change only minimally. Hence, the providers are only
slightly effected ( Mean Rank Difference  14 for varied
sizes groups) by the grouping of recommendations. The
requesters that give the recommendation to the providers
are not affected by the grouping of recommendations.

3.3 Reputation Model
Once a peer has obtained its identity, it joins the P2P network
using the standard Join method of the particular P2P
network. The peer (requester) searches for one or more files
using the Search method provided by the network. On the
basis of the responses received, as a result of its search
request, the requester generates a list of peers who have the
requested file(s). The number of peers who offer a particular
file is denoted by RANGE. The requester selects the peer
(provider) with the highest reputation from the list and
initiates the cryptographic protocol. The cryptographic
protocol is presented in detail in the next section. In the
protocol, the requester uses the Download method of the
network, to download the file from the provider. Subsequently, it verifies the integrity, authenticity, and the quality
of the file. Depending on its verification results, it sends a
recommendation between MIN_RECOMMENDATION and
MAX_RECOMMENDATION to the provider. The recommendations are constrained to boundaries in order to make sure
that one recommendation does not completely nullify or
drastically improve the reputation of a provider. Once the
provider receives the recommendation, it averages the
previous recommendations received by it and the recent
recommendation to calculate its reputation. The abovementioned steps are repeated for every transaction.
There is a big body of work in Decision Theory, Game
Theory, and Probability [35], [36], [37] which can be used
for selecting appropriate values of the above-mentioned
parameters and the definition of function F() depending on
the levels of the threat faced by the peers in the P2P
network. In this paper, we define the function F() as the
arithmetic average of the recommendations received by the
provider. The proposed reputation model is independent of
the topology of the P2P network, addressing schemes for its
nodes, bootstrap mechanisms, joining and leaving protocols
of peers, and the name service. In other words, the choice of

1005

any of these components has no impact on the reputation
model and vice versa.
If the system allows the peers to issue both positive and
negative recommendations to other peers, some peers
might get victimized by bad mouthing, i.e., a requester
can potentially issue a negative recommendation to the
provider even if the provider deserved a positive recommendation for a given transaction. On the other hand, if
only positive recommendations are allowed then it would
be hard to differentiate a relatively new peer from a chronic
bad peer. Therefore, here we make an assumption that both
positive and negative recommendations are allowed and a
given peer will stop interacting with peers who regularly
issue negative recommendations.

3.4 Reputation Exchange Protocol
Once the requester has selected the provider with the
highest reputation, it initiates the reputation exchange
protocol with the provider. In the reputation exchange
protocol, the requester is denoted by R while the provider is
denoted by P. Here R!P : X denotes that the requester
(R) sends a message X to the provider (P). The symbol PK2
represents the private key of the peer P and PK1 represents
the public key of the peer P. EK ðÞ represents encryption of
the phrase ðÞ with key K, while EBK ðXÞ represents
blinding phrase X with key K. HðÞ denotes a one way hash
of the value . This protocol only assumes that insert &
search functions are available and are not resilient to peers
that may not follow the recommended join & leave protocol
of the network. The steps in the reputation exchange
protocol are as follows:
Step 1: R!P : RTS & IDR
The requester sends a REQUEST FOR TRANSACTION
(RTS) and its own IDENTITY CERTIFICATE (IDR) to the
provider. The provider needs the identity certificate of the
requester as the provider has to show it to the future
requesters in Step 7.
Step 2: P !R : IDP & TID & EPK2 ðHðT ID k RT SÞÞ
The provider sends its own IDENTITY CERTIFICATE
(IDP), the CURRENT TRANSACTION ID (TID) and the
signed TID, EPK2 ðHðTIDkRTSÞ. The signed TID is needed
to ensure that the provider does not use the same
transaction id again. In the end of the protocol, this signed
TID is signed by the requester also and stored into the
network where it will be accessible to other peers.
Step 3: R : LT ID ¼ Max ðSearch ðP K1 k T IDÞ)
The requester obtains the value of the LAST TRANSACTION ID (LTID) that was used by the provider, from the
network. The requester concatenates the public key of the
provider with the string TID and performs the search. Any
peer having the TID for the provider replies back with the
TID and the requester selects the highest TID out of all the
TIDs received. The highest TID becomes the LTID. It is
possible that the provider might collude with the peer who
stores its last LTID and change the LTID. As the LTID and
related information would be signed by the requester, the
provider cannot play foul.
Step 4: R : IF ðLT ID  T IDÞ GO TO Step 12
If the value of the LTID found by the requester from the
network is greater than or same as the TID offered by the
provider, it implies that the provider has used the TID in

1006

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

some other transaction. Hence, it is trying to get another
recommendation for the same transaction number (TID).
The requester suspects foul play and jumps to Step 12.
Step 5: R!P : Past Recommendation Request & r
If the check in Step 4 succeeds, i.e., the requester is sure
that the provider is not using the same transaction number,
it requests the provider for its previous recommendations.
In other words, if the current transaction is the Nth
transaction for the provider, the requester makes a request
for N  1th; N  2th and so on recommendations till
N  rth recommendation where r is less than N. The value
of r is decided by the requester and it is directly
proportional to the requester’s stake in the transaction.
Step 6: P !R: CHAIN, EPK2 (CHAIN)
CHAIN ¼ ðfRECN1 kEZN1K2 ðHðRECN1 Þgk
fRECN2 kEZN2 K2 ðHðRECN2 ; RECN1 ÞÞgk
fRECN3 kEZN3 K2 ðHðRECN3 ; RECN2 ÞÞgk . . . .
fRECN4 kEZN4 K2 ðHðRECNr ; RECNr1 ÞÞgÞ
The provider sends its past recommendations
ðRECN1 ; RECN2 . . . RECN3 Þ which were provided by
peers ðZN1 ; ZN2 ; . . . ZN3 Þ. The provider signs the CHAIN
so that the requester can hold the provider accountable for
the chain. As the recommendations have been signed by the
previous requesters, the provider could not have maliciously
changed them. If the requester (say Zl ) has signed both the
(lth) and the previous (l  1th) recommendation using its
private key ZK2 , as EZnK2 ðHðRECN3 kRECNðl1Þ ÞÞ, there is
no way a provider can modify the CHAIN. In other words,
the provider cannot simply take away a bad recommendation and put in a good recommendation in order to increase
its reputation.
Step 7:
R : Result ¼ V erifyðRECN1 ; RECN2 . . . RECNr Þ
If Result !¼ V erified GO TO STEP 12
The requester verifies the CHAIN by simple public key
cryptography. If it has the certificates of all the peers with
whom the provider has interacted in the past, the verification is simple. In the case it does not have the required
certificates; it obtains the certificates from the provider
itself. The provider had obtained its requester’s certificate in
Step 1. In addition, the requester checks for liar farms as
mentioned in paragraph 2 of Section 3.2. If the verification
fails the requester jumps to Step 12.
Step 8: P !R : F ile or Service
The provider provides the service or the file as per the
requirement mentioned during the search performed for
the providers.
Step 9:
R !P : B1 ¼ EBKa ðRECk TID kERK2 fHðREC; kTIDÞgÞ
Once the requester has received a service, it generates a
BLINDING KEY, Ka. The requester concatenates the RECOMMENDATION (REC) and the TRANSACTION ID (TID) it
had received in Step 2 and signs it. Subsequently, it blinds
the signed recommendation with the blinding key, Ka. The
recommendation is blinded in order to make the provider
commit to the recommendation received before it sees the
value of the recommendation such that it does not disown
the recommendation if it is low. The provider receives the

VOL. 22,

NO. 7, JULY 2010

blinded recommendation from the requester. The blinded
recommendation is also signed by the requester. The
blinded recommendation contains the Chain that the
provider can subsequently use to validate its reputation to
another requester.
Step 10:
a. P !R : B1kEPK2 ðHðB1Þ; nonceÞ; nonce
b. R!P : Ka
The provider cannot see the recommendation but it signs
the recommendation and sends the NONCE and the signed
recommendation back to the requester. The requester
verifies the signature and then sends the blinding key Ka
to the provider which can unblind the string received in
Step 10a and checks its recommendation.
Step 11: Insert
ðIDR; fREC k TID k ERK2 fHðRECÞ k HðTIDÞggÞ
The requester signs: the recommendation that was
given to the provider (REC), the transaction id (TID), and
its own identity certificate and stores it in the network
using the Insert method of the P2P network. This completes
the transaction.
Step 12: Step 12 explains the steps a requester executes
when it expects foul play:
ABORT PROTOCOL
R: InsertðIDR; fCHAIN k T ID k ERK2 fHðCHAINÞk
HðT IDÞggÞ
If the verification in Step 7 fails, the requester takes the
CHAIN that was signed by the provider and the Transaction Id (TID), signs it and uses the INSERT method of the
network to insert the chain and its own identity certificate
into the network. As a result, any subsequent requester
will be able to see failed verification attempt and will
assume a MIN RECOMMENDATION recommendation for
that TID for the provider. The requester cannot insert fake
recommendations into the network because it has to
include the TID signed by the provider. If the requester
reaches Step 12 from Step 4. It will request for the Chain
from the Provider and subsequently will perform R :
InsertðIDR; fCHAINkT ID k ENRK2 fHðT ID k RT SÞÞggÞ.

3.5 Analysis of the Protocol
The requester needs to initiate only one search request in
the network in order to collect the recommendations
received by the provider in the past. This protocol
handles the problem of irregular availability of the peers
in the network, which is one of the major problems in
P2P networks.
1.

The provider will not (intentionally) send the wrong TID
in Step 2. Let the id that the provider sends be TID0
and the last Transaction Id for the provider be LTID.
The TID0 should always be equal to LTID þ 1. If
TID0 > LTID þ 1, then there will be unexplained
missing recommendations. If TID0 < LTID þ 1, then
the provider will be caught in Step 4 of the protocol,
as the last id used by the provider was made a public
information (by the previous requester) and is
available to all the peers now. If a peer is taking

DEWAN AND DASGUPTA: P2P REPUTATION MANAGEMENT USING DISTRIBUTED IDENTITIES AND DECENTRALIZED RECOMMENDATION...

2.

3.

4.

the role of a provider for the first time, then the TID
will be 0.
The provider will not abort the transaction in Step 8. It is
possible for the provider to abort the transaction
after giving the requester the requested information
or file in Step 8. It is also possible for requester to
abort the transaction after Step 9. In both scenarios,
the provider will not have a recommendation for the
transaction id, TID. If the provider does not sign the
blinded recommendation that the requester sent her,
the requester can release the recommendation in
Step 11 without obtaining provider’s signature.
In its next transaction TID þ 1, the provider will
not be able to show the recommendation for
transaction, TID to the requester of transaction,
TID þ 1. Therefore, the new requester will search
the network using the Search method for TID. If it
finds TID, it will also find the recommendation
provided to the provider in the transaction. The
requester will be accountable because the TID was
signed by the provider. The provider will have to
accept the recommendation because it will also
include the signature of the provider, TID & EPK2
(H (TID)). If the new requester does not find the
recommendation, then it can believe the provider
and provide him minimal recommendation for the
transaction, TID.
If the provider returns the signed blinded
recommendation in Step 10, B1 & EPK2 (H (B1)),
but the requester does not send the key, Ka and
jumps to Step 10 without performing the intermediate steps, then the provider can search the
network and get the signed recommendation of the
requester. If the requester never performs Step 10,
then in the next transaction, TID þ 1, the new
requester will search for LTID and he will not find
it. Hence, the transaction TID can be considered as
aborted and the next transaction can be done with
transaction id, TID.
Collusion by rogues or liar farms. All reputation
systems are susceptible to collusion due to the
nature of a reputation system. Two or more rogues
might collude in order to increase each others
reputation. It has been shown in [38], [39] that when
a large number of autonomous entities have to
collude to do anything malicious then such an event
is highly unlikely. The impact of collusion can be
mitigated by categorizing recommendations by
identities, by authorizing agencies, by time, etc.,
and identifying the outliers. The list of colluders can
be published, thereby, protecting other peers from
the attack. Peers have a motivation against collusion
because once identified they will not be able to
participate in the network. The chain of recommendations of the colluders will provide the evidence
that certain peers are colluding, thereby, preventing
rogue peers from incriminating good peers.
Multiple requesters and concurrency. In the current
protocol, a provider will not be able to use the same
identity in concurrent transactions. The first option

1007

for protocol extension is that the providers introduces all its requesters to each other. Subsequently,
the verification in Step 4 is done among the group
of requesters and the results are adjusted in order
to incorporate TID difference due to multiple
requesters. After incorporating the extensions, it
would still be a biparty protocol where the provider
is the first party and the group of requesters is the
second party.

3.6 Salient Features of the Protocol
The main features of the protocol are as follows:
1.

2.

3.

4.

5.

Legitimate global reputation information w.r.t. a
given provider is available to all peers at one place
(with the provider itself). The requester does not
have to initiate multiple search requests in the
network in order to collect the recommendations
received by the provider in the past. It only has to
issue one search request to retrieve the last transaction information of the provider and it can verify all
the recommendations of the provider. This not only
reduces the turnaround time of the transaction but
also saves considerable volume of resources.
The provider is accountable for all its past
transactions. It cannot maliciously meddle with
its transaction history by adding or deleting any
recommendation because the recommendations are
chained in a sequence and signed by the past
requesters. The provider cannot change any recommendation because they are digitally signed by
the requesters.
As the global information of the provider is stored by
the provider itself, this protocol is not affected by
erratic availability of past recommenders or any other
peer in the network. As long as the requester and the
provider are connected to the network, the transaction can be completed fruitfully. Even if one of them
leaves the network it can come back and complete the
transaction. In short, it handles the problem of irregular
availability of the peers in the network, which is one of the
major problems in P2P networks.
The requester cannot (gainfully) maliciously abort
the transaction in the middle. In other words, the
requester cannot take the service from the provider
and then logoff without giving a recommendation to
the provider. If it does so it would be equivalent of
giving MAX_RECOMMENDATION to the provider.
The provider can take advantage of this by generating multiple premature disconnections. This attack
does not work because then the future requester will
find the signed TID which will counter the provider’s claim that it was not issued a recommendation.
The requester cannot harm the provider’s reputation
by logging off the network abruptly and cannot get
any benefit for itself by this action.
This protocol cannot stop a requester from giving a
“bad” recommendation to the provider even if the
latter provides a legitimate file. This protocol does
not stop bad mouthing nor does it prevent ballot
stuffing [19]. The proposed scheme necessitates

1008

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

collusion among a large number of peers for ballot
stuffing or bad mouthing to work, and hence it is
unlikely to be successful in the proposed system
although it is not impossible.

4

EXPERIMENTS AND RESULTS

4.1 Evaluation of Self-Certification
We simulated a peer-to-peer network to understand the
effect of the Network Size (N), the Number of Transactions
(T), and the Group Size (d) on the Mean Rank Difference
(M) over all the peers in the network. The Mean Rank
Difference (M) is calculated by averaging the rank difference of all the peers in the network for one instance of the
simulation. The rank difference for a peer is the difference
in the rank of the peer when the proposed identity
mechanisms are used, and when it is not used. Specifically,
we tried to answer the following two questions:
Is the mean rank difference a good predictor of the
rank difference of individual nodes? What is the
variation in the rank difference of individual peers in
the simulation for a given value of d? Mathematically, what percentage of nodes has their rank
difference equal to the mean rank difference?
2. Does the network size (N), group size (d), or the
number of transactions (T) in the network impact the
mean rank difference in the network? In other
words, what is the expected mean rank difference
for other network configurations which are different
(in terms of size, group size d, or number of
transactions) than the networks simulated by us?
The simulated network consisted of 1,000 peers in which
the peers performed 20,000 transactions per instance simulation, at a group size d ¼ 3. The peers were assigned unique IP
addresses. Additionally, each peer was assigned a goodness
factor to account for the fact that a good peer is likely to
participate in higher number of transactions (as a provider)
than a bad peer. Hence, the reputation of a good peer is likely
to escalate faster than the reputation of a bad peer.
MAX RECOMMENDATION was set to 1 while the value
of MIN RECOMMENDATION was set to 2. This was done
to ensure that a peer lost more reputation on performing a
malicious transaction as compared to the reputation gained
by doing a good transaction. In addition, the results did not
vary much when the values of MAX RECOMMENDATION
and MIN RECOMMENDATION were selected to be outside
the set ½2; 1. The percentage of malicious peers was varied
from 10 percent to 90 percent. The probability that a peer
would cheat was set to 1/2 in order to account for the fact
that in the real world honesty is not constant and varies with
time and stakes.
For each iteration of the simulation, a randomly selected
peer became the provider and another randomly selected
peer assumed the role of a requester. After the transaction, the
requester gave a recommendation to the provider. For each
recommendation received by the provider, its reputation was
incremented by its goodness factor. After 20,000 transactions,
the ranks of the peers were calculated without using the
proposed identity management mechanism (d ¼ 3). The
1.

VOL. 22,

NO. 7, JULY 2010

differences of the ranks were averaged for all peers in the
network and the results were statistically analyzed.
The above steps were repeated 20 times and results
averaged, for each of the following three scenarios:
The group size was varied from 5 to 50 (Step5 5) and
the other parameters were constant. This case
enabled us to investigate the impact of averaging
on mean ranks of peers,
2. The group size and the network size were kept
constant and varied the number of transactions from
2,000 to 20,000 (step 2,000), and
3. The number of transactions and the group size were
kept constant to 20,000 and 10, respectively, and the
network size was varied from 200 to 1,000 peers
(step 200).
Finally, we calculated the mean rank difference for each set of
20 simulations and statistically analyzed using regression
analysis, T-Test, and Analysis of Variance test (ANOVA) [40]
to deduce the answers for the above two questions.
1.

4.2 Self-Certification Results and Analysis
In the first experiment (N ¼ 1;000, T ¼ 20 K and d ¼ 3), the
rank difference averaged across the peers was 13:246  0:81
with a 95 percent confidence level. Although the result of
the first experiment (Fig. 1) does not resemble a normal
distribution, we decided to treat it as normally distributed
data as per the recommendations of the central limit
theorem [41] for a sample size larger than 30. We inferred
that 68 percent of the nodes in a P2P network will have a
rank difference in the range of 13:246  13:07. In other
words, 680 nodes for every 1,000 nodes will have a rank
difference of less than 27. Besides the standard error for this
analysis was very low (0:4), hence the Mean Rank
Difference gives a reasonably good picture of the rank
differences of the individual nodes.
In order to answer the second question, we did an
incremental regression analysis and an ANOVA test to
ascertain how do the variation in network size, number of
transactions, and group size change the mean rank difference.
Heuristically, we expected that the mean rank difference
should change with a change in the group size but it should
remain invariant when either of the other two parameters is
varied. Our null hypothesis (H0 ) was that none of the three
factors had any impact on the mean rank difference. The
following regression equation quantifies the impact of the
variability of the three factors on mean rank difference:
Mean Rank Difference ¼  þ 1  d þ 2  N þ 3  T :
The Significance F value for the ANOVA test was less
than zero. Hence, the null hypothesis (H0 ) was proved to be
untrue and had to be rejected. In other words, the group
size, network size, and transactions had an effect on the
mean rank difference and the coefficients, 1 ; 2 , and 3
could not be zero. This inference was substantiated by the
fact that the p values for the coefficients, 1 ; 2 , and 3 were
extremely low ( 1080 ). Hence, all the three parameters
had an impact on the value of the mean rank difference.
This result was contrary to our heuristic.
5. Step X: The parameter is incremented by X.

DEWAN AND DASGUPTA: P2P REPUTATION MANAGEMENT USING DISTRIBUTED IDENTITIES AND DECENTRALIZED RECOMMENDATION...

1009

Fig. 1. Variation in mean rank difference. (a) d ¼ 3. (b) Goodness ¼ 1.

Fig. 2. Variation in mean rank difference (line fit plots). (a) Transactions d ¼ 3. (b) Group size. (c) Network size.

Interestingly, the values of the coefficients, 1 ¼ 0:52 
0:02; 2 ¼ 0:02; 3 ¼ 0:0008, and  ¼ 19:5  0:98 at the
95 percent confidence level, were in partial conformance
to our heuristic. The value of 1 (Fig. 2) was the highest
among the three coefficients; hence, as expected, a small
variation in the group size had the highest impact among
the three factors on the mean rank difference. The increase
in the number of transactions in the network had a minimal
impact (2 ¼ 0:02) on the mean rank difference (Fig. 2). This
is ascribed to the fact that with every additional transaction,
with a peer in a new6 security zone, the probability of the
fact that the next transaction of the provider will again be
with a peer in a newer security zone becomes lesser and
lesser. In addition, with every transaction of the provider
with a peer in the old security zone, the mean rank
difference increased. Hence, the reputation calculated with
the proposed identity mechanism did not increase while the
information calculated without it increased by a very small
factor. Therefore, the mean rank difference increased.
The value of 2 ¼ 0:02 was against our initial hypothesis
that the network size should not change the mean rank
difference (Fig. 2). In order to find out why the network size
was changing the mean rank difference, we had to perform
an additional experiment. In this experiment, we made an
6. None of the peers belonging to that security zone has been a requester
for this provider.

assumption that irrespective of the reputation, the probability of a node being involved in a transaction is the same.
In other words, the goodness factor of all the nodes was set
to 1 and a bad node and a good node had an equal
likelihood of participating in a given transaction. This was
different from the assumption made in the earlier experiments where a good node had a higher goodness factor,
thereby, accounting for involvement in a larger number of
transactions. The result of this experiment showed that
there was little or no variation in the mean rank difference
w.r.t. the network size, when the likelihood of participation
of all the nodes was equal. The correlation coefficient (r)
was 0.09247 as is visible in Fig. 1. Hence, we inferred that in
the first set of experiments, the increase in the network size
raised the number of highly reputed nodes or the nodes
with a high value of goodness factor. The reputation
difference between the instances, when the proposed
identity mechanism was used and when it was not used,
was higher for the higher ranked (highly reputed) nodes as
compared to the lower ranked nodes; hence, the change in
the rank for higher rank nodes was larger. As a result, the
mean rank difference for the network was higher.

4.3

Evaluation of the Cryptographic Protocol and
the Reputation Model
The protocol was evaluated with a simulating 5,000 peers
participating in 20,000-140,000 transactions. The RANGE

1010

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

VOL. 22,

NO. 7, JULY 2010

Fig. 3. Variation in total number of malicious transactions. (a) When reputations are used d ¼ 3. (b) Variations in number of rogues.

variable was set to 10. In other words, it was assumed that
each file was available with 10 possible providers. The
requesters did not have any a priori knowledge of rogue
nodes. The rogue nodes performed maliciously with a
probability of 1/2, i.e., rogues cheated in one out of every
two transactions.7 Each simulation was executed five times
and the results were averaged. The number of repetitions
was set to 5 because after the fifth iteration of the
simulation, the average values were more or less constant.

4.3.1 Cumulative and Individual Benefits of Using
Reputation
We wanted to quantify the holistic and individualistic
benefits of using the proposed reputation model for a P2P
network. We measured the change in the number of
malicious transactions with an increase in the total number
of transactions in the network. The number of rogues was
set to a constant at 50 percent and the number of
transactions was incrementally raised from 20,000 to
140,000. As is visible in Fig. 3, the total number of malicious
transactions increased considerably with an increase in the
number of transactions when the proposed model was not
used but are more or less constant when the proposed
model was used. In the presence of an increasing number of
rogues (10-90 percent), when the total number of transactions is constant (¼ 140;000), the rate of increase in the
number of malicious transactions was much less when
reputations were used (Fig. 3).
Subsequently, we analyzed the experience of each peer
when the reputations are not used as compared to when
they are used. As visible in Fig. 4, when the reputations
were not used, the mean of the number of malicious
transactions experienced by each good node was 7:966 
5:52 with a 95 percent confidence. This mean drastically
reduced, when the reputation model is used, to 0:4  1:2
with a 95 percent confidence. From the first three simulations, we concluded that the proposed model reduces the
number of malicious transaction from the perspective of the
network and from the perspective of each peer.
Last, the performance of the proposed system was
compared with Eigen Trust reported by Kamvar et al.
[42]. In order to replicate the simulations performed by
7. The probability was set to 1/2 because if a rogue cheats in all its
transactions then it would be identified quickly so a good strategy for the
rogue is to cheat in alternate transactions.

Kamvar et al., we set the number of peers to 100 with
40 malicious peers and 60 good peers. The number of
transactions was set to 1,500. We did not consider the other
parameters mentioned by Kamvar et al. because their
strategy encapsulates the search function also but the
proposed system is used on top of any search function.
The results have been illustrated in Fig. 5. The proposed
system is more effective in reducing the number of
“inauthentic downloads” or “malicious transactions.” In a
network where 60 percent nodes are malicious, the
proposed system reduces the number of malicious transactions from 15 percent (in Eigen Trust) to 2 percent. Fig. 5
also shows that with an increasing number of malicious
nodes in the network, the proposed system increasingly
becomes more effective.

4.4 Overall Evaluation of the System
In order to evaluate the combined benefit of self-certification, the cryptographic protocol, we modified the experiments done for the evaluation of the cryptographic
protocol, and added an availability factor, AF, to each
node. The availability factor accounts for the erratic
availability of the past recommenders of a given peer. AF
values from 50 percent to 90 percent were randomly
allocated to peers. The number of malicious transactions
were counted and compared with the results obtained in
Section 4.3. As is visible in Fig. 6, the number of malicious
transactions in the system are the same when the protocol is
used separately and when used along with self-certification.

5

DISCUSSION

5.1 Network Traffic
We compared the cryptographic protocol with P2PRep [43].
Bandwidth consumption is the Achilles heel of any peer-topeer network. The proposed protocol generates less network
traffic than the P2PRep voting protocol. In both, P2Prep and
the proposed protocol, the requester maintains a list of
possible information providers when the discovery protocol
finishes. P2PRep is a stateless system while the proposed
system is stateful. P2PRep is highly communication intensive
because of its statelessness. In phase 2 of P2PRep, the initiator
polls the peers in the network for a vote on the provider.
Subsequently, each peer sends a message containing a vote to
the initiator. As a result, the amount of traffic increases

DEWAN AND DASGUPTA: P2P REPUTATION MANAGEMENT USING DISTRIBUTED IDENTITIES AND DECENTRALIZED RECOMMENDATION...

1011

Fig. 5. Number of malicious transactions (proposed system versus
Eigen Trust).

5.2 More Relevant Information
In P2PRep, a requester polls peers, which are topologically
closer to the provider. This strategy may not return a high
percentage of relevant recommendations. In a large network, it is highly likely that only a fraction of the peers have
had any previous transaction with the possible provider.
Topological proximity does not increase or decrease the
probability of a transaction between two peers. In other
words, the fact that two peers are neighbors does not
necessitate the fact that either of the two peers can vouch for
the trustworthiness of the other peer. Unlike in P2PRep, a
requester in the proposed system can make sure that it
retrieves the complete past history of the provider. Hence, a
requester using the proposed protocol obtains a higher
volume of relevant information than a requester in P2PRep.
5.3 Low Stake Fast Transaction
In the proposed protocol, it is recommended for the
requester to search the network for LTID but it is not
mandatory. The requester does not have to ever disclose the

Fig. 4. Variation in total number of malicious transactions. (a) Number of
peers versus malicious transactions d ¼ 3. (b) Mean rank difference
versus number of connections. (c) Mean rank difference versus number
of transactions.

linearly as the number of peers who reply to the initiator
increase. On the other hand, the proposed system uses
minimal communication as the security of system lies in the
reputation values stored by the provider itself.

Fig. 6. Number of malicious transactions when self-certification, identity
management, and the cryptographic protocol are combined.

1012

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

LTID in the elicitation-storage protocol. In other words,
even if the peer who wants to perform a transaction does
not know the LTID, it can still perform the transaction. For
example, if a peer only wants a “standards file,” which is
commonly available, it does not have to search for LTID and
still the probability of the provider cheating is low. In
P2PRep, the vote polling cannot be bypassed because the
provider also receives the poll request. Hence, if the
provider does not receive any poll request, it will know
that the requester has not performed any security checks.
Hence, the probability of the provider cheating is high.

5.4 Stale LTID
It is possible (though unlikely) that the requester might
never be able to retrieve the correct LTID from the network
or it retrieves a stale LTID. If the LTID retrieved by the
requester is less than the one offered by the provider then
the requester can simply go ahead with the transaction
because then it will be the responsibility of the provider to
explain the transactions of the missing LTID. If the LTID
retrieved by the requester is greater than the one offered by
the provider, then the provider is surely trying to cheat. The
requester can abort the transaction.

[2]

[3]
[4]

[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]

6

CONCLUSIONS and FUTURE WORK

This paper presents self-certification, an identity management mechanism, reputation model, and a cryptographic
protocol that facilitates generation of global reputation data
in a P2P network, in order to expedite detection of rogues.
A reputation system for peer-to-peer networks can be
thwarted by a consortium of malicious nodes. Such a group
can maliciously raise the reputation of one or more
members of the group. There is no known method to
protect a reputation system against liar farms and the
absence of a third trusted party makes the problem of liar
farms even more difficult.
The self-certification-based identity generation mechanism reduces the threat of liar farms by binding the network
identity of a peer to his real-life identity while still
providing him anonymity. The Identity mechanism is based
on the fundamental that the ranks of the peers are more
relevant than the absolute value of their reputation. The cost
of this security is the difference in the ranks of the providers
because of the use of the proposed mechanism.
The global reputation data are protected against any
malicious modification by the third party peer and are
immune to any malicious modifications by their owner. The
proposed protocol reduces the number of malicious
transactions and consumes less bandwidth per transaction
than the other reputation systems proposed in its category.
It also handles the problem of highly erratic availability
pattern of the peers in P2P networks.
Currently, the reputation of the provider is considered
and the reputation of the requester is ignored. This system
can be extended to encapsulate the reputations of both the
provider and the requester. In addition, instead of generic
number values, the reputation values can be modified in
accordance with the context of the reputation.

[13]
[14]
[15]
[16]
[17]

[18]
[19]
[20]

[21]
[22]

[23]
[24]
[25]

[26]

REFERENCES
[1]

H. Garett, “Tragedy of Commons,” Science, vol. 162, pp. 1243-1248,
1968.

[27]

VOL. 22,

NO. 7, JULY 2010

I. Stoica, R. Morris, D. Liben-Nowell, D. Karger, M.F. Kaashoek, F.
Dabek, and H. Balakrishnan, “Chord: A Scalable Peer-to-Peer
Lookup Service for Internet Applications,” Proc. ACM SIGCOMM,
pp. 149-160, Aug. 2002.
S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Schenker,
“A Scalable Content-Addressable Network,” SIGCOMM Computer
Comm.. Rev., vol. 31, no. 4, pp. 161-172, 2001.
A. Rowstron and P. Druschel, “Pastry: Scalable, Decentralized
Object Location, and Routing for Large-Scale Peer-to-Peer
Systems,” Proc. IFIP/ACM Int’l Conf. Distributed Systems Platforms
(Middleware), pp. 329-350, Nov. 2001.
G. Networks, “Groove Networks,” http://www.groove.net/
products/workspace/securitypdf.gtml, 2009.
R.L. Rivest and B. Lampson, “SDSI: A Simple Distributed Security
Infrastructure,” Proc. Crypto ’96, pp. 104-109, Aug. 1996.
N. Li and J.C. Mitchell, “RT: A Role-Based Trust-Management
Framework,” Proc. Third DARPA Information Survivability Conf. and
Exposition (DISCEX III), Apr. 2003.
D. Ferraiolo and R. Kuhn, “Role-Based Access Controls,” Proc.
15th Nat’l Computer Security Conf., May 1992.
D. Chaum, “Blind Signatures for Untraceable Payments,” Proc.
Advances in Cryptology (Crypto ’82), 1983.
L. Zhou, F. Schneider, and R. Renesse, “COCA: A Secure
Distributed Online Certification Authority,” ACM Trans. Computer
Systems, vol. 20, no. 4, pp. 329-368, Nov. 2002.
M. Chen and J.P. Singh, “Computing and Using Reputations for
Internet ratings,” Proc. Third ACM Conf. Electronic Commerce,
pp. 154-162, 2001.
P. Resnick, R. Zeckhauser, and E. Friedman, “Reputation
Systems,” Comm. ACM, vol. 43, pp. 45-48, Dec. 2000.
E. Friedman and P. Resnick, “The Social Cost of Cheap
Pseudonyms,” J. Economics and Management Strategy, vol. 10,
no. 2, pp. 173-199, 2001.
L. Xiong and L. Liu, “PeerTrust: Supporting Reputation-Based
Trust in Peer-to-Peer Communities,” IEEE Trans. Knowledge and
Data Eng., vol. 16, no. 7, pp. 843-857, July 2004.
A. Abdul-Rahman and S. Hailes, “Supporting Trust in Virtual
Communities,” Proc. Hawaii Int’l Conf. System Sciences, Jan. 2000.
K. Aberer and Z. Despotovic, “Managing Trust in a Peer-2-Peer
Information System,” Proc. 10th Int’l Conf. Information and Knowledge Management (CIKM ’01), pp. 310-317, Nov. 2001.
A.I. Schein, A. Popescul, L.H. Ungar, and D.M. Pennock,
“Methods and Metrics for Cold-Start Recommendations,” Proc.
25th Ann. Int’l ACM SIGIR Conf. Research and Development in
Information Retrieval, pp. 253-260, 2002.
C. Dellarocas, “Immunizing Online Reputation Reporting Systems
against Unfair Ratings and Discriminatory Behavior,” Proc. ACM
Conf. Electronic Commerce, pp. 150-157, Oct. 2000.
C. Dellarocas, Building Trust On-Line: The Design of Reliable
Reputation Mechanism for Online Trading Communities. MIT Sloan
School of Management, 2001.
E. Damiani, S.D.C. di Vimercati, S. Paraboschi, and P. Samarati,
“Managing and Sharing Servents’ Reputations in p2p Systems,”
IEEE Trans. Knowledge and Data Eng., vol. 15, no. 4, pp. 840-854,
July 2003.
B.C. Ooi, C.Y. Kiau, and K. Tan, “Managing Trust in Peer-to-Peer
Systems Using Reputation-Based Techniques,” Proc. Fourth Int’l
Conf. Web Age Information Management, Aug. 2003.
L. Liu, S. Zhang, K.D. Ryu, and P. Dasgupta, “R-Chain: A SelfMaintained Reputation Management System in p2p Networks,”
Proc. 17th Int’l Conf. Parallel and Distributed Computing Systems
(PDCS), Nov. 2004.
R. Zhou, K. Hwang, and M. Cai, “Gossiptrust for Fast Reputation
Aggregation in Peer-to-Peer Networks,” IEEE Trans. Knowledge
and Data Eng., vol. 20, no. 9, pp. 1282-1295, Aug. 2008.
Z. Xu, Y. He, and L. Deng, “A Multilevel Reputation System for
Peer-to-Peer Networks,” Proc. Sixth Int’l Conf. Grid and Cooperative
Computing (GCC ’07), pp. 67-74, 2007.
M. Gupta, P. Judge, and M. Ammar, “A Reputation System for
Peer-to-Peer Networks,” Proc. 13th Int’l Workshop Network and
Operating Systems Support for Digital Audio and Video (NOSSDAV),
2003.
M. Piatek, T. Isdal, A. Krishnamurthy, and T. Anderson, “One
Hop Reputations for Peer to Peer File Sharing Workloads,” Proc.
Fifth USENIX Symp. Networked Systems Design and Implementation
(NSDI ’08), pp. 1-14, 2008.
J. Douceur, “The Sybil Attack,” Proc. IPTPS ’02 Workshop, 2002.

DEWAN AND DASGUPTA: P2P REPUTATION MANAGEMENT USING DISTRIBUTED IDENTITIES AND DECENTRALIZED RECOMMENDATION...

[28] M. Castro, P. Druschel, A. Ganesh, A. Rowstron, and D.S.
Wallach, “Secure Routing for Structured Peer-to-Peer Overlay
Networks,” Proc. Fifth Symp. Operating Systems Design and
Implementation, pp. 299-314, Winter 2002.
[29] J. Camenisch and E.V. Herreweghen, “Design and Implementation of the Idemix Anonymous Credential System,” technical
report, IBM Research Division, 2002.
[30] L. Alliance, “Identity Systems and Liberty Specification
Version 1.1 Interoperability,” Project Report, Liberty Alliance
Project, technical report, 2003.
[31] M. Hauswirth, A. Datta, and K. Aberer, “Handling Identity in
Peer-to-Peer Systems,” Proc. Sixth Int’l Workshop Mobility in
Databases and Distributed Systems, in Conjunction with 14th Int’l
Conf. Database and Expert Systems Applications, Sept. 2003.
[32] P. Zimmermann, The Official PGP User’s Guide. MIT Press, 1995.
[33] P. Dewan, “Injecting Trust in Peer-to-Peer Systems,” technical
report, Arizona State Univ., 2002.
[34] A. Clausen, “How Much Does it Cost to Buy a Good Google
Pagerank?” unpublished, Oct. 2003.
[35] G. Shafer and J. Pearl, Readings in Uncertain Reasoning. Morgan
Kaufmann, 1990.
[36] F.K. Robert and A. Wilson, The MIT Encyclopedia of the Cognitive
Sciences (MITECS). Bradford Books, 1999.
[37] D.P. Foster and H.P. Young, “On the Impossibility of Predicting
the Behavior of Rational Agents,” technical report, John Hopkins
Univ., 1999.
[38] T. Rabin and M. Ben-Or, “Verifiable Secret Sharing and Multiparty Protocols with Honest Majority,” Proc. 21st Ann. ACM Symp.
Theory of Computing, pp. 73-85, 1989.
[39] C. Cachin, K. Kursawe, A. Lysyanskaya, and R. Strobl, “Asynchronous Verifiable Secret Sharing and Proactive Cryptosystems,”
citeseer.nj.nec.com/cachin02asynchronous.html, 2002.
[40] D.C. Montgomery, Design and Analysis of Experiments. J. Wiley and
Sons, 2000.
[41] D. Rumsey, Statistics for Dummies. J. Wiley and Sons, 2003.
[42] S.D. Kamvar, M.T. Schlosser, and H. Garcia-Molina, “The
Eigentrust Algorithm for Reputation Management in P2P Networks,” Proc. 12th Int’l World Wide Web Conf., pp. 640-651, 2003.
[43] E. Damiani, D. di Vimercati, S. Paraboschi, P. Samarati, and F.
Violante, “A Reputation-Based Approach for Choosing Reliable
Resources in Peer-to-Peer Networks,” Proc. Conf. Computer and
Comm. Security (CCS ’02). pp. 207-216, 2002.

1013

Prashant Dewan received the PhD degree in
computer science in 2004 and the MS degree in
2002 from Arizona State University. He is a
research scientist in Intel Corporation. His
research interests are distributed computing,
peer-to-peer networks, virtualization-based security, and reputation systems.

Partha Dasgupta is an associate professor in
the Department of Computer Science at Arizona
State University. He leads the Distributed
Systems Lab in Arizona State University. His
research interests are multicore architectures,
networking, and sensor networking.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

Operating System
Partha Dasgupta, Arizona State University
Richard J. LeBlanc, Jr., Mustaque Ahamad, and Umakishore Ramachandran,
Georgia Institute of Technology

distributed operating system is a control program running on a set of
computers that are interconnected by a network. This control program
unifies the different computers into a single integrated compute and
storage resource. Depending on the facilities it provides, a distributed operating
system is classified as general purpose, real time, or embedded.
The need for distributed operating systems stems from rapid changes in the
hardware environment in many organizations. Hardware prices have fallen rapidly
in the last decade, resulting in the proliferation of workstations, personal computers, data and compute servers, and networks. This proliferation has underlined the
need for efficient and transparent management of these physically distributed
resources (see sidebar titled “Distributed operating systems”).
This article presents a paradigm for structuring distributed operating systems,
the potential and implications this paradigm has for users, and research directions
for the future.

Clouds is a generalpurpose operating
system for distributed
environments. It is
based on an objectthread model adapted
from object-oriented
programming.
34

Two paradigms
Operating system structures for a distributed environment follow one of two
paradigms: message-based or object-based. Message-based operating systems
place a message-passing kernel on each node and use explicit messages to support
interprocesses communication. The kernel supports both local communication
(between processes on the same node) and nonlocal or remote communication,
which is sometimes implemented through a separate network-manager process. In
a traditional system such as Unix, access to system services is requested via
protected procedure calls, whereas in a message-based operating system, the
requests are via message passing. Message-based operating systems are attractive
for structuring operating systems because the policy, which is encoded in the server
processes, is separate from the mechanism implemented in the kernel.
0018-9162/91/1100-0034$01.00Q 1991 IEEE

COMPUTER

Object-based distributed operating
systems encapsulate services and resources into entities called objects. Objects are similar to instances of abstract
data types. They are written as individual modules composed of the specific
operations that define the module interfaces. Clients request access to system services by invoking the appropriate system object. The invocation
mechanism is similar to a protected procedure call. Objects encapsulate functionality much as server processes do in
message-based systems (see sidebar titled “What can objects do?”).
Among the well-known messagebased systems are the V-system ’ developed at Stanford University and the
Amoeba system2 developed at Vrije
University in Amsterdam, Netherlands.
These systems provide computation and
data services via servers that run on
machines linked by a network.
Most object-based systems are built
on top of an existing operating system,
typically Unix. Examples of such systems include Argus: Cronus,4and Eden?
These systems support objects that respond to invocations sent via the message-passing mechanisms of Unix.
Mach6 is an operating system with
distinctive characteristics. A Unix-compatible system built to be machineindependent, it runs on a large variety
of uniprocessors and multiprocessors.
It has a small kernel that handles the
virtual memory and process scheduling,
and it builds other services on top of the
kernel. Mach implements mechanisms
that provide distribution, especially
through a facility called memory objects, for sharing memory between separate tasks executing on possibly different machines.

Distributed operating systems
Networked computing environments are now commonplace. Because powerful
desktop systems have become affordable, most computing environments are
now composed of combinations of workstations and file servers. Such distributed
environments, however, are not easy to use or administer.
Using a collection of computers connected by a local area network often poses
problems of resource sharing and environment integration that are not present in
centralized systems. To keep user productivity high, the distribution must be
transparent and the environment must appear centralized. The short-term solution adopted by current commercial software extends conventional operating systems to allow transparent file access and sharing. For example, Sun added Network File System to provide distributed file access capabilities in Unix. Sun-NFS
has become the industry-stangard distributed file system for Unix systems. The
small systems world, dominated by IBM PC-compatible and Apple computers,
has software packages that perform multitasking and network-transparent file access - for example, Microsoft Windows 3.0, Novel1Netware, Appletalk, and PCNFS.
A better long-term solution is the design of an operating system that considers
the distributed nature of the hardware architecture at all levels. A distributed operating system is such a system. It makes a collection of distributed computers
look and feel like one centralized system, yet keeps intact the advantages of distribution. Message-based and object-based systems are two paradigms for structuring such operating systems.

Clouds approach
Clouds is a distributed operating system that integrates a set of nodes into a
conceptually centralized system. The
system is composed of compute servers,
data servers, and user workstations. A
computeserver is a machine that is available for use as a computational engine.
A data server is a machine that functions as a repository for long-lived (that
is. persistent) data. A user workstation
is a machine that provides a programming environment for developing applications and an interface with the com-

pute and data servers for executing those
applications on the servers. Note that
when a disk is associated with a compute server, it can also act as a data
server for other compute servers. Clouds
is a nativeoperatingsystem that runson
top of a native kernel called Ra (after
the Egyptian sun god). It currently runs
on Sun-3/50 and Sun-3/60 computers
and cooperates with Sun Sparcstations
(running Unix) that provide user interfaces.
Clouds is a general-purpose operating system. That is, it is intended to
support all types of languages and ap-

What can objects do?
Conceptually, an object is an encapsulation of data and a
set of operations on the data. The operations are performed
by invoking the object and can range from simple data-manipulation routines to complex algorithms, from shared library
accesses to elaborate system services.
Objects can also provide specialized services. For example, an object can represent a sensing device, and an invocation can gather data from the device without knowing
about the mechanisms involved in accessing it - or its location. Similarly, an object that handles terminal or file 110 contains defined read and write operations.

November 1991

Objects can be active. An active object has one or more
processes associated with it that communicate with the external world and handle housekeeping chores internal to the object. For example, a process can monitor an object’s environment and can inform some other entity (another object) when
an event occurs. This feature is particularly useful in objects
that manage sensor-monitoring devices.
Objects are a simple concept with a major impact. They can
be used for almost every need - from general-purpose programming to specialized applications - yet provide a simple
procedural interface to the rest of the system.

35

an abstraction of storage and threads to
implement computations. This decouples computation and storage, thus maintaining their orthogonality. In addition,
the object-thread model unifies the treatment of I/O, interprocess communication, information sharing, and long-term
storage. This model has been further
augmented to support atomicity and reliable execution of computations.
Multics was the starting point for many
ideas found in operating systems today,
and Clouds is no exception. These ideas
include sharable memory segments and
single-level stores using mapped files.
Hydra first implemented the use of objects as a system-structuring concept.
Hydra ran on a multiprocessor and provided named objects for operatingsystem services.

Clouds paradigm
This section elaborates on the objectthread paradigm of Clouds, illustrating
the paradigm with examples of its use.

Figure 1. A Clouds object.

plications, distributed or not. All applications can view the system as a monolith, but distributed applications may
choose to view the system as composed
of several separate compute and data
servers. Each compute facility in Clouds
has access to all system resources.
The system structure is based on an
object-thread model. The object-thread
model is an adaptation of the popular
object-oriented programming model,
which structures a software system as a
set of objects. Each object is an instance
of an abstract data type consisting of
data and operations on the data. The
operations are called methods. The object type is defined by a class. A class
can have zero or more instances, but an
instance is derived from exactly one
class. Objects respond to messages. Send36

ing a message to an object causes the
object to execute a method, which in
turn accesses or updates data stored in
the object and may trigger sending messages to other objects. Upon completion, the method sends a reply to the
sender of the message.
Clouds has a similar structure, implemented at. the operating system level.
Clouds objects are large-grained encapsulations of code and data that are contained in an entire virtual-address space.
An object is an instance of a class, and a
class is a compiled program module.
Clouds objects respond to invocations.
An invocation results from a thread of
execution entering the object to execute an operation (or method) in the
object.
Clouds provides objects to support

Objects. A Clouds object is a persistent (or nonvolatile) virtual address
space. Unlike virtual-address spaces in
conventional operating systems, the contents of a Clouds object are long-lived.
That is, a Clouds object exists forever
and survives system crashes and shutdowns unless explicitly deleted - like a
file. As the following description of objects shows, Clouds objects are somewhat “heavyweight,” that is, they are
best suited for storage and execution of
large-grained data and programs because
of the overhead associated with invocation and storage of objects.
Unlike objects in some object-based
operating systems, a Clouds object does
not contain a process (or thread). Thus,
Clouds objects are passive. Since contents of a virtual address space are not
accessible from outside the address space,
the memory (data) in an object is accessible only by the code in the object.
A Clouds object contains user-defined
code, persistent data, a volatile heap for
temporary memory allocation, and a
persistent heap for allocating memory
that becomes a part of the object’s persistent data structures (see Figure 1).
Recall that the data in the object can be
manipulated only from within the object. Data can pass into the object when
an entry point is invoked (input parameters). Data can pass out of the object

COMPUTER

~~

when this invocation terminates (result
parameters).
Each Clouds object has a global system-level name called a sysname, which
is a bit string that is unique over the
entire distributed system. Therefore, the
sysname-based naming scheme in Clouds
creates a uniform, flat, system-name
space for objects. Users can define highlevel names for objects - a naming
service translates them to sysnames.
Objects are physically stored in data
servers but are accessible from all compute servers in the system, thus providing location transparency to the users.
Threads. The only form of user activity in the Clouds system is the user
thread. A thread is a logical path of
execution that traverses objects and
executes the code in them. Thus, unlike
a process in a conventional operating
system, a Clouds thread is not bound to
a single address space. A thread is created by an interactive user or under
program control. When a thread executes an entry point in an object, it
accesses or updates the persistent data
stored in it. In addition, the code in the
object may invoke operations in other
objects. In such an event. the thread
temporarily leaves the calling object,
enters the called object, and commences execution there. The thread returns
to the calling object after the execution
in the called object completes and returns results. These arguments and results are strictly data; they cannot be
addresses. This restriction is mandatory
because addresses in one object are
meaningless in the context of another
object. In addition, object invocations
can be nested as well as recursive. After
the thread completes execution of the
operation it was created to execute, it
terminates.
The nature of Clouds objects prohibits a thread from accessing any data
outside the current address space (object) in which it is executing. Control
transfer between address spaces occurs
through object invocation, and data
transfer between address spaces occurs
through parameter passing.
Several threads can simultaneously
enter an object and execute concurrently. Multiple threads executing in the
same object share the contents of the
object’s address space. Figure 2 shows
thread executions in the Clouds object
spaces. The programmer uses systemsupported primitives such as locks or

November 1991

~~

~~

~

Distributed object space (persistent, virtual memory)

”r

Thread

2J
Stack

Stack

Figure 2. Distributed object memory.

semaphores to handle concurrency control within the object.
Interaction between objects and
threads. The structure created by a system, composed of objects and threads,
has several interesting properties.
Inter-object interfaces are procedural.
Object invocations are equivalent to
procedure calls on long-lived modules
that do not share global data. The
invocations work across machine
boundaries.
The storage mechanism used in Clouds
differs from those found in conventional operating systems. Conventionally,
files are used to store persistent data.
Memory is associated with processes
and is volatile (that is, the contents of
memory associated with a process are
lost when the process terminates). Objects in Clouds unify the concepts of
persistent storage and memory to create a persistent address space. This unification makes programming simpler.
Persistent objects provide a structured
single-level store that is cosmetically
similar to mapped files in Multics and
SunOS.
Some systems use message-passing
for communicating shared data and coordinating computations. Clouds shares

data by placing the data in an object.
Computations that need access to shared
data invoke the object where the data
exists. Clouds does not support messages and files at the operating system level, although it does allow objects to
simulate them if necessary (see sidebar
titled “No files? No messages?”on the
next page).
In a message-based system, the user
must determine the desired level of
concurrency at the time of writing an
application, and program it as a certain
number of server processes. The objectthread model of Clouds eliminates this
requirement. An object can be written
from the viewpoint of the functionality
it is meant to provide, rather than the
actual level of concurrency it may have
to support. At execution time, the level
of concurrency is specified by creating
concurrent threads to execute in the
objects that compose the user-level
application. The application objects,
however, must be written to support
concurrent executions, using synchronization primitives such as semaphores
and locks.
To summarize:
The Clouds system is composed of
named address spaces called objects.
37

No files? No messages?
The persistent objects supported in an operating system
like Clouds provide a structured permanent storage mechanism that can be used for a variety of purposes, including the
simulation of files and messages. An object can store data in
any form and invocations can be used to
Manipulate or process the stored data,
Ship data in and out of the object in forms not necessarily
the same as those used for storage, and
Allow controlled concurrent access to shared data, without
regard to data location.
There is no need for files in a persistent programming environment. Conventional systems use files as byte-sequential
storage of long-lived data. When persistent shared memory is
available, there is no need to convert data into byte-sequential form, store it in files, and later retrieve and reconvert it.

Objects provide data storage, data manipulation, data sharing, concurrency
control, and synchronization.
Control flow is achieved by threads
invoking objects.
Data flow is achieved by parameter
passing.
Programming in the Clouds model.
For the programmer, Clouds has two
kinds of objects: classes and instances.
A class is a template used to generate
instances. An instance is an object invocable by user threads. Thus, to write
application programs for Clouds, a programmer writes one or more Clouds
classes that define the application code
and data. The programmer can then

clouds-class rectangle;
int x, y;
entry rectangle;
entry size (int x, y)
entry int area ();
end-class

The data can be kept in memory in a form controlled by the
programs (for example, lists or trees), even when the data is
not in use.
In fact, objects that store byte-sequential data can simulate
files, and they can have read and write invocations defined to
access this data. Such an object will look like a file, even
though the operating system does not explicitly support files.
The same is true for messages. The functional equivalence
of messages and shared memory is well known. If desired, a
buffer object with defined send and receive invocations can
serve as a port structure between two (or more) communicating processes.
We feel that files, messages, and disk I/O are artifacts of
hardware structure. Given an object implementation, these
features are neither necessary nor attractive. New programming paradigms based on object-oriented styles use persistent memory effectively. They do not use files and messages.

create the requisite number of instances of these classes. The application is
then executed by creating a thread to
execute the top-level invocation that
runs the application.
The following is a simple example of
programming in the Clouds system. The
object Rectangle consists of x and y
dimensions of a rectangle. The object
has two entry points, one for setting the
size of the rectangle and the other for
computing the area. The object definition is shown in Figure 3a.
Once the class is compiled, any number of instances can be created either
from the command line or via another
object. Suppose the Rectangle class is
instantiated into an object called RectOl.

//persistent data for rect.
//constructor
//set size of rect.
//return area of rect.

rectangle-ref rect;

//“rect” is a class that refers to
//an object of type rectangle.

rect.bind (“RectOl ”);

//call to name server,
//binds sysname to RectOl
//invocation of RectOl
Ilwill print 50

rectsize ( 5 , 10);
printf(“%d\n” rect.area() );

(b)
Figure 3. Example code for Clouds.

38

Now RectOl.size can be used to set the
size and RectOl.area can be called to
return the area of the rectangle. A command in the Clouds command line interpreter can call the entry point in the
object. Entry points can also be invoked
in the program, allowing one object to
call another.
Objects have user names, which are
assigned by the programmer when objects are created (compiled or instantiated). A name server then translates
the user name to a sysname. (Recall
that a sysname is a unique name for an
object, which is needed for invoking the
object.) The code fragment in Figure 3b
details the steps in accessing the Clouds
object Recto1 and invoking operations
on it.
Clouds provides a variety of mechanisms to programmers. These include
registering user-defined names of objects with the name server, looking up
names using the name server, invoking
objects both synchronously and asynchronously, and synchronizing threads.
I/O to the user console is handled by
read and write routines (and by printf
and scanf library calls). These routines
read/write ASCII strings to and from
the controlling user terminal, irrespective of the actual location of the object
or the thread.
User objects and their entry points
are typed by the language definition.
The compiler performs static type-checking on the object and entry point types
at compile time. It performs no runtime
type checking. Clouds objects are coarseCOMPUTER

grained, unlike the fine-grained entities
found in such object-oriented programming languages as Smalltalk. Since an
object invocation in Clouds is at least an
order of magnitude more expensive than
a simple procedure call, it is appropriate to use a Clouds object as a module
that may contain several fine-grained
entities. These fine-grained objects are
completely contained within the Clouds
object and are not visible to the operating system.
Currently, we support two languages
in the Clouds operating system. DC++
is an extension of C++ that systems
programmers use. Distributed Eiffel is
an extension of Eiffel that targets application developers. The design of both
DC++ and Distributed Eiffel supports
persistent fine-grained and large-grained
objects, invocations, thread creation,
synchronization, and user-level object
naming.

Network

f

t+

1

Objects

-’:I

Clouds environment
The Clouds system integrates a set of
homogeneous machines into one seamless environment that behaves as one
large computer. The system configuration is configured as three logical categories of machines, each supporting a
different logical function. These are
compute servers, data servers, and user
workstations.
The system’s core consists of a set of
homogeneous machines of the compute
server category. Compute servers do
not have any secondary storage. These
machines provide an execution service
for threads. Data servers provide secondary storage. They store Clouds objects and supply the objects’ code and
data to the compute servers. The data
servers also support the distributed synchronization functions. The third machine category, the user workstation,
provides user access to Clouds compute
servers. Compute servers, in turn, know
how and when to access data servers.
The logical machine categories do not
require a one-to-one scheme for mapping to physical machines. Although a
diskless machine can function only as a
compute server, a machine with a disk
can simultaneously act as a compute
and data server. This enhances computing performance, since data accessed
via local disk is faster than data accessed over a network. However, in our
prototype system, shown in Figure4, we
November 1991

Figure 4. Clouds system architecture.

use a one-to-one mapping to simplify
the system’s implementation and configuration.
A suite of programs that run on top of
Unix on Sun workstations provide the
user interface to Clouds. These programs include Distributed Eiffel and
DC++ compilers, a Clouds user shell
(under X Windows), a user I/O manager, and various utilities. The user interface with these programs is through the
familiar Unix utilities (including Unix
editors).

A user invokes an Clouds object by
specifying the object, the entry point,
and the arguments to the Clouds shell.
The shell sends an invocation request to
a compute server, and the invocation
proceeds under Clouds using a Clouds
thread. The user communicates to the
thread via a terminal window in the X
Window System. All output generated
by the thread (regardless of where it is
executing) appears on the user terminal
window, and input to the thread is provided by typing in the window.

User environment. A user writes
Clouds programs in DC++ or Distributed Eiffel and compiles them on the Unix
workstation. The compiler loads the
generated classes on a Clouds data server. Now these classes are available to all
Clouds compute servers. Any node (or
user on a Unix machine) can create
instances of these classes and generate
invocations to the objects thus created.
Note that once created, the objects become part of the persistent object memory and can be invoked until they are
explicitly deleted.

System environment. As we mentioned earlier, the hardware environments consists of compute servers and
data servers with some nodes providing
both functions. Starting a user-level
computation on Clouds involves first
selecting a compute server to execute
the thread. This is a scheduling decision
and may depend on such factors as scheduling policies, the load at each compute
server, and the availability of resources
needed for the computation. Once this
decision has been made, the second task
is to bring into the selected compute

39

Distributed shared

server the object in which the thread
executes. This requires a remote-paging facility. Coupled with this requirement is the fact that all objects are potentially shared in the Clouds model;
therefore, the entity that provides the
remote-paging facility must also maintain the consistency of shared pages.
This is satisfied in Clouds by a mechanism called distributed shared memory.
DSM supports the notion of shared
memory on a nonshared memory (distributed) architecture (see sidebar titled “Distributed shared memory”). The
data servers execute a coherence protocol that preserves single-copy semantics for all the objects.’With DSM, concurrent invocation of the same object
by threads at different compute servers
is possible. Such a scenario would result
in multiple copies of the same object
existing at more than one compute server with DSM providing the consistency
maintenance.
Suppose a thread is created on compute server A to invoke object 0,.The
compute server retrieves a header for
the object from the appropriate data
server, sets up the object space, and
starts the thread in that space. As the
thread executes in the object space, the
code and data of the object accessed by
the thread is demand-paged from the
data servers (possibly over the network).
If the thread executing in 0, generates an invocation to object 0,,the system may choose to execute the invoca40

Ory

tion on either A itself or on a different
compute server, B. In the former case, if
the required pages of object 0, are at
other nodes, they have to be brought to
node A using DSM. Once the object has
been brought into A , the invocation
proceeds the same way as when 0, resides at A. On the other hand, the system may choose to execute the invocation on compute server B . In this case,
the thread sends an invocation request
to B, which invokes the object 0, and
returns the results to the thread at A.
This scenario is similar to the remote
procedure call found in other systems
such as the V system,’ but it is more
general because B does not have to be
the node where 0, currently resides.
The compute and data server scheme
makes all objects accessible to all compute servers. The DSM coherence protocol ensures that the data in an object
is seen consistently by concurrent threads
even if they are executing on different
compute servers. The distributed synchronization support provided by data
servers allows threads to synchronize
their actions regardless of where they
execute.

Clouds implementation
The Clouds implementation uses a
minimalist approach towards operating
system development (like the V system,
Amoeba, and Mach 3.0). With this ap-

proach, each level of the implementation consists of only those functions
that cannot be implemented at a higher
level without a significant performance
penalty. Traditional systems such as
Unix provide most operating-system
services in one big monolithic kernel.
Unlike such systems, we differentiate
between the operating-system kernel
and the operating system itself. This
approach makes the system modular,
easy to understand, more portable, and
convenient to enhance. High-level features can be implemented as user-level
libraries, objects, or services that use
the low-level mechanisms in the operating system. Further, it provides a clean
separation of policy from mechanisms;
that is, the policies are implemented at
a high level using the lower level mechanisms.
The current Clouds implementation
has three levels. At the lowest level is
the minimal kernel, Ra, which provides
the mechanisms for managing basic resources, namely, processor and memory. The next level up is a set of system
objects, which are trusted-software modules providing essential system services. Finally, other noncritical services
such as naming and spooling are implemented as user objects to complete the
functionality of Clouds.

The Ra kernel. This native minimal
kernel supports virtual memory management and low-level scheduling. Ra
implements four abstractions:
Segments. A segment is a variablelength sequence of uninterpreted bytes
that exists either on the disk or in physical memory. Segments have systemwide unique names and, once created,
segments persist until they are explicitly destroyed.
Virtualspaces.Avirtual spaceis the
abstraction of an addressing domain
and is a monotonically increasing range
of virtual addresses with possible holes
in the range. A segment can be mapped
to a contiguous range of addresses in a
virtual space.
ZsiBas. An IsiBa is the abstraction
of system activity and can be thought of
asa lightweight process. (Its name comes
from ancient Egyptian: Isi means light,
and Ba means soul.) An IsiBa is simply
a kernel resource that is associated with
a stack to realize a schedulable entity.
There are several types of stacks in the
system (for example, kernel, interrupt,

COMPUTER

and user), and an IsiBa can use an instance of any stack type. A Clouds process is an IsiBa in conjunction with a
user stack and a Ra virtual space. One
or more Clouds processes are used to
build a Clouds thread. IsiBas can also
be used for a variety of purposes
inside system objects, including interrupt services, event notification, and
watchdogs.
*Partitions. A partition is an entity
that provides nonvolatile data storage
for segments. A Clouds compute server
has access to one or more partitions, but
a segment belongs to exactly one partition. T o access a segment, the partition
containing the segment has to be contacted. The partition communicates with
the data server where the segment is
stored to page the segment in and out
when necessary. Note that Ra only defines the interface to the partitions. The
partitions themselves are implemented
as system objects.
Figure 5 shows the relationship between virtual spaces, segments, and partitions.
The implementation of Ra is separated into machine-dependent and machine-independent parts. All Ra components use the class mechanisms of
C++, a scheme that enhances Ra’s object structure. Ra consists of 6,000 lines
of machine-dependent C++ code, 6,000
lines of machine-independent C++ code,
and 1,000 lines of Sun (68020) assembly
code. It currently runs on the Sun-3
class machines. More details about the
implementation are in Dasgupta et aL8
System objects. Ra can be thought of
as the conceptual motherboard. Operating-system services are provided on
top of Ra by system objects. System
objects are independently compiled
modules of code that have access to
certain Ra-defined operations. Ra exports these operations as kernel classes
that are inherited by the system objects.
Conceptually, the system objects are
similar to Clouds objects that live in
their own virtual space. However, for
the sake of efficiency, system objects
live in the kernelspace, are linked to the
Ra kernel at system configuration time,
and are not directly invocable from the
user level. System objects are implicitly
invoked through a system-call interface
available to user-level objects.
Some system objects implement lowlevel functions inside the operating sys-

November 1991

I

Figure 5. Virtual spaces, segments, and partitions.

tem; these functions include the buffer
manager, uniform I/O interface, and
Ethernet driver. Other system objects
implement high-level functions that are
invoked indirectly as a result of a system call. These objects include the thread
manager, object manager, and user 110
manager.
The following bulleted paragraphs
describe some of the important system
objects.
Thread manager. As mentioned earlier, a thread can span machine boundaries and is implemented as a collection
of Clouds processes. There is some information associated with a thread, such
as the objects it may have visited, the
user workstation from which it was created, and the windows on the user workstation with which it has to communicate when I/O requests are made during
the computation. The thread manager
is responsible for the creation, termination, naming, and bookkeeping necessary to implement threads.
User object manager. User-level
objects are implemented through a system object called the object manager.
The object manager creates and deletes
objects and provides the object-invocation facility. An object is stored in a Ra
virtual space. The invocation of an object by a thread is handled mainly by the
object manager in conjunction with the
thread manager. Briefly, when a thread
invokes an object, the stack of the invoking thread is mapped into the same

virtual address space as the object, and
the thread is allowed to commence execution at the entry point of the object.
When the execution of the operation
terminates, the object manager unmaps
the thread stack from the object and
remaps it in the object where the thread
was previously executing. If there was
no previous object, then the object manager informs the thread manager, and
the thread is terminated.
DSM clients and servers. DSM clients and servers are partitions that interact with the data servers to provide
one-copy semantics for all object code
and data used by the Clouds nodes.
When node A needs a page of data, the
DSM client partition requests it from
the data server. If the page is currently
in use in exclusive mode at node B , the
data server forwards the request to the
DSM server at node B , which supplies
the page to A . The DSM server allows
maintenance of both exclusive and
shared locks on segments and provides
other synchronization support.
User I/O manager. This system object provides support for Clouds computations to read from and write to user
terminals. A user terminal is a window
on a Unix workstation. When a thread
executes a write system call, the I/O
manager routes the written data to the
appropriate controlling terminal. Reads
are handled similarly. The user U0
manager is a combination of a Ra system object and a server on each Unix
workstation.
41

Networking and RaTP. Two system
objects handle networking: the Ethernet driver and the network protocol.
All Clouds communication uses a transport layer protocol called the Ra transport protocol. RaTP is similar to the
communication protocol VMTP9 used
in the V-system, and provides efficient,
reliable, connectionless message transactions. A message transaction is a send/
reply pair used for client-server type
communications. RaTP has been implemented both on Ra (as a system object) and on Unix, allowing Clouds-toUnix communication.

Status and current performance. The
Clouds implementation and features
described thus far are in use. The kernel
performance is good. Context-switch
time is 0.15 milliseconds. The time to
service a page fault when the page is
resident on the same node costs 2.3 ms
for a zero-filled, 8-kilobyte page; it costs
1.5 ms for a nonzero-filled page.
Networking is one of Clouds' most
heavily used subsystems, especially since
our current implementation uses diskless compute servers. All objects are
demand-paged to the servers over the
network when used. The RaTP protocol handles the reliable data transfer
between all machines. The Ethernet
round-trip time is 1.59 ms; this involves
sending and receiving a short message
(72 bytes) between two compute servers. The RaTP reliable round-trip time
is 3.56 ms. To transfer an 8-kilobyte
page reliably from one machine to another costs 12.3 ms, compared to 70 ms
using Unix file-transfer protocol and 50
ms using Sun-NFS.
Object invocation costs vary widely
depending upon whether the object is
currently in memory or has to be fetched
from a data server. The maximum cost
for a null invocation is 103ms, while the
minimum cost is 8 ms. Note that due to
locality, the average cost is much closer
to the minimum than the maximum.

Clouds and distributed
systems research
The Clouds project includes continuing systems research on several topics.
Using persistent objects. Persistent,
shared, single-level storage is the central theme of the Clouds model. There-

42

fore, the thrust of several related research projects was to effectively support and exploit persistent memory in a
distributed setting. Another area of research is in harnessing the distributed
resources to speed up the execution of
specific applications compared to a single-processor implementation. We summarize some of these projects here.
Distributed programming. Using the
DSM feature of Clouds, centralized algorithms can run as distributed computations with the expectation of achieving speedup. For example, sorting
algorithms can use multiple threads to
perform a sort, with each thread being
executed at a different compute server,
even though the data itself is contained
in one object. The threads work on the
data in parallel, and those parts of the
data that are in use at a node migrate to
that node automatically. We have shown
that even though the data resides in a
single object, the computation can be
run in a distributed fashion without
incurring a high overhead. These
experiments are helping us understand the trade-off between computation and communication and the granularity of computations that warrant
distribution.
Types of persistent memory. Persistent memory needs a structured way of
specifying attributes, such as longevity
and accessibility, for the language- level
objects contained in Clouds objects. To
this end we provide several types of
memory in objects. The sharable, persistent memory is called per-object memory. We also provide per-invocation
memory that is not shared, yet is global
to the routines in the object and lasts for
the length of each invocation. Similarly,
per-thread memory is global to the routines in the object but specific to a particular thread, and lasts until the thread
terminates. Such a variety of memory
structures provides powerful programming support in the Clouds system.1°
Lisp programming environment. If the
address space containing a Lisp environment can be made persistent, several advantages accrue, including not having to save/load the environment on
startup and shutdown. Further, invoking entry points in remote Lisp interpreters allows interenvironment operations that a r e useful in building
knowledge bases. Other features that

naturally arise from the distributed nature of the system include concurrent
evaluations and load sharing. An implementation of the Clouds Lisp Distributed Environment (Clide) is currently in
experimental usage.
Object-oriented programming environment. Persistent memory is being
used to structure object-oriented programming environments. These environments support multigrained objects
inside Clouds objects and visibilitylmigration for these language-defined objects within Clouds objects.

Reliability in distributedsystems. One
goal of Clouds is to provide a highly
reliable computing environment. The
issue of reliability has two parts: maintaining consistency of data in spite of
failures and assuring forward progress
for computations. A consistency problem can occur when a thread executes at
several nodes or several nodes supply
objects to a thread because of the DSM
abstraction. In that case, a node or communication link failure causes the computation results to be reflected at some
nodes but not at others. A consistency
mechanism should provide the atomicity property guaranteeing that a thread
computation either completes at all
nodes or has no effect on system state.
Thus, if failures occur, the effects of all
partially completed computations are
undone.
Consistency by itself does not promise progress, because a failure undoes
the partially completed work. To ensure forward progress, objects and computation must be replicated at nodes
with independent failure modes. The
following aspects of the Clouds system
address the consistency and progress
requirements.
Atomicity. The Clouds consistencypreservation mechanisms present one
uniform object-thread abstraction that
lets programmers specify a wide range
of atomicity semantics. This scheme,
called Invocation-Based Consistency
Control, automatically locks and recovers persistent data. Locking and recovery are performed at the segment level,
not at the object level. (An object can
contain multiple data segments. The
layout and number of segments are under the control of the user programmer.
The segments may contain intersegment
pointers, and objects support dynamic

COMPUTER

memory allocation on each segment.)
Because segments are user-defined,the
user can control the granularity of locking. Custom recovery and synchronization are still possible but are unnecessary in many cases.
Instead of mandating customization
of synchronization and recovery for
applications that do not need strict atomicity, the new scheme supports a variety of consistency-preserving mechanisms. The threads that execute are of
two kinds, namely, s-threads (or standard threads) and cp-threads (or consistency-preserving threads). The s-threads
are not provided with any system-level
locking or recovery. The cp-threads, on
the other hand, are supported by welldefined locking and recovery features.
When a cp-thread executes, all segments it reads are read-locked, and the
segments it updates are write-locked.
The system automatically handles locking at runtime. The updated segments
are written by a two-phase commit mechanism when the cp-thread completes.
Because s-threads do not automatically
acquire locks, nor are they blocked by
any system-acquired locks, they can freely interleave with other s-threads and
cp-threads.
There are two varieties of cp-threads,
namely, the gcp-thread and the lcpthread. The gcp-thread semantics provide global (heavyweight) consistency
and the Icp-thread semantics provide
local (lightweight) consistency. All
threads are s-threadswhen created. Each
operation has a static label that declares
its consistency needs. The labels are S
(for standard), LCP (for local consistency preserving), and GCP (for global
consistency preserving). Various combinations of different consistency labels
in the same object (or in the same thread)
lead to many interesting (as well as dangerous) execution-time possibilities,
especially when s-threads update data
being readhpdated by gcp or Icp threads.
(For a discussion of the semantics, behavior, and implementation of this
scheme, see Chen and Dasgupta.”)
Fault tolerance. Transaction-processing systems guarantee data consistency
if computations do not complete (due
to failures). However, they do not guarantee computational success.The Clouds
approach to fault-tolerant or resilient
computations is called parallel execution threads. PET tries to provide uninterrupted processing in the face of pre-

hovember 1991

PET #1

PET #2

1-

1

Replicated
copies

I

processing

I

Figure 6. Parallel execution threads.

existing (static) failures, as well as system and software failures that occur
while a resilient computation is in
progress (dynamic failures).12
To obtain these properties, the basic
requirements of the system are
replication of objects, for tolerating
static and dynamic failures;
replication of computation, for tolerating dynamic failures; and
An atomic commit mechanism to
ensure correctness.
The PET system works by first replicating all critical objects at different
nodes in the system. The degree of replication depends on the degree of resilience required.
Initiating a resilient computation creates separate replicated threads (gcpthreads) on a number of nodes. The
number of nodes is another parameter
provided by the user and reflects the
degree of resilience required. The separate threads (or PETS) run independently as if there were no replication. A
thread invokes a replicated object by
choosing certain copies of the object, as
shown in Figure 6. The replica selection
algorithm tries to ensure that separate
threads execute at different nodes to
minimize the number of threads affected by afailure. After one or more threads
complete successfully by executing at
operational nodes, one thread is chosen
to be the terminating thread. All updates made by this thread are propagated to a quorum of replicas, if available.

If there is a failure in committing this
thread, another completed thread is
chosen. If the commit process succeeds,
all the remaining threads are aborted.
This method allows a trade-off in the
amount of resources used (that is, number of parallel threads started for each
computation) and the desired degree of
resilience (that is, number of failures
the computation can tolerate, while the
computation is in progress).

he goal of Clouds was to
build a general-purpose, distributed, computing environment
suitable for a wide varietyof users in the
computer science community. We have
developed a native operating system
and an application-development environment that is being used for a variety
of distributed applications.
Providing a conduit between Clouds
and Unix saved us considerable effort.
We did not have to port program development and environment tools (such as
editors and window systems) to a new
operating system, and we can develop
applications that harness the new system’s data and computation distribution capabilities in the familiar Unix
environment. The Clouds system has
been a fruitful exercise in providing an
experimental platform for determining
the worthiness of the object-thread paradigm.

=

References
1. D.R. Cheriton, “The V Distributed System,” Comm. A C M ,Vol. 31, No. 3, Mar.
1988, pp. 314-333.

2. S.J. Mullender et al., “Amoeba: A Distributed OperatingSystemfor the 1990s,”
Computer, Vol. 23, No. 5, May 1990, pp.
44-53.
3. B.Liskov, “Distributed Programming in
Argus,” Comm. A C M , Vol. 31, No. 3,
Mar., 1988, pp. 300-313.
4. R.E.Schantz,R.M.Thomas,andG.Bono,
“The Architecture of the Cronus Distributed Operating System,” Proc. Sixth
Int’l Con& on Distributed Computing
Systems, CS Press, Los Alamitos, Calif.,
Order No. 697,1986, pp. 250-259.
5. G.T. Almes et al., “The Eden System: A
Technical Review,” ZEEE Trans. Software Eng., Piscataway, N.J., Vol. SE-11,
No. 1, Jan. 1985, pp 43-58.

43

M. Accetta et al., “Mach: A New Kernel
Foundation for Unix Development,”
Proc. Summer Usenix Conf.,Usenix, 1986,
93-112.
K. Li and P. Hudak, “Memory Coher-

encein Shared Virtual Memory Systems,”
ACM Trans. Computer Systems, Vol. 7,
NO.4, NOV.1989, pp. 321-359.
P. Dasgupta et al., “The Design and Implementation of the Clouds Distributed
Operating System,” Usenix Computing
Systems J., Vol. 3, No. 1,Winter 1990,pp.
11-46.
D.R. Cheriton, “VMTP: A Transport
Protocol for the Next Generationof Communication Systems.” Proc. SIGcomm,
1986, pp. 406-415.
10. P.Dasgupta and R.C. Chen, “Memory
Semanticsin Persistent Object Systems,”
in Implementation of Persistent Object
Systems,StanZdonick, ed., Morgan Kaufmann Publishers,SanMateo,Calif., 1990.

Partha Dasgupta is an associate professor at
Arizona State University. He was technical
director of the Clouds project at Georgia
Institute of Technology, as well as coprincipal investigator of the institute’s NSF-Coordinated Experimental Research award. His
research interests include distributed operating systems, persistent object systems, operating system construction techniques, distributed algorithms, fault tolerance, and
distributed programming support.
Dasgupta received his PhD in computer
science from the State University of New
York at Stony Brook. He is a member of the
IEEE Computer Society and ACM.

Mustaque Ahamad is an associate professor
in the School of Information and Computer
Science at the Georgia Institute of Technology, Atlanta. His research interests include
distributed operating systems, distributed
algorithms, fault-tolerant systems, and performance evaluation.
Ahamad received his BE with honors in
electrical engineering from the Birla Institute of Technology and Science, Pilani, India. He obtained an MS and a PhD in computer science from the State University of
New York at Stony Brook in 1983and 1985.

Richard J. LeBlanc, Jr. is a professor in the
School of Information and Computer Science at the Georgia Institute of Technology.
As director of the Clouds project, he is studying language concepts and software engineering methodology for a highly reliable,
object-baseddistributed system. His research
interests include programming language design and implementation, programming environments, and software engineering.
LeBlanc received his BS in physics from
Louisiana State University in 1972 and his
MS and PhD in computer sciences from the
University of Wisconsin-Madison in 1974and
1977. He is a member of the ACM, the IEEE
Computer Society, and Sigma Xi.

Umakishore Ramachandran is an associate
professor in the College of Computing at the
Georgia Institute of Technology. He is currently involved in several research projects,
including hardwarekoftware trade-off in the
design of distributed operating systems. He
participated in the design of several distributed systems, including Charlotte at University of Wisconsin-Madison, Jasmin at BellCore, Quicksilver at IBM Almaden Research
Center, and Clouds. His primary research
interests are in computer architecture and
distributed operating systems.
Ramachandran received his PhD in computer science from the University of Wisconsin-Madisonin 1986. In 1990,he received an
NSF Presidential Young Investigator Award.
He is a member of the ACM and the IEEE
Computer Society.

11. R. Chen and P. Dasgupta, “Linking Con-

sistency with ObjectKhread Semantics:
An Approach to Robust Computations,”
Proc. Ninth Int’l Conf. Distributed Computing Systems, IEEE CSPress, LosAlamitos, Calif., 1989, Order No. 1953, pp.
121-128.
12. M. Ahamad, P. Dasgupta, and R.J.LeBlanc, “Fault-Tolerant Atomic Computations in an Object-based Distributed System,” Distributed Computing,Vol. 4, No.
2, May 1990, pp. 69-80.

Acknowledgments
This research was partially supported by
the National Aeronautics and Space Administration under contract NAG-1-430 and by
the NationalScienceFoundation grants DCS8316590 and CCR-8619886 (CERIII program).
The authors acknowledge Martin McKendry and Jim Allchin for starting the project
and designing the earliest version of Clouds;
David Pitts, Gene Spafford, and Tom Wilkes
for the design and implementation of the
kernel and programming support for Clouds
version 1; Jose Bernabeu, Yousef Khalidi,
and Phil Hutto for their efforts in making the
version 1 kernel usable and for the design
and implementation of Ra; Sathis Menon R.
Ananthanarayanan, Ray Chen, and Chris
Wilkenloh for significant contribution to the
implementation of Clouds version 2, as well
as for managing the software development
effort. Ray Chen and Mark Pearsondesigned
and implemented IBCC and Clide, respectively. We also thank M. Chelliah, Vibby
Gottemukkala, L. Gunaseelan, Ranjit John,
Ajay Mohindra, and Gautam Shah for their
participation in and contributions to the
project.
44

Readers may contact Partha Dasgupta at the Department of Computer Science and
Engineering,Arizona StateUniversity, Tempe, AZ85287;e-mail partha@enuxha.eas.asu.edu.

COMPUTER

Proceedings of 2015 IEEE 12th International Conference on Networking, Sensing and Control
Howard Civil Service International House, Taipei, Taiwan, April 9-11, 2015

Designing an Adaptive Lighting Control System for
Smart Buildings and Homes
Yuan Wang

Partha Dasgupta

Arizona State University
Tempe, AZ, USA
Email: Yuan.Wang.4@asu.edu

Arizona State University
Tempe, AZ, USA
Email: partha@asu.edu

Abstract—Lighting control in smart buildings and homes can
be automated by having computer controlled lights and blinds
along with illumination sensors that are distributed in the building. However, programming a large building light switches and
blind settings can be time consuming and expensive. We present
an approach that algorithmically sets up the control system that
can automate any building without custom programming. This is
achieved by making the system self calibrating and self learning.
This paper described how the problem is NP hard but can
be resolved by heuristics. The resulting system controls blinds
to ensure even lighting and also adds artificial illumination to
ensure light coverage remains adequate at all times of the day,
adjusting for weather and seasons. In the absence of daylight, the
system resorts to artificial lighting. Our method works as generic
control algorithms and are not preprogrammed for a particular
place. The feasibility, adaptivity and scalability features of the
system have been validated through various actual and simulated
experiments.

I. I NTRODUCTION
Work environments (and homes) benefit from having even
and adequate lighting in spaces that are occupied. Lighting
control in large buildings can be challenging to automate, specially as blinds and lights have to be custom programmed for
building architecture, geography, weather conditions, seasons
and so on. This paper presents an approach to self learning,
adaptive lighting control that is not preprogrammed or having
a-priori information about the building. Further, such systems
can save energy by reducing the use of artificial lighting during
daytime hours and unoccupied spaces.
Integrating daylight and artificial lighting in automated
system can be challenging. Natural lighting is not stable, even
at a fixed location; sunlight’s impact varies during times of a
day, weather changes, seasons and so on. Our system harvests
daylight and then fills in the deficiencies using artificial lights,
with attention to provision of even lighting and avoiding light
that is too bright (or has glare).
A complete lighting control system contains two interacting
modules: daylight control module and artificial lighting control
module. The aims of these two modules are different. Daylight
control module is mainly used for reducing energy costs while
artificial lighting control module is good for providing a more
comfortable working environment. Automating and balancing
the lighting control system such that uniform and stable
lighting is maintained at occupied locations while energy

978-1-4799-8069-7/15/$31.00 ©2015 IEEE

consumptions are kept as low as possible turns out to be a
hard problem.
In this paper, a Wireless Sensor Network(WSN)-based
lighting control system is introduced. We use static lights
that can be turned on/off by the system and venetian blinds
on windows, whose angles can be set by the system. The
control system is not custom programmed for the environment,
i.e. it does not know which light switch controls which
light, which blind setting affects which window or even the
physical location of rooms and walls. Thus the system is
self calibrating, and adaptive to changes in outside lighting,
weather, seasons and so on.
The formal model of the problem leads to non-linear integer
programming and proves to be NP-Hard. We use a heuristic
lighting control algorithm and show that it solves the problem
well and efficiently (and is competitive with the optimal
solution).
The rest of this paper is organized as follows. Section
II introduces the related work of the problem. Section III
formalizes the lighting control problem using mathematical
model. Section IV presents the heuristic algorithms for lighting
control. Experimental work and simulation results are discussed in section V. We conclude the paper in section VI and
talk about some future improvements of the system as well.
II. R ELATED W ORK
To make use of natural sunlight is called daylight harvesting.
Electrical blinds are set up at each window and are controlled
by feedback systems that adjust blind angles based on daylight
levels. It is used in some of the lighting control systems such
as [1], [2], etc. for energy savings. Studies show that daylight
harvesting can save lighting energy up to 77% [3]. The idea
behind is to make use of the sunlight if applicable when light
level is not sufficient [1].
The above method can adapt to environmental changes
but it usually needs a long adjustment cycle until the blind
settings are finally set up, which results in users not liking
too frequent blind movements or hunting. [4] proposed a
technique called SunCast that can better predict sunlight values
by using historical data and approximate simulation results.
The drawback of this system is that every time ambient
environment or building patterns change, the system needs

450

to collect historical data and rebuild the mathematical model,
leading to delays of many months.
WSN technologies have been applied into various areas
such as [5]. It consists of portable wireless sensor motes such
as Crossbow’s TelosB or MICAz to monitor the values of
physical conditions, such as temperature, light, humidity, and
so on. WSN data collections use several specific protocols
such as Collection Tree Protocol [6].
Some customized lighting control systems are targeted for
special cases. [7] is designed for theater arts area and [8]
was mainly designed for entertainment and media production.
Some systems use occupancy sensors [9] to switch off lights
in unoccupied positions. [10] presented a mathematical model
for lighting control problem in which a luminary impact is
continuous such as light emitting diodes(LEDs) luminaries
rather than discrete values, and the expected illumination level
is given as a single value rather than a range. [11] used
smart illuminance sensors with infrared ray communication
technology to retrieve the lighting ID binding with each
lighting fixture. Through analyzing lighting ID information,
sensors can recognize nearby luminaries, which is helpful for
systems to know the group information for each actuator.

minimize

⟨E(x, b), σ(L1 (x, b), ..., Lm (x, b))⟩

subject to

min ≤ Lj (x, b) ≤ max, j = 1, ..., m
xi ∈ {0, 1}, i = 1, ..., n

x,b

bi ∈ B, i = 1, ..., n′
By applying the ϵ-constraint method designed for solving
multi-objective optimization problems, the new objective function can be defined as:
minimize

⟨σ(L1 (x, b), ..., Lm (x, b))⟩

subject to

min ≤ Lj (x, b) ≤ max, j = 1, ..., m

x,b

E(x, b) ≤ ϵ
xi ∈ {0, 1}, i = 1, ..., n
bi ∈ B, i = 1, ..., n′

(2)

One important feature for lighting impacts is sensor readings
are additive, i.e. let Impactij to be the impact of light i on
sensor j when only light i is on, and Impactbkkj to be the
impact of daylight on sensor j when only blind k is on and
set to a particular setting bk , then we have

III. F ORMAL M ODEL
Lj (x, b) =
In this model we have a set of switches that control arbitrary
lights (one per switch) and a set of blind control switches (each
sets angle of one blind)and a set of light sensors. We have
established that illumination is additive, that is, the impact of
two light bulbs is the sum of the impact of each at a fixed
point.
Assume a place has n light switches, n′ blind switches and
m light-level sensors, placed by the human designer of the
place. The sensors are connected to the control system via a
WSN and the switches are activated via actuators connected
to the system. The physical locations of lights, blinds, sensors
and their correlations are initially unknown to the control
system.
The ultimate task is to compute the positions of switches
(lights and blinds). Let x = ⟨x1 , ..., xn ⟩ denote the assignment
for light switches where xi ∈ {0, 1} and 0 denotes off and
1 denotes on, b = ⟨b1 , ..., bn′ ⟩ denote the assignment for
automatic blinds where each blind has finite settings and 0
denotes fully off, i.e., let B to be the set of discrete blind
settings, then bi ∈ B and bi = 0 indicates blind i is fully off.
The goal is to optimize the energy E(x, b) and the comfort
C(x, b). E(x, b) can be measured by the number of artificial
lights on. For C(x, b), there are two criteria needed to satisfy,
lighting level and lighting uniformity. The former is satisfied
when every sensor reading stays in an accepted range i.e. if
sensor j’s reading is Lj (x, b), then min ≤ Lj (x, b) ≤ max.
The latter can be satisfied by minimizing the standard deviation(represented by σ) of the sensor readings. Hence the
problem can be stated as:

(1)

{
f (bk ) =

n
∑

′

Impactij · xi +

i=1

n
∑

Impactbkkj · f (bk )

k=1

(3)

bk =
̸ 0
bk = 0

1
0

Note for a specific light i, Impactij (≥ 0, ∈ Z) is always a
constant and for a specific short time period and a fixed blind
setting, we assume Impactbkkj (≥ 0, ∈ Z) also to be a constant.
[12] and [13] gave a general definition of nonlinear integer
programming problem. It can be stated as:
max/min
subject to

f (x)
hi (x) = 0, i ∈ I = 1, ..., p
gj (x) ≤ 0, j ∈ J = 1, ..., q
x ∈ Zn

(4)

where x is a vector of decision variables, and some of
the constraints hi , gj : Zn → R or the objective function f : Zn → R are non-linear functions. In equation
(2), let g1 (x, b) = Lj (x, b) − max, g2 (x, b) = E(x, b) −
ϵ, h1 (x, b, y) = Lj (x, b) − min − y = 0, y ≥ 0, y ∈ Z. It
can be transformed to:

451

minimize

⟨σ(L1 (x, b), ..., Lm (x, b))⟩

subject to

g1 (x, b) ≤ 0
g2 (x, b) ≤ 0

x,b

h1 (x, b, y) = 0
xi ∈ {0, 1}, i = 1, ..., n
bi ∈ B, i = 1, ..., n′
y ≥ 0, y ∈ Z

(5)

Since the equivalent version of equation (2) equation (5)
satisfies the format of equation (4) where the objective function
and partial constrains(g1 (x, b) and h1 (x, b, y)) are nonlinear,
our problem belongs to nonlinear integer programming problem. According to [12], it is NP-hard. Therefore, any polynomial time computed solution would be an approximation.
′
To find the best setting, a naive approach is to try all 2n+n
positions for n+n′ switches, which is unacceptable due to the
time complexity. Therefore, we propose a heuristic algorithm
for computing an approximate optimal combination of blind
and light settings. This paper focuses on the system control
during the day. In the evening when sunlight doesn’t exist,
artificial lighting becomes the only lighting source and the
detailed control approaches can be referred to the method
described in [14].
Conceptually, for better saving energy, during the day, daylight will be considered first to provide illuminance. Artificial
lights will be used to even and compensate the lighting when
necessary. Detailed control approaches will be discussed in
section IV.
IV. C ONTROL A PPROACHES
A. Calibration
In an earlier publication[14] we described the calibration
procedure for artificial lights. The calibration of the blinds
follow a similar method and hence we outline it. While there
is no daylight, artificial lights are calibrated by turning on one
switch at a time and noting the sensor readings or sensors
where this light has impact. From this data, we compute
zones (or rooms where each light is located). From the impact
measures, we are then, heuristically able to decide which light
switches should be on for the lighting to be even, even when
there is daylighting.
In this section, we describe the calibration steps for electrical blinds, which is used to calculate Impactbiji (a.k.a. blind
i’s impact on sensor j at angle bi ). We assume there is no any
other lighting source involved except daylight at this stage.
Specifically, suppose there are n′ blinds in an area, each blind
has k settings. The process of calibration is:
1) Turn off all blinds and check existing values on each
sensor
2) Turn on one blind up to a particular angle at a time
3) Calculate the blind’s impact on each sensor(sensor value
- existing value)
4) Repeat step 2 n′ × k times until all settings are counted
It is clear that the above calibration steps run in linear time(O(kn′ )) and the space complexity is (O(kmn′ )).
Compared to the exhaustive search solution which runs and
records all the combinations of blind settings(complexity is
′
O(k n )), our calibration largely saves both time and space
complexities. All impact data will be stored in database for
further processing.

three main stages: blind prediction stage, blind adjustment
stage and artificial lighting control stage. Blind prediction will
be initiated first to compute predicted blind settings based on
predicted calibration datasets. Predicted settings will be passed
into blind adjustment stage for adjusting electrical blinds in
real time. Artificial lighting will be used when inside lighting
level is either not sufficient or not distributed evenly.
The system relies on the calibration datasets stored in
database. Those datasets will be passed into data collection
and modeling server for further processing. This server is
doing some preliminary work, for example, classfication. In
this paper, we assume that the historic data used for blind
settings computation at particular day has to be picked up
from the category of that day. For example, computations
in sunny days use the historic data from sunny days. To
build a more precise computational model, the system also
collects some conditions from outside world such as comfort
conditions. New generated results(settings) will be put into
database, which are collected by the system itself for better
learning scale factors and creating predicted datasets. The
system therefore can be viewed as a learning-based closedloop control system, which will become more robust and
precise when dataset is becoming larger.
Figure 1 reflects the system control workflow. Detailed
methodologies operated at three main stages are discussed
below.
C. Blind-Prediction
Blind prediction relies on the predicted calibration datasets
generated by data collection and modeling part. It applies a
scale factor(offset) onto the historic calibrated data for similar
days and produces the one for blind prediction use.
1) Count the minimum number of blinds needed to be
turned on: suppose C = {Cibi }, i ∈ [1, n′ ] where
bi = arg max Cix
x∈B

Cix =

m
∑

Impactxij · f (x)

(6)

j=1

To reach this goal, simply find minimum number of elements in the sorted array C such that sum of them are greater
or equal than lowerbound(m × min).
2) Base-level candidates are computed: From step 1, it is
known at least w blinds is needed. Therefore, each base-level
candidate setting should have at least w elements selected
to be turned on, and the total contribution on sensors under
each candidate should be greater or equal than lowerbound
value. To solve the problem, it needs w iterations. At each
iteration, an unselected blind is picked up whose setting(i, bi )
is generated based on:

B. System Control Workflow
The goal is to compute desired blind and light settings for a
time point T . The proposed lighting control approach contains

452

Cibi >=

lowerbound − existing
, iteration ∈ [0, w − 1]
w − iteration

Data Collection &
Modeling

system inputs

database
calibration datasets

updated
data

Comfort Conditions
User Wishes
Energy Consumption

predicted calibration datasets

Blind Prediction

predictive
values

Blind Adjustment
Fig. 1.

approximate
optimal values

Artificial Lighting Control

System Control Workflow

3) More candidates are generated from base-level ones:
Since adding δ blinds to compute needs O(n′δ ) time, to ensure
low response time, we set δ = 2. Suppose each base-level
candidate has w elements which have non-zero settings, there
are n′ blinds in total, each blind has k settings. Then if each
candidate wants to pick up an unselected blind, there would be
k(n′ − w) choices. If there are s base-level candidates, finally
there would be s × k × (n′ − w) new candidates that have
w + 1 elements which have non-zero settings. Similarly when
another unselected blind is trying to be picked up, total amount
of candidates that have w + 2 elements which have non-zero
settings is becoming s × k × (n′ − w) × k × (n′ − w − 1). Thus
total number of candidates would be s + s × k × (n′ − w) +
s × k × (n′ − w) × k × (n′ − w − 1). Note for each candidate
∑
q,
Cp ≤ upperbound(m × max) where Cp = Cibi and
p∈q,

p = (i, bi ).
4) Standard deviation of sensor readings generated by each
candidate is calculated: According to equation 3, for each
candidate c, we are able to calculate its impact on sensor j
Lj (c)(note there is no artificial lighting at this point). Then it
is easy to know the standard deviation of all sensors’ readings
under c. The candidate that generates the lowest standard
deviation of sensor readings would be selected as the final
blind setting, at the blind prediction stage.
Suppose there are n′ blinds in total. Since step 1 has a time
complexity O(n′ ), step 2, step 3 and step 4 all have a time
complexity O(n′2 ), Blind-Prediction has a time complexity
O(n′2 ). Compared to feedback system, our approach predicts
and finds the approximate blind settings in a very short time
without having physical blind movements, which results in
more satisfactory feelings to users.
D. Blind-Adjustment
Since the calibration data used in the prediction stage is
predicted based on historical data, there exists errors at the
prediction stage. The adjustment step is added into the control
plan to reduce the offset in real time. To minimally reduce
the number of blind movements, each blind(if selected) will
be adjusted to maximum impact angle(when lighting is not
sufficient) or minimum impact angle(when lighting is beyond

expectation) only. Algorithm 1 describes the steps of blind
adjustment. Specially, setting denotes the predicted blind
setting generated by blind prediction stage, lur denotes the
real sensor readings under setting, lup denotes the predicted
sensor readings under setting. With the blind-adjustment step,
the system is able to adapt the environmental changes more
quickly and make corresponding adjustments more properly
compared to a pure learning system.
Algorithm 1 Blind Adjustment
Input: setting, lur , lup
Output: new blind settings
1: scale = λ × (lur / lup )
2: for (i, bi ) ∈ setting do
3:
max = maxx∈B Cix × scale, min = minx∈B Cix × scale
4:
Cibi = Cibi × scale
5:
dif fmax ← |Cibi − max|
6:
dif fmin ← |Cibi − min|
7: end for
8: if lur < lowerbound then
9:
choose values from dif fmax , sum of the values ≥ offset
10: else
11:
if lur > upperbound then
12:
choose values from dif fmin , sum of the values ≥ offset
13:
end if
14: end if
15: adjust blind angles based on the chosen values

E. Artificial Lighting Involved into the System
The biggest advantage of involving blind control into the
system is saving lighting resources. However, artificial lighting
cannot be fully ignored in some circumstances. When sunlight
is not sufficient, artificial lighting is required for completing
the inside lighting levels. When there are variations among
sensor readings, artificial lighting is needed to reduce them.
According to the objective function described in section III,
the selection of value ϵ introduced in equation 2 needs to be
adjusted based on real environments. Since computational time
is important and low time complexity is desired, ϵ is always
set to be minimum required lighting + 2 artificial lights(unless
there is a user-specific requirement), and thus the total time
complexity would be no more than O(N 2 )(N is number of

453

TABLE I
L IGHTING L EVEL C OMPARISON BETWEEN B RUTE F ORCE M ETHOD
O UR P ROPOSED M ETHOD (LUX)

actuators). The extension work described in [14] detailedly
describes how to adjust artificial lights to get a more even and
balanced environment.

# of Blinds
8
9
10
11
12
13
14

V. E XPERIMENTAL W ORK AND S IMULATION R ESULTS
A. Implementation Details
To demonstrate the feasibility and effectiveness of our
approach, we used an experimental setup. A 8ft × 8ft test cell
was instrumented with 9 lights (15W incandescent, 120V ),
two automated blinds and 9 sensors connected via a WSN
and actuators to a computer [14]. The blinds are placed on
two windows and communicate with the control server through
serial port. They can be controlled by slave commands sent
from the server. Each blind can be adjusted from 0 degree(fully
off) to 90 degree(fully on). Blind control programs are written
in Java.
B. Feasibility
The experiment is run during the day. It is to verify the
feasibility of our proposed lighting control algorithm. Control
strategies are based on the contents introduced in section IV.
One special sensor is put on the window to detect the offset
(scale) between the experimental day’s sunlight level and the
previous days. Blinds’ impacts on sensors at previous days are
stored in the database. By applying the scale factor, a predicted
calibration dataset is obtained for blind prediction. Min value
is set 100. Max value is set 130.
To check the difference between our computed results and
optimal ones, we first run our proposed method at 8 am. Then
with the same configurations, we run a brute force algorithm
checking all combinations of all actuators using real data rather
than predicted ones. Sensor readings for both experiments are
recorded. We repeat this experiment at other time points like
10am, 12pm, 2pm and 4pm.
The results are shown in Figure 2. From the results we can
see compared to the brute force, the proposed approach has a
similar light intensity performance and a very short increase
on standard deviation.
C. Adaptivity and Scalability
Our previous paper [14] has shown the artificial lighting
control part satisfies the adaptivity and scalability features, that
is when either pattern changes or amount of lights is increased,
the system is still able to compute settings in a reasonable
time. Now we need to study these two features for the blind
control part of the lighting control system. To this end, we
perform simulations on a more complex, synthetic setup, with
randomly generated impact values.
Suppose there are n′ blinds, n lights, m sensors, each blind
has k settings. To match the scenario in the previous real
experiment, we set n = 9, m = 9 and k = 7. Historic
calibration dataset A will be given in the following way: to
each blind, every setting’s impact on a specific sensor m is
randomly given an integer value from 1 to 100. The scale
factor γ is set to be a random decimal value from 1 to 2,
which matches the factor used in our real experiments. The

Min
175
191
219
229
246
277
298

Max
306
298
350
333
369
383
426

Brute Force
181
260
292
309
296
298
365

AND

Proposed Method
294
269
236
318
340
364
403

real data is also derived from the dataset A, with a scale value
in the range of 1 to 2 as well. Existing illumination level
on each sensor is randomly set from 1 to 50. If blind i’s total
impact on sensors is blindi , then the average impact of a blind
n′
∑
blindi ′
is
/n . lowerbound is selected as n′ /2 × average
k
i=1
impact, upperbound is selected as (n′ /2 + 3) × average
impact. Each artificial light’s impact on sensor is randomly
set between 1 to 100, same to the configurations described in
[14].
We did 7 groups of simulation experiments with an increased number of blinds. We compared the results getting
from brute force with our approach. TABLE I describes
the average lighting levels generated by brute force method
and our proposed method for each simulation experiment. It
indicates settings computed by our proposed lighting control
algorithm can generate the impact values that fit into the
acceptable range. Figure 3 shows the standard deviation and
computational time results for brute force method and our
proposed method respectively. From the results, we can see
the standard deviation of the proposed method is nearly 1.5
- 2 times as compared with the optimal solutions while
computational time is far less than the latter one. Compared to
the brute force, the proposed approach has an extremely better
time performance with a similar light intensity performance
and a reasonable increase on standard deviation. In other
words, the blind control part of the lighting control system
satisfies the adaptivity and scalability features.
VI. C ONCLUSION AND F UTURE W ORK
To enable automated lighting control, under varying conditions of occupancy, weather, seasons and other lighting influences it is essential to have a complete system that is effective
and adaptive. Such system must be deployable in a simple,
cost effective system without the need for customizations
and reprogramming as conditions change. It also needs to
create a comfortable environments with energy savings as a
goal. This paper presents such a complete core system that
contains both daylight harvesting module and artificial lighting
control module. It also tests the feasibility and effectiveness
in both experimental and simulated scenarios. The underlying
system can be deployed in buildings and homes without

454

40

Standard Deviation

Light Intensity (Lux)

130
125
120
115
110
105

brute force
our approach

100
95

35
30
25
20
15

brute force
our approach

10
5

90

0

8am

9am

10am

11am

12pm

1pm

2pm

3pm

4pm

8am

9am

10am

11am

Time
(a) Light Intensity

1pm

2pm

3pm

4pm

(b) Standard Deviation
Fig. 2.

Evaluation of System Feasibility
5

30

brute force
our approach

4

25

3

log10 Time

Standard Deviation

12pm

Time

20

15

10

2
1
0

brute force
our approach

5

-1
-2

0
8

9

10

11

12

13

8

14

9

10

11

12

13

14

# of Blinds

# Of Blinds
(a) Standard Deviation
Fig. 3.

(b) Computational Time
Evaluation of System Adaptivity and Scalability

excessive costs. The lighting control problem is built using
non-linear integer programming model, which is NP-hard.
A heuristic algorithm is proposed to solve the problem to
compute approximate optimal solutions.
Future improvements to the lighting control system include
the investigation of data classification, sensor placement and
scale factor learning. Current data classification only consider
seasons (such as summer or winter) or weather (such as sunny
or cloudy) conditions. More conditions such as daylight illumination level, geographical information need to be collected
for better classification. With the use of the system, more data
will be collected in the database. These data can be used to
better predict the scale factors. The results will be applied into
the current system control plan. When dataset is made larger,
the system will become more robust and precise.
R EFERENCES
[1] S. Matta and S. Mahmud, “An intelligent light control system for power
saving,” in IECON 2010-36th Annual Conference on IEEE Industrial
Electronics Society. IEEE, 2010, pp. 3316–3321.
[2] V. Singhvi, A. Krause, C. Guestrin, J. Garrett Jr, and H. Matthews,
“Intelligent light control using sensor networks,” in Proceedings of the
3rd international conference on Embedded networked sensor systems.
ACM, 2005, pp. 218–229.
[3] P. Ihm, A. Nemri, and M. Krarti, “Estimation of lighting energy savings
from daylighting,” Building and Environment, vol. 44, no. 3, pp. 509–
514, 2009.
[4] J. Lu and K. Whitehouse, “Suncast: fine-grained prediction of natural
sunlight levels for improved daylight harvesting,” in Proceedings of
the 11th international conference on Information Processing in Sensor
Networks. ACM, 2012, pp. 245–256.

[5] A. Mainwaring, D. Culler, J. Polastre, R. Szewczyk, and J. Anderson,
“Wireless sensor networks for habitat monitoring,” in Proceedings of
the 1st ACM international workshop on Wireless sensor networks and
applications. ACM, 2002, pp. 88–97.
[6] O. Gnawali, R. Fonseca, K. Jamieson, D. Moss, and P. Levis, “Collection
tree protocol,” in Proceedings of the 7th ACM Conference on Embedded
Networked Sensor Systems. ACM, 2009, pp. 1–14.
[7] C. Feng, L. Yang, J. W. Rozenblit, and P. Beudert, “Design of a wireless
sensor network based automatic light controller in theater arts,” in
Engineering of Computer-Based Systems, 2007. ECBS’07. 14th Annual
IEEE International Conference and Workshops on the. IEEE, 2007,
pp. 161–170.
[8] H. Park, J. Burke, and M. B. Srivastava, “Design and implementation of
a wireless sensor network for intelligent light control,” in Proceedings
of the 6th international conference on Information processing in sensor
networks. ACM, 2007, pp. 370–379.
[9] J. Lu, T. Sookoor, V. Srinivasan, G. Gao, B. Holben, J. Stankovic,
E. Field, and K. Whitehouse, “The smart thermostat: using occupancy
sensors to save energy in homes,” in Proceedings of the 8th ACM
Conference on Embedded Networked Sensor Systems. ACM, 2010,
pp. 211–224.
[10] A. Schaeper, C. Palazuelos, D. Denteneer, and O. Garcia-Morchon,
“Intelligent lighting control using sensor networks,” in Networking,
Sensing and Control (ICNSC), 2013 10th IEEE International Conference
on. IEEE, 2013, pp. 170–175.
[11] M. Miki, A. Amamiya, and T. Hiroyasu, “Distributed optimal control
of lighting based on stochastic hill climbing method with variable
neighborhood,” in Systems, Man and Cybernetics, 2007. ISIC. IEEE
International Conference on. IEEE, 2007, pp. 1676–1680.
[12] R. Hemmecke, M. Köppe, J. Lee, and R. Weismantel, “Nonlinear integer
programming,” arXiv preprint arXiv:0906.5171, 2009.
[13] Wikipedia. Nonlinear programming.
[14] Y. Wang and P. Dasgupta, “Designing adaptive lighting control algorithms for smart buildings and homes,” in Networking, Sensing and
Control (ICNSC), 2014 IEEE 11th International Conference on. IEEE,
2014, pp. 279–284.

455

Active Files: A Mechanism for Integrating
Legacy Applications into Distributed Systems
Partha Dasgupta
Department of Computer Science
Arizona State University
partha@asu.edu

Abstract
Despite increasingly distributed internet information
sources with diverse storage formats and access-control
constraints, most of the end applications (e.g., filters and
media players) that view and manipulate data from these
sources operate against a traditional file-based interface.
These legacy applications need to be rewritten to access remote sources, or need to rely upon ad hoc intermediary applications that aggregate the data into a passive file before
executing the legacy application.
This paper presents a simple, elegant, programmable
method for allowing natural integration of legacy applications into distributed system infrastructures. The approach
called active files, enables multiple information sources to
be encapsulated as a local file that serves as their logical
proxyw. This local file is accessed though a sentinel process, which automatically starts when the file is opened, aggregates data from multiple sources, and filters all access to
and from the file. More importantly, the integration of active
files into client applications is transparent: an active file is
virtually indistinguishable from a regular file. Active files
find a variety of applications in both distributed and nondistributed systems. We discuss active files, their semantics,
their usage and their implementations in Windows NT.

1. Introduction
The growth of the internet has been accompanied by an
explosion in information sources. These sources are distributed, store data in different formats, are dynamically
changing, and may be subject to a variety of constraints including consistency, privacy, and access-control. However,
most of the end applications that view and manipulate data
 This research was sponsored by DARPA/AFRL-Rome agreements
F30602-96-1-0320 and F30602-99-1-0517, by NSF award CCR-9876128,
and Microsoft.

Ayal Itzkovitz and Vijay Karamcheti
Department of Computer Science
New York University
fayali, vijaykg@cs.nyu.edu

from these sources (e.g., browser helper applications such
as filters and media players) assume a traditional file-based
interface, treating files simply as a passive, persistent, uninterpreted sequence of bytes. Consequently, the integration of such legacy applications into distributed systems has
required either significant code modification to use a new
distributed system-aware API or relies on the ad hoc use
of intermediary applications that isolate the end application
from the data sources. These intermediaries perform necessary operations such as access control, filtering, and format
conversion before aggregating the data into a passive file
that can be handed down to legacy applications.
However, there are significant shortcomings to both
these approaches. The first, supported by constructs such as
dynamic HTML and cgi-bin scripts to couple activity with
network-accessed files, and component frameworks such
as DCOM, and CORBA to support object-specific access
interfaces, has seen restricted use except in a few scenarios. Reasons include the vastly different semantics provided
by these constructs, their relatively complicated APIs, and
their heavyweight implementations. The second approach
although more popular, has the disadvantage that the data
collected by the intermediary is completely decoupled from
both the original sources of the information and the end application. Consequently, it is unable to track changes in the
original sources or be controlled by the end application. For
example, an end application that searches through a collection of distributed databases cannot see changes in these
databases, nor influence the progress of the search when an
intermediary first aggregates data from these databases and
presents it to the search application as a file.
In this paper, we present an elegant, easy to use, and
programmable concept, that allows natural integration of
legacy applications into distributed system infrastructures.
This association is done by a construct we call active files,
which enables distributed sources of information to be encapsulated in the form of a local file that serves as their
logical proxy. An active file has the look and feel of a regular file, but is associated with a sentinel (process) that can

act on the streams of data that enter the file on a write or
exit the file on a read. More importantly, from the perspective of the end-application, active files are indistinguishable
from non-active files. There is no reprogramming, or recompilation necessary for using active files. The sentinel
can perform a variety of tasks, including aggregating data
from distributed sources, filtering data entering/exiting the
file, and performing actions that have external side-effects.
Active files provide a convenient abstraction for alleviating several shortcomings of intermediary approaches. The
sentinel process can control flow of data between distributed
sources and the end application, enforcing consistency, privacy, and access-control constraints required by the former
while simultaneously yielding control to the end application. For example, an active file can provide the illusion of
accessing a single file even though the file data is physically
located on multiple remote sites with varied authentication
and access-control policies. In addition, it can monitor how
the application uses this file, caching only the most frequently accessed contents for performance. Moreover, the
cache can be kept consistent with any updates performed to
its contents at any of the remote sources. Note that all of
these behaviors can be expressed without modifying either
the end application or the original sources of data.
Active files can also enhance regular file functionality in
non-distributed systems. Traditionally, once a regular file is
made accessible to a process, no control can be exercised
on when or how the process uses the file. In general, the
owner/creator of a file may wish to control and log its accesses, filter the data supplied or stored, or may just want
some side effect (such as notification) to be triggered as
a result of the access. For instance, a logfile that accepts
log entries from many processes may want to enforce some
form of locking. A file containing sensitive data would like
to log every access from users, even if these users are trusted
users. Active files provide an elegant mechanism for expressing many such diverse applications.
Active files differ significantly both from approaches
such as Ufo [1] and Prospero [13] that overload/extend the
file system interface to provide seamless access to remote
files, and approaches such as Watchdogs [3] that rely on
kernel support for notification about file access. Unlike the
hard-coded functionality of the former, active files are completely programmable, enabling the expression of general
per-file behaviors in a simple, uniform, and conceptually
elegant fashion. Moreover, even though an access notification mechanism is sufficient to implement locking, filtering, and other features, the heavyweight nature of kernel involvement restricts its applicability. In contrast, active files
can be implemented efficiently at the user level, and consequently can be used for a much larger set of applications.
We describe an implementation of active files in Windows NT. Our implementation relies on the binary inter-

ception of Win32 file API calls [2, 11]. The intercepted
calls enable the sentinel process to both attend to application demands and constraints of the distributed information
sources, without requiring the active participation of either.
We show that several implementation approaches are possible that trade off cost for programming convenience.
The remainder of the paper is as follows. Sections 2 and
3 discuss semantics and uses of active files, with their implementation and programming described in Sections 4 and
Section 5. The performance overheads of active files are
presented in Section 6. Section 7 discusses related work.
Appendix A describes the Windows NT implementations.

2. Active Files
An active file is a regular file that is associated with an executable program. When an active file is opened, the associated executable is run as a sentinel process. The sentinel
connects with the user process using pipes and can directly
access both the remote information source(s) and the local
file. User process writes are sent to the sentinel along the
write pipe, and user process reads extract data out of the
read pipe. Logically, the sentinel contains two threads that
handle the flow in both directions between the user process,
the remote sources, and the local data file (see Figure 1).

2.1. File system interface
An active file is represented in the file system by two passive
files: a data file, and an executable. Directory operations
such as creating, copying, and deleting result in corresponding operations on the passive components. For instance, a
copy operation produces a second active file with the same
data and executable components as the first one.
User processes interact with a sentinel process using
standard file API calls such as CreateFile, OpenFile,
ReadFile, WriteFile, and CloseHandle. Other
API calls such as GetFileSize are passed on to the sentinel for handling as appropriate. Consequently, from the
user process’ perspective, interactions with active files are
indistinguishable from interactions with ordinary (passive)
files. Associating the file handle used by the user process to
the two pipes is the responsibility of the implementation.

2.2. Semantics of the sentinel process
The sentinel process is started and terminated when a user
process opens and closes the active file. If multiple user processes open the same active file, multiple sentinels are created, which synchronize amongst themselves in a programdependent fashion using semaphores, shared memory or
other forms of interprocess communication (IPC).

Sentinel
Process
User
Process

Write Pipe

Distributed
Services

write thread
Read Pipe

Proxy

Network

read thread
Data
read/write

Executable

Data

Active File

Figure 1. The logical view of an active file and a user process.
The sentinel process is best viewed as an entity that aggregates information from and distributes information to remote sources, serving as a two-way filter between the user
application and the information sources. The data file associated with an active file acts as a local cache. The active file
processes data sent to it by the user process, writing it to the
data part and optionally also sending it to the remote location. It also reads the data part of the active file, processing
it before making it available to the user process. The sentinel can be a null filter, in which case the active file has the
semantics of a passive file. The sentinel can also be practically any program; the system puts no restrictions of its
capabilities. Note that an active file can have an empty data
part. In this case, the sentinel either directly interacts with
the user process by producing/consuming required data, or
acts as a conduit between the application and the network.
Writing the sentinel process is straightforward, although
the specifics depend on how the abstraction is implemented.
Figure 2 shows the code for a null filter in the simplest implementation strategy, which directly mimics the logical abstraction. The sentinel process consists of two threads. The
first reads data in from the network, writes it to the data file
(the cache) and then sends the data to a read pipe that transports it to the application. The second thread reads off of the
write pipe and writes the received data to the cache as well
as forwards it on to the original source. Section 4 reports on
other implementation strategies and associated sentinel programming. These strategies support handshaking between
the user and sentinel processes, and represent more aggressive implementations where some functionality is migrated
between the sentinel and user processes.

2.3. Security implications
Although this paper does not focus on the security aspects
of active files, a few points need discussion. Opening an
active file is predicated upon access to the passive file components, and launches a program under the user-id of the

application that opened the file. This program can, of course
have any side effect, including malicious ones, such as destroying data and activating viruses. However, these effects
are no different from those initiated by any other executable
started under the same user-id. Note that the operating system already places restrictions on how the latter can access the user machine. In applications with additional security requirements, orthogonal techniques such as certificates, code signing, and sandboxing [9] can be used.

3. Uses of Active Files
The active file is a general-purpose construct that has a large
number of potential applications in both sequential and distributed systems, limited only by what can be expressed in
the sentinel process. Similar to concepts such as files, pipes,
and scripts, the active file can be used for many scenarios,
when combined with other system programs.
In general, the sentinel process can encapsulate four fundamental actions (see Figure 3 shows these actions): (1)
data generation, (2) input and output filtering, (3) aggregation, and (4) distribution. The first two primarily interact
with the local data file, while the other two involve remote
information sources. Larger applications are constructed by
composing these actions in different ways. Note that the
data file need not actually exist: the sentinel process just
creates the illusion of its existence for client applications.
Data generation The sentinel process can completely obviate the existence of a physical (passive) file that stores the
data associated with the active file. An example of such use
is when the sentinel process just contains a random number generator. In this case, the corresponding active file
appears to client programs as a data file that contains an
infinite stream of random numbers.
Input and output filtering The sentinel can introduce actions on either all or a subset of the read and write accesses
to the active file. This admits a range of uses, from keep-

HANDLE hin, hout, hcache, hpipe;
DWORD RWThrd(DWORD dir) f
char buf[1024];
DWORD rbytes, wbytes;
while( ... ) f
if (dir == READ) f
/* read from remote source */
ReadFile(hpipe,buf,1024,&rbytes,NULL);
WriteFile(hout,buf,rbytes,&wbytes,NULL);
WriteFile(hcache,buf,rbytes,&wbytes,NULL);
g else f
/* write to remote source */
ReadFile(hin,buf,1024,&wbytes,NULL);
WriteFile(hcache,buf,wbytes,&rbytes,NULL);
WriteFile(hpipe,buf,wbytes,&rbytes,NULL);

g

g

return 0;

g
int main(int argc, void *argv[]) f
HANDLE hthrd[2];
DWORD tid;
/* create handles */
hin = GetStdHandle(STD_INPUT_HANDLE);
hout = GetStdHandle(STD_OUTPUT_HANDLE);
/* handles to source, cache */
hpipe = OpenPipe(argv[1],...,...);
hcache = OpenFile(argv[2],...,...);
/* create threads */
hthrd[0]= CreateThread(0,0,RWThread,0,0,&tid);
hthrd[1]= CreateThread(0,0,RWThread,1,0,&tid);
WaitForMultipleObjects(2,hthrd,TRUE,INFINITE);

g

Figure 2. Sentinel implementing a null filter.
ing a log of actions to filtering the data read from and written into the data file. A simple example of such filtering is
a compressed file. In this case, the sentinel process compresses and decompresses the file data as it is written and
read. An advantage of this approach over compressed file
systems is that file compression can be handled on a per-file
basis with different compression algorithms used for different types of files. Additionally, both compression and
decompression can be demand-driven and performed incrementally. Note that the client application is completely unaware that it is interacting with a compressed file.
Filtering can also be used to provide a file-based interface to the Windows system registry, considerably simplifying system configuration. The sentinel checks the registry, providing a simplified version (e.g., a plain text file)
to the client application. Any modifications by the client application can in turn be parsed by the sentinel process and
translated into appropriate registry modifications.
An active file can also combine logging and filtering actions for concurrent and intelligent logging. Assume that
several processes log events using the same log file. As the

client
application

3 aggregation
input & output
filtering
distribution
2
data file
4
(logical)
2
data
generation 1
sentinel process

Figure 3. Fundamental active file actions.
sentinel receives each log record, it locks the file, writes
the record and unlocks the file. The processes generating
the logs do not need to know about log file locking. Moreover, the sentinel can perform a variety of functions in the
background such as cleaning up the logs. Achieving similar
functionality with passive files would require the client applications to essentially embed all of the code and locking
protocols for the log managers.
Aggregation The sentinel can aggregate information
from various sources, presenting it to client applications
as a conventional file. Examples of these sources include
other local or remote files, databases, network connections,
or even other processes.
An example of active-file based aggregation is seamless
access to remote files that are not accessible via networkmapped shares. The sentinel accesses the remote file using a standard protocol (e.g., FTP or HTTP), creates a local
copy, and makes the copy available to the client application.
The sentinel can also merge multiple remote files into a single local file. From the client’s viewpoint, remote file accesses are indistinguishable from local ones. Similar transparent access to remote files can also be provided without
ever making a local copy. The sentinel directly reads data
from and writes data to a network connection.
Aggregation can also be used to dynamically construct
files containing data from various sources. An example
might be an active file that reflects the latest stock quotes
(downloaded by the sentinel from a server) every time the
file is opened. Similarly, an inbox file of an E-mail program can be such that reading it causes new messages to be
retrieved possibly from multiple remote POP servers.
Distribution Sentinel processes can also distribute information to various sources, triggered by file operations
against the active file. As with aggregation, these source
include other local or remote files, databases, network connections, and other processes.
An example of active-file based distribution is an E-mail
application where the outbox-file can be programmed to
send email to a particular recipient, every time some data
is written to it. This concept can be extended such that the

sentinel process parses the data written to the file to extract
the “To” addresses and send the data to each recipient.
In general, the sentinel process can be used to produce
side effects in the active file’s environment. These side effects can be both synchronous (i.e., triggered by file operations) or asynchronous (i.e., take place in the background).

4. Implementing Active Files
We present four approaches to implement active files. Common to all the approaches is the ability to intercept system
calls (and especially those for file system). While this is a
general technique known to work for many operating systems, we describe our implementation for Windows NT.
Using interception, we redivert, at runtime, the file system API calls initially intended for the Kernel32 DLL, to
stub functions that implement the features of the active
files. We use the “Mediating Connectors” toolkit from
USC/ISI [2] to perform this interception. The toolkit allows simple runtime interception and replacement of selected Win32 API calls (or calls to any DLL) with calls to
another routine. Moreover, interception can be done in a
secure fashion such that the application cannot undo it.
The four approaches—process, process-plus-control,
DLL-with-thread, and DLL-only—reflect different partitioning of the functionality of active files between the user
and an external process, and represent different tradeoffs between runtime overheads and the convenience of programming active files. We sketch the implementation approaches
below, deferring a discussion of how the sentinel is programmed in each case to Section 5. Appendix A provides
Windows-NT specific implementation details.

4.1. Process-based implementation
The process-based implementation approach is the simple
and intuitive method, directly reflecting active file semantics. When a process performs an OpenFile (or CreateFile)
operation on an active file, the call goes to a stub routine,
which first creates a new process for running the executable
associated with the active file. The stub also passes the created process the name of the data part of the active file for
use by the sentinel. Then, the stub creates two pipes and attaches them to the standard input and output of the sentinel
process. Finally, the stub stores the handles of the pipes in
a structure, returning to the application process a fictitious
handle that points to this structure.
The ReadFile/WriteFile calls are also instrumented. The
instrumented calls translate operations on the fictitious handle into reads or writes on the appropriate pipe.
This straightforward implementation approach though
convenient to program to, suffers from two problems. First,

it can only support a subset of the file operations. Operations such as ReadFileScatter (or seek in Unix) and GetFileSize cannot be implemented as there is no method of
passing control information between the user process and
the sentinel process. The second problem is performance:
each file operation requires two protection-domain crossings. These shortcomings are alleviated in the rest of the
implementation approaches, albeit at the cost of slightly increasing the difficulty of programming active files.

4.2. Process-plus-control based implementation
This approach solves the problem of handshaking between
the user and sentinel processes by adding a control channel
in addition to the two pipes. As before, the active file stub
functions handle all interactions with the control channel
and the data pipes. The user process continues to interact
with an active file using standard file operations.
In this approach, all API requests from the application
are first transmitted to the sentinel process via the control
channel and the response of the sentinel process is read from
the read pipe. So when the application process wants to
read 50 bytes, a “read 50” command is sent to the sentinel,
and then 50 bytes are read from the read pipe. When the
application wants to write 30 bytes, a “write 30” command
is sent on the control channel and then 30 bytes are written
to the write pipe, which is retrieved by the sentinel process.
Hence, all other file operations are now passed to the sentinel process as commands with arguments. The active file
stubs read the results off the read pipe, and present appropriately packaged responses (as return codes, or structures)
to the user process. A set of headers is provided to the application programmer to enable easier handling of the control
channel when writing the sentinel process. Details of the
programming interface are discussed in Section 5. Note that
the writer of the sentinel process has complete flexibility in
deciding when to listen to the control channel. Given different models of usage, the sentinel process might choose
to eagerly inject data into the read pipe (anticipating read
requests from the user) and eagerly wait to read data from
the write pipe (anticipating write requests).
Both the process and process-plus-control implementations of active files are clean and useful. However, as
expected, and as verified later in Section 6, they suffer
from performance problems, being relatively heavyweight
due to excessive context switching and data copying. The
additional functionality provided by active files can offset these performance problems in most cases. However,
when performance is an overriding concern, active files can
be made more efficient by trading off some of their programming convenience and trespassing into the protection
boundaries of the application process. We describe two
such approaches in the remainder of the section.

Sentinel
Process
User
Process

Sentinel
Process

User
Process

User
Process

thread

User
Process
stubs
and
routines

buffers

Sentinel DLL
Sentinel DLL

control channel
Process-based

Process-plus-control

DLL-with-thread

DLL-only

Figure 4. Four implementation approaches.

4.3. DLL-with-thread based implementation

5. Programming the Sentinel Process

Instead of a stand-alone process, this approach encapsulates
sentinel fnctionality into a separate DLL, referred to as the
sentinel DLL. The initialization routine of this DLL, activated at load time, is the one responsible for orchestrating
interactions between the user application, the remote information services, and the local data part of the active file.
Opening an active file “injects” [15] the sentinel DLL associated with the file into the application and starts a thread
for running the orchestration routine. An application write
results in the write stub signalling a write to the sentinel
thread. The sentinel thread then provides a buffer to the active file write routine, which copies data from the user buffer
to this target buffer. Reads are handled similarly. While the
implementation preserves active file interface and semantics, its architecture is somewhat non-intuitive.
The sentinel process is no longer a process running separate from the application, but just a thread in the application. There is no inter-process context switching needed –
enhancing the performance. File data is not copied from
user space to kernel space and then to user space (as is the
case with pipes), instead using only one user-level copy.

We describe below the programming of the sentinel process for each of the above approaches. As expected, the
process-based implementations present active file implementers with a simpler interface than do the DLL-based
implementations. One can define a common API that spans
across all implementations; however, this complicates the
simple programming in the first two cases. We are currently
exploring automatic translation strategies for taking an active file written for a process-based implementation and producing the DLLs necessary in the DLL-based strategies.

4.4. DLL-only based implementation
Although the DLL-with-thread approach performs significantly better than the other two (see Section 6), it still has
a performance limitation: all file operations require context
switching between the requesting user thread and the sentinel thread. The DLL-only implementation approach eliminates this switch by directly routing file system API calls
to appropriate routines in the sentinel DLL.
The active file programmer can express any functionality in these routines including additional thread and process
creation. Although this approach provides the same functionality as the other approaches, the programmer needs to
be involved in the low-level details of writing the sentinel
DLL and cannot take advantage of a clean simple interface.

5.1. Process-based implementation
Figure 1, described earlier, showed how a sentinel process
is written in the process-based active file implementation.
This case uses two threads, one that collects data from information resources and/or the data portion of the active
file and makes it available to standard output, and another
thread that reads in data from standard input and distributes
it to the information sources, optionally updating the local
cache part. This approach has the advantage that the sentinel process can be developed as a standalone executable
independent of its interactions with other processes.

5.2. Process-plus-control based implementation
The sentinel process in this implementation involves only a
single thread, which typically blocks on a read on the control channel. Upon receiving a command from the application, the thread wakes up and performs the operation (which
might entail putting data into the read pipe or taking data off
the write pipe). This implementation relies on a description
of all the control messages that can be expected on the control channel. Return codes, if any, are passed back along
with the data via the read pipe. Again, the formats of such
return values must match the formats expected by the application stubs and are defined by our implementation.

5.3. DLL-with-thread based implementation
In this implementation, the sentinel process is no longer
a process but a thread, started in a routine called SentinelThrdMain that has to be defined in the DLL associated
with the active file. The thread can use three library calls to
communicate with the application. The three calls are:
1. AF GetControl – gets control messages sent by the application to read/write data or perform other file operations. These messages might affect the information
sources or only make local state changes.
2. AF SendDataToAppl – communicates read responses
or file information requests to the application.
3. AF GetDataFromAppl – gets client writes.
Similar to the process-plus-control strategy, the thread
in the SentinelThrdMain routine runs a dispatch loop using
calls to AF GetControl.

5.4. DLL-only based implementation
The programming interface in this case is simply a passthrough, which passes the API calls generated by the application unmodified to the sentinel. The sentinel is a DLL that
replaces Win32 file system calls, with calls programmed by
the active file implementor. This DLL has to provide a set of
routines such as OpenFile, CreateFile, ReadFile, and WriteFile. These routines are invoked whenever the application
accessing the file calls the corresponding Win32 functions.
This clearly is the most efficient implementation, however
places the most burden on the programmer.

6. Overhead of Active Files
This section present the performance of three different implementation of active files, in which the sentinel is a process in the local machine, an injected thread, or a direct
DLL implementation of the Read/Write file operations.

Sentinel
Path 3

Memory
Cache

Application

Path 1
Remote
Service

Path 2
Disk
Cache

Figure 5. Three critical execution paths.

Figure 5 shows three different critical paths of execution of an active file, emulating different caching options,
carried out by the sentinel. Path 1 represents the case of
no cache in the sentinel process. Whenever the application
issues a Read, a message is sent on the control channel, asking the sentinel to retrieve the data from the remote service.
When the data arrives at the sentinel, it pushes it over to the
application to satisfy the Read operation. When an application issues a Write, the buffer is sent directly to the sentinel,
which then sends an update message to the remote service.1
Path 2 represents the case where the data is cached in the
active file on disk. Here, the sentinel interacts with its local
file rather than contacting the remote service for getting or
updating data, driven by application needs.
Path 3 represents a similar case as the second path, except that the cache resides in the sentinel’s memory rather
than on disk. The sentinel code uses a memory buffer to
store application Writes or respond to application Reads.
Performance Results We ran the experiments on a cluster of PCs (300 MHz Pentium II), interconnected with
100Mbps Fast Ethernet. Figure 6 shows measurements for
an application that reads and writes fixed-size blocks from
an active file (we instrumented the application by intercepting the open/read/write/close calls and handling them as
described before). Our measurements are for a variety of
block sizes, and time 1000 calls of each.
The Read and Write results reflect the latency and bandwidth effects of the sentinel respectively. Since an application is blocked on a read operation, the overhead of interaction with the sentinel process and any processing therein
adds to the latency of the read operation. Since writes are
issued without waiting for their completion, any increase in
the overhead of a write stems from bandwidth restrictions
imposed by the sentinel interaction and processing.
As the graphs show, the different active file implementations impact the latency and bandwidth of file operations
to different extents. The process-based implementation has
the largest impact on latency and bandwidth, while the
DLL-only implementation has negligible impact incurring
the same costs as if the application were directly accessing
the information sources. The thread implementation represents an intermediate point between these extremes, trading
off performance for programming convenience. The DLL
implementation introduces only a very thin layer of code
(injected into the application process); consequently, it incurs no extra system calls or context switches.2
The process implementation’s overheads stem from the
extra buffer copying and process context switching occur1 For clarity, we only describe the unoptimized interaction between the
application and the sentinel. The implementations are optimized to improve buffer reuse and reduce synchronization overheads.
2 The Read operation, normally a system call, is sometimes diverted to
a user-mode memcpy() call improving performance over the original.

Time (µs)

|

Time (µs)

160.0

|

40.0




8

|




|

|

32
128
512
Read block size (bytes)

|

0.0

2048

Process
Thread
DLL





















|

|











|

|

|

|

|

|









|

|

80.0

|

0.0




200.0

|

70.0




240.0

120.0

|

140.0




|

210.0

|

280.0






280.0

|

|

350.0




320.0

|

|

420.0




Process
Thread
DLL

|

|

490.0






|

560.0

8

|

32
128
512
Write block size (bytes)

|

2048

(a) Sentinel uses a remote source.

240.0

|

200.0

|

160.0

|

120.0

|

80.0

|

40.0

|

Time (µs)

Time (µs)

|






|

Process
Thread
DLL




8



|

32


|

|

|

128

512

2048

0.0





|

|

|



|

|

|

0.0

280.0











90.0

|




|

180.0

320.0



|

270.0




|

360.0

Process
Thread
DLL

|

450.0






|

540.0

|

630.0

|

720.0

8

Read block size (bytes)




|





|




|

32
128
512
Write block size (bytes)



|

2048

(b) Sentinel uses a local on-disk cache.

|
|

30.0

|

Time (µs)

90.0

60.0

|

60.0

30.0

|

Time (µs)

120.0

|

90.0

150.0

8


|


|

Process
Thread
DLL





|

32
128
512
Read block size (bytes)


|
2048







0.0

|

|

|

0.0

|


|









|

120.0







|

|




180.0

|

|

150.0




Process
Thread
DLL

|

180.0






|

210.0

|

210.0

|

8





|





|



|

32
128
512
Write block size (bytes)



|

2048

(c) Sentinel uses an in-memory cache.
Figure 6. ReadFile and WriteFile overheads (in s) of different active file implementations—processwith-control (Process), DLL-with-thread (Thread), and DLL-only (DLL)—for three critical caching paths
that involve the (a) network, (b) local disk, and (c) local memory respectively. The baseline costs for
directly accessing these paths is indistinguishable from the DLL-only case and is not shown.

ring in the critical path. Completing the read operation requires a thread in the sentinel process to receive the read request, copy the buffer, send a message, and context switch
before the application thread can finish the operation. The
results for the Write case are a bit better because data
streaming hides some of the latency. However, each Write
still requires at least one buffer copy and message receive
(and two context switches between processes).
The thread implementation, on a file operation, lets the
application simply switch over to the sentinel thread, which
performs the necessary operations without requiring costly
interactions across process boundaries.
Note that the above measurements represent only baseline overheads of interacting with the sentinel in different
implementations. Since overheads incurred within the sentinel are influenced by its functionality, it is difficult to predict what these costs might be in a particular case. However,
our results above show that the active files framework on its
own does not introduce extra cost: the eventual cost of using active files is determined only by the functionality that
they implement, not by the cost of interacting with them.

7. Discussion and Related Work
Active files permit natural integration of legacy applications
into distributed systems by automatically interposing a sentinel process between a legacy application written assuming
a traditional file-based interface and one or more remote
sources of information. Our mechanism can be viewed in
the context of two groups of related work: those efforts
whose goal is to allow legacy applications to interface with
distributed information sources, and those that rely upon
kernel support to extend file-system functionality.
Accessing distributed information sources The traditional approach to using legacy end applications in distributed environments has relied upon the ad hoc use of intermediary applications that first aggregate the data from remote sources and filter it, before passing it on to the legacy
application. Although suitable for some applications, a
shortcoming of this approach is that the aggregated data is
decoupled both from the original sources of information and
the end application, which may want to control the aggregation process. In contrast, the sentinel process interacts
with both the end application and the remote sources, and
can both track changes in the original sources as well as be
controlled by the demands of the end application.
A more direct approach for accessing remote sources relies upon code modification to use a new distributed systemaware API. These APIs exemplified by component-based
approaches such as OLE [12], COM [16], DCOM [8], and
CORBA [14], allow remote sources to be accessed through
well-defined interfaces: the interface permits actions in addition to just accessing or updating the data. Despite their

generality, these frameworks have seen restricted use because of differences in semantics, complicated APIs, and
heavyweight implementations. One contribution of the active files mechanism is to encapsulate use of these frameworks within a sentinel process, which can optionally cache
some of the accesses while presenting the client with an intuitive and familiar file-based interface. Such use has the
potential of effectively addressing two of the shortcomings
of component-based approaches: ease of use and efficiency.
In some cases, access to remote sources of information
requires server-side processing, exemplified by approaches
such as dynamic HTML, cgi-bin scripts, and servlets [5].
Active files can easily emulate such behavior with the sentinel process either creating a remote process on demand,
or talking to an existing service. Moreover, active files
can support applications not possible with these approaches
such as aggregating content from various sites.
Extending file-system functionality The mediating behavior of the sentinel process can be thought of as generalizing existing operating system structures such as named
pipes, file-based interfaces to devices (e.g., /dev in Unix),
and file-based interfaces for process management (e.g.,
/proc in Solaris). Active files are similar to these structures
in providing a connection-oriented service between client
and server processes, but differ in their ability to support
the complete file-access API by virtue of the presence of the
control channel. The latter feature makes them completely
indistinguishable from normal files, facilitating their use in
many diverse scenarios. Moreover, instead of requiring that
client-server interactions always cross protection domains,
Active files admit a spectrum of implementations that tradeoff functionality versus runtime overhead.
Several approaches for extending file-system functionality have been proposed. Most of these approaches rely
on kernel modifications that enable the stacking of vnodes
and templates as in the SunOS [17], Alex [4], and Ficus [10] file systems, or notification about file access as
in the Watchdogs [3] approach. Extensible operating systems such as SPIN [7], Exokernel [6], and VINO [18] rely
on more aggressive modifications to permit introduction of
user-specified kernel functionality. Although these mechanisms provide the necessary hooks to implement active filelike functionality, they are either too heavyweight or are inapplicable on commodity operating systems. In contrast,
active files can be implemented efficiently at the user level,
permitting their use for a much larger set of applications.
Some recent efforts such as Janus [9] and Ufo [1] have
used API or system-call interception to extend file-system
functionality without requiring kernel modification. Janus
restricts the set of files a process can access and Ufo provides seamless access to remote files. In contrast to the
hard-coded functionality of these approaches, active files
are completely programmable, enabling the expression of

these and other general per-file behaviors in a simple and
uniform fashion. Moreover, unlike both these systems
that implement process-centric control, active files enable
resource-centric control: the file itself can specify the kind
of access control policies that need be implemented as well
as custom aggregation and caching behaviors.

8. Conclusions
Active files are a new extensible and programmable concept
for transparently associating actions with local data files.
Because active files use a familiar file interface for reading and writing data and can be easily programmed with
user-specified functionality, they enable the natural integration of legacy applications into distributed environments by
making it possible for such applications to seamlessly access remote services. We have described four user-level
approaches for implementing active files in the Windows
NT operating system. These approaches possess different
efficiency and programming characteristics, with the most
efficient implementation incurring negligible overheads as
compared to direct application-level operations.

References
[1] A. D. Alexandrov, M. Ibel, K. E. Schauser, and C. J.
Scheiman. Ufo: A personal global file system based on userlevel extensions to the operating system. ACM Transactions
on Computer Systems, 16(3):207–233, Aug. 1998.
[2] R. Balzer. Mediating connectors 2.0, 1998.
http://www.isi.edu/software-sciences/
safe-execution-environments.html.
[3] B. N. Bershad and C. B. Pinkerton. Watchdogs: Extending the UNIX File System. In Proc. 1988 Winter USENIX
Conference, pages 267–275, 1988.
[4] V. Cate. Alex - A global filesystem. Proc. USENIX File
Systems Workshop, pages 1–11, May 1992.
[5] WWW Consortium. http//www.w3.org.
[6] D. R. Engler et al. Exokernel: an operating system architecture for application-level resource management. In Proc.
15th Symp. on Operating Systems Principles, 1995.
[7] B. N. Bershad et al. Extensibility, safety and performance in
the SPIN operating system. In Proc. 15th Symp. on Operating Systems Principles, 1995.
[8] R. I. Frank E. DCOM : Microsoft Distributed Component
Object Model. IDG Books Worldwide, 1997.
[9] I. Goldberg, D. Wagner, R. Thomas, and E. A. Brewer. A secure environment for untrusted helper applications. In Proc.
6th Usenix Security Symposium, July 1996.
[10] R. G. Guy et al. Implementation of the Ficus replicated file
system. In Proc. 1990 Summer USENIX Conf., June 1990.
[11] G. Hunt and D. Brubacher. Detours: Binary interception of
Win32 functions. In Proc. 3rd USENIX Windows NT Symposium, July 1999.
[12] Microsoft OLE DB 2.0 Programmer’s Reference and Data
Access SDK. Microsoft Press, 1998.

[13] B. C. Neuman. The Prospero File System: A global file system based on the virtual system model. Computing Systems,
5(4):407–432, Fall 1992.
[14] Object Management Group. http://www.omg.org.
[15] J. Richter. Advanced Windows (3rd Edition). Microsoft
Press, 1997.
[16] D. Rogerson. Inside COM. Microsoft Press, 1997.
[17] D. S. H. Rosenthal. Evolving the Vnode interface. In Proc.
1990 Summer USENIX Conference, 1990.
[18] M. Seltzer, Y. Endo, C. Small, and K. Smit. An introduction to the VINO architecture. Technical Report TR-34-94,
Harvard University, Cambridge, MA, 1994.

A. Active Files on Windows NT
Active files have an active part and a passive part. Both are
saved as (passive) files, relying on NTFS streams capability
to package them as a single data file, which exhibits compatible behavior for standard file operations such as copying
and renaming. The active part is either an executable (in the
process-based approaches) or a DLL (in the DLL-based approaches), while the passive part is a data file. Note that as
discussed earlier the data file can be empty.
Whenever an instrumented application tries to open an
active file, the active part of the file is executed as a process,
or injected as a DLL into the application. To implement
this action, we leverage the fact that in Windows NT, the
operating system’s functionality is found in DLLs. Therefore, Win32 applications are complied with “loose links”,
with address resolution of API functions done at loading
time. At compile time, the linker constructs an import address table (IAT) for the process, which becomes the target
for all API calls. At load time, the appropriate DLLs containing the implementation of these APIs are loaded and the
addresses in the IAT table are resolved [2, 11, 15].
We manipulate the import table of a running process, so
that it can use active files. The idea is that all file operations
(that reside in kernel32.dll) can now pass through our injected DLL, which implements the connection between the
client application and the active part of the active files; all
without the client application being designed (or prepared)
for it. We use the Mediating Connectors [2] toolkit to provide the DLL injection and API interception functions [15].
Specifically, the client application, when executing file
operations will call the corresponding Win32 API-functions
(e.g. OpenFile, ReadFile, WriteFile, etc.). These calls are
diverted to active file implementation routines.

A.1. Implementation details
Our implementation consists of two parts, the code to be put
in the application and the code to be put in the sentinel. The
code that needs to become part of the application is put in a
DLL and injected into the application. Note that this code

is part of the active file implementation: it is different from
the code contained in the end-user application for using the
active file, which accesses the active file as it would an ordinary (passive) file. Moreover, the application-side code is
independent of the active file functionality, that is, all active
files share this code. The sentinel code also has two parts,
a part that is written by the programmer of the active file
(the actual logic of the sentinel process) and the code that
is supplied by the active file implementation (in the form of
headers and library calls.) Again, the headers and library
calls are independent of the code written by the programmer. The combination of the programmer-written code and
the supplied code is called the active file sentinel code.

A.2. Process-Based Implementations
In both process-based implementations, the application-side
code is quite straightforward. It is a set of stubs, one for
each instrumented API call. For example, the stub for
OpenFile() (or CreateFile) checks to see if the file name corresponds to an active file or not (by checking the extension).
If the file is not an active file, the stub calls the standard
Win32 OpenFile routine. If the file is an active file, either
two (in the simple process based approach) or three (in the
process-plus-control based approach) anonymous pipes are
created. These pipe handles are duplicated using the DuplicateHandle function. Next, a new process running the executable associated with the active file is started and passed
the duplicate pipe handles. Finally, a dummy handle is acquired and supplied as the return file handle to the process
that initiated the OpenFile call. An association is also made
between the dummy handle and the two or three pipe handles associated with the active file. The CloseHandle call
just shuts down the created pipes.
The application-side code for the two process-based implementations differs slightly in how it handles file operations on an open file. In the simple process-based implementation, whenever the application calls ReadFile on some
file handle, our stub gets control. The stub checks if this
ReadFile is against the dummy handle we created. If not,
we pass it to the file system. If yes, then we read the requested amount of bytes from the read pipe using the actual
pipe handle. The WriteFile operation is implemented similarly. Operations such as ReadFileScatter that do not have
direct correspondence with operations on pipes are simply
dropped (with an appropriate return code) by the client stub.
The only difference in the application stub for the
process-plus-control based implementation is that it writes
the command into the control pipe prior to reading or writing from the data pipes. These commands are picked up
by the sentinel process, which either injects data or extracts data as required at the other end. Note that the control channel allows the active file programmer to provide

application-specific functionality even for calls that do not
have corresponding pipe operations. These calls are just
passed on to the sentinel process, and potentially influence
data put into the data pipes. This implementation provides
the server code with header files containing information
about the types of control messages supported on the control pipe, and library stubs to interpret these messages.

A.3. DLL Based Implementations
Instead of having the sentinel run as an external process, the
two DLL-based implementations fold the sentinel into the
application process. The sentinel code is injected into the
process as part of the DLL that in previous implementations
contained only the client stubs. The DLL-with-thread based
implementation uses library routines to simulate the data
transfer and synchronization. There are six routines, three
on the client side and three on the server side:
1. AF SendControl: This routine sends a control message from the client to the server. The routine does
not block.
2. AF GetControl: This is used in the server to get a control message. Blocks till such a message is received.
Of course, these “messages” are implemented using
events and shared memory.
3. AF SendDataToSentinel: A client sends data to the
server (on writes) using this routine. As before, the
data is passed using a shared memory buffer.
Provides complementary
4. AF GetDataFromAppl:
functionality to AF SendDataToSentinel: Blocks until
data is received.
5. AF SendDataToAppl: Similar to (3), but in the other
direction.
6. AF GetDataFromSentinel: Similar to (5), but at the
client end.
The OpenFile API call, instead of running the executable
as in the process-based implementations, loads the DLL
containing the sentinel and starts a thread that runs a routine
called SentinelThrdMain. SentinelThrdMain is the replacement for the main() routine used in the earlier approaches.
Reading, writing and other control commands call the above
library routines as appropriate.
The DLL-only based implementation is actually even
simpler. The implementation just substitutes calls to special
routines contained in the sentinel DLL (these routines must
be named AF ReadFile, AF WriteFile, and AF Control),
whenever the corresponding file operations are called by the
user program. Note that this approach requires the implementer of the sentinel to handle all of the synchronization
and data management.

Secure Wireless Gateway1
Austin Godber and Partha Dasgupta
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ, U.S.A.
{godber, partha}@asu.edu

from miles away, and to provide wireless Internet connectivity to
the public in many metropolitan areas. The versatile nature of
802.11b has also made it an ideal target for “hacking.”

ABSTRACT
Wireless LANs (WLAN), using the IEEE 802.11b standard, have
been shown to be inherently insecure. Given the widespread use
of this type of WLAN for public and corporate access, it is
important to have an “idiot proof” method for securing WLAN
from hacking, sniffing, and unauthorized access.

Regardless of how the WLAN is being used, a standard WLAN
implementation will always suffer from problems inherent in the
underlying 802.11b standard. 802.11b is a broadcast media,
where the interface to the LAN is often a bridge (access point).
Using a bridge results in all network traffic on the local network
segment being broadcast to anyone who cares to listen. The range
of a WLAN is supposed to be limited, but the use of special
equipment (high-gain Yagi antennas) easily extends the
eavesdropping distance to miles. When a router, instead of a
bridge is used to bridge the WLAN to the LAN, only the wireless
traffic gets broadcast, but this is just marginally better from a
security point of view.

In this paper, we present a simple solution using IPSEC that
provides an inexpensive, easy to implement, wireless gateway,
and an access point that is secure. The client configuration
involves no additional software, and the simple steps needed to
configure a client are provided using a captive portal. Thus, the
gateway is designed to minimize the intrusion to the end user, will
only be slightly different from using a standard wireless network,
and will require no additional software or hardware.

1.1 WLAN Insecurities

Categories and Subject Descriptors

Not only can one eavesdrop on the network traffic, but there is
also no effective mechanism to authenticate users of the wireless
segment. Being a broadcast media, one cannot rely upon any
physical security to control access to the network; therefore, a
reliable authentication mechanism is essential.

C.2.1 [Communication Networks]: Network Architecture and
Design – Wireless communication.

General Terms
Security

The 802.11b standard tried to address both the eavesdropping and
authentication problems by using an encryption protocol called
WEP (Wired Equivalent Privacy). WEP is based on the RC4
stream cipher, which uses a static 64 or 128 bit key. A theoretical
attack [1] against WEP was published in the summer of 2001 and
an implementation [2] and open source exploits, like WEPcrack
and Airsnort, quickly followed. The published results indicate that
the WEP key can be extracted using a ciphertext-only attack, i.e.
just by eavesdropping, regardless of the length of the key.

Keywords
Wireless LAN, IPSec, 802.11b

1. INTRODUCTION
Wireless LANs (WLAN), using the IEEE 802.11b standard, are
being deployed ubiquitously at a remarkable pace. Low cost, ease
of operation, platform independence, and product variety make
them appealing to everybody.
Corporations, as well as
individuals are deploying WLANs for a variety of reasons.
WLANs are being used as general, easy to deploy extensions of
local area networks. They have been used to link remote LANs

In addition to WEP, wireless base station manufacturers attempt
to provide client authentication by filtering traffic, based on IP
addresses or MAC addresses. Both of these addresses are low
layer mechanisms that are capable of being spoofed; thus,
providing no real security. It is relatively easy to steal MAC
addresses or IP addresses by eavesdropping and using them later
to gain access to the network.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and
that copies bear this notice and the full citation on the first page. To
copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior specific permission and/or a fee.
WiSe’02, September 28, 2002, Atlanta, Georgia, USA.
Copyright 2002 ACM 1-58113-585-8/02/0009…$5.00.

Furthermore, the majority of the installations of WLANs use
default settings, i.e. no WEP key or filtering is enabled. This
provides a “backdoor” entry into a network, behind all the
1

41

This research in partially supported by DARPA/Spawar, AFOSR
and NSF.

IPsec
Windows 2000
Cllient

Internet

IPsec
OpenBSD Gateway
802.11b Card

802.11b Card

Figure 1 - Wireless IPsec Gateway
just as useful in other applications, such as office, campus or
home networks.

firewalls, leaving a gaping hole in all the security provisions that
are deployed elsewhere in the network.

A public wireless gateway, like this, faces a unique set of
challenges.
Providing public Internet access to unknown
individuals exposes the provider with two major risks. The first
risk is that since the provider has only a finite amount of
bandwidth; there is the possibility that the public users may
completely consume that bandwidth, leaving the other users
without reasonable service (denial of service). Clearly, this
situation needs to be avoided. The provider is generously
providing a free service, but it is unlikely that he would want to
do so at the expense of not being able to serve all users fairly.
The second risk is the possibility that an anonymous users may be
taking part in activities, which could result in the provider's ISP
shutting down service, for example sending spam email, attacking
other computers, and participating in DDoS attacks. Third, a
malicious user could capture sensitive information, such as
passwords, files, and other data by eavesdropping on other users’
communications, using the same WLAN.

1.2 Securing the WLAN
Despite its shortcomings, 802.11b is still an innovative and
compelling technology. One of the most interesting applications
of 802.11b is that it can be used to provide a public wireless
gateway to the Internet. Networks, like this, are appearing in
many major metropolitan areas around the world and there are
many projects and groups dedicated to expanding these networks.
Commercial providers use 802.11b to serve customers at cafés,
airports, hotels, and shopping centers.
The majority of the commercial providers provide security and
authentication, using the “captive portal.” As soon as a new user
connects to the WLAN, a router behind the access point, using
MAC filtering captures all packets from the users machine and
sends them to a particular server; regardless of the destination.
The user cannot have access to the Internet, until he or she opens
the web browser.

The security of corporate LANs and privacy of home networks
depend on the administrator’s ability to reliably prevent
unauthorized access and to avoid the monitoring of traffic.
Security through obscurity does not provide adequate protection;
even from casual network abuse. Just like the 80’s, when modem
dialups were attacked using “Wardialing”, and the 90’s had port
scanning of new Internet hosts, the early 21st century has its own
hot new communication technology: 802.11b.
With new
technology comes a new form of scanning and 802.11b scanning
is known as “Wardriving.” It has become just as effective as its
predecessors at sniffing out vulnerable networks. Anyone
equipped with a Laptop, a wireless card, and the right software
can drive around town, looking for vulnerable wireless networks.

The first http request from the browser reaches the designated
server and the user sees a web page prompting him/her for a
username and password or for a credit card number. After the user
provides the required authentication information, the MAC filter
is disabled and all the traffic from the users’ computer is allowed
to flow normally. This mechanism has some serious deficiencies.
First, if the captive portal does not use SSL to get the
authentication information, passwords or credit card numbers can
be stolen by an eavesdropper. Second, after the authentication is
completed the MAC address of the authenticated user can be
stolen and be spoofed to gain unauthorized entry (MAC addresses
on most cards are software updateable). Third, the communication
can be overheard by a third party. Finally, the above attacks,
denial of service and hacking can be easily launched.

Wardriving has become so sophisticated that users with GPS
receivers have generated maps of WLANs and share these maps
on web sites devoted to sharing WLAN information. Since so
much information about a WLAN can be gathered by an
individual and an individual can share the collected information
so easily, a WLAN will not go unnoticed for long. Therefore,
reliable security is necessary.

2. MOTIVATION
In this paper, we describe a method and its implementation that
mitigates much of the security problems of 802.11b based
WLANs. Our major design goals are ease of use and needing no
special software to be installed on the client machine. The
approach will allow any person to access the WLAN, and proceed
to use it, with minimal hassle. Both registered users and “free”
access are supported, and we can provide varying levels of quality
of service; depending on the user profile. Thus, the target of this
method is the commercial WLAN service provider; although it is

A mechanism to securely authenticate users would form an
important first step in addressing these unique problems. In
addition to client authentication, a public wireless gateway should
provide bandwidth shaping, a mechanism for securely controlling
client sessions, and all but the initial authentication should be
done transparently from the client's point of view.

42

tunnels to reach the Internet. All other wireless traffic cannot pass
though the routing module. The routing module properly routes
traffic between the subnet and the Internet. The subnet could be a
private subnet with non-routable IP addresses, or it could use real
Internet accessible IP addresses.

To summarize, the features we provided in the approach are
described below:
1.

A method of authenticating registered as well as nonregistered (casual) users, without divulging sensitive
information.

2.

Providing different levels of service for authentic users
(paying customers) and casual users.

3.

Providing secure (eavesdropping-free) channels for all
users.

4.

The use of simple, off-the-shelf components (hardware
and software) to enable the above.

Note that this method eliminates the vulnerabilities of the 802.11b
standard. The weak WEP protocol is not used. MAC filtering is
not employed. Only clients having the key for its assigned
encrypted tunnel are allowed to access the Internet. Since the
clients do not share any encrypted tunnels; there is no possibility
of data compromises, due to eavesdropping.
The routing module also provides a bandwidth shaping function.
Due to the fact that the routing module has separate tunnels with
each client; it is aware of the authenticity level of the client.
Paying customers or regular users are provided with a high level
of quality of service. Other casual or anonymous users are
provided with limited bandwidth and with some other restrictions
on port access to ensure that disruptive behavior is not facilitated.
As stated before, all traffic not coming from tunnels is redirected
to the captive portal and cannot reach the Internet.

3. APPROACH
A successful implementation of a wireless gateway would not
require the client to install any additional software, given that the
client is using a standard, off-the-shelf, business operating system.
The mechanisms employed to interact with the gateway should
require nothing more than a few simple steps for authentication.
This method should only be slightly harder to accomplish, than
using 802.11b under normal circumstances.

4. IMPLEMENTATION

Ideally, a wireless gateway would have the following components:
an encryption and authentication module, a web and DHCPD
module, and a routing and bandwidth-shaping module. These
modules could coexist in a single gateway machine or, if done
carefully, they could be separated into different machines.

The generalized implementation overview, presented above,
consisted of the following items: obtaining an IP address;
blocking non-authenticated traffic; authenticating to the gateway;
then, permitting the newly authenticated client traffic to pass; and
traffic shaping.

The encryption and authentication module would be responsible
for authenticating each client, using some cryptographically
secure authentication method. It would also be responsible for
establishing an encrypted tunnel between the client and the
gateway machine.

One possible implementation of this model would be to use SSL
for both the initial authentication and for providing the encrypted
tunnel. SSL has had great success in authenticating web traffic
and is available for many platforms and configurations. However,
the SSL approach is not feasible; unless all the networking
applications are reconfigured to use SSL; instead of regular
sockets. Therefore, using SSL would violate our goal of minimal
intrusiveness.

The web and DHCPD module would handle network services,
like web and DHCP. The main goal of the web server would be
to facilitate the initial client authentication using the captive
portal mechanism, described above.

We used IPsec for creating the tunnel, as is done with popular
implementations of the VPN mechanism. IPsec is available on a
wide number of recent platforms, although it is, perhaps, overtly
more complex than necessary [3]; it is best suited for this
gateway. Given the desired system parameters and the client
requirements, Windows 2000 was chosen as the client platform,
while OpenBSD was chosen as the gateway platform.

The basic protocol for an initial client authentication would be as
follows: a user would start his machine and it would make a
DHCP request. The gateway would give the client a lease. At
this point, since the client has not yet been authenticated, the
gateway only permits the client to communicate to the gateway
itself, all other traffic is denied. The user of the client machine
will try to establish an Internet connection, and at some point it
will attempt to establish a web connection. This HTTP traffic will
be intercepted and the client’s web browser will be redirected to
the captive portal web page.

Windows 2000 was the obvious choice for the client. It is a very
popular business operating system that already has a built in IPsec
implementation. Its popularity also makes it very likely to be in
use by the users who would be interested in using the secure
wireless gateway.

The captive portal web page will instruct the client how to
properly authenticate with the gateway machine. The client will
then follow the presented authentication protocol, which may vary
from client to client, based on its privilege level.
The
authentication information is passed between the client and the
portal using SSL. The policies of the authentication system may
permit some services to be provided to a “casual user”.

Even though Windows 2000 is the operating system used in the
test implementation of this wireless gateway, it is not the only
possible client operating system. Any OS, with a standards
compliant IPsec client that uses ISAKMP may be used to connect
to the secure wireless gateway. Many commercial IPsec clients
are available for other Windows operating systems. In addition,
there are many UNIX-like operating systems that have IPsec
capabilities.

Once the authentication is completed, the client sets up an
encrypted tunnel between itself and the routing module of the
gateway. Details about how this tunnel is implemented and set up
are included in the next section. The routing module shares a
separate encrypted tunnel with each authenticated client. The
gateway allows only traffic originating through these encrypted

Due to OpenBSD’s obscurity, our motivation for choosing
OpenBSD as the gateway operating system may not be as
obvious. OpenBSD is a freely available open source derivative of

43

filtering or firewalling. The user space programs ipf and ipnat are
responsible for configuring this functionality on our gateway.

the 4.4 BSD operating system. It is well known and respected for
its reliability and security in a networked environment. Its security
and integrated cryptography, including the KeyNote Trust
Management System [4], make it an ideal choice.

The proper configurations of the firewall and NAT modules are
essential to ensure the functionality of the gateway. The firewall
rules specify what to do with packets that reach a network
interface, based on the packet’s protocol, port number, or
destination address.

Using IPsec provides the most versatile gateway solution. IPsec
is designed to allow encrypted and authenticated network traffic
between host machines over an existing TCP/IP network. IPsec
and its associated authentication mechanism, ISAKMP will
manage the authentication and encryption phases between the
client and gateway computers. This combination will allow the
administrator of a gateway to choose the best authentication
mechanism for each case. Authentication could be based on
preshared secret keys, or it could utilize a certificate based
authentication scheme. This adaptability makes the gateway
implementation appropriate for a large number of situations.

On the wireless network interface the default rule is to drop all
incoming packets, unless the packet is:

4.1 IPsec Background
IPsec is an attempt to provide additional security to IPv4 based
networks, while maintaining compatibility with the existing
network infrastructure. It provides two new network protocols:
the Authentication Header (AH) [5] and the Encapsulating
Security Payload (ESP) [6], which together can encrypt and
authenticate network data packets.

1.

An ESP packet (tunneled IPsec traffic) from any IP
address, destined for the wireless gateway.

2.

From any IP address, destined for port 80 at any IP
address

3.

From any IP address, destined for port 500 on the
wireless gateway

4.

From any IP address, destined for port 53 on the DNS
server specified by the DHCP lease

No traffic on the wireless network will be forwarded; unless it is
IPsec traffic (ESP), as indicated by the first exception. Traffic on
port 80 (HTTP traffic) is permitted, but will be handled by the
NAT module and redirected to the web server on the gateway
machine. This combination ensures that web traffic that has not
passed through an IPsec tunnel will only reach the gateway
machine, but never reach the Internet.

IPsec has two modes of operation: the tunnel mode and the
transport mode. The tunnel mode protects the original IP header
and reveals only the IP address of the IPsec gateway machine.
The transport mode does not protect this original IP header and
encrypts only the payload.

The third exception is necessary to allow ISAKMP to perform its
security negotiations and key exchange. The final exception
allows access to DNS service.

In addition to the AH and ESP protocols, IPsec specifies a key
management protocol: ISAKMP. ISAKMP negotiates which
encryption and authentication algorithms are acceptable for use.
It then handles the initial authentication and key exchange and all
future key exchanges for the given session.

As indicated earlier, all HTTP traffic that did not pass through the
IPsec tunnel will automatically be redirected to the web server on
the gateway machine. Any client who is not using IPsec will only
see the web page provided by the gateway machine. This process
allows the gateway machine to provide instructions to clients, who
have just connected to the network, but have not yet established
an IPsec tunnel with the gateway via ISAKMP.

IPsec is an extremely flexible suite of protocols; thus, it is
extremely complex. Much of this complexity is handled by
ISAKMP, and in the end, we are provided with a versatile solution
for negotiating connections and handling authentication. We
utilize this flexibility by providing the owner of a gateway with
options between authentication mechanisms. With the help of
ISAKMP, the gateway administrator can choose either shared key
or certificate based authentication.

ISAKMP was chosen to handle the negotiation of IPsec tunnel
connections and all session key exchanges. An initial, proof of
concept, configuration establishes two different types of IPsec
connections: public and private, using shared key authentication.
The public connection will use a shared key that will be presented
to any user who does not have a pre-established arrangement with
the owner of the gateway. This key will be on the captive portal
web page on the gateway machine, along with instructions on how
to configure their clients.

4.2 Gateway Configuration
Our evaluation gateway machine is a Micron laptop with a
133MHz processor and 32 MB RAM, running OpenBSD 2.9.
The wired network interface card is a Dlink DWE 650 PC-card.
The client machine is a Compaq Laptop with a 600MHz
processor, and 128 MB memory. The wireless network interface
card, used by both the client and the gateway machine, is a
Buffalo WLI PCM-11 PC-Card. We ran all the modules, captive
portal, and routing on the single OpenBSD gateway machine.

Users, who have made arrangements with the owner of the
gateway machine, will already have their shared key for
authentication and instructions on how to use it.
To evaluate specific authentication and encryption algorithms for
performance; we have restricted our gateway to use tunnel mode,
with 3DES, as the encryption algorithm and HMAC_SHA, as the
authentication algorithm. These choices are of reasonable
security and are guaranteed to work in our cross platform
environment.

The routing module is a simple, yet essential component. The
OpenBSD Gateway must simply route all Internet destined
packets from the wireless subnet to the appropriate network
interface and vice versa. Our implementation uses a private range
of IP addresses with the Gateway using Network Address
Translation (NAT) to maintain stateful traffic information for our
subnet. Routing and NAT are both closely related to packet

4.3 Client Configuration
One of the major requirements of this project was to have a
minimal impact on the end user, who will be using Windows 2000

44

It is clear that using IPsec degrades performance. The
performance we obtained was limited due to the fact that
encryption was done on a slow machine and a faster processor at
the gateway would narrow the performance gap between the
unencrypted and encrypted communications.

(and can be extended to Windows XP). Therefore, the client
configuration is very short and simple. The client machine must
be configured to obtain its IP address automatically from a DHCP
server. Once the user logs on and attempts to view any web page,
the web browser displays the captive portal web page. At this
point, the client is not authenticated with the IPsec gateway.

As can be clearly seen, the algorithm used can greatly influence
the throughput. The gateway can specify which algorithm to use
during the ISAKMP negotiation phase. It is also obvious that
throughput will drop with the addition of extra clients to the
network.

The captive portal web page provides the users with two options
for configuring their client machines for use with the gateway.
The first option walks the users through a few steps to configure
the Windows 2000 IPsec subsystem. This uses the built in “IPsec
Policies Management” snap-in in the Microsoft Management
Console.

Despite all of these issues, the use of IPsec is still acceptable,
given the intended function of the gateway. Since the gateway is
intended to be an access point to the Internet; it stands to reason
that the gateway will have a relatively low speed, less than local
area network speeds, upstream network connection. It is likely to
be able to handle traffic to several wireless clients, on average, at
an acceptable speed.

The second configuration option allows the users to download a
Microsoft supplied tool (ipsecpol.exe) and a short script that will
automatically configure the client machine. The ipsecpol.exe tool
is the only way to configure the IPsec subsystem from the
command line. The fact that the command line tool is not
included with the operating system seems to merely be an
oversight on Microsoft’s part. This tool may already be installed
on Windows 2000 machines that have the Windows 2000
Resource Kit installed.

In cases where a higher throughput is required three solutions
exist. If the reduction in security would be acceptable, the
gateway could be configured to use a less CPU intensive set of
algorithms for IPsec. Another option would be to replace the
rather underpowered 133 MHz gateway machine with a more
substantial computer.

The public users will receive their shared key with the
configuration instructions on the SSL protected captive portal
page. Since the private class users have a pre-established
agreement with the owner of the gateway machine; the simplest
method of configuration is using the ipsecpol.exe tool and a
configuration script that contains the shared key.

Perhaps the best solution for those who need the security and the
additional throughput, would be to use a hardware cryptographic
accelerator card. There are a number of crypto cards or network
interface cards that handle DES or 3DES encryption in hardware.
This could potentially bring throughput back up very close to its
original throughput, without sacrificing security.

Of course, some situations may require a more sophisticated and
scalable authentication method. For those instances, certificate
based authentication would be ideal and easily implemented. A
certificate based solution makes more sense in an enterprise type
setting, where a public key infrastructure is already in place. A
haphazard PKI could result in a loosely bound certificate
structure, which may be vulnerable to attacks.

6. FUTURE WORK
Future work on this wireless gateway will improve configuration
convenience, expand authentication options, and perhaps include
facilities for bandwidth shaping.
An important feature of any system project should be ease of use
by those who must interact with the system in any way. Handling
gateway configuration via a simple script interface would greatly
simplify the configuration process. Simple configuration scripts
could be easily written, however with advanced authentication
techniques the automation of the configuration process becomes
more difficult.

5. PERFORMANCE
Performance of the above gateway configuration depends on the
number of client machines, the particular IPsec algorithms
utilized, and the speed of the gateway processor. Throughput
measurements are summarized in Table 1. The tests performed
involved the HTTP download of a 1 MB file between the client on
the wireless network and a server on the wired network, local to
the gateway machine. This test was done a few times to check the
sanity of the tests, but extensive testing was not performed since
there is adequate IPsec performance testing elsewhere.

Additional authentication options would add considerable value
to this wireless gateway solution. Specifically, adding the ability
to authenticate to the gateway, using a certificate-based solution,
would be ideal. A certificate-based authentication scheme would
be ideal for an enterprise, which already utilizes certificate-based
authentication elsewhere.

The client machine was a COMPAQ laptop with a 600MHz
processor and the gateway machine was a MICRON laptop with
a133MHz processor.

The addition of bandwidth shaping functionality with the goal of
providing guaranteed service to a class of users would bring this
gateway to full maturity.

Table 1 - Throughput

Unencrypted

604 KB/s

7. CONCLUSION

WEP (40bit)

458 KB/s

IPsec (DES/MD5)

355 KB/s

IPsec (3DES/SHA)

209 KB/s

Our wireless gateway solution utilizes commodity operating
systems and technologies to resolve fundamental security issues in
an innovative new networking technology. The solution requires
inexpensive gateway hardware and a proven operating platform.
To maximize end user convenience, the solution is targeted to
work with a common client operating system with no client side

45

modifications. The result is a secure, reliable, and easy to use
wireless extension to any home, school, or corporate LAN with
the intent of providing convenient Internet access to the user.
The single machine gateway solution can be tailored for specific
operating environments. With appropriate hardware support, a
sufficiently fast processor and hardware-accelerated cryptography,
this gateway solution could easily be constructed on a simple to
install and configure embedded gateway platform.
This
embedded gateway could easily replace today’s standard,
vulnerable wireless access points which are quickly being
mapped, probed, and very likely abused by potentially malicious
individuals.

8. REFERECES
[1]

Fluhrer, S., Mantin, I. and Shamir, A. 2001. Weakness in
the Key Scheduling Algorithm of RC4. Eighth Annual
Workshop on Selected Areas in Cryptography, Toronto,
Canada, August 2001.

[2]

Stubblefield, A., Ioannidis, J., Rubin, A., "Using the
Fluhrer, Mantin, and Shamir Attack to Break WEP", ATT
Labs Technical Report, TD4ZCPZZ, August 2001.

[3]

Ferguson, N., and Schneier, B. “A Cryptographic
Evaluation of IPsec.” Preprint, January 2000.

[4]

Blaze, M., Feigenbaum, J., and Keromytis, D. “The
KeyNote Trust Management System.”
RFC 2704,
September 1999.

[5]

Kent, S., and Atkinson, R., “IP Authentication Header,”
RFC 2402, November 1998.

[6]

Kent, S., and Atkinson, R., “IP Encapsulating Security
Payload (ESP),” RFC 2402, November 1998.

46

Run-Time Support and Storage Management for
Memory-Mapped Persistent Objects
Bruce R. Millard
Arizona State Univ.
Tempe, Arizona

Partha Dasgupta
Arizona State Univ.
Tempe, Arizona

Sanjay Rao
Ravindra Kuramkote
Tandem Computers
Univ. of Utah
Cupertino, California Salt Lake City, Utah

Abstract

One method of storing and using objects is to use
memory mapping. A persistent store comprises of a set of
objects that are mapped into the address space of an
application upon demand. Memory mapping makes use of
operating system support and makes the storage and
manipulation of persistent object somewhat simpler. Also,
this technique can provide sharing at a much finer grain
than what is provided by database systems.
This paper presents the design and implementation of a
persistent store called SPOMS. SPOMS is a runtime
system that provides a store for persistent objects and is
language independent. The objects are created via calls to
SPOMS and when used, SPOMS directly maps them into
the spaces of all requesting processes. The objects are
stored in native format and are concurrently sharable. The
store can handle distributed applications.
The system uses the concept of a compiled class to
manage persistent objects. The compiled class is a
template that is used to create and store objects in a
language independent manner and so that object reuse can
occur without recompilation or re-linking of an
application that uses it
A prototype of SPOMS has been built on top of the
Mach operating system. The paper presents the
motivations, the design, implementation details and also
contains a discussion of related and future work.

Conventional object-oriented programming systems
allow application programmers to structure each
application as a set of objects. They do not allow longterm storage of the objects, nor do they allow sharing and
concurrency within the object spaces. Persistent object
systems and object-oriented databases have been
developed to address some of these shortcomings.
Persistent object systems provide the ability to store,
share and dynamically reuse the object instances and
object structures that the application creates or uses.
A persistent object system depends upon a store (on
secondary memory) to provide the persistence. In addition
the store is responsible for the provision of services such
as object identity or naming, type security, retrieval,
sharing support, locking and concurrency control,
consistency control and other features necessary for the
management of long-lived objects.
Persistent stores have used many techniques to provide
the above. The two most common are the object-oriented
database and the specialized object-container based
stores. We are investigating the use of memory mapping
combined with a storage manager to provide a distributed
object store. This paper presents the design and
implementation of SPOMS which is a memory-mapped
store built on top of the Mach operating system.

Related Work

Introduction

Simply put, object-oriented programming allows
programmers to deal with program structuring units called
objects. Each object is an instance of a class. A class
defines the code and data contained in the object or
instance. The advantages of the programming paradigms
that use the object-oriented concept are relatively well
known and widely accepted.
With the maturing of object-oriented programming
systems, a few important shortcomings have been noted.
One of the primary concerns is the inability (in most
language based systems) to elegantly store, re-use and
share instances of objects that are created during the
execution of an application. Persistent object systems have
addressed this problem in a variety of ways.
Early work involved using files as object holders and

Conventional object-oriented programming systems
[Cot361 do not provide adequate support for storing and
sharing the objects and data types used in application
programs. Persistent Object Systems are an extension that
adds the features of persistence and sharability to language
defined objects [An91]. In particular, the concept allows
graphs of objects to be stored and retrieved transparently,
and allows these object structures to be exchanged and
used concurrently between separate applications. These
features are generally provided by the use of files coupled
with "pointer-swizzling'' techniques or by the use of
Object-Oriented Databases. Operating systems and
architectural support for persistent objects have also been
studied.
508

0-8186-3770493
$3.000 1993 IEEE

have been used by Cricket, MONADS and Casper. Cricket
[ShZw90] maps the entire object store to the address space
of all applications using the store. MONADS mo90 uses a
large address space to structure the object store and
selectively maps the needed objects into the application
space. Casper [Va92] is a Napier88 system implemented
on Mach that uses memory mapping and shadow paging
to provide a distributed resilient persistent store. Mach
facilities are used to map the store to the address space of
the processes using the store and then external pagers are
used to detect accesses to the mapped pages. The access
detection can be used to provide coherence in distributedmemory consistency control via copy-on-write and
commits.

providing the programmer with routines to load and store
objects from the files. Eiffel [Me921 allows a tree of
objects to be copied into a file and then the file containing
the saved objects to be re-loaded into memory. In the
Coral3 system [MeLa871 Smalltalk objects are stored in
the file system. Locking is provided at the object holders
to allow proper (controlled) sharing of persistent objects
across applications. The majority of the current work
involves a technique called pointer swizzling (or object
faulting) and the use of persistent object stores or objectoriented databases.
The pointer swizzling technique [Ri90, Co*84,
HoMo90, Wi901 is used by languages where all object
instances are accessed through pointers, one pointer per
instance. For each persistent object, the initial pointer
points to a stub routine instead of the actual object. When
this pointer is accessed, the stub routine gets executed.
The stub routine generates a fault, causing the object to be
loaded from a store (or file) and then it changes the
pointer (swizzles it) to point to the actual object.
Subsequent object accesses get the real object and not the
stub. The pointer swizzling technique is, generally,
combined with an object store or a database that actually
provides the storage management. Mneme [Mo90] is a
good example of a persistent object store and is similar to
a "low-level" database. Mneme stores objects in its own
format. Each object has a unique name (or id) and is
stored as a vector of data and pointers. The pointers are
pointers to other objects in the store, and hence are objectids. Mneme supports locking, consistency control and
distribution.
The object-oriented database is an elaborate, general
purpose object store . It stores all data as objects that can
be accessed and used by regular object-oriented languages
(typically slightly modified C++). The application
program retrieves objects from the database, processes
them and stored them back into the database. The database
handles object identity [KhCo86], storage management,
concurrency control and transaction processing. Objectoriented databases also fits the requirements of a persistent
object system. Some of the research object-oriented
databases developed in recent years include O r i o n
[Kim*88, Kim901, GemStone [Ma*86, PeSt871, EXODUS
[Ca*89], O2 [Ve*90], ObServerlEncore [Sk*86, E1*89,
HoZd871, C o s m o s [Sr*91], Iris [Fi*89], Z e i t g e i s t
[Ford*88]. Object-oriented databases have also been
commercialized and many off-the-shelf versions are
available. These include G e m s t o n e [Br*89, Sr*90],
O N T O S [Ah91], VERSANT [Ve92], OrionllTASCA,
Objectstore [St88, At901.
Some operating systems implement their own versions
of objects at the kernel level. These include A r g u s
[Li*87], C r o n u s [Sc*86] Eden [A1*85] and C l o u d s
[Da*91]. Systems such as the C o m m a n d o s operating
system [MaGu89] and Guide [Ba*91] provide extensive
language-level object support via operating system
routines.
Memory mapping of objects into applications spaces

Motivations and Goals
Our work was motivated by the need for a simple
external store for persistent objects that is:
language independent,
memory mapped and
distributed
Most persistent stores use pointer swizzling as the
access mechanism and thus are tied to the language
primitives and need modifications to both the language
and the compiler. Having language independence would
allow programmers of pre-packaged objects to write them
in any language of choice (including non object-oriented
languages like C). The objects can then be accessed by
applications that are written in other languages via a set of
runtime library routines.
Memory mapping provides a whole set of advantages.
First, all objects are stored in their native format. Their is
no need to convert from one storage format to another and
vice versa. Second, the sharing of the objects between
applications happen in real-time. That is, changes made by
one application are instantly visible in another application.
This can be termed concurrent sharing or fine-grained
sharing (as opposed to sequential sharing or coarsegrained sharing possible in load-store or database
systems). Finally, distributed shared memory systems can
be used to provide distributed implementations of the
store.

The Design of SPOMS
SPOMS (Shared Persistent Object Management
System) is an object-oriented language support system
that provides user programs with a simple, language
independent, transparent interface to persistent objects.
These objects are concurrently sharable and mapped to the
application's virtual address space as needed from
distributed persistent object stores. Persistence, sharing
and demand loading are provided without changing the
language's compiler.
SPOMS is a runtime storage manager as well as a set
of language pre- and post- processors that create the
objects to be contained in the store. Central to the design

509

called external pagers or memory managers to manage
objects that can be mapped into the virtual memory of a
task. These servers can manage permanent storage,
consistency, and sharing of data objects between tasks.
The basic abstraction used by the EMMI is the memoryobject described above. EMMI allows the operating
system to access data represented by the memory object
and leaves the control of data usage to the memory
manager.
The memory manager allows a user to create memoryobjects that are nothing but chunks of memory identified
by ports. A Mach thread maps such objects to its tasks
address space to access the memory object. Once the
object is mapped, page faults on this object are sent by the
kernel to the port identifying this object which are, thus,
received by the memory manager. If threads in two tasks
map the same memory object, then the kernel sends page
fault requests for each page only once when either of the
threads touch the page. Consequently, it becomes the
responsibility of the memory manager to maintain
consistency of these pages using appropriate locking.
SPOMS is implemented as a set of cooperating
memory-objets and external pagers. To provide
distribution the external pagers need to be programmed to
allow coherence between sites using a scheme similar to
the Mach NetMemoryServer [Fo*88]. The
NetMemoryServer is an implementation of Distributed
Shared Memory on Mach that provides multiple-reader
and single-writer coherency between memory-objects on a
network of computers.

of SPOMS is the concept of a compiled-class.
The compiled-class is a storage unit, an intermediateclass, a template as well as a language independent entry
into SPOMS. All objects stored by SPOMS must be
created and manipulated though SPOMS. Before any
object is created a compiled-class needs to be installed.
A programmer writes a definition and implementation
of one or more objects in a file. This file is pre-processed,
compiled and post-processed to yield a language
independent module called the compiled-class. The
compiled-class is then registered with SPOMS. Once
registered the compiled-class is available for access from
any application.
An application program can ask SPOMS to create an
instance of any compiled class contained in SPOMS.
When created, SPOMS places the an instance (or copy) of
the compiled class in the address space of the application.
The application can then make calls on the methods
contained in the instance. This instance becomes a
persistent instance and can be concurrently accessed by
other applications as well as used later by the application
that created it.

Memory Management and Mach
Since SPOMS manages memory-mapped persistent
objects, it needs significant memory management support
from the underlying operating system. The necessary
featuresare:
The ability to map memory segments into an
application at a specified point in its address
space.
The ability to protect a part of the memory
segment from access by the application which
has mapped it (for locking control).
The ability to be informed if an application
page-faults on a memory segment controlled by
SPOMS.
Mach [Ac*86, Ra*87] provides all the underlying
mechanisms needed to provide shared, persistent objects
that are accessible on demand at run-time and in addition
has some other interesting programming features, e.g.,
kernel threads and an expressive IPC. The Mach approach
to virtual memory management [Yo*87, Yo"891 permits
local, single-copy sharing of code and data, object faulting
and transparent on-demand object access, and is
implementation extensible to a distributed environment..
Mach virtual memory management specifics of interest
to this research are embodied in the memory-object and
the External Memory Management Interface (EMMI). A
memory-object is an object on secondary storage that can
be mapped to the virtual address space of a task. It can be
implemented by any object that has a port and that can
read and write data. The memory-object mapping routines
allow the protection associated with a memory-object to
be changed in ways that permit the locking of shared
objects.
The Mach EMMI allows user-level server programs

SPOMS Architecture
SPOMS has three major subsystems: the language
specific Compilation Subsystem, the Object Management
Subsystem, and the Runtime Subsystem. Users interact
with the Compilation Subsystem converting class
definitions (source code) into compiled-classes (compiled
and partially linked object code). Users also employ the
Compilation Subsystem to create object instantiations.
Lastly, the Compilation Subsystem compiles and links
applications that reference SPOMS objects and compiledclasses. The Compilation Subsystem relies on information
maintained by the Object Management Subsystem to
complete its work. The Object Management Subsystem
stores compiled-classes and objects, keeps track of related
information (such as entry points, size, authorizations),
and manages the capabilities that users utilize when
referencing objects and compiled-classes.
The SPOMS Runtime Subsystem responds to
application object faults on persistent objects (either data
reference faults or initial method references via the stub),
compiled-class references for dynamically loading
transient compiled-classes, and compiled-class references
for dynamically creating sharable, persistent object
instances. The Runtime Subsystem passes the capabilities
passed by the application on object or compiled-class
references to the Object Management Subsystem to map
510

system. Several sharing mechanisms exist in currently
available operating systems. Available for code sharing
are: dynamically linked libraries (several operating
systems), memory-mapped files (some Unix operating
systems), shared memory (some Unix operating systems),
and the memory-object and External Memory
Management Interface of Mach. Data sharing mechanisms
include all of these except dynamically linked libraries.
The memory-mapped file and shared memory
implementations in current Unixes make it difficult (and
usually impossible) to do object faulting. Without object
faulting all shared objects referenced by a process need to
be mapped into the process at process instantiation or
extensive library support for object loading is required. In
addition, these mechanisms do not scale to a distributed
environment at all simply. The essentials of the Mach
approach are discussed above.

SPOMS objects into the application's virtual memory. In
the case of compiled-classes, the runtime system also
needs to link/load the code associated with its methods, if
any.

Design Issues
Now that we have presented the motivation and an
overview of SPOMS there are a few design issues that
effect the design and implementation that should be
discussed. These issues are:
What is concurrent sharing and what methods
are available to achieve it?
What methods are available to implement
persistence?
How can dynamic loading (runtime access) of
objects be done?
Are there solutions to the above that admit
"simple" solutions to providing a distributed
SPOMS?
This section will address these questions in the above
order, saving most distributed operation issues to last.

Persistence
A SPOMS goal is to provide m e n [ persistence.
Transparent persistence means that the base programming
language of an object or application should not require
modification to use a persistent object. Our means of
implementing this are concentrated in the Compilation
Subsystem. The Compilation subsystem does preprocessing of source code and post-processing of the baselanguage compiler output to implement language level
transparent persistence.
Operating system support for persistence is also
necessary. One approach to providing persistence is to use
files to maintain the long-term state of persistent objects.
This approach involves extensive library code for reading
and updating such objects, as discussed above. The file
approach also requires some additional mechanism to
permit concurrent sharing of the objects. Another
approach to persistence is the database approach which
uses well-understood Database Management techniques.
While these techniques are well understood and adequate
for the purposes for which they were intended, they apply,
for the most part, only to data, have reduced flexibility
where sharing is concerned, and suffer from the same need
for extensive library code as does the file approach.
There are two alternatives to the file and database
approach: memory-mapped files and the memory-object
with external pager approach of Mach. While the
memory-mapped file approach appears at first glance to be
the same as the file approach it embeds the sharing
technique needed for SPOMS. However, the memorymapped file approach does not permit object faulting for
pure-data objects, requires a lot of library code, and is not
extensible to a distributed implementation. The Mach
approach can accomplish all these goals as discussed
above.

Concurrent Sharing

I

Concurrent sharing in SPOMS means that the methods
associated with an object can be executed concurrently. It
also means that data associated with a SPOMS object
should be concurrently available to those processes using
the methods or, in the case of pure-data objects, directly
by the referencing object, i.e., processes or SPOMS
objects. This last property requires some form of
concurrency control policy. The first property, on-theother-hand, necessitates re-entrant code. Since one of the
reasons for providing concurrent sharing of objects is to
provide single (local) copies of objects, the underlying
system should support simultaneous access to a single
copy of object code and data.
Concurrency control policies can be viewed either from
the user process perspective or as seen by the system.
From the process view the programmer asks, "What
limitations are placed on free access to the data?" or "How
transparent is the access?" On-the-other-hand, the system
implementer must know whether to provide singlewriter/multiple-reader coherency, multiple readedwriter,
or none at all. For this part of SPOMS, we chose to ignore
coherency at the local system level because the single
copy implementation should make this feasible and
because this approach will provide the greatest flexibility
to the application and object programmers. Initially, it was
felt that concurrency control could be supplied entirely by
the programmer. However, there are difficulties to
supplying a set of generic, user-level, concurrency control
primitives, which are even worse in a distributed
environment, and so eventually the system, either SPOMS
or its host environments, will need to supply such
primitives.
The mechanisms necessary to support shared code or
data ultimately must be provided by the host operating

Dynamic Loading
Making shared, persistent objects available on demand
requires two steps: first, recognizing the demand and,
511

distributed environment for SPOMS. Both the SPOMS
Run-time and Object Management Subsystems need
consideration in this regard. We assume that SPOMS
objects should be accessible during execution from
multiple sites. This access should be transparent and
timely from the application's perspective. Distributed
Shared Memory (DSM) [LiHu89, NiLo911 can
accomplish this most effectively. By using an
implementation of DSM that provides multiple-reader and
single-writer coherency, both object code and data can be
shared over the network. Other methods, e.g., RPC and
explicit message-passing library code, could be used to
provide runt-time object sharing. However, these alternate
approaches conflict with the goal of local single copies.
The SPOMS Object Management Subsystem returns
capabilities to users when a SPOMS object or compiled
class is created. To distribute the Object Management
subsystem functionality it is necessary to locate the
object's storage site given its capability. The altematives
here are to given each SPOMS object or compiled class a
network-wide unique identifier and distribute identifierlocation information or embed the location information in
its capability. We chose the latter approach. Not only does
this approach allow an object to be located at run-time but
permits SPOMS to support replicated objects
transparently.

second, actually making the object available. When an
object's method is invoked the invocation mechanism can
include code to insure the object is mapped before
attempting to execute the method's code. If data only
objects are considered, the recognition step must include
some form of objecaage faulting. The SPOMS approach
to demand recognition is to provide a stub routine that is
invoked when an object that has methods is first invoked.
This stub is used to guarantee that the object is mapped to
the applications address space. In SPOMS data only
objects are pre-mapped to a process's virtual address space
so that a reference to it will produce a "catchable" memory
protection fault.
After recognizing the demand for a dynamically loaded
object, the object still must be loaded. There are, however,
a few complications: where to load the object, how does
the application know where, what are the object's intemal
addressing needs, and are address modifications within the
object necessary (pointer swizzling)? These questions can
be answered in a more straight-forward way if they are
asked as, "Is the object loaded at the same address in
every application every time?" If they are loaded at a
predetermined address then where to load the object is
known, the application knows where the object is loaded,
and the object's intemal addresses are known a priori with
no pointer swizzling necessary. The only problems with
this approach are: how to deal with overlapping objects
and is there enough virtual address space to accommodate
unlimited (large) numbers of objects. To accommodate
overlapping objects, object remapping would need to be
done. A better approach would be to assure that all objects
are loaded at their own non-overlapping virtual address.
Given this solution, it is necessary to have a large virtual
address space (i.e., probably 64 address bits or more).
The alternative to loading objects at a fixed address is
to use position independent code (PIC) and load objects
wherever space is available. The PIC approach requires
PIC generating compilers. Depending on the compiler,
objects may still have to be loaded at the same virtual
address for all concurrently referencing applications. If
this latter property is the case then object remapping
(caused by overlapping objects) is still necessary. While
we believe that the PIC approach is more general and,
hence, preferable, the current trend to 64 bit computers
[HeJo91, HePa901 makes this unnecessary. In addition,
the use of fixed address objects has less run-time overhead
than PIC objects.
Another advantage of using fixed addresses for loading
objects is that pointers to dynamically loaded objects
always point to the correct place. Many object-oriented
language programs rely on pointers to objects, methods or
data within other objects. The use of pointers within
objects that persist, i.e., persistent pointers, adds further
complexity.

SPOMS Implementation
In this section we present a quick overview of a
prototype SPOMS that has been implemented at Arizona
State University. Since this implementation was a
prototype, we intentionally left out some of the properties
of the full SPOMS specification so that we could
concentrate on essential fundamentals. Three areas left to
future work deserve some mention - see the section on
Future Work below for our plans for these and other mas.
First, fixed, predetermined addresses for dynamically
loaded objects are provided because: we had no PIC
generating compiler, pre-linking is easy, and we aren't
convinced position independent loading is essential.
Second, the dynamic compiled-class linking and loading
implementation has been left until the details of dynamic
object loading are worked out. Third, the distributed Runtime and Object Management Subsystems have been left
till later because they will be built from the local only
versions. As a last simplification, we limited our language
support to the C and C++ languages.
As depicted in Figure 1, there are five components to
the prototype SPOMS:
The user and programmatic interface.
The Compilation Subsystem.
The Shared Object MAnager (SOMA).
The Persistent Object Store (POS).
The Run-time Subsystem @OS ,et al).

Distributed SPOMS
At this time we are only considering a homogeneous

512

Compilation Subsystem Ipc interface, SOMA has an IPC
interface to EOS for use during run-time (discussed
below) and a direct-user interface for services such as:
object directory lookups, object group creation and
management, object access control administration, and
deletion of objects. The direct-user services are built in to
SOMA while the user application part of the direct-user
interface is currently under development. SOMA, also,
acts as a database manager for the Persistent Object Store

.....
.,.-.-..
....
33
\

Persistent
Object

(posh

POS resides in the Mach file system and has three subcomponents: compiled-classes, SPOMS objects and the
database that describes them. Currently, compiled-classes
and objects are stored independently as collections of files
and the database as three others.
EOS, the External Object Server, is the essential
component of the SPOMS Run-time Subsystem. EOS is
an a Mach external pager. It provides the initial memorymapping of a SPOMS object (a Mach memory-object) into
the application's virtual address space. Since this initial
call only provides the mapping, thereafter EOS provides
any subsequent page-fault handling of references to the
object.
The SPOMS Run-time Subsystem encompasses the
application, the External Object Server (EOS), SOMA and
POS. As mentioned the application contains library code,
i.e., the branch table and the stub routine, linked in by the
Compilation Subsystem. The stub routine interacts with
EOS to provide demand memory-mapping of an object.
During compilation/linking, the branch table is initialized
to invoke the stub routine rather than an object's methods.
The stub routine passes the object's id to EOS. EOS then
requests object information from SOMA (if not already in
use by another process), maps the object to the user's
virtual address space, and returns the necessary
information to the stub routine. The stub routine fixes-up
the branch table so that subsequent method invocations for
the object go to the object (rather than the stub) and then
branches to the method code with the original request.
To accommodate a large number of applications and
objects, EOS uses two groups of threads: one group
handles paging activity and the other group handles user
requests. In addition to the stub requests the second group
handles user requests to explicitly release an object.
Explicit object release is provided so that a process can
reuse the space set aside for the object. Under "normal"
circumstances this space remains allocated until the
process terminates.
SOMA is a complicated program. It not only maintains
the POS but also interacts with the user (as mentioned
above), interacts with EOS at run-time, interacts with the
Compilation Subsystem, manages the object and
compiled-class capabilities used to access them, manages
the virtual address space usage for applications and
objects, and controls the coherence of the POS database.
Because of the number of tasks SOMA has, SOMA uses
three sets of threads to do its job. Each set of threads has a
manager and some worker threads. The manager thread

Figure 1. Prototype SPOMS Architecture.
The Programmatic Interface to SPOMS is part of the
specifications of the program files passed to the
Compilation Subsystem. Source code and Import files for
applications and objects are needed to create compiledclasses and applications. Export files contain definitions of
the methods exported by the object.
The Compilation Subsystem is responsible for
generating the compiled-class and registering it with
SPOMS. It compares object invocation semantics in the
object source code with that in the export file and
massages formal method specifications to include
parameters needed by the run-time code. After
compilation the number and names of methods, size of the
object code, the object code is post-processed to generate
the compiled-class. The complied class is then sent to
SOMA using Mach IPC.
The Compilation Subsystem is also responsible for
massaging the application source code to provide for the
transparent, dynamic loading and, subsequent, access of
SPOMS objects. This is accomplished using the Import
files. Import files contain the object and method names
that the application (or SPOMS object) wants to use. This
information is sent to SOMA by the Compilation
Subsystem prior to pre-processing to retrieve low level
object and entry point identifiers. The information
retrieved from SOMA is used to create a branch table.
(Invocations of SPOMS objects are always made through
the branch table.) The Compilation Subsystem massages
the source to use the branch table. In addition to the
branch table a stub routine is linked into the application
for use during run-time. The Compilation Subsystem uses
yacc to parse import and export files and lex to massage
the source code files before compilation.
SOMA is responsible for storing compiled-classes and
keeping track of the creation of compiled-classes by the
compilation subsystem. SOMA also handles the naming
of compiled-classes and instances. In addition to the
513

monitors a port associated with one of its external
interfaces, i.e., user interface (UI), EOS interface (EOSI),
Compilation Subsystem interface (CSI). When a message
arrives at the port, the manager selects one of the worker
threads to handle the details of the request. The EOSI
threads are given priority over the other threads because
their work involves requests form executing applications.
To reduce execution time overhead the POS database has
a copy both on secondary store and in memory. Since the
threads access the POS database, possibly, concurrently
SOMA has a set of concurrency control routines that work
in conjunction with a commit control package to maintain
coherence of the database. SPOMS capabilities have
encoded information that is used to access the information
in the database. These capabilities contain the user id of
the creator or owner of the object and a low level SOMA
resource identifier.
SOMA has one last function that requires special
mention. In this version of SPOMS all objects are linked
at a predetermined address. This address may be explicitly
specified by the user or may be selected by S0M.A. If the
selection is left to SOMA, the user still has the option of
selecting an Address Group from which SOMA selects the
address. Address selection within a group occurs from the
next available address for that group. The address space of
an object belonging to a group is guaranteed to not overlap
that of any other object in that group. This functionality
was provided to allow some degree of flexibility in object
placement and to attempt to reduce addressing conflicts
while we only have 32 bits of address space available.

independent, transparent interface to persistent objects.
The objects are concurrently sharable and mapped to the
application's virtual address space as needed from
distributed persistent object stores. In this system
persistence, sharing and demand loading are provided
without changing the language's compiler.
The system's primary purpose is to provide an
environment that has extensive facilities for object reuse
and distributed object sharing. SPOMS extends the
concept of code reuse to data and to the concurrent sharing
of compiled code and persistent data in a distributed
environment. Object reuse, thus, provided without
requiring recompilation or re-linking of an application that
uses it. A new stage of object (called a compiled-class)
was also described.

References
Acceta, M., R. Baron, D. Golub, R. Rashid, A.
Tevanian and M. Young, "Mach: A New Kemel
Foundation For UNIX Development," Proceedings
of Summer Usenix, July 1986.
[Al*85] Almes, G., A. Black, E. Lazowska and J. Noe, "The
Eden System: A technical Review," IEEE
Transactions on Software Engineering, SE-1 1,
January 1985.
Andrews, G., Concurrent Programming: Principles
and Practice, Benjamin-Cummings, 1991.
Atwood, T., "Two Approaches to Adding Persistence
to C++," Proceedings of the 4th International
Workshopin Persistent Object System, 1990.
[Ba*91] Balter, R., J. Bernadat, D. Decouchant, et al,
"Architecture and Implementation of Guide, an
Object Oriented Distributed System," Computing
Systems, Vol. 4 No. 1, Winter 1991.
[Br*89] Bretl, R. , et al., "The Gemstone Data Management
System, '' Object-Oriented Concepts, Databases and
Applications, W. Kim and F. Lochovsky eds.,
Addison-Wesley Publishing Co. 1989.
[Ca*86] Carey, M., D. Dewitt, D. Frank, G. Graefe, J.
Richardson, E. Shekita and M. Muralikrishna, "The
Architecture of The EXODUS Extensible DBMS,"
Proceedings of the 12th International Workshop on
Object-Oriented Database Systems, September 1986.
[Ca*89] Carey, M.J., D.J. DeWitt, J.E. Richardson, and E. J.
Shekita, " Storage Management for Objects in
EXODUS," Object Oriented Concepts, Databases
and Applications, Frontier Series, Addison Wesley,
1989.
Cox, B., Object-Oriented Programming: An
[Co86]
EvolutionaryApproach, Addison-Wesley, 1986.
[Co*84] Cockshot, W . , M. Atkinson, et al, "Persistent Object
Management System," Software Practice and
Experience, January 1984
[Da*91] Dasgupta, P., R. J. LeBIanc, M. Ahmad and U.
Ramachandran, "The Clouds Distributed Operating
System," IEEE Computer, November 1991.
[ElNa89] Elmasri, R. and S. Navathe, Fundamentals of
Database Systems, Benjamin-Cummings, CA, 1989.
[Ac*86]

Future Work
As mentioned, three areas were left out of the
prototype. As soon as we can obtain C and C++ PIC
generating compilers, we will begin experimenting with
relocatable objects. Relocatable objects will allow use to
pursue methods of dealing with persistent pointers.
Second, the dynamic compiled-class linking and loading
and the effects of compiled-class inheritance are currently
under study for inclusion in SPOMS. Third, distributed
Run-time and Object Management Subsystems are also
currently under consideration. While distributed shared
memory for the run-time system seems to have a wellunderstood implementation, the consequences on SOMA,
EOS and the need for user level synchronization are still
being studied.
We also plan to extend the number of applications
using SPOMS and add to the number of languages
supported. Adding support for more languages will force
us to address issues in heterogeneous language semantics.
At this time we have no plans to include heterogeneous
computers.

Conclusions
This paper has presented a runtime support system that
provides user programs with a simple, language

514

Rashid, R. J., A. Tevanian, M. Young, D. Golub, R.
J. Baron, D. Black, W. I. Bolosky and J. Chew,
"Machine IndependentVirtual Memory Management
for Paged Uniprocessor and Multiprocessor
Architectures," Proceedings of the Second
Internafional Conference on Architectural Support
for ProgrammingLanguages and Operating Systems,
October 1987.
Richardson, J., "Compiled Item Faulting: A New
Pi901
Technique for Managing 110 in a Persistent
Language," Fourth International Workshop on
Persistent Object Systems, Sept. 1990.
Rosenberg, J., "The MONADS Architecture: A
Bo901
Layered View," Fourth International Workshop on
Persistent Object Systems, September 1990.
[Sc*86] Schantz, R. E., R. H. Thomas and G. Bono, "The
Architecture of the Cronus Distributed Operating
System," Proceedings of Sixth International
Symposium on Distributed Computer Systems, May
1986.
[ShZw90] Shekita, E., and M. Zwilling, "Cricket: A Mapped
Persistent Object Store," Fourth International
Workshop on iersistent Object Systems, September
1990.
[Sk*86] Skarra, A. , S. Zdonik and S. Reiss, "An Object
Server for an Object-Oriented Database System,"
Proceedings of Workshop on Object-Oriented
Databases, 1986.
[Sr*90] Sriram, D. , R. Logcher, et al., "A Case Study in
Computer-Aided Cooperative Productive
Development," IESL Technical Report IESL-90-01,
1990.
[Sr*91] Sriram, D. , et al., "COSMOS: an Object-Oriented
Expert System Shell," Technical Report, IESL, MIT,
1991.
Stroustrup, B., "Parameterized Types for C++,"
[St881
Proceedings of the 1988 Unix C + + Conference,
1988.
[Va*92] Vaughan, F. , et al., "Casper: a Cached Architecture
Supporting Persistence," Computing Systems, Vol. 5,
No. 3, Summer 1992.
Versant Object Technology, C++/VERSANT
[Ve92]
Reference Manual, Version 1.7, June 1992.
[Ve*90] Velez, F., V. Darnis, D. DeWitt, P. Futtersack, G.
Harris, D. Maier and M. Raoux, "Implementing the
O2 Object Manager: Some Lessons," Proceedings of
the Fourth International Workshop on Persistent
Object Systems, September 1990.
Wilson, P, "Pointer Swizzling at Page Fault Time:
mi901
Efficiently Supporting Huge Address Space on
Standard Hardware," Technical Report, Electrical
Eng. and Computer Science Department, University
of Illinois at Chicago, 1990.
[Yo*87] Young, M., A. Tevanian, R. Rashid, D. Golub, J.
Eppinger, J. Chew, W. Bolosky, D. Black, R. Baron,
"The Duality of Memory and Communication in the
Implementation of a Multi-processor Operating
System," Proceedings of the 11th Symposium on
Operating Systems Principles, Nov. 1987
Young, Michael W., "Exporting a User Interface to
[Yo891
Memory Management from a CommunicationOriented Operating System," Ph.D. Dissertation,
School of Computer Science, Carnegie Mellon
University, CMU-CS-89-202,November 1989.

Forin, A., J. Barrera, M. Young and R. Rashid,
"Design, Implementation, and Performance
Evaluation of a Distributed Shared Memory Server
for Mach," Carnegie-Mellon University Technical
Report, CMU-CS-88-165, Aug. 1988.
pord*88] Ford, et al., "Zeitgeist: Database Support for ObjectOriented Programming," Proceedings of 2nd
International Workshop on Object-orienfedDatabase
Systems, 1988.
[HeJo91] Hennessy, J. and N. Jouppi, "Computer Technology
and Architecture: An Evolving Interaction," IEEE
Computer, September 1991.
[Hepa901 Hennessy, J. and D. Patterson, C o m p u t e r
Architecture a Quantitative Approach, Morgan
Kaufmann, 1990.
Hoskins, A. L. and J. E. B. Moss, "Towards
[HoMo90]
Compile-Time Optimizations for Persistence,"
Fourth International Workshop on Persistent Object
Systems, Sept. 1990.
[HoZd87] Hornick, M. and S. Zdonik, "A Shared Segmented
Memory System for an Object-Oriented Database,"
ACM Transactions on Ofice Information Systems,
Vol. 5, No. 1, 1987.
[KhCo86] Khoshafian, S. and G. Copeland, "Object Identity,"
Proceedings of Object-orienfed Programming
System, Language and Application, 1986.
[Kim*88] Kim, W., N. Ballou, H. T. Chou, J. Garza and D.
Woelk, "Integrating an Object-Oriented
Programming System with a Database System,"
Proceedings of the OOPSLA-88 Conference,
September 1988.
[Kim901 Kim, W., "Architectural Issues in Object-Oriented
Databases," Journal of Object-Oriented
Programming, March/April1990.
[Li*87] Liskov, B., D. Curtis, P. Johnson and R. Scheifler,
"Implementationof Argus," Proceedings of the 11th
ACM Symposium on Operating System Principles,
November 1987.
LiHu891 Li, K. and P. Hudak, "Memory Coherence in Shared
Virtual Memory Systems," ACM Transactions on
Computer Systems, 7:4, pp 321-359, November 1989.
[Ma*86] Maier, D. , J. Stein, A. Otis and A. Purdy,
"Development of an Object-Oriented Database,"
Proceedings OOPSLA-86, November 1986.
Marques, J. A. and P. Guedes, "Extending
[MaGu89]
the Operating System Supportfor an Object-Oriented
Environment," Proceedings OOPSLA-89, October
1989.
[Me921 Meyer, Bertrand, Eiffel: The Language, prentice
Hall, New York, 1992.
[MeLa871 Merrow, T. and J. Laursen, " A Pragmatic System for
Shared Persistent Objects," Proceedings of the
OOPSLA-87 Cogerence, October 1987.
[MOW] Moss, J. E. B., "Design of the Mneme Persistent
Object Store, " ACM Transactions on Information
Systems, V01.8, No. 2, April 1990.
[NiLo91] Nitzberg, B. and V. Lo, "Distributed Shared
Memory: A Survey of Issues and Algorithms," IEEE
Computer, August 1991.
[Pest871 Penney, D. J. and J. Stein, "Class Modifications in
the Gemstone Object-OrientedDBMS," Proceedings
of the OOPSLA-87 Conference, October 1987.

[Ra*87]

po*88]

515

IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust

A multi-factor approach to securing software on client
computing platforms
Raghunathan Srinivasan,
Partha Dasgupta

Vivek Iyer, Amit Kanitkar

Sujit Sanjeev

Jatin Lodhia

Microsoft Corp.

Goldman Sachs Group Inc.

Google Inc.

Arizona State University
Tempe, USA
{raghus, partha}@asu.edu

each instance of the application different, and attacks that work
on one instance do not work on another. ASLR [2] is an
example, but that idea is taken to finer degrees of granularity
on each stack frame and heap frame in this work. The second
approach is to build obfuscation and shielding methodology to
make stealing secrets from client machines harder. For
example, memory in client applications holds encryption keys,
passwords, sensitive data, and private keys in case of PKI
implementations. Stealing secrets by copying zones of memory
is particularly simple (keys for example, have high entropy).
Two approaches to hide keys more effectively are provided in
this work. The third approach is Remote Attestation. Remote
attestation is a set of protocols that uses a trusted service to
probe the memory of a client computer to determine whether
one (or more) application has been tampered with or not. These
techniques can be extended to determine whether the integrity
of the entire system has been compromised. While the idea
sounds easy, given the power of the adversary (malware), a
very careful design has to be done to prevent the malware from
declaring to the server that the system is safe.

Abstract—Protecting the integrity of software platforms,
especially in unmanaged consumer computing systems is a
difficult problem. Attackers may attempt to execute buffer
overflow attacks to gain access to systems, steal secrets and patch
on existing binaries to hide detection. Every binary has inherent
vulnerabilities that attackers may exploit. In this paper we
present three orthogonal approaches; each of which provides a
level of assurance against malware attacks beyond virus
detectors. The approaches can be added on top of normal
defenses and can be combined for tailoring the level of protection
desired. This work attempts to find alternate solutions to the
problem of malware resistance. The approaches we use are:
adding diversity or randomization to data address spaces, hiding
critical data to prevent data theft and the use of remote
attestation to detect tampering with executable code.
Keywords—Computer security, attacks, memory randomization,
secure key storage in memory, remote attestation, integrity
measurement.

I.

INTRODUCTION

The magnitude of the threat from malware, especially on
“consumer computing” platforms is well known and well
understood. Malware today can hide from virus detectors, steal
secrets, live stealthily for extended periods of time, effectively
prevent removal efforts, and much more. The ability to run
sensitive applications and store sensitive data on consumer
platforms is very critical. A smartly designed malware can
have more power than any other application in the system [1].
A sensitive application running on an un-trusted environment
can be subject to a variety of attacks. Attestation of client
computers using hardware attestation modules, or using
hypervisors to scan computers have not had much success due
to the cumbersomeness of the solutions. In this paper, three
orthogonal approaches are represented, each of which which
provides a level of assurance against malware attacks beyond
virus detectors. The approaches can be added on top of normal
defenses and can be combined for tailoring the level of
protection desired. This work attempts to find alternate
solutions to the problem of malware resistance.

The methods presented in this work have been
implemented completely in software. We opine that such
approaches, judiciously combined with traditional malware
prevention methods, can make computing safer without adding
much overhead to the applications and operating systems. In
the remainder of the paper, we present work related to our
approaches and provide brief overview and implementation
details of our approaches.
II.

The first approach is the ability to provide “software
diversity” for legacy software. Currently a malware designer
can perform offline analysis of an application to discover
vulnerabilities in it. These vulnerabilities can be exploited to
launch various kinds of attacks on multiple systems. Every
copy of an application that is shipped to consumers is exactly
the same, and contains the same weaknesses in the same binary
locations. Software diversity breaks up the uniformity making
This material is based upon work supported in part by the National
Science Foundation under Grant No. CNS-1011931. Any opinions, findings,
and conclusions or recommendations expressed in this material are those of
the author(s) and do not necessarily reflect the views of the NSF.

978-0-7695-4211-9/10 $26.00 © 2010 IEEE
DOI 10.1109/SocialCom.2010.147

RELATED WORK

There are several variants of the buffer overflow attacks
like stack overflows, heap corruption, format string attacks,
integer overflow and so on [3]. C and C++ are very commonly
used to develop applications; due to the efficient “unmanaged”
executions these languages are not safe. A vast majority of
vulnerabilities occur in programs developed with these
languages [4]. Randomization is a technique to inject diversity
into computer software. The first known randomization of
stack frame was proposed by placing a pad of random bytes
between return address and local buffers [5]. Random pads
make it difficult to predict the distance between buffers and the
return address on the stack. An attacker has to launch custom
attacks for every copy of the randomized binary. Other than
simple ASLR [2] all randomization techniques involve recompiling the program and hence do not work for binary
distributions. We implement techniques to randomize the stack
and the heap frames, at a fine grain level, of every application

993

in the system (without access to source code). This in turn
serves to randomize relative addresses within every copy of the
application, making construction of overflow based attacks
difficult.

for the attacker to fake any results that are produced by the
attestation code.
III.

Many approaches have been developed to ensure secure
software based key management. Centralized key storage
employs techniques where the generation, storage, distribution,
revocation and management throughout the lifetime of the key
happens on a single central machine. This offers advantages
such as ease of backup, recovery, and securing a single
machine secures the keys [6]. Secondary storage or a
detachable device has also been used to hide keys by
encrypting the key with a very strong password, but this can be
attacked using a key logger that logs typed passwords on the
system [7]. In the same research another method is presented
to store keys in the system, it involves breaking the key into
multiple parts and distributing it in different places.
Distributing the key reduces the memory entropy footprint
making it harder to detect the pieces that comprise the key.
Another solution for key management is distributed key storage
using secret sharing [8]. This could be an option for large
organizations, but it is not feasible for normal end users of
cryptography. The solutions proposed in this paper try to
prevent key theft from memory disclosure attacks, irrespective
of the number of copies of the key present in memory, and
prevents key exposure altogether.

RANDOMIZATION OF STACK FRAMES AND ALLOCATED
HEAP CHUNKS

If the memory layout in each copy of the binary was
different on every machine it would make it extremely hard
attacks to occur. The relative addresses of memory objects
remain constant for every application, this allows an attacker to
determine the number of bytes of offset from the current
location of memory and launch attacks. The stack frame and
heap chunks are randomized in this work so that every
allocation of stack or heap has a different amount of “gap”
added to the allocation. We do not assume existence of source
code. The stack frame is randomized post-compilation by
rewriting the memory accesses in binary to relocate the
locations of the stack variables. The heap chunks are
randomized by changing the system library code and the kernel
code.
A. Randomization of Stack Frame
We randomize the size of the run-time stack frames to
make every instance of an executing binary have a unique
memory layout. The binaries are instrumented by analyzing
the disassembly of the code segment in a binary. We do not
inject additional bytes of code in the binary but rewrite existing
bytes in the code segment. The randomization process is
carried out after the application binary is installed at the enduser machine. During randomization only those instructions
that are relevant to the run-time stack need to be rewritten. By
shifting the vulnerable character buffer down by a random
amount, the distance between the return address and the buffer
becomes different for every copy of the binary. This makes it
difficult to use the same attack string against different copies of
the binary. Instructions of the following type need to be
rewritten in the binary when adding a random pad:

Integrity measurement involves checking if the program
code executing within a process, or multiple processes, is
legitimate or has been tampered. It has been implemented
using hardware, virtual machine monitors, and software based
detection schemes. Some hardware based schemes operate off
the TPM chip provided by the Trusted Computing Group [9,
10, and 11]. Some hardware based schemes operate off a coprocessor which can be placed into the PCI slot of the platform
[12 and 13]. Terra uses a trusted virtual machine monitor
(TVMM) and partitions the hardware platform into multiple
virtual machines that are isolated from one another [14].
In Pioneer [15] the integrity measurement is done without
the help of hardware modules or a VMM. The verification
code for the application resides on the client machine. The
verifier (server) sends a random number (nonce) as a challenge
to the client machine. The response to the challenge
determines if the verification code has been tampered or not.
The verification code then performs attestation on some entity
within the machine and transfers control to it. Pioneer assumes
that the challenge cannot be re-directed to another machine on
a network; however in many real world scenarios a malicious
program can redirect challenges to another machine which has
a clean copy of the attestation code. In its checksum
procedure, Pioneer incorporates the values of Program Counter
and Data Pointer, both of which hold virtual memory
addresses. An adversary can load another copy of the client
code to be executed in a sandbox like environment and provide
it the challenge. This way an adversary can obtain results of
the computation that the challenge produces and return it to the
verifier. In this paper remote attestation is implemented by
downloading new (randomized, obfuscated, binary) attestation
code for every instance of the operation. This makes it difficult

•

Instructions that create space on the stack frame for
local variables and buffers.

•

Instructions that de-allocate space used by the locals of
the function on the stack frame. These instructions are
executed right before the function returns. In most
functions, the stack de-allocation is done implicitly by
the leave instruction that restores the stack pointer to
the frame pointer; hence we don’t need to explicitly
modify any instruction for correct de-allocation of the
random pad memory.

•

Instructions that access local variables and buffers on
the stack frame. All local variables and buffers are
accessed with the help of the frame pointer EBP. All
stack locals are located below the frame buffer at lower
addresses in the Intel x86 architecture. Because of the
random pad, the local buffers shift further down from
the frame pointer.

Implementation: The prototype randomizer was developed
in C and compiled using gcc, the objdump disassembler was
used for disassembly of binaries. Fig. 1 shows the flow of the
randomizer. The binary file is fed to the disassembler, its

994

Extract relevant
instructions

Binary
file

Disassembler

Sub routine separation and
instruction analysis

Parser

Randomizer

Randomized
binary

Figure 1. Workflow of randomizer

output is parsed for identification of instruction operands that
need to be modified in the binary. The parser separates out and
analyzes each sub-routine in order to accomplish fine-grained
randomization such that every function is padded separately
with a random padding conforming to the constraints of that
specific function. Thereafter these instructions are directly
rewritten in the binary to change the layout of the stack frames
at run-time. The prototype works as a 2-pass randomizer. In
the first pass, each sub-routine is analyzed to determine the
maximum padding that can be provided to that routine. Every
instruction in the routine that accesses memory regions has an
upper limit on the relative address that can be accessed by it.
We process every instruction and check the maximum
available random pad to that instruction. The least of these
values becomes the pad for the function. The randomizer also
looks for instructions that are sensitive to alignment of memory
operands and takes a conservative approach of not randomizing
sub-routine with very sensitive instructions. The random pad is
then clipped to the nearest factor of 32 to resolve many
alignment requirements of several instructions. In certain cases
it is also necessary to place an upper-limit on the maximum
padding given to each sub-routine as it increases the chances of
a stack overflow causing the process to crash. In the second
pass, the randomizer goes through the instructions in the
disassembly and locates them in the executable binary file.
While tracing every instruction the randomizer also keeps track
of the sub-routine in which the instruction is present. With the
help of the data structure built for every sub-routine during the
first pass, the randomizer statically rewrites and instruments the
corresponding instruction in the binary executable.

on the stack allocating instruction is only one byte, we can
allocate a maximum of 128 bytes of stack with such an
instruction. If the routine already allocates 128 bytes of stack,
then its stack frame cannot be randomized.
B. Randomization of allocated heap chunks
We single out the library functions that play a vital role in
heap memory management (the functions that perform the free,
allocate and resize operations). These functions are wrapped
with randomization code, the library entry points of these
functions are hooked to point to the wrappers. Source code
access of binaries is not utilized as the underlying heap
memory management mechanism is modified. We adopt a
dual random padding strategy per every memory allocation.
This is done by appending a random pad below as well as
above the pointer to the heap memory chunk returned by the
allocation algorithm. Fig. 2 gives an overview of this.
Implementation: This approach was implemented by
identifying the memory management functions to be patched in
the GNU C library. The most important of these functions that
we identified are – malloc(), free(), realloc() and memalign().
Other related functions which we identified are calloc(),
valloc() and pvalloc() which need not be patched as they are
based entirely on malloc() in the current version of the GNU C
library. We generate two random integers i and j, which are
multiples of 8 to respect the internal memory alignment rules.
The upper limit of the random numbers generated can be
selected heuristically. These two random integers are added to
the size parameter contained in the original request, making an
allocation call for i+ j+ original request. A successful malloc()
operation returns the pointer to a newly allocated memory
chunk. The value of the pointer returned to the calling function
(user application) is shifted by i bytes. These two random
numbers are stored so that other memory management
functions like free() and realloc() know the size of the allocated
unit and calculate the actual start address of the memory chunk
and thus the boundary tag information stored above it. Once a
call to free() is made by the requestor function, execution
enters the free() public wrapper. We extract the value i set by
malloc() which lies just above the chunk address passed as an
argument to free(). Using this value, the original starting
address of the memory chunk’s user space can be calculated.
This value can be passed to the internal free() function called
here forth. Similar calculations are made for realloc() and
memalign() library calls.

We randomized copies of the following applications: Open
Office, pidgin, pico, ls, gcc, netstat, ifconfig, route, xcalc, tail,
date, nslookup, sum, head, wc, md5sum, pwd, users, cat,
cksum, hostid, logname, echo, size, firefox, viewres, xwininfo,
oclock, ipcs, pdfinfo, pdftotext, eject, lsmod, clear, vlc, and gij.
Thus we cover both “console” applications and graphics
applications. It is clear that we have a proof of concept
implementation that can cover all applications that we tested it
on. All the binaries used in testing were release quality,
optimized utilities that are part of Linux distributions. Since
we only manipulated the size of the run-time stack, we do not
expect this approach to have any run-time performance penalty
whatsoever. We found that on an average the randomizer
modified the run-time stack of more than 75% of the subroutines in every application. Some of the routines are not
randomized as we take a conservative approach to not make
any changes to routines containing alignment sensitive
instructions such as FXSAVE. We are also restricted by the
length of the stack allocation instruction as we do not inject
additional bytes into the program. If the width of the operand

IV.

SECURE KEY STORAGE USING VMM AND DISK STRIPING

Stealing secrets from memory of executing programs is an
effective method for circumventing security systems,

995

ensures that the application calling the cryptographic hypercall
is legitimate.

Previous chunk size
Size/Status (In use)
Chunk
pointer

Implementation: The system was implemented on the
Linux 2.6.23 kernel. We used the same guest and host
operating systems.
We utilized the lguest modules to
implement the VMM. We used the DES module to perform
cryptographic routines. We added all the required hypercalls
for performing these operations. We performed attestation by
creating a nop placeholder for the attestation code inside the
guest kernel. When the guest kernel issues an attestation
request, the VMM provides executable code which is to be
injected at this location. The guest kernel uses copy_to_user()
call to inject bytes at the specified placeholder. The sequence
of operations to perform cryptographic operations can be
summed up as:

Random Pad 1
USER DATA

Random Pad 2
Previous chunk size
Size/Status (In use)
Forward pointer
Backward pointer

•

The guest user space application issues requests for
performing cryptographic operations. It passes the
type of operation and the data to be operated upon as
input. The request issues a software interrupt and the
context switches from guest user space to guest kernel
space.

•

The guest kernel forwards the request to the trusted
VMM. The secure VMM injects code into the running
guest kernel on receiving a request. The injected code
is responsible for returning the guest physical address
where the user space program is loaded. It also brings
the user application’s pages into memory in case they
are swapped out. The injected code is changed every
time an attestation request is made.

•

Control is transferred back to the VMM which reads
the contents of the user space program directly from
the memory, using the address obtained above. The
secure VMM computes and compares the hash value
of the memory content to pre computed hash values
obtained from the original binary image of the
program.

•

The requested cryptographic operations are performed
by the VMM, and the results written back to the
memory location passed by the guest kernel, the guest
kernel copies the results to the guest user space.

Unused memory
Randomized memory chunk
Figure 2. Allocated memory chunks in the heap

especially encryption. Encryption keys have to be stored as
clear-text in memory when the application performing
encryption executes. This information is susceptible to
memory forensics based attacks. For example, the AACS
encryption for high-definition DVD players uses a master key
to keep other keys, and uses the unbroken AES encryption
method. It has been documented that a particular HD-DVD
encryption key was stolen from memory [16]. In this paper we
present two methods of safely storing encryption keys. This
first involves hypervisor support. The keys are placed in a
hypervisor below the operating system. The second method
presented in this paper is disk striping, in which the keys are
kept on disk every time the key is not actively in use (even if
the key handling application is running). The keys are split
into tiny chunks of a few bits each and placed in hidden blocks
of the disk that are not part of the file system.
A. Hypervisor Based Key Storage
Virtual Machines (VM) are primarily used for executing
multiple machines on a single hardware platform. Virtual
Machine Monitors (VMM) are also widely used to provide a
trusted computing base to security sensitive applications as
they can provide logical isolation between different VMs. We
offload the cryptographic operations of the system to the secure
VMM. The guest operating system(s) interact with the user
and receive the request to perform cryptographic operations.
The guest operating systems make a hypercall to the VMM to
perform the actual encryption/decryption. We leverage the
secure nature of the VMM where a guest OS cannot read the
contents of the host VMM, but the VMM can read the contents
of every guest OS. Any attacks launched by the attacker
Mallory are restricted on the guest space. Due to this Mallory
cannot get information about the key using forensic analysis on
the guest memory. We also implement attestation which

B. Disk StripingBased Storage
We hide the key on secondary storage by writing to the
unused sectors of files on the hard disk without using the file
system calls of the OS. Each file on the storage media has an
end of file (EOF) marker. The OS allocates space for files in
disk blocks and does not reclaim the space beyond the EOF
marker if a particular block is partially used. The space beyond
the EOF on a sector is used in this research to store the key.
We do not add key information to random files, but we place
new files during installation of the code. Once the storage area
is determined, we scatter the key throughout this area such that
the attacker cannot retrieve the key even after knowing the
sector where it is stored. We refer to the location of a bit of the
key in scattered array as bit-address. Every bit of the key has a
bit-address associated with it which forms the bit-address
pattern. This bit-address pattern is unique to every installation

996

Position of
Fetch blocks

Location of Jump to
fetch blocks

Total size of all fetch
blocks

0x21B7

0xCF7

0xA40

0x21DA

0xCC5

0xA03

0x21B1

0xCF2

0xA86

0x21C0

0xCBB

0xA1F

Operating system support is not used in this framework as it
would require a secure OS, or a loadable kernel module that
performs the attestation. The first scenario is unlikely to occur,
and the second scenario would require frequent human
interaction to load the kernel modules in the system (to prevent
automatic exploits of the kernel loading modules). The
approach taken in this paper is designed to detect changes
made to the code section of a process. Trent is a trusted entity
who has knowledge of what the structure of an un-tampered
copy of the process (P
P) to be verified. Trent provides
executable code (C
C) to Alice which Alice injects on P. C takes
overlapping MD5 hashes on the sub-regions of P and returns
the results to Trent.

TABLE I. DATA FROM DIFFERENT INSTALLATION INSTANCES

of the system. If the bit-address pattern is stored directly,
then it can be easily read by the attacker. Instead of storing the
bit-address pattern, the bit-address fetching block generates the
address of each bit at run time. There is one logical block of
code per bit address to be generated. The bit-address fetching
block is generated for the application during installation time.
We generate a bit-address and then generate the bit-address
fetching block to match the addresses. This is achieved by
performing random arithmetic operations on the result value till
we achieve the desired value for that particular bit. The
arithmetic operations are chosen from a pool of operations.
We also obfuscate the location of the bit-address fetching block
in the binary.

A software protocol provides opportunities for an attacker
(Mallory) to forge results. The attacker (Mallory) can perform
a replay attack in which Trent is provided with results that are
the response to a previous attestation challenge. Mallory may
tamper with the results generated by the attestation code to
provide the expected results to Trent. Mallory may re-direct
the challenge to another machine which executes a clean copy
of the application P, or Mallory may execute the challenge
inside a sandbox to determine its results and send the results to
Trent. This paper addresses these by incorporating specialized
tests and generates random code to mitigate the effects of these
attacks. We obtain machine identifiers through system
interrupts to determine whether the challenge was replayed.
We take measurements on the client platform that determine
whether the attestation code was executed in a sandbox.
Lastly, we perform extensive randomization of the attestation
code by changing the arithmetic operations and memory
locations read by every instruction.

Implementation: This part of the research was implemented
using the gcc compiler, and Ubuntu 8.04 Linux OS. We were
able to randomize the locations of the bit-address fetch blocks
and the locations to the jump call to all the blocks. Table 1
shows a small sample of data from the multiple installation
sequences. It can be seen that the size of each bit-address
fetching block was also different in each installation.
V.

When P contacts Trent the remote attestation starts. Trent
provides P with a binary attester code C (that is signed by
Trent). C is generated each time it is “provided” and is
composed of randomized and obfuscated binary, and hence it is
difficult to reverse engineer C. Since C is downloaded code,
Trent has to be trusted not to provide malware to Alice. P runs
C, and C hashes the memory space of P in random overlapping
sections and then encrypts the hashes with a nonce that is
contained in C and sent to Trent. The nonce is located in a
different location for each instance of C making it impossible
for a compromised P to mimic C’s behavior. When Trent gets
the results from C it verifies that P has not been tampered with
and it is executing correctly. Now that we know P is correct, P
can be entrusted to verify the integrity of the security sensitive
application that execute on Alice. Fig. 3 shows an overview of
the framework.

INTEGRITY MEASUREMENT USING REMOTE
ATTESTATION

Remote attestation is a framework for allowing a remote
entity to obtain integrity measurements on an un-trusted client
machine. In order for remote attestation to work, we need to
have access to a remote verifier (Trent) that is a trusted
(uncompromised) host and is accessible over a network. A
single trusted host can be used for a large number of clients;
ensuring that sensitive applications running on the clients are
not tampered by malicious code on the client machines. In this
framework we determine whether an application (P
P) installed
by Alice (the client) has been modified. In the consumer
computing scenario, we envision the deployment of “attestation
servers”, where an end-user contracts with the service provider
to test the safety of the applications on the end-platform. This
is similar to the way virus detector updates are done today.

Implementation: The remote attestation scheme was
implemented on Ubuntu 8.04 (Linux 32 bit) operating system
using the gcc compiler; the application P and attestation code C
were written in the C language. C computes a MD5 hash of P
to determine if the code section has been tampered.
Downloading MD5 code is an expensive operation as it is
large, also MD5 code cannot be randomized as it may lose its
properties, and hence the MD5 code permanently resides on P.
To prevent Mallory from exploiting this aspect a two phase
hash protocol is implemented. Trent places a mathematical
checksum inside C which computes the checksum on the

Remote Attestation has been implemented traditionally
with the help of hardware modules [9, 10, and 11] and the use
of VMM [17] has also been suggested. It involves the trusted
server (Trent) communicating with the hardware device
installed on the client’s (Alice) machine. However, these
modules are unsuitable for legacy platforms, and have the
stigma of Digital Rights Management attached with them. The
use of a VMM also requires greater hardware resources and
compute power. In our framework remote attestation is
implemented entirely in software without kernel support.

997

Future work would be to extend the techniques described
above to enhance the security of the operating system itself by
randomizing the OS and/or providing remote attestation of the
OS code. Such attempts would of course raisse other issues
such as a variance in the OS code due to hardware differences
amongst makes and models of computers. Work is needed to
study the techniques of combining the approaches into a single
system with multiple defense mechanisms. Randomization of
the address space does imply changes to the code itself and
hence code on different machines may have different MD-5
hashes and these have to be traced by the remote attester.

Trent

Alice
Application
P
injected
code C

REFERENCES

Figure 3. Remote attestation

[1]

region of P containing the MD5 executable code along with
some other selected regions.
The operations of the
mathematical checksum are randomized by creating a pool of
operations for every instruction, and selecting one instruction
randomly from each pool. Trent receives the results of the
arithmetic checksum, verifies the results, and sends a message
back to C which proceeds with the rest of the protocol if Trent
responds in affirmative.
The checksums are taken on
overlapping sub regions to make prediction of results more
difficult for Mallory.
This creates multiple levels of
indeterminacy for an attack to take place. To determine
whether C was bounced to another machine, Trent obtains the
address of the machine that C is executing on. Trent had
received an attestation request from Alice, hence has access to
the IP address of MAlice. If C returns the IP address of the
machine it is executing on, Trent can determine if both values
are the same. Although IP addresses are dynamic, there is little
possibility that any machine will change its IP address in the
small time window between a request by Alice to
measurements being taken and provided to Trent.
C
determines the IP address of MAlice using system interrupts. An
attacker will find it difficult to tamper with interrupts on a
system given that there are many interrupts and changing their
implementation is not simple. To determine that P was not
executed in a sandbox environment, C determines the number
of processes having an open connection to Trent on the client
machine. This is obtained by determining the remote address
and remote port combinations on each of the port descriptors in
the system. C performs communication to Trent by using the
socket descriptor provided by P. This implies that in a pristine
situation there must be only one such descriptor on the entire
system, and the process utilizing it must be the process inside
which C is executing. If there is only one such process, C
computes its own process id and compares the two values. An
error is reported if these conditions are not met.
VI.

[2]
[3]
[4]
[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

CONCLUSION

In this paper we presented solutions to detect the presence
of compromised binaries, storing secret keys in memory, and
modifying the memory layout of binaries entirely in software.
These three scenarios represent most of the attacks that occur
in the computing world. Mitigating these situations allows us
to improve the security of end user platforms. These
techniques can be used independently and are not dependant on
the software vendors to provide source code of their products.

[16]
[17]

998

R. Srinivasan and P. Dasgupta, “Towards more effective virus
detectors,” Communications of the Computer Society of India, vol 31-5,
pp. 21-23. August 2007.
Web link, ASLR: “Address space layout randomization,” retrieved on
April 25 2010, http://pax.grsecurity.net/docs/aslr.txt
J. Foster, V. Osipov, N. Bhalla, and N. Heinen, “Buffer Overflow
Attacks: Detect, Exploit, Prevent,” Syngress Publishing: 2005.
R. Seacord, “Secure Coding in C and C++,” Addison-Wesley: 2005.
S. Forrest, A. Somayaji, and D.H. Ackley, “Building diverse computer
systems,” HOTOS ’97: Proceedings of the 6th Workshop on Hot Topics
in Operating Systems (HotOS-VI), IEEE Computer Society, pages 67–
72, May 1997.
Web Link, "nCipher Solutions," Retrieved on April 20 2010,
http://iss.thalesgroup.com/Resources/Product%20Data%20Sheets/keyAu
thority.aspx
A. Shamir and N. van Someren, “Playing ‘Hide and Seek’ with Stored
Keys,” Third International Conference on Financial Cryptography, pp.
118-124, 1999.
R. Canetti, Y. Dodis, S. Halevi, E. Kushilevitz and A. Sahai, “Exposureresilient functions and all-or-nothing transforms,” Advances in
Cryptology – EUROCRYPT, pp. 453-469, 2000.
R. Sailer, X. Zhang, T. Jaeger, and L. Van Doorn, “Design and
implementation of a TCG-based integrity measurement architecture,”
Proceedings of the 13th USENIX Security Symposium, pp. 223 -228,
2004
K. Goldman, R. Perez, and R. Sailer, “Linking remote attestation to
secure tunnel endpoints,” STC '06: Proceedings of the first ACM
workshop on Scalable trusted computing, pp. 21 - 24, 2006.
F. Stumpf, O. Tafreschi, P. Röder, and C. Eckert, “A robust integrity
reporting protocol for remote attestation,” WATC’06: Second Workshop
on Advances in Trusted Computing, 2006.
L. Wang and P. Dasgupta, “Kernel and application integrity assurance:
Ensuring freedom from rootkits and malware in a computer system,”
Advanced Information Networking and Applications Workshops, pp.
583 – 589, 2007
N.L. Petroni Jr., T. Fraser, J. Molina, and W.A. Arbaugh, “Copilot-a
coprocessor-based kernel runtime integrity monitor,” Proceedings of the
13th conference on USENIX Security Symposium, vol 13, 2004.
T. Garfinkel, B. Pfaff, J. Chow, M. Rosenblum, D. Boneh, “Terra: A
virtual machine-based platform for trusted computing,” ACM SIGOPS
Operating Systems Review, pp. 193 – 206, 2003
A. Seshadri, M. Luk, E. Shi, A. Perrig, L. Van Doorn, and P. Khosla ,
“Pioneer: Verifying code integrity and enforcing untampered code
execution on legacy systems,” ACM SIGOPS Operating Systems
Review, vol 39 -5, pp. 1 – 16, 2005
Web link, “HD-DVD Content Protection already hacked?” Retrieved on
April 4 2010, http://www.techamok.com/?pid=1849
R. Sahita, U. Savagaonkar, P. Dewan, and D. Durham, “Mitigating the
Lying-Endpoint Problem in Virtualized Network Access Frameworks,”
Springer: Managing Virtualization of Networks and Services, pp. 135 –
146, 2007.

The Five Color Concurrency Control
Protocol: Non-Two-Phase Locking
in General Databases
PARTHA DASGUPTA
Georgia Institute of Technology
and
ZVI M. KEDEM
Courant Institute of Mathematical

Sciences

Concurrency control protocols based on two-phase locking are a popular family of locking protocols
that preserve serializability
in general (unstructured)
database systems. A concurrency control
algorithm (for databases with no inherent structure) is presented that is practical, non two-phase,
and allows varieties of serializable logs not possible with any commonly known locking schemes. All
transactions are required to predeclare the data they intend to read or write. Using this information,
the protocol anticipates the existence (or absence) of possible conflicts and hence can allow non-twophase locking.
It is well known that serializability
is characterized by acyclicity of the conflict graph representation
of interleaved executions. The two-phase locking protocols allow only forward growth of the paths in
the graph. The Fiue Color protocol allows the conflict graph to grow in any direction (avoiding twophase constraints) and prevents cycles in the graph by maintaining transaction access information
in the form of data-item markers. The read and write set information
can also be used to provide
relative immunity from deadlocks.
Categories and Subject Descriptors:
action processing
Additional

H.2.4 [Database

Key Words and Phrases: Locking,

Management]:

Systems-concurrency,

trcms-

serializability

1. INTRODUCTION
In this paper we present a locking protocol for database concurrency control that
applies to general databases. The locking strategy is non-two-phase. The protocol
uses five kinds of locks. The five categories of locks are read locks, intent locks,
write locks, and two types of marker locks. We use five colors to assign mnemonic
names to these locks.
This research was partially supported by NSF under grants MCS 81-04882 and MCS 81-10097, by
ONR under contract N0014-85-K-0046, and by NASA under contract NAG-1-430.
Authors’ addresses: P. Dasgupta, School of Information
and Computer Science, Georgia Institute of
Technology, Atlanta, GA 30332; Z. M. Kedem, Department of Computer Science, Courant Institute
of Mathematical
Sciences, New York University, New York, NY 10012.
Permission to copy without fee all or part of this material is granted provided that the copies are not
made or distributed for direct commercial advantage, the ACM copyright notice and the title of the
publication and its date appear, and notice is given that copying is by permission of the Association
for Computing Machinery.
To copy otherwise, or to republish, requires a fee and/or specific
permission.
0 1990 ACM 0362~5915/90/0600-0281$01.50
ACM Transactions on Database Systems, Vol. 15, No. 2, June 1990, Pages 281-307.

282

l

P. Dasgupta and Z. M. Kedem

The protocol requires each transaction to predeclare the data it intends to read
or write. This can be achieved by static data analysis by the query compiler. The
predeclared read set and write set need not be the exact read/write sets, but can
be a superset of the actual sets. The performance, however, depends upon the
closeness of the predeclared sets and the actual sets.
Unlike the two-phase locking protocol, the Five Color protocol uses early
release of read locks and late acquisition of write locks to enhance concurrency.
The early release of read locks makes this protocol violate the two-phase locking
rule. This feature, however, has to be closely controlled, as it can cause nonserializable behavior. It is widely known that two-phase locking “is in a sense,
the best that can be formulated” [34, p. 3801. The optimality of two-phase locking
implies that in the absence of any information
about the transactions or the
database, all locking protocols must be two-phase. The Five Color protocol allows
non-two-phase locking by keeping track of transaction ordering using the predeclared read and write sets, and by addition of a check called validation. We
first present a brief introduction
to the concepts of serializability,
the model of a
multiuser database, the factors that limit concurrency in two-phase locking, and
some related work. Section 2 contains a comprehensive description of the Five
Color protocol, including an intuitive description of how it functions and why it
ensures serializability.
Section 3 explains the formal properties of the protocol
and derives a proof of correctness, and Section 4 outlines a modification to the
protocol. Sections 5 and 6 deal with deadlocks and livelocks, and Section 7
discusses performance issues.
1 .l Serializability
A database is viewed as a collection of data items, which can be read or written
by concurrent transactions. Interleaving of updates can leave the database in an
inconsistent state. A sufficient condition to guarantee correctness of concurrent
database access is serializability of the actions (reads or writes) performed by the
transactions on the data items; that is, the interleaved execution of the transactions should be equivalent to some serial execution of the transactions [3,31]. In
this paper, we assume serializability
to be the criterion of correctness.
Locking of data items is one of the methods of achieving consistency in the
face of concurrent updates. For databases with no inherent structure (e.g.,
databases not organized as DAG’s, trees, etc.), the two-phase locking protocol is
the most popular locking protocol. However, two-phase locking is restrictive with
respect to the amount of concurrency it allows.
Informally,
a log is a sequence of actions issued by various transactions on
several data items in the database. The transaction actions may be interleaved
with one another. Serializability
is a syntactic property of a log. It has been
shown that recognizing serializability
is an NP-complete problem [28, 291. The
NP-completeness of the serializability
recognition problem implies that we cannot have a scheduler that allows all serializable logs and disallows nonserializable
ones and works in polynomial time (unless P = NP). However, certain subclasses
of serializable logs are efficiently
recognizable in polynomial time. Efficient
algorithms can be built that control the actions of transactions to ensure that
the logs produced by a set of transactions fall into one of these easily recognizable
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

The Five Color Concurrency Control Protocol

l

283

classes of serializability.
The two-phase locking protocol is one such algorithm
which produces a class of polynomially recognizable serializable logs, namely, the
two-phase locked logs.
1.2 The Model
A database D is a set of distinct data items (x1, x2, . . . , x, 1. A transaction system
T is a set of transactions (T1, TP, . . . , T,) that operate on the database. The
read set (write set) of a transaction Ti is the set of all items Ti reads (writes).
A transaction that intends to read (or write) a data item x issues a read (or
write) request to the transaction manager. The transaction manager is responsible
for determining whether or not granting of the request may cause a violation of
the correctness criterion (generally serializability).
The transaction manager then
takes appropriate action by granting, rejecting, or delaying the request.
A trace of a transaction is a sequence of successful read and write requests it
makes to the transaction manager. A trace is written as a sequence of actions of
the form Ri(x) or Wi(x), where Ri(x) (or Wi(x)) means a transaction Ti issues a
read (write) on data item x. Note that we are not interested in the values read or
written, but in the syntactic properties of the string of reads and writes on the
data items.
We will assume at most one read and at most one write per data item in any
trace. If a transaction reads as well as writes a particular data item, we assume
the read will precede the write. Multiple reads and writes are handled in an
obvious way: The first read is used to read the value of the data item and store
it in local storage, and the other reads on the same data item are processed
locally. Similarly, all writes except the last one are written to local storage, and
the last one appears on the log. Thus there is no loss of generality.
A log of a transaction manager is a sequence of reads and writes granted by
the transaction manager. As an example, three transaction traces and one possible
transaction manager log are depicted below (the notation is from Bernstein
et al. [3]):
TI: R,(x)
TP: Ro(Y)
Ts: RsC4
Log: RI(X)

R,(Y)
W,(Y)
K(Y)

W,(Y)

FL(Y)

W,(Y)

Rs(d

W,(x)
Rsb)

h(y)

Rs(Y)

WI(Y)

Rzb)

W,(x)

Note that when A,(x) precedes AA
in some transaction trace, then A,(x)
to precede AA
in any log in which the transaction participates.

has

1.3 Increasing Concurrency
In database concurrency control we are interested in protocols that maximize
concurrency and work efficiently.
All known concurrency control protocols
restrict the logs to a subset of all the possible serializable logs. Two-phase locking
has been deemed to be quite restrictive since, intuitively,
it holds locks for “longer
than necessary.” That is, after a data item is read, the lock on it is held until no
other locks will be necessary (typically until commit). Indeed this is necessary
for providing serializability
if no other mechanisms other than locking the data
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

284

l

P. Dasgupta and Z. M. Kedem

items accessed are being used to control concurrency. In addition, a two-phase
locking protocol can cause deadlocks, which further degrade its performance.
We contend that this seemingly restrictive nature of two-phase locking arises
from the fact that the protocol does not assume any a priori knowledge of the
intentions of the transactions. Also, a priori knowledge of read and write sets
can be used for deadlock prevention in two-phase protocols. We use a priori
knowledge to allow non-two-phase executions; in particular, the protocol:
(1) allows read locks to be released as soon as the data is read;
(2) allows reading of (some) write locked data items;
(3) prevents deadlocks due to lock acquisition (another form of deadlock is
possible; see Section 5).
The following (trivial) example illustrates the restrictive nature of two-phase
locking and shows how added information can be used to remove some of the
restrictions:
TI: R,(x), WI(X), R,(Y),
Tz: Rzb)
‘I’S:IL(y)

W,(Y)

In this case, Tz (and TJ reads the value of x (and y) and does nothing else.
Thus Tz and T3 can read x (or y) between any two actions of T1 and still produce
a correct serializable execution sequence. However, if two-phase locking is used,
then Tz (or T3) is restricted to read x (or y) at only certain points, depending
upon the locking sequence. For example, suppose T, uses the locking sequence
shown in Table I.
This locking sequence prevents Tz from reading x, between W, (x) and R1(y).
However, T1’s locking sequence can be changed so that TZ can read x between
W,(x) and R, (y), but then T3 will not be allowed to read y between W1 (x) and
RI(Y).

Since we know that Tz and Ts read x and y and do nothing else, we could allow
TP and T3 to read x and y interleaved between any steps of T1 [ 161. However, the
two-phase locking protocol does not rely upon, or have access to, information
about the complete data access patterns of the transactions.
The above example also shows that locking sequences of two-phase locked
transactions affect the amount of concurrency, depending on the transaction
mix. A particular locking sequence in fact may favor one transaction over another.
Since it is impossible to predict which transactions will run concurrently, it is
not easy to choose locking sequences. In fact, it is commonly believed that locking
should not be handled by transactions or application programs, but should be
the responsibility of lower level, consistency preserving routines, that is, the
transaction manager.
In order to make locking transparent, practical two-phase locking schemes use
read and write requests to obtain locks (see 2V2PL in Section 1.4). A read or
write request on an unlocked data item causes the lock to be obtained. All locks
are released only when the transaction terminates. Thus it is intuitively clear
that the locks are held for extended periods.
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

The Five Co/or Concurrency
Table I.
Data-x

Control Protocol

285

A Locking Sequence Used by
Transaction T,
Comments

Data-y

LS, (xl
RI (4
LX, (xl
WI(X)

t
L&(Y)
R,(Y)
L&(Y)

T1 cannot read x here.
i

Ul(X)
W,(Y)
U,(Y)

Note. LS = lock shared, LX = lockexclusive,
U = unlock.

In the following sections, we propose a concurrency control protocol that, in
cases like the above, would allow T2 and T3 to read x and y interleaved between
any action of T, . This would be achieved by either early release of read locks, or
by allowing reading of (older values) of write locked data. The locking is handled
entirely by the transaction manager. Holding of read locks on read-only items is
minimized. The protocol is inherently non two-phase and is relatively immune
from deadlocks.
.

1.4 Related Work
Database concurrency control has been an active area of research and has resulted
in the development of many protocols for achieving serializability.
These basic
mechanisms used by the protocols are locking, timestamps, and multiple versions
[2,3,5,343. A synopsis of the concurrency control methods is beyond the scope
of this paper.
A family of protocols called two-version protocols use “before” and “after”
values of the data for concurrency control. Also, some protocols use information
about the transaction or the database to enhance concurrency. The Five Color
protocol uses information about the transactions (in the form of predeclared read
and write sets) as well as before/after values. The following paragraphs contain
brief descriptions of protocols in these two broad families.
Bayer et al. [l] present a protocol where the reads from a transaction are never
delayed, and are granted immediately. They use three kinds of locks, namely,
Read, Analyze, and Commit locks. The Analyze lock is used by writes to generate
a new (after) version of an item. The Read locks are compatible with all the
other locks; thus readers can see the before value of any transaction that is
writing to a data item and has an Analyze lock. Readers can read while the writer
is committing,
and a graph is maintained to assure they either read only
committed (after) versions or only uncommitted (before) versions.
A protocol similar to the Bayer et al. protocol is described in Bernstein et al.
[5]. This protocol is called the two-version two-phase locking protocol (2V2PL).
Under this protocol, the transaction manager uses read and write requests from
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

286

l

P. Dasgupta and Z. M. Kedem

the transactions to set locks. A read request causes a shared read lock to be
obtained. A write request causes a write lock to be obtained. (If the transaction
already possesses a read lock in the item, the lock is upgraded to a write lock.)
The write lock is compatible with other read locks, but not with a write lock.
Reader’s reading a write locked item gets to see the older (before) value. After a
transaction commits, all write locks it possesses are upgraded to an exclusive
“Certify” lock which is incompatible with all locks. The incompatibility
of the
Certify lock does away with the need for maintaining
a dependency graph and
introduces a slight delay for transactions that want to read an item that is being
committed.
In both the protocols, the locking is two phase; that is, the readers hold read
locks until they commit and writers cannot commit until all the read locks on
the items they are updating are released. This causes writing transactions to wait
for the readers to terminate. This can also cause writer starvation when the read
traffic is high. The Five Color protocol is designed to avoid this situation, as all
the read locks are released as soon as the reads are over, and writers do not have
to wait for reader termination in order to commit. Also, upgrading of locks leads
to “trivial” deadlocks [ 18, 251. For example, if two 2V2PL transactions hold read
locks on a data item, and both try to upgrade the locks, then they enter a
deadlock. The Five Color protocol allows lock upgrading, but avoids trivial
deadlocks. Of course these features are possible only because of the read-write
set predeclaration.
Stearns and Rosenkrantz [33] describe a set of actions and conditions that can
be used to control concurrency when using before values. For each conflict they
define a set of “actions” that can be taken (nondeterministically).
They also
define a set of conditions for proper consistency. Some of the actions may cause
an abort or rollback at a later stage. These sets of rules give rise to a family of
protocols, and the performance is dependent upon the choice of actions taken in
the particular execution. The concurrency control can be thought of as a nondeterministic table-driven mechanism. The tables show the actions for each case
of conflict. The cases of conflict are (1) a younger transaction reading a before
value of an older transaction, (2) an older transaction reading the before value
of a younger transaction, and so on. The actions are to allow the read or write,
abort one of the transactions, or delay the requester. Some of the actions have
restrictions as to the “phase” of the requester, and in some cases one of many
actions can be chosen. Timestamps are used to decide the older-younger relationships.
It has been shown that some available information
about the transactions or
the database can be used for increasing concurrency. For instance, Kedem and
Silberschatz have developed the tree and directed acyclic graph (DAG) protocols
that can be used on databases structured like a tree or a DAG, respectively [22,
321. These protocols allow non-two-phase locking and provide higher concurrency
than two-phase locking for transactions that traverse the tree or the DAG. In
the DAG protocol a transaction is allowed to lock a child if it has locks on the
majority of its parents (except for the first node locked), and unlocking may be
done in any order. Thus the transaction can access only those data items that
form a rooted subgraph of the original graph. These and similar protocols can be
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

The Five Color Concurrency

Control Protocol

287

used in specialized applications and have practical limitations, but have received
substantial theoretical interest.
If the write set of a transaction is known in advance, timestamp protocols can
be made abort free (or progressive) [8]. This can provide significant improvement
in performance as aborts cause severe limitations of throughput in timestampbased systems.
Semantic knowledge about transaction actions has also been used to speed up
transaction processing by Garcia-Molina
[ 151. A partial loss of serializability
can
be tolerated in some applications and can be used for better performance [14].
This may not be of interest in general purpose databases where consistency is a
major issue.
Read and write sets are used to control concurrency in a system for distributed
databases (SDD-1) [6]. Transactions are divided into classes depending upon
their read and write sets, and then a conflict graph analysis is performed. This
is a static classification and is used to determine the nature of the conflicts. The
conflict type is used to determine which protocol to use. (SDD-1 uses different
protocols for different situations.) SDD-1 is a timestamp-based system. In our
approach the usage of read and write set information
is dynamic. The protocol
uses locking and static analysis if conflicting transactions is not performed.
2. THE

FIVE COLOR

PROTOCOL

The Five Color protocol is a non-two-phase locked protocol that ensures serializability in general (unstructured)
databases. It derives its name from the five
types of locks it uses.
A transaction T acts upon a set of data items D. A data item x E D is in the
read set (Rd) of a transaction T (that is, x E Rd(T)) if the transaction intends
to read x. Any item that the transaction T intends to write is contained in the
write set (Wr) of T. Note that Rd(T) need not be a subset of Wr(T) or vice versa,
and Rd(T) and Wr(T) may be supersets of the data items actually read and
written by the transaction T.
Each transaction
is required to declare its read set and write set to the
transaction manager before it issues any actions. The read and write sets can be
parameters to the begin-transaction statement that is executed by a transaction
when it starts. Since the read set (and write set) are allowed to be supersets of
the data items actually read (and written) by the transaction, they could be
statically determined during query compilation. Sometimes the transaction may
read and write different sets of data depending upon some statically undeterminable conditions. In this case the declared read and write sets should include all
the items the transaction may act upon. However, for better performance the
difference between the declared and actual read and write sets should be small.
Since the Five Color protocol transaction manager reads all the items in the read
set of a transaction, extra reads will result if the read set is larger than necessary.
2.1 The Basic Algorithm
After the transaction manager acquires the read and write sets of the transaction,
it obtains shared locks on all the data items in the read set, reads them, and
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

288

l

P. Dasgupta and Z. M. Kedem

stores the values in local storage. Since at this point, the locks seem no longer
necessary, we would like to release them. We would also like to keep the data
items in the write set locked by a shared (intend-to-write)
lock while the
transaction is running to prevent other concurrent transactions from updating
them and causing missing updates. The intend-to-write
lock should be shared,
as we would like other concurrent transactions to be able to read the data item
before it is actually written. The shared locks on the write set can then be
upgraded to exclusive locks at commit time, and the values actually updated.
Thus our intended protocol is along the following lines:
(1) get shared read locks on the read-only items and shared intend-to-write
locks
on the write set;
(2) read the read set into local storage and release the locks on the read-only
items;
(3) service the reads and writes issued by the transaction from and to local
storage;
(4) upgrade the shared locks on the write set to exclusive locks and perform
actual writes to the database during commit, and finally release all locks.
As mentioned earlier, the merits of this approach are the early release of shared
read locks and the short holding period of exclusive locks. The protocol is
obviously non two-phase, as some shared locks are released before some other
shared locks are upgraded to exclusive locks. The problem with this initial scheme
is that it can produce nonserializable schedules and is thus unacceptable.
Now our aim is to use this basic idea, as described above, and add some checks
to ensure serializability.
We show that this is possible if we use some marker
locks and a ualidation phase. The algorithms used to handle the marker locks are
nontrivial and are described in detail in the following sections.
2.2 Locking
The Five Color protocol uses five types of locks, namely, Green (GL), Yellow
(YL), Red (RL), White (WL), and Blue (BL). The compatibility
matrix for these
locks are shown in Table II.
The Green lock is a shared lock used for reading the read-only part of the read
set. The Yellow lock is a shared intend-to-write
lock and is stronger than the
Green lock. It is used to lock the items in the write set, as a preparatory measure,
before they are actually updated. The Green lock is compatible with an existing
Yellow lock, allowing a Yellow locked item to be read as a read-only data item by
another transaction.
Yellow locks are not compatible with other Yellow locks,
preventing simultaneous update attempts. (The Five Color protocol allows upgrading of Yellow to Red, but not Green to Yellow or Red.)
Note that a Green lock can be obtained on a Yellow locked item, but a Yellow
lock cannot be obtained on a Green locked item. This feature makes the compatibility matrix asymmetric. This asymmetry is not a major feature of the algorithm
but has to be provided to prevent a particular race condition that can arise in
the lock acquisition phase, described later.
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

The Five Co/or Concurrency Control Protocol
Table II.

l

289

Lock Compatibility
Old

New

White

Blue

Green

Yellow

Red

White
BllZ
Green
Yellow
Red

Y
Y
Y
Y
Y

Y
Y
Y
Y
Y

Y
Y
Y
N
N

Y
Y
Y
N
N

Y
Y
N
N
N

Note.

The table is asymmetric.

The Red lock is the exclusive lock used for writing and is compatible only with
the White and Blue marker locks. Neither the Green locks nor the Red locks are
held over extended lengths of time. Only Yellow, Blue, and White locks exist
nearly as long as the transaction does.
The White and Blue locks are the marker locks’ and are compatible with all
other locks. These are used by transactions to keep track of data items read or
written by other transactions and cause triggering, as described later [9]. Briefly,
the semantics of the White and Blue locks are as follows:
(1) A data item x is White locked by a transaction T, if T has read x, or if there
is a transaction T’, such that T’ must follow T in a serialization of the
execution, and T’ has read x.
(2) A data item x is Blue locked by a transaction T, if there is a transaction T’,
such that T’ must follow T in a serialization of the execution, and T’ intends
to write x.
2.3 Transaction

Phases

A transaction T goes through several phases. When the transaction is initiated
(arrival point), the transaction manager obtains Yellow locks on the data items
in Wr(T) and Green locks on Rd(T)-Wr(T),
that is, the read-only data items.
This is the lock acquisition phase.
After all the locks are obtained, the transaction is considered for validation. If
the transaction passes validation, then it has to acquire some Blue and White
locks. It gets Blue (and White) locks on data items written (and read) by some
other concurrently
running transactions. It also has to assign White and Blue
locks on the items in its read and write sets to some other transactions. This is
called the lock inheritance phase, and the exact details of which data items are
locked are explained later. After completion of the lock acquisition phase, the
transaction reaches its initial locked point.
Subsequently all the items in Rd(T) are read into local storage, the Green locks
are converted (downgraded) to White locks and the transaction
enters the
1 The reason for calling the Blue and White locks, “locks” and not just “markers” are as follows.
Markers are used by agents to mark objects. Irrespective of how many times an object is marked, it
becomes unmarked when the marker is removed. With a lock we can ask questions such as Which
agents hold locks on this object? or What are the objects locked by this agent? Conceptually markers
are too weak to answer both these questions.
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

290

’

P. Dasgupta and Z. M. Kedem

processingphase. Then the transaction commences execution (startpoint).
When
the transaction completes execution all Yellow locks are converted to Red locks.
This is the Final Locked Point. The updated items are written to the database,
all locks are released, and the transaction terminates.
The following is an informal outline of how a transaction manager handles a
transaction.
+ Arrival Point (Transaction T arrives)
(1) Get Yellow locks on Wr(T) and compute Before/After sets (explained later)
(2) Get Green locks on Rd(T)-Wr(T)

(3) Do validation and lock inheritance processing (explained later)
+-Initial

Locked Point (ILP)

(1) Read values of Rd(T) into local storage
(2) Downgrade Green locks to White locks
(3) Start transaction processing
+ Start Point

(1) Let T commence processing
if T issues read(x), then return the value of x from local storage
if T issues write(x), then write x in local storage
(2) Processing ends
(3) Upgrade Yellow locks to Red locks
+ Final Locked Point (FLP)

(1) Write updated items to the database
(2) Release all (White, Blue, and Red) locks held by T
+ Termination

2.4 Transaction

Point(Transaction

T terminates)

Manager Algorithms

A transaction
is said to be live if it has arrived, but has not terminated. A
transaction is active during the period from initial locked point to the final locked
point. For each live transaction
T, the transaction
manager maintains two
temporary sets during lock acquisition phase, called Before(T) and After(T).
These sets are accessed and updated only during the lock acquisition phase. The
set Before(T) is a set of transactions that are live, conflict with T, and must
come before T in a serialization order. Similarly, After(T) contains those live
and conflicting transactions that must come after T in the serialization order.
The serialization order is determined by the actual order in which the locks are
requested by the concurrent transactions. (Sometimes the Before and After sets
contain a few recently terminated transactions, but that is of no consequence.)
As the transaction manager acquires locks on behalf of a transaction, it can
determine which transactions must come before or after this transaction in the
serialization order by looking at the existing locks. These transactions are placed
in the Before(T) and After(T) sets. The Before(T) and After(T) sets are constructed as follows.
Suppose T wants a Green lock on a data item x, and a set of transactions
iTi, Tj, *. . ,) already possess Blue locks on x. The existence of Blue locks held
by (Tt, Tj, * * . ,] implies that some transaction or transactions later than all
Of (Ti, Tj, . . . ,) have written x. As T wants to read x, it must come after all of
(Tip Tj, * * a9). Thus (Ti, Tj, . . . ,) must logically precede T, and they are added to
Before(T).
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

The Five Color Concurrency

Control Protocol

l

291

However, if some transaction Ti is holding a Yellow lock on x (when T is trying
to Green lock it), this implies Ti will update x after T reads it. Hence Ti should
come after T, and Ti is added to After(T).
Similarly, during an attempt to Yellow lock an item x, all transactions holding
Blue or White locks on x are added to Before(T).
Validation is simply checking whether Before(T) n After(T) = 0. If not, this
implies that there are transactions that must come before as well as after T in
the serialization order, and thus the resulting execution could be nonserializable.
To prevent this from occurring, the transaction T is rescheduled. Avoiding
livelocks (or starvation) due to rescheduling is dealt with in Section 6.
If the transaction passes validation, then the lock inheritance processing has to
be done. Some Blue and White locks are granted to various transactions as a
result of lock inheritance. This is done as follows.
Suppose T has passed validation. For each transaction T in After(T), T is
delayed until T has reached initial locked point, and T is given White locks on
all the data items White or Green locked by 7 and Blue locks on all the data
items Blue or Yellow locked by 7. Then all transactions in Before(T) are given
White locks on the read set of T, Blue locks on the write set of T. Finally, all
transactions in Before(T) get White locks on all data items White locked by T,
and Blue locks on all data items Blue locked by T. Note that during these last
two steps of the lock acquisition phase of transaction T, some transactions other
than T get some Blue and White locks. (These other transactions
are the
transactions in Before(T).) After lock inheritance, the transaction actually starts
executing, entering its processing phase.
The algorithms stated above are formally restated in pseudo-Pascal. Some of
the abbreviations used are
WL
WL-ed
WLS(T)
LOCK( White, x)
(similarly,

(i) Getting

+
+
-+
+

White
White
Set of
Obtain
to a

Lock
Locked
data items White locked by T
a White lock on data item x. If lock unavailable
conflict, wait until it can be obtained.

due

for all other colors)

Yellow locks:

Before(T) t @
for all x E Wr(T) do
begin
LOCK( Yellow, x);
Before(T) c Before(T)
end,

U (T; 1x is WL-ed or BL-ed by Ti)

(ii) Getting Green Locks:
After(T) c a
for all x E (Rd(T) - Wr(T)) do
begin
LOCK(Green, x);
Before(T) c Before(T) U (Ti 1x is BL-ed by Ti);
After(T) c After(T) U (T; 1x is YL-ed by TiJ
end;
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

292

(iii)

l

P. Dasgupta and Z. M. Kedem

Validation

and lock inheritance:

(* Validation
*)
if (After(T) n Before(T) # 0)
then RESCHEDULE
T
else
begin
for all 7 E After(T) do
begin
Wait for 7 to reach initial locked point;
BLS(T) c BLS(T) U BLS(T) U Wr(T);
(* T gets Blue locks on the write set of T
and all items Blue locked by 7 *)
WLS(T) + WLS(T) u WLS(7) u Rd(T)
(* T gets White locks on the read set of 7
and all items White locked by T *)
end,
for all 7 E Before(T) do
if 7 has not terminated then
begin
BLS(T) t BLS(T) U BLS(T) U Wr(T);
(* 7 gets Blue locks on all items Blue
locked by T and the write set of T *)
WLS(T) t WLS(7) u WLS(T) u Rd(T)
(* T gets White locks on all items White
locked by T and the read set of T *)
end
end;

We stress that there is no assumption of atomicity of any part of the above
transaction
manager algorithms, except in the test and set needed for the
implementation
of the LOCK function. The LOCK function is the standard
locking primitive.
If the lock requested cannot be granted due to a conflict,
then the process is suspended (or queued) until the lock can be granted.
These algorithms can be executed concurrently
with all the activities of the
other transactions on the database system, including lock acquisition phases
of other transactions.
In our protocol, locks can be held only by unterminated transactions. Hence if
a transaction 7 in Before(T) commits before or while T does lock inheritance,
then r does not have to be given any locks during the lock inheritance of T. Also,
during lock inheritance, a transaction T gets Blue/White locks on items locked
by transactions in After(T). But all transactions in After(T) are guaranteed to be
live’ when transaction T is in the lock inheritance phase (Lemma 3), and thus
we do not have to keep records of the locks held by dead transactions.
2.5 Intuitive Discussion
The transaction
manager of the Five Color protocol handles all the locking
and concurrency control needed to run transactions. The transactions themselves do not have any knowledge of the protocols. A transaction
has the
* In fact, Lemma 3 shows that the transactions in After(T) will be “active” when T reaches initial
locked point, which is a stronger condition than “live” during lock inheritance.
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

The Five Color Concurrency

Control Protocol

l

293

following structure:
Begin-Transaction (read set, write set)
...
(Statements]
...
End-Transaction.
The Begin-Transaction statement starts up the lock acquisition phase of the
transaction. The transaction manager does the acquiring of locks (using the read
set and write set information provided as parameters) and then performs the
validation and inheritance phases. After all the preprocessing is completed, the
transaction starts executing the statements, needing no further assistance from
the transaction manager. When the End-Transaction statement is reached, the
transaction manager is called upon to do the Yellow to Red lock upgrades, actual
writes, and commit.
Though acquiring locks is done on behalf of the transaction, by the transaction
manager, in the rest of our discussions we refer to this event as “a transaction
obtains a lock” because of simplicity and conceptual clarity.
As described in Section 2.1, the basic algorithm that the Five Color protocol
uses is as follows. First, the write set is locked using the Yellow lock. Then the
read set is locked with Green locks. After the read set is read into local storage,
the Green locks are downgraded to White locks.
After the locks are obtained, the validation and lock inheritance processing is
done, the transaction commences execution. After the execution ends, (commit
point) the Yellow locks are upgraded to Red locks, the data written out, and all
locks released. The need for validation, and how it is done is discussed later in
this section.
Suppose transaction T, is running, and it has released all the Green locks on
the read set. Now T2 can update an item which was read by T1, making Ts a
logically later transaction in the serialization order. In two-phase locking Tz has
to wait until T1 releases the read lock, and this may mean waiting until T1
commits.
Also, suppose T1 has a Yellow lock on y which it intends to update later. Tg,
which has obtained a Yellow lock on x, can also attempt to read (or get a Green
lock) on y, making T2 come logically before T, . The locking rules will permit this
to happen, leading to a nonserializable condition.
Thus, early release of read locks and late acquisition of write locks as is done
by the Five Color protocol leads to nonserializable executions, a simple case of
which has been depicted above. The validation and lock inheritance algorithms
have been designed to detect such situations and provide serializability. The
algorithms use the following strategy.
A transaction T, holds White locks on all items it has read. T1 also holds White
locks on data items read by other concurrent transactions that should come after
T1 in the serialization order. Similarly, T1 holds Blue locks on all items written
(or to be written)3 by concurrent transactions that should come after T, in the
3 The fact that T, holds Blue locks on data items that have not yet been updated can lead to an
anomaly in certain situations. See Section 4 for a modification that avoids this.
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

294

l

P. Dasgupta and Z. M. Kedem

serialization order. (This property is proved later in Lemma 4.) This implies T1
“knows” about all the data items read or written by “later” transactions.
Suppose a new transaction Tz arrives in this situation, and Yellow locks an
item that is Blue or White locked by T1. This implies Tz will update an item that
has been read or written by some transaction that is after T1 in the serialization
order. Thus TP should be after T, in the serialization order. As expected, the
Yellow locking of this item causes T, to be placed in Before
(because T1 is
before T2 in the serialization order). In fact, all transactions such as T, which
should come before TQ get into Before(
Now Tz can cause a nonserializable condition by attempting to Green lock an
item, which is already Yellow locked by a transaction such as T1. Green locking
a Yellow locked item implies that T, is getting a view of the database as it was
before T1 updates the item it Yellow locked; thus Tz should precede T1. But this
action would cause T1 to get into After(T*), and TP would fail validation.
The fact that each transaction T, has White and Blue locks on the read and
write sets of all transactions that come after T1 is ensured as follows: Whenever
a new transaction Tz acquires the Green and Yellow locks, its After(TJ
set
contains all concurrent transactions that should come after Tz in the serialization
order. T2 then acquires White (and Blue) locks on the read set (and write set) of
all transactions in After(T:!). Similarly, it gives all the transactions in Before
White locks on all its White locked objects, and Blue locks on all its Blue locked
objects and Wr(T,).
The maintenance of the White and Blue locks might seem contrived and
unnecessary, but it has to be done to prevent conditions typified by the following example. Consider three transactions, T1, TP, and TB, having the following
traces:
T1:
T2:
T3:

RI W

WI 07
W*(X)W,(Z)
%WW,(Z)

[time +]
A particular sequence of execution under the Five Color protocol is depicted in
Table III.
When T3 arrives, TP has terminated and T1 is executing. Since Tz has updated
X, and T, has read an older value, T, must be after T, in the serialization order.
The locking rules will allow T3 to read Y as it existed before T1 updates it and to
update Z which has already been updated by T,. However, this is nonserializable.
Since at this point Tz is no longer alive, and T, does not touch Z, there seems to
be no basis for disallowing T3 from reading Y and writing Z, unless the Blue and
White locks, and the Before and After sets are used.
When T3 Yellow locked Z, Z was Blue locked by T, . This caused T, to become
a member of Before(
When T3 Green locked Y, since Y was Yellow locked by
T1, T, became a member of After(T3). Now that the intersection of Before
and After(T3) is not empty, T3 fails validation, and the nonserializable execution
is not allowed. This is an instance of triggering that has been mentioned in
Section 2.2 and discussed further in Section 7.
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

The Five Color Concurrency
Table III.

Execution

T,

of the Example History

295

under the Five Color Protocol

T3

T2

Control Protocol

Notes

Arrives,
Yellow locks y
Green locks x
reads x
Converts Green
on x to White.
Arrives,
Yellow locks x
Yellow locks z

T, E Before

Gets Blue lock
onxandz
Updates x
Updates z
Terminates
Arrives
Yellow locks z
Green locks y
...

Executing
...

3. PROPERTIES

T, E Before
T, E After(T,)
(fails validation,
see discussion)

AND CORRECTNESS

We now state the properties of the Five Color protocol and prove it achieves
serializability.
The following lemmas are useful to understand the details of the
algorithms, but the actual proofs may be skipped at the first reading. In order to
show that the Five Color protocol assures serializability,
we define a standard
precedence relation -+ based on the well-known notions of read-write and writewrite conflicts [3,13]. The acyclicity of this relation implies serializability
(though
not vice versa [28]). We show that the Five Color protocol assures acyclicity of
the + relation.
All transactions that partake in the relations in the definitions are assumed to
be transactions that have already passed the validation phase. Transactions that
have not passed validation also generate relationships with other transactions,
but since these transactions do not have any effect on the database, we choose
to ignore them in the correctness proofs.
Definitions.
(1) The initial locked point (ILP) of a transaction is reached
when all the Yellow and Green locks have been obtained.
(2) The final locked point (FLP) of a transaction is reached when all the Red
locks have been obtained. If the transaction does not obtain any Red locks, then
FLP = ILP.
(3) A transaction is active if it has reached ILP but has not reached FLP.
(4) Define a binary relation (+) over transactions. Two transactions Ti and
Tj are related by the precedence relation (Ti + Tj), if and only if (i) Ti reads
some data item x from the database, and at some later point Tj writes x into the
database (read-write conflict), or (ii) Ti writes some data item x into the database,
ACM Trmsactions on Database Systems, Vol. 15, No. 2, June 1990.

296

l

P. Dasgupta and Z. M. Kedem

and at some later point Tj reads x from the database (write-read conflict), or (iii)
Ti writes some data item x into the database, and at some later point Tj writes x
into the database (write-write
conflict).

LEMMA 1.

The relation Ti + Tj occurs if and only if one of the following

cases

take place :
(i) Ti gets a Green lock on x and then Tj gets a Yellow lock on x after Ti releases
the Green lock. This arc is defined as of type [a + /? 1aG, BY].”
(ii) Tj gets a Yellow lock on x, then while Tj holds the Yellow lock, Ti gets a
Green lock on x, and then Tj converts the Yellow lock into a Red. This arc is defined

asoftype[ff~PIPY,aG,PRl.
(iii) Ti gets a Yellow lock on x, and later, after Ti unlocks x, Tj gets a Green lock
on x. This arc is defined as of type [LY+ /3 1aY, aR, PG].
(iv) Ti Yellow locks X, and later Tj Yellow locks x. This arc is defined as of type

by b + P I aY>PYI.
PROOFOF LEMMA 1.

Simple, but lengthy.

Cl

Cases (ii) and (iii) in Lemma 1 look similar but cause very different results. In
case (ii), Tj gets a Yellow lock on x and then, while the Yellow lock is held, Ti
gets a Green lock on x. In this case, Tj will update x, but Ti gets to see x as it
existed before Ti updated it. Hence the serialization order of the transactions
should be Ti + Tj. However, in case (iii), under similar circumstances, if Tj gets
a Yellow lock, and after this Yellow lock has been converted to Red, and released,
Ti gets a Green lock on x, then the serialization order should obviously be Tj +
Tie (The roles of Ti and Tj in the second example have been reversed, to be
similar to the first example.)
The arcs of the precedence relating graph (+) are caused by read-write, writeread, and write-write
conflicts. These conflicts happen when both the transactions have actually processed the conflicting reads and writes. However, for ease
of modeling we assume that the arcs are created earlier. We define that an arc
Ti + Tj is created when either Ti or Tj reaches ILP, whichever is later. (Ti and
Tj must of course conflict, as stated in Lemma 1.) This occurs after all the locks
for Ti and Tj have been obtained, but before any actual conflict due to actual
reading or writing has taken place. Also, if a transaction has not reached ILP,
there is no arc either from or to it.
Note that the precedence graph thus formed is not identical to the graph
formed by the read-write, write-read, and write-write
conflicts at some given
point in time. This is because the arcs of the precedence graph are created prior
to occurrence of the conflicts. Thus this precedence graph is in fact a superset of
the conflict graph, which will become identical to the conflict graph when all
activity on the database ceases. However, our correctness criterion is the
’ Notations such as [CY+ fl] cxG, PY] denote types of edges. If two transactions T, and Tz are related
as T, 4 T1, this relation can be caused in many ways. LYstands for the transaction to the left of the
+ symbol, and @is the transaction on its right. R, Y, and G denote getting a Red, Yellow, or Green
lock. The sequence to the right of the vertical bar shows the sequence of lock acquisitions by a and
/3 which led to the formation of the -+ relation.
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

The Five Co/or Concurrency

Control Protocol

acyclicity of the conflict graph (see below) and acyclicity
implies acyclicity of the conflict graph.

297

of the precedence graph

LEMMA 2. If Ti + Tj and Tj reached its ILP before Ti did, then the arc can
only be of type [a + p ] BY, aG, @RI.
PROOF OF LEMMA 2. Since Tj has reached its ILP, it has obtained all the
Green/Yellow locks. The arc Ti + Tj could have been caused by four cases
(Lemma 1). Consider each case separately:
[a! + p ] cuG,/3Y]. After Ti gets a Green lock on x, Tj cannot get a Yellow lock
on x until Ti converts the Green lock to a White lock. Hence Tj cannot reach the
ILP before Ti does and this case is impossible.
[cy + p ] ,8Y, aG, ,8R]. This case is possible, as Ti may obtain a Green lock on
x after Tj has placed a Yellow lock on x.
[a + p ] cxY, aR, PG]. In this case Ti has to get a Red lock on x before Tj gets
a Green lock on x. This makes it impossible for Tj to get to the ILP before Ti;
thus this case is impossible.
[LY+ p ] CYY,BY]. Again, Ti has to release the Yellow lock on x before Tj can
place another Yellow lock on x. Hence Tj cannot reach its ILP before Ti, making
this case impossible too.
Thus only the second case can take place, and hence the arc is of type [LY--j

PI PY, aG,@RI. q
LEMMA 3. If Ti +- Tj and Tj reached its ILP before Ti did, then Tj is still active
when Ti reaches its ILP.
PROOF OF LEMMA 3. The arc Ti + Tj is of the type [a +
(Lemma 2). Thus Tj gets a Yellow lock first, then Ti gets a Green
Tj upgrades its Yellow lock to Red. After Ti gets a Green lock
convert its Yellow lock to a Red lock until Ti converts its Green
lock. Thus when Ti reaches its ILP, Tj must be active. 0

fl] fly, aG, ,8R]
lock, and finally
on x, Tj cannot
lock to a White

Lemmas 2 and 3 show that, unlike two-phase locking, the precedence graph
can grow “backwards” (see Section 5). A transaction Tz, which arrives later than
transaction T1, may precede T, in the precedence order, if T, is active when T,
arrives.

LEMMA 4. If T1 + Tz -+ - . . + T, is a path in the precedence relation, and T1
is active, then

LEMMA 4.1. T1 possesses White lochs 0;~ Rd(T,)
LEMMA 4.2. T1 possesses Blue lochs on Wr(T,)

(i.e., Rd(T,)
(i.e., Wr(T,)

C WLS(T,)).
C BLS(T,)).

PROOFOFLEMMA4.1. Proof is by induction on N, the number of transactions
in the path. We show it is true for n = 2, then assume it is true for n = N and
show it follows for n = N + 1.
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

298

P. Dasgupta and Z. M. Kedem

l

n = 2. Let T1 + Tz, where T1 is active. The arc in the path can be of four
types. Consider each case separately:
[a -+ p ] CYG,PY]. T2 can obtain the Yellow lock on the data item (say x) after
T, has converted the Green lock it was holding to a White lock. When Tz gets a
Yellow lock on x while T, holds a White lock on it, T1 is added to Before(
Then T, inherits White locks on Tz’s read set. Hence Rd(T2) C WLS(T1).
[(Y + p ) BY, LYG,@RI. When T1 gets a Green lock on x while Tz is holding a
Yellow lock on x, Tz gets into the set After(Ti).
Then T, inherits White locks on
the read set of Tz. Hence Rd(T,) C WLS(T,).
[a + /3 ] LYY, aR, PG]. T1 cannot be active in this case, as T1 has released a
Red lock before the edge could be created and hence crossed its FLP state.
[a -+ /3 ] (YY, BY]. T1 cannot be active in this case as T1 has released the
Yellow lock, which implies Ti has upgraded the Yellow to Red and released
the Red lock, crossing its FLP.
Thus T, has White locks on the read set of Tz.
n = N.

We assume Lemma 4 holds for some value N of n.

n = N + 1. Consider a path consisting of n (=N + 1) transactions. By
definition all the transactions in this path have reached their ILP’s. Let Tk be
the transaction that reached its ILP last, among all the transactions in the path:
T,+T,+

. . . + Tk-1 + Tk + T,,,

+ Tk+z +

. . . + T,.

Now there are three cases:
(1) 2<k<n-1
(2) k= 1 or 2
(3) n - 1 or n.
First consider case (1). Consider the instance when Tk just reached its ILP. At
this point the following arcs are created:
Tk-i + Tk

and

Tk + Tk+l.

Thus prior to this there were two shorter paths, each path having a length less
than N, for which Lemma 4 holds:
(a) T, + . . . + Tkel
(b) T,,, + . . . + T,.
For path (a), T1 is alive (by definition) and thus T1 has White locks on Rd(Tk-i)
and Blue locks on Wr(Tkml), since (k - 1) < iV. As Tk reached ILP later than
Tk+l, it follows that T k+l was active when Tk reached ILP, and thus had White
locks on Rd(T,), and Blue locks on Wr(T,) (because of path (b)).
By Lemma 2, the arc Tk + Tk+l must be of type [(Y + p ] PY, (YG, PR]. Hence
Tk gets a Green lock on some x that is Yellow locked by Tk+l. Thus Tk+l is in
After(Tk), and since Tk gets White locks on all items White locked by Tk+l, Tk
gets White locks on Rd(T,).
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

The Five Color Concurrency

The Tkel + Tk arc can be of four types. Treating

Control Protocol

l

299

them separately:

[LY--, /3 ] CYG,PY]. In this case Tk sets a Yellow lock on a data item x, which
is in Rd(Tkyl), and thus x is White locked by T,. This makes T1 E Before(
and thus T, inherits White locks on all WLS(Tk), which contains Rd(T,). Thus
T, gets White locks on Rd(T,).
[(Y += fl] BY, cuG,PR]. This cannot happen as Tkml must reach ILP before Tk.
For if Tk gets a Yellow lock, and then Tkml gets a Green lock, Tk becomes a
member of the set After(Tkel).
Now Tkwl has to wait for Tk to reach ILP, before
it can reach ILP. (This is a condition during lock inheritance; please see the
algorithm description at the end of Section 2.3.)
[(Y+= /3 ] aY, CYR,/3G]. In this case Tk sets a Green lock on a data item x, which
is in Wr(Tkml), and thus x is Blue locked by T1. This causes T1 E Before(
and thus T, inherits White locks on all WLS(Tk), which contains Rd(T,). Thus
T, gets White locks on Rd(T,).
[cy + ,6 ] cry, BY]. In this case Tk sets a Yellow lock on a data item x, which
is in WR(Tkml), and thus x is Blue locked by T1. This makes T1 E Before(
and thus T1 inherits White locks on all WLS(T&
which contains Rd(T,). Thus
T, gets White locks on Rd(T,).
Thus for all cases, T1 has White locks on the read set of T,.
For the situations where iz = 1, or 2, or n - 1, or n, the above proof can be
modified. We sketch the case for k = 1 and k = 2, and leave the rest to the reader.
When k = 1, we have one path before T1 reaches ILP: Tz + . . . + T,. Since
this path has less than N transactions, Lemma 4 holds, and TP has White locks
on Rd(T,). It can then be shown that when T, reaches ILP (Tz is active) T,
inherits all the White locks from TB, making T1 have White locks on Rd(T,).
When k = 2, we have the paths T1 and T3 -+ . a. + T,. When Tz reaches ILP,
it can be seen that T1 is in the Before set of Tz and hence gets White locks on
everything T2 has a White lock on, and this includes Rd(T,) from Lemma 3. 0

PROOFOF LEMMA 4.2. Similar to proof of Lemma 4.1. Substitute
White and write set for read set in the proof of Lemma 4.1 above. 0

Blue for

Lemma 4 shows the most important property of the Five Color protocol. This
implies that if a transaction T1 is active, it “knows” about the read and write set
of all transactions that come after T,. This property is used to achieve serializability by causing a validation conflict when a cycle is created by some transaction.

THEOREM1. The protocol ensures that the precedence graph is acyclic and
hence preserves serializability.
PROOF. We show that the precedence graph is acyclic. The proof is by
contradiction.
cycle:

Assume there can be a cycle in the + relation.
T1 + Tz +

Choose a minimal

. . . + Tk--l + Tk + T1.
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

300

l

P. Dasgupta and Z. M. Kedem

Assume that Tk is the transaction to reach its ILP last, compared with all the
other transactions participating
in the cycle. It will be shown that if Tk accessed
data items in a manner that caused the cycle in the 3 relation, then Tk would
have been rescheduled at the validation phase.
Tk causes the creation of two arcs, which cause the cycle:
Al: Tj+-l+
A2:Tk+Tl.

T,;

As Tk is the last transaction to reach its ILP, T1 must have reached its ILP
before Tk; hence (by Lemma 2) the arc (Tk +T1)mustbeoftype[a--,P]/?Y,
(YG, PR]. Thus Tk gets a Green lock on a data item Yellow locked by T1. Hence
T1 E After(Tk). Also, by Lemma 3, T1 must have been active when Tk reached
its ILP. Thus by Lemma 4, T, has White locks on Rd(Tkel) and Blue locks on
Wr(Tkel).
The arc (TkV1 + Tk) could be of four types. Let us treat them
separately:
[a + ,6 ] aG, BY]. In this case, T1 E Before
(see the proof of Lemma 4).
Thus T1 E (Before
n After(Tk)), and hence Tk should have been rescheduled
at validation, and the cycle could not have resulted.
[a --$ 0 ] PY, cuG, @RI. This type of arc could have been caused only if Tk
reached ILP before Tk-i, which is not the case.
[a + p ] LYY, aR, PG]. In this case T1 E Before
(see the proof of Lemma
4). Thus again T, E (After(Tk) n Before(T
and Tk should have been rescheduled.
[a! --, P ] aY, PY].

In this case, again T, E Before(

The rest is as above.

Thus there can be no cycle in the + relation under the protocol, and hence the
protocol ensures consistency of updates. Cl
4. A MODIFICATION
The algorithm as described has an undesirable feature. It does not hamper
consistency but may cause more aborts than necessary. Consider the following
situation:
(1)
(2)
(3)
(4)

T, gets Green
T1 downgrades
TP gets Yellow
T3 gets Green

lock on x.
Green lock to White lock.
lock on x.
lock on x.

At step 3, T, is in Before
and thus gets a Blue lock on x. When T3 gets a
Green lock on x, Before
contains T1. Actually T1 and T3 are unrelated. The
anomaly exists as T2 has not yet written x when T3 reads x. There would have
been no problem if Tz had terminated before T3 read x, and in this case T3 would
have to be after T1.
Thus in a path of transaction T, + Tz, . . . , + Tk, T1 should have Blue locks
only on those data items that have been updated by Tz, . . . , Tk, and not on all
data items in the write set of TS, . . . , Tk.
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

The Five Color Concurrency Control Protocol

301

The following is a very brief description of a method to prevent the above
anomaly. Introduce another type of lock, called an Intent-Blue lock (I-Blue lock).
During lock inheritance, Blue locks are obtained on items already Blue locked by
other transactions, while I-Blue locks are obtained on the write sets (uncommitted
updates) of the transactions concerned. When a transaction commits, all the
I-Blue locks held on its write set by other transactions are changed to Blue locks.
All other algorithms remain the same.
This modification is not incorporated in the algorithm as described in Section
2. This is to avoid introduction of extra complexity, which has no bearing on the
correctness of the protocol, and simplify understanding of the protocol. This is
not a correctness issue, nor an important point in the concepts used in the Five
Color protocol.
5. DEADLOCKS
In the Five Color protocol there is potential for two types of deadlocks, one due
to locking and the other due to waiting for other processes to reach their ILPs.
Let us address them separately.
The first form of deadlock is the one encountered in traditional
locking
protocols and is caused by transactions in a circular wait, trying to obtain locks.
This form of deadlock can be prevented in this protocol, using the predeclared
read and write sets. We define an ordering of resources and acquire all locks in
an increasing order of resources. First, all the Yellow locks are obtained, in
increasing order, and then all the Green locks are obtained in that order. The
Yellow to Red conversion is also done in the increasing order of resources (data
items).5
THEOREM 2.
is deadlock free.

The preordered locking strategy used in the Five Color protocol

Definition.
Suppose transaction Ti requests a lock on item x in mode ml, and
item x is already locked in mode m2 by transaction Tj. If mode ml is incompatible
with mode m2, we say Ti waits-for Tj. The waits-for relation is denoted as
Ti 3 Tj. The directed graph defined by the waits-for relation over all transactions is called the waits-for graph.
PROOF. It can be shown that a cycle in the waits-for graph is an instance of
a deadlock condition. Suppose deadlocks can take place in our system. Consider
an instance of a deadlock involving k transactions, with the cycle in the waitsfor graph being

Step (i). Suppose none of the transactions T, to Tk are in the process of
acquiring Red locks, that is, they are in the Green or Yellow locking phase. Now
T, to Tk cannot be all in the Green locking phase or the Yellow locking phase. (If
there is one type of locking, this strategy is known to be deadlock free.) Thus
’ If we use only one type of (exclusive) lock, this strategy of obtaining locks in a predefined fashion
is known to be deadlock free. However, it is not true in general, where several types of locks with
various compatibilities
are used. In our case, however, it is deadlock free, as shown in Theorem 2.
ACM Transactions on Database Systems, Vol. 15, No. 2, June 1990.

302

l

P. Dasgupta and Z. M. Kedem

some transactions are waiting for a Green lock, and others are waiting for a
Yellow lock. Hence at some point, a transaction Ti is waiting for a Green lock on
a data item x, for which Tj is holding a Yellow lock. This is a contradiction
as
the Green lock is compatible with an existing Yellow lock. Thus at least one
transaction must be holding a Red lock.
Step (ii). Suppose one of the transactions,
say Ti, is in the process of
upgrading its Yellow locks to Red locks, and holds at least one Red lock. A
transaction which is trying to upgrade a Yellow lock on x, to a Red lock will have
to wait only if some other transaction holds a Green lock on x (as no other
transaction can hold a Yellow lock on x). Thus if T, is waiting for Tz, then T2
must be holding a Green lock. Since Tz is also waiting, it must be waiting to get
another Green lock, since the acquiring of Green locks are done after the acquiring
of Yellow locks, and all Green locks are released before any Red locks are acquired.
Thus T, is in the phase of acquiring Green locks.
Similarly, a transaction trying to acquire a Green lock on x will wait for another
transaction only if that transaction holds a Red lock on x, as Red is the only lock
incompatible with the Green lock.
ThusifT13Tz%--.
4 Tk 4 T1 is a cycle of waiting transactions and T1 is
in the Red lock acquiring phase, then so is TB, T, . . . , and TP, T, . . . , are in the
Green lock acquiring phase (that is, the cycle comprises only transactions in
the Green or Red lock acquiring phases).
The proof that there cannot be a cycle in a set of transactions having the
above properties is very similar to the proof that there cannot be a cycle in a set
of transactions acquiring exclusive locks in a predefmed order, and is not included
here for brevity [20]. Cl
The second form of deadlock is peculiar to this protocol. This form of deadlock
involves one or more transactions in the lock inheritance phase. Note that in the
lock inheritance phase, a transaction T has to wait for all transactions in After(T)
to reach ILP. This can cause deadlocks. The following is an example of a deadlock
caused by two transactions in the lock inheritance phase.
(i)
(ii)
(iii)
(iv)

T, Yellow locks
T, Yellow locks
T, Green locks
T2 Green locks

x.
y.
y.
x.

In this sequence of events the following problem takes place. Steps (i) and (ii)
cause no surprises, but in Step (iii) as T1 tries to Green lock y, which is Yellow
locked by Tz, Tz becomes a member of After(T1). Similarly, when Tz Green locks
x, T1 becomes a member of After(Tp).
Now both transactions will pass the validation and wait for all transactions in
their After sets to reach ILP. T, will thus wait for Tz to reach ILP before it can
reach ILP, and vice versa. Thus we have a deadlock. This deadlock effectively
prevents the nonserializable log that could result if the transactions were allowed
to continue.
The deadlock can involve transactions waiting for locks as well as transactions
waiting for other transactions to reach ILP point. However, as proved above
there are no deadlocks involving only transactions waiting for locks.
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

The Five Color Concurrency Control Protocol

303

The deadlocks can be detected by standard deadlock detection algorithms, or
by timeouts. As the deadlock occurs before the transactions start execution, there
is no rollback involved and the overhead suffered is small.
Since the transactions which participate in deadlocks do not do any processing,
we believe timeouts may be an easier and more efficient method to deal with
deadlocks. Lack of processing means there are no processing delays and the
timeouts can be fine tuned better to take into account the locking delays and
cause deadlock warnings if something takes too long to happen.
Every deadlock cycle has at least one transaction waiting for the completion
of the lock acquisition phase of another transaction. We propose that this is the
point where a timeout should be introduced. The delay for the timeout can be a
function of the number of locks the other transaction has to acquire. This will
keep the probability of detection of false deadlocks low.
6. LIVELOCKS
There is a chance of livelocks or starvation in this protocol. This is due to the
rescheduling of transactions because of validation failure or due to deadlocks.
There is no guarantee that an aborted transaction will finally be able to run.
However, we feel that the chances of starvation are extremely small. But low
probability of starvation is still not a guarantee against starvation, so we propose
the following method of avoiding livelocks.
If a transaction gets aborted due to validation failure or deadlock resolution
some number of times, we label the transaction as starvation prone. The write
set of a starvation-prone
transaction is then expanded to contain its read set.
The transaction thus acquires only Yellow locks during lock acquisition, and as
a result has an empty After set. This transaction is guaranteed to pass validation,
avoiding the livelock problem. Also it never waits for any other transaction to
complete lock inheritance. It may still, however, participate in deadlocks. But
note that to detect the deadlocks we are timing out and aborting a transaction
that waits for transactions in its After set. Since the starvation-prone
transaction
has an empty After set, it will not get aborted even if it participates in a deadlock.
Thus it is guaranteed to run to completion,
7. DISCUSSION
As stated earlier, the Five Color protocol differs significantly from the two-phase
locking protocol in the way the precedence (+=) relation may grow. In the twophase locking protocol the precedence arcs are created when a transaction locks
a data item. When a transaction locks an item (or upgrades a lock), it places
itself after some other transaction, never before. Thus we say the precedence
relation grows only in the forward direction. In fact, this is the property of the
two-phase locking protocol that ensures serializability.
Thus, in our protocol, a path under the precedence order can grow in both
directions. Suppose transaction T has reached its ILP and possesses a Yellow
lock on x. A new transaction T arrives and gets a Green lock on x. When 7 reaches
ILP, the arc 7 += T is created. Now, even after T terminates, as long as 7 is
active, T’ may come and place itself before T, and hence before T. In this way, it
is possible for future transactions to be logically placed in the past.
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

304

l

P. Dasgupta and Z. M. Kedem

Some locking protocols, for example, the Bayer et al. protocol [l] and the
2V2PL protocol discussed in Section 1.4, allow one transaction to read an older
value (before value) of a data item that is being updated by another transaction.
This situation is the same as setting a Green lock on a Yellow locked item.
However, there are some important differences.
Since the Bayer and 2V2PL protocols are two phase, they do not allow the
precedence graph to grow in both directions. A transaction T1 may read the
before value of x while T2 is updating x, but in this case Tz must wait for T1 to
commit before it can commit. The Five Color protocol does not place this
restriction.
Multiversion
timestamp protocols and the Sterns et al. protocols [33] also
allow transactions to read before values. It is difficult to compare these protocols
with the Five Color protocol as the mechanisms used by them are quite different.
Thus the basic mechanism by which two-phase locking prevents serializability
is not present in our protocol. Serializability
is ensured in this case by the Blue
and White locks and by the validation procedure. Intuitively,
if T, + . . . + Tz
is a chain of transactions, then T1 “knows” about Rd(TP) and Wr(T,), because
it has White and Blue locks, respectively, on these data items. If any transaction
7 attempts to read any data item in Wr(T2) (or write any item in Rd(T1)), then
due to the “triggering” caused by Green (Yellow) locking of a Blue (White) locked
item, T, becomes a member of Before(T) and T1 inherits White (Blue) locks on
Rd(7) (Wr(T)). Thus information
about the read and write sets flow up a chain
in the form of “inherited”
White and Blue locks. Now if r may cause a cycle in
the + relation by attempting to read an item Yellow locked by T1, then T, would
become a member of After(T) and violate the validation constraint. The other
cases of information flow when the chain grows in the reverse direction, or when
two chain as concatenated by a transaction is similar and is covered in the proofs
(Lemma 4).
This property of the Five Color protocol allows the protocol to produce
non-two-phase
histories. For example, the Five Color protocol allows nonstrictly-serializable6
histories [3]. The following
is an example of a nonstrictly-serializable
history that is allowed by the Five Color protocol. Some
timestamp-based protocols allow the following log, if the timestamp ordering
is Ts < TP < T1. The response of the protocol to this log is left to the reader.
RI (-4 Rz(Y)
W,(Y)
Lo&!
Serial order: T, + Tz + T1

Rs(z)

Note. Though T, completes execution
in the serialization order.

W,(z)

before T3 commences, TB precedes T1

Since the sets of histories produced by the Five Color protocol and the twophase locking protocols are incomparable (that is, there exist histories produced
by two-phase locking that are not produced by Five Color and vice versa), we
cannot conclusively argue superiority of one over the other.
6 A log L is strictly serializable if there exists a serial order L. of L such that if T, and TP are in L
and their actions do not interleave, then T, and T, appear in the same order in L. as in L. A log that
is serializable, but does not have the above property, is nonstrictly serializable.
ACM Transactions on Database Systems, Vol. 15, No. 2, June 1990

The Five Co/or Concurrency

Control Protocol

305

The following are the key properties of the Five Color protocol. Some of these
properties may lead to a higher concurrency level for the Five Color protocol, in
certain situations, than what is possible for the family of two-phase protocols.
(1) Early release of read locks. The Five Color protocol releases read locks as
soon as the data is read. This allows other transactions to update these data
items without having to wait for the reading transaction to commit. (In practical
two-phase protocols, such as 2V2PL, all locks are held till commit point.)
(2) Allowing reading of data items to be written later. This allows reading
transactions faster access to data items that would have been exclusively locked
for quite some time otherwise. The Bayer protocol, Sterns protocol, 2V2PL, and
some other protocols have this feature. However, in all locking protocols, the
updating transaction then has to wait for the reading transaction to commit
before it can commit.
(3) Noncompatibility
of Yellow locks. This property prevents some deadlocks.
Consider the 2V2PL protocol. If two protocols issue read requests on x and then
issues write requests on x, they will both try to upgrade the read locks they
possess into write locks, causing a deadlock. This is termed a trivial deadlock
[25]. The deadlock is trivial to detect but just as serious as any deadlock, as one
of the transactions have to be aborted. The incompatibility of the Yellow lock
avoids chances of trivial deadlocks and causes delay of an update transaction
while another is updating the data item (delay is better than deadlock). A
simulation study we conducted showed that trivial deadlocks are the most
common form of deadlock in 2V2PL type protocols. However, it must be noted’
that the Five Color protocol avoids this problem since it has the knowledge of
read-write sets, which 2V2PL does not.
(4) Preordering of locking requests. This avoids deadlocks due to locking. The
avoidance of such deadlocks as well as trivial deadlocks may cause the Five Color
protocol to have a lower chance of deadlocks than protocols using unordered
locking. However, any protocol that has knowledge of the read and write sets is
able to do ordered locking. When attempting to do ordered locking, care has to
be taken to see that locks are not held longer than necessary. The Five Color
protocol has been designed with that in mind.
7.1 Conclusions
We have presented a locking protocol that uses an unconventional locking
strategy and knowledge about the read and write sets of the transactions to allow
non-two-phase locking on a general database. We show that this protocol ensures
serializability and has properties that may allow it to perform better than the
two-phase locked protocols.
The Five Color protocol is substantially more complicated than the two-phase
locking protocols. In fact, the simplicity and elegance of the two-phase locking
protocols are their major attractions. Nevertheless, we believe that the Five Color
protocol is worth serious consideration.
Thus we conclude that it is possible for the Five Color Protocol to achieve
more concurrency by anticipating the absence of conflicts in a large number of
ACM Transactions

on Database Systems, Vol. 15, No. 2, June 1990.

306

l

P. Dasgupta and Z. M. Kedem

cases,due to the information available to the transaction manager about the read
sets and write sets of the transaction.
ACKNOWLEDGMENTS

We would like to acknowledge the anonymous referees for their in-depth comments and for pointing out several shortcomings that escaped our notice. We
would also like to thank Phil Bernstein and Magdi Morsi for their help and
comments.
REFERENCES

1. BAYER, R., HELLER, H., AND REISER, A. Parallelism and recovery in database systems. ACM
Trans. Database Syst. (June 1980), 139-156.
2. BERNSTEIN, P. A., AND GOODMAN,N. Concurrency control algorithms for multiversion database systems. In Proceedings of the ACM SZGACT/SZGOPS Symposium on Principles of Distributed Computing, 1982.
3. BERNSTEIN, P. A., AND GOODMAN, N. Concurrency control is distributed database systems.
ACM Comput. Suru. 13,2 (June 1981), 185-222.
4. BERNSTEIN, P. A., AND GOODMAN, N. Multiversion concurrency control-Theory
and algorithms. ACM Trans. Database Syst. 8,4 (Dec. 1983), 465-483.
5. BERNSTEIN, P. A., HADZILACOS,V., AND GOODMAN,N. Concurrency Control and Recovery in
Database Systems. Addison-Wesley, Reading, Mass., 1987.
6. BERNSTEIN, P. A., SHIPMAN, D. W., AND ROTHNIE, JR., J. B. Concurrency control in a system
for distributed databases (SDD-1). ACM Trans. Database Syst. 5, 1 (Mar. 1980), 18-51.
7. BERNSTEIN, P. A., SHIPMAN, D. W., AND WONG, W. S. Formal aspects of serializability in
database concurrency control. IEEE Trans. Softw. Eng., BSE-5,3 (1979), 203-216.
8. BUCKLEY, G., AND SILBERSCHATZ,A. Obtaining progressive protocols for a simple multiversion
database model. Presented at the 9th International Conference on Very Large Data-Bases, Oct.
1983.
9. CASANOVA,M. A. The concurrency control problem of database systems. In Lecture Notes in
Computer Science, 116. Springer-Verlag, New York, 1981.
10. DATE, C. J. An Introduction to Database Systems, ~012. Addison-Wesley, Reading, Mass., 1983.
11. DASGUPTA,P. Database concurrency control: Versatile approaches to improve performance.
Ph.D. dissertation, State University of New York, Stony Brook, Sept. 1984.
12. DASGUPTA,P., AND KEDEM, Z. M. A non-two-phase locking protocol for general databases
(extended abstract). Presented at the 9th International Conference on Very Large Data Bases,
Oct. 1983.
13. ESWARAN,K. E., GRAY, J. N., LORIE, R. A., AND TRAIGER, I. L. On notions of consistency and
predicate locks in a database system. Commun. ACM 14, 11 (Nov. 1976), 624-634.
14. FISCHER,M. J., AND MICHAEL, A. Sacrificing serializability to attain high availability of data
in an unreliable network. In Proceedings of the ACM SZGACT/SZGMOD Symposium on Principles
of Database Systems, 1982.
15. GARCIA-M• LINA, H. Using semantic knowledge for transaction processing in a distributed
database. ACM Trans. Database Syst. 8, 2 (June 1983), 186-213.
16. GARCIA-M• LINA, H., AND WIEDERHOLD, G. Read-only transactions in a distributed database.
ACM Trans. Database Syst. 7,2 (June 1982), 209-234.
17. GRAY, 3. N. Notes on database operating systems. Rep. RJ2188, IBM Research, 1978.
18. GRAY, J. N., HOMAN, P., KORTH, H., AND OBERMARK, R. L. A straw man analysis of the
probability of waiting and deadlock in a database system. Tech. Rep. RJ 3066, IBM, San Jose,
Calif., 1981.
19. GRAY, J., LORIE, R. A., PUTZOLU, F., AND TRAIGER, I. In Granularities of locks and degrees of
consistency in a shared database. In Modeling in Data Base Management Systems, G. M. Nijssen,
Ed. North-Holland, New York, 1976.
ACM Transactionson DatabaseSystems,Vol. 15,

No. 2, June 1990.

The Five Color Concurrency Control Protocol

307

20. HAVENDER,J. W. Avoiding deadlocks in multiuser systems. IBM Syst. J. 7, 2 (1968), 74-84.
21. KANELLAKIS, P. C., AND PAPADIMITRIOU, C. H. The complexity of distributed concurrency
control. In Proceedings of the 22nd FOCS, 1981, pp. 185-197.
22. KEDEM, Z., AND SILBERSCHATZ,A. A family of locking protocols modeled by directed acyclic
graphs. IEEE Trans. Softw. Eng. (1982), 558-562.
23. KEDEM, Z., AND SILBERSCHATZ,A. A non-two-phase locking protocol with shared and exclusive
locks. In Proceedings of the Conference on Very Large Data Bases, Oct. 1981.
24. KEDEM, Z., AND SILBERSCHATZ,A. Controlling concurrency using locking protocols. In Proceedings of the 20th FOCS, 1979, pp. 274-285.
25. KORTH, H. Locking primitives in a database system. J. ACM 30,l (Jan. 1983), 55-79.
26. KUNG, H. T., AND ROBINSON, J. T. On optimistic methods for concurrency control. ACM
Trans. Database Syst. 6,2 (June 1981), 213-226.
27. LINDSAY, B. G. et al. Notes on distributed database systems. Tech. Rep. RJ2571(33471), IBM
Research, Aug. 14,1979.
28. PAPADIMITRIOIJ,C. H. The serializability of concurrent database updates. J. ACM 26, 4 (Oct.
1979), 631-653.
29. PAPADIMITRIOLJ,C. H., BERNSTEIN, P., AND ROTHNIE, J. B. Computational problems related
to database concurrency control. In Proceedings of the Conference on Theoretical Computer
Science (Waterloo, Ont., Canada, 1977), pp. 275-282.
30. PAPADIMITRIOU,C. H., AND KANELLAKIS, P. C. On concurrency control by multiple versions.
In Proceedings of the ACM Symposium on Principles of Database Systems, Mar. 1982.
31. ROSENKRANTZ,D. J., STEARNS,R. I., AND LEWIS II, P. M. System level concurrency control
for distributed database systems. ACM Trans. Database Syst. 3,2 (June 1978), 178-198.
32. SILBERSCHATZ,A., AND KEDEM, Z. M. Consistency in hierarchical database systems. J. ACM
27,1 (Jan. 1980), 72-80.
33. STEARNS,R. E., AND ROSENKRANTZ,D. J. Distributed database concurrency controls using
before values. In Proceedings of the Conference on Vey Large Data Bases, Ott: 1981.
34. ULLMAN, J. D. Principles of Database Systems, 2nd ed. Computer Science Press, Potomac, Md.,
1982.
35. YANNAKAKIS, M. A theory of safe locking policies in database systems. J. ACM 29, 3 (1982),
718-740.
Received August 1983; revised February 1986, November 1988, August 1989; accepted August 1989

ACM Transactionson DatabaseSystems,Vol. 15,No. 2, June 1990.

Demo: Spanning an Underlay over a Host WPAN Cluster
Meddage S. Fernando, Pushkar M. Mulay, Michael A. Cartwright, Daming D. Chen,
Amiya Bhattacharya and Partha Dasgupta
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, P.O. Box 878809
Tempe, AZ 85287-8809

{saliya, pmmulay, macartwr, ddchen, amiya, partha}@asu.edu
The virtual underlay network client runs remotely to connect to
the host gateways over IP network, presenting an interface as in
Figure 2. The remaining LED color and the assignable node-ids
for the virtual motes are chosen by the user along with one of the
two candidate gateways, and the underlay network is initialized.
Subsequently, users can change the blinking frequency to exert
visible control over the underlay network, just as they could do so
from the host network interface at their individual gateways.

ABSTRACT
A prototype for WPAN slicing and virtualization is showcased in
this demo, where we form a contiguous wireless PAN “underlay”
spanned over two neighboring IEEE 802.15.4 mote networks.

Categories and Subject Descriptors
C.2.1 [Computer-Communication etworks]: Network Architecture and Design – wireless communication.
C.3 [Special-purpose and Application-based System]: Real-time
and embedded systems.

General Terms
Design, Experimentation.

1. OVERVIEW AD MOTIVATIO
Applications running on motes in sensor-actuator networks are
inherently limited due to the static nature of wireless PANs, i.e.
while the target area may span ownership or administrative
domains, the motes are limited to communicate with other motes
within the same domain. An abstraction of a virtual sensor
network, laid across multiple domains of wireless sensor networks
will be demonstrated.
Figure 1: User Interface for a Host WPA substrate

Two homogeneous WPANs, while operating on the same channel,
can use distinct PAN-ids in frame headers to maintain isolation in
case of overlapped radio coverage. In our demo, the same physical
mote is allowed multiple incarnations for different (virtual) sensor
networks, using multithreading along with MAC-layer bridging of
a mote’s MAC. We built a middleware on top of TinyOS 2.1.x
using TOSThreads 0 and TinyLD [2] to include the multiple
incarnation support for motes. When programmed with this
support, motes can form a network substrate capable of ondemand hosting of a virtual WPAN composed of newly incarnated
virtual motes. However, the supported virtual network need not be
contained within a single host substrate, but can assume a crossdomain span over neighboring networks with a little co-ordination
between host network gateways. (Since the virtual network is
composed using cross domain communication using neighboring
motes, we call this an “underlay”).

Figure 2: User Interface for the Virtual WPA Underlay

3. ACKOWLEDGMETS

2. DEMO COTET AD USER IPUT

The work is supported by NSF under Grants no. CNS-1011931
and CNS-1037547.

The setup involves two neighboring small multi-hop sensor
WPANs, each with five TelosB motes programmed with the
middleware support for host substrates. Transmission power is
adjusted to ensure multi-hop mesh connectivity but not single hop
reachability. Host networks have IP gateways with user control as
in Figure 1. Once the user parameters are chosen, the two host
networks identify themselves by blinking the assigned LEDs with
chosen frequency, and get ready for the application thread
pertaining to the underlay network.

4. REFERECES
[1] Klues, K., et al., 2009. TOSThreads: Thread-safe and nonevasive preemption in TinyOS. In Proc. 7th ACM Conference
on Embedded Networked Sensor Systems (SenSys 2009),
Berkeley, California, USA, (November 2009).
[2] Musăloiu-E, R., Liang, C. M., and Terzis, A., 2008. A
modular approach for WSN applications. HiNRG Technical
Report 21-09-2008, Johns Hopkins University (2008).

Copyright is held by the author/owner(s).
MobiSys’11, June 28–July 1, 2011, Bethesda, Maryland, USA.
ACM 978-1-4503-0643-0/11/06.

391

Community Sensor Grids: Virtualization for
Sharing across Domains
Amiya Bhattacharya

Meddage S. Fernando

Partha Dasgupta

Department of Computer Science
New Mexico State University
Las Cruces, NM 88003

Department of Computer Science
New Mexico State University
Las Cruces, NM 88003

School of Computing and Informatics
Arizona State University
Tempe, AZ 85287

amiya@nmsu.edu

saliya@nmsu.edu

partha@asu.edu

ABSTRACT

1. INTRODUCTION

Wireless sensor networks have been traditionally designed to be
privately owned and used. Hence the two hallmark features of
sensor networks, namely customized network applications and the
collaborative in-network processing, are not achievable beyond
the boundary of the users' administrative domains (except for
limited scope data sharing through Internet gateways). This
position paper advocates a contrarian notion of sensor grid that
preserves the operational continuity of the participating sensornets
and yet allows controlled sharing of sensor node hardware
capabilities over multiple administrative domains. This is
achieved by stitching together virtualized nodes donated by
participating sensornets into a transient virtual sensornet underlay,
which is called a Community Sensor Grid. Possible approaches
for node virtualization support are discussed for some existing
sensornet OS platforms. The formation, operation and dissolution
of the transient underlay are to be supported by a P2P overlay
formed by the Internet gateways of participating sensornets.

Wireless Sensor Networks (WSNs or sensornets), a class of selforganizing wireless ad-hoc networks made of sensor nodes (also
known as motes), are considered to be “a new family of networks”
primarily due to two reasons. First, an individual sensor node is
resource-constrained in terms of processing power, storage,
bandwidth and battery life—but the network, being composed of a
large number of such nodes, offers substantial processing
capability and battery life as a whole [7]. Second, nodes perform
both in-situ and in-network processing while propagating queries
and responses to make the network more energy-efficient than one
using a centralized scatter-gather approach [25][26].
A sensornet has a commonly accepted structure of being a cloud
of sensor nodes hanging off of a gateway at the edge of the
Internet. In the context of the overall Internet architecture, it exists
as a collection of one or more “privately owned” subnets, under a
single administrative domain [20]. While being under a single
administrative authority is particularly suited for applicationspecific programming of processing and forwarding
functionalities at every node, one needs complete control of the
sensornet. This, in turn, translates to ownership.

Categories and Subject Descriptors
[Computer-Communication Networks]:
Network
C.2.1
Architecture and Design—packet-switching networks, store and
forward networks, wireless communication.
C.3 [Special-purpose and Application-based System]: Real-time
and embedded systems.

A downside of private ownership is that it does not lend itself
easily to a variety of usage scenarios, especially community-based
sharing. If a community accessible sensor grid across multiple
administrative domains is desired, the current state-of-the-art is to
interconnect several isolated sensornet clouds with Internet
gateways—using published services as building blocks for dataonly accesses.

General Terms
Design, Performance, Security, Standardization, Management.

In not too distant future, sensor nodes are likely to be deployed in
various capabilities by private citizens, organizations,
governments and social groups alike. While private use such as
securing property and monitoring assets may be the primary
reasons behind these deployments, their ubiquitous presence and
span offer a great potential for community use (beyond the
obvious private use situations). As seen in Figure 1, an appealing
possibility is where a user with proper credentials can join a peerto-peer (P2P) network to discover available sensor nodes with
advertized capabilities in an intended area of coverage—thereby
forming an overlay network using a subset of these nodes to inject
a new sensing task without disturbing the ongoing primary tasks
at any node.

Keywords
Wireless PAN, Mote virtualization, Concurrency, Internet
integration, Overlay and underlay networks.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
MobiVirt’08, June 17, 2008, Breckenridge, CO.
Copyright 2008 ACM 978-1-60558-328-0/08/06…$5.00.

49

Figure 1: Virtual sensing/actuating service provides naming, discovery, custom applications and in-network processing.
sensornet, while not requiring power-consuming over-the-air
reprogramming of motes.

In this position paper, we introduce the concept of a community
sensor grid, which is a virtual sensornet built using parts of
several physical sensornets. The community sensor grid is a
transient sensornet made of nodes with inherent heterogeneity in
ownership,
hardware-software
platforms, and
exposed
capabilities. A sensor grid is composed of nodes contributed by
more than one private (physical) sensornets, all having Internet
gateways which participate in a P2P network to offer processing
view of virtual sensor nodes based on a common sharing
semantics. We address the issues of deployment, scalability,
security and topology management.

A more flexible level of programmability is however offered
externally under a very different usage scenario where the system
designer is perpetually unaware about the users’ need.
Community-shared sensornet research testbeds such as MoteLab
[28] and Kansei [10] allow remote users to program a sensornet
using a Web-based interface, so as to run an experiment during a
reserved slot scheduled ahead of time. While the experiments run
on the wireless data plane, these sensornet infrastructures
typically offer an Ethernet-based wired control plane for
programming and data collection. Building a set of predetermined services is never a viable alternative for testbed
design, as it sacrifices flexibility for experimentation.

2. MODELS OF SHARING
The majority of the efforts towards Internet integration of
sensornets to date rely on a data-only sharing model. A sensornet
is often conceived as a low-power WPAN (Wireless Personal
Area Network) topologically, while as a database or file system
from an application-level point of view [6][11]. An Internet client
program usually interacts with this application-level processing
view of the sensornet through a proxy server. The proxy service
usually runs at an IP-enabled 32-bit master node or micro-server
which controls the motes that belong to the WPAN. The sensing
capabilities of the motes are often pre-determined and advertized
by the micro-servers handling queries. Enabling IP protocol stack
on motes is getting increasingly popular due to usability of
existing client programs that are based on TCP, UDP or SNMP
[1][8]. Middleware support in some systems allows the Internet
client programs to control operational parameters such as
sampling intervals and reporting filters at the motes [2]. Even
real-time streaming of sensor data for live feed to the
computational grid [19] or publishing to the Web [23]
concentrates only on the relevant APIs for interacting with the
proxies, supporting only the data-only sharing model.

In summary, we can conclude that, (i) the majority of the external
access mechanisms support a data-only access model in
sensornets, and (ii) total access model, whenever supported, is
exclusively enforced by reservation.

3. SENSORNET OF VIRTUAL MOTES
Let us first explore what a new model of sharing, namely nonexclusive total access, has to offer.

3.1 Motivation
We envision that home security kits built around WPAN-based
motes will be widely available for use in the near future.
Moreover, convergence to a single MAC/PHY standard (e.g.,
IEEE 802.15.4) also seems inevitable for inter-operability of
products from different manufacturers. Such private deployments,
however, will vary widely in terms of network size. A small
deployment of a few nodes will typically form a single hop star
topology around the Internet gateway, while a medium size
network will self-organize in the form of the familiar multihop
mesh to maximize coverage. Larger deployments may call for a
tiered organization around more than one IP-enabled master
nodes connecting to the Internet gateway.

In effect, private sensornets offer only a limited form of external
programmability support (such as altering some parameters).
Arguments in favor of this design may be that publishing sensor
readings is a service usually sufficient for traditional use of a

50

communicate with another virtualized mote across the ownership
boundaries as long as both virtualized motes are participating in
the same guest network. The host networks ensure preservation of
the primary activities within all the deployed motes, whereas the
guest network is formed to share community resources. Figure 2
shows how several virtualized motes from multiple host
sensornets are stitched together to blur domain boundaries while
forming a guest sensornet.

Let us now explore how one can temporarily “borrow” several
nodes from several separate domains, virtual sensornet that
extends the area of coverage beyond some physical boundary. A
non-exclusive total access model of sharing would be quite
appropriate. Also, as the area under coverage may span over both
private and public properties, privacy rights of participating
individuals must be reflected by due assignment of sensing and
monitoring capabilities.
Our motivation is to explore the possibilities of forming scalable
community-based transient sensornets based on a non-exclusive
total access model, so that ordinary citizens are empowered to
program networked sensing applications expanding the reach
beyond their ownership. In what follows, we attempt to present a
scalable architecture under the following design assumptions:
1. A community of users (private, corporate and/or
government) is willing to deploy sensor node hardware to
cover areas under their jurisdiction. The covered areas may
be isolated to start with.
2. There exists a level of willingness from all members of the
community to share the sensing and processing capabilities
of their sensornet infrastructure as needed, without
interrupting the primary sensing task for the respective
deployments.
3. A typical user belonging to the community often borrows
sensor node capabilities from parts of the community
sensornet infrastructure to form a transient virtual sensornet
over a desired area of coverage. The transient network can be
dissolved after the task completes and results are gathered.
4. The rules of sharing are dictated by the owner of the resource
and at no time the sensing task of a local or remote user
violates the authority, privacy and security of the provider.
5. Optionally, a supporting business model accounts for
balancing the cost of sharing resources (such as hardware
and battery costs) between private and community use.

Spanning guest sensornet formed to run a new injected task

Host sensornets running the native tasks

Figure 2: Formation of a guest sensornet with expanded
boundary using mote-level virtualization
To the best of our knowledge, the design goals stated here have
not yet been addressed by the networked sensing research
community. However, it is worth noting that the salient theme is
reminiscent of the resource sharing in the PlanetLab
(http://www.planet-lab.org)
infrastructure
for
distributed
computing and the GENI (http://www.geni.net) infrastructure
under development for future Internet design. While we
understand that it is a significant challenge to implement
virtualization on resource constrained platforms such as the
motes, we postpone these discussions until the next section.
Instead, we present an example to motivate the benefits and a
business model associated with such type of community-based
sharing.

3.2 A Case for Mote-level Virtualization
As stated above, we are driven by the envisioned availability of
WPAN deployments to build a scalable architecture for nonexclusive total access to sensornets from outside their
administrative domains. The need for virtualization at the mote
level arises from the non-exclusive total access requirement, as
the primary sensing task of the owner must co-exist with the
newly injected sensing task for the remote user. To contrast with
the terminologies in OS-level virtualization in the multitasking
workstation/server environments, we opt for calling them the host
task and the guest task respectively.

Let us consider a scenario in urban sensing, where a city is
interested in improving its constant vigilance on brush fire to
actuate an early response system. City authority has property
rights only to the streets and public places for sensornet
deployment, but no access to private properties for sensor
deployment. However, citizens can be enticed to deploy motebased sensornets within their property at their own cost, and have
tax rebates for allowing the city guest access. The problem is not
solved efficiently by periodic or queried data-only access, as the
application requires adaptive sampling and extensive use of single
hop exchange of temperature readings among neighbors to detect
the spreading contour. To utilize the sensor nodes’ computing
power for adaptive sampling, city engineers need total access of
the sensor motes. Data exchange with neighbors is allowed in the

In the simplest scenario, there are only one host task and at most
one guest task executing in a mote (in the general case there may
be more guests, up to a resource limited maximum). In the
macroscopic view of time, a mote is thus capable of participating
in more than one sensornets simultaneously. The host task
participates in forming the host network, while the guest task
forms the guest network. Naturally, the mote must maintain two
different network identities for transmitting and receiving packets
over the radio. Among the two overlapped wireless networks, the
host network strictly observes the boundary enforced by
ownership. In other words, a mote with the host network identity
cannot communicate with another mote outside the host network.
A virtualized mote with the guest network identity is allowed to

51

milliseconds) in comparison to changing WPAN id or node id, as
the later are implemented in software as a filter. However, channel
switching cannot be avoided because the guest sensornet spans
over multiple host sensornets, and the host sensornets can natively
be on different channels to start with. Channel assignment to the
guest sensornet can still be done so as to let the least number of
motes to suffer the time lag due to channel switching. The second
contributor to performance loss is the deafness imposed when the
radio is tuned to the other channel. None of the existing mote
hardware platforms supports a radio implementing any multichannel MAC protocol. At this time, the problem needs to be
handled entirely by co-ordination of motes based on timesynchronization. However, other wireless standards such as IEEE
802.11 have multi-channel MAC protocol implementations,
which may be available in the mote platform in the future [15].

city’s guest network, which spans beyond the boundaries of the
private properties. In absence of such a guest network,
neighboring motes on two sides of a property boundary line
would not have participated in data exchange as neighbors, and
the application had to use the data-only model of sensing,
executing the contour detection algorithm at a remote centralized
location.

4. SENSOR NODE VIRTUALIZATION
Due to the constant pressure of keeping the systems footprint low,
embedded operating systems and middleware platforms for
sensornets have wide variations in providing the building blocks
needed for mote level virtualization. To be successful, an
architectural solution such as the one we present here should
ideally be supported on most popular platforms. Unfortunately,
since most platforms are research prototypes under active
development, it is a challenge to do a thorough comparative
capability evaluation of even the most dominant platforms. We
present here some findings from our continuing survey.

4.3 Runtime Injection and Protection
TinyOS offers minimal support for injecting a new task to be run
concurrently without stopping the current one. Achieving this
capability under TinyOS requires early decision on the
components that will be used by the injected task. These
components can then be statically compiled as part of the host
task. Using the middleware support of a virtual machine such as
Maté [16], one can then propagate the guest task as byte code that
later invokes execution of the required components [29]. While
supporting a similar event-based programming framework, SOS
[12] attempts to support runtime binding of dynamic components
based on software memory protection [14]. In addition, SOS
offers a virtual machine called DVM that can accept dynamic
components as well, without prior decision made at the
compilation of the native host task [2]. Byte codes are typically
propagated as epidemic [18], whereas frequent full over-the-air
reprogramming is more power hungry [13].

4.1 Concurrency Support
Programming sensornets has traditionally been under an eventdriven framework, such as in TinyOS, the de facto standard
platform [17]. TinyOS supports non-preemptive scheduling of
tasks, which can only be preempted by events. The underlying
assumption is that, due to low duty cycle of tasks (an implicit
assumption for sensornets anyway), the lack of preemption
capabilities would not cause perceivable performance penalty on
concurrency seen at a macroscopic level. Other operating systems,
such as Contiki and SOS, also conform to this philosophy [9][12].
A traditional thread-based approach to concurrency is supported
by the kernel of MANTIS OS, which offers preemptive
scheduling [3]. However, there exist efforts to implement thread
abstractions on operating systems primarily supporting eventdriven tasking—with the assumption of no visible performance
penalty for low duty cycle tasks [21]. Running distinct host and
guest threads is particularly simple on platforms with the thread
support, as demonstrated by the TinyMOS implementation, which
can run a typical TinyOS program as a MANTIS thread [27].

While running distinct host and guest threads under a platform
like MANTIS is appealing for concurrency, byte code
implementation of the guests has some advantages in enforcing
access control by sandboxing. Capabilities such as switching
channel, network, or host id should be owned by the host system
only. Explicit steps are to be taken to prevent guest threads from
listening in promiscuous mode or scanning channels, so as to
protect the privacy of host’s native sensing tasks.

4.2 Supporting Host and Guest Networks
A mote, having the host and the guest threads running
concurrently, participates in two (or more) distinct sensornets—
namely the host and the guest nets. Even though the guest threads
may be considered to create one or more virtualized mote entity,
the guest sensornets are all real. To be more specific, there are
new WPANs formed by the virtualized motes carrying frames
with headers that differ in either the channel ID or the network (or
group) ID from the frames of the original host WPANs.

5. INTERNET INTEGRATION
It is well understood that the narrow waist of sensornet protocols
is a translucent abstraction of MAC layers. TinyOS 2 implements
it explicitly as a layer called SP [24]. We believe that this the
most ideal layer to implement mote virtualization. Since this lies
below the IPv6 address assignment as per the 6LoWPAN standard
[1][22], we refer to the guest sensornet as an underlay. The
underlay needs to incorporate security solutions using shared keys
between gateway and the motes similar to the traditional sensor
networks, thus providing both privacy and authentication.

Thus, there is a need for explicitly changing channel, network-id
and host-id at every transition of control between the host and the
guest threads. While changing channel, network, or node id’s for
a mote is not common between wired reprogramming, the
operating systems offer explicit support for this, which has been
used for purposes such as throughput optimization [15].

Every participant in the community sensor grid must have a
sensornet with an Internet gateway. The gateway forms the point
of presence of the internal sensornets on the Internet, and is
responsible for exposing the sensornet capabilities. The gateways
must register themselves to form a P2P network where resources
and associated usage policies can be discovered by other peers.
Users access this P2P network via an “account manager,” which

Switching between networks is possibly going to be the prime
contributor to the performance loss due to mote virtualization.
First, changing channel has a longer delay (of the order of

52

P2P overlay network. We address some issues of deployment,
scalability, security and topology management.

has a public-key based certificate. When a user wants to create a
private network, the account manager negotiates the level of
service with each gateway that controls a participating sensornet.
The negotiation is based on the account manager’s certificate and
the gateway’s security policy. A user can subsequently form
another overlay network consisting of all the gateways to the
sensornet he/she wants to use. After such negotiations are
completed, the account manager and the gateways exchange a
group session key and then the group session key is made
available to the user application, which can then proceed to pass
on the program to inject and collect sensor readings. The overlay
network must be set up before the community sensornet underlay
(guest sensornet) is formed, and must remain until the underlay is
dissolved. In case a sensornet gateway leaves the overlay with or
without notification, the community sensornet underlay needs to
be reformed.

Our ongoing work is to realize the above architecture to enable
deployment of virtual sensornets of community sensor grids. The
deployment issues include mote virtualization (with guest and
host applications), dynamic application invocation and
termination, network overlay and underlay formation, Internet
gateway management and access control. We are in the process of
designing a prototype implementation using motes and gateways.

8. ACKNOWLEDGMENTS
This material is based upon work supported by the National
Science Foundation under Grants no. CNS-0551734 and CNS0617671, as well as a Graduate Research Enhancement Grant
from the Office of Vice-President of Research, Graduate Studies,
and International Programs at the New Mexico State University.

6. RELATED WORK
Our approach towards forming community sensor grids falls
under the general area of participatory sensing, which is emerging
to be an active area of research in both academia and industry.
Naturally, the design we propose here shares some of the design
goals that are common to these research projects, while addressing
unique problems that are complementary in nature.

9. REFERENCES

The goal of the urban participatory sensing project at the Center
for Embedded Networked Sensing (CENS) is to engage sensors
built into portable devices such as mobile phones and PDAs in
sharing their capabilities [4]. A tiered architecture and related
protocols for people-centric urban sensing is under development
as part of the MetroSense project at Dartmouth, which involves
mobile devices in opportunistic interactions with wireless Internet
gateways and available static sensornet infrastructure [5].

[2] R. Balani et al., 2006. Multi-level software reconfiguration
for sensor networks. In proceedings of the 6th ACM/IEEE
International Conference on Embedded Software
(EMSOFT’06, Seoul, Korea, October 2006), 112-121.
DOI=10.1145/1176887.1176904.

[1] Arch Rock Corporation, 2007. A sensor network architecture
for the IP enterprise. In proceedings of the 6th International
Conference on Information Processing in Sensor Networks
(IPSN 2007, Cambridge, Massachusetts, April 2007), 575.
DOI=10.1145/1236360.1236447.

[3] S. Bhatti et al., 2005. MANTIS OS: An embedded multithreaded operating system for wireless micro-sensor
platforms. Mobile Networks and Applications, 10, 4 (August
2005), 563-579, Springer Science.
DOI=10.1145/1160162.1160178.

The community sensor grid architecture is aimed at enabling
private sensornet infrastructures for participatory sensing. While
using or supporting mobile devices is outside its scope at this
time, the proposed P2P based control is amenable to extensions
that will support mobile control stations or sinks.

[4] J. Burke et al., 2006. Participatory sensing. In proceedings of
the 1st Workshop on World-Sensor-Web (WSW’06, Boulder,
Colorado, October 2006).
[5] A. Campbell et al., 2006. People-centric urban sensing. In
proceedings of the 2nd Annual International Workshop on
Wireless Internet (WICON 2006, Boston, Massachusetts,
August 2006). DOI=10.1145/1234161.1234179.

One important difference between the community sensor grid and
most other participatory sensing projects is the sharing model
supported by the underlying architecture. Building a set of
predefined services into the participating entities usually suffices
in most cases, as long as the target applications can be developed
based on data-only sharing. In contrast, we attempt to dynamically
load and run foreign applications in an existing and operational
sensornet. This makes concurrency, reprogramming and
authentication to be the central theme.

[6] Q. Cao et al., 2008. The LiteOS Operating System: Towards
Unix-like abstractions for wireless sensor networks. In
proceedings of the 7th International Conference on
Information Processing in Sensor Networks (IPSN 2008, St.
Louis, Missouri, April 2008), 233-244.
DOI=10.1109/IPSN.2008.54.

7. CONCLUSION AND ONGOING WORK

[7] D. Culler, D. Estrin and M. Srivastava, 2004. Overview of
sensor networks. IEEE Computer, 37, 8 (August 2004), 4149. DOI=10.1109/MC.2004.93.

This paper presents a concept of a community sensor grid that
allows the sharing of the sensing and processing facilities of
motes across many domains, each domain being a traditional
sensornet. The sharing allows guests to participate in sensing
activities (access controls apply) using sensornets deployed by
several different organizations or private owners. The result is a
virtual sensornet, that is transient and with inherent heterogeneity
in ownership, hardware/software, and exposed capabilities. The
Internet gateways of the underlying physical sensornets form a

[8] A. Dunkels et al., 2003. Full TCP/IP for 8-bit architectures.
In proceedings of the 1st International Conference on Mobile
Systems Applications and Services (MobiSys ’03, San
Francisco, California, May 2003), 85-98.
DOI=10.1145/1066116.1066118.
[9] A. Dunkels et al., 2006. Run-time dynamic linking for
reprogramming wireless sensor networks. In proceedings of

53

the 4th International Conference on Embedded Networked
Sensor Systems (SenSys’06, Boulder, Colorado, November
2006), 15-28. DOI=10.1145/1182807.1182810.

Embedded Networked Sensor Systems (SenSys’07,
Cambridge, Massachusetts, April 2007), 369-370.
DOI=10.1145/1322263.1322299.

[10] E. Ertin et al., 2006. Kansei: A testbed for sensing at scale.
In proceedings of the 5th International Conference on
Information Processing in Sensor Networks (IPSN 2006,
Nashville, Tennessee, April 2006), 399-406.
DOI=10.1145/1127777.1127838.

[20] K. Mayer and W. Fritsche, 2006. IP-enabled wireless sensor
networks and their integration into the Internet. In
proceedings of the 1st International Conference on Integrated
Internet Ad hoc and Sensor Networks (InterSense’06, Nice,
France, May 2006). DOI=10.1145/1142680.1142687.

[11] J. Gehrke and S. Madden, 2004. Query processing in sensor
networks. IEEE Pervasive Computing, 3, 1 (January-March
2004), 46-55. DOI=10.1109/MPRV.2004.1269131.

[21] W. P. McCartney and M. Sridhar, 2006. Abstractions for safe
concurrent programming in networked embedded systems. In
proceedings of the 4th International Conference on
Embedded Networked Sensor Systems (SenSys’06, Boulder,
Colorado, November 2006), 167-180.
DOI=10.1145/1182807.1182825.

[12] C.-C. Han et al., 2005. A dynamic operating systems for
sensor nodes. In proceedings of the 3rd International
Conference on Mobile Systems Applications and Services
(MobiSys’05, Seattle, Washington, June 2005), 163-176.
DOI=10.1145/1067170.1067188.

[22] G. Montenegro et al., 2007. Transmission of IPv6 Packets
over IEEE 802.15.4 Networks. The Internet Engineering
Task Force (IETF) RFC 4944, September 2007.

[13] J. Hui and D. Culler, 2004. The dynamic behavior of a data
dissemination protocol for network programming at scale. In
proceedings of the 2nd International Conference on
Embedded Networked Sensor Systems (SenSys ’04,
Baltimore, Maryland, November 2004), 81-94.
DOI=10.1145/1031495.1031506.

[23] S. Nath et al., 2006. SensorMap: A web site for sensors
world-wide. In proceedings of the 4th International
Conference on Embedded Networked Sensor Systems
(SenSys’06, Boulder, Colorado, November 2006), 373-374.
DOI= 10.1145/1182807.1182861.

[14] R. Kumar, E. Kohler and M. Srivastava, 2007. Harbor:
Software-based memory protection for sensor nodes. In
proceedings of the 6th International Conference on
Information Processing in Sensor Networks (IPSN 2007,
Cambridge, Massachusetts, April 2007), 340-348.
DOI=10.1145/1236360.1236404.

[24] J. Polastre et al., 2005. A unifying link abstraction for
wireless sensor networks. In proceedings of the 3rd
International Conference on Embedded Networked Sensor
Systems (SenSys’05, San Diego, California, November
2005), 76-89. DOI= 10.1145/1098918.1098928.
[25] G. J. Pottie and W. J. Kaiser, 2000. Wireless integrated
network sensors. Communications of the ACM, 43(5):51-58,
(May 2000). DOI=10.1145/332833.332838.

[15] H. K. Le, D. Henriksson and T. Abdelzaher, 2007. A control
theory approach to throughput optimization in multi-channel
collection sensor networks. In proceedings of the 6th
International Conference on Information Processing in
Sensor Networks (IPSN 2007, Cambridge, Massachusetts,
April 2007), 31-40. DOI=10.1145/1236360.1236365.

[26] K. Sohrabi et al., 2000. Protocols for self-organization of a
wireless sensor networks. IEEE Personal Communications,
7, 5(October 2000), 16-27. DOI=10.1109/98.878532.
[27] E. Trumpler and R. Han, 2006. A systematic framework for
evolving TinyOS. In proceedings of the 3rd Workshop on
Embedded Networked Sensors (EmNets 2006, Cambridge,
Massachusetts, May 2006).

[16] P. Levis and D. Culler, 2002. Maté: A tiny virtual machine
for sensor networks. In proceedings of the 10th International
Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS X, San Jose,
California, October 2002), 85-95.
DOI=10.1145/605397.605407.

[28] G. Werner-Allen, P. Swieskowski and M. Welsh, 2005.
MoteLab: A wireless sensor network testbed. In proceedings
of the 4th International Conference on Information
Processing in Sensor Networks (IPSN’05, Los Angeles,
California, April 2005), 483-488.
DOI=10.1109/IPSN.2005.1440979.

[17] P. Levis et al., 2004. The emergence of networking
abstractions and techniques in TinyOS. In proceedings of the
1st Symposium on Networked Systems Design and
Implementation (NSDI’04, San Francisco, California, March
2004), 1-14.

[29] Y. Yu et al., 2006. Supporting concurrent applications in
wireless sensor networks. In proceedings of the 4th
International Conference on Embedded Networked Sensor
Systems (SenSys’06, Boulder, Colorado, November 2006),
139-152. DOI=10.1145/1182807.1182822.

[18] P. Levis et al., 2004. Trickle: A self-regulating algorithm for
code propagation and maintenance in wireless sensor
network. In proceedings of the 1st Symposium on Networked
Systems design and Implementation (NSDI’04, San
Francisco, California, March 2004), 15-28.
[19] H. B. Lim et al., 2007. The national weather sensing grid. In
proceedings of the 6th International Conference on

54

Panel I: Wireless Networking:
Interoperability and Security Challenges
Organizer and Moderator:
AIjan Durresi
Department of Computer Science, Louisiana State University
Description:

Wireless networks have been compared to the Tower of Babel in their multitude of
technologies and standards. In addition, serious security flaws have been found in
WLANs, and with today's threats to our security and safety, we may need to rethink
whether safety and traceability are more important than anonymity and privacy. Ibis
panel will discuss important problems in the confusing, complex, but very interesting
world of wireless and mobile technologies. In particular, the panel will address tradeoff's
pertinent to using WLAN and 3G/4G technologies for providing future: wireless services.
The discussion will be focused on two key aspects of wireless networks: interoperability
and

security. Bring your questions and outrageous opinions on this topic!

Panelists:

Mohammed Atiquzzaman, University of Oklahoma
Joseph Hui, Arizona State University
Raj Jain, Nayna Networks
Songwu Lu, University of California, Los Angeles

Panel II:
PKI to the Masses
Organizer and Moderator:
Partha Dasgupta
Department of Computer Science, Arizona State University
Description:
Identity theft is on the rise. We give out secret information (such as SSN, passwords)

albeit to authorized

parties and they get misused. PKI solves this problem, by keeping
private keys private. However, PKl is currently not in use in the mass market. A
discussion of why it is necessary (or unnecessary) and how to achi(;:ve it will be the
primary focus of this panel.
Panelists:
Daniel Massey, University of Southern California

Cristina Nita-Rotaru, Purdue University
William Scott, General Dynamics
Wei Zhao, Texas A&M University

x

TABLE OF CONTENTS
Session 1.1: Multicast
1.1.1 Improving Server Broadcast Efficiency Through Better Utiliz.ation of Client Receiving Bandwidth .

....

.

....

5

Ashwin Natarajan, Ying Cai and Johnny Wong. Iowa State University
1.1.2 A Buffer Management Scheme for Tree-Based Reliable Multicast Using
Infrequent Acknowledgments

...............................................................................................................................

13

Jinsuk Baek and Jehan-Fran�ois Paris, University of Houston
1.1.3 Selection Algorithms for Anycast Relay Routing ....................................................................................... 21
Jianping Zheng. Chinese Academy of Sciences; Keqin Li, Lucent Technologies; and Zhimei Wu.
Chinese Academy of Sciences

Session 1.2: Network Planning
1.2.1 Aggressive Telecommunications Overbooking Ratios ................................................................................ 31
Robert BaIl, Mark Clement, Feng Huang, Quinn Snell and Casey Deccio, Brigham Young University
1.2.2 Settlement and Capacity Planning in Multi-Service Provider Infrastructures .............................................. 39
Lisa Amini, Giuseppe A. Paleologo and Olivier Verscheure,lBM T. J. Watson Research Center
1.2.3 Algebraic Geometric Code Based IP Traceback . .
.

.

... .

.. . . ...

.... ..

..

.

.

.. .

. ...
...

.

.

.. .... ...

.

...

. .
..

......

..... .
.

.... .

.....

.

... .........

49

Chunyan Bai, Guiliang Feng and Gesan Wang, University of Louisiana at Lafayette

Session 2.1: Wireless Resource Management
2.1.1

'Connection-Ievel Performance Analysis for Adaptive Bandwidth Allocation in

Multimedia Wireless Cellular Networks

. .......

.

.

...... . . . .

. .

........

..

.

. . .......... . ..

. . .... . .. ...

.. .. .
.

..

..................... .......................

61

Nidal Nasser and Hossam Hassanein. Queen's University
2.1.2 Adaptive Channel Switching for Admission Control with QoS and Priority in WCDMA Uplink .. ...
Bing Cao, Teresa A. Dahlberg. University of North Carolina at Charlotte

. . . . ...

..

2.1.3 A Novel Velocity-Dependent Directional Probability Function Based Call
Admission Control Scheme in Wireless Multimedia Communications ... ... . . .. . . .... ... .......... .
Mohammad Mahfuzul Islam, Manz.ur Murshed and Laurence S. Dooley, Monash University
..

... .

.

... .

..

..

..

..

..

.

69

...... 77

.....

..

Session 2.2: Network Management
2.2.1 Toward Efficient Distributed Network Monitoring

.

....... . . . .

..

. ......

.. .

. . .
...

..

... ....

..... . . ..
..

..

.

.. .

....... .
.

...

... ... :... 87

.

.. . ...

.

Xiaojiang Du. Indiana University - Purdue University Indianapolis
2.2.2 Admission Control in Deadline-Based Network Resource Management.. .... ..... ... .. ..
...

..

.

.

..

....

...... .....
.

.........

95

Yanni Ellen Liu, University of Manitoba: Johnny W. Wong, University of Waterloo
2.2.3

On the Performance of Maestro2 High Performance Network Equipment.

Using New Improvement Techniques ................................................................................................................... 103
Shinichi Yamagiwa, Kevin Ferreira and Luis Miguel Campos. PDM&FC; Keiichi Aoki.
Masaaki Ono and Koichi Wada. University of Tsukuba; Munehiro Fukuda. University of Washington Bothell;
Loenel Sousa, IST/lNESC-ID.

xi

A Concurrent Programming Environment for Memory-Mapped
Persistent Object Systems
Mei-Mei Fu and Partha Dasgupta
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287-5406
mfu@cs.eas.a&edu
partha@ cs.eas .asu.edu
Abstract

between secondary storage and main memory. It has
been estimated that on average 15% of code in an
application is due to this memory and storage mismatch.
Persistent Object Systems and Object-Oriented Databases have been developed to address some of these
problems.
Persistent Object Systems provide the ability to store,
share and dynamical1 re-use object instances and object
structures that an appication creates or uses. A Persistent
Object System should provide a persistent programming
environment. This paper describes the design and
implementation of such a persistent programmlng environment which is an extension of the conventional objectoriented programming systems by adding features of orthogonal persistence[At*83], run-time sharability, uniform
view of system storage, and dynamic re-usability for
language defined objects.

The advantages of object-oriented programming system
are well known. However, in general, they do not support
long-lived objects, nor do they allow concurrent sharing or
dynamic re-use of language defined objects. Persistent object systems address some of these shortfalls.
I n this paper we present a design and implementation of
a persistent object system that uses memory-mapping to
directly map objects from the persistent store into the
address space of user programs. Memory mapping makes
the management and manipulation of persistent objects
simpler. I n particular, complex object structures can be
executed (shared) concurrently on behalf of separate
applications. A simple language extension has been
designed and added to C++ to make the programming of
persistence, sharing, synchronization and consistency
control expressible. With efficient run-time support for
persistent pointer resolution and consistency maintenance,
this approach can provide much finer-grain execution
concurrency and sharing, easier object navigation, simpler
programability and possibly better performance, than
object-oriented database system.

2. Related work
A number of researches have attempted in provide persistence to object-oriented programming systems. Most
language-based approaches, such as PS-Algol[C0901 and
Eiffel[Me92], use files as object holders and provide the
programmers with explicit commands to load and store
objects from the files. Eiffel allows programmers to store a
tree of objects (by deep copy) into a file and later users can
retrieve the objects by load the file into memory. In the
Coral3 system[MeLa87] Smalltalk objects are saved in a
file system. Locking is provided by the file system to allow
controlled sharing of persistent objects across applications.
Other system such as Object Design adds persistence as
storage class to C++ to develop a high level DML, and
Eastman Kodak (KOS) provides persistence in C++
through inheritance form a virtual base class[At90].
The majority of current work involve a techni ue called
pointer swizzling which is generally combine8 with an
object store or a database that actually provides the storage
management. For instance, Mneme[Mo90] is a persistent
object store using its own format to preserves objects. Each
object has a unique name and is stored as a vector of data
and pointers. The pointers are pointers to other objects in
the store, and hence are object-ids. Mneme supports
locking, consistency control and distribution.
The object-oriented database (OODB) was created to
solve the impedance mismatch problem with relational
databases. This mismatch occurs as relational query
languages, such as SQL, are superior at accessing the

1. Introduction
Object-oriented programming have become recognized
as a powerful software development paradigm in the software community. This paradigm allows application programmers to structure each application as a set of structuring units called objects. Each object is an instance of a
class. A class contains the code and data which defines the
behavior of the objects. Classes and objects provide information hiding with encapsulated data and support objectoriented design and software reuse. The advantages of
object-oriented programming are well known. However,
most general-purpose environments that support objectoriented programming suffer from some of the following
shortfallsi
Significant state information stored in objects or object
rrraDhs is lost with the termination of the application
i h k h creates them.
The complex object graphs or structures created by one
application can not be exported to, shared by or
dynamically re-used in other programs. (Software reuse
allows classes to be re-used, but not instances).
Objects can not be executed concurrently on behalf of
two separate applications.
, Extensive overhead is needed to convert data formats

.
.

.

291

0730-3157193$3.00 0 1993 IEEE

database than general-purpose programming languages, but
are inferior in data structuring, algorithm support and processing speed. This causes the need for two languages for
applications using a data-.
In object-orienteddatabases,
data is stored as objects that can be accessed and used by

the base language. We do not want to tie the design to any
particular operating system and the im lementation
currently is supportable on both SunOS and
[Yo*87].
Later we would consider including Window NT [Cu921.

Ah

3.1 Approaches to persistence
There are three major approaches to designing persistent
object systems. These are the pointer-swizzling approach,

general-purpose ob'ect-oriented languages.
Object-oriented database systems are computational
complete and fit some of the requirements of a persistent
programming system. The OODB's handle object identity
[KhCo86], storage management, concurrency control and
transaction processing. Some of the research objectoriented databases developed in recent years include
Orion [Kim*88], Gem Stone[Ma*861, Exodus[Ca*891, @
Ve*90]. ObServer/Encore[Sk*86], Cosmos[Sr*9 11, Iris
L i*871, and ZeitgeisWo*88].
Some operating systems provide su port for persistent
objects at the kernel level. For exampg, Clouds supports
persistent large-grained objects[DaCh90], Commandos
[MaGu89] provides various kinds of objects using a virtual
object manager and a storage system, and MONADS is an
operating system coupled with a machine architecture for
providing large
istent objects store[R0921.
Cricket and Ger[Va*921 are two systems that utilize
the memory mapping facility provided by Mach operating
system. Cricket maps the entire object store which is in one
file to the address space of an application. Casper uses
memory mapping and shadow paging to provide a distributed persistent store. Other systems that use operating
system support for objects include Emerald[Ju*881, ORCA
[Ba*881, ArgusLi*87]. Cronus[Sc*86], and Eden.

the database approach and the memory mapped approach.
Pointer swlzzling is used by most of the current systems
where objects are accessed via pointers. Initially, the
pinter for each persistent object points to a stub routine
instead of the actual object. When the object is accessed at
the first time, the stub routine gets executed and generates a
fault (object faulting) causing the object to be loaded from a
store and then it changes the pointer, i.e., "swizzles "it, to
point to the actual object in-core address. Subsequent
object accesses get the real object and not the stub routine.
We are interested in a system where the swizzling
management and overhead can be avoided. Also we feel
that swizzling techniques do not allow the level of
concurrency that we are interested in.
Object-Oriented Databases also solve the persistent
object problem. As mentioned before the database approach
solves the database impedance mismatch problem, but
creates more impedance mismatches from a rogramming
environment point of view. For example, latabases are
designed to handle a huge number of objects. This raises
the issues of quick navigation, query-based retrieval,
structuring the store, effective naming for all objects,
content based addressing, etc. Persistent programming
environments on the other hand are expected to contain
much smaller number of objects with different requirements
on naming and retrieval.
In addition. databases are not as concerned about storing
the elaborate structures of the object graphs as the
programming environments should. All objects in a
database need to have unique names, while in the programming system the names of objects are often context or
scope dependent or even anonymous. Sharing in databases
is typically done in a sequential fashion, while sharing in
concurrent programmin environments should be at a finer
grain. Transaction mecfanisms used in database systems
are too restrictive and expensive for general purpose
programming. Dynamic memory allocation is another issue
that differ the databases and programming systems. Objects
in database systems are relatively static in size, however,
the size of an object in general-purpose gramming environment is various by time. Dynamic location is usually
an integral part of programming systems.
The memory mapped approach is a newer ap roach as
more recent operating systems are providing this acility. In
this approach the persistent objects are stored in secondary
storage in their native formats and then memory mapped
into the address spaces of the processes that need to use
them. The storage systems look like an extension of the
primary memory. The advantage of memory mapping is the
elegance and simplicity in addition to the properties of no
format conversions and the availability of run-time sharing.
However, memory mapping creates a set of interesting
situations such as the pointer resolution problem, which
will be addressed latter.

3. Motivations, goals and approaches
This work is motivated by the need to provide an
extension to conventional object-oriented programming
systems that provides persistence and sharing without
extensive changes to the language implementation and
usage paradigm of the environment. We approach the
persistent object system design from the viewpoint of a
general-purpose programming system. The goals of this
research include:
The programming environment should support regular
(transient) as well as persistent objects. Persistence
should be a potential property of all data types. This is
called orthogonal persistence.
The programming methodology used for regular and
persistent objects should not be different, that is persis
tence independence [At*83] should be supported.
Dynamic allocation of objects and dynamic allocation
of memory within objects should be supported for
persistent objects. as it is supported fop regular objects.
Run-time (fine-grain) sharing of objects and object
graphs should be supported and functions for
concurrency control should be provided. We define runtime sharing as concurrent execution of an object on
behalf of two separate applications.
The in-core representation and stored image of a per
sistent object should be identical.
The programmers proficient in the base langua e
should not be burdened with mastering the features k r
persistent programming. Hence, extensions should be
limited and easy to use.
Our work extends a base object-oriented environment by
adding persistence and sharability to it. We chose C++

aI"u

P

292

4. The design issues of the MMPOS

example:

The overall picture of the Memory Mapped Persistent
Object System ( M M P O S ) is depicted in Fig. 1. The system
consists of a persistent object store that is reachable from
the programming environment as a part of the environment.
Our design heavily uses memory-mapped files and preprocessed pointers as the two major mechanisms for
supporting sharing, persistence and dynamic memory allocation. The main advantage of this approach is in its simplicity. The persistent object store resides in a set of files
called object containers. Each container (file) contains
multiple objects. When an application uses an object from a
contamer, the whole container is mapped into the memory
space of the application. All persistent objects are stored in
the container in their native format, that is, they are
runnable when they get mapped into memory.
Since we use C++ as the base language, all objects are
accessed through pointers. Pointers can point to transient
objects, which are absolute virtual memory addresses.
Pointers can point to persistent objects and such pointers
are called "persistent pointers". To a programmer, there is
no difference between a regular pointer and a persistent
pointer, but at run-time, there is a need to differentiate
them. Fferences to persistent objects are made by statically defined names or persistent pointers. The persistent
object can itself have persistent pointers to other persistent
objects (or persistent data segments) in the same container
or in other containers. This issue is further discussed in
section 4.4.

declares a persistent array of 50 integers in the container
"my-personal db". Persistent objects and pure data
segments can be aynamically allocated using a runtime
function described in the following section.

PERSIST int data [ SO] @"mygersonal db";

procur 2

pr-1

4.1 The Language Extension
We are not interested in designing a new language but
making the programming of persistence, sharing, synchronization and consistency control expressible. We decided to
add a minimal set of extensions to the base language C++.
This includes one keywords and eight run-time functions.
As the nature of general-purpose programming systems, we
give the programmers full rights to define object
persistence, object sharability and storage container
assignments.
a) The Keyword PERSIST
P E R S I S T is a keyword which controls the persistence
of statically defined objects. Any instance of any class or
type can be persistent. Each persistent object is associated
with a unique container. The following statement declares
two objects 0 1 and 02 of class circle contained in
container "/shape/Circles".

-

PERSIST circle 01,02 @"/shape/Circles";

If the container does not exist, it is created. If the objects
do not exist, they are created. If the objects and the
container exist already then the program reuses the existing
ones. The container is not mapped until an object within the
container is really accessed. The objects 01 and 02 are
used by the program as any other regular objects. When an
object is created, it acquires a persistent name and this
name can be used by other applications to access the object.
All names for named objects must be unique within a
container. The names of the persistent objects can be
changed to local names within a program by using the
reference type ( T & a=b; ) provided by C++. The
persistence property is not limited to class instances, any
type of structures can be declared to be persistent. For

Fig. 1 A Memory-mapped Persistent Object System
b) Run-time Library
In order to provide dynamic allocation of persistent
memory, concurrency control and check-pointing of
persistent objects, eight run-time functions are added to
C++ to provide a complete persistent programming
environment. They are briefly described as follows.
pnew ( ) creates rsistent object or data segment of a
given type in a specigd container at run-time and returns a
persistent pointer to the given type to the caller. For
instance: pp = pnew (circle,*'/shape/Circles");
allocates a new persistent object of type circle pointed by
pp in the /shape/Circles container. pp is a persistent
pointer. In the programming language level, persistent
pointers behave exactly like any other pointers. Thus
persistent objects can be chained into a complex object
graph.
pdel deallocates a persistent object or a data seg-ment
pointed by the given persistent pointer during the course of
execution. A typical use is pdel (pp);.
slock sets a shared lock on the object which invokes
this locking function. The implicit argument of all locking
primitives is "this" object pointer.
xlock sets the exclusive lock on "this" object while the
execution is entering a critical section.
cblock suspends the execution of a process until a
given condition is satisfied. This function is added to
increase the degree of concurrency. A typical use:

.

cblock ( k>l) ;
unlock unlocks "this" object.
ckpt checkpoints the specified persistent object to the
persistent object store. A typical use is ckpt (object1)
ckpt can only be completed for xlock-free objects.

.

293

Checkpoint can not be made to a pinned object unless
c k p t ( 1 is called by the same process that pinned the
object.
p i n pins a copy of the given object in memory. This
called to assure that the system does not checkpoint the
changes of the object back to the persistent store. A typical

should remember that the concurrency control is already
included within the methods in the class definition.
class b u f f e r {
private :
int
data[100] ;
b u f f e r *next;
public:
buffer(int i=O) {
f o r ( i - 0;i < l O O ; i t t )
d a t a [i]=i;
next = null;

.

use: p i n ( o b j e c t l );
c) Example

The following is a trivial example that shows the
features of the programming environment. In this example,
we have a persistent object of type b u f f er, named buf ,
which is concurrently used by multiple applications. The
b u f f e r class definition contains an integer array, a
pointer and four public methods to manipulate them.
Entries of the array can be read or written, and new
instances can be added or referenced via the n e x t pointer
by separate programs at the same time. Note that locking is
used to synchronize the accesses from separate ograms.
Two user programs show the creation anfuse of the
persistent instance of the class b u f f e r . Program 1 creates
or uses the persistent object buf in container / f oo/data.
If the object exists it is used, otherwise it is created. The
program also dynamically creates an anonymous instance
of the class and assigns it into the same container. After
reading or writing random elements of the array, in
program1 an anonymous object is created, chained to the
b u f object and checkpointed to the persistent store.
Meanwhile, program 2 is accessing the same persistent
object b u f with a local alias room and updates it after a
write to ensure that the change is saved. The persistent
anonymous object is also accessible to program 2. We

1

i n t r e a d ( i n t index) {
i n t n;
s l o c k () ;
n = data[index] ;
unlock ( ) ;
return (n);

1

w r i t e ( i n t datum, i n d e x ) {
xlock () ;
d a t e [ i n d e x ] = datum;
unlock ( ) ;

1

add-buf ( b u f f e r *bp) {
xlock () ;
n e x t = bp;
unlock ( ) ;

1

b u f f e r *get-next ( ) (
r e t u r n (next);

1

Program 1
#include <iostream.h>
#include "buffer.h"
PERSIST b u f f e r buf @ " / f o o / d a t a " ;

Program2
# i n c l u d e <iostream.h>
# i n c l u d e " b u f f e r .h"

PERSIST b u f f e r & room-buf
main 0
{
i n t i n d x , d l , d2;
b u f f e r *p;
indx- random() % 100;
d l = buf r e a d ( i n d x ) ;
p = pnew ( b u f f e r , " / f o o / d a t a " ) ;
p i n (PI ;
p->write ( d l , i n d x ) ;
i n d x = random() % 100;
buf . w r i t e (d2, i n d x ) ;
buf add-buf ( p ) ;
c k p t (PI ;
buf add-buf ( p ) ;

main ( )
{
int
i n d x , d l , d2;
b u f f e r *pp

.

.

1

.

@"/foo/data";

indx- random() % 100;
d l = room. r e a d ( i n d x ) ;
i n d x = random() % 100;
room.write (d2, i n d x ) ;
c k p t (room) ;
pp = room.get-next ( ) ;

1

4.2 Memory mapping facility
Memory-mapping is an operating system supported

. . . . . . . . .

address spaces of two or more processes, the raw contents
of the file is visible to both processes. Any changes made
by one process is immediately seen by the others. Thus, the
memory-mapped segment represents a true shared memory
segment which is not only concurrently shared but prone to
race conditions. Processes accessing the shared segment
have to cooperate using synchronization primitives such as
locks or semaphores. The programming environment
should support such mechanisms to allow synchronization
built into the methods of each persistent object and hence

facility. Currently, several operating systems such as
SunOS, Solaris, Unix system V. Mach, BSDI, OSF/l and
Windows NT provide this functionality.
Memory mapping allows a segment of non-volatile
memory to be mapped into a process address space.
Multiple processes can map the Same segment of memory
and concurrently share it. This memory segment can be a
file. When a file containing some data is mapped into the

294

applied to all processes accessing the object. This is one of
the major reason that we designed locking primitives in the
language extension.
A major problem that comes with this strategy is pointer
resolution in shared addresses. A memory mapped segment
is not guaranteed to be located at the same address every
time it is mapped, therefore, intra-segment pointers can not
be absolute addresses. Making all references as relative
addresses however precludes pointers from referring to
other segments (or containers) and this is not acceptable in
our design as we allow inter-container pointers.
Pointer swizzling is another method of pointer resolution but this technique is unusable in run-time shared
memory mapped objects. In a memory-mapped persistent
object system, all persistent objects are shared (or potentially sharable) and hence a shared pointer cannot be
swizzled. A swizzled pointer that reflects the correct
address in one process will be totally wrong in the context
of another process. Thus pointer resolution is again an important issue for memory-mappedpersistent object system
which desire truly run-time sharing among concurrent
processes.

tainer or in other containers. The resolution of persistent
pointer is transparent to the programmers. Names of persistent objects can be de-referenced to yield persistent
pointers. Dynamically created persistent objects are
accessed via persistent pointers just as dynamically created
regular objects are accessed via regular pointers.
The persistent pointers cannot be memory addresses
unless static memory assignment is used. This is due to the
fact that a container cannot be mapped at the same virtual
address in all processes sharing the container. We have also
ruled out the use of relative address, as it does not allow
cross-container reference while our system does. Basically.
there are at least two different ways of implementing
persistent pointers. One is to merge the object addressibility
and object identity with the compromising of limited
number of objects. The other is to keep addressibility and
identity completely separate at the price of run-time
efficiency.
Merging of addressing and identity yields the most efficient implementation. This is also the preferred implementation if the target machine supports a 64bit address space.
In this scheme each object (and each container) is allocated
a globally consistent address in the huge address space.
However, the 64 bit scheme has other problems such as the
creation of sparse fragmented memory allocation and hence
the lack of locality of memory references.
We have used a surrogate based scheme that works in 32
bit address spaces. A surrogate is a unique identity of each
object. A surrogate is created when the object is created.
The persistent pointer for an object is really the surrogate of
the object and not its address. The translation of surrogates
to addresses is done at runtime and is transparent to the
programmer.
The surrogate-based approach separates object addressibility and identity. That is, a regular pointer is a memory
address while a persistent pointer is an object identifier
which is translated into a memory address at run-time. As
we mentioned earlier, pointer swizzling does not work in
memory-mapped system. So we need some sort of run-time
translation on each access to persistent objects. This can be
done by modifying a scheme termed smart pointers [StSlI.
A smart pointer is not a pointer but an instance of a class
which overloads the pointer operations with some articular
expected actions, such as computing of the actuiaddress,
at run-time. This scheme is elegant, yet can be inefficient as
discussed in [Ro92]. So we modified it by using the preprocessor to directly transform all pointers and names of
persistent objects as instances of smart pointers and then
instead of overloading pointer operations by the smart
pointers, we directly replace all pointer accesses in the
program with in-line code.
To enable this in our system we use the C++ template
construct. The template construct allows users to define
common data structures and behaviors for a set of classes.
Each class instantiated from the template have the same
data structuresand behaviors with its own given type.
In our design, the preprocessor inserts a template for the
class -sptr for all pointers used in the program.

4 3 Object containment
The object graphs are important structures in an objectoriented program. Essentially it is a method of developing
object containment. That is, complex objects can 'contain'
as well as share simpler objects. Instead of physical containment, each object points to other objects logically contained within this object. This containment structure may
recursively create very large and complicated graph smctures in memory. The ability to create, store, re-use and
share these significant structures is the beauty of persistent
object systems.
The question of object containment is however a problem in all systems. If a persistent object contains references
to other objects, are these objects loaded/stored along with
the top level object? In systems using a persistent store, the
retrieval of persistent object may cause retrieval of all the
objects referenced by this object (called deep copy) or
retrieval as these contained objects are really accessed (lazy
copy). While there are many approaches to this issue, the
solution which we used is a kind of combination of deep
copy and lazy copy.
In our design, an object is stored in a container and a
container contains multiple objects. Container assignment
is made by programmers, hence objects within one container are usually context or scope related. When a persistent object gets accessed, the entire container that contains the object is mapped into the application's address
space. Thus, in most cases, objects pointed by this object
are retrieved at the same time. A cross container reference
causes the referenced container to be mapped when the
reference is used. This approach makes use of the semantic
locality of an application program.

4.4 Persistent pointers
As we mentioned above, references to persistent objects

template<class T>
class -sptr {
public :
char flag;
int con-id;

are made through names and persistent pointers. A persistent pointer is a unique system-wide identifier for a persistent object. It is used as any other pointer by the programmers in the programming level. It can be used to
access objects and persistent memory within the same con-

295

//flag=l if persistent
//container id

int
T

4.5 Concurrency and consistence control
In our design, concurrency control is heavily dependent
on the language provided locking primitives to build synchronization schemes into the methods of shared objects.
Since all sharing processes manipulate the same copy of a
shared object, thus, synchronization is applied to all the
collaborattvely concurrent processes. A locking manager is
need to handle different lock requests, maintain a queue of
blocked processes. resume suspended processes schedule
resumed processes, and resolve recursive accesses. etc.
Checkpointing is the mechanism of choice for maintaining consistency in our design. Checkpointing in shared
memory environments is however more complex than that
meets the eye. We plan to use some research done in
Clouds for consistency control adapted for this environment
[ChDagl]. Distributed locking is necessary when this is
extended to distributed systems. Distributed locking is
currently prohibitively expensive as every locklunlock request generated messages on the network. We are investigating lock caching protocols that can make distributed
locking cheaper.

//object id
//memory address

obj-id;
*addr;

1;
All pointer types, such as
circle

*pp;

are replaced by the prepmessor as:
-sptr<circle> pp;

Then in the code any de-referencing of the pp is replaced
as following.
pp->methodl (argl, arg2) ;

is replaced by
if (pp.f lag) {
if (container is not mapped)
map-container(pp.con_id);
pp.addr=get-Vaddr(pp.con-id,
pp. obj-id) ;

I
pp. addr->methodl (argl, arg2) ;

The pointers in our language, now consist of a flag, a
container id and an object id. For regular pointers, the
flag is set to 0 and addr contains the address of the
object. For a persistent pointer, f l a g is set to 1 and
c o n id and o b j id contain the surrogate of the
persisnt object. Eaclipersistent object resides in a unique
container which is distinct with the container-id. At runtime, the pointer flag is checked, if the access is made to a
persistent object, the container mapping and object address
translation is performed. The get .Vaddr( ) is a routine
that looks up a hash table and fiiids the actual memory
address for the referenced object in the context of the
zpmically.
process. The replacement should be execut!
Similar replacement have to be done for the . ,
"*"
and "==" operations. These replacement can be made even
more efficient by inserting in-line assembly code. With this
scheme, the number of object is no longer limited by the
address space, meanwhile, the distinction between object
addressibility and identity is retained.

4.6 Run-time environment
Our current design and ongoing prototype are based on
the C++ programming environment. In C++, objects are
data only. Methods of each defined class are linked into the
applicatton program, and all instances of classes contain
only the data portion of the class defmition. The invocation
of object methods is actually done by passing the address of
the object data to the class defined methods which have
been linked to the program. The encapsulation is in fact
complied away at run-time, yet insisted by the compiler at
compiling time.
The overall picture of the present prototype is shown in
Fig. 2. The container and objects are handled by a object
manager called SPOM. SPOM treats the object store like
Unix treats the file system. The application program
communicates with the SPOM to get access to object
containers, translate object names to ids, create new objects
and delete existing object.

&'I,

z
he-processor

Fig. 2 The overall system structure
296

Each container consists of two files, one called table file,
contains a table of object names, object ids, object sizes and
persistent heap pointers. The data file holds the native
object data and a persistent heap for allocating anonymous
persistent objects. The SPOM is also responsible for
assigning the object ids and container ids. Another function
of SPOM is to handle mapping requests form the
applications. Mapping requests are inserted into the
application programs by the pre-processor. The entire
container is mapped when any object in the container is
used. In our current design, the detection of an access to an
unmapped container is done at the point of translating the
object identifier to the virtual address. This can be handled
without problem if we separate the addressibility and object
identity, especially in systems (such as SunOS) which do
not support external pager. It would be a problem if we mix
the object ids with their memory addresses, and it can only
be detected as a memory fault. External pager is needed in
such design to bring in the required container. The latter
has been carried out on the Mach operating system.

[Cu92] H. Custer. Inside Windows NT.Microsoft Press. 1992.
[ChDa91] R.Chen and P. Dasgupta, "implementing Invocation
Based Consistency Control," Proc. 11th lntl. Conf. on
Distributed Computing System, June, 1991
[DaCh90] P. Dasgupta and R. Chen, "Memory Semantics in Large
Grained Persistent Objects," Proc. of the 3r Intl. Workshop in
Persistent Object System, pp.219-231,1990.
[Fi*87] D.Fishman. et al., "IRIS: An Object-Oriented Database
ManagementSystem," ACM Tram. on Office Informotion
System, 1987.
[Fo*88] Ford, et al., "ZEITGEIST Database Support for ObjectOriented Programming," Proc. of 2nd lntl. Workshop on
Object-OrientedDatabase Systems, 1988.
[Ju*88] E. Jul. H. Levy, N. Hutchinson and A. Black, "FineGrained Mobility in the Emerald System," ACM Trans. on
Computer Systems. Vol. 6, No. 1. Feb.1988.
[KhCo86] S. Khoshafian and G. Copeland. "Object Identity,"
Proc. ofOOPSLA-86.1986.
[Kim*88] W. Kim, N. Ballou. H.T. Chou. J Carza and D.Woelk.
"Integrating an Object-Oriented Programming System with
a Database System,"In Proc. OOPSLA-88Cor&erence, 1988.
[Li*87] B. Liskov. D. Curtis, P. Johnson and R. Scheifler.
"Implementation of Argus," In Proc. 11th ACM Symposium
on Operating SystemsPrinciples. Nov. 1987.
[Ma*86] D. Maier. J. Stein, A. Otis and A. Purdy. "Development
of an Object-Oriented Database." I n Proc. OOPSLA-86
Conference. November 1986.
[MaGu89] A. Marques and P. Guedes, "Extending the Operating
Support for an Object-Oriented Environment," In Proc.
OOPSU-89 Coqference.October 1989.
[Me921 Bertrand Meyer. Eiffel: The Language, Prentice-Hall.
[MeLa871 T. Merrow and J. Lawsen, "A Pragmatic System for
Shared Persistent Objects," In Proc. of OOPSLA-87
Conference, October 1987.
[Mo90] J. Moss, "Design of the Mneme Persistent Object Store,"
ACM Tram. on lnformotwnSystems, April. 1990.
[Ro92] J. Rosenberg, "Architectural and Operating System
Support for Orthogonal Persistence," Computing System.
Vol. 5 , No. 3, Summer 1992.
[Sc*86] R. E. Schantz, R. H. Thomas and G. Bono, "The
Architecture of the Cronus Distributed Operating System."
Proc. Sixth Intl. Conference on Distributed Computer
System, May 1986.
[Sk*86] A. Skarra, et al., "An Object Server for an ObjectOriented Database System," Proc. of Workshop on ObjectOriented Databases, 1986.
[Sr*91] D. Sriram. et al., "COSMOS: an Object-Oriented Expert
System Shell." Technical Report, IESL, MIT 1991.
[St911 B. Stroustmp. The CU Programming Language, The 2nd
edition, AT&T Bell Laboratories. New Jersey, AddisionWesley, New York. 1991.
[Va*92] f. Vaughan, et al.. "Casper: A Cached Architecture
supporting Persistence," U S E " Computing Systems. Vol.
5 , No.3, Summer 1992.
[Ve*90] F. Velez, V. Darnis, D. DeWitt and P. Futtersack,
"Implementing the 02 Object manager: some Lessons." Proc.
of the 4th I t d . Workshop on Persistent Object Systems, pp.
129-136. September, 1990.
[Yo*87] M.J. Young, A Tevanian, R. Rashid. D.J. Colub. JJ.
Eppinger, J. Chew, W.J. Bolosky, D. Black and R. Baron,
'The Duality of Memory and Communication in the
Implementation of a Multiprocessor Qperating System,"
Proc. of the 11th Symposium on OS Principles,Nov. 1987.

5. Conclusion and future work
The memory-mapped approach to persistent object
systems is a interesting and feasible attempt as well. It has
advantages such as providing uniform view of storage,
eliminating most impedance mismatch described in section
3, suppomng dynamic persistent memory allocation, and
allowing sharing complex object graph at a fine-grain level.
Although the design seems relatively straightforward, there
are some pitfalls. The pointer resolution is difficult to
implement. Type-security has not been addressed.
hogrammers are expected responsible for ensuring that
only persistent pointers are saved inside persistent objects.
Storing transient pointers to persistent object will lead to
dangling-pointer problems which often are undetectable
errors.
A persistent programming environment that extends a
popular language without adding too much overhead for the
programmer is appreciated. The design hides the complexity of pointer resolution in mapped memory system and
make them look and work like pointers in private memory
system. We present one such design and also discuss
several approaches to the implementation. The prototype
system is currently under implementation on both SunOS
and Mach systems.

Reference
[At901 T. Atwood, 'Two Appraoches to Adding Persistence to
C u , " Proc. of the 4th Intl. workshop on Persistnet Object
System, 1990.
[At*83] M. Atkinson. P. Bailey, et al.. "An Approach to Persistent
Programming," Computer Journal, 1983
[Ba*88] H.E. Bal, M.F. Kaashoek and AS. Tannenbaum, "A
Distributed Implementation of a Shared Data-ObjectModel",
Workshop on Experiences in Distributed and Multiprocessor,
1988.
[Ca*89] M. J. Carey, D. J. DeWitt, J. E. Richardson and E. J.
Shekita, "Storage Management for objects in EXODUS." in
Object-Oriented Concepts, Databases and Appl.. Frontier
Series, Addison Wesley. 1989.
[C090] W. Cockshott, PS-Algol Implementations: Appl. in
Persistent Object-Oriented Programming, Ellis Horwood
Limited, 1990.

297

Choices in Database Work&+on-Server

Architecture

Iris. S. Chu and Marianne S. Winslett
Computer Science Department
University of Illinois
1304 W. Springfield Ave., Urbana, IL 61801
Abstract

regardless of architecture; in this paper, we extend the
dimensions of categorization used in [DeWiSO],by considering alternative architectures for access to data in
memory. While benchmark suites such as the 001
benchmark [CattSl] could be used to compare the
performance of different systems, current benchmarks
omit some important factors, such as degradation with
increasing levels of concurrency, and application development effort. This paper compares these different architectures in performance as well as non-performance
aspects, and their possible impact on applications.

The workstation-server model is emerging as the
standard computing environment for engineering and
scientific applications. Most object-oriented database
systems choose either the object-server architecture, an
which individual objects are passed between the server
and the workstation, o r the page-server architecture,
in which a disk page is the unit of transport between
the server and the workstation. I n this paper we e%tend [De Wig01 to consider the advantages and disadvantages of an implementation in which the application has direct access to pages, rather than to objects.

1

2

Introduction

There are many possible variations of the objectserver and the pageserver architectures. Existing
object-server OODBMSes are similar to one another,
compared to the large difference that two pageserver
systems can exhibit. In this section we review the
major characteristics of object-server and page-server
systems, and add a description of the techniques used
to access data in memory in the two systems.

Most current DBMSes adopt some variant of the
client-server architecture. Relational databases typically take the query-server approach where clients
send requests to the server as queries and the server
sends the results of the queries to the clients. This approach has the advantage of minimizing the amount
of data being transmitted since only data which satisfies the query needs to be sent from the server to the
requesting clients. Since applications employing an
object-oriented database typically have a large navigational component, most OODBMSes take the dataserver approach where clients request specific data
items from a server for processing, to exploit the computing power and memory of the client machines.
Data-server systems can be categorized as pageserver
systems if the clients and the server interact using a
physical unit such as a page or a segment, or as objectserver systems if the unit of interaction is a logical unit
such as an object or a logical cluster of objects.
As [DeWiSO] showed, the performance of page
server systems is likely to be superior to that of objectserver systems for the target applications of objectoriented database systems,such as CAD, when good
data clustering is possible. The DeWitt experiments
used the same techniques to access data in memory,

2.1

Object-Server Systems

In the object-server architecture, objects or logical
clusters of objects are the unit of transfer between
the server and the workstation, with the side effect
that much of the DBMS functionality is replicated on
both the workstation and the server. MCC’s Orion
1 [KimSO] and Versant[VersSO] are good examplee of
object-server DBMS architecture.
An object-server DBMS employs a dual-buffer management scheme by partitioning database buffer space
into a page buffer pool and an object buffer pool. The
page buffer pool holds disk pages and resides on the
server, while the object buffer pool is placed on the
client. The object buffer manager resides on the client
and maintains an OID-to-handle table to locate objects cached in the object buffer pool.

298

0730-3157193$3.000 1993 IEEE

Object Servers and Page Servers The Present

Distributed Authentication for Peer-to-Peer Networks
Shardul Gokhale and Partha Dasgupta
Arizona State University
Tempe, AZ 85287-5406
{shardul, partha}@asu.edu

cryptographically secure, fault-tolerant, scalable
trust model for resilient peer-to-peer networks.
Our design employs a secure group based
approach for the creation of a distributed trust
model. In our opinion, the pure peer-to-peer
organization and the flexibility provided by this
model make it suitable for ad hoc networks.
During the formation of a troup a node generates
an identity within a troup. However, it may
choose not to reveal the identity during
verification. These troups are dynamic in nature
and support the addition and deletion of nodes as
well as the troup-merge operations. Overlapping
troups can form a network-wide security
infrastructure.
The rest of the paper is organized as follows:
Section 2 provides the background and motivation
for the development of such a trust model. The
core idea of the trust model based on a secure
group construct is illustrated in Section 3. Section
4 provides the security protocols for this trust
model. It also illustrates that the constructs are
cryptographically secure. The implementation of
the verification protocol and performance results
are provided in Section 5.

Abstract
A public key infrastructure is generally (and
effectively) used for cryptographically secure
authentication in the networks. Ad-hoc networks are
formed in haphazard manner. Security services for adhoc networks cannot assume the existence of a
particular infrastructure. Peer-to-peer technology is
promising in addressing security issues in ad-hoc
networks. We provide a novel; cryptographically
secure representation of trust based on secure groups troups. We show how troups can be constructed in a
distributed manner using RSA accumulators. The
troup-membership is verified using the zero-knowledge
protocol of modular exponentiation. Each node in a
group has an identity within a group, but it is not
required to reveal the identity during verification. This
trust model is not centrally controlled and can be
deployed incrementally in the network. This paper
presents the protocols and a prototype implementation
of the troups based authentication system.

1

Introduction

Ad hoc networks are formed in a “haphazard”
manner and do not rely on an established
infrastructure. These physical properties of ad-hoc
networks are similar to peer-to-peer networks at
logical level. We feel that peer-to-peer techniques
can be used on ad-hoc networks to enhance
efficient usage of the underlying topology.
Security
(i.e.
authentication
and
trust
management) is based on the notion of trust and it
is not well defined for ad-hoc networks.
Centralized security solutions for ad-hoc networks
could be significantly vulnerable [2].
Recently, a lot of effort has been put in
providing security infrastructure for ad hoc
networks [2, 3, 4, 5]. These efforts aim at
distributing CA functionality to a set of nodes in
the network. In this paper, we propose, troups, a

2

Background

Recent publications [2, 4, 5] propose the idea
of making existing authentication models in the
Internet (Hierarchical and PGP) [1] suitable for
mobile ad hoc networks.
2.1

Threshold Cryptography

In [2, 4] the use of threshold cryptography is
proposed to distribute CA functionality to various
nodes. In [4] private key of certifying authority is
shared between a few nodes. These nodes can sign

1

We call such trust-based groups, troups. In this
paper, we propose a new protocol for troup
formation - creation of RSA accumulator in a
distributed manner. To support the dynamic
membership in a troup, we have developed troup
mutation protocols. These protocols are based on
CLIQUES protocols. Membership verification is
based on the zero-knowledge protocol for
modular exponentiation. Thus, even a nonmember can verify troup membership.
This representation of trust also supports
transitive trust. For the network shown in Fig. 1,
an immediate trust relationship exists between
nodes 1 and 3, and nodes 3 and 4. Node 1 can
verify the transitive trust that it has with node 4 by
verifying the immediate trust between itself and
node 3, and nodes 3 and 4. Transitive trust is
helpful in establishing chains of trust in a
network. Troups also provide bi-directional trust
representation. If a member within a troup starts
behaving maliciously, the other troup members
can revoke the trust simply by removing the
member from the troup.

a certificate using the respective shares. The node,
for which this certificate is generated, collects
these shares and combines them to generate a
certificate. A centralized trusted node is needed to
initialize the shares. Due to the support provided
for generating additional shares, a compromised
node can steal the CA’s private key if has k
shares. This problem is significant because it is
difficult to revoke the CA’s signing key.
2.2

Certificate Chains

The approach proposed in [5] is based on the
PGP authentication system. In this pure peer-topeer approach a node updates a sub-graph of the
certification graph of the network periodically.
Whenever two nodes want to authenticate each
other, sub-graphs are merged with or without
helper nodes in an attempt to create a certificate
chain. If the certificate chain exists, the node is
authenticated. Due to dynamic nature of ad hoc
networks, revocations play an important role.
2.3

Security and Cost

4
No network can be 100% secure. The security
of the network is trade-off between the security
needs and the cost associated with it. The
aforementioned security models provide proactive
approach, so revocation is a costly operation. A
network that is easier to compromise can be made
resilient to such attacks. As ad hoc networks are
dynamic and formed without infrastructure, a
resilient approach may be better for security.

3

We
use
Collision-Free
One
Way
Accumulators [9,10] for the construction of
troups. To verify membership, we use zeroknowledge proof for modular exponentiation [11].
In [9], Benaloh and de Mare, propose a
cryptographically secure construct – One-way
Accumulator (OWA). The same construct is used
for generation of troups. Each to-be-member
generates an exponent, y i - that is to be
accumulated. They also agree on the seed of the
accumulator, x , and modulus, N .
Accumulation z
x y i * y 2 * ... * y n mod
N
and auxiliary value
aux i
x y 1 * y 2 * ... * y i  1 * y i  1 * ... * y n mod N
The accumulation (troup identifier) is public,
- used to represent a group. As the membership of
an accumulator cannot be forged, the
accumulation becomes the public key of the
group. To verify the membership of troup ( z ), the
member can present the values ( auxi , y i ) to the
verifier. The membership is verified if

New Trust Model

1

Troup A

Troup B
3

Tools

4

2

Fig. 1. A Trust Model based on Troups

In our model, trust is represented by a group
membership. Whenever two nodes in a network
decide to establish a trust relationship, a new
group, which consists of the two nodes, is formed.

z

2

y

auxi i mod N . However, if the values

verification function, authentic ( z , y ' , accu ' ), is
changed in order to accept only a particular set of
y ' . We propose the following technique for RSA
accumulator construction:
1. Each participant generates two large prime
numbers, p1i , p 2i and product y i p1i * p 2i .

( auxi , y i ) are revealed to the verifier it can be
used maliciously. The other approach would be to
prove the knowledge of the two values ( auxi , y i )
such that z can be constructed by modular
exponentiation. The zero-knowledge protocol for
modular exponentiation proposed in [11] by
Camenisch and Michels can be used for this
purpose.
4.1

2. Participants send respective yi to the CCA.
3. CCA calculates temporary auxiliary values,
tauxi , that correspond to, yi .

Troups Formation and Membership
Verification

taux i

4. CCA
participants.
5. The
participants

In this section, we propose secure generation
of accumulator in a distributed manner. z is the
accumulation and N is RSA modulus. The
accumulator is based on an elementary
accumulator function i.e. x y mod N . Security of
the troup is considered only against passive
attackers. Just for the illustration, assume a central
computing authority (CCA) and an accumulator
seed, x . In actual troups implementation a
designated troup controller replaces CCA and
accumulator seed is constructed in a contributive
manner. A regular RSA accumulator is generated
as follows:
1. Each participant generates a large prime
number, y i .

auxi

tauxi

p2 i

then

calculate

mod N .

Thus, each participant effectively generates
two members. The attacker can snoop on yi
values but can never know p1i and p 2i . To ensure
that ( tauxi , yi ) are not used during membership
verification, a verification function is also made to
check for the bit-length of the exponent. If the
prover exceeds the restricted bit-length of the
exponent, the verification protocol terminates
with rejection. The restricted bit-length is such
that bit-length ( yi ) > restricted bit-length > bit-

2. Participants send respective y i to the CCA.
3. CCA calculates auxiliary values,

x y1* y 2 *...* yi 1* yi 1*...* y n mod N
transmits
to respective
tauxi

length ( p1i or p 2i ). Protocol proposed in [11]
satisfies this requirement.
The security of troups trust network depends
on the security of all the overlapping groups in the
network. The strongly one-way property of the
accumulator ensures that finding ( y ' , accu ' ) for a
specific troup is hard. Security of troups network
may be compromised if it is easy to find
collisions. The RSA Accumulator [10] makes
troups secure against collisions. Under strong
RSA assumption, collision-free property of RSA
accumulator ensures that finding ( y ' , accu ' ) for
any group in troups trust network is hard to find.
The seed of the accumulator can be generated
in a secure manner using a multiparty diffiehellman key agreement protocol. In this paper, we
present specific techniques based on CLIQUES
[7] for troups. The advantage of using CLIQUES
for RSA accumulator generation is that the
communication for accumulator construction can
be integrated with the CLIQUES key agreement

auxi ,

corresponding to, y i .
4. CCA transmits auxi appropriately.
To prevent passive attacker from listening
( auxi , yi )
values
during
accumulator
construction, the communication can be encrypted
with a secret key. The extra round needed for
encryption can be avoided by slight modification
in the protocol. The strongly one-way property of
the accumulator is defined in [11] as “…given
only ( y1 ,… , y N ) and z , it is hard to find a pair
( y ' , accu ' ) such that authentic( z , y ' , accu ' ) =
ok with y ' { y1 ,..., y N } .” This property suggests
that an accumulator is not compromised even if
the attacker can choose the values of y ' . To
ensure that the attacker does not choose the values
y i , used during the accumulator construction; the

3

modular exponentiation. A detailed design of the
protocol can be found in [11], but we are
providing short description for completeness. The
protocol provided in [11] hides all the numbers
that are used in modular exponentiation. E.g. To
prove knowledge of a , b , d , and n , such that
a b { d mod n ,
the
prover
generates
commitments for all the numbers and does not
reveal the actual numbers. In the case of troups d
and n are already known to the verifier. This
does not pose a threat to the security of zeroknowledge protocol. Such an approach is already
used in [12].
The verifier and prover start off by agreeing
on a commitment scheme for the protocol. The
prover sends commitments for auxi , p1i , z , n ,
and the intermediate results of square and
multiply algorithm. Commitments for z and n
are opened by the prover. The prover then sends
the responses to verifier for the challenges sent. If
the verifier can reconstruct commitment for z
from the initial commitments the verification
succeeds. Details about implementation of this
protocol and performance results are provided in
section 6.

protocol. In general, any technique that confirms
to the above requirements e.g. STR, TGDH [13]
can be used to create of troups.
Protocol for troup formation:
The troup formation protocol shares the
communication for accumulator generation with
the IKA.1 protocol of CLIQUES; and hence it
takes n rounds for completion. First n-1 rounds
include collection of exponents from all the
participants. In the last round (n), the last
participant receives all the exponents. As this
round results in the completion of the IKA
CLIQUES protocol, the last participant knows the
seed, x , and the exponents ( y1 , y 2 … , y n ). It

x y1*y2*...*yi1*yi1*...*yn modN ,

then broadcasts, tauxi

corresponding to i th member. The members
calculate respective auxiliary values, auxi , and
accumulation, z .
For the first n-1 members:
1. M i receives a set of i-1 exponents from first

M i 1 members.
2. M i generates two prime numbers , p1i and
p 2i . The value y i p1i * p 2i is added to the

4.2

set of exponents.
The highest indexed participant, M n acts as
the troup controller, which calculates intermediate
auxiliary values for all the members and
broadcasts them to the members.
1. M n calculates the seed, x , in the last round
of IKA protocol.
2. The
intermediate
auxiliary
value
y * y * ... * y * y * ... * y
,
which
taux i
x
mod N
1

2

i 1

i 1

Troups Mutation

Troup membership changes throughout its
lifetime. To facilitate troup changes we have
retained the concept of group controller from
CLIQUES. Protocols for troup mutation are
based on CLIQUES AKA protocols. Troup
mutation relies on the ability of AKA protocols to
generate the new seed value. The specific
communication with the troup members and rights
to add/delete a member depend on troup policy. In
general, any troup member that retains the
contribution of earlier participants can be a troup
controller and has ability to mutate a troup.

n

corresponds to i th member, is calculated.
M n broadcasts these values to the members.
Each member needs to find an auxiliary value for
membership proof. They proceed as follows.
p
1. calculate auxi tauxi 2 i mod N . ( auxi , p1i )
becomes the primary key of troup member.
2. The accumulation z is also calculated. z is
the public identifier of the troup.

Member Addition
To join a troup, the participant provides a new
seed and an accumulator contribution. It is
assumed that the incoming member becomes the
troup controller. The addition of a member can be
considered as an extension of the last step of the
troup genesis.

Protocol for membership verification:
The protocol used for membership
verification is the zero-knowledge protocol for

4

needs to prove following two statements in zeroknowledge.
x The factors - p1i and p 2i - of the public

Member Addition protocol:
1. The new group controller generates its
contribution for the seed and accumulator
y n 1 .
2. The old troup controller sends the contents of
the last broadcast message to the new troup
controller. This message includes the set of
exponents contributed by all the troup
members for generation of accumulator i.e.
( y1 ,… , y N )
3. The new controller generates the intermediate
values required for calculation of the auxiliary
values.

identifier yi are known to the prover.
x One of the factors
verification
of

z

membership.

i.e.

mod n .

The commitment of a number x will be
generated as C x g x h r mod n' where ( g , h ,
and n' ) is the commitment scheme used for the
protocol [11].
The protocol is executed in the following
manner.
1. The prover generates commitments C z ,

Mass Join and Group Fusion:
It may sometimes be necessary to add
multiple members to the troup. It can be done by
chaining the members to be added as in the case
of CLIQUES. The new group controller then
calculates the intermediate values for accumulator
and broadcasts those values to the members. A
special case of mass join is group fusion. In this
case, the incoming members have pre-established
relationship. These relationships can be used to
calculate new seed as in CLIQUES.

C auxi , C p1i and C n which correspond to z ,
auxi , p1i , and n .
2. The verifier proves modular exponentiation
by using the zero-knowledge proof for
modular exponentiation.
3. The verifier proves the representation of
g yi to the bases C p1i and h , i.e.

g y { C p1i h v

Member Exclusion:
To remove a member from a troup, the troup
controller generates a new seed using earlier
contribution (as specified in the AKA). This new
seed is used for accumulation of values. As the
outgoing member cannot differentiate the new
seed from any random number, the security of a
troup is guaranteed. Multiple members can also be
removed using this strategy. In fact it is easier to
remove multiple members - in the sense that,
fewer numbers of intermediate values need to be
generated.
4.3

aux i

p1 i

p1i is used in the

5

Implementation and Performance

The protocol for membership verification is
implemented in C++ using Crypto++ API for
cryptographic functions. The verification protocol
is between two agents, viz. ProverAgent and
VerifyAgent. The ProverAgent proves knowledge
of ( auxi , p1i ) such that on modular
exponentiation in mod n , troup identifier z is
generated.
This stateful protocol is divided into the
following five states.
Initialization State (T1): In this state, a secure
environment for the execution of zero-knowledge
protocol is generated. Specifically, a commitment
scheme for the verification protocol is generated.
The commitment scheme implemented in this
protocol is same as the one used in [11]. The
ProverAgent generates a group of appropriate
order, N, generators, g and h, and sends it to the
VerifyAgent. The ProverAgent uses the

Member Identification

To verify, the membership of a particular
group, a member is not required to identify.
However, if desired, a member could be made to
reveal the identity within a troup. The product y i
provided by the node during the construction of
an accumulator, becomes the public identifier of
the node. To verify that a node is a member, it

5

commitment scheme - g, h, and N for generating
commitments.
Commitment State (T2): The ProverAgent
generates
various
commitments.
The
commitments C z , C auxi , C p1i and C n correspond

Commitment scheme is set at 128-bit:
100000
90000
80000

Execution time (ms)

70000

z , auxi , p1i , and n . It also generates
commitments for individual bits of exponent p1i ,
modular powers of auxi , and intermediate values
to

60000
50000
40000
30000
20000

of square and multiply algorithm. These
commitments are saved by the ProverAgent and
sent to the VerifyAgent at the end of this state.
The ProverAgent uses these commitments for
generating responses and VerifyAgent uses them
for verification.
Challenge State (T3): In this state, the
VerifyAgent saves the commitments sent by
ProveAgent. The VerifyAgent randomly generates
the challenges. It saves the challenges, as they are
required during verification and sends them to the
ProverAgent.
Response State (T4): Depending upon the
challenges sent by the VerifyAgent and initial
commitments, the ProverAgent calculates the
responses. It sends the responses to the
VerifyAgent and deletes the state for the current
protocol.
Verification State (T5) The VerifyAgent
verifies the responses. If all the responses are
proper according to the zero-knowledge proof, the
verifier accepts the membership.
For calculating the performance we measured
T4 – T1 and T5 – T1 as the time consumed by
ProverAgent and VerifyAgent resp. Actually, the
ProverAgent is idle for time period T3 – T2 and
the VerifyAgent is actually idle for T4 – T3 + T2 –
T1. In practical implementation time can be
multiplexed.
The performance of verification protocol
mainly depends on the following factors:
1. Bitlength of secret values ( auxi , p1i ):

10000
0
0

50

100

150

200

250

300

Size (bitlength)

ProveAgent

VerifyAgent

Fig. 2 Size of secret values changed

The bitlength of secret values is set at 128-bit:
250000

Execution time (ms)

200000

150000

100000

50000

0
0

50

100

150

200

250

300

Size (bitlength)

ProveAgent

VerifyAgent

Fig. 3 Size of commitment scheme changed

The bit-length of both the values varies from 32bit to 256-bit.
600000

Execution time (ms)

500000

2. Bitlength of commitment scheme ( g , h , n ):
The performance measurements are done on
windows 2000 PCs running on P3 processors.
Both the machines were lightly loaded at the time
of performance measurements. We measured the
performance of the protocol by changing the
bitlength of the exponents as well as commitment
scheme.

400000
300000
200000
100000
0
0

50

100

150

200

250

300

350

Size (bitlength)
ProverAgent

VerifyAgent

Fig 4 Size of commitment scheme and secret values
changed

6

As it can be seen from Fig. 2, execution time
of the verification protocol increases linearly with
the increase in the bit-length of the secret values.
Fig. 3 illustrates that the execution time rises
sharply when commitment scheme is 128-bit and
224-bit. The increase for in-between intervals is
not so sharp. Graph in fig. 4 is combination of
both the changes. It can be concluded from the
above experimental results do not look promising
enough to implement the protocol in low-end
mobile devices. The sharp rise in curves at 128-bit
and 224-bit commitment scheme seems specific to
implementation of modular exponentiation.
Support from optimized hardware and software
specific to this functionality will be needed for
practical implementation of this protocol.

6

[4]

[5]

[6]

[7]

Conclusion
[8]

We have presented a novel way of
representing trust relations between the nodes in
the network using secure groups construct, troups.
We have shown how troups can be created in a
pure peer-to-peer manner. We have suggested
protocols for handling changes in troups
membership. Each node has an identity inside a
troup. A node may choose not to reveal its
identity during the verification process,
maintaining anonymity in the network. A network
of nodes can thus be secured incrementally and
distributed manner using these constructs. The
mobility of the nodes is handled by localizing the
impact of trust revocation. The verification
protocol is based on the zero-knowledge protocol
for
modular
exponentiation.
We
have
implemented the protocol to test its performance.

[9]

[10]

[11]

References
[12]

[1] Adams, C. M., Mike Burmester, Yvo
Desmedt, M. K. Reiter, and Philip
Zimmermann, “Which PKI (public key
infrastructure) is the Right One? (Panel
session).” ACM Conference on Computer and
Communications Security 2000: 98-101.
[2] Zhou, L., and Z. J. Haas “Securing Ad Hoc
Networks”, IEEE Network, 1999.
[3] Weimerskirch, André, and Gilles Thonet “A
Distributed Light-Weight Authentication

[13]

7

Model for Ad-hoc Networks,” v. 2288 of
LNCS, 2002.
Luo, Zerfos, Kong, Lu and Zhang, “Selfsecuring Ad Hoc Wireless Networks,” ISCC
‘02
Botelho, Luis, Steven Willmott, Tianning
Zhang and Jonathan Dale, “Self Organized
Public Key Management For Mobile Ad Hoc
Networks,” Technical Reports in Computer
and Communication Sciences, 2002.
Abdul-Rahman, A. and Stephen Hailes, “A
Distributed Trust Model,” In proceedings of
the 1997 New Security Paradigms Workshop,
ACM Press, 1998: 48-60.
Steiner, Michael, Gene Tsudik, and Micheal
Waidner, “CLIQUES: A New Approach to
Group Key Agreement.” IEEE Transactions
on Parallel and Distributed Systems, August
2000.
Ateniese, Giuseppe, Michael Steiner and
Gene
Tsudik,
“New
Multi-party
Authentication Services and Key Agreement
Protocols.” IEEE Journal of Selected Areas in
Communications, April 2000.
Benaloh, Josh, and Michael de Mare, “One
Way
Accumulators:
A
Decentralized
Alternative to Digital Signatures,” Advances
in Cryptology – Proceedings of Eurocrypt
’93, Springer-Verlag, 1993.
Baric, Niko, and Birgit Pfitzmann, “Collisionfree Accumulators and Fail-stop Signature
Schemes without Trees” Advances in
Cryptology – Eurocrypt ’97, v. 1233 of
LNCS, Springer-Verlag, 1997: 480-494.
Camenisch, Jan, and Markus Michels,
“Proving in Zero-knowledge that a Number is
the Product of Two Safe Primes.” Advances
in Cryptology – Eurocrypt 1999, v.1592 of
LNCS, Springer – Verlag, 1999: 106-121.
Sander, Tomas, Amnon Ta-Shma and Moti
Yung. “Blind, Auditable Membership
Proofs,” Financial Cryptography 2000: 53-71.
Kim, Yongdae, Adrian Perrig and Gene
Tsudik, “Simple and Fault-Tolerant Key
Agreement for Dynamic Collaborative
Groups.” 7th ACM conference on Computer
and Communication Security, November
2000.

Metacomputing with MILAN
A. Baratloo P. Dasgupta V. Karamcheti Z.M. Kedem
fbaratloo,dasgupta,vijayk,kedemg@cs.nyu.edu
Courant Institute of Mathematical Sciences, New York University

Abstract
The MILAN project, a joint effort involving Arizona
State University and New York University, has produced
and validated fundamental techniques for the realization
of efficient, reliable, predictable virtual machines, that
is, metacomputers, on top of environments that consist of
an unreliable and dynamically changing set of machines.
In addition to the techniques, the principal outcomes of
the project include three parallel programming systems—
Calypso, Chime, and Charlotte—which enable applications
be developed for ideal, shared memory, parallel machines
to execute on distributed platforms that are subject to failures, slowdowns, and changing resource availability. The
lessons learned from the MILAN project are being used to
design Computing Communities, a metacomputing framework for general computations.

1. Motivation
MILAN (Metacomputing In Large Asynchronous
Networks) is a joint project of Arizona State University and
New York University. The primary objective of the MILAN
project is to provide middleware layers that would enable
the efficient, predictable execution of applications on an unreliable and dynamically changing set of machines. Such a
middleware layer, will in effect create a metacomputer, that
is a reliable stable platform for the execution of applications.
Improvements in networking hardware, communication
software, distributed shared memory techniques, programming languages and their implementations have made it feasible to employ distributed collections of computers for executing a wide range of parallel applications. These “metacomputing environments,” built from commodity machine
nodes and connected using commodity interconnects, afford
significant cost advantages in addition to their widespread
availability (e.g., a machine on every desktop in an organization). However, such environments also present unique
 On leave from Arizona State University

challenges for constructing metacomputers on them, because the component machines and networks may: (1) exhibit wide variations in performance and capacity, (2) become unavailable either partially or completely because
of their use for other (non-metacomputing related) tasks.
These challenges force parallel applications running on
metacomputers to deal with an unreliable, dynamically
changing set of machines and have thus, limited their use
on all but the most decoupled of parallel computations.
As part of the MILAN project, we have been investigating fundamental techniques which would enable the effective use of metacomputing environments for a wide class of
applications, originally concentrating on parallel ones. The
key thrust of the project has been to develop run-time middleware that builds an efficient, predictable, reliable virtual
machine model on top of unreliable and dynamically changing platforms. Such a virtual machine model would enable
applications developed for idealized, reliable, homogeneous
parallel machines to run unchanged on unreliable, heterogeneous metacomputing environments. Figure 1 shows the
MILAN middleware in context. Our approach for realizing
the virtual machine takes advantage of two general characteristics of computation behavior: adaptivity and tunability.
Adaptivity refers to a flexibility in execution. Specifically, a computation is adaptive if it exhibits at least
one of these two properties: (1) it can statically (at
start time) and/or dynamically (during the execution)
ask for resources satisfying certain characteristics and
incorporate such resources when they are given to it,
and (2) it can continue executing even when some resources are taken away from it.
Tunability refers to a flexibility in specification.
Specifically, a computation is tunable if it is able to
trade off resource requirements over its lifetime, compensating for a smaller allocation of resources in one
stage with a larger allocation in another stage and/or a
change in the quality of output produced by the computation.
Our techniques leverage this flexibility in execution and
specification to provide reliability, load balancing, and pre-

applications
written for reliable
metacomputers

application

virtualization layer

application

application

MILAN run-time middleware

unreliable and
dynamically changing
set of machines

Internet

Figure 1. The MILAN middleware in context.
dictability, even when the underlying set of machines is unreliable and changing dynamically.
The principal outcomes of the MILAN project are (1)
a core set of fundamental resource management techniques [21, 3, 23, 9] enabling construction of efficient, reliable, predictable virtual machines, and (2) the realization of
these techniques in three complete programming systems:
Calypso [3], Chime [27], and Charlotte [6]. Calypso extends C++ with parallel steps interleaved into a sequential
program. Each parallel step specifies the independent execution of multiple concurrent tasks or a family of such tasks.
Chime extends Calypso to provide nested parallel steps and
inter-thread communication primitives (as expressed in the
shared memory parallel language, Compositional C++ [8]).
Charlotte provides a Calypso-like programming system and
runtime environment for the Web. In addition to these systems, the MILAN project has also produced two general
tools: ResourceBroker [4] and KnittingFactory [5], which
support resource discovery and integration in distributed
and web-based environments, respectively. More recently,
as part of the Computing Communities project, we have
been examining how the experience gained from designing, implementing, and evaluating these systems can be extended to supporting general applications on metacomputing environments.
The rest of the paper is organized as follows. Section 2
overviews the fundamental techniques central to all of the
MILAN project’s programming systems. The design, implementation, and performance of the various programming
systems and general tools is described in detail in Section 3.
Finally, Section 4 presents the rationale and preliminary design of Computing Communities, a metacomputing framework for general computations.

load evenly for efficient execution. This knowledge can
not be assumed for distributed multiuser environments, and
hence, it is imperative that programs adapt to machine availability. That is, a program running on a metacomputer must
be able to integrate new machines into a running computation, mask and remove failed machines, and balance the
work load in such a way that slow machines do not dictate
the progress of the computation.
The traditional solution to overcome this type of dynamically changing environment has been to write selfscheduling parallel programs (also referred to as the master/slave [16], the manager/worker [17], or the bag-oftasks [7] programming model). In self-scheduled programs,
the computation is divided into a large number of small
computational units, or tasks. Participating machines pick
up and execute a task, one at a time, until all tasks are done,
enabling the computation to progress at a rate proportional
to available resources. However, self scheduling does not
solve all the problems associated with executing programs
on distributed multiuser environments. First, self scheduling does not address machine and network failures. Second,
a very slow machine can slow down the progress of faster
machines if it picks up a compute-intensive task. Finally,
self scheduling increases the number of tasks comprising a
computation and, thereby, increases the effects of the overhead associated with assigning tasks to machines. Depending on the network, this overhead may be large and, in many
cases, unpredictable.
The MILAN project extends the basic self-scheduling
scheme in various ways to adequately address the above
shortcomings. These extensions are embodied in five techniques: eager scheduling, two-phase idempotent execution
strategy (TIES), dynamic granularity management, preemptive scheduling, and predictable scheduling for tunable
computations. We describe the principal ideas behind each
of these techniques here, deferring a detailed discussion of
their implementation and impact on performance to the description of various programming systems in Section 3.

2 Key Techniques
To execute parallel programs on networks of commodity
machines, one frequently assumes a priori knowledge—at
program development—of the number, relative speeds, and
the reliability of the machines involved in the computation.
Using this information, the program can then distribute its
2

2.1 Eager Scheduling

sent back on the network. Finally, bunching allows the programmer to write fine-grained parallel programs that are automatically and transparently executed in a coarse-grained
manner.

Eager scheduling extends self scheduling to deal with
network and machine failures, as well as any disparity in
machine speeds. The key idea behind eager scheduling, initially proposed in [21], is that a single computation task can
be concurrently worked upon by multiple machines. Eager scheduling works in a manner similar to self scheduling at the beginning of a parallel step, but once the number of remaining tasks goes below the number of available machines, eager scheduling aggressively assigns and
re-assigns tasks until all tasks have been executed to completion. Concurrent assignment of tasks to multiple machines guarantees that slow machines, even very slow machines, do not slow down the computation. Furthermore, by
considering failure as a special case of a slow machine (an
infinitely slow machine), even if machines crash or become
less accessible, for example due to network delays, the entire computation will finish as long as at least one machine
is available for a sufficiently long period of time. Thus, eager scheduling masks machine failures without the need to
actually detect failures.

We have implemented factoring [19], an algorithm that
computes the bunch size based on the number of remaining
tasks and the number of currently available machines.

2.4 Preemptive Scheduling
Eager scheduling provides load balancing and fault isolation in a dynamic environment. However, our description
so far has considered only non-preemptive tasks which run
to completion once assigned to a worker. Non-preemptive
scheduling has the disadvantage of delivering sub-optimal
performance when there is a mismatch between the set of
tasks and the set of machines. Examples of situations include when the number of tasks is not divisible by the number of machines, when the tasks are of unequal lengths, and
when the number of tasks is not static (i.e., new tasks are
created and/or terminated on the fly). To address inefficiencies resulting from these situations, the MILAN project
complements eager scheduling with preemptive scheduling
techniques. Our results, discussed in Section 3, show that
despite preemption overheads, use of preemptive scheduling on distributed platforms can improve execution time of
parallel programs by reducing the number of tasks that need
to be repeatedly executed by eager scheduling [23].

2.2 Two-phase Idempotent Execution Strategy
(TIES)
Multiple executions of a program fragment (which is
possible under eager scheduling) can result in an incorrect
program state. TIES [21] ensures idempotent memory semantics in the presence of multiple executions. The computation of each parallel step is divided into two phases.
In the first phase, modifications to the shared data region,
that is the write-set of tasks, are computed but kept aside
in a buffer. The second phase begins when all tasks have
executed to completion. Then, a single write-set for each
completed task is applied to the shared data, thus atomically updating the memory. Note that each phase is idempotent, since its inputs and outputs are disjoint. Informally,
in the first phase the input is shared data and the output is
the buffer, and in the second phase the input is the buffer
and the output is shared memory.

We have developed a family of preemptive algorithms,
of which we present three here. The Optimal Algorithm
is targeted for situations where the number of tasks to be
executed is slightly larger than the number of machines
available. This algorithm precomputes a schedule that
minimizes the execution time and the number of context
switches needed. However it requires that the task execution time be known in advance and therefore is not always
practical. The Distributed, Fault-tolerant Round Robin Algorithm is suited for a set of n tasks scheduled on m machines, where n > m. Initially, the first m tasks are assigned to the m machines. Then, after a specified time
quantum, all the tasks are preempted and the next m tasks
are assigned. This continues in a circular fashion until all
tasks are completed. The Preemptive Task Bunching Algorithm is applicable over a wider range of circumstances. All
n tasks are bunched into m bunches and assigned to the
m machines. When a machine finishes its assigned bunch,
all the tasks on all the other machines are pre-empted and
all the remaining tasks are collected, re-bunched (into m
sets), and assigned again. This algorithm works well for
both large-grained and fine-grained tasks even when machine speeds and task lengths vary.

2.3 Dynamic Granularity Management
The interplay of eager scheduling and TIES addresses
fault masking and load balancing. Dynamic granularity
management (bunching for short) is used to amortize overheads and mask network latencies associated with the process of assigning tasks to machines. Bunching extends self
scheduling by assigning a set of tasks (a bunch) as “a single
assignment.” Bunching has three benefits. First, it reduces
the number of task assignments, and hence, the associated
overhead. Second, it overlaps computation with communication by allowing machines to execute the next task (of
a bunch) while the results of the previous task are being
3

2.5 Predictable Scheduling

Calypso [3] is a parallel programming system and a runtime system designed for adaptive parallel computing on
networks of machines. The work on Calypso has resulted
in several original contributions which are summarized below.
Calypso separates the programming model from the execution environment: programs are written for a reliable virtual shared-memory computer with unbounded number of
processors, i.e., a metacomputer, but execute on a network
of dynamically changing machines. This presents the programmer with the illusion of a reliable machine for program
development and verification. Furthermore, the separation
allows programs to be parallelized based on the inherent
properties of the problem they solve, rather than the execution environment.
Programs without any modifications can execute on a
single machine, a multiprocessor, or a network of unreliable
machines. The Calypso runtime system is able to adapt executing programs to use available resources—computations
can dynamically scale up or down as machines become
available, or unavailable. It uses TIES and allows parts of
a computation executing on remote machines to fail, and
possibly recover, at any point without affecting the correctness of the computation. Unlike other fault-tolerant systems, there is no significant additional overhead associated
with this feature.
Calypso automatically distributes the work-load depending on the dynamics of participating machines, using eager scheduling and bunching. The result is that fine-grain
computations are efficiently executed in coarse-grain fashion, and faster machines perform more of the computation
than slower machines. Not only is there no cost associated
with this feature, but it actually speeds up the computation,
because fast machines are never blocked while waiting for
slower machines to finish their work assignments—they bypass the slower machines. As a consequence, the use of
slow machines will never be detrimental to the performance
of a parallel program.

While the techniques described earlier enable the building of an efficient, fault-tolerant virtual machine on top of
an unreliable and dynamically changing set of machines,
they alone are unable to address the predictability requirements of applications such as image recognition, virtual reality, and media processing that are increasingly running on
metacomputers. One of the key challenges deals with providing sufficient resources to computations to enable them
to meet their time deadlines in the face of changing resource
availability.
Our technique [9] relies upon an explicit specification of
application tunability, which refers to an application’s ability to absorb and relinquish resources during its lifetime,
possibly trading off resource requirements versus quality
of its output. Tunability provides the freedom of choosing
amongst multiple execution paths, each with their own resource allocation profile. Given such a specification and
short-term knowledge about the availability of resources,
the MILAN resource manager chooses an appropriate execution path in the computation that would allow the computation to meet its predictability requirements. In general,
the resource manager will need to renegotiate both the level
of resource allocation and the choice of execution path in
response to changes in resource characteristics. Thus, application tunability increases its likelihood of achieving predictable behavior in a dynamic environment.

3. Programming Systems
We describe, in turn, three programming systems—
Calypso, Chime, and Charlotte—which provide the concrete context in which the techniques described earlier have
been implemented and evaluated. We then discuss the design and implementation of the ResourceBroker, a tool for
dynamic resource association. KnittingFactory provides the
same functionality as ResourceBroker, albeit for Web metacomputing environments, and is not being discussed because of space considerations.

3.1.1 Calypso Programs
A Calypso program basically consists of the standard
C++ programming language, augmented by four additional
keywords to express parallelism. Parallelism is obtained by
embedding parallel steps within sequential programs. Parallel steps consist of one or more task (referred to as jobs in
the Calypso context), which (logically) execute in parallel
and are generally responsible for computationally intensive
segments of the program. The sequential parts of programs
are referred to as sequential steps and they generally perform initialization, input/output, user interactions, etc.
Figure 2 illustrates the execution of a program with two
parallel steps and three sequential steps. It is important to
note that parallel programs are written for a virtual shared-

3.1 Calypso
Commercial realities dictate that parallel computations
typically will not be given a dedicated set of identical machines. Non-dedicated computing platforms suffer from
non-uniform processing speeds, unpredictable behavior,
and transient availability. These characteristics result from
external factors that exist in “real” networks of machines.
Unfortunately, load balancing, fault masking, and adaptive
execution of programs on a set of dynamically changing
machines are neglected by most programming systems. The
neglect of these issues has complicated the already difficult
job of developing parallel programs.
4

child process that executes as a worker.
The manager is responsible for the management of the
computation as well as the execution of sequential steps.
The current Calypso implementation only allows one manager, and therefore it does not tolerate the failure of this process. The computation of parallel jobs is left to the workers.
In general, the number of workers and the resources they
can devote to parallel computations can dynamically change
in a completely arbitrary manner, and the program adapts to
the available machines. In fact, the arbitrary slowdown of
workers due to other executing programs on the same machine, failures due to process and machine crashes, and network inaccessibility due to network partitions are tolerated.
Furthermore, workers can be added at any time to speed up
an already executing system and to increase fault tolerance.
Arbitrary slowdown of the manager is also tolerated; this
would, of course, slow down the overall execution though.

sequential step

routine

time

parallel
step

parbegin
routine[n+2]
{sequential code;}
routine[2]
{sequential code;}
parend

3.1.3 Manager Process
The manager is responsible executing the non-parallel
step of a computation as well as providing workers with
scheduling and memory services.

Figure 2. An execution of a program with two
parallel steps and three sequential steps; the
first parallel step consists of 9 jobs, the second parallel step consists of 6 jobs.

Scheduling Service: Jobs are assigned to workers based
on a self-scheduling policy. Moreover, the manager has the
option of assigning a job repeatedly until it is executed to
completion by at least one worker—this is eager scheduling,
and provides the following benefits:



memory parallel machine irrespective of the number of machines that participate in a given execution.
This programming model is sometimes referred to as
a block-structured parbegin/parend or fork/join model [13,
25]. Unlike other programming models where programs are
decomposed (into several files or functions) for parallel execution, this model together with shared memory semantics, allows loop-level parallelization. As a result, given a
working sequential program it is fairly straightforward to
parallelize individual independent loops in an incremental
fashion—if the semantics allows this.
Shared-memory semantics is only provided for shared
variables, i.e., variables that are tagged with the shared
keyword.
A parallel step starts with the keyword
parbegin and ends with the keyword parend. Within
a parallel step, multiple parallel jobs may be defined using
the keyword routine. Completion of a parallel step consists of completion of all its jobs in an indeterminate order.




As long as at least one worker does not fail continually,
all jobs will be completed, if necessary, by this one
worker.
jobs assigned to workers that later failed are automatically reassigned to other workers; thus crash and network failures are tolerated.
Because workers on fast machines can re-execute jobs
that were assigned to slow machines, they can bypass
a slow worker to avoid delaying the progress of the
program.

3.1.2 Execution Overview

In addition to eager scheduling, Calypso’s scheduling
service implements several other scheduling techniques for
improved performance. Bunching masks network latencies
associated with the process of assigning jobs to workers.It
is implemented by sending the worker a range of job Ids
in each assignment. The overhead associated with this implementation is one extra integer value per job assignment
message, which is negligible.

A typical execution of a Calypso program consists of a
central process, called the manager, and one or more additional processes, called workers. These processes can reside
on a single machine or they can be distributed on a network.
In particular, when a user starts a Calypso program, in reality, she is starting a manager. Managers immediately fork a

Memory Service: Since multiple executions of jobs caused
by eager scheduling may lead to an inconsistent memory
state, managers implements TIES as follows. Before each
parallel step, a manager creates a twin copy of the shared
pages and unprotects the shared region. The memory management service then waits until a worker either requests a
5

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

efficiency

page or reports the completion of a job. The manager uses
the twin copy of the shared pages to service worker page requests. The message that workers send to the manager to report the completion of a job also contains the modifications
that resulted from executing the job. Specifically, workers
logically bit-wise XORs the modified shared pages before
and after executing the job, and send the results (diffs) to
the manager. When a manager receives such a message, it
first checks whether the job has been completed by another
worker. If so, the diffs are discarded, otherwise, the diffs are
applied (by an XOR operation) to manager’s memory space.
Notice that the twin copies of the shared pages, which are
used to service worker page requests, are not modified. The
memory management of a parallel step halts once all the
jobs have run to completion, and the program execution
then continues with the next sequential step.

64

16

16

4

10
1

number of
machines

256

4

number of tasks

(a) PVM

3.1.4 Worker Process

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

efficiency

A worker repeatedly contacts the manager for jobs to
execute. The manager sends the worker an assignment (a
bunch of jobs) specified by the following parameters: the
address of the function, the number of instances of the job,
and a range of job Ids. After receiving a work assignment, a
worker first access-protects the shared pages, and then calls
the function that represents an assigned job. The worker
handles page-faults by fetching the appropriate page from
the manager, installing process’ address space, and unprotecting the page so that subsequent accesses to the same
page will proceed undisturbed. Once the execution of the
function (i.e. the job) completes, the worker identifies all
the modified shared pages and sends the diffs to the manager and starts executing the next job in the assignment.
Notice that bunching overlaps computation with communication by allowing a worker to execute the next job while
the diffs are on the network heading to the manager.
Additional optimizations have been implemented, including the following:

64

16

16

4

10
1

number of
machines

256

4

number of jobs

(b) Calypso

Figure 3. Parallel ray tracing with different
number of parallel tasks.

Caching: For each shared page, the manager keeps track of
the logical step-number in which the page was last modified. This vector is piggybacked on a job assignment
the first time a worker is assigned a job in a new parallel
step. Hence, the associated network overhead is negligible.
Workers use this vector on page-faults to locally determine
whether the cached copy of a page is still valid. Thus, pages
that have paged-in by workers are kept valid as long as possible without a need for an invalidation protocol. Modified
shared pages are re-fetched only when necessary. Furthermore, read-only shared pages are fetched by a worker at
most once and write-only shared pages are never fetched.
As a result, programmer does not declare the type of coherence or caching technique to use, rather, the system dynamically adapts. Invalidation requests are piggybacked on work

assignment messages and bear very little additional cost.
Prefetching: Prefetching refers to obtaining a portion of
the data before it is needed, in the hope that it will be required sometime in the future. Prefetching has been used in
a variety of systems with positive results. A Calypso worker
implements prefetching by monitoring its own data access
patterns and page-faults, and it tries to predict future data
access based on past history. The predictions are then used
to pre-request shared pages from the manager. Depending on the regularity of a program’s data access patterns,
prefetching has shown positive results.
6

3.1.5 Performance Experiments

the executable is run, using the manager-worker scheme of
Calypso.
The manager process consists of two threads, the application thread and the control thread. The application thread
executes the code programmed by the programmer. The
control thread executes, exclusively, the code provided by
the Chime library. Hence, the application thread runs the
program and the control thread runs the management routines, such as scheduling, memory service, stack management, and synchronization handling.
The worker process also consists of two threads, the application thread and the control thread. The application
threads in the worker and manager are identical. However,
the control thread in the worker is the client of the control
thread in the manager. It requests work from the manager,
retrieves data pages from the manager and flushes updated
memory to the manager at the end of the task execution.

The experiments were conducted on up to 17 identical 200 MHz PentiumPro machines running Linux version
2.0.34 operating system, and connected by a 100Mbps Ethernet through a non-switched hub. The network was isolated to eliminate outside effects.
A publicly available sequential ray tracing program [10]
was used as the starting point to implement parallel versions
in Calypso and PVM [16]. The sequential program, which
traced a
 image in in 53 s, is used for calculating
the parallel efficiencies.
The PVM implementation used explicit master/slave
programming style for load balancing, where as for Calypso, load balancing was provided transparently by the
run-time system. To demonstrate the effects of adaptivity, the PVM and Calypso programs were parallelized using
different number of tasks and executed from 1 to 16 machines. The performance results are illustrated in Figure 3.
As the results indicate, the PVM program is very sensitive
to the number and the computation requirement of the parallel tasks, and at most, a hand-tuned PVM program outperforms a Calypso program by
. Notice that independent
of the number of machines used, the interplay of bunching,
eager scheduling, and TIES allows the Calypso program to
achieves its peak performance using 512 tasks—fine grain
tasks: as the result of bunching, fine-grain tasks, in effect,
execute in coarse-grain fashion; the combination of eager
scheduling and TIES compensates any over-bunching that
may occur.

512 512

3.2.2 Chime and CC++

Block structured scoping of variables and non-isolated
distributed parallel execution.

As mentioned earlier, Chime provides a programming
interface that is based on the Compositional C++ or
CC++ [8] language definition. CC++ provides language
constructs for shared memory, nested parallelism and synchronization. All threads of the parallel computation share
all global variables. Variables declared local to a function
are private to the thread running the function, but if this
thread creates more threads inside the function, then all the
children share the local variables.
CC++ uses the par and parfor statements to express parallelism. Par and parfor statements can be nested. CC++
uses single assignment variables for synchronization. A single assignment variable is assigned a value by any thread
called the writing thread. Any other thread, called the reading thread can read the written value. The constraint is that
the writing thread has to assign before the reading thread
reads, else the reading thread is blocked until the writing
thread assigns the variable.
These language constructs provide significant challenges
to a distributed (DSM-based) implementation that is also
fault tolerant. We achieved the implementation by using a pre-processor to detect the shared variables and parallel constructs, providing stack-sharing support—called
distributed cactus stacks—to implement parent-child variable sharing and innovative scheduling techniques, coupled
with appropriate memory flushing to provide synchronization [28].

Support for nested parallelism.

3.2.3 Preprocessing CC++

4%

3.2 Chime
Chime is a parallel processing system that retains the
salient features of Calypso, but supports a far richer set of
programming features. The internals of Chime are significantly different from Calypso, and it runs on the Windows
NT operating system [27]. Chime is the first system that
provides a true general shared memory multiprocessor environment on a network of machines. It achieves this by implementing the CC++ [8] language (shared memory part) on
a distributed system. Thus in addition to Calypso features
of fault-tolerance and load balancing Chime provides:






True multi-processor shared-memory semantics on a
network of machines.

Inter-task synchronization.

Consider the following parallel statement:

3.2.1 Chime Architecture

parfor ( int i=0; i<100; i++) f
a[i] = 0;
g;

A program written in CC+ is preprocessed to convert it to
C++ and compiled and linked with the Chime library. Then
7

parent
nested threads

siblings

{
top level threads

continuation

main process

Figure 4. A DAG for a nested parallel step.
Figure 5. Graphical representation of a cactus
stack.

This creates 100 tasks, each task assigning one element
of the array a. The preprocessor converts the above statement to something along the following lines:
1.
2.

3.
4.
5.
6.
7.

for (int i=0; i<100;i++) f
add task entry and &i
in the scheduling table;
g
SaveContext of this thread;
if worker f
a[i] = 0;
terminate task;
g
else f
suspend this thread and
request manager to
schedule threads till
all tasks completed;
g

terminates. Upon termination, the worker control thread regains control, flushes the updated memory to the manager
and asks the manger for a new assignment.
3.2.4 Scheduling
The controlling thread at the manager is also responsible for task assignment, or scheduling. The manager uses
a scheduling algorithm that takes care of task allocation to
the workers as well as scheduling of nested parallel tasks in
correct order. Nested parallel tasks in an application form a
DAG as shown in Figure 4.
Each nested parallel step consists of several sibling parallel tasks. It also has a parent task and a continuation that
must be executed, once the nested parallel step has been
completed. A continuation is an object that fully describes
a future computation. To complicate the scenario, a continuation may itself have nested parallel step(s).
The manager maintains an execution dependency graph
to capture the dependencies between the parallel tasks and
schedules them and their corresponding continuations in
correct order. Eager scheduling is used to allocate tasks to
the workers.

The above code may execute in the manager (top level
parallelism) or the worker (nested parallelism). Assume the
above code executes in the manager. Then the application
thread of the manager executes the code. Lines 1 and 2 create 100 entries in the scheduling table, one per parallel task.
Then line 3 saves the context of the parent task, including
the parent stack. Then the parent moves to line 7 and this
causes the application thread to transfer control to the control thread.
The control thread now waits for task assignment requests from the control threads of workers. When a worker
requests a task, the manager control thread sends the stored
context and the index value of i for a particular task to the
worker.
The control thread in the worker installs the received
context and the stack on the application thread in the worker
and resumes the application thread. This thread now starts
executing at line 4. Note that now the worker is executing at line 4, and hence does one iteration of the loop and

3.2.5 Cactus Stacks
The cactus stacks are used to handle sharing of local variables (see Figure 5). For top level nesting, the manager process is suspended at a point in execution where its stack
and context should be inherited by all the children threads.
When a worker starts, it is sent the contents of the manager’s stack along with the context. The controlling thread
of the worker process then installs this context as well as the
stack, and starts the application thread.
8

However, if a worker executes a nested parallel step, the
same code as the above case is used, but the runtime system
behaves slightly differently. The worker, after generating
the nested parallel jobs, invokes a routine that adds the jobs
and the continuation of the parent job to the manager’s job
table, remotely. The worker suspends and the controlling
thread in the worker, sends the worker’s complete context,
including the newly grown stack, to the manager.
The stack for a nested parallel task, therefore, is constructed by writing the stack segments of its ancestors onto
the stack of a worker’s application thread. Upon completion, the local portion of the stack for a nested parallel task
is unwound leaving only those portions that represent its
ancestors. This portion of the stack is then XOR’ed with its
unmodified shadow and the result is returned to the manager.

700

350
time (s)

300

40

100
0
2

3

4

time (s)

time/task (ms)

5

378 380

time (s)

350
300
250

245
214

246
215

212 220

207 215 213

210

200
150

Very Coarse Coarse Grain Medium Grain Fine Grain
Grain (5)
(10)
(21)
(1500)
Optimal

Eager Scheduling

Round Robin

Figure 8. Performance
Scheduling.

0
1

4

400

20

50

3

Figure 7, the synchronization overhead is about 86 ms per
occurrence, showing that synchronization does not add too
much overhead over basic thread creation.

0

60

0
2

Figure 7. Performance of Synchronization.

100

200

50

machines

100
50

80

100

1

120

250
150

150
300

0

time/task (ms)

time/task

200

100

140

time

250

400

200

Many performance tests have been done on Chime [27],
evaluating its capabilities in speedups, load balancing, and
fault tolerance. The results are competitive to other systems, including Calypso. We present three micro-tests that
show the performance of the nested parallelism (including
cactus stacks), the Chime synchronization mechanisms, and
preemptive scheduling mechanisms.

400

time/task

500

3.2.6 Performance Experiments

450

300

time

600

of

Task Bunching

Preemptive

5

machines

To measure the impact of preemptive scheduling algorithms for programs with different grain sizes, we decomposed a matrix-multiply algorithm on two

matrices into 5 tasks (very coarse grain), 10 tasks (coarse
grain), 21 tasks (medium grain), and 1500 tasks (fine grain).
All experiments used three identical machines. Given the
equal task lengths, our experiments were biased against preemptive schedulers. As shown in Figure 8, on the overall,
preemptive scheduling has definite advantages over nonpreemptive scheduling, not withstanding its additional overheads. Specifically, for coarse-grained and very coarsegrained tasks, round robin scheduling effectively complements eager scheduling in reducing overall execution time.
For most other task sizes, the preemptive task bunching algorithm yields the best performance; for fine-grained tasks

Figure 6. Performance of Nested Parallelism.

1500 1500

For the nested parallelism overhead, we run a program
that recursively creates two child threads until 1024 leaf
threads have been created. Each leaf thread assigns one integer in a shared array and then terminates. Figure 6 shows
that the total runtime of the program asymptotically saturates as number of machines are increased, due to the bottleneck in stack and thread management at the manager. The
time taken to handle all overhead for a thread (including
cactus stacks) is 74 ms.
To measure the synchronization overhead, we use 512
single assignment variables, assign them from 512 threads
and read them from 512 other threads. As can be seen in
9

it minimizes the number of preemptions that are necessary.

3.3.1 Charlotte Programs
A Charlotte program is written by inserting any number
of parallel steps onto a sequential Java program. A parallel
step is composed of one or more routines, which are (sequential) threads of control capable of executing on remote
machines.
A parallel step starts and ends with the invocation of
parBegin() and parEnd() methods, respectively. A
routine is written by subclassing the Droutine class and
overriding its drum() method. Routines are specified
by invoking the addRoutine() method with two arguments: a routine object and an integer, n, representing the
number of routine instances to execute. To execute a routine, the Charlotte runtime system invokes the drun()
method of routine objects, and passes as arguments the
number of routine instances created (i.e. n) and an identifier
in the range ; : : : ; n representing the current instance.
A program’s data is logically partitioned into private and
shared segments. Private data is local to a routine and is
not visible to other routines; shared data, which consists
of shared class-type objects, is distributed and is visible
to all routines. For every basic data-type defined in Java,
Charlotte implements a corresponding distributed shared
class-type. For example, Java provides int and float
data-types, whereas Charlotte provides Dint and Dfloat
classes. The class-types are implemented as standard Java
classes, and are read and written by invoking get() and
set() method calls, respectively. The runtime system
maintains the coherence of shared data.

3.3 Charlotte
Many of the assumptions made for (local-area) networks
of machines are not valid for the Web. For example, the machines on the Web do not have a common shared file system,
no single individual has access-rights (user-account) on every machine, and the machines are not homogeneous. Another important distinction is the concept of users. A user
who wants to execute a program on a network of machines,
typically performs the steps: logs onto a machine under her
control (i.e. the local machine), from the local machine logs
onto other machines on the network (i.e. remote machines)
and initializes the execution environment, and then starts
the program. In the case of the Web, no user can possibly
hope to have the ability to log onto remote machines. Thus,
another set of users who control remote machines, or software agents acting on their behalf, must voluntarily allow
others access. To distinguish the two types of users, this
section uses the term end-users to refer to individuals who
start the execution (on their local machines) and await results, and volunteers to refer to individuals who voluntarily
run parts of end-users’ programs on their machines (remote
to end-users). Similarly, volunteer machines is used to refer
to machines owned by volunteers.

(0

Simplicity and security are important objectives for volunteers. Unless the process of volunteering a machine is
simple—for example as simple as a single mouse-click—
and the process of withdrawing a machine is simple, it is
likely that many would-be volunteer machines will be left
idle. Furthermore, volunteers need assurance that the integrity of their machine and file system will not be compromised by allowing “strangers” to execute computations on
their machines. Without such an assurance, it is natural to
assume security concerns will outweigh the charitable willingness volunteering.

]

3.3.2 Implementation
Worker Process: A Charlotte worker process is implemented by the Cdaemon class which can run either as a
Java application or as a Java applet. At instantiation, a
Cdaemon object establishes a TCP/IP connection to the
manager and maintains this connection throughout the computation.
Two implementation features are worth noting. First,
since Cdaemon is implemented as an applet (as well as an
application), the code does not need to be present on volunteer machines before the computation starts. By simply embedding the Cdaemon applet in an HTML page, browsers
can download and execute the worker code. Second, the
Cdaemon class, unlike its counterpart the Calypso worker,
is independent of the Charlotte program it executes. Thus,
not only are Charlotte workers able to execute parallel routines of any Charlotte program, but only the necessary code
segments are transfered to volunteer machines.

Charlotte [6] is the first parallel programming system to
provide one-click computing. The idea behind one click
computing is to allow volunteers from anywhere on the
Web, and without any administrative effort, to participate in
ongoing computations by simply directing a standard Javacapable browser to a Web site. A key ingredient in one-click
computing is its lack of requirements: user-accounts are not
required, the availability of the program on a volunteer’s
machine is not assumed, and system-administration is not
required. Charlotte builds on the capability of the growing
number of Web browsers to seamlessly load Java applets
from remote sites, and the applet security model, which enables Web browsers to execute untrusted applets in a controlled environment, to provide a comprehensive programming system.

Manager Process: A manager process begins with the
main() method of a program and executes the non-parallel
steps in a sequential fashion. It also manages the progress of
parallel steps by providing scheduling and memory services
10

JPVM

RMI

Charlotte

JPVM

RMI

Charlotte

JPVM

RMI

Charlotte

JPVM

RMI

16

0.8

14
12

0.8

10
8

0.4

6

effeciency

0.6

10
9
8
7
6
5
4
3
2
1
0

speedup

1

1

speedup

effeciency

Charlotte

0.6
0.4

4

0.2

0.2

2
0

0
1

3

5

7
9
11
volunteers

13

0

15

2.5

5

7.5

10

effective volunteers

Figure 9. Performance comparison of Charlotte, RMI and JPVM programs.

Figure 10. Load balancing of Charlotte, RMI
and JPVM programs.

to workers. They are based on eager scheduling, bunching,
and TIES.

The first series of experiments compares the performance of the three parallel implementations of ray tracer,
see Figure 9. In the case of Charlotte, the same program
with the same runtime arguments was used for every run—
the program tuned itself to the execution environment. For
RMI and JPVM programs, on the other hand, executions
with different grain sizes were timed and the best results are
reported—the programs were hand-tuned for the execution
environment. The results indicate that when using 16 volunteers, the Charlotte implementation runs within 5% and
10% of hand-tuned JPVM and RMI implementations, respectively. It is encouraging to see that the performance of
Charlotte is competitive with other systems that do not provide load balancing and fault masking.
The final set of experiments illustrates the efficiency
of the programs when executing on machines of varying
speeds—a common scenario when executing programs on
the Web. Exactly the same programs with the same granularity sizes as the previous experiment were run on n,
 n  , groups of volunteers, where each group consisted of four machines: one normal machine, one machine
slowed down by 25%, one machine slowed down by 50%,
and one machine slowed down by 75%. Each group has
a computing potential of 2.5 volunteer machines. The results are depicted in Figure 10. As the results indicate,
the Charlotte program is the only one able to maintain
its efficiency—the efficiency of the Charlotte program degraded by approximately 5%. In contrast, the efficiency of
RMI and PVM programs dropped by as much as 60% and
45%, respectively.

Distributed Shared Class Types: Charlotte’s distributed
shared memory is implemented in pure Java at the data-type
level; that is, through Java classes as stated above. For each
primitive Java type like int and float, there is a corresponding Charlotte class-type Dint and Dfloat. The
member variables of these classes are a value field of the
corresponding primitive type, and a state flag that can
be not valid, readable, or dirty. It is important to
note that different parts of the shared data can be updated
by different worker processes without false sharing, as long
as the CRCW-Common condition is met. (That is, several
workers in a step can update the same data element, as long
as all of them write the same value.) The shared memory
is always logically coherent, independently of the order in
which routines are executed.
3.3.3 Performance Experiments

1

The experiments were conducted in the same execution environment as in Section 3.1.5. Programs were compiled (with compiler optimization turned on) and executed
in the Java Virtual Machine (JVM) packaged with Linux
JDK 1.1.5 v7. TYA version 0.07 [22] provided just-in-time
compilation.
A publicly available sequential ray tracing program [24]
was used as the starting point to implement parallel versions in Charlotte, Java RMI [14], and JPVM [15]. Java
RMI is an integral part of Java 1.1 standard and, therefore,
it is a natural choice for comparison. JPVM is a Java implementation of PVM, one of the most widely used parallel
programming systems. For the experiments, a

image was traced. The sequential program took 154 s to
complete, and this number is used as the base in calculating
the speedups.

500 500

4

3.4 ResourceBroker
ResourceBroker [4] is a resource management system
for monitoring computing resources in a distributed mul11

resource
management
layer

resource manager

agent

subagent

subagent

subagent

subagent

subagent

agent

agent layer

applications
layer

calypso computation

pvm computation

Figure 11. The components of ResourceBroker that comprise of the resource management and the
agent layers.

tiuser environments and for dynamically assigning them to
concurrently executing computations. Although applicable to a wide variety of computations, including sequential
ones, it especially benefits adaptive parallel computations.
Adaptive parallel computations can effectively use networked machines because they dynamically expand as machines become available and dynamically acquire machines
as needed. While most parallel programming systems provide the means to develop adaptive programs, they do not
provide any functional interface to external resource management systems. Thus, no existing resource management
system has the capability to manage resources on commodity system software, arbitrating the demands of multiple
adaptive computations written using diverse programming
environments. Indeed, existing resource management systems are tightly integrated with the programming system
they support and their inability to support more than one
programming system severely limits their applicability.
ResourceBroker is built to validate a set of novel mechanisms that facilitate dynamic allocation of resources to
adaptive parallel computations. The mechanisms utilize
low-level features common to many programming systems,
and unique in their ability to transparently manage adaptive
parallel programs that were not developed to have their resources managed by external systems. The ResourceBroker
prototype is the first system that can support adaptive programs written in more than one programming system, and
has been tested using a mix of programs written in PVM,
MPI, Calypso, and PLinda.

purpose computing. However, the rise of commodity operating systems and the need for application binary compatibility have made such approaches less attractive, necessitating instead that general computations also be supported
on metacomputing environments. To enable the latter, we
have designed and will implement the Computing Community (CC) framework.
A Computing Community (CC) is a collection of machines (with dynamic membership) that form a single, dynamically changing, virtual multiprocessor system. It has
global resource management, dynamic (automatic) reconfigurability, and the ability to run binaries of all applications designed for a base operating system. The physical
network disappears from the view of the computations that
run on the CC.
The CC brings flexibility of well-designed, distributed
computing environments to the world of non-distributed
applications-including legacy applications-without the need
for distributed programming, new APIs, RPCs, objectbrokerage, or similar mechanisms.

4.1 Realizing a CC
We are in the process of building a CC on top of the Windows NT operating system, with the initial software architecture shown in Figure 12. The CC comprises three synergistic components: (1) Virtual Operating System (2) Global
Resource Manager (3) Application Adaptation System.
The Virtual Operating System (VOS) is a layer of software that non-intrusively operates between the applications
and the standard operating system. The VOS presents the
standard Windows NT API to the application, but can execute the same API calls differently, thereby extending the
OS’s power. The VOS essentially decouples the virtual entities required for executing a computation from their mappings to physical resources in the CC.
The Global Resource Manager manages all CC resources, dynamically discovering the availability of new re-

4 Computing Communities: Metacomputing
for General Computations
So far, we have addressed metacomputing for parallel
computations. Operating systems such as Amoeba [29],
Plan-9 [26], Clouds [12] and to an extent Mach [1] had targeted the use of distributed systems for seamless general
12

global handle allocator

local to global handle
traslator

VOS kernel
file proxy

P1

P2

network proxy

P3

operating system

GDI/Windowing proxy

P4

operating system

tunability
policy

interface to
external
resources

use
user
applicatio
r
application
n
system call API interception layer
trust
management

directory service
global scheduler
QoS arbitrator
security tunability
adaptation service

use
user
applicatio
r
application
n

P5

P6

operating system

Figure 12. Software architecture for Computing Communities.
Consider a user U who starts an application A (and its
GUI) on some machine M1 . Soon, U abruptly moves to
another machine M2 . Now U can instruct the CC to connect
the virtual screen, virtual keyboard, and the virtual mouse of
A to the physical resources of M2 . The CC complies and U
continues working on M2 , as if A executed there. Later the
CC might decide it preferable to run the application on M2 .
The scheduler then transparently moves A to M2 preserving
process state and open files and network connections.
The above simple scenario shows a particular aspect of
the power of virtualization. In general:

sources, integrating them into the CC, and making them
available for use by CC computations. It handles resource
requests from other components of the system and satisfies
them as per scheduling requirements.
The Application Adaptation System enables the computations to take full advantage of CC resources and provides dynamic reconfiguration capabilities. Adaptation
techniques allow computations to become aware of and
gracefully adapt themselves to changes in CC resource characteristics.
Figure 13 shows a conceptual view of a CC. It takes a
set of operating systems, and a set of resources, and via a
layer of middleware converts it into an integrated community. CCs can expand and contract dynamically, and the
computations are completely mobile within CCs. In short,
using the CC framework, the computation transparently acquires the benefit of operating in a distributed environment.






4.2 The Virtualization Concept
Under a standard OS, a process runs in a logical address
space, is bound to a machine, and interacts with the OS local to this machine. In fact, the processes (and their threads)
are virtualizations of the real CPUs. However, such virtualization is low-level and limited in scope.
In the CC, virtualization is defined at a much higher
level, and all physical resources (CPU, memory, disks, and
networks) as well as the OSs on to all the machines are aggregated into a single, unified (distributed) virtual resource
space.
A process in the CC is enveloped in a virtual shell (Figure 14), which makes the process feel that it is running on
a standard OS. However, the shell creates a virtual world
made of the aggregate of the physical worlds in the CC.

The users can move their virtual ”home machines” at
will, even for applications that are currently executing.
This is the ultimate mobile computing scenario.
A critical service running on machine M1 can be
moved to machine M2 if M1 has to be relinquished.
Schedulers can control the complete set of resources.
The provision of multiple physical resources for a single virtual resource delivers important new capabilities ranging from duplicating application displays on
multiple screens to replicating processes for fault tolerance.

The CC functionality relies upon three key mechanisms:
API interception, proxies, and translations between physical and logical handles. API interception allows the API
calls from an application to the operating system to be intercepted and the behavior of the API call to be modified.
After intercepting a call, the virtual operating system (VOS)
does one of the following operations. (1) Passes the call on
to the local Windows NT operating system. (2) Passes the
call to a remote Windows NT operating system. (3) Executes the call inside the VOS. (4) Executes some VOS code
13

P1

P2

Operating
System
R1

R2

P3

P4

Operating
System
R3

R4

P1

Operating
System

R5

R6

P2

Operating
System

R7

R1

Processes (Pi), operating systems, and
resources (Rj) in a conventional system

R2

P3

Operating
System
R3

R4

R5

P4

Operating
System
R6

R7

CC augments the original system with thin per-process, perresource, and global system-wide entities

Figure 13. A conceptual view of Computing Communities.

Acknowledgements
CC

This paper describes work of not only the authors but
also other participants in the MILAN project, particularly
Fangzhe Chang, Ayal Itzkovitz, Mehmet Karaul, Holger
Karl, Donald McLaughlin, Shantanu Sardesi, Peter Wyckoff, and Yuanyuan Zhao. This research was sponsored
by the Defense Advanced Research Projects Agency and
Rome Laboratory, Air Force Materiel Command, USAF,
under agreement number F30602-96-1-0320; by the National Science Foundation under grants number CCR-9411590 and CCR-95-05519; by Deutsche Forschunggemeinschaft; by Intel; and by Microsoft. The U.S. Government is
authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation
thereon. The views and conclusions contained herein are
those of the authors and should not be interpreted as necessarily representing the official policies or endorsements,
either expressed or implied, of the Defense Advanced Research Projects Agency, Rome Laboratory, or the U.S. Government.

)

_h

al

open (virt_h)
P1

en

e
(r

op

Network

Figure 14. Virtualization of a process.
and then passes the call to a local or remote Windows NT
system.
In order to reallocate processes to machines, a general
form of process migration is necessary. To move a process
from one location to another, just moving the state is not
enough, all connections and handles have to be moved. This
can be achieved by having proxies that emulate the connections of the process after the process has moved. For example, if a process P moving from M1 to M2 has an open
networking connection to M3 , a proxy is created on M1 ,
which keeps the original connection to M3 open, and then
forwards messages between P and M3 , after P has moved.
Equally essential to successful virtualization of resources for migrating processes is the use of virtual handles. For example when a process opens a file on top of a
VOS, the VOS intercepts this call and stores the returned
physical handle but returns to the process a handle, which
we refer to as virtual. The virtual handle can be used by
the process, regardless of migrations, to access that file, due
to the transparent translation service provided by the VOS.
The virtual handles are used to virtualize I/O connections,
sub-processes, threads, files, network sockets, etc.

References
[1] M. Accetta, R. Baron, D. Golub, R. Rashid, A. Tevanian,
and M. Young. Mach: A new kernel foundation for UNIX
development. Proceedings Summer USENIX, July 1986.
[2] Y. Aumann, Z. M. Kedem, K. Palem, and M. Rabin. Highly
efficient asynchronous execution of large-grained parallel
programs. In Proceedings of the Annual Symposium on the
Foundations of Computer Science (FOCS), 1993.
[3] A. Baratloo, P. Dasgupta, and Z. M. Kedem. Calypso: A
novel software system for fault-tolerant parallel processing
on distributed platforms. In Proceedings of International
Symposium on High-Performance Distributed Computing
(HPDC), 1995.
[4] A. Baratloo, A. Itzkovitz, Z. M. Kedem, and Y. Zhao. Mechanism for just-in-time allocation of resources to adaptive
parallel programs. In Proceedings of International Parallel
Processing Symposium (IPPS/SPDP), 1999.

14

[23] D. McLaughlin. Scheduling Fault-tolerant, Parallel Computations in a Distributed Environment. PhD thesis, Arizona
State University, December 1997.
[24] L. McMillan.
An instructional ray-tracing renderer
written for UNC COMP 136 Fall ’96.
Available at
http://graphics.lcs.mit.edu/˜capps/iap/
class3/RayTracing/RayTrace.java, 1996.
[25] OpenMP Architecture Review Board. OpenMP: A Proposed
Industry Standard API for Shared Memory Programming,
1997.
[26] R. Pike, D. Presotto, S. Dorward, B. Flandrena, K. Thompson, H. Trickey, and P. Winterbottom. Plan 9 from Bell labs.
In USENIX, editor, Computing Systems, Summer, 1995.,
volume 8, pages 221–254, Berkeley, CA, USA, Summer
1995. USENIX.
[27] S. Sardesai. Chime: A Versatile Distributed Parallel Processing Environment. PhD thesis, Arizona State University,
July 1997.
[28] S. Sardesai, D. McLaughlin, and P. Dasgupta. Distributed
Cactus Stacks: Runtime stack-sharing support for distributed parallel programs. In Proceedings of the International Conference on Parallel and Distributed Processing
Techniques and Applications, 1998.
[29] A. S. Tanenbaum and S. Mullender. An overview of the
Amoeba distributed operating system. Operating Systems
Review, 15(3):51–64, July 1981.

[5] A. Baratloo, M. Karaul, H. Karl, and Z. M. Kedem. KnittingFactory: An infrastructure for distributed web applications. Concurrency: Practice and Experience, 10(11–
13):1029–1041, 1998.
[6] A. Baratloo, M. Karaul, Z. M. Kedem, and P. Wyckoff.
Charlotte: Metacomputing on the Web. In Proceedings of
the International Conference on Parallel and Distributed
Computing Systems, 1996.
[7] N. Carriero and D. Gelernter. Linda in context. Communications of the ACM, 32(4):444–458, Apr. 1989.
[8] M. Chandy and C. Kesselman. Compositional C++: Compositional parallel programming. In Proceedings of the International Workshop on Languages and Compilers for Parallel Computing, 1992.
[9] F. Chang, V. Karamcheti, and Z. M. Kedem. Exploiting
application tunability for efficient, predictable parallel resource management. In Proceedings of International Parallel Processing Symposium (IPPS/SPDP), 1999.
[10] R. L. Cook, T. Porter, and L. Carpenter. Distributed ray
tracing. Computer Graphics, 18(3):137–145, July 1984.
[11] P. Dasgupta, Z. M. Kedem, and M. Rabin. Parallel Processing on Networks of Workstations: A Fault-Tolerant,
High Performance Approach. In Proceedings of 15th International Conference on Distributed Computing Systems
(ICDCS), 1995.
[12] P. Dasgupta, R. LeBlanc Jr., M. Ahamad, and U. Ramachandran. The Clouds distributed operating system. IEEE Computer, 1991.
[13] E. W. Dijkstra. Solution of a problem in concurrent programming control. Communications of the ACM, 8(9):569,
Sept. 1965.
[14] T. Downing. Java RMI: Remote Method Invocation. IDG
Books Worldwide, 1998.
[15] A. Ferrari. JPVM—network parallel computing in Java. In
Proceedings of the Workshop on Java for High-Performance
Network Computing, 1998.
[16] A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek,
and V. Sunderam. PVM: Parallel virtual machine. MIT
Press, 1994.
[17] W. Gropp, E. Lust, and A. Skjellum. Using MPI: Portable
parallel programming with the message-passing interface.
MIT Press, 1994.
[18] S. Huang and Z. M. Kedem. Supporting a flexible parallel
programming model on a network of workstations. In Proceedings of International Conference on Distributed Computing Systems (ICDCS), 1996.
[19] S. F. Hummel, E. Edith Schonberg, and L. E. Flynn. Factoring: A method for scheduling parallel loops. Communications of the ACM, 35(8):90–101, Aug. 1992.
[20] Z. M. Kedem and K. Palem. Transformations for the automatic derivation of resilient parallel programs. In Proceedings of the IEEE Workshop on Fault-Tolerant Parallel and
Distributed Systems, 1992.
[21] Z. M. Kedem, K. Palem, and P. Spirakis. Efficient robust
parallel computations. In Proceedings of the ACM Symposium on the Theory of Computing (STOC), 1990.
[22] A. Kleine. Tya archive. Availabe at http://www.
dragon1.net/software/tya.

Biography
Arash Baratloo is a Ph.D. candidate at the Courant Institute of Mathematical Sciences, New York University. He
has received M.Eng. and B.S. degrees in computer science
from Cornell University. His current research interests include load balancing and fault tolerance in distributed and
parallel systems.
Partha Dasgupta is an Associate Professor at Arizona
State University. He received his Ph.D. in Computer Science in 1984 from SUNY Stony Brook. His research interests encompass distributed computing, middleware, operating systems and parallel computing infrastructure.
Vijay Karamcheti is an Assistant Professor of Computer Science in the Courant Institute of Mathematical Sciences at New York University. Vijay received his Ph.D. in
Electrical and Computer Engineering from the University of
Illinois at Urbana-Champaign in 1998. His current research
focuses on computer architecture, and system software issues in parallel and distributed computing.
Zvi Kedem received his D.Sc. in Mathematics from the
Technion - Israel Institute of Technology in 1974. He has
published in: functional analysis, algebraic computational
complexity, VLSI complexity, analysis and design of sequential and parallel algorithms, computer graphics, concurrency control, databases and knowledge-based systems,
data mining, compilation for special-purpose processor arrays, distributed systems, and metacomputing. He is a fellow of the ACM.
15

Countering Rogues in Wireless Networks 
Austin Godber and Partha Dasgupta
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ
godber, partha@asu.edu

Abstract
Wired networks are prone to the same attacks as wireless
ones, including snifﬁng, spooﬁng and Man-in-the-middle
attacks (MITM). In this paper we show how wireless networks are particularly vulnerable to a simple MITM that
can make even rudimentary web surﬁng dangerous. We describe how we performed the attack and its ramiﬁcations.
We argue why it is essential to have a VPN tunnel from the
client to some trusted host (not access point) in order to
avoid being compromised.

1 Introduction
With the explosive popularity of wireless networks,
based on the 802.11b and its descendant technologies, have
come great beneﬁts and great risks. The beneﬁts include reduced cost of deploying additional networked devices due
to the inexpensive network devices and no longer requiring expensive wiring. These networks also have the ability
to extend institution networks over many miles at a reasonable price. These beneﬁts have made it enormously popular
within practically every computing community, be it corporate, educational, or private home users.
The installed base of 802.11b devices that ﬂourished
from the availability of inexpensive consumer grade equipment ensures that the risks of 802.11b will continue to pose
a threat for a very long time. So, despite the development of
new networking technologies that can perhaps solve these
problems, other solutions that work with the existing technologies, should be explored.
At ﬁrst glance, a wireless network seems to have tremendous and obvious risks. The risks to WiFi clients include
eavesdropping with the intent of learning what the client is
doing or with the intent of intercepting authentication in-

formation. In addition to eavesdropping, wireless networks
are prone to jamming, spooﬁng, rogue access points, and
possible Man-in-the-middle attacks.
However, at a second look, it seems that the same problems are also present in wired networks. For example, a
rogue access point might be seen as a threat to the network
administrator, but not a threat to the clients on the network.
Hence, wireless security problems can be solved by all the
known solutions to the security problems in wired networks.
The above claim, however, is not true. We have discovered a particular Man-in-the-middle attack that is invitingly
easy to implement on wireless network, and is not so easy
to do on wired network. In addition, we show why eavesdropping is much more of a threat in wireless networks. The
solution to these problems is the use of encryption. However, in this paper we show why encryption between a wireless client and a base station is not effective (even if proper,
strong encryption is used).
Since this new class of threats targets the client, it falls
outside the common logic for combating Rogue APs. The
common concern for Rogue APs is that they provide unauthorized access to internal corporate networks; and if an AP
is not connected to the internal network, it is not a threat.

1.1 Privacy in wireless and wired networks
In this section, we ﬁrst discuss eavesdropping. In a wired
network eavesdropping is possible, but in most cases it is
not practical. In most corporate networks, clients are connected to switches and hence the trafﬁc between the client
and the network is not readily visible to other clients. The
switches feed trafﬁc into administered routers, which eventually lead to border routers into some backbone network.
Here, to snoop, the routers have to be reprogrammed (difﬁcult) and the amount of trafﬁc ﬂowing through them makes
the overhead of snifﬁng and ﬁltering prohibitive.

 This work is partially supported by DARPA/Spawar, AFOSR and NSF.

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

For home users or small businesses, Internet service is
provided by dial-up, cable-modem, or DSL. All of these are
not prone to snifﬁng by end users. Again, the trafﬁc terminates within the ISP’s administered networks and most
ISPs have other, more signiﬁcant tasks to perform than to
sniff data trafﬁc.
Wireless networks allow clients to sniff other people’s
packets. Outsiders can sniff trafﬁc on corporate networks.
It seems an encrypted channel between the client and access
point solves the problem. Turns out, as described later in the
paper, it does not. We need an encrypted channel between
a client and an authenticated trusted server on a wired network to achieve the same level of privacy that unencrypted
wired networks provide.

1.2 Man-in-the-middle
The Man-in-the-middle (MITM) attack is possible in
both wired and wireless networks. In a wired network,
one either needs to spoof DNS requests or ARP requests
or compromise a valid gateway machine to obtain access
to the clients trafﬁc. In a wireless network, since there is
no authentication of the network or the client is haphazardly using an untrusted hotspot, the MITM is relatively
simple. The attacker connects (using wireless) to a wireless network. He then provides service to other clients with
another access point that has the same SSID as the host network. Clients associate with the attackers “rogue” access
point and trafﬁc is routed though the attackers router. The
attacker can not only sniff, but can actually change the trafﬁc, insert viruses into downloaded ﬁles, change web pages,
and use known vulnerabilities in browser scripting to attack
the client machines when they visit well known web pages.
This attack is a particularly nefarious and easy to perform,
even on sophisticated users. In addition, the encryption between the access point (rogue) and client does not protect
the client.
Most web access is not encrypted as there seems to be
no private information worth protecting (except the information about which sites were visited). However the MITM
attack on public web pages creates a new vulnerability. Using a public wireless network to access cnn.com, can cause
a client to be exposed to attacks. Downloading a program
from a known download site can be terribly dangerous.
Thus, we argue that wireless networks are inherently at
higher risk than their wired counterparts. Our solution to
this problem is to require the wireless client to VPN all
trafﬁc (not just “sensitive” or corporate trafﬁc) to a trusted
wired network. By utilizing a secure VPN solution, the
client is no longer at risk from malicious network attacks
on the wireless segment. Also, the termination point of the
VPN must be carefully chosen and authenticated.

1.3 802.11b Wireless Network Conﬁgurations
We can classify wireless networks into two different
classes of wireless network scenarios: large institutional
networks and small individual wireless hotspots. Even
though the risks are generally the same over both deployments, it may be useful to consider them separately.
1.3.1 Rogue Access Points
A rogue access point is an access point deployed on a
large centrally administered network outside the administrative controls established for the authorized wireless access points. Without mutual authentication to the network,
clients could inadvertently connect to one of these Rogue
APs and thus be at risk.
There are precautions that can be taken by the network
administrators to detect and prevent these Rogue APs, however, only securing this network will not resolve a clients
vulnerabilities in other networks. A client compromised
elsewhere could then return to the secured institutional
wireless network and create additional an additional threat.
1.3.2 Hostile Hotspots
A Hostile Hotspot is a wireless hotspot, or a public wireless Internet point of presence where the owner or administrator of that hotspot has malicious intentions and tampers
with the trafﬁc it handles. Casual use of such hotspots puts
clients and their home networks at risk. These networks
are the real risk to wireless users whose home network has
deployed an effective local security solution.

2 Related Work
Much work has gone into securing wireless networks and
wireless clients. The majority of the work has focused on
devloping methods to prevent unauthorized access to the
wireless network. Such efforts would include improving
media access control and detecting and preventing rogue access points. We shall outline some of these solutions.

2.1 WEP and MAC Filtering
The ﬁrst attempts at providing some security to WiFi networks were WEP and MAC Address Filtering. WEP utilizes the RC4 stream cipher and manual key distribution
to provide data conﬁdentiality and integrity. WEP’s weaknesses have long been legendary [3, 11]. It is argued that
it is better than nothing and at least prevents casual attacks.
However, in the attack scenarios we present here it provides
no protection what so ever.

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

SSID:
CORP
CH:
1
AP MAC: AA:BB:CC:DD
WEP:
SECRET

Rogue AP

SSID:
CORP
CH:
6
AP MAC: AA:BB:CC:DD
WEP:
SECRET

Route
Module

MITM
Module

WiFi Client

Figure 1. Example MITM Conﬁguration.
MAC Address ﬁltering is the attempt to restrict access
to the wireless network by only allowing certain MAC addresses to connect. Since MAC addresses can be changed
from their factory default and valid MACs can be sniffed
from the network it accomplishes nothing more than perhaps keeping honest people honest.

2.2 802.1x

Soon after the massive adoption of 802.11b, a new and
improved security mechanism was introduced, 802.1x[5].
This mechanism made modiﬁcations to the clients, APs and
added an authentication server that would allow clients to
authenticate to the network. The protocol was made to be
general enough so that many different authentication protocols could be utilized as seen ﬁt by those deploying the
network.
This newer solution is not without its ﬂaws either [9]; in
fact, it suffers from the same fundamental ﬂaw that 802.11b
suffers from: there is no authentication of the network.
Without this mutual authentication, there is no guarantee
that the client connects to the desired network and thus cannot trust the AP it connects to.
802.1x and TKIP, which amounts to an improved version
of WEP, have been packaged into a new security solution
called WiFi Protected Access (WPA). This interim solution
addresses client access to the network and WEP’s previous
vulnerabilities. TKIP still relies on a pre shared key, thus is
still vulnerable to MITM attack from valid network clients.
The latest standard, 802.11i, also leverages these solutions
but is supposed to add secure deauthentication and disassociation among other things. This standard is still in the draft
phase and is expect out in the end of 2003.

2.3 Detecting Rogue Access Points
There are recommended standard practices for deploying
a wireless network infrastructure in an institutional setting.
Among the recommended practices is monitoring both your
wired and wireless networks for indications of Rogue Access Points. Good record keeping and doing radio site audits will help detect these rogues. These techniques rely on
monitoring 802.11b Sequence Control numbers. Depending on your deployment scenario, monitoring the trafﬁc on
the wired LAN can also aid in detection of Rogue APs.

2.4 Other Security Solutions
There have been other attempts to address these security
problems. Some from Academia [1, 2, 4] and some from
the open source community[13, 10].
These and many more solutions arose out of the particular needs of the groups designing them, all with the intent of
improving security. However, in the case of highly mobile
networks where the clients have a large degree of network
promiscuity, a partial ﬁx, or ﬁx at home, will not solve the
problem. A client from an entity that has deployed and ultra secure local solution will not beneﬁt from that solution
when away from the home network.

3 Vulnerabilities
As mentioned earlier wireless networks suffer from the
same vulnerabilities as wired networks. Both can experience the same hodgepodge of IP based and higher vulnerabilities. The difference begins at the Data Link Layer and
the inherent broadcast nature of the wireless physical layer,
which doesn’t beneﬁt from the restricted physical access of
traditional wired networks.

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

3.1 Mutual Authentication
The ﬁrst fundamental difference is that wireless networks are a broadcast media with no effective mechanism
of media access control. This design and several solutions
target their effort at authenticating the user without guaranteeing that the client is connecting to the network he desires.
Technically, wired networks also do not provide mutual
authentication, however, wired networks (not being broadcast through the air) beneﬁt from the physical security of
the network jacks that connect to it.

3.2 Network Promiscuity
This important difference is where trouble begins and
the new class of vulnerabilities become apparent. In the
past, network connectivity was static. A computer sat on
a desk at one location connected either to a corporate network or to an ISP where accountability and reliability of
the provider was important to that provider. The provider
had an incentive to protect both his own network and the
client computers that utilized it. The incentive to protect the
client computer comes from knowing that if it is compromised, that would increase the chance of the provider being
compromised.
With wireless networking, things have changed. The inherent mobility of wireless clients and lack of network authentication have given rise to an age of high risk network
usage, a type of network promiscuity if you will. Mobility implies that a computer will move between administrative domains. Each of these domains will have different goals and levels of administrative competence. In the
wired scenario, the network provider had incentive to protect the client thus his network, this is no longer necessarily
the case. Since a computer will cross domains there may
now be incentive for a domain administrator to interfere
with a client computer’s operation with the intent of compromising another administrative domain. With the lack of
network authentication the risk is greater than a few malicious administrators. Valid network clients masquerading
as valid network access points can also be in a position to
cause harm.

4 Proof of Concept Experiment
To further illustrate the unique vulnerabilities encountered due to the lack of mutual authentication and network
promiscuity that arises with mobile computing we implemented a proof of concept software download MITM attack. Unlike the attacks in the previous section this attack is
intended to take advantage of the roaming between administrative domains. Once a wireless client is compromised

by installing trojaned software, it brings that threat to any
other network it encounters.
The scenario we shall consider is a Rogue Access Point
in a corporate or university setting where a WEP key is established and only veriﬁed MAC addresses are permitted.
This Rogue AP could be created by a valid user, using the
authentication information he was given for his personal
use. It could also be created by an outside attacker who
has retrieved the WEP key via Airsnort and a MAC address
that he has observed by snifﬁng network trafﬁc. Please note
that the attack could be modiﬁed to ﬁt several different scenarios.
The attacker will ﬁrst authenticate to the existing network as a valid client with one WiFi card. A second WiFi
card will be used to create the Rogue AP. It will emulate
a valid AP as best it can. He can use the same SSID and
require the same WEP key.
As clients connect, some will doubtlessly accidentally
connect to the Rogue AP. If the attacker wants to target a
speciﬁc wireless client he can do so. If the attacker knows
the target clients MAC address he could force the clients
disassociation from the legitimate AP until the client associates with the Rogue AP. Now, he will have complete control over the trafﬁc of any client that has associated with
him.

4.1 Experiment Setup
We perform this experiment with two Sony Vaio Z1 laptops running the Linux operating system. The unsuspecting
client will be conﬁgured to connect to the corporate network
with SSID “CORP” and have the WEP key entered into his
machine (Figure 1).
The gateway machine has a D Link DWL-650 PCMCIA
WiFi card and a Netgear MA101 USB WiFi card. The Netgear card is conﬁgured to be a client on the “CORP” network with the WEP key “SECRET” and use the Linux Atmel driver[12].
The D Link card is conﬁgured with the Linux hostap
driver[8] to operate in Master mode, or to behave like an Access Point, with SSID “CORP”. It also uses the “SECRET”
WEP key.
After the proper conﬁguration of the wireless interfaces
an ARP proxy bridge was established between the two interfaces using parprouted[6]. After setting the appropriate
routes, the gateway machine was ready to begin transparently bridging trafﬁc. Appendix A contains a script that can
be used for conﬁguring the interfaces and bridge.
With the bridge enabled it is time to target a download.
We set up a sample target download web page which contained a downloadable binary, a link to that downloadable
binary and an MD5SUM of that binary. This download

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

Rogue Access Point
Other Traffic
Target−IP
Port 80

netsed
No Rule Match

Replacement

Figure 2. Software Download MITM Detail.
scenario is relatively common, where the MD5SUM is intended to verify that package was downloaded properly.
Since the clients trafﬁc is already passing through the
gateway machine it makes our job much easier. To accomplish this in a wired network is possible via ARP spooﬁng,
DNS spooﬁng, or by compromising a legitimate gateway
machine. Since we have already established ourselves as
a legitimate gateway, all that is required is that we redirect
the clients trafﬁc destined to the Target website through our
user space proxy. In this case, the redirection is handled via
Netﬁlter in Linux. The following iptables command will
accomplish this:
# iptables -t nat -A PREROUTING \
-p tcp -d Target-IP --dport 80 \
-j DNAT --to Gateway-IP:10101
This command redirects any TCP trafﬁc destined for TargetIP on port 80 to Gateway-IP on port 10101. In this case,
listening on port 10101 of the gateway is a program called
netsed[16].
Netsed proxies this HTTP trafﬁc to the destined host
while watching for strings speciﬁed in the rules given to it.
Upon a match, netsed will replace the identiﬁed string with
the replacement string it was given. This is a simple search
and replace. This netsed command was issued:
# netsed tcp 10101 Target-IP 80 \
s/href=file.tgz/href= \
http:%2f%2fGateway-IP%2ffile.tgz \
s/REALMD5SUM/FAKEMD5SUM

This command tells netsed to listen on port 10101 (where
Netﬁlter will direct all port 80 trafﬁc for the Target-IP) and
replace every occurrence of the legitimate link with the fake
link and every occurrence of the real MD5SUM with the
fake MD5SUM (the %2f is ASCII hex for the / character
and will be properly interpreted by the web server). The
net effect of doing these replacements is to replace the valid
HTML link with a link to a trojaned version of the software desired by the client. It also manages to replace the
MD5SUMs so the client is assured that the download has
completed safely.

4.2 Experiment Conclusions
This particular implementation is only one particular
way of accomplishing this attack. In fact it is even a rather
naive attack, in that it reveals the real download IP to the
client. In addition to that netsed will not match strings that
cross packet boundaries. These, and other problems, could
easily be addressed by someone with malicious intent. In
fact, there are many variations on this attack. This approach
could be used to do all sorts of nasty things to the client but
we expect that this particular attack sufﬁciently illustrates
the risks.

5 Solution
The previous example should convince the reader that
even casual web browsing over a wireless link is suscepti-

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

SSID: CORP

SSID: CORP

Web Server

1
0

WiFi Client

Rogue AP
Valid AP

1
0
0
1
0
1

VPN
Endpoint

Figure 3. Example VPN Proxy Conﬁguration in a Compromised Wireless Network.
ble to tampering of considerable consequence. The solution
to this problem is to require all trafﬁc to pass through a VPN
to a trusted, secure, wired network. If, however, this example was not sufﬁcient to convince the reader that all trafﬁc
should pass through the VPN then consider this scenario.

The VPN endpoint could be provided by the client’s
home corporation, home ISP, or perhaps a trusted third
party. The important fact is, that arrangements for the VPN
(secret exchange or certiﬁcate issuance) must take place out
of band or on a secure network and not in a situation where
the initial transaction would be vulnerable.

5.1 CNN - “Trustworthy” Websites
5.2.1 Hotspot Provider
Consider a wireless client who thinks his web browsing habits are safe because he only visits large legitimate
websites, like CNN. He doesn’t expect the administrators of
that site to attempt to compromise his computer. This user
may be a little behind on browser or email client updates.
He doesn’t worry though, since CNN wouldn’t include a
malicious javascript or similar client software exploit. On
an unprotected wireless segment, the trust he places in the
website provider is irrelevant, since, as our example shows,
anyone could insert malicious code into any web content
requested.

5.2 VPN Requirements
When considering a solution to this problem one must
recall the nature of network promiscuity. A solution that is
local to one network will not protecting the client reliably.
The risk of a client using a Hostile Hotspot and bringing
trouble back home still remains. By tunneling all trafﬁc
through a secure VPN connection to a trusted network network promiscuity is no longer a concern.
The VPN must satisfy the following requirements:
1. Provided by trustworthy entity
2. Authentication information preestablished
3. VPN endpoint in secure wired network
4. Must handle all client trafﬁc

This being said, it is not sufﬁcient or at least not practical
for a hotspot provider to provide this service. Since in the
course of a mobile client’s life it will pass through many
networks where the ownership is not immediately apparent.
The client cannot be guaranteed to know who to perform an
out of band protocol with.
One might ask, isn’t it sufﬁcient for the client to interact with the service provider if that provider has a valid,
signed SSL certiﬁcate from a legitimate certiﬁcate authority? Wouldn’t this enable a secure browser based transaction to facilitate a VPN connection? We would not consider
this to be the case. Without knowing the WiFi provider’s
reputation the valid certiﬁcate is a guarantee of nothing
more than that provider having given the certiﬁcate authority several hundred dollars. Under certain circumstances,
this is a small price to pay.

5.3 VPN Selection
There are many VPN solutions, both free and commercial. The selection of a speciﬁc VPN solution will depend
upon the users requirements. For testing purposes we have
utilized a PPP through SSH VPN as described in Building
Linux Virtual Private Networks[7]. This of course has drawbacks since any UDP trafﬁc is subject to unnecessary retransmission by TCP.
So, by requiring all trafﬁc to VPN through a preestablished VPN provider we have prevented the possibility of a

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

vast array of attacks on the least secure segment of network
the client’s trafﬁc is likely to encounter. In addition to this
added security, the client’s trafﬁc can also be anonymized
for privacy reasons at the VPN endpoint.

6 Conclusion
With this understanding of the fundamental differences
between wired and wireless networks our task of working
toward a secured wireless infrastructure in the future should
be made easier. The idea of network promiscuity is with us
today and with the increased popularity of light weight computing devices such as PDAs and IP enabled cell phones it
is likely to be here with us tomorrow.
It is also important to keep an eye on network
bridge technologies when considering oneself with network
promiscuity. Even if WiFi evolves successfully into a secure technology does that device participate in other local
networks, like Bluetooth[14]? Perhaps this scenario isn’t
very realistic yet, but in the long term, such things may become a threat in the future.
Future work will likely include a thorough evaluation of
VPN technologies to determine their strengths and weaknesses with this application in mind. As well as improving
techniques of detecting and countering attacks similar to the
ones discussed here[15].

References
[1] P. Bahl, S. Venkatachary, and A. Balachandran. Secure wireless internet access in public places. In Procedings of the
IEEE ICC 2001, June 2001.
[2] D. B. Faria and D. R. Cheriton. Dos and authentication in
wireless public access networks. In ACM Wireless Security
Workshop (WiSe’02), 2002.
[3] S. Fluhrer, I. Mantin, and A. Shamir. Weaknesses in the key
scheduling algorithm of RC4. Lecture Notes in Computer
Science, 2259:1–??, 2001.
[4] A. Godber and P. Dasgupta. Secure wireless gateway. In
ACM Wireless Security Workshop (WiSe’02), 2002.
[5] IEEE 802.1 Working Group. 802.1x - port based network
access control.
[6] V. Ivaschenko. parprouted: Proxyarp routing daemon.
[7] O. Kolesnikov and B. Hatch. Building Linux Virtual Private
Networks. New Riders, Indianapolis, Indiana, 2002.
[8] J. Malinen. Host ap driver for intersil prism2/2.5/3.

[9] A. Mishra and W. A. Arbaugh. An initial security analysis
of the ieee 802.1x standard.
[10] NoCatNet. Nocat open authentication package.
[11] A. Stubbleﬁeld, J. Ioannidis, and A. Rubin. Using the
ﬂuhrer, mantin, and shamir attack to break wep, 2001.
[12] The Atmelwlandriver Team. Opensource linux driver for
Atmel AT76C503A-based wireless devices.
[13] WAVEsec Team. Wavelan security using ipsec.
[14] O. Whitehouse. Redfang - the bluetooth hunter.
[15] J. Wright. Detecting wireless lan mac address spooﬁng.
[16] M. Zalewski. netsed.

A Bridge Conﬁguration
#!/bin/sh
#
#
#
#
#
#
#

This script, of course, assumes that
you have already properly configured
the wireless NICs with iwconfig
eth1 - associated to CORP
in Managed Mode
wlan0 - is in Master mode
with essid CORP

# Turn on IP Forwarding
echo "Turning on IP Forwarding ..."
echo 1 > /proc/sys/net/ipv4/ip_forward
# Give the NICs IPs
ifconfig wlan0 10.6.6.2 \
netmask 255.255.255.0 \
broadcast 10.6.6.255
ifconfig eth1 10.6.6.3 \
netmask 255.255.255.0 \
broadcast 10.6.6.255
# Create the bridge
parprouted wlan0 eth1
# Set
route
route
route

some routes
add -host 10.6.6.1 dev eth1
add -host 10.6.6.7 dev wlan0
add default gw 10.6.6.1

echo "Bridge enabled"

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

Designing Adaptive Lighting Control Algorithms
for Smart Buildings and Homes
Yuan Wang
Arizona State University
Tempe, AZ, USA
Email: Yuan.Wang.4@asu.edu

Abstract-Artificial lighting is often the main lighting provision
for workplaces. This paper describes algorithms for optimizing
lighting control in large (smart) buildings that are extensible to
smart home use. Systems that provide uniform lighting, under
varying outdoor light levels, at occupied locations turns out to
be a hard problem. We present methods that work as generic
control algorithms and are not preprogrammed for a particular
building. Our system uses wireless sensors and wired actuators
for the lighting control. But the system does not know about
locations and correlations between lights and sensors. The model
shows that the control problem is NP-Hard. A heuristic algorithm
is proposed and validated to solve the problem to compute
approximate optimal solution.

I.

INTRODU CTION

Artificial lighting is essential in large buildings and homes.
Lighting has very important effect on people's health and pro­
ductivity as shown by recent studies [ 1]. Controlling lighting
based on occupancy, daylight effects and energy costs can
impact energy usage.
Currently there are 1.5 million commercial buildings and
1 14 million homes in the US with a growth rate of about
3% each year. Smart buildings and homes need, among
other features, the ability to control lighting. Deploying such
systems in existing buildings is most feasible if the system uses
wireless sensors, simple actuators and does not need custom
programming. These goals have driven our research.
Automating the lighting control systems such that uniform
lighting, under varying outdoor light levels, is maintained at
occupied locations turns out to be a hard problem. We are
working on algorithms that can be used in generic systems
that are not preprogrammed for a particular building. That is
the system does not know about locations and correlations
between lights and sensors.
Several existing approaches are given to increase lighting
comfort levels. Some of lighting control systems may be able
to provide enough lighting for a workplace, but people may
still feel uncomfortable when illumination level is sufficient
but not uniform. According to [ 1], three problems are insuf­
ficient light or uneven light or lights too bright. Controlling
lighting levels automatically need customized control systems
that rely on extensive pre-programming involving detailed
custom models andlor lengthy set-up. When the pattern in
a workplace changes, it needs expensive customization and
updates performed by trained personnel, which is costly and

978-1-4799-3 106-4114/$3 1.00 ©20 14 IEEE

Partha Dasgupta
Arizona State University
Tempe, AZ, USA
Email: partha@asu.edu

time-consuming. The "sequential lighting changes" approach
is sometimes used in lighting control system [2] for calculating
light settings. This method adjusts light settings based on
feedback data, which is able to generate a reasonable result,
but the average computational time is high due to the steps of
prediction and adjusting. It also increases the number of times
lights are switched on/off (unnecessarily) due to its feedback
design [2].
In this paper, a Wireless Sensor Network(WSN)-based light­
ing control system is introduced to efficiently and adaptively
control artificial lights to provide a stable and uniform lighting
environment. The system measures light at preset locations and
turns on or off light switches to achieve the goal. The system
is not pre-programmed and learns about effects light switches
have on sensors via calibration. The work is formalized using
non-linear integer programming model that proves to be NP­
Hard. A heuristic algorithm is proposed and validated to
solve the problem to compute approximate optimal solution.
The approach can be used in different places where uniform
lighting environment is required.
For the sake of brevity and simplicity we only describe
the operation of the system while there is no outside lighting.
Addition of varying levels of outside lights is an extension(see
section V). Also, energy consumption minimization can be
incorporated without major changes.
The rest of this paper is organized as follows. Section II
introduces the background and related work of the problem.
Section III presents the heuristic algorithms for lighting con­
trol. Experimental work and simulation results are discussed
in section IV. Section V talks about potential extensions of
the algorithm. We conclude the paper in section VI.
II.

B ACKGROUND AND RELATED WORK

A. Problem Description

Assume a place has n light switches and m light-level
sensors, placed by the human designer of the place. The
sensors are connected to the control system via a W SN and the
switches are activated via actuators connected to the system.
The physical location of lights, sensors and their correlations
are initially unknown to the control system.
The ultimate task is to compute the positions (on/off) of
n light switches. Let x
(Xl, ... ,xn ) denote the assignment
for light switches where Xi E {O, I} and 0 denotes off and 1
=

279

TABLE I
CALlBRATlON:LIGHTS' IMPACTS ON SENSORS(LUX)

denotes on. The goal is to optimize the comfort level. There
are two criteria needed to satisfy, lighting level and lighting
uniformity. The former is satisfied when every sensor reading
stays in an accepted range i.e. if sensor j's reading is Lj(x),
then min � Lj(x) � max. The latter can be satisfied by
minimizing the standard deviation(represented by 0") of the
sensor readings. Hence the problem can be stated as:

II

12

13

81

20

230

350

82

680

10

0

B. Related Work

minimize

(O"(L1(x), ..., Lm(x)))

subject to

min � Lj(x) � max, j= I, ..., m
XiE {O, l}, i=l, ..., n

x

( 1)

For lighting impacts, one important feature is that sensor
readings are additive [3], i.e. let Impacti j to be the impact
of light i on sensor j when only light i is on and it is dark
outside, then we have

n
Lj(x)= L Impacti j . Xi
i =l

(2)

[4] and [5] gave a general definition of nonlinear integer
programming problem. It can be stated as:
max/min

f(x)

subject to

hi(x)=0, iE I= I, ... , p
gj(x) � 0, jE J =l, ... ,q
xE Z n

(3)

where x is a vector of decision variables, and some of the
constraints hi, gj : zn ---+
or the objective function f :
zn ---+ are non-linear functions. In equation 1, let gl (X) =

lR

lR

Lj(x)-max, h1(x, y)=Lj(x)-min-y=O, y;::: O, yE Z.

It can be transformed to:
minimize

(0"(L1(x), ..., Lm(x)))

subject to

g l (X) � 0

x

h1(x, y)=0

(4)

XiE {O, l}, i=l, ..., n
y;::: O, yE Z
Since equation (4) satisfies the format of equation (3) where
the objective function is nonlinear, our problem belongs to
nonlinear integer programming problem. According to [4], it
is NP-hard. Therefore, any polynomial time computed solution
would be an approximation. To find the best setting, a naive
approach is to try all 2n positions of n switches, which is un­
acceptable due to the time complexity. Therefore, we propose
a heuristic algorithm for computing an approximate optimal
light setting. The heuristic plan has three main steps. First,
calibration, i.e. calculating Impacti ,(section III-A). Second,
sensors and lights are partitioned into small zones based on
calibration data(section III-B). Third, an approximate optimal
light setting is generated for each zone (section III-C).

Recently W SN technologies have been applied into a lot
of areas such as [6] [7]. It consists of distributed portable
wireless sensors to monitor physical conditions, such as light,
temperature, humidity, etc. By using W SN, people can easily
detect the change of surrounding environments and make cor­
responding adjustments for actuators. In lighting control area,
several existing work [3][8][9] applied W SN technologies into
lighting control system for energy conservation purposes. It is
involved with using some specific protocols in W SN area such
as ZigBee [ 10]. [ 1 1] proposed an approach to integrate small
wireless sensor or actuator nodes into an IP-based network so
that it is possible to provide web services at each node.
Several customized lighting control systems are designed
for some specific areas. For example, [8] was mainly designed
for entertainment and media production area while [ 12] was
mainly designed for theater arts area. These systems generally
have some specific requirements that normal lighting control
systems do not have. For example, in theater arts area, it needs
to capture the positions of actors in real time. Therefore, the
objective functions for the lighting control in specific places
need to be adjusted accordingly and some particular sensors
may need to be enrolled. When more than one lighting re­
source are involved, such as whole lighting and local lighting,
[9] proposed an approach that every user carries a sensor
to detect the local intensity. [ 13] introduced an user-friendly
interface for a networked lighting control system.
[ 14] presented a mathematical model for lighting control
system that could be applied into the case that a luminary
impact is continuous such as light emitting diodes(LEDs)
luminaries and the goal illumination level is given as a single
value rather than a particular range. [ 15] used infrared ray
communication technology for smart illuminance sensor to
retrieve the lighting ID binding with each lighting fixture.
Through analyzing lighting ID information, sensors can rec­
ognize nearby luminaries so that the control efficiency is
improved.
III.

CONTROL ApPROACHES

A. Calibration

Our system first determines the lighting levels, lighting
locations and sensor correlations via calibration. We note that
each light has an impact on a set of sensors. Calibration is
used for calculating Impacti ;' Suppose there are n lights in
an area. The process of calibration is:
1) Turn off all lights and turn on one light at a time
2) Record the light's impact on each sensor
3) Repeat step 1 n times until all lights are counted

280

The impact data will be inserted into a table as shown
in Table I. Assume there are 3 lights(h, h and l3) and
2 sensors(sl and S2) in an area. Table I shows all lights'
impacts on sensors in lux values. For example, when only
h is on, its impact on Sl is 20 lux(Jmpach1) and on S2 is
680 lux(J mpach2).
B. Zoning/Partitioning

From the calibration data we can partition the whole area
into smaller zones. The idea of zones is that some switches
have no impact on some sensors as they are in different rooms
or floors. We assume that every light and sensor can only
belong to one zone and artificial lighting is the only lighting
resource in the area.
Assume there are A sensors and B lights. For each j, j
1,... , A, we define a function fj : {O,l}B -+
which is a
mapping from all lights status(on/off) to a non-negative value
registered by sensor Sj. It is clear that fJ(x)
Lj(x). Using
the same example as shown in Table I, if II is on, l2 is off, and
l3 is on, h(l,O,1) L1(l,O,1) 1· 20+0·230+1·350
370. All fjs can be collected into F : {O,l}B -+
with for
every choice of which lights are on/off gives the value of lux
for all sensors.
The goal is to partition the whole area for dropping down
the computational size (number of lights in a zone). For
each j, j
I, ... , A, we define a vector gj of length B to
indicate which lights are assigned to sensor S j. We also set
up a threshold " which is used to decide whether a light is
partitioned into a zone or not. When only li is on, if the value
of fJ is smaller than " then the ith value of the vector gj is
0; otherwise it is 1. Proceeding with the example of Table I,
we show the procedure of getting gl and g2. Let's set,
30.
Then since when only h is on, the value of h is 20 which is
smaller than,, 1st value of the vector gl is O. The value of 12
is 680 which is larger than ,, 1st value of the vector g2 is 1.
Applying this method, in consequence, we have gl
(0, I, 1)
and g2
(1,0,0), i.e. lights hand b are assigned to sensor
Sl while light h is assigned to S2.
=

lR+,

=

=

=

=

lR�

=

=

=

=

C. Final Computation

The last step is to compute desired light settings. Note that
the computations in all zones are independent and can run
concurrently. We describe the final computation for a zone,
with lights and sensors belonging to that zone only.
The proposed lighting control algorithm contains 4 parts.
Part 1 counts the minimum number of lights needed to
be turned on. Let m to be the number of sensors, C
{C1,... ,Cn } to be the set of lights' contributions to the whole
=

m

zone where Ci

=

L Impactij,

j=l

lowerbound is equivalent

to min x m and upperbound is equivalent to max x m.
To count the minimum number of lights turned on, simply
find minimum number of elements in the sorted array C such
that sum of them are greater or equal than lowerbound. It
is obvious that a linear search would be enough to solve

the problem. If sum of all values in C is still smaller than
lowerbound, simply turn on all lights.
Part 2 and Part 3 compute candidates of final light setting.
A candidate c is a set that only contains light numbers selected
to be on, i.e. if i E c then Xi
1, otherwise Xi O.
Part 2 computes the base-level candidates. From part 1, it
is known at least w lights needed to be turned on to adjust
light intensity greater or equal than lowerbound. Therefore,
for each base-level candidate setting, it should contain w
elements and sum of the w elements' contributions to the
=

=

L Ck) should be slightly greater or equal
kEsettin g
than lowerbound. The proposed approach is based on the
whole zone(

fact that lux level light intensity is additive[3] and greedy
algorithm, which is shown in Algorithm 1.
Algorithm 1

Base-Level Candidates Selection

Input: C, n, w, lowerbound
Output:

base-level candidate

setting

1: searchvalue +-lowerbound/w,
2:
3:
4:

find index of the value that
searchvalue in C and put it
find index of the value that
searchvalue in C and put it
add Sl and S2 to s

5: for
6:
7:

10:

j

-

change

to

Sl

s +- 0

1 do

searchvalue +-(lowerbound

8:
9:

E s do
= 1 --+ w

Si

for

Sl, S2 ,

is the smallest value larger than
in Sl
is the largest value smaller than
in S2

-

L Ck)/(W - j)

kEsi

Si

in line2 and repeat

end for

add

Si

to

setting

1 1: end for
12: for
13:

Si

for

E s do
= 1 --+ w

j

-

1 do

14:

searchvalue +-(lowerbound

15:

if

else

change

18:
19:

L Ck)/(W - j)

j is odd then
change Sl to Si in line2 and repeat

16:
17:

-

S2

to

Si

in line3 and repeat

end if

20:

end for

2 1:

add

Si

to

setting

22: end for
23: return

setting

Part 3 generates more candidates from base-level candidates.
Unlike light impacts, standard deviation is not additive, thus
increasing number of lights may negatively impact the stan­
dard deviation. However, due to time and quality tradeoffs,
computing more sets likely results in getting closer to the
theoretical optimality. Adding <5 lights to compute needs O (nO )
time. To ensure low response time a low complexity is
desirable, and based on the theoretical analysis and simulation
experiments, we set <5
2. It indicates that we will at most
add 2 lights to the existing candidates.
Suppose each base-level candidate has w elements, there
are n lights in total. Then if each candidate wants to add an
un selected light into itself, there would be n - w choices.
=

28 1

If there are k base-level candidates, finally there would be
k x (n - w) new candidates that have w + 1 elements
added. Similarly when another unselected light is trying to
add into the candidates that have w + 1 elements, total
amount of candidates that have w + 2 elements is becoming
k x (n - w) x (n - w 1). Thus total number of candidates
would be k + k x (n - w) + k x (n - w) x (n - w -1). Note

B. Feasibility

The experiment is run at night. Each sensor is placed under
each light, at a distance of 60 inches, and they are numbered
o to 8. Experimental design and elements are shown in Fig.
1. The steps are:
1) Calibrate lights' impacts on sensors using the approach
described in section III-A.
2) Set expected lux level range from 49 to 5 1(Test 1).
3) Turn off all lights. Run control server to compute light
settings. When selected lights are turned on, record the
reading on each mote.
4) Change the step two's expected lux level range to be 99
to 10 1(Test2), 149 to 15 1(Test3) and 199 to 20 1(Test4)
respectively. Rerun step three.

-

for each candidate

c,

L Cp :s; upperbound.

pEe
Part 4 calculates the standard deviation of sensor readings
generated by each candidate. According to equation 2, for each
candidate c, we are able to calculate its impact on sensor j
Lj ( c) . Then it is easy to know the standard deviation of all
sensors' readings under c. The candidate that generates the
lowest standard deviation of sensor readings would be selected
as the final light setting.
D. Time Complexity of Heuristic Lighting Control Algorithm

Suppose there are n lights, m sensors in a zone. Calibration
has a time complexity O(n). Zoning/Partitioning has a time
complexity O(m n). Final computation has 4 subparts. Part l's
time complexity is O(n). The worst case for Part 2 is O(n2).
As discussed earlier, the time complexity for Part 3 is nO . In
our case, 0
2, which makes Part 3 time complexity to be
O(n2). Obviously Part 4's time complexity is equivalent to
Part 3, which is also O(n2). Therefore, for every zone, the
total time complexity is O(n2).
When considering the whole area, suppose there are z zones,
and zone Zi(i E [1, z ]) has ni lights. If total number of lights
=

The results are shown in Fig. 2. From the figure, we can see
for each test, the computed light setting's impact on every mote
slightly fluctuates at the middle point of the expected range.
Take test 3 as an example. Lights 0, 1,2,3,6,7,8 are selected
to be on. Every mote's reading is falling between 149 and
15 1. Average of mote readings for test 3 is 149.22, standard
deviation is 3.80.
All other tests show the similar trends, which shows the
feasibility of our real system, that is using the results of our
computation and then applying it to a real, calibrated room,
it provides the results that we predicted. Prior to this test,
we had already measured the additive property and had some
experience with daylight measurements.
300

z

is N, we have N

=

L ni.

• Testl, Lights 5,7 on, Avg=49.89, Stdev-2.26

Since computations in zones

250

i=l

X

:::J 200

on,

....

•

Avg=149.22, Stdev=3.80
on,

•

•

.a.

.a.

.a.

•

•

•

•

Avg=199.67, Stdev=4.30
•

•

.a.

.a.

•

•

•

•

•

•

•

•

.a.

.a.

.a.

•

•

•

•

•

•

<::"
u

a. 150
.5
fA, 100

EX PERIMENTAL WORK AND SIMULATION RESULTS

.a.

...

:::;

A. Implementation Details

To demonstrate the feasibility and effectiveness of our
approach, we used an experimental setup. A 8ft x 8ft test cell
was instrumented with 9 lights (l5W incandescent, 120V),
and 9 sensors connected via a W SN and actuators to a
computer. The computer is a Intel Core2 running Linux. The
sensors use Crossbow's TelosB motes containing visible light
sensors called Hamamatsu S 1087. The sensor network uses
the Collection Tree Protocol [ 16] to send lighting data to
a designated gateway sensor. Applications running on the
motes are implemented in NesC and TinyOS environment. The
computer runs a server called SenServer developed in Java and
connects to the gateway sensor via serial port. A ProXR Relay
Controller is connected to Control Server through USB port
to control artificial lights. Since the official driver of the relay
controller is designed for Windows platform, Control Server
is developed in C# that runs in a Windows machine. It is
implemented based on the contents discussed in section III.
Eclipse Standard 4.3 and Microsoft Visual Studio 20 10 are
used for Java and C# developments respectively.

... Test3, Lights 0,1,2,3,6,7,8

eTest4, Lights 0,1,2,3,4,5,6,7,8

are running concurrently, the total complexity is equivalent
to O( (max(nl, ..., nz)) 2). It is clear max(nl, ..., nz) :s; N,
so the whole area's time complexity is O(N2).
IV.

.Test2, Lights 0,1,3,6,7 on, Avg=l00.11, Stdev=3.48

50

•

•

•

•

Mote 10
Fig. 2.

Experimental Results of 9 lights and 9 motes

C. Adaptivity and Scalability

The test cell experiments are limited, as the number of
lights are low, the space is limited and the geometry is rather
simple. To be able to study adaptivity and scalability we
did simulations on a more complex, synthetic setup, with
randomly generated impact values.
Adaptivity and scalability are two important features of the
system. Adaptivity means when room pattern changes such
as adding or removing some lights, changing lights' posi­
tions, etc, the system can automatically detect environmental
changes and make corresponding adjustments. In other words,
when room pattern changes, the lighting control algorithm

282

(c) Testbed Layout
Fig. 1.

Experimental Design

should be able to adjust light settings accordingly in a short
time without any modifications. Adaptivity is also necessary
when there is additional illumination due to outside lights.
Scalability means when amount of lights is growing large, the
lighting control algorithm could still compute the light settings
in a reasonable time. Apparently customized algorithms do not
have the adaptivity feature and custom systems also use brute
force methods to determine lighting levels and hence have
exponential complexity, thus not being scalable.
When pattern changes, the only part that might get changed
in the objective function(Equation 1) is Impacti;' Therefore,
to test if the heuristic algorithm is adaptive, Impacti; should
be randomly assigned. In addition, expected lux level for this
experiment is set between 320 and 500 lux to match normal
office lighting range [l7]. The simulation experiment runs as
follows:
1) Assume there are 30 lights and 25 sensors in a zone.
Each light i has an impact value on sensor j(Impacti;)'
There should be 750(30 x 25) impact values.
2) Randomly assign an integer value from 0 to 100(spec­
ification of 15W, 120V light bulb) to be one impact
value. Repeat this step 750 times until all impact values
are successfully assigned.
3) Run brute force algorithm(O(2n)), the proposed light­
ing control algorithm(O(n2)), and a revised O(n3)
algorithm(o
3) that adds one more light to the candi­
date sets of O(n2), respectively. Record computational
time, lux level light intensity and standard deviation for
each method.
4) Repeat step 1-3 1000 times. Take an average of standard
deviation, light intensity and computational time results
respectively.
5) Increase light's amount and rerun steps 1-4.
=

The result is shown in Fig. 3. Fig. 3(c) describes time per­
formances of different methods. Since when amount of lights
increases, the computational time of brute force method is
increasing exponentially, which quickly arrives at a very large
point of value while polynomial time algorithms have a pretty
low computational time. Therefore, in order to put all data
together, for Fig. 3(c), we take a common 10garithm(loglO)
on the real computational time data. From the graph, we
can see that our proposed 0 (n2) algorithm reaches a similar
performance in light intensity and standard deviation compared

to the revised O(n3) algorithm, but the computational time is
far less than the latter one. Compared to the brute force, the
proposed approach has an extremely better time performance
with a similar light intensity performance and a reasonable
increase on standard deviation.
We also do some other evaluations to verify the system's
scalability. For example, we run the above algorithms on very
large dataset like 1000 number of lights. Result shows that the
proposed O(n2) approach can still compute the light setting
in a desired range while other methods are not applicable
due to the time performance. When size is 1000, the O(n2)
algorithm is running in around 106 time complexity which
reduces substantial workloads from brute force's 1.07 x 10301
time complexity.
V.

EXT ENDING T HE ALGORIT HM

Our goal is to produce a stable and uniform lighting
environment in workplaces by adjusting artificial lights. In
this paper, we assume artificial lighting is the only lighting
source in a workplace and there is no other lighting involved.
However, when ambient light exists, the algorithm needs to
be extended to allow for the impact produced by such light.
Ambient light will produce a non-zero and varying impact on
different sensors and is not controllable via light switches. This
impact will have to be detected and used in the computation,
in other words the computation needs to compute deltas
between current conditions and desired levels and recomputed
switch settings. The idea can be also applied when required
illumination levels in an area are different.
The selection of the final computation results can be done in
an energy efficient manner by computing the energy costs of
each light being turned on. From this we can select candidate
setting that may not produce the best uniformity but its
energy is consumed in significantly lower. In addition, the goal
lighting level can be changed for energy savings especially at
peak load hours.
Placement of sensors is an important point we did not
address in the paper. In the current work, we assume sensors
in an area are distributed in a good way that can very well
capture the light intensity. In the future work, we will do more
investigations on sensor layouts, which will better capture the
light intensity in an area.
Although the brute force method can't be directly applied
due to the time issue, it still might be worthy to add into the

283

70
c:
0
',0:;

"'

60

50

'>

40
Qj
Q 30
'l:I
0
'l:I
c: 10

.... 0(2AN)
__ 0(NA2)

... 2
"'

...... 0(NA3)

"'
....

\I')

30

31

3

2

33

5

34

3

36

37

38

39

40

Amount of Lights

in both experimental and simulated scenarios. The underlying
system of wireless sensors and wired actuators can be de­
ployed at homes and in larger building without excessive costs.
We build a formal model of the lighting control problem and
show it is a hard problem (NP-Hard). A heuristic algorithm
is proposed to solve the problem to compute approximate
optimal solution. Experimental results show the efficiency
and effectiveness of the heuristic algorithm. The proposed
approach can be augmented to acconunodate day lighting,
failures, manual overrides, and occupancy detectors.

(a) Standard Deviation

REFERENCES

600

� 500

--'"

.u;

c:
Q) 400

..
c:

=

Qj
>

�
)(
.3

.......

"I

,.

.....

.... 0(2AN)

300

__ 0(NA2)

200

...... 0(NA3)

100

30

31

32

33

34

35

36

37

38

39

40

Amout of Lights

[1] Lighting at work. Health and Safety Executive, 1997.
[2] R. Mohamaddoust, A. T. Haghighat, M. J. Motahari Sharif, and N. Ca­
panni, "A novel design of an automatic lighting control system for
a wireless sensor network with increased sensor lifetime and reduced
sensor numbers," Sensors, vol. II, no. 9, pp. 8933-8952, 2011.
[3] V. Singhvi, A. Krause, C. Guestrin, J. Garrett Jr, and H. Matthews,
"Intelligent light control using sensor networks," in Proceedings of the
3rd international conference on Embedded networked sensor systems.

ACM, 2005, pp. 218-229 .
[4] R. Hemmecke, M. Koppe, J. Lee, and R. Weismantel, "Nonlinear integer
programming," arXiv preprint arXiv:0906.5171, 2009.
[5] Wikipedia. Nonlinear programming.
[6] A. Mainwaring, D. Culler, J. Polastre, R. Szewczyk, and J. Anderson,
"Wireless sensor networks for habitat monitoring," in Proceedings of
the lst ACM international workshop on Wireless sensor networks and
applications.
ACM, 2002, pp. 88-97.
[7] Q. Li, M. De Rosa, and D. Rus, "Distributed algorithms for guiding
navigation across a sensor network," in Proceedings of the 9th annual
international conference on Mobile computing and networking. ACM,
2003, pp. 313-325.
[8] H. Park, J. Burke, and M. B. Srivastava, "Design and implementation of
a wireless sensor network for intelligent light control," in Proceedings

(b) Light Intensity

6.5
5.5
�c: 4.55
8 4
Qj 3.5
11\
- 3
I- 2.5
� 2
�1.5
0.5

--

----

....-

--

....-

....-

-0(2AN)
-0(NA2)
-0(NA3)

of the 6th international conference on Information processing in sensor

ACM, 2007, pp. 370-379.
[9] M. Pan, L. Yeh, Y. Chen, Y. Lin, and Y. Tseng, "Design and implemen­
tation of a wsn-based intelligent light control system," in Distributed
networks.

Computing Systems Workshops, 2008. 1CDCS'08. 28th 1nternational

IEEE, 2008, pp. 321-326.
[10] Y. Wang and Z. Wang, "Design of intelligent residential lighting control
system based on zigbee wireless sensor network and fuzzy controller,"
in Machine Vision and Human-Machine Interface (MVHl), 2010 Inter­
national Conference on. IEEE, 2010, pp. 561-564.
[11] L. Schor, P. Sommer, and R. Wattenhofer, "Towards a zero-configuration
wireless sensor network architecture for smart buildings;' in Proceedings
Conference on.

Amount of Lights
(c) Computational Time
Fig. 3.

Comparison of Different Approaches

system because it can produce the theoretical best result. One
idea is to combine our current work with brute force method.
The current work can be used during the initial step to quickly
generate an accepted light setting. The brute force method can
finely adjust the setting in the backend afterwards. Optimal
results can be also stored for future use.
VI.

CONCLUSION

To enable automated lighting control, under varying con­
ditions of occupancy, needs, outside lighting influences and
other perturbations it is essential to have a core algorithm
that is effective and adaptive. Such algorithm must be deploy­
able in a simple, cost effective system without the need for
customizations and reprograrmning as conditions change. This
paper presents such a core algorithm and tests its effectiveness

of the First ACM Workshop on Embedded Sensing Systems for Energy­
Efficiency in Buildings. ACM, 2009, pp. 31-36.
[12] c. Feng, L. Yang, J. W. Rozenblit, and P. Beudert, "Design of a wireless
sensor network based automatic light controller in theater arts," in
Engineering of Computer-Based Systems, 2007. ECBS'07. 14th Annual

IEEE, 2007,
pp. 161-170.
[13] T. Hiroyasu, A. Nakamura, S. Shinohara, M. Yoshimi, M. Miki, and
H. Yokouchi, "Intelligent lighting control user interface through design
of illuminance distribution," in Intelligent Systems Design and Applica­
tions, 2009. ISDA'09. Ninth International Conference on. IEEE, 2009,
pp. 714-719.
[14] A. Schaeper, C. Palazuelos, D. Denteneer, and O. Garcia-Morchon,
"Intelligent lighting control using sensor networks," in Networking,
IEEE International Conference and Workshops on the.

Sensing and Control (lCNSC), 2013 10th 1EEE 1nternational Conference
on. IEEE, 2013, pp. 170-175.
[IS] M. Miki, A. Amamiya, and T. Hiroyasu, "Distributed optimal control
of lighting based on stochastic hill climbing method with variable
neighborhood," in Systems, Man and Cybernetics, 2007. ISle. IEEE
International Conference on. IEEE, 2007, pp. 1676-1680.
[16] O. Gnawali, R. Fonseca, K. Jamieson, D. Moss, and P. Levis, "Collection
tree protocol," in Proceedings of the 7th ACM Conference on Embedded
Networked Sensor Systems. ACM, 2009, pp. 1-14.
[17] Wikipedia. Lux. [Online]. Available: http: //en.wikipedia.orglwikilLux

284

2011 IEEE International Conference on Privacy, Security, Risk, and Trust, and IEEE International Conference on Social Computing

Protecting cryptographic keys on client
platforms using virtualization and raw disk
image access
Sujit Sanjeev

Jatin Lodhia

Goldman Sachs Group Inc.

Google Inc.

Partha Dasgupta
Raghunathan Srinivasan
Arizona State University
Arizona State University
Tempe, USA
raghunathansrinivas@gmail.com
partha@asu.edu

have keys embedded in them. Software like Truecrypt [3]
and BitLocker [4] allow users to store data in an encrypted
form on secondary storage. Storing encrypted data on
computers or secondary storage merely changes the
vulnerable point in the system. Instead of targeting the data
directly, malware writers can target the encryption keys. A
breach of trust in the machine can lead to a malware stealing
keys. The use of cryptography in computer software has
evolved to a major extent and the stakes are high for the
users if their trust is compromised.

Abstract— Software cryptosystems face the challenge of secure
key management. Recent trends in breaking cryptosystems
suggest that it is easier to steal the cryptographic keys from
unsecure systems than to break the algorithm itself, a
prominent example of such an attack is the cracking of the
HD-DVD encryption. This paper presents two methods to hide
cryptographic keys in an unsecure machine. The first method
uses virtualization to isolate the sections of memory that
contain cryptographic keys from an untrusted guest operating
system (OS). Virtualization is an effective method to provide
isolation between trusted and un-trusted components of a
system. This work makes the Virtual Machine Monitor (VMM)
as a cryptographic service provider for guest OS. The second
method provides techniques to securely retrieve and store keys
in secondary storage. The information about key storage and
retrieval is stored inside the application binary. On execution
this section retrieves the key from secondary storage.
Keywords-component; Key hiding; Secret
Virtualization; Raw disk interface; Lguest; Linux

I.

Keys can be stored on external tamper resistant hardware
devices such as smart cards [5]. The device has a processing
chip that can perform cryptographic tasks such as encryption
and digital signatures.
Though hardware based
cryptosystems provide better security, they are not feasible
for all classes of end users due to various deployment issues,
hence software based key hiding systems are required.
Software based techniques operate within the realms of the
OS and resort to techniques like scattering, key stores and so
on. In any of these systems, the cryptographic key has to be
brought into system memory for any crypto operation. This
can provide an attacker a window of opportunity to steal
keys. Once a key is stolen, all encrypted operations using
this key can be compromised. The most notable example of
this kind of attack is the cracking of HD-DVD encryption [6,
7].

Hiding;

INTRODUCTION

Computers are an integral component of enterprises,
government establishments and consumer computing. They
are used to carry out a variety of critical and sensitive
computations. Secure computing models have evolved to
use advanced cryptographic techniques such as digital
signatures and high end encryption. Protocols such as Secure
Sockets Layer (SSL) provide the consumers with a sense of
security by encrypting all communications. These protocols
use encryption to secure communication among machines,
but provide no protection for the platforms themselves that
execute them. A machine can get compromised in a variety
of different ways. A user may knowingly or unknowingly
execute unverified applications on the computing platform.
It is estimated that fault density in programs can range from
2 – 75 per 1000 LOC [1]. In addition OS consists of many
device drivers, which have error rates much higher than
kernel code [2]. Malware writers attempt to exploit existing
bugs to compromise machines in an attempt to steal secret
data. Secret data may be bank account information, credit
card information, social security numbers, passwords, and so
on. Such information can be stored and exchanged securely
using cryptography. Browsers use digital certificates which
978-0-7695-4578-3/11 $26.00 © 2011 IEEE
DOI

This paper presents two methods to preventing malware
from stealing keys from computing platforms. The first
involves the use of virtualization where the property of
memory isolation is used to separate key storage from
system memory. With this model, all applications execute in
the guest OS, while software cryptosystems run in a separate
and secure hardware protection domain. This provides
higher levels of isolation from the untrusted parts of the
system. The key is stored in the Virtual Machine Monitor
(VMM), outside the domain of the guest OS.
All
cryptographic operations are performed by the VMM, the
guest OS requests crypto operations by executing the
appropriate hypercalls. As long as the VMM is not
compromised by the guest, the keys stay secure and
protected from the guest.

1026

The second method uses accesses to raw disk image for
hiding keys on secondary storage media. To effectively hide
the key on secondary storage, the keys are directly written by
an application to the unused sectors of files on the secondary
storage without using the file system calls of the OS. The
key bits are scattered all over in a pattern which is unique to
every installation of the cryptographic application. This
information is generated during installation time and is hard
coded into to application binary as a key fetch block. The
key fetch block is padded with ‘junk’ instructions;
additionally the location of the key fetch block is different in
each installation. This ensures that an attacker cannot use
information gained by analyzing one instance of the
application to steal keys from another installation of the
application. It must be noted that hardware methods to
acquire cryptographic keys are difficult to protect against as
they can provide the exact contents of the RAM and even the
HDD image. The works presented in this paper only protect
against methods based in software to steal keys.

keys/secrets at address offsets of well known programs can
lead to better results.
Centralized key storage employs techniques where the
generation, storage, distribution, revocation and management
throughout the lifetime of the key happens on a single central
machine. This offers advantages such as ease of backup,
recovery, and securing a single machine secures the keys.
One such commercially available solution is Thales
keyAuthority [14]. In Distributed key storage, keys are
stored on different machines. The key management system
takes care of using the disparate keys to achieve the required
cryptographic function. One such system developed by IBM
is the Distributed Key Management System (DKMS) [15].
Reducing the number of key copies present in memory is
another method to protect cryptographic keys from memory
disclosure attacks [16]. It avoids caching of the key by the
operating system and also disallows swapping of the
memory area of the key. The authors conclude that though
the number of copies of keys present is reduced, once a
sufficiently large memory dump is obtained; there are high
chances that it would contain the single copy of key. They
suggest that in order to eliminate leakage of key via memory
disclosure attacks, consumers would have to resort to special
hardware devices.

The rest of this work is organized as follows, section 2
discusses the related work in the area of key storage and key
stealing, section 3 discusses threat model for both methods,
section 4 describes the implementation of VMM based key
storage, section 5 describes disk secondary storage based key
hiding, section 6 concludes the paper.
II.

The authors in [17] present the concept of networked
cryptographic devices which are resilient to capture. It is a
server aided protocol in which the networked nature of the
device is exploited. The cryptographic operations are
performed by interactions with a remote server. In this way,
even if the networked device where the cryptographic
operations are performed is compromised, the attacker
cannot derive the entire key from it. This scheme is useful
for large firms, but not useful for end users who may not
have access to networked cryptographic devices. All or
nothing transforms have also been used to store the
transform of the key in memory [18]. This uses the concept
that the actual key is not stored directly in memory, only its
transform is stored, and the transform is such that by
obtaining partial bits of the transform, an adversary cannot
determine the actual key. The drawback with this scheme is
that for performing the cryptographic operation, the key has
to be eventually brought to memory, and malware can access
the key at that instant.

RELATED WORK

A cryptographic key has to be stored somewhere in
memory prior to the crypto operation. Due to this it is
difficult to prevent attacks that analyze memory layout to
steal keys. It has been shown in practice that without secure
clearing of data, information from kernel memory can
survive periods of days even if the system is in continuous
use [8], in addition many regions in the system do not respect
the lifetime sensitivity of the secret data [9]. This can allow
pattern matching techniques to easily extract secret keys.
One such method performs reverse engineering including
reconstruction of process lists and data structures within the
process, which hold cryptographic keys [10]. Since all binary
images of a program are the same, an attacker can extract
information about programs from one binary and use it to
attack binaries installed on other machines. Another method
to locate cryptographic keys in large data sets uses the
property that the keys have higher entropy than other data in
programs [11]. The same work also describes a method for
finding RSA secret keys stored on hard drives. Physical
memory forensics such as Volatools [12] allow attackers to
extract encrypted material from memory dumps even after it
is encrypted using commercially available software
TrueCrypt. This is achieved by analyzing the symbols
exported by Device Mapper kernel module. These symbols
are parsed to find kernel objects and symbols recursively till
the master_key member of CRYPTO_INFO object is found.
In [13] a comprehensive list of various techniques to extract
information from memory dumps is presented. The work
presents that it is difficult to extract data from a raw snapshot
of RAM, however intelligent analysis such as inspecting for

III.

THREAT MODEL

A. Threat Model for VMM based key storage
The VMM and the Host OS will not interact with the
outside world directly, and only the Guest OS will do this
operation. The main functionality of the Host OS is to run
the virtual machines and support the VMM. This allows the
VMM and the Host OS to be the trusted components in the
system. This includes the assumption that the attacker cannot
modify the VMM by tampering with or replacing the CPU,
memory or disk. The guest operating system is assumed to
be vulnerable; it could be mounted with attacks such as
rootkits, keyloggers, and so on. It runs all the end user
applications such as web, ftp and email servers, which are

1027

preventing malicious program from requesting cryptographic
operations, this also thwarts attacks like code injection where
a malicious program can inject code on the guest OS
component which normally requests the cryptographic
operations.

Guest User space (Untrusted)
All user services such as ftp, http etc run here
1

6

10

The attestation process is divided into two steps, in the
first step the VMM injects a Trusted Helper Program (THP)
in the kernel of the guest OS from which the cryptographic
request originated. The THP locates the memory and pages
of the user space program which requests the cryptographic
operation. This model ensures that only the designated
program to execute cryptographic operations requests it. The
key used to perform the cryptographic operations is stored in
the hypervisor module along with the crypto routines.
Figure 1 shows the pictorial representation of each of the
steps involved in this scheme. In step 1 the guest user space
program issues requests for cryptographic operations such as
encryption, decryption, and signatures by executing a new
system call ‘security’. The data on which the cryptographic
operation is to be performed is passed as a parameter to the
call. The system call executes a software interrupt which
causes the execution to context switch from the guest user
space to the guest kernel space. In step 2, the guest kernel
forwards the cryptographic request to the trusted VMM
using hypercalls. The hypercall executes a software interrupt
which causes the control to be transferred to the hypercall
handler. In step 3, the secure VMM receives the hypercall to
perform cryptographic operations and in response generates
code to be injected (Ci) on the running guest kernel.
Ci is responsible for obtaining the guest physical address
where the user space program that executed the security
system call is present. It is also responsible to get the user
space process’s pages into memory, in case they are swapped
out. Generating Ci ensures that it can be randomized for
every attestation instance ensuring that the operations
performed by it cannot be predicted by any malicious logic.
In step 4 the guest kernel executes Ci which returns the
address of the page where the user space program resides.
Control is transferred back to the VMM in step 5. In step 6
the secure VMM reads the contents of the user space
program directly from memory using the address obtained in
step 4. In step 7 the secure VMM computes and compares
the hash value of the memory content to pre-computed hash
values obtained from the original image of the program. In
step 8 the VMM performs the requested operation using the
cryptographic key. In steps 9 the results are written back to
the guest kernel, which writes the result into the user space
program in step s10.

Guest Kernel space (Untrusted)
Connected to the network, vulnerable to attacks
2

4

5

6

9

VMM (Trusted)
Trusted component in the system running at
highest privilege
3

DES with key

7

8

Figure 1. Key storage and attestation model

vulnerable to standard attacks. Given the vulnerability, an
attacker may attempt to obtain the physical memory dump of
the guest to observe cryptographic keys.
B. Threat model for disk image based key storage
The installation of the key storage software happens on a
clean computer. If there is already the presence of a virus or
a root kit on the machine, then the virus may be able to
hinder the installation process, make a patch on the software
being installed or obtain the key directly from the installer as
the installer program is not secure. This will render the
software ineffective. Once the software is installed the
computer can get infected with malware. Attackers will
attempt offline analysis of the software for finding its
vulnerabilities and information about how/where it stores the
keys and will use this information to crack any other
installation. Any malware attempting to break the system
does processing locally only, i.e., it does not send entire
memory/disk to remote site. Finally, no extra hardware is
available for securing the key.
IV.

VMM BASED KEY STORAGE

Memory isolation is a key property of virtualization
leveraged to safely store cryptographic keys in this solution.
This provides a feature that programs running inside a guest
VM cannot access or modify the data and programs running
in the VMM or other guest VMs. All cryptographic routines
used to perform encryption and decryption are provided by
the VMM to the guest OS. The VMM receives the plain text
or the encrypted cipher text from the guest OS and returns
the encrypted or plain text correspondingly.
The
cryptographic key is never released to the guest OS. This
prevents attackers from obtaining information about the key
using analysis of the guest OS.
This scheme also
implements a remote attestation scheme where the VMM
verifies whether the application requesting the cryptographic
procedure is a legitimate program or not. In addition to

Implementation: The VMM was implemented using
Lguest which is a linux based hypervisor. The Linux kernel
used for implementing the framework was 2.6.23-rc9 on
Ubuntu. The same kernel build was used to create the guest
and the host OS. QEMU was used to install Ubuntu on a
disk image for the guest OS. A new entry ‘.long
sys_security’
was
added
in
the
‘/usr/src/Linux/arch/i386/kernel/syscall_table.S’ to create a
system call ‘security’ for the guest kernel. Similarly, the

1028

TABLE I.

OPERATING TIMES ON GUEST OS

Time for one round of
encryption (micro sec)

Time for injection and attestation
(micro sec)

31

9

encryption on the Host OS takes 8 microseconds, the actual
cryptographic operational time was measured to be 5 micro
seconds, and hence the communication and data overhead
inside the Host is estimated to be 3 microseconds. Table 2
shows the times as measured by the guest process. Given
that one round of cryptographic operations take 5 micro sec
on the Host, and the time for attestation and code injection
is measured as 9 micro seconds, the estimated kernel
switches and hypercalls can be estimated to be 17 micro
seconds (31 – (9+5)). This is understandably higher than
the host, since guest systems are slower due to mechanisms
such as shadow page tables.

‘unistd.h’ file was changed to show the new security call
‘#define __NR_security 325’, and the number of system
calls was changed to 326. In ‘Linux/syscalls.h’ a line
‘asmlinkage int sys_security (char *input, char *output, int
operation)’ was added to declare the system call. The
security system call was implemented to invoke two
hypercalls, hcall(LHCALL_ENCRYPT,__pa(k),__pa(q),0),
hcall(LHCALL_DECRYPT,__pa(k),__pa(r),0). __pa is a
kernel routine used to convert virtual addresses to actual
physic al addresses. This macro has to be applied to the
pointers being passed since they actually point to addresses
within the guest address space and hence the VMM needs
the actual address location to read and write to that address.
TABLE II.

V.

This section presents a technique to store encryption keys
in secondary storage media without the use of external
hardware and without using any password based encryptions
schemes. This scheme cannot prevent attacks that steal keys
from RAM. RAM based attacks are addressed in the
virtualization based key storage approach previously
described. This scheme is aimed at preventing attacks which
identify location of keys in the secondary storage and steal
them. To mitigate the problem of keys being stolen from the
running copy of a process (as in HD-DVD encryption), this
scheme incorporates randomization of the entire binary
(
crypto) which performs the cryptographic operation. The
location inside crypto where the key is stored is modified for
every installation of crypto. This implies that the location of
the key will be revealed to the attacker (Mallory) only if the
system is compromised, however, it still prevents Mallory
from using this information to steal keys from a different
installation of crypto. This serves to decrease the ‘return of
investment’ for Mallory, as the entire analysis must be
performed on every instance of attack. In conventional off
the shelf binaries every copy of the binary is the same, which
makes stealing of keys simpler for Mallory. Mallory has to
analyze the structure of the off the shelf binary in a
controlled environment and determine which locations in its
memory cryptographic keys get loaded. Once this is
determined, this information can be used to steal keys off
every other installation of the binary.

OPERATING TIMES ON HOST OS

Time for one round
of encryption
(micro sec)

Time for crypto
operations
(micro sec)

Host communication
(micro sec)

8

5

3

STORING KEYS IN SECONDARY STORAGE MEDIA

The hypercall transfers control from the guest kernel to
the VMM.
The hypercall uses the kernel routines
copy_to_user and copy_from_user to perform data transfer
on these addresses. The two hypercalls were implemented to
perform the major cryptographic API operations invoked.
crypto_alloc_ablkcipher initializes a cipher transform for
DES. crypto_ablkcipher_setkey sets the key to be used
during the operation, the key is declared in the VMM.
crypto_ablkcipher_encrypt and crypto_ablkcipher_decrypt
are calls to the encryption and decryption routines of the
cryptographic API framework which in turn invokes the
DES encrypt/decrypt routines. crypto_free_ablkcipher and
ablkcipher_request_free are used to perform the cleanup
operations. The guest kernel is created with a empty space
routine filled with ‘nop’, when the guest kernel makes the
cryptographic hypercall, the hypervisor injects code at this
location. The injected code provides the address of the user
space program that called the ‘security’ system call to the
hypervisor. The hypervisor obtains the memory addresses
and reads the contents, hashes them, and compares it to a
pre computed hash value to determine if the user program
executing the cryptographic system call is the designated
application to perform it. If the program is indeed the
legitimate program, the hypervisor performs the required
cryptographic operation and returns the results to the guest
kernel, which in turn returns the results to the guest user
land application.
The system was implemented on machine running on a
Intel® Pentium® dual core processor with 1GB Physical
RAM. The time taken for cryptographic operations on the
Host OS is shown in Table 1. The data encrypted was 8
bytes long, and the algorithm used was DES. One round of

The cryptographic keys are themselves stored in the
unused portions of the last sectors of files on the file system
without using the file open system call of the operating
system. Within this storage space the key bits are scattered
all over in a pattern which is unique to every installation of
the software. The pattern is generated during installation of
crypto. The keys are written to the raw disk using the ‘/dev/’
interface in Linux. The key is scattered in a storage area in a
random pattern which is different for each installation, the
storage area is called scattered array. A special block of
code is generated during installation to retrieve the key
stored in the scattered array. The execution of this code
results in the location of the scattered array being generated.
The locations are then used to retrieve the key. The block of

1029

code generating the location of the scattered array is called
the storage fetch block. The portion of code generating the
location information is called the bit-address fetching block.
Thus the root of trust here is the hard coding of fetching
information (both about the location of the storage on disk
and the exact pattern of scattering in the area designated for
storage) in the code section of the executable. To make sure
that there is no underlying pattern matching the fetching
codes for different installations; these blocks of codes are
generated randomly and are padded with random “junk”
code for obfuscation. Additionally, the location of this
fetching code within the software is different for each
installation which makes it even more difficult to generate an
attack scenario which encompasses multiple installations of
crypto. To make sure that the attacker cannot exploit the
software by hijacking the fetching code to execute in a
controlled environment, we use self attestation inside the bitaddress fetching block.

.ELF

Storage fetch
block
Fetch scattered array

fn fetch key:

Code block to
generate jump
address
Jump
Construct key

Fig. 2 shows the structure of the modified binary after the
installation process is complete. The installation suite (s)
generates the locations on the raw disk to store the scattered
array. s next generates the storage fetch block and inserts it
inside crypto. s also places a call to the storage fetch block to
generate the required inode/sector information where the
scattered array is present. The scattered array is fetched to
the program memory prior to cryptographic operations when
the bit-address fetching block is executed. Since the bitaddress fetching block can generate the location of the key,
Mallory will attempt to locate this block. The location of the
bit-address fetching block is obfuscated by removing it from
sequential flow, the block is reached after the execution of a
JMP statement whose target is generated by a block of code.
At the end of the block, the location jumps back to the
cryptographic procedure.

Use key in application

Bitaddress
fetching
blocks

Compiler data
started here before
modification

Jump back
Fig 2. Structure of modified application crypto

dummy files (in the order of hundreds) in addition to the
key storing file. This will make it difficult for the malware
to know which file has the key even if the malware is able to
track the disk reads executed by the software. As the names
of the key bearing files which were given by the user, are a
subset of the permutation, it is difficult to distinguish the
user made file from the rest of the machine made files.

There are two possible techniques to store the keys on the
disk. The first method tried out during this research was to
remove a block from the free list of OS file system and use it
for storage. However, utilities like fcsk found out orphan
disk blocks and put them back in the free list. Due to this, no
claim can be made to how long these files will remain
pristine. Instead, this research implemented a technique of
storing keys on the last disk block sector of some files. The
OS always allocates a fixed chunk of disk area to be used for
a newly created file. It does not reclaim the space beyond
the end of file (EOF). The space beyond EOF on this sector
can be used to store the key. In order to make sure that this
file is never changed or appended, special files are created
during the installation of the software and the user is asked to
assign names for these files.

Each file created is filled with random data to allow 200
to 400 bytes of space left in the last blocks of the file after
the EOF. This space is used to store the cryptographic keys.
Figure 3 shows an instance of a scattered array. The location
of a bit of the key in the scattered array is termed as the bitaddress in the rest of this work. Every bit of the key has a
bit-address associated with it which forms the bit-address
pattern.
The bit-address pattern is unique to every
installation of the key storage system. If bit 0 of the key is
present at the 11th position of the scattered array then the bitaddress of bit 0 is 11. The bit address pattern for the figure
will be 11, 1, …, n (24th bit), …, 5 (bit 128), and so on.
The bit-address pattern cannot be stored directly as it
may result in Mallory obtaining the pattern. Instead of
storing the bit-address pattern, a bit-address fetching block is
created which generates the address of each bit at run time.
One logical block of code is generated for every bit-address
to be calculated during installation. Hence if there are 128
bits then we have 128 bit-address fetching blocks. The bitaddress is generated prior to generating the fetching block

We do not assign software generated random names to the
files containing the key. This is because random software
generated names either have patterns which can be easily
detected or are too random to give away the fact that it is not
human readable. As the user knows that this file will be
used to store the keys, it can be assumed that the user will
not modify it. By permuting characters present in the names
given by the user, the installation suite generates many

1030

1

2

3

Bit 1

4

5

.........

11

Bit 0

Bit 128

.........

n

Bit 24

Key Bit 0 is stored at location 11
Key Bit 1 is stored at location 1
Key Bit 24 is stored at location n
.......
Key Bit 128 is stored at location 5
Fig. 3. Scattered Array

for the number of operations to generate an address. 
operations generate a bit address such that min    max,
where  is not a fixed number. Each operation is one of
seven basic mathematical operations (ADD, SUBTRACT,
MULTIPLY, DIVIDE, AND, OR, NOT). During bit
address fetching block generation the values of registers and
memory locations used is initialized and one operation is
chosen to be performed. The result of this operation is
tallied against the required value of the bit-address, if the
result differs from the required bit-address value, further
operations are applied. If the result of these operations
reaches a range of ± 5% of the desired value, and min   
max, the code generation is stopped and a last operation is
added to make result becomes equal to the desired value.
The last operation is either an addition or a subtraction
operation on the result. If  < min mutually cancelling
operations are added till the bit-address value is generated.

for the address. This means that the fetching block must
evaluate to a specific value. This is achieved by generating
an initial value and then performing a series of random
arithmetic operations on it till the desired bit-address value is
generated. This bit-address fetching block is made using
different machine level arithmetic operations. If the bitaddress fetching block is located at the same location in
every instance of installation of crypto, an attacker can figure
out the address from one instance of the installation and
break the secret. To avoid this, the bit-address fetching block
is moved around during installation. In addition, the ‘junk’
code present inside it serves to modify its size and location.
Implementation: The execution of bit-address fetching
block results in the formation of the bit-address in a
predetermined register/memory address which is then used
for accessing the bit from the scattered array. Construction
of the block is done by first setting a min and a max bound
TABLE III.

If max < , the block generation is restarted. This scheme
can generate bit-addresses as well as inode number or the
sector number where the scattered array is stored on the
secondary storage media which in this case is the hard drive.

DATA FROM DIFFERENT INSTALLATION INSTANCES

Position
of fetch
block

Location of
jump to fetch
block

Location of
attestation
block

Hash value
generated

0x21B7

0xCF7

0x255D

0x145F0A

0x21B7

0xCF7

0x2317

0x67977

0x21C0

0xCBB

0x23C7

0x3ACCA

0x21C0

0xCBB

0x2542

0xF08AE

0x21CE

0xCE5

0x244E

0x4AB06

0x21CE

0xCE5

0x272E

0x13D852

0x21DA

0xCC5

0x23A3

0x30697

0x21DA

0xCC5

0x25C6

0xE5BF3

0x219D

0xCEB

0x23AD

0xA2D

0x219D

0xCEB

0x2232

0x63B7

0x21B2

0xCFD

0x2497

0xEAE06

0x21B2

0xCFD

0x21EC

0x66C3E

0x21B1

0xCF2

0x2248

0x489E0

0x21B1

0xCF2

0x2594

0x115E85

The location of the bit-address fetching block can be
revealed by the presence of a jump/call instruction to this
block. The target address for the call to the bit-address
fetching block is generated during execution by a small
block that is similar to the bit-address fetching block to
prevent this scenario. The location of the block which
calculates the target address is also randomized in the binary
and padded with many junk calculations to obfuscate the
calculations. To prevent any malicious code from executing
the fetching blocks, self attestation is performed on the
running image of the executable. The code computes hashes
on sections of its process image and compares the results
with the expected results hardcoded inside it. The attestation
covers the fetching blocks and the application within which
it is executed. A simple inline hash function is used to avoid
the hash call from being observed on process tracers and
tools like objdump. It can be considered difficult for an
attacker to change the hash values stored in the binary as
their locations are randomized in each installation of the
system. Multiple numbers of such attestation codes are
inserted in crypto, each attesting a different section of crypto.

1031

This framework was implemented on Ubuntu 8.04 Linux
OS using Intel® Pentium® 4 processors executing with 1
GB of RAM. A 32 bit key was utilized for testing the
framework. The key hiding application was provided 2 selfattestation blocks, and the size of each bit-address fetch
block was set as 80 bytes. A scatter array size of 2Kb was
used to store the keys, and the entire application generated
was 14KB in length. This implementation uses a few simple
obfuscation techniques to keep the implementation simple.
Table 3 provides a short summary of results obtained for the
scheme. It shows that even with the margin of as small as
0x50 bytes, a good measure of randomness was obtained in
the system. This shows that any malware which tries to
attack the application cannot do a remote analysis and use
the information gained there to attach another instance of the
installation of the same application.
VI.

REFERENCES

[1]

[2]

[3]
[4]

[5]
[6]

CONCLUSION

[7]

Cryptographic keys on systems are a target of attacks for
malware writers. Malware may use a variety of techniques
including digital forensics to extract keys from an end user
machine. The lack of randomness in cryptographic systems
in terms of memory and lack of memory isolation among
various processes in an OS provides opportunities for
malware to steal keys. This paper presents methods for
storing keys inside a Hypervisor and in secondary storage
medium. A Hypervisor isolates the memory of a guest OS
from other guest systems and the Host OS/Hypervisor. The
cryptographic keys are always stored in the Hypervisor, this
results in a system where a compromised guest OS can
perform cryptographic operations without exposing the key.
The second technique effectively randomizes the location of
a key inside every machine making it difficult for malware to
obtain the key from secondary storage media without
extensive case by case analysis. This work can be extended
such that the hypervisor uses the disk hiding scheme to store
its keys.

[8]

[9]

[10]
[11]

[12]

[13]
[14]
[15]

VII. ACKNOWLEDGEMENTS
[16]

This material is based upon work supported in part by the
National Science Foundation under Grant No. CNS1011931. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the
author(s) and do not necessarily reflect the views of the NSF.

[17]

[18]

1032

T. J. Ostrand and E. J. Weyuker, "The distribution of faults in a large
industrial software system," in Proceedings of the 2002 ACM
SIGSOFT International Symposium on Software Testing and
Analysis, 2002, pp. 64.
A. Chou, J. Yang, B. Chelf, S. Hallem and D. Engler, "An empirical
study of operating systems errors," in Proceedings of the Eighteenth
ACM Symposium on Operating Systems Principles, 2001, pp. 73-88.
Web link. TrueCrypt - free open-source on-the-fly encryption.
Retrieved October 4 2010. www.truecrypt.com
Web link. BitLocker. Retrieved October 4 2010.
http://windows.microsoft.com/enUS/windows7/products/features/bitlocker
Web link. CAC: Common access card. Retrieved October 4 2010.
www.cac.mil
Web link, "Hi-Def DVD Security is bypassed," vol. 2009. Retrieved
January 26 2007. http://news.bbc.co.uk/2/hi/6301301.stm?lsm
Web link, "Hackers discover HD DVD and Blu-ray processing key all HD titles now exposed," Retrieved February 13 2007.
http://www.engadget.com/2007/02/13/hackers-discover-hd-dvd-andblu-ray-processing-key-all-hd-t/
J. Chow, B. Pfaff, T. Garfinkel and M. Rosenblum, "Shredding your
garbage: Reducing data lifetime through secure deallocation," in 14th
Conference on USENIX Security Symposium, 2005, pp. 331-346.
J. Chow, B. Pfaff, T. Garfinkel, K. Christopher and M. Rosenblum,
"Understanding data lifetime via whole system simulation," in 13th
USENIX Security Symposium, 2004. 2004, pp. 321-336.
T. Pettersson, "Cryptographic key recovery from linux memory
dumps," in Chaos Communication Camp 2007.
A. Shamir and N. v. Someren, "Playing ‘hide and seek’with stored
keys," in Third International Conference on Financial Cryptography,
1999, pp. 118-124.
A. Walters and N. Petroni, "Volatools: Integrating volatile memory
forensics into the digital investigation process," in Black Hat DC,
2007.
T. Vidas, "The acquisition and analysis of random access memory,"
Journal of Digital Forensic Practice, vol. 1, pp. 315-323, 2006.
Anonymous "Thales ISS keyAuthority," vol. 2010.
Anonymous "IBM security: IBM distributed key management system
(DKMS)," IBM.
K. Harrison and S. Xu, "Protecting cryptographic keys from memory
disclosure attacks," in 37th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks, 2007, pp. 137143.
P. MacKenzie and M. K. Reiter, "Networked cryptographic devices
resilient to capture," in IEEE Symposium on Security and Privacy,
2001, pp. 12-25.
R. Canetti, Y. Dodis, S. Halevi, E. Kushilevitz and A. Sahai.
Exposure-resilient functions and all-or-nothing transforms. Presented
at 19th International Conference on Theory and Application of
Cryptographic Techniques.

Trusting Routers and Relays in Ad hoc Networks.
Prashant Dewan and Partha Dasgupta
Arizona State University
dewan@asu.edu, partha@asu.edu

Abstract
The current generation of ad hoc networks relies on other
nodes in the network for routing information and for routing
the packets. These networks are based on the fundamental
assumption that the nodes will cooperate and not cheat. This
assumption becomes invalid when the network nodes have
tangential or contradictory goals.
A novel method of enhancing routing strategies, and
enhancing cooperation is to use “reputations” computed
from peer recommendations. Reputation assignment and use
cajole cooperation from the nodes in the network even if they
do not share the same goal. This paper provides a
mechanism that can use reputations in ad hoc network for
trusting routers and relays. In addition, it enumerates the
issues involved in using reputation in ad hoc networks.
The simulations show that the throughput of the network
increases by 0% - 71.6% when the neighbor reputations and
shortest path are considered, for deciding the next hop. The
throughput of the network improves from 3% to 143% when
the next hop of the packet is decided using only reputations
and ignoring the shortest path. The average hop length is the
same irrespective of the fact that reputations are used.

1. Introduction
Wireless ad hoc networks are self-configuring, adaptive
networks, which can be deployed in areas deprived of any
existing network infrastructures. As mobile ad hoc networks
do not need any infrastructure to deploy these networks
rapidly. These networks are generally used in situations of
crisis, where the existing network has been mutilated due to
natural or organized calamities. Besides, these networks are
used in places where the existing infrastructure is not
available because of monetary or strategic reasons. For
example, they can be used in battle zones, villages and in
areas suffering from natural calamities.
The current research on efficient routing in mobile
networks [Perkins et al. *1999], [Park et al. *1997] makes the
fundamental assumption that all the nodes in the network
have the same or, at least, similar goals; and hence it needs to
cooperate with other nodes in the network. However this
assumption does not hold in certain scenarios. A node of an
ad hoc network may be compromised. Compromised nodes
become antagonistic to un-compromised nodes; and

therefore, they are not reliable for retrieving the routing
information or for actual routing.
Another example of ad hoc networks is a network
where the entity controlling the nodes has disparate goals.
Such a network needs external motivation to make the
nodes of the ad hoc network cooperate. In addition, this
mechanism should be able to weed out rogue or
compromised nodes and inject a certain level of inter–
node trust. Network nodes can use this trust to estimate
the probability that the packet that they send will
eventually reach its destination.
Several incentive mechanisms have been proposed in
[Obreiter et al. *2003] where the authors classify the
incentive mechanisms as account based and reputation
based mechanisms. Account based mechanisms are based
on the assumption that the nodes in an ad hoc network try
to accumulate the maximum amount of micro payments
and then exchange it for real world currency.
Account based mechanisms do not appear to work in
collaborative groups of nodes in the ad hoc network,
where the nodes set common goals. The nodes in such
collaborative networks try to achieve a common goal and
not to make money. Such techniques can be used in noncollaborative ad hoc networks where the network does
not have one goal but several with tangential goals
belonging to the members of the network. In such
situations micro payments can be cashed for real world
money and hence motivate the network nodes to
cooperate and make money.
The reputations of the nodes can be used for
instilling the motivation to cooperate in the nodes. This
motivation also leads to the establishment of trust and
confidence among the nodes. The reputation of a node
also motivates it to act in a trust worthy fashion and not
to maliciously tamper with any packet. If a node becomes
indifferent to its reputation and continuous to act
maliciously, it is weeded out of the network.
Ad hoc networks do not depend on the presence of
any centrally trusted authority. Hence there is a need for
distributed protocols which facilitates the nodes in an ad
hoc network to collect, store and manage the reputations
of other nodes. In addition, the nodes should be able to
quickly use the reputation information to take routing
decisions without having a significant impact on the
routing performance.

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

2. Design Goals
1.

2.

3.

4.
5.

In this research, the following goals were set.
The system should be robust to collaboration attacks. If
a certain number of nodes are compromised, they should
not able to collaborate and bring the whole network
down.
The reputation information should be easy to use and the
nodes should be able to ascertain the best available
nodes for routing without requiring human intervention.
The system should have a low performance cost because
low routing efficiency can drastically affect the
efficiency of the applications running on the ad hoc
network.
Nodes should neither be able to fake nor manipulate
their reputations in the system.
Nodes should be able to punish the wrong doer by
providing him with a bad reputation

3. Overview of Reputations & Routing
A reputation can be defined as an import of the past
behavior of an entity. It has been used by the entities, in the
brick and mortar world to find out the probability of
achieving the desired level of satisfaction from their
transaction with the other entity whose reputation is being
evaluated. The reputations were used in the networks in
general, and Internet in particular, only in the late 1990s. An
example of a successful reputation based system is Ebay.
Although in the brick and mortar world, the reputations
have been used subjectively, such representation cannot be
used in the digital world, like the Ad hoc networks. Hence we
assign numerical values to reputations in order to make them
objective and hence more usable. A more exhaustive
description of reputations and their use in networks in
general, and self-configuring networks in particular, can be
found in [Dewan et al. *2003].
Routing in ad hoc networks has been well researched.
An exhaustive overview of the different routing protocols
used in ad hoc networks is provided by [Royer et al. *1999].
The current methods of routing use the “ask the neighbor”
method recursively, till they reach the destination or another
node, which has the route to the destination. Furthermore
these routing techniques are classified into Table Based
Routing, and On Demand Routing. The table based routing is
used for updating the routes periodically. The nodes store the
routes to all the destinations while the on demand routing is
used for updating the routes on demand.
Before sending a packet to its destination, a node has to
know, at least, the possible next hops for the destination of
the packet. In addition, the node needs to know the metric
that shows the cost involved in sending the packet via that
path. For example, the path length can be such a metric. This
paper proposes the use of reputation of the next hop node
besides the path length, to choose the path to be followed by
the packet. In other words, the node performs a trade of

between the shortest path and the neighbor reputation.
Also, the current routing protocols make the
assumption that the nodes will not cheat. ARAN
[Sanzgiri et al. *2002] is a security protocol in which
makes non-cooperation difficult by using cryptographic
techniques like digital certificates.

4. Definitions
In section 3.1 we define some of the more commonly
used terms in this paper.

4.1 Recommendation
The recommendation is the value assigned to the
service provider by the service seeker during a
transaction. All the recommendations of the service
provider are combined to evaluate its reputation.
Recommendations can be contextual for a given node.
For example, a node can get different recommendations
for its availability, accuracy, and efficiency.
In this paper, we consider only one context: whether
a given node is a “good” router or a “bad” router. In most
cases a good node routes the received packets to the next
hop even if it has no vested interest in the packet. A bad
node maliciously drops the packets or tampers with the
contents of the packet or routes it in the wrong direction.
For example, consider a network that consists of
three nodes
AÆ BÆ C
If node A wants to send a message to node C and it
finds out that the only way that it can send the message to
node C is via B. It sends the message to B, which in turn
routes it to C. If C acknowledges receiving the message
to A, A can deduce that B routed the message properly
and hence gives B a recommendation of +1.
Mathematically, the recommendation will appear as
follows,
RepAB= +1

4.2 Reputation
Mathematically, reputation is the mean of the
recommendations received by a node. Suppose node B
received 100 packets and routed 80 packets but dropped
20 packets, the senders of routed packets give him +1 and
the senders of the dropped packets give him –1. Hence
his total reputation becomes
RB= (80-20)/100= 60/100= 0.6

4.3 Node Identifier
Each node possesses a certificate, which was issued
to it when the network was established. No node
possesses multiple identities. The reputation is assigned
to the only identity that the node possesses. The nodes in
the network can easily verify the identity of a particular
node in the network by using challenge response

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

mechanisms.
If a node gets compromised and does not co-operate, its
reputation goes down quickly and soon it is ostracized and
weeded out of the system, even if it possesses an authentic
identity.

4.4 Threshold Reputation
The threshold reputation Rthresh is the minimum
reputation a node expects from a possible next hop on a path.
If the next hop node does not possess the requisite reputation,
only then the source node sends a packet to a node whose
reputation is less than the threshold.

5. Router Reputations
In the proposed technique all the nodes in the network
use the reputations of their neighboring nodes to find out the
best node, to which the packet should be forwarded.

5.1 Finding Trusted Routers
Each node in the system possesses a reputation. When it
joins the network with a certain identifier the reputation of a
node is one, which is the maximum possible reputation.
During the course of time, the reputations of the nodes
decrease or remain unchanged. They are based on the number
of packets forwarded by the node.
We consider three scenarios here for deciding the next
hop for a packet. In the first scenario, only the shortest path
to the destination is used. In the second scenario, the shortest
path to the destination and reputation of the neighbors are
used. In the third scenario, only the reputations of the
neighbors having a path to the destination are used.
In scenario 2, once the routing information is available, a
node chooses the next hop, which provides the shortest path
to the destination. At this point we add another factor to the
routing decision, reputation. The node sorts all the possible
paths to a given destination by using the length of the paths
to the destination, which is followed by the reputations of the
next hop nodes. The sender then selects the first M available
next hops having a reputation greater than the threshold
reputation Rth from the sorted list. It then sends M copies,
numbered from 1 to M of the packet on these paths. The node
which receives this packet, sorts its available next hops by
shortest path to the destination and reputations and sends it to
the first node with a reputation greater than Rthresh .
The important thing to note here is that although the first
node sends M copies, the subsequent nodes only send one
copy each. In addition, M should be as small as possible
because an increase in M will lead to a multifold increase in
the traffic. In our simulations, we use M=1. The packets have
identification numbers called PID and hence any intersecting
paths merge. The merging nodes send the list of the numbers
of a given packet received by the node.
Once the packet reaches the destination, it makes a list of
the numbers of copies that it received and sends it to the
sender. After the sender receives the packets, it finds out the

next hops of the successful paths; and gives the
recommendation of +1 to the next hop node(s) in the
network and a recommendation of –1 to the next hop
nodes of the paths, which failed. The next hop nodes pass
on their recommendations and give a recommendation of
–1 to their next hop if the sender gives them –1.
In scenario 3, a node generates the list of its
neighbors having a path to the destination. The node sorts
the neighbor list on the basis of their reputations. It
selects the top M nodes from the list and sends the packet
to these M nodes. The sender then selects the first M
available next hops having a reputation greater than the
threshold reputation Rth from the sorted list. It then sends
M copies, numbered from 1 to M of the packet on these
paths. The node, which receives the packet, sends one
copy of the packet to the node having the highest
reputation in the set of neighbors having a path to the
destination.
In other words, the responsibility of making the
packet reach its destination relies on the next hop node of
the sender. If the packet is dropped anywhere in the path
between the source and the destination, the next hop node
of the source gets a bad recommendation which can be
cascaded to all the nodes in the path. In order to maintain
high reputations, the intermediate nodes in the path, treat
the packet as their own and forward it to the next possible
hop till it reaches the destination.

5.2 The Incentive
Due to the autonomous nature of the nodes in ad hoc
networks, they need incentives to maintain their high
reputation levels. The nodes that have reputation levels
lower than the threshold are chucked out of the network.
The packets sent by nodes with higher reputation levels
get a priority over the packets sent by nodes with lower
reputation levels. Hence, nodes with higher reputations
experience a lower latency compared to nodes with lower
reputation assuming all other conditions remain constant.

5.3 Recommendations Management
Managing recommendations in the network is a
difficult task, which entails collecting, storing and
deducing the reputations from the recommendations.
There are two models for managing reputation. In the
first model, the recommendations are stored on various
nodes of the network; and any node can search for any
other node’s recommendations using the public key of
the other node. In the second model the recommendation
receiver stores the recommendations that it receives and
presents them to other nodes that wants to know the
reputation of this node.
The advantage of the first technique is that, it is
harder to maliciously manipulate the reputations because
they are not stored at the node-which is effected by them.
The disadvantage of this technique is that, any node.

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

.

Figure 1
looking for the recommendation of another node, needs to
search of the order of O (log N), where N is the number
of nodes in the network. This search can be expensive if
the size of the network is large.
The advantage of the second approach is that any
node looking for the reputation of another node can get
the recommendations directly from the owner node, for
example, the node that is affected by the
recommendations. The disadvantage of the second
approach is that it becomes easier to maliciously modify a
recommendation. Hence heavy weight cryptographic
mechanisms are needed in order to prevent any malicious
tampering.
In the simulation we assume the first model. In
addition we assume that the nodes do not falsify their
reputation.
A more detailed discussion of both the techniques
can be found in [Dewan et al. *2003]. In short, the
tradeoff is between the amount of resources available to
each node and the potential size of the ad hoc network.

5.4 Discussion
5.4.1
Motivation for Autonomous nodes. The nodes
in an ad hoc network are more or less autonomous. Hence
this technique motivates them to allocate their resources
to other nodes in the network. The ad hoc networks are
built upon the assumption that the nodes will cooperate.
5.4.2
Increased Throughput. As the sender only
sends the packet to highly trusted nodes, based on their
history, it lowers the risk that its neighbors will
intentionally drop the packet. The neighbors in turn
forward the packets to nodes trusted by them. As a result
the number of intentionally dropped packets is reduced
and hence the throughput of the system is higher.

5.4.3
Non-cooperative Nodes are Ostracized. Noncooperative nodes are slowly weeded out of the network.
A node held by an individual who does not want to
cooperate with the other nodes in the network or nodes
which were earlier cooperating but were compromised in
a battle and became non cooperating start accumulating
negative reputation. As a result most of the rogue nodes
i.e. the nodes having low reputation do not receive any
packet, and hence cannot inflict any damage by dropping
packets.
5.4.4
Poor Nodes are Penalized. Some nodes in the
network might be unable to route packets for other nodes
due to the scarcity of resources. Such nodes get lower
reputation because of this disability and might be
subsequently weeded out for no fault of theirs.
This problem is solved by attaching a list of resources
available to the given node in its certificate. This class of
nodes is penalized for failing to route packets- but the
penalty inflicted on them is a fraction of what is inflicted
on a node with a large volume of resources.
For example, consider node A, which possesses half
the memory and half the processing power of most of the
other nodes in the network. If node A fails to route a
packet sent to it by some other node, it gets a
recommendation of –0.5 instead of -1. In this way the
system is democratized.
5.4.5
Good Nodes become a bottleneck. The good
nodes receive more and more packets as their reputation
increases. As a result the good nodes are not able to
handle the amount of load received by them. Hence they
drop some of the packets and their reputation goes down.
The senders of the packets use the second rank nodes to
route their packets. Slowly the system comes to
equilibrium.

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

5.4.6
PKI is Expensive and Routing is Resource
Intensive. Nodes in a mobile ad hoc network are severely
constrained by resources like memory and power. They
may not have the resources to perform expensive
cryptographic operations like PKI. Even if the protocol is
useful some of the nodes might not have the resources to
execute it. This is one major bottleneck that needs to be
circumvented

6. Algorithm
In this algorithm we assume that the nodes already
know their neighbors. The neighbors of a node are
defined as the nodes within the transmission range of the
source node. Neighbor discovery can be done via
broadcasting or by asking other neighbors. Initially the
reputations of all the neighbors are 1, i.e. good.
This algorithm is independent of the routing scheme
used. The only requirement is that the current node should
know the route to the destination from itself. It also
assumes that the nodes store their own recommendations
and the recommendations are not stored by any central
agency. Before executing this protocol, the node carrying
the packet collects the reputations of its neighbors. The
reputations are only based on the previous routing history
of the node and do not consider the possibility of
malicious tampering of the packet by uncooperative
nodes.

Destination using any standard routing
technique like AODV.
Step 2 If C is uncooperative drop
the packet
Step 3
(Scenario 2) C-> Find the first
neighbor in the shortest route to the
destination whose reputation is above
the threshold, Rthresh
(Scenario 3) C-> Find the neighbor
in
the
set
of
routes
to
the
destination
having
the
best
reputation.
Step 4 If the neighbor is found
go to Step 1
Step 5 If the neighbor is not
found, sends it to the uncooperative
neighbor of the highest reputation.
Done

If ACKNOWLEGEMENT Received
S->Grant +1 to the NEIGHBOR

7. Example
This example emulates scenario 2. In figure 1, the
Source wants to send a packet to the Target. The value of
M, number of paths established by the source with the
target is 2 and the minimum amount of reputation
expected, Rthresh=0.5. The nodes 3,8,10,11,15 are rogue or
compromised nodes, which have a history of noncooperation. In other words their reputation is below the
threshold reputation.
Using any routing protocol, it finds out that its
possible next hops for reaching the Target is 1, 12,14, and
15. Out of the 4 possible neighbors, 1 and 15 need 7 hops
to reach the target while 12 and 14 need 5 hops to reach
the target. The node sorts the list on the basis of the
number of hops required to reach the target. The sorted
table is shown as Table 1.

IF No ACKNOWLEDGEMENT Received
S->Grant -1 to the NEIGHBOR
Update Reputation of the Neighbor
Algorithm
Next Hop

Reputation

12
14
1
15

0.4
0.5
0.6
0.2

Let the current node
packet is be denoted by C

where

the

S-> Copy the source node, S to the
current node, C.
While (C <> D)
Do
Step
1
C->Find
Route
to

to

Table 1

Let Source be S and Destination be
D

Hops
Destination
5
5
7
7

It selects nodes 14 and 12. It sends the packet to node
14 since it provides the shortest path at a reputation above
the threshold. It needs to select one more next hop from
nodes 12,1 and 15. It does not select 12 because its
reputation is below the threshold level. For the same
reason it does not select 15. It selects node 1. It makes
two copies of the packet and numbers them. The first
packet is sent to node 14 and the second packet is sent to

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

node 1.
Node 14 sends it to node 16, which routes it to 20
and subsequently to the target. If reputation had not been
used it would have sent it to 10 and because 10 is a rogue
node, the packet would have never reached the target.
The packet, which was sent to 1, reaches 3 via 2. As
node 3 is a rogue node, it drops the packet. Once the
target receives the packet, it acknowledges the packet
number to the source. The source finds out that the packet
sent to 1 never reached. Hence it gives a recommendation
of –1 to node 1 and +1 recommendation to 14. The
recommendations get cascaded and nodes 2 and 3 get a
recommendation of –1 while 14,16,17,18,19,20and 21 get
a recommendation of +1. The update reputation table is
shown in Table 2
Next Hop
12
14
1
15

Old
Reputation
0.4
0.5
0.6
0.2

New
Reputation
0.4
0.75
-0.2
0.2

Hops
to
Destination
5
5
7
7

Table 2

8. Simulation
8.1 Setup
The simulation is performed on a 2.4 GHZ, Linux
Red Hat machine using C language. Two scenarios are
simulated. The first scenario is routing performed without
using the reputation scheme that is proposed in this paper,
and the second scenario is the routing performed with the
reputation scheme. The third scenario is routing
performed without calculating the shortest path and using
the reputation scheme.
The main focus of the simulation is to compare the
throughputs of the system in all the scenarios i.e. compare
the number of packets that reached their destination and
the number of packets dropped, in the above scenarios.
The second important parameter is the average number of
hops in the network in the above scenarios.
The simulator is designed as an N*N matrix of nodes
where N=100, and L= 500 links between them. The links
are randomly generated. Each node keeps track of its
neighborhood nodes and its reputation. The value of M,
the number of copies of the same packet sent by a node, is
set to 1. Hence if a packet reaches a node whose all
neighbors for a given route are not cooperative, the packet
is sent to one of the uncooperative nodes, which
subsequently drop it. The threshold reputation is set to
0.7. Any node whose reputation falls below threshold
during the simulation is considered to be uncooperative.
P=1000 packets are sent from randomly selected sources
to randomly selected destinations.

Initially it is assumed that all the nodes are
cooperative. In the subsequent simulations more nodes
are made uncooperative till the number of cooperative and
uncooperative nodes is equal. If the packet reaches its
destination, all the intermediate nodes get a
recommendation of +1. If the packet is dropped then all
the nodes in the route before the node that dropped the
packet get a recommendation of –1. The reputation of a
node is the average of the recommendations received by
it.
One important negative effect of reputations, which
has not been measured in this simulation, is the number of
nodes whose reputation went below the threshold,
because of the fact that they did not have sufficient
resources to forward packets. Table 3 shows the important
parameters

8.2 Simulation Analysis
The following salient observations are made from the
results of the simulation.
1. The number of packets reached is always more in
when the reputations of neighbors is used for
routing. When the number of uncooperative nodes
is zero then the numbers of packets that reached
their destination is same in both scenarios.
2. Using reputations along with shortest path
information to the destination increases the
throughput of the system to almost 71% when
50% of the nodes are rouge. This is because the
packet is now routed via cooperative nodes, which
do not have a history of dropping the packets.
Hence the number of packets that reach their
destination is higher.
3. When reputations are used for the selection of the
next hop without using the shortest path
information, the number of packets that reach their
destination increases by 143% when 50% of the
nodes are rouge.
4. The increase in the throughput is explained by the
following example. Consider three nodes Alpha,
Beta, Gamma. Beta is the shortest path from
Alpha to Gamma. Here Beta is initially a good
node but turns uncooperative. Supposed Alpha has
to send 5 different messages to Gamma. When
Alpha sends the first message, Beta drops it.
Hence Beta’s reputation reduces below the
maximum. But its reputation is still above the
threshold. In scenario 2, Alpha keeps on sending
the packets to Beta till its reputation goes below
the threshold, as it is the shortest route to Gamma.
Hence all the packets are dropped till the
reputation of Beta goes below the threshold and
no more packets are sent to Beta.
In contrast, in scenario 3, after the first packet is
dropped and the reputation of Beta goes down

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

5.

6.

maximum, Alpha does not send any more packets
to Beta as long as other routes are available.
Hence the number of packets that are dropped
reduces.
The percentage of packets that reach their
destination, as compared to the packets sent
without the uncooperative nodes is 35%.
Although this value is very low for ad hoc
networks, this figure can be attributed to the
sparseness of the simulated network. Only 500
links are present out of a possible of 49,950 links.
It is only 0.1% of the number of links in a fully
connected network. Hence it must have resulted in
a large number of networks, which are not
interconnected. Such a scenario is very much
possible in mobile ad hoc networks.
Such a sparsely connected network has been used

7.

in order to estimate the impact in worse scenarios.
A network with higher number of links will
obviously display a better throughput. The average
number of hops, sum of the number of hops
traversed by successful packets divided by the
total number of successful packets. It is the same
whether reputations are or are not used for routing.
The average number of hops is reduced when the
number of uncooperative nodes has increased.
This can be attributed to the fact that the number
of packets reaching their destination is reduced.
The packets whose source and destination have
less number of nodes are less likely to find
uncooperative nodes in their path than the packets,
which have longer path lengths.

Number of Nodes, N=100, Number of Links, L=500, Number of Packets, P=1000
Scenarios
Rouges Reached Dropped Average
Number
of Hops
1.
1
0
353
647
2
2.
2
0
353
647
2
3.
3
0
365
635
2
4.
1
10
285
715
2
5.
2
10
300
700
2
6.
3
10
317
683
2
7.
1
20
194
806
2
8.
2
20
220
780
2
9.
3
20
254
746
2
10.
1
30
158
842
2
11.
2
30
190
810
2
12.
3
30
223
777
2
13.
1
40
108
892
2
14.
2
40
139
861
2
15.
3
40
180
820
2
16.
1
50
53
937
1
17.
2
50
91
909
1
18.
3
50
129
871
1
Table 3

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

Throughput
Increase
(%)
0
3.3
5.2
5.9
13.4
30.9
20.2
41.1
28.7
66.6
71.6
143.3

Reputation Routing Performance(Packets
Reached)

Rogues

50

Reputation Routing without
Shortest Path

40
30
20

Reputation Routing With
Shortest Path

10

Default Routing

0
0

50

100

150

200

250

300

350

400

Packets
Figure 2
References

9. Conclusion
Ad hoc networks provide a communication medium
where infrastructure networks are absent. Ad hoc
networks rely on cooperation of the network nodes for
routing. In the absence of a common goal the nodes need
an external motivation to cooperate.
Reputations of the nodes and the subsequent
advantages associated with having high reputation can
provide the motivation for the node to commit their own
resources to others. Using reputations with shortest path
information increases the throughput from 3 to 72% over
routing by using only the shortest path information. Using
reputations without shortest path increases the throughput
up to 143%.
Reputation systems need techniques for collecting
and storing recommendations and making sure that the
rogue nodes cannot misrepresent their reputation. Another
set of challenges is posed by the resource constraints of
the ad hoc network nodes. Encryption and addition work
performed for routing can overload the nodes and hence
raise overall latency. In other words, this extra load will
provide increased throughput and security at the cost of
more work and higher latency.

[Dewan et al. *2003] Prashant Dewan, Austin Godber, and
Partha Dasgupta. 2003. A Self-Certification Scheme for
Managing Reputations in Peer-to-Peer Networks.
(submitted)
[Obreiter et al. *2003] Philipp Obreiter, Birgitta Koenig-Ries,
and Michael Klein. 2003. Stimulating Cooperative Behavior
of Autonomous Devices - An Analysis of Requirements and
Existing Approaches. Paper read at Second International
Workshop on Wireless Information Systems (WIS2003),
April 22,2003, at Angers, France.
[Park et al. *1997] Vincent D. Park, and M. Scott Corson. 1997.
A Highly Adaptive Distributed Routing Algorithm for
Mobile Wireless Networks. Paper read at INFOCOM.
[Perkins et al. *1999] Charles E. Perkins, and Elizabeth M.
Royer. 1999. Ad-hoc On-Demand Distance Vector Routing.
Paper read at 2nd IEEE Workshop on Mobile Computing
Systems and Applications, February 1999, at New Orleans.
[Royer et al. *1999] Elizabeth M. Royer, and C-K Toh. 1999. A
Review of Current Routing Protocols for Ad-Hoc Mobile
Wireless Networks. IEEE Personal Communications
Magazine, April 1999, 46-55.
[Sanzgiri et al. *2002] Kimaya Sanzgiri, Bridget Dahill, Brian
Neil Levine, Clay Shields, and Elizabeth M. Belding-Royer.
2002. A Secure Routing Protocol for Ad Hoc Networks.
Paper read at International Conference for Networking
Protocols, November 12-15, 2002.

Proceedings of the 2003 International Conference on Parallel Processing Workshops (ICPPW’03)
1530-2016/03 $ 17.00 © 2003 IEEE

PRIDE: Peer-to-Peer Reputation Infrastructure for
Decentralized Environments
Prashant Dewan & Partha Dasgupta
Department of Computer Science and Engineering
Ira A. Fulton School of Engineering
Arizona State University
{dewan,partha}@asu.edu

ABSTRACT

of the providers. The requester downloads content from
the provider and grants a recommendation to the provider.
Additionally the requester signs and stores the transaction
number of the provider in the network. The provider stores
the recommendations locally and shows them to the next
requester as a proof of its reputation. As a result the requesters do not have to perform a network search for the
provider’s reputation information. The salient features of
PRIDE are self-certification, IP Based Safeguard and the
elicitation-storage protocol.

Peer-to-peer (P2P) networks use the fundamental assumption that the nodes in the network will cooperate and will
not cheat. In the absence of any common goals shared by
the nodes of a peer-to-peer network, external motivation to
cooperate and be trustworthy is mandated. Digital Reputations can be used to inject trust among the nodes of a
network. This paper presents PRIDE, a reputation system
for decentralized peer-to-peer networks. PRIDE uses selfcertification a scheme for identification of peers using digital certificates similar to SDSI certificates, an elicitationstorage protocol for exchange of recommendations and IP
Based Safeguard (IBS) to mitigate a peer’s vulnerability to
’liar farms.’

2.

Categories and Subject Descriptors: C2.3Distributed
SystemsPeer-to-Peer Networks, Security, Identity
General Terms: Peer-to-Peer Networks, Security, Identity,
Reputations
Keywords: Reputation Systems, Security, Peer-to-Peer

1.

INTRODUCTION

In Gnutella1 70% of the users do not share any files and
nearly 50% of the queries are answered by the top 1% of the
nodes [1]. Free riding in Gnutella leads to degradation of
the system performance and the absence of common goals
among peers makes the network vulnerable to malicious behavior. Digital Reputations can be used to inject the necessary motivation in the Gnutella servents. Digital reputation systems like EBay and P2Prep[4] assume that peers will
value their reputations because of the benefit that can be derived from a good reputation, and will continue performing
transactions in the same ’fashion’, as they have performed
in the past. PRIDE complements Gnutella with minimal
modification of the Gnutella protocol. PRIDE does not need
any central server to identify the peers. Peers generate their
own identities using self certification and locally store the
recommendations received by them. A peer (requester) uses
the Gnutella query for locating content providers. Subsequently it selects the ’best’ peer (provider) from the list
(obtained in the query phase), based on the reputations
1

THREAT MODEL

One of the main goals of a reputation system for a p2p
network is to reduce the chances of a peer getting cheated in
a transaction. For example, if a requester downloads music
from a provider, it should be able to ascertain the probability
of the fact that the music files are complete. Similarly the
provider would like to ascertain that the recommendation
it will receive from the requester, will at least be in conformance with the quality of the service provided. Digital
reputations can mitigate both of the above threats. A reputation system brings in its own set of issues and threats.
It is important to have a identifier allocation strategy for
peers in order to restrict any peer from generating a liar
farm (multiple identities) [5] to raise its own reputation. In
addition, the reputation information has to be tamper proof
and easily retrievable while upholding the accountability of
the reputation issuer. The other challenges for any reputation system are ”ballot stuffing,” ”bad mouthing,” negative
and positive discrimination [3].

3.

SELF-CERTIFICATION

Each peer runs its own certificate authority (CA) which
signs the identity certificate(s) of the peer. All the certificates used in self-certification are digitally signed statements, similar to SDSI certificates [6]. An identity certificate acts as a ’proxy’ to the peer and binds the public key
of a peer to the other information of the peer. Additionally,
the IP-ADDRESS field, a mandatory field for the identity
certificate, specifies the IP address range from which this
identity can be used. If the identity is used from an IP address out of the range, the other party in the transaction
suspects foul play and aborts the interaction . From here
on, the word ’identity’ is used to refer to identity certificate.
Identities are needed to link together (at least some of the)
transactions performed by the peer. Two transactions performed using the same identity can be traced back to the

one of the largest p2p networks

Copyright is held by the author/owner(s).
WWW2004, May 17–22, 2004, New York, New York, USA.
ACM 1-58113-912-8/04/0005.

480

identity. The identity may or may not be traceable to the
peer. Only when the peer calculating the reputation of an
identity knows the transactions of the identity, the reputation of the identity can be calculated. IP addresses cannot
be used as identities because IP addresses are shared among
peers in two different time spans. In addition, the peers
generally do not manage their own IP addresses. A peer
can perform two roles in the network, a requester: a peer
that requests for service or a provider: a peer that provides
the service. At the end of each transaction the requester
issues a recommendation (SDSI certificate) to the provider.
A positive recommendation increases the reputation of the
identity of the provider by one while a negative recommendation diminishes it by one. The requester also submits the
corresponding identity certificate to the provider. An example of a transaction initiated when the requester downloads
some files from the provider.

4.

higher. The experiments show that the mean variation of
ranks of peers in a network that uses IBS is 13.2 ± 0.2 with
a 95% confidence level. IBS and liar farms have been extensively discussed in [5].

5.

ELICITATION-STORAGE PROTOCOL

A requester obtains a list of providers who have the required content. The requester also receives the reputation of
each of the providers in the list. The requester uses IBS to
normalize the reputation of the peers and selects the ’best’
peer based on the reputation of the possible providers and
initiates the ES protocol. On the requester’s initiation, the
provider that is selected by the requester on the basis of its
reputation, generates a new transaction id (TID) by using
the last transaction id as a seed for a one-way function. The
requester verifies if the same TID has been used for any other
transaction by the provider. Once the TID is verified, the
requester checks (at least some of) the past recommendations2 received by the provider and, once satisfied, performs
the transaction. Following IBS the requester recalculates the
reputation of the peer by averaging the recommendations
received in each security zone. If the recalculated value of
peer’s reputation is above the requester’s threshold it performs the transaction and if not it contacts the second peer
in the list and so on. Once the transaction (file download)
is complete, the requester gives a signed recommendation
to the provider, which is stored by the provider. In addition, the requester signs the TID and stores it in the P2P
network. The details of the protocol can be found in [4]

LIAR FARMS & IP BASED SAFEGUARD

Using self-certification any peer can generate a large number of identities, feign a large set of peers and maliciously
increase the reputation of one or more of its identities by giving false recommendations (similar to ballot stuffing). Such
a farm of identities is called a ’liar farm.’ A liar farm can
be countered if all the identities of a peer can be mapped
back to the peer. If self certification is used, a peer’s identities cannot be mapped back to it without its consent.
IP Based Safeguard uses security zones that are the subsets of the IP space. We assume that only the information
provider receives the recommendation, and the information
requester provides the recommendation. The requester selects a provider with which it wants to perform a transaction
and verifies the identity of the old requesters (recommendation issuers) of the provider. Each peer maintains a local
database of verified identities. The requester first checks
its local database for a valid identity, matching the identity associated with the recommendation. If it does not find
any such valid identity, it verifies the identity of the old
requesters of the provider, by performing a cryptographic
challenge-response at the IP address in the identity certificate of the previous requester(s). If the challenge-response
fails for a particular recommendation, that recommendation
is not included in the reputation for the provider identity.
Dellarocas recommends [2] peer annonymity to prevent bad
mouthing and discrimmination. This is an ongoing area of
research.
Once the requester has a list of (apparently) valid recommendations, it sorts them by the order of the IP addresses
(in the corresponding identity certificates), determines a security distance, d and divides the linear IP space into slices
(security zones) of length d. It averages all the recommendations received by the provider from identities whose IP is
in the same security zone. Finally, it adds the averages of
each security zone to calculate the reputation of the provider
identity. IBS is based on the fundamental assumption that
it will be difficult for any malicious peer to generate identities that have totally non-contiguous IP addresses. By increasing the security distance d, a requester can reduce the
probability of the provider having an identity farm. This
reduced probability is at the expense of reduction in accuracy of the reputation of the provider. By decreasing d, the
probability of a peer being victimized to a liar farm rises,
although the accuracy of the reputation of the provider is

6.

CONCLUSION

PRIDE increases the satisfaction level of the peers from
the system by directing the requesters to the providers who
have a history of better performance. The peers in PRIDE
weed out the rogues by giving them bad recommendations,
thereby lowering the reputations of bad peers. It provides
the providers an incentive to provide accurate and timely
information in order to obtain good recommendations from
requesters. The current domain of transaction is confined
to downloading files. In the future the domain of a transaction can be expanded to include higher stake transactions
in E-Markets like E-Bay. More work needs to be done for
integrating globally trusted Certificate Authorities and corresponding hierarchies in the name spaces into local name
spaces used by PRIDE

7.

REFERENCES

[1] E. Adar and B. Huberman. Free Riding on Gnutella, 10/02/00.
[2] C. Dellarocas. Immunizing Online Reputation Reporting
Systems Against Unfair Ratings and Discriminatory Behavior.
In ACM Conference on Electronic Commerce, pages 150–157,
2000.
[3] C. Dellarocas Building Trust On-line : The Design of Reliable
Reuptation Reporting System, eBusiness@MIT working paper
101. MIT Sloan School of Management, Cambridge, Mass.,
2001.
[4] P. Dewan. Injecting Trust In Peer-to-Peer Systems. Ph.D.
Dissertation Proposal, Arizona State University, June 16,2003
[5] P. Dewan. Countering identity farms in reputation systems for
P2P network. Technical report, Arizona State University,
Tempe, January 2004.
[6] R. L. Rivest and B. Lampson. SDSI A Simple Distributed
Security Infrastructure. In Crypto, Santa Barbara ,USA, 1996.
2
Recommendation is the reputation information pertaining
to one interaction between two distinct peers

481

Implementing Consistency Control Mechanisms in the
Distributed Operating System*

Partha Dasgupta
Department of Computer Science
and Engineering
Arizona State University
Tempe, AZ 85287-5406

Raymond C. Chen
Distributed Systems Group
Research and Development Division
Siemens-Nixdorf Information Systems
Cambridge, MA 02142

Abstract

IBCC was designed to be implemented at the operating system level. This paper describes such an operating
system-level implementation. The implementation uses the
hardware virtual memory system to implement automatic
read/write locking and version creation. The mechanisms
have been implemented in the Clouds u.2 kernel (hereafter
referred to as Clouds) as part of the Clouds distributed operating system.
This paper first presents an overview of Clouds and
Invocation-Based Consistency Control. The paper then
presents the design and implementation of the IBCC mechanisms and discusses the performance aspects of the implementation.

This paper presents an implementation of a kernel-level consistency control mechanism called Invocation-Based Consistency Control (IBCC). IBCC is designed to support
general-purpose persistent object-based distributed computing. IBCC provides mechanisms that support a range of
powerful, well-defined consistency semantics. For more sophisticated applications, the IBCC mechanisms can be used
to implement custom recovery and synchronization.
The paper presents an operating-system level implementation of IBCC as part of the Clouds distributed operating system that uses memory faulting to initiate locking
and intermediate version creation. The design takes into
account the interactions that result from supporting distributed shared memory and can be implemented independently of the choice of the recovery technique. Performance
aspects are discussed as well as the overhead incurred by supporting IBCC in terms of additional data structures needed
in the operating system, and the additional amount of required code.

2

Related Work

A number of languages and transaction facilities have been
proposed and some implemented that support generalpurpose reliable distributed computing. Languages that s u p
port reliable persistent objects include Argus, [16] Eden,
[l], Aualon [ll] Arjuna [12], E [20], Trellis/Owl [la], and
Napier88 [17]. Many of these language implementations use
compiler-generated calls to a run-time support/storage s y s
tem where the run-time system is either dedicated to s u p
porting one language or some form of general-purpose stable
storage manager or transaction facility such as Camelot [23],
EXODUS [4], or Cricket [22]. Most of the above systems
are implemented on top of existing operating systems such
as Unid[21] or Mach [26]. Implementation of consistency
mechanisms in a general-purpose operating system seems to
be rare.
Locus [25] supports transactions over a reliable d i s
tributed file system. The Locus implementation uses logging
on top of the shadowing-based reliable filesystem to provide
fine granularity recovery. Clouds u.1 provided nested transactions, recoverable and non-recoverable data segments, custom locking facilities, and user-definable commit handling
via exception handlers [9].
Our implementation relies heavily on using page
fault/access violation exceptions to initiate lock requests and

1 Introduction
This paper presents an implementation of a consistency control mechanism called Invocation-Based Consistency Control
(IBCC) [7]. IBCC is designed for general-purpose, persistent object, distributed computing. IBCC provides mechanisms that can be used to achieve consistency ranging from
no system-guaranteed consistency (process-based computing) to strict atomic transactions.
The IBCC mechanisms support powerful, well-defined
consistency semantics. For more sophisticated applications,
the mechanisms can be used to implemeni , .Lorn recovery
and synchronization. Since consistency support incurs overhead, IBCC also supports low-overhead computation (the
equivalent of processes), allows processes and transactions
to co-exist and interact, and thus allows programmers to
control the trade-offs between consistency and efficiency.
*This work was supported in part by NSF grant C C R 8 6

lUnix is a trademark of AT&T

19886.

10

CH2996-7/91/0000/0010$01.OO 0 1991 IEEE

CZou d s

Thread

version creation. This memory reference exception technique
has been used by others. The Bubba database system [3] and
Cricket database storage system, and the 801 processor [5]
use memory protections and access violations to set locks.

3

Clouds

Note: “T e’’
i “Thread transforms to”
Though the label remains the same, this is a transforming
invocation.

The Clouds distributed operating system [8] is designed to
support general-purpose distributed computing. Clouds provides data persistence and protection through the use of
large-grained persistent objects. Computation is performed
using threads (of control).
A large-grained persistent object is a long-lived address
space that contains both code and data. Data stored in an
instance of a large-grained persistent object is accessible only
to the code within that instance. Threads execute concurrently within objects, and data stored in an instance of n
object is accessible to all threads of control executing within
that instance. The persistent object memory is therefore
computationally shared. Clouds has no user-level concept
of files or secondary storage. All persistent data resides in
object memory which is automatically backed on secondary
storage.
Threads enter objects by invoking an object-defined o p
eration. Object invocation is location-transparent and may
be nested and/or recursive. While a distributed system,
Clouds has the look and feel of a centralized system.
Clouds is a native operating system written in C++ [24]
and MC68020 assembler that runs on Sun-3 workstations.
Clouds has been operational for over a year and is being
used as an experimental testbed for research in distributed
operating systems, the persistent object paradigm [lo], distributed programming languages and environments [19], and
reliable distributed computing.

4

Operation Consistency Label

Label

Figure 1: Consistency Label Compatibility
IBCC functions by providing a set of automatic recovery and synchronization mechanisms that control the visibility and permanence of altered data. These mechanisms are
not explicitly invoked. Instead, the mechanisms come into
play as a result of static labels placed on object methods by
the programmer, grouping of data into locking units called
locking segments, and extensions to the semantics of object
invocation/return.
Threads carry a dynamic consistency label. Object o p
erations are labeled with static consistency labels. When a
thread invokes an object operation, the system checks the
values of the thread and operation labels. Depending on the
values of the labels, the thread label may change and commit
processing may occur when the operation terminates.

4.1

Consistency Levels

The mechanisms of IBCC support three types of consistency
as defaults: global, local, and standard. Global consistency
corresponds to multi-object consistency where a set of c o o p
erating objects must be kept consistent both internally and
with respect to each other. Local consistency corresponds
to single-object consistency. Standard is the degree of consistency guarantee that programmers are currently used to
- which is to say virtually none at all. With standard consistency, the system attempts to keep data consistent but
makes no guarantees.

Invocation-Based Consistency Cont rol

Invocation-Based Consistency Control is a set of flexible
mechanisms that control recovery and synchronization. Although strong consistency properties can be easily attained,
the mechanisms themselves do not claim to guarantee consistency. Security can be traded off for flexibility. The intent is
to use IBCC to build objects that appear to behave in a consistent fashion where “consistent” is application-dependent
and to make recovery and synchronization features available to all users of the system, where a user is anyone or
anything requesting operating system sei
Users thus
IS for various
include compilers and run-time support .
languages. A detailed description of IBC
n be found in
[7, 61. We present an overview here for con&,:cteness.
IBCC does not support the notion of a transaction or
separate the notions of transactions and processes. Transactions and processes are unified into one computational entity,
the thread, and the effects of both can be obtained through
proper use of the mechanisms.

4.2

Operation Labelling

Each entry-point is marked with one of four labels by the
object programmer:
0

GCP (Global Consistency Preserving)

0

LCP (Local Consistency Preserving)

0

S (Standard)

0

I (Inherited)

Each thread of execution in the system also bears a consistency label that is set to either GCP, LCP, or S. A thread
is called an S-thread, GCP-thread, or LCP-thread if the
thread’s current consistency label is S, GCP, or LCP respectively.
When a thread with consistency label X invokes an o p
eration T marked with consistency label Y,the invocation
may be a transforming invocation depending on the value

11

of X and Y.Figure 1 summarizes the thread transformation
scenarios and how the thread consistency label may change.
An entry T A means that the invocation is a transforming
invocation, and the value of the thread’s consistency label
changes to A. If the entry is in bold, that indicates that
commit processing will occur when invocation terminates.
A transforming invocation that causes the thread label to
transform to some label L is called an L-transforming invocation or L-invocation for short.
The IBCC synchronization and recovery mechanisms behave differently depending on the value of the thread consistency label. With the exception of operations bearing the I
label, the thread label always transforms to match the label
on the operation.
A GCP, LCP, or S label on an object operation indicates a constraint placed on the execution of the operation
by the programmer/compiler. The label indicates that the
implementation assumes that the recovery/synchronization
mechanisms will behave in a certain fashion.
The one exception to this is the I label. The inherited
label indicates a lack of a consistency constraint. An inherited operation is supposedly capable of supporting any type
of consistency. Therefore, upon invoking an inherited operation, the thread label does not change. Instead, if the thread
bears a thread consistency label L, the operation behaves as
if it had an L label. Thus an inherited operation invoked
by a GCP-thread would perform automatic synchronization
and recovery like a GCP-operation.

Persistent data is grouped into user-defined locking granules called locking segments. The granularity of system-level
locking is the locking segment. If a locking segment is locked,
all data contained in that segment is locked. The systemlevel locking rules ensure that all version stacks created by
GCP/LCP invocations are linear. Thus, there always exists
a latest version.
S-threads always access the lateut version of data. Unlike GCP/LCP invocations, S-invocations do not perform
system-level locking. Therefore, S-threads never block as
a result of IBCC synchronization. S-invocations can read
uncommitted changes made by GCP/LCP-invocations. S
invocations can also overwrite data read/write-locked by
GCP/LCP-invocations. If an S-thread makes a change to
an intermediate version created by a GCP/LCP-invocation,
that change gets committed/aborted when the intermediate
version is committed or aborted as a result of the termination of the GCP/LCP-invocation. GCP/LCP-invocations
may also read changes made by S-threads that exist in the
base version but have not been committed to the stable version.

-+

4.3

4.5

GCP and LCP transforming invocations both perform commit processing when they terminate, although the scale
of the commit processing is different. When GCP/LCPinvocations attempt to write to persistent data, the update
is not immediately reflected in the stable version of the data.
Instead, the system creates an intermediate version of the
segment containing that data. Further updates to that data
by the same transforming invocation are reflected in that
intermediate version. When a GCP/LCP-invocation terminates (returns), all intermediate versions created by that
invocation are committed or aborted atomically.
S-invocations perform no commit processing when they
terminate.
Like GCP/LCP-invocations, when an Sinvocation attempts to write to persistent data, the update
is not immediately reflected in the stable version, but rather
in the latest version of the segment. Base versions are eventually committed to stable storage but there is no guarantee
as to when the commit may occur.
GCP and LCP transforming invocations can be thought
of as creating recovery blocks. A GCP-invocation creates a
dynamic recovery block that may include multiple objects.
An LCP-invocation creates a static recovery block that includes only one object.

Version Model

IBCC synchronization and recovery lead to version stacks of
persistent data where the older versions are a t the bottom
of the stack. The stack contains two types of versions: base
versions and intermediate versions. The base version is the
original version at the bottom of the version stack; it resides in secondary storage and is cached in primary storage
(memory). Any intermediate versions (if they exist) reside
in primary memory and may be backed by secondary storage. Primary memory is assumed to be volatile. Although
secondary storage is assumed to be non-volatile, it is not assumed to be reliable. Each base version has an associated
stable version that resides on stable storage. Thus, while
base and intermediate versions may survive failures, only
the stable version is guaranteed to survive a system failure.

4.4

Commit Processing

Locking and Recovery

Threads executing GCP or LCP-invocations automatically
lock any data (locking segments) they access before the
access is allowed to proceed. This system-level locking is
read/write and 2-phase. The operating system automatically requests a system-level lock on behalf of the executing thread/invocation when the thread attempts to read (or
write) persistent data that it has not already read-locked (or
write-locked). Locks gained during a transforming invoc&
tion are released only upon termination of the transforming
invocation. Further details are available in 161.

5

Implementation Assumptions

This implementation design makes two assumptions about
the underlying hardware. First, the address space of the
machine can be broken up into a number of address ranges
that can be individually read/write-protected. Second, when
an attempt is made to access a protected address range in a
manner prohibited by the protections, the operating system
gains control via an exception handler and the hardware
indicates whether the attempted access was a read or write.

12

7.1

These assumptions are necessary as the design calls for
using the protectable address ranges to implement locking segments and for using memory exceptions to initiate
system-level locking and version creation. Each locking segment is implemented using one or more protected address
range. The implementation architecture presented in this
paper is designed to be implemented on paged virtual memory architecture, or segmented/paged virtual memory architecture like the Sun-3.

6

The implementation must maintain a number of important
data structures. The three major data structures are the
object header (also known as a Virtual Space Descriptor),
the segment table, and the Transformation Segment. Each
Clouds object has an object header that is used by the kernel
when handling page faults. Clouds objects are composed of
segments and the object header defines size and location
of each segment, the protection mask on each segment (no
access/read-only/full access), and the consistency label for
each object entry-point. For the purposes of IBCC systemlevel locking, each segment is a locking segment.
Each Clouds segment has a segment table with it that indicates which segment pages/blocks are held in which physical page frames. The page frames referenced by the segment
table always contain the latest version of the segment.
Each thread has a transformation segment (t-segment)
associated with it. A t-segment contains information on the
locks held, objects visited, and intermediate versions crested
by every non-terminated transforming invocation initiated
by the thread.

Virtual Memory and the Single-level
Store

In file-based environments, persistent data is accessed
through reads and writes of files/relations, and the operating
system or database gains control at each read and write request, This allows the system to intercept reads and writes
of persistent data, and any necessary locking or recovery
processing is executed.
However, in a system using a single-level store such as
Clouds, persistent data is accessed through direct reads and
writes of primary memory, not through operating system
requests. Thus, the operating system can not intercept
reads/writes of persistent data unless a memory fault (page
fault or access violation), software exception (divide by zero,
etc.) or hardware interrupt occurs.
The implementation therefore uses the virtual memory
system protection mechanisms to initiate locking and version creation? Page-level read/write protection is used to
intercept reads and writes of unlocked data. If a GCP/LCP
invocation does not hold a read (or write) lock on a locking
segment, the pages of that locking are set to disallow reads
(or any access). Attempting a disallowed access results in an
access violation allowing the operating system to take control. Read-faults when accessing a segment are interpreted
aa requests for a read-lock on the segment, and write-faults
are interpreted as requests for a write-lock on the segment.
Writing to a write-locked page results in an intermediate
version of that page being created.

7

Major Data Structures

7.2

Object Invocation

On each local object invocation, the kernel checks the object
header to get the consistency label of the entry-point being
invoked. The consistency label is then checked against the
thread’s current consistency label to determine if the invocation will be a transforming invocation. If so, the transforming invocation is recorded on the t-segment associated
with the thread.
If the invocation is an S transforming invocation (from
GCP/LCP to S), no further IBCC-related processing occurs, and the local invocation proceeds as usual. If the invocation is a GCP/LCP transforming invocation, a shadow
object header is created from the header of the object that is
being invoked. The shadow object header is identical to the
object header except that the memory protection masks on
all persistent data segments are set to disallow any access
to the data. The object invocation then proceeds using the
shadow object header to control the memory mappings in
place of the actual object header.

Functional Behavior
7.3

The IBCC implementation performs the following tasks: initiates transforming invocations, performs recovery processing when terminating transforming invocations, implements
the implicit system-level locking, automatically creates intermediate versions when necessary, d o w s P-invocations to
access the latest versions of all data without blocking. These
functions are initiated by object invocation, page faults, or
object return. This section describes the major data structures in the IBCC implementation and the flow of control of
object invocation, page faulting, and object return.

Access Violations

If a read/write of a segment causes an access violation, the
kernel determines if the invocation holds a system-lock on
the segment. If no lock is held, the kernel obtains a lock on
behalf of the invocation. If the access is a write, the kernel
must also create a new version of the segment, which entails
preserving the current version (discussed in further detail in
section 8.2. The kernel then sets the memory mappings in
the (shadow) object header to allow the attempted access.

7.4

2Note that this does not prevent the use of logging-based recovery mechanisms as long as pages can be provided by the system
when servicing page faults.

Object Return

On an object return, if the terminating invocation is a transforming invocation, the thread consistency label (as well as

13

the system must also preserve the contents of older versions
until they are overwritten as a result of a commit.
Having DSM in the system adds further complexity. In a
system without DSM, the operating system on a node knows
whether an object is stored locally or remotely. In a system
with DSM, the DSM coherence system hides this knowledge
from the rest of the operating system. An object controlled
by DSM may reside anywhere in the distributed system but
appears to all sites to be stored locally. If a computation
executing on the local site alters a set of pages controlled
by DSM (thus moving them to the local site), the pages
may have been moved to one or more other sites and not be
present locally at the time commit processing begins. This
design therefore calls for augmenting DSM to handle version creation, precommit, commit, and abort processing for
DSM-controlled segments.
Local recovery processing is controlled by a Recovery
Management System. Page fetch and recovery requests are
handled by a Recovery Control System which makes requests
on the local Recovery Management System or on the DSM
system or both, depending on the nature of the request.
When directed to create a version, the DSM system must
create a new version of the segment and preserve the state of
the current segment version. Due to the fact that S-threads
can access data without blocking, to preserve single-copy
semantics, every page in the current version of the segment
must be preserved before allowing the GCP/LCP invocation
that created the new version to proceed. Likewise, commits,
precommits, and aborts affect every page in the segment
version. These operations on a segment, therefore involve
processing on every site that holds a copy of a page of that
segment.

other attributes, see [6] for details) is reset. If the terminating invocation is a GCP or LCP invocation, the kernel then
performs any necessary commit processing.

8

DSMandIBCC

The Clouds system supports distributed shared memory
(DSM) [15] to allow threads on several sites to simultaneously access the same objects. Although multiple threads on
different sites may be reading or writing the same page, a
DSM coherence protocol similar to a multi-processor cache
coherence protocol ensures that single-copy semantics are
preserved. The IBCC implementation must function in a
DSM environment. Integrating IBCC and DSM necessitated
adding functionality to DSM, and requires the use of distributed locking.

8.1

Distributed Locking and Shared
Memory

In a persistent object system supporting distributed shared
memory, local locking is insufficient. In system with DSM,
two GCP-threads on different sites may attempt to update
the same locking segment. When this happens, only one
thread can gain the lock; the other thread must block. Locking requests and locking information must therefore be managed globally.
Distributed locking can be implemented in two ways:
by adding locking to the DSM system or by using a Distributed Lock Manager. This implementation uses a Distributed Lock Manager. The DSM system is not responsible
for locking, only for maintaining coherence.
Using a separate Distributed Lock Manager has three
major advantages. A separate, orthogonal Distributed Lock
Manager can support both the system-level locking required
for invocation-based consistency control and a standard useraccessible read/write locking service. Furthermore, by isolating the two functions (DSM coherence and distributed
locking), either system can be experimented with or altered
without affecting the other. This is a significant advantage
as Clouds is being used as a research testbed.
In this design, the logic that determines whether systemlevel locking must be performed can be isolated in a third
system, the Virtual Memory Manager. Neither the Distributed Lock Manager nor the DSM coherence system
need know about consistency labelling or the S/LCP/GCP
semantics. If distributed locking were implemented in
the DSM coherence system, the DSM coherence system
would have to be augmented to implement not only IBCC
read/write locking but also the semantics of interactions between S-invocations and GCP/LCP-invocations.

8.2

Implementation Architecture

9

An implementation of invocation-based consistency control
must correctly perform the following:
0

On each object invocation, determine if the invocation
is a transforming invocation.

0

Intercept reads/writes of unlocked persistent data by
GCP/LCP-invocations, interpret the accesses as lock
requests, and process the lock requests.

0

Properly create, commit, and destroy intermediate
versions.

0

Ensure that S-invocations always access the latest version of persistent data without blocking.

The IBCC implementation architecture (shown in figure 2) consists of an Invocation Processing Layer, Object
Information System, Transformation Information System,
Page Information System, Distributed Lock Manager, Virtual Memory Manager, Object Invocation System, and a Version Manager consisting of a Recovery Control System, DSM
System, and Recovery Management System.
The Invocation Processing Layer and Virtual Memory
Manager are the control layers. They gain control on object
invocation/return and page faults. The remainder of the

Version Management and DSM

As discussed earlier, IBCC presents a version stack-based
model of recovery to the user. To correctly implement IBCC,
the system must properly create, commit, and abort intermediate versions. Once an intermediate version is created,

14

0bject Invoke

0bject Ret urn

Memory Fault

Virtual Memory Manager

Invocation Processing Layer

p F )
Information

Figure 2: Implementation Architecture
intermediate version of a segment is being created and that
the state of the current version should be preserved.
The commit0 routine commits the latest version. The
Version Manager then ensures that all threads using the latest version access the newly committed version. Similarly,
abort() deletes the latest intermediate version. All threads
working on the latest version will then automatically access
the next-oldest version.

components can be classified as support systems that provide
services required by the control layers. The remainder of
this paper describes the functional descriptions of each of
the subsystems, control layers, and their implementation in
the Clouds kernel.

9.1

Version Manager

The Version Manager is responsible for moving versions (of
pages) between primary memory and secondary storage, and
for correctly maintaining stable storage. This includes creating, committing and aborting intermediate versions, flushing
base versions to stable storage, and fetching the latest version from secondary storage. In this architecture, the Version Manager consists of a Recovery Contru
‘rm(RCS),
(RMS).
DSM system, and Recovery Management S:.
The Version Manager provides a get 0
ruitive that
returns a page frame containing the contentb ,.,f the latest
version of that page. The Version Manager ensures that all
fetches of the latest version (get Os) get the new version.
The Version Manager also supports create(), commit 0 ,
and abort 0 for managing intermediate versions. The
c r e a t e 0 function informs the Version Manager that a new

9.1.1

Version Manager Implementation

In the current configuration of the Clouds system, all persistent objects are backed by data servers. The data servers use
the Unix filesystem to store Clouds object images. A similar
scheme is used to implement paging/swapping devices for
Clouds sites.
The current implementation commits pages by writing
the versions back across the network to the appropriate data
server. If multiple data servers are involved, the pages are
committed sequentially, one server at a time. This scheme
leaves two points of vulnerability. First, if the Unix filesystem fails, the data will be lost. Second, if a commit involves
multiple data servers, a failure during commit processing

1s

100 lines of code and 160 bytes to the object header.
The current object header allows for an object to define up to 140 entry points. Each entry point requires
one extra byte in the object header.

may result in a partial commit.
Well-known techniques exist for addressing the commit
and reliable storage problems [13, 141. However, given that
Clouds is being used for research as an experimental testbed,
it was felt that implementing these techniques would not
have been cost-effective.

9.1.2

0

The Transformation Information System (TIS) maintains t-segments. Although each thread has an associated transformation segment, the segment is not
accessed unless the thread invokes a transforming invocation. The TIS creates t-segments only when
necessary, therefore threads which perform only Sinvocations incur no overhead as a result of t-segment
managment .
The Transformation Information System had to be
added to the system and required approximately 1000
lines of code to implement.

0

The Page Information System (PIS) maintains information on whether a page of a segment/object is
present in the system, whether the page is present in
physical memory, and the page frame containing the
page if present in physical memory. The PIS is u p
dated when pages are invalidated or moved between
memory and secondary storage (either local or remote
disk).
The Page Information System was already necessary
for the proper functioning of the Clouds system and
required 900 lines of code to implement.

0

The Distributed Lock Manager provides distributed
read/write locking. Locking is performed on locking
names, (name, integer) and are granted on the basis of
locking ids, (thread name, integer). This enables the
lock manager to provide conventional read/write locking for threads as well as and the system-level locking
required by invocation-based consistency control.
The Distributed Lock Manager required approximately 3500 lines to implement of which approximately 2500 lines are shared with an implementation
of distributed semaphores.

Other Systems

We now present a brief overview of some of the remaining
systems in the implementation.
0

0

0

0

The Virtual Memory Manager (VMM) implements
automatic system-level locking and initiates intermediate version creation. (Shadow version destruction
via commit/abort processing is handled by the object
return mechanism.) In the ClOUds implementation,
the Virtual Memory Manger is implemented directly
in the page fault handler.
The clouds Virtual Memory Manager is approximately 700 lines of code. Adding the logic necessary
to support IBCC required adding approximately 100
lines of code to the Virtual Memory Manager.
The Invocation Processing Layer (IPL) controls the
initiating and terminating of transforming invocations. The IPL gains controls at the commencement
and termination of every object invocation. The IPL
must handle local and remote object invocation and
object return.
The Invocation Processing Layer as originally implemented consisted of approximately 1000 lines of
code. This code included the remote object invocation facility. Supporting the functionality required by
invocation-based consistency control required an additional 300 lines of code as well as an extra 24 bytes
of data in the remote invocation control message.
The Object Invocation System is responsible for properly saving and restoring the virtual memory mappings and hardware state such aa registers necessary
for an object invocation, as well as maintaining a perinvocation attributes. These attributes include the
consistency label of the invocation whether the current invocation is a transforming invocation.
The original implementation of threads and object invocation consisted of approximately 3000 lines of code.
The modifications required entailed adding less than
50 lines of code and two integers to the invocation
record that is pushed and popped on object invocation/return.

10

Performance and Overhead

In keeping with the IBCC philosophy of supporting lowoverhead computing, one goal of the implementation design was to minimize the amount of overhead incurred by
S-invocations in an IBCC system as compared to the overhead incurred by threads in a similar non-IBCC system.
The majority of the performance overhead is incurred as
a result of the distributed locking, version creation, and commit/abort processing performed by GCP/LCP-invocations.
However, this overhead is necessary for correctness and is
incurred only by the GCP/LCP computations desiring the
consistency provided by Invocation-Based Consistency Control.
The measurements given in this section were taken on a
Clouds kernel running on a Sun-3/50 with a 12 MHz 68020
processor using a microsecond timer chip. The Unix utilities (DSM server and lock manager) were running on a Sun

The Object Information System maintains the object
headers and shadow object headers, and makes the
information available in the headers available to the
rest of the kernel.
The kernel code necessary to implement and manage
object headers total approximately 2500 lines of code.
Modifying the original design to support the additional required functionality involved adding less than

16

References

Sparcstation-1. The network involved was a 1 Mbit/second
Cheapernet.
Measurements indicate commit times of approximately
33 msec per 8K page. This time reflects the time necessary
to reliably send the page to a Unix server process acting as
a network disk and wait for the acknowledgement.
Measurements indicate that lock requests on the Distributed Lock Manager which must be forwarded to a remote
site require approximately 9 msec to complete, and lock requests that can be serviced locally require under 2 msec complete. Approximately 1.3 msec of this time is spent getting
into and out of the kernel (via the page fault handler) and
determining the segment to be locked.
Committing 3-page segment requires 131 msec. The time
breaks down into 33 msec per altered page (to send the page
to the DSM owner site), plus 9 msec to unlock the segment
(assuming the lock is managed off-site), plus 11 msec to send
and process the control message to the DSM owner site that
signals the segment commit, plus 12 msec of miscellaneous
processing.
Creating a version (and preserving the current version)
of a segment page requires approximately 33 msec. As in
the commit processing, the majority of this time is spent
communicating across the network. In this case, an 8K page
is transmitted across the network. Either the faulting site
sends the current version of the page to the DSM owner
site to be preserved or the DSM owner site sends the latest
version of the page to the faulting site.
Querying the Transformation Information System to determine if a lock is already held by an invocation requires
0.1 msec. If a lock must be gained during a fault in addition
to a creating a new version, servicing the fault requires an
additional 2 msec or 9 msec depending on whether the lock
is controlled locally or remotely.
Neither the Clouds kernel nor the Unix servers were optimized or tuned. Recent measurements on a tuned version
of the kernel yielded 16 msec for per 8K page transmission
and 4 msec to transmit a control message. IBCC performance times should thus drop drastically once the performance improvements that yielded these measurements are
merged into the main Clouds release. Commit times per
page should drop to between 16 and 19 msec. Remote lock
request times should drop to approximately 5 msec.

11

G. T. Almes, A. P. Black, E. D. Lazowska, and J. D.
Noe. The Eden System: A Technical Review. IEEE
Transactions on Software Engineering, S E l 1 ( 1):43-59,
January 1985.
R. Ananthanarayanan. An Implementation Architecture for Synchronization in a Distributed System. Technical Report (in progress).
Bora, H. and Alexander, W. and Clay, L. and Copeland,
G. and Danforth, S. and Franklin, M. and Hart, B. and
Smith, M. and Valduriez, P. Prototyping Bubba: A
Highly Parallel Database System. IEEE Transactions
on Data and Knowledge Engineering, 2(1), 1990.
M. J. Carey, DeWitt D. J., J. E. Richardson, and E. J.
Shekita. Object and File Management in the EXODUS
Extensible Database System. In Proceedings of the 12th
International VLDB Conference, August 1986.
Albert Chang and Mark F. Mergen. 801 Storage: Architecture and Programming. ACM Transactions on
Computer Systems, 6(1), February 1988.
Raymond C. Chen. Consistency Control and Memory
Semantics for Persistent Objects. PhD thesis, College of
Computing, Georgia Institute of Technology, Atlanta,
GA, 1991.
Raymond C. Chen and Partha Dasgupta. Linking Consistency with Object/Thread Semantics: An Approach
to Robust Computation. In Proceedings of the 9th International Conference on Distributed Computing Systems, June 1989.
P. Dasgupta, R. C. Chen, S. Menon, M. P. Pearson,
R. Ananthanarayanan, U. Ramachandran, M. Ahamad,
R. J. LeBlanc, W. F. Appelbe, J. M. Bernabku-Aubin,
P. W. Hutto, M. Y. A. Khalidi, and C. J. Wilkenloh. The Design and Implementation of the Clouds Distributed Operating System. Computing Systems, 3(1),
1990.

P. Dasgupta, R. LeBlanc, and W. Appelbe. The Clouds
Distributed Operating System. In Proceedings of the
8th International Conference on Distributed Computing
Systems, June 1988.
Partha Dasgupta and Raymond C. Chen. Memory Semantics in Large Grained Persistent Objects.
In Proceedings of the 4th International Workshop on
Persistent Object Systems (POS). Morgan-Kaufmann,
September 1990.

Acknowledgements

The Distributed Lock Manager discussed in this paper was
implemented by R. Ananthanarayanan. The remote object
invocation facility and a portion of the Object Invocation
System were originally implemented by Mark Pearson. We
would also like to thank Sathis Menon, Ajay Mohindra,
Chris Wilkenloh, for their efforts in adding to the functionality of the Clouds kernel?

David Detlefs, Maurice P. Herlihy, and Jeannette M.
Wing. Inheritance of Synchronization and Recovery
Properties in Avalon/C++. IEEE Computer, Digital
Equipment Corporation 1988.
G. N. Dixon and S. K. Shrivastava. Exploiting Type
Inheritance Facilities to Implement Recoverability in
Object-Based Systems. In Proceedings of the 6th
Symposium on Reliability in Distributed Software and
Database Systems. IEEE, March 1987.

3Bibliographytruncated due to length limitations. References
available on request.

17

2011 IEEE International Conference on Privacy, Security, Risk, and Trust, and IEEE International Conference on Social Computing

Software based remote attestation for OS kernel and
user applications
Raghunathan Srinivasan

Partha Dasgupta

Tushar Gohad

Arizona State University
raghunathansrinivas@gmail.com

Arizona State University
Tempe, AZ, USA
partha@asu.edu

Monta Vista Software, LLC
tusharsg@gmail.com

The Trusted Platform Module (TPM) chip has been used
extensively to build hardware based solutions. In most cases,
some integrity measurement values are stored in the Platform
Configuration Registers (PCR) and anytime a measurement is
to be taken a private key stored in the hardware is used to
sign the integrity values read from the system software [4],
[5]. The TPM based schemes are fairly robust for determining
the integrity of applications during system initialization but
have certain drawbacks as well [6].
Software based solutions for Remote Attestation vary in
their implementation technique. Most methods involve taking
a mathematical or a cryptographic checksum over a section
of the program in question (P), and reporting the results
of verification to a trusted external server (Trent) [7], [8].
TEAS [9] proves mathematically that it is difficult for an
attacker to forge integrity results obtained on a client platform,
provided the integrity measurement code changes for every
attestation instance, however, an implementation framework
is not provided in the work.
To provide a secure computing platform a root of trust needs
to be established on the client machine. To provide a root
of trust on a platform, this work presents two schemes. The
first sceme presents methods to obtain the integrity of the OS
text section, system call table and the Interrupt Descriptor
Table (IDT). Once the attestation is completed for the OS
areas, the attestation proceeds with the second scheme which
measures the integrity of a particular client application. The
second scheme builds on a prior publication [10] and adds
a component where an external agent can determine whether
the application that was attested earlier is still executing. The
OS provides system call interface which is extensively used
by the user application scheme, this way the OS serves as
a root of trust The user application attestation scheme is
summarized as follows. Alice has a user level application
P that Alice wants attested. Trent is a trusted server who
generates challenges that determine whether P is compromised or pristine. The challenge is executable code C which is
injected on P at Alice’s machine (MAlice ). For details, refer
to the prior publication [10]. The extension provided to the
prior publication is explained as follows. It is possible that
once Remote Attestation is completed for the user application,
Mallory may replace the attested software (P) with a corrupted
version (P  ). Alice would have no knowledge of such a

Abstract—This paper describes a software based remote attestation scheme for providing a root of trust on an untrusted
computing platform. To provide a root of trust, this work focuses
on obtaining the integrity of the OS running on the platform,
and then leverages the techniques to obtain the integrity of a
user application. A trusted external entity issues a challenge to
the client platform. The challenge is executable code which the
client must execute, and the code generates results which are sent
to the external entity. These results provide the external entity
an assurance as to whether the client application and the OS at
the client end are in pristine condition. This work also presents
a technique where it can be verified that the application which
was attested, did not get replaced by a different application once
the challenge got completed.
Index Terms—Remote attestation, integrity measurement, device drivers, code injection

I. I NTRODUCTION
A consumer computing platform can be compromised by
malicious logic in many different ways. Preventing compromises requires safe coding, developing secure Operating
Systems (OS), and developing secure kernel modules. Fault
densities in OS kernels can range from 2 - 75 per 1000 Lines
of Code (LOC) [1]. OS kernels are often supplemented by
many device drivers or kernel modules which have higher error
rates [2]. Buffer overflow is a common vulnerability that exists
in many application software which may allow malware to
compromise systems. Operating systems do not offer isolations
to user space programs from one another [3], in addition a
rogue kernel module or a device driver can overwrite any
section of kernel memory. The use of anti-malware software is
a workaround the problem of protecting the operating system
as anti-malware software can only detect known malicious
logic. This is because they are primarily signature based
scanners, in addition, smart malware can render these detection
techniques ineffective. Consequentially, a user (Alice) has to
request integrity measurement of the platform from an entity
that operates beyond the bounds of the operating system.
Remote attestation is a set of protocols that uses a trusted
service to probe the memory of a client computer to determine
whether one (or more) application has been tampered with or
not. Primarily used for DRM, these techniques can be extended
to determine whether the integrity of the entire system has
been compromised. Remote attestation has been implemented
using Hardware devices, VMM and software based techniques.
978-0-7695-4578-3/11 $26.00 © 2011 IEEE
DOI

1048

Application P

II describes the related work for integrity measurement, section
III describes the threat model for each framework, section
VI-A describes the software based remote attestation, section
VI-B presents the design of the verified code execution component, section V describes the kernel integrity measurement
scheme, section VI-C presents the implementation of all the
above works and section VIII concludes the paper.

Trent

F1

F0

main routine
Injected
code C
Fig. 1.

II. R ELATED W ORK
Code attestation involves determining whether the process
executing is in the same state after it was installed or has been
modified by any malicious code. This section discusses various
schemes implemented previously to perform attestation.
Some hardware based schemes that determine the integrity
of a client platform operate off the TPM chip provided by the
Trusted Computing Group [4], [5]. These schemes may involve
the kernel or an application executing on the client obtaining
memory reads, and providing it to the TPM. The TPM signs
the values with its private key and may forward it to an external
agent for verification. Co-processor schemes that are installed
on the PCI slot of the PC have been used to measure the
integrity of the kernel as mentioned in section 2.1. One scheme
[3] computes the integrity of the kernel at installation time
and stores this value for future comparisons. The core of the
system lies in a co-processor (SecCore) that performs integrity
measurement of a kernel module during system boot. The
kernel interrupt service routine (SecISR) performs integrity
checks on a kernel checker and a user application checker.
The kernel checker proceeds with attesting the entire kernel
.TEXT section and modules. The system determines that
during installation for the machine used for building the prototype, the .TEXT section began at virtual address 0xC0100000
which corresponded to the physical address 0x00100000, and
begin measurements at this address. The Copilot [11] is a
hardware coprocessor that constantly monitors the host kernel
integrity. It cannot handle dynamic kernel modules and userlevel applications and it does not have a mechanism for a
kernel patch.
Integrity Measurement Architecture (IMA) [12] is a software based integrity measurement scheme that utilizes the
underlying TPM on the platform to measure the integrity
of applications that are loaded on the client machine. IMA
maintains a list of integrity values of many files in the system.
When an executable, library, or kernel module is loaded, IMA
performs an integrity check prior to executing it. IMA does
not provide means to determine whether any program that is
in execution got tampered in memory. Terra uses a trusted
virtual machine monitor (TVMM) and partitions the hardware
platform into multiple virtual machines that are isolated from
one another [13]. Terra is installed in one of the VMs (TVMM)
and is not exposed to external applications like mail, gaming,
and so on. The TVMM takes measurements on the VMs prior
to loading them. The TVMM acts as a root of trust on the
machine.
R
VIS [14] is a hardware assisted (IntelVirtualization
TechnologyTM ) virtualization scheme which determines the

Overview of verified code execution

change as long as P  performs all the functionalities of P.
To prevent Mallory from achieving such attacks, a framework
for verified code execution is presented in this paper. This
involves server making some changes to the code section of
P during remote attestation. Trent uses C to change a function
call in P to call a new function F1 instead of calling F0 . When
the changed section of P executes it communicates back to
Trent. This communication tells Trent that the attested program
indeed completed execution. All changes made by Trent to
the attested program are non-persistent and remain in-core to
prevent Mallory from analyzing the changes made to P. In
addition, this keeps the binary image of P unmodified. Fig. 1
shows the overview of the verified code execution process.
For the OS attestation part this paper provides a framework
where an external entity can determine the state of the OS Text
section, System call table, and the IDT on the client machine
entirely in software. This is achieved by providing newly
generated code to a kernel module which executes the provided
code. The response from the code indicates whether the OS is
compromised. It may be argued that once the OS is attested,
it can be used to attest the application rendering the second
scheme redundant. However, many applications execute on a
client platform, and each gets updated frequently. If the OS
performs integrity measurement on each binary, for security
requirements the definitions should reside somewhere in the
protected space. These definitions will have to be updated in
the protected space frequently as each software gets updated.
This can be considered a major overhead in the system. Instead
of this, the simpler solution is to have an external agent such
as the application vendor, or a network administrator provide
integrity measurement for the applications as the definitions
need to be updated only at one location.
Remote attestation for both schemes is implemented in this
work by downloading and executing new instance of attestation code for every attestation request by the client (Alice).
New code makes it difficult for the attacker (Mallory) to forge
any results generated by the integrity checking mechanism.
This is because Mallory has no prior knowledge of the structure or content of the code, as a result of which, Mallory has
no knowledge of the operations performed by the downloaded
code. To launch a successful attack, Mallory would have to
perform an ‘impromptu’ analysis of the executable code.
The remainder of this work is organized as follows, section

1049

integrity of client programs that connect to a remote server.
VIS contains an Integrity Measurement Module which reads
the cryptographically signed reference measurement (manifest)
of a client process. VIS requires that the pages of the client
programs are pinned in memory (not paged out), VIS restricts
network access during the verification phase to prevent any
malicious program from avoiding registration.
Detecting kernel level rootkits from within the Operating
System requires scanning the kernel memory to determine
whether or not the system call table and other sections in the
high memory has been modified [15]. Many rootkits change
the pointers of the system calls present in the system call
table. This causes execution to jump to rootkit code instead
of OS code during system call invocation. To detect these
changes an initial measurement is stored which contains the
state of the kernel memory in pristine state. During attestation
the runtime memory is scanned using the ‘/dev/kmem’ file.
The runtime memory is compared against the stored pristine
state to determine whether a rootkit has modified the kernel
image.
In Pioneer [7], the integrity measurement is done entirely
in software. The verification code for the application resides
on the client machine. The verifier (server) sends a random
number (nonce) as a challenge to the client machine. The
response to the challenge determines if the verification code
has been tampered or not. The verification code performs
attestation on an entity within the machine and transfers
control to it. Pioneer assumes that the challenge cannot be
redirected to another machine on a network, however, in
many real world scenarios a malicious program can attempt
to redirect challenges to another machine which has a clean
copy of the attestation code. Pioneer incorporates the values of
Program Counter and Data Pointer, in its checksum procedure;
both the registers hold virtual memory addresses. An adversary
can load a copy of the client code in a sandbox and obtain
integrity measurements from it. In TEAS [9], the authors
propose a remote attestation scheme in which the verifier
generates program code to be executed by the client machine.
Randomized code is incorporated in the attestation code to
make analysis difficult for the attacker. The analysis provided
by them proves that it is very unlikely that an attacker can
clearly determine the actions performed by the verification
code; however implementation is not described as part of
TEAS.
Genuinity [8] implements a remote attestation system in
which the client kernel initializes the attestation for a program.
It receives executable code and maps it as directed by the
trusted authority. The executable code performs various checks
on the client program, returns the results to the kernel on
the remote machine, which returns the results back to the
server. The server checks if the results are in accordance
with the checks performed, if so the client is verified. This
protocol requires operating system (OS) support on the remote
machine for many operations including loading the attestation
code into the correct area in memory. A problem with this
scheme is that the results are not communicated to the server

by the downloaded code. This may allow a malicious OS to
analyze and modify certain values that the code computes.
Genuinity has been shown to have weaknesses by two works
[16], [17]. The first work suggests placing attack code on the
same physical page as the checksum code such that the attack
code resides on the zero-filled locations in the page. Authors
of Genuinity countered these findings by stating that the attack
scenario does not take into account the time required to
extract test cases from the network, analyze it, find appropriate
places to hide code and finally produce code to forge the
checksum operations [18]. The attacks would require complex
re-engineering to succeed against all possible test cases. The
second work suggests that Genuinity reads 32 bit words for
performing a checksum and hence will be vulnerable if the
attack is constructed to avoid the lower 32 bits of memory
regions. This claim is countered by the authors of Genuinity
where they state that genuinity reads 32 bits at a time, and not
the lower 32 bits of an address.
Every attack scenario has its limitations as countered by
the authors of Genuinity. In this paper, remote attestation is
implemented by downloading new (randomized and obfuscated) attestation code for every instance of the operation.
This operation makes it difficult for the attacker to forge
any results that are produced by the attestation code. To
launch a successful attack, Mallory would have to perform an
‘impromptu’ analysis of the operations performed and report
the forged results to Trent within a specific time frame. This
can be considered difficult to achieve.
Program analysis requires disassembly of code and the
control flow graph (CFG) generation. CFG generation involves
identifying blocks of code such that they have one entry point
and only one branch instruction with target addresses. The
execution time of these algorithms is non-linear (n2 ) [19]. The
linux tool ‘objdump’ is one of the simplest linear sweep tools
that perform disassembly. It moves through the entire code
once, disassembling each instruction as and when encountered.
This method suffers from a weakness that it misinterprets
data embedded inside instructions hence carefully constructed
branch statements induce errors [20]. Linear sweep is also
susceptible to insertion of dummy instructions and self modifying code. Recursive Traversal involves decoding executable
code at the target of a branch before analyzing the next
executable code in the current location. This technique can
also be defeated by opaque predicates [21], where one target
of a branch contains complex instructions which never execute.
III. T HREAT M ODEL AND ATTACK S CENARIOS
This paper presents two schemes to attest the integrity
of applications on an untrusted platform. The first scheme
presents a method to perform remote attestation of the OS text
section, system call table, and the Interrupt descriptor table
(IDT). The second scheme presents a method to attest the
integrity of a user application and to means to check whether
the verified application continued execution or got replaced
by an attacker. For the sake of simplicity, the user application

1050

scheme is discussed prior to discussing the kernel attestation
scheme.
For the kernel attestation it is assumed that the kernel may
be compromised, system call tables may be corrupted, and
malware may change interrupt descriptors. The kernel attestation scheme measures the integrity of the OS text section,
Interrupt descriptor table and system call table. It is assumed
for the kernel attestation scheme that the trusted server (Trent )
is the OS vendor or a corporate network administrator who has
clear knowledge about the mappings in the clean OS image.
For the user application attestation we assume that the client
machine may be infected with user level malware, but not
infected with a kernel level rootkit as these infections would
be revealed by the kernel attestation scheme. We assume that
Mallory (the attacker) can start a clean copy of Alice’s installed application P to execute it in a controlled environment.
Mallory can also attempt to re-direct the challenge to another
machine which runs a clean copy of P. We also assume that
the client machine MAlice is not running behind a NAT. This
assumption is made as C takes measurements on MAlice to
determine if it is the same machine that contacted Trent. If
MAlice is behind a NAT the connections would appear as
coming from a router and not the machine, while C would
respond with the machine IP. It can also be noted that in case
Trent is a network administrator, the NAT does not come into
play at all.
In both schemes we assume that Alice will trust Trent to
provide non malicious code since Trent is a trusted server, and
Alice has means such as digital signatures that verify that C
was generated by Trent.

{
....
N = memory range
x,y,z = <random values placed
during compilation>
a = 0;
while (a<N){
checksum1 += Mem[a];
if ((a % y) == 0)
checksum2 += checksum1/x;
a = a -z;
a++;
}
send checksum2;
....
}
Fig. 2.

Snippet from the checksum code

MD5 of the application memory, x is not generated, however
y and z are generated to change the regions on which the MD5
values are obtained.
To prevent Mallory from analyzing the injected code, certain
obfuscations are placed in C such as changing the flow
of execution, changing locations of variables on the stack,
inserting dummy instructions, and placing incorrect executable
code in C. The incorrect code is fixed just prior to execution at
MAlice during attestation. Further details on these obfuscations
are present in our prior publication.
Trent also maintains a time threshold (T) by which the
response from MAlice is expected. If C does not respond in a
stipulated period of time (allowing for network delays), Trent
will know that something went wrong at MAlice . This includes
denial of service based attacks where Trent will inform Alice
that C is not communicating back.

IV. G ENERATING CODE FOR ATTESTATION
Trent is a trusted server that provides integrity measurement
code C to Alice. Alice injects the code on the user application
P. P transfers control to C and allows it to report measurements to Trent. Trent must prevent Mallory from analyzing the
operations performed by C. To achieve this, Trent can utilize a
combination of obfuscation techniques which are summarized
in this section. They are described in detail in our previous
publication [10].
Fig. 2 shows a sample snippet of the C mathematical
checksum code. The send function used in the checksum
snippet is implemented using inline ASM. The checksum is
dependent on the constants defined in the source code of
C. These constants It is evident that in order to forge any
results, Mallory must determine the value of checksum2 being
returned to Trent. This requires that Mallory identify all the
instructions modifying checksum2 and the locations on stack
that it uses for computation. Since constants like x are changed
during compilation, the checksum being sent out differs in
every instance.

V. R EMOTE K ERNEL ATTESTATION
To measure the integrity of the kernel, this work implements a scheme which executes newly injected code that
determines and provides results back to the trusted server.
The difference between the user application attestation and
the kernel attestation lies in its implementation. T rent is a
trusted server who provides kernel integrity measuement code
(Ckernel ) to Alice. The code is received on MAlice by a user
level application Puser which has been provided by T rent
previously. Puser provides the code to the Kernel on Alice’s
machine using a series of ioctl calls. Ckernel is received into a
kernel module Pkernel . It is assumed that Alice has means
such as digital signature verification scheme to verify that
the code was sent by T rent . On execution, Ckernel obtains
integrity measurements (Hkernel ) on the OS Text section,
system call table, and the interrupt descriptors table. These
results are passed to the userland application which forwards
them to T rent . These results can be encrypted using a simple
one time pad scheme if required, however as the executable

Similarly the other two constants ‘y’ and ‘z’ defines the
subregions and the overlap on which the checksum are taken.
These constants effect the overall output of the checksum. For

1051

code is generated for each instance, this is not a required
operation.
T rent is assumed to be a corporate network administrator,
or an OS vendor; hence T rent has access to a pristine copy
of the kernel executing on MAlice on which the expected
integrity values to be generated by Ckernel are obtained.
Although this seems like the trusted server would need infinite
memory requirements to keep track of every client, most OS
installations are identical as they are installed ‘off the shelf’.

jump_target = - ((address_C_kernel
+ jump_locations[1]
+ length_ofcall)
- address_routine2);
code_in_mem_buffer[call_locations[index]
+1]
= jump_target;
Fig. 3.

Fixing call targets in Ckernel

A. Implementation
The ‘code in mem buffer’ is an array that holds the Ckernel
routine prior to send to the client, and ‘call locations’ is
an array which holds the offsets in Ckernel where the call
instructions are present.

The framework for this paper was implemented on an x86
based Ubuntu 8.04 machine executing a 2.6.24-28-generic
kernel. This section describes the integrity measurement of
the OS text section (which contains the code for system
calls and other kernel routines), the system call table and the
interrupt descriptor table. The overall scheme is similar to the
techniques described in [15].
In Linux, the exact identical copy of the kernel is mapped
to every process in the system. The Text section was found at
virtual memory 0xC0100000. The end of kernel text section
was located to be at 0xC03219CA. The system call table was
located at 0xC0326520 and was found to be 1564 bytes. The
Interrupt descriptor table was located at 0xC0410000 to be of
size 2048 bytes. At the server side the Ckernel is compiled as
part of a device driver (kernel module). The driver has an ioctl
which on being called by a userland application provides the
executable code of Ckernel as a character buffer. This buffer is
sent to persistant storage along with the integrity measurement
value expected from it for a particular kernel. For the sake of
simplicity, the server and the client had the same OS image
during our tests. This process is repeated to generate a large
number of Ckernel routines. On an attestation instance, any
one of the generated Ckernel routines is read and loaded in
memory. This process is essential as the loader removes certain
instructions specific to ‘ftrace’ and replaces them with multi
byte no-ops during module insertion.
The trusted server Trent communicates to a user level
application Puser . Puser can be assumed to be an application
provided by Trent . Puser communicates to a kernel module
Pkernel (also provided by Trent ) using a character device
called ‘remote attestation device’ which is created using a
command ‘mknod /dev/remote attestation device c 100 0’.
The last two numbers in the command provide the MAJOR NUM and MINOR NUM for the device. At the client
end, kernel modules can be relocated, hence the target to
function calls can be potentially different during each boot
instance. To execute any function in the kernel module Pkernel
which are essential, T rent obtains their addresses using
various ioctl calls through Puser , one for each such address.
Once Ckernel is brought into memory at the server end during
attestation initiation, the call instructions are identified in
the code and the correct target address is patched on the
call instruction. The patching routine essentially performs
function as shown in figure 3. The address of routine 2 is
the routine which Ckernel will call to perform any function.

The patched Ckernel is then sent to Puser . Puser executes
another ioctl which receives Ckernel and places it in the location specified. After the code is injected, T rent issues a message to Puser requesting the kernel integrity measurements.
Puser executes an ioctl which causes the Pkernel to execute
the injected code. Ckernel reads various memory locations in
the kernel and passes the data to the MD5 code, Ckernel also
computes arithmetic checksums on the read data. The MD5
code returns the MD5 checksum value to Ckernel which in turn
returns the value to the ioctl handler in the Pkernel . Pkernel
then passes the MD5 and arithmetic checksum computations
back to Puser which forwards the results to the T rent .
The measurement code also obtains the location of the system call table in memory by processing the ‘SIDT’ instruction.
The SIDT instruction contains the runtime location of the
system call table to be processed by the software interrupt
(INT 80 instruction). The code to process the runtime system
call table was obtained from [22] is shown in figure 4 and
is explained as follows. The IDTR structure is used to store
the base address of the IDT. The Idt descriptor structure is
used to store the contents at a particular IDT descrptor entry.
The ‘SIDT’ instruction is parsed to obtain the location of the
system call. The address of the system call is a 32 bit value.
The high descriptor value is left shifted by 16 bits to place in
bits 31:16, then the low descriptor value is slotted in bits 15:0
by doing a bitwise OR operation.
If required the disable interrupt instruction can be issued by
Ckernel to prevent any other process from obtaining hold of the
processor. It must be noted that in multi processor systems, the
disable interrupt instruction (CLI) may not prevent a second
processor from smashing kernel integrity measurement values.
However, as the test cases are different for every attestation
instance, Mallory may not gain anything by smashing integrity
measurement values.
It can be noted that even if the client does not communicate
the address of the functions, Pkernel can be designed such
that the MD5 driver provided by T rent and the MD5 code
reside on the same page. This means that the higher 20 bits of
the address of the MD5 code and the downloaded code will

1052

struct Idt_Descriptor {
unsigned short offset_low;
unsigned short selector;
unsigned char zero;
unsigned char type_flags;
unsigned short offset_high;
} __attribute__ ((packed));
struct IDTR {
unsigned short limit;
void *base;
} __attribute__ ((packed));
struct IDTR idtr;
struct Idt_Descriptor idtd;
void *system_call;
asm volatile("sidt %0" : "=m"(idtr));
memcpy(&idtd, idtr.base +
0x80*sizeof(idtd), sizeof(idtd));
system_call_table_location = (void*)
((idtd.offset_high<<16) | idtd.offset_low);
Fig. 4.

Determining runtime location of system call table

be the same and only the lower 12 bits would be different.
This allows the T rent to determine where Ckernel will reside
on the client machine, and automatically calculate the target
address for the MD5 code.

for a compromised P to mimic C’s behavior and forge the
results that would be generated by C on a clean instance of P.
Some additional information in M1 allows Trent to determine
whether the challenge was replayed, or performed inside a
controlled environment. Trent has a local copy of P on which
the same sets of tests are executed as above to produce a value
M0 . When Trent gets the results from C, Trent compares M1
and M0 ; if the two values are the same then Alice is informed
that P has not been tampered.
To determine that P was not executed in a sandbox environment, C determines the number of processes having an open
connection to Trent on the client machine. This is obtained
by scanning the remote address and remote port combinations
on each of the port descriptors in the system. This is further
explained in the previous publication. To determine whether
C was bounced to another machine, Trent obtains the address
of the machine that C is executing on. Trent had received
an attestation request from Alice, hence has access to the IP
address of MAlice . If C returns the IP address of the machine
it is executing on, Trent can determine if both values are the
same. It may be argued that IP addresses may be spoofed, but
additional safeguards like scanning the port descriptors on the
machine and reusing the existing connection to Trent prevents
mitigates spoofing based attacks. Communication with Trent is
achieved by executing the software interrupt with the interrupt
number for the OS call socketcall as presented in our prior
work [10].
B. Verified code execution
Once Remote Attestation determines the integrity of a
program, the server begins communication and sharing of
sensitive data with the client program. However, Mallory (the
attacker) may choose to wait till the attestation process is
complete, and substitute the client program P with a corrupted
program Pc . To prevent Mallory from doing this, Trent has to
obtain some guarantee that the process that was attested earlier
is the same process performing the rest of the communication.
Trent cannot make any persistent changes to the binary, hence
Trent has to change the flow of execution in the client process
such that the sequence of events reported will allow Trent
to determine whether the attested process is executing. As
discussed before, Trent knows the layout of the program P.
At the end of Remote Attestation, Trent sends a new group of
messages to C. The message contains executable code F1 that
Trent instructs C to place at a particular location in P. Trent
also instructs C to modify a future function call F0 in P such
that instead of calling F0 , P calls F1 . F1 communicates back
to Trent, this way Trent knows that the copy of P which was
attested in the previous step is still executing. At the end of
its execution, F1 undoes all its stack operation and jumps to
the address where F0 is located.
If F1 executes a return instruction then control would move
back to P and the functionality of F0 would be lost. It
cannot do a function call to F0 as this may cause loss of
some parameters passed by P. The stack is defined during
compilation time and allocated only during runtime. The

VI. ATTESTATION OF USER APPLICATION
A. Software based remote attestation
This section contains a brief overview of the user application
attestation scheme, further details are present in our prior
publication [10]. Alice (the user) has previously installed an
application (P). Alice wants to get the application attested to
determine whether it has been compromised. Alice contacts
a Trusted authority Trent who provides remote attestation
services. When Alice contacts Trent using P, the remote attestation starts. The trusted server (Trent) has prior knowledge
of the code structure and process image of P of P. Trent
provides code (C) generated on the fly to perform an integrity
check on it. To prevent a replay based attack, Trent changes
the operations performed by C in every attestation instance.
Generating code on the fly makes it difficult for Mallory
(the attacker) to determine the results of the computations
performed by C beforehand. The attestation code C is injected
by P on itself. This allows C to execute within the process
space of P, utilizing all descriptors of P on MAlice without
creating new ones. The advantage of this is that C can
determine whether more than one set of descriptors are present
for P.
When P runs C, C hashes the code section of P in random
overlapping sections and sends the measurement value M1 to
Trent. The overlapping subregions are defined in the code of C
and is changed in every attestation instance to make it difficult

1053

__asm__

("mov %ebp, %esp \n"
"pop
%ebp \n"
"jmp
0x8048bff \n");
Fig. 5.

int fix_address(void){
int length_ofF1= 0xbd;
int location_F0 = 0x08048bff;
int location_F1 = 0x08048c92;
int offset_of_jmp_in_F1 = 0xa3;
int eip_offset_for_jmp = 5;
....
T_address = location_F0 (location_F1
+ offset_of_jmp_in_F1
+ eip_offset_for_jmp );
Write_back [0xa4] = T_address &
0x000000FF;
Write_back [0xa5] = (T_address &
0x0000FF00) >>8;
Write_back [0xa6] = (T_address &
0x00FF0000) >>16;
Write_back [0xa7] = (T_address &
0xFF000000) >>24;
....
}

Tail portion of F1

compiler generates code which first saves the base pointer on
the stack; this saves the frame for the previous function. It then
moves the current stack pointer (which points to the location
beyond the last push), into the base pointer. This is done so
because locations on the local stack are addressed relative to
the current base pointer. The generated code subtracts some
memory from the current stack pointer; this allocates the
memory for the local variables. The last two instructions in
a function are usually leave and ret. The leave instruction
reverses all net stack operations performed by the function
by moving the base pointer value into the stack pointer and
pops the value stored in stack location for the base pointer of
the previous frame. The ret instruction resumes execution at
the return address stored on the stack.
F1 allocates some stack for its local memory during execution. However, instead of letting it return to P, a jump
instruction is executed after the stack operations are reversed.
F1 can either execute the MOV instruction to place the value
of the base register on the stack pointer (this reverses stack
allocation), pop the current stack value into the base pointer,
and then jump to the start of F0 . Another approach would
be to execute the leave instruction and jump to F0 . Both the
methods allow F0 to receive all parameters passed on the stack
by P and resume normal execution. When F0 executes a return
instruction, the control moves back to P.

Fig. 6.

Fixing Jump target

program by correcting the target of the jump instruction as
seen in fig. 6. The code calculates the actual 4 byte address
for the JMP instruction and then writes it back in the binary
in the little-endian format.

VII. R ESULTS
Table I provides cumulative results for various operaR
R
4
with 1 GB of RAM and an
tions on an IntelPentium
TM
R
2 Quad machine that had 3 GB of RAM. As
IntelCore
expected, the machine with the Intel Pentium 4 processor
has slightly lower performance than a platform with 4 Intel
Core 2 processors. The application attestation code takes lower
time than the kernel attestation scheme as the size of the
application (11 Kilobytes) is small compared to the size of
the OS text section, system call tables, and IDT. The network
delay shown in the table is for each send/receive operation
occurring between the client and server machines. Hence
if the two machines perform 10 sends/receive, the network
delay value will be greater than other components. The code
generation time does not include the time required to issue
the make command for the kernel module. This is because the
make command had variable response rates and took order
of a few seconds. Due to this aspect, code generation can be
seen as a major overhead for the server for each attestation. To
alleviate the load on the server, code generation can occur for
a number of test cases beforehand, and use any one of them
in a random fashion during attestation.
The application attestation scheme can detect modifications
made to the text section of the application. It cannot detect
changes made in the runtime on the heap and the stack section.

C. Implementation
The implementation details of the user application attestation is described in detail in our previous publication. The
implementation of verified code execution is described as
follows. As part of sending messages to C, Trent provides
the code of F1 , the location where F1 should be placed, and
a particular address inside P which corresponds to a function
call to F0 . C places the code F1 at the specified location, and
changes the target of the call instruction inside P to point to
F1 . When F1 executes it communicates to Trent and informs
Trent that it executed. It can be noted here that F1 can use the
existing connection to Trent or open a new connection. Since
Trent generated F1 , Trent can place a random secret inside F1
which gets communicated to Trent. On receiving the secret,
Trent knows that F1 executed.
The tail portion of F1 is provided with code similar in
functionality as shown in fig. 5. F1 clears its stack by moving
the base pointer into the stack pointer. It then pops the base
pointer value of the previous routine into EBP. Then finally
executes a jump to F0 . As F1 is compiled as a stand alone
function, the gcc complier generates an incorrect target address
for the Jump instruction. This is fixed by the server side

1054

[4] F. Stumpf, O. Tafreschi, P. Röder, and C. Eckert, “A robust integrity
reporting protocol for remote attestation,” in Second Workshop on
Advances in Trusted Computing (WATC 06 Fall), November 2006.
[5] R. Sailer, X. Zhang, T. Jaeger, and L. Van Doorn, “Design and
implementation of a TCG-based integrity measurement architecture,” in
Proceedings of the 13th USENIX Security Symposium, 2004, pp. 223–
238.
[6] P. Loscocco, P. Wilson, J. Pendergrass, and C. McDonell, “Linux kernel
integrity measurement using contextual inspection,” in Proceedings of
the 2007 ACM workshop on Scalable trusted computing. ACM, 2007,
pp. 21–29.
[7] A. Seshadri, M. Luk, E. Shi, A. Perrig, L. van Doorn, and P. Khosla,
“Pioneer: verifying code integrity and enforcing untampered code execution on legacy systems,” ACM SIGOPS Operating Systems Review,
vol. 39, no. 5, pp. 1–16, 2005.
[8] R. Kennell and L. Jamieson, “Establishing the genuinity of remote computer systems,” in Proceedings of the 12th USENIX Security Symposium,
2003, pp. 295–308.
[9] J. Garay and L. Huelsbergen, “Software integrity protection using timed
executable agents,” in Proceedings of the 2006 ACM Symposium on
Information, computer and communications security. ACM New York,
NY, USA, 2006, pp. 189–200.
[10] R. Srinivasan, P. Dasgupta, T. Gohad, and A. Bhattacharya, “Determining the integrity of application binaries on unsecure legacy machines
using software based remote attestation,” in Information Systems Security, ser. LNCS. Springer Berlin/Heidelberg, 2011, vol. 6503, pp.
66–80.
[11] N. Petroni Jr, T. Fraser, J. Molina, and W. Arbaugh, “Copilot-a
coprocessor-based kernel runtime integrity monitor,” in Proceedings
of the 13th conference on USENIX Security Symposium-Volume 13.
USENIX Association, 2004, p. 13.
[12] R. Sailer, “I.B.M. research - integrity measurement architecture,”
Retrieved November 3, 2010: http://domino.research.ibm.com/comm/
research people.nsf/pages/sailer.ima.html, 2008.
[13] T. Garfinkel, B. Pfaff, J. Chow, M. Rosenblum, and D. Boneh, “Terra: A
virtual machine-based platform for trusted computing,” ACM SIGOPS
Operating Systems Review, vol. 37, no. 5, p. 206, 2003.
[14] R. Sahita, U. Savagaonkar, P. Dewan, and D. Durham, “Mitigating the
lying-endpoint problem in virtualized network access frameworks,” in
Managing Virtualization of Networks and Services, ser. Lecture Notes
in Computer Science, A. Clemm, L. Granville, and R. Stadler, Eds.
Springer Berlin - Heidelberg, 2007, vol. 4785, pp. 135–146.
[15] J. Levine, J. Grizzard, and H. Owen, “Detecting and categorizing kernellevel rootkits to aid future detection,” Security & Privacy, IEEE, vol. 4,
no. 1, pp. 24–32, 2006.
[16] U. Shankar, M. Chew, and J. Tygar, “Side effects are not sufficient
to authenticate software,” in Proceedings of the 13th USENIX Security
Symposium, 2004, pp. 89–102.
[17] A. Seshadri, A. Perrig, L. Van Doorn, and P. Khosla, “Swatt: Softwarebased attestation for embedded devices,” in Security and Privacy, 2004.
Proceedings. 2004 IEEE Symposium on. IEEE, 2004, pp. 272–282.
[18] R. Kennell and L. Jamieson, “An analysis of proposed attacks against
genuinity tests,” CERIAS Technical Report, Purdue University, Tech.
Rep., 2004.
[19] K. Cooper, T. Harvey, and T. Waterman, “Building a control-flow
graph from scheduled assembly code,” Dept. of Computer Science, Rice
University, Tech. Rep., 2002.
[20] B. Schwarz, S. Debray, and G. Andrews, “Disassembly of executable
code revisited,” in Reverse Engineering, 2002. Proceedings. Ninth Working Conference on. IEEE, 2003, pp. 45–54.
[21] C. Collberg, C. Thomborson, and D. Low, “Manufacturing cheap, resilient, and stealthy opaque constructs,” in Proceedings of the 25th ACM
SIGPLAN-SIGACT symposium on Principles of programming languages.
ACM, 1998, pp. 184–196.
[22] Weblink, “Kernel mode hooking,” Retrieved May 5, 2011: http://
codenull.net/articles/kmh en.html.

TABLE I
T IME TO COMPUTE MEASUREMENTS
Time (ms)
Generation of challenge
Execution of C
Execution of Ckernel
Network delay

Intel Pentium 4
12.8
0.6
175
21

Intel Core 2 Quad
5.5
0.4
54.3
15

The stack and the heap contain the current program execution
state and are highly variable. On the Intel x86 architecture
for Linux, the OS enforces that these two sections cannot
contain executable pages. Hence the modifications made to
the application would reside only on the code section of the
application.
The kernel attestation scheme can detect rootkits that make
changes to the system call table and the IDT as described in
[15]. This scheme cannot determine rootkits that do not change
the system call table or the IDT.
VIII. C ONCLUSION AND F UTURE W ORK
This paper presents a technique to obtain the integrity of a
user application entirely in software. A trusted external entity
provides Alice with generated code that when executed on the
client side provides guarantee that the client side application
is not compromised. This paper also extends remote attestation by verifying whether the application attested continued
executing or was replaced by the attacker. The check involves
placing new code in the in-core image of the application and
replacing a function call inside the application to point to the
new code. The execution of the new code provides an attesting
server the guarantee that the application was not replaced. A
series of such changes made inside the attested application
can dramatically reduce the opportunities that an attacker may
have to hijack authenticated sessions by tampering with the
client end software. This paper also presented a technique to
obtain the integrity measurement of the OS text section, system
call table and Interrupt descriptor table. These measurements
are important as the remote attestation scheme for the user
application requires the assistance of system calls and the
interrupt interface to obtain its measurements. This scheme
was implemented on Intel x86 architecture using Linux and
its performance was measured. As future work the Remote
Attestation scheme can be implemented with hardware assisted
virtualization as every consumer x86 computing platform is
currently manufactured with this ability.
R EFERENCES
[1] T. Ostrand and E. Weyuker, “The distribution of faults in a large
industrial software system,” in Proceedings of the 2002 ACM SIGSOFT
international symposium on Software testing and analysis. ACM, 2002,
p. 64.
[2] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler, “An empirical
study of operating systems errors,” in Proceedings of the eighteenth
ACM symposium on Operating systems principles. ACM, 2001, pp.
73–88.
[3] L. Wang and P. Dasgupta, “Coprocessor-based hierarchical trust management for software integrity and digital identity protection,” Journal
of Computer Security, vol. 16, no. 3, pp. 311–339, 2008.

1055

Linking Consistency with Object/Thread Semantics:
An Approach to Robust Computation+
Raymond C. Chen

Partha Dasgupta

School of Information and Computer Science
Georgia Institute of Technology
Atlanta, G A 30332

allows users to write their own commit routines t o provide custom recovery for similar purposes. Quicksilver ([HMSCSS]) goes
further, allowing customized commit protocols as well as commit and recovery routines. However, writing algorithms that
correctly use these custom locking and recovery schemes is left
t o the application programmers. These algorithms can be quite
complex and intricate. Locus ([wPLP85]) allows processes and
actions t o co-exist. However, a transaction locks the portions
of the fles it accesses and the locks apply t o both processes
and transactions. This preserves serializability for transactions
but penalizes processes when a transaction is accessing the same
data.

Abstract

This paper presents an object/thread based paradigm that links
data consistency with object/thread semantics. The paradigm can
be used to achieve a wide range of consistency semantic5 from strict
atomic transactionsto standard process semantics. The paradigmsup
ports three types of data consistency. Object programmers indicate
the type of consistency desired on a per-operation bgsis and the system
performs automatic concurrency control and recovery management to
ensure that those consistency requirements are met. This allows programmers to customize consistency and recovery on a per-application
basis without having to supply complicated, custom recovery management schemes.
The paradigm allows robust and non-robust computation to o p
erate concurrently on the same data in a well-defined manner. The
operating system need support only one vehicle of computation - the
thread.

1

We have developed and are implementing a paradigm that
we feel meets the following goals:
Allows the object programmer t o tailor consistency and
concurrency in a manner appropriate to each application.

Introduction

The ability of a system to maintain consistent data in the face of
hardware breakdowns, computation aborts, and other forms of
failure, is loosely termed robustness. Robust computation transforms data from one consistent state to another in spite of failures, where the definition of consistent is application dependent.
Thus robust systems may be termed “consistency-preserving”
systems. In a general purpose (distributed) system, there is a
need for both robust as well as non-robust computations and
data, as many applications do not want t o (or have to) pay the
overhead costs needed in managing robust computations.

Supports a range of consistency requirements from besteffort consistency to strict atomic transactions.
Enables the operating system t o handle concurrency and
consistency issues in a transparent manner, without requiring the programmer t o develop complex locking and
recovery schemes tailored for each application.
Presents one uniform world-view that encompasses both
robust and non-robust computation, allows them to coexist in the same system and if desired, concurrently access
the same data with well-defined results.

The most popular paradigm for robust computation are the
atomic transaction based paradigms. The transaction paradigm
is based on the totality concept - in principle, all the effects
of a transaction are reflected in the stable (or permanent) state
or none are. However, the transaction paradigm can be quite
restrictive and most systems that provide transactions ([All831
[LS83] [PNSB] [SBD*84]) also provide “escape” mechanisms by
which users may take advantage of application-specific semantics
to increase concurrency and decrease overhead without sacrificing correctness. For example, Argus ([LDH*87]) allows atomic
actions t o touch non-atomic data, thus allowing actions t o communicate. A paradigm developed for Clouds u.1 provides recoverable and non-recoverable data segments, as well as custom locking and commit handling ([AM831 [WilS7]). These
can be tailored for a variety of applications that cannot run
as strict transactions. The AuaZon/Camelot system ([HW87])

This paradigm is designed for systems supporting an object/thread model of computation ([WCC*74] [Jon791 [Lis821
[LS83] [All831 [ABLN85] [BMS85] [B*85]). The only vehicle of
computation in this paradigm is the thread. Consistency and
concurrency control are expressed through the semantics of objects, object invocation, and thread creation.
The paradigm is being implemented as part of the Clouds
u.2 operating system, a passive object-based general-purpose

distributed system. Clouds v.2 is built around the Ra kernel,
an extendible, minimal kernel that provides light-weight processes, segment-based virtual memory management, short-term
scheduling, and the ability to plug in system-level objects t o perform additional operating system level functions ([BAHK*89]).
Ra has been implemented on Sun 3/60 workstations, is operational and the Clouds v.2 implementation effort is currently
underway.

‘This work was supported in part by NSF grant CCR-86-19886and
NASA grant NAG-1-430.

CH2706-0~9/~/0121$01.00
0 1989 IEEE

121

Thread Label
at invocaiion

Section 2 of this paper presents the basic object/thread model.
Sections 3-6 present the paradigm while sections 6 and 7 deal
with issues arising due to concurrent execution.

2

Objects and Threads

GCP

Opemiion Consistency Label
LCP
S

GCP

Nochange

T+LCP

LCP

T-rGCP

T-rLCPt

S

T+GCP

T-rLCP

An object is a long lived entity containing state (persistent data)
and a set of operations that operate on this state. The object
can be thought of as a virtual address space that is named and
permanent. In fact, objects can be implemented as permanent
virtual spaces ([DLASB]). This virtual space contains the data
as well as the operators. The operations are called entry-points
t o the object.

T-rS

I
I
Nochange

Nochange
Nochange Nochange
T-S

Note: "T 4' 3 "Thread transforms to"
t

Though the label remains the same, this is a transforming
invocation.
Figure 1: Consistency Label Compatibility

Operations on objects are invoked by threads of execution
(or threads for short). A thread is the carrier of execution in
a distributed system much like a process is the carrier of execution on a centralized system. The entry of a thread into an
object space is called object invocation. Object invocation allows parameters t o be passed t o the operation being invoked at
the discretion of the object programmer. A thread begins its
computation at an entry point in an object and flows though all
objects (if any) invoked by this operation. Invoked objects may
reside on any site in the distributed system. The object invocation terminates when the operation completes (returns). Note
that an operation on an object can invoke operations on other
objects (or operations on the same object). Objects are passive
in the sense that they define the data and the operations, but
do not execute by themselves. The threads are the active component of this programming model. This programming model
is based on the processing environment provided by the Clouds
operating system.

3

I

the thread currently supports. When a thread invokes an object, the thread and operation labels are checked to see if they
are compatible. If they are not, the thread transforms to meet
the requirements of the object operation. The thread transforms
back t o its previous state when the operation terminates, whereupon the system performs the appropriate synchronization and
recovery processing.
Objects contain persistent data items and reside in a singlelevel store backed by stable storage ([LS79]). The versions residing in the single-level store are known as the base (or permanent)
versions while those residing on stable storage are known as the
stable versions. Stable versions are presumed to survive system
failures. No such assumption is made about the base versions.

4

Consistent Operations

Each entry-point is marked with one of four labels by the object
programmer:

The Basics

Global Consistency Preserving (GCP)
Local Consistency Preserving (LCP)

Our approach is based on the notion of three types of consistency
that an object programmer would want to maintain - global
consistency, local consistency, and standard. Global consistency
reflects a need on part of the object programmer to maintain a
group of cooperating objects in a consistent manner. Properly
used, global consistency guarantees that the operating system
will automatically control concurrency and recovery across a set
of objects so that their permanent states will stay both internally
consistent and consistent with respect to each other.

Standard (S)
Inherited (I)
Each thread of execution in the system also bears a consistency label. A thread may be a global consistency preseruing thread (gcp-thread),local consistency preserving thread (lcpthread), or a standard thread (s-thread) depending on whether
the state of the thread's consistency label is set to global consistency preserving, local consistency preserving, or standard
at that time. Both gcp-threads and lcp-threads reflect a commitment towards at least a minimal degree of data consistency
and can be classified as different types of consistency-preserving
threads (cp-threads). Thus in cases where the discussion applies
t o both lcp-threads and gcp-threads, we shall refer t o both gcpthreads and lcp-threads as cp-threads and distinguish between
them only when necessary.

Local consistency, on the other hand, is appropriate for those
circumstances where inter-object consistency may be too strong
a criterion. Sometimes, consistency within one object, or intrs
object consistency is enough. Local consistency, properly used,
guarantees that the system will control concurrency and recovery in a way that the object state will always stay internally
consistent.
The standard degree of consistency is the degree of consistency that users in the process world have become accustomed
t o -that is to say no guarantees at all. If nothing fails, the data
will be consistent, but the system can not guarantee consistency
in the face of failure.

When a thread with consistency label X invokes an operation
T marked with consistency label Y,if X is not compatible with
Y, then the thread fmnsforms to a thread bearing consistency
label Y.The consistency label of the thread reverts t o the original value X when operation T completes. This process is called
thread transformation and T is termed a transforming invarofion. Thread label/operation label compatibility is summarized
in figure 1.

Object programmers label object operations. These labels
indicate to the operating system the type of consistency desired by the object programmer for each operation. Threads
carry consistency labels which indicate the type of consistency

122

4.1

4.3

Global Consistency Preserving

If a thread invoking a standard ( s ) operation is not an s-thread,
then the thread is transformed into an s-thread and the current
invocation is a transforming invocation. Upon completing the
invocation, the thread reverts t o its previous state.

If a thread invokes a global consistency preserving (gcp) opera
tion and the invoking thread is not a gcp-thread, the thread becomes a gcp-thread and the current invocation is a transforming
invocation for that thread.

Standard invocations do not commit or abort their changes,
thus they do not have a change-set associated with them. If a
thread updates the value of a data item in a standard invoca
tion, that update does not create a shadow version. The update
is instead applied directly to the latest version of the data item.
The system will eventually propagate the changes to stable storage, however, the system makes no guarantees as to when this
will occur. This is consistent with the "best-effort" semantics of
s-threads and the notion of a process. (The definition of "latest
version" will be discussed in greater detail in section 6.3.)

When a gcp-thread attempts to update a persistent data
item, the update is not immediately reflected in the permanent
version of the data item. Instead, a new shadow version of that
item is created. Further updates to that item by the same thread
are reflected in the shadow version.
Let a change-set be the set of the names of all data items
for which a gcp-thread has created a shadow version. A changeset is associated with each gcp transforming invocation. A data
item is named by a change-set if the name of the data item
is contained in the change-set. Every time a gcp-thread updates a persistent data item if the item is not named by the
change-set of the transforming invocation for that gcp-thread,
the name of the item being updated is added t o that change-set.
Upon completion of the transforming invocation, the change-set
for that invocation will be atomically committed (or aborted)
t o the base and stable versions. A change-set is committed by
atomically setting the values contained in both the base and s t a
ble versions of all items named by the change-set equal t o the
values contained in the shadow versions of all items named by
the change-set. This normally requires executing a 2-phase commit protocol ([Gra79]). We refer to this process as committing
changes or committing a change-set. A change-set is aborted by
simply discarding the shadow versions of all data items named
by the change-set.

4.4

Inherited

Inherited operations are compatible with all threads and indicate
the absence of a consistency requirement. Thus, inherited entrypoints inherit the consistency label of the entering thread. If the
invoking thread is a gcp-thread, the invocation behaves as a gcp
invocation would. The same is true of s-threads or lcp-threads.
Inherited operations can be used to provide the object programmer with even more control over object consistency. Inherited operations can be used for operations that do not access
persistent data or as Wters". Inherited entry-points provide a
means by which objects may first examine the consistency label on the thread before allowing further processing t o occur.
Depending on the state of the thread and the object implementation, the object may allow the invocation to proceed (possibly
invoking an internal entry-point t o perform the actual processing) or it may reject the invocation by returning to the caller.
This allows an object programmer t o provide a uniform external
interface for all threads while using different implementations for
different thread types (gcp, lcp, or s).

If a gcp-thread invokes a gcp entry-point, the thread remains
a gcp-thread and the operation is executed. Since the thread was
already a gcp-thread when the current operation was invoked,
the thread must have already invoked a transforming invocation.
Therefore, updates made during the current operation are not
committed when the current operation completes. The updates
are reflected in the change-set of the transforming invocation
and are committed (or aborted) when the transforming invocation commits (or aborts) its change-set. Since gcp-threads
operate on shadow versions, no change made while the thread
was a gcp-thread will be made permanent unless the transforming invocation successfully completes and all shadow versions
are successfully committed.
4.2

Standard

5

Thread Creation

We allow threads t o create other threads to execute object operations. The newly-created thread is said to be the child of
the thread that created it (which in turn is referred to as the
parent thread). A parent thread may monitor the status of its
children, kill, suspend, and resume them. All threads execute
concurrently.

Local Consistency Preserving

In addition to consistency labels, all threads are also assigned
a nesting attribute. Threads may be either top-level threads or
nested threads. If the parent thread is a gcp-thread, the child
thread may be created as either a top-level thread or a nested
thread. Otherwise the child is created as a top-level thread.

Local consistency preserving (lcp) operations are similar t o gcp

operations in that all changes made during an lcp invocation
are made on new shadow versions and atomically committed
(or aborted) when the lcp operation completes. However, every
invocation of an lcp operation is a transforming invocation that
transforms the invoking thread into an lcp-thread regardless of
the previous state of the thread. That is, all changes made in
an lcp invocation are committed or aborted when the invocation
terminates even though the thread may have been executing as
a gcp-thread before invoking the lcp operation.

Top-level threads are s-threads when first created. Nested
threads are gcp-threads when created and the first operation invoked by the thread is a transforming invocation for that thread.
If a nested gcp-thread transforms into a non-gcp thread, it is
treated as a top-level thread until the invocation that transformed the thread into a non-gcp thread terminates.

Like gcp operations, each lcp invocation has a change-set
associated with it and that change-set is committed (or aborted)
when the invocation completes.

Top-level threads commit their changes to permanent state.
Nested threads commit their changes t o their parent. That is,
changes committed by a nested gcp-thread are not immediately
reflected in the permanent system state. Instead, those changes

123

6.2

are passed onto the parent. This will be discussed in greater
detail in section 6.

6

The cp-thread locking rules outlined in section 6.1 are a straightforward adaptation of the rules laid out for nested actions in
[Mos81]. These rules lead to modeling uncommitted changes t o
a data item by versions on a version stack.

Locking and Visibility

Visibility becomes an issue when two or more threads concurrently executing within the same object attempt to read or write
the same data item. Consider the read or write a touch on that
item. In our paradigm, the operating system enforces certain
system level locking rules on permanent data by requesting and
releasing system-level locks on behalf of some threads that attempt to touch that data. These locking requests and the locks
themselves are neither visible nor controllable by the user. This
locking is 2-phase ([EGLT76]) by definition as will become obvious in the discussion below and is performed automatically by
the operating system.

6.1

Version Stacks

Every persistent data item can be viewed as possessing a version stack with the permanent version at the base. The entire
stack exists in volatile memory, however, the permanent versions
are backed by stable versions residing on stable storage and all
commits t o permanent versions are mirrored on the stable versions.
The versions on the version stack are referred to as versions

0, 1,2, ...n- 1,n where version 0 is the permanent version, version

n is the top version and n is the height (or depth) of the stack.
Version creation for gcp-threads is based on the nesting level of
the thread in the thread-tree. Version creation for lcp-threads is
based on the number of times different unfinished lcp invocations
in the same object have touched the same item. In both cases,
the idea behind the version management is to allow gcp and
lcp invocations to operate on private shadow versions and then
atomically commit or abort those versions.

Consistency-Preserving Threads

Consistency-preserving threads automatically lock any data they
touch. Read/Write locks are automatically requested by the operating system on behalf of the executing cp-thread when the
thread attempts t o read/write a data item.
The locking rules for interacting cp-threads are similar to the
ones described in [Mos81] for nested actions. Thread creation
may lead to a tree of one or more cp-threads, all with a common top-level ancestor. We call these thread trees consistencypreserving thread-trees.

When a gcp-thread T of depth d gains a write-lock on a data
item it has not previously touched, a new version of that item
(call it X )is created and initialized to the value of the version
on top of the version stack. Let n be the depth of the version
stack. If d - n > 1, then d - n - 1 new versions are also created,
initialized t o the value of the top version on the stack, and placed
onto the version stack as versions n 1,n 2, ...d - 1. X is then
placed on top of the stack as version d . If d - n = 1, X is simply
placed on top of the version stack as version d.

Let T be a cp-thread in a cp-thread tree. A thread U is an
ancestor of T if it lies on the path from T to the root of the
thread-tree. This path includes T itself, thus a thread is always
an ancestor of itself. A cp-thread T has a depth of n if and only
if the path from T to the root of the tree is n nodes long where
the length of the trivial path (a node to itself) is length 1. A live
cp-thread is a non-terminated thread that has invoked but not
yet completed at least one transforming consistency-preserving
invocation. A cp-thread T may write-lock permanent data only
if all other live cp-threads that hold a read or write-lock on that
data are ancestors of T. A cp-thread T may read-lock permanent
data only if all other live cp-threads that hold a write-lock on
that data are also ancestors of T. A cp-thread can not read
(or write) a data item unless it obtains a read-lock (or writelock) on that item. If a cp-thread attempts to read (or write) a
permanent data item and the system can not grant that thread
the read (or write) lock due t o the locking rules, the thread
blocks until the lock can be granted.

+

+

When an lcp-thread touches a data item it has not touched
in the current lcp invocation, a new version is created, initialized
t o the value of the top-most version on the version stack, and
placed on the stack.
The locking rules together with the rule that a thread may
update permanent state only if it has no live descendants ensure
that there exists only one version at each level of the stack. Thus
the stack is always a linear stack and never a cactus stack.
When a cp-thread commits, all items in its change-set are
committed or aborted. A gcp-thread of depth n commits a
change by copying the top version of the item on the version
stack (version n) to the next-most-recent version on the version
stack (version n - 1) and then discarding the top version. Thus
nested gcp-threads commit their changes to intermediate versions that will then be committed or aborted by their parents.
However, having a depth of one, top-level gcp-threads commit
their changes t o the permanent versions.

In addition, t o prevent undesirable interactions between concurrently executing parents and children on a cp-thread tree, a
thread may update permanent state only if it has no live descendants. This constraint ensures that all non-terminated children
of a thread will see the same (intermediate) versions of that
thread’s changes (if any) to permanent state.

A gcp-thread tree may create a version stack with a depth of
greater than one if non-root threads update data. An lcp-thread
may also create a version stack with a depth of greater than one
through recursive invocations. However the semantics of local
consistency demand that the effects of an lcp invocation be permanent if the invocation completes successfully. Therefore, an
lcp-thread commits a change by copying the top version on the
stack to all other versions on that stack and then discarding the
top version. In effect, an lcp-thread commit is a write-through
commit.

Any locks held by a nested cp-thread are propagated t o its
parent if the thread successfully commits or released if the thread
aborts. (Aborts are presumed t o return only a failure code to the
caller.) Locks held by top-level cp-threads are released when the
transforming invocation commits or aborts. These are the only
situations where system-level locks are released. Aborting a cpthread in a thread-tree automatically aborts all its descendants.

124

Instantaneous V i e w ( i n memory)

I

/

\

/

I

/

I

/

I

cp-thread trees operate on the instantaneous view and alter
the stable view by committing portions of the instantaneous view
t o the stable view. This allows programmers to control consistency in the stable view and is consistent with the usual notion
of a transaction. The change-set of a transforming invocation for
a top-level cp-thread at commit time can be regarded as the set
of data items that have to be committed to bring the stable view
into agreement with the portion of the instantaneous view that
has been altered as a result of that invocation. However, while
the locking and visibility rules defined in section 6.1 regulate the
interactions between cp-threads, they do not address the s i t u a
tions that may arise when s-threads and cp-threads attempt to
operate concurrently on the same persistent data.

i

/

I /

For in our paradigm, standard threads do not acquire systemlevel locks. We feel that this would be too restrictive. Forcing sthreads t o acquire system-level locks would be treating them like
de facto transactions. This seems redundant given that a programmer can prevent s-threads from interacting with cp-threads
inside an object by using inherited entry-points or lcp/gcp operation labels. If the programmer wishes t o use s-threads in
an object and synchronize their activity, user-level locking or
some other form of explicit concurrency control may be used.
This is again consistent with the process paradigm. However,
this lack of full automatic synchronization leads to three situations that must be defined: s-threads reading writes made by
cp-thread trees, cp-threads reading writes made by s-threads,
and s-threads overwriting updates made by cp-thread trees.

Stable V i e w (on Stable Storage)

Figure 2: Instantaneous and Stable Views

All cp-threads abort a change t o an item by discarding the
top version of the version stack for that item.

6.3

Views

The locking rules in section 6.1 are designed to allow only one
cp-thread to hold a write-lock on a data item and update it
without blocking. However, these updates affects only the topmost version on the version stack. It follows that for cp-threads,
the top-most version on the stack can be thought of as the latest
version since it contains the latest update and is the only version
that can be read or written by a cp-thread.

7.1

s-threads, however, may operate directly on the permanent
version of a data item. Assuming for the moment that no cpthread is working with data items used by s-threads, the perma
nent versions being updated by s-threads are also the top-most
versions on the version stacks for those items.

7.2

This leads to two appealing global views of the system. One
is the stable view consisting of the stable versions of the persistent state. This view would be present even if the system were
t o crash that instant and then recover. Another view is the instantaneous view. This view consists of the latest version of each
persistent data item, which corresponds t o the top version on the
version stack for every persistent data item whether that version
be a shadow version or a permanent version. This instantaneous
view may not be valid in the case of failures. However, it is an
intuitively appealing view in that, at the instant the system is
examined, it is an accurate snapshot of the latest changes made
to the system (see figure 2).

7

Standard Reads of Consistent Writes

s-threads do not acquire system-level locks, hence they will never
be blocked by the system when accessing permanent state. Since
s-threads see the instantaneous view of the system, they see
the latest versions of all data including changes made, but not
committed, by cp-threads. Thus, if an s-thread attempts t o read
data touched by a cp-thread, the s-thread will not block and will
read the latest (instantaneous) version of the data.

Consistent Reads of Standard Writes

S-threads may write t o data that have been read-locked by a cpthread. The write will update the latest (instantaneous) version
of the data. A cp-thread commit or abort affects only versions
of data items created by that cp-thread and its committed descendants and placed on version stacks. Reads by a cp-thread
do not create new versions on the version stack. If a cp-thread
and/or its committed descendants read a data item updated by
an s-thread and neither the cp-thread nor any of its descendants
updates that item, the commit processingfor that cp-thread will
not include the update made by the s-thread.

7.3

Standard Writes and Consistent Writes

We allow s-threads t o overwrite uncommitted updates made by
cp-threads. If this happens, the s-thread overwrite does not invalidate the system-level locks on the overwritten data. The
overwrite appears in the instantaneous view and will be committed or aborted when that item is committed or aborted by
the concurrently executing cp-thread ezactly as if the update
had been made by the cp-thread. Thus, in this case, the permanence of the s-thread overwrite is in doubt and is governed
by the commit processing of the cp-thread holding the system-

Thread Interactions

In our paradigm, s-threads always operate on the instantaneous
view. Changes made by s-threads become immediately visible
t o all other s-threads since the change is reflected in the instantaneous view. If the changes are made to a permanent version,
barring a system failure, those changes will eventually be propagated t o the stable view. This is consistent with the usual
process semantics.

I25

rializable and the above-mentioned actions will behave as strict
atomic transactions. If the objects are programmed so that only
cp-threads may update permanent data or so that cp-threads do
not read updates made by s-threads (if any), then every “action”
(cp-thread tree) will be serializable with respect to every other
cp-thread tree.

I

A

2

B

3

C

4

-

5

G e t > (ret>

~-

6

D

Serializability breaks down only if s-threads are allowed t o
view a cp-thread’s intermediate results and make their own results visible to other cp-threads. However, each object has full
control over the thread interactions that may occur within it.
Although interactions between s-threads and cp-threads are not
controlled by the system, they are under the control of the object
programmer on a per-object basis.

7

(A>
(ret>

Thus this paradigm allows for consistent updates t o perma
nent object state in accordance with the view/failure atomicity
paradigm and nested action semantics per [Mos81]. Interesting and complex semantics may also be achieved by using welldefined system properties that in other systems would have to be
programmed in a completely custom manner. The system properties may also be supplemented by using user-level concurrency
control.

Figure 3: A Thread of Execution
level write-lock on the overwritten data item. This, however is
consistent with the “best-effort” semantics of s-threads.

7.4

Mutable Threads

Figure 3 shows an example of a gcp-thread invoking a number of object operations. Operation A (in object #1) is a gcp
entry-point. Operation E (in object #2) is an lcp entry-point.
Operations Cand D (in object #3 and #2 respectively) are standard entry-points. In this example, all the invocations except
the invocation of operation A (or invocation A) are transforming invocations. The changes made in segments #1, # 5 , and
#7 while the thread is a gcp-thread are visible in the instantaneous view as soon as they are made. However, the affected
items are system-locked and the changes will not be committed or aborted until the transforming invocation (not shown)
that made the thread a gcp-thread completes. Changes made in
thread segment #2 and #4 are simultaneously committed to the
stable view or aborted when invocation E completes. Changes
made in thread segment #3 (invocation C)are performed without system-locking. Thus, those changes are immediately visible
in the instantaneous view regardless of the state of invocation
E. They may eventually be committed to the stable view by a
cp-thread or committed by the system or aborted. However they
will not be committed or aborted as a direct result of any of the
invocations shown in figure 3. Likewise, changes made in thread
segment #6 become visible in the instantaneous view without
affecting system-level locks and the changes will eventually be
committed or aborted (but not as a direct result of any of the
invocations shown in figure 3).

9

A Robust Object File System

Robust file objects on a disk system can be defined as in figure 4. These robust file objects may be read from and written
to. The read and write operations are lcp but appear t o behave
like atomic reads and writes t o users of the object. Reads will
always read consistent data and writes will not leave the object
in an inconsistent state. The truncate operation (not shown)
truncates the file at a specified block. The segment directives
form system-locking segments (see section 10). Parameters are
assumed t o be in/out by value.
The data in a file object are contained in a number of fixed
size “virtual” blocks. Before being used to hold data, virtual
blocks must be backed by real storage blocks allocated by the
disk block allocator. If a file needs t o grow (caused by a write
at the end of the file), all file objects call the disk block alloca
tor object. The block allocator manages a disk map, allocation
statistics, and keeps track of which object has been allocated
what blocks. This information is all bundled into one systemlocking segment.

Given this set of thread creation and consistency semantics, we
can support the functionality of top-level and nested actions. If
a standard thread invokes a gcp operation, the thread is transformed into a gcp-thread for the duration of the object invocation and becomes the equivalent of a top-level action until the
operation completes or invokes an lcp or standard operation. A
top-level action can be also created by creating a top-level thread
t o invoke a gcp operation. A nested action can be created by
creating a nested thread t o invoke a gcp object operation.

The block allocator has three lcp-entry points: get-block,
release-block, and garbage-collect, and a standard entry-point,
get-status. The get-status operation is a non-blocking, readonly operation that exploits an s-thread’s ability to read systemlocked data without blocking. The operation reads the variables
containing the allocation statistics which allows users to quickly
obtain the approximate state of the allocator. The get-block
operation allocates a backing storage block for a block in the
object. The release-block operation releases all backing blocks
beyond a specified block effectively truncating the file at that
block. Both get-block and release-block update the allocation
statistics to reflect the blocks allocated/released. Since get-block
and releascblock are lcp operations, and gel-status is a read-only
operation, the block allocator’s internal data structures always
appear t o be in a consistent state.

I f a set of objects are programmed so that only cp-threads are
allowed to execute object operations, all operations will be se-

A write call may append more than one block at a time, thus
it is possible for a number of blocks to be allocated to a file ob-

8

Comments

I26

struct block {
char data[BLOCKSIZE];
int blockid;

determine which segment was being accessed
if the segment is not a data segment, jump to

1
class file {
segment file-data {
int numblocks;
block filedata[ ]

the real access violation handler
if access was a read
a) check to see if a read lock is held on that segment
b) if not, get read lock (call system-lock manager)
c) set page table entries for the segment to allow reads
d) mark read lock held on the segment
e) return from interrupt
if write lock not held, get write-lock (call
system-lock manager)
if thread is a gcpthread of depth n and the version
stack is of depth n, set the pte to allow write
and return from interrupt
mark page in segment as shadowed, if not already marked
preserve current version by copying the page
push the version-stack record of the page
(page #, thread depth, retrieval key)
onto the version stack for that page
set Drotection mask on Dte to allow write access
10) return from interrupt

//# backed by allocator
//real data;

1
segment block-data {
//list of integers
public:

1
lcpentry status read(int,
char [BLOCKSIZE]);
lcpentry status write(int,
char [BLOCKSIZE]);
lcpentry status truncate(int);
lcpentry status get-blockids(id1ist ids);

1
lcpentry status
write(int position, int nblocks, char buflnblocks*BLOCKSIZE])
for ( i d i = position; i < nblocks + position; i++) {
if (i >= numblocks)
if (int blockid = get-block(i)) {
numblocks++;
/ f then add blockid to linked
// list of blockaids
} else abort();

Figure 5: An Access Violation Handler
garbage that has t o be collected. However, since commits should
happen much more frequently than aborts, this should be a good
trade-off. This example demonstrates how programmers using
this paradigm can use a consistent set of object/thread semantics
t o make application-specific tradeoffs concerning concurrency,
resilience, efficiency, fairness, and implementation complexity.

strncpy(data[i],buf, BLOCKSIZE);

1

10

return(SUCCESS);

1

The semantics presented here, while abstract, naturally lend
themselves to being implemented in and supported by the operating system. The semantics are a super-set of the basic object/thread semantics as defined in section 2, and certain f e a
tures such as the automatic system-level locking would be difficult and/or prohibitively expensive to implement at a language
level.

lcpentry status
read(& position, int nblocks, char buflnblocks*BLOCKSIZE])

t

Implementation

if (position + nblocks > numblccks)
return(FAILURE);
for (int i = position; i < position + nblocks; i++)
strncpy(data[i], buf, BLOCKSIZE);

This paradigm is being implemented as part of the Clouds
v.2 operating system. User-levelobjects, object invocation, commit/abort processing, and system-level locking are being implemented as system-level objects in the R a kernel. User objects
are composed of R a segments that contain either code or data.
System-level locking is performed on locking segments. A locking
segment may contain one or more (possibly discontinuous) R a
segments which may contain arbitrary variables or data structures. This allows flexible grouping of variables and data structures into logical locking groups.

return(SUCCESS);

1
Figure 4: Partial Definition of Robust File Object
ject during a write operation, only to have that write operation
abort. This would result in storage blocks allocated to an object
but not used. The garbage collection entry-point can be called
periodically to reclaim these blocks. The garbage collector requests from each object the list of blocks used by that object
(using get-blockids). Blocks allocated on the disk map but not
used by any object can be reclaimed.

The algorithms for handling nested and top-level cp-thread
commits are adapted from nested and top-level action commit
algorithms. However, since the system-level locking applies to
persistent data residing in a single-level store (memory backed
by secondary storage), system-level locking and version-stack
management are being implemented using the virtual memory
system and protection mechanisms. If a cp-thread does not hold
a write (read) lock on a segment, the thread’s page table entries
(pte’s) for that segment will be set to prohibit write (read or
write) access. Attempts t o access read/write or write-protected

This implementation of file objects has atomic read/write
semantics and trades off garbage collection for increased concurrency, fairness, and faster commit processing. Commit processing is faster since all commits affect only one object. Concurrency and fairness are increased since the block allocator object
is never locked by any one thread for very long. The price we
pay is that aborts become more expensive because they result in

127

data results an access violation, whereupon control passes to the
operating system access violation handler (see figure 5 ) . The
access violation handler then determines if the thread should b e
blocked or allowed to proceed, whether a shadow version must
first be created, and which page table entries should be reset
t o allow read or write access. A simple access violation handler
is shown in figure 5 . This version shadows only the updated
pages in a system-locking segment. A more sophisticated version
would attempt to decrease the number of access violations by
shadowing other pages around a touched page in anticipation of
them being touched later.

[DLA88]

[EGLT76]

[Gra79]

[HMSC88]

11

Conclusions

This paper presents an object-based paradigm which supports a
wide range of robust programming and has but one computation
abstraction -the thread. Consistency labeling mechanisms and
thread creation semantics may be used to achieve action/nestedaction semantics. However, thread transformation can be used
to achieve threads of execution that do not behave like actions
but are globally consistent in certain segments, non-consistent
(standard) in others, and locally consistent in others without
having to supply complicated custom recovery schemes. This
paradigm enables operating system designers to support robust
and non-robust computation in a uniform manner while also
giving object programmers fine control over the degree of consistency maintained within their objects and the methods used
t o achieve that consistency.

12

Acknowledgements

The example in section 9 resulted from a discussion with Rob
McCurley. We also wish to thank Bill Appelbe, Glenn Benson,
Rich LeBlanc, and the other members of the Distributed Systems Group at Georgia Tech for their feedback and suggestions.

References
[ABLN85]

G. T. Almes, A. P. Black, E. D. Lazowska, and J. D.
Noe. The Eden System: A Technical Review. IEEE
Tmnsactions on Software Engineering, SE11(1):43-59,
January 1985.
J. E. Allchin. An Archiieciure for Reliable Deceniralized
[A11831
Systems. PhD thesis, School of Information and C o n
puter Science, Georgia Institute of Technology, 1983.
J. E. Allchin and M. S. McKendry. Synchronization
[AM831
and Recovery of Actions. In Proceedings of ihe Second
A CM SIGACT-SIGOPS Symposium on Principles of
Distributed Compuiing, ACM, Montreal, August 1983.
K. P. Birman et al. An Overview of the ISIS Project.
[B*85]
Disiributed Processing Technieal Commiiiee Newsleiier,
IEEE Computer Society, 7(2), October 1985. (Special
issue on Reliable Distributed Systems).
[BAHK*89] Jose M. Bernabhu A u b h , Phillip W. Hutto,
M. Yousef A. Khalidi, Mustaque Ahamad, William F.
Appelbe, Partha Dasgupta, Richard J. LeBlanc, and
Umakishore Ramachandran. The Architecture of Ra: A
Kernel for Clouds. In Proceedings of ihe Twenty-Second
Annual Hawaii Iniemaiional Conference on System Sciences, January 1989.
J . C. Berets, R. A. Mucci, and R. E. Schantz. Cronus:
[BMS85]
A Testbed for Developing Distributed Systems. In IEEE
Miliiay Communicaiions Conference, IEEE Communications Society, October 1985.

[HW87]

[Jon79]

[LDH*87]

[Lis821

[LS79]

[LS83]

[MosBl]

[PN85]

[SBD*84]

[WCC*74]

[Wilt371

[WPLP85]

P. Dasgupta, R. LeBlanc, and W. Appelbe. The Clouds
Distributed Operating System. In Proceedings of the
8th International Conference on Distribuied Compuiing
Systems, June 1988.
K. P. Eswaren, J. N. Gray, R. A. Lorie, and I. L.
Traiger. The Notions of Consistency and Predicate
Locks in Database Systems. Communicaiions of ihe
AGM, 19(11):624-633, November 1976.
.
James N. Gray. Notes On Database Operating Systems.
In Operating Systems: An Advanced Course, SpringerVerlag, Berlin, 1979.
Roger Haskin, Yoni Malachi, Wayne Sawdon, and Gregory Chan. Recovery Management in Quicksilver. ACM
Transactions on Computer Systems, 6(1):82-106, February 1988.
M. P. Herlihy and J. M. Wing. Avalon: Language Support for Reliable Distributed Systems. In Proceedings
of ihe Seventeenih Iniemaiional Symposium on FauliTolerant Computing, July 1987.
A. K. Jones. The Object Model: A Conceptual Tool for
Structuring Software. In Operating Systems: An Advanced Course, pages 7-16, Springer-Verlag, NY, 1979.
B. Liskov, M. Day, M. Herlihy, P. Johnson, G. Leavens,
R. Scheifler, and W. Weihl. Argus Refennce Manual.
Laboratory for Computer Science, Massachusetts Institute of Technology, Cambridge, MA, March 1987.
B. Liskov. On Linguistic Support for Distributed P r e
grams. IEEE finsaclions on Software Engineering,
SE8(3):203-210, May 1982.
B. W. Lampson and H. E. Sturgis. Crash Recovery in
a Distributed Storage System. 1979. Computer Science Laboratory, Xerox Palo Alto Research Center, Palo
Alto, CA.
B. Liskov and R. Scheiiler. Guardians and Actions:
Linguistic Support for Robust, Distributed Programs.
ACM Tmnsaciions on Programming Languages and Systems, 5(3):381-404, July 1983.
J. E. B. Moss. Nested Tmnsaciions: An Approach io
Reliable Disin'buied Computing. PhD thesis, Laboratory for Computer Science, Massachusetts Institute of
Technology, 1981.
C. Pu and J. D. Noe. Nesied rrclnsaciions for General Objects: The Eden Implemenialion. Technical Report 85-12-03, Department of Computer Science, University of Washington, Seattle, WA, December 1985.
A. Z. Spector, J. Butcher, D. S.Daniels, D. J . Duchamp,
J. L. Eppinger, C. E. Fineman, A. Heddaya, and P. M.
Schwarz. Support for Distributed Transactions in the
TABS Prototype. In Proceedings of ihe Fourth Symposium on Reliability in Distributed Software and Database
Systems, pages 184206, October 1984.
W. Wulf, E. Cohen, W. Corwin, A. Jones, R. Levin,
C. Pierson, and F. Pollack. HYDRA: The Kernel of a
Multiprocessor Operating System. Communicaiions of
the AGM, 17(6):337-345, June 1974.
C. T. Wilkes. Programming Methodologiesfor Resilience
and Availability. PhD thesis, School of Information and
Computer Science, Georgia Institute of Technology, Atlanta, GA, 1987.
Matthew J. Weinstein, Thomas W. Page, Jr., Brian K.
Livezey, and Gerald J . Popek. Transactions and Synchronization in a Distributed Operating System. In
Proceedings of the Tenth ACM Symposium on Opemiing System Principles, pages 115-126, December 1985.

Hardened Networks: Incremental Upgrading of the Internet for Attack Resilience
Partha Dasgupta
Arizona State University
Tempe, AZ 85287

Shu Zhang
Arizona State University
Tempe, AZ 85287
shu.zhang@asu.edu

partha@asu.edu
additional control points (for enhanced flow control), that is,
there is no change to the protocols and protocol stacks at the
hosts. Thus, the applications do not have to be changed. All
TCP/IP enabled devices benefit from the hardened network.
Even the network itself is better protected against the DoS
attacks due to the source authentication and flow control.
The hardening of the network infrastructure is attained by
replacing regular network routers with “hardened routers”
that can perform a few extra functions, in addition to routing.
Completely revamping the Internet network protocol is obviously infeasible; hence in our approach, the hardened routers
are completely compatible with the existing routers and can
be incrementally introduced – making deployment feasible.
We call this approach “Network Hardening.”

Abstract
Network security is conventionally implemented at the
edge of the network (such as SSL, SSh), or router-based
filtering. They require the awareness from the users and
the understanding of the complicated configuration. They
do little to provide resilience to network attacks.
In this paper, we discuss a different approach to enhance
of Network Security. We use smarter routers to build security mechanisms (source authentication, flow control,
encryption) into the fabric of the network. It allows for
incremental upgrading as well as compatibility with all
current protocols. Since the security mechanisms are at
the router level, there is no impact on the end user. We
also show some implementation and simulation results.

2.OUR APPROACH

1.INTRODUCTION

2.1. OVERVIEW OF HARDENED NETWORK

When the current de-facto standard TCP/IP protocol suite
was designed in 1979, public access to the network layer was
not part of the design. Hence, the security features were not of
major concern. As a result, the suite has plenty of vulnerabilities [1] — making the Internet a playground for hackers.
Even though TCP/IP forms the backbone of today's Internet, it lacks the most basic mechanisms for security, such as
authentication or encryption. IP blindly delivers any packet to
any specified IP. It is also easy to set fake control information
in IP headers or sniff without being detected.
The current state of the art in network security uses techniques that work on the “edge of the network.” Many security
protocols are being used to secure the Internet communications, such as SSL [6], SSh [7], and IPSec [5].
These protocols require hosts to be explicitly configured
(VPN [5] is transparent to the end applications, but the client
computer needs to be configured). In addition, these techniques do little to provide resilience to a plethora of attacks
on TCP/IP that are designed to exploit their vulnerabilities.
The Attack prevention techniques rely heavily on firewalls
[4]. The use of firewalls to prevent spoofing and scanning has
had some success, but it failed to protect against more sophisticated attacks (DDoS [10]), and also restricts the network
activities of the internal users.
The goal of our research is to make the Internet secure and
resilient to attacks. Our solution is counter to the conventional approach.. Instead of running protocols on the hosts or
filtering programs[3] on firewalls, our solution builds the
hardening at the router level using cryptographic schemes and

0-7803-7945-4/03/$17.00 (C) 2003 IEEE

As stated earlier, we harden the Internet by replacing
routers with hardened routers. For simplicity, we assume that
the Internet is a set of Autonomous Systems (ASes).
The key idea of “hardening” is the enhancement of some
functions in a router. This enhancement is cryptographybased, including both encryption and authentication. The encryption of IP payloads secures the privacy of the Internet
communications. Cryptographic techniques permit the construction of a trusted subsystem with distributed reach, which
is crucial for identifying the attackers. .
We classify the routers in an AS into three categories:
Border routers: These are on the inter-AS path, and run
Border Gateway Protocol for inter-AS routing.
Access routers: The routers are directly connected to the
end host machines inside the same AS.
Core Routers: All routers, other than Border routers and
Access routers, are Core Routers.
To create a Hardened AS, we harden the Border Routers
and Access Routers, so that these routers can do encryption
and information gathering. Core Routers are left untouched.
A hardened router is a regular router, which has the following additional abilities: sign/verify and encrypt/decrypt packets when needed; provide traffic statistics; filter/restrict packets when instructed by AS controller.
To facilitate global flow monitoring, router configuration
management and digital certificate issuing, we add a “Hardened AS Controller” to each hardened AS.

599
595

per is limited to address the solutions to these problems, but
not to defeat attacks, which was illustrated in [9].

2.2. BENEFITS
To illustrate the Hardened Network, consider the network
shown in Fig. 2.
A tta c k er 2
R3
H1
A tta c k e r 1
F
H2

R5

R4
R1

L

3.HARDENED NETWORK

V ic tim
H3

The hardened network is composed of Hardened ASes. Fig.
3 shows the components of each hardened AS.

R2

H a rd e n e d B o rd e r R o u te r

R6
C o re R o u te r

Figure. 2. Connections in Hardened Network

This is a hybrid network consisting of normal routers (R1,
R2 …) and two hardened routers F and L. H1 and H2 are hosts
involved in an attack and H3 is the victim. F is the first hardened router between H2 and H3, which means there is no
hardened router between H2 and F. L is the last Hardened
Router meaning no Hardened Router between L and H3.
When F receives an IP packet, it marks the packet with its
IP address and encrypts the payload and the first byte of the
IP address using the key shared with L before it forwards it.
In order to enhance the performance, we only require all the
first hardened routers to mark and encrypt each IP packet. All
the hardened routers between the first hardened router and the
last hardened router simply forward all the packets. So when
the marked and encrypted packet gets to L, L decrypts the
payload using the shared key and verifies the IP address. In
this way, we can guarantee that the identification in the
marker field is not repudiated by F.
Consider the effect of hardening when H1 and H2 send out
the DDoS attack traffic to H3, indicated as dotted lines. The
flow control component in L detects the congestion and
packet dropping. According to the identification information,
L immediately traces the attack traffic from H2 back to F and
from H1 back to itself. Then, it informs F about the attack.
After that, both L and F start dropping the attack traffic.
The attack traffic from H2 can be stopped closer to the
source, but not that from H1. When more routers are hardened, we can stop the attack more effectively. Even in this
network, we can still reduce the load to victim, so that it can
serve more good traffic. Hence, only the two hardened routers
present can ensure that the traffic flowing through them is
resilient to attacks. Due to the signature, the command from L
to drop the traffic cannot be forged or be denied.
Another advantage of this system is the fact that the detection and traceback are not done at the victim, but close to the
victim, from where most of the attack traffic passes. The
traceback result cannot be denial. While most the current
DDoS traceback techniques ([11][12][13]), the traceback is
done at the victim network when it is under extreme stress.
Also they need some form of authentication to verify the
traceback information.
Since the traffic flow between the first and the last hardened routers is encrypted, the packets are immune to sniffing
without installing theVPN software.
When to encrypt/decrypt and how to perform key exchange
and the secure communications among Hardened ASes are
the key issues of “Hardened Network”. The scope of this pa-

H ost
H ost
H a rd e n e d A c c e s s R o u te r

H a rd e n e d A S C o n tro lle

H a rd e n e d B o rd e r R o u te r

Figure 3. Hardened AS

3.1. THE HARDENED ROUTERS
There are two types of hardened routers: the “Hardened
Border routers” and “Hardened Access routers.”
Hardened Border Router: The hardened border router consists of a Local-Key-Base-In, a Local-Key-Base-Out, a Manager Process, a Key Exchange Agent, and a Local-HRIB,
shown in Fig 4. The Local-Key-Base-In/out stores all the decryption/encryption keys. The Manager process manages both
Key Bases. The Key Exchange Agent is responsible for exchanging the encryption key with Hardened AS Controllers.
The Local-HRIB is used to save all the routing information
and other information related to the hardened network that
have been acquired by exchanging routing information.
Hardened Access Router: The hardened access router handles traffic originating from, or destined to, the hosts directly
connected to them. The Local-Key-Base-In for the encryption
keys stores the keys shared by the two local access routers.
Apart from those components present in a hardened border
router, the hardened access router has one more database, the
Local-Info-Base, which stores all the host information of the
local hardened AS.
Sub_CA

Configuration Base

Local-Key-Base-In
Rule Base

Local-HRIB

Local-Key-Base-Out
Manager

Key
Exchange
Agent

Key Base

Monitor
Process
Recovery
Process

Manager

Key
Exchange
Agent

Figure 4. Hardened Border Router (L) Hardened AS Controller(R)

3.2. HARDENED AS CONTROLLER
The hardened AS Controller is a major component of the
hardened AS. It is responsible for the management of all the
other components within the same hardened AS, the attack
detection and traceback, and also the coordination of attack
recovery with controllers of other ASes.
The Hardened AS Controller consists of a Sub-CA, a Configuration Base, a Key Base, a Manager Process, a Key Exchange Agent, a Rule Base, a Monitor Process, and a Recovery Process, shown in Fig 4. The Sub-CA issues certificates

600
596
595

when network situation changes. If it happens before the new
route is propagated and after the encryption has been done,
the packet will be dropped due to no decryption key. In order
to avoid it, the IP packet format sent out by the first hardened
router is shown in Fig. 5.

to local routers. The Configuration Base stores the router configuration information. The Key Base saves the keys for decryption. The Manager Process manages the Key Base. The
Key Exchange Agent is responsible for exchanging keys with
hardened routers. The rule base stores rule information for
detection. The Monitor Process monitors the traffic. The Recovery Process traces the source of the attack.

In com in g IP P a cket C o n ten t
O rig in al IP h ead er (40 octs)

O p tion a l IP H ead er F ield (va riab le len gth )

IP P ayloa d

O u tg oin g IP P a cket C o n ten t
O rigin al O p tion al F ield
(v aria b le L en g th )

A ctu al IP H ea d er
(40 octs)

3.3. HARDENED NETWORK ROUTING --HBGP

H ard en ed N etw ork
O p tion al field (8 octs)

A ctu al E n cryp ted IP
P a yload

A ctu a l IP H ead er (40 oc ts)
First 32 octs of O rig in al IP H ead er

As stated earlier, the knowledge of the “last hardened
router” is actually a very critical part of this hardening concept. Since if we cannot figure out the last hardened router
correctly, the packets will end up unrecognized and dropped.
All the border routers in an AS get up-to-date routing information by running Border Gateway Protocol (BGP [2]).
We decided to compatibly extend BGP so that the hardened
border routers would not only get the routing information
from BGP messages, but also the last hardened AS information of each reachable route. So we can acquire this important
information without significantly increasing the cost. We call
this extension Hardened Border Gateway Protocol (HBGP).
In BGP, the routing information is transmitted using
UPDATE [2] messages. In every UPDATE[2] message a
variable length of sequences of Path Attributes is presented.
In HBGP, we defined a new path attribute, HASPATH. In
order to be compatible with BGP, this new attribute is also a
triple, <attribute type, attribute length, attribute value>.
For HASPATH, the attribute type is set to be optional,
transitive, and partial. Transitive and partial flags make normal border routers propagate this attribute even though they
do not understand it. The attribute value always includes the
last hardened AS information.
If a HBGP speaker advertises the route to its own autonomous system, it does not modify the HASPATH attribute. If
advertises the route to a neighboring autonomous system, it
needs to update the HASPATH as follows:
1. If no HASPATH is present, the router needs to attach the
HASPATH attribute to the message with the local hardened AS controller IP address as the attribute value.
2. If the HASPATH is already present, do nothing.
We only use BGP for the purpose of Hardened Network information, not intent to secure BGP. Also the security of
HBGP relies on that of BGP.

O rigin al S ou rce IP

T h e last H a rden ed A S C on tro ller IP

A ctu a l E n cry p ted IP P ay lo ad
E (first b y te of IP (F irst H a rden ed R o uter) , O rigin a l D estin ation IP ,O rigin al IP Pa yload )

s ha red key

Figure 5. Actual IP Packet Content

The other hardened routers the packet passes through simply forward it. Only before the hardened router in the last
Hardened AS on the route sends the packet to the neighboring
AS, the router decrypts the packet, and matches the IP addresses in the payload and the header. If match, it restores and
forwards the original packet; otherwise, it drops the packet.
Hardened access routers share keys with each other. They
encrypt/decrypt the traffic only transmitted inside the AS.
Except these operations, the first/last hardened routers have
to calculate the number of packets for each destination. They
report them to the local AS Controller for flow control.

3.5. IMPLEMENTATION & SIMULATION
We implemented HBGP by extending the BGP implementation of Gated. The hardened network information gets
propagated on the mixed hardened/normal Linux routers.
We implemented hardened router with 128-bit RC4 encryption algorithm. We set up a 100Mbps network using 3 1Ghz
PCs. One PC worked as a normal/hardened router by changing to different kernels. One PC sent out TCP blocks with
different sizes to the third machine. We compared the transmit delay and the router CPU usage, shown in Fig. 6 and 7.
In Fig. 6, there was not much difference between normal
and hardened network. Because the router can encrypt in line
speed by devoting more CPU resource, shown in Fig. 7.
Hence the encryption is not the bottleneck.
Processing time vs. Block size
0.002
0.0018

Normal

Hardened

Processing tim e (seconds)

0.0016

3.4. THE “FIRST/LAST HARDENED ROUTER”
For each new route, a hardened border router establishes a
shared secret with the last hardened AS Controller of this
route. If it does not have a valid key, its Key Exchange Agent
contacts with the last hardened AS Controller to agree on a
shared secret using SSL. Then the hardened border router
stores the last hardened AS information in Local RIB and
saves the shared key.
For every packet, the first hardened router the packet encounters encrypts the payload. But the route might change

0.0014
0.0012
0.001
0.0008
0.0006
0.0004
0.0002

597
595
601

4

89

38

96

69

48

36

Block size (byes)

Figure. 6. Processing Delay of Normal and Hardened Network

16

81

40

30

20

15

8

2

4

6

9

8

24
10

76

51

38

25

18

99

12

96

93

67

64

61

51

48

0

CPU Usage Comparison
Hardened Router

T ra ff ic A m o u n t

Normal Router

12000

T ra ffic w itho u t A t ta c k
100

Number of Packets

90
80
70

CPU Usage (%)

D ro p

10000

60
50
40

N o t if y

8000

D ro p
d e te c t e d

6000

s t a r te d

4000

N o t if y
D ro p

D ro p

N o tify
N o tify

20

0
1

10

2

3

4

5

6

7

8

9

T im e ( 1 0 0 S e c o n d s )

0
1

4

7

10

13

16

19

22

25

28

31

34

37

40

43

46

49

52

55

58

61

64

67

70

73

76

79

82

85

88

91

Seconds

Figure. 9. Traffic Amount at Server end

4.CONCLUSION

Figure. 7. CPU Usage Comparison of Normal/Hardened Network

We simulated the Hardened Network using SSFnet. We
simulated a 100Mbps network with 26 ASes on a 1Ghz PC.
Each AS had one router and one host. We measured the
packet transmission delay by hardening any pair of backbone
routers or end routers. Fig. 8 shows the comparison of these
measurements, classified by the number of hops. Hardening
the backbone routers introduced more overheads. Because
Backbone routers have to handle more traffic; therefore more
traffic will be affected.

In this paper, we discussed a new approach that can be implemented to provide infrastructure-based security to a wide
area, large-scale open networking, such as the Internet. The
methods outlined can be incrementally deployed and are effective against a wide range of attacks, including DDoS attack. The approach calls for embedding cryptographic and
control protocols into the fabric of the network, and keeping a
consistent, compatible TCP/IP interface to all un-upgraded
networking ASes. Our preliminary implementation and performance studies show the feasibility of this approach.

Comparison of Packet Transmission Delay
8

REFERENCES

7
Hardened Backbone Router

Transmission Delay per packet in %

D ro p

N o tify

2000

30

T ra ffic w ith A tta c k (1 8 h a rd e n e d
ro ute rs )
T ra ffic w ith A tta c k (1 h a rde n e d
ro ute r)

Hardened End Router

[1] Steven M. Bellovin, “Security Problems in the TCP/IP Protocol
Suite,” AT&T Bell Laboratories, Murray Hill, New Jersey,
April 1989.
[2] Y. Rekhter, “A Border Gateway Protocol 4,” Internet Draft,
RFC 1771, March 1995, Network Working Group.
[3] P. Ferguson, and D. Senie, “Network Ingress Filtering: Defeating Denial of Service Attacks which Employ IP Source Address
Spoofing,” RFC2827, May, 2000, Network Working Group.
[4] Michael Lyu, Lorrien Lau, “Firewall Security: Policies, Testing
and Performance Evaluation,” COMPSAC2000, IEEE 0-76950792-1/00.
[5] D. Maughan, M. Schertler, M. Schneider, and J. Turner. “Internet Security Association and Key Management Protocol,”
Internet Draft, draft-ietf-ipsec-isakmp-09.txt, March 1998.
[6] A. Frier, P. Karlton, and P. Kocher, “The SSL 3.0 Protocol,”
Netscape Communications Corp. November 18, 1996.
[7] Y. Ylonen, “SSH Transport Layer Protocol,” Internet Draft,
March 20,2002, Network Working Group.
[8] Intel Corporation, “IXP 1200 Network Processor Datasheet,”
Spetember 2000.
[9] Shu Zhang, Partha Dasgupta, “Denying Denial-of-Service Attacks: A Router Based Solution,” appearing in the proceeding of
2003 International Conference on Internet Computing.
[10] CERT Coordination Center, “Denial of Service Attacks,”http://www.cert.org/tech_tips/denial_of_service.html.
[11] Dawn Xiaodong Song and Adrian Perring, “Advanced and
Authenticated Marking Schemes for IP Traceback,” in Proceedings of the 2001 IEEE Infocom Conference.
[12] S.M.Bellovin, ICMP Traceback Messages. Internet Draft:
Draft-bellovin-itrace-00.txt, Mar. 2000.
[13] H.Burch, and B. Cheswick, “Traceing Anonymous Packets to
Their Approximate Source”, in proceeding of Usenix LISA’00

6

5

4

3

2

1

0
3 hops

4 hops

5 hops

6 hops

7 hops

8 hops

9 hops

Figure. 8. Trans. Delay for hardening Backbone/end Routers

These graphs described the overhead the Hardened Network will introduce. It is caused by encryption/decryption
and the traffic statistics, which are the techniques we use to
detect and trace the source of DDoS. The network will benefit
from these functionalities when more and more routers are
hardened. We simulated a Ping attack to one of the ASes, the
victim to see how the Hardened Network responded.
Since the detail algorithms of information gathering and
traffic control are illustrated in [9], here we only use Fig. 9 to
demonstrate the benefit of the Hardened Network.
Fig. 9 shows the traffic that the victim got under normal
and attack situations for hardening all the 18 end routers and
only hardening the gateway of the victim. It indicates that
with the traffic statistics, we can detect the DDoS attack at the
earlier stage, and also we can notify the first hardened router
to drop the attack traffic using the identification.
Also the performance of the Hardened Network depends
significantly on the speed of encryption. Currently, we are
working to build routers using the Intel IXP 1200 network
processor [8], but the work has not been completed.

602
595
598

Works in Progress
Editor: Anthony D. Joseph n University of California, Berkeley n adj@eecs.berkeley.edu

Location-Aware Computing,
Virtual Networks

EDITOR’S INTRO
This issue’s Works in Progress department looks at different topics and applications
in location-aware computing: letting users set and control privacy policies, coldstarting recommender systems, aggregating contextual information, and applying
location-based services to public transportation environments. The department also
includes a report on middleware to support transient virtual networks over lowpower wireless personal-area-network nodes.
—Anthony D. Joseph

n n n n n n n n n n n n n

SIMPLIFYING
USER-CONTROLLED
PRIVACY POLICIES
Mark S. Ackerman, Tao Dong,
Scott Gifford, Jungwoo Kim,
Mark W. Newman, Atul Prakash,
and Sarah Qidwai, University
of Michigan, Ann Arbor

Location-aware computing infrastructures are becoming widely available.
However, a key problem remains: letting users manage their privacy while
also giving them interesting applications that take advantage of location
information.
Several systems have attempted to
provide interfaces for expressing policies to give users substantial control
over their privacy. Examples include
restricting the times and places of
access, editing the location resolution
(for instance, room level versus building level), and excluding other people
from accessing your location.
Unfortunately, preliminary experience with such systems indicates that
users have trouble creating detailed

28	

PER VA SI V E computing 

policies and predicting the effects of
their privacy preferences in advance
(for example, see the work of Scott
Lederer and his colleagues, “Personal
Privacy through Understanding and
Action: Five Pitfalls for Designers,” in
Designing Secure Systems That People
Can Use, L. Cranor and S.L. Garfinkel, eds., O’Reilly, 2005, pp. 421–445).
To address the problem of poor prediction, Lederer suggested the notion of
privacy dials, which give users a simple
interface for controlling their location
privacy at any time from a mobile
device. A privacy dial can control the
granularity at which location information is available to others (both the
location resolution and whether the
user is identified). Interfaces such as
privacy dials can be useful, but they
push the burden completely back to the
user to maintain the settings accurately
and actively at all times. Because privacy dials must be easy to use, they are
also coarse-grained tools; for example,
it’s difficult to use different settings for
different users.
In our work, we’re exploring
whether there’s a middle ground

between these two ends of the spectrum. In particular, we’re examining
ways to greatly simplify privacy-policy
creation for users. Our work uses the
contextual information from applications that help users coordinate or
communicate with others, such as
their calendars, messaging contacts,
and address books. Our assumption
is that this contextual information,
produced through everyday applications, can help create privacy policies for location-aware systems. Users
can then more easily create high-level
policies.
An example of where this would be
valuable for users can be seen in the
“Where Is Mark?” application (see
Figure 1). In this application, users can
determine whether they’d like their
location to be shared shortly before
a meeting, which lets other meeting
participants know whether everyone
will be on time. (The application was
named for an often-tardy faculty participant.) Users need to set policies
for making their location available:
amount of time prior to the meeting,
whether to include their exact location, and so on. Asking them to set the
policies at a low level would be frustrating and lead to low compliance. On
the other hand, it’s quite easy to ask
them whether they want such a policy
set for the participants in a meeting
scheduled on Google Calendar. Customizing the policy is only a matter of
setting the amount of time prior to the

Published by the IEEE CS n 1536-1268/09/$26.00 © 2009 IEEE

meeting and whether they want their
exact location provided.
This work has resulted in a new
kind of infrastructure, one that is
privacy sensitive. The Whereabouts
system is the base infrastructure
that provides for high-speed policy
invocation and a secure publish-subscribe mechanism for data sharing
given a set of user-specified policies
(see K. Borders et al., “CPOL: HighPerformance Policy Evaluation,”
Proc. ACM Conf. Computers and
Communication Security, ACM
Press, 2005, pp. 147–157). The architecture is currently centralized but
can be distributed for more privacy
protection.
In addition, the project is creating two privacy-management utilities. The first, Policy Mirror, lets
users see the effects of any policy,
given their previous location traces
and those of other users. In other
words, Policy Mirror lets users see
what will happen on the basis of
what they’ve done in the past.
The second utility, Privacy Circles,
lets users share policies. A general rule
of thumb in HCI is that only 1 percent of users create new customizations, but a much larger number will
use other people’s customizations (see
W.E. Mackay, “Patterns of Sharing
Customizable Software,” Proc. ACM
Conf. Computer-Supported Cooperative Work,” ACM Press, 1990, pp. 209–
221). Thus, we want to make it easy for
users to use and share privacy policies.
The Privacy Circles and Policy Mirror utilities require an additional level
of system support. We’ve also constructed the Designers’ Ubiquitous
Computing Testbed (DUCT) and the
Replay utility, as part of the infrastructure. DUCT Replay lets users
replay past event streams, such as from
location-aware sensors or identification
services.
For more information, contact
Mark Ackerman, Atul Prakash, or
Mark Newman at {ackerm, aprakash,
mwnewman}@umich.edu.

OCTOBER–DECEMBER 2009	

Figure 1. The GUI for the “Where Is
Mark?” application. At 9:04 p.m., it’s
obvious that Mark (blue dot) is not
only late for the 9:00 p.m. study group,
but is at least 5 minutes away from the
University of Michigan campus.

n n n n n n n n n n n n n

WHAT DO YOU LIKE HERE?
COLD STARTING
LOCATION SERVICES
David García, Paulo Villegas,
and Alejandro Cadenas, Telefónica I+D

Recommender systems have achieved
a satisfactory level of performance in
many cases. Technologies such as collaborative filtering are currently welltested and mostly reliable, but some
rough edges remain. One active R&D
area is the augmentation of recommender systems with contextual information to better match a user’s instant
interests. Location-based services
include the time and location contexts
of a request. Recommender systems
will use this information to constrain
their suggested best matches for a user
to what’s available here and now.
However, most recommender systems suffer from the cold-start prob-

lem: when users first enter the system,
no information about them exists
to help guide the recommendation
algorithm—neither a user profile (for
content-based recommendation) nor
recorded past user activity (for collaborative filtering). The system must
somehow acquire the initial user
data. In location-based services, the
problem can be more acute because
the user data is context-dependent
and might not be directly reusable
across contexts.
We’re prototyping a mobile service for personalized, context-aware
leisure recommendations (see Figure
2). The service will suggest appropriate nearby activities (restaurants,
bars, cinemas, and so on), adapted to
the location, the time, and the user’s
tastes. When a new visitor enters a service area, we must characterize user
preferences on the basis of the available local services. Some of the information thus acquired about user tastes
might be reusable across different geographical areas, if we map profiles to
new local offerings.
To create the initial profile, we’re
implementing an automatic procedure that builds a questionnaire and
submits it to users to help define their
tastes. Given users’ natural reluctance
to answer lengthy surveys, exacerbated
by the complexities of answering them
through the limited usability a mobile
system interface, the procedure keeps
the questions to a minimum. Our prototype uses a decision tree that, at each
branching level, employs a defined utility function and the user’s previous
answers to discriminate among the
options available in the current spatialtemporal context and select the next

PER VA SI V E computing

29

WORKS IN PROGRESS

WORKS IN PROGRESS

Location
provider
User
location
Other
context
sources

Social
services

A decision tree is
built based on user
context to
discriminate
relevant
database
items
Context
information
User’s
tags and
reviews

Personalized questionnaire

Decision tree
buildup
User
profile

User profile is updated
with questionnaire
results

Recommendation
Item
engine
database
Geocoded leisure
services information

User

User receives
recommended
leisures services,
based on personal
profile and
current context

Leisure
services
provider

Figure 2. Mobile service for contextaware recommender system. A
questionnaire automatically generated
from a decision tree helps the system
overcome the recommender cold-start
problem.

question. We designed the process to
be incremental, with each step adding
more discriminative capacity to the
profile. The user can stop answering
at any moment, and the answers collected to that point will still provide a
profile that the recommendation algorithm can use.
To build the decision tree, our
database needs fine-grained characterizations of spatially tagged items.
Accordingly, the system fetches social
data (folksonomy-based user tags,
reviews, and categories gathered from
online social services) for each local
feature and uses statistical processing
to structure that information into data
that our defined utility functions can
use. A pending design issue is how to
evolve the acquired user tastes over
time, so we can differentiate between
short-term desires and longer-term
stable preferences.
For more information, contact
Paulo Villegas at paulo@tid.es.

30	

PER VA SI V E computing 

n n n n n n n n n n n n n

LOCATION-BASED CONTEXTMANAGEMENT PLATFORM
Alejandro Cadenas and
Antonio Sánchez-Esguevillas,
Telefónica I+D
Javier Aguiar and Belén Carro,
University of Valladolid

Over the past few months, a research
group composed of University of Valladolid professors and Telefónica I+D
engineers has been developing a global
convergent architecture for managing
user contexts. The architecture bases
a convergent-control layer on the IP
Multimedia Subsystem (IMS) framework (see Figure 3). The control layer
captures a user’s context from different context providers, such as sensors
or applications, over different access
networks. Any service or application
can subscribe to the centralized context management element—namely,
the Context Management Enabler in
Figure 3—to receive context notifications for specific subscribers via a contextual protocol we defined based on
the Session Initiation Protocol (SIP) for
transport.
The design and simulation phase is
nearly finished, and we’re preparing for

a field deployment. The Context Management Enabler platform is based on
the SIP registrar and proxy services of
Mobicents, a Java open source SIP application server. The platform deploys the
Fraunhofer FOKUS IMS control layer.
The access network is the data network
of the University of Valladolid’s Higher
Technical School of Telecommunications Engineering. We’ve designed the
context providers to be the Bluetooth
modules of the professors’ mobile
phones, detected at a Bluetooth dongle
installed in each room, office, and lab of
the school’s building. Through an intelligent aggregation of location information and class timetables for each professor, the Context Management Enabler
composes a contextual status that students can check to verify the professors’
availability for tutoring.
This ongoing work will identify
implementation-specific issues of the
proposed reference architecture. It will
also provide valuable performance
benchmark data for system and network
modeling in large-scale deployments.
For more information, contact Alejandro Cadenas at cadenas@tid.es.

n n n n n n n n n n n n n

BUSTRACKER: DIGITALLY
AUGMENTED PUBLIC
TRANSPORTATION
Sean Mailander, Ronald Schroeter,
and Marcus Foth, Queensland University
of Technology

Public transportation is an environment with great potential for applying location-based services through
mobile devices. The BusTracker study
is looking at how real-time passenger
information systems can provide a core
platform to improve commuters’ experiences. These systems rely on mobile
computing and GPS technology to provide accurate information on transport
vehicle locations. BusTracker builds
on this mobile computing platform
and geospatial information. The pilot

www.computer.org/pervasive

WORKS IN PROGRESS

WWW

University campus
Context Management
Enabler

HTTP

Session Initiation Protocol (SIP)
IP Multimedia
System (IMS)
Ethernet

Room 2L023
Professor
Bluetooth
Student

Figure 3. Global architecture for user’s context management. The deployed infrastructure captures the professor’s context,
which the Context Management Enabler processes. Any application can request the context information by implementing the
appropriate contextual protocol, such as the Web-based application shown on the right.

study is running on the open source
BugLabs computing platform, using
a GPS module for accurate location
information.
Previous research to enhance the
user experience in urban environments has led to applications such as
CityWare (www.cityware.org.uk),
which uses Bluetooth nodes at public locations and a link from a user’s
Bluetooth device to his or her Facebook profile. CityWare presents information about the people an individual
encounters most frequently. However,
this system doesn’t fully exploit the
public transportation environment
where familiar strangers, as Stanley
Milgram described them (The Individual in a Social World: Essays and
Experiments, McGraw-Hill, 1977),
are together for extended periods at
regular frequencies with little environmental stimulation. The characteristics of such spaces offer opportunities

OCTOBER–DECEMBER 2009	

to test digital-augmentation scenarios
that foster social connections between
individuals or use ambient visualizations of historic presence data that
don’t require commuters to directly
interact.
The BusTracker study is initially
investigating the provisioning of realtime scheduling information to users
through innovative design solutions
on Web systems, mobile applications,
and urban information displays. Once
these interfaces are in place, the study
will look at how to use the interfaces to
engage commuters—either by embedding portals to social networking sites
or by creating novel social networking experiences. Both approaches will
exploit real-time location information
to add new value to existing social
networking.
In the first case, adding real-time
location information can enhance
existing social networking sites by

supporting a collective presence
online. For example, all passengers
on a particular bus can join a collaborative group to chat, share podcasts, signal intended destinations, or
ask for advice on tourist attractions.
In the case of new applications, realtime location information can display
accurate scheduling information. It
can also assist in capacity management and on-demand public transport
by letting people signal their intended
trips in advance. Other applications
of this kind might inform individuals
of friends who are on closely aligned
trips and suggest impromptu rendezvous through minor trip modifications, such as catching an earlier train.
Or an application might suggest waiting an extra half hour at work to miss
peak-hour crowds.
For more information, contact Sean
Mailander at s.mailander@qut.edu.au
or see www.urbaninformatics.net.

PER VA SI V E computing

31

WORKS IN PROGRESS

WORKS IN PROGRESS

Virtualized plane

V
i
r
t
u
a
l
i
z
a
t
i
o
n

Physical plane

Internet

Figure 4. A virtual WPAN covering multiple physical WPANs. The shaded area shows a
contour running through three physical sensor WPANs.
n n n n n n n n n n n n n

LIGHTWEIGHT
VIRTUALIZATION
OF LOW-POWER WPAN
SENSOR NODES
Amiya Bhattacharya and Partha
Dasgupta, Arizona State University

Wireless personal area networks
(WPANs) of embedded sensors are
traditionally conceived to be privately
owned and deployed with a single spe-

cific application in mind. An interesting possibility of participatory sensing
emerges if we consider forming a transient, virtual, and fully programmable
sensor network by stitching together a
closely spaced cluster of real WPANs,
especially if minimal disturbance to the
native applications can be ensured on the
constituent sensor networks.
Work is underway at Arizona State
University and New Mexico State University to develop middleware for sup-

porting lightweight virtualization on
resource-constrained WPAN nodes
(popularly known as motes) along with
MAC-layer bridging on their wireless interfaces. Power-efficient virtual
WPANs require both technologies.
Lightweight virtualization of WPAN
nodes turns out to be quite useful for
incremental deployment of a wireless
sensor-network infrastructure that
accommodates heterogeneous mote
hardware and operating system platforms, provided all motes support the
identical MAC standard. Users can
deploy each batch of identical motes
to build the host WPANs, all of which
can then support a virtual WPAN to
perform untethered networked sensing applications. Spanning multiple
domains is the most noteworthy feature
that users can leverage to operate a virtual sensor network over host WPANs,
even if they’re across multiple ownership
domains. Figure 4 shows a shaded area
with a contour running through three
physical sensor WPANs. A simple,
power-efficient contour-detection algorithm can be used if the zone is covered
wholly within a single virtual WPAN.
This new kind of participatory sensing
and sensor data processing infrastructure is termed a community sensor grid,
based on its similarity with the participation model used in computational grids.
For more information, contact Amiya
Bhattacharya or Partha Dasgupta at
{amiya, partha}@asu.edu.

IN THE MIDDLE OF A PERVASIVE COMPUTING PROJECT?
In addition to feature-length articles, IEEE Pervasive Computing invites
work-in-progress submissions of 250 words or less on topics ranging from
hardware technology and software infrastructure to environmental sensing
and human-computer interaction.
Works in progress are not formally peer-reviewed, but submissions must be
approved by the WiPs department editor, Anthony D. Joseph. If accepted,
they are edited by the magazine’s staff for grammar and style conventions.
Submit a WiPs report on your project to pvcwips@computer.org.

32

PER VA SI V E computing

www.computer.org/pervasive

