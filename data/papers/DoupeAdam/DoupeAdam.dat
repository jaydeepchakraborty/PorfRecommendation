Ten Years of iCTF: The Good, The Bad, and The Ugly
Giovanni Vigna, Kevin Borgolte, Jacopo Corbetta, Adam Doupe,
Yanick Fratantonio, Luca Invernizzi, Dhilung Kirat, and Yan Shoshitaishvili
(vigna,kevinbo,jacopo,adoupe,yanick,invernizzi,dhilung,yans)@cs.ucsb.edu
The SecLab Group
University of California in Santa Barbara
Abstract

of courses focused on security, at both the undergraduate
and graduate levels. In addition, most courses on topics
such as web development, operating systems, networkbased applications, etc., now include a section on security
issues.
Teaching computer security is hard for a number of reasons. First of all, security is a fast-moving target. New
classes of vulnerabilities are discovered every year, and
new protection mechanisms are introduced. For example,
it would be infeasible to teach how buffer overflow exploits work today, without mentioning the fact that Address Space Layout Randomization, stack canaries, or
No-Execute bits are ubiquitous protection mechanisms,
which make this class of attacks more difficult to carry
out.
In addition, it is not easy to create environments in
which vulnerabilities and protection mechanisms can be
reliably tested: they are difficult to set up, maintain,
and evaluate, and, therefore, some of the system security
classes offered today lack a valid “hands-on” component.
Security competitions provide a way in which students
and security practitioners in general can test their skills
in a competitive, hands-on environment. Given that security competitions usually span a somewhat limited time
frame (from a few hours to a few days), these events are
not the framework in which most security skills are acquired. Instead, it is the preparation period that precedes
the competition itself that has proven to be extremely valuable in promoting security learning. In a way similar to
the training of athletes who prepare for a running sprint,
the skills are learned in the months spent training on the
track, waiting for that brief moment at the actual competition. Participants in security competitions spend the
months preceding the event strategizing about defenses,
analyzing previous competitions to understand what is to
be expected, preparing and developing tools, testing new
approaches, and so on (see for example the results of the
questionnaire described in [2], which show that half of
the respondents developed ad hoc tools in preparation for

Security competitions have become a popular way to foster security education by creating a competitive environment in which participants go beyond the effort usually required in traditional security courses. Live security competitions (also called “Capture The Flag,” or CTF competitions) are particularly well-suited to support handson experience, as they usually have both an attack and a
defense component. Unfortunately, because these competitions put several (possibly many) teams against one
another, they are difficult to design, implement, and run.
This paper presents a framework that is based on the
lessons learned in running, for more than 10 years, the
largest educational CTF in the world, called iCTF. The
framework’s goal is to provide educational institutions
and other organizations with the ability to run customizable CTF competitions. The framework is open and leverages the security community for the creation of a corpus
of educational security challenges.

1

Introduction

Computer security education has become one of the top
priorities within governments and organizations of all
kinds. While the most basic requirement is that computer
users understand enough about security not to do something that will harm them or their environment (e.g., installing a “codec” that promises to display jumping kittens
videos or more questionable material, but instead creates
a backdoor in a system that handles sensitive data), there
is an increasing demand for “security experts.”
Even though security expertise is difficult to quantify
and test [4, 6], a basic set of skills that go beyond basic security education is very valuable in the current job
market (a simple query on LinkedIn for “Security Expert”
returns thousands of positions available, many in Fortune
500 companies). Therefore, it is not surprising that educational institutions are increasing steadily the number
1

Year
2003
2004
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013

the competition). This is the period in which most of the
learning is performed. The actual execution of the competition adds the pressure to perform, the experience of
being under active attack by a capable opponent, and the
test of teamwork.
Because of the extra motivation provided by a competitive environment, security competitions have become increasingly popular and bigger in size [1]. The competitions take two main forms: they can be challenge-based or
interactive. Even though both forms are often referred to
as “Capture The Flag” (CTF) competitions, they are very
different, and only an interactive competition can properly
be called a CTF competition.
More precisely, challenge-based competition are structured in a way that presents to the participants a number of
challenges that address different skills (e.g., reversing binaries, performing forensic analysis on file systems, manipulating network traffic) at different levels of complexity (which are usually associated with different amounts
of points when challenges are solved). These challenges
are a form of take-home test, and do not include any interactions with other teams. Instead, interactive (or “live”)
competitions focus precisely on the interaction between
teams. Every participant is given the same system (usually a server with a number of network-accessible services) and their task is to identify flaws in their own copy
of the server, patch (if possible) their own services without breaking the service’s functionality, and use the same
knowledge to attack the other participants. As proof of
having been able to exploit an opponent’s service, an attack involves grabbing a file or other data on the opponent’s machine; this piece of data is referred to as a “flag”
(and this is where the “Capture The Flag” label comes
from). This type of exercise provide opportunities for exercising both attack and defense skills, and live exercises
are therefore very different from challenge-based competition.
Unfortunately, designing and running interactive competitions is much harder than running challenge-based
competition, and it is not surprising to see that, of the top
50 competitions listed on the CTFTime.org web site [1],
only 8 are interactive. The challenges in designing, implementing, and running a live security competition (or
“proper” CTF) are many. First of all, it is necessary to
have the resources to support the creation of the game network (hardware, software, and human resources) and the
skills necessary to monitor its usage. Then, one has to
make sure that the game does not get “out of hand” in a
number of ways: a team might perform a denial-of-service
(voluntarily or inadvertently), a flaw in the game administration infrastructure might bring the whole competition
down, and, if the competition is short-lived, it might make
months of work completely useless (as the vulnerable services have been disclosed and a new competition will re-

Theme
Open Source Windows
UN Voting System
Bass Tard Corporation
Spam Museum
Hillbilly Bank
Copyright Mafia
Softerror.com Terrorist Network
Rise of the Botnet
Mission Awareness
Money Laundering
SCADA Defense
Nuclear Cyberwar

Teams
7
15
9
22
25
36
39
57
73
89
92
123

Table 1: The UCSB iCTF throughout the years.

quire a completely new setup to be fair to all participants).
Finally, scaling these competitions can be a daunting task.
Small competitions with a dozen of teams might be relatively easy to manage, but if a competition has more than a
hundred teams and thousands of participants, it becomes
very difficult to provide a seamless experience.
At UCSB, we have been the first institution to run distributed educational CTF competitions, which we called
iCTF (from “International CTF”) [10]. We started in 2003
and have since designed, implemented, and run 11 security competitions (see Table 1), and, to this day, the iCTF
has been consistently the world’s largest interactive CTF
focused on computer security education. In more than ten
years of competitions, we have experimented with different designs, scoring systems, combinations of challengebased and interactive competitions, and ways to collect
interesting datasets that might support research into security education in particular, and system security in general [5, 7, 3, 2, 9, 8].
The success and the overwhelming enthusiasm with
which iCTF participants responded to the event prompted
many requests from education institutions and other organizations for a way to reproduce the iCTF concept on a
smaller scale, maybe within a class or at a company training. We released a first version of a simplified form of
iCTF software in 2010, but the capability of the software
were very limited and required substantial skills to be configured and run.
However, our latest design for the iCTF, made us realize that there was an opportunity to create a framework
(described in detail in the rest of this paper) that could be
leveraged to easily create custom live security competitions, solving some of the challenges associated with the
creation and execution of this kind of exercises automatically.
One key aspect of the framework is to allow thirdparties to provide services and scoring components, so
2

this information to fingerprint the scorebot and make their
services appear to be online just to the scorebot. This is
achieved by masking the scorebot’s network connections
so that they appear to be coming from random, constantlychanging teams.
Upon successful exploitation, a team must submit the
stolen flag to our website within a given period of time to
receive points for it. Note that this website acts as a central
point of information: it shows the current standings on
the scoreboard, lets players chat among one another and
with the organizers, and it is used to disclose hints and
additional challenges.
The actual state of the game is kept secure in a central
database (Section 2.2), that is protected from all direct access by the players.
Figure 1: System architecture overview.

2.1
that the community can contribute to a repository of instances of vulnerable software. As a result, competitions
can be tailored to specific skill-set levels or types of vulnerabilities.
In the rest of this paper, we describe our iCTF framework and how it supports the creation of customizable live
security competitions, hoping that, with the help of the security community, it will be possible to make this type of
exercise available in a large set of educational settings.

2

Services Design

A service is a program that, from a high-level point of
view, has a stated benign purpose (e.g., it implements a
web forum) and contains some flags: secrets that are not
supposed to be revealed to unauthorized users during normal operation (e.g., private messages in the web forum).
The goal of the opponent teams is to steal a flag and submit it to the organizers. Services are written with a variety
of deliberately incorporated vulnerabilities, so that stealing flags is indeed possible, unless the defending team is
able to patch all vulnerabilities successfully. Each vulnerability should be verified by the organizers to be actually
exploitable in practice, to prevent unnecessary frustration
for the players.
Generally speaking, a service listens on a given port,
so that external programs can interact with it and invoke
certain functions. This functionality should cover at least
the following high-level categories: a setflag functionality that allows one to set the currently active flag; a getflag functionality that allows the benign and authorized
retrieval of a flag; some additional benign functionality
that represents the normal behavior of the service.
For instance, a service might implement a simple social
network, where it is possible to create a user (with a given
username and password) and to set a status. In this example, the user’s status is the secret flag, and the status can
only be read if the user is logged in by providing his username and password. Here, the setflag functionality would
take as input the username, the password, and a status to
be set, and it would then login and update the status of
the user; the getflag functionality would take as input the
same username and password, and retrieve the user’s status after logging in. Note that the password is only known
to the organizer and acts as an authentication token. On
the other hand, to steal a flag, the attacker needs to discover and exploit a vulnerability in the service in order to
read the status of a user whose password is not known.

The Platform

Although every year we introduce a novel theme and new
game mechanics for the iCTF, the underlying platform
that lets us run a CTF game remains the same. In our
game, every team is assigned a virtual machine (VM),
which is usually hosted by the teams themselves1 . These
VMs run vulnerable programs that are accessible over the
network. The players’ task is to keep these programs online and functional at all times and, if possible, patch them
so that other teams cannot take advantage of the incorporated vulnerabilities. Shutting these services down is not
an option, as their activity is vital for the players to participate in the game. The status of these services is constantly tested by a scorebot, which exercises the programs
on a regular basis to ensure that the players are keeping the
services up, and that their core functionality has not been
crippled by an incautious patch.
Each service contains a “flag,” a unique string that the
competing teams have to steal so that they can demonstrate the successful exploitation of a service. This flag is
also updated from time to time by the scorebot. We try
to ensure that the players cannot tell when the scorebot
is interacting with their services, because they could use
1 In the past two years, we decided to host the VMs at UCSB instead,

to lower the setup burden for the teams.

3

2.1.1

One key point that requires attention when implementing an interactive security exercise is how such “secrets”
are being stored within the service. In our implementation, we take into account the following considerations:

Implementation Details

Some components must be developed to integrate a service into our infrastructure.
First, a set of scripts must be developed to exercise the
various functionalities of the service. In particular, the
setflag script takes as input the flag id, a token, and a
flag, and is in charge of installing a new flag into the service. The getflag script takes as input the flag id and its
token and should retrieve the corresponding flag, so that
the scoring infrastructure can determine whether the service is properly functioning or not. Note that the getflag
script can retrieve the flag associated to a given flag id
because it has access to its associated token, not by exploiting a vulnerability.
Other scripts exercise other service-specific benign
functionality, so that the setflag and getflag scripts do not
stick out in any obvious way, and to ensure that the service
is fully operational and has not been tampered with (e.g.,
because of a careless patching process).
Finally, exploit scripts play an important role in the
game. These scripts should be developed by the attacker
teams (and, following best practice, by the organizers,
to test the service’s exploitability). Conceptually, an exploit script needs to steal the flag associated with a given
flag id, without knowing the associated token. Note
that the flag id specifies which of the many flags of the
service the script must retrieve. For this reason, the only
input the exploit script takes is the flag id, and it must
return the flag.

• The flag should change over time, so that the scoring
system can distinguish between distinct attacks depending on various circumstances (as they will submit different secrets).
• The defending team should not be able to make a service unexploitable (in the sense that proper exploitation is not observable by the organizers) by simply
changing the value of the flag. In other words, the defending team should be forced to protect a service by
patching or neutralizing vulnerabilities, not by tampering with the flag or scorebot.
• The defending team should not know which is the
currently active flag, so that naı̈ve detection techniques, like searching for the flag value in network
traces, do not work or would lead to false positives,
and, in turn, decrease their score.
To this end, we designed a novel mechanism to store
and retrieve flags. First, a new flag is periodically set and
retrieved by our scoring infrastructure so that, at any point
in time, each service has one and only one active flag. The
scorebot does so by using the setflag and getflag functionality.
Second, the setflag and getflag functionality should
blend with benign service interactions and not stick out
in an obvious way. Not only the setflag and getflag
should resemble benign interactions, but benign interactions should resemble setflag and getflag functionalities as
well. In fact, the setflag and getflag functionalities can be
used for benign interactions by simply setting or retrieving inactive or incorrect flags.
Third, from a conceptual point of view, the service
stores not only one flag, but multiple flags. We stress that,
at any point in time, only one of these flags is actually considered active by the scorebot. The defending team does
not know which flag is active at the moment and it must
protect all of them. To achieve this goal, each service conceptually stores a list of (flag id, token, flag) tuples.
In our previous social network example, flag id corresponds to the username, token to his/her password, and
flag to the status. Clearly, the implementation and storage details can vary depending on the service: a service
might use an in-memory data structure, another might
rely on the filesystem, and a third might use an external
database.

2.1.2

Integration with the Infrastructure

To ease the integration and testing of a given service, a set
of information must be provided through a JSON file. In
our current setup, this file is used to provide the following
information to the infrastructure:
• Information about the author;
• File paths of the scripts (setflag, getflag, . . . );
• TCP or UDP port used;
• Service ID;
• Service name;
• Service description;
• flag id description (i.e., what component of the
service represents the flag id, and, to some degree,
how it is being used by the service; e.g., the username, in our social network example).
The last four items are also communicated to the participating teams, so that they know how an exploit script
for a service should utilize the flag id, and what should
be returned as the flag.
4

2.1.3

Deployment of Services

The final step in designing iCTF services is deciding how
they are deployed on the teams’ VMs, both during the
game and for testing.
Service-specific installation scripts can prove problematic, especially when services require certain non-default
utilities or services to be present on the VM. Therefore,
we decided to use the Debian package format, which is
well-tested, has native dependency-handling, and can automatically start the service. In this way, once the package
is successfully installed, the service can be trusted to be
running and properly configured.
As a side effect, building Debian package encourages
standardizing the compilation and deployment procedure:
this is a useful feature in case organizers need to urgently
modify and re-deploy a service due to issues discovered
while the game is in progress (like an unintended denialof-service vulnerability that was only discovered during
the game).
As much as possible, the packages also standardize
each service’s permissions within the VM. It is typically
undesirable to run all services with the same user and with
files accessible across services: with such a setup, a single service would potentially allow obtaining flags for all
other services. In educational settings, it is often preferred
for each service to be independent, thus rewarding the
ability to attack a range of different services (as opposed to
leveraging a single type of exploit). Similarly, one might
prefer limiting the amount of damage a single exploit can
cause, for instance by prohibiting services from modifying themselves: this makes it considerably harder (or even
impossible) for attackers to install backdoored versions.
In our default setup, each service runs as a dedicated user
and has access only to the files on which it needs to operate.

underlying database directly, however this approach had
many drawbacks: changes to the database schema broke
other people’s code, the database schema was designed in
an ad hoc, as-needed basis, and it was difficult to cache
or otherwise limit the number of queries each part of the
system performed. Thus, we believe that a better design
decision in this case is to decouple the underlying storage
engine and database schema from the way the other components of the system interact with the central database.
After a number of years of experimentation with different design approaches, we settled on the components of
the system pulling information from the central database.
We experimented with designing an iCTF where the central database would push important game state-changes
to other components of the system, however, when there
were problems in this pushing aspect, the central database
system had to be restarted. Whereas, in a pull-oriented
architecture, each component is responsible for deciding
how often to pull from the central database and for taking
action based on changes in the state of the game.
After running the iCTF competition for more than a
decade, we have converged on (a few) best practices. One
is that, for keeping track of any player point or score totals
(or any changing value over time), it is much better to have
an append-only table, where each separate action and its
effects on the score are recorded separately. It also aids in
debugging (and, possibly, rescoring) if each entry in this
table has a description of why the score was changed and
which component changed the score.
Moreover, it is critically important for every game action to be recorded in the central database. This not
only empowers debugging, which often happens during the competition, but also eases the burden of postcompetition analysis. In order to accomplish this properly, each component of the system must log all events
within the central database.

2.2

2.3

The Central Database

The central database is responsible for enforcing the rules
of the game and for keeping track of its state. Essentially,
the central database is, as the name suggests, the central
component of the entire system. Every other component
will pull information about the state of the game from the
central database, and will notify the central database as to
the changes of the state.
The central database is technically composed of two
pieces: an HTTP-based RESTful API, with which the
other components of the system interact, and the actual
database, which stores all the data associated with the
competition. We settled on this design because it allows
for the decoupling of the underling database storage engine from the other components in the system. One year
we experimented with having every component access the

The Scorebot

The scorebot infrastructure is responsible for monitoring
the status of the services. It achieves this by interacting
with the services using their setflag and getflag functionality. The scorebot periodically (the period or the tick duration is selected by the organizer) generates a new active
flag tuple (flag id, token, flag). These values are used
to invoke the setflag and getflag scripts associated with
each service, so that the respective service’s functionalities are exercised and the active flag is set and retrieved.
In addition to setting up the active flags, the scorebot
is also responsible for generating benign traffic to the services. The scorebot does this in two ways: first, it executes setflag and getflag scripts randomly with dummy
flag id, token, and flag values; second, it executes a
set of benign scripts that are (optionally) provided by the
5

service writer, so that all the service’s benign functionality is properly exercised, and that one can be assured that
patching the vulnerabilities in a service did not affect its
intended behavior.
The values returned by these scripts (including the benign scripts) are used to determine the status of the service. A service is considered “up” if all executed scripts
return success. It is considered “down” if one (or more)
script fails to connect to the service. If a script fails because a service does not communicate properly, e.g., by
sending improper data such as an invalid flag, the service
is considered “non-functional”.
Benign scripts should be independent from each other
and should be executed in any order without affecting
the state of the game. However, the execution order of
the setflag and getflag scripts, which set and retrieve the
active flag, must be preserved. In fact, if the getflag
script would be executed before the corresponding setflag
script completes its operation, it would fail, and the service would be incorrectly considered as “non-functional”.
While the scorebot independently executes these scripts
with random delays, it maintains the order of execution
when needed by utilizing locks. While the introduction
of these random delays make it possible to exercise the
service in benign ways between the flag updates and retrievals, it also prevents the scorebot from being fingerprinted through simple traffic analysis.

2.4

and uploaded to our system. In cases where absolute fairness is not of great concern (e.g., a company educational
exercise where the instructors can rely on a “do not peek”
policy), the organizers can save a great deal of time by
pre-selecting some of the available services.
2.4.2

After services are created or chosen, our system proceeds to generate the virtual machines required to host the
game. These virtual machines are distributed as VirtualBox Appliances, allowing us to leverage VirtualBox features such as internal networking, which is VirtualBox’s
simple Software-Defined Networking implementation.
The organizer must decide on several options:
VM Hosting. For scalability reasons, the organizer may
leave the hosting of the teams’ VMs to the teams
themselves. When this option is chosen, our system
generates a VPN concentrator VM for the organizer
and VPN client VMs for the teams, which also act as
routers for them.
Infrastructure Distribution. Again, for scalability reasons, our system can generate either a single VM
containing all the infrastructure components, or single VMs for each piece.
Network Topology. By default, our system uses VirtualBox’s internal networking feature, which automatically creates several LANs, depending on the first
two chosen settings. However, VirtualBox’s internal
network requires all VMs to be on the same physical host, which once again brings up scalability concerns. To address this issue, our system can be configured to generate VMs with bridged networks, so
that they can be hosted on different physical hosts.

Putting it Together

One aspect that has become quite clear to us over the many
iCTFs that we have organized is that, in addition to having quality components to a CTF design, these components need to function well together. In this section, we
will describe how the different parts of the system fit together into an end-to-end CTF experience, both from the
organizers’ and the players’ perspectives.
In this context, a CTF organizer is an entity that wishes
to host a CTF competition for a certain amount of players.
Our design for the system allows organizers to host CTFs
without our involvement, going as far as drawing from our
repository of services if they do not want to develop their
own.
2.4.1

Generating the Infrastructure

The easiest way to host a CTF using our system is for
an organizer to use internal networking, run a single infrastructure VM, and host all of the teams’ VMs. This
setup, spun up on a single physical host, is the most turnkey solution: once booted, no configuration is necessary,
and the game is ready to be started from the infrastructure’s administrative interface. The obvious downside to
this setup is scalability. Once a game grows to include
more than a dozen teams, hosting all of the virtual machines on one physical host becomes impractical and can
be a source of rather unpleasant problems for organizers
and players.

Choosing the Services

An important feature of our CTF framework is the presence of a repository of ready-to-use services. However,
using these services comes with a serious drawback: often, solutions to previously-seen services (“write-ups”)
become available on the Internet once a service is used in a
CTF. In most cases, to ensure a fair game, original services
should be developed for each CTF. These services would
have to be created to conform to our service specification

2.4.3

Game On!

If the teams must host their own VMs, the organizer must
distribute them before the game. In most cases, the organizer will want to encrypt these VM, and release decryption keys at the start of the CTF, so that it is not possible
6

References

to extract and analyze the services before the actual competition has started. The CTF can be started (and, later,
stopped) using the administrative web interface.

3

[1] CTF Time. https://ctftime.org, 2014.
[2] C HILDERS , N., B OE , B., C AVALLARO , L., C AVEDON , L.,
C OVA , M., E GELE , M., AND V IGNA , G. Organizing Large Scale
Hacking Competitions. In Proceedings of the Conference on Detection of Intrusions and Malware and Vulnerability Assessment
(DIMVA) (Bonn, Germany, July 2010).

Conclusions

The iCTF competition has demonstrated that it is possible
to create interesting security exercises that involve more
than a hundred teams and thousands of students. This
paper presents a framework whose goal is to make the
basic concepts behind the iCTF available, on a smaller
scale, to educators across the world. The framework allows for the creation of custom competitions that are tailored to the skill-set of the participants, and, in addition,
supports contributions from the security community, and,
in the future, can be leveraged to create datasets to support security research. Parts of the framework have been
used as the basis for running various editions of the iCTF.
The iCTF framework will be made available for download
at http://ictf.cs.ucsb.edu. This release includes a
number of vulnerable services that can be immediately
used to create live security exercises.

[3] D OUPE , A., E GELE , M., C AILLAT, B., S TRINGHINI , G.,
YAKIN , G., Z AND , A., C AVEDON , L., AND V IGNA , G. Hit ’em
Where it Hurts: A Live Security Exercise on Cyber Situational
Awareness. In Proceedings of the Annual Computer Security Applications Conference (ACSAC) (Orlando, FL, December 2011).
[4] (ISC)2 . Cissp. https://www.isc2.org/CISSP, 2014.
[5] S HOSHITAISHVILI , Y., I NVERNIZZI , L., D OUPE , A., AND V I GNA , G. Do You Feel Lucky? A Large-Scale Analysis of RiskRewards Trade-Offs in Cyber Security. ACM Symposium on Applied Computing (March 2014).
[6] T IMMAY. Why You Should Not Get a CISSP. DEFCON 20, July
2012.
[7] VAMVOUDAKIS , K., H ESPANHA , J., K EMMERER , R., AND V I GNA , G. Formulating Cyber-Security as Convex Optimization
Problems. In Control of Cyber-Physical Systems, vol. 449 of Lecture Notes in Control and Information Sciences. Springer, July
2013, pp. 85–100.
[8] V IGNA , G. Teaching Hands-On Network Security: Testbeds and
Live Exercises. Journal of Information Warfare 3, 2 (February
2003), 8–25.

Acknowledgments
This work was supported by the National Science Foundation, through grants CNS-0820907, CNS-0716753, and
CNS-0939188, and by the ARO through MURI grant
W911NF-09-1-0553. We want to especially thank Carl
Landwehr, Jeremy Epstein, and Karl Levitt at the National Science Foundation for their support to cybercompetitions.

[9] V IGNA , G. Teaching Network Security Through Live Exercises.
In Proceedings of the Third Annual World Conference on Information Security Education (WISE) (Monterey, CA, June 2003),
C. Irvine and H. Armstrong, Eds., Kluwer Academic Publishers,
pp. 3–18.
[10] V IGNA , G. The UCSB iCTF. http://ictf.cs.ucsb.edu, 2014.

7

Federated Access Management for Collaborative
Network Environments: Framework and Case Study
Carlos E. Rubio-Medrano, Ziming Zhao, Adam Doupé and Gail-Joon Ahn
The Laboratory of Secure Engineering for Future Computing (SEFCOM)
Arizona State University
Tempe, AZ, USA

{crubiome, zmzhao, doupe, gahn}@asu.edu

ABSTRACT
With the advent of various collaborative sharing mechanisms
such as Grids, P2P and Clouds, organizations including private and public sectors have recognized the benefits of being
involved in inter-organizational, multi-disciplinary, and collaborative projects that may require diverse resources to be
shared among participants. In particular, an environment
that often makes use of a group of high-performance network facilities would involve large-scale collaborative projects and tremendously seek a robust and flexible access
control for allowing collaborators to leverage and consume
resources, e.g., computing power and bandwidth. In this
paper, we propose a federated access management scheme
that leverages the notion of attributes. Our approach allows
resource-sharing organizations to provide distributed provisioning (publication, location, communication, and evaluation) of both attributes and policies for federated access
management purposes. Also, we provide a proof-of-concept
implementation that leverages distributed hash tables (DHT)
to traverse chains of attributes and effectively handle the
federated access management requirements devised for interorganizational resource sharing and collaborations.

1.

INTRODUCTION

Traditionally, collaborative information sharing heavily
relies on client-server or email-based systems. By recognizing the inherent deficiencies such as a central point of
failure and scalability issues, several alternatives have been
proposed to support collaborative sharing of resources, including Grid computing, Peer-to-Peer (P2P) networking [11]
and Cloud computing [27]. Given all the diverse contexts of
collaboration, achieving effective access control is a critical requirement. The sharing of sensitive information and
resources is necessarily to be highly controlled by defining
what is shared, who and under which conditions is allowed to
share. In particular, users without pre-existing relationships
may try to collaborate and request the information. It is required for a resource provider to be able to cope with a large
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SACMAT’15, June1–3, 2015, Vienna, Austria.
c 2015 ACM ISBN 978-1-4503-3556-0/15/06 ...$15.00.
Copyright 
DOI: http://dx.doi.org/10.1145/2752952.2752977.

number of collaborators and guarantee the information and
resources be released only to trusted collaborators within
the community. In addition, resources are constructed with
various types and domain policies, and each collaborating
party may enforce security policies in their systems with
different degrees of assurance. Therefore, building systematic mechanisms for sharing resources across collaborative
network environments is indeed an important challenge.
Furthermore, organizations including private and public
sectors have recognized the benefits of being involved with
inter-organizational, multi-disciplinary collaborative projects
that may require diverse resources shared among participants, e.g., data, computation time, storage, etc. In particular, an environment that often makes use of a group of highperformance network facilities would involve large-scale collaborative projects and tremendously seek a robust and flexible access control for allowing collaborators to leverage and
consume resources. For example, under the US Department
of Energy (DoE), numerous research laboratories and scientists have collaborated and performed their experiments
demanding specific network bandwidth and designated computing resources from each other. They even exchanged
data and resources with other foreign researchers, which lead
them to utilize high-performance network environments such
as ESnet [29], GÉANT [7], and NORDUnet [21]. Despite the
necessary administrative tasks such as resource scheduling
and provisioning, there is a need to properly mediate the
way such resources are to be safely shared in the context of
collaborations. Because most of these providers depict their
own in-house authentication and authorization services, a
well-defined, inter-organizational and implementation-independent approach is needed. With this in mind, this paper presents our approach to address the aforementioned
challenges by leveraging the concept of attributes: observable properties that are exhibited by access control entities,
e.g., users and protected resources, that become relevant under a given security context [19], focused on DoE networks
and their collaborators’ networks. Using attributes as an
underlying framework, we propose an approach based on
the concept of a federation between participant organizations, allowing them to provision: specify, publish, locate,
and communicate attributes for federated access management purposes in a distributed way, thus allowing for the
specification and automated evaluation of both local (intradomain) and federated (inter-domain) policies. With this in
mind, this paper makes the following contributions:
• We formulate the main components involved in federated
access management. We show how attributes in the lo-

cal context can be leveraged in a federated context such
that access permissions for inter-organizational resource
sharing can be properly granted.
• We also provide a well-defined description of attributes,
which includes the use of data types, standardized names,
and run-time values, so that participants can unambiguously use those to define inter-organizational attributes
and policies for federated access management purposes.
• We propose an attribute generation approach by means of
a set of so-called attribute derivation rules (AD-Rules).
Moreover, we also introduce attribute derivation graphs
(AD-Graphs) that allow to compose AD-rules.
• We provide an initial step toward automated attribute
discovery based on distributed hash tables (DHT) [28],
which allows for efficient discovery and retrieval of attributes within a federated and distributed context. In
addition, we provide a proof-of-concept implementation
of our attribute provisioning scheme, including an evaluation approach that shows the feasibility of our approach
for real-life implementations.
This paper is organized as follows: we start by articulating
problem statements and technical challenges with respect to
federated access management in Section 2. Then, we describe our approach in Section 3 followed by the proof-ofconcept implementation and evaluation results in Section 4,
which shows the practicability of our approach for supporting real-world collaborations among the DoE-affiliated highperformance network facilities. We overview the related
work in Section 5 and discuss some relevant topics related to
our approach as well as matters for future work in Section 6.
Finally, Section 7 provides concluding remarks.

2.

BACKGROUND

As previously mentioned, DoE-affiliated high-performance
network facilities have identified the need to provide automated means for resource sharing between different administrative (and security) domains. As an example, the
Open Grid Forum [9] introduced a multi-organizational effort called the network services interface (NSI) [23] that is
composed of a set of well-defined protocols that allow participants to collaborate on research endeavors by implementing
inter-organizational services in an automated way. The protocol devised for a given NSI service is implemented by socalled network service agents (NSA) which are expected to
support all service-related tasks within the context of a given
administrative domain. Fig. 1 shows an example depicting a
data transfer between two hosts that are located within the
administrative boundaries of two different organizations and
whose networking path involves the participation of a third
network serving as a bridge. In this example, each participating network implements the protocol devised for the NSI
connection service by means of a dedicated NSA. Following
such a protocol, a connection request R is first serviced by
the local NSA where R originates (NSA1 in Fig. 1). On each
network, the local NSA is in charge of reserving local ports
and bandwidth to create a connection within its network
boundaries. NSA1 is also in charge of contacting the other
NSAs involved in serving R (NSA2 and NSA3 ) so that they
can make reservations within their inner networks. In addition, all involved NSAs must handle network connections

Figure 1: An NSI inter-domain data transfer: An end-user
presents credentials to the software agent labeled as NSA1 ,
requesting for data stored in a host under the ESnet domain
to be transferred to a host located under the NORDUnet
domain, which is in turn managed by the agent known as
NSA3 . The GÉANT domain (managed by NSA2 ) serves as
a bridge for the connection purpose.
between independent networks by physically interconnecting any relevant service termination points (STPs), which
are abstract (high-level) representations of actual network
ports and are labeled from A to F in Fig. 1. Once the connection path between the source and destination hosts is
completed, the requested data transfer takes place.
In this collaborative setting within DoE-affiliated highperformance network facilities, we articulate the following
the federated access management requirements that need to
be accommodated:
1. Participating organizations should be allowed to define
its own set of federated access management policies governing the way a given service, e.g., the aforementioned
connection service, is provided in response to both local
and external requests. As an example, ESnet may want
to give priority over local resources to requests originated
within its local domain.
2. Participating organizations may also agree on a set of
inter-domain federated access management policies governing a subset of service interactions between them. As
an example, ESnet and GÉANT may agree on a policy
allowing for a collaborative project between both organizations to be guaranteed with high quality of service by
reserving sufficient bandwidth for data transfers.
3. Participating organizations may implement their own inhouse federated access management systems, which may
in turn handle their own set of local credentials and possibly their own set of locally-relevant attributes. This may
potentially result in problems such as attribute incompatibility, or different attributes being assigned to the same
access control entity by different domains, e.g., users getting credentials issued by each service in response to their
access request, possibly result in a large set of credentials
to be handled. However, organizations may not favor a
complete replacement of their current authentication and
access control modules, as such an effort may involve considerable financial and organizational effort. As an example, ESnet may find it difficult to replace the current set
of locally-issued credentials for the more than 40 research
institutions currently being served by the network [29].
4. Every access control entity, e.g., end-users and protected
resources, involved in serving a given access request is
expected to provide a set of security-relevant properties,
e.g., user credentials or resource descriptors, which may

have in turn been assigned either by its local security
domain or by an external one, in such a way that proper
policy evaluation based on such properties can take place.
If a given entity fails to show those properties, even when
they may have been legitimately assigned beforehand, the
evaluation of a relevant access control policy may fail thus
causing legitimate access to be denied as a result. In
practice, such properties are commonly assumed to exist
at policy evaluation time, either locally or remotely, e.g.,
stored in a dedicated centralized database. In addition,
security-relevant properties may be in turn derived by
processing other related properties. As an example, user
credentials may be used to obtain the set of collaborative
projects the user is involved in, without requiring the user
to explicitly enumerate them, granting access only to the
resources those projects are entitled to. However, existing
infrastructures are not capable of seamlessly locating and
transforming security properties in a distributed setting
such as the one depicted in Fig. 1, which is composed of
several independently-managed security domains.
5. Finally, existing federated approaches for security, e.g.,
OpenID [26] or Shibboleth [18], are focused on authentication: support for authorization is limited and is mostly
left for third parties to implement from scratch, e.g., attribute and policy definition, discovery, and evaluation.
Consider the example of the three participant organizations on the data transfer requests depicted in Fig. 1. Each
organization agrees on an inter-organizational policy P 1 that
allows for data transfers between participants, e.g., from
STPs A to F, if all of the following conditions are met: first,
the requester is a member of a collaborative group labeled
as G. Second, the size of the data to be transferred is less
than or equal to 10 Tb. Third, the available bandwidth on
each network is higher than or equal to 1 Gbit/s.

3.

APPROACH

A well-defined approach for the specification and provisioning of both policies and security-relevant properties (attributes) is critical to enable inter-organizational resource
collaboration—specifically an approach that goes beyond
credential-sharing by including heterogeneous attributes obtained from different federated access management entities,
which may have been assigned by different security domains.
As depicted in a recent report by the National Institute for
Standards and Technology (NIST) [19], proper provisioning mechanisms may become a crucial component for the
successful development of new technologies and new infrastructures based on attributes. Inspired by recent successful approaches for federated authentication, we propose a
federated and distributed solution for the specification, location, generation, and communication of both attributes
and policies for federated access management purposes that
is intended to support automated resource sharing and the
establishment of collaborative projects among independent
organizations, each possibly implementing their own security domain as well as their own dedicated federated access management infrastructure. A graphical depiction of
our approach is shown in Fig. 2: a locally-defined attribute
a 1 belonging to a given user is transformed into a series of
federation-recognized attributes (a 2 , a 3 , a 4 ) that are in turn
provided by other organizations engaged in a federation and
may be used for access control decisions.

a4

a1
a2

a3

Figure 2: A federated access management framework: the
local attribute a 1 is transformed into the federated attributes labeled as a 2 , a 3 and a 4 by leveraging attribute
derivation rules (AD-Rules) implemented by remote peers.
In order to participate in our proposed federation, participating organizations under DoE-affiliated high-performance
network facilities must fulfill the following:
1. Attribute identification: Participating organizations are
to identify security-relevant properties within their local
domains that may serve as local attributes for federated
access management purposes. As an example, in Fig. 1,
ESnet should identify any relevant metadata belonging
to the data to be transferred that can be used to obtain
the properties that are relevant under policy P 1 , e.g., its
size in bytes.
2. Attribute mapping: Participants must map local attributes onto a set of publicly-known federated attributes to
be used in the context of an inter-domain collaboration.
Following our running example, a standard definition of
an attribute depicting the size of a given chunk of data,
e.g. a convention name, size unit, etc., would allow the
specification and enforcement of policies across organizational domains. Because participant organizations may
in turn have their own in-house definitions for local attributes, e.g., names, data ranges, etc., a consensual interorganizational definition of federated attributes is needed.
With this in mind, existing approaches based on ontological representations such as the one proposed by Paci,
et al. [24], may be utilized to mitigate the existence of
different attribute definition schemes, also known as attribute heterogeneity. Due to the nature of DoE-affiliated
network facilities, we assume such a common knowledge
base on attributes has been established beforehand.
3. Attribute discovery: Participants should allow organizational peers to leverage the federated attributes they provide by means of a discovery service. Following our running example, ESnet and GÉANT should be able to locate each other’s attributes when constructing an interdomain policy for shared connections.
4. Federated access management administration: Organizations should implement a proper administrative model
for creating, updating, and removing both local as well
as federated attributes and federated access management

ADR

policies that restrict access to protected resources within
collaborative projects.
5. Policy conflict resolution: Finally, participants should be
able to detect and resolve conflicts when constructing federated access management policies, e.g., contradictory or
redundant rules, etc. As an example, let’s assume the
ESnet domain also provides a local policy P 2 that allows
for intra-domain transfers to take place, e.g., from STPs
A to B in Fig. 1, if the requesting end-user is a member
of a certain local group and the data to be transferred
has not been obtained from a particular server storing
sensitive data located within the network scope. In such
a setting, the inter-organizational policy P 1 depicted in
Section 2 may be in conflict with P 2 if the data to be
transferred comes from such a data-sensitive server, as
P 1 may authorize the transfer but P 2 may deny it.
With respect to the evaluation of federated access management policies, participants are responsible for the following:
1. Policy retrieval : Upon receiving a given federated access
management request R, participants should retrieve the
set L containing local policies relevant to R. Following
our running example, ESnet should retrieve the P 2 policy
regarding data transfers originating in its local domain.
2. Attribute provision: Participants should provision any local and federated attributes as specified in the policies
contained in L. To enable this provisioning, participants
are to make their federated attributes available for other
peers to provision upon request. In addition, participants should make (allowable) attribute transformations
available to their peers. Following our running example, GÉANT transforms the credentials presented by an
end-user in the ESnet domain into an attribute depicting
membership to the federated collaborative project G that
is required in the inter-organizational policy P 1 .
3. Policy dispatch: Participants should dispatch policy evaluation requests for relevant federated policies I that are
relevant to R. Conversely, participants should evaluate
and provide results for any policy evaluation requests
they receive as part of a request evaluation process initiated by a federated peer. Back to our running example,
participant networks should retrieve all attributes relative to a connection request that happen to be under the
scope of their local security domain and should dispatch
both attribute and policy evaluation requests, e.g., P 1 ,
to the other networks involved in the construction of the
network path.
4. Results aggregation: Finally, the policy decisions for both
sets L and I should be derived and combined to produce
a final decision for the request R, which is to be communicated to the requesting entity, e.g., the end-user under
the ESnet domain in Fig. 1.

3.1 Model Description
We start the discussion of our model for federated access
management by defining the following components:
• actors are end-users (i.e. human agents) or subjects
(i.e. computer processes) acting on behalf of users;
• targets are the protected resources within a security
domain;

Permissions
Federated
Attributes

AC
Entities

AA

PA

Operators

Targets

ADR
Local
Attributes

Attributes

Figure 3: A model for federated access management: attributes are related to entities, e.g., end-users and protected
resources, by means of the attribute assignment (AA) relation. Attributes (both local and federated ) may be transformed into federated attributes by means of AD-Rules
(ADR). Permissions are related to attributes by means of
the permission assignment (PA) relation.

• context is the running (executing) environment, e.g.
operating system, supporting platform, etc., where a
given request is issued and/or served.
Fig. 3 shows a visual representation of our model: attributes are related to access control entities by means of
the attribute assignment (AA) relation, allowing each entity
to exhibit many different attributes and a single attribute to
be potentially exhibited by more than one entity. Federated
attributes are publicly-known attributes that may be relevant in the context of a given collaboration project. Local attributes are related to federated attributes through attribute
derivation rules (AD-Rules), which are shown as directed arrows in a dotted line in Fig. 3. The precise definition of such
AD-Rules, e.g., how local attributes are ultimately related
to federated ones, is defined by peers within the context of
a given collaboration. As we will discuss in Section 3.4, ADRules can be organized into a graph-like structure known as
an attribute derivation graph (AD-Graph), which provides a
representation of how attributes are related to permissions,
which are in turn related to federated attributes by means
of the permission assignment (PA) relation. Permissions are
depicted as a combination of a protected source (target) and
an operation that can be performed on it. A given attribute
may be related to one or more permissions, and a given permission may be related to one or more attributes.
A description of our proposed approach is shown in Fig.
4. The basic components are actors (ACT), targets (TAR),
and context (CON), which together construct the set E of
access control entities. Moreover, we also consider the sets
operations (OPER) and permissions (P). We define the sets
names (N) and values (V), which are used for defining the
sets of attributes (A) and federated attributes (F). The relationships between the elements of our model are described by
defining the attribute assignment (AA) and permission assignment (PA) relations, as well as our proposed AD-Rules.
The definition of AD-Graphs is based on the concepts of
graph theory and the definition of AD-Rules. The access
control decision process is modeled by functions provisionedAttributes, expectedAttributes, relatedPermissions, and checkAccess. Function provisionedAttributes calculates the set of
attributes that can be provisioned from a given AD-Graph

based on the local and federated attributes initially exhibited by a set of access control entities. Function expectedAttributes returns the set of attributes that are related to a
given permission, by inspecting the PA relation. The mirror
function relatedPermissions returns the set of all permissions that are associated in the PA relation with a given
attribute. Finally, function checkAccess implements the authorization checking functionality by first calculating the set
of attributes provisioned by the entities in a given access
control request and comparing it with the set of attributes
that are related to the requested permission, which is only
granted if the set of provisioned attributes (obtained from
the provisionedAttributes function) is a subset of the set of
attributes related to such permission, which is obtained from
the expectedAttributes function.

3.2 Attributes
We define attributes as an abstraction of security-relevant
properties that are exhibited by access control entities, namely, actors, targets, policies, and any applicable context. Their
physical nature, e.g., if the attribute represents a file’s metadata or an end-user credential, and the way those attributes
are collected from the access control entities remain dependent on each organizational domain.
As shown in Fig. 4, we define attributes to have the following three components: (1) a data type, which restricts
the nature and the possible range of values defined for the
attribute, (2) a name, which is later used for defining ADRules on them and is defined in the context of a given
inter-organizational setting, and (3) a value, which is used
when evaluating such AD-Rules. Examples of attributes
include: <Double, data.size, 100.0>, <String, data.source,
“server.esnet”>, and <Date, system.date, “10 -10 -2015 ”>.

3.3 Federated Attributes
Federated attributes are obtained by processing local attributes from access control entities under a given organizational domain. Such processing is to be modeled through the
AD-Rules, thus allowing federated attributes to be related
to access rights (permissions).
As an example, AD-Rules may provide functionality intended to validate a given local attribute by inspecting its
value component and producing a proper federated attribute
as a result. Thus, a validated federated attribute ensures
that a given collaboration state remains secure.
As described in Section 3.1, permissions can be assigned
to federated attributes, which then serve as a layer of association between local attributes and permissions defined in
another organizational domain for collaborative purposes.
Such a layer helps identify the local attributes that may be
involved in granting a given inter-domain permission, as well
as the set of constraints represented by AD-Rules that may
be involved in such a process. Moreover, our approach allows for AD-Rules to take federated attributes as an input or
may also take both local as well as federated ones as an input to produce federated attributes as a result, as depicted
in Fig. 3, thus allowing for expressing richer inter-domain
policies based on processing already existing federated attributes.

• ACT, the set of actors.
• TAR, the set of targets.
• CON, the set of context instances.
• OPER, the set of operations.
• P ⊆ TAR × OPER, the set of permissions.
• E = ACT ∪ TAR ∪ CON, the set of access control entities.
• N, the set of names.
• V, the set of values.
• T, the set of data types.
• A = { a | a = <type, name, value>where type ∈ T, name
∈ N, value ∈ V, the set of attributes.
• F ⊆ A, the set of federated attributes.
• AA ⊆ A × E, the attribute assignment relation mapping
attributes with a given access control entity.
• PA ⊆ P × A, the permission assignment relation mapping
permissions and attributes.
• ADR = { r | r: 2A → 2F }, the set of attribute derivation rules mapping sets of attributes to sets of federated
attributes.
• ADG, the set of directed, weakly connected, and possibly
cyclic attribute derivation graphs. A graph g = <NODES,
ARCS>∈ ADG if NODES ⊆ 2A and ARCS ⊆ ADR. We
say (n 1 , arc , n 2 ) ∈ g if n 1 , n 2 ∈ NODES and arc ∈ ARCS
and n 1 ⊆ domain(arc ) and n 2 ⊆ codomain(arc ).
• provisionedAttributes: 2E × ADG → 2A , a function mapping a set of entities E ′⊆ E with the set of attributes that
the entities in E ′can provision from a given ADG g. An
attribute f is said to be provisioned by an entity e ∈ E ′if
there exists a set of attributes A′= { a | a ∈ A, e ∈ E ′, (a ,
e) ∈ AA } ⊆ A and a set of paths P = {p | p = x 0 , x 1 ,
...x n , n ≥ 0} in g such that ∀ p ∈ P, x 0 ∈ A′and x n = f,
and ∀ x i , x j in p, 1 ≤ i < n, j = i + 1, ∃ r ∈ ADR such
that r (x i ) = x j .
• expectedAttributes: P → 2A , a function returning the set
of attributes that are related to a given permission p. Formally, returns all a ∈ A such that (p, a) ∈ PA.
• REQ = {req = <act, p = <tar, oper >, ctx >| act ∈ ACT,
p ∈ P, ctx ∈ CON}, the set of access control requests,
allowing an actor act to request for a permission p to be
granted.
• checkAccess: REQ × ADG → {true, false}, a boolean function that checks if a given request req = (act, p = (tar,
oper ), ctx ) ∈ REQ should be granted or denied based on
a given attribute derivation graph adg. Formally, the function returns true if provisionedAttributes({act, tar, ctx },
adg) ⊆ expectedAttributes(p), and returns false otherwise.

Figure 4: A model description of our approach.

3.4 Attribute Derivation Rules and Graphs

attributes and federated attributes. For this purpose, ADRules are said to be non-injective ∗ , as two or more elements
from an input set of attributes (domain) may be mapped to
the same element in the output set (co-domain).
In addition, AD-Rules can be chained together to produce
a graph-like structure showing how attributes can be provisioned. Such attribute derivation graphs (AD-Graphs) are
directed, because AD-Rules represent unidirectional edges
(due to their nature as functions). Moreover, AD-Graphs
are also weakly connected, as there is no requirement for
all nodes (attributes) to be connected to each other. Finally, AD-Graphs are also possibly cyclic, as a customized

As introduced in Section 3.1, attribute derivation rules
(AD-Rules) are expected to provide a mapping between local

∗
A function f : A → B is said to be injective or one-to-one,
∀ a, a ′∈ A, a 6= a′⇒ f (a) 6= f (a ′).

a set of starting nodes (local attributes) and a given ending node (federated attribute). Then, determining if such a
federated attribute grants the requested permission over the
desired resource. A general procedure for resolving an access
request, derived from the model shown in Fig. 3: given a federation F, a permission P, and a set of input attributes I the
procedure starts by obtaining the set of expected federated
attributes granting P, e.g., by parsing local and federated
access management policies. If the required set is found to
be a subset of I —that is, I contains the attributes required
for P, access is granted. Otherwise, the procedure extracts
a set of paths from an AD-Graph in F, each of these paths
starting with an attribute in the set I and ending with an
attribute in the required set. Then, each path is traversed
by executing each of the included AD-Rules. If new federated attributes are generated, and such attributes happen
to include the attributes in required, access is granted and
the procedure terminates. Otherwise, access is denied.

3.5 Attribute Provisioning

Figure 5: A distributed AD-Graph depicting policy P 1 : local attributes (shown in grey) are transformed into federated ones (shown in white). As an example, the AD-Rule
labeled as r 8 transforms attributes G (group membership),
Size (data size) and Bw-e , Bw-g , Bw-n (bandwidth) into
the federated attribute Ta that is related to the TP (data
transfer) permission.
chaining of AD-Rules may end up introducing a cycle in the
produced AD-Graph.
AD-Graphs may also support collaborative processing by
allowing a division into proper subgraphs, each subgraph
implemented in a different security domain: as mentioned in
Section 2, each participating domain is in charge of defining
its own permissions, local and federated attributes, as well
as the AD-Rules and AD-Graphs to generate those. ADGraphs can be modeled as a distributed graph: a given ADGraph G defined for a federation F may be divided into a
set of subgraphs G′1 , G′2 , ... G′n , such that each G′i is to
be processed by a different domain in F.
As an example, the AD-Graph in Fig. 5 implements the
inter-organizational policy P 1 described in Section 2 as follows: the ESnet local attribute Cred-e , which depicts a
locally-issued credential, is transformed by the AD-Rule labeled as r 3 into the federated attribute L that features membership to a local group within ESnet. L is subsequently processed by the AD-Rule r 6 in the GÉANT domain, producing
the federated attribute G , which in turn depicts membership to an inter-organizational collaborative group. Later,
G , along with attributes Size , Bw-e , Bw-g , and Bw-n are
taken as input for the AD-Rule labeled as r 8 , producing the
Ta attribute as a result. This attribute features an access
token related to the TP permission authorizing the data
transferring process shown in Fig. 1. Such a permission is
included in Fig. 5 for the illustrative purposes.
Leveraging the previous definitions, the problem of resolving an access request to a shared resource within a federation
can be first modeled as a path traversal problem within a distributed graph: determining if there exists a path between

In our approach, attribute provisioning is crucial to handle federated access management requests in the context
of inter-organizational resource sharing. Such a process includes allowing for participating organizations to know about
the AD-Rules that are implemented by other organizations
and are involved in a given AD-Graph G. Concretely, participants need up-to-date information about G so that they
can extract correct paths within G that can produce the
desired federated attributes. With this in mind, attribute
provisioning can therefore be divided into two process: path
discovery and path traversal.
The path discovery process allows for each organization to
distribute information about its locally-implemented ADRules to the federated access management federation, so
that they can potentially maintain a representation of G for
path calculation. However, there are several practical challenges: first, each organization needs to be notified when
changes to G occur, e.g. adding or removing a given ADRule, which may create a large set of communication messages between participants. Second, there is an added maintenance cost, e.g. processing time, that participating organizations must incur for handling and maintaining an up-todate G. Finally, storage efficiency may become an issue when
a large G must be locally maintained. An alternative approach would be creating a central database storing G, along
with a set of replicas for enhanced availability. However,
such a scheme may suffer from service bottlenecks and consistency issues when communicating updates to the replicas.
In addition, a centralized server may become the subject of
a denial of service (DoS) attack, which could certainly limit
the availability of the overall attribute provisioning scheme,
thus potentially preventing participating organizations from
serving federated access management requests. With this
in mind, there is a need for a distributed approach that allows for participating organizations to release information
about the AD-Rules they implement in such a way that the
administration burden, e.g., number of communication messages, is significantly reduced. In addition, such an approach
should also prevent organizations from having to store a
complete AD-Graph locally for path discovery purposes and
should provide support against attacks targeting a single
point of failure. We present an implementation tailored for
meeting such goals in Section 4.

Bw-e

r1

Net-e

Bw-n

r5

Net-n

NORDUnet

Bw-g

r7

G

GÉANT

ESnet

GÉANT

r3

Cred-e

G

r4

Cred-n

G

r6

L

ESnet

L

Bw-e

ESnet

Data

Size

NORDUnet

r2

GÉANT

Size

Ta

r8

G

GÉANT

NORDUnet

ESnet

Bw-g
Bw-n

Figure 6: An illustrative DHT ring depicting the AD-Graph
of Fig. 5: federated peers store entries containing information about the AD-Rules implemented by other peers in the
context of federated access management.
Following the model described in Fig. 3, the path traversal
process allows participating organizations to invoke the ADRules included in a given path p in G that may ultimately
produce a given federated attribute. Invocation of such ADRules should be done by following a sequence starting from
the first AD-Rule in p up to the last one. Each time an ADRule is executed, the produced set of attributes is added to a
set of input attributes for the next AD-Rule in the sequence.
In addition, the invocation of an AD-Rule r enables to locate
the federated domain implementing r, the set of input attributes, as well as the set of produced attributes. A request
for the invocation of r should include the set of attributes
that serve as its input. Finally, the attributes produced by r,
if any, should be then communicated back to the requesting
organization.

4.

IMPLEMENTATION AND EVALUATION

In this section, we describe our proof-of-concept implementation and evaluation results. We elaborate how we accommodate the concerns described in Section 3.5. Also, we
discuss how the path discovery process was implemented
with the concept of distributed hash tables (DHT) [28]. In
addition, we discuss our implementation on the path traversal process which is based on a client-server architecture for
the remote invocation of our proposed AD-Rules.

4.1 Path Discovery
Fig. 6 illustrates the path discovery process based on our
running example. We allow for participants in a federation F
to join a DHT ring to publish and retrieve information about
the AD-Rules that may produce federated attributes. This
process may be in turn decomposed into two inner components, namely, AD-Rule publishing and AD-Rule retrieval.
The procedure for publishing an AD-Rule is conducted
as follows: each domain is in charge of inserting an entry
into the DHT for each AD-Rule they implement for a given
AD-Graph under the context of F. Such an entry should
include information about the input attributes (either local

or federated ones), the name of the AD-Rule, and the set
of federated attributes to be produced as a result. Moreover, some information on how to execute such AD-Rule
should be also provided, e.g., a universal resource locator
(URL). As an example, the ESnet domain will publish an
entry into the DHT containing information about the ADRule r 1 , including the local input parameter Net , which
conceptually depicts information about the current state of
the local network, and the federated attribute Bw , which
provides a standard representation of the current bandwidth
capacity. In addition, such an entry should contain a valid
URL for other federated peers invoking the AD-Rule r 1 remotely. Following the insertion procedure for DHTs [28],
such an entry may end up being stored for future location
at a different federated peer, following a hashing scheme
based on the standardized naming convention for federated
attributes introduced in Section 3.2. In Fig. 6, the entry for
the AD-Rule labeled as r 1 (published by ESnet) ends up being stored by the DHT node under the scope of the GÉANT
domain. Conversely, AD-Rules may be retired from a given
AD-Graph by removing their corresponding entries from a
given DHT ring. Recall such procedure may not necessarily
remove the production of federated attributes in the context
of an AD-Graph, as such attributes may be produced by another AD-Rule in the DHT ring, e.g., removing the entry for
the AD-Rule r 6 does not prevent an attribute G from being
produced by the AD-Rule labeled as r 4 .
The retrieval procedure for entries containing information
about AD-Rules is to be conducted as follows: a participating domain D interested in producing a given attribute A
may retrieve the set S of entries corresponding to A in the
DHT ring, e.g., by hashing the A’s identifier. Then, by inspecting the information about AD-Rules contained in S, D
must determine if there exists a local or federated attribute
under its local domain that can be used as an input parameter to an AD-Rule to produce A. If so, information from the
corresponding entry in the set S is retrieved and the ADRule is invoked. However, if no suitable entry is found, e.g.
all input attributes to the entries in S are out of scope or cannot be locally produced, D may attempt to explore the DHT
ring once again for entries producing the attributes taken as
an input to the entries in S, thus potentially producing a set
P of graph paths in an AD-Graph stored in the DHT. Such
a process may be repeated up to the point when no more
entries can be obtained from the DHT or a cycle in the ADGraph stored in the DHT is detected, e.g., when an iteration
retrieves entries that were previously retrieved in the past,
or a path can be traversed. A path in P is traversed, e.g.,
by calling the sequence of AD-Rules contained in it, only if
it starts with an attribute under the scope of D and ends
with the desired attribute A. Considering our running example, an entity under the ESnet domain may provision an
attribute Ta depicted in Fig. 5 as follows: the DHT featured
in Fig. 6 retrieves the entry for the AD-Rule labeled as r 8
from the ring node implemented by NORDUnet. As the
input parameters of r 8 are all federated attributes, ESnet
inspects the DHT ring once again for determining proper
AD-Rules provisioning those attributes. Then, entries generating Bw-e (r 1 ), Bw-g (r 7 ), Bw-n (r 5 and r 9 ), Size (r 2 )
and G (r 4 , r 6 ), are returned. For the federated attribute Bwe , ESnet can provide the local attribute Net-e required for
r 1 , thus creating a traversable path within the distributed
AD-Graph. In addition, for the federated attribute Size ,

Table 1: Performance (ms) for policy P 1 .

Runtime Performance of our Attribute Provisioning Framework
3000
ALT

ALT
411
484
531
492
470
498

ATT
352
921
1,613
14,214
35,201
85,254

OPT
83
1,405
2,144
14,706
35,671
85,652

ESnet can also provide the required local attribute Data required for r 2 , thus creating a path as well. In the case of
G , the entry belonging to r 4 may be discarded as its input
attribute (Cred-n ) is local only to NORDUnet. However, in
the case of the entry for r 6 , ESnet may inspect the DHT
ring once again for an entry producing the input attribute
L. Next, the entry for r 3 is returned taking Cred-e as an
input. Since Cred-e is local to ESnet, another traversable
path is constructed. With respect to an attribute Bw-n , the
AD-Rules labeled as r 5 can be also discarded as its input
attribute (Net-n ) is local to NORDUnet. However, r 9 can
be used as it takes the federated attribute G as an input,
and a path producing G has been already obtained. Similarly, an attribute Bw-g can be obtained from r 7 as such
an AD-Rule takes G as an input. The setting depicted in
Fig. 5 and Fig. 6 allows for the AD-Rules labeled as r 7 and
r 9 to disclose network-related information, e.g., bandwidth,
only when membership to an inter-organizational project (as
depicted by the G attribute) can be shown.

4.2 Path Traversal
Our implementation supports the process of path traversal by allowing for each participant domain D to implement
a software agent that is capable of handling requests for the
invocation of the AD-Rules that are under the scope of D.
Information on locating such agent and invoking the implemented AD-Rules should be consistent with the entries
published in the DHT ring described in Section 4.1, e.g.,
ESnet may provide a TCP/IP agent that implements the
AD-Rule labeled as r 1 in Fig. 5 and Fig. 6. For a given
path P composed of n entries obtained from a DHT ring,
the traversal procedure would include requesting for the execution of each entry starting from the entry at the first
position and collecting the attributes produced by the ADRule being invoked (if any). The process continues as soon
as new attributes are produced on every AD-Rule invocation and finishes either when a given AD-Rule depicted by
an entry in the path is not able to produce any attributes
or the final entry (at position n - 1) has been executed and
the final attributes have been produced as a result.

4.3 Experimental Results
We have implemented the DHT functionality discussed
before by leveraging the Open Chord 1.0 API [15]: an open
source implementation of the Chord DHT [28] that allows
for remote peers to implement a DHT ring by communicating with each other over TCP/IP sockets. In addition,
our proposed AD-Rules, as discussed in Section 3.4, were
implemented by leveraging a client-server architecture over
TCP/IP sockets with the standard java.net package.
In the first experiment, we examined the inter-organizational policy P 1 shown in Fig. 5 and Fig. 6. Such an ADGraph is stored in a DHT ring composed of three nodes and
each of them simulates three participating organizations in

ATT

2500

2000

Time (ms)

Processing Time
10
50
100
100
2,500
5,000

1500

1000

500

0
10

50

100

Average Execution Time for AD-Rules (ms) (5-5, 10-10, 20-20)

Figure 7: Experimental results for our implementation.

our running example. In addition, each of these nodes has
been augmented with a server module implementing each
of the AD-Rules included in the aforementioned AD-Graph.
As an example, the ESnet domain was simulated by DHT
node as well as a server module implementing the r 1 , r 2 and
r 3 AD-Rules. In addition, the processing time of each ADRule included was simulated by introducing a code to halt
the execution for a certain period of time. In our experiments, we measured the average location time (ALT) for
constructing a given path P within the AD-Graph implementing P 1 policy. Also, we measured the average traversing time (ATT) for P to return a federated attribute as a
result. Finally, we calculated the overall provisioning time
(OPT) by consolidating both ALT and ATT. Table 1 shows
our experimental results when attempting to provision the
federated attribute Ta simulating an entity in the ESnet domain holding the local attributes Cred-e , Net-e and Data as
shown in our example. Since the length (number of DHT entries) of the paths under the experiments remains the same,
e.g. the same number of involved attributes and AD-Rules,
variation in the OPT for each experiment is mostly due to
the preconfigured execution time of the AD-Rules included
in such paths, whereas the ALT involved in constructing
those paths remains manageable.
In the second experiment, we measured the response time
in provisioning attributes over various AD-Graphs. On each
experiment, we produced an AD-Graph depicting a varying number of paths (branches) and each of them includes
the different number of composing nodes (links). In addition, we simulated the execution time of each AD-Rule
involved in the produced AD-Graph by using a configurable
parameter. We maintained the DHT and server configuration as described earlier. On each experiment instance, we
attempted to provision the attribute produced by the DHT
entry located at the last node of each path in the simulated
AD-Graph. As an example, for a path composed of l nodes,
we issue a request for the attribute produced by the DHT
entry located at position l -1, assuming that we can include
the attribute in the request as the input for the entry depicted in position 0 of the path. Fig. 7 shows our results
when constructing AD-Graphs of size (b-l ) where b stands
for the number of branches and l stands for the number
of links on each AD-Graph, e.g., the first three-column set
shows the evaluation results when setting up an execution

time of 10 ms for AD-Rules and constructing AD-Graphs of
size (5-5), (10-10) and (20-20) respectively.
As described before, we obtained both the ALT and the
ATT on each experiment, which are used to calculate the
OPT. In the first experiment, most of the overall provisioning time is spent on the path traversal, which is mostly influenced by both the execution time of each AD-Rule in a
given AD-Graph, as well as the length of the path. Similarly the ALT observed in the second experiment, while it
was also affected by the length of the path, remains just as
a small fraction of the OPT, mostly due to the nature of
distributed network settings based on DHTs.

5.

RELATED WORK

The problem of providing security guarantees in interorganizational settings has been largely addressed in literature. In particular, several federated identity [4] approaches
have been introduced to allow partnering organizations to
reuse locally-issued credentials when accessing resources located under the scope of an external security domain. As
an example, OpenID [26] and Shibboleth [18] have recently
gained acceptance in both industry and academia respectively for user-credential sharing. Our approach builds on
this idea by allowing participants to exchange federated attributes, thus potentially allowing for such attributes to serve
as tokens granting access to shared resources, in an approach
also inspired by Kerberos [20], OAuth [14] and more recently,
Facebook Login [8], which strives to allow third-party applications to leverage the user credentials defined for the popular social network to access application-dependent resources.
Moreover, our AD-Rules are inspired by the idea depicted
in the credential-discovery protocol proposed by the RT Framework [17], which allows for credentials issued by independent domains to be located and leveraged for federated access management purposes. Similar to the RAMARS Framework [12], our AD-Rules are depicted in a graph-like structure that allows for user-defined attributes to be transformed
into a set of widely-recognized credentials. However, the
RAMARS framework assumes each security domain implementing the transformation functions may be partially trusted
by modeling trust in the range [0,1]. In our approach, we
assume all federated peers fully trust each other for the implementation of the federation goals as discussed in Section 3
and the model presented in Fig. 4, due to the nature of DoEaffiliated high-performance network facilities.
In addition, recent approaches leveraging federated identity for sharing resources include the work of Broeder et
al. [2] and Ananthakrishnan et al. [1]. Moreover, Klingenstein [16] and Chadwick and Inman [5] incorporate the concept of end-user attributes with the federated identity. Our
approach includes attributes originated from different access
control entities rather than considering attributes and credentials from end-users.
In the context of attribute-based models, Zhang et al. [30]
introduced their attribute-based access control matrix, which
extends classical theory in the field of access control to accommodate attributes as well as the notion of security state.
Moreover, Priebe et al. [25] presented an approach leveraging the concepts of ontologies and the semantic web in order
to formalize the notion of attributes. An approach close to
ours was introduced by Covington and Sastry [6], who presented a contextual attribute access control (CABAC) model
which was realized in mobile applications. However, our

approach goes a step further by describing the way such attributes are mapped to access rights (permissions) by means
of AD-Rules and AD-Graphs. Recently, a notable approach
was proposed by Jin et al. [13], whose approach formalizes
a series of attribute-based model families. However, our approach introduces a notion of security token and AD-Rules
to capture the mapping between attributes and corresponding access rights.

6. DISCUSSION AND FUTURE WORK
Attribute Provisioning. As shown in Section 4, efficient provisioning of federated attributes is crucial for processing federated access management policies in order to resolve policies in a timely manner. The attribute provisioning
scheme presented in Section 3.5 supports this goal by reducing the number of communication messages between participating domains to determine if a given AD-Graph depicts a
path between a pair of attributes. Each participant organization should decide the number of times it will attempt to
retrieve new entries from a DHT ring when constructing a
given path. As an example, an organization may set a limit
of three explorations of the DHT ring while trying to find a
set of input attributes for AD-Rules that fall under the scope
of its local domain. Setting a low limit of explorations might
prevent participants from discovering a potential path in the
AD-Graph, however a large limit may increase attribute provisioning time, thus possibly affecting the overall processing
time of a given federated access management policy. In addition, due to the fact DHTs require participants to locally
store only a subset of all the entries included in a given
ring, our scheme allows participants to store only a subset of
AD-Rules entries, thus potentially relieving them from storing information related to the complete AD-Graph. In this
way, the process of adding and removing AD-Rules is significantly simplified, thus providing a means for modifying
a given AD-Graph to better meet the specific goals devised
for collaborations, e.g., adding new AD-Rules to handle user
credentials from a new participating domain.
Trust Model. Our current approach assumes all participants in our federation fully trust each other for the implementation of both the AD-Rules as well as the model defined
in Fig. 4. This strong assumption requires that participants
faithfully produce federated attributes by providing verified
and accurate AD-Rules and communicating those in a timely
manner. However, such an assumption may not always hold
in practice. As an example, the incorrect implementation
of a given AD-Rule may potentially compromise the overall
security of a federated environment. Future work may focus
on incorporating a trust model among participants and a
risk analysis framework such that incidents can be detected
and proper countermeasures can be deployed as a result.
Privacy. Following the fully-trusted assumption just described, a basic privacy model may be implemented on top of
our approach by allowing for sensitive information contained
in locally-defined attributes not to be revealed to other organizational peers when producing federated attributes. For
instance, in Fig. 5, sensitive information in attribute Cred ,
e.g., a user’s full name, may be replaced by a pseudonym
in the L attribute produced by the AD-Rule labeled as r 3 .
An alternative approach may allow for end-users to hide
sensitive attributes at request time by incorporating techniques such as the privacy-preserving attribute-based credentials (PABC) proposed by Camenisch et al. [3].

Policy Language and Conflict Resolution. Efficient
discovery and retrieval of policies (as shown in Section 3)
may benefit from the use of a standard policy language, in a
similar technique to the one used by the XACML role-based
access control (RBAC) Profile [22]. Moreover, a comprehensive policy specification framework is critical to detect and
resolve conflicts that may arise between federated and local
policies, or the intersection of the two, e.g., contradictory
rules, following an approach similar to the one proposed by
Hu et al. [10].
Integration with NSI. Finally, we plan to work on integrating our approach with the NSI effort presented in Section 2, in such a way that the collaborative efforts devised
by participant organizations can be better met by securily
leveraging DoE-affiliated high-performance facilities.

7.

CONCLUDING REMARKS

In this paper, we have explored the problem of implementing well-defined, consistent, and inter-organizational access
management for collaborative resource sharing. In our proposed approach and experiments, we also showed that participants could engage in a federation under a well-defined
set of responsibilities, including the use of standardized attribute definitions, attribute provisioning, and distributed
policy evaluation. We believe our approach may also be
applicable to any other collaborative settings beyond highperformance network environments, e.g. collaborative projects in the health-care domain would certainly benefit for
automated approaches that allow for information to be safely
shared between independently-run organizations, possibly
improving the patient experience and encouraging the development of groundbreaking advancements.

8.

ACKNOWLEDGEMENTS

We would like to thank the anonymous reviewers for their
valuable comments that helped improve the presentation
of this paper. This work was partially supported by the
grant from the United States Department of Energy (DESC0004308). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the
authors and do not necessarily reflect the views of the funding agency.

9.

REFERENCES

[1] R. Ananthakrishnan, J. Bryan, K. Chard, I. Foster, T. Howe,
M. Lidman, and S. Tuecke. Globus nexus: An identity, profile,
and group management platform for science gateways. In
Proceedings of 2013 IEEE International Conference on
Cluster Computing (CLUSTER), pages 1–3, Sept 2013.
[2] D. Broeder, R. Wartel, B. Jones, P. Kershaw, D. Kelsey,
S. Lüders, A. Lyall, T. Nyrönen, and H. J. Weyer. Federated
identity management for research collaborations. Technical
report, CERN, 2012.
[3] J. Camenisch, A. Lehmann, G. Neven, and A. Rial.
Privacy-preserving auditing for attribute-based credentials. In
Proceedings of European Symposium on Research in
Computer Security (ESORICS), pages 109–127, 2014.
[4] D. W. Chadwick. Federated identity management. In
Foundations of Security Analysis and Design V, pages
96–120. Springer, 2009.
[5] David W Chadwick and George Inman. Attribute aggregation
in federated identity management. IEEE Computer,
42(5):33–40, 2009.
[6] M. J. Covington and M. R. Sastry. A contextual
attribute-based access control model. In Proceedings of the
2006 International Conference on the Move to Meaningful
Internet Systems (OTM), pages 1996–2006. Springer, 2006.

View publication stats

[7] Europe’s National Research and Education Networks (NRENs).
Geánt Project Home, 2015. http://www.geant.net/.
[8] Facebook Inc. Facebook Login, 2015.
https://www.facebook.com/about/login/.
[9] Open Grid Forum. An Open Global Forum for Advanced
Distributed Computing, 2015. https://www.ogf.org/.
[10] H. Hu, Gail-J. Ahn, and K. Kulkarni. Detecting and resolving
firewall policy anomalies. IEEE Transactions on Dependable
and Secure Computing, 9(3):318–331, 2012.
[11] Jing J. and Gail-J. Ahn. Role-based access management for
ad-hoc collaborative sharing. In Proceedings of 11th
Symposium on Access Control Models and Technologies
(SACMAT), pages 200–209. ACM, 2006.
[12] Jing Jin and Gail-Joon Ahn. Authorization framework for
resource sharing in grid environments. Grid and Distributed
Computing, 63:148–155, 2009.
[13] X. Jin, R. Krishnan, and R. Sandhu. A unified attribute-based
access control model covering dac, mac and rbac. In
Proceedings of the 26th Annual IFIP WG 11.3 conference on
Data and Applications Security and Privacy (DBSec), pages
41–55. Springer, 2012.
[14] M. Jones and D. Hardt. The oauth 2.0 authorization
framework: Bearer token usage. Technical report, RFC 6750,
October, 2012.
[15] Kaffille, Sven and Loesing, Karsten. Open Chord, 2015.
http://sourceforge.net/projects/open-chord/.
[16] N. Klingenstein. Attribute aggregation and federated identity.
In Proceedings of the 2007 International Symposium on
Applications and the Internet Workshops (SAINT), pages
26–26, Jan 2007.
[17] Ninghui Li, J.C. Mitchell, and W.H. Winsborough. Design of a
role-based trust-management framework. In Proceedings of the
2002 IEEE Symposium on Security and Privacy, pages
114–130, 2002.
[18] R. L. Morgan, S. Cantor, S. Carmody, W. Hoehn, and
K. Klingenstein. Federated Security: The Shibboleth Approach.
EDUCAUSE Quarterly, 27(4):12–17, 2004.
[19] National Institute of Standards and Technology. Guide to
Attribute Based access Control (ABAC) Definition and
Considerations, 2013. NIST Special Publication 800-162 Draft.
[20] B.C. Neuman and T. Ts’o. Kerberos: an authentication service
for computer networks. Communications Magazine, IEEE,
32(9):33–38, Sept 1994.
[21] Nordic Council of Ministers. Nordic Infrastructure for Research
& Education (NORDUnet), 2015. https://www.nordu.net/.
[22] OASIS. XACML v3.0 Core and Hierarchical Role Based Access
Control (RBAC) Profile Version 1.0, 2014.
http://docs.oasis-open.org/xacml/3.0/xacml-3.
0-rbac-v1-spec-cd-03-en.html.
[23] Open Grid Forum. Network Services Interface (NSI), 2015.
https://redmine.ogf.org/projects/nsi-wg.
[24] F. Paci, R. Ferrini, A. Musci, K. Steuer, and E. Bertino. An
interoperable approach to multifactor identity verification.
IEEE Computer, 42(5):50–57, May 2009.
[25] T. Priebe, W. Dobmeier, and N. Kamprath. Supporting
attribute-based access control with ontologies. In Proceedings
of the First International Conference on Availability,
Reliability and Security (ARES), pages 465–472, Washington,
DC, USA, 2006. IEEE.
[26] D. Recordon and D. Reed. Openid 2.0: A platform for
user-centric identity management. In Proceedings of the Second
ACM Workshop on Digital Identity Management, DIM ’06,
pages 11–16, New York, NY, USA, 2006. ACM.
[27] M. S. Singhalm, S. Chandrasekhar, Ge Tingjian, R. Sandhu,
R. Krishnan, Gail-J. Ahn, and E. Bertino. Collaboration in
multi-cloud applications: Framework and security issues. IEEE
Computer, 2013.
[28] I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, and
H. Balakrishnan. Chord: A scalable peer-to-peer lookup service
for internet applications. In Proceedings of the 2001
Conference on Applications, Technologies, Architectures, and
Protocols for Computer Communications, pages 149–160, New
York, NY, USA, 2001. ACM.
[29] US Department of Energy. Energy Sciences Network (ESnet),
2015. http://www.es.net/.
[30] X. Zhang, Y. Li, and D. Nalla. An attribute-based access
matrix model. In Proceedings of the 2005 ACM symposium on
applied computing (SAC), pages 359–363, New York, NY,
USA, 2005. ACM.

Target Fragmentation in Android Apps
Patrick Mutchler
Stanford University
pcm2d@stanford.edu

Yeganeh Safaei
Arizona State University
ysafaeis@asu.edu

Abstract—Android apps declare a target version of the Android
run-time platform. When run on devices with more recent
Android versions, apps are executed in a compatibility mode
that attempts to mimic the behavior of the older target version.
This design has serious security consequences. Apps that target
outdated Android versions disable important security changes
to the Android platform. We call the problem of apps targeting
outdated Android versions the target fragmentation problem.
We analyze a dataset of 1,232,696 free Android apps collected
between May, 2012 and December, 2015 and show that the target
fragmentation problem is a serious concern across the entire app
ecosystem and has not changed considerably in several years. In
total, 93% of current apps target out-of-date platform versions
and have a mean outdatedness of 686 days; 79% of apps are
already out-of-date on the day they are uploaded to the app
store. Finally, we examine seven security related changes to the
Android platform that are disabled in apps that target outdated
platform versions and show that target fragmentation hamstrings
attempts to improve the security of Android apps.

I. I NTRODUCTION
Android has become the most popular smartphone platform
worldwide, with more than one billion active devices [1].
Android faces two major security challenges in delivering
secure code to users, fragmentation within devices and fragmentation within apps. Device fragmentation is a well known
concern [2]. Google does not control the distribution of
Android devices or Android software updates. Google instead
relies on a network of other businesses to deliver up-to-date
Android software to users, in the form of new devices or
software updates. Because of the distributed nature of this
process, a large number of Android devices are running outof-date versions of the Android platform. Critical security
patches do not reach millions of Android users, extending the
lifetime of security vulnerabilities. In comparison to device
fragmentation, fragmentation within apps has received little
attention despite carrying similar security consequences.
Android exposes numerous essential library features to apps
through the Android API. New versions of the Android platform can introduce changes to the behavior of existing library
features. For example, Android 4.4 changed the behavior of
AlarmManager.set to batch alarms set for similar times.
This change helps improve battery life at the expense of
inexact alarms. Behavioral changes such as this present a
problem for apps, which might suddenly break when a device
updates to a new Android version.
Google assigns each Android platform version an integer
called an API level. To maintain a degree of forwards compatibility and prevent apps from dramatically changing their

Adam Doupé
Arizona State University
doupe@asu.edu

John Mitchell
Stanford University
mitchell@cs.stanford.edu

behavior without warning, every app has a target API level.
Apps that are run on devices with a higher API level than
their target API level are executed in a compatibility mode that
attempts to match the behavior of devices with the target API
level as closely as possible. For example, apps that set their
target API level to 18 (Android 4.3) will use the unbatched
alarm behavior even when run on up-to-date Android devices.
Android platform changes can include important new security features that either resolve known problems with the
Android APIs or provide extra protection against attack. Any
of these features disabled by the compatibility mode will be
unavailable to apps that target outdated Android levels. This
design means that, even when running an up-to-date device, an
app that targets an outdated API level will not have access to
the most current security features. Google does not publish an
app’s target API level on the Google Play store so there is no
simple way for users to know if an app targets an outdated API
level. Instead, users are entirely at the mercy of app developers.
We call this problem of apps targeting outdated API levels the
target fragmentation problem.
The most well-known consequence of the target fragmentation problem relates to a remote code execution vulnerability
in Android WebView [3]. API level 17 added new behavior to
resolve this vulnerability, however this change is only applied
to apps that target API level 17 or higher. Years after the
vulnerability was disclosed and fixed in the Android platform,
apps can still be vulnerable, even when run on the new
platform, by targeting outdated API levels. Several studies
examining this vulnerability have reported the percentage of
apps that target levels 16 or below [4, 5]. But this vulnerability
is just one example of how the target fragmentation problem
makes Android apps less secure and no research has studied target fragmentation in its own right. A more complete
analysis of the target fragmentation problem is essential to
understanding the security of the Android ecosystem.
To the best of our knowledge, this paper reports the first
study to identify and measure the target fragmentation problem
and its security implications at a broad scale. In this paper we
study target fragmentation in five datasets of free Android apps
collected from the Google Play store over almost four years.
In total, these datasets include 1,232,696 apps. We measure
trends in the target fragmentation problem using metadata
obtained from the Google Play store. Finally, we study the
security implications of the target fragmentation problem with
respect to several concrete vulnerabilities. The major research
questions and results of our study are as follows:

What is the state of the target fragmentation problem in
the Android app ecosystem? We examine a dataset of 60,086
free apps collected from the Google Play store in December,
2015 and find that 93% of apps target out-of-date API levels.
We define a measure of “outdatedness” and find that apps, on
average, target API levels that are 686 days out-of-date.
Do developers choose to target outdated Android versions or is fragmentation caused by developers abandoning
their apps? To account for unmaintained apps, we define a
measure of “negligent outdatedness” that measures outdatedness from the date an app was uploaded to the app store and
show that target fragmentation is not just caused by stagnant
apps. We find that apps collected in December, 2015 have a
mean negligent outdatedness of 536 days.
Is target fragmentation a problem in the most popular
apps? We examine the target fragmentation problem with
respect to app popularity and conclude that target fragmentation is a serious problem even among the most popular apps.
We find that 88% of apps collected in December, 2015 and
installed more than one million times target out-of-date API
levels. These apps have a mean outdatedness of 607 days and
a mean negligent outdatedness of 493 days, only slightly lower
than the general population.
Is the target fragmentation problem becoming less
severe over time? We compare the target fragmentation results
from the December, 2015 dataset with four other datasets
collected from the Google Play store between May, 2012
and July, 2014. These datasets combined contain 1,232,696
apps. We find that, other than a growing tail of extremely outof-date apps, outdatedness distributions among the four most
recently collected datasets are very similar, suggesting that the
severity of the target fragmentation problem has not changed
considerably in several years.
What are the specific security implications of the target
fragmentation problem today? We expand the discussion of
the target fragmentation problem by examining seven security
relevant changes in the Android platform and provide the
first quantitative analysis of the broad implications of target
fragmentation on the security of the Android ecosystem.
II. BACKGROUND
Packaged with each Android app is a manifest file. The
manifest file is an XML document that contains information
about an app such as the list of application components,
requested permissions, and system events to which the app
responds [6]. The manifest contains two attributes relevant to
this study: a minimum API level (minSdkVersion) and
a target API level (targetSdkVersion)1 . The meaning
of the minimum API level is very straightforward. An app
cannot be installed on a device with an Android level below
the minimum API level. This design ensures that apps are not
installed on devices that lack essential functionality.
1 Manifests can also include a maximum API level, but since Android 2.0.1
the maximum API level is no longer used for anything other than filtering
searches on the Google Play store.

API Level
14
15
16
17
18
19
20
21
22
23

Version Code
4.0–4.0.2
4.0.3–4.0.4
4.1–4.1.2
4.2–4.2.2
4.3–4.3.1
4.4–4.4.4
4.4W–4.4W.2
5.0–5.0.2
5.1–5.1.1
6.0–6.0.1

Codename
Ice Cream Sandwich
Jelly Bean
KitKat
Lollipop
Marshmallow

Release Date
Oct, 2011
Dec, 2011
Jul, 2012
Nov, 2012
Jul, 2013
Nov, 2013
Jun, 2014
Nov, 2014
Mar, 2015
Oct, 2015

TABLE I: An abbreviated Android version history.

An app’s target API level is used to maintain forwards
compatibility with new Android platforms. If the API level
of a device is higher than an app’s target API level, the device
will enable compatibility features to match the behavior of the
target API level as closely as possible. The set of compatibility
features enabled in each API level can be found in the Android
documentation [7]. Note that apps can be safely installed on
devices running lower API levels than the target API level.
Developers can target the most current API level without
making their app incompatible with older Android devices.
If an app does not declare a target API level or if the target
API level is lower than the minimum API level, the target API
level is set to the minimum API level. For the remainder of this
paper we distinguish between the raw target API level, which
is the target level listed in the manifest file, and the target API
level, which is the target level computed using this rule and
actually used by the Android operating system. Approximately
8% of apps today do not declare a valid raw target API level.
The targetSdkVersion and minSdkVersion attributes take integer values called “API levels” that correspond
to the different Android version codes. For the remainder of
this paper we use the API level values (e.g., 17, 18, 19) rather
than the version codes (e.g., 4.2, 4.3, 4.4) when discussing
different API versions. The correspondence between recent
API levels and Android version codes is presented in Table I.
A. Security Concerns
It is not immediately obvious from the Android documentation that targeting outdated API levels can have security implications. On the documentation page for the
targetSdkVersion attribute [8], Google suggests that developers should “increase the value of [the targetSdkVersion]
attribute to match the latest API level,” but there is no mention
of the security consequences of targeting outdated API levels.
On the “Security Tips” page [9] there is no mention of target
API level. One might assume that the security consequences
of targeting outdated API levels are minimal or nonexistent.
However, there are important security changes in recent
Android versions that are not applied to apps that target
outdated API levels. For example, API levels 17 and 19 both
contain changes that prevent code injection vulnerabilities in
widely used features. Several other API levels change popular
features to have safer default behaviors, providing an extra
layer of protection. Table II lists the major security changes
to the Android platform that can be disabled by targeting

API
16
17
17
19
19
21
21

Platform Change
Access to file URLs from JavaScript is disabled by default
Content Providers are no longer exposed to foreign apps by default
Unannotated app methods are not callable from JavaScript code
isValidFragment is added to prevent Fragment Hijacking
JavaScript URLs are executed in a separate WebView context
Context.bindService no longer accepts Implicit Intents
WebView blocks mixed content by default

TABLE II: Selected security-relevant changes to the Android platform. Apps that target API levels below the listed levels do not have
the benefit of any security protection provided by these changes.

outdated Android levels. The details of these changes and
the vulnerabilities they close are discussed in Section V. If a
large number of apps target out-of-date API levels then these
changes, no matter how well intentioned, are made ineffective
and apps are put at unnecessary risk.
III. M ETHODOLOGY
Our study analyzes a dataset of 1,232,696 free apps collected from the Google Play app store between May, 2012
and January, 2016. To collect these apps we developed a
system to crawl the Google Play store to identify new apps,
scrape metadata from the Google Play store, and download
actual app files. This system was operational during five
brief time windows, naturally separating our dataset into five
smaller datasets (Datasets A, B, C, D, and E, in reverse
chronological order) that correspond to these time windows.
Table III describes the details of these datasets and lists the
most current API level at the time each dataset was collected.
A. Collecting Apps
Our system first crawls the Google Play store for apps to
download. We consider an app unique if it has a unique app
id2 . To crawl the Google Play store, we use the following four
techniques: (1) crawl the Google Play designated categories
for popular appsand collections, (2) crawl random known
developer pages to look for new apps, (3) search on the Google
Play store using words from known app descriptions, and (4)
extract all app ids from URLs on crawled pages.
Google publishes some metadata about apps on the store.
We scrape and collect metadata about each crawled app
including the date the most recent version of that app was
uploaded to the app store and the number of devices on which
the app has been installed3 .
To download apps we use a method similar to the one
described by Viennot, Garcia, and Nieh [10]. In an attempt to
efficiently collect as diverse a set of applications as possible,
we only download apps with never before seen app ids. For
each dataset except Dataset A, we attempted to download
every app with a never before seen app id identified during
2 The value of the id query parameter of the app’s Google Play URL, for
example com.instagram.android in the URL play.google.com/store/
apps/details?id=com.instagram.android. Note that this is not necessarily the
same as the app’s package name
3 Precise install counts are not available. Google Play reports a range of
possible install counts with two buckets per order of magnitude. For example,
an app might have a reported install count between 10,000 and 50,000.

Dataset
A
B
C
D
E

App Count
60,086
219,115
165,489
645,862
142,144

Crawl Date
December, 2015
June, 2014
January, 2014
July, 2013
May, 2012

Most Current API
23
19
19
17
15

TABLE III: An overview of the five datasets used in this study and
the most current API level at the time each dataset was collected.

crawling. Due to time constraints (and technical challenges),
Dataset A is only a subset of the available apps. We discuss
the effect of this collection method in Section VI-B.
B. Analysis
The Google Play store publishes each app’s minimum
API level but does not publish target API levels. We use
apktool [11], a static analysis tool that converts packaged
apps into human readable files, to extract manifests and record
their target API levels. Because our database of apps is
extremely large, it is impractical to perform complex static
analysis. All of the static analysis used in this study is
purely syntactic, which we perform by processing the smali
representation of app bytecode extracted by apktool.
IV. E VALUATION
In this section we quantify the extent of the target fragmentation problem. We begin by demonstrating that the majority
of sampled apps target outdated API levels. We define an
outdatedness metric that measures the severity of the target
fragmentation problem for individual apps as well as across
a population of apps. We show that the target fragmentation
problem is primarily caused by developer negligence rather
than apps that lie fallow on the app store. We compare our
outdatedness metric between popular and unpopular apps and
prove that target fragmentation is a problem even among the
most popular apps. Finally, we show that outdatedness curves
are similar in the four most recently collected datasets. This
result suggests that, unless the target fragmentation problem is
reexamined, it may continue with the same scale in the future.
Due to the large sizes of our datasets, we believe that these
results apply broadly to the entire Google Play ecosystem.
A. Target Fragmentation Today
Figure 1 shows the distribution of target API levels for apps
in Dataset A, the dataset containing the most current apps. It
is immediately clear that the huge majority of apps do not
target API level 23, the most current API level at the time
Dataset A was collected. More precisely, we find that 93% of
apps in Dataset A target API levels 22 or lower.
Apps that target more outdated API levels are more likely
to miss crucial security changes. We are not just interested
in if an app is outdated but also in how outdated an app
is. We define a quantitative measure called “outdatedness” as
the difference (in days) between the release date of an app’s
target API level and the release date of the most current API
level at the time of the app was collected. We find a median
outdatedness of 704 days and a mean outdatedness of 686 days

500

20

400

15

300

App Count

% Apps

25

10
5
0

200
100

0

5

10
15
Target API Version

20

Fig. 1: The distribution of target API levels in dataset A, collected two
months after the release of API level 23. API level 20 is specifically
intended for wearable devices, explaining why few apps target it.

0

0

100

200

300
400
500
Days Since App Upload

600

700

Fig. 3: The distribution of the number of days between when an
app was uploaded to the Google Play store and when that app was
collected.

If we analyze the raw target API levels in Dataset A we
find that 8.2% of apps either do not include a raw target API
level, set a value that is lower than their minimum API level, or
have an otherwise invalid raw target API level. These apps use
their minimum API level as their target API level. These apps
account for the majority of the long tail in the distribution
of target API levels. For example, 63% of apps that target
API levels 15 or lower do so because they use their minimum
API level as their target API level. This represents a major
opportunity for eliminating the long tail in the distribution
of target API levels by convincing developers to use the
targetSdkVersion attribute correctly.
B. Stale Apps
Fig. 2: The cumulative distribution of outdatedness in Dataset A. A
point at (X,Y) means that Y% of apps have an outdatedness of X
days or less. The low resolution of outdatedness values causes the
jagged nature of this curve.

for apps in Dataset A. We can use the cumulative distribution
of outdatedness (Figure 2) as a measure of the severity of
the target fragmentation problem over a population of apps.
By examining the low end of the curve we see that the large
majority of apps target outdated API levels. The long tail at
the top of the curve shows us that a considerable number of
apps target API levels that are many years out of date.
We might expect the distribution of target API levels to
resemble a skewed normal distribution, with the percentage
of apps targeting each API level decreasing as API levels get
more and more out-of-date. But instead, as seen in Figure 1,
we find that the percentage of apps targeting API levels 7
through 16 is relatively flat. Nearly as many apps targeting API
level 7 as API level 16. API level 9 sticks out in particular,
being targeted by nearly 3% of all apps in Dataset A even
though it does not offer any critical compatibility features.
Why are apps targeting such an out-of-date API level?

Not all apps on the Google Play store are regularly maintained by their developers. Figure 3 shows the distribution of
the number of days between an app’s collection and when
it was uploaded to the Google Play store by its developers.
Note that this date could either be the first time the app was
published or the date the most recent app update was pushed.
Only 38% of apps in Dataset A were uploaded after the release
of API level 23 in October, 2015. Clearly, apps that have
stagnated on the Google Play store will not target the new
API levels as they are released. The presence of unmaintained
apps can skew the target API distribution and falsely imply that
developers who actively maintain their apps fail to update their
apps to target new API levels. Here, we attempt to distinguish
outdatedness caused by unmaintained apps and outdatedness
that persists through app maintenance.
We define an app’s “negligent outdatedness” as the difference (in days) between the release date of its target API
level and the release date of the most current API level at
the time it was uploaded to the Google Play store. Negligent
outdatedness measures missed opportunities for developers to
target their apps to current API levels. Figure 4 describes a
concrete example to clarify the difference between outdatedness and negligent outdatedness. We find a median negligent

Negligent Outdatedness

100

Outdatedness

80
API 22
Released

App
Uploaded

API 23
Released

App
Collected

Fig. 4: A example app demonstrating outdatedness and negligent
outdatedness. This app targets API level 21, was uploaded after the
release of API level 22, and was collected after the release of API
level 23. Outdatedness is measured from the highest API level before
app collection. Negligent outdatedness is measured from the highest
API level before the app was uploaded.

60
% Apps

API 21
Released

40
20
0

C. Popular Apps
The large majority of apps on the Google Play store are
not downloaded by many users. Just 2% of apps in Dataset A
have been installed at least 1,000,000 times yet these apps
account for 74% of total installs among apps in Dataset A.
It is important to understand the relationship between app
popularity and the target fragmentation problem.
Figure 6 compares the cumulative outdatedness distribution
between apps of different popularities. We see that the outdatedness curves are very similar, with the most popular apps
(installed at least one million times) targeting only marginally
less out-of-date API levels. Comparing the negligent outdatedness distributions between apps of different popularities
gives similar results (and we do not include it here for space
reasons). Apps that have been installed at least one million
times have a mean outdatedness of 607 days and a mean
negligent outdatedness of 493 days.

0

500

1000
1500
Negligent Outdatedness (Days)

2000

2500

Fig. 5: The cumulative distribution of negligent outdatedness in
Dataset A using an Adoption Window of 30 days.

100
80
60
% Apps

outdatedness of 377 days and a mean negligent outdatedness
of 536 days among apps in Dataset A.
We can be even more generous with our definition of
negligent outdatedness and include some lag time to allow
developers to retarget their apps after a new API level is
released. Instead of choosing the most current API level at the
time an app was uploaded to the app store, we choose the most
current API level that has been available for at least N days at
the time an app was uploaded to the app store. We call this lag
time an “adoption window.” Adding an adoption window of 30
days only marginally affects negligent outdatedness, reducing
the median negligent outdatedness to 327 days and the mean
negligent outdatedness to 496 days.
Figure 5 shows the cumulative distribution of negligent
outdatedness using an adoption window of 30 days. Even
with a generous adoption window, apps still fail to target the
appropriate API levels. Nearly 80% of apps are negligently
targeted to outdated API levels. We note that developers have
access to new Android platforms before they are released,
so it is possible for apps to be up-to-date on day zero of a
new API level. It is clear from these results that the target
fragmentation problem cannot be explained by stale apps but
is the result of developer negligence, either due to ignorance
of the consequences of targeting out-of-date API levels or due
to a deliberate choice to target an out-of-date API level.

40
Install Count

20
0

> 1m
100k - 1m
< 100k

0

500

1000
1500
Outdatedness (Days)

2000

2500

Fig. 6: A comparison of the cumulative outdatedness distributions
of different app popularities showing that outdatedness statistics are
robust against changes in app popularity. Apps with with 1,000,000 or
more installs have an outdatedness curve that is only marginally better
than the rest of the population. Data points are linearly interpolated
to improve readability.

D. Target Fragmentation Over Time
It is clear from the analysis of Dataset A that target fragmentation is a serious problem today. Even the most popular
apps are targeting outdated, and often extremely outdated, API
levels. By analyzing datasets collected at different dates we can
see how the problem has changed over time. We repeat the
previous analyses on the four remaining datasets and compare
these results against the results from Dataset A.
Figure 7 compares the outdatedness distributions of our five
datasets. There is only one clear trend over time: a growing
tail of apps that target extremely outdated API levels, with the
90th percentile of outdatedness more than doubling between
Datasets E and A. This is a natural property of the target
fragmentation problem because the maximum outdatedness
grows over time. As long as there are apps that target the
lowest API levels we expect to see this tail continue to grow.

100

80

% Apps

60
40
Dataset

20
0

A
B
C
0

500

1000
1500
Outdatedness (Days)

2000

D
E
2500

Fig. 7: The cumulative distribution of outdatedness for all five
datasets. The four most recently collected datasets (A through D)
follow similar curves but with growing tails. Data points are lineraly
interpolated to improve readability.

Comparing the low end of the outdatedness curves does
not show an obvious pattern. We see that in each dataset
the vast majority of apps target out of date API levels and
that, excluding than Dataset E, there does not appear to be
a dramatic difference in the lower end of the outdatedness
curves. We note that Datasets A and C were each collected
two months after a platform release and Datasets B and D
were collected seven and nine months after a platform release,
respectively. This difference appears to have a greater impact
on the low end of the outdatedness curves than any pattern
over time, with more than 20% of apps in Datasets B and
D targeting current API levels and less than 10% of apps in
Datasets A and C targeting current API levels.
There is one very promising trend between our datasets. We
find a clear downward trend in the percentage of apps that do
not specify a raw target API level (Figure 8) and therefore set
their target API level to their minimum API level. Because
developers often want to support as many devices as possible,
apps generally have very low minimum API levels, making it
extremely dangerous to fail to specify a target API level. This
trend suggests that developers have become more aware of the
target API feature and that the number of developers targeting
their minimum API level will vanish over time.
V. S ECURITY I MPLICATIONS
The target fragmentation problem means that any security
change to the Android platform will be less effective, so
long as the change is disabled in compatibility mode. In
this section we explore the practical consequences of the
target fragmentation problem on seven security changes to the
Android platform. Apps targeting outdated API levels may
not necessarily be vulnerable but instead are at a heightened
risk for security vulnerabilities. In all but one case we only
show that apps are unnecessarily at a heightened risk. Because
Google has deemed the outdated behavior unsafe enough to
deserve a change to the Android platform, widespread use
of outdated behavior is troubling on its own. We also cite

% Apps Without Valid targetSdkVersion

70
80

60
50
40
30
20
10
0

05/12

07/13
01/14
06/14
Dataset Collection Date

12/15

Fig. 8: The percentage of apps in each dataset that do not include a
valid targetSdkVersion attribute in their manifest and therefore
set their target API version to be equal to their minimum API version.

research showing that the conditions necessary for these apps
to be exploitable are frequent. The statistics in this section are
computed on Dataset A, the most recently collected dataset.
A. WebView Defaults
WebView [12] is a UI element that acts as an embedded
web browser within Android apps. In Dataset A, 91% of apps
include at least one WebView instance4 . There are three major
security changes to the default behavior of WebView that
are disabled by targeting outdated API levels. The following
sections describe these changes and show that apps that target
current API levels are less likely to use the unsafe behaviors.
1) File Scheme Same Origin Policy: According to the Same
Origin Policy, JavaScript code only has access to content
loaded from the same origin. Treating all file: URLs as
belonging to the same origin is a security risk in systems with
mutually distrusting files. If a WebView loads any untrusted
content using a file: URL, then that content has full
access to every other file accessible by the app. In API level
16, the default behavior of WebView was changed to treat
all file: URLs as belonging to separate origins, however
this change only applies to apps targeting API levels 16 or
higher. Apps that wish to override this behavior and treat
all file: URLs as belonging to the same origin can use
the methods setAllowFileAccessFromFileURLs and
setAllowUniversalAccessFromFileURLs.
We find that 82% of apps that target API levels 15 or lower
and 18% of apps that target API levels 16 or higher use the
unsafe policy for file: URLs. Because apps that target API
levels 16 or higher and want to use the unsafe policy must do
so explicitly, we can use the percentage of these apps that
include calls to setAllowFileAccessFromFileURLs
or setAllowUniversalAccessFromFileURLs as an
upper bound on apps that want to use the unsafe policy. If we
4 Because we are unable to ensure that ad libraries do not contain vulnerabilities, this figure includes WebViews from ad libraries as well as primary
app functionality.

assume this percentage is uniform across both app populations
then we conclude that 64% of apps that target API level 15 or
lower unnecessarily use the unsafe policy and would become
more secure if they were retargeted to API level 16. These
apps represent 9.6% of all apps in Dataset A. We discuss the
validity of this assumption in Section VI-A.
Apps that allow unsafe file access are only at a heightened
risk. To be exploited, an app must load untrusted content
using a file: URL. Two ways an app might be exploited
are by loading file: URLs received from other apps or
by navigating to untrusted web pages, which could drop a
file on disk and redirect the WebView to that file. See Chin
and Wagner [13] for a detailed description of this exploit.
Research has shown that 23% of web browser apps unsafely
load file: URLs from foreign apps [14] and that WebView
apps frequently load untrusted web pages [13, 15, 4].
2) JavaScript URLs: Apps can load web content in a
WebView by calling loadUrl. In apps that target API levels
18 or lower, calling loadUrl on a JavaScript Pseudo-URL5
executes the script in the currently rendered web page. This
behavior is exploitable in apps that load unfiltered URLs
retrieved from foreign apps. A malicious app can send a
request to load a URL at foo.com and then send a script to be
executed in the context of foo.com to read private content.
This attack is known as Cross-Application Scripting [16].
Apps that target API levels 19 or higher load JavaScript
Psuedo-URLs in an empty context and must instead use the
method evaluateJavascript to execute JavaScript code
in the current WebView context. We find that 60% of apps
that target API levels 19 or higher contain at least one call to
this method. Again, we can use this as an upper bound on the
number of apps that intentionally execute JavaScript code in
this manner. Because 90% of apps that target API levels 18 or
lower include a WebView and enable JavaScript, we conclude
that 30% of apps that target API levels 18 or lower have no
need for evaluating JavaScript code in this manner and would
become more secure if they were retargeted to API level 19.
These apps represent 9.4% of all apps in Dataset A.
3) Mixed Content: A web page that includes web elements
retrieved over HTTP when it is loaded over HTTPS is said
to include mixed content. Loading mixed content is a security
risk, and several major browsers block mixed content [17, 18].
WebView blocks mixed content by default in apps that target API levels 21 or higher. Apps that wish to override
this behavior and allow mixed content can use the method
setMixedContentMode to specify a custom policy.
We find that 76% of apps that target API levels 20 or
lower and 42% of apps that target API levels 21 or higher
allow mixed content. If we assume that the percentage of apps
that want to allow mixed content is uniform across both app
populations then we conclude that 34% of apps that target
API levels 20 or lower unnecessarily allow mixed content and
5 A URL that starts with the protocol javascript and contains the
JavaScript code to execute.

would become more secure if they were retargeted to API level
21. These apps represent 18% of all apps in Dataset A.
B. JavaScript Interface Remote Code Execution
Android allows apps to expose app-level objects to
JavaScript code running in a WebView by using a feature
called the JavaScript Interface [19]. In 2012, a remote code execution attack on the JavaScript Interface was published [20].
Because JavaScript code has access to all of the public
methods of objects added to the JavaScript Interface, malicious
scripts could access the Java Reflection APIs by calling
getClass (a method inherited from java.lang.Object)
on the exposed object. From there, the malicious script could
build any arbitrary Java object and execute arbitrary code.
Android addressed this vulnerability in API level 17 by
forcing apps to annotate methods that should be callable
from JavaScript code. Calling an unannotated method from
JavaScript code does nothing. Because developers were unlikely to need to expose getClass to JavaScript code, this
limited the damage that a malicious script could do. But this
change is not applied for apps that target API levels 16 or
lower. Apps that use the JavaScript Interface, target API levels
16 or lower, and load untrusted web content can be exploited.
We identify apps that use the JavaScript Interface by looking
for calls to addJavascriptInterface in smali code.
50% of apps in Dataset A use the JavaScript Interface. Of these
apps, 15% target API levels 16 or lower. Any of these apps
that load untrusted JavaScript code in their WebView can be
exploited. Identifying apps that can load untrusted JavaScript
code is beyond the scope of this study so we cannot say what
portion of these apps are exploitable, but we note that existing
work has shown that it is not uncommon for apps to load
untrusted web content in their WebViews [13, 15, 4].
C. Exported Content Providers
App components that manage access to structured data are
called Content Providers [21]. Content Providers are declared
in an app’s manifest file, and they can be either made local to
the app or exposed to other apps with the exported attribute.
Unintentionally exported Content Providers that hold sensitive
data are a security flaw as they are accessible to every app on
the device. Yet if the exported attribute is not specified it
falls back to a default value. API level 17 changed the default
value to false but apps that target API levels 16 or below
use a default value of true. Research has shown that 65%
of apps that export a Content Provider leak private data [22].
9.7% of apps that target API levels 16 or lower and 8.0%
of apps that target API levels 17 or higher include at least one
exported Content Provider. 4.9% of apps that target API levels
16 or lower include a Content Provider that is exported due
to default behavior. Assuming that the percentage of apps that
want to export a Content Provider is uniform across both app
populations we conclude that 1.7% of apps that target API
levels 16 or lower unnecessarily export a Content Provider.
These apps represent 0.3% of apps in Dataset A.

D. Fragment Injection
In 2013, security researchers identified a vulnerability in
the PreferenceActivity class [23]. Malicious apps can
send crafted messages to exported classes that inherit from
PreferenceActivity. The messages are interpreted as
Fragment instances and loaded dynamically using the Reflection APIs, executing arbitrary code from the malicious app.
API level 19 added the method isValidFragment to
PreferenceActivity to close this vulnerability. Developers are expected to use this method to check the package
name of injected fragments and reject unauthorized fragments.
Apps that target API levels 19 or higher inherit an implementation of isValidFragment that always raises an exception
and are therefore safe by default, but apps that target API levels
18 or lower inherit an implementation that always returns
true, which offers no protection against this attack.
We find that 1.7% of apps that target API levels 18 or
lower contain at least one exported class that inherits from
PreferenceActivity and does not override the unsafe
implementation of isValidFragment6 . Unlike previous
examples, this is sufficient information to prove that these
apps are exploitable rather than just at heightened risk. The
exploitable apps account for 0.5% of all apps in Dataset A.

to retarget their apps. It is essential to give users access to this
information. To this end, we created an open-source tool [26]
that reports the target API level of each app on a user’s phone
and notifies users of any security implications.
VI. D ISCUSSION

A Service is an app component that performs operations
without user interaction. UI components can interact with
a Service using the method bindService. Apps specify
which Service to interact with by using an Intent [24]. Intents
can be explicit or implicit. Explicit Intents list a unique Service
using its class name. Implicit Intents only specify a general action to perform and the system chooses an appropriate Service
to handle the request. Communicating with a Service using
an Implicit Intent is not safe. Because Services are not user
facing components, users have no control over which Service
responds to an Implicit Intent. If multiple Services match the
Implicit Intent used in bindService then a random one of
those Services is chosen. Malicious apps can create a Service
that matches the Implicit Intent and impersonate trusted code.
In apps that target API levels 21 or higher, passing an Implicit
Intent to bindService throws a security exception.
83% of apps that target API levels 20 or lower contain at
least one call to bindService. These apps make up 43%
of all apps in Dataset A. Statically identifying which of these
apps use Implicit Intents to bind Services is nontrivial and
beyond the scope of this study. However, prior research [25]
has found that 19% of apps are potentially vulnerable to
Service Hijacking because they use Implicit Intents.

The data presented in this study makes two conclusions
clear: that target fragmentation exists across the entire Android ecosystem and that target fragmentation has practical
implications on Android security. Making new Android versions compatible with un-updated apps ensures that apps do
not suddenly break, but distributing security changes in this
manner has clear limitations. This approach makes security
changes optional and mixes security changes with non-security
changes. Developers cannot pick-and-choose just the security
changes but must integrate all of the platform changes from a
new API level. Developers could try to sidestep this problem
by setting a maximum API level but this feature is not
enforced and would exacerbate device fragmentation problem
by discouraging platform updates.
With this in mind, we consider the alternative to the current
system: enforcing all security changes to the Android platform
regardless of target API level. Nearly four months since the
release of API level 23, less than 1% of active devices have
updated to version 23 [27]. This slow update process means
that developers have ample time to update their apps to work
with new behavior. There have also been security changes in
the past that were both mandatory and breaking that did not
appear to cause great pain. In Android level 21, a uniqueness
requirement was added for custom permissions to ensure
that malicious apps could not access protected content. This
change was mandatory and could prevent apps from being
reinstalled after a device update but searching on developer
forums reveals few complaints. We suspect that if developers
were simply forced to adjust to all security changes that it
would not be a problem for the majority of apps. If this is
not feasible then, at the very least, users should be informed
if their apps target out-of-date API levels so that they can be
empowered to make security conscious decisions.
The JavaScript Interface vulnerability described in Section V-B is a good case study to support our argument. After
failing to fix the problem in API level 17 with an optional
change, API level 19 banned all access to getClass from
the JavaScript Interface. This change is not made optional for
apps targeting lower API levels and would be unneeded if not
for the existence of apps that target API levels 16 or lower.
The vulnerability was only truly addressed with a change that
is applied to all apps, regardless of their target API version.

F. Detecting Outdated Apps

A. Alternative Explanations

Google Play does not publish an app’s target API level.
Security conscious users are not empowered to make decisions
based on target API levels and must instead trust developers

Ignorance is not the only reason why a developer might
not target the most current API level. One alternative is that
developers do not retarget their apps to the most current API
level because few devices run the most current API level. If
this were the case we would expect most apps to be no more
than one API level out of date. Although 16% of devices were

E. Service Hijacking

6 We assume that all implementations of isValidFragment correctly
validate the package name of injected fragments. Future work could expand on
our results by looking for incorrect implementations of isValidFragment.

running API levels 22 or 23 when Dataset A was collected,
only 23% of apps in Dataset A targeted these levels.
Another possibility is that developers choose not to retarget
apps to higher API levels unless there is a security concern.
Because API levels 22 and 23 do not include security changes,
developers may have chosen not to retarget their apps. However, we show in Section V that numerous apps target outdated
API levels even though they miss relevant security changes.
54% of apps target API levels below 21. If we look at the
Fragment Injection vulnerability we find that apps that use a
PreferenceActivity are not more likely to target API
levels 19 or higher (66%) than the rest of the app population
(69%). This suggests that developers, as a whole, do not
consider security when deciding what API level to target.
A final option is that developers choose not to target
the most current API level to avoid breaking critical app
functionality. This is a real possibility but, if true, shows that
Google’s “all or nothing” design is flawed because it forces
developers to make an impossible choice and sacrifice security.
B. Threats to Validity
We only downloaded a subset of available apps for Dataset
A so there is some possibility of selection bias in our results.
However, a dataset size of 60,086 apps is within the normal
range for studies like ours. Because these apps were selected
randomly from the available apps we believe that the dataset
is large enough to smooth out any selection bias.
Our dataset is not a uniform snapshot of the apps available
on the Google Play store because we do not download updated
versions of previously downloaded apps. We do not have
longitudinal statistics on individual apps and we tend to have
young versions of apps. If apps that have been present on
the app store for a very long time are considerably more
likely to target current API levels then the statistics presented
in this paper may overestimate the problem from the user’s
perspective. However, there is an 18 month gap between the
collection of Dataset A and Dataset B. If long-lived apps target
outdated API levels less frequently then we would expect to
see some indication in the results for Dataset A.
Because we only study free apps, it is possible that our
results do not apply to non-free apps. We have no reason to
suspect that development practices are significantly different
for non-free apps. However, even if there is a considerable
difference between free and non-free apps, free apps comprise
89% of apps on the Google Play store [28] so any problem
present in free apps is critical and should be addressed.
Much of the analysis done in Section V assumes that apps
use certain dangerous features at uniform rates no matter
which API level they target. If this assumption is not true
then our conclusion that many apps unnecessarily use these
dangerous features because they target outdated API levels
might not be valid. In particular, it is possible that behavioral
changes discourage developers from retargeting their apps,
and we cannot assume that usage of dangerous features is
uniform across the app population. However, the differences
we observe in the usage of dangerous features are so great

that it would be extremely surprising for these differences to
be caused by different intended behavior in apps.
VII. R ELATED W ORK
The most similar work to this study is by McDonnell, Ray,
and Kim [29], who study the use of deprecated API methods
and the adoption of updated API methods in ten open source
Android apps. Unlike our study, which focuses on target API
levels, they focus on the usage of methods changed in new
API levels. Targeting an API level is not dependent on using
methods added in that platform version and apps should target
the most current API level even if they do not use any newly
added methods. Their results say nothing about the security
consequences of outdated apps but do show that developers
are slow to adapt to the changing Android platform.
The target fragmentation problem has been discussed in
relation to specific vulnerabilities in several studies. Thomas
et al. [5] study the changes in Android 17 that closed the
JavaScript Interface vulnerability in depth. Their study focuses
on the slow adoption of new devices and its effect on the
lifetime of the vulnerability but they also find that 22% of
studied apps use the JavaScript Interface and targeted API
levels below 17. Mutchler et al. [4] identify apps that load
untrusted content in WebView and note that the JavaScript
Interface vulnerability puts these apps at risk.
The vulnerabilities mentioned in this paper have been studied without mention of target fragmentation. Lu et al. [30]
build a static analyzer to identify vulnerabilities including
Service Hijacking. Georgiev, Jana, and Shmatikov [15] provide
a tool to prevent attacks through the JavaScript Interface. Chin
and Wagner [13] statically analyze apps and find unsafe use
of file: URLs. Jin et al. [31] build a tool to detect a variety
of XSS-like vulnerabilities in WebView apps.
Because both apps and devices must be updated in order to
take advantage of new security features, target fragmentation
and device fragmentation are linked. Thomas, Beresford, and
Rice [2] study device fragmentation using volunteers who
install their device monitor app. They find that 88% of devices
are vulnerable to at least one of selected vulnerabilities and
that devices are updated infrequently (1.26 times per year on
average). Zhou et al. [32] find that more than 1,000 of 2,423
factory images can be exploited through misconfigurations of
device drivers. Xing et al. [33] identify how apps can exploit
the OS update process to obtain sensitive system permissions.
Mulliner et al. [34] provide a scalable method for applying
third party patches to vulnerable Android devices.
The app update process has also been studied. Möller et
al. [35] investigate the update patterns of Android users and
find that only half of users install an app update within
one week of the update being published. McIlroy, Ali, and
Hassan [36] mine update data from 10,713 apps and find that
only 1% of apps receive at least one update per week.
Several other studies analyze large datasets of Android apps.
Viennot, Garcia, and Nieh [10] crawl, download, and analyze
1,100,000 apps to obtain statistics about permission and library
distributions as well as identify apps that unsafely embed

credentials. Other studies [37, 38] also study permission and
library usage. Kavaler et al. [39] compare usage of Android
classes and questions asked on StackOverflow. Finally, several
businesses like AppBrain [28] maintain statistics on the app
metadata published on the Google Play Store.
VIII. C ONCLUSION
Android apps specify a target API level and run in a
compatibility mode on devices with higher API levels. The
compatibility mode can disable important security changes in
the Android platform. We call the problem of apps targeting
outdated API levels the target fragmentation problem. In this
study we analyze a dataset of more than one million Android
apps collected over four years and show that the large majority
of collected apps target outdated API levels. We examine
the practical implications of target fragmentation on seven
security changes to the Android platform and show that target
fragmentation hamstrings new security features.
We believe that applying security changes in this optional
manner is a flawed approach that sacrifices security at the
altar of compatibility. Developers become a new obstacle
to securing apps and users have no means of ensuring that
their apps target the most current API levels. The target
fragmentation problem is further compounded by the coupling
of security changes and non-security changes. We hope that by
shedding light on this problem, developers can become more
aware of the consequences of targeting outdated API levels
and this flawed design can be reexamined and changed so that
there is less opportunity for Android apps to operate without
access to important security features.
R EFERENCES
[1] I.
Lunden.
Android
breaks
1b
mark
for
2014,
81% of all 1.3b smartphones shipped. [Online]. Available:
http://techcrunch.com/2015/01/29/android-breaks-1b-mark-for2014-81-of-the-1-3b-smartphones-shipped-in-total
[2] D. R. Thomas, A. R. Beresford, and A. Rice, “Security metrics for
the android ecosystem,” in Proceedings of the 5th ACM Workshop on
Security and Privacy in Smartphones and Mobile Devices, 2015.
[3] Webview addjavascriptinterface remote code execution. [Online].
Available: http://labs.mwrinfosecurity.com/blog/2013/09/24/webviewaddjavascriptinterface-remote-code-execution
[4] P. Mutchler, A. Doupé, J. Mitchell, C. Kruegel, and G. Vigna, “A largescale study of mobile web app security,” in Mobile Security Techologies,
2015.
[5] D. R. Thomas, A. R. Beresford, T. Coudray, T. Sutcliffe, and A. Taylor,
“The lifetime of android api vulnerabilities: Case study on the javascriptto-java interface,” Security Protocols XXII, 2015.
[6] App manifest. [Online]. Available: http://developer.android.com/guide/
topics/manifest/manifest-intro.html
[7] Build.version codes. [Online]. Available: http://developer.android.com/
reference/android/os/Build.VERSION CODES.html
[8] <uses-sdk>. [Online]. Available: http://developer.android.com/guide/
topics/manifest/uses-sdk-element.html
[9] Security tips. [Online]. Available: http://developer.android.com/training/
articles/security-tips.html
[10] N. Viennot, E. Garcia, and J. Nieh, “A measurement study of google
play,” in Proceedings of the 2014 ACM Conference on Measurement and
Modeling of Computer Systems, 2014.
[11] Apktool. [Online]. Available: http://code.google.com/p/android-apktool
[12] Webview. [Online]. Available: http://developer.android.com/reference/
android/webkit/WebView.html
[13] E. Chin and D. Wagner, “Bifocals: Analyzing webview vulnerabilities
in android applications,” in Information Security Applications, 2014.

[14] D. Wu and R. K. Chang, “Analyzing android browser apps for
file://vulnerabilities,” in Information Security, 2014.
[15] M. Georgiev, S. Jana, and V. Shmatikov, “Breaking and fixing originbased access control in hybrid web/mobile application frameworks,”
in Proceedings of the 2014 Network and Distributed System Security
Symposium, 2014.
[16] M. Backes, S. Gerling, and P. von Styp-Rekowsky, “A local cross-site
scripting attack against android phones,” 2011. [Online]. Available:
http://infsec.cs.uni-saarland.de/projects/android-vuln/android xss.pdf
[17] What
is
mixed
content?
[Online].
Available: http://developers.google.com/web/fundamentals/security/preventmixed-content/what-is-mixed-content
[18] Mixed content blocking in firefox. [Online]. Available: http://support.
mozilla.org/en-US/kb/mixed-content-blocking-firefox
[19] Binding javascript code to android code. [Online]. Available: http://
developer.android.com/guide/webapps/webview.html#BindingJavaScript
[20] N. Bergman. Abusing webview javascript bridges. [Online]. Available:
http://50.56.33.56/blog/?p=314
[21] <provider>. [Online]. Available: http://developer.android.com/guide/
topics/manifest/provider-element.html
[22] Y. Z. X. Jiang, “Detecting passive content leaks and pollution in android
applications,” in Proceedings of the 20th Network and Distributed
System Security Symposium, 2013.
[23] R. Hay. A new vulnerability in the android framework: Fragment
injection. [Online]. Available: http://securityintelligence.com/newvulnerability-android-framework-fragment-injection
[24] Intents and intent filters. [Online]. Available: http://developer.android.
com/guide/components/intents-filters.html
[25] E. Chin, A. P. Felt, K. Greenwood, and D. Wagner, “Analyzing interapplication communication in android,” in Proceedings of the 9th
international conference on Mobile systems, applications, and services,
2011.
[26] Api parser. [Online]. Available: http://github.com/yeganehs/API-Parser
[27] Dashboards. [Online]. Available: http://developer.android.com/about/
dashboards/index.html
[28] Google Play Stats. [Online]. Available: http://appbrain.com/stats
[29] T. McDonnell, B. Ray, and M. Kim, “An empirical study of api stability
and adoption in the android ecosystem,” in Proceedings of the 29th IEEE
Conference on Software Maintenance, 2013.
[30] L. Lu, Z. Li, Z. Wu, W. Lee, and G. Jiang, “Chex: statically vetting
android apps for component hijacking vulnerabilities,” in Proceedings
of the 2012 ACM conference on Computer and communications security,
2012.
[31] X. Jin, X. Hu, K. Ying, W. Du, H. Yin, and G. N. Peri, “Code injection
attacks on html5-based mobile apps: Characterization, detection and
mitigation,” in Proceedings of the 2014 ACM SIGSAC Conference on
Computer and Communications Security, 2014.
[32] X. Zhou, Y. Lee, N. Zhang, M. Naveed, and X. Wang, “The peril of fragmentation: Security hazards in android device driver customizations,” in
Proceedings of the 2014 IEEE Symposium on Security and Privacy,
2014.
[33] L. Xing, X. Pan, R. Wang, K. Yuan, and X. Wang, “Upgrading your
android, elevating my malware: Privilege escalation through mobile os
updating,” in Proceedings of the 2014 IEEE Symposium on Security and
Privacy, 2014.
[34] C. Mulliner, J. Oberheide, W. Robertson, and E. Kirda, “Patchdroid:
Scalable third-party security patches for android devices,” in Proceedings of the 29th Annual Computer Security Applications Conference,
2013.
[35] A. Möller, F. Michahelles, S. Diewald, L. Roalter, and M. Kranz,
“Update behavior in app markets and security implications: A case study
in google play,” in Proceedings of the 3rd Workshop on Research in the
Large., 2012.
[36] “Fresh apps: An empirical study of frequently-updated mobile apps in
the google play store.”
[37] M. L. Dering and P. McDaniel, “Android market reconstruction and
analysis,” in Proceedings of the 2014 IEEE Military Communications
Conference, 2014.
[38] R. Johnson, Z. Wang, C. Gagnon, and A. Stavrou, “Analysis of android
applications’ permissions,” in Proceedings of the 2012 IEEE Conference
on Software Security and Reliability Companion, 2012.
[39] D. Kavaler, D. Posnett, C. Gibler, H. Chen, P. Devanbu, and V. Filkov,
“Using and asking: Apis used in the android market and asked about in
stackoverflow,” in Social Informatics, 2013.

Behind Closed Doors: Measurement and Analysis
of CryptoLocker Ransoms in Bitcoin
Kevin Liao, Ziming Zhao, Adam Doupé, and Gail-Joon Ahn
Arizona State University
{kevinliao, zmzhao, doupe, gahn}@asu.edu
called bitcoins1 , to other addresses by issuing transactions,
which are then broadcast to the public blockchain.
Since all confirmed transactions are visible to the public, the
blockchain’s inherent transparency has proven to be ineffective
in preserving the anonymity of its users (legitimate users
and cybercriminals alike). While Bitcoin addresses alone are
not explicitly tied to any real-world entities, a number of
recent research efforts have shown that monetary movements
and address links can be traced throughout the blockchain
data structure [3]–[8]. Even though there have been many
attempts to enhance user privacy with varying degrees of
success (i.e. generating multiple addresses, using bitcoin mixers
such as Bitcoin Fog [8], or using privacy-enhancing overlays
such as Coinjoin [9]), user privacy is further undermined
when real-world information and quasi-identifiers found on
the Internet can be imputed to users’ Bitcoin addresses. Given
Bitcoin’s meteoric rise in popularity and scale, such a condition
was inevitable and the overlap between publicly available
data and blockchain data has improved identification and
attribution throughout a vast, connected network of users—
there are addresses tied to forum usernames, anonymous online
marketplaces, Bitcoin exchanges, and popular Bitcoin services.
Privacy-preserving online services, such as the Tor hidden
network [10] and the Bitcoin system, while undoubtedly
useful in many aspects, play nontrivial roles in the burgeoning
I. I NTRODUCTION
cybercrime landscape. The fact remains that an elegant solution
for distinguishing legitimate and illicit use of these services
Increasingly, Bitcoin [1] is becoming a staple utility among is far from reach since the goals of Tor, Bitcoin, and the like
cybercriminals [2]—two of the digital currency’s main attrac- are to protect privacy en masse. While Bitcoin does enable
tions are its provisions for pseudoanonymity and its irreversible criminal enterprises to better obfuscate money laundering
transaction protocol. Unfortunately, these provisions engender schemes compared to traditional financial systems, we have
the dichotomous incentives between legitimate users, who wish seen that digital footprints embedded in the Bitcoin blockchain
to transfer money efficiently and securely, and cybercriminals, can reveal salient information about its users. Given the recent
who leverage these properties to commit irrevocable and prevalence of CryptoLocker [11] — a family of ransomware
supposedly untraceable financial fraud.
that encrypts files on a victim’s system and demands a ransom
Although the notion of digital currencies has existed long to be paid (through MoneyPak or Bitcoin) for the decryption
before Bitcoin’s debut, Bitcoin was proposed by an individual key — from September 2013 through early 2014, we use this as
under the pseudonym Satoshi Nakamoto in 2008. Nakamoto an opportunity to better understand the mechanics of a digital
introduced a distributed public ledger that serializes a record money laundering economy and to generate threat intelligence
of all confirmed transactions known as the blockchain. A fun- on brazen cybercrimes. More generally, we aim to answer the
damental breakthrough in technology, the blockchain enables who, why, and how behind CryptoLocker in hopes that our
the Bitcoin system to operate under a decentralized peer-to- findings may be extendable to future cybercrime forensics and
peer network where users are identifiable by public keys, or
more commonly referred to as Bitcoin addresses, intended
1 The Bitcoin system and peer-to-peer network are referred to as “Bitcoin”
to provide pseudonymity. Users can transfer digital currency, while the unit of currency is referred to as “bitcoin” or abbreviated as “BTC”.

Abstract—Bitcoin, a decentralized cryptographic currency that
has experienced proliferating popularity over the past few years,
is the common denominator in a wide variety of cybercrime.
We perform a measurement analysis of CryptoLocker, a family
of ransomware that encrypts a victim’s files until a ransom
is paid, within the Bitcoin ecosystem from September 5, 2013
through January 31, 2014. Using information collected from
online fora, such as reddit and BitcoinTalk, as an initial starting
point, we generate a cluster of 968 Bitcoin addresses belonging
to CryptoLocker. We provide a lower bound for CryptoLocker’s
economy in Bitcoin and identify 795 ransom payments totalling
1,128.40 BTC ($310,472.38), but show that the proceeds could
have been worth upwards of $1.1 million at peak valuation. By
analyzing ransom payment timestamps both longitudinally across
CryptoLocker’s operating period and transversely across times
of day, we detect changes in distributions and form conjectures
on CryptoLocker that corroborate information from previous
efforts. Additionally, we construct a network topology to detail
CryptoLocker’s financial infrastructure and obtain auxiliary information on the CryptoLocker operation. Most notably, we find
evidence that suggests connections to popular Bitcoin services,
such as Bitcoin Fog and BTC-e, and subtle links to other
cybercrimes surrounding Bitcoin, such as the Sheep Marketplace
scam of 2013. We use our study to underscore the value of
measurement analyses and threat intelligence in understanding
the erratic cybercrime landscape.
Index Terms—Bitcoin, CryptoLocker, cybercrime, ransomware, security.

analytics. Our contributions are the following:
• We design and implement a framework that collects data
from the blockchain and automatically identifies ransom
payments to Bitcoin addresses belonging to CryptoLocker.
From this, we measure CryptoLocker’s economy in Bitcoin
and provide a lower-bound estimate of financial damages
from September 5, 2013 through January 31, 2014.
• We present a novel approach to analyzing Bitcoin transactions by examining ransom payment timestamps both
longitudinally across CryptoLocker’s operating period, as
well as tranversely across times of day, to distinguish
trends and changes in timestamp distributions.
• We construct a non-trivial, topological network of CryptoLocker addresses and systematically examine CryptoLocker’s financial infrastructure and money laundering
strategies. By leveraging external, real-world data, we
find connections to popular services such as Bitcoin Fog
and BTC-e, and speculate connections to other Bitcoin
cybercrime, such as the Sheep Marketplace heist.
II. BACKGROUND
To understand our analysis of the CryptoLocker economy,
we first discuss the Bitcoin protocol in Section II-A, and we
next discuss the CryptoLocker ransomware in Section II-B.
A. Anatomy of a Bitcoin Transaction
Bitcoin is a decentralized cryptographic currency that was
proposed by Satoshi Nakamoto in 2008 [1]. A bitcoin can
be abstracted as a chain of transactions among owners who
are identifiable by public keys generated from an asymmetric
encryption scheme2 . We will refer to these public keys as
Bitcoin addresses, or simply addresses, throughout the rest of
this paper.
To transfer bitcoins, a user issues a transaction, which
consists of a set of inputs, a set of outputs, and a change
address. The inputs are Bitcoin addresses that belong to the
payer. The outputs are Bitcoin addresses that identify the payee
accounts, and the change address (which is optional) is where
the leftover bitcoins from the transactions are sent (the change
address belongs to the payer). Bitcoin’s transaction protocol
stipulates that inputs to a new transaction must reference the
exact value of outputs from previous transactions. In other
words, users must specify from whom they received the bitcoins,
thus forming a chain of transactions, and inputs to a new
transaction may reference as many previous transactions needed
to sufficiently fund the new output. These are known as multiinput transactions. Finally, the payer digitally signs a hash3 of
the transaction from which he or she received the bitcoins and
the public key address of the payee.
We illustrate Bitcoin’s transaction protocol in Figure 1. We
see that there are three transactions, A, B, and C, and we will
refer to their respective owners as Alice, Bob, and Charlie. In
transaction A, a transaction of 2.0 BTC sent to Alice is used as
2 Elliptic

Curve Digital Signature Algorithm (ECDSA)

3 SHA-256

Fig. 1. The diagram illustrates the anatomy of bitcoin transactions. We have
transactions A, B, and C owned by Alice, Bob, and Charlie, respectively. We
can see that previous transactions to Alice and Bob are referenced in their
respective transactions to Charlie, forming a chain of transactions.

an input to a transaction of 1.5 BTC to Charlie. Since the input
value exceeds the output value, the remaining 0.5 BTC is sent to
a change address belonging to Alice. In transaction B, we see a
similar scenario in which Bob transfers 1.0 BTC to Charlie, but
a change address is not necessary because the input and output
values were equivalent. In transaction C, we see that Charlie
transfers 2.0 BTC to an unnamed user. To issue this transaction,
transaction C references the two previous transactions, from
Alice and Bob, that Charlie received as inputs. Again, the total
input value exceeds the required output value, so 0.5 BTC are
sent to a change address belonging to Charlie.
We can see that the validity of a bitcoin is dependent on
the correctness of each signature in the transaction chain, it is
simple to verify a transaction’s history but difficult to tamper
with confirmed transactions that are deeply embedded in the
blockchain (usually six blocks of confirmation is considered
secure against double-spending attacks [12]). Therefore, Bitcoin
transactions are essentially irreversible. This feature, coupled
with Bitcoin’s pseudonymity, enables cybercriminals to commit
financial fraud that is virtually impossible to reverse and
difficult to trace.
B. CryptoLocker Ransomware
On September 5, 2013, CryptoLocker emerged as a new
family of ransomware that encrypted files on a victim’s system
until a ransom is paid [11]. The decryption keys were withheld
by the threat actors who demanded ransoms to be paid either
through MoneyPak or Bitcoin within 72 hours, otherwise the
decryption keys would (allegedly) be destroyed and recovery
of the encrypted files would be virtually impossible.
CryptoLocker’s infection vector took two forms. In its initial
release, CryptoLocker threat actors primarily targeted business
professionals via spam emails taking the form of “customer
complaints” against recipients’ organizations. Malicious executable files were attached in ZIP archives, which would
aggressively encrypt all the files on a system if opened. Later
versions of CryptoLocker, beginning on October 7, 2013, were
distributed through Gameover ZeuS [13], a peer-to-peer botnet
that used Cutwail spam botnet to send massive amounts of
spam email impersonating well-established online retailers and

III. M EASUREMENT M ETHODOLOGY
We next explain our approach to collecting information
from the blockchain and various online fora in an effort
to measure CryptoLocker’s economy and generate threat
intelligence on the CryptoLocker operation. We begin by
highlighting how we generated an address cluster belonging to
CryptoLocker, which we call SCL , from two seed addresses in
Section III-A. Based on information from previous efforts and a
preliminary examination of SCL , we designed and implemented
a framework for identifying ransom payments from the set of
all transactions sent to our cluster in Section III-B.

900

USD per BTC

financial institutions. These emails typically spoofed invoices,
order confirmations, or urgent unpaid balances to lure victims
into following malicious links which redirected to CryptoLocker
exploit kits.
From September 2013 through early 2014, CryptoLocker
infections were most prevalent in the United States. A study
from the Dell SecureWorks Counter Threat Unit (CTU) research
team [11] shows that from October 22, 2013 to November
1, 2013, 22,360 systems were infected in the United States,
constituting 70.2% of global CryptoLocker infections. During
this period, CryptoLocker was also prevalent in Canada, Great
Britain, India, and several countries in the Middle East and
South Central Asia. In a later sample, gathered from December
9, 2013 to December 16, 2013, during a sinkhole when CryptoLocker activity was limited, CryptoLocker infections became
more dispersed. The concentration of infected systems in the
United States dramatically declined to 23.8% (1,540 infected
systems) and CryptoLocker activity became more prevalent in
Great Britain (1,228 infected systems constituting 19.0% of
global infections), Australia (836 infections constituting 12.9%
of global infections), several other European countries, China,
India, and Brazil.
Although the United States is disproportionately represented
in total global CryptoLocker infections, most ransom payments
from the United States were issued through MoneyPak, an
invariably more economical option than Bitcoin. This was due
to bitcoin’s price volatility at the time. Conversion rates soared
from $120/BTC in September 2013 to well over $1,300/BTC in
late November 2013. We show the exchange rate of US dollars
and bitcoin throughout CryptoLocker’s operational period in
Figure 2. As a result, the CryptoLocker threat actors adjusted
the ransom demand on several occasions to ensure that ransom
demands were not exorbitant. Since, in almost all cases in the
United States, there were no advantages to paying through
the Bitcoin system, individuals who elected to pay via Bitcoin
presumably resided in countries outside of the United States
where MoneyPak was unavailable [11]. For this reason, bitcoin
ransom payments represent only a small portion of the total
financial damages caused by CryptoLocker.

600

300

Sep 2013

Oct 2013

Nov 2013

Dec 2013

Date

Jan 2014

Feb 2014

Fig. 2. The plot shows exchange rates between US dollars and bitcoins during
CryptoLocker’s operational period from September 2013 through January
2014.

for forensic analysis of the blockchain. Initially, we found two
known CryptoLocker addresses by manually investigating a
reddit thread4 in which victims and researchers posted Bitcoin
addresses belonging to the ransomware. We will refer to one
of these seed addresses, which collected 27 ransom payments,
as Aseed1 , and the other seed address, which collected 23
ransom payments, as Aseed2 , throughout the remainder of this
paper. An interested reader can find the hashes of all Bitcoin
addresses mentioned in the Appendix. To expand our dataset of
addresses, we use the following two clustering heuristics (based
on the Bitcoin transaction protocol detailed in Section II-A)
to generate a set of Bitcoin addresses controlled by the same
user(s), known as clusters [4]:
1) Multi-input Transactions: A multi-input transaction occurs when a user u makes a transaction in which
the payment amount p exceeds the available bitcoins
(references of prior payments to u) in u’s wallet5 . In
such a case, the Bitcoin protocol inputs a set of bitcoins
B from u’s wallet to sufficiently fund p. Therefore, we
can conclude that if the bitcoins in B are owned by a
set of distinct input addresses Si , the input addresses in
Si are controlled by the same user u.
2) Change Addresses: The Bitcoin protocol generates a new
change address in u’s wallet to collect change when the
sum of inputs in B exceeds p. When the set of output
addresses So contains two addresses such that one is a
newly generated address an and the other corresponds
to a payment’s destination address ad , we can conclude
that an is a change address and is controlled by u.
From Aseed1 , we generate a set of 968 Bitcoin addresses
belonging to the CryptoLocker cluster, SCL , which happens
to include Aseed2 . Although our study does not claim to be
representative of the entire CryptoLocker population within the
Bitcoin ecosystem, which is difficult to quantify due to a lack

A. Collecting and Generating Addresses
To collect Bitcoin addresses belonging to CryptoLocker,
we used an approach similar to M. Spagnuolo’s study on
CryptoLocker using BitIodine [7], an open source framework

4 https://www.reddit.com/r/Bitcoin/comments/1o53hl/disturbing bitcoin
virus encrypts instead of/
5 A “wallet” is a collection of private keys. It is the Bitcoin equivalent of a
bank account.

of confirmed Bitcoin addresses belonging to CryptoLocker,
we can systematically measure a subset of CryptoLocker’s
economy, SCL , and make inferences about CryptoLocker and
its constituents.
B. Ransom Identification Framework

IV. DATA OVERVIEW
We begin by validating the accuracy of our data and showing
how conservative our estimates are in Section IV-A. We then
perform valuations of the CryptoLocker economy and measure
ransom payments at different stages in its operational period
in Section IV-B.

The goal of our ransom identification framework is to distinguish ransom payments from the set of all transactions to SCL . A. Data Validation
Foremost, we designed and implemented our identification
We realize that it is difficult to both comprehensively
framework to be precise — we wanted to identify ransom measure SCL ’s economy and precisely identify all ransom
payments with a high degree of confidence and minimize the payments to SCL beyond a reasonable doubt. In turn, the
number of extraneous transactions included in our dataset. stringent transaction value and timestamp parameters used in
Second, we built our framework to be lightweight — rather our ransom identification framework provide a lower bound
than querying the entire Bitcoin blockchain, we chose a semi- estimate of ransom payments and SCL ’s economy. To gauge
automatic approach to crawl and parse transactions to SCL how conservative our previous estimates are using the aforemenusing the Blockchain API [14]. For each address in SCL , points tioned framework, we take three different measurements with
of interest included the total number of transactions, total sent varying transaction value and timestamp parameters. The three
and received bitcoins, and the number of ransom payments measurement methods are as follows: Method 1) transactions to
received. For each transaction, we were interested in input SCL without any transaction value or timestamp filters, Method
and output addresses, bitcoins transferred, and timestamps (in 2) transactions to SCL filtered only by transaction values, and
UNIX epoch time).
Method 3) transactions to SCL filtered by both transaction
The time and ransom parameters in our identification frame- values and timestamps (ransom identification framework).
work reflected findings from previous studies on CryptoLocker
ransomware [7], [11] and our own cursory analysis of SCL .
TABLE I
M EASUREMENTS BY M ETHOD
The heuristics we use are as follows:
• Payments of approximately 2 BTC (±0.1 BTC) between
Method
Transactions
BTC
USD
September 5, 20136 (CryptoLocker release date) and
Method 1
1,071
1,541.39
539,080.69
November 11, 2013 to allow for a three-day ransom period
Method 2
933
1,257.27
373,934.76
after CryptoLocker authors decreased the ransom amount
Method
3
795
1,128.40
310,472.38
to 1 BTC around November 8
• Payments of approximately 1 BTC (±0.1 BTC) between
November 8, 2013 and November 13, 2013 to allow for
Table I and Figure 3 show the daily volumes of bitcoins
a three-day ransom period after CryptoLocker authors de- paid to SCL , and their values in US dollars commensurate with
creased the ransom amount to 0.5 BTC around November daily exchange rates7 , using the three different measurements.
10
We see that there are clear disparities between our estimates
• Payments of approximately 0.5 BTC (±0.05 BTC) be- from Method 1, which accounted for all transactions to S
CL ,
tween November 10, 2013 and November 27, 2013 to compared to our estimates from Method 2 or 3, particularly in
allow for a three-day ransom period after CryptoLocker the month of November. The causes of the discrepancies on
authors decreased the ransom amount to 0.3 BTC around November 13 and 14 are four 7 BTC transactions, which are
November 24
filtered by Methods 2 and 3, that we cannot find any conclusive
• Payments of approximately 0.3 BTC (±0.05 BTC) be- evidence on. The causes of the discrepancies from November
tween November 24, 2013 and December 31, 2013
25 through November 27 are eight 4 BTC transactions and a
• Late payments of approximately 10 BTC (±0.1 BTC)
single transaction8 of 15.6 BTC (November 25). We discover
between November 1, 2013 and November 11, 2013 when that these transactions eventually end up in the secondary
CryptoLocker introduced their “CryptoLocker Decryption money laundering address used in the Sheep Marketplace scam
Service” for victims who failed to pay ransoms within of 2013 (see Section VI-B). Comparing estimates between
the given time frame
Method 2, which filtered transactions based on known ransom
• Late payments of approximately 2 BTC (±0.1 BTC)
amounts demanded by CryptoLocker (i.e. 2 BTC, 0.5 BTC, 0.3
between November 11, 2013 and January 31, 2014 when BTC), and Method 3, which filtered transactions by ransom
CryptoLocker decreased the cost of their “CryptoLocker amounts and their respective periods of activity, we see smaller
Decryption Service”
differences between our estimates, which is a good indication
• Payments of approximately 0.6 BTC (±0.1 BTC) between
that the time intervals chosen in our ransom identification
December 20, 2013 and January 31, 2014
6 Time

intervals are in Universal Time Coordinates (UTC) from the start of
the initial day to the end of the final day.

7 There are many BTC/USD exchange rates available. We use the “24h
average” BTC/USD exchange rate from http://www.quandl.com.
8 https://blockchain.info/tx/43355544/

80

BTC

60

40

20

0
Sep 2013

Oct 2013

Nov 2013

Dec 2013

Jan 2014

Feb 2014

Dec 2013

Jan 2014

Feb 2014

Date
USD (in thousands)

30

20

10

0
Sep 2013

Oct 2013

Nov 2013

Date
Method 1

Method 2

Method 3

Fig. 3. The plots show longitudinal trends in the value of ransom payments to SCL from September 2013 through January 2014. We compare data yielded by
our identification framework to two other measurements detailed in Section III-B. Method 1 measures all transactions to SCL , Method 2 measures ransom
payments filtered by known bitcoin ransom demands, and Method 3 measures ransom payments filtered by both ransom demands and timestamps (ransom
identification framework).

framework (including the 72-hour buffer periods) produce in price should they have been exchanged at the height of
reliable estimates. This is important because filtering by ransom bitcoin’s exchange rate. In Figure 4, we show a valuation
amounts is a relatively strong and straightforward heuristic, in US dollars of the CryptoLocker economy throughout its
whereas filtering by time intervals is an invariably weaker operational period. We can see our estimate of $310,472.38 with
heuristic. This is because the date that a ransom is paid is the assumption that the CryptoLocker threat actors cashed out
dependent on the date that an individual’s system is infected ransom proceeds at the end of each day (“daily cash in”). In the
and the 72-hour payment window, so it is entirely possible “cumulative cash in” curve, we assume that the CryptoLocker
that a system could have been infected by an older version threat actors aggregated the bitcoins collected from ransom
of CryptoLocker demanding an outdated ransom. We consider payments, and we perform a valuation commensurate with
these anomalous ransom payments and omit these transactions the USD/BTC exchange rate on each day. Based on the
by choosing Method 3 for the purpose of maintaining reliable latter assumption, we estimate that the peak valuation of
and precise data for analyzing trends in ransom payments in the CryptoLocker economy occurred on November 29, 2013
Section V.
when they had collected a total of 1,044.13 BTC worth
upwards of $1.18 million. The USD/BTC exchange rate also
reached its peak on that day at $1,332.26/BTC. This estimate
B. CryptoLocker Economy in Bitcoin
is corroborated by the valuation of CryptoLocker provided
Using Method 3, we identify 795 ransom payments to
by Spagnuolo et al. at $1.1 million taken on December 15,
SCL , which contribute a total of 1,128.40 extorted bitcoins.
2013 [7].
Using daily bitcoin to USD exchange rates, we estimate
It is difficult to accurately measure or visualize the rate of
that these ransom payments valued $310,472.38. However,
CryptoLocker infections from Figure 3 due to the changing
as we mentioned before these figures are conservative and the
ransom demands and exchange rates for bitcoins, so we
payouts to SCL may have been as much as 1,541.39 BTC and
construct a time series plot showing the frequency of ransom
$539,080.69 based on Method 1, though we cannot be certain
payments to SCL throughout CryptoLocker’s operational period.
that the unaccounted transactions are ransom payments.
From Figure 5, we begin to see low levels of activity starting
Since the exchange rates for bitcoins were quite volatile on September 9, 2013 when the first ransom9 of 1.99 BTC is
throughout the months of CryptoLocker’s operation, the value
9 https://blockchain.info/tx/33208314/
of these extorted bitcoins would have seen a meteoric increase

TABLE II
S UMMARY OF C RYPTO L OCKER R ANSOM T YPES
Type

Time period

No. ransoms

BTC

USD

2 BTC

Sep. 5 - Nov. 11

422

843.77

146,623.33

10 BTC (Late)

Nov. 1 - Nov. 11

9

89.80

23,780.19

1 BTC

Nov. 8 - Nov. 13

43

43.04

15,904.49

0.5 BTC

Nov. 10 - Nov. 27

116

58.01

46,415.23

2 BTC (Late)

Nov. 11 - Jan. 31

10

20.00

14,492.46

0.3 BTC

Nov. 24 - Dec. 31

144

43.19

37,759.44

0.6 BTC

Dec. 20 - Jan. 31

51

30.59

25,497.24

Sep. 5 - Jan. 23

795

1,128.40

310,472.38

Total

Valuation (USD in millions)

1.2

30

Number of ransoms

0.9

0.6

20

0.3

10

0.0
Sep 2013

Oct 2013

Nov 2013

Dec 2013

Date

Cumulative cash in

Jan 2014

Feb 2014

Daily cash in

Fig. 4. The plot shows valuations of SCL (using Method 3) in USD. The
“cumulative cash in” curve performs a valuation commensurate with the total
bitcoins extorted in USD while the “daily cash in” curve performs a valuation
commensurate with daily bitcoins extorted to USD.

paid to SCL . On October 8, 2013 through October 11, 2013,
we see a sharp increase in ransom payments (27, 21, 37, and
29, respectively), which is consistent with our knowledge on
CryptoLocker’s use of the spam botnet Gameover ZeuS starting
on October 7, 2013 [13]. As a result, we see that the original
ransom of 2 BTC constituted 422 of the 795 identified ransoms
and nearly half of the total transaction volume in US dollars.
This was undoubtedly CryptoLocker’s most prolific period in
terms of successful infections. Over the next two months, we
see undulating periods of activity which leads us to believe
that CryptoLocker may have been distributed in several batches
throughout its operation. SCL experiences a significant decline
in ransom payments starting in mid-December of 2013 and
eventually comes to a close by the end of January 2014; we
are not aware of any further CryptoLocker addresses after this
period.
V. DATA A NALYSIS
Our goal is to gain insight on CryptoLocker’s targets and
changes in targets throughout its operation by statistically
determining distinct distributions in the times of day that
different ransom types (i.e. 2 BTC, 1 BTC) were paid to
SCL . We achieve this by performing Kolmogorov-Smirnov
(goodness of fit) tests on ransom payment timestamps to

0
Sep 2013

Oct 2013

Nov 2013

Dec 2013

Date

Jan 2014

Feb 2014

Fig. 5. The plot shows number of ransoms paid to SCL on each day from
September 2013 to January 2014.

determine whether different ransom types come from different
populations in Section V-A. We analyze these trends and
explain our findings in Section V-B.
A. Kolmogorov-Smirnov Tests
The Kolmogorov-Smirnov test for goodness of fit is based
on the maximum difference between either an empirical and
a hypothetical cumulative distribution (one-sample) or two
empirical cumulative distributions (two-sample) [15], [16].
For our study, we use two-sample Kolmogorov-Smirnov tests
to determine, using ransom payment timestamps, whether or
not samples from different ransom types come from different
parent populations at the 99.5% confidence level. Our sample
populations include 2 BTC, 1 BTC, 0.5 BTC, 0.3 BTC, and 0.6
BTC ransoms, but exclude 10 BTC (Late) and 2 BTC (Late)
ransoms, because we do not have sufficient sample sizes. We
provide metadata on each type of ransom in Table II.
We begin by stating the null hypotheses of our KolmogorovSmirnov tests: fn1 (x) and fn2 (x) are samples of two empirical
cumulative distribution functions f1 (x) and f2 (x), and that
H0 : f1 (x) = f2 (x)

− ∞ ≤ x ≤ +∞

(1)

The alternative hypotheses are that
H1 : f1 (x) 6= f2 (x)

− ∞ ≤ x ≤ +∞

(2)

00:00

00:00

22:00

22:00

18:00

2BTC

16:00

16:00

10BTC(L)
1BTC
0.5BTC
2BTC(L)

Time (UTC)

20:00

18:00

Time (UTC)

20:00

Ransom type

14:00
12:00
10:00

14:00
12:00
10:00

0.3BTC

08:00

08:00

0.6BTC

06:00

06:00

04:00

04:00

02:00

02:00

00:00

00:00
Oct 2013

Nov 2013

Dec 2013

Jan 2014

0.0e+00

5.0e−06

Date

1.0e−05

1.5e−05

2.0e−05

Density

Fig. 6. The plot shows trends in the times of day that ransoms were paid to SCL .
TABLE III
F IVE - NUMBER SUMMARY AND MEAN (H:M:S UTC)

2 BTC

Sample

Min.

Q1

Median

Q3

Max.

Mean

2 BTC

00:01:20

08:55:36

13:55:06

18:01:00

23:58:33

13:12:42

1 BTC

00:34:31

14:11:09

17:07:49

21:11:29

23:54:33

16:28:47

0.5 BTC

00:07:27

05:51:07

15:16:48

19:23:24

23:55:19

13:14:47

0.3 BTC

00:06:59

04:15:56

11:33:30

17:29:14

23:54:45

11:16:19

0.6 BTC

00:00:01

08:27:37

13:53:14

16:54:00

23:11:33

12:36:46

1 BTC

0.5 BTC

0.3 BTC

0.6 BTC

2 BTC

0.3028 | 0.2770 0.1449 | 0.1815 0.1802 | 0.1671 0.1068 | 0.2566

1 BTC

0.2476 | 0.3089 0.3532 | 0.3006 0.3461 | 0.3582

0.5 BTC

0.1865 | 0.2158 0.2067 | 0.2907

0.3 BTC

0.1908 | 0.2819

0.6 BTC
Fail to reject H0

D | D crit

Reject H0

D | D crit

Fig. 7. The table compares the test statistic D and the critical value Dcrit
for all permutations of ransom types. If D > Dcrit , we may reject the
null hypothesis and assume that the samples come from two different parent
populations at the 99.5% confidence level. These are shown in red.

In Figure 7, we compare D and Dcrit for each permutation.
For permutations where D < Dcrit , we fail to reject the
null hypothesis, which tells us that the two samples did not
come from different populations at the 99.5% confidence level.
For permutations where D > Dcrit , we may reject the null
hypothesis, which tells us that the two samples come from two
different populations at the 99.5% confidence level.
B. Analysis of Timestamp Data

Before we go into detail on the time series and density plots
in Figure 6 and a statistical summary of time distributions
in Table III, we put into perspective a hypothetical time
distribution that CryptoLocker victims from the United States
We then compute empirical cumulative distribution functions,
paid primarily in bitcoin (which we know to be false from
f1 (x) and f2 (x), from the two samples. To compute the test
the CryptoLocker study by CTU researchers [11]). What we
statistic D from f1 (x) and f2 (x), we find the maximum
do know is that CryptoLocker targeted business professionals,
absolute difference over all values of x given by
so we would expect ransom payments to be transacted during
D = max|Fn1 (x) − Fn2 (x)|
(3) typical working hours, or at the very least, during the daytime.
x
Let us assume a typical 9:00 a.m. to 5:00 p.m. working day
After computing D for all permutations of our sample pop- for business professionals in the United States. In Pacific Time
ulations, we compute critical values, Dcrit , at the 99.5% (PT, UTC-08:00), 9:00 a.m. to 5:00 p.m. would correspond to
confidence level for each permutation given by
17:00 UTC to 1:00 UTC on the following day. In Eastern Time
(ET, UTC-05:00), 9:00 a.m. to 5:00 p.m. would correspond to
p
Dcrit = 1.73 (n1 + n2 )/n1 n2
(4) 14:00 UTC to 22:00 UTC. Based on Figure 6 and Table III,

the only sample that could follow such a distribution is the 1
BTC sample, however, we cannot conclude that 1 BTC ransom
payments were primarily paid by victims from the United
States without further evidence. Because we have no reason
to believe that the marginal number of CryptoLocker victims
in the United States who opted to pay using bitcoin would
choose to pay in the nighttime, we assume that ransoms paid
to SCL came from outside of the United States.
From our Kolmogorov-Smirnov tests, we find that the
timestamp sample of 2 BTC ransoms differs from samples
of 1 BTC and 0.3 BTC ransoms. Additionally, we find that
the sample of 1 BTC ransoms differs from the sample of 0.3
BTC ransoms. Thus, we know that all three samples come
from different populations at the 99.5% confidence level. In
the 2 BTC sample, the median time of payment is 13:55:06
UTC with an interquartile range (Q3 − Q1 ) of 09:05:24 hours.
From the density curve in Figure 6, we see that it has a
unimodal distribution, which suggests that a large portion of
ransom payments during this time came from the same region.
Referring back to the CTU researchers’ study, we see that from
October 22, 2013 to November 1, 2013, they measure 1,767
infections from Great Britain, which has the second highest
percentage of total global infections (5.5%) after the United
States (70.2%). If we use our rough 9:00 a.m.–5 p.m. “working
day conjecture”, we would expect ransom payments from Great
Britain (GMT, UTC±00:00) to be concentrated between 9:00
UTC and 17:00 UTC. This is consistent with the first and third
quartiles from the 2 BTC sample (08:55:20 UTC and 18:01:00
UTC), but again, we make no solid claims without further
evidence.
Turning to the 1 BTC sample, the median time of payment
is 17:07:49 UTC with an interquartile range of 07:00:20 hours.
We see that it also has a unimodal distribution and restate that
the distribution of timestamps from this sample roughly models
what we would expect if a majority of these ransoms were paid
from the United States. Next, we take a look at the 0.3 BTC
sample, which has a median time of payment of 11:33:30 and
an interquartile range of 13:13:18. We see that the large spread
can be attributed to the 0.3 BTC sample’s bimodal probability
distribution, which suggests that the ransom sample came from
two distinct sources. One of these sources closely resembles the
time distribution from our 2 BTC sample, which we imputed
to ransom payments from Great Britain. The second source
of ransoms in early December falls between a 6-hour time
interval from 23:00 UTC to 5:00 UTC on the following day.
Referring back to the CTU researchers’ study, from December
9, 2013 to December 13, 2013, their measurements show that
CryptoLocker infections become more dispersed with 23.8%
from the United States, 19.0% from Great Britain, and 12.9%
from Australia. We continue to see CryptoLocker infections in
Great Britain, which is consistent with one of the peaks in the
bimodal 0.3 BTC sample. If we convert the time interval from
23:00 UTC to 5:00 UTC using our “working day conjecture”
and set 23:00 UTC as 9:00 a.m., we would expect these ransoms
to come from time zones with an offset of UTC+10:00. This
corresponds to the Australian Eastern Time Zone, which is,

Fig. 8. A visualization of outgoing bitcoin transactions from the CryptoLocker
cluster, SCL , network. Based on Louvain Modularity for community detection,
we distinguish 17 distinct sub-communities in the SCL network. We see that
the ransom balances from all addresses within a community are transferred to
a single aggregate address at the center. We group several sub-communities
together based on shared addresses and the times they were active (i.e.
Community 1, or c1 ) and identify eight communities of interest (c1 through
c8 ).

again, consistent with measurements by the CTU researchers’
CryptoLocker study, however we make no solid claims that a
large portion of the 0.3 BTC ransoms undoubtedly came from
Australia.
VI. F INANCIAL I NFRASTRUCTURE
We next turn to understanding CryptoLocker’s financial
infrastructure by analyzing communities in the CryptoLocker
cluster, SCL . We begin by overviewing SCL ’s topology in
Section VI-A. Examining SCL at a lower level, we impute realworld data found on various online fora and Bitcoin services
to distinct communities in Sections VI-B to VI-C. We find
connections to BTC-e and Bitcoin Fog and speculations to the
Sheep Marketplace scam and other bitcoin cybercrime.
A. Transaction Graph
To better understand CryptoLocker’s financial infrastructure,
we use Gephi10 , an open source software for network visualization and exploration, to visualize outgoing transactions
from SCL (Figure 8). In total, the network contains 993
nodes (addresses) and 1,020 weighted (by BTC), directed
edges, which delineate one or more transactions between two
addresses. The 993 addresses in the network are comprised
of 968 addresses from SCL and 25 additional addresses that
are recipients of outgoing transactions from SCL , which we
10 http://gephi.github.io/

11/16/2013
Sheep 50,000
BTC balance

11/19/2013
Anomalous tx
from a1 to Sheep

11/21/2013
Sheep goes offline

11/26/2013
Community 4's
tx to Sheep

11/29/2013
Sheep reaches
96,000 BTC balance

12/6/2013
Community 7's
tx to Sheep

10/15/2013
a1 s last activity
11/29/2013

11/16/2013

10/15/2013

11/1/2013

12/1/2013

12/6/2013

Fig. 9. Timeline of activity related to the Sheep Marketplace.

will call aggregate addresses. Using the Louvain Method for to the Sheep Marketplace is provided in Figure 9 (with the
community detection, we distinguish 17 distinct communities assumption that these speculations are true).
in the CryptoLocker network [17]. An interested reader can
1) Case study on Aseed1 : We first manually analyze Aseed1
see that the typical community is comprised of addresses to understand CryptoLocker’s early operation. We discover
in SCL (child nodes) that report to a central aggregate an anomalous transaction sharing the aforementioned Sheep
address to which all collected ransom payments are sent to Marketplace pattern, which prompts us to look for this same
(parent node). We characterize eight communities of interest pattern in other communities.
(arbitrarily and not chronologically named c1 through c8 ) that
Incoming transactions: On September 7, 2013, Aseed1
yield external information on addresses in SCL from our receives a payment of one BTC from a temporary address
analysis. Communities 2 through 8 are canonical communities a1 active for only that day. An outgoing payment for the same
in that they each correspond to only one aggregate address. amount is processed four days later. We believe this was a test
Community 1, on the other hand, is not centralized around one trial for CryptoLocker’s payment system. On September 13,
single aggregate address, but rather is comprised of multiple 2013, Aseed1 receives its first ransom payment of two BTC.
small communities and aggregate addresses. We will expand Throughout the rest of the month, Aseed1 receives 26 additional
on c1 ’s topology in Section VI-B, which we learn to be ransom payments of approximately two BTC each, which totals
CryptoLocker’s earliest community.
to 53.9081 BTC (worth about $6,600 at the time of payment).
On September 29, 2013, Aseed1 receives an anomalous payment
B. BTC-e and the Sheep Marketplace
of 0.0002 BTC from an inconclusive single-use address a2 .
Outgoing Transactions: From September 27, 2013 to October
Before we begin our analysis of CryptoLocker’s communities,
15,
2013, Aseed1 makes nine outgoing payments amounting
it helps to provide background on one pattern we found in
to
53.9081
BTC (the exact number of bitcoins collected from
several communities, namely an indirect connection to the
ransom
payments)
to aggregate addresses in c1 . Aseed1 ’s
Sheep Marketplace scam of 2013. The Sheep Marketplace
remaining
balance
after
its distribution of ransom payments
was the successor of the Silk Road [18], [19], an anonymous
is
the
unaccounted
0.0002
BTC. Considering that the exact
online marketplace specializing in the trade of narcotics, after
amount
of
bitcoins
from
ransom
payments is transferred to
its infamous takedown in February 2011. Launched in March
aggregate
addresses,
as
opposed
to the entire balance, we
2013, the Sheep Marketplace quickly gained traction as the
hypothesize
that
transfers
from
child
addresses to aggregate
leading anonymous online drug marketplace until, on November
addresses
were
automated,
rather
than
performed manually.
21, 2013, the owners shut down the site and absconded with
An
Anomalous
Transaction:
On
November
19, 2013, over a
96,000 bitcoins (over $100 million) belonging to its users [20].
month
after
A
’s
last
activity
and
right
before
the Sheep
We find that several aggregate addresses and child addresses
seed1
11
Marketplace
shutdown,
we
find
that
the
unaccounted
0.0002
make transactions to BTC-e , one of the largest bitcoin
BTC
is
transferred
as
part
of
a
multi-input
transaction
of 15
exchanges available where users can convert bitcoins into other
BTC
to
an
address
A
belonging
to
BTC-e.
From
cryptocurrencies, Dollars, Euros, and Rubles. From BTC-e,
exchange1
the
BTC-e
address,
the
15
BTC
are
included
in
a
multi-input
the bitcoins are then transferred to two money laundering
addresses used in the Sheep Marketplace scam. While it is transaction of 1,000 BTC to an address that we later learn to
unclear what sort of connection the CryptoLocker operation and be Sheep’s primary money laundering address Asheep1 , which
the Sheep Marketplace may share, if there is any connection has processed over 466,000 BTC as of February 2015.
Our efforts to find external data on Asheep1 leads us to a
at all, we continue by pointing out instances in which the two
are somehow linked. A timeline of SCL ’s activity in relation reddit thread that details how user sheeproadreloaded2 [21]
traced the $100 million of stolen bitcoins from the Sheep
11 https://btc-e.com/
Marketplace, through Bitcoin Fog, to the money laundering

address Asheep1 . Examining the transaction history of Asheep1 ,
3) Community 4: We next take a look at c4 , which is
we find that it has a balance of 50,000 BTC on November 16, comprised of 110 addresses. Throughout mid-November (when
2013. Throughout the rest of the month, we notice a massive CryptoLocker decreased the ransom amount due to the rising
increase in activity as it receives a total of 96,500 BTC while value of bitcoins), addresses in c4 receive numerous ransom
sending a total of 50,000 BTC to auxiliary addresses, most payments of 0.5 BTC. On November 26, 2013, the balances
notably a secondary money laundering address, Asheep2 .
from addresses in c4 are transferred to a single-use aggregate
Considering the time of Aseed1 ’s final transaction just two address Ac4 belonging to BTC-e, which collects a total of
days prior to the Sheep Marketplace’s alleged theft, and its 96.6 BTC. On the same day, the aggregate address transfers
discontinued use after transferring its balance to aggregate its balance into a multi-input transaction of 1,000 BTC to
addresses, we speculate that Aseed1 ’s anomalous 0.0002 BTC the Sheep Marketplace’s secondary money laundering address,
may have been part of the Sheep scam. Posted within a week of Asheep2 .
4) Community 7: Community c7 is a medium-sized comAseed1 ’s anomalous transaction on November 22, 2013, we find
12
munity
containing 83 addresses. Its aggregate address Ac4 also
a reddit thread detailing a similar occurrence in which a user
belongs
to BTC-e and collects 82.2 BTC from December 5
reports having 3.9 BTC stolen from his or her Mt. Gox [22]
to
December
9, 2013. On December 6, 9, and 10, we see
account and transferred to an intermediate address before
that
the
aggregate
address transfers its bitcoins in multi-input
ending up in Asheep2 . This indicates that Aseed1 ’s anomalous
transactions
of
400
BTC, 500 BTC, and 300 BTC, which are
transaction is not an isolated incident and we use this finding
also
sent
to
Sheep’s
secondary address, Asheep2 .
as a point interest for other CryptoLocker addresses.
2) Community 1: While canonical communities c2 through C. Botnets, Speculations, and Misc.
c8 each correspond to only one aggregate address, we group five
From communities c2 , c3 , c5 , c6 , and c8 , we characterize
small communities, which we will call sub-communities, and
how CryptoLocker’s operation evolved over time and we find
six aggregate addresses into c1 . The largest sub-community
subtle connections to other bitcoin cybercrime.
in c1 , containing 42 addresses (including addresses Aseed1
1) Community 2: By far the largest community in the Crypand Aseed2 ), is the connecting component between the four
toLocker network, c2 accounts for one-third of the addresses
surrounding sub-communities. We combine these five subin SCL . We find that c2 quickly follows c1 and is active
communities into a single community, c1 , based on connectivity
from October through mid-November, CryptoLocker’s most
and the times they were active.
prolific period of operation. Consistent with CryptoLocker’s
We find that addresses in c1 were active during the onset use of Gameover ZeuS and the Cutwail botnet starting on
of CryptoLocker’s attacks between September 9, 2013 and October 7, 2013, the increased distribution of ransomware
October 15, 2013. According to c1 ’s decentralized topography called for more addresses to collect ransom payments. Out of
and the reuse of addresses for ransom collection (i.e. Aseed1 the 328 addresses in c , 318 are single-use addresses. Thus, we
2
and Aseed2 ), we can assume with a high degree of confidence infer that the CryptoLocker threat actors opted to dynamically
that CryptoLocker’s threat actors did not dynamically generate generate addresses, instead of reusing addresses, to obfuscate
new addresses for its nascent attacks. The value in doing so their activity.
would be to obfuscate relationships between CryptoLocker
At the center of c2 is an aggregate address Ac2 belonging
addresses and money laundering addresses.
to BTC-e, which has received a total of 5,332.8 BTC. There
Examining c1 at a low level, we find that the addresses in is a notable disparity between how the extorted bitcoins were
c1 ’s largest sub-community report to two single-use aggregate handled between c1 and c2 . In c1 , we see a clear indication
addresses which collect 40 BTC and 20 BTC. From here, that the CryptoLocker threat actors decide to use Bitcoin
we see that these balances are processed through long chains mixers, particularly Bitcoin Fog, to launder their proceeds.
of single-input transactions with fractional bitcoins chipped In c2 , however, we see that the threat actors decide to generate
away in each transaction, which is characteristic of the Bitcoin new addresses for each ransom payment and directly transfer
Fog mixer [8]. Bitcoin Fog is a mixing service accessible via their proceeds to BTC-e without any means of obfuscation. The
Tor and allows users to deposit their bitcoins to up to five downfalls of the former method, which include a prolonged
newly generated addresses. To deter obvious indications of return on proceeds, transaction fees, diminished anonymity
using Bitcoin Fog, the service takes a randomized fee between for large transactions, and trust in a third party, may have
1–3% of the transaction value and processes bitcoins over deterred CryptoLocker’s growing enterprise from continued
a randomized timespan between 6 and 96 hours. The four use of Bitcoin Fog. In the latter method, the CryptoLocker
remaining sub-communities in c1 are all similar in size and authors might have assumed that newly generated addresses
structure. Each sub-community consists of between 14 and 18 would be sufficient in preserving privacy.
addresses which lead to aggregate addresses holding 20 to 40
2) Community 3: Community c3 is a medium-sized commuBTC in preparation for tumbling.
nity consisting of 52 addresses. On February 12, 2014, 14.63
BTC are transferred to c3 ’s single-use aggregate address Ac3
predominantly in 0.3 BTC ransom payments. Considering the
12 https://www.reddit.com/r/Bitcoin/comments/1r9rtp/i just had 39 btc
stolen from my mtgox account/
time frame of 0.3 BTC ransom payments, we presume that

c3 is one of CryptoLocker’s later communities. The balance a simulation of Bitcoin in a university setting, and proved
from c3 ’s aggregate address is transferred on March 3, 2014 that behavior-based and transaction-based clustering techniques
as part of a 100 BTC multi-input transaction to an address could effectively deanonymize up to 40% of Bitcoin users
a3 which has processed 374 BTC as of April 2014. Further in the simulation [4]. Ron and Shamir perform an analysis
analysis of c3 and its aggregate address does not yield any of the entire transaction graph and examine how users spend
useful information, however, we note that this transaction is bitcoins, how bitcoins are distributed among users, and means
one of SCL ’s last movements.
by which users protect their privacy in the Bitcoin system [5].
3) Community 5: c5 is a medium-sized community contain- Meiklejohn et al. explored Bitcoin wallets by using clustering
ing 57 addresses. On January 14, 2014, c5 ’s aggregate address heuristics (similar to Androulaki et al.) in order to classify
Ac5 collects a sum of 50 BTC from its component addresses. the owners of Bitcoin wallets and discussed the growing
After just one hour, the balance is transferred as part of a discrepancy between potential anonymity and actual anonymity
multi-input transaction of 1,134.99 BTC to an address, a4 , that in the Bitcoin protocol [6].
has processed over 277,000 bitcoins as of early 2014. In our
In the growing literature on measuring cybercrime, our
search for external information, we find another reddit thread13 study on CryptoLocker is related to a number of works aimed
posted on February 3, 2014 explaining how the reddit user’s at analyzing cybercrime in the Bitcoin ecosystem. Christin
coins were subject to an unauthorized withdrawal attempt to performed a comprehensive measurement analysis of the Silk
a4 (though the transaction was unsuccessful due to insufficient Road, detailing the anonymous marketplace’s constituents
funds). We also find a BitcoinTalk thread14 alleging that the and discussing socioeconomic and policy implications of the
address is related to the BitPay hack while another comment results [18]. In a later study, Soska and Christin perform a
in a separate blogpost15 suggests that the address belongs to longitudinal study of online anonymous marketplaces, which
Mt. Gox.
includes the Sheep Marketplace, and examines how these
4) Community 6: Community c6 , the second largest commu- virtual marketplaces have grown and evolved [19]. Spagnuolo
nity in the CryptoLocker cluster, is comprised of 141 addresses. et al. created an expandable framework, called BitIodine [7],
The community’s aggregate address Ac6 collects a total of 100 used for forensic analysis of the Bitcoin blockchain; they
BTC in ransoms payments on February 12, 2014 (another investigate addresses corresponding to the Silk Road owner,
one of CryptoLocker’s later communities). One day later, the known as Dread Pirate Roberts, and CryptoLocker ransomware.
balance is transferred as part of a 528.74 BTC multi-input Möser evaluated the effectiveness of several mixing services,
transaction to an address, a5 , which has processed over 11,000 commonly used to launder bitcoins, on preserving privacy [8].
bitcoins as of February 2014. One reddit post 16 mentions that Vasek et al. investigated the impact of distributed denial-ofa5 is related to the potential Mt. Gox address, a4 . Thus, we service attacks on popular Bitcoin services [23]. Ron and
assume that a5 is, at the very least, tied to some kind of Bitcoin Shamir used the blockchain to timeline events leading to Silk
cybercrime.
Road owner Dread Pirate Robert’s accumulation of wealth
5) Community 8: c8 is comprised of just 22 addresses. The before his arrest [24]. In a similar fashion to many of the
aggregate address Ac8 receives just 6.67 BTC on February 12, aforementioned works, we have performed the first in-depth,
2014. One day later, we see that the balance is transferred to systematic analysis of the CryptoLocker network in the Bitcoin
a5 . Again, we assume that this community may be linked to ecosystem.
some kind of Bitcoin cybercrime.
VIII. D ISCUSSION AND F UTURE W ORK
VII. R ELATED W ORK
Our study corroborates findings from previous studies on
The public’s interest in Bitcoin has continually grown CryptoLocker ransomware, but we want to emphasize that
throughout the years, undeterred by countless hacks and scams. the measurements and analysis in our own study were solely
Therefore, it is important for users to fully understand the drawn from blockchain analysis and crawling publicly available
implications and limitations of the Bitcoin system. Numerous data. In some cases of cybercrime analysis, this can be
studies have thoroughly examined the flawed provisions for a substantial limitation, but in others, this can be quite a
privacy inherent in the Bitcoin system. Reid and Harrigan valuable resource. We concede that our findings are, at best,
performed one of the first analyses of anonymity in the Bitcoin controvertible assumptions unless further concrete evidence
system and were able to attribute external identifying informa- on the CryptoLocker operation proves otherwise; although
tion to addresses using a nascent representation of the Bitcoin we show that our findings are consistent with other studies
transaction network [3]. Androulaki et al. tested Bitcoin’s on CryptoLocker, our study cannot standalone prove any of
privacy provisions, in both the actual Bitcoin environment and the conjectures made beyond a reasonable doubt. However,
13 https://www.reddit.com/r/Bitcoin/comments/1wvz66/who is taking my
bitcoins/
14 https://bitcointalk.org/index.php?topic=399024.0
15 http://btcanalytics.blogspot.com/2014/02/
bitcoins-most-mysterious-wallet.html
16 https://www.reddit.com/r/DarkNetMarkets/comments/1xw39e/ok so
everyones trying to find out the giant/

considering how rapidly the cybercrime landscape is changing,
there will be circumstances in which we, as researchers, do
not possess substantial evidence on nascent cybercrimes. In
this regard, we may use some of the techniques discussed in
our study to form a rudimentary understanding on these new
schemes.

We have performed an in-depth, measurement analysis on
CryptoLocker’s economy and financial infrastructure. Initially,
from two seed CryptoLocker addresses gathered from online
fora, we generate a cluster of 968 addresses belonging to
the CryptoLocker enterprise. From there, we identify and
quantify ransoms paid (in bitcoin) by victims and produce an
estimate upwards of $310,472.38 in financial damages. Based
on our analysis of ransom timestamps, we form conjectures on
regions where CryptoLocker infections were prevalent from
trends in the times that ransom payments were made and
compare our findings with other studies on CryptoLocker.
Finally, we visualize the CryptoLocker operation’s underlying
financial infrastructure and analyze the topology in a modular
fashion. We find subtle links between CryptoLocker, the Sheep
Marketplace heist, and other Bitcoin cybercrime schemes.
Our findings suggest that Bitcoin cybercrime may be much
more interconnected than originally considered and that further
analysis could uncover and confirm any existing relationships
in the underground cybercrime landscape.
Given the increased prevalance of ransomware in the cybercrime landscape as of late, we believe that our study may enable
further research in longitudinally measuring the evolution,
economies, and strategies of ransomware criminal enterprises.
We plan to further develop our heuristics for identifying
ransom transactions from the Bitcoin blockchain in order to
comprehensively measure the economies of CryptoLocker, as
well as newer families of ransomware, and better understand
their underlying financial infrastructures and money laundering
strategies.
ACKNOWLEDGEMENTS
This research was supported in part by grants from Army
Research Office and Center for Cybersecurity and Digital
Forensics at Arizona State University. The information reported
here does not reflect the position or the policy of the funding
agencies.
R EFERENCES
[1] S. Nakamoto. (2012) Bitcoin: A peer-to-peer electronic cash system.
[Online]. Available: http://www.bitcoin.org/bitcoin.pdf
[2] S. T. Ali, D. Clarke, and P. McCorry, “Bitcoin: Perils of an unregulated
global p2p currency,” in Security Protocols XXIII. Springer, 2015, pp.
283–293.
[3] F. Reid and M. Harrigan, An analysis of anonymity in the bitcoin system.
Springer, 2013.
[4] E. Androulaki, G. O. Karame, M. Roeschlin, T. Scherer, and S. Capkun,
“Evaluating user privacy in bitcoin,” in Financial Cryptography and Data
Security. Springer, 2013, pp. 34–51.
[5] D. Ron and A. Shamir, “Quantitative analysis of the full bitcoin
transaction graph,” in Financial Cryptography and Data Security.
Springer, 2013, pp. 6–24.
[6] S. Meiklejohn, M. Pomarole, G. Jordan, K. Levchenko, D. McCoy, G. M.
Voelker, and S. Savage, “A fistful of bitcoins: characterizing payments
among men with no names,” in Proceedings of the 2013 conference on
Internet measurement conference. ACM, 2013, pp. 127–140.
[7] M. Spagnuolo, F. Maggi, and S. Zanero, “Bitiodine: Extracting intelligence from the bitcoin network,” in Financial Cryptography and Data
Security. Springer, 2014, pp. 457–468.
[8] M. Moser, R. Bohme, and D. Breuker, “An inquiry into money laundering
tools in the bitcoin ecosystem,” in eCrime Researchers Summit (eCRS),
2013. IEEE, 2013, pp. 1–14.

[9] S. Meiklejohn and C. Orlandi, “Privacy-enhancing overlays in bitcoin,”
in Financial Cryptography and Data Security. Springer, 2015, pp.
127–141.
[10] P. Syverson, R. Dingledine, and N. Mathewson, “Tor: the secondgeneration onion router,” in Proceedings of the USENIX Conference
on Security Symposium. USENIX Association, 2004.
[11] K. Jarvis. (2014) Cryptolocker ransomware, 2013. [Online].
Available: http://www.secureworks.com/cyber-threat-intelligence/threats/
cryptolocker-ransomware/
[12] G. O. Karame, E. Androulaki, and S. Capkun, “Double-spending fast
payments in bitcoin,” in Proceedings of the 2012 ACM conference on
Computer and communications security. ACM, 2012, pp. 906–917.
[13] B. Stone-Gross. (2012) The lifecycle of peer-to-peer (gameover)
zeus. [Online]. Available: https://www.secureworks.com/research/the
lifecycle of peer to peer gameover zeus
[14] Blockchain api. [Online]. Available: https://blockchain.info/api/
blockchain api/
[15] F. J. Massey Jr, “The kolmogorov-smirnov test for goodness of fit,”
Journal of the American statistical Association, vol. 46, no. 253, pp.
68–78, 1951.
[16] I. T. Young, “Proof without prejudice: use of the kolmogorov-smirnov
test for the analysis of histograms from flow systems and other sources.”
Journal of Histochemistry & Cytochemistry, vol. 25, no. 7, pp. 935–941,
1977.
[17] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, “Fast
unfolding of communities in large networks,” Journal of Statistical
Mechanics: Theory and Experiment, vol. 2008, no. 10, 2008.
[18] N. Christin, “Traveling the silk road: A measurement analysis of a large
anonymous online marketplace,” in Proceedings of the 22nd international
conference on World Wide Web, 2013, pp. 213–224.
[19] K. Soska and N. Christin, “Measuring the longitudinal evolution of the
online anonymous marketplace ecosystem,” in Proceedings of the 24th
USENIX Conference on Security Symposium. USENIX Association,
2015, pp. 33–48.
[20] S.
Kerner.
Sheep,
bitcoin
and
the
$100
million heist. [Online]. Available: http://www.eweek.com/security/
sheep-bitcoin-and-the-100-million-heist.html
[21] sheepreloaded2. I just chased him through a bitcoin tumbler, and
when he came out with 96,000 btc, i was waiting for him... [Online].
Available: https://www.reddit.com/r/SheepMarketplace/comments/1rvlft/
i just chased him through a bitcoin tumbler and
[22] R. McMillan. The inside story of mt. gox bitcoin’s $460 million disaster.
[Online]. Available: http://www.wired.com/2014/03/bitcoin-exchange/
[23] M. Vasek, M. Thornton, and T. Moore, “Empirical analysis of denialof-service attacks in the bitcoin ecosystem,” in Financial Cryptography
and Data Security. Springer, 2014, pp. 57–71.
[24] D. Ron and A. Shamir, “How did dread pirate roberts acquire and
protect his bitcoin wealth?” in Financial Cryptography and Data Security.
Springer, 2014, pp. 3–15.

A PPENDIX

Aseed1
Aseed2
Aexchange1
Asheep1
Asheep2
Ac2
Ac3
Ac4
Ac5
Ac6
Ac7
Ac8
a1
a2
a3
a4
a5

Table of Bitcoin Addresses
1KP72fBmh3XBRfuJDMn53APaqM6iMRspCh
18iEz617DoDp8CNQUyyrjCcC7XCGDf5SVb
161yYpWYCx8cWGYW95QaZ9NUuR3fd5n4xt
1CbR8da9YPZqXJJKm9ze1GYf67eKAUfXwP
174psvzt77NgEC373xSZWm9gYXqz4sTJjn
1AEoiHY23fbBn8QiJ5y6oAjrhRY1Fb85uc
1D5DoY5KxGtcatoxR5M3fSEeRpxfkgbsA3
15F4g9sou83dNojKBikHx9vAkgnr1AhAEH
1C1ypgQSiFdkwkRN7F9VrYFJG4wjyGPsfB
1KpYNzAjTXZHwY782S4JMQbAXD8Ymyyw8s
1AzMZUvHuakMgEaukDuzdAMujkXxgrHCGZ
1LXbUwPDaBGAwxAQzutsLoTQJhTeyGskQb
1wffa72YQFTYJGMwuxbvWHFNAFGNKX3Bm
1Jfb6hbqfzwdLdR6Bspfh4HkND2mFj8LL
14CR4HYnJpvNFmtpoq34JUdHcA2rXeehkd
1Facb8QnikfPUoo8WVFnyai3e1Hcov9y8T
15WtLXz24WitRWtfdEzWPWZJYYDEBjjhUf

Going Native: Using a Large-Scale Analysis of
Android Apps to Create a Practical Native-Code
Sandboxing Policy
Vitor Afonso∗ , Antonio Bianchi† , Yanick Fratantonio† , Adam Doupé‡ ,
Mario Polino§ , Paulo de Geus¶ , Christopher Kruegel† , and Giovanni Vigna†
∗ CAPES

Foundation
Email: vitor@lasca.ic.unicamp.br
† UC Santa Barbara
Email: {antoniob, yanick, chris, vigna}@cs.ucsb.edu
‡ Arizona State University
Email: doupe@asu.edu
§ Politecnico di Milano
Email: mario.polino@polimi.it
¶ University of Campinas
Email: paulo@lasca.ic.unicamp.br

Abstract—Current static analysis techniques for Android applications operate at the Java level—that is, they analyze either
the Java source code or the Dalvik bytecode. However, Android
allows developers to write code in C or C++ that is cross-compiled
to multiple binary architectures. Furthermore, the Java-written
components and the native code components (C or C++) can
interact.
Native code can access all of the Android APIs that the Java
code can access, as well as alter the Dalvik Virtual Machine,
thus rendering static analysis techniques for Java unsound or
misleading. In addition, malicious apps frequently hide their
malicious functionality in native code or use native code to launch
kernel exploits.
It is because of these security concerns that previous research
has proposed native code sandboxing, as well as mechanisms
to enforce security policies in the sandbox. However, it is not
clear whether the large-scale adoption of these mechanisms is
practical: is it possible to define a meaningful security policy
that can be imposed by a native code sandbox without breaking
app functionality?
In this paper, we perform an extensive analysis of the native
code usage in 1.2 million Android apps. We first used static
analysis to identify a set of 446k apps potentially using native
code, and we then analyzed this set using dynamic analysis.
This analysis demonstrates that sandboxing native code with
no permissions is not ideal, as apps’ native code components
perform activities that require Android permissions. However,
our analysis provided very encouraging insights that make us
Permission to freely reproduce all or part of this paper for noncommercial
purposes is granted provided that copies bear this notice and the full citation
on the first page. Reproduction for commercial purposes is strictly prohibited
without the prior written consent of the Internet Society, the first-named author
(for reproduction of an entire paper only), and the author’s employer if the
paper was prepared within the scope of employment.
NDSS ’16, 21-24 February 2016, San Diego, CA, USA
Copyright 2016 Internet Society, ISBN 1-891562-41-X
http://dx.doi.org/10.14722/ndss.2016.23384

believe that sandboxing native code can be feasible and useful in
practice. In fact, it was possible to automatically generate a native
code sandboxing policy, which is derived from our analysis, that
limits many malicious behaviors while still allowing the correct
execution of the behavior witnessed during dynamic analysis for
99.77% of the benign apps in our dataset. The usage of our system
to generate policies would reduce the attack surface available to
native code and, as a further benefit, it would also enable more
reliable static analysis of Java code.

I.

I NTRODUCTION

Mobile operating systems allow third-party developers to
create applications (hereafter referred to as apps) that extend
the functionality of the mobile device. Apps span across all
categories of use: banking, socializing, entertainment, news,
health, sports, and travel.
Google’s Android operating system currently enjoys the
largest market share, currently at 84.7%, of all current smartphone operating systems [25]. The official app market for Android, the Google Play Store, has around 1.4 million available
apps [2] (according to AppBrain, a third-party Google Play
Store tracking site) with over 50 billion app downloads [38].
Android apps are typically written in Java, and then
compiled to bytecode that runs on an Android-specific Java
virtual machine, called the Dalvik Virtual Machine (DVM).1
These apps can interact with the filesystem, the Android APIs
(to access phone features such as GPS location, call history,
microphone, or SMS messages), and even other apps.
The wealth of information stored on smartphones attracts
miscreants who want to steal the user’s information, send out
1 In recent versions, the bytecode is instead compiled and executed by a
new runtime, called ART. For simplicity, in the rest of the paper we will only
refer to the DVM. However, everything we describe conceptually applies to
ART as well.

premium SMS messages, or even have the user’s device join
a botnet [10].
Static analysis of Android applications has been proposed
by various researchers to check the security properties of the
apps that the user installs [5], [7], [17], [18], [22], [23], [28],
[29], [39], [42]–[45].
All the proposed static analysis techniques for Android
apps have operated at the Java level—that is, these techniques
process either the Java source code or the Dalvik bytecode.
However, Android apps can also contain components written
in native code (C or C++) using the Android NDK [19]. Some
of the reasons why developers might use this feature, as stated
by the NDK documentation [19], are:

Compartmentalization: The native code of the app
should communicate with the Java part only using specific, limited channels, so that the native component
cannot modify, interact with, or otherwise alter the
Java runtime and code in unexpected ways.

•

Usability: The restrictions enforced by the sandbox
must not prevent a significant portion of benign apps
from functioning.

•

Performance: The sandbox implementation must not
impose a substantial performance overhead on apps.

Even though previous research has focused on the mechanism of native code sandbox enforcement [33], [35], to this
point no research has focused on how to generate a security
policy that a sandbox can enforce so that the policy is be both
practical (i.e., it would not break benign apps) and useful (i.e.,
it would limit malicious behaviors).

For certain types of apps, [native code] can be helpful so you can reuse existing code libraries written
in these languages, but most apps do not need the
Android NDK.

Sun and Tan [35], in their paper presenting the native code
sandboxing mechanism NativeGuard, state:

Typical good candidates for the NDK are CPUintensive workloads such as game engines, signal
processing, physics simulation, and so on.

We decide to follow a heuristic approach and by
default grant no permission to the [sandboxed native
code] in NativeGuard. The approach is motivated
by the observation that it is rare for legal native
code to perform privileged operations, as it is a “bad
practice” according to the NDK.

Using the NDK, the C or C++ code will be compiled and
packaged with the app. Android provides an interface (JNI)
for Java code to call functions of native code and vice versa.
While attempting to allow native code in Android apps is
noble, there are serious security implications of allowing apps
to execute code outside the Java ecosystem.

Sun and Tan are correct that the NDK considers native code
performing privileged operations to be bad practice, however,
we need data to confirm this intuition. We must know: what is
the native code in real-world apps doing? How do real-world
apps use native code? For instance, what if native code is
used to perform exactly the same actions as Java code? In this
case, it would not be possible to meaningfully constrain the
permission of native code components, and enforcing the leastprivilege principle would not grant any security benefits. We
also need clarification as to how tightly coupled the communication is between the native code and the Java code. Enforcing
compartmentalization might break or negatively affect tightlycoupled apps.

The existence of native code severely complicates static
analysis of Android apps. First, to our knowledge, no static
analysis of Android apps attempts to statically analyze the
native code included in the app. Thus, malware authors can
include the malicious payload/behavior in a native code component to evade detection. Furthermore, the native code in an
Android app has more capabilities than the Java code. This is
because the native code has direct access to the memory of
the running process, and, because of this access, can read and
modify the Dalvik Virtual Machine and its data.2 Effectively,
this means that the native code can completely modify and
change the behavior of the Java code—rendering all static
analysis of the Java code unsound.

To answer these questions, we perform a large-scale analysis of real-world Android apps. Specifically, we look at how
apps use native code, both statically and dynamically. We
statically analyze 1,208,476 Android apps to see if they use
native code, then we dynamically analyze the 446,562 that
were determined to use native code. Our system is able to
monitor the dynamic execution of an app, while recording
activities performed by its native code components (e.g.,
invoked system calls, interactions between native and Java
components). From this analysis, we shed light on how realworld Android apps use native code.

In light of these security problems with native code usage
in Android applications, researchers have turned to sandboxing
mechanisms, which limit the interaction between the native
code and the Java code [8], [33], [35]. This follows the leastprivilege principle: The native code does not need full access
to the Java code and thus should be sandboxed.
A native code sandbox should be security-relevant and
usable with benign, real-world apps. These requirements result
in the following properties:
•

•

In addition, our dynamic analysis system can be used
to generate a native code sandboxing policy that allows for
normal execution of the native code behaviors observed during
the dynamic analysis of a set threshold of apps, while reducing
the attack surface and thus limiting many malicious behaviors
(e.g., root exploits) of malicious apps.

Least-Privilege: The native code of the app should
have access only to what is strictly required, thus
reducing the chances the native component could
extensively damage the system.

2 Even if the Dalvik Virtual Machine memory is initially mapped as readonly, a native code component can change the memory permission by using
the mprotect syscall.

The main contributions of this paper are the following:
2

•

We develop a tool to monitor the execution of native
components in Android applications and we use this
tool to perform the largest (in terms of number of apps
and detail of information acquired) study of native
code usage in Android.

•

We systematically analyze the collected data, providing actionable insights into how benign apps use native
code. Moreover, we release the full raw data and we
make it available to the community [1].

•

Our results show that completely eliminating permissions of native code is not ideal, as this policy
would break, as a lower bound, 3,669 of the apps in
our dataset. However, we propose that our dynamic
analysis system can be used to derive a native code
sandboxing policy that limits many malicious behaviors, while allowing the normal execution of the native
code behaviors observed during the dynamic analysis
of a set threshold of apps (99.77% in our experiment).
II.

Runtime.load, and Runtime.loadLibrary. Native
code in shared libraries can be invoked at loading time,
through calls to native methods and through callbacks in
native activities. When a library is loaded, its _init and
JNI_OnLoad functions are called.
Native methods. Native methods are implemented in shared
libraries and declared in Java. When the Java method is called,
the framework executes the corresponding function in the
native component. This mapping is done by the Java Native
Interface (JNI) [21]. JNI also allows native code to interact
with the Java part to perform actions such as calling Java
methods and modifying Java fields.
Native activity. Native code is invoked in native activities
using activities’ callback functions, (e.g., onCreate and
onResume), if defined in a native library.
C. Malicious Native Code
Malicious apps can use native code to hide malicious
actions from static analysis of the Java portion of the app.
These actions can be calls to methods in Java libraries, such
as sending SMS messages, or complex attacks that involve
exploiting the kernel or privileged processes to compromise
the entire OS. These root exploits are possible because native
code is allowed to directly call system calls. Another possible
way that attackers can directly call system calls to execute root
exploits is by exploiting vulnerabilities in native code used by
benign apps.

BACKGROUND

To understand the analysis that we perform on Android
applications and our proposed policy, it is necessary to review
the Android security mechanisms, how native code is used in
Android, the damage that malicious native code can cause, and
the previously proposed native code sandboxing mechanisms.
A. Android Security Mechanisms

As previous research has shown [35], because native code
shares the same memory address space as the Dalvik Virtual
Machine, it can completely modify the behavior of the Java
code, rendering static analysis of the Java code fundamentally unsound. For instance, malicious code can use functions
exported by libDVM.so to identify where the bytecode implementing a specific Java method is placed in memory. At this
point, the native code can dynamically replace the method at
run time.

When apps are installed on an Android phone, they are
assigned a new user (UID) and groups (GIDs) based on the
permissions requested by the app in its manifest. Every app is
executed in a separate process, which is a child of Zygote, a
process started when the system is initialized. Moreover, interprocess communication is done using intents which all flow
through an Android system-level process called Binder [11].
On Android, some operations and resources are protected
by permissions. Apps must declare the permissions needed in
the manifest, and at installation time the requested permissions
are presented to the user, who decides to continue or cancel
the installation. Permissions are enforced app-wise using Linux
access-control mechanisms and by system services that check
if the app is allowed to access certain resources or perform the
requested operation [16].

D. Native Code Sandboxing Mechanisms
Several approaches have been proposed to sandbox native code execution. For instance, NativeGuard [35] and Robusta [33] move the execution of native code to a separate
process. Two complementary goals are obtained: (1) the native
code cannot tamper with the execution of the Java code and (2)
different security constraints can be applied to the execution
of the native code.

B. Native Code
Native code in Android apps is deployed in the app as ELF
files, either executable files or shared libraries. There are four
ways in which the Java code of an Android app can execute
native code: Exec methods, Load methods, Native methods,
and Native activity.

Communication between the Java code and the native code
is then ensured by modifying the JNI interface to make the two
processes communicate through an OS-provided communication channel (e.g., network sockets).

Exec methods. Executable files can be called from
Java by two methods, namely Runtime.exec and
ProcessBuilder.start. Hereinafter we refer to these
methods as Exec methods.
Load methods. Native code in shared libraries can be loaded
by the framework when a NativeActivity is declared in the
manifest, along with its library name, or by the app through
the following Java methods, which are hereinafter referred to as
Load methods: System.load, System.loadLibrary,

While moving native code to a separate process is a natural
mechanism to achieve the aforementioned goals (because it
relies on OS-provided security mechanisms, such as process
memory separation or process permissions), other solutions
are possible. For instance, thread-level memory protection
(as proposed in Wedge [8]). However, applying this solution
in Android would require significant modifications to the
underlying Linux kernel.
3

TABLE I.

R ESULTS OF THE STATIC ANALYSIS .

Apps
267,158
42,086
288,493
242,380
221,515
446,562

III.

actions performed by the analyzed apps, including system
calls, JNI calls, Binder transactions, calls to Exec methods,
loading of third-party libraries, calls to native activities’ native
callbacks, and calls to native methods. The system calls were
captured using the strace tool, and the other information we
obtained through instrumentation.

Type
Native method
Native activity
Exec methods
Load methods
ELF file
At least one of the above

To monitor JNI calls, calls to native methods, and library
loading, we modified libdvm. However, we do not want to
monitor all JNI calls, just JNI calls to the app’s native code,
rather than calls to native code in the standard libraries that
Android includes. To avoid monitoring JNI calls in standard
libraries and calls to native methods in standard libraries,
we modified the “Method” structure to include a property
indicating whether it belongs to a third-party library or not.
When a third-party library is loaded, this property is set
accordingly.

A NALYSIS I NFRASTRUCTURE

We designed and implemented a system that dynamically
analyzes Android applications to study how native code is
used and to automatically generate a native code sandboxing
policy. Our analysis consists of an instrumented emulator, and
it records all events and operations executed from within native
code, such as invoked syscalls and native-to-Java communication. The dynamic instrumentation is completely generic, and
it allows the usage of any manual or automatic instrumentation
tool. The version of the Android system used was 4.3.

We modified libbinder to track and monitor Binder
transactions. We record the class of the remote function being
called and the number that identifies the function. To map
the identifiers to function names, we parse the AIDL (Android
Interface Definition Language) files and source files that define
Binder interfaces. To find files that have such definitions, we
search for uses of the macros DECLARE_META_INTERFACE
and IMPLEMENT_META_INTERFACE and classes that extend “IInterface.” Furthermore, to match identification numbers to names, we search in “.cpp” files for enumerations
that use IBinder::FIRST_CALL_TRANSACTION and, in
“.java” files, for variables defined using IBinder.FIRST_CALL_TRANSACTION. We use the names assigned FIRST_CALL_TRANSACTION as the functions with identifier 1, the
ones assigned FIRST_CALL_TRANSACTION + NUM as the
functions with identifier 1+NUM and, for the enumerations
that only use FIRST_CALL_TRANSACTION to define the
first element, we consider they are increasing the identifier
one by one.

Since our goal was to obtain a comprehensive characterization of native code usage in real world applications, we used
a corpus of 1,208,476 distinct—different package names and
APK hashes—free Android apps that we have continuously
downloaded from the Google Play store from May 2012–
August 2014. The age of the apps varies throughout the timeframe, as we currently do not download new versions of apps.
A. Static Prefiltering
Performing dynamic analysis of all 1,208,476 apps by
running each app would take a considerable amount of time;
therefore, by using static analysis, we filtered the apps that
had some indication of using native code. The characteristics
we looked for in the apps are the following: having a native
method, having a native activity, having a call to an Exec
method, having a call to a Load method, or having an ELF
file inside the APK.

Calls to Exec methods are identified by instrumenting
libjavacore. Finally, to monitor the use of native callbacks
in native activities, we modified libandroid_runtime.

We used the Androguard tool [12] as a basis for the static
analysis. To identify native methods we searched for methods
declared in the Dalvik bytecode with the modifier3 “native.”
Native activities were identified by two means: (1) looking for
a NativeActivity in the manifest and (2) looking for classes
declared in the Dalvik bytecode that extend NativeActivity.
Finally, calls to Exec and Load methods were identified by
investigating method invocations in the bytecode.

We determine which actions were performed by native code
and which by Java code after the dynamic analysis. To make
this determination, we observe when threads change execution
context from Java to native and from native to Java. Thus,
we process all system calls, keeping a list of threads that
are executing native code. We add a thread to this list when
one of the following happens: Exec method is executed—we
add the child process, which is then used to call execve, a
custom (third-party) shared library is loaded, a native method
is executed, or a callback in the native component of a native
activity is executed. When these actions are completed and the
execution control changes back to Java, the thread is removed
from the list.

Of the 1,208,476 apps statically analyzed, 446,562 apps
(37.0%) used at least one of the previously mentioned ways
of executing native code. Table I presents the number of apps
that use each of these characteristics.
B. Dynamic Analysis System

We also remove a thread from the list when one of the
JNI methods in Table II is executed. The Call*<TYPE>
functions are used to call Java methods, and the NewObject*
functions are used to create instances of classes, which results
in the execution of Java constructors. When these methods
return, the thread is placed back on the list. Additionally, we
remove a thread from the list when the clinit method, which

Now that we have identified which Android apps use native
code, we now want to understand how apps use native code.
During the dynamic analysis we monitor several types of
3 Modifier here is an attribute of a method, similar to public. An
example Dalvik method signature would be: .method public native
example().

4

Fig. 1.

Ideally, it would have been possible to use more sophisticated dynamic instrumentation systems. However, the large
scale of our analysis motivated our choice to use a simpler
approach, as it would have required a prohibitive amount of
resources to run on hundreds of thousand of apps. While
our dynamic instrumentation system is acceptable for the
purposes of understanding the lower bound on what behaviors
native code performs, the incompleteness inherent in dynamic
analysis can affect the native code policies generated by our
system. However, if Google or another large company were
to adopt the idea of using a dynamic analysis system to
automatically generate a native code security policy, they
could use substantial resources to run the applications for
longer periods of time, use sophisticated dynamic analysis
approaches [32], or even introduce the instrumentation into
the Android operating system and sample the behaviors from
real-world devices.

Possible transitions between native code and Java.

TABLE II.
JNI METHODS THAT CAUSE A TRANSITION FROM NATIVE
<TYPE> CAN BE THE FOLLOWING : O BJECT; B OOLEAN ; B YTE ;
C HAR ; S HORT; I NT; L ONG ; F LOAT; D OUBLE ; VOID .

TO JAVA .

During dynamic analysis, 33.6% (149,949) of the apps
identified by static analysis as potentially having native code
actually executed the native code. Table III presents the number
of apps that executed each type of native code. These numbers
constitute a lower bound of the apps that could actually execute
native code.

Call<TYPE>Method
CallNonVirtual<TYPE>Method
Call<TYPE>MethodA
CallNonVirtual<TYPE>MethodA
Call<TYPE>MethodV
CallNonVirtual<TYPE>MethodV
CallStatic<TYPE>Method
CallStatic<TYPE>MethodA
CallStatic<TYPE>MethodV
NewObject
NewObjectV
NewObjectA

In order to understand, for our study, why the native
code was not reached during dynamic analysis, we manually
analyzed, statically and dynamically, 20 random apps that were
statically determined to have native code. For 40% (8) of them,
we established through analysis of the decompiled code that
the native code was unreachable from Java code (also known
as deadcode). The remaining applications were too complex
to be manually inspected, and we were not able to ascertain
whether the native code components were not reached due
to deadcode. For this reason, we dynamically analyzed and
manually interacted with them and we did not find any path
that led to the execution of the native code. Thus, we believe
that also in this case the native code component was not
reached due to deadcode, even if we were not able to be
completely certain, due to the incomplete nature of manual
analysis.

is the static initialization block of a class, is executed. Figure 1
presents all mentioned transitions.
To understand how isolating the native code from the Java
code would impact the performance of the apps, we also
monitor the amount of data exchanged between native and Java
code. We measured the amount of data passed in parameters
of calls from native code to Java methods and vice versa, as
well as the size of the returned value. We also capture the size
of data used to set fields in Java objects. The results of this
analysis are presented in Section IV-B.
IV.

We further investigated why there was deadcode in these
apps. In each case, the native code was deadcode in third-party
libraries. In fact, in our experience, it often happens that an
app includes a third-party library, to then actively use only
a (sometimes very limited) subset of its functionality, thus
leading to deadcode. Hence, we expect this to be the case
for many apps where our analysis did not reach native code.
As an additional experiment, we also manually and extensively
dynamically exercised another 20 random apps. We observed
no cases of significant changes in the results compared to the
Google Monkey automated analysis (neither additional native
code components were reached nor more syscalls were called).

E VALUATION AND I NSIGHTS

We ran both the static pre-filter and dynamic analysis
across numerous physical machines and private-cloud virtual
machines. In total, we used 100 cores and 444 GB of memory.
Moreover, the analysis was run in parallel.
The dynamic analysis was performed using an instrumented Android emulator (as described in the previous section), and to keep the analysis time feasible we limited the
analysis to two minutes for each app. To dynamically exercise
each application, we followed an approach similar to what is
used in Andrubis [40]: we used the Google Monkey [20] to
stimulate the app with random events, and we then automatically generated a series of targeted events (by means of sending
properly-crafted intents) to stimulate all activities, services, and
broadcast receivers defined in the application.

To further understand the coverage of our dynamic analysis system we performed two additional experiments, one
measuring the Java method coverage and one measuring the
native code coverage. Section VII discusses these experiments
in depth.
5

TABLE III.

T HE NUMBER OF APPS THAT EXECUTED EACH TYPE OF
NATIVE CODE .

Apps
72,768
19,164
132,843
27,701
149,949
TABLE IV.

TABLE VI.

T OP FIVE MOST COMMON ACTIONS PERFORMED BY APPS
THAT CALLED STANDARD BINARIES IN THE SYSTEM . F OR THE
INTERESTED READER , WE REPORT THE FULL VERSION OF THIS TABLE
IN [1].

Type
Native method
Native activity
Load library
Call executable file (27,599 standard,
148 custom and 46 both)
At least one of the above

Apps
19,749
3,384
3,362
1,041
861

OVERVIEW OF ACTIONS PERFORMED BY CUSTOM SHARED
LIBRARIES IN NATIVE CODE .

The top five most common actions performed by apps in
native methods, native activities, and custom binaries called
through Exec are presented in Table V. Table VI presents the
top five most common actions performed by the apps that used
Exec to call standard (system) binaries.

Writing log messages
Performing memory management system calls, such as mmap
and mprotect
Reading files in the application directory
Calling JNI functions
Performing general multiprocess and multithread related system
calls, such as fork, clone, setpriority, and futex
Reading common files, such as system libraries, font files, and
“/dev/random”
Performing other operations on files or file descriptors, such as
lseek, dup, and readlink
Performing operations to read information about the system,
such as uname, getrlimit, and reading special files (e.g.,
“/proc/cpuinfo” and “/sys/devices/system/cpu/possible”)
Performing system calls to read information about the process
or the user, such as getuid32, getppid, and gettid
Performing system calls related to signal handling
Performing cacheflush or set_tls system calls or performing nanosleep system call
Reading files under “/proc/self/” or “/proc/<PID>/”, where PID
is the process’ pid
Creating directories

By analyzing the system calls and the Java methods called
from native code, we identified 3,669 apps that perform
an action requiring Android permissions from native code.
Table VII presents the top five most popular permissions used,
how many apps use them, and how we detected its use. We
used PScout [6] to compute the permissions required by each
Java method. Comparing the permissions used in native code
with the permissions requested by the app, we found that only
81 apps use, in native code, all the permissions requested by
the app.
In addition to this being the first concrete look into how
many apps use native code and what that native code does, we
can draw two important conclusions: (1) if the native code is
separated in a different process, it is necessary to give some
permissions to the native code and (2) the permissions of
the native code can be more strict (less permissive) than the
permissions of the Java code.

A. Native Code Behavior—An Overview

It is interesting to note how conclusion (1) shows that
the drastic measure adopted in NativeGuard [35], which
does not grant any permissions to the native code, would
break 3,669 of apps. This observation reinforces even
more our belief that security policies should be generated following a data-driven approach. For instance, a reasonable tradeoff would be to allow to the native code
only the INTERNET, WRITE_EXTERNAL_STORAGE, and
READ_EXTERNAL_STORAGE permissions (the three most
commonly used in native code), thus blocking only 152
applications.

We present in this section an overview of the actions
performed by native code on Android. We split the actions into
those performed by shared libraries (including those performed
during library loading, native methods, and native activities)
and those that are the result of invoking custom, executable,
and binaries through Exec methods. We also present the actions
performed using standard binaries (i.e., not created by the app),
but in this case based on their names and parameters, instead
of looking at the system calls.
94.2% (125,192) of the apps that used custom shared
libraries executed only a set of common actions in native code,
and Table IV contains the common actions.

B. Java—Native Code Interactions
To better understand the performance implications of separating the native code from the Java code of the apps, we
measured the number of interactions per millisecond between
Java and native code, i.e., the number of calls to JNI functions,
calls to native methods, and Binder transactions.

TABLE V.

T OP FIVE MOST COMMON ACTIONS PERFORMED BY APPS IN
NATIVE CODE , THROUGH SHARED LIBRARIES (SL) AND CUSTOM
BINARIES (CB). F OR THE INTERESTED READER , WE REPORT THE FULL
VERSION OF THIS TABLE IN [1].

SL
3,261
1,929
1,814
1,594
1,242

Description
Read system information
Write file in the app’s directory or in the sdcard
Read logcat
List running processes
Read system property

CB
72
39
35
5
144

Description
ioctl system call
Write file in the app’s directory
Operations on sockets
Create network socket
Terminate process or thread group

The mean of interactions per millisecond is 0.00142,
whereas the variance is 0.00003 and the maximum value
is 0.22. NativeGuard’s [35] performance evaluation with the
Zlib benchmark shows a 34.36% runtime overhead for 9.81
interactions per millisecond and 26.64% for 3.96 interactions
per millisecond. Therefore, our experiment shows that isolating
6

TABLE VII.

T HE FIVE MOST COMMON ( BY NUMBER OF APPS ) ACTIONS IN NATIVE CODE THAT REQUIRE A NDROID PERMISSION . F OR THE INTERESTED
READER , WE REPORT THE FULL VERSION OF THIS TABLE IN [1].

Apps
1,818
1,211
1,211
132

Permission
INTERNET
WRITE EXTERNAL STORAGE
READ EXTERNAL STORAGE
READ PHONE STATE

79

ACCESS NETWORK STATE

Description
Open network socket or call method java.net.URL.openConnection
Write files to the sdcard
Read files from the sdcard
Call
methods
getSubscriberId,
getDeviceSoftwareVersion,
getSimSerialNumber
or
getDeviceId
from
class
android.telephony.TelephonyManager or Binder transaction to call
com.android.internal.telephony.IPhoneSubInfo.getDeviceId
Call method android.net.ConnectivityManager.getNetworkInfo

TABLE VIII.
T OP FIVE MOST COMMON TYPES OF COMMAND PASSED
WITH THE “- C ” ARGUMENT TO S U , SEPARATED BETWEEN THE APPS THAT
MENTION THEY NEED ROOT PRIVILEGES IN THEIR DESCRIPTION OR NAME
AND THE ONES THAT DO NOT MENTION IT. F OR THE INTERESTED READER ,
WE REPORT THE FULL VERSION OF THIS TABLE IN [1].

Does not
Mention
Root
12

Does
Mention
Root
10

five most common types of actions that these apps tried to
execute using su, along with the number of apps that attempt
to execute that command, and if the app mentioned that it
requires root or not. This table gives insights into what the
app is trying to accomplish as root. The table shows that
the most common action used with the “-c” argument of su
is calling a custom executable. Because apps cannot use su
in the emulator, these actions did not work properly during
dynamic analysis, so we cannot obtain more information on
their behavior.

Description
Custom

executable

(e.g.,

su -c sh /data/data/com.test.etd062.ct/files/occt.sh)

1
2
1

13
12
8

1

7

Reboot
Read system information
Change permission of file in app’s directory
Remove file in app’s directory

D. JNI Calls Statistics
Understanding the JNI functions called by native code can
reveal how the native components of apps interact with the app
and the Android framework. Table IX presents the types of JNI
functions that were used by the apps and how many apps used
them. The most relevant actions for security considerations in
this table are: (1) calling Java methods and (2) modifying fields
of objects. Calling methods in Java libraries from native code
can be used to avoid detection by static analysis. Moreover,
modifying fields of Java objects can change the execution of
the Java code in ways that static analysis cannot foresee.

native code in a different process should not have a substantial
performance impact on average.
Additionally, we measure the number of bytes exchanged
between the Java code and native code per second. The mean
of bytes exchanged per second is 1,956.55 (1.91 KB/s) and the
maximum value is 6,561,053.27 (6.26 MB/s). Only 11 apps
exchanged more than 1 MB/s. We believe the amount of data
exchanged between Java and native code would not incur a
significant overhead, although it could vary greatly depending
on the specific app.

Calling Java methods, both from the Android framework
and from the app can be performed by some of the methods
presented in Table II, more precisely the ones whose name
starts with “Call.” As Table IX shows, we identified 35,231
apps that have native code which calls Java methods. More
specifically, 24,386 apps used these functions to call Java
methods from the app and 25,618 apps used them to call Java
methods from the framework. Table X presents what groups
of methods from the framework were called, along with the
amount of apps that called methods in each group.

C. Usage of the su Binary
Unlike common Linux distributions, in Android, users do
not have access to a super user account and, therefore, are
prevented from performing certain actions, such as uninstalling
pre-installed apps. Thus, to have greater control over the
system, many users perform a process known as “rooting,” to
be able to perform actions as the “root” user. Usually, during
this process, a suid executable file called su is installed, as
well as a manager app that restricts which apps can use this
binary to perform actions as root. Because this process is
so common among users, there are many apps that provide
functionality that can only be performed by the root user,
such as changing the fonts of the system or changing the DNS
configuration.

E. Binder Transactions
1.64% (2,457) of the apps that reached native code during
dynamic analysis performed Binder transactions. Table XI
presents the top five most commonly invoked classes of the
remote methods. The most common class remotely invoked
by this process is IServiceManager, which can be used
to list services, add a service, and get an object to a Binder
interface. All apps that used this class obtained an object to a
Binder interface and two apps also used it to list services. This
data shows that using Binder transactions from native code is
not common. From a security perspective this is good as the
use of Binder transactions represent a way in which native
code can perform critical actions while staying undetected by
static analysis.

Our analysis identified 1,137 apps that try to run su.
Surprisingly, 28.23% (321) of these apps do not mention in
their description or in their name that they need root privileges.
Some of these apps use the “-c” argument of su to specify
a command to be executed as root. Table VIII presents the top
7

TABLE IX.

Apps
94,543
71,470
53,219
49,321
45,773
41,892
35,231
19,372
18,601
14,330
6,918
2,203
47
37

G ROUPS OF JNI CALLS USED FROM NATIVE CODE .

TABLE XII.

Description
Get class or method identifier and class reference
Get or destroy JavaVM, and Get JNIEnv
Manipulation of String objects
Register native method
Manipulate object reference
Thread manipulation
Call Java method
Manipulate arrays
Manipulate exceptions
Create object instance
Modify field of an object
Manipulate direct buffers
Memory allocation
Enter or exit monitor

TABLE X.

T OP 10 GROUPS OF JAVA METHODS FROM THE A NDROID
FRAMEWORK CALLED FROM NATIVE CODE .

Apps

Description
Get path to the Android
package associated with the context of the caller
Get class name
Manipulate data structures
Methods related to cryptography
Manipulate native types
Read system information
Audio related methods
Read app information
String manipulation and encoding
Input/output related methods
Reflection

7,423
6,896
5,499
4,082
3,817
3,769
3,018
2,070
1,192
575
483

T OP 10 MOST USED STANDARD LIBRARIES .

Apps

Name

24,942

libjnigraphics.so

2,646

libOpenSLES.so

2,645

libwilhelm.so

349
347

libpixelflinger.so
libGLES android.so

183

libGLESv1 enc.so

183

gralloc.goldfish.so

182

libOpenglSystemCommon.so

182

libGLESv2 enc.so

181

lib renderControl enc.so

TABLE XIII.

Description
Manipulate Java
bitmap objects
Audio input and output
Multimedia output
and audio input
Graphics rendering
Graphics rendering
Encoder for GLES 1.1
commands
Memory allocation
for graphics
Common functions
used by OpenGL
Encoder for GLES 2.0
commands
Encoder for rendering
control commands

T OP 10 MOST USED CUSTOM LIBRARIES .

Apps
19,158
17,343
16,450
13,556
11,486

Name
libopenal.so
libCore.so
libmain.so
libstlport shared.so
libcorona.so

11,480
11,458
11,090

libalmixer.so
libmpg123.so
libmono.so

10,857
10,408

liblua.so
libjnlua5.1.so

Description
Rendering audio
Used by Adobe AIR
Common name
C++ standard libraries
Part of the Corona SDK, a development platform for mobile apps
Audio API of the Corona SDK
Audio library
Mono library, used to run .NET
on Android
Lua interpreter
Lua interpreter

F. Usage of External Libraries
Understanding the libraries used by the apps in native code
can help us comprehend their purpose. Table XII presents the
top 10 most used system libraries and Table XIII presents the
top 10 must used custom libraries by apps in native code. It
demonstrates that apart from the bitmap manipulation library,
which was used by 16.6% (24,942) of the apps that reached
native code, no standard library was used by a great number
of apps. On the other hand, several custom libraries were used
by more than 7.5% of the apps that executed native code.
V.

mechanisms prevent native code from modifying Java code,
which allows static analysis of the Java part to produce
more reliable results. However, this is not enough, considering
that the app can still perform dangerous actions—that is, by
interacting with the Android framework/libraries and by using
system calls to execute root exploits.
Our goal here is to reduce the attack surface available to
native code, by restricting the system calls and Java methods
that native code can access. In particular, we propose to use
our dynamic analysis system to generate security policies. A
security policy represents the normal behavior, which can be
seen as a sort of whitelist that represents the syscalls and
Java methods that are normally executed from within native
code components of benign applications. These policies also
implicitly identify which syscalls and Java methods should be
considered as unusual or suspicious (as they do not belong to
the common syscalls), such as the ones used to mount root
exploits.

S ECURITY P OLICY G ENERATION

One step to limit the possible damage that native code
can do is to isolate it from the Java code using the native
code sandboxing mechanisms discussed in Section II-D. These
TABLE XI.

T OP FIVE MOST COMMON CLASSES OF THE METHODS
INVOKED THROUGH B INDER TRANSACTIONS . F OR THE INTERESTED
READER , WE REPORT THE FULL VERSION OF THIS TABLE IN [1].

Apps

Class

2,427
740
725
327
303

android.os.IServiceManager
android.media.IAudioFlinger
android.media.IAudioPolicyService
android.gui.IGraphicBufferProducer
android.gui.SensorServer

One aspect to be considered is what action is taken when
an unusual syscall is executed. Similar to the design choice
adopted by SELinux, we envision two modes: permissive and
enforcing. In permissive mode, the system would log and
report the usage of unusual behavior, while in enforcing mode
the system would block the execution of such unusual behavior
8

TABLE XV.

S YMBOLS USED TO REPLACE THE ARGUMENTS OF
SYSTEM CALLS .

<USER-PATH>
<SYS-PATH>
<URANDOM-DEV>
<ASHMEM-DEV>
<LOG-DEV>
<NEG INT>
<STD IN/OUT/ERR>
<NON STD FD>
<POS INT>

them and manually analyzed a subset of them. The findings of
this analysis are presented in Section VI.

A file path in the apps’
directory or in the sdcard
A file path different than the
ones represented by <USER-PATH>
“/dev/random” or “/dev/urandom”
“/dev/ashmem”
“/dev/log/system”, “/dev/log/main”,
“/dev/log/events” or “/dev/log/radio”
A negative number
A file descriptor equal 0, 1, or 2
A file descriptor different than 0, 1, or 2
An integer greater than 0

The policies restrict the possible actions of native code,
thus following the principle of least privilege and making it
harder for malicious apps to function. Previously, malicious
code could easily hide in native code to evade static analysis. With our example policies enforced by a sandboxing
mechanism, the native code does not (depending on the exact
threshold) have the ability to perform any malicious actions
in native code, and therefore attackers will have to move the
malicious behavior to the Java code, where it can be found by
existing Java static analysis tools. Furthermore, the policies do
not prevent the correct execution of the dynamically-executed
behavior of many benign apps. Using the rules generated
with the 99% threshold, only 1,483 apps (0.12% of the total
apps in our dataset) would be affected. Of course, as the
dynamic analysis performed by our system is incomplete (in
that it can not execute all possible app code), this number
is a lower bound. This can be alleviated by an organization
wishing to use our system in one of two ways: (1) increase
the completeness of the dynamic analysis or (2) deploying the
sandboxing enforcement mechanism in reporting mode. Both
choices will reveal more app behaviors.

and stop the application. Depending on the context, it might
make sense to use permissive or the more aggressive enforcing
mode. As an alternative, one could selectively pick permissive
or enforcing mode depending on whether the unusual syscall is
well-known to be used by root exploits. The policy generation
process for syscalls is described in Section V-A, while the one
for Java methods is described in Section V-B. We discuss the
possibilities and the implications of this choice in Section VI.
It is worth noting that while this will not guarantee perfect
protection from attacks, by applying the security principle of
least privilege to the native code, we gain the dual security
benefits of (1) increasing the precision of Java static analysis
and (2) reducing the impact of malicious native code.

Another benefit of enforcing a native code sandboxing
policy is that it would prevent the correct execution of several
root exploits. For this work, we considered the 13 root exploits
reported in Table XVI. These exploits require native code to
be successful. Our example security policy would hinder the
execution of 10 of them. This follows because the policies
attempt to reduce the attack surface of the OS for native code,
while at the same time maintaining backward compatibility.
Table XVI presents which of the considered exploits are
successfully blocked, along with which entry of the policy
they violate.

A. System Calls
Based on the system calls performed by the apps in
native methods, in native activities, during libraries loading,
and by programs executed by Exec methods, our system can
automatically generate a security policy of allowed system
calls. To compile this list, we first normalize the parameters
of the system calls and later iterate over them, selecting the
ones performed by most apps, until the list of selected system
calls is comprehensive enough to allow at least a (variable
threshold) percentage of the apps that executed native code
to run properly. In Android, inter-process communication is
done through Binder. Native code can directly use Binder
transactions to call methods implemented by system services.
At the system call level, these calls are performed by the
using the ioctl system call. To consider these actions in our
automatically generated whitelist, we substitute ioctl calls
to Binder with the Binder transactions performed by the apps.

The root exploits that are prevented by our example
security policy are blocked due to rules related to four
system calls, namely socket, perf_event_open,
symlink, and ioctl. More precisely, two exploits
need to create sockets with PF_NETLINK domain and
NETLINK_KOBJECT_UEVENT (15) protocol, however, the
rules only allow PF_NETLINK sockets with protocol 0.
One of the exploits needs the perf_event_open system
call, which is not allowed by the policy. Two exploits
need to create symbolic links that target system files or
directories, but the policy only allows symbolic links to
target “USER-PATH,” which means files or directories in
the app’s directory or in the SD Card. Finally, five exploits
use ioctl to communicate with a device. One of the rules
allows ioctl calls to any device, namely ioctl(<NON
STD FD>,SNDCTL_TMR_TIMEBASE or TCGETS,*).
However, this rule specifies the valid request value (the
second parameter), whereas the exploits use different values,
therefore they would be blocked.

To understand the possible policies that could be generated,
we performed this process using a threshold (the percentage
of apps that use native code whose dynamically-executed
behavior would function properly when enforcing this policy)
of 99%. Table XIV presents the actions obtained by this
procedure. The system call arguments that were normalized
were replaced by symbols in the form <*> and * (meaning
anything). Some of the arguments that are file descriptors were
changed to a file path representation of it. All arguments that
were not normalized represent a numeric value or a constant
value that was converted by strace to a string representation.
For the system calls that do not have the arguments next to
it in the policies, the policy accepts calls with any arguments.
Table XV presents more details about the symbols used.

The table also reports the details about the three exploits
that would not be currently blocked. In one case (CVE-20111149), the exploit would still work because our example policy
allows the invocation of the mprotect syscall, since it is used
by benign applications. In the two remaining cases (RATC
and Zimperlinch), the exploits rely on repeatedly invoking the
fork syscall to exhaust the number of available processes.

To better understand which types of apps would be blocked
by our example policy (when in enforcing mode), we studied
9

TABLE XIV.

A LLOWED SYSTEM CALLS AUTOMATICALLY GENERATED USING A THRESHOLD OF 99% APPS UNAFFECTED BY THE POLICY.

accept(*,*,*)
access(<SYS-PATH>, W OK)
access(<USER-PATH>,R OK)

access(<SYS-PATH>, F OK)
access(<SYS-PATH>,X OK)
access(<USER-PATH>,
R OK|W OK|X OK)

access(<SYS-PATH>,R OK)
access(<USER-PATH>, F OK)
bind

BINDER(android.os.IServiceManager.
brk
cacheflush(*,*,0,*,*)
CHECK SERVICE TRANSACTION)
cacheflush(*,*,0,0,*)
chdir
chmod(<USER-PATH>,*)
clone(child stack=*,flags=CLONE VM|CLONE FS|CLONE FILES|CLONE SIGHAND|CLONE THREAD|CLONE SYSVSEM)
connect(*,{sa family=AF UNIX,
connect(*,{sa family=AF UNIX,
connect(*,{sa family=AF INET,*,*},*)
path=@”jdwp-control”},*)
path=@”android:debuggerd”},*)
connect(*,{sa family=AF UNIX,
dup
dup2
path=<SYS-PATH>},*)
epoll create(*)
epoll ctl(*,*,*,*)
epoll wait
execve
exit(<NEG INT>)
exit(0)
exit group(<POS INT>)
exit group(0)
fcntl64(<NON STD FD>,F DUPFD,*)
fcntl64(<NON STD FD>,F GETFD)
fcntl64(*,F GETFL)
fcntl64(<NON STD FD>,F SETFD,*)
fcntl64(<NON STD FD>,F SETFL,*) fcntl64(<NON STD FD>,F SETLK,*)
fdatasync(*)
fork
fstat64
fsync(*)
ftruncate(*,*)
futex
getcwd
getegid32
geteuid32
getgid32
getpeername
getpgid(0)
getpid
getppid
getpriority(PRIO PROCESS,*)
getrlimit(RLIMIT DATA,*)
getrlimit(RLIMIT NOFILE,*)
getrlimit(RLIMIT STACK,*)
getrusage(RUSAGE CHILDREN,*)
getrusage(RUSAGE SELF,*)
getsockname
getsockopt(*,SOL SOCKET,SO ERROR,*,*)
getsockopt(*,SOL SOCKET,
getsockopt(*,SOL SOCKET,SO RCVBUF,*,*) gettid
SO PEERCRED,*,*)
getuid32
ioctl(<ASHMEM-DEV>,*,*)
ioctl(*,FIONBIO,*)
ioctl(<LOG-DEV>,*,*)
ioctl(*,SIOCGIFADDR,*)
ioctl(*,SIOCGIFBRDADDR,*)
ioctl(*,SIOCGIFCONF,*)
ioctl(*,SIOCGIFFLAGS,*)
ioctl(*,SIOCGIFHWADDR,*)
ioctl(<STD IN/OUT/ERR>, SNDCTL TMR
ioctl(*,SIOCGIFINDEX,*)
ioctl(*,SIOCGIFNETMASK,*)
TIMEBASE or TCGETS, *)
ioctl(*,SNDCTL TMR TIMEBASE
ioctl(<URANDOM-DEV>,
listen
or TCGETS,*)
SNDCTL TMR TIMEBASE or TCGETS,*)
lseek(*,*,SEEK CUR)
lseek(*,*,SEEK END)
lseek(*,*,SEEK SET)
lstat64
madvise(*,*,MADV DONTNEED)
madvise(*,*,MADV NORMAL)
madvise(*,*,MADV RANDOM)
mkdir(<SYS-PATH>,*)
mkdir(<USER-PATH>,*)
mmap2
mprotect
mremap(*,*,*,MREMAP MAYMOVE)
munmap
nanosleep
open(<SYS-PATH>,*,*)
open(<SYS-PATH>,*)
open(<USER-PATH>,*,*)
open(<USER-PATH>,*)
pipe
poll
prctl(PR GET NAME,*,0,0,0)
prctl(PR SET NAME,*,*,*,*)
prctl(PR SET NAME,*,*,*,0)
prctl(PR SET NAME,*,0,0,0)
ptrace(PTRACE TRACEME,*,0,0)
readlink(<USER-PATH>,*,*)
recvfrom
recvmsg
rename(<USER-PATH>,<USER-PATH>)
rmdir(<USER-PATH>)
rt sigprocmask(SIG BLOCK,*,*,*)
rt sigprocmask(SIG SETMASK,*,*,*)
rt sigreturn(*)
rt sigtimedwait([QUITUSR1],
sched getparam
sched getscheduler
NULL, NULL, 8)
sched yield
select
sendmsg
sendto
setitimer(ITIMER REAL,*,*)
setpriority(PRIO PROCESS,*,<POS INT>)
setpriority(PRIO PROCESS,*,0)
setrlimit(RLIMIT NOFILE,*)
setsockopt(*,SOL IP,*,*,*)
setsockopt(*,SOL SOCKET,*,*,*)
set tls(*,*,*,*,*)
set tls(*,*,0,*,*)
sigaction
sigprocmask(SIG BLOCK,*,*)
sigprocmask(SIG SETMASK,*,*)
sigprocmask(SIG UNBLOCK,*,*)
sigreturn
sigsuspend([])
socket(PF INET,SOCK DGRAM,
socket(PF INET,SOCK DGRAM,
socket(PF INET,SOCK DGRAM,
IPPROTO ICMP)
IPPROTO IP)
IPPROTO UDP)
socket(PF INET,SOCK STREAM,
socket(PF INET,SOCK STREAM,
socket(PF NETLINK,SOCK RAW, 0)
IPPROTO IP)
IPPROTO TCP)
socket(PF UNIX, SOCK STREAM, 0) stat64
statfs64(<SYS-PATH>,*)
statfs64(<USER-PATH>,*)
symlink(<USER-PATH>,<USER-PATH>)
tgkill(*,*,SIGTRAP)
umask
uname
unlink(<USER-PATH>)
utimes
vfork
wait4

10

TABLE XVI.

T HIS TABLE SHOWS THE LIST OF CONSIDERED ROOT EXPLOITS , ON WHICH SYSCALL - LEVEL BEHAVIOR THEY RELY, AND WHICH
EXPLOITS ARE SUCCESSFULLY BLOCKED BY OUR POLICY.

Name / CVE
Exploid (CVE-2009-1185)
GingerBreak (CVE-2011-1823)
CVE-2013-2094
Vold/ASEC [34]
RATC (CVE-2010-EASY)
CVE-2013-6124
CVE-2011-1350
Zimperlinch
CVE-2011-1352
CVE-2011-1149
CVE-2012-4220
CVE-2012-4221
CVE-2012-4222

Description
Needs a NETLINK socket with NETLINK_KOBJECT_UEVENT protocol
Needs a NETLINK socket with NETLINK_KOBJECT_UEVENT protocol
Uses perf_event_open system call
Creates symbolic link to a system directory
Relies on invoking many times the fork syscall
Creates symbolic links to system files
ioctl call used violates our rules
Relies on invoking many times the fork syscall
ioctl call used violates our rules
It relies on the mprotect syscall
ioctl call used violates our rules
ioctl call used violates our rules
ioctl call used violates our rules

Blocked
Yes
Yes
Yes
Yes
No
Yes
Yes
No
Yes
No
Yes
Yes
Yes

TABLE XVII.

L IST OF ALLOWED METHODS (JAVA METHODS CALLED
FROM NATIVE CODE ) AUTOMATICALLY GENERATED FOR ALLOWING A
MINIMUM OF 97%, 98% AND 99% OF APPS THAT REACHED NATIVE CODE .

The fork syscall is allowed by our policy as some benign
applications do use it. However, note that this kind of exploit
could be blocked by a security policy that would take into
account the frequency of invocations of a given syscall: In fact,
no benign application would ever invoke the fork syscall so
frequently. We believe that considering this additional aspect
of native code behavior is a very interesting direction for future
work.

Allowed
apps (%)
97
97
97
98
98
98
98
98
98
98
98
98
98
98
98
98
98
99
99
99
99
99
99
99
99
99
99

Although our example security policy does not block all
exploits, we believe the adoption of native sandboxing to
be useful. In fact, it does sensibly reduce the attack surface
available to native code components, and it is able to successfully block a number of root exploits. Similarly, we believe
that useful policies can be generated by our dynamic analysis
system that will be able to block future exploits.
B. Java Methods
Even with the system call restrictions, native code can still
perform dangerous actions by invoking Java methods. This can
be accomplished by using certain JNI functions, as discussed in
Section III-B. Static analysis of the Java component of apps
cannot identify these calls, therefore, the possibility of apps
calling methods in Java libraries poses a threat to the system
and can be abused by malicious apps.
We performed the same process presented in Section V-A
to automatically generate policies that restrict the use of
methods in Java libraries. Table XVII presents these policies,
using different values as the minimum percentage of allowed
apps that reached native code during dynamic analysis. We
used 97%, 98%, and 99% as the values for the minimum.
The methods authorized for each threshold include the ones
associated with lower thresholds.

Method
java.lang.Integer.doubleValue
android.content.ContextWrapper.getPackageName
java.lang.String.getBytes
java.lang.Double.doubleValue
android.content.ContextWrapper.getClassLoader
android.content.ContextWrapper.getFilesDir
java.io.File.getPath
android.content.ContextWrapper.getExternalFilesDir
android.view.WindowManagerImpl.getDefaultDisplay
java.lang.String.toLowerCase
android.app.Activity.getWindowManager
java.util.ArrayList.add
android.view.Display.getMetrics
android.app.Activity.getWindow
android.view.View.getWindowVisibleDisplayFrame
java.util.Calendar.getInstance
android.view.View.getDrawingRect
java.util.Calendar.get
android.os.Bundle.getByteArray
android.content.ContextWrapper.getPackageManager
android.content.res.AssetManager$AssetInputStream.read
java.lang.Long.doubleValue
java.lang.ClassLoader.loadClass
android.app.ApplicationPackageManager.getPackageInfo
android.content.res.AssetManager$AssetInputStream.close
java.lang.Float.doubleValue
java.lang.Class.getClassLoader

VI.

I MPACT OF S ECURITY P OLICIES

Considering both our policies—Java methods and system
calls—, and the 99% threshold, we would block 0.23% (2,730)
of all the apps in our dataset. To understand what the impact
of implementing (and enforcing with the strictest enforcement
mechanism) these policies would be on users, we analyzed the
popularity (lower number of installations) of the apps whose
behavior seen during the dynamic analysis would be blocked.
Figure 2 presents the cumulative distribution of the popularity
of the apps that would be blocked. As the figure shows, among
the applications for which our policy would block at least one
behavior that has been executed at runtime, 1.87% (51) of them
have more than 1 million installations.

Using the list of apps associated with a minimum of
allowed apps of 99% (the most permissive of our thresholds), we would block 1,414 apps (0.12%). The method
java.lang.ClassLoader.loadClass, which is allowed when using 99% as a threshold, causes the invocation
of the static initialization block (<clinit>) of a class.
Therefore, it could be used to execute the static initialization
block of classes in Java libraries. However, as far as we know,
these blocks do not contain important operations that need to
be contained.
11

1.0

VII.

DYNAMIC C OVERAGE

0.6

0.8

Dynamic analysis is inherently incomplete, and in this
section we attempt to measure the code coverage of the
dynamic analysis that we used, using function coverage of the
Java code and function coverage of the native code. Both code
coverage methods have large overhead, so we were only able
to analyze a subset of the apps.
A. Java Method Code Coverage

0.2

0.4

To measure the code coverage based on the Java methods
executed, we instrumented the DVM. The instrumented code
records the execution of every method of the app under
analysis. Since this instrumentation introduces more overhead
and slows the emulator, we did the experiment with 25,000
apps randomly selected and used a kernel driver, instead
of strace, to record the system calls executed. The code
coverage obtained was 8.31%

0.0

1.87 % −> 1M+

1e+00

1e+02

1e+04

1e+06

1e+08

B. Native Code Coverage

Number of installs

While code coverage of the Java methods allows us to
gain insight into the high level code coverage of our dynamic
analysis system, it does not shed light on the core issue we are
interested in: how much of an app’s native code is the dynamic
analysis able to execute? To answer this question, we modified
both the Android emulator and the Android framework to
support measuring function coverage of the native code.

Fig. 2. Popularity of apps that would be blocked by enforcing our policy.
X-axis is in logarithmic scale, and the Y -axis is the percentage of apps that
would be blocked.

One technical challenge here is that the native code coverage must understand not only which native libraries are
loaded by an app, but also which part of the native library
is actually executed. Thus we need to: (1) trace the executed
native functions and (2) statically determine the total number
of native functions. This will allow us to calculate the function
coverage of the native code.

Because manual analysis is very time-consuming, we did
not perform it on all blocked apps. However, we did a general
investigation of the blocked apps and manually analyzed the
ones that showed traces of suspicious behavior. We identified
three types of suspicious activities among these apps, and we
discuss them here.

To the best of our knowledge, there is no previously
released tool to trace the execution of the native code of an app.
Android Open Source Project implements a tracing mechanism
since version 4.4. This tracing mechanism is implemented
using a kernel device called qemutrace that is part of the
goldfish kernel. The kernel send information to assist the
emulator to trace correctly the execution, e.g., the PID of the
running process each time there is a context switch, a message
that notifies that a fork or an execve is executed, etc. The whole
tracing system significantly slows down the performance of the
emulator. However, this tracing system is too general: we are
interested only in the execution of the native code of a specific
app. We need to trace only functions of loaded libraries of the
app under analysis.

Ptrace. Overall, 280 apps used ptrace. 276 of these only
call ptrace to trace itself without checking the result. We
assume that the developers do this as a defensive measure to
prevent the analysis of the app, because an app cannot be
traced by another process if there is already a process tracing
it. Therefore, for these 276 apps we believe that the app’s
functionality would remain intact with our policy. Four apps,
on the other hand, create a child process, which try to attach
ptrace to the parent, checking the result of the call and
changing behavior if the call failed.
Modifying Java code. We identified 7 apps that modify the
Java section of the app from native code. All these apps perform this action from the library libAPKProtect.so [3].
This library is provided by an obfuscation service, thus making
it harder for reverse engineering tools to decompile the app.
This functionality can also be used by malicious apps and
illustrates the importance of isolating native code.
Fork and inotify. We identified 57 apps that create a child
process in native code and use inotify to monitor the apps’
directory, in order to identify when they are uninstalled. In fact,
the spawned child process uses inotify to detect when the
app is uninstalled and, when this happens, it opens a survey in
the browser. This behavior is not a malicious action; however,
executing code after being uninstalled is suspicious, as the user
does not expect the app to be running after being uninstalled.

For this reason, we created two ways to limit the tracing to
the interesting part only. First, we only want to trace processes
with a specific UID because each app in Android is executed
with it own UID. In addition, we are interested only in portions
of the executable memory where the native libraries have been
loaded.
To inform the emulator about the UID of the currently
executing process we leverage the existing qemutrace device. We added the UID into the message sent for each
context switch. To send the information about the map of
the memory to the emulator we cannot use the qemutrace
12

After the dynamic execution, we compute the code coverage using all the data gathered during the execution. We use
IDA Pro to find all functions boundaries of libraries. Then, we
use the map of the memory to translate the virtual addresses
traced by the emulator. Next, we flag all the functions whose
boundaries include at least one address of the trace. The code
coverage is then calculated.

Library

device, since it can only pass 32 bit integers as messages.
Moreover, we also need a mechanism to extract the libraries
from the emulated system. To solve both problems we instrumented the Android framework. We found that the function
java.lang.Runtime.doLoad is able to intercept all the
loading operations. Our hook inside the doLoad function
blocks the loading (and the app) while syncing all the gathered
data to the external emulator. The mapping of the memory and
the PID are read from /proc/self/. The path of the loaded
library is one of the parameters of the doLoad function.
Hence, when doLoad returns, the emulator knows the address
space reserved for the new library, and the content of the native
library.

Our tracing system slows down the execution of the apps by
around 10 times. Therefore, we only ran it on a small subset
of the apps, more specifically, we analyzed 177. The code
coverage of most libraries is less that 1%. Some small libraries,
on the other hand, were covered by 100%. Furthermore,
the average coverage was 7%. More details about executed
libraries and coverage can be seen in Figure 3.
VIII.

libstarwisp-core.so
libnarumiengine.so 1
libjnimain.so
libHealthGuide.so 1
libgl2jni.so 1
libmame4all-jni.so
1
libprivateProperty.so
1
libdevicescape-jni.so
libobjc.so
libgideros.so 1
libopenal.so 13
libgame.so 13
libsoundpool.so
43
libjnlua5.1.so 20
libSFT.so
1
libadvanced_memory_booster_ii.so
1
librompecabezashyorin.so 1
libvi_voslib.so 2
libCocoonJSLib.so 3
libcocos2dcpp.so 5
libcabs.so 1
libmain.so 1
libDevStudio.so
1
libfelpay.so
1
liblocSDK3.so
libSimplePlayer.so 1
librompecabezascallofduty.so 1
libyoyo.so 9
libeasy3d_utils.so 2
libstlport_shared.so
liblingsAudio.so 2
libBMapApiEngine_v1_3_5.so
3
libDahuaEncrypt.so
libkeygen.so
libCore.so
41
libdictdroid.so
libqzAudio.so 1
libaacdecoder.so 6
libBaiduMapSDK_v2_3_1.so
1
librhodes.so
1
libapp_BaiduMapApplib_v2_1_2.so 2
libUtils.so
1
libOpenAL_android.so 1
libmonodroid.so
libgsengine.so
2
libnobexjni.so 3
0.0
0.1

1
1

1

1

1

16

1

1
1

11

0.2

0.3

Coverage

0.4

0.5

0.6

Fig. 3. Per library coverage of executed functions. Horizontal axis contains
libraries name, vertical, instead contains the function coverage. For each bar
we also show the number of libraries that has been found in all executed
applications

T HREATS TO VALIDITY

Our study is affected by a few limitations, which we discuss
in this section. An intrinsic limitation of the automaticallygenerated security policies is that we base their automatic
generation on data and insights obtained by means of dynamic
analysis, which is well-known to be incomplete and affected by
code coverage issues. In fact, dynamic analysis does not ensure
that all native code is exercised in the apps that actually use
it, and for those apps that used native code, dynamic analysis
may not have exercised all code paths in the native code.
Consequently, the policies that our tool generated might not be
complete, they might block more applications when adopted at
large-scale, and the performance overhead of isolating native
code could be higher. However, using a more-sophisticated
instrumentation tool could possibly improve the amount of
native code behavior that our system observes, or deploying
the automatically generated policies in a native sandbox with
reporting mode would help to observe the behaviors that the
policies would block.

sophisticated analysis techniques or increased resources, or by
obtaining the actual behavior of native code in the wild, by
instrumenting real-world Android devices.
Another limitation is that our approach restricts access to
permissions from native code, but it still allows the native
code to invoke (some) Java methods. This aspect would make,
in principle, Java-only analysis more precise, but still not
completely sound, as a malicious application could introduce
hidden execution paths by invoking a native method, which,
in turn, could invoke a Java method. However, we note that
our automatically-generated policy only allows native code to
invoke a very narrow subset of Java methods defined in the
Android framework (Table XVII), through which it is virtually
impossible to perform any security-sensitive operation. Thus,
our policy, although not perfect, would drastically reduce the
possibility of introducing malicious behaviors.

Nonetheless, we believe this work to be a significant first
step in a very important direction. In fact, to the best of our
knowledge, this work is the first, largest, and most comprehensive study on how real-world applications use native code. Our
results demonstrate that it is infeasible to adopt a completely
restrictive sandboxing policy. In addition, we propose a system
to automatically generate a native code sandboxing policy
following a data-driven approach. This system could be used
by large organizations that are interested in automatically
generating a native code sandboxing policy. Furthermore, the
completeness issues could possibly be addressed by increasing
the fidelity of the dynamic analysis, either through more

Lastly, we consider all the apps we obtained from Google
Play as benign, but we cannot be completely certain that there
are no malicious apps among them. The effects of having
malicious apps in our dataset vary depending on how the
malware works. In the worst case it could cause our policies
to allow some malicious actions.
13

IX.

state, this feature does more harm than good and that native
code sandboxing is the correct approach to properly limit
its potentially malicious side-effects. However, a native code
sandboxing mechanism without a proper policy will never be
feasible. We hope that, in addition to shedding light on the
previously unknown native code usage of Android apps, this
paper demonstrates an approach to automatically generate an
effective and practical native code sandboxing policy.

R ELATED W ORK

In this section we relate our work to the vast amount of
research published in the field of Android security.
Large Measurement Studies. Several works have analyzed
large datasets of Android apps, but with goals that differ
from ours. Viennot et al. [37] did a large measurement study
on 1,100,000 applications crawled from the Google Play app
store. In particular, they collected meta-data and statistics taken
from the Google Play store itself. As part of their study, they
measured the frequency with which Android applications make
use of native code components. Another important measurement study has been performed by Lindorfer et al. [27]. In
their work, they analyzed 1,000,000 apps, of which 40% are
malware. To perform the analysis, the authors used Andrubis,
a publicly-available analysis system for Android apps that
combines static and dynamic analysis. When focusing on
native code, our work significantly extends their study.

ACKNOWLEDGMENT
This material is based upon work supported by CAPES
Foundation under Award No. BEX 12269/13-1, by NSF under
Award No. CNS-1408632, by DHS under Award No. 2009-ST061-CI0001, and by Secure Business Austria. Any opinions,
findings, and conclusions or recommendations expressed in
this publication are those of the author(s) and do not necessarily reflect the views of CAPES Foundation, NSF, DHS,
or Secure Business Austria.

Application Analysis Systems. Several systems have been
proposed to perform behavioral analysis of Android applications based on dynamic analysis [13], [14], [30], [31], [36],
[41]. Moreover, several other works have been proposed to
identify malicious Android apps [4], [9], [23]. Our analysis
complements all these research efforts by performing a large
scale study, based on dynamic analysis, specifically focused
on native code usage.

This material is also based on research sponsored by
DARPA under agreement number FA8750-12-2-0101. The
U.S. Government is authorized to reproduce and distribute
reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained
herein are those of the authors and should not be interpreted as
necessarily representing the official policies or endorsements,
either expressed or implied, of DARPA or the U.S. Government.

Protection Systems. Fedler et al. [15] proposes a protection
system from root exploits by preventing apps from giving
execution permission for custom executable files and by introducing a permission related to the use of the System
class. PREC [24] is a framework intended to protect Android
systems from root exploits. PREC uses two steps, learning and
enforcement. During the learning phase, the analysis generates
a model of the normal behavior for a given app. Then, during
the enforcement phase, the system makes sure that the app
does not deviate from the normal behavior. Our work has the
advantage that the generated policies can be applied to all apps,
whereas PREC generates per-app models. Hence, our results
are more general. Moreover, our analysis also monitors, in
addition to system calls, JNI function calls, Binder transactions
and calls from Java to native methods.

R EFERENCES
[1]

[2]
[3]
[4]

Native Code Isolation. Another way to protect the system
is by isolating native code. The challenge of isolating native
code components used by managed languages has been extensively studied. For instance, Klinkoff et al. [26] focus on the
isolation of .NET applications, whereas Robusta [33] focuses
on the isolation of native code used by Java applications.
Recently, NativeGuard [35] proposed a similar mechanism to
isolate native code in the context of Android. Our work is
complementary to these sandboxing mechanisms and fills the
knowledge gap necessary to define security policies on the
execution of native code in Android that are both usable in realworld applications and effective in blocking malicious behavior
of native components.
X.

[5]

[6]

[7]

[8]

C ONCLUSION
[9]

While allowing developers to mix Java code and native code enables developers to fully harness the computing
power of mobile devices, we believe that, in the current
14

V. Afonso, A. Bianchi, Y. Fratantonio, A. Doupé, M. Polino, P. de Geus,
C. Kruegel, and G. Vigna, “Full version of Tables 5, 6, 7, 8, and
11.” [Online] Available: https://github.com/ucsb-seclab/android going
native.
AppBrain, “Number of Available Android Applications,” [Online]
Available: http://www.appbrain.com/stats/number-of-android-apps.
A. Apvrille and R. Nigam, “Obfuscation in Android Malware, and How
to Fight Back,” in Virus Bulletin, 2014.
D. Arp, M. Spreitzenbarth, M. Hübner, H. Gascon, and K. Rieck,
“DREBIN: Effective and Explainable Detection of Android Malware
in Your Pocket,” in Proceedings of the 21st Annual Network and
Distributed System Security Symposium (NDSS), 2014.
S. Arzt, S. Rasthofer, C. Fritz, E. Bodden, A. Bartel, J. Klein, Y. Le
Traon, D. Octeau, and P. McDaniel, “FlowDroid: Precise Context, Flow,
Field, Object-sensitive and Lifecycle-aware Taint Analysis for Android
Apps,” in Proceedings of 35th annual ACM SIGPLAN conference on
Programming Language Design and Implementation (PLDI), 2014.
K. W. Y. Au, Y. F. Zhou, Z. Huang, and D. Lie, “Pscout: Analyzing the
Android Permission Specification,” in Proceedings of the 2012 ACM
conference on Computer and Communications Security (CCS), 2012.
L. Batyuk, M. Herpich, S. A. Camtepe, K. Raddatz, A.-D. Schmidt,
and S. Albayrak, “Using Static Analysis for Automatic Assessment
and Mitigation of Unwanted and Malicious Activities Within Android
Applications,” in Proceedings of the 2011 6th International Conference
on Malicious and Unwanted Software (MALWARE), 2011.
A. Bittau, P. Marchenko, M. Handley, and B. Karp, “Wedge: Splitting
Applications into Reduced-Privilege Compartments,” in Proceedings
of the 5th USENIX Symposium on Networked Systems Design and
Implementation (NSDI), 2008.
I. Burguera, U. Zurutuza, and S. Nadjm-Tehrani, “Crowdroid: Behaviorbased Malware Detection System for Android,” in Proceedings of the
1st ACM workshop on Security and privacy in smartphones and mobile
devices (SPSM), 2011.

[10]

[11]

[12]

[13]
[14]

[15]

[16]

[17]

[18]

[19]
[20]
[21]
[22]

[23]

[24]

[25]

[26]

[27]

[28]

V. Chebyshev and R. Unuchek, “Mobile Malware Evolution:
2013,”
[Online]
Available:
http://securelist.com/analysis/
kaspersky-security-bulletin/58335/mobile-malware-evolution-2013/,
Feb. 2014.
E. Chin, A. P. Felt, K. Greenwood, and D. Wagner, “Analyzing InterApplication Communication in Android,” in Proceedings of the 9th
international conference on Mobile systems, applications, and services
(MobiSys), 2011.
A. Desnos, “Androguard: Reverse Engineering, Malware and Goodware
Analysis of Android Applications... and More (Ninja!),” [Online] Available: https://code.google.com/p/androguard/.
Droidbox, “Android Application Sandbox,” [Online] Available: https:
//code.google.com/p/droidbox/.
W. Enck, P. Gilbert, B. Chun, L. Cox, J. Jung, P. McDaniel, and
A. Sheth, “TaintDroid: an Information-flow Tracking System for Realtime Privacy Monitoring on Smartphones,” in Proceedings of the 9th
USENIX Symposium on Operating Systems Design and Implementation
(OSDI), 2010.
R. Fedler, M. Kulicke, and J. Schütte, “Native Code Execution Control
for Attack Mitigation on Android,” in Proceedings of the Third ACM
workshop on Security and privacy in smartphones & mobile devices
(SPSM), 2013.
A. P. Felt, E. Chin, S. Hanna, D. Song, and D. Wagner, “Android
Permissions Demystified,” in Proceedings of the 18th ACM conference
on Computer and Communications Security (CCS), 2011.
A. P. Fuchs, A. Chaudhuri, and J. S. Foster, “SCanDroid: Automated
Security Certification of Android Applications,” Manuscript, Univ. of
Maryland, http://www. cs. umd. edu/˜ avik/projects/scandroidascaa,
2009.
C. Gibler, J. Crussel, J. Erickson, and H. Chen, “AndroidLeaks: Detecting Privacy Leaks in Android Applications,” Tech. rep., UC Davis,
Tech. Rep., 2011.
Google, “Android NDK,” [Online] Available: https://developer.android.
com/tools/sdk/ndk/index.html.
——, “UI/Application Exerciser Monkey — Android Developers,” [Online] Available: http://developer.android.com/tools/help/monkey.html.
R. Gordon, Essential JNI: Java Native Interface. Prentice-Hall, Inc.,
1998.
M. Grace, Y. Zhou, Z. Wang, and X. Jiang, “Systematic Detection of
Capability Leaks in Stock Android Smartphones,” in Proceedings of
the 19th Annual Network and Distributed System Security Symposium
(NDSS), 2012.
M. Grace, Y. Zhou, Q. Zhang, S. Zou, and X. Jiang, “Riskranker:
Scalable and Accurate Zero-Day Android Malware Detection,” in
Proceedings of the 10th international conference on Mobile systems,
applications, and services (MobiSys), 2012.
T.-H. Ho, D. Dean, X. Gu, and W. Enck, “PREC: Practical Root Exploit
Containment for Android Devices,” in Proceedings of the 4th ACM
conference on Data and application security and privacy (CODASPY),
2014.
IDC Corporate, “IDC: Smartphone OS Market Share 2014, 2013,
2012, and 2011,” [Online] Available: http://www.idc.com/prodserv/
smartphone-os-market-share.jsp.
P. Klinkoff, E. Kirda, C. Kruegel, and G. Vigna, “Extending .NET
Security to Unmanaged Code,” International Journal of Information
Security, 2007.
M. Lindorfer, M. Neugschwandtner, L. Weichselbaum, Y. Fratantonio,
V. van der Veen, and C. Platzer, “Andrubis - 1,000,000 Apps Later:
A View on Current Android Malware Behaviors,” in Proceedings of
the 3rd International Workshop on Building Analysis Datasets and
Gathering Experience Returns for Security (BADGERS), 2014.
L. Lu, Z. Li, Z. Wu, W. Lee, and G. Jiang, “CHEX: Statically Vetting
Android Apps for Component Hijacking Vulnerabilities,” in Proceed-

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]
[39]

[40]

[41]

[42]

[43]

[44]

[45]

15

ings of the 2012 ACM Conference on Computer and Communications
Security (CCS), 2012.
C. Mann and A. Starostin, “A Framework for Static Detection of Privacy
Leaks in Android Applications,” in Proceedings of the 27th Annual
ACM Symposium on Applied Computing (SAC), 2012.
G. Portokalidis, P. Homburg, K. Anagnostakis, and H. Bos, “Paranoid
Android: Versatile Protection for Smartphones,” in Proceedings of the
26th Annual Computer Security Applications Conference (ACSAC),
2010, pp. 347–356.
C. Qian, X. Luo, Y. Shao, and A. T. Chan, “On Tracking Information
Flows through JNI in Android Applications,” in Proceedings of the 44th
Annual IEEE/IFIP International Conference on Dependable Systems
and Networks (DSN), 2014.
V. Rastogi, Y. Chen, and W. Enck, “AppsPlayground: Automatic Security Analysis of Smartphone Applications,” in Proceedings of the
third ACM conference on Data and application security and privacy
(CODASPY), 2013.
J. Siefers, G. Tan, and G. Morrisett, “Robusta: Taming the Native Beast
of the JVM,” in Proceedings of the 17th ACM conference on Computer
and Communications Security (CCS), 2010.
A. D. . Space, “Local Root Vulnerability in Android 4.4.2,”
[Online] Available: http://blog.cassidiancybersecurity.com/post/2014/
06/Android-4.4.3,-or-fixing-an-old-local-root.
M. Sun and G. Tan, “NativeGuard: Protecting Android Applications
from Third-Party Native Libraries,” in Proceedings of the 2014 ACM
conference on Security and privacy in wireless & mobile networks
(WiSec), 2014.
K. Tam, S. J. Khan, A. Fattori, and L. Cavallaro, “CopperDroid:
Automatic Reconstruction of Android Malware Behaviors,” in Proceedings of the 22nd Annual Network and Distributed System Security
Symposium (NDSS), 2015.
N. Viennot, E. Garcia, and J. Nieh, “A Measurement Study of Google
Play,” in Proceedings of the 2014 ACM SIGMETRICS International
Conference on Measurement and Modeling of Computer Systems (SIGMETRICS), 2014.
C. Warren, “Google Play Hits 1 Million Apps,” [Online] Available:
http://mashable.com/2013/07/24/google-play-1-million/, Jul. 2013.
F. Wei, S. Roy, X. Ou et al., “Amandroid: A Precise and General
Inter-component Data Flow Analysis Framework for Security Vetting of
Android Apps,” in Proceedings of the 2014 ACM SIGSAC Conference
on Computer and Communications Security (CCS), 2014.
L. Weichselbaum, M. Neugschwandtner, M. Lindorfer, Y. Fratantonio,
V. van der Veen, and C. Platzer, “ANDRUBIS: Android Malware Under
The Magnifying Glass,” Vienna University of Technology, Tech. Rep.
TR-ISECLAB-0414-001, 2014.
L. K. Yan and H. Yin, “Droidscope: Seamlessly reconstructing the os
and dalvik semantic views for dynamic android malware analysis,” in
Proceedings of the 21st USENIX Security Symposium, 2012.
Z. Yang and M. Yang, “Leakminer: Detect Information Leakage on
Android with Static Taint Analysis,” in Proceedings of the 2012 Third
World Congress on Software Engineering (WCSE), 2012.
Z. Zhao and F. C. C. Osono, ““TrustDroidTM ”: Preventing the Use of
SmartPhones for Information Leaking in Corporate Networks Through
the Use of Static Analysis Taint Tracking,” in Proceedings of the 2012
7th International Conference on Malicious and Unwanted Software
(MALWARE), 2012.
Y. Zhou, Z. Wang, W. Zhou, and X. Jiang, “Hey, You, Get Off of My
Market: Detecting Malicious Apps in Official and Alternative Android
Markets,” in Proceedings of the 19th Annual Network and Distributed
System Security Symposium (NDSS), 2012.
Y. Zhou and X. Jiang, “Detecting Passive Content Leaks and Pollution
in Android Applications,” in Proceedings of the 20th Annual Network
and Distributed System Security Symposium (NDSS), 2013.

deDacota: Toward Preventing Server-Side XSS
via Automatic Code and Data Separation
Adam Doupé

Weidong Cui

UC Santa Barbara

Microsoft Research

adoupe@cs.ucsb.edu
Marcus Peinado
Microsoft Research

marcuspe@microsoft.com

wdcui@microsoft.com
Christopher Kruegel
UC Santa Barbara

chris@cs.ucsb.edu

Mariusz H. Jakubowski
Microsoft Research

mariuszj@microsoft.com
Giovanni Vigna
UC Santa Barbara

vigna@cs.ucsb.edu

ABSTRACT

1.

Web applications are constantly under attack. They are
popular, typically accessible from anywhere on the Internet,
and they can be abused as malware delivery systems.
Cross-site scripting flaws are one of the most common
types of vulnerabilities that are leveraged to compromise a
web application and its users. A large set of cross-site scripting vulnerabilities originates from the browser’s confusion
between data and code. That is, untrusted data input to
the web application is sent to the clients’ browser, where it
is then interpreted as code and executed. While new applications can be designed with code and data separated from
the start, legacy web applications do not have that luxury.
This paper presents a novel approach to securing legacy
web applications by automatically and statically rewriting
an application so that the code and data are clearly separated in its web pages. This transformation protects the
application and its users from a large range of server-side
cross-site scripting attacks. Moreover, the code and data
separation can be efficiently enforced at run time via the
Content Security Policy enforcement mechanism available
in modern browsers.
We implemented our approach in a tool, called deDacota, that operates on binary ASP.NET applications. We
demonstrate on six real-world applications that our tool is
able to automatically separate code and data, while keeping
the application’s semantics unchanged.

Web applications are prevalent and critical in today’s computing world, making them a popular attack target. Looking
at types of vulnerabilities reported in the Common Vulnerabilities and Exposures (CVE) database [11], web application
flaws are by far the leading class.
Modern web applications have evolved into complex programs. These programs are no longer limited to server-side
code that runs on the web server. Instead, web applications
include a significant amount of JavaScript code that is sent
to and executed on the client. Such client-side components
not only provide a rich and fast user interface, they also
contain parts of the application logic and typically communicate with the server-side component through asynchronous
JavaScript calls. As a result, client-side scripts are an integral component of modern web applications, and they are
routinely generated by server-side code.
There are two kinds of cross-site scripting (XSS) vulnerabilities: server-side and client-side. The latter is essentially
caused by bugs in the client-side code, while the former is
caused by bugs in the server-side code. In this paper we focus on server-side XSS vulnerabilities (unless specified otherwise, we will use XSS to refer to server-side XSS). XSS
vulnerabilities allow attackers to inject client-side scripting
code (typically, JavaScript) into the output of web applications. The scripts are then executed by the browser as
it renders the page, allowing malicious code to run in the
context of the web application. Attackers can leverage XSS
attacks to leak sensitive user information, impersonate the
victim to perform unwanted actions in the context of the
web application, or launch browser exploits.
There has been a significant amount of research effort on
eliminating XSS vulnerabilities. The main line of research
has focused on sanitizing untrusted input [3, 13, 18, 22, 25,
27, 34, 38, 40, 44, 46, 48–50]. Sanitization attempts to identify
and “clean up” untrusted inputs that might contain JavaScript code. Performing correct sanitization is challenging,
for a number of reasons. One reason is that it is difficult
to guarantee coverage for all possible paths through the application [3, 48]. As part of this problem, it is necessary to
find all program locations (sources) where untrusted input
can enter the application, and then verify, along all program
paths, the correctness of all sanitization functions that are
used before the input is sent to the client (sinks). Furthermore, it is not always clear how to properly sanitize data,
because a single input might appear in different contexts in
the output of the application [40].

Categories and Subject Descriptors
D.2.5 [Testing and Debugging]

Keywords
Static Analysis; Cross-Site Scripting; XSS; Content Security
Policy; CSP
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright 2013 ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516708.

INTRODUCTION

The root cause of XSS vulnerabilities is that the current
web application model violates the principle of code and data
separation. In the case of a web page, the data is the HTML
content of the page and the code is the JavaScript code.
Mixing JavaScript code and HTML data in the same channel
(the HTTP response) makes it possible for an attacker to
convince a user’s browser to interpret maliciously crafted
HTML data as JavaScript code. While sanitization tries to
turn untrusted input, which could potentially contain code,
into HTML data, we believe the fundamental solution to
XSS is to separate the code and data in a web page—the
way HTML and JavaScript should have been designed from
the start. Once the code and data are separated, a web
application can communicate this separation to the browser,
and the browser can ensure no code is executed from the data
channel. Such communication and enforcement is supported
by the new W3C browser standard Content Security Policy
(CSP) [42].
While new web applications can be designed with code
and data separated from the start, it has been a daunting
task to achieve code and data separation for legacy applications. The key challenge is to identify code or data in the
output of a web application. Previous solutions have relied
on either developers’ manual annotations or dynamic analysis. For example, BEEP [20] requires developers to manually
identify inline JavaScript code. Blueprint [28] requires developers to manually identify the data by specifying which
application statements could output untrusted input. XSSGUARD dynamically identifies application-intended JavaScript code in a web page by comparing it with a shadow
web page generated at run time [4]. The main problem
preventing these solutions from being adopted is either the
significant manual effort required from application developers or the significant runtime performance overhead. In fact,
Weinberger et al. [47] showed how difficult it is to manually
separate the code and data of a web application.
In this paper, we present deDacota, the first system that
can automatically and statically rewrite an existing web application to separate code and data in its web pages. Our
novel idea is to use static analysis to determine all inline
JavaScript code in the web pages of an application. Specifically, deDacota performs static data-flow analysis of a
given web application to approximate its HTML output.
Then, it parses each page’s HTML output to identify inline JavaScript code. Finally, it rewrites the web application to output the identified JavaScript code in a separate
JavaScript file.
The problem of statically determining the set of (HTML)
outputs of a web application is undecidable. However, as we
observe in our evaluation, the problem is typically tractable
for real-world web applications. These applications are written by benign developers and tend to have special properties that allow us to compute their outputs statically. For
instance, the majority of the inline JavaScript code is static
in the web applications we tested.
Dynamic inline JavaScript presents a second-order problem. Here, the JavaScript code itself (rather than the HTML
page) is generated dynamically on the server and may depend on untrusted inputs. Again, the potential for XSS
vulnerabilities exists. deDacota provides a partial solution
to this problem by producing alerts for all potentially dangerous instances of dynamic JavaScript generation in the

application and by safely sanitizing a large subclass of these
instances.
We implemented a prototype of deDacota to analyze and
rewrite ASP.NET [31] web applications. We then applied
deDacota to six open-source, real-world ASP.NET applications. We verified that all known XSS vulnerabilities are
eliminated. We then performed extensive testing to ensure
that the rewritten binaries still function correctly. We also
tested deDacota’s performance and found that the page
loading times between the original and rewritten application are indistinguishable.
The main contributions of this paper are the following:
• A novel approach for automatically separating the code
and data of a web application using static analysis
(Section 4).
• A prototype implementation of our approach, deDacota, applied to ASP.NET applications (Section 5).
• An evaluation of deDacota, showing that we are able
to apply our analysis to six real-world, open-source,
ASP.NET applications. We show that our implementation prevents the exploitation of know vulnerabilities and that the semantics of the application do not
change (Section 6).

2.

BACKGROUND

In this section, we provide the background necessary for
understanding the design of deDacota.

2.1

Cross-Site Scripting

Modern web applications consist of both server-side and
client-side code. Upon receiving an HTTP request, the
server-side code, which is typically written in a server-side
language, such as PHP or ASP.NET, dynamically generates
a web page as a response, based on the user input in the
request or data in a backend database. The client-side code,
which is usually written in JavaScript and is executed by
the browser, can be either inline in the web page or external
as a standalone JavaScript file.
Cross-site scripting (XSS) vulnerabilities allow an attacker
to inject malicious scripts into web pages to execute in the
client-side browser, as if they were generated by the trusted
web site. If the vulnerability allows the attacker to store malicious JavaScript on the server (e.g., using the contents of
a message posted on a newsgroup), the vulnerability is traditionally referred to as “stored” or “persistent XSS.” When
the malicious code is included in the request and involuntarily reflected to the user (copied into the response) by the
server, the vulnerability is called “reflected XSS.” Finally, if
the bug is in the client-side code, the XSS vulnerability is
referred to as “DOM-based XSS” [24]. We call the first two
types of vulnerabilities “server-side XSS vulnerabilities” and
the latter “client-side XSS vulnerabilities.”
The root cause for server-side XSS is that the code (i.e.,
the client-side script) and the data (i.e., the HTML content)
are mixed together in a web page. By crafting some malicious input that will be included into the returned web page
by the server-side code, an attacker can trick the browser
into confusing his data as JavaScript code.

2.2

Code and Data Separation

The separation of code and data can be traced back to
the Harvard Architecture, which introduces separate storage and buses for code and data. Separating code and data
is a basic security principle for avoiding code injection attacks [19]. Historically, whenever designs violate this principle, there exists a security hole. An example is the stack
used in modern CPUs. The return addresses (code pointers) and function local variables (data) are co-located on
the stack. Because the return addresses determine control
transfers, they are essentially part of the code. Mixing them
together with the data allows attackers to launch stack overflow attacks, where data written into a local variable spills
into an adjacent return address. In the context of web applications, we face the same security challenge, this time
caused by mixing code and data together in web pages. To
fundamentally solve this problem, we must separate code
and data in web pages created by web applications.

2.3

Content Security Policy

Content Security Policy (CSP) [42] is a mechanism for
mitigating a broad class of content injection vulnerabilities
in web applications. CSP is a declarative policy that allows a
web application to inform the browser, via an HTTP header,
about the sources from which the application expects to load
resources such as JavaScript code. A web browser that implements support for CSP can enforce the security policy
declared by the web application.
A newly developed web application can leverage CSP to
avoid XSS by not using inline JavaScript and by specifying
that only scripts from a set of trusted sites are allowed to
execute on the client. Indeed, Google has required that all
Chrome browser extensions implement CSP [1]. However,
manually applying CSP to a legacy web application typically requires a non-trivial amount of work [47]. The reason
is that the authors of the web application have to modify
the server-side code to clearly identify which resources (e.g.,
which JavaScript programs) are used by a web page. Moreover, these scripts have to be separated from the web page.
CSP essentially provides a mechanism for web browsers
to enforce the separation between code and data as specified by web applications. Our work solves the problem of
automatically transforming legacy web applications so that
the code and data in their web pages are separated. The
transformed web applications can then directly leverage the
browser’s support for CSP to avoid a wide range of XSS
vulnerabilities.

3.

THREAT MODEL

Before discussing the design of deDacota, we need to
state our assumptions about the code that we are analyzing
and the vulnerabilities we are addressing.
Our approach involves rewriting a web application. This
web application is written by a benign developer—that is,
the developer has not intentionally obfuscated the code as a
malicious developer might. This assumption also means that
the JavaScript and HTML are benign and not intentionally
taking advantage of browser parsing quirks (as described in
Blueprint [28]).
deDacota will only prevent server-side XSS vulnerabilities. We define server-side XSS vulnerabilities as XSS vulnerabilities where the root cause of the vulnerability is in

server-side code. Specifically, this means XSS vulnerabilities where unsanitized input is used in an HTML page. We
explicitly do not protect against client-side XSS vulnerabilities, also called DOM-based XSS. Client-side XSS vulnerabilities occur when untrusted input is interpreted as JavaScript by the client-side JavaScript code using methods such
as eval, document.write, or innerHTML. The root cause of
these vulnerabilities is in the JavaScript code.
In this paper, we focus solely on separating inline JavaScript code (that is, JavaScript in between <script> and
</script>). While there are other vectors where JavaScript
can be executed, such as JavaScript code in HTML attributes (event handlers such as onclick) and inline Cascading Style Sheet (CSS) styles [16], the techniques described
here can be extended to approximate and rewrite the HTML
attributes and inline CSS.
Unfortunately, code and data separation in an HTML
page is not a panacea for XSS vulnerabilities. In modern
web applications, the inline JavaScript code is sometimes
dynamically generated by the server-side code. A common
scenario is to use the dynamic JavaScript code to pass data
from the server-side code to the client-side code. There may
be XSS vulnerabilities, even if code and data are properly
separated, if the data embedded in the JavaScript code is not
properly sanitized. deDacota provides a partial solution to
the problem of dynamic JavaScript (see Section 4.5).

4.

DESIGN

Our goal is to statically transform a given web application
so that the new version preserves the application semantics
but outputs web pages where all the inline JavaScript code
is moved to external JavaScript files. These external files
will be the only JavaScript that the browser will execute,
based on a Content Security Policy.
There are three high-level steps to our approach. For each
web page in the web application: (1) we statically determine
a conservative approximation of the page’s HTML output,
(2) we extract all inline JavaScript from the approximated
HTML output, and (3) we rewrite the application so that
all inline JavaScript is moved to external files.
Hereinafter, we define a running example that we use to
describe how deDacota automatically transforms a web application, according to the three steps outlined previously.

4.1

Example

Listing 1 shows a simplified ASP.NET Web Form page.
Note that everything not in between the <% and %> is output
directly to the browser. Everything between matching <%
and %> is C# code. A subtle but important point is that
<%= is used to indicate that the C# code will output a string
at that location in the HTML output.
In Listing 1, Line 2 sets the title of the page, and Line 3
sets the Username variable to the name parameter sent in the
query string. The Username is output to the browser inside
a JavaScript string on Line 7. This is an example of the C#
server-side code passing information to the JavaScript clientside code, as the intent here is for the JavaScript username
variable to have the same value as the C# Username variable.
Internally, ASP.NET compiles the ASP.NET Web Form
page to C#, either when the application is deployed, or ondemand, as the page is accessed. The relevant compiled
C# output of Listing 1 is shown in Listing 2. Here, the
ASP.NET Web Form page has been transformed into an

1 < html >
2
<% Title = " Example ";
3
Username = Request . Params [" name "]; % >
4
< head > < tile > <%= Title % > </ title > </ head >
5
< body >
6
< script >
7
var username = " <%= Username % >";
8
</ script >
9
</ body >
10 </ html >

h<html>, Line 2i

h<head><tile>, Line 5i

hExample, Line 6i

Listing 1: Example of a simple ASP.NET Web Form
page.
h</title></head><body><script>var username = ", Line 7i

1 void Render ( TextWriter w ) {
2
w . Write ( " < html >\ n " ) ;
3
this . Title = " Example " ;
4
this . Username = Request . Params [ " name " ];
5
w . Write ( " \ n < head > < tile > " ) ;
6
w . Write ( this . Title ) ;
7
w . Write ( " </ title > </ head >\ n < body >\ n
< script >\ n
var username = \" " ) ;
8
w . Write ( this . Username ) ;
9
w . Write ( " \";\ n
</ script >\ n </ body >\ n
</ html > " ) ;
10 }
Listing 2: The compiled C# output of Listing 1.

equivalent C# program. The ASP.NET compiler creates a
class (not shown) that represents the ASP.NET Web Form.
A method of the class is given a TextWriter object as a parameter. Anything written to this object will be sent in the
HTTP response. TextWriter.Write is a method call equivalent of writing to the console in a traditional command-line
application.
From comparing Listing 1 to Listing 2, one can see that
output not between <% and %> tags is written to the TextWriter object. The code between the <% and %> tags is
inlined into the function (Lines 3 and 4), and the code that
is between the <%= and %> tags is written to the TextWriter
object (Lines 6 and 8). We also note that TextWriter.Write
is one of a set of methods used to write content to the HTTP
response. However, for simplicity, in the remainder of this
paper, we will use TextWriter.Write to represent all possible ways of writing content to the HTTP response.

4.2

Approximating HTML Output

In the first phase of our approach, we approximate the
HTML output of a web page. This is a two-step process.
First, we need to determine, at every TextWriter.Write location, what is being written. Second, we need to determine
the order of the TextWriter.Write function invocations.
We use a different static analysis technique to answer each
of the two questions. To determine what is being written
at a TextWriter.Write, we use the points-to analysis algorithm presented in [10] modified to work on .NET byte-code,
instead of C. This points-to analysis algorithm is inclusionbased, demand-driven, context-sensitive, field-sensitive, and
partially flow-sensitive. The points-to analysis algorithm
computes the set of strings that alias with the parameter of
TextWriter.Write. If all strings in the alias set are constant
strings, the output at the TextWriter.Write will be defined

h*, Line 8i

h";</script></body></html>, Line 9i

Figure 1: Approximation graph for the code in Listing 1 and Listing 2. The dotted node’s content is
not statically determinable.

as the conjunction of all possible constant strings. Otherwise, we say the output is statically undecidable. To determine the ordering of all TextWriter.Write method calls, we
build a control-flow graph, using standard techniques, that
only contains the TextWriter.Write method calls.
We encode the information produced by the two static
analyses—the ordering of TextWriter.Write method calls
and their possible output—into a graph that we call an approximation graph. Figure 1 shows the approximation graph
for the code in Listing 1 and Listing 2. Each node in the
graph contains the location of the TextWriter.Write that
this node represents as well as the possible constant strings
that could be output at this TextWriter.Write location.
Content that cannot be determined statically is represented
by a wild card * (the dotted node in Figure 1). The strings
that may be output at the TextWriter.Write will be used
to identify inline JavaScript, and the location of the TextWriter.Write will be used for rewriting the application.
In Figure 2 we show the approximation graph of a more
complex page. The graph in Figure 2 contains a branch,
where each node in the branch maps to the same TextWriter.Write method. This happens when the points-to
analysis says that the TextWriter.Write method can output one of multiple strings. The other way there can be a
branch in the approximation graph is when there is a branch
in the control flow of the web application. The graph in Figure 2 also contains a loop that includes the nodes shown in
bold. However, because we cannot statically determine the
number of times a loop may execute, and we want our analysis to be conservative, we collapse all nodes of a loop (in
the approximation graph) into a single node. This new node
now has undecidable content (represented by a *). The new
node also keeps track of all the TextWriter.Write methods
that were part of the original loop.

h<script>, Line 20i

hsetupAdmin();, Line 30i

hsetupGuest();, Line 30i

hvar test = ", Line 40i

h*, Line 50i

h";, Line 60i

h</script>, Line 70i

Figure 2: Approximation graph with branches and
a loop. The loop will be collapsed into one node to
create the final approximation graph.

After collapsing all loops in the graph, we derive a conservative approximation of the HTML output of a web page.
The approximation graph is a directed acyclic graph (DAG),
and any path from the root node to a leaf node will represent
one possible output of the web page.

4.3

Extracting Inline JavaScript

In the second phase, our approach uses the approximation graph described previously to extract all possible inline
JavaScript. The output of this phase is a set containing all
possible inline JavaScript that may appear in the web page.
In an approximation graph, each unique path from the
root node to a leaf node represents a potential output of the
page. A naı̈ve algorithm would enumerate all paths and,
thus, all outputs, and parse each output string to identify
inline JavaScript. However, even without loops, the number
of unique paths even in a simple web page may quickly explode and become unmanageable (this is the path-explosion
problem faced in static analysis).
To reduce the impact of the path explosion problem, we
extract the inline JavaScript directly from the approximation graph. We first search for the opening and closing tags
of HTML elements in the graph. We ignore tags that appear
in comments. Then, for each pair of JavaScript tags (i.e.,
<script> and </script>), we process all the unique paths
between the opening and closing tags. For each path, we
obtain an inline JavaScript that the program might output.
While our current prototype is relatively simplistic in parsing the starting and ending JavaScript files, it could be possible to use the parsing engine from a real browser. However,
this is not as straight-forward as it seems, as our input is a
graph of all potential HTML output, not a single document.
We leave this approach to future work.
All identified inline JavaScript pieces are then passed to
the last phase of our approach, which decides how to rewrite
the application.

4.4

Application Rewriting

The goal of the third phase is to rewrite the application
so that all identified inline JavaScript will be removed from
the HTML content and saved in external JavaScript files.
In the HTML code, an inline JavaScript is replaced with a
reference to the external JavaScript file as follows:
<script src="External.js"></script>
It is not uncommon that multiple possible inline JavaScript snippets exist between an opening and closing JavaScript tag because there may be branches between the tags
in the approximation graph. To know which exact inline
JavaScript is created, we need to track the execution of the
server-side code.
The inline JavaScript identified in the previous phase falls
into two categories: static and dynamic (i.e., contains undecidable content). Because we cannot statically decide the
content of a dynamic inline JavaScript, we must track the
execution of the server-side code to create its external JavaScript file(s) at runtime. Therefore, we can avoid tracking
the execution of the server-side code only for the case in
which there is a single, static inline JavaScript code.
For a pair of opening and closing script tags that require
tracking the execution of the server-side code, we rewrite
the application as follows. At the TextWriter.Write that
may output the opening script tag, we first check if the output string contains the tag. We need to perform this check
because a TextWriter.Write site may be used to output either inline JavaScript code or other HTML. If we find the
opening script tag in the output, we use a session flag to
indicate that an inline JavaScript rewriting has started. We
write out everything before the start of the opening script
tag. We remove the opening script tag itself. The remaining
content is stored into a session buffer. Note that both session
flag and buffer are unique to each opening script tag. Then,
for all subsequent TextWriter.Write method calls that are
part of the inline JavaScript we are rewriting, except for the
last (that writes the closing tag), we append their output
to the session buffer if the session flag is on. For the last
TextWriter.Write method call (i.e., the one that writes the
closing script tag), any string content that occurs before the
closing script tag is appended to the session buffer. Any
content after the closing script tag is just written to the
output. At this point, the session buffer contains the entire
inline JavaScript code. We save this code to an external file
and add a TextWriter.Write method call that outputs the
reference to this JavaScript file.
To support JavaScript caching on the client side, the name
of the JavaScript file is derived from its content, using a
cryptographic hash of the JavaScript content. An unintended benefit of this approach is that inline JavaScript that
is included on multiple pages will be cached by the browser,
improving application performance by reducing the size of
the page and saving server requests.
Listing 3 shows the result of applying this rewriting process to the inline JavaScript code in Listing 2. The changes
shown are only those made to Lines 7–9 in Listing 2.

4.5

Dynamic Inline JavaScript

At this point in our analysis, we have successfully separated the JavaScript code from the HTML data in the web
application. If the web application’s JavaScript is static,
and by static we mean statically decidable, then the application is now immune to XSS vulnerabilities. However, if

1
2
3
4
5
6
7
8
9
10

w . Write ( " </ title > </ head >\ n

< body >\ n

");

Session [ " 7 " ] = " \ n
var username = \" " ) ;
Session [ " 7 " ] += this . Username ;
Session [ " 7 " ] += " \";\ n
";
var hashName = Hash ( Session [ " 7 " ]) + " . js " ;
WriteToFile ( hashName , Session [ " 7 " ]) ;
w . Write ( " < script src =\" " + hashName + "
\" > </ script > " ) ;

11
12 w . Write ( " \ n

</ body >\ n </ html > " ) ;

Listing 3: The result of the rewriting algorithm
applied to Listing 2. Specifically, here we show the
transformation of Lines 7–9 in Listing 2.

the web application dynamically generates JavaScript with
undecidable content, and that content is not properly sanitized inside the JavaScript code, an attacker can exploit this
bug to inject a malicious script. The approach discussed so
far does not mitigate this attack, because it simply moves
the vulnerable JavaScript to an external file.
To understand how dynamic JavaScript can result in a
vulnerability, consider our example application in Listing 2.
There is an XSS vulnerability on Line 8 because the Username variable is derived from the name parameter and output
directly to the user, without sanitization. An attacker could
exploit this vulnerability by setting the name parameter to
";alert(’xss’)//. This would cause the resulting inline
JavaScript to be the following, thus executing the attacker’s
JavaScript code:
<script>
var username = "";alert(’xss’)//";
</script>
Therefore, the code section of the application is dynamically generated with untrusted input and even with the code
and data separated, there is still an XSS vulnerability.
We attempt to mitigate this problem, and therefore improve the security of the application, in two ways. First, we
identify cases in which we can safely rewrite the application.
Second, we notify the developer when we make an inline to
external transformation that is potentially unsafe.
For the first case, when the undetermined output is produced in certain JavaScript contexts, we can include it in a
safe fashion via sanitization. Specifically, during static analysis we pass the dynamic inline JavaScript to a JavaScript
parser. Then, we query the parser to determine the contexts in which the undetermined output (i.e., the * parts)
is used. Here, for context we are referring specifically to
the HTML parsing contexts described by Samuel et al. [38].
Possible contexts are JavaScript string, JavaScript numeric,
JavaScript regular expression, JavaScript variable, etc. If
an undetermined output is in a string context, we sanitize
them in a way similar to how Blueprint [28] handles string
literals in JavaScript.
Like Blueprint, on the server side we encode the string
value and store the encoded data in JavaScript by embedding a call to a decoding function. Then when the JavaScript is executed on the client side, the decoding function
will decode the encoded data and return the string. Unlike
Blueprint, we do not require any developer annotations

because our static analysis can automatically identify which
JavaScript context an undetermined output is in.

4.6

Generality

While the description of our approach so far was specific
to ASP.NET Web Forms, the high-level idea of automatically separating code and data in a legacy web application
can be generalized to any other web application frameworks
or templating languages. There are still challenges that remain to apply our approach to another language, or even
another template in the same language. The two main steps
of our approach that must be changed to accommodate a
different language or templating language are: (1) understand how the output is created by the web application and
(2) understand how to rewrite the web application. Only
the first step affects the analysis capability (as the rewriting
process is fairly straightforward).
To automatically separate the code and data of a different language or templating language, one must understand
how the language or template generates its output. After
that, one would need to implement a static analysis that
can create an approximation graph. For instance, in the default Ruby on Rails template, ERB, variables are passed to
the template either via a hash table or class instance variables [37]. Therefore, one could approximate the output of
an ERB template by statically tracking the variables added
to the hash table and class instance variables (using pointsto analysis). Once an approximation graph is created, detecting inline JavaScript can be performed in the manner
previously described.
The main factor to affect the success of applying our approach to another web application framework or templating
language is the precision of the static analysis, or in other
words, how precise and detailed the approximation graph
would be. The more dynamicism in the language or framework, such as run-time code execution and dynamic method
invocation, the more difficult the analysis will be. Simply,
the more of the control-flow graph that we are able to determine statically, the better our analysis will be. As an
example the default templating language in Django only allows a subset of computation: iterating over a collection
instead of arbitrary loops [12]. This restriction could make
the analysis easier and therefore the approximation graph
more precise.

5.

IMPLEMENTATION

We implemented the automated code and data separation approach described in Section 4 in a prototype called
deDacota. This prototype targets ASP.NET Web Forms
applications. ASP.NET is a widely used technology; of the
Quantcase top million websites on the Internet, 21.24% use
ASP.NET [8].
deDacota targets binary .NET applications. More precisely, it takes as input ASP.NET Web Forms binary web
applications, performs the three steps of our approach, and
outputs an ASP.NET binary that has all inline JavaScript
code converted into external JavaScript files. We operate
at the binary level because we must be able to analyze the
ASP.NET system libraries, which are only available in binary form.
We leverage the open-source Common Compiler Infrastructure (CCI) [32] for reading and analyzing the .NET
Common Language Runtime byte-code. CCI also has mod-

ules to extract basic blocks and to transform the code into
single static assignment (SSA) form. We also use CCI to
rewrite the .NET binaries.
For the static analysis engine, we leverage the points-to
analysis engine of KOP (also known as MAS) [10]. KOP was
originally written for the C programming language. Therefore, we wrote (using CCI) a frontend that processes .NET
binaries and outputs the appropriate KOP points-to rules.
Then, after parsing these rules, the static analysis engine can
provide either alias analysis or points-to analysis. The KOP
points-to analysis is demand-driven, context-sensitive, fieldsensitive, and, because of the CCI single static assignment,
partially flow-sensitive.
An important point, in terms of scalability, is the demanddriven ability of the static analysis engine. Specifically, we
will only explore those parts of the program graph that are
relevant to our analysis, in contrast to traditional data-flow
techniques which track data dependencies across the entire
program. The demand-driven nature of the static analysis
engine offers another scalability improvement, which is parallelism. Each analysis query is independent and, therefore,
can be run in parallel.
We also extend the KOP points-to analysis system to
model string concatenation. We do this by including special edges in the program graph that indicate that a variable is the result of the concatenation of two other variables. When computing the alias set of a variable, we first
do so in the original way (ignoring any concatenation edges).
Then, for each variable in the alias set that has concatenation edges, we compute the alias set for each of the two
variables involved in the concatenation operation. We concatenate strings in the two alias sets and add them to the
original alias set. The undecidable variables are tracked,
so that their concatenated result contains a wildcard. This
process is recursive, and handles arbitrary levels of concatenation.
ASP.NET uses the idea of reusable components, called
Controls. The idea is that a developer can write a control once and then include it in other pages, and even other
controls. This relationship of including one control inside
another creates a parent-child relationship between the controls (the parent being the control that contains the child
control).
In an ASP.NET Web Form page, child controls are first
added to the parent’s ChildControls collection, which is
similar to an array. Then, during rendering, a parent renders its child controls either by iterating over the ChildControls or by referencing a child control based on its index
in the ChildControls. Because the KOP points-to analysis
does not model the array relation, we cannot precisely decide which child Control is being selected during rendering.
To handle this problem, we need to track the parent-child
relationships directly.
These parent-child relationships form a tree. Figure 3
shows the parent-child relationship of some of the user controls of default.aspx in the application BlogEngine.NET
(one of the programs used in our evaluation). When building the control graph, we must statically recreate this tree.
To create this relationship statically, we take an approach
similar to approximating the HTML output. The entry function for an ASP.NET page is FrameworkInitialize, which is
similar to the main function for a C program. Starting from
this method, we create a control-flow graph of all the calls

to AddParsedSubObject, which is the function that adds a
child control to a parent. This gives us the order of the
AddParsedSubObject calls. At each of the calls, we use the
points-to analysis to find which control is the parent and
which is the child. This information, along with the order of
the calls to AddParsedSubObject, allows us to recreate the
parent-child control tree.

6.

EVALUATION

There are three properties that we must look at to evaluate the effectiveness of deDacota. First, do we prevent
XSS vulnerabilities in the data section of the application by
applying code and data separation? Second, do we correctly
separate the code and data of the application—that is, does
the rewriting preserve the application’s semantics? Third,
what is the impact on the application’s performance? To
evaluate the security of our approach, we look at ASP.NET
applications with known vulnerabilities. To evaluate the correctness of our rewriting procedure, we apply our approach
to applications that have developer-created integration tests.
Then, we carried out performance measurements to answer
the third question. Finally, we discuss the relation between
separating code and data in the output and sanitizing the
input.

6.1

Applications

We wish to evaluate deDacota on ASP.NET web applications that are real-world, are open-source, and contain
known vulnerabilities. Real-world applications are important for showing that our approach works on real-world code,
open-source is important for other researchers to replicate
our results, and known-vulnerable is important because we
aim to automatically prevent these known vulnerabilities.
Unfortunately, there is no standard (or semi-standard)
ASP.NET web application benchmark that meets all three
requirements. Furthermore, finding these application proved
to be a challenge. Compared to other languages such as
PHP, there are fewer open-source ASP.NET applications (as
most ASP.NET applications tend to be proprietary). Therefore, here we present a benchmark of six real-world, opensource, ASP.NET applications, four of which are knownvulnerable, one of which is intentionally vulnerable for education, and one of which has a large developer-created test
suite.
Table 1 contains, for each application, the version of the
application used in our evaluation, the CVE number of the
vulnerability reported for the application, the number of
ASP.NET Web Form pages, and the number of developerwritten ASP.NET Controls. To provide an idea of the size
of the applications, we also show the number of lines of code
(LOC) of the ASP.NET controls (Web Forms and Controls)
and C# code.
The web applications BugTracker.NET [7], BlogEngine.NET [5], BlogSA.NET [6], and ScrewTurn Wiki [41] all
contain an XSS vulnerability as defined in the associated
CVE.
WebGoat.NET [17] is an open-source ASP.NET application that is intentionally vulnerable. The purpose is to provide a safe platform for interested parties to learn about web
security. Among the vulnerabilities present in the application are two XSS vulnerabilities.
ChronoZoom Beta 3 [9], is an open-source HTML5 “interactive timeline for all of history.” Parts are written in

default.aspx

SearchOnSearch

PostList

SearchBox

PostCalendar

InfoBox

menu

PostCalendar

PageList

RecentPosts

Figure 3: Control parent-child relationship between some of the controls in default.aspx from the application
BlogEngine.NET. The siblings are ordered from left to right in first-added to last-added order.
Application
BugTracker.NET
BlogEngine.NET
BlogSA.NET
ScrewTurn Wiki
WebGoat.NET
ChronoZoom

Version
3.4.4
1.3
1.0 Beta 3
2.0.29
e9603b9d5f
Beta 3

Known Vulnerability
CVE-2010-3266
CVE-2008-6476
CVE-2009-0814
CVE-2008-3483
2 Intentional
N/A

# Web Forms
115
19
29
30
67
15

# Controls
0
11
26
4
0
0

ASP.NET LOC
27,257
2,525
2,632
2,951
1,644
3,125

C# LOC
8,417
26,987
4,362
9,204
10,349
18,136

Total LOC
35,674
29,512
6,994
12,155
11,993
21,261

Table 1: ASP.NET Web Form applications that we ran deDacota on to test its applicability to real-world
web applications.
ASP.NET Web Forms, but the main application is a JavaScript-heavy HTML page. We use ChronoZoom because,
unlike the other applications, it has an extensive test suite
that exercises the JavaScript portion of the application. To
evaluate the correctness of our rewriting, we converted the
main HTML page of ChronoZoom, which contained inline
JavaScript, into an ASP.NET Web Form page, along with
nine other HTML pages that were used by the test suite.
These six real-world web applications encompass the spectrum of web application functionality that we expect to encounter. These applications constitute a total of 100,000
lines of code, written by different developers, each with a
different coding style. Some had inline JavaScript in the
ASP.NET page, some created inline JavaScript in C# directly, while others created inline JavaScript in C# using
string concatenation. Furthermore, while analyzing each application we also analyzed the entire .NET framework (which
includes ASP.NET); all 256 MB of binary code. As our analysis handles ASP.NET, we are confident that our approach
can be applied to the majority of ASP.NET applications.

6.2

Security

We ran deDacota on each of our test applications. Table 2 shows the total number of inline JS scripts per application and a breakdown of the number of static inline JS
scripts, the number of safe dynamic inline JS scripts, and the
number of unsafe dynamic inline JS scripts. There were four
dynamic inline JS scripts created by the ASP.NET framework, and these are represented in Table 2 in parentheses.
We chose to exclude these four from the total dynamic inline JS scripts because they are not under the developer’s
control, and, furthermore, they can and should be addressed
by changes to the ASP.NET library. Furthermore, it is important to note that our tool found these dynamic inline JS
scripts within the ASP.NET framework automatically.

From our results it is clear that modern web applications
frequently use inline JS scripts. The applications used a
range of five to 46 total inline JS scripts. Of these total
inline JS scripts 22% to 100% of the inline JS scripts were
static.
deDacota was able to safely transform, using the technique outlined in Section 4.5, 50% to 70% of the dynamic
inline JS scripts. This result means that our mitigation technique worked in the majority of the cases, with only zero to
four actual unsafe dynamic inline JS scripts per application.
We looked for false negatives (inline JavaScript that we
might have missed) in two ways. We manually browsed to
every ASP.NET Web Form in the application and looked for
inline JavaScript. We also searched for inline JavaScript in
the original source code of the application to reveal possible
scripts the previous browsing might have missed. We did
not find any false negatives in the applications.
To evaluate the security improvements for those applications that had known vulnerabilities, we manually crafted
inputs to exploit these know bugs. After verifying that the
exploits worked on the original version of the application, we
launched them against the rewritten versions (with the Content Security Policy header activated, and with a browser
supporting CSP). As expected, the Content Security Policy in the browser, along with our rewritten applications,
successfully blocked all exploits.

6.3

Functional Correctness

To evaluate the correctness of our approach, and to verify
that we maintained the semantics of the original application, we used two approaches. First, we manually browsed
web pages generated by each rewritten application and interacted with the web site similar to a normal user. During
this process, we looked for JavaScript errors, unexpected behaviors, or CSP violations. We did not find any problems or
deviations. Second, and more systematically, we leveraged

Application

Total JS

Static

BugTracker.NET
BlogEngine.NET
BlogSA.NET
ScrewTurn Wiki
WebGoat.NET
ChronoZoom

46
18
12
35
6
5

41
4
10
27
6
5

Safe
Dynamic
3
10
1
4
0
0

Unsafe
Dynamic
2 (4)
4 (4)
1 (4)
4 (4)
0 (4)
0 (4)

Table 2: Results of running deDacota against the
ASP.NET Web Form applications. Safe Dynamic
is the number of dynamic inline JS scripts that we
could safely transform, and Unsafe Dynamic is the
number of dynamic inline JS scripts that we could
not safely transform.
Application
ChronoZoom (original)
ChronoZoom (transformed)
BlogEngine.NET (original)
BlogEngine.NET (transformed)

Page Size
50,827
20,784
18,518
19,269

Loading Time
0.65
0.63
0.15
0.16

Table 3: Performance measurements for two of the
tested applications, ChronoZoom. Page Size is the
size (in bytes) of the main HTML page rendered
by the browser, and Loading Time is the time (in
seconds) that the browser took to load and display
the page.
the developer-written testing suite in ChronoZoom. Before
we applied our rewriting, the original application passed 160
tests. After rewriting, all 160 tests executed without errors.

6.4

Performance

To assess the impact of deDacota on application performance, we ran browser-based tests on original and transformed versions of two of the tested applications. Our performance metric was page-loading time in Internet Explorer
9.0, mainly to determine the impact of moving inline JavaScript into separate files. The web server was a 3 GB HyperV virtual machine running Microsoft IIS 7.0 under Windows
Server 2008 R2, while the client was a similar VM running
Windows 7. The physical server was an 8 GB, 3.16 GHz
dual-core machine running Windows Server 2008 R2.
Table 3 shows test results for two web applications, summarizing performance data from page-loading tests on the
client. The table columns list the average sizes of the main
HTML pages retrieved by the browser by accessing the main
application URLs, along with the average time used by the
browser to retrieve and render the pages in their entirety.
All the numbers were averaged over 20 requests.
As Table 3 indicates, deDacota’s transformations incurred no appreciable difference in page-loading times. Because the original ChronoZoom page contained a significant
amount of script code, the transformed page is less than half
of the original size. On the other hand, the BlogEngine.NET
page is slightly larger because of its small amount of script
code, which was replaced by longer links to script files. The
page-loading times mirror the page sizes, also indicating that
server-side processing incurred no discernible performance
impact.

6.5

Discussion

The results of our rewriting shed light on the nature of
inline JavaScript in web applications. Of the four applications that have dynamic JavaScript, 12.2% to 77.8% of the

total inline JavaScript in the application is dynamic. This is
important, because one of BEEP’s XSS prevention policies
is a whitelist containing the SHA1 hash of allowed JavaScript [20]. Unfortunately, in the modern web JavaScript is
not static and frequently includes dynamic elements, necessitating new approaches that can handle dynamic JavaScript.
The other security policy presented in BEEP is DOM
sandboxing. This approach requires the developer to manually annotate the sinks so that they can be neutralized.
Blueprint [28] works similarly, requiring the developer to
annotate the outputs of untrusted data. Both approaches
require the developer to manually annotate the sinks in the
application in order to specify the trusted JavaScript. To
understand the developer effort required to manually annotate the sinks in the application, we counted the sinks (i.e.,
TextWriter.Write call sites) inside the 29 Web Forms of
BlogSA.NET and there were 407. In order to implement
either BEEP or Blueprint a developer must manually analyze all sinks in the application and annotate any that could
create untrusted output.
Unlike BEEP and Blueprint, deDacota is completely
automatic and does not require any developer annotations.
deDacota cannot prevent XSS vulnerabilities in dynamic
inline JavaScript completely. If a developer wishes to prevent all XSS vulnerabilities after applying deDacota, they
would only need to examine the sinks that occur within the
unsafe dynamic inline JavaScript. In BlogSA.NET, there
are three sinks within the single unsafe dynamic JavaScript.
One could further reduce the number of sinks by using taint
analysis to check if untrusted input can reach a sink in the
dynamic JavaScript.

7.

LIMITATIONS

The goal of deDacota is to automatically separate the
JavaScript code from the HTML data in the web pages of a
web application using static analysis. We have shown that
deDacota is effective with real-world web applications. In
this section, we discuss its limitations in general.
The programming language of .NET has the following dynamic language features: dynamic assembly loading, dynamic compilation, dynamic run-time method calling (via
reflection), and threading. The use of these features may
compromise the soundness of any static analysis including
ours in deDacota. However, these language features are
rarely used in ASP.NET web applications in practice. For
instance, those applications we tested did not use any of
these features. Furthermore, deDacota is affected only if
the use of these features determines the HTML output of an
application.
On one hand, we handle loops conservatively by approximating that a loop can produce anything. On the other
hand, we treat the output of a loop as a * in the approximation graph and assume it does not affect the structure of
the approximation graph in a way that impacts our analysis.
For instance, we assume the output of a loop does not contain the opening or closing script tag. Our analysis will be
incorrect if this assumption is violated. While we found that
this assumption holds for all the web applications we tested,
it is possible that this assumption will not hold for other
programs, thus requiring a different approach to handling
loops.
We do not offer any formal proof of the correctness of
deDacota. While we believe that our approach is correct in

absence of the dynamic language features, we leave a formal
proof of this to future work.
deDacota currently supports the analysis of string concatenations. The support for more complex string operations such as regular expressions is left for future work. A
potential approach is to leverage an automata-based string
analysis engine [50].
Our approach to sanitizing dynamic JavaScript code may
not preserve an application’s semantics when the dynamic
content being sanitized as a string is meant to be used in
multiple JavaScript contexts.
When deploying deDacota in practice, we recommend
two practices to mitigate its limitations. First, all tests
for the original web application should be performed on
the rewritten binary to detect any disruptions to the application’s semantics. Second, CSP’s “Report Only” mode
should be used during the testing and initial deployment.
Under this mode, the browser will report violations back to
the web server when unspecified JavaScript code is loaded.
This helps detect inline JavaScript code that is missed by
deDacota.
Finally, our prototype does not handle JavaScript code
in HTML attributes. We do not believe that there is any
fundamental limitation that makes discovering JavaScript
attributes more difficult than inline JavaScript. The only
minor difficulty here is in the rewriting. In order to separate a JavaScript attribute into an external JavaScript, one
must be able to uniquely identify the DOM element that
the JavaScript attribute affects. To do this, it would require generating a unique identifier for the HTML element
associated with the JavaScript attribute.

8.

RELATED WORK

A broad variety of approaches have been proposed to address different types of XSS, though no standard taxonomy
exists to classify these attacks and defenses. In general, XSS
defenses employ schemes for input sanitization or restrictions on script generation and execution. Differences among
various techniques involve client- or server-side implementation and static or dynamic operation. We group and review
XSS defenses in this context.

8.1

Server-Side Methods

While CSP itself is enforced by browsers [42], our approach for leveraging CSP is a static, server-side XSS defense. There has been much previous research in server-side
XSS defenses [3, 4, 13, 14, 22, 28, 34, 35, 38, 40, 43]. Serverbased techniques aim for dynamically generated pages free
of XSS vulnerabilities. This may involve validation or injection of appropriate sanitizers for user input, analysis of
scripts to find XSS vulnerabilities, or automatic generation
of XSS-free scripts.
Server-side sanitizer defenses either check existing sanitization for correctness or generate input encodings automatically to match usage context. For example, Saner [3] uses
static analysis to track unsafe inputs from entry to usage,
followed by dynamic analysis to test input cases for proper
sanitization along these paths. ScriptGard [40] is a complementary approach that assumes a set of “correct” sanitizers and inserts them to match the browser’s parsing context. Bek [18] focuses on creating sanitization functions automatically analyzable for preciseness and correctness. Sani-

tization remains the main industry-standard defense against
XSS and related vulnerabilities.
A number of server-side defenses restrict scripts included
in server-generated pages. For example, XSS-GUARD [4]
determines valid scripts dynamically and disallows unexpected scripts. The authors report performance overheads
of up to 46% because of the dynamic evaluation of HTML
and JavaScript. Templating approaches [13, 36, 38] generate
correct-by-construction scripts that incorporate correct sanitization based on context. In addition, schemes based on
code isolation [1,2,26] mitigate XSS by limiting DOM access
for particular scripts, depending on their context.
Certain XSS defenses [21, 22, 27, 29, 34, 35, 39, 44, 49] use
data-flow analysis or taint tracking to identify unsanitized
user input included in a generated web page. These approaches typically rely on sanitization, encoding, and other
means of separating unsafe inputs from the script code.
Some schemes prevent XSS bugs dynamically, while others
focus on static detection and elimination.
Other approaches [14, 28, 33] combine server-side processing with various client-side components, such as confinement of untrusted inputs and markup randomization. Such
schemes may parse documents on the server and prevent
any modifications of the resulting parse trees on the client.
In addition, randomization of XHTML tags can render foreign script code meaningless, defeating many code-injection
attacks.

8.2

Client-Side Methods

Client-side XSS defenses [20,23,30,42,45,47] mitigate XSS
while receiving or rendering untrusted web content. Some
of these schemes rely on browser modifications or plug-ins,
often reducing their practical applicability. Others use custom JavaScript libraries or additional client-side monitoring
software. CSP itself [42] is a browser-based approach, but its
incorporation into WWW standards should facilitate wide
acceptance and support by all popular browsers.
Some client-side XSS defenses focus on detecting and preventing leakage of sensitive data. For example, Noxes [23]
operates as a personal-firewall plug-in that extracts all static
links from incoming web pages, prompting the user about
disclosure of information via dynamically generated links.
Vogt et al. [45] also aim to address this problem, but use
taint-tracking analysis within a browser to check for sensitive data released via XSS attacks. In contrast, deDacota
simply prevents any XSS exploits that could enable such
leakage.
Client-side HTML security policies mitigate XSS via content restrictions, such as disallowing unsafe features or executing only “known good” scripts. Using a browser’s HTML
parser, BEEP [20] constructs whitelists of scripts, much like
XSS-GUARD’s server-side approach [4]. BEEP assumes no
dynamic scripts whose hashes cannot be pre-computed, limiting its practicality with modern web applications; moreover, it has been shown that even whitelisted scripts may
be vulnerable to attacks [2]. Another custom content security policy is Blueprint’s page descriptions, which are
interpreted and rendered safely by a custom JavaScript library [28]. Script policies enforced at runtime [15, 30] are
also useful for mitigating XSS exploits.
In general, standardized HTML security policies [42, 47]
offer promise as a means of escaping the recent proliferation
of complex, often ad hoc XSS defenses. CSP simplifies the

problem by enforcing fairly strong restrictions, such as disabling eval() and other dangerous APIs, prohibiting inline
JavaScript, and allowing only local script resources to be
loaded. While new web applications can be designed with
CSP in mind, legacy code may require significant rewriting. deDacota works on both old and new applications,
facilitating adoption of CSP by developers, primarily by automating the separation process.

9.

CONCLUSION

Cross-site scripting vulnerabilities are pervasive in web
applications. Malicious users frequently exploit these vulnerabilities to infect users with drive-by downloads or to
steal personal information.
While there is currently no silver bullet to preventing every possible XSS attack vector, we believe that adhering to
the fundamental security principle of code and data separation is a promising approach to combating XSS vulnerabilities. deDacota is a novel approach that gets us closer to
this goal, by using static analysis to automatically separate
the code and data of a web application. While not a final
solution, deDacota and other tools that automate making
web applications secure by construction are the next step in
the fight against XSS and other kinds of vulnerabilities.

10.

ACKNOWLEDGMENTS

The authors extend their thanks to David Molnar, Alex
Moshchuk, Helen Wang, and Chris Hawblitzel for their helpful discussions, Herman Venter for all his help and support with CCI, and David Brumley for his insightful suggestions which helped to focus the paper. This work was
supported by the Office of Naval Research (ONR) under
Grant N000140911042, the Army Research Office (ARO) under Grant W911NF0910553, the National Science Foundation (NSF) under Grants CNS-0845559 and CNS-0905537,
and Secure Business Austria.

11.

REFERENCES

[1] Akhawe, D., Saxena, P., and Song, D. Privilege
Separation in HTML5 Applications. In Proceedings of
the USENIX Security Symposium (USENIX) (2012).
[2] Athanasopoulos, E., Pappas, V., and Markatos,
E. P. Code-Injection Attacks in Browsers Supporting
Policies. In Proceedings of the Workshop on Web 2.0
Security and Privacy (W2SP) (2009).
[3] Balzarotti, D., Cova, M., Felmetsger, V.,
Jovanovic, N., Kirda, E., Kruegel, C., and
Vigna, G. Saner: Composing Static and Dynamic
Analysis to Validate Sanitization in Web Applications.
In Proceedings of the IEEE Symposium on Security
and Privacy (Oakland, CA, 2008).
[4] Bisht, P., and Venkatakrishnan, V.
XSS-GUARD: Precise Dynamic Prevention of
Cross-Site Scripting Attacks. In Proceedings of the
Conference on Detection of Intrusions and Malware
and Vulnerability Assessment (DIMVA) (Paris,
France, 2008).
[5] blogengine.net - an innovative open source blogging
platform. http://www.dotnetblogengine.net, 2013.
[6] BlogSA.NET. http://www.blogsa.net/, 2013.
[7] BugTracker.NET - Free Bug Tracking.
http://ifdefined.com/bugtrackernet.html, 2013.

[8] Top in Frameworks - Week beginning Jun 24th 2013.
http://trends.builtwith.com/framework, 2013.
[9] Chronozoom - A Brief History of the World. http:
//chronozoom.cloudapp.net/firstgeneration.aspx,
2013.
[10] Cui, W., Peinado, M., Xu, Z., and Chan, E.
Tracking Rootkit Footprints with a Practical Memory
Analysis System. In Proceedings of the USENIX
Security Symposium (USENIX) (Bellevue, WA, 2012).
[11] CVE Details. Vulnerabilities by Type.
http://www.cvedetails.com/vulnerabilities-bytypes.php, 2013.
[12] Django. http://djangoproject.com, 2013.
[13] Google. Google AutoEscape for CTemplate.
http://code.google.com/p/ctemplate/.
[14] Gundy, M. V., and Chen, H. Noncespaces: Using
Randomization to Enforce Information Flow Tracking
and Thwart Cross-Site Scripting Attacks. In Network
and Distributed System Security Symposium (NDSS)
(2009).
[15] Hallaraker, O., and Vigna, G. Detecting
Malicious JavaScript Code in Mozilla. In Proceedings
of the IEEE International Conference on Engineering
of Complex Computer Systems (ICECCS) (Shanghai,
China, 2005).
[16] Heiderich, M., Niemietz, M., Schuster, F., Holz,
T., and Schwenk, J. Scriptless Attacks: Stealing the
Pie Without Touching the Sill. In Proceedings of the
ACM Conference on Computer and Communications
Security (CCS) (2012).
[17] Hoff, J. WebGoat.NET.
https://github.com/jerryhoff/WebGoat.NET, 2013.
[18] Hooimeijer, P., Livshits, B., Molnar, D.,
Saxena, P., and Veanes, M. Fast and Precise
Sanitizer Analysis with Bek. In Proceedings of the
USENIX Security Symposium (USENIX) (2011).
[19] Howard, M., and LeBlanc, D. Writing Secure
Code, second ed. Microsoft Press, 2003.
[20] Jim, T., Swamy, N., and Hicks, M. Defeating Script
Injection Attacks with Browser-Enforced Embedded
Policies. In Proceedings of the International World
Wide Web Conference (WWW) (2007).
[21] Johns, M., and Beyerlein, C. SMask: Preventing
Injection Attacks in Web Applications by
Approximating Automatic Data/Code Separation. In
Proceedings of the ACM Symposium on Applied
Computing (SAC) (2007).
[22] Jovanovic, N., Kruegel, C., and Kirda, E.
Precise Alias Analysis for Static Detection of Web
Application Vulnerabilities. In Proceedings of the
Workshop on Programming Languages and Analysis
for Security (PLAS) (2006).
[23] Kirda, E., Kruegel, C., Vigna, G., and
Jovanovic, N. Noxes: A Client-Side Solution for
Mitigating Cross-Site Scripting Attacks. In
Proceedings of the ACM Symposium on Applied
Computing (SAC) (2006).
[24] Klein, A. DOM Based Cross Site Scripting or XSS of
the Third Kind. http://www.webappsec.org/
projects/articles/071105.shtml, 2005.

[25] Livshits, B., and Chong, S. Towards Fully
Automatic Placement of Security Sanitizers and
Declassifiers. In Proceedings of the Symposium on
Principles of Programming Languages (POPL) (2013).
[26] Livshits, B., and Erlingsson, U. Using Web
Application Construction Frameworks to Protect
Against Code Injection Attacks. In Proceedings of the
Workshop on Programming Languages and Analysis
for Security (PLAS) (2007).
[27] Livshits, V. B., and Lam, M. S. Finding Security
Vulnerabilities in Java Applications with Static
Analysis. In Proceedings of the USENIX Security
Symposium (USENIX) (2005).
[28] Louw, M. T., and Venkatakrishnan, V.
Blueprint: Robust Prevention of Cross-site Scripting
Attacks for Existing Browsers. In Proceedings of the
IEEE Symposium on Security and Privacy (2009).
[29] Martin, M., and Lam, M. S. Automatic Generation
of XSS and SQL Injection Attacks with Goal-Directed
Model Checking. In Proceedings of the USENIX
Security Symposium (USENIX) (2008).
[30] Meyerovich, L., and Livshits, B. ConScript:
Specifying and Enforcing Fine-Grained Security
Policies for JavaScript in the Browser. In Proceedings
of the IEEE Symposium on Security and Privacy
(2010).
[31] Microsoft. ASP.NET. http://www.asp.net/.
[32] Microsoft Research. Common Compiler
Infrastructure. http://research.microsoft.com/enus/projects/cci/, 2013.
[33] Nadji, Y., Saxena, P., and Song, D. Document
Structure Integrity: A Robust Basis for Cross-Site
Scripting Defense. In Proceeding of the Network and
Distributed System Security Symposium (NDSS)
(2008).
[34] Nguyen-tuong, A., Guarnieri, S., Greene, D.,
and Evans, D. Automatically Hardening Web
Applications Using Precise Tainting. In Proceedings of
the IFIP International Information Security
Conference (2005).
[35] Pietraszek, T., and Berghe, C. V. Defending
against Injection Attacks through Context-Sensitive
String Evaluations. In Proceedings of the Symposium
on Recent Advances in Intrusion Detection (RAID)
(2005).
[36] Robertson, W., and Vigna, G. Static Enforcement
of Web Application Integrity Through Strong Typing.
In Proceedings of the USENIX Security Symposium
(USENIX) (Montreal, Quebec CA, 2009).
[37] Ruby on Rails. http://rubyonrails.org/, 2013.
[38] Samuel, M., Saxena, P., and Song, D.
Context-Sensitive Auto-Sanitization in Web
Templating Languages Using Type Qualifiers. In
Proceedings of the ACM Conference on Computer and
Communications Security (CCS) (2011).

[39] Saxena, P., Akhawe, D., Hanna, S., Mao, F.,
McCamant, S., and Song, D. A Symbolic
Execution Framework for JavaScript. In Proceedings of
the IEEE Symposium on Security and Privacy (2010).
[40] Saxena, P., Molnar, D., and Livshits, B.
ScriptGard: Automatic Context-Sensitive
Sanitization for Large-Scale Legacy Web Applications.
In Proceedings of the ACM Conference on Computer
and Communications Security (CCS) (2011).
[41] ScrewTurn Wiki. http://www.screwturn.eu/, 2013.
[42] Stamm, S., Sterne, B., and Markham, G. Reining
in the Web with Content Security Policy. In
Proceedings of the International World Wide Web
Conference (WWW) (2010).
[43] Su, Z., and Wassermann, G. The Essence of
Command Injection Attacks in Web Applications. In
Proceedings of the Symposium on Principles of
Programming Languages (POPL) (2006).
[44] Tripp, O., Pistoia, M., Fink, S. J., Sridharan,
M., and Weisman, O. TAJ: Effective Taint Analysis
of Web Applications. In Proceedings of the ACM
SIGPLAN Conference on Programming Language
Design and Implementation (PLDI) (2009).
[45] Vogt, P., Nentwich, F., Jovanovic, N., Kirda,
E., Kruegel, C., and Vigna, G. Cross-Site
Scripting Prevention with Dynamic Data Tainting and
Static Analysis. In Proceeding of the Network and
Distributed System Security Symposium (NDSS)
(2007).
[46] Wassermann, G., and Su, Z. Sound and Precise
Analysis of Web Applications for Injection
Vulnerabilities. In Proceedings of the ACM SIGPLAN
Conference on Programming Language Design and
Implementation (PLDI) (2007).
[47] Weinberger, J., Barth, A., and Song, D. Towards
Client-side HTML Security Policies. In Proceedings of
the USENIX Workshop on Hot Topics in Security
(2011).
[48] Weinberger, J., Saxena, P., Akhawe, D.,
Finifter, M., Shin, R., and Song, D. A Systematic
Analysis of XSS Sanitization in Web Application
Frameworks. In Proceedings of the European
Symposium on Research in Computer Security
(ESORICS) (Leuven, Belgium, 2011).
[49] Xie, Y., and Aiken, A. Static Detection of Security
Vulnerabilities in Scripting Languages. In Proceedings
of the USENIX Security Symposium (USENIX)
(2006).
[50] Yu, F., Alkhalaf, M., and Bultan, T. Stranger:
An Automata-based String Analysis Tool for PHP. In
Proceedings of the International Conference on Tools
and Algorithms for the Construction and Analysis of
Systems (TACAS) (2010).

Moving Target Defense for Web Applications using
Bayesian Stackelberg Games
Sailik Sengupta, Satya Gautam Vadlamudi, Subbarao Kambhampati
Yochan Group, School of CIDSE
Arizona State University
{sailiks, gautam, rao}@asu.edu

Marthony Taguinod, Adam Doupé, Ziming Zhao, Gail-Joon Ahn
SEFCOM Lab, School of CIDSE
Arizona State University
{mtaguino, doupe, zmzhao, gahn}@asu.edu

ABSTRACT
The present complexity in designing web applications makes software security a difficult goal to achieve. An attacker can explore
a deployed service on the web and attack at his/her own leisure.
Moving Target Defense (MTD) in web applications is an effective
mechanism to nullify this advantage of their reconnaissance but the
framework demands a good switching strategy when switching between multiple configurations for its web-stack. To address this issue, we propose modeling of a real-world MTD web application
as a repeated Bayesian game. We then formulate an optimization
problem that generates an effective switching strategy while considering the cost of switching between different web-stack configurations. To incorporate this model into a developed MTD system,
we develop an automated system for generating attack sets of Common Vulnerabilities and Exposures (CVEs) for input attacker types
with predefined capabilities. Our framework obtains realistic reward values for the players (defenders and attackers) in this game
by using security domain expertise on CVEs obtained from the National Vulnerability Database (NVD). We also address the issue of
prioritizing vulnerabilities that when fixed, improves the security
of the MTD system. Lastly, we demonstrate the robustness of our
proposed model by evaluating its performance when there is uncertainty about input attacker information.

1.

INTRODUCTION

Present day web applications are widely used by businesses to
provide services over the Internet. Oftentimes, sensitive business
and user data are managed by these applications. Vulnerabilities in
these web applications pose serious threats to the confidentiality
and integrity of both businesses and users [16].
There exist numerous static (white-box) and dynamic (blackbox) analysis tools for identifying vulnerabilities in a system [2,
6]. These have become less effective in present times due to the
increasing complexity of web applications, their dependency on
downstream technologies, and the limited development and deployment time [22]. Worse yet, the attackers, with time on their side,
can perform reconnaissance and attack. To address this challenge,
we consider a Moving Target Defense (MTD) based approach [4],

A shorter version of this paper appears in: Proceedings of the
15th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2016), May 9–13, 2016, Singapore.

which complements the existing vulnerability analysis techniques
through a defense-in-depth mechanism.
The MTD based approach dynamically shifts a system over time
to increase the uncertainty and complexity for the attackers to perform probing and attacking [24], while ensuring that the system
is available for legitimate users. As the window of attack opportunities decreases, the effort in finding and successfully executing
an attack increases. Moreover, if an attacker succeeds in finding a
vulnerability at one point in time, it may not be exploitable at another time because of the moving defense system, making the web
application more resilient [18].
Various aspects that support Moving Target Defense approach
such as, using multiple implementation languages, multiple database
instances with synchronization, etc. are considered in different layers of web application architecture, along with ways to switch between them. However, the design of good quality switching strategies itself is left as an open problem. This is key to effectively leverage various move options—thereby maximizing the complexity for
the attacker and minimizing the damage for the defender.
Our aim in this paper is to design effective switching policies for
movement in the MTD system that maximize the security of the
web application, given the set of components and configurations
of the system which can be “moved around”, while simultaneously
considering realistic costs for “moving them around”. In web applications, the defender (leader) deploys a system up-front. The attacker observes (or follows) the system over time before choosing
an attack. These characteristics motivate us to formulate the MTD
system as a repeated Bayesian Stackelberg Game (BSG). For this
formulation to be meaningful in real-world applications, we use
real world attack data for our model. We propose a framework to
define attacker types for our game and automatically generate attack options for each of them by mining and characterizing Common Vulnerabilities and Exposures (CVEs). We develop a system
that leverages the knowledge in public attack databases and expertise of system administrators for obtaining meaningful game utilities and switching costs respectively.
For computing the movement policy for the defender, we initially
expected to be able to use existing solvers developed for physical
security systems [17]. Unfortunately, none of them considered the
cost of switching between strategies. Since this is highly relevant in
cyber-security systems, we had to formulate an optimization problem to consider these costs when generating strategies.
The increased complexity in an MTD systems exacerbates the
difficulty of prioritizing vulnerabilities that need to be fixed next.
We define this problem formally and propose a preliminary solu-

tion. Lastly, we talk about metrics to measure the robustness of
switching strategies generated by various models when the uncertainty about attacker types vary in the real world.
In section 2, we introduce the reader to the different ways people
have tried to address the problem of generating switching strategies for cyber-security systems. We introduce domain terminology
related to MTD systems for web-applications in section 3. In section 4, we develop the Bayesian Game model for this system defining attacker types, attack classification, rewards generated from security databases and strategy switching costs. To find an effective
switching strategy, we propose a solver that maximizes system security while accounting for switching costs in section 5. We empirically study the effectiveness and robustness of the strategy generated by our framework in section 6, comparing it to the stateof-the-art. We also formulate the problem of identifying critical
vulnerabilities and propose a preliminary solution in section 6. We
conclude the paper in section 7, highlighting promising research
directions.

2.

security systems. Unfortunately, these works, to our knowledge, do
not consider the cost the defenders incur when asked to switch from
a particular strategy to another. Hence, we propose a solver that
maximizes the defender’s reward and minimizes the overall cost
of switching between web-application configurations. Our solver
is essentially an extension of the DOBSS solver [13]. Although
there has been furhter development since DOBSS, the more recent solvers for BSGs make additional assumptions about the game
structure—either about the action sets of the defender, or the presence of hierarchical structure among attacker types [1], which do
not hold for the web application domain.
The use of Common Vulnerabilitiy Scoring System (CVSS) for
rating attacks is well studied in security [7]. We describe this metric later. CVSS provides a strong backbone for obtaining utilities
for our game theoretic model. None of the existing works (to our
knowledge) talk about the pragmatic aspect of prioritizing vulnerabilites in MTD systems. Also, there does not seem to be any standard metrics to capture the robustness of strategies generated by a
model. We address both these issues in the upcoming sections.

RELATED WORK

Although there exists prior work on the design of switching strategies for MTD systems, most of it is domain specific. Evaluation of
these strategies on real-world MTD systems for web applications
is scarce. We discuss some of these works, highlighting their limitations in the domain of web applications, thus motivating the need
for our solution. Existing efforts describe the use of randomized
switching strategies, and show its effectiveness for MTD systems
[24]. We empirically demonstrate that our strategy outperforms this
state-of-the-art for web applications, especially when the cost of
switching is negligible.
Attacker-defender scenarios have been modeled earlier as stochastic games for attack-surface shifting [9]. Other works model the
MTD problem as a repeated game where the defender uses uniform
random strategy with the exception that the same defense configuration is not deployed in two consecutive rounds [23]. This work
needs an in-depth analysis of code, which is unrealistic for complex
web applications.
Switching strategies for MTD systems based on detection of probes
by attackers are presented by [15]. Unfortunately, an accurate detection of attacks in web applications is difficult, if not impossible.
Furthermore, such strategies can lead to a detrimental performance
in repeated games if an intelligent attacker biases the system to
switch more towards MTD configurations where the attacker attains higher reward. In [8], the MTD system is modeled as a game
called PLADD, based on FlipIt games [19]. This work assumes
that different agents control the server in different game rounds,
which is impractical for most cyber-seccurity applications, essentially web applications. These techniques also fail to capture the
reconnaissance aspect of the attackers which is shown to be an important aspect in the attack phase [12].
In [3], a game theoretic leader-follower type approach is presented for a dynamic platform defense where the strategies are chosen so as to be diverse, based on statistical analysis rather than
being uniformly distributed. They find similarity among different
configurations of the MTD system, which is difficult in the domain
of web applications. The work fails to consider the uncertainty in
the attacker model and the costs for switching.
These aspects of uncertainty in the attacker model and attacker
reconnaissance are handled effectively via Bayesian Stackelberg
Games (BSG), making it an appropriate choice for modeling the
web applications domain. Our modeling could help us leverage the
existing solution methods in the physical security domains [17]
and provide scalable and optimal switching strategies for cyber-

3.

MOVING TARGET DEFENSE FOR WEB
APPLICATIONS

In this section, we present a brief overview of the web application domain and its functionality which will be useful for understanding the challenges involved in generating solution strategies.

3.1

Configuration

A configuration set for a web application stack is denoted as C =
C1 × C2 · · · × Cn where there are n-technological stacks. Here, Ci
denotes the set of technologies that can be used in the i-th layer
of the application stack. A valid configuration c is an n-tuple that
preserves the system’s operational goals.
Consider a web application that has two layers (n = 2) where
the first layer denotes the coding language the web-application was
coded in and the second layer denotes the database that stores the
data handled by this application. Say, the technologies used in each
layer are C1 = {Python, PHP} and C2 = {MySQL, postgreSQL}.
A valid configuration can be (PHP, MySQL). The diversity of an
MTD system, which is the number of valid configurations, can be
4 (at max) in this case.

3.2

Attack

Software security is defined in terms of three characteristics Confidentiality, Integrity and Availability [10]. In a broad sense, an
attack on a web application is defined as an act that compromises
any of the aforementioned characteristics. The National Vulnerability Database (NVD) is a public directory of known vulnerabilities
and exposures affecting all technologies that can be used in a web
application. The Common Vulnerabilities and Exploits (CVEs) in
this database list vulnerabilities and corresponding attacks that can
be used to compromise an application using the affected technology. As each CVE has an exploit associated with it, we use the
terms vulnerability and attack interchangeably going forward.

3.3

Switching Strategy

This is a decision making process for the defender to select the
next valid system configuration c0 given c as the present system
configuration (where both c, c0 ∈ C). If pc represents the probability that c is chosen in a given deployment cycle through
randomizaP
tion, a switching strategy is f : C → pc where
pc = 1 ∀ p c ∈
c∈C

[0, 1]. To add to the complexity, the cost for switching from a configuration c to another configuration c0 can be nontrivial and non-

uniform. Thus, the aim of a good strategy is to maximize the effectiveness of an MTD system while trying to minimize the cost for
switching. Present state-of-the-art MTD system for web applications use a uniformly distributed switching strategy (pc = 1/|C|)
and assume that switching between configurations incur a uniform
cost [18].
We now develop a game theoretic system to generate switching strategies for the MTD web application that 1) shows a uniformly distributed switching strategy is sub-optimal and 2) considers the non-negative non-uniform costs of switching between different configurations of an MTD system.

4.

GAME THEORETIC MODELING

In this section, we model the setup of MTD systems in as a repeated step Bayesian Game.

4.1

Agents and Agent types

There are (N =) two players in our game, a defender and an attacker. The set θi is the set of types for player i (= {1, 2}). Thus, θ1
and θ2 denotes the set of defender and attacker types respectively.
The j−th attacker type is represented by θ2j .
When an attacker attacks an application, its beliefs about what
(resource/data) is most valuable to the application owner (defender)
remains consistent. Thus, we assume that the attacker knows that
there is only one type of defender when (s)he attacks a particular
web application. Thus, we have |θ1 | = 1.
We consider finite types of attackers. Each attacker type is defined in our model using a 3 tuple,
θ2i = hname, {(expertise, technologies) . . . }, probabilityi
where the second field is a set of two dimentional values that express an attacker’s expertise (∈ [0, 10]) in a technology. The rationale for using values in this range stems from the use of Common
Vulnerability Scoring System (CVSS) described later. Lastly, the
set of attacker types have a discrete probability distribution associated with it. The probability Pθ2j represents the defender’s belief
about the attacker type θ2j attacking their application. Obviously,
the probability values of all attacker types sum up to one.
X
Pθ2j = 1
θ2j ∈θ2

Note that one can define attacker expertise over a ‘category of attacks’ (like ‘BufferOverflowAttacks’) instead of technology specific
attacks. We feel the latter is more realistic for our domain. This definition captures the aspect that an attacker type can have expertise
in a set of technologies. Since, these attacker types and the probability distribution over them are application specific, it is defined by
a domain expert and taken as an input to our proposed model. For
instance, a defender using a no-SQL database in all configurations
of his MTD system, assigns zero probability to an ‘SQL_database’
attacker type because none of their attacks can compromise the security of his present system.
The assumption that the input probability distribution over all
the attacker types can be accurately specified is a strong one. We
later discuss how errors in judgment can affect the effectiveness of
a switching strategy and define a measure to capture the robustness
of the generated policy in such circumstances.

4.2

Agent actions

We define Aθi as a finite set of actions available to player i. The
defender action set, Aθ1 is a switch action to a valid configuration,
c of the web application. The maximum number of actions (or pure

strategies) for the defender can ideally be |C1 | × |C2 | · · · × |Cn |.
This might be lower since a technology used in layer x might not
be compatible when paired with a technology used in layer y (6= x)
rendering that configuration invalid.
For the attacker, Aθ2 represents the set of all attacks used by
atleast one attacker type. A particular attack a belongs to the set
Aθ2 if it affects atleast one of the technologies used in the layers
for our web application (C1 ∪ C2 · · · ∪ Cn ).
We now define a function f : (θ2t , a) → {1, 0} for our model.
The function implies an attack a is a part of the attacker type θ2t ’s
arsenal Aθ2t (⊆ Aθ2 ) if the value of the function is 1. This function value is based on the similarity between (i) the expertise of the
attacker type contrasted with the ‘exploitability’ necessary to execute the attack, and (ii) the attacker’s expertise in the technology for
which the attack can be used. We provide a concrete definition for
the function f after elaborating on what we mean by exploitability
of an attack.
For (almost all) CVEs listed in the NVD database, we have a sixdimensional CVSS v2 vector representing two independent scores
– Impact Score (IS) and Exploitability Score (ES). For an attack action a, ESa (∈ [0, 10]) represents the ease of exploitability (higher
is tougher). For each attack, the database also lists a set of technologies it affects, say T a .
Let us consider the set of technologies an attacker type t has
expertise in is Tt . Now we define the function f as,

1, iff Tt ∩ T a 6= φ ∧ ESa ≤ expertiset
f (θ2t , a) =
0 otherwise

4.3

Reward values for the Game

Now that we have attack sets for each attacker type, the general
reward structure for the proposed game is defined as follows:

 +xa if a ⊂ υ(c)
A
−ya if a can be detected or a ⊂ c0
Ra,θ
=
,c
2i
 0
otherwise

−x
if
a ⊂ υ(c)

d
D
+y
if
a can be detected or a ⊂ c0
Ra,θ
=
d
,c
2i

0
otherwise
A
D
where Ra,θ
and Ra,θ
are the rewards for the attacker type
2i ,c
2i ,c
and the defender respectively, when the attacker type θ2i uses an
attack action a against a configuration c (∈ C). The function υ(c)
represents the set of security vulnerabilities (CVEs) that configuration c has. Also, c0 refers to a honey-net configuration. A honey-net
is a configuration setup with intentional vulnerabilities to invite attackers for catching (or observing) them.
Note that the reward values when a attacker does not attack (NOOP action), is zero. Moreover, a defender gets zero reward for successfully defending a system. We reward him positively only if
he/she is able to reveal some more information or catch the attacker
without impacting operation requirements for the non-malicious
users (or using honey-nets). He gets a negative reward if an attacker
successfully exploits his(/her) system.
To obtain reward values for the variables xa , ya , xd and yd , we
make use of CVSSv2 metric. This metric provides the Impact (IS)
and Exploitability Scores (ES), stated above, which are combined
to calculate a third score called Base Score (BS) [11]. Using these,
we now define the following:

xd
xa

=
=

−1 ∗ IS
BS

Note that BS considers both the impact and the exploitability.
When the IS for two attacks are the same, the one that is easier

to exploit gets the attacker a higher reward value. The ease of an
attack can be interpreted in terms of the resource and effort spent
by an attacker for an attack Vs. the reward (s)he attains by harming
the defender. Although the robustness of our framework provides
provisions for having yd and ya , detecting attacks on a deployed
system or setting up honey-nets is still in its nascent stages. Hence,
there are no actions where values of yd or ya are required in our
present application.
Before we move on, we describe briefly what security dimensions the independent scores (IS and ES) are actually trying to capture in the context of a real world software system. For this purpose,
we first define the 6 independent values that generate these scores.
• Access Vector (AV) is dependent on the amount of access an
attacker needs to exploit a vulnerability. Thus, an attack that
needs physical access to a system will have lower score than
one that can be exploited over the Internet by any machine.
• Access Complexity (AC) represents the complexity of exploiting an attack. A buffer overflow attack on an Internet
service is less complex than an e-mail client vulnerability in
which a user has perform attachment downloads followed by
executing it and hence has lower AC value.
• Authentication (Au) level required to execute the attack. For
example, if no sign-up account is required to exploit the system, this value is high. In contrast, if one needs multiple accounts to exploit the vulnerability, the value is low.
• Confidentiality Impact (C) scores are low if only some (nonrelevant) information gets leaked. Highest impact occurs when
say, the entire database is compromised if the vulnerability is
successfully exploited.
• Integrity Impact (I) refers to the attacker’s power to modify files or behaviour of a system if he executes the exploit
successfully. The more the power– say the attacker is able
to change code or remove arbitrary files in the system– the
higher this value.
• Availability Impact (A) represents the power of a successful
exploit to bring down the availability of a system. A successful Denial of Service (DoS) that brings down an application
server, will have high impact.
From these values, one can obtain the two independent scores using
the following formulas,
20 ∗ (AV ) ∗ (AC) ∗ (Au)

ES

=

IS

= −10.41 ∗ (1 − (1 − C)(1 − I)(1 − A))

A rigorous treatment of assigning these values can be found in [11].
The CVSS values are generated by security experts across the globe
and the database is updated every single day.
Our model takes a time range as input. It then parses all the CVEs
(a) from the NVD in that time range to finally filter out the ones
that can affect atleast one of the configurations in our system (a ⊂
υ(ci )). Note that old CVEs are irrelevant for generating attack sets
for a relatively new MTD system as they either have no effect on the
updated versions of the technologies they can affect or have popular
solutions to prevent them while developing the application. For our
application, we obtain this input range from our security experts.

4.4

Switching Cost

The switching costs can be represented by a K n×n matrix where
the n rows (and columns) denote the n system configurations. The

cell Kij denotes the cost of switching when the defender moves
from configuration i to configuration j. As mentioned earlier, the
values in K are all non-negative. Our security experts, who have
written the code to automatically move from one configuration to
another, hand code these values in each cell of the martix. We provide some guidance in choosing these values here and give a concrete example on how we selected these for our application later.
If there is no common technology between configurations c and
c0 involved in a switch operation, the cost will be large. Also, switching technologies in a specific layer may incur more cost than switching technologies in other layers. In the developed MTD system, we
find that switching between databases incur large costs because the
structure of the data needs to be changed for shifting, and the time
required to copy huge amounts of data from one database to another
must also be accounted for.
The matrix K for our system turns out to be symmetric, i.e.
Kij = Kji ∀ i, j ∈ {1, . . . n}. Also, Kii = 0, which implies
that there is no cost if no configuration switch occurs. Note that
although our security experts think this is the structure of rewards
for the developed system, the modelling is generic enough to allow
for asymmetric costs. Lastly, we choose the values of Kij in the
range [0, 10]. The reason for this upper bound becomes clear in the
upcoming section.

5.

SWITCHING STRATEGY GENERATION

In this section, we first introduce the notion of Stackelberg Equilibrium for our security game, that gives us a defender strategy that
maximizes his reward (and thus the security of the system). We
briefly talk of optimization methods, relevant to our domain, that
can produce this. Finally, we incorporate the costs of switching into
the objective function and propose our solver.

5.1

Stackelberg Equilibrium

The strategy generated for the designed game needs to capture
the reconnaissance aspect. Note that the game starts only after the
defender has deployed the web application, acting as a leader. This
now becomes a repeated game in which an attacker can observe a finite number of switch moves and probabilistically learn the switching strategy (since |C|  ∞) of the defender. Thus, the defender
has to select a strategy that maximizes his reward in this game,
given that the attacker knows his strategy. This is exactly the problem of finding the Stackelberg Equilibrium in a Bayesian Game
[20]. The resulting mixed strategy is the switching strategy for the
defender in our MTD system. Unfortunately, this problem becomes
NP-hard in our case because of multiple attacker types [5].
Before we find a strong Stackelberg Equilibrium for our proposed game, we state a couple of well founded assumptions we
make. Firstly, an attacker chooses a pure strategy, i.e., a single attack action that maximizes his reward value. This assumption is
popular in prior work on security games because for every mixed
strategy for the attacker, there is always a pure strategy in support
for it [14]. Secondly, we assume that the pure strategy of an attacker
type is not influenced by the strategy of other attacker types. This
is not limiting for our web application domain since an attacker
type’s attack selection is independent of the attack action chosen
by another type.
To solve for the optimal mixed strategy, one can use the Decomposed Optimal Bayesian Stackelberg Solver (DOBSS) [13]. This
optimizes the expected reward of the defender over all possible
mixed strategies for the defender (~
x), and pure strategies for each
~θ ). We
attacker type (~nθ2i ) given the attacker type uncertainty (P
2i
now define the objective function of the Mixed Integer Quadratic

Program (MIQP) in Equation 1.
X X X
D
max
Pθ2i Ra,θ
xc naθ2i
2i ,c
x,n,v

Moving Target Defense
Web application

(1)

?

Incorporating Switching Costs

As defined in the last section, the cost for switching from a configuration i to a configuration j can be represented as Kij . The
probability the system is in configuration i and then switches to
configuration j is xi · xj . Thus, the cost incurred by the defender
for a switch action fromP
i toP
j is Kij · xi · xj . The expected cost
Kij · xi · xj .
for any switch action is
i∈C j∈C

To account for cost, we can subtract this from the objective function of Equation 1 with a cost-accountability factor α (≥ 0) to
obtain
X X X
XX
D
Pθ2i Ra,θ
x nθ2i −α·
max
Kij ·xi ·xj
2i ,c c a
c∈C θ2i ∈θ2 a∈Aθ
2i

i∈C j∈C

Unfortunately, this results in a Bilinear Mixed Integer Programming problem, which is not convex. To ameliorate this problem,
we now introduce new variables wij that essentially represent an
approximate value of xi · xj . We first use the piecewise linear McCormick envelopes to design a convex function using these wij s that estimates a good solution to this problem [21]. Along with
these constrains, we introduce further constrains which we describe
after introducing the final MIQP convex optimization problem as
follows,
max

x,n,v

X X

X

D
θ2i
Pθ2i Ra,θ
xc na
− α·
2i ,c

c∈C θ2i ∈θ2 a∈Aθ
2i

XX

Kij wij

i∈C j∈C

(2)

X

s.t.

xc

=

1

(3)

naθ2i

=

1

(4)

c∈C

X
a∈Aθ

0 ≤ v θ2i −

X

2i

A
Ra,θ
x
2i ,c c

Figure 1: A moving target defense web application system
xc ∈ [0 . . . 1], nθa2i ∈ {0, 1}, v θ2i ∈ R
∀ c ∈ C, θ2i ∈ θ2 , a ∈ Aθ2i

i∈C

j∈C

j∈C

constrains (10) and (11).
If we now allow the maximum cost of switching to be 10, we can
see that the values for the cost is comparable in magnitude to the
value of the defenders rewards. This helps us to provide a semantic
meaning for the cost-accountability factor, α.
The first term in the objective function seeks to maximize the
defender’s reward, which in turn maximizes the security of the web
application. The second term on the other hand, seeks to reduce
the expected cost of the switching actions. Thus, α represents how
much importance is given to the cost of switching Vs. the level of
security desired. Consider the extreme cases, when α = 0, we are
producing the most secure strategy (which is the Stackelberg Equilibrium) and considering switching between configurations incur
zero costs. This is the sub-set of solutions that are developed mostly
for physical security systems. In contrast to that, when α = 1, we
are saying that we consider switching costs as important as the security of our MTD application.
Choosing the correct value of α is not trivial and often dependent
on the specific web-application. For example, if a banking system
someday seeks to operate on a MTD system, we hope it puts more
weight on security than switching costs, selecting low α values. To
provide a sense to the reader, we later show in the experimental section, how strategies and reward values are effected with changing
alpha values.

wij
wij

≥ 0 ∀ i, j
≤ xi ∀ i, j

(6)
(7)

wij

≤ xj ∀ i, j

(8)

6.

(9)

The goal of this section is to answer three key questions. Firstly,
does our proposed Bayesian Stackelberg Game (BSG) model generate better strategies that the state-of-the-art? Secondly, can we
effectively compute the set of critical vulnerabilities? Lastly, who
are the sensitive attacker types and how robust is our model?

1 ∀ i, j

wij

=

wij

= xi ∀ i

(10)

wij

= xj ∀ j

(11)

j∈C

X

where M is a large positive number. ~nθ2i and v θ2i give the pure
strategy and its corresponding reward for the attacker type θ2i respectively, and ~
x gives the mixed switching strategy for the defender. (5) solves the dual problem of maximizing rewards for each
attacker type (v θ2i ) given the defender’s strategy. This ensures that
attackers always select the best attack action. The constrains (6), (7)
and (8) represent the McCormick envelope that provides lower and
upper
bounds on each wij . Since we consider all possible switches,
P P
xi · xj = 1. This is enforced by constrain (9). Lastly, for
j∈C i∈C
P
P
xj ) = xi . This is represented by the
xi · xj = xi · (
each i,

(5)

j∈C i∈C

X

Automated Code
converter

𝑐1 = {Python, MySQL}

≤ (1 − naθ2i )M

c∈C

XX

𝑐4 = {php,
postgreSQL}

𝑐3 = {php, MySQL}

2i

x,n,v

𝑐2 = {Python,
postgreSQL}

c∈C θ2i ∈θ2 a∈Aθ
2i

Notice that this does not consider that switching costs between defender strategies. Essentially, this means the formulation assumes
that switching costs are uniform. Before we address this limitation
in the upcoming subsection, we take a little digression.
For our scenario, we have many attack actions. Thus, we observe that solving the MIQP version is more efficient (in computation time and memory usage) than solving the Mixed Integer Linear
Program (MILP) version of the DOBSS. This can be attributed to
the fact that the MILP formulation results in an increase in the dimensions
solution space. Theoretically, the MIQP solves for
P of theP
|C| + θ2i ∈θ2 aj ∈Aθ |aj | variables where as the MILP solves
P
P 2i
for |C| ∗ θ2i ∈θ2 aj ∈Aθ |aj | variables.

5.2

𝑐1 = {Python,
MySQL}

6.1

EMPIRICAL EVALUATION

Test Bed Description

PHP,
MySQL
Python,
MySQL
PHP, postgreSQL
Python,
postgreSQL

PHP,
MySQL

Python,
MySQL

PHP, postgreSQL

Python,
postgreSQL

Name

0

2

6

10

2

0

9

5

6

9

0

2

10

5

2

0

Script Kiddie
(SK)
Database
Hacker (DH)
Mainstream
Hacker (MH)

Table 1: Swithing costs for our system

• Switching between different languages while keeping the same
database dialect incurs minimal cost - workload is primarily
done is primarily on rerouting to the correct server with the
source language
• Switching between different database dialects while keeping
the same language incurs a slightly higher cost due to the
conversion required for the database structure and its contents. One also has to account for copying large amounts of
data to the database used in the current system configuration.
• Switching between different database dialects AND different
languages incur the most cost due to the combination of the
costs of conversion for the database as well as the penalty for
rerouting to the correct server with the source language.
The attacker types along with the attack action set size are defined in Table 2. We mined the NVD for obtaining CVE data from
January, 2013 to August, 2016 to generate these attack sets. If the
stakes of getting caught are too high for an attacker type given an
MTD system, he/she may choose not to attack. Hence, we have a
NO-OP action for each attacker type.
The optimization problems for the experiments were solved using Gurobi on an Intel Xeon E5 2643v3@3.40GHz machine with
6 cores and 64GB of RAM.

6.2

Strategy Evaluation

We evaluate our method using Bayesian Stackelberg Games on
our real life web application against the Uniform Random Strategy (URS), which is the state-of-the-art in such systems [18]. We
plot the values of the objective function in Equation 2 for both the

Prob.

|Aθ2i |

0.15

34

0.35

269

0.5

48

Table 2: Attacker types and attack action counts
BSG
U RS

−4
Obj

To answer the questions mentioned above, we develop a real
world MTD web application (shown in Figure 1) with 2 layers.
The key idea of applying MTD to web applications requires you to
have several versions of the same system, each written in either a
different language, using a different database, etc.
This diversity is not ubiquitous in legacy web applications, due
to cost, time, and resources required to build several versions of the
same web application. To aid this, we developed a framework to
automatically generate the diversity necessary for this web application. The current prototype is able to convert a web application
coded in Python to an equivalent one coded in PHP, and vice versa,
as well as a web application using a MySQL database to an identical version that uses PostgreSQL, and vice versa. In the future,
as more and more variations are developed, the set of defender’s
actions will increase.
The present set of valid configurations for our system is C =
{(PHP, MySQL), (Python, MySQL), (PHP, postgreSQL), (Python,
postgreSQL )}. The costs for switching between configurations is
shown in Table 1. These cost values generated by our system administrators are based on the following considerations:

(Technologies,
Expertise)
(PHP,4),
(MySQL,4)
(MySQL,10),
(postgreSQL,8)
(Python,4),
(PHP,6),
(MySQL,5)

−6
−8

0

0.2

0.4

0.6

0.8

1

α

Figure 2: Objective function values for Uniform Random Strategy Vs. Bayesian Stackelberg Game with switching costs as α
varies from 0 to 1.
strategies as α varies from 0 to 1. For URS, we use the exact values of wij = 0.25 ∗ 0.25 = 0.0625 ∀ i, j. The plot is shown in
Figure 2. Both are straight lines because although the value of α
changes, the strategy for URS is same (by definition) and the one
generated by BSG also remains the same. The latter case came as
a surprise to us initially. On further investigation, we noticed that
in the formulated game for our web-application, the Stackelberg
Equilibrium for our application (luckily) coincides with the least
switching cost strategy.
These attacker and defender strategies is shown in Table 3 alongwith the value of the defender’s reward (i.e. the first term in the objective function in Equation 2). Notice that, not only is the mixed
strategy generated by BSG more secure than URS, it leverages
fewer configurations than all valid configurations |C| = 4 the system has to offer. This result is in unison with previous work in
cyber-security which show that having many configurations does
not necessary imply that all of them have to be used for providing
the best security [3].

6.2.1

Studying the effect of α-values
To empirically show that our solver is actually considering costs
of switching, we change the value for switching from (PHP, postgreSQL ) to (Python, postgreSQL) and vice-versa from 2 (yellow boxes in Table 1) to 10. We plot this scenario in Figure 3. As
soon as α ≥ 0.4, the BSG generates (0.25, 0.25, 0.25, 0.25, 0.25)
(which is URS) as the most optimal strategy. After analysis, we
note that this happens because the most powerful attack actions in
the arsenal of the attacker types are for the systems (PHP, MySQL)
and (Python, MySQL). When, one does not prioritize switching
costs (α ∈ {0, 0.1, 0.2, 0.3}), the system keeps switching between
the more secure configurations nullifying the good attacks of the
attackers. As switching costs start to get more significant (α ∈
{0.4, 0.5, . . . 1.2}), the objective function value reduces if it sticks
to the stronger configurations since switching costs are now high
for these. It switches to the URS in this case. Beyond that, it switches

Method

Mixed Strategy

Defender’s
Reward

Attack sets (SK,
DH, MH)

k

CV sets

URS

(0.25, 0.25, 0.25,
0.25)

-5

1
2

BSG

(0, 0, 0.5, 0.5)

-3.25

CVE-2016-3477,
CVE-2015-3144,
CVE-2016-3477
CVE-2014-0185,
CVE-2014-0067,
CVE-2014-0185

{(CVE-2014-0185)}
{(CVE-2014-0185,
CVE-2015-5652)}

P (configuration= c)

Table 3: Comparison between the strategies generated by Uniform Random Strategy (URS) Vs. Bayesian Stackelberg Game
(BSG)
php_mysql
py_mysql
php_psql
py_psql

0.4

0.2

0
0

0.5

1

1.5

2

2.5

α→
BSG
U RS

Obj

−5
−10
−15

0

0.5

1

1.5

2

2.5

to the strategy (0.25, 0.5, 0, 0.25) as α keeps on increasing. When
α becomes close to 2, it completely ignores the security of the
system and tries to minimize the switching cost by proposing the
strategy (0.5, 0.5, 0, 0) as the cost for switching between (PHP,
MySQL) and (Python, MySQL) is the least (= 2).
In the bottom of Figure 3, we showcase the change in the values of objective function. At the start, the BSG generates a better
strategy when compared to URS. When the BSG strategy becomes
the same as the URS (for 0.4 ≤ α ≤ 1.2), we observe that the
objective function value for BSG is lower than URS. This is not
surprising since BSG is merely trying to estimate the value xi · xj
with the variables wij , whereas URS is using the exact value. As
we increase α further, we are essentially discouraging an MTD system, since now the cost of switching has become so high, whereas
naive URS pays no heed to this.

6.3

Identifying Critical Vulnerabilities

In real-world development teams, it is impossible to solve all the
vulnerabilities, especially in a system with so many technologies.
In current software systems, given a set of vulnerabilities, a challenging question often asked is which vulnerabilities should one fix

CPU Time
3m15s
421m27s

Table 4: Most critical vulnerability in the MTD system and the
time required to generate it.

to improve the security?
For an MTD system, this becomes a tough problem since the defender needs to reason about multiple attacker types– their probabilities and attack actions. For a given k, the set of k vulnerabilities,
which on being fixed, result in the highest gain in defender strategy,
is termed as the k critical vulnerability set (k−CV).
To address this problem, we remove each k-sized attack set from
the set of all attacks (A02 = A2 \ D ∀ D ⊂ A2 & |D| = k)
and evaluate the objective function (Equation 2). The sets A02 that
yield the highest objective values, provide the vulnerabilities D that
should be fixed to improve the defender’s system.
We tried to study this complicated behaviour for some toy examples before applying it to our application. An interesting phenomenon we noticed was that a k-set critical vulnerabilities (k−CV)
is not always a subset of the (k + 1)−CV. Suppose we want to find
3 vulnerabilities that we want to fix. Since it is not just a super-set
of the 2-CV, we need to solve this problem from the scratch with
k = 3. Hence, there is going to be combinatorial explosion here.
As

the value of k increases, we end up solving |A02 | = |Ak2 | MIQP
problems to identify the k−CVs.

6.3.1

α→

Figure 3: Top: Showcases the change in probabilities associated
with a particular configuration. Bottom: Objective function
values for Uniform Random Strategy Vs. Bayesian Stackelberg
Game with switching costs as α varies from 0 to 2.5 when the
cost of switching are as showcased in Table 1 with the values in
the yellow boxes being 10.

Objective
Value
-2.435
-1.973

Finding Critical Vulnerabilities in the Developed System

For our system, we start with k = 1, we increase number of critical vulnerabilities to be found by 1 at each step. The result remains
the same for α ∈ [0, 1] for our system. We do not play around
with α beyond this, mostly because this would be unrealistic for
any practical application. Unfortunately, the brute force approach
and the scalability of algorithms for solving normal extensive form
BSGs proves to be a key limitation.
This is not a surprise since the total number of unique CVEs
spread out among
the attackers is 287. When k = 3, we end up

solving 287
optimization
problems, which fails to scale in both
3
time and memory. Thus, we only show critical vulnerabilities identified up to k = 2 (in Table 4) using α = 0.2.
At present, we are trying to develop a single MIQP formulation
that tries to approximately generate the k-CV set. To reduce the
combinatorial explosion, we plan to use switch variables that can
turn attack actions on and off. This comes at the cost of increasing
the number of variables in the formulated optimization problem.

6.4

Model Robustness & Attacker
Type Sensitivity

It is often the case that a web application administrator (defender)
cannot accurately specify the probability for a particular attacker
type. In this section, we see how this uncertainty affects the optimal rewards generated by the system. We provide a notion for
determining sensitive attacker types and measuring the robustness
of a switching strategy.
For each attacker type i, we vary the probability Pθ2i by ±x%
x
)) where x is the sensitivity factor, which
(Pθnew
= Pθ2i (1 ± 100
2i
can be varied from a low value to a high value as needed. Note that
x
now p = Pθ2i × 100
needs to be adjusted or distributed amongst the

probabilities of the remaining attacker types. To make sure that this
distribution is done such that the sensitivity of attacker i actually
stands out, we propose to distribute p amongst the other attacker
types using a weighted model as per their existing probabilities as
shown below. For attacker j (6= i), its new probability would be:
= Pθ2j (1 ∓

P p
Pθ
k(6=i)

)
2k

NLR(BSG)

Pθnew
2j

M ainstreamHacker(M H)

2

Rn −Ro
Rn

−100

−50

CONCLUSIONS AND FUTURE WORK

In this paper, we propose a method to generate a switching strategy for real-world web application based on the Moving Target Defense (MTD) architecture. To find an effective switching strategy,
we model the system as a repeated Bayesian game. We develop
methods to assign attack actions to attacker types and generate realistic utilities based on expertise of security professionals. For obtaining real-world attack data, we mine vulnerabilities in the National Vulnerability Database (NVD) and obtain utilities based on
the Common Vulnerability Scoring System (CVSS). We formulate
an optimization problem which outputs a switching strategy that
maximizes system security while accounting for switching costs.
The generated strategy is shown to be more effective than the stateof-the-art for a real-world application. We also provide metrics that
can be used to validate the robustness of switching strategies, absent in literature for multi-agent cyber-security systems. Lastly, we
propose the problem of identifying critical vulnerabilities and provide a solution.
The techniques in this paper are not limited to only web applications. The attack actions mined from the security databases relate to

50

100

50

100

NLR(URS)

DatabaseHacker(DH)
ScriptKiddie(SK)

5

0
−100

Evaluation Based on the Developed System

0

M ainstreamHacker(M H)

(13)

We compute the attacker sensitivity for our system varying the
probability of each attacker type from −100% to +100% (of its
modeled probability) with 10% step sizes. We plot the results in
Figure 4 using Equation 13. The Mainstream and Database hacker
(MH & DH) are the least sensitive attacker types. The NLR values
for both these attackers are 0. This is the case since the real world
attack action used by these types remain the same even when their
probabilities change. On the other hand, if the probability associated with the Script Kiddie (SK) is underestimated in our model,
we see that the strategies deviate substantially from the optimal.
For our experiments in this section, we use α = 0.2. The max
NLR for our BSG strategy is 2.35 Vs. 9 for URS. The average of
the 60 NLR values is 0.061 for BSG and 0.88 for URS. These values indicate our model is more robust to variance in attacker type
uncertainty than the present state-of-the-art.

7.

1

0

Note that NLR values are ≥ 0. Higher values of NLR represent
more sensitive attacker types. Inaccurate probability estimates for
the sensitive attackers can be detrimental to the security of our
application. Note that lower NLR values indicate that a generated
strategy is more robust.

6.4.1

ScriptKiddie(SK)

(12)

~θ , then the sign in
When x% is subtracted from the probability P
2i
the above equation becomes positive, and vice-versa.
We now formally define the loss in reward to the defender as the
probability distribution over the attacker types change. Let Ro be
the overall reward for the defender when he uses the mixed strategy
for the assumed (and possibly incorrect) model of attacker type un~θnew ). Let Rn be the defender’s
~θ2 ) on the true model (P
certainty (P
2
optimal reward value for the true model. We compute the Normalized Loss in Rewards (NLR) for the defender’s strategy as follows:
NLR =

DatabaseHacker(DH)

−50

0

Varying sensitivity of attacker types (%)

Figure 4: NLR values for BSG and URS genereated strategies
when attacker types probabilities vary in [−100%, 100%].
all kinds of technologies, like operating systems, coding languages
etc. Hence, the modelling should be relevant to any software applications using the MTD architecture. It would be interesting to see
how effective they are in such scenarios.
Investigating the reward structure for a particular problem has
helped design provably fast solvers in the physical security domains. We believe this direction of research might help in developing faster solvers, alleviating the scalability problem of identifying
critical vulnerabilities, for the cyber-security domain as well.

8.

ACKNOWLEDGMENTS

This work was partially supported by the grants from National
Science Foundation (NSF-SFS-1129561) and the Center for Cybersecurity and Digital Forensics at Arizona State University.

REFERENCES
[1] K. Amin, S. Singh, and M. Wellman. Gradient methods for
stackelberg security games. AAMAS, 2016.
[2] D. Balzarotti, M. Cova, V. Felmetsger, N. Jovanovic,
E. Kirda, C. Kruegel, and G. Vigna. Saner: Composing static
and dynamic analysis to validate sanitization in web
applications. In Security & Privacy 2008. IEEE Symposium,
pages 387–401, 2008.
[3] K. M. Carter, J. F. Riordan, and H. Okhravi. A game
theoretic approach to strategy determination for dynamic
platform defenses. In ACM MTD Workshop, 2014, MTD ’14.
ACM, 2014.
[4] M. Carvalho and R. Ford. Moving-target defenses for
computer networks. Security Privacy, IEEE, 12(2):73–76,
Mar 2014.
[5] V. Conitzer and T. Sandholm. Computing the optimal
strategy to commit to. In Proceedings of the 7th ACM
Conference on Electronic Commerce, EC ’06, pages 82–90,
New York, NY, USA, 2006. ACM.
[6] A. Doupé, L. Cavedon, C. Kruegel, and G. Vigna. Enemy of
the state: A state-aware black-box web vulnerability scanner.
In USENIX Security Symposium, 2012.

[7] S. H. Houmb, V. N. Franqueira, and E. A. Engum.
Quantifying security risk level from cvss estimates of
frequency and impact. JSS, 83(9):1622–1634, 2010.
[8] S. Jones, A. Outkin, J. Gearhart, J. Hobbs, J. Siirola,
C. Phillips, S. Verzi, D. Tauritz, S. Mulder, and A. Naugle.
Evaluating moving target defense with pladd. Technical
report, Sandia National Labs-NM, Albuquerque, 2015.
[9] P. Manadhata. Game theoretic approaches to attack surface
shifting. In Moving Target Defense II, volume 100 of AIS,
pages 1–13. Springer New York, 2013.
[10] J. McCumber. Information systems security: A
comprehensive model. In Proceedings of the 14th National
Computer Security Conference, 1991.
[11] P. Mell, K. Scarfone, and S. Romanosky. Cvss v2 complete
documentation, 2007.
[12] H. Okhravi, T. Hobson, D. Bigelow, and W. Streilein.
Finding focus in the blur of moving-target techniques.
Security & Privacy, IEEE, 12(2):16–26, 2014.
[13] P. Paruchuri, J. P. Pearce, J. Marecki, M. Tambe, F. Ordonez,
and S. Kraus. Playing games for security: An efficient exact
algorithm for solving bayesian stackelberg games. In
AAMAS, 2008, pages 895–902, 2008.
[14] J. Pita, M. Jain, J. Marecki, F. Ordóñez, C. Portway,
M. Tambe, C. Western, P. Paruchuri, and S. Kraus. Deployed
ARMOR protection: the application of a game theoretic
model for security at the los angeles international airport. In
AAMAS 2008, Industry and Applications Track Proceedings,
pages 125–132, 2008.
[15] A. Prakash and M. P. Wellman. Empirical game-theoretic
analysis for moving target defense. In ACM MTD Workshop,
2015, 2015.
[16] J. Silver-Greenberg, M. Goldstein, and N. Perlroth.
JPMorgan Chase Hacking Affects 76 Million Households. In
The New York Times, 2014.
[17] A. Sinha, T. Nguyen, D. Kar, M. Brown, M. Tambe, and
A. X. Jiang. From physical security to cyber security.
Journal of Cybersecurity, 2016.
[18] M. Taguinod, A. Doupé, Z. Zhao, and G.-J. Ahn. Toward a
Moving Target Defense for Web Applications. In
Proceedings of 16th IEEE IC-IRI, 2015.
[19] M. Van Dijk, A. Juels, A. Oprea, and R. L. Rivest. Flipit: The
game of “stealthy takeover”. Journal of Cryptology,
26(4):655–713, 2013.
[20] H. Von Stackelberg. Market structure and equilibrium.
Springer SBM, 2010.
[21] D. S. Wicaksono and I. Karimi. Piecewise milp under-and
overestimators for global optimization of bilinear programs.
AIChE Journal, 54(4):991–1008, 2008.
[22] D. Wichers. Owasp top-10. OWASP, 2013.
[23] M. Winterrose, K. Carter, N. Wagner, and W. Streilein.
Adaptive attacker strategy development against moving
target cyber defenses. arXiv:1407.8540, 2014.
[24] R. Zhuang, S. A. DeLoach, and X. Ou. Towards a theory of
moving target defense. In ACM MTD Workshop, 2014, pages
31–40. ACM, 2014.

Moving Target Defense for Web Applications using
Bayesian Stackelberg Games
Sailik Sengupta, Satya Gautam Vadlamudi, Subbarao Kambhampati
Yochan Group, School of CIDSE
Arizona State University
{sailiks, gautam, rao}@asu.edu

Marthony Taguinod, Adam Doupé, Ziming Zhao, Gail-Joon Ahn
SEFCOM Lab, School of CIDSE
Arizona State University
{mtaguino, doupe, zmzhao, gahn}@asu.edu

ABSTRACT
The present complexity in designing web applications makes software security a difficult goal to achieve. An attacker can explore
a deployed service on the web and attack at his/her own leisure.
Moving Target Defense (MTD) in web applications is an effective
mechanism to nullify this advantage of their reconnaissance but the
framework demands a good switching strategy when switching between multiple configurations for its web-stack. To address this issue, we propose modeling of a real-world MTD web application
as a repeated Bayesian game. We then formulate an optimization
problem that generates an effective switching strategy while considering the cost of switching between different web-stack configurations. To incorporate this model into a developed MTD system,
we develop an automated system for generating attack sets of Common Vulnerabilities and Exposures (CVEs) for input attacker types
with predefined capabilities. Our framework obtains realistic reward values for the players (defenders and attackers) in this game
by using security domain expertise on CVEs obtained from the National Vulnerability Database (NVD). We also address the issue of
prioritizing vulnerabilities that when fixed, improves the security
of the MTD system. Lastly, we demonstrate the robustness of our
proposed model by evaluating its performance when there is uncertainty about input attacker information.

1.

INTRODUCTION

Present day web applications are widely used by businesses to
provide services over the Internet. Oftentimes, sensitive business
and user data are managed by these applications. Vulnerabilities in
these web applications pose serious threats to the confidentiality
and integrity of both businesses and users [16].
There exist numerous static (white-box) and dynamic (blackbox) analysis tools for identifying vulnerabilities in a system [2,
6]. These have become less effective in present times due to the
increasing complexity of web applications, their dependency on
downstream technologies, and the limited development and deployment time [22]. Worse yet, the attackers, with time on their side,
can perform reconnaissance and attack. To address this challenge,
we consider a Moving Target Defense (MTD) based approach [4],

A shorter version of this paper appears in: Proceedings of the
15th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2016), May 9–13, 2016, Singapore.

which complements the existing vulnerability analysis techniques
through a defense-in-depth mechanism.
The MTD based approach dynamically shifts a system over time
to increase the uncertainty and complexity for the attackers to perform probing and attacking [24], while ensuring that the system
is available for legitimate users. As the window of attack opportunities decreases, the effort in finding and successfully executing
an attack increases. Moreover, if an attacker succeeds in finding a
vulnerability at one point in time, it may not be exploitable at another time because of the moving defense system, making the web
application more resilient [18].
Various aspects that support Moving Target Defense approach
such as, using multiple implementation languages, multiple database
instances with synchronization, etc. are considered in different layers of web application architecture, along with ways to switch between them. However, the design of good quality switching strategies itself is left as an open problem. This is key to effectively leverage various move options—thereby maximizing the complexity for
the attacker and minimizing the damage for the defender.
Our aim in this paper is to design effective switching policies for
movement in the MTD system that maximize the security of the
web application, given the set of components and configurations
of the system which can be “moved around”, while simultaneously
considering realistic costs for “moving them around”. In web applications, the defender (leader) deploys a system up-front. The attacker observes (or follows) the system over time before choosing
an attack. These characteristics motivate us to formulate the MTD
system as a repeated Bayesian Stackelberg Game (BSG). For this
formulation to be meaningful in real-world applications, we use
real world attack data for our model. We propose a framework to
define attacker types for our game and automatically generate attack options for each of them by mining and characterizing Common Vulnerabilities and Exposures (CVEs). We develop a system
that leverages the knowledge in public attack databases and expertise of system administrators for obtaining meaningful game utilities and switching costs respectively.
For computing the movement policy for the defender, we initially
expected to be able to use existing solvers developed for physical
security systems [17]. Unfortunately, none of them considered the
cost of switching between strategies. Since this is highly relevant in
cyber-security systems, we had to formulate an optimization problem to consider these costs when generating strategies.
The increased complexity in an MTD systems exacerbates the
difficulty of prioritizing vulnerabilities that need to be fixed next.
We define this problem formally and propose a preliminary solu-

tion. Lastly, we talk about metrics to measure the robustness of
switching strategies generated by various models when the uncertainty about attacker types vary in the real world.
In section 2, we introduce the reader to the different ways people
have tried to address the problem of generating switching strategies for cyber-security systems. We introduce domain terminology
related to MTD systems for web-applications in section 3. In section 4, we develop the Bayesian Game model for this system defining attacker types, attack classification, rewards generated from security databases and strategy switching costs. To find an effective
switching strategy, we propose a solver that maximizes system security while accounting for switching costs in section 5. We empirically study the effectiveness and robustness of the strategy generated by our framework in section 6, comparing it to the stateof-the-art. We also formulate the problem of identifying critical
vulnerabilities and propose a preliminary solution in section 6. We
conclude the paper in section 7, highlighting promising research
directions.

2.

security systems. Unfortunately, these works, to our knowledge, do
not consider the cost the defenders incur when asked to switch from
a particular strategy to another. Hence, we propose a solver that
maximizes the defender’s reward and minimizes the overall cost
of switching between web-application configurations. Our solver
is essentially an extension of the DOBSS solver [13]. Although
there has been furhter development since DOBSS, the more recent solvers for BSGs make additional assumptions about the game
structure—either about the action sets of the defender, or the presence of hierarchical structure among attacker types [1], which do
not hold for the web application domain.
The use of Common Vulnerabilitiy Scoring System (CVSS) for
rating attacks is well studied in security [7]. We describe this metric later. CVSS provides a strong backbone for obtaining utilities
for our game theoretic model. None of the existing works (to our
knowledge) talk about the pragmatic aspect of prioritizing vulnerabilites in MTD systems. Also, there does not seem to be any standard metrics to capture the robustness of strategies generated by a
model. We address both these issues in the upcoming sections.

RELATED WORK

Although there exists prior work on the design of switching strategies for MTD systems, most of it is domain specific. Evaluation of
these strategies on real-world MTD systems for web applications
is scarce. We discuss some of these works, highlighting their limitations in the domain of web applications, thus motivating the need
for our solution. Existing efforts describe the use of randomized
switching strategies, and show its effectiveness for MTD systems
[24]. We empirically demonstrate that our strategy outperforms this
state-of-the-art for web applications, especially when the cost of
switching is negligible.
Attacker-defender scenarios have been modeled earlier as stochastic games for attack-surface shifting [9]. Other works model the
MTD problem as a repeated game where the defender uses uniform
random strategy with the exception that the same defense configuration is not deployed in two consecutive rounds [23]. This work
needs an in-depth analysis of code, which is unrealistic for complex
web applications.
Switching strategies for MTD systems based on detection of probes
by attackers are presented by [15]. Unfortunately, an accurate detection of attacks in web applications is difficult, if not impossible.
Furthermore, such strategies can lead to a detrimental performance
in repeated games if an intelligent attacker biases the system to
switch more towards MTD configurations where the attacker attains higher reward. In [8], the MTD system is modeled as a game
called PLADD, based on FlipIt games [19]. This work assumes
that different agents control the server in different game rounds,
which is impractical for most cyber-seccurity applications, essentially web applications. These techniques also fail to capture the
reconnaissance aspect of the attackers which is shown to be an important aspect in the attack phase [12].
In [3], a game theoretic leader-follower type approach is presented for a dynamic platform defense where the strategies are chosen so as to be diverse, based on statistical analysis rather than
being uniformly distributed. They find similarity among different
configurations of the MTD system, which is difficult in the domain
of web applications. The work fails to consider the uncertainty in
the attacker model and the costs for switching.
These aspects of uncertainty in the attacker model and attacker
reconnaissance are handled effectively via Bayesian Stackelberg
Games (BSG), making it an appropriate choice for modeling the
web applications domain. Our modeling could help us leverage the
existing solution methods in the physical security domains [17]
and provide scalable and optimal switching strategies for cyber-

3.

MOVING TARGET DEFENSE FOR WEB
APPLICATIONS

In this section, we present a brief overview of the web application domain and its functionality which will be useful for understanding the challenges involved in generating solution strategies.

3.1

Configuration

A configuration set for a web application stack is denoted as C =
C1 × C2 · · · × Cn where there are n-technological stacks. Here, Ci
denotes the set of technologies that can be used in the i-th layer
of the application stack. A valid configuration c is an n-tuple that
preserves the system’s operational goals.
Consider a web application that has two layers (n = 2) where
the first layer denotes the coding language the web-application was
coded in and the second layer denotes the database that stores the
data handled by this application. Say, the technologies used in each
layer are C1 = {Python, PHP} and C2 = {MySQL, postgreSQL}.
A valid configuration can be (PHP, MySQL). The diversity of an
MTD system, which is the number of valid configurations, can be
4 (at max) in this case.

3.2

Attack

Software security is defined in terms of three characteristics Confidentiality, Integrity and Availability [10]. In a broad sense, an
attack on a web application is defined as an act that compromises
any of the aforementioned characteristics. The National Vulnerability Database (NVD) is a public directory of known vulnerabilities
and exposures affecting all technologies that can be used in a web
application. The Common Vulnerabilities and Exploits (CVEs) in
this database list vulnerabilities and corresponding attacks that can
be used to compromise an application using the affected technology. As each CVE has an exploit associated with it, we use the
terms vulnerability and attack interchangeably going forward.

3.3

Switching Strategy

This is a decision making process for the defender to select the
next valid system configuration c0 given c as the present system
configuration (where both c, c0 ∈ C). If pc represents the probability that c is chosen in a given deployment cycle through
randomizaP
tion, a switching strategy is f : C → pc where
pc = 1 ∀ p c ∈
c∈C

[0, 1]. To add to the complexity, the cost for switching from a configuration c to another configuration c0 can be nontrivial and non-

uniform. Thus, the aim of a good strategy is to maximize the effectiveness of an MTD system while trying to minimize the cost for
switching. Present state-of-the-art MTD system for web applications use a uniformly distributed switching strategy (pc = 1/|C|)
and assume that switching between configurations incur a uniform
cost [18].
We now develop a game theoretic system to generate switching strategies for the MTD web application that 1) shows a uniformly distributed switching strategy is sub-optimal and 2) considers the non-negative non-uniform costs of switching between different configurations of an MTD system.

4.

GAME THEORETIC MODELING

In this section, we model the setup of MTD systems in as a repeated step Bayesian Game.

4.1

Agents and Agent types

There are (N =) two players in our game, a defender and an attacker. The set θi is the set of types for player i (= {1, 2}). Thus, θ1
and θ2 denotes the set of defender and attacker types respectively.
The j−th attacker type is represented by θ2j .
When an attacker attacks an application, its beliefs about what
(resource/data) is most valuable to the application owner (defender)
remains consistent. Thus, we assume that the attacker knows that
there is only one type of defender when (s)he attacks a particular
web application. Thus, we have |θ1 | = 1.
We consider finite types of attackers. Each attacker type is defined in our model using a 3 tuple,
θ2i = hname, {(expertise, technologies) . . . }, probabilityi
where the second field is a set of two dimentional values that express an attacker’s expertise (∈ [0, 10]) in a technology. The rationale for using values in this range stems from the use of Common
Vulnerability Scoring System (CVSS) described later. Lastly, the
set of attacker types have a discrete probability distribution associated with it. The probability Pθ2j represents the defender’s belief
about the attacker type θ2j attacking their application. Obviously,
the probability values of all attacker types sum up to one.
X
Pθ2j = 1
θ2j ∈θ2

Note that one can define attacker expertise over a ‘category of attacks’ (like ‘BufferOverflowAttacks’) instead of technology specific
attacks. We feel the latter is more realistic for our domain. This definition captures the aspect that an attacker type can have expertise
in a set of technologies. Since, these attacker types and the probability distribution over them are application specific, it is defined by
a domain expert and taken as an input to our proposed model. For
instance, a defender using a no-SQL database in all configurations
of his MTD system, assigns zero probability to an ‘SQL_database’
attacker type because none of their attacks can compromise the security of his present system.
The assumption that the input probability distribution over all
the attacker types can be accurately specified is a strong one. We
later discuss how errors in judgment can affect the effectiveness of
a switching strategy and define a measure to capture the robustness
of the generated policy in such circumstances.

4.2

Agent actions

We define Aθi as a finite set of actions available to player i. The
defender action set, Aθ1 is a switch action to a valid configuration,
c of the web application. The maximum number of actions (or pure

strategies) for the defender can ideally be |C1 | × |C2 | · · · × |Cn |.
This might be lower since a technology used in layer x might not
be compatible when paired with a technology used in layer y (6= x)
rendering that configuration invalid.
For the attacker, Aθ2 represents the set of all attacks used by
atleast one attacker type. A particular attack a belongs to the set
Aθ2 if it affects atleast one of the technologies used in the layers
for our web application (C1 ∪ C2 · · · ∪ Cn ).
We now define a function f : (θ2t , a) → {1, 0} for our model.
The function implies an attack a is a part of the attacker type θ2t ’s
arsenal Aθ2t (⊆ Aθ2 ) if the value of the function is 1. This function value is based on the similarity between (i) the expertise of the
attacker type contrasted with the ‘exploitability’ necessary to execute the attack, and (ii) the attacker’s expertise in the technology for
which the attack can be used. We provide a concrete definition for
the function f after elaborating on what we mean by exploitability
of an attack.
For (almost all) CVEs listed in the NVD database, we have a sixdimensional CVSS v2 vector representing two independent scores
– Impact Score (IS) and Exploitability Score (ES). For an attack action a, ESa (∈ [0, 10]) represents the ease of exploitability (higher
is tougher). For each attack, the database also lists a set of technologies it affects, say T a .
Let us consider the set of technologies an attacker type t has
expertise in is Tt . Now we define the function f as,

1, iff Tt ∩ T a 6= φ ∧ ESa ≤ expertiset
f (θ2t , a) =
0 otherwise

4.3

Reward values for the Game

Now that we have attack sets for each attacker type, the general
reward structure for the proposed game is defined as follows:

 +xa if a ⊂ υ(c)
A
−ya if a can be detected or a ⊂ c0
Ra,θ
=
,c
2i
 0
otherwise

−x
if
a ⊂ υ(c)

d
D
+y
if
a can be detected or a ⊂ c0
Ra,θ
=
d
,c
2i

0
otherwise
A
D
where Ra,θ
and Ra,θ
are the rewards for the attacker type
2i ,c
2i ,c
and the defender respectively, when the attacker type θ2i uses an
attack action a against a configuration c (∈ C). The function υ(c)
represents the set of security vulnerabilities (CVEs) that configuration c has. Also, c0 refers to a honey-net configuration. A honey-net
is a configuration setup with intentional vulnerabilities to invite attackers for catching (or observing) them.
Note that the reward values when a attacker does not attack (NOOP action), is zero. Moreover, a defender gets zero reward for successfully defending a system. We reward him positively only if
he/she is able to reveal some more information or catch the attacker
without impacting operation requirements for the non-malicious
users (or using honey-nets). He gets a negative reward if an attacker
successfully exploits his(/her) system.
To obtain reward values for the variables xa , ya , xd and yd , we
make use of CVSSv2 metric. This metric provides the Impact (IS)
and Exploitability Scores (ES), stated above, which are combined
to calculate a third score called Base Score (BS) [11]. Using these,
we now define the following:

xd
xa

=
=

−1 ∗ IS
BS

Note that BS considers both the impact and the exploitability.
When the IS for two attacks are the same, the one that is easier

to exploit gets the attacker a higher reward value. The ease of an
attack can be interpreted in terms of the resource and effort spent
by an attacker for an attack Vs. the reward (s)he attains by harming
the defender. Although the robustness of our framework provides
provisions for having yd and ya , detecting attacks on a deployed
system or setting up honey-nets is still in its nascent stages. Hence,
there are no actions where values of yd or ya are required in our
present application.
Before we move on, we describe briefly what security dimensions the independent scores (IS and ES) are actually trying to capture in the context of a real world software system. For this purpose,
we first define the 6 independent values that generate these scores.
• Access Vector (AV) is dependent on the amount of access an
attacker needs to exploit a vulnerability. Thus, an attack that
needs physical access to a system will have lower score than
one that can be exploited over the Internet by any machine.
• Access Complexity (AC) represents the complexity of exploiting an attack. A buffer overflow attack on an Internet
service is less complex than an e-mail client vulnerability in
which a user has perform attachment downloads followed by
executing it and hence has lower AC value.
• Authentication (Au) level required to execute the attack. For
example, if no sign-up account is required to exploit the system, this value is high. In contrast, if one needs multiple accounts to exploit the vulnerability, the value is low.
• Confidentiality Impact (C) scores are low if only some (nonrelevant) information gets leaked. Highest impact occurs when
say, the entire database is compromised if the vulnerability is
successfully exploited.
• Integrity Impact (I) refers to the attacker’s power to modify files or behaviour of a system if he executes the exploit
successfully. The more the power– say the attacker is able
to change code or remove arbitrary files in the system– the
higher this value.
• Availability Impact (A) represents the power of a successful
exploit to bring down the availability of a system. A successful Denial of Service (DoS) that brings down an application
server, will have high impact.
From these values, one can obtain the two independent scores using
the following formulas,
20 ∗ (AV ) ∗ (AC) ∗ (Au)

ES

=

IS

= −10.41 ∗ (1 − (1 − C)(1 − I)(1 − A))

A rigorous treatment of assigning these values can be found in [11].
The CVSS values are generated by security experts across the globe
and the database is updated every single day.
Our model takes a time range as input. It then parses all the CVEs
(a) from the NVD in that time range to finally filter out the ones
that can affect atleast one of the configurations in our system (a ⊂
υ(ci )). Note that old CVEs are irrelevant for generating attack sets
for a relatively new MTD system as they either have no effect on the
updated versions of the technologies they can affect or have popular
solutions to prevent them while developing the application. For our
application, we obtain this input range from our security experts.

4.4

Switching Cost

The switching costs can be represented by a K n×n matrix where
the n rows (and columns) denote the n system configurations. The

cell Kij denotes the cost of switching when the defender moves
from configuration i to configuration j. As mentioned earlier, the
values in K are all non-negative. Our security experts, who have
written the code to automatically move from one configuration to
another, hand code these values in each cell of the martix. We provide some guidance in choosing these values here and give a concrete example on how we selected these for our application later.
If there is no common technology between configurations c and
c0 involved in a switch operation, the cost will be large. Also, switching technologies in a specific layer may incur more cost than switching technologies in other layers. In the developed MTD system, we
find that switching between databases incur large costs because the
structure of the data needs to be changed for shifting, and the time
required to copy huge amounts of data from one database to another
must also be accounted for.
The matrix K for our system turns out to be symmetric, i.e.
Kij = Kji ∀ i, j ∈ {1, . . . n}. Also, Kii = 0, which implies
that there is no cost if no configuration switch occurs. Note that
although our security experts think this is the structure of rewards
for the developed system, the modelling is generic enough to allow
for asymmetric costs. Lastly, we choose the values of Kij in the
range [0, 10]. The reason for this upper bound becomes clear in the
upcoming section.

5.

SWITCHING STRATEGY GENERATION

In this section, we first introduce the notion of Stackelberg Equilibrium for our security game, that gives us a defender strategy that
maximizes his reward (and thus the security of the system). We
briefly talk of optimization methods, relevant to our domain, that
can produce this. Finally, we incorporate the costs of switching into
the objective function and propose our solver.

5.1

Stackelberg Equilibrium

The strategy generated for the designed game needs to capture
the reconnaissance aspect. Note that the game starts only after the
defender has deployed the web application, acting as a leader. This
now becomes a repeated game in which an attacker can observe a finite number of switch moves and probabilistically learn the switching strategy (since |C|  ∞) of the defender. Thus, the defender
has to select a strategy that maximizes his reward in this game,
given that the attacker knows his strategy. This is exactly the problem of finding the Stackelberg Equilibrium in a Bayesian Game
[20]. The resulting mixed strategy is the switching strategy for the
defender in our MTD system. Unfortunately, this problem becomes
NP-hard in our case because of multiple attacker types [5].
Before we find a strong Stackelberg Equilibrium for our proposed game, we state a couple of well founded assumptions we
make. Firstly, an attacker chooses a pure strategy, i.e., a single attack action that maximizes his reward value. This assumption is
popular in prior work on security games because for every mixed
strategy for the attacker, there is always a pure strategy in support
for it [14]. Secondly, we assume that the pure strategy of an attacker
type is not influenced by the strategy of other attacker types. This
is not limiting for our web application domain since an attacker
type’s attack selection is independent of the attack action chosen
by another type.
To solve for the optimal mixed strategy, one can use the Decomposed Optimal Bayesian Stackelberg Solver (DOBSS) [13]. This
optimizes the expected reward of the defender over all possible
mixed strategies for the defender (~
x), and pure strategies for each
~θ ). We
attacker type (~nθ2i ) given the attacker type uncertainty (P
2i
now define the objective function of the Mixed Integer Quadratic

Program (MIQP) in Equation 1.
X X X
D
max
Pθ2i Ra,θ
xc naθ2i
2i ,c
x,n,v

Moving Target Defense
Web application

(1)

?

Incorporating Switching Costs

As defined in the last section, the cost for switching from a configuration i to a configuration j can be represented as Kij . The
probability the system is in configuration i and then switches to
configuration j is xi · xj . Thus, the cost incurred by the defender
for a switch action fromP
i toP
j is Kij · xi · xj . The expected cost
Kij · xi · xj .
for any switch action is
i∈C j∈C

To account for cost, we can subtract this from the objective function of Equation 1 with a cost-accountability factor α (≥ 0) to
obtain
X X X
XX
D
Pθ2i Ra,θ
x nθ2i −α·
max
Kij ·xi ·xj
2i ,c c a
c∈C θ2i ∈θ2 a∈Aθ
2i

i∈C j∈C

Unfortunately, this results in a Bilinear Mixed Integer Programming problem, which is not convex. To ameliorate this problem,
we now introduce new variables wij that essentially represent an
approximate value of xi · xj . We first use the piecewise linear McCormick envelopes to design a convex function using these wij s that estimates a good solution to this problem [21]. Along with
these constrains, we introduce further constrains which we describe
after introducing the final MIQP convex optimization problem as
follows,
max

x,n,v

X X

X

D
θ2i
Pθ2i Ra,θ
xc na
− α·
2i ,c

c∈C θ2i ∈θ2 a∈Aθ
2i

XX

Kij wij

i∈C j∈C

(2)

X

s.t.

xc

=

1

(3)

naθ2i

=

1

(4)

c∈C

X
a∈Aθ

0 ≤ v θ2i −

X

2i

A
Ra,θ
x
2i ,c c

Figure 1: A moving target defense web application system
xc ∈ [0 . . . 1], nθa2i ∈ {0, 1}, v θ2i ∈ R
∀ c ∈ C, θ2i ∈ θ2 , a ∈ Aθ2i

i∈C

j∈C

j∈C

constrains (10) and (11).
If we now allow the maximum cost of switching to be 10, we can
see that the values for the cost is comparable in magnitude to the
value of the defenders rewards. This helps us to provide a semantic
meaning for the cost-accountability factor, α.
The first term in the objective function seeks to maximize the
defender’s reward, which in turn maximizes the security of the web
application. The second term on the other hand, seeks to reduce
the expected cost of the switching actions. Thus, α represents how
much importance is given to the cost of switching Vs. the level of
security desired. Consider the extreme cases, when α = 0, we are
producing the most secure strategy (which is the Stackelberg Equilibrium) and considering switching between configurations incur
zero costs. This is the sub-set of solutions that are developed mostly
for physical security systems. In contrast to that, when α = 1, we
are saying that we consider switching costs as important as the security of our MTD application.
Choosing the correct value of α is not trivial and often dependent
on the specific web-application. For example, if a banking system
someday seeks to operate on a MTD system, we hope it puts more
weight on security than switching costs, selecting low α values. To
provide a sense to the reader, we later show in the experimental section, how strategies and reward values are effected with changing
alpha values.

wij
wij

≥ 0 ∀ i, j
≤ xi ∀ i, j

(6)
(7)

wij

≤ xj ∀ i, j

(8)

6.

(9)

The goal of this section is to answer three key questions. Firstly,
does our proposed Bayesian Stackelberg Game (BSG) model generate better strategies that the state-of-the-art? Secondly, can we
effectively compute the set of critical vulnerabilities? Lastly, who
are the sensitive attacker types and how robust is our model?

1 ∀ i, j

wij

=

wij

= xi ∀ i

(10)

wij

= xj ∀ j

(11)

j∈C

X

where M is a large positive number. ~nθ2i and v θ2i give the pure
strategy and its corresponding reward for the attacker type θ2i respectively, and ~
x gives the mixed switching strategy for the defender. (5) solves the dual problem of maximizing rewards for each
attacker type (v θ2i ) given the defender’s strategy. This ensures that
attackers always select the best attack action. The constrains (6), (7)
and (8) represent the McCormick envelope that provides lower and
upper
bounds on each wij . Since we consider all possible switches,
P P
xi · xj = 1. This is enforced by constrain (9). Lastly, for
j∈C i∈C
P
P
xj ) = xi . This is represented by the
xi · xj = xi · (
each i,

(5)

j∈C i∈C

X

Automated Code
converter

𝑐1 = {Python, MySQL}

≤ (1 − naθ2i )M

c∈C

XX

𝑐4 = {php,
postgreSQL}

𝑐3 = {php, MySQL}

2i

x,n,v

𝑐2 = {Python,
postgreSQL}

c∈C θ2i ∈θ2 a∈Aθ
2i

Notice that this does not consider that switching costs between defender strategies. Essentially, this means the formulation assumes
that switching costs are uniform. Before we address this limitation
in the upcoming subsection, we take a little digression.
For our scenario, we have many attack actions. Thus, we observe that solving the MIQP version is more efficient (in computation time and memory usage) than solving the Mixed Integer Linear
Program (MILP) version of the DOBSS. This can be attributed to
the fact that the MILP formulation results in an increase in the dimensions
solution space. Theoretically, the MIQP solves for
P of theP
|C| + θ2i ∈θ2 aj ∈Aθ |aj | variables where as the MILP solves
P
P 2i
for |C| ∗ θ2i ∈θ2 aj ∈Aθ |aj | variables.

5.2

𝑐1 = {Python,
MySQL}

6.1

EMPIRICAL EVALUATION

Test Bed Description

PHP,
MySQL
Python,
MySQL
PHP, postgreSQL
Python,
postgreSQL

PHP,
MySQL

Python,
MySQL

PHP, postgreSQL

Python,
postgreSQL

Name

0

2

6

10

2

0

9

5

6

9

0

2

10

5

2

0

Script Kiddie
(SK)
Database
Hacker (DH)
Mainstream
Hacker (MH)

Table 1: Swithing costs for our system

• Switching between different languages while keeping the same
database dialect incurs minimal cost - workload is primarily
done is primarily on rerouting to the correct server with the
source language
• Switching between different database dialects while keeping
the same language incurs a slightly higher cost due to the
conversion required for the database structure and its contents. One also has to account for copying large amounts of
data to the database used in the current system configuration.
• Switching between different database dialects AND different
languages incur the most cost due to the combination of the
costs of conversion for the database as well as the penalty for
rerouting to the correct server with the source language.
The attacker types along with the attack action set size are defined in Table 2. We mined the NVD for obtaining CVE data from
January, 2013 to August, 2016 to generate these attack sets. If the
stakes of getting caught are too high for an attacker type given an
MTD system, he/she may choose not to attack. Hence, we have a
NO-OP action for each attacker type.
The optimization problems for the experiments were solved using Gurobi on an Intel Xeon E5 2643v3@3.40GHz machine with
6 cores and 64GB of RAM.

6.2

Strategy Evaluation

We evaluate our method using Bayesian Stackelberg Games on
our real life web application against the Uniform Random Strategy (URS), which is the state-of-the-art in such systems [18]. We
plot the values of the objective function in Equation 2 for both the

Prob.

|Aθ2i |

0.15

34

0.35

269

0.5

48

Table 2: Attacker types and attack action counts
BSG
U RS

−4
Obj

To answer the questions mentioned above, we develop a real
world MTD web application (shown in Figure 1) with 2 layers.
The key idea of applying MTD to web applications requires you to
have several versions of the same system, each written in either a
different language, using a different database, etc.
This diversity is not ubiquitous in legacy web applications, due
to cost, time, and resources required to build several versions of the
same web application. To aid this, we developed a framework to
automatically generate the diversity necessary for this web application. The current prototype is able to convert a web application
coded in Python to an equivalent one coded in PHP, and vice versa,
as well as a web application using a MySQL database to an identical version that uses PostgreSQL, and vice versa. In the future,
as more and more variations are developed, the set of defender’s
actions will increase.
The present set of valid configurations for our system is C =
{(PHP, MySQL), (Python, MySQL), (PHP, postgreSQL), (Python,
postgreSQL )}. The costs for switching between configurations is
shown in Table 1. These cost values generated by our system administrators are based on the following considerations:

(Technologies,
Expertise)
(PHP,4),
(MySQL,4)
(MySQL,10),
(postgreSQL,8)
(Python,4),
(PHP,6),
(MySQL,5)

−6
−8

0

0.2

0.4

0.6

0.8

1

α

Figure 2: Objective function values for Uniform Random Strategy Vs. Bayesian Stackelberg Game with switching costs as α
varies from 0 to 1.
strategies as α varies from 0 to 1. For URS, we use the exact values of wij = 0.25 ∗ 0.25 = 0.0625 ∀ i, j. The plot is shown in
Figure 2. Both are straight lines because although the value of α
changes, the strategy for URS is same (by definition) and the one
generated by BSG also remains the same. The latter case came as
a surprise to us initially. On further investigation, we noticed that
in the formulated game for our web-application, the Stackelberg
Equilibrium for our application (luckily) coincides with the least
switching cost strategy.
These attacker and defender strategies is shown in Table 3 alongwith the value of the defender’s reward (i.e. the first term in the objective function in Equation 2). Notice that, not only is the mixed
strategy generated by BSG more secure than URS, it leverages
fewer configurations than all valid configurations |C| = 4 the system has to offer. This result is in unison with previous work in
cyber-security which show that having many configurations does
not necessary imply that all of them have to be used for providing
the best security [3].

6.2.1

Studying the effect of α-values
To empirically show that our solver is actually considering costs
of switching, we change the value for switching from (PHP, postgreSQL ) to (Python, postgreSQL) and vice-versa from 2 (yellow boxes in Table 1) to 10. We plot this scenario in Figure 3. As
soon as α ≥ 0.4, the BSG generates (0.25, 0.25, 0.25, 0.25, 0.25)
(which is URS) as the most optimal strategy. After analysis, we
note that this happens because the most powerful attack actions in
the arsenal of the attacker types are for the systems (PHP, MySQL)
and (Python, MySQL). When, one does not prioritize switching
costs (α ∈ {0, 0.1, 0.2, 0.3}), the system keeps switching between
the more secure configurations nullifying the good attacks of the
attackers. As switching costs start to get more significant (α ∈
{0.4, 0.5, . . . 1.2}), the objective function value reduces if it sticks
to the stronger configurations since switching costs are now high
for these. It switches to the URS in this case. Beyond that, it switches

Method

Mixed Strategy

Defender’s
Reward

Attack sets (SK,
DH, MH)

k

CV sets

URS

(0.25, 0.25, 0.25,
0.25)

-5

1
2

BSG

(0, 0, 0.5, 0.5)

-3.25

CVE-2016-3477,
CVE-2015-3144,
CVE-2016-3477
CVE-2014-0185,
CVE-2014-0067,
CVE-2014-0185

{(CVE-2014-0185)}
{(CVE-2014-0185,
CVE-2015-5652)}

P (configuration= c)

Table 3: Comparison between the strategies generated by Uniform Random Strategy (URS) Vs. Bayesian Stackelberg Game
(BSG)
php_mysql
py_mysql
php_psql
py_psql

0.4

0.2

0
0

0.5

1

1.5

2

2.5

α→
BSG
U RS

Obj

−5
−10
−15

0

0.5

1

1.5

2

2.5

to the strategy (0.25, 0.5, 0, 0.25) as α keeps on increasing. When
α becomes close to 2, it completely ignores the security of the
system and tries to minimize the switching cost by proposing the
strategy (0.5, 0.5, 0, 0) as the cost for switching between (PHP,
MySQL) and (Python, MySQL) is the least (= 2).
In the bottom of Figure 3, we showcase the change in the values of objective function. At the start, the BSG generates a better
strategy when compared to URS. When the BSG strategy becomes
the same as the URS (for 0.4 ≤ α ≤ 1.2), we observe that the
objective function value for BSG is lower than URS. This is not
surprising since BSG is merely trying to estimate the value xi · xj
with the variables wij , whereas URS is using the exact value. As
we increase α further, we are essentially discouraging an MTD system, since now the cost of switching has become so high, whereas
naive URS pays no heed to this.

6.3

Identifying Critical Vulnerabilities

In real-world development teams, it is impossible to solve all the
vulnerabilities, especially in a system with so many technologies.
In current software systems, given a set of vulnerabilities, a challenging question often asked is which vulnerabilities should one fix

CPU Time
3m15s
421m27s

Table 4: Most critical vulnerability in the MTD system and the
time required to generate it.

to improve the security?
For an MTD system, this becomes a tough problem since the defender needs to reason about multiple attacker types– their probabilities and attack actions. For a given k, the set of k vulnerabilities,
which on being fixed, result in the highest gain in defender strategy,
is termed as the k critical vulnerability set (k−CV).
To address this problem, we remove each k-sized attack set from
the set of all attacks (A02 = A2 \ D ∀ D ⊂ A2 & |D| = k)
and evaluate the objective function (Equation 2). The sets A02 that
yield the highest objective values, provide the vulnerabilities D that
should be fixed to improve the defender’s system.
We tried to study this complicated behaviour for some toy examples before applying it to our application. An interesting phenomenon we noticed was that a k-set critical vulnerabilities (k−CV)
is not always a subset of the (k + 1)−CV. Suppose we want to find
3 vulnerabilities that we want to fix. Since it is not just a super-set
of the 2-CV, we need to solve this problem from the scratch with
k = 3. Hence, there is going to be combinatorial explosion here.
As

the value of k increases, we end up solving |A02 | = |Ak2 | MIQP
problems to identify the k−CVs.

6.3.1

α→

Figure 3: Top: Showcases the change in probabilities associated
with a particular configuration. Bottom: Objective function
values for Uniform Random Strategy Vs. Bayesian Stackelberg
Game with switching costs as α varies from 0 to 2.5 when the
cost of switching are as showcased in Table 1 with the values in
the yellow boxes being 10.

Objective
Value
-2.435
-1.973

Finding Critical Vulnerabilities in the Developed System

For our system, we start with k = 1, we increase number of critical vulnerabilities to be found by 1 at each step. The result remains
the same for α ∈ [0, 1] for our system. We do not play around
with α beyond this, mostly because this would be unrealistic for
any practical application. Unfortunately, the brute force approach
and the scalability of algorithms for solving normal extensive form
BSGs proves to be a key limitation.
This is not a surprise since the total number of unique CVEs
spread out among
the attackers is 287. When k = 3, we end up

solving 287
optimization
problems, which fails to scale in both
3
time and memory. Thus, we only show critical vulnerabilities identified up to k = 2 (in Table 4) using α = 0.2.
At present, we are trying to develop a single MIQP formulation
that tries to approximately generate the k-CV set. To reduce the
combinatorial explosion, we plan to use switch variables that can
turn attack actions on and off. This comes at the cost of increasing
the number of variables in the formulated optimization problem.

6.4

Model Robustness & Attacker
Type Sensitivity

It is often the case that a web application administrator (defender)
cannot accurately specify the probability for a particular attacker
type. In this section, we see how this uncertainty affects the optimal rewards generated by the system. We provide a notion for
determining sensitive attacker types and measuring the robustness
of a switching strategy.
For each attacker type i, we vary the probability Pθ2i by ±x%
x
)) where x is the sensitivity factor, which
(Pθnew
= Pθ2i (1 ± 100
2i
can be varied from a low value to a high value as needed. Note that
x
now p = Pθ2i × 100
needs to be adjusted or distributed amongst the

probabilities of the remaining attacker types. To make sure that this
distribution is done such that the sensitivity of attacker i actually
stands out, we propose to distribute p amongst the other attacker
types using a weighted model as per their existing probabilities as
shown below. For attacker j (6= i), its new probability would be:
= Pθ2j (1 ∓

P p
Pθ
k(6=i)

)
2k

NLR(BSG)

Pθnew
2j

M ainstreamHacker(M H)

2

Rn −Ro
Rn

−100

−50

CONCLUSIONS AND FUTURE WORK

In this paper, we propose a method to generate a switching strategy for real-world web application based on the Moving Target Defense (MTD) architecture. To find an effective switching strategy,
we model the system as a repeated Bayesian game. We develop
methods to assign attack actions to attacker types and generate realistic utilities based on expertise of security professionals. For obtaining real-world attack data, we mine vulnerabilities in the National Vulnerability Database (NVD) and obtain utilities based on
the Common Vulnerability Scoring System (CVSS). We formulate
an optimization problem which outputs a switching strategy that
maximizes system security while accounting for switching costs.
The generated strategy is shown to be more effective than the stateof-the-art for a real-world application. We also provide metrics that
can be used to validate the robustness of switching strategies, absent in literature for multi-agent cyber-security systems. Lastly, we
propose the problem of identifying critical vulnerabilities and provide a solution.
The techniques in this paper are not limited to only web applications. The attack actions mined from the security databases relate to

50

100

50

100

NLR(URS)

DatabaseHacker(DH)
ScriptKiddie(SK)

5

0
−100

Evaluation Based on the Developed System

0

M ainstreamHacker(M H)

(13)

We compute the attacker sensitivity for our system varying the
probability of each attacker type from −100% to +100% (of its
modeled probability) with 10% step sizes. We plot the results in
Figure 4 using Equation 13. The Mainstream and Database hacker
(MH & DH) are the least sensitive attacker types. The NLR values
for both these attackers are 0. This is the case since the real world
attack action used by these types remain the same even when their
probabilities change. On the other hand, if the probability associated with the Script Kiddie (SK) is underestimated in our model,
we see that the strategies deviate substantially from the optimal.
For our experiments in this section, we use α = 0.2. The max
NLR for our BSG strategy is 2.35 Vs. 9 for URS. The average of
the 60 NLR values is 0.061 for BSG and 0.88 for URS. These values indicate our model is more robust to variance in attacker type
uncertainty than the present state-of-the-art.

7.

1

0

Note that NLR values are ≥ 0. Higher values of NLR represent
more sensitive attacker types. Inaccurate probability estimates for
the sensitive attackers can be detrimental to the security of our
application. Note that lower NLR values indicate that a generated
strategy is more robust.

6.4.1

ScriptKiddie(SK)

(12)

~θ , then the sign in
When x% is subtracted from the probability P
2i
the above equation becomes positive, and vice-versa.
We now formally define the loss in reward to the defender as the
probability distribution over the attacker types change. Let Ro be
the overall reward for the defender when he uses the mixed strategy
for the assumed (and possibly incorrect) model of attacker type un~θnew ). Let Rn be the defender’s
~θ2 ) on the true model (P
certainty (P
2
optimal reward value for the true model. We compute the Normalized Loss in Rewards (NLR) for the defender’s strategy as follows:
NLR =

DatabaseHacker(DH)

−50

0

Varying sensitivity of attacker types (%)

Figure 4: NLR values for BSG and URS genereated strategies
when attacker types probabilities vary in [−100%, 100%].
all kinds of technologies, like operating systems, coding languages
etc. Hence, the modelling should be relevant to any software applications using the MTD architecture. It would be interesting to see
how effective they are in such scenarios.
Investigating the reward structure for a particular problem has
helped design provably fast solvers in the physical security domains. We believe this direction of research might help in developing faster solvers, alleviating the scalability problem of identifying
critical vulnerabilities, for the cyber-security domain as well.

8.

ACKNOWLEDGMENTS

This work was partially supported by the grants from National
Science Foundation (NSF-SFS-1129561) and the Center for Cybersecurity and Digital Forensics at Arizona State University.

REFERENCES
[1] K. Amin, S. Singh, and M. Wellman. Gradient methods for
stackelberg security games. AAMAS, 2016.
[2] D. Balzarotti, M. Cova, V. Felmetsger, N. Jovanovic,
E. Kirda, C. Kruegel, and G. Vigna. Saner: Composing static
and dynamic analysis to validate sanitization in web
applications. In Security & Privacy 2008. IEEE Symposium,
pages 387–401, 2008.
[3] K. M. Carter, J. F. Riordan, and H. Okhravi. A game
theoretic approach to strategy determination for dynamic
platform defenses. In ACM MTD Workshop, 2014, MTD ’14.
ACM, 2014.
[4] M. Carvalho and R. Ford. Moving-target defenses for
computer networks. Security Privacy, IEEE, 12(2):73–76,
Mar 2014.
[5] V. Conitzer and T. Sandholm. Computing the optimal
strategy to commit to. In Proceedings of the 7th ACM
Conference on Electronic Commerce, EC ’06, pages 82–90,
New York, NY, USA, 2006. ACM.
[6] A. Doupé, L. Cavedon, C. Kruegel, and G. Vigna. Enemy of
the state: A state-aware black-box web vulnerability scanner.
In USENIX Security Symposium, 2012.

[7] S. H. Houmb, V. N. Franqueira, and E. A. Engum.
Quantifying security risk level from cvss estimates of
frequency and impact. JSS, 83(9):1622–1634, 2010.
[8] S. Jones, A. Outkin, J. Gearhart, J. Hobbs, J. Siirola,
C. Phillips, S. Verzi, D. Tauritz, S. Mulder, and A. Naugle.
Evaluating moving target defense with pladd. Technical
report, Sandia National Labs-NM, Albuquerque, 2015.
[9] P. Manadhata. Game theoretic approaches to attack surface
shifting. In Moving Target Defense II, volume 100 of AIS,
pages 1–13. Springer New York, 2013.
[10] J. McCumber. Information systems security: A
comprehensive model. In Proceedings of the 14th National
Computer Security Conference, 1991.
[11] P. Mell, K. Scarfone, and S. Romanosky. Cvss v2 complete
documentation, 2007.
[12] H. Okhravi, T. Hobson, D. Bigelow, and W. Streilein.
Finding focus in the blur of moving-target techniques.
Security & Privacy, IEEE, 12(2):16–26, 2014.
[13] P. Paruchuri, J. P. Pearce, J. Marecki, M. Tambe, F. Ordonez,
and S. Kraus. Playing games for security: An efficient exact
algorithm for solving bayesian stackelberg games. In
AAMAS, 2008, pages 895–902, 2008.
[14] J. Pita, M. Jain, J. Marecki, F. Ordóñez, C. Portway,
M. Tambe, C. Western, P. Paruchuri, and S. Kraus. Deployed
ARMOR protection: the application of a game theoretic
model for security at the los angeles international airport. In
AAMAS 2008, Industry and Applications Track Proceedings,
pages 125–132, 2008.
[15] A. Prakash and M. P. Wellman. Empirical game-theoretic
analysis for moving target defense. In ACM MTD Workshop,
2015, 2015.
[16] J. Silver-Greenberg, M. Goldstein, and N. Perlroth.
JPMorgan Chase Hacking Affects 76 Million Households. In
The New York Times, 2014.
[17] A. Sinha, T. Nguyen, D. Kar, M. Brown, M. Tambe, and
A. X. Jiang. From physical security to cyber security.
Journal of Cybersecurity, 2016.
[18] M. Taguinod, A. Doupé, Z. Zhao, and G.-J. Ahn. Toward a
Moving Target Defense for Web Applications. In
Proceedings of 16th IEEE IC-IRI, 2015.
[19] M. Van Dijk, A. Juels, A. Oprea, and R. L. Rivest. Flipit: The
game of “stealthy takeover”. Journal of Cryptology,
26(4):655–713, 2013.
[20] H. Von Stackelberg. Market structure and equilibrium.
Springer SBM, 2010.
[21] D. S. Wicaksono and I. Karimi. Piecewise milp under-and
overestimators for global optimization of bilinear programs.
AIChE Journal, 54(4):991–1008, 2008.
[22] D. Wichers. Owasp top-10. OWASP, 2013.
[23] M. Winterrose, K. Carter, N. Wagner, and W. Streilein.
Adaptive attacker strategy development against moving
target cyber defenses. arXiv:1407.8540, 2014.
[24] R. Zhuang, S. A. DeLoach, and X. Ou. Towards a theory of
moving target defense. In ACM MTD Workshop, 2014, pages
31–40. ACM, 2014.

Do You Feel Lucky? A Large-Scale Analysis of
Risk-Rewards Trade-Offs in Cyber Security
Yan Shoshitaishvili, Luca Invernizzi, Adam Doupe, and Giovanni Vigna
UC Santa Barbara
Santa Barbara, CA, USA

{yans,invernizzi,adoupe,vigna}@cs.ucsb.edu
ABSTRACT
A crucial part of a cyber-criminal’s job is to balance the
risks and rewards of his every action. For example, an expert
spammer will tune a bot’s email-sending rate to achieve a
good throughput with an acceptable risk of being detected.
Then, such a cyber-criminal has to choose how to launder the
money he made with spamming, and he will have to consider
many options (money mules, Bitcoin, etc.) that will offer
different returns and risks. Although understanding these
trade-offs and coming as close as possible to their optimum
is what discriminates winners and losers in the cyber-crime
world, there has been little study on this matter, as setting
up a large-scale study to study how cyber-criminals deal with
these risk-reward trade-offs is challenging.
Computer security competitions provide a great opportunity both to educate students and to study realistic cybersecurity scenarios in a controlled environment. Looking to
study the risk-reward trade-offs seen in real cyber-security incidents, we designed and hosted a novel format for a Capture
the Flag cyber-security contest, involving 89 teams comprising over 1,000 students across the globe. In this paper, we
describe the intuition, intent, and design of the contest. Additionally, we present an analysis of the data set collected,
evaluate its effectiveness in modeling risk-reward behavior,
examine the strategies of the competing teams, and estimate
the effectiveness of such strategies.
1

Introduction

Computer security incidents commonly display a risk-reward trade-off. Examples of this in the wild are plentiful: a
spear-phishing campaign might target an increased amount
of users at the cost of an increased risk of being reported, or
a bot could send spam at a higher frequency and risk being
blacklisted. Studying this trade-off could provide valuable
insight into the decisions that cyber-criminals make when
designing exploitation software and methods. However, the
collection of real-world data on these topics is difficult, and
the reproduction of these scenarios for study in a controlled
environment has been an open problem. Seeing the poten-

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$10.00.

tial for leveraging cyber-security competitions to generate
realistic models and datasets [4], we designed and organized
an experiment, disguised as a “Capture the Flag” exercise,
to generate a public dataset to assist in studying risk-reward
behavior.
A “Capture the Flag” (CTF) competition is a cyber-security
exercise in which every team is both an attacker and a target.
Generally, the completion of an attack by a participating
team results in a reward of points, and the team with the
most points at the end of the competition wins. The twist
in our competition was that, in order to score the most
points, the participants had to make risk-reward trade-offs,
which in turn were designed to model trade-offs faced by
cyber-criminals in the course of their actions. Our experiment involved 89 academic teams from around the world,
comprising over 1,000 students. We analyzed this dataset to
uncover the behavior of teams when faced with a risk-reward
trade-off. Using this data, we have attempted to recreate
the strategy that some of the top teams used and their level
of accepted risk. Specifically, we have shown that the teams
did, in fact, give thoughts to the risk inherent in their actions.
Furthermore, the teams that understood this risk were able
to use their understanding to improve their standing in the
competition, validating the success of our design.
In this paper, we make the following contributions:
• We describe the novel design of a research experiment on
risk-reward trade-offs in the form of a live cyber-security
exercise.
• We analyze the data generated by the players’ participation
in our experiment, describing team strategies and insights
into risk-reward trade-offs made by the participants.
In addition to these contributions, we have made the
dataset itself available for use by other researchers so that
they can further study other aspects of the risk-reward tradeoffs than we were able to do here [7]. While this dataset is
not a perfect simulation of cyber-criminal behavior (after
all, the student participants were not, actually, professional
cyber-criminals), we have put the participants in the shoes of
cyber-criminals, staging a scenario similar to the one cybercriminals face every day. Moreover, we have had participants
with a wide range of expertise, ranging from researchers in
the field of Computer Security to amateurs (much like cybercriminals range from skilled professionals to “script kiddies”).
To our knowledge, this is the only existing large-scale dataset
that attempts to represent risk-rewards trade-off choices in
Computer Security. Aside from implications in the field of
Computer Security, we believe this dataset would be useful
in other areas, such as game theory or optimization. In fact,

this dataset has already been used by other research groups
to study visualizations of cyber-security incidents [10].
2

Background

We centered our CTF design around a risk-reward tradeoff because of the ubiquitous nature of trade-offs in cybersecurity. In this section, we describe why this trade-off is
important and detail its implications on some of the history
of computer security. We also present some details of other
cyber-security competitions to provide some context about
educational security exercises.
2.1

Risk-reward trade-off

A trade-off between risk and reward is a common element
in many issues pertaining to computer security. Aspects of
this trade-off can be seen in worms, drive-by downloads, and
spamming. In all of these examples, cyber-criminals face a
trade-off between being effective and being stealthy.
One obvious example of this is in the fall in popularity of
classical self-spreading worms [1]. In the early 2000s, worms,
which spread by taking advantage of remotely-exploitable
vulnerabilities in various commonly-used software products,
achieved huge success at compromising and utilizing a very
large number of machines. However, the spread and infection model of such worms is extremely easy to detect,
and techniques were developed to effectively achieve such
detection [6]. In recent years, self-spreading worms have
plummeted in efficacy, and are no longer a serious computer
security threat1 .
Cyber-criminals quickly moved away from using a wormlike infection vector, and began to instead utilize drive-by
downloads [8]. This approach carries a considerably smaller
risk of detection, as the malware delivery vector is not easily
distinguishable from ordinary web traffic. However, a tradeoff of the rate of new infections must be made: botnets built
with such methods grow slower than those built using worms.
As drive-by downloads increased in popularity, systems
were developed to detect such attacks [3, 8]. Here, cybercriminals were faced with another risk-reward trade-off. Most
drive-by download detection systems utilize a rich honeyclient
system that emulates a web browser. Because such detection
systems have finite resources, certain decisions have to be
made as to which set of browsers to emulate. Among other
methods, cyber-criminals can evade detection of their driveby downloads by identifying the browsers emulated by the
detection systems and avoid sending the malicious code to
such browsers [9]. This method, while it can help cybercriminals to avoid detection, again comes at a cost.
Yet another interesting case of risk-reward trade-off is
seen in spamming operations. Spammers generally use precompiled email lists to select their targets in spam campaigns.
Additionally, as such campaigns progress, emails that are
continuously found to be non-existent (through mail-server
feedback), are removed from the spam lists. This is done by
spammers with the goal of cleaning up their email lists, so
that they do not waste resources sending emails to accounts
that do not exist. However, this approach can raise the
risk of spam mitigation methods that target this mechanism
to the spammers’ disadvantage. This approach has been
1
Or at least, not at the level that they once were, infecting
massive amounts of machines in a short time-frame. While
some modern botnets spread via worm-like functionality, they
are clearly not on the level they once were.

implemented and tested by anti-malware researchers [11],
demonstrating that this is indeed a very relevant trade-off
between risk and reward for the spammers.
2.2

Capture The Flag Competitions

A Capture the Flag (CTF) competition is a live security
exercise comprising a number of cyber-security challenges.
Successful completion of such challenges yields a key (called
the flag) that the successful player can usually redeem for
points. Such competitions are frequently played by teams of
people as a type of cyber-security-based team sport.
The origin of the CTF concept can be traced back to
the DEFCON security conference, held yearly in Las Vegas,
Nevada. DEFCON started in 1992 and formalized the CTF
concept in its fourth year. Since then, the concept has spread
around the world, and several CTFs are held every month
worldwide.
While CTF competitions are mostly held for the enjoyment of the organizers and teams (similar to amateur sporting events), they can be leveraged for data collection. For
example, cyber-security companies frequently host CTFs
as a recruitment tool2 . Additionally, Pwn2Own3 , an event
in which participants must hack into computers with specific software configurations, is used by the vendors of such
software as a penetration-testing event.
CTF competitions, such as our experiment, can also serve
as tools to educate students. As computer security becomes
an increasingly important issue for corporations and governments, the question of training the next generation of
security professionals rises in significance.
Practical security education started with early efforts by
Computer Science professors to teach applicable security
practices, introducing students to topics such as buffer overflows, SQL injections, and related security concepts. These
methods, however, constitute a passive way of learning computer security and do not adequately prepare students for
challenges that they might face in real-world security scenarios. To improve and enhance the security education of
computer science students, educational cyber-security competitions were introduced to motivate students and to perform
live exercises in a controlled environment. This controlled
environment is important, as it allows students to fully exercise offensive security skills without the risk or worry of
illegality.
3

The Experiment

To study risk-reward choices in a realistic scenario, we staged
an experiment, disguised as a CTF competition, that required
teams to create policies and make decisions in a risk-aware
fashion. We choose a theme centered around illegal money
laundering for our competition. This activity is modeled after
cyber-criminal money laundering operations and implements
some of the ideas presented in Section 2.1.
The theme of the competition was that each team was a
“gray-hat” hacker who had the challenge of laundering dirty
money (termed money) into clean money (termed points).
This was done by the completion of specific security-related
tasks. In this way, the theme was created after the goal of
the contest: analyze the way in which people approached a
risk-reward trade off in a computer security situation.
2
3

https://stripe-ctf.com/
http://pwn2own.zerodayinitiative.com

3.1

Design Goals

Several ideas influenced the design of our contest, called
the Educational Live Security Exercise (ELSE). The most
important consideration, of course, was the development of
a competition to properly model the risk-reward trade-offs
described in Section 2.1. However, an additional consideration was the assurance of a fair and exciting competition.
In many CTF competitions, one team dominates the entire competition by utilizing experience in a specific field,
completely ignoring whole portions of a competition. This
imbalance is especially common in competitions that include
both a team-vs-team and a challenge board component, as it
is often possible to strategize in a way that allows a team to
ignore one of the components. We wanted all aspects of our
competition to be important, so we devised a competition
format that would put heavy emphasis on both team-vs-team
competition, and on the challenge board.
In a similar vein, we wanted to prevent a team from skyrocketing in points and creating an unassailable lead, because
this can have the effect of discouraging other players. In
many CTFs, one team will frequently pull ahead early on
and maintain a dominant lead until the end, making the
CTF much less interesting for everyone else involved. We
wanted to promote a format where teams that pulled ahead
had to fight to maintain their position.
Finally, we wanted our CTF to educate participating students, not just on the core aspects of practical computer
security, but on the perspective of hackers when evaluating
risks against rewards. By making this concept of risks and
rewards explicit in our competition, we believe that the students developed an understanding and intuition of risks and
trade-offs far better than they would have in a class. We
hope that the next time these students analyze a piece of
malware or research the underground economy they will be
able to better assess the hackers’ motivations and, therefore,
predict the hackers’ possible responses.
3.2

Design Overview

The key game mechanic of our competition was the conversion or laundering of dirty money into clean points. At the
end of the game, the team with the most clean points was
the winner, and dirty money was not considered.
The teams earned dirty money by solving security challenges, described in Section 3.3.3, from a challenge-board.
This dirty money could only be laundered into clean points by
successfully exploiting vulnerable network programs, known
as services and described in Section 3.3.1, which ran on vulnerable virtual machines that were provided to each team.
Exploiting a service would produce a flag, described in Section 3.3.2, which could then be used to launder the money
into points. The launder process is described in Section 3.3.5.
If the laundering was unsuccessful, the team lost their money
and did not earn any points. Even when laundering was
successful, however, the amount of points gained was not
equal to the amount of money laundered: two factors, the
payoff and cut (described in Section 3.3.4) were used in
determining the amount of point gained. When laundering
money into points, the chance of success depended on the
risk factors, described in Section 3.3.4, associated with the
service from which the flag being used for the laundering was
acquired.
The competition was divided into ticks, each of which lasted
two minutes. Every tick, the state of the game changed. At

Service Name

Description

convicts

A restricted bash shell, exposed remotely over telnet. The restrictions did not include the exec command, with which it was possible to read the flag.
A Ruby-on-Rails e-commerce web application. In
a two-step attack, it was possible to first control
the id of a payment transaction, and then to retrieve credit-card information of other users.
A SMTP-like service, with a heap-overflow vulnerability.
A bulletin-board service, with a path-traversal vulnerability.
A website to manage money-laundering mules.
Due to a bug in authorization enforcement, it was
possible to read other users data.
A job-hunting website in PHP that the mules use
to advertise their services. Some testing functionalities were not disabled: through those, it was
possible to retrieve the flag.
A website with obfuscated JavaScript, in which the
flag was hidden.
A website offering a messaging system for mules.
Flags could be retrieved by using the XML-HTTP
API, left enabled by the developers.
A web application used by mule managers to send
important alerts to the mules. Flags could be obtained with an SQL injection attack
A binary service to send short messages to the
mules. It could be exploited via a return-into-libc
attack

egoats

mailgateway
msgdispatcher
muleadmin

mulemanager

mulemassageappointment
muleuser

sendalert

smsgateway

Table 1:

Vulnerable services that the teams had to run on their machine while simultaneously exploiting the service’s of the other teams.

each tick, the risk factors, payoff, and cut changed randomly
for each service. With this design, we were able to study
changes in the teams’ risk-reward trade-offs throughout the
competition.
3.3

Design Details

The 2011 iCTF experiment required a carefully-planned infrastructure, which was designed and deployed for maximum
network performance and uptime.
3.3.1

Services

Each service was a network application, run by the participants on their networks, which included intended security
vulnerabilities. A team had to exploit such vulnerabilities
in the services of other teams, while protecting their own
services from exploitation. The teams ran these services in a
virtual machine that we provided to them so that they could
connect it to the competition network.
We ensured that teams kept their services available by
exercising their functionality with a scorebot that would
connect to each teams’ services and verify that they were
functional.
For the 2011 iCTF, we created 10 services with intended
vulnerabilities ranging from logic bugs to memory corruption
flaws. Table 1 contains a description of each service, along
with the vulnerability present in the service.
3.3.2

Flags

Each service has access to a flag, which the attacker must
capture by exploiting the services. This flag is proof that the
attacker exploited the specific team’s service. For example,
the flag of a message board service might be the password of
a specific user. To recover the flag, an attacker would have to
compromise the service to the point of being able to recover
user passwords. To require teams to continue carrying out

1

Rn (N )

0.8

0.6

0.4

0.2

0
0

200

400

600

800 1000 1200 1400

N

Figure 1: Graph of risk as N increases. The x-axis is the total amount
of money previously laundered through the team, N , and the y-axis is
the factor of risk from N .
1

Rq (Q)

0.8

0.6

0.4

0.2

0
0

500

1000 1500 2000 2500 3000
Q

Figure 2: Graph of risk as Q increases. The x-axis is the total amount
of money previously laundered through the service, Q, and the y-axis
is the factor of risk from Q.

successful attacks, these flags were changed every tick by our
scorebot in the course of exercising the services.
3.3.3

Challenges and Money

In order to utilize the flags recovered from services, a team
first had to solve several self-contained security challenges to
generate money. We hosted these challenges, and the teams
could access the challenges through a challenge board. The
typical challenge consisted of a single file and a question.
Successful analysis of the file (or analysis of the web page
referred to in the link) would yield the key in the form of
a string to be entered into the challenge submission form.
For example, several challenges involved obfuscated binaries,
with the answer being the value contained in some encrypted
variable. Submission of this value into the web form would
net the submitting team an amount of money that depends
on the difficulty of the challenge.
3.3.4

Risk

Risk, in the context of our competition, is the probability
that a team successfully converts money into points. There
are four factors that influence a team’s laundering success,
each designed to capture a different aspect of risk. The
resulting risk function is a percentage from 0% to 100%, and
the risk was the chance that the laundering was successful.

The risk factors must be considered in the context of two
participating teams, with the attacking team attempting to
launder money through a service of the defending team.
The first factor, R, is the service-specific risk, which varies
randomly from zero to one at every tick. This factor is meant
to capture the inherent riskiness of laundering.
The second factor, M , is the amount of money the team is
trying to launder. The team chooses the amount of money
to launder, and the idea here is that the more money the
team tries to launder, the larger the risk of getting caught is.
Thus, the teams had to balance laundering more money, at
possibly favorable conditions but higher risk, or laundering
less money.
The third factor, N , is the total amount of money that the
attacking team has laundered through the defending team.
For each laundering attempt, the attacking team must exploit
the defending team’s service. The idea here is that using
one defending team, or money mule, over and over increases
the risk of getting caught. This factor also had the added
benefit of forcing the teams to spread out the laundering
among multiple teams.
The fourth factor, Q, is the total amount of money the
attacking team has laundered through that service. For each
laundering attempt, the team must exploit another team’s
service. The idea here is that using one service, or laundering
method, over and over increases the risk of getting caught.
This factor also had the added benefit of rewarding teams for
crafting exploits for more services, because the new services
could be used for laundering at a lower risk.
Each of these parameters are directly related to risk, in
much the same way that real-world risk shifts. For instance,
trying to launder more money (M in our case) increases the
chances of getting caught.
The overall risk function, O, averages three different risk
attributes to encode the intuitions described previously.
The service-specific risk, R, is combined with the amount
of money laundered, M , in the following way:
R·M
10
In this way, as the service-specific risk, R, decreased, the
team could launder more money, M , for the same overall
risk. The factor of 10 was decided to encourage the teams
to keep the amounts to launder small.
The risk associated with the total amount of money the
team has laundered through that team, N , is given in the
following sigmoid function:

Rn (N ) =

1
2



N − 700
+1
300 + |N − 700|



In this way, as the team laundered more and more money
through the given team, the risk factor associated with N
will increase slowly from 0.1 when N is 0, to 0.5 when N
is around 700, and then greatly increase from there to 0.9.
Figure 1 shows this function as N increases.
The total amount of money that the team has laundered
through that service, Q, is given by a function that is similar
to N , except that the inflection point is around 1, 500 instead
of 700. This allowed the teams to launder more money
through a service before it became significantly riskier. The

function for Q is as follows, and Figure 2 shows this function
as Q increases:

Rq (Q) =

1
2



Q − 1500
+1
300 + |Q − 1500|

1
3



R·M
10



We 0wn Y0u
MoreSmokedLeetChicken
FluxFingers
PPP
CISSP Groupies
ENOFLAG
Pwnies
PeterPEN
FAUST
HackerDom



Finally, the overall risk, O, is calculated based on the
amount of money the team decided to launder, M , the servicespecific risk, R, the total amount of money laundered through
the team, N , and the total amount of money laundered
through the service, Q as follows:

O (R, M, N, Q) =

Team Name


+ Rn (N ) + Rq (Q)

Points = (M − (M ∗ C)) ∗ D ∗ P
The cut and payoff create another dimension of “risk”:
laundering money through a service with a high cut or payoff
but low risk in a given tick could be less beneficial than
waiting until the service has smaller cut or payoff values,
even at the cost of a higher risk.
If the conversion failed, the team would lose the money
that it attempted to launder.
New, random, CP R values (Cut, Payoff, and Risk) for
every service were given to the teams every tick.
3.3.5

Laundering

The goal of the competition was to earn dirty money by solving challenges and convert it into clean points by laundering
them through exploited services. When a team exploited a
service, and had some money to convert, they would submit
the captured flag to the submission server and choose an
amount of money to launder. This submission server would
calculate the risk as described in Section 3.3.4, roll a virtual
die to determine if the attempt was successful. If the roll
was successful, the submission server would calculate the
appropriate gain in points by the teams involved, and adjust

Mo

Ser

La

P

Co

28
32
28
27
19
20
21
23
26
19

3,635
5,400
4,285
3,825
2,890
2,750
3,125
3,600
2,885
2,690

8
4
6
3
3
6
3
5
5
3

420
1,836
185
451
1,485
1,246
485
96
646
628

1,790
1,704
1,105
965
926
835
827
671
644
602

49
32
26
25
32
30
26
19
22
22

Table 2:

Team performance. The number of challenges solved (Ch),
the amount of money earned (Mo), the number of services exploited
(Ser), the number of times the team laundered (La), the amount of
points laundered (P), and the conversion ration (Co, in percent) for
the top 10 teams in the competition.
Service Name

Thus, when a team attempted to launder money, the
function O was calculated to give a value between 0 to 1 on
the chance of getting caught. Thus 0.3 meant that there
was a 70% chance of successfully laundering from money to
points, while there was a 30% chance of losing the money.
If the laundering is successful, then the amount of dirty
money the team laundered is converted into clean points.
Specifically, from the amount of money the team attempted
to launder, M , a percentage of it is removed and given to
the team that owned the service that was exploited. This
percentage is called the cut, C, a service- and tick-specific
value. The cut represents payments to, for example, the
money mules themselves, and serves to funnel money into
teams that might otherwise solve fewer challenges, allowing
such teams to launder this money. Next, the resulting dirty
money is further reduced by another service- and tick-specific
value called payoff, P, which represents the effectiveness of
the laundering process (specifically, the idea that any money
laundering process has procedural overhead other than money
mule salaries). Finally, in order to encourage teams to keep
their services up and running, the final amount is further
reduced by the percentage of time that the team’s services
have been up, D. The specific function is given next:

Ch

convicts
egoats
mailgateway
msgdispatcher
muleadmin
mulemanager
mulemassageappointment
muleuser
sendalert
smsgateway

Fl

Sub

Att

Def

Pts

5,927
6,375
5,237
489
6,801
7,519
1,505
2,417
2,833
1,947

18,212
13,220
6,385
495
47,104
34,684
1,961
2,951
5,526
1,947

31
17
4
6
64
53
5
8
22
3

48
54
43
40
68
72
56
44
55
7

1,035
1,404
197
67
6,636
7,127
232
168
4,243
7

Table 3:

Service statistics. The number of captured flags (Fl), submitted flags (Sub, not unique since multiple teams could submit the same
flag), teams with functional exploits (Att), teams that were exploited
(Def), and total points earned (Pts) for every service.

points accordingly. However, if the laundering failed, the
team would simply lose its money.
4

Data Analysis

The 2011 iCTF competition was attended by over 1,000
students in 89 teams from around the world. All teams were
from academic institutions, spread across 18 countries. The
teams generated an enormous amount of traffic, totaling 241
gigabytes over 8 hours.
Of the 51 challenges presented via the challenge board, 45
were solved by at least one team. 72 teams were able to solve
at least one challenge, with an average of 13.5 challenges
solved by each team. Team More Smoked Leet Chicken solved
32 challenges, while the first place team, We 0wn Y0u, solved
24. A summary of the top 10 teams who solved challenges,
earned money, and laundered points is presented in Table 2.
Through analysis of the network traffic and scoring data,
we were able to conclude that all services were successfully
exploited during the competition. A summary of the services
and their exploitation statistics is presented in Table 3.
It is important to emphasize that strategy and risk awareness was critical for success in the competition. This is evident by the fact that neither the amount of solved challenges,
nor the amount of exploited services, and not even the total
number of flags submitted determined the outcome of the
competition. Instead, a deep understanding and utilization
of the rules was required for success.
In the rest of this section, we delve into the details of how
exactly such an understanding looked, and how teams in the
competition behaved. We will demonstrate that the teams
did, in fact, play in a risk-aware fashion and, in turn, their
awareness of risk reflected their final standings in the game.
4.1

Conversion Ratio

A crucial measure of a team’s performance in our competition
is their conversion ratio—that is, the percentage of money

Figure 3: Relationship of the correlations between money and points
and between conversion ratio for subsets of the teams, by point cutoff.

that they were able to convert into points. While this is not
the only deciding factor in a teams’ final performance (for
example, it is obvious that the team needs money to launder
into points), we feel that it is an extremely important one.
It is intuitive that, with a low conversion ratio, even a team
that solves all of the challenges in the competition could fail
to achieve a large amount of points.
We calculated the correlation, defined as the covariance
of two variables divided by the product of their standard
deviations, between conversion ratio and the final point value
for the top 10 teams. This resulted in a correlation coefficient
of 0.81 between the conversion ratio and the final point values.
In contrast, the correlation coefficient between the amount of
money earned and the final point values is only 0.70. While
both money and conversion ratio are obviously correlated
to the final amount of points (and, thus, the final ranking),
the conversion ratio is clearly a more important measure of
success. To validate this hypothesis further, we examined
the correlation for all teams in the competition, as opposed
to only the top 10. We calculated the correlation at different
point cutoffs, with a point cutoff of 0 including every team
that scored points (a total of 62), and a point cutoff of 1,000
excluding all but the top three teams. We present the result
in Figure 3.
The correlation between conversion ratio and points becomes more significant than the correlation between money
and points at a points cutoff of 567. This cutoff excludes all
but the top 11 teams. From this we conclude that at all but
the highest level of the competition, money was the deciding
factor for competition placement. However, past a certain
level, conversion ratio becomes the dominating variable, becoming relatively more and more important as we limit the
analysis to the top-scoring teams.
4.2

Risk Analysis

Analysis of the dataset leaves no doubt that teams had different thresholds as to what they felt was acceptable risk. It is
interesting to examine how this threshold changed for teams
throughout the competition. In Figure 4, we present the
risks at which the top 5 teams laundered money throughout
the competition. It is apparent that the beginning of the
competition was rather chaotic for the teams. Those teams
that captured flags early in the competition laundered them
at varying amounts of risk, and all teams seemed to have
experimented with higher-risk laundering in the middle of
the game. However, three of the top four teams seem to have

Figure 4: The risk thresholds for the top 5 teams over the course of
the competition, averaged over a sliding window of 20 minutes. A risk
of 0 means guaranteed success, and a risk of 1 means certain failure.

Figure 5: The risk thresholds for the top 5 teams for different payoff
(P ) values. A risk of 0 means guaranteed success, and a risk of 1 means
certain failure.
settled on a risk threshold between 0.2 and 0.3 (less than a
30% chance of losing their money).
The teams were willing to accept different amounts of risk
for different levels of payoff. To visualize this, we calculated
the average risk of all laundering attempts with different
payoffs and plotted this for every team. As can be inferred
from Figure 5, the teams had payoffs below which they would
not launder and had higher risk thresholds for higher payoffs.
In fact, the first-place team, We 0wn Y0u, did not attempt
to launder if the payoff was less than 0.75. The strategic
reader will note that the optimal laundering attempt would
have a high payoff and a low risk, but such conditions were
rare. As a result, each of the top teams created a different
strategy as to what risk thresholds they would accept for
what payoffs.
4.3

Expected Value Analysis

To gleam further insight into the individual team strategies,
their evolution throughout the competition, and their effect
on the final standing of the teams involved, we calculated the
expected values of each laundering attempt. For every such
attempt, the expected value is the amount of points that a
team would expect to gain, statistically, from a laundering
attempt after factoring in the CPR values as described in
Section 3.3.4. For example, with a risk of 0.3, and a laundering success point value of 100 points, the expected value of
the laundering attempt would be 30 points.
We first looked at the overall strategies of every team that
attempted to launder money more than 200 times. This
threshold was chosen to allow us to analyze teams that

actually took the laundering aspect of the game seriously,
and teams with less than 200 attempts were unlikely to have
laundered enough to develop a coherent strategy. Specifically,
we wanted to compare how the expected values of the teams’
laundering attempts were influenced by the amount of money
that a team earned and, in turn, specifically how this affected
the outcome of the competition. For each team, we calculated
the expected value of each laundering attempt and plotted
this for each team, in order of the final rankings. For a
reference point, we included the amount of money that each
team acquired throughout the competition.
This bar plot, presented in Figure 6 with first place on the
left and last place on the right, reveals several interesting
things. First, we see that the difference between the first
and second-place teams were, again, largely influenced by
their laundering strategy. Specifically, We 0wn Y0u, despite
having considerably less money than More Smoked Leet
Chicken, were able to maintain a very high expected return
for their laundering attempts. This allowed them to more
efficiently convert their limited money into a high amount of
points. The attentive reader will notice that FluxFingers, the
third-place team, actually had the highest average expected
value in the game and had more money than the first-place
team. However, as can be noted by the outliers of near-0
expected values, FluxFingers made a series of mistakes and
wasted a considerable amount of money. The volume of
these mistakes, which is lost in the way the graph presents
information, makes this loss significant. The expected value
was important on the lower end of the competition, as well.
For example, HakM@rit was able to place higher than teams
with more money by maintaining a high expected value.
However, the difference in money between the top teams and
the lower teams was too great to overcome with laundering
strategy.
To dive deeper into the evolution of the teams’ strategies throughout the competition, we analyzed the change
in expected value of the top five teams. We calculated a
20-minute sliding window of the expected value of each of the
top teams’ laundering attempts. Figure 7 shows the changes
in these expected values over the course of the competition.
As expected, We 0wn Y0u, the first-place team, maintained
the highest expected value for most of the competition. The
second-place team spent the first two thirds of the competition with an extremely low expected value, using their sheer
amount of money as a substitute for a good strategy. The
teams’ strategies seem to have evolved into a more stable
approach to laundering as the competition progressed. We
believe this is due to an increased understanding of the game,
and the interface to the competition infrastructure, as the
game continued.
4.4

Strategy Analysis

Due to the amount of variables in the risk function, and their
dependence on the performance of other teams as well as
previous laundering attempts, calculating an optimal strategy
for our competition was a difficult challenge. The top three
teams in the competition submitted summaries [2, 5, 12] of
their experience during the contest, detailing their strategies,
successes, and failures.
Because we released the idea behind the competition in
advance, all of the teams entered the contest with some
automation for the risk calculation. However, because the
actual risk function was not released until shortly before the

Figure 6:

A whisker-plot of the expected values of the laundering
attempts of every team that laundered more than 200 times. The expected value, labeled on the left Y axis, is the amount of points that a
team would expect to gain from a laundering attempt after factoring
in the CPR values. The money amounts of every team, labeled on the
right Y axis and denoted with a dashed line, are specified as well, for a
more complete analysis.

Figure 7:

The expected values for the top 5 teams over the course of
the competition, averaged over a sliding window of 20 minutes. The
expected value is the amount of points that a team would expect to
gain from a laundering attempt after factoring in the CPR values.

competition, the teams faced issues when actually attempting
to use their automation. More Smoked Leet Chicken even
went as far as handling the submission manually, looking at
the output of the laundering service (as the final risk value
was printed as a debug statement), and decide how much
more money to launder through that service in that tick.
4.5

Case Study

For further insight into the importance of adapting a proper
approach to risk management to a teams’ final standing, we
decided to do a case study of three teams with similar resources but different outcomes. The three teams we selected,
Persistent Threat Hacking Club, CInsects, and VUBAR, had
all gathered the same amount of money (1640) throughout
the competition but, due to different risk management, finished the competition in very different positions. Specifically,
Persistent Threat Hacking Club placed 33rd, CInsects placed
19th, and VUBAR placed 14th.
We analyzed these three teams using the same approach
we used for the five top-scoring teams. We calculated the
average risk that each team took, the average payoff that they
accepted, and the change of their accepted risk over time and

Team Name
VUBAR
CInsects
PTHC

Rank
14
19
33

Avg Risk

Avg Payoff

0.058
0.14
0.27

0.87
0.86
0.57

Table 4:

The rank, average risk, and average payoff for three selected
teams in the competition.

5

Conclusion

Cyber-security competitions are often viewed as an entertaining exercise in which hackers can match skills against one
another. However, such competitions can also be utilized to
study interesting aspects of security incidents. In this paper,
we presented a novel approach in utilizing a CTF competition
to create a useful model of the risk-reward trade-offs present
in such incidents. This approach, which was implemented to
host a large-scale experiment involving over 1,000 players in
89 teams around from 18 countries, succeeded in generating
a dataset that we hope will be useful for further study of
such security activity. We described this approach in this
paper and present some analysis of the dataset, showing that
our experiment did encourage and record risk-aware behavior
among the participants.
References

Figure 8: The risk thresholds for three selected teams over the course
of the competition. A risk of 0 means guaranteed success, and a risk of
1 means certain failure.

Figure 9: The risk thresholds for the three selected teams for different
payoff (P ) values. A risk of 0 means guaranteed success, and a risk of
1 means certain failure.

across different payoff values. Our analysis confirmed our
initial suspicions: among the three teams, a more consistent
and conservative approach to risk led to a better standing
in the final rankings. Specifically, it seemed that Persistent
Threat Hacking Club, with an average risk of 0.27, did not
have a specific strategy in terms of risk, with their risk
threshold varying widely through time. On the other hand,
VUBAR, with an average risk of 0.058, seemed to keep their
risk tightly under control. Additionally, VUBAR didn’t
accept any payoff under 0.75, and only slightly raised their
accepted risk when given a payoff of 0.9. Finally, CInsects
settled for payoffs as small as 0.55 and let their risk soar
when presented with high payoffs (see Table 4 and Figures 8
and 9).

[1] Z. Chen. Modeling the spread of active worms. In INFOCOM
2003. Twenty-Second Annual Joint Conference of the IEEE
Computer and Communications. IEEE Societies, pages 1890–
1900, 2003.
[2] M. S. L. Chicken. 2011 iCTF writeup. http://ictf.cs.ucsb.
edu/archive/iCTF_2011/iCTF2011_MSLC_writeup.pdf.
[3] M. Cova, C. Kruegel, and G. Vigna. Detection and analysis
of drive-by-download attacks and malicious javascript code.
In Proceedings of the 19th international conference on World
wide web, WWW ’10, pages 281–290, New York, NY, USA,
2010. ACM.
[4] A. Doupé, M. Egele, B. Caillat, G. Stringhini, G. Yakin,
A. Zand, L. Cavedon, and G. Vigna. Hit ’em where it hurts: a
live security exercise on cyber situational awareness. In Proceedings of the 27th Annual Computer Security Applications
Conference, pages 51–61. ACM, 2011.
[5] Fluxfingers. 2011 iCTF writeup. http://ictf.cs.ucsb.edu/
archive/iCTF_2011/iCTF2011_FluxFingers_writeup.txt.
[6] G. Gu, P. Porras, V. Yegneswaran, M. Fong, and W. Lee.
Bothunter: detecting malware infection through ids-driven
dialog correlation. In Proceedings of 16th USENIX Security
Symposium on USENIX Security Symposium, SS’07, pages
12:1–12:16, Berkeley, CA, USA, 2007. USENIX Association.
[7] U. C. S. Lab. ictf 2011 dataset. http://ictf.cs.ucsb.edu/
data/ictf2011/.
[8] N. Provos, P. Mavrommatis, M. A. Rajab, and F. Monrose. All
your iframes point to us. In Proceedings of the 17th conference
on Security symposium, SS’08, pages 1–15, Berkeley, CA, USA,
2008. USENIX Association.
[9] M. Rajab, L. Ballard, N. Jagpal, P. Mavrommatis, D. Nojiri, N. Provos, and L. Schmidt. Trends in circumventing web-malware detection. Technical report, Google,
Tech. Rep., Jul. 2011, ht tp://static. googleusercontent. com/external content/untrusted dlcp/research. google.
com/en/us/archive/papers/rajab-2011a. pdf, 2011.
[10] N. Stockman, K. Vamvoudakis, L. Devendorf, T. Hollerer,
R. Kemmerer, and J. Hespanha. A mission-centric visualization tool for cybersecurity situation awareness. Technical
report, University of California, Santa Barbara, August 2012,
https://www.cs.ucsb.edu/research/tech reports/reports/201207.pdf, 2012.
[11] G. Stringhini, M. Egele, A. Zarras, T. Holz, C. Kruegel, and
G. Vigna. B@bel: Leveraging email delivery for spam mitigation. In Proceeding of the 2012 USENIX Security Symposium,
2012.
[12] We 0wn Y0u. 2011 iCTF writeup. http://ictf.cs.ucsb.
edu/archive/iCTF_2011/iCTF2011_We0wnYou_writeup.pdf.

State-aware Network Access Management
for Software-Defined Networks
Wonkyu Han† , Hongxin Hu‡ , Ziming Zhao† , Adam Doupé† ,
Gail-Joon Ahn† , Kuang-Ching Wang‡ , and Juan Deng‡
†
‡
Arizona State University
Clemson University
{whan7, zzhao30, doupe, gahn}@asu.edu, {hongxih, kwang, jdeng}@clemson.edu
ABSTRACT
OpenFlow, as the prevailing technique for Software-Defined Networks (SDNs), introduces significant programmability, granularity,
and flexibility for many network applications to effectively manage and process network flows. However, because OpenFlow attempts to keep the SDN data plane simple and efficient, it focuses
solely on L2/L3 network transport and consequently lacks the fundamental ability of stateful forwarding for the data plane. Also,
OpenFlow provides a very limited access to connection-level information in the SDN controller. In particular, for any network
access management applications on SDNs that require comprehensive network state information, these inherent limitations of OpenFlow pose significant challenges in supporting network services.
To address these challenges, we propose an innovative connection tracking framework called S TATE M ON that introduces a global
state-awareness to provide better access control in SDNs. S TATE M ON is based on a lightweight extension of OpenFlow for programming the stateful SDN data plane, while keeping the underlying network devices as simple as possible. To demonstrate the
practicality and feasibility of S TATE M ON, we implement and evaluate a stateful network firewall and port knocking applications for
SDNs, using the APIs provided by S TATE M ON. Our evaluations
show that S TATE M ON introduces minimal message exchanges for
monitoring active connections in SDNs with manageable overhead
(3.27% throughput degradation).

1.

INTRODUCTION

Over the past few years, Software-Defined Networks (SDNs)
have evolved from purely an idea [12, 13, 18] to a new paradigm
that several networking vendors are not only embracing, but also
pursuing as their model for future enterprise network management.
According to a recent report from Google, SDN-based network
management helped them run their WAN at close to 100% utilization compared to other state-of-the-art network environments with
about 30% to 40% network utilization [22].
As the first widely adopted standard for SDNs, OpenFlow [28]
essentially separates the control plane and the data plane of a network device and enables the network control to become directly
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SACMAT’16, June 05-08, 2016, Shanghai, China
c 2016 ACM. ISBN 978-1-4503-3802-8/16/06. . . $15.00

DOI: http://dx.doi.org/10.1145/2914642.2914643

programmable as well as the underlying infrastructure to be abstracted for network applications. With OpenFlow, only the data
plane exists in the network device, and all control decisions are
conveyed to the device through a logically-centralized controller.
In this way, OpenFlow can tremendously help administrators access and update configurations of network devices in a timely and
convenient manner and provide this ease of control to SDN applications as well.
While the abstraction of a logically centralized controller, which
is a core principle of SDNs is powerful, a fundamental limitation
of OpenFlow is the lack of capability to enable the maintenance of
network connection states inside both the controller and switches.
First, OpenFlow-enabled switches only forward the first packet of
a new flow to the controller so that the controller can make a centralized routing decision. Because the controller is unaware of subsequent packets of the flow, including those that change the state
of a network connection (e.g., TCP FIN), the controller has no
knowledge of the state of the connections in its network. Second,
OpenFlow-enabled switches are incapable of monitoring network
connection states as well. The “match-action” abstraction of OpenFlow heavily relies on L2/L3 fields (e.g., src_ip and dst_ip) and
the limited L4 fields (only src_port and dst_port), yet essential
information for identifying and maintaining the state of connections is contained in other L4 fields, such as TCP flags and TCP
sequence and acknowledgment numbers.
The lack of knowledge of network connection states in SDNs
brings significant challenges in building state-aware access control
management schemes [30]. In particular, some critical security services, such as stateful network firewalls that perform network-wide
access control, cannot be realized in SDNs. A stateful network firewall, which is a key network access control service in a traditional
network environment [17, 20, 34] and requires state-awareness,
keeps track of the states of connections in the network and makes
a decision for its access (e.g., ALLOW or DENY) according to the
states of connections in networks. However, it is impossibly hard
to realize them in current SDNs due to the inherent limitations of
OpenFlow.
Some recent research efforts [29, 30, 14, 36, 11, 6, 10, 37] extended the OpenFlow data plane abstraction to support stateful network applications. They attempted to let individual switches, rather
than the controller, track the state of connections. We believe that,
not only does this design go against the spirit of SDN (because it
brings the control plane back to switches and makes switches manipulate connection states and performs complex actions beyond a
simple forwarding operation), these existing approaches are only
applicable for designing applications that need only local states
on a single switch [10]. However, such solutions force SDN applications individually access every single switch to collect entire

network states, consequently network-wide monitoring to detect
abnormalities and enforcing network-wide access control of flows
become extremely difficult.
To overcome the limitations of existing approaches, we argue
that utilizing the SDN controller for global tracking connections
is more advantageous than existing solutions in terms of its state
visibility across SDN applications that is crucial to some security
applications such as a stateful network firewall. To bring such a
state-aware network access management in SDNs, we propose a
novel state tracking framework called S TATE M ON. S TATE M ON
models active connections in SDNs and monitors global connection
states in the controller with the help of both a global state table that
records the current state of each active connection and a state management table that governs the state transition of new and existing
connections. S TATE M ON also introduces a lightweight extension
to OpenFlow, called OpenConnection, that programs the data plane
to forward the state-changing packets to the controller. At the same
time, it retains the simple “match-action” programmable feature of
OpenFlow and avoids scalability problems over the communication
channel between the controller and switches. In essence, S TATE M ON follows the general SDN principle of logical-to-physical decoupling and avoids embedding complicated control logic in the
physical devices, therefore, keeping the SDN data plane as simple
as possible.
In addition, to demonstrate the practicality and feasibility of S TATE M ON and state-aware network access management applications in
SDNs, we design a stateful network firewall based on the APIs
provided by S TATE M ON. Our firewall application provides more
in-depth access control than a stateless SDN firewall [21]. It detects and resolves connection disruptions and unauthorized access
attempts targeting active connections in SDNs. To demonstrate
the generality of S TATE M ON, we re-implement a prior work (port
knocking) based on S TATE M ON (Section 5.2.3). Our experimental results show that S TATE M ON and network access management
applications (stateful firewall and port knocking) introduce manageable performance overhead to manage network access control.
Contributions: The contributions are summarized as follows:
• We propose a connection tracking framework called S TATE M ON that enables SDN to support state-aware access control
schemes by leveraging global network states. S TATE M ON
keeps the data plane as simple as possible, thus being compliant with the spirit of SDN’s design principle.
• We propose the OpenConnection protocol, which is a lightweight
extension to OpenFlow and retains the simple “match-action”
programmable feature of OpenFlow to enable a stateful SDN
data plane.
• We implement a prototype of S TATE M ON using Floodlight [1]
and Open vSwitch. Our experiments demonstrate that S TATE M ON introduces a minimal increase of communication messages with manageable performance overhead (3.27% throughput degradation).
• We design a stateful network firewall application, using the
APIs provided by S TATE M ON. Our experiments show that
the stateful firewall provides more control than existing stateless firewalls and it can effectively detect and mitigate certain
connection-related attacks (e.g., connection disruptions and
unauthorized access) in SDNs.
This paper is organized as follows. We overview the motivating problems in Section 2. Section 3 presents the design of state-

SDN Applications
Route
App

Load-balance
App

Firewall
App

SDN Controller
Webserver C

Stateless Policy
B → A ALLOW

Connection
Disruption/
Termination
Forward Flow

Reverse Flow

A→B

Host A

B→A
S1

S2

S3

fe1: A → B forward

fe2: A → B forward

fe3: A → B forward

fe6: B → A forward

fe5: B → A forward

fe4: B → A forward

Webserver B

Figure 1: Standard OpenFlow Operation and its Stateless Property.
aware S TATE M ON. Section 4 describes the design of stateful network firewall supported by S TATE M ON, and the implementation
and evaluation details are in Section 5. Section 6 discusses the related work of this paper, and Section 7 describes several important
issues. In Section 8, we conclude this paper.

2. BACKGROUND AND PROBLEM STATEMENT
To understand our proposed solution to adding state-awareness
to SDNs, we provide an overview of the current OpenFlow operation. When an OpenFlow-enabled switch receives a packet, it first
checks its flow tables to find matching rules. If no such rules exist, this means it is the first packet of a new flow. The switch then
forwards the packet to the controller, and it is the controller’s job
to decide how to handle the flow and to install flow table rules in
the appropriate switches. Specifically, the packet is encapsulated
in an OFPT_PACKET_IN message sent to the controller, and the
controller then installs corresponding rules called flow entries into
the switches along the controller’s intended path for the flow. Once
these flow entries are installed, all subsequent packets of this flow
are automatically forwarded by the switches, without sending the
packet to the controller.
For example, in Figure 1, host A wants to initiate a TCP connection with web server B. The first packet (TCP SYN) sent by host A
is checked by the ingress switch S1 and forwarded to the controller
because S1 has no flow table entry for the packet. The controller
allows the flow from host A to server B by installing flow entries
f e1 , f e2 , and f e3 , into switches S1, S2, and S3, respectively. The
flow from host A to server B is called a forward flow. Using the
same process, the response packet (TCP SYNACK) generated by
server B will trigger the controller to install f e4 , f e5 , and f e6 into
S3, S2, and S1, respectively. The flow from server B to host A is
called a reverse flow.
As can be seen from Figure 1, neither the OpenFlow-enabled
switch nor the controller has the ability to track and maintain connection states, which makes it impossible to directly develop stateful access control based on OpenFlow in SDNs. As a result, existing SDN controllers (e.g., Floodlight) only have a stateless firewall
application that enforces ACL (Access Control List) rules to monitor all OFPT_PACKET_IN behaviors.
Using Figure 1 as an example, these stateless firewall applications can only specify simple rules, such as “packets from server B
to host A are allowed.” In contrast, a stateful firewall is a critical component in traditional systems and networks which provides
more control over whether a packet is allowed or denied based on

Table 1: Existing Stateful Inspection and Management Methodologies for SDNs (D = data plane, C = control plane, A = application plane).
Solution
App-aware [29]
FAST [30]
FlowTags [14, 15]
OpenNF [16]
UMON [36]
P4 [11]
Conntrack [6]
OpenState [10]
SDPA [37]
S TATE M ON

Inspection
D C A
X
X
X
X
X X
X
X
X
X
X
X X

D
X
X

Storage
C A

X
X
X
X
X
X
X
X

X

Implementation
FW, LB
FW
Proxy cache
IDS, Net-Monitor
SW Switch
SW Switch
SW Switch
FW, HW Switch
FW, Port-Knock

Description
Maintain App-table in a switch; A switch performs handshaking on behalf of servers.
Controller compiles the state machine and installs it into switches.
Add tags to in-flight packets for keeping middleboxes’ state rather than checking state via switches or the controller.
OpenNF enables dynamic migration of middlebox states from one to another by supporting some operations (e.g., move, copy and share).
Put UMON tables in the middle of OpenvSwitch pipelines to perform anomaly detection.
A proposal for embedding programmable parser inside of switches to allow administrators to flexibly configure and define the data plane.
Build the conntrack module on top of existing OpenvSwitch implementation to enable stateful tracking of flows.
Perform state checking using the state table in conjunction with an extended finite state machine that is directly programmable by the controller.
Insert the forwarding processor in packet processing pipeline to enable stateful forwarding scheme; It also includes hardware-based design.
Using OpenConnection protocol, the controller centrally manages network states and provides them to SDN applications via APIs.

connection state information. For example, a stateful firewall rule
could specify “packets from server B to host A are allowed, if and
only if host A initiates the connection to server B.” These stateful rules are incredibly useful for security purposes, for instance to
specify that a web server should be able to accept incoming connections but never initiate an outgoing connection. However, despite
the great security benefit of these stateful policies, it is challenging to build a stateful firewall in SDNs without the full support of
stateful packet inspection [21], which is critical to provide effective
network access control management.
In addition to the development of a stateful firewall application,
the knowledge of connection states in SDNs can also help maintain the network’s availability. The SDN controller and applications can install, update, or delete flow entries for their own purposes. However, these actions may interrupt established connections, which may consequently damage the availability of services
in the network. Consider the case of a load balancer application,
which switches flows between two web servers (Servers B and C
in Figure 1). If the flows are changed while a network connection
is still in progress, the availability of the service would be affected.
Also, attackers, who are able to perform a man-in-the-middle attack
on OpenFlow-enabled switches [9], can also disrupt existing connections in the network by intentionally updating flow entries. The
root cause of these issues is that the controller and the SDN applications have no knowledge of the connection states, which results
in creating potential chances of unauthorized access into existing
connections by attackers. We argue that a critical functionality of
OpenFlow or any other SDN implementation is that the controller
should be able to identify the conflicts between active connections
and any pending flow entry update and provide network administrators with an early warning before a conflicting flow entry takes
effect. Existing verification tools [23, 24, 25, 27] cannot detect
and address such conflicts, because they are unaware of connection states in the network. By tracking global connection states in
the network, the controller will be able to deal with such conflicts
and help maintain the availability of the services in the network.
We summarize existing solutions in Table 1 that are mostly applicable only for designing applications that need states locally.
Among those solutions, only OpenNF [16] and P4 [11] attempt to
utilize the control plane of SDNs for state checking and consolidating network states. OpenNF focuses on collecting states of network
middleboxes (e.g., IDS, Net-Monitor) to support dynamic middlebox migration, and P4 is a proposal for next generation of OpenFlow to support state inspections. However, the former is not applicable for collecting generic network states (e.g., connection state),
and the latter does not include a workable implementation. Thus,
we argue that a global connection monitoring framework, which
can be aggregated by the controller, is imperative for network-wide
connection monitoring and access management. Such a global connection awareness not only enables stateful firewall applications to
detect indirect policy violations considering dynamic packet modi-

State-aware Access Control Applications
Other Applications
(e.g., port knocking)

Stateful Network Firewall

STATEMON
APIs

Controller
Flow
Programming
Module

Topology
Manager

...

...

Connection
Tracking
Module

OpenFlow
Messages

Global State
Table
State
Management
Table

OpenConnection
Messages

OpenFlow Channel

OpenConnection Channel

Control Channel

Port
Port

Port
Flow
Table
(1)

...

Flow
Table
(n)

OpenConnection
Table

Port

Pipeline

OpenConnection-enabled Switch

Figure 2:

S TATE M ON Architecture Overview

fication in SDNs, but also helps identify connection disruptions and
unauthorized access occurred in existing connections.

3.

S TATE M ON DESIGN

In this section, we first present the key design goals of our S TATE M ON framework. Then, we illustrate the overall architecture and
working modules of S TATE M ON and further show how they meet
our design goals.

3.1 Design Goals
To enable stateful access management applications and overcome
the limitations of existing approaches, we propose a novel stateaware connection tracking framework called S TATE M ON to support building stateful network firewall for SDNs. S TATE M ON is
designed with the following goals in mind:
• Centralization: S TATE M ON should, in adhering to the principles of SDN, manage a global view of all network connection states in a centralized manner at the control plane.
• Generalization: S TATE M ON should support any state-based
protocols and provide state information to SDN applications.
• High Scalability: S TATE M ON should minimize message exchanges between the controller and switches so that the control channel will not be the performance bottleneck when
monitoring all network connection states.

Connection Match Fields
OpenFlow Fields

Flags

SEQ

ACK

Actions

...

OC_CON_SIG Match Fields
OpenFlow Fields

Extended Fields

Flags

SEQ

ACK

...

Extended Fields

Packet In

OpenFlow-based
packet process

OpenConnection-based packet process
Direct packet to
OpenConn table?
NO

Figure 3: Structure of An Entry in An OpenConnection Table.

YES

Match signal
entry?

YES

Send OC_CON_SIG
message

NO

3.2

Match connection
entry?

S TATE M ON Architecture Overview

Figure 2 shows an overview of the S TATE M ON architecture, which
adds new modules in both the control plane (controller) and the data
plane (switches) of the OpenFlow system architecture.
To achieve the centralization goal, S TATE M ON modules in switches
use only the match-action abstraction to perform packet lookups,
forwarding, and other actions based on the OpenConnection table
(Section 3.3), whereas modules in the controller track a global view
of states (Section 3.4). A controller uses the OpenConnection protocol to program OpenConnection tables, which are added to the
OpenFlow processing pipeline by introducing a “Goto OpenConnection Table” instruction (Goto-OCT) in OpenFlow action set.
To achieve the generality goal, S TATE M ON maintains a pair of
global state table and state management table for each state-aware
application. A state-aware application initializes those tables and
registers callback functions using the APIs provided by S TATE M ON. The global state table records network-wide connection state
information. Each entry in this table represents an active connection by specifying the flow entries that govern the active connection (e.g., f e1 , · · · , f e6 in Figure 1) and its connection state (e.g.,
ESTABLISHED in TCP). The state management table keeps state
transition rules and actions that should be performed on each state
(e.g., send an OpenConnection message to the controller).
S TATE M ON uses three methods to minimize the communication
overhead between the controller and switches to meet the high scalability design goal. First, S TATE M ON leverages existing OpenFlow protocols such as OFPT_PACKET_IN message for monitoring connection states. For example, the first packet of a new flow
delivered by OFPT_PACKET_IN message would not trigger a separate OpenConnection message. Second, S TATE M ON identifies
ingress and egress switches for each connection and only installs
necessary OpenConnection entries into those switches to perform a
state-based inspection. Thus, S TATE M ON minimizes the increase
of additional table entries and avoids the potential overhead that
can be generated by other intermediate switches on the path. Third,
the OpenConnection protocol sends only expected state-changing
packets from switches to the controller.

3.3 OpenConnection Protocol
On receipt of a packet, an OpenConnection-enabled switch starts
with the OpenFlow-based packet process. For any new flow, the
first packet of this flow is forwarded to the controller via an OFPT_
PACKET_IN message. Then, the controller determines whether
that packet should be sent. If so, the controller will install new
flow entries into corresponding switches to handle future packets of
the same flow. S TATE M ON also listens to the OFPT_PACKET_IN
message. If this message carries a packet that any state-aware application wants to monitor (Section 3.5), S TATE M ON will install
OpenConnection entries in OpenConnection tables (Section 3.3.1)
of corresponding switches using OpenConnection messages (Section 3.3.2) and add a Goto-OCT instruction in the flow entries to
start OpenConnection processing pipeline.

3.3.1 OpenConnection Table
Before illustrating how OpenConnection-enabled switches process packets, we first explain the structure of the OpenConnection

NO

Drop packet

YES
Update SEQ/ACK
numbers of signal/
connection entries

Excute
action set

Figure 4: Flowchart for OpenConnection Packet Processing.
Table 2: OpenConnection Messages (C: controller, S: switch)
Message Name

Direction

OC_CON_SIG

S→C

OC_ADD
OC_UPDATE
OC_REMOVE

C→S
C→S
C→S

Description
Encapsulate entire packet (including payload)
and forward it to connection tracking module
Install a new entry in an OpenConnection table
Update an OpenConnection entry
Remove an OpenConnection entry

table. An OpenConnection entry, which is shown in Figure 3,
has (1) connection match fields, (2) actions for a decision of forward, drop, and update fields, etc., and (3) OC_CON_SIG match
fields that triggers switches to send OC_CON_SIG message when
matched. To achieve generality, both connection and OC_CON_SIG
match fields are directly programmable by state-aware SDN applications (Section 3.5).
If and only if a packet matches connection match fields, the
packet will be processed by both the OpenFlow and OpenConnection pipeline as shown in Figure 4. In case the packet also
matches the OC_CON_SIG match fields, which means the packet
is a state changing packet, such as FIN in TCP, it will be encapsulated in an OC_CON_SIG message and forwarded to the connection tracking module of S TATE M ON in the controller. The connection tracking module will maintain the state and manage associated
switches accordingly. Upon completion of these OpenConnectionbased packet process, the action set that includes the rest of the
OpenFlow actions will be executed.
The design of the OpenConnection table is aligned in spirit to the
design of the flow table, so that the data plane can process packets
using the simple “match-action” paradigm. However, OpenConnection tables are more scalable than OpenFlow tables, because
OpenConnection table entries are only installed in the OpenConnection tables of the two endpoint switches that directly connect
the initiating host and the receiving host of a connection. In contrast, using OpenFlow for each new flow, corresponding flow entries must be installed in all flow tables of switches that the flow
traverses.

3.3.2 OpenConnection Message Exchanging Format
We define four OpenConnection messages to enable state-based
connection monitoring. OpenConnection messages help the connection tracking module of S TATE M ON monitor the overall process
of connection establishment and tear-down behaviors occurring in
the data plane. Table 2 summarizes the four OpenConnection messages with a brief description of each.
The OC_CON_SIG message is used to encapsulate the statechanging packet and conveying it to the controller (switch-to-controller
direction). The main difference from OpenFlow OFPT_PACKET_IN

Table 3: State Management Table Example for TCP connection. (A (or B) refers a pair of hIP, porti.)
State
INIT
SYN_SENT
SYNACK_SENT
ESTABLISHED
FIN_WAIT
CLOSED

Transition Conditions
Message Type
Source
Match Fields
OFPT_PACKET_IN Ingress
A→B, TCP, Flag=SYN
OFPT_PACKET_IN Egress B→A, TCP, Flag=SYNACK
OC_CON_SIG
Ingress
A→B, TCP, Flag=ACK
Ingress
A→B, TCP, Flag=FIN
OC_CON_SIG
Egress
B→A, TCP, Flag=FIN
Egress
B→A, TCP, Flag=FIN
OC_CON_SIG
Ingress
A→B, TCP, Flag=FIN
-

-

-

is that the OC_ CON_SIG message is only for S TATE M ON (so
that it will not be effective to other SDN applications), and it also
contains a randomly generated unique identifier for the connection
to distinguish the affiliation of the message. The other messages
are sent from the controller to the switches to program an OpenConnection table. The connection tracking module generates a
OC_ADD message to install a new entry in an OpenConnection table. For instance, to monitor a TCP connection, it installs an entry
to match TCP ACK packet at its ingress switch of the flow path.
OC_UPDATE is used for updating an OpenConnection table entry.
If a connection is terminated (or by timeout mechanism), the connection tracking module sends an OC_REMOVE message to remove
all associated entries. Compared with OpenFlow, which exchanges
messages between the controller and multiple switches, OpenConnection introduces only a constant number of message exchanges
between the controller and two endpoint switches for handling a
specific state-based connection. Using TCP as an example, OpenConnection uses eight messages in total for a TCP connection (see
Table 5): (1) three OC_CON_SIG messages, (2) two OC_ADD messages, (3) one OC_UPDATE message, and (4) two OC_REMOVE
messages.

3.4 Tracking Connection States
For generality, S TATE M ON maintains a pair of global state table
and state management table for each state-aware application. The
connection tracking module listens to OFPT_PACKET_IN messages to initialize an entry in the global state table for a connection
and listens to OC_CON_SIG messages to update the states of the
connection based on state transition rules in the state management
table provided by the application.

3.4.1 Global State Table
The global state table records network-wide connection state information. However, simply extracting a connection’s state from a
specific switch is not sufficient to account for the overall global
state of a connection. Because OpenFlow-enabled switches are
able to rewrite packets’ headers at any point using the Set-Field
action, a packet’s header may look different at its ingress and egress
switches. This poses a challenge for the controller to identify which
packets belong to the same connection. To solve this problem,
S TATE M ON bonds a connection’s state (e.g., ESTABLISHED) with
its associated network rules (i.e.,the forward and reverse flow entries) to effectively monitor and track an active connection.
We design the entry in the global state table as a 5-tuple entry denoted hCI , CE , σF , σR , Sa i. Connection information at the
ingress switch (CI ) contains a set of packet header fields along
with its incoming physical switch port, pi . Connection information at the egress switch (CE ) contains the same elements, except
po which refers to the outgoing physical switch port. For instance,
CI for a TCP connection can be defined as hsrc_ip, src_port, dst_ip,
dst_port, network_protocol, pi i. Note that some fields in CI and CE
(e.g., src_ip, src_port, dst_ip, dst_port) might not be identical

Next State
SYN_SENT
SYNACK_SENT
ESTABLISHED

Message Type
OC_ADD
OC_ADD
OC_UPDATE

OpenConnection Events
Destination OC_CON_SIG Match Fields
Ingress
A→B, TCP, Flag=ACK
Egress
B→A, TCP, Flag=FIN
Ingress
A→B, TCP, Flag=FIN

FIN_WAIT

∞
5
5
1800

CLOSED
INIT

Timeout

60
OC_REMOVE
OC_REMOVE

Ingress
Egress

0

due to dynamic packet modification (Set-Field action) in SDNs.
σF is a series of identifiers of flow entries that enable the forward
flow, and σR is also a series of identifiers for the reverse flow. For
example, the forward flow and the reverse flow in Figure 1 would
be σF = hf e1 , f e2 , f e3 i and σR = hf e4 , f e5 , f e6 i, respectively.
The last element, Sa , denotes the state of a connection and it will
be further elaborated in Section 3.4.2.
The elements in a global state table entry have several properties.
The relation between CI and CE is to be determined by σF or σR
σ
−1
−1 σR
−−→ CI−1 . CI−1 and CE
are
such that CI −−F→ CE and CE
directly derived from CI and CE by replacing the source with the
destination and changing the incoming port (pi ) to the outgoing
port (po ). For example, if CI =hsrc_ip: 10.0.0.1, src_port: 3333,
dst_ip: 10.0.0.2, dst_port: 80, network_protocol: tcp, pi : 2i then
CI−1 =hsrc_ip: 10.0.0.2, src_port: 80, dst_ip: 10.0.0.1, dst_port:
3333, network_protocol: tcp, po : 2i.

3.4.2 State Management Table
An entry in the state management table is a 5-tuple denoted as
hState, Transition Conditions, Next State, OpenConnection Events,
Timeouti. When an OFPT_PACKET_IN or OC_CON_SIG message is received, the connection tracking module compares its originated location and header of the encapsulated packet with the Source
and Match Fields of the current state in the state management table. If the packet meets the Transition Conditions of the current
state, the state will be updated to the Next State and OpenConnection Events will be triggered. OpenConnection events instruct
the connection tracking module to send OC_ADD, OC_UPDATE,
or OC_REMOVE to corresponding switches. The Match Fields in
OpenConnection Events will configure the OpenConnection table
entries in corresponding switches to initialize connection and OC_
CON_SIG match fields. Timeout allows S TATE M ON to automatically close a connection.
Table 3 shows how a state-aware application can use the state
management table for the TCP state transitions. A TCP connection
starts with INIT state that transitions to SYN_SENT when it receives an OFPT_PACKET_IN message that contains a TCP SYN
flag. S TATE M ON identifies the location of the ingress switch (I)
from the message, and it sends an OC_ADD message back to I
with its match fields. S TATE M ON locates the egress switch (E)
as well by listening for the second OFPT_PACKET_IN message.
OC_CON_SIG messages collected from I or E are then used to
update the connection states. CLOSED is a temporary state only
used for sending OC_REMOVE messages and removing the associated entries. Note that one state can transition to multiple Next
States based on matching conditions and generate a variety of
actions as defined by SDN applications.

3.5

S TATE M ON APIs

S TATE M ON provides three types of application programming interfaces (APIs) for SDN applications so that the applications only
need to implement their business logic. The APIs can be used (1)

Table 4: S TATE M ON APIs
Category

API Name
InitGST()

Type I

InitSMT()
SetInterest()
SearchEntry()

Type II

Key Parameters
Match fields in
CI and/or CE
5-tuple of state
management table
Range of match
fields with wildcard
Raw packet or
ConnectionID

GetConnState()

ConnectionID

DeleteEntry()

ConnectionID
Type of message
and raw packet
ConnectionID and
next state

ConnAttempt()
Type III
StateChange()

Description
Initialize the global
state table
Initialize the state
management table
Search an associated
global state entry
Obtain current state
of a connection
Delete a connection
Callback function:
return one of actions
(allow or drop)

to configure both the global state table and the state management
table (Type I), (2) to retrieve state information from the global
state table (Type II), and (3) to register callback functions in
S TATE M ON to subscribe specific state-based events (Type III).
The APIs are summarized as follows:
• Type I is used to configure the two state-specific tables in
S TATE M ON: the global state table and the state management
table. To customize the global state table, SDN applications
can specify match fields for CI or CE (e.g., IP and port number) to distinguish one connection from another. Applications can also define a state set for the connection along with
its transition rules for the state management table.
• Type II APIs are built for sending queries (applications to
S TATE M ON) to retrieve network states, which SDN applications are interested in. Because all connection information is
recorded in the global state table, those queries are directly
conveyed to the global state table.
• Type III APIs are used to register callback functions in
S TATE M ON. For example, when a global state entry is updated, S TATE M ON can call this function to subscribing applications to allow them to execute their own business logic.

4.

STATEFUL FIREWALL DESIGN

In this section, to demonstrate the practicality and feasibility of
S TATE M ON and state-aware network access management applications in SDNs, we illustrate how a stateful firewall can take advantage of S TATE M ON to implement its state-aware access control
logic in SDNs.
The stateful firewall application first calls Type I APIs to initialize its global state table and state management table. We focus on TCP connections as a state-based protocol for this application. To enforce a stateful firewall policy such as “host B can
communicate with host A if and only if host A initiates the connection,” our firewall uses the state management table shown in
Table 3. Then, S TATE M ON calls the registered callback function
(Type III) when a state changing event occurs. The application only needs to implement the logic in the callback function: (1)
a packet (or flow) heading from host B to host A should be denied when its state is in INIT or SYNACK_SENT and (2) a packet
(or flow) heading from host B to host A should be allowed when
its state is in SYN_SENT or ESTABLISHED. Thus, the connection attempt (e.g., TCP SYN) initiated from host B cannot be made
whereas the attempt from host A will pass.
To show some benefits of our stateful firewall, we focus on following features: (1) state-aware firewall policy enforcement, (2)

Algorithm 1: Obtaining Affected Entry Set (AES)
Input: New (or Updated) flow entry (nf ) and existing flow entries
(F E = {e1 , e2 , ...}) at the same switch.
Output: Affected entry set AES = {a1 , a2 , ...} such that ai ∈ F E.
/* First, append the new flow entry (nf ) to AES
*/
AES.append(nf );
/* F Et : a set of flow entries installed in table t
*/
F Et ←− retrieveEntries(nf.getSwitchID, nf.getT ableID);
foreach e ∈ F Et do
/* Check if nf has higher priority than e and is
dependent with e
*/
if nf .priority ≥ e.priority and nf .match ∩ e.match 6= ∅ then
AES.append(e);
/* Recursively perform identical operation if e
has Goto-OCT instruction
*/
if e.getInstruction contains GotoT able then
temp_e.match ←− e.applyActions();
temp_e.setT ableID(e.getInstruction.getT ableID);
AES_child = self.(temp_e, E);
AES.append(AES_child);
return AES;

connection disruption prevention, and (3) unauthorized access prevention against active connections.

4.1 State-aware Firewall Policy Enforcement
Since S TATE M ON provides global network states to the firewall,
our firewall application utilizes the state information for the following scenarios: (1) a host attempts to establish a new connection, (2)
the state of an active connection has been updated, and (3) the firewall application updates the firewall policy.
First, when host A attempts to open a new connection to host
B, both host A and host B exchange initiating signal packets to
establish the connection. As soon as S TATE M ON receives these attempts, the firewall would get relevant information via the Type
III callback function defined when it called ConnAttempt(). If
this attempt violates the pre-defined stateful firewall policy, the initiating packet is immediately denied and the firewall stops the controller from executing the rest of the OFPT_PACKET_IN handling
process so that no flow entry is sent to the switches.
Second, if a global state entry is updated, the stateful firewall will
also be notified via Type III callback function, StateChange().
Our firewall application performs pair-wise comparison, the current
state of the connection against existing stateful firewall policies.
The firewall searches the associated global state entry by calling
SearchEntry() and acquires the connection information from the
entry. To consider Set-Field actions, it retrieves tracked space
denoted T (I, E), getting hsrc_ip, src_porti from I and hdst_ip,
dst_porti from E. By putting them together, we obtain T (I, E) =
hI.src_ip, I.src_port, E.dst_ip, E.dst_porti. Using the combination of T (I, E) and its current state, the firewall checks for rule
compliance with firewall policies. If the update of the state is not
allowed by the policy, the application raises an alarm to network
administrators and the update is denied by setting the return value
of StateChange() to drop. In case the stateful firewall application
wants to remove the connection, it may invoke DeleteEntry() function to remove the associated entries from the OpenConnection and
flow tables.
The final scenario deals with the case of updating firewall policies. When the firewall application updates a stateful rule in its
policy set, all active connections are examined against the new rule
to identify potential violations. Because each firewall policy has a
priority, computing dependency relations of firewall rules after the

Table 5: Additional State Management Table Entries for Unauthorized Access Prevention
State
SYNACK_SENT
SYNACK_SENT
ESTABLISHED
ESTABLISHED
DETECTED

Message Type
OC_CON_SIG
OC_CON_SIG
OC_CON_SIG
OC_CON_SIG
-

Transition Conditions
Source
Match Fields
Ingress A→B, TCP, Flag=ACK
Ingress A→B, TCP, Flag=ACK
Egress
A→B, TCP, Flag=FIN
Ingress B→A, TCP, Flag=FIN
-

Next State
ESTABLISHED
ESTABLISHED
DETECTED
DETECTED
ESTABLISHED

updates are vital for identifying overlaps between rules. All violating connections are to be deleted from the network by calling
the API DeleteEntry(). As a result, the associated OpenConnection
and flow entries will be flushed from the OpenConnection tables
and flow tables.

4.2 Connection Disruption Prevention
A malicious SDN application can manipulate existing flow entries or install new flow entries to disrupt active connections that
consequently damage the availability of services in the network.
To prevent this type of attack, detecting these attempts before they
take effect in the network is mandatory, so our firewall application
proactively analyzes the expected impact of updates on active connections. To this end, the application computes the Affected Entry
Set (AES) as described in Algorithm 1. When a new flow entry is
to be inserted into the network or an existing flow entry is about to
be updated, the application computes its dependencies with existing flow entries in the same switch. To this end, it first retrieves all
flow entries F E from a specific switch and computes affected flow
entries by new (or updated) flow entry nf . The application next
selects the exact flow table affected by nf and builds F Et which is
a subset of F E. Then, it compares the priority and matching conditions between e and nf , to decide whether e is affected. If nf is
dependent on e and has higher priority than e, the application adds
e into AES. If e has a goto instruction, the application further visits
the specified flow table to find AESchild . Considering Set-Field
actions e may have, the actions will be applied first in advance before pipelining to another flow table. The firewall makes use of
AES to detect the connection disruption attacks.
Detection of connection disruption attacks: Newly installed (or
updated) flow entry nf triggers the application to compute AES
and check AES against active connections obtained from S TATE M ON. The application then compares AES with σF and σR of each
of active connections and invokes the connection tracking module
′
to re-calculate σF′ and σR
. The updated σF′ may change the reσ′

′
′
lation between CI and CE i.e., CI −−F→ CE
. If CE 6= CE
, the
firewall concludes that the candidate flow entry nf will disrupt an
active connection. nf may also disrupt the reverse flow of the conσ′

−1
nection. If CE
−−R
→ CI′−1 and CI−1 6= CI′−1 , the firewall also
concludes nf will disrupt an active connection.
Countermeasure: When the controller receives the request of installation of a new flow entry nf which may cause a connection
disruption or interruption, S TATE M ON treats it as a candidate flow
entry and holds it until S TATE M ON evaluates its impact on the net′
work. Upon completion of computing AES and σF′ (or σR
), if the
′
firewall detects any error such as CE 6= CE
or CI−1 6= CI′−1 , it
raises an alarm to the administrator about the attempt. The administrator can decide whether it is legitimate and an intended request.
If it turns out nf is valid, S TATE M ON allows it to be installed in
the network. Otherwise, the firewall rejects the installation of nf .

4.3 Unauthorized Access Prevention
An attacker can attempt unauthorized access into an active connection by performing a man-in-the-middle attack such as TCP se-

Message Type
OC_ADD
OC_ADD

OpenConnection Events
Destination OC_CON_SIG Match Fields
Egress
A→B, TCP, Flag=FIN
Ingress
B→A, TCP, Flag=FIN

Timeout
5
5
1800
1800
0

quence inference attack to spoof packets. TCP protocol is inherently vulnerable to sequence inference attacks [33, 32]. We do not
fundamentally solve these known vulnerabilities but can partially
prevent specific types of unauthorized access to an active connection (e.g., TCP termination attacks). If an attacker successively
infers the sequence number of the next packet, he/she will be able
to create a spoofed termination packet by setting the TCP flags with
FIN (i.e., man-in-the middle attack [9]). Our firewall can leverage
S TATE M ON to detect such an attack by customizing the state management table and adding OpenConnection entries.
Detection of connection termination attacks: The key idea of
the detection mechanism is to add additional checking logic in the
egress switch for the forward flow (or the ingress switch for the
reverse flow) by installing new OpenConnection entries. In addition to the state management table described in Table 3, the firewall
adds additional transition rules (Table 5) to install OpenConnection entries and detect connection termination attacks. The firewall
first creates a new OpenConnection Events (the first line in Table 5)
for the SYNACK_SENT state that instructs the egress switch to install a new OpenConnection entry that matches the forward flow.
OC_CON_SIG match fields of this entry will match the TCP FIN
packet that belongs to the forward flow. Benign TCP FIN requests
sent from the initiating host will be checked at its ingress switch by
Table 3, so S TATE M ON transitions the state of the connection to the
ESTABLISHED state. Hence, OC_CON_SIG fields of the third entry in Table 5 will not match the packet. However, attacking packet
which is forged by an attacker in the middle of the flow path will
match the OC_CON_SIG conditions of the third entry at the egress
switch which results the state to be DETECTED. DETECTED state
defined in the fifth line in Table 5 is a temporary state that is used
to inform the existence of a TCP termination attack to the firewall.
In the case of the reverse flow, the firewall leverages the second
and the fourth entry for detecting connection termination attacks.
In such a way, the firewall can capture this type of attack with the
help of S TATE M ON.
Countermeasure: To protect the network from the aforementioned unauthorized access (e.g., TCP termination attack), the firewall can take two countermeasures: (1) return actions in the Type
III callback function with drop to drop the spoofed packet and
(2) rollback the connection state (DETECTED to ESTABLISHED)
to maintain the connectivity between end hosts. In addition, the
firewall may add complementary business logic in a Type III
callback function to implement post processing behaviors such as
sending warning messages to the network administrator.

5. IMPLEMENTATION AND EVALUATION
5.1 Implementation
To implement S TATE M ON, we chose a widely used controller,
Floodlight, and a reference OpenFlow software switch implementation, Open vSwitch (ovswitch). The routing module and link discovery modules in Floodlight are used to provide network topology information to the connection tracking module. To track existing flow entries in the network and build its reachability graph,
we used header space analysis [24] which translates each flow en-

5.2 Evaluation
To manage the state of a connection, existing solutions add the
transition logic of the connection in the data plane (Table 3). The
fundamental question, therefore, is how many additional messages
and/or performance overhead are introduced to achieve the same
goal in S TATE M ON. To this end, we conducted experiments using
three virtual machines, each of which had a quad-core CPU and
8GB memory and ran a Linux operating system (Ubuntu). One
virtual machine was used to run the Floodlight controller and each
of another ran Mininet [3] to simulate two networks. After we built
two separated networks, we connected them using a GRE tunnel to
flexibly add new hosts and links in one network without impacting
the other network. We also modified the size of the network by
changing the number of intermediaries (i.e., network switches).

5.2.1

S TATE M ON

To measure the worst-case performance of S TATE M ON, it was
configured to monitor every connection in the network. However,
in a real-world deployment, S TATE M ON only needs to monitor
connections specified by state-aware applications, which will only
improve the performance.
We first conducted experiments on an OpenConnection-enabled
switch to test the overhead created by S TATE M ON in the data plane.
OpenConnection enabled-switch spent less than 1µ for checking
the affiliation of incoming traffic in an OpenConnection table when
the table is set to have 100 entries. Creating and updating the corresponding entries in the OpenConnection table have been completed
within 2µs on average.
In the controller side, the connection tracking module is in charge
of installing/deleting an entry in the global state table and computing next state using the state management table. This module spent
less than 3µs on average to complete those two tasks when there

20
OpenConnection
OpenFlow

Messages per connection

18
16
14
12
10
8
6
4
2
0

VoIP

Bot

DoS

FTP

Web

(a) Messages per connection of each PCAP file.
4

3

x 10

OpenFlow
OpenConnection

2.5

Sent Messages

try into a transition function that consists of a set of binaries, 0,
1, and x (for wildcard), to represent its matching conditions and
actions. We also added OFPT_PACKET_IN listener within the
controller along with an OpenConnection message handler to receive the state changing packets and program OpenConnection tables. Each global state entry has a unique identifier to distinguish it
from other entries for ease of maintenance. The connection tracking module leverages the OFPT_FLOW_MOD OpenFlow message
to construct controller-to-switch OpenConnection messages.
In the data plane, we implemented the OpenConnection table
along with OpenConnection message handler. Because current versions of ovswitch can only support OpenFlow up to version 1.3.0,
which cannot inspect TCP flags and sequence/acknowledgment numbers, we implemented a parsing module to additionally retrieve
TCP flags and sequence/acknowledgment numbers. Then, we modified the legacy OpenFlow pipelining logic to enable OpenConnectionbased packet processing. In total, less than 500 lines of C code were
added to the ovswitch code base.
To implement the stateful firewall we leveraged a built-in firewall application in Floodlight to add a stateful checking module.
A stateful checking module in the firewall is able to access the
global state table by using S TATE M ON APIs for checking and enforcing its stateful firewall policy. We added the state parameter
to REST interface methods provided by the built-in firewall so that
users can define a stateful policy using REST requests. To prevent
connection disruption and unauthorized access, we added a listener
in the Static Flow Pusher module in Floodlight, so the application
is able to intercept potentially malicious or accidentally harmful
flow entry update requests and analyze their impacts on active connections before they become effective.

2

1.5

1

0.5

2

4

6

8

10

12

14

16

18

20

22

24

26

28

30

Number of Switches

(b) Message exchanges with different number of switches.
Figure 5: Message Exchanges in S TATE M ON
exist 100 connections in the network. To evaluate how much of the
delay can be attributed to network latency, we compared the numbers of message exchanges generated by both OpenFlow protocol
and OpenConnection protocol. We collected real network traffics
(five PCAP files) from different sources (available at [4, 7]) to generate real network traffic. Our testing framework (1) automatically
identifies source and destination IP addresses of each packet in a
PCAP file, (2) dynamically generates hosts for those IP addresses
in a network, and (3) sends the packet through their network interfaces. Figure 5(a) shows the number of message exchanges.
The first traffic is collected from VoIP traffic and consists of 32
connection attempts and 29 successful establishments. Network
traffic generated by this file caused the controller to generate 324
OpenFlow messages along with 215 OpenConnection messages,
which mean 10 OpenFlow messages and 7 OpenConnection Messages per connection on average. For counting OpenFlow messages, we excluded unrelated messages, such as OFPT_HELLO,
OFPT_ECHO_REQUEST, and FEATURE_ REPLY, and filtered out
unrelated OFPT_PACKET_IN messages used to handle connectionless packets, such as LLDP, ARP, and DNS. Therefore, OpenConnection protocol actually generated much fewer messages than
OpenFlow protocol. To account for theoretical number of OpenFlow messages, we develop the equation (1). For one way flow, we
need one OFPT_PACKET_IN message and n number of OFPT_
FLOW_MOD messages where n is the number of switches on the
path. Because a connection requires bi-directional flows, it is computed by 2 ∗ (1 + n).
BOF (n) = 2 ∗ (1 + n)

(1)

However, the number of OpenConnection messages does not depend on n. Because S TATE M ON requires eight messages for monitoring a connection, every PCAP type in Figure 5(a) creates ≤ 8

20

State−aware Policy Enforcement
Connection Disruption Prevention
Unauthorized Access Prevention

16

2.5

14
12
10
8
6
4
2
0

0

20

40

60

80

100

N−th Execution

Completion Time (milli sec)

Bandwidth(Gbits/sec)

3

Pure vSwitch w/ Floodlight
State−aware StateMon

18

2

1.5

1

Figure 6: Throughput between End Hosts
0.5

OpenConnection messages per connection. Considering the third
traffic that contains DoS attacks, it has generated a large number of
OpenFlow messages due to substantial connection attempts, while
the count of OpenConnection messages remained unchanged. This
results clearly show S TATE M ON creates minimal message exchanges
under any circumstances. Figure 5(b) shows how S TATE M ON scales
with respect to increasing the number of switches in the network.
To stress an overhead, we maintained 300 connections when measuring Figure 5(b). As expected, OpenFlow message count was linearly increased in accordance with the growing number of switches
while S TATE M ON maintains a constant number of message exchanges no matter how many switches exist in the network.
To discover overall overhead of S TATE M ON including network
latency, we first measured the time for establishing a connection
using a TCP handshake with and without S TATE M ON. As defined in Table 3, S TATE M ON exchanges 4 messages to monitor a
TCP handshake. While a TCP handshake took 3.356ms on average without S TATE M ON, it took 3.651ms on average with S TATE M ON. This means S TATE M ON only introduced a 0.295ms delay,
which is 8.79% overhead for a TCP handshake. To evaluate the
overall performance degradation caused by S TATE M ON, we used
the throughput between hosts as another metric. We used Iperf [2]
for this experiment. Iperf client (host in network A) initiated a new
connection with Iperf server (host in network B) and exchanged a
set of packets to measure the throughput. In an Open vSwitch and
Floodlight setting without S TATE M ON, the throughput scored an
average of 10.74 Gbits/sec (100 runs). With S TATE M ON enabled,
the throughput scored 10.40 Gbits/sec on average, with only 3.27%
throughput degradation.

5.2.2 Stateful Network Firewall
We configured the number of firewall policies to be 1k and fixed
the size of global state entries with 10k to measure the overhead of
our stateful firewall.
For performing state-aware firewall policy enforcement, the firewall spent 1.02ms on average. When a host attempts to establish a
new connection, it took 0.83ms to complete the searches with existing firewall policies, and the attempt was immediately denied in
real-time (0.01ms). Whenever a global state entry is updated, the
firewall performed a pair-wise comparison of the update with existing state-based rules within 1.16ms, and it took 0.26ms to delete
the violating connection from the network. In case of firewall policy updates, the firewall finished its dependency checking mostly
within 0.5ms, and spent a similar time (0.31ms) for deleting the
conflicting connection from the network.
Preventing connection disruptions in the network is another key
feature in our firewall. To this end, the firewall computes the Affected Entry Set (AES), and generating AES took less than 0.35ms
on average. In addition to AES, the firewall computes updated flow
′
′
′
entries, namely σF′ or σR
, to further compute CE and CI , respec′
tively. By comparing the relation the old CE and the updated CE ,

0

20k

40k

60k
80k
Number of connections

100K

Figure 7: Scalability Analysis of Stateful Firewall
the firewall draw a conclusion of potential connection disruption iff
′
CE 6= CE
. All these tasks were completed in 0.49ms on average.
To detect/prevent unauthorized access into active connections,
the firewall manipulates the state management table as described in
Section 4.3. As shown in Table 5, the firewall proactively installs
necessary rules in the state management table. Once a connection
has successively been established between two end hosts, the firewall asks S TATE M ON to install an additional OpenConnection entry to monitor the terminating packet at its egress switch. Since the
firewall will be directly notified by S TATE M ON when a connection
termination attack is detected, the firewall only implements a logic
to drop the attack packet. The firewall drops this packet and recovers the connection’s state to its previous one, ESTABLISHED.
Duration time for handling this type of unauthorized access took
around 0.44ms in total.
We also checked the scalability of the stateful firewall application by measuring the duration time for completing three types of
strategies. We gradually increased the number of existing connections from 20k to 100k. As shown in Figure 7, state-aware policy
enforcement took almost constant time (≈ 1ms) no matter how
many connections exist in the network. The firewall spent more
time in preventing connection disruptions than that of unauthorized
access prevention due to the computation overhead incurred by Algorithm 1. However, overall duration time for both cases linearly
increased with respect to increasing number of connections and
took less than 3 milliseconds at 100k connections, which is manageable.

5.2.3 Other Application: Port Knocking
Even though we mainly focused on TCP connection in this paper,
a key design goal is that S TATE M ON can support different statebased protocols, such as port knocking. Port knocking is a method
to open a closed port by checking a unique knock sequence, a series
of connection attempts destined to different ports [26]. Thus, we
developed this application to demonstrate how other network access management schemes can be also implemented using S TATE M ON in SDNs.
For example, an application may want to allow a connection iff
a series of requests matches a specific port order of A, B, C, and
D. By modifying the state management table in S TATE M ON, the
application can receive state-changing packets by listening OFPT_
PACKET_IN messages. In other words, the initial state can transition to the first knock state (e.g., PORT_KNOCK1) when the packet
is destined to port A, waiting for the subsequent knocking sequence
(port B). Such a way, the application opens the closed port of a
server if the state becomes the OPEN state.

To evaluate the overhead incurred by S TATE M ON-based application, we re-implemented the port knocking that has been demonstrated in prior work [26], which performs the same functions but
locally maintains the state in the switch. We installed the state
transition rules for the port knocking in the switch. To complete
the knocking sequence, it took 104.96ms without S TATE M ON,
and S TATE M ON-based application spent 113.83ms in total (8.45%
overhead).

6.

RELATED WORK

As explained in Table 3, majority of existing solutions are focused on performing stateful inspection in the data plane [29, 30,
14, 36, 11, 6, 10, 37]. There is some debate as to whether this
design goes against the spirit of SDN’s control and data plane separation. In addition, none of these approaches give much attention on how to leverage the logically centralized controller for providing a global state visibility of the network to applications. In
contrast, the unique contribution of S TATE M ON comes from its
consolidated state checking mechanism enabled by OpenConnection protocol and the connection tracking module. Specifically,
S TATE M ON can provide global state-based connection information to SDN applications along with several APIs that allows them
to define application-specific states. Even though OpenNF [16] attempts to achieve a similar state sharing, it mainly collects a state of
middleboxes (e.g., firewall, proxy, and load-balancer), not generic
network states.
A number of verification tools [31, 21, 27, 25, 23, 24] for checking network invariants and policy correctness in SDNs have been
recently proposed. FortNOX [31] was proposed as a software extension to provide security constraint enforcement for OpenFlowbased controllers. However, the conflict detection algorithm provided by FortNOX is incapable of analyzing stateful security policies. FlowGuard [21] was recently introduced to facilitate not only
an accurate detection but also a flexible resolution of firewall policy violations in dynamic OpenFlow-based networks. However, the
design of FlowGuard fully relies on flow-based rules in the data
plane and is only capable of building a stateless firewall application for SDNs. Anteater [27] is indeed an offline system and cannot be applied for a real-time flow tracking. VeriFlow [25] and
NetPlumber [23] are able to check the compliance of network updates with specified invariants in real time. VeriFlow uses graph
search techniques to verify network-wide invariants and deals with
dynamic changes. NetPlumber utilizes Header Space Analysis [24]
in an incremental manner to ensure real-time response for checking
network policies through building a dependency graph. Nevertheless, none of those tools are capable of checking stateful network
properties in SDNs.

7.

DISCUSSIONS

The OpenFlow protocol is evolving continuously, and the latest
version (v1.5.0) has been recently released [8]. The newest version
of OpenFlow attempts to add TCP flags for the extended matching
criteria to address the problem of insufficient L4 header inspection
capability as we have discussed.
However, the newest version of OpenFlow could not answer critical questions related to the maintenance and manipulation of network connection states. Especially, it does not articulate how to
leverage TCP flags to monitor states in both the switch and controller. We expect that our design of OpenConnection in S TATE M ON could provide an inspirational solution for OpenFlow to build
and enable its future stateful inspection scheme.

While we took great efforts to realize state-aware applications
for SDNs, the deployment of S TATE M ON to real-world production
networks requires additional considerations in terms of network security. For example, defense mechanisms against DDoS attacks
discussed in [35] may need to be considered in S TATE M ON. In addition, the current design and implementation of S TATE M ON utilize OpenFlow-based controller and switch modules, hence it only
works in the context of an OpenFlow-based environment. However,
the main idea of S TATE M ON, which is to provide state tracking
framework for various network applications, can be also realized in
other network paradigms, such as Network Function Virtualization
(NFV) [5, 19] .

8. CONCLUSION
In this paper, we have articulated network access control issues
in SDNs and presented a state-aware connection tracking framework called S TATE M ON that facilitates the control and data planes
of SDN to enable stateful inspection schemes. In the control plane,
we have designed a novel connection tracking mechanism using
a global state table and a state management table to track active
connections. To enable a state-aware data plane, we have introduced a new OpenConnection protocol, which defines four message formats and a state-aware OpenConnection table. We have implemented S TATE M ON using Floodlight and Open vSwitch along
with two access management applications (i.e., a stateful network
firewall application and a port knocking application) for SDNs, to
demonstrate the flexibility of S TATE M ON. Our experimental results have demonstrated that S TATE M ON and two state-aware network access management applications showed manageable performance overhead to enable critical state-aware protection of SDNs.

Acknowledgments
This work was partially supported by grants from National Science
Foundation (NSF-IIS-1527421, NSF-CNS-1537924 and NSF-CNS1531127), Intel corporation and Center for Cybersecurity and Digital Forensics at Arizona State University.

9. REFERENCES
[1] Floodlight: Open SDN Controller.
http://www.projectfloodlight.org.
[2] Iperf. https://iperf.fr/.
[3] Mininet: An Instant Virtual Network on Your Laptop.
http://mininet.org.
[4] Public PCAP Files for download.
http://www.netresec.com/?page=PcapFiles.
[5] Service Function Chaining (SFC) Architecture.
https://tools.ietf.org/html/draft-ietf-sfc-architecture-02.
[6] Stateful Connection Tracking & Stateful NAT.
http://openvswitch.org/support/ovscon2014/17/
1030-conntrack_nat.pdf.
[7] The Internet Traffic Archive. http://ita.ee.lbl.gov/.
[8] OpenFlow Switch Specification Version 1.5.1 (Protocol
version 0x06), December, 2014.
https://www.opennetworking.org/images/stories/downloads/
sdn-resources/onf-specifications/openflow/
openflow-switch-v1.5.1.pdf.
[9] K. Benton, L. J. Camp, and C. Small. Openflow vulnerability
assessment (poster). In Proceedings of ACM SIGCOMM
workshop on Hot topics in software defined networking
(HotSDN’13), pages 151–152. ACM, 2013.
[10] G. Bianchi, M. Bonola, A. Capone, and C. Cascone.
Openstate: programming platform-independent stateful

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

openflow applications inside the switch. ACM SIGCOMM
Computer Communication Review, 44(2):44–51, 2014.
P. Bosshart, D. Daly, G. Gibb, M. Izzard, N. McKeown,
J. Rexford, C. Schlesinger, D. Talayco, A. Vahdat,
G. Varghese, et al. P4: Programming protocol-independent
packet processors. ACM SIGCOMM Computer
Communication Review, 44(3):87–95, 2014.
M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. McKeown,
and S. Shenker. Ethane: Taking control of the enterprise. In
Proceedings of the ACM SIGCOMM 2007 conference. ACM,
2007.
M. Casado, T. Garfinkel, A. Akella, M. J. Freedman,
D. Boneh, N. McKeown, and S. Shenker. Sane: a protection
architecture for enterprise networks. In Proceedings of the
15th conference on USENIX Security Symposium. USENIX
Association, 2006.
S. Fayazbakhsh, V. Sekar, M. Yu, and J. Mogul. Flowtags:
Enforcing network-wide policies in the presence of dynamic
middlebox actions. In Proceedings of ACM SIGCOMM
Workshop on Hot Topics in Software Defined Networking
(HotSDN’13), August 2013.
S. K. Fayazbakhsh, L. Chiang, V. Sekar, M. Yu, and J. C.
Mogul. Enforcing network-wide policies in the presence of
dynamic middlebox actions using flowtags. In Proceedings
of the 11th USENIX Conference on Networked Systems
Design and Implementation, pages 533–546. USENIX
Association, 2014.
A. Gember-Jacobson, R. Viswanathan, C. Prakash,
R. Grandl, J. Khalid, S. Das, and A. Akella. Opennf:
Enabling innovation in network function control. In
Proceedings of the 2014 ACM Conference on SIGCOMM,
pages 163–174. ACM, 2014.
M. G. Gouda and A. X. Liu. A Model of Stateful Firewalls
and its Properties. In International Conference on
Dependable Systems and Networks (DSN), pages 128–137.
IEEE, 2005.
A. Greenberg, G. Hjalmtysson, D. A. Maltz, A. Myers,
J. Rexford, G. Xie, H. Yan, J. Zhan, and H. Zhang. A clean
slate 4d approach to network control and management. ACM
SIGCOMM Computer Communication Review, 35(5):41–54,
2005.
R. Guerzoni et al. Network functions virtualisation: an
introduction, benefits, enablers, challenges and call for
action, introductory white paper. In SDN and OpenFlow
World Congress, 2012.
D. Hartmeier and A. Systor. Design and Performance of the
OpenBSD Stateful Packet Filter (pf). In USENIX Annual
Technical Conference, FREENIX Track, pages 171–180,
2002.
H. Hu, W. Han, G.-J. Ahn, and Z. Zhao. Flowguard: building
robust firewalls for software-defined networks. In
Proceedings of ACM SIGCOMM Workshop on Hot Topics in
Software Defined Networking (HotSDN’14), pages 97–102.
ACM, 2014.
S. Jain, A. Kumar, S. Mandal, J. Ong, L. Poutievski,
A. Singh, S. Venkata, J. Wanderer, J. Zhou, M. Zhu, et al.
B4: Experience with a globally-deployed software defined
wan. In ACM SIGCOMM Computer Communication Review,
volume 43, pages 3–14. ACM, 2013.
P. Kazemian, M. Chang, H. Zeng, G. Varghese,
N. McKeown, and S. Whyte. Real time network policy
checking using header space analysis. In Proceedings of the

[24]

[25]

[26]
[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]
[35]

[36]

[37]

10th USENIX conference on Networked Systems Design and
Implementation, pages 99–112. USENIX Association, 2013.
P. Kazemian, G. Varghese, and N. McKeown. Header space
analysis: static checking for networks. In Proceedings of the
9th USENIX conference on Networked Systems Design and
Implementation. USENIX Association, 2012.
A. Khurshid, X. Zou, W. Zhou, M. Caesar, and P. B.
Godfrey. Veriflow: verifying network-wide invariants in real
time. In Proceedings of the 10th USENIX conference on
Networked Systems Design and Implementation, pages
15–28. USENIX Association, 2013.
M. Krzywinski. Port knocking from the inside out. SysAdmin
Magazine, 12(6):12–17, 2003.
H. Mai, A. Khurshid, R. Agarwal, M. Caesar, P. Godfrey, and
S. T. King. Debugging the data plane with anteater. In
Proceedings of the ACM SIGCOMM 2011 conference, pages
290–301, 2011.
N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar,
L. Peterson, J. Rexford, S. Shenker, and J. Turner. Openflow:
enabling innovation in campus networks. ACM SIGCOMM
Computer Communication Review, 38(2):69–74, 2008.
H. Mekky, F. Hao, S. Mukherjee, Z.-L. Zhang, and
T. Lakshman. Application-aware data plane processing in
sdn. In Proceedings of ACM SIGCOMM Workshop on Hot
Topics in Software Defined Networking (HotSDN’14), pages
13–18. ACM, 2014.
M. Moshref, A. Bhargava, A. Gupta, M. Yu, and
R. Govindan. Flow-level state transition as a new switch
primitive for sdn. In Proceedings of ACM SIGCOMM
Workshop on Hot Topics in Software Defined Networking
(HotSDN’14), pages 61–66. ACM, 2014.
P. Porras, S. Shin, V. Yegneswaran, M. Fong, M. Tyson, and
G. Gu. A security enforcement kernel for openflow networks.
In Proceedings of ACM SIGCOMM Workshop on Hot Topics
in Software Defined Networking (HotSDN’12), August 2012.
Z. Qian and Z. M. Mao. Off-path tcp sequence number
inference attack-how firewall middleboxes reduce security.
In Security and Privacy (SP), 2012 IEEE Symposium on,
pages 347–361. IEEE, 2012.
Z. Qian, Z. M. Mao, and Y. Xie. Collaborative tcp sequence
number inference attack: how to crack sequence number
under a second. In Proceedings of the 2012 ACM conference
on Computer and communications security, pages 593–604.
ACM, 2012.
C. Roeckl and C. M. Director. Stateful inspection firewalls.
Juniper Networks White Paper, 2004.
S. Shin, V. Yegneswaran, P. Porras, and G. Gu. Avant-guard:
scalable and vigilant switch flow management in
software-defined networks. In Proceedings of the 20th ACM
conference on Computer and communications security
(CCS’13), pages 413–424. ACM, 2013.
A. Wang, Y. Guo, F. Hao, T. Lakshman, and S. Chen. Umon:
Flexible and fine grained traffic monitoring in open vswitch.
In Proceedings of the 11th International Conference on
emerging Networking EXperiments and Technologies
(CoNEXT’15), December 2015.
S. Zhu, J. Bi, C. Sun, C. Wu, and H. Hu. Sdpa: Enhancing
stateful forwarding for software-defined networking. In
Proceedings of the 23rd IEEE International Conference on
Network Protocols (ICNP 2015), pages 10–13.

Position Paper: Towards a Moving Target Defense
Approach for Attribute-based Access Control
Carlos E. Rubio-Medrano, Josephine Lamp, Marthony Taguinod,
Adam Doupé, Ziming Zhao and Gail-Joon Ahn
Arizona State University

{crubiome, jalamp, mtaguino, doupe, zzhao30, gahn}@asu.edu
ABSTRACT
In recent years, attribute-based access control has been recognized as a convenient way to specify access mediation policies that leverage attributes originating from diﬀerent security domains, e.g., independently-run organizations or supporting platforms. However, this new paradigm, while allowing for enhanced ﬂexibility and convenience, may also open
the door to new kinds of attacks based on forging or impersonating attributes, thus potentially allowing for attackers
to gain unintended access to protected resources. In order
to alleviate this problem, we present an ongoing eﬀort based
on moving target defense, an emerging technique for proactively providing security measurements: we aim to analyze
attribute-based data obtained at runtime in order to dynamically change policy conﬁgurations over time. We present
our approach by leveraging a case study based in electronic
health records, another trending methodology widely used
in practice for mediating access to sensitive healthcare information in mission-critical applications.

Keywords
Attribute-based Access Control; Moving Target Defense; Electronic Health Records; Policy Mutation

1. INTRODUCTION
Recently, attribute-based access control (ABAC) [1], has
attracted the interest of both academia and industry as
a convenient means of protecting computer systems from
security-related incidents. As ABAC evolves into a mature paradigm and various implementations are successfully
deployed in practice, attributes originating from diﬀerent
sources may be leveraged for expressing rich policies that
better meet the speciﬁc needs of customized environments
[4]. Such a paradigm, while allowing for enhanced ﬂexibility and convenience, may also introduce non-trivial security
vulnerabilities. As an example, consider an ABAC policy
managed by an organization A that leverages attributes from
an outside independently-run organization B, in such a way
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

ABAC’16, March 11 2016, New Orleans, LA, USA
c 2016 ACM. ISBN 978-1-4503-4079-3/16/03. . . $15.00
⃝
DOI: http://dx.doi.org/10.1145/2875491.2875499

that end-users holding attributes issued by B can safely access the resources being shared by A. In such a setting, the
policy makers in A should somehow trust the way attributes
are created and assigned to end-users in the context of B.
However, as time evolves, such an assumption may not always hold in practice, as organization B may be the subject
of security incidents itself, e.g., hacking, or may not have a
strict control on the way its attributes are created and distributed. This would potentially allow for malicious third
parties to compromise them, for instance, by means of a
well-crafted forgery process.
In this paper, we describe an ongoing eﬀort to alleviate this problem by leveraging an approach inspired from
moving target defense (MTD) [2], a promising paradigm
based on the idea of proactively changing, e.g., moving, system conﬁgurations in an eﬀort to deter potential future attacks. In our approach, we aim to analyze attribute information collected from runtime traces of mission-critical applications, a.k.a., the attribute bag. Taking an ABAC policy
as an input, our approach ﬁrst obtains the list of original attributes listed in such policy and subsequently inspects the
attribute bag in an eﬀort to locate attributes that are correlated to the original ones. Later on, these newly-extracted
attributes are used to enhance the original policy, producing a new policy that is forwarded to the access mediation
infrastructure for enforcement. The intuition behind our
approach is that the entities involved in a given access request, e.g., end-users and protected resources, typically exhibit additional trusted attributes besides the ones listed
in the original policy. This way, if the original attributes
are compromised, the newly-extracted ones, which we assume stay uncompromised, may still deter the unintended
exploitation of the original policy. In addition, we aim to
mitigate the harm to usability, e.g., end-users no longer able
to access previously-available resources, by striving to obtain
a high degree of correlation between the original attributes
and the newly-extracted ones.
We present our approach in the context of an emerging
application domain: electronic health records (EHRs) [3],
which has also become the focus of many implementations
in practice as well as in research endeavors due to its notable beneﬁts of providing better quality of patient care. We
show how attributes belonging to both patients and healthcare providers, e.g., doctors and nurses, collected from both
EHRs and access logs can be leveraged to provide stronger
security guarantees by means of our approach, as it allows
for correlated attributes to be discovered and later used to
provide enhanced policies that can prevent future attacks,

Permissions

without aﬀecting the overall usability of a given EHR system.
This paper is organized as follows: we start by brieﬂy
reviewing some important background topics, along with a
running example and some other key considerations for our
approach in Section 2. Our proposal is later described in
Section 3, and we ﬁnalize this position paper by outlining
the status of our research as well as future work in Section 4.

Operators

Attributes

Attribute
Sources

Targets

Manage

Manage

2. BACKGROUND
Moving target defense (MTD) [2] is an emerging paradigm
for providing security guarantees by proactively changing,
e.g., moving, the conﬁgurations of a protected system. Opposed to traditional approaches, which assume security conﬁgurations remain immutable, MTD strives to reduce the
possibility of a successful attack by negating any advantages
the attacker may have. For instance, complicating the reconnaissance process in which an attacker gathers information about the current conﬁgurations of the victim system;
or by deterring ongoing attacks that were crafted based on
previously-discovered (and later changed) conﬁgurations. In
addition, eﬀects to the usability of the protected system, e.g.,
response time and end-user access patterns, should be minimized, in an eﬀort to prevent runtime inconveniences that
may complicate the adoption of MTD-based techniques.
Attribute-based access control (ABAC) [1] is also a trending technique for mediating access to sensitive resources
within a computer system. Fig. 1 presents a depiction of the
model considered for the purposes of this paper. Besides the
traditional sets of attributes, permissions and access entities
that have been previously discussed in the literature, our approach includes the concept of attribute sources and policy
makers. The former are in charge of deﬁning, creating and
assigning attributes to access entities, whereas the latter are
in charge of crafting policies by establishing a relationship
between attributes and permissions. Our model depicts an
open world scenario where attributes originating from diﬀerent sources may become relevant under the security domain
that is deﬁned by the policy makers. However, despite being
represented as diﬀerent sets in Fig. 1, the sets of attribute
sources and policy makers may not necessarily be mutually
exclusive in practice, as a policy maker may also play the
role of an attribute source when managing attributes deﬁned
within a certain security domain. In order to safely leverage
attributes, policy makers should somehow trust their corresponding sources. As an example, an attribute whose value
can be deliberately modiﬁed by the assigned entity may not
provide strong security guarantees, as such an entity may
be allowed to modify the attribute’s value at will to meet
the requirements deﬁned in a given policy. Therefore, policy makers should have conﬁdence on the attribute creation
and assignment process carried out by the sources. In our
model, we assume such attribute trust score can be modeled
as a binary value in the set {0,1}. This way, only attributes
depicting a trust score of 1 may be safely used for policy
crafting.
Electronic health records (EHRs) [3] increase the eﬃciency
of healthcare organizations by improving communication between clinicians and other health institutions leading to improved continuity and coordination of care. They also support clinician decision making by providing comprehensive
information about patients, thereby increasing quality of patient care. Complete and relevant information must be ac-

PA

AA

Access
Entities

Trust

Policy
Makers

Figure 1: A depiction of an ABAC model: attributes
are related to access entities (e.g., end-users and
protected resources) by means of the attribute assignment (AA) relation. Access rights (permissions)
are in turn related to access entities by the permission assignment (PA) relation. Policy makers are
in charge of establishing the PA relation by leveraging the attributes provided by the attribute sources,
who are in charge of managing the AA relation.
cessible to clinicians in a timely manner on a need-to-know
only basis within the set of privileges allowed by the patient, while unauthorized accesses to private data must be
prevented.
Table 1 shows an extract of attribute-based and access log
data depicting EHRs from diﬀerent users. As an example,
the entry depicted in the ﬁrst row shows an access request
to the EHR belonging to a user identiﬁed by the attribute
PatientID with a value of 11234. Such an access request
was denied, as shown by the value of the Decision attribute
set to False in the last column. Other attributes are shown
in Table 1 for illustrative purposes, and will be further discussed, along with their corresponding coloring scheme, in
Section 3.

3.

OUR APPROACH

As described in Section 1, we aim to develop an approach
based on MTD theory in such a way attacks to attributebased policies can be eﬀectively prevented. With this in
mind, we ﬁrst describe the attack model we take into consideration, followed by a description of our proposed approach and ﬁnalize with a short discussion on how our solution meets the goals for MTD as described in Section 2.

3.1

Attack Model and Assumptions

In this paper, we assume an attack model where the attributes listed in a given ABAC policy, e.g., in a policy rule
consisting of one or more constraints, become compromised
by an attacker, thus creating an unintended attribute-access
entity assignment, as depicted by the AA relation discussed
in Section 2. Diﬀerent ways an attacker may be able to compromise a given attribute may include, but may not be limited to the following: an unintended software error, forgery
or a hacking incident compromising the infrastructure where
attributes are created and assigned to entities, e.g., a remote
credential server.
In addition, we also assume the following: ﬁrst, the data
containing attribute-based information can be properly collected and is available for analysis. In the context of EHRs,
the application domain used for our running example, data
collection may include a preprocessing step in which data

Table 1: A sample set of attribute-based access log data depicting EHRs.
EHR ID Patient ID Patient Loc Personnel Loc
Role
Certification Decision
A11234
A11234
Surgery
General
Surgeon
MD
FALSE
A43452
A43452
ER
ER
Nurse
RA
TRUE
A83422
A83422
General
General
Physician
MD
TRUE
A56102
A56112
Pediatrics
Lab
Lab Tech
CLIA
FALSE
A23108
A23108
ER
ER
Physician
MD
TRUE
A76313
A77777
General
General
Nurse
RN
FALSE
A89736
A89736
Radiology
Radiology
Nurse
RA
TRUE
A24912
A24912
Surgery
Surgery
Surgeon
MD
TRUE
A87632
A87632
Radiology
Radiology
Physician
MD
TRUE
A34028
A34028
General
Pharmacy
Physician
MD
TRUE
End-Users

EId

PId

1

EHR
Data

EId

PId

Loc

3

2

Original
Policy

PLc

Mutation
Engine

Mutated
Policy

4

Access
Mediation
Module

Figure 2: A graphical depiction of our approach: the
original policy, defining a set of original attributes
(1), is fed to the mutation engine along with data
depicting an attribute bag (2). Such an engine identifies new attributes from users that are correlated
to the original ones (3), producing a mutated policy
that is later used for access mediation (4).
from the access logs is combined with information extracted
from EHRs themselves. As an example, Table 1 shows data
collected for each access request made in the context of an
EHR. For each request, the following items are shown: ﬁrst,
the resource being accessed, the result of evaluating the request, and a description of the attributes (included values)
shown at request time, a.k.a., the attribute bag. Second, we
assume that the software framework handling the speciﬁcation and runtime evaluation of ABAC policies, as well as
the software modules implementing our approach (including
the collection procedure described above), are out of reach
for an attacker. As an example, even when a given ABAC
policy, along with its listed attributes, may be known to the
attacker, he/she has no way to deliberately change its contents, either by removing the policy as a whole or by adding
or removing attribute-based rules at will.

3.2 Correlation-based Policy Mutation
A graphical depiction of our approach is shown in Fig. 2.
Initially, we model an ABAC policy P as a set of constraintbased rules R = {r1 , r2 , r3 , ..., rn } for some n > 0. For each
rule r ∈ R, we introduce the set Sr of attributes that are
listed in it. In addition, we also model the attribute bag as
described before as a set of attributes A such that A ∩ Sr
̸= ∅ for all r ∈ R.
Given the original policy P, our approach then aims to

produce a mutated policy P′ as follows: for each rule r ∈
R, we locate the set of attributes Cr = {c1 , c2 , ... cp } ⊆
A, Cr ̸= Sr , that are correlated to the set of attributes Sr .
Then, r may become a new rule r ′ by randomly choosing an
attribute c ∈ Cr such that Sr′ = Sr ∪ c 1 . Later, the set of
modiﬁed rules R′ = {r′ 1 , r′ 2 , ... r′ n } is combined together
to create the new mutated policy P′ . As shown in Fig. 2, P′
is forwarded to a policy evaluation module for further enforcement. We repeat the above procedure periodically in an
eﬀort to produce many diﬀerent policy mutations. For such
a purpose, an interactive approach may randomly produce
modiﬁed rules as shown above by selecting only a subset
of the set Cr of correlated attributes each time, in such a
way that the resulting rules may vary from time to time.
In addition, as time evolves, new correlated attributes may
be collected in the attribute bag, thus possibly producing
diﬀerent mutated policies as a result.
We reiterate that ﬁnding the set Cr of correlated attributes
is core to our approach. For such a purpose, we aim to ﬁnd
patterns relating the attributes in the attribute bag with
the ones contained in the set Sr of original attributes. For
illustrative purposes, assume a sample original policy based
on the data shown in Table 1, which contains a single rule
granting access to an EHR if the value of attribute EHR ID
is equal to the value depicted by the Patient ID one.
Our attribute correlation process can be then described
as follows: we start by ﬁrst ﬁnding the relationship between
the attributes in the original set Sr , e.g., EHR ID and Patient ID, and the access decision with a value of true, in
an eﬀort to identify within the data records depicting the
attribute bag, the ones that belong to the requested access
being granted according to our original policy. In Table 1,
such relation is represented by the cells colored in green.
Next, we strive to ﬁnd relationships between the original
set Sr (green), as identiﬁed by the previous step, and some
other attributes in the attribute bag. As an example, in
Table 1, the values of attributes Patient Loc and Personnel Loc are the same when the values of the attributes Patient ID, EHR ID are equal as well, and the access decision
depicts the value of true. Such a relationship is displayed
in Table 1 in the orange color. In order for this step to
be meaningful for our approach purposes, this relationship
should be as strong as possible, that is, the vast majority
of the records depicting the original attributes should also
depict the newly-correlated ones. Referring back to Table 1,

1

In case Cr = ∅, then r ′ = r.

the number of cells colored in green and the ones colored in
orange should be the same or stay within a close margin.
In a subsequent step, we also obtain the relationship between the original green attributes, the true access value,
and the inverse of the attributes depicted in orange obtained
from the previous step, e.g., the cells where the values for
the Patient Loc and Personnel Loc are not the same. Such a
relation is shown in the cells colored in purple in Table 1, and
represents the entities having legitimate access according to
the original policy but not holding the correlated attributes
depicted in orange. Following the intuition described for
the orange attributes, the number of cells colored in purple should be minimal with respect to the number of cells
in green and orange, as a large number would imply a potential impact to the usability of our approach, e.g., entities
getting previously-granted access denied as a consequence of
implementing our solution.
Next, we strive to identify the relationship between the
candidate orange attributes and the false access decision
value, in an eﬀort to make sure these newly-discovered correlated attributes are not shared by entities getting the false
access decision in the attribute bag data. The intuition behind this is that the orange attributes should only be assigned to the entities getting legitimate access according to
our original policy. Such a relationship is represented by
cells depicting the yellow coloring in Table 1. Ideally, the
number of cells in yellow should be minimal in respect to
the number of cells depicting the green and orange colorings, e.g., close to zero, as a large number of such yellow
cells would imply a potential security vulnerability.
With all this in mind, our approach should identify the
candidate orange attributes in such a way that their relation to the original ones (green) is strong, whereas the relation with both the yellow and purple ones is kept to a
minimum for safety and usability purposes, respectively. If
such conditions are met, the orange attributes are said to
depict the set Cr as described before, and can be then used
to create mutations of the original policy. Following our running example, the newly mutated policy may include a new
rule adding location of requirement of the values of Patient
Loc and Personnel Loc to be equal along with the previous
constraint relating the values of EHR ID and Patient ID.

3.3 Discussion
Following the discussion on MTD presented in Section 2,
our approach strives to reduce the probability of carrying
on a successful attack based on the model described in Section 3.1, by limiting the amount of time available for an
attacker to exploit a compromised attribute. For such a
purpose, we continuously mutate policies that leverage correlated attributes, such as the orange ones discussed above.
This way, even when an attacker may be able to compromise
an attribute in the original policy, the newly-correlated ones
may be able to deter the attack. Moreover, our approach is
also intended to avoid considerable impact to the usability of
the system being protected. As mentioned before, end-users
should not experience the rejection of previously-granted access requests as a result of the modiﬁcations made following
the MTD paradigm. We achieve this goal by calculating the
purple relation as describe above, and requiring it to be considerably less than the relation represented by the orange
one. Not enforcing such requirement may deviate in mutated policies that may reject previously-granted requests,

thus harming usability in a considerable way. Finally, even
when an attacker may be aware of our proposed approach,
we believe the continuous mutation of policies over time, as
described in Fig. 2, as well as by randomly selecting a subset
of orange attributes to appear on each mutation, may introduce a signiﬁcant level of deterrence against possible attacks,
e.g., predicting the next policy mutation. For such a purpose, we also assume the subset of orange attributes cannot
by compromised by an attacker, at least until the next policy mutation. We base such assumption on the fact that in
case an attacker can potentially modify any attribute at will
at any time (including both the green and orange ones), not
only our approach can be circumvented, but also the original
attribute-based policy (and any other policies) that may be
in place for access mediation purposes.

4.

CONCLUSIONS

In this position paper, we have presented an on-going approach for leveraging MTD in the context of attribute-based
policies. As of today, we are working towards reﬁning the
approach presented in Section 3. Concretely, we are formalizing our intuitions into a series of algorithms that leverage
well-established techniques such as association analysis [5].
In addition, we have started the codiﬁcation and evaluation
process on custom-designed synthetic data based on previous examples found in the literature. We also plan to expand such a process by incorporating data obtained from a
real-life EHR through a partnership with a healthcare organization. Finally, for the sake of eﬃciency, we plan to
perform diﬀerent performance measurements that include a
variety of attack scenarios.

Acknowledgments
This work was partially supported by a grant from the National Science Foundation (NSF-SFS-1129561), by a grant
from the Department of Energy (DE-SC0004308) and by a
grant from the Center for Cybersecurity and Digital Forensics at Arizona State University.

5.

REFERENCES

[1] V. C. Hu, D. Ferraiolo, R. Kuhn, A. Schnitzer,
K. Sandlin, R. Miller, and K. Scarfone. Guide to
attribute based access control (abac) deﬁnition and
considerations. NIST Special Publication, 800:162, 2014.
[2] S. Jajodia, A. K. Ghosh, V. Swarup, C. Wang, and
X. S. Wang. Moving target defense: creating
asymmetric uncertainty for cyber threats, volume 54.
Springer Science & Business Media, 2011.
[3] J. Jin, G.-J. Ahn, H. Hu, M. J. Covington, and
X. Zhang. Patient-centric authorization framework for
electronic healthcare services. Computers & Security,
30(2):116–127, 2011.
[4] C. E. Rubio-Medrano, Z. Zhao, A. Doupe, and G.-J.
Ahn. Federated access management for collaborative
network environments: Framework and case study. In
Proceedings of the 20th ACM Symposium on Access
Control Models and Technologies, SACMAT ’15, pages
125–134. ACM, 2015.
[5] R. Srikant and R. Agrawal. Mining quantitative
association rules in large relational tables. In ACM
SIGMOD Record, volume 25, pages 1–12. ACM, 1996.

Enemy of the State:
A State-Aware Black-Box Web Vulnerability Scanner
Adam Doupé, Ludovico Cavedon, Christopher Kruegel, and Giovanni Vigna
University of California, Santa Barbara
{adoupe, cavedon, chris, vigna}@cs.ucsb.edu

Abstract
Black-box web vulnerability scanners are a popular
choice for finding security vulnerabilities in web applications in an automated fashion. These tools operate in
a point-and-shoot manner, testing any web application—
regardless of the server-side language—for common security vulnerabilities. Unfortunately, black-box tools
suffer from a number of limitations, particularly when
interacting with complex applications that have multiple actions that can change the application’s state. If
a vulnerability analysis tool does not take into account
changes in the web application’s state, it might overlook
vulnerabilities or completely miss entire portions of the
web application.
We propose a novel way of inferring the web application’s internal state machine from the outside—that is, by
navigating through the web application, observing differences in output, and incrementally producing a model
representing the web application’s state.
We utilize the inferred state machine to drive a blackbox web application vulnerability scanner. Our scanner
traverses a web application’s state machine to find and
fuzz user-input vectors and discover security flaws. We
implemented our technique in a prototype crawler and
linked it to the fuzzing component from an open-source
web vulnerability scanner.
We show that our state-aware black-box web vulnerability scanner is able to not only exercise more code of
the web application, but also discover vulnerabilities that
other vulnerability scanners miss.

1 Introduction
Web applications are the most popular way of delivering
services via the Internet. A modern web application is
composed of a back-end, server-side part (often written
in Java or in interpreted languages such as PHP, Ruby,
or Python) running on the provider’s server, and a client

part running in the user’s web browser (implemented in
JavaScript and using HTML/CSS for presentation). The
two parts often communicate via HTTP over the Internet
using Asynchronous JavaScript and XML (AJAX) [20].
The complexity of modern web applications, along
with the many different technologies used in various abstraction layers, are the root cause of vulnerabilities in
web applications. In fact, the number of reported web
application vulnerabilities is growing sharply [18, 41].
The occurrence of vulnerabilities could be reduced
by better education of web developers, or by the use
of security-aware web application development frameworks [10, 38], which enforce separation between structure and content of input and output data. In both cases,
more effort and investment in training is required, and,
therefore, cost and time-to-market constraints will keep
pushing for the current fast-but-insecure development
model.
A complementary approach for fighting security vulnerabilities is to discover and patch bugs before malicious attackers find and exploit them. One way is to use
a white-box approach, employing static analysis of the
source code [4, 15, 17, 24, 28]. There are several drawbacks to a white-box approach. First, the potential applications that can be analyzed is reduced to only those
applications that use the target programming language.
In addition, there is the problem of substantial false positives. Finally, the source code of the application itself
may be unavailable.
The other approach to discovering security vulnerabilities in web applications is by observing the application’s output in response to a specific input. This method
of analysis is called black-box testing, as the application is seen as a sealed machine with unobservable internals. Black-box approaches are able to perform largescale analysis across a wide range of applications. While
black-box approaches usually have fewer false positives
than white-box approaches, black-box approaches suffer

view.php

index.php
S_0

login.php

index.php
login.php

S_1

view.php

index.php
Figure 2: State machine of a simple web application.

Figure 1: Navigation graph of a simple web application.

from a discoverability problem: They need to reach a
page to find vulnerabilities on that page.
Classical black-box web vulnerability scanners crawl
a web application to enumerate all reachable pages and
then fuzz the input data (URL parameters, form values,
cookies) to trigger vulnerabilities. However, this approach ignores a key aspect of modern web applications:
Any request can change the state of the web application.
In the most general case, the state of the web application is any data (database, filesystem, time) that the web
application uses to determine its output. Consider a forum that authenticates users, an e-commerce application
where users add items to a cart, or a blog where visitors
and administrators can leave comments. In all of these
modern applications, the way a user interacts with the
application determines the application’s state.
Because a black-box web vulnerability scanner will
never detect a vulnerability on a page that it does not
see, scanners that ignore a web application’s state will
only explore and test a (likely small) fraction of the web
application.
In this paper, we propose to improve the effectiveness
of black-box web vulnerability scanners by increasing
their capability to understand the web application’s internal state. Our tool constructs a partial model of the web
application’s state machine in a fully-automated fashion.
It then uses this model to fuzz the application in a stateaware manner, traversing more of the web application
and thus discovering more vulnerabilities.
The main contributions of this paper are the following:
• A black-box technique to automatically learn a
model of a web application’s state.
• A novel vulnerability analysis technique that leverages the web application’s state model to drive
fuzzing.
• An evaluation of our technique, showing that both
code coverage and effectiveness of vulnerability
analysis are improved.

2 Motivation
Crawling modern web applications means dealing with
the web application’s changing state. Previous work in
detecting workflow violations [5, 11, 17, 30] focused on
navigation, where a malicious user can access a page that
is intended only for administrators. This unauthorized
access is a violation of the developer’s intended workflow of the application.
We wish to distinguish a navigation-based view of the
web application, which is simply derived from crawling
the web application, from the web application’s internal
state machine. To illustrate this important difference, we
will use a small example.
Consider a simple web application that has only three
pages, index.php, login.php, and view.php. The
view.php page is only accessible after the login.php
page is accessed. There is no logout functionality. A
client accessing this web application might make a series
of requests like the following:
hindex.php, login.php, index.php, view.php,
index.php, view.phpi
Analyzing this series of requests from a navigation
perspective creates a navigation graph, shown in Figure 1. This graph shows which page is accessible from
every other page, based on the navigation trace. However, the navigation graph does not represent the information that view.php is only accessible after accessing
login.php, or that index.php has changed after requesting login.php (it includes the link to view.php).
What we are interested in is not how to navigate the
web application, but how the requests we make influence
the web application’s internal state machine. The simple web application described previously has the internal
state machine shown in Figure 2. The web application
starts with the internal state S 0. Arrows from a state
show how a request affects the web application’s internal state machine. In this example, in the initial state,
index.php does not change the state of the application,
however, login.php causes the state to transition from
S 0 to S 1. In the new state S 1, both index.php and
view.php do not change the state of the web application.
The state machine in Figure 2 contains important information about the web application. First, it shows that
login.php permanently changes the web application’s

state, and there is no way to recover from this change.
Second, it shows that the index.php page is seen in two
different states.
Now the question becomes: “How does knowledge of
the web application’s state machine (or lack thereof) affect a black-box web vulnerability scanner?” The scanner’s goal is to find vulnerabilities in the application, and
to do so it must fuzz as many execution paths of the
server-side code as possible1 . Consider the simple application described in Figure 2. In order to fuzz as many
code paths as possible, a black-box web vulnerability
scanner must fuzz the index.php page in both states S 0
and S 1, since the code execution of index.php can follow different code paths depending on the current state
(more precisely, in state S 1, index.php includes a link
to view.php, which is not present in S 0).
A black-box web vulnerability scanner can also use
the web application’s state machine to handle requests
that change state. For example, when fuzzing the
login.php page of the sample application, a fuzzer will
try to make several requests to the page, fuzzing different
parameters. However, if the first request to login.php
changes the state of the application, all further requests to
login.php will no longer execute along the same code
path as the first one. Thus, a scanner must have knowledge of the web application’s state machine to test if the
state was changed, and if it was, what requests to make
to return the application to the previous state before continuing the fuzzing process.
We have shown how a web application’s state machine
can be leveraged to improve a black-box web vulnerability scanner. Our goal is to infer, in a black-box manner,
as much of the web application’s state machine as possible. Using only the sequence of requests, along with the
responses to those requests, we build a model of as much
of the web application’s state machine as possible. In
the following section, we describe, at a high level, how
we infer the web application’s state machine. Then, in
Section 4, we provide the details of our technique.

3 State-Aware Crawling
In this section, we describe our state-aware crawling approach. In Section 3.1, we describe web applications and
define terms that we will use in the rest of the paper.
Then, in Section 3.2, we describe the various facets of
the state-aware crawling algorithm at a high level.
1 Hereinafter, we assume that the scanner relies on fuzzer-based
techniques. However, any other automated vulnerability analysis technique would benefit from our state-aware approach.

3.1 Web Applications
Before we can describe our approach to inferring a web
application’s state, we must first define the elements that
come into play in our web application model.
A web application consists of a server component,
which accepts HTTP requests. This server component
can be written in any language, and could use many
different means of storage (database, filesystem, memcache). After processing a request, the server sends back
a response. This response encapsulates some content,
typically HTML. The HTML content contains links and
forms which describe how to make further requests.
Now that we have described a web application at a
high level, we need to define specific terms related to
web applications that we use in the rest of this paper.
• Request—The HTTP request made to the web application. Includes anything (typically in the form
of HTTP headers) that is sent by the user to the web
application: the HTTP Method, URL, Parameters
(GET and POST), Cookies, and User-Agent.
• Response—The response sent by the server to the
user. Includes the HTTP Response Code and the
content (typically HTML).
• Page—The HTML page that is contained in the response from a web application.
• Link—Element of an HTML page that tells the
browser how to create a subsequent request. This
can be either an anchor or a form. An anchor always generates a GET request, but a form can generate either a POST or GET request, depending on the
parameters of the form.
• State—Anything that influences the web application’s server-side code execution.
3.1.1 Web Application Model
We use a symbolic Mealy machine [7] to model the web
application as a black-box. A Mealy machine is an automaton where the input to the automaton, along with
the current state, determines the output (i.e., the page
produced by the response) and the next state. A Mealy
machine operates on a finite alphabet of input and output symbols, while a symbolic Mealy machine uses an
infinite alphabet of input and output symbols.
This model of a web application works well because
the input to a web application, along with the current
state of the web application, determines the output and
the next state. Consider a simple e-commerce web application with the state machine show in Figure 3. In this
state graph, all requests except for the ones leaving a state

GET /logout.php
S_0

POST /login.php
GET /logout.php

no_items

POST /add_item.php
GET /delete_item.php

item_in_cart

S_1

POST /login.php
POST /purchase.php
purchased_item

GET /logout.php
POST /login.php

S_2

Figure 3: The state machine of a simple e-commerce application.
bring the application back to the same state. Therefore,
this state graph does not show all the request that can be
made to the application, only the subset of requests that
change the state.
For instance, in the initial state S 0, there is only
one request that will change the state of the application, namely POST /login.php. This change logs
the user into the web application. From the state
no items, there are two requests that can change the
state, GET /logout.php to return the user to state S 0
and POST /add item.php to add an item to the user’s
shopping cart.
Note that the graph shown in Figure 3 is not a
strongly connected graph—that is, every state cannot
be reached by every other state. In this example, purchasing an item is a permanent action, it irrecoverably
changes the state (there is no link from purchased item
to item in cart). Another interesting aspect is that
one request, GET /logout.php, leads to three different states. This is because once the web application’s
state has changed, logging out, and then back in, does
not change the state of the cart.

The intuition here is that a Mealy machine will, when
given the same input in the same state, produce the same
output. Therefore, if we send the same request and get a
different output, the state must have changed. By detecting the web application’s state changes only using inputs
and outputs, we are agnostic with respect to both what
constitutes the state information and where the state information is located. In this way, we are more generic
than approaches that only consider the database to hold
the state of the application, when in fact, the local file
system or even memory could hold part of the web application’s state.

3.2 Inferring the State Machine

Before we can cluster pages, we model them using the
links (anchors and forms) present on the page. The intuition here is that the links describe how the user can
interact with the web application. Therefore, changes to
what a user can do (new or missing links) indicate when
the state of the web application has changed. Also, infinite sections of a web application will share the same
link structure and will cluster together.

Inferring a web application’s state machine requires the
ability to detect when the state of the web application has
changed. Therefore, we start with a description of the
state-change detection algorithm, then explain the other
components that are required to infer the state machine.
The key insight of our state-change algorithm is the
following: We detect that the state of the web application
has changed when we make an identical request and get
a different response. This is the only externally visible
effect of a state-change: Providing the same input causes
a different output.
Using this insight, our state-change detection algorithm works, at a high level, as follows: (1) Crawl the
web application sequentially, making requests based on
a link in the previous response. (2) Assume that the state
stays the same, because there is no evidence to the contrary. (3) If we make a request identical to a previous
request and get a different response, then we assume that
some request since the last identical request changed the
state of the web application.

The state-change detection algorithm allows us to infer
when the web application’s state has changed, yet four
other techniques are necessary to infer a state machine:
the clustering of similar pages, the identification of statechanging requests, the collapsing of similar states, and
navigating.
Clustering similar pages. We want to group together
pages that are similar, for two reasons: To handle infinite
sections of web applications that are generated from the
same code (e.g., the pages of a calendar) and to detect
when a response has changed.

With our page model, we cluster pages together based
on their link structure. Pages that are in different clusters
are considered different. The details of this approach are
described in Section 4.1.
Determining the state-changing request. The statechange detection algorithm only says that the state has
changed, however we need to determine which request
actually changed the state. When we detect a state
change, we have a temporal list of requests with identical
requests at the start and end. One of the requests in this
list changed the state. We use a heuristic to determine
which request changed the state. This heuristic favors
newer requests over older requests, POST requests over
GET requests, and requests that have previously changed

the state over those that have never changed the state.
The details are described in Section 4.2.
Collapsing similar states. The state-change detection
algorithm detects only when the state has changed, however, we need to understand if we returned to a previous state. This is necessary because if we detect a state
change, we want to know if this is a state we have previously seen or a brand new state. We reduce this problem to a graph coloring problem, where the nodes are
the states and an edge between two nodes means that the
states cannot be the same. We add edges to this graph
by using the requests and responses, along with rules to
determine when two states cannot be the same. After the
graph is colored, states that are the same color are collapsed into the same state. Details of this state-merging
technique are provided in Section 4.3.
Navigating. We have two strategies for crawling the web
application.
First, we always try to pick a link in the last response.
The rational behind choosing a link in the last response
is that we emulate a user browsing the web application.
In this way, we are able to handle multi-step processes,
such as previewing a comment before it is committed.
Second, for each state, we make requests that are the
least likely to change the state of the web application.
The intuition here is that we want to first see as much of a
state as possible, without accidentally changing the state,
in case the state change is permanent. Full details of how
we crawl the web application are provided in Section 4.4

Page

/html/body/div/span/a

/html/body/div/form

/user

/post

profile.php

edit.php

(id, page)

(0)

(0, 1)

(all, sorted)

(5)

(NULL)

(text, email, id)

(5)

Figure 4: Representation of a page’s link vectors stored
in a prefix tree. There are five links present on this tree,
as evidenced by the number of leaf nodes.
our state-change detection algorithm, we are not interested in changes to the content, but rather to changes in
the navigation structure. We focus on navigation changes
because the links on a page define how a user can interact with the application, thus, when the links change, the
web application’s state has changed.
Therefore, we model a page by composing all the anchors and forms. First, every anchor and form is transformed into a vector constructed as follows:
hdompath, action, params, valuesi

4 Technical Details

where:
• dompath is the DOM (Document Object Model)
path of the HTML link (anchor or form);

Inferring a web application’s state machine requires concretely defining aspects such as page clustering or navigation. However, we wish to stress that this is one implementation of the state machine inference algorithm and
it may not be optimal.

• action is a list where each element is from the href
(for anchors) or action (for forms) attribute split
by ‘/’;

4.1 Clustering Similar Pages

• params is the (potentially empty) set of parameter
names of the form or anchor;

Our reason for grouping similar pages together is
twofold: Prevent infinite scanning of the website by
grouping the “infinite” areas together and detect when
the state has changed by comparing page responses in an
efficient manner.

• values is the set of values assigned to the parameters
listed in params.
For instance, an anchor tag with the href attribute of
/user/profile.php?id=0&page might have the following link vector:

4.1.1 Page Model

h/html/body/div/span/a, /user, profile.php, (id, page), (0)i

The output of a web application is usually an HTML
document (it can actually be any arbitrary content, but
we only consider HTML content and HTTP redirects).
An HTML page is composed of navigational information (anchors and forms) and user-readable content. For

All link vectors of a page are then stored in a prefix
tree. This prefix tree is the model of the page. A prefix
tree for a simple page with five links is shown in Figure 4. The link vector previously described is highlighted
in bold in Figure 4.

APT

(/html/body/div/span/a, /html/body/div/form)

REDIRECT

(/html/body/table/div/a)

(/user, /post)

/messages

(/comments)

(profile.php, edit.php)

show.php

(all.php)

((id, page), (all, sorted), (text, email, id))

(id)

((0), (0, 1), (5), (NULL), (5))

((5), (5, 3), (1), (YES), (10))

(1)

(sorted)

(NULL)

(ASC)

(DSC)

(RAND)

Figure 5: Abstract Page Tree. Every page’s link vector is stored in this prefix tree. There are seven pages in this tree.
The page link vector from Figure 4 is highlighted in bold.
HTTP redirects are handled as a special case, where
the only element is a special redirect element having the
target URL as the value of the location attribute.
4.1.2 Page Clustering
To cluster pages, we use a simple but efficient algorithm.
As described in the previous section, the model of a page
is a prefix tree representing all the links contained in the
page.
These prefix trees are translated into vectors, where
every element of this vector is the set of all nodes of a
given level of the prefix tree, starting from the root. At
this point, all pages are represented by a page link vector.
For example, Figure 4 has the following page link vector:
h(/html/body/div/span/a, /html/body/div/form),
(/user, /post),
(profile.php, edit.php),
((id, page), (all, sorted), (text, email, id)),
((0), (0, 1), (5), (NULL), (5))i
The page link vectors for all pages are then stored in
another prefix tree, called the Abstract Page Tree (APT).
In this way, pages are mapped to a leaf of the tree. Pages
which are mapped to the same leaf have identical page
link vectors and are considered to be the same page. Figure 5 shows an APT with seven pages. The page from
Figure 4 is bold in Figure 5.
However, we want to cluster together pages whose
page link vectors do not match exactly, but are similar
(e.g., shopping cart pages with a different number of elements in the cart). A measure of the similarity between
two pages is how many elements from the beginning of
their link vectors are the same between the two pages.
From the APT perspective, the higher the number of ancestors two pages (leaves) share, the closer they are.

Therefore, we create clusters of similar pages by selecting a node in the APT and merging into one cluster,
called an Abstract Page, all the leaves in the corresponding subtree. The criteria for deciding whether to cluster
a subtree of depth n from the root is the following:
• The number of leaves is greater than the median
number of leaves of all its siblings (including itself);
in this way, we cluster only subtrees which have a
larger-than-usual number of leaves.
• There are at least f (n) leaves in the subtree, where
f (n) is inversely related to n. The intuition is that
the fewer ancestors a subtree has in common (the
higher on the prefix tree it is), the more pages it must
have to cluster them together. We have found that
1
) works well by exthe function f (n) = 8(1 + n+1
perimental analysis on a large corpus of web pages.
• The pages share the same dompath and the first element of the action list of the page link vector; in this
way, all the pages that are clustered together share
the same link structure with potentially different parameters and values.

4.2 Determine the State-Changing Request
When a state change is detected, we must determine
which request actually changed the web application’s
state. Recall that we detect a state change when we make
a request that is identical to a previous request, yet has
different output. At this point, we have a list of all the
requests made between the latest request R and the request R′ closest in time to R such that R is identical to R′ .
We use a heuristic to determine which request in this list
changed the web application’s state, choosing the request
i between R′ and R which maximizes the function:
score(ni,transition , ni,seen , distancei )

where:
• ni,transition is the number of times the request caused
a state transition;
• ni,seen is the number of times the request has been
made;
• distancei is how many requests have been made between request R and request i.
The function score is defined as:
score(ni,transition , ni,seen , distancei ) =
n
+1 2
BOOSTi
1 − (1 − i,transition
ni,seen +1 ) + distancei +1
BOOSTi is .2 for POST requests and .1 for GET requests.
We construct the score function to capture two properties of web applications:
1. A POST request is more likely to change the state
than a GET request. This is suggested by the HTTP
specification, and score captures this intuition with
BOOSTi .
2. Resistant to errors. Because we cannot prove that
the selected request changed the state, we need to be
resistant to errors. That is why score contains the ratio of ni,transition to ni,seen . In this way, if we accidentally choose the wrong state-changing request once,
but then, later, make that request many times without changing the state, we are less likely to choose
it as a state-changing request.

4.3 Collapsing Similar States
Running the state detection algorithm on a series of requests and responses will tell us when the state has
changed. At this point, we consider each state unique.
This initial state assignment, though, is not optimal, because even if we encounter a state that we have seen in
the past, we are marking it as new. For example, in the
case of a sequence of login and logout actions, we are
actually flipping between two states, instead of entering
a new state at every login/logout. Therefore, we need
to minimize the number of different states and collapse
states that are actually the same.
The problem of state allocation can be seen as a graphcoloring problem on a non-planar graph [27]. Let each
state be a node in the graph G. Let two nodes a and b be
connected by an edge (meaning that the states cannot be
the same) if either of the following conditions holds:
1. If a request R was made when the web application
was in states a and b and results in pages in different
clusters. The intuition is that two states cannot be

the same if we make an identical request in each
state yet receive a different response.
2. The two states a and b have no pages in common.
The idea is to err on the conservative side, thus we
require that two states share a page before collapsing the states into one.
After adding the edges to the graph by following the
previous rules, G is colored. States assigned the same
color are considered the same state.
To color the nodes of G, we employ a custom greedy
algorithm. Every node has a unique identifier, which is
the incremental number of the state as we see it in the
request-response list. The nodes are ordered by identifier, and we assign the color to each node in a sequential
way, using the highest color available (i.e., not used by
its neighbors), or a new color if none is available.
This way of coloring the nodes works very well for
state allocation because it takes into account the temporal
locality of states: In particular, we attempt to assign the
highest available color because it is more likely for a state
to be the same as a recently seen state rather than one
seen at the beginning of crawling.
There is one final rule that we need to add after the
graph is colored. This rules captures an observation
about transitioning between states: If a request, R, transitions the web application from state a1 to state b, yet,
later when the web application is in state a2 , R transitions
the web application to state c, then a1 and a2 cannot be
the same state. Therefore, we add an edge from a1 to a2
and redo the graph coloring.
We continue enforcing this rule until no additional
edges are added. The algorithm is guaranteed to converge because only new edges are added at every step,
and no edges are ever removed.
At the end of the iteration, the graph coloring output
will determine the final state allocation—all nodes with
the same color represent the same state (even if seen at
different stages during the web application crawling process).

4.4 Navigating
Typical black-box web vulnerability scanners make concurrent HTTP requests to a web application to increase
performance. However, as we have shown, an HTTP
request can influence the web application’s state, and,
in this case, all other requests would occur in the new
state. Also, some actions require a multi-step, sequential
process, such as adding items to a shopping cart before
purchasing them. Finally, a user of the web application
does not browse a web application in this parallel fashion, thus, developers assume that the users will browse
sequentially.

def f u z z _ s t a t e _ c h a n g i n g ( f u z z _ r e q u e s t ) :
make_request( fuzz_request )
if s t a t e _ h a s _ c h a n g e d () :
if s t a t e _ i s _ r e v e r s i b l e () :
m a k e _ r e q u e s t s _ t o _ r e v e r t _ s t a t e ()
if not b a c k _ i n _ p r e v i o u s _ s t a t e () :
r e s e t _ a n d _ p u t _ i n _ p r e v i o u s _ s t a t e ()
else :
r e s e t _ a n d _ p u t _ i n _ p r e v i o u s _ s t a t e ()

Listing 1:
request.

Psuedocode for fuzzing state-changing

Our scanner navigates a web application by mimicking
a user browsing the web application sequentially. Browsing sequentially not only allows us to follow the developer’s intended path through the web application, but it
enables us to detect which requests changed the web application’s state.
Thus, a state-aware crawler must navigate the application sequentially. No concurrent requests are made, and
only anchors and forms present in the last visited page
are used to determine the next request. In the case of
a page with no outgoing links we go back to the initial
page.
Whenever the latest page does not contain unvisited
links, the crawler will choose a path from the current
page towards another page already seen that contains
links that have not yet been visited. If there is no path
from the current page to anywhere, we go back to the
initial page. The criteria for choosing this path is based
on the following intuitions:
• We want to explore as much of the current state as
possible before changing the state, therefore we select links that are less likely to cause a state transition.
• When going from the current page to a page with an
unvisited link, we will repeat requests. Therefore,
we should choose a path that contains links that we
have visited infrequently. This give us more information about the current state.
The exact algorithm we employ is Dijkstra Shortest
Path Algorithm [14] with custom edge length. This edge
length increases with the number of times we have previously visited that link. Finally, the edge length increases
with how likely the link is to cause a state change.

5 State-Aware Fuzzing
After we crawl the web application, our system has inferred, as much as possible, the web application’s state
machine. We use the state machine information, along
with the list of request–responses made by the crawler, to

drive a state-aware fuzzing of the web application, looking for security vulnerabilities.
To fuzz the application in a state-aware manner, we
need the ability to reset the web application to the initial
state (the state when we started crawling). We do not use
this ability when crawling, only when fuzzing. It is necessary to reset the application when we are fuzzing an
irreversible state-changing request. Using the reset functionality, we are able to recover from these irreversible
state changes.
Adding the ability to reset the web application does
not break the black-box model of the web application.
Resetting requires no knowledge of the web application,
and can be easily performed by running the web application in a virtual machine.
Our state-aware fuzzing starts by resetting the web application to the initial state. Then we go through the requests that the crawler made, starting with the initial request. If the request does not change the state, then we
fuzz the request as a typical black-box scanner. However,
if the request is state-changing, we follow the algorithm
shown in Listing 1. The algorithm is simple: We make
the request, and if the state has changed, traverse the inferred state machine to find a series of requests to transition the web application to the previous state. If this
does not exist, or does not work, then we reset the web
application to the initial state, and make all the previous requests that the crawler made. This ensures that the
web application is in the proper state before continuing
to fuzz.
Our state-aware fuzzing approach can use any fuzzing
component. In our implementation, we used the fuzzing
plugins of an open-source scanner, w3af [37]. The
fuzzing plugins take an HTTP request and generate variations on that request looking for different vulnerabilities. Our state-aware fuzzing makes those requests while
checking that the state does not unintentionally change.

6 Evaluation
As shown in previous research [16], fairly evaluating
black-box web vulnerability scanners is difficult. The
most important, at least to end users, metric for comparing black-box web vulnerability scanners is true vulnerabilities discovered. Comparing two scanners that discover different vulnerabilities is nearly impossible.
There are two other metrics that we use to evaluate
black-box web vulnerability scanners:
• False Positives. The number of spurious vulnerabilities that a black-box web vulnerability scanner
reports. This measures the accuracy of the scanner. False positives are a serious problem for the
end user of the scanner—if the false positives are

Application
Gallery
PhpBB v2
PhpBB v3
SCARF
Vanilla Forums
WackoPicko v2
WordPress v2
WordPress v3

Description
Photo hosting.
Discussion forum.
Discussion forum.
Stanford conference and research forum.
Discussion forum.
Intentionally vulnerable web application.
Blogging platform.
Blogging platform.

Version
3.0.2
2.0.4
3.0.10
2007-02-27
2.0.17.10
2.0
2.0
3.2.1

Lines of Code
26,622
16,034
110,186
798
43,880
900
17,995
71,698

Table 1: Applications that we ran the crawlers against to measure vulnerabilities discovered and code coverage.
high, the user must manually inspect each vulnerability reported to determine the validity. This requires a security-conscious user to evaluate the reports. Moreover, false positives erode the user’s
trust in the tool and make the user less likely to use
it in the future.
• Code Coverage. The percentage of the web application’s code that the black-box web vulnerability
scanner executes while it crawls and fuzzes the application. This measures how effective the scanner
is in exercising the functionality of the web application. Moreover, code coverage is an excellent metric for another reason: A black-box web vulnerability scanner, by nature, cannot find a vulnerability
along a code path that it does not execute. Therefore, greater code coverage means that a scanner
has the potential to discover more vulnerabilities.
Note that this is orthogonal to fuzzing capability:
A fuzzer—no matter how effective—will never be
able to discover a vulnerability on a code path that
it does not execute.
We use both the metrics previously described in our
evaluation. However, our main focus is on code coverage. This is because a scanner with greater code coverage will be able to discover more vulnerabilities in the
web application.
However, code coverage is not a perfect metric. Evaluating raw code coverage percentage numbers can be misleading. Ten percent code coverage of an application
could be horrible or excellent depending on how much
functionality the application exposes. Some code may
be intended only for installation, may be only for administrators, or is simply dead code and cannot be executed. Therefore, comparing code coverage normalized
to a baseline is more informative, and we use this in our
evaluation.

6.1 Experiments
We evaluated our approach by running our state-awarescanner along with three other vulnerability scanners

Scanner Description
wget
GNU command-line
website downloader.
w3af
Web Application Attack and Audit Framework.
skipfish Open-source,
highperformance vulnerability scanner.
state- Our state-aware vulaware- nerability scanner.
scanner

Language
C
Python

Version
1.12
1.0-stable

C

Python

2.03b

1.0

Table 2: Black-box web vulnerability scanners that we
compared.
against eight web applications. These web applications
range in size, complexity, and functionality. In the rest of
this section, we describe the web applications, the blackbox web vulnerability scanners, and the methodology we
used to validate our approach.
6.1.1 Web Applications
Table 1 provides an overview of the web applications
used with a short description, a version number, and lines
of executable PHP code for each application. Because
our approach assumes that the web application’s state
changes only via requests from the user, we made slight
code modifications to three web applications to reduce
the influence of external, non-user driven, forces, such as
time. Please refer to Appendix A for a detailed description of each application and what was changed.
6.1.2 Black-Box Web Vulnerability Scanners
This section describes the black-box web vulnerability
scanners that were compared against our approach, along
with the configuration or settings that were used. Table 2 contains a short description of each scanner, the
scanner’s programming language, and the version number. Appendix B shows the exact configuration that was
used for each scanner.

wget is a free and open-source application that is used
to download files from a web application. While not a
vulnerability scanner, wget is a crawler that will make
all possible GET requests it can find. Thus, it provides an
excellent baseline because vulnerability scanners make
POST requests as well as GET requests and should discover more of the application than wget.
wget is launched with the following options: recursive, download everything, and ignore robots.txt.
w3af is an open-source black-box web vulnerability
scanner which has numerous fuzzing modules. We enabled the blindSqli, eval, localFileInclude, osCommanding, remoteFileInclude, sqli, and xss fuzzing plugins.
skipfish is an open-source black-box web vulnerability
scanner whose focus is on high speed and high performance. Skipfish epitomizes the “shotgun” approach, and
boasts about making more than 2,000 requests per second to a web application on a LAN. Skipfish also attempts to guess, via a dictionary or brute-force, directory
names. We disabled this behavior to be fair to the other
scanners, because we do not want to test the ability to
guess a hidden directory, but how a scanner crawls a web
application.
state-aware-scanner is our state-aware black-box vulnerability scanner. We use HtmlUnit [19] to issue the
HTTP requests and render the HTML responses. After crawling and building the state-graph, we utilize the
fuzzing plugins from w3af to generate fuzzing requests.
Thus, any improvement in code coverage of our crawler
over w3af is due to our state-aware crawling, since the
fuzzing components are identical.
6.1.3 Methodology
We ran each black-box web vulnerability scanner against
a distinct, yet identical, copy of each web application.
We ran all tests on our local cloud [34].
Gallery, WordPress v2, and WordPress v3 do not require an account to interact with the website, thus each
scanner is simply told to scan the test application.
For the remaining applications (PhpBB v2, PhpBB v3,
SCARF, Vanilla Forums, and WackoPicko v2), it is difficult to fairly determine how much information to give the
scanners. Our approach only requires a username/password for the application, and by its nature will discover
the requests that log the user out, and recover from them.
However, other scanners do not have this capability.
Thus, it is reasonable to test all scanners with the same
level of information that we give our scanner. However,
the other scanners lack the ability to provide a username
and password. Therefore, we did the next best thing: For
those applications that require a user account, we log into
the application and save the cookie file. We then instruct

the scanner to use this cookie file while scanning the web
application.
While we could do more for the scanners, like preventing them from issuing the logout request for each application, we believe that our approach strikes a fair compromise and allows each scanner to decide how to crawl
the site. Preventing the scanners from logging out of the
application also limits the amount of the application they
will see, as they will never see the web application from
a guest’s perspective.

6.2 Results
Table 3 shows the results of each of the black-box web
vulnerability scanners against each web application. The
column “% over Baseline” displays the percentage of
code coverage improvement of the scanner against the
wget baseline, while the column “Vulnerabilities” shows
total number of reported vulnerabilities, true positives,
unique true positives among the scanners, and false positives.
The prototype implementation of our state-awarescanner had the best code coverage for every application.
This verifies the validity of our algorithm: Understanding state is necessary to better exercise a web application.
Figure 6 visually displays the code coverage percent
improvement over wget. The most important thing to
take from these results is the improvement state-awarescanner has over w3af. Because we use the fuzzing component of w3af, the only difference is in our state-aware
crawling. The results show that this gives state-awarescanner an increase in code coverage from as little as half
a percent to 140.71 percent.
Our crawler discovered three unique vulnerabilities,
one each in PhpBB v2, SCARF, and WackoPicko v2.
The SCARF vulnerability is simply a XSS injection on
the comment form. w3af logged itself out before fuzzing
the comment page. skipfish filed the vulnerable page under “Response varies randomly, skipping checks.” However, the content of this page does not vary randomly, it
varies because skipfish is altering it. This random categorization also prevents skipfish from detecting the simple XSS vulnerability on WackoPicko v2’s guestbook.
This result shows that a scanner needs to understand the
web application’s internal state to properly decide why a
page’s content is changing.
Skipfish was able to discover 15 vulnerabilities in
Vanilla Forums. This is impressive, however, 14 stem
from a XSS injection via the referer header on an error
page. Thus, even though these 14 vulnerabilities are on
different pages, it is the same root cause.
Surprisingly, our scanner produced less false positives
than w3af. All of w3af’s false positives were due to
faulty timing detection of SQL injection and OS com-

Scanner
state-aware-scanner
w3af
skipfish
wget
state-aware-scanner
skipfish
w3af
wget
state-aware-scanner
skipfish
w3af
wget
state-aware-scanner
skipfish
w3af
wget
state-aware-scanner
w3af
wget
skipfish
state-aware-scanner
skipfish
w3af
wget
state-aware-scanner
w3af
wget
skipfish
state-aware-scanner
w3af
skipfish
wget

Application
Gallery
Gallery
Gallery
Gallery
PhpBB v2
PhpBB v2
PhpBB v2
PhpBB v2
PhpBB v3
PhpBB v3
PhpBB v3
PhpBB v3
SCARF
SCARF
SCARF
SCARF
Vanilla Forums
Vanilla Forums
Vanilla Forums
Vanilla Forums
WackoPicko v2
WackoPicko v2
WackoPicko v2
WackoPicko v2
WordPress v2
WordPress v2
WordPress v2
WordPress v2
WordPress v3
WordPress v3
WordPress v3
WordPress v3

% over Baseline
16.20%
15.77%
10.96%
0%
38.34%
5.10%
1.04%
0%
115.45%
60.21%
16.16%
0%
67.03%
55.66%
21.55%
0%
30.89%
1.06%
0%
-2.32%
241.86%
194.77%
101.15%
0%
14.49%
12.49%
0%
-18.34%
9.84%
9.23%
3.89%
0%

Reported
0
3
0

Vulnerabilities
True Unique
0
0
0
0
0
0

False
0
3
0

4
3
5

3
2
1

1
0
0

1
1
4

0
2
0

0
0
0

0
0
0

0
2
0

1
0
0

1
0
0

1
0
0

0
0
0

0
0

0
0

0
0

0
0

17
5
4
5

15
5
3
5

2
1
1
1

2
0
1
0

0
0

0
0

0
0

0
0

1
0
3
1

0
0
0
0

0
0
0
0

1
0
3
1

Table 3: Results of each of the black-box web vulnerability scanners against each application. The table is sorted by
the percent increase in code coverage over the baseline scanner, wget.
manding. We believe that using HtmlUnit prevented
our scanner from detecting these spurious vulnerabilities, even though we use the same fuzzing component
as w3af.
Finally, our approach inferred the state machines of
the evaluated applications. These state machines are very
complex in the large applications. This complexity is
because modern, large, application have many actions
which modify the state. For instance, in WackoPicko v2,
a user can log in, add items to their cart, comment on
pictures, delete items from their cart, log out of the application, register as a new user, comment as this new user,
upload a picture, and purchase items. All of these actions interact to form a complex state machine. The state
machine our scanner inferred captures this complex series of state changes. The inferred WackoPicko v2 state
machine is presented in Figure 7.

7 Limitations
Although dynamic page generation via JavaScript is supported by our crawler as allowed by the HtmlUnit framework [19], proper AJAX support is not implemented.
This means that our prototype executes JavaScript when
the page loads, but does not execute AJAX calls when
clicking on links.
Nevertheless, our approach could be extended to handle AJAX requests. In fact, any interaction with the web
application always contains a request and response, however the content of the response is no longer an HTML
page. Thus, we could extend our notion of a “page” to
typical response content of AJAX calls, such as JSON or
XML. Another way to handle AJAX would be to follow
a Crawljax [33] approach and covert the dynamic AJAX
calls into static pages.

250%

state-aware-scanner
w3af
skipfish
210%
Percentage Improvement Over wget

230%

190%
120%
100%
80%
60%
40%
20%
0%
-20%

Ga

ller
y

Ph
p

BB
v

Ph
p
2

BB
v

3

SC
AR
F

Va
Wo
Wa
Wo
nil
cko
rd P
rd P
la F
Pic
r es
r es
o ru
s
sv
k
ov
v2
ms
3
2

Figure 6: Visual representation of the percentage increase of code coverage over the baseline scanner, wget. Important
to note is the gain our scanner, state-aware-scanner, has over w3af, because the only difference is our state-aware
crawling. The y-axis range is broken to reduce the distortion of the WackoPicko v2 results.
Another limitation of our approach is that our scanner
cannot be used against a web application being accessed
by other users (i.e., a public web application), because
the other users may influence the state of the application
(e.g., add a comment on a guestbook) and confuse our
state change detection algorithm.

8 Related Work
Automatic or semi-automatic web application vulnerability scanning has been a hot topic in research for many
years because of its relevance and its complexity.
Huang et al. developed a tool (WAVES) for assessing web application security with which we share many
points [24]. Similarly to us, they have a scanner for finding the entry points in the web application by mimicking
the behavior of a web browser. They employ a learning mechanism to sensibly fill web form fields and allow deep crawling of pages behind forms. Attempts to
discover vulnerabilities are carried out by submitting the
same form multiple times with valid, invalid, and faulty
inputs, and comparing the result pages. Differently from
WAVES, we are using the knowledge gathered by the
first-phase scanner to help the fuzzer detect the effect of
a given input. Moreover, our first-phase scanner aims not
only at finding relevant entry-points, but rather at building a complete state-aware navigational map of the web
application.

A number of tools have been developed to try to automatically discover vulnerabilities in web applications,
produced as academic prototypes [4,17,22,25,28,29,31],
as open-source projects [8,9,37], or as commercial products [1, 23, 26, 35].
Multiple projects [6,16,42,43] tackled the task of evaluating the effectiveness of popular black-box scanners
(in some cases also called point-and-shoot scanners).
The common theme in their results is a relevant discrepancy in vulnerabilities found across scanners, along with
low accuracy. Authors of these evaluations acknowledge
the difficulties and challenges of the task [21, 43]. In
particular, we highlighted how more deep crawling and
reverse engineering capabilities of web applications are
needed in black-box scanners, and we also developed the
WackoPicko web application which contains known vulnerabilities [16]. Similarly, Bau et al. investigated the
presence of room for research in this area, and found improvement is needed, in particular for detecting secondorder XSS and SQL injection attacks [6].
Reverse engineering of web applications has not been
widely explored in the literature, to our knowledge.
Some approaches [13] perform static analysis on the
code to create UML diagrams of the application.
Static analysis, in fact, is the technique mostly employed for automatic vulnerability detection, often combined with dynamic analysis.

385
POST /cart/action.php?action=purchase
397
GET /users/logout.php
400
POST /users/login.php
POST /users/register.php

POST /passcheck.php

325

POST /comments/add_comment.php

471
GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

GET /users/logout.php

POST /passcheck.php

417
GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

POST /users/login.php
POST /users/register.php

726

POST /users/login.php
POST /users/register.php

POST /passcheck.php

523

424

POST /comments/add_comment.php
GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

POST /users/login.php
POST /users/register.php
GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

543

POST /comments/add_comment.php

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

549

0

POST /users/login.php
POST /users/register.php

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

GET /users/logout.php

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

POST /cart/action.php?action=purchase

147

91

879

POST /comments/add_comment.php

GET /users/logout.php

794

1615
GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

POST /users/login.php
POST /users/register.php

POST /comments/add_comment.php

813

231

POST /comments/add_comment.php

350

POST /comments/add_comment.php

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

POST /comments/add_comment.php

261

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

1536

970

POST /cart/action.php?action=delete

290

GET /users/logout.php

POST /cart/action.php?action=purchase

POST /cart/action.php?action=delete

1641

874

POST /comments/add_comment.php

GET /users/logout.php

1725

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15
GET /users/logout.php

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15
POST /users/login.php
POST /users/register.php

904

POST /users/login.php
POST /users/register.php

GET /users/logout.php

907

POST /cart/action.php?action=purchase

POST /comments/add_comment.php

899

POST /comments/add_comment.php

1669

1735

POST /comments/add_comment.php

200

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

POST /users/login.php
POST /users/register.php

POST /passcheck.php

169

POST /users/login.php
POST /users/register.php

POST /comments/add_comment.php

POST /users/login.php
POST /users/register.php

POST /users/login.php
POST /users/register.php

POST /comments/add_comment.php

780

93

POST /passcheck.php GET /users/logout.php

POST /comments/add_comment.php

1055
GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

GET /users/logout.php

1157

1240

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

GET /users/logout.php

857

POST /comments/add_comment.php
GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

1328

POST /cart/action.php?action=purchase

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

1756

GET /users/logout.php

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

POST /comments/add_comment.php

1769

1389

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15
1782

GET /users/logout.php

POST /cart/action.php?action=delete

1248

POST /users/login.php
POST /users/register.php

884

GET /users/logout.php

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

1256

GET /cart/action.php?action=add&picid=7
GET /cart/action.php?action=add&picid=8
GET /cart/action.php?action=add&picid=9
GET /cart/action.php?action=add&picid=14
GET /cart/action.php?action=add&picid=15

POST /comments/add_comment.php

POST /users/login.php
POST /users/register.php

889

POST /comments/add_comment.php

894

Figure 7: State machine that state-aware-scanner inferred for WackoPicko v2.
Halfond et al. developed a traditional black-box vulnerability scanner, but improved its result by leveraging
a static analysis technique to better identify input vectors [22].
Pixy [28] employed static analysis with taint propagation in order to detect SQL injection, XSS and shell
command injection, while Saner [4] used sound static
analysis to detect failures in sanitization routines. Saner
also takes advantage of a second phase of dynamic analysis to reduce false positives. Similarly, WebSSARI [25]
also employed static analysis for detecting injection vulnerabilities, but, in addition, it proposed a technique for
runtime instrumentation of the web application through
the insertion of proper sanitization routines.

Felmetsger et al. investigated an approach for detecting a different type of vulnerability (some categories of
logic flaws) by combining execution traces and symbolic
model checking [17]. Similar approaches are also used
for generic bug finding (in fact, vulnerabilities could be
considered a subset of the general bug category). Csallner et al. employ dynamic traces for bug finding and for
dynamic verification of the alerts generated by the static
analysis phase [12]. Artzi et al., on the other hand, use
symbolic execution and model checking for finding general bugs in web applications [3].
On a completely separate track, efforts to improve web
application security push the developers toward writing
secure code in the first place. Security experts are ty-

ing to achieve this goal by either educating the developers [40] or designing frameworks which prohibit the
use of bad programming practices and enforce some security constraints in the code. Robertson and Vigna developed a strongly-typed framework which statically enforces separation between structure and content of a web
page, preventing XSS and SQL injection [38]. Also
Chong et al. designed their language for developers to
build web applications with strong confidentiality and
integrity guarantees, by means of compile-time and runtime checks [10].
Alternatively, consequences of vulnerabilities in web
applications can be mitigated by trying to prevent the
attacks before they reach some potentially vulnerable
code, like, for example, in the already mentioned WebSSARI [25] work. A different approach for blocking attacks is followed by Scott and Sharp, who developed a
language for specifying a security policy for the web application; a gateway will then enforce these policies [39].
Another interesting research track deals with the problem of how to explore web pages behind forms, also
called the hidden web [36]. McAllister et al. monitor
user interactions with a web application to collect sensible values for HTML form submission and generate
test cases that can be replayed to increase code coverage [32]. Although not targeted to security goals, the
work of Raghavan and Garcia-Molina is relevant for our
project for their contribution in classification of different
types of dynamic content and for their novel approach
for automatically filling forms by deducing the domain
of form fields [36]. Raghavan and Garcia-Molina carried out further research in this direction, by reconstructing complex and hierarchical query interfaces exposed
by web applications.
Moreover, Amalfitano et al. started tackling the problem of reverse engineering the state machine of clientside AJAX code, which will help in finding the web application server-side entry points and in better understating complex and hierarchical query interfaces [2].
Finally, we need to mention the work by Berg et al.
in reversing state machines into a Symbolic Mealy Machine (SMM) model [7]. Their approach for reversing
machines cannot be directly applied to our case because
of the infeasibility of fully exploring all pages for all the
states, even for a small subset of the possible states. Nevertheless, the model they propose for a SMM fits our
needs.

9 Conclusion
We have described a novel approach to inferring, as
much as possible, a web application’s internal state machine. We leveraged the state machine to drive the stateaware fuzzing of web applications. Using this approach,

our crawler is able to crawl—and thus fuzz—more of the
web application than a classical state-agnostic crawler.
We believe our approach to detecting state change by differences in output for an identical response is valid and
should be adopted by all black-box tools that wish to understand the web application’s internal state machine.

Acknowledgements
This work was supported by the Office of Naval Research
(ONR) under Grant N000141210165, the National Science Foundation (NSF) under grant CNS-1116967, and
by Secure Business Austria.

References
[1] A CUNETIX. Acunetix Web Vulnerbility Scanner. http://www.
acunetix.com/.
[2] A MALFITANO , D., FASOLINO , A., AND T RAMONTANA , P. Reverse Engineering Finite State Machines from Rich Internet Applications. In 2008 15th Working Conference on Reverse Engineering (2008), IEEE, pp. 69–73.
[3] A RTZI , S., K IEZUN , A., D OLBY, J., T IP, F., D IG , D., PARAD KAR , A., AND E RNST, M. Finding bugs in web applications
using dynamic test generation and explicit state model checking.
IEEE Transactions on Software Engineering (2010).
[4] BALZAROTTI , D., C OVA , M., F ELMETSGER , V., J OVANOVIC ,
N., K IRDA , E., K RUEGEL , C., AND V IGNA , G. Saner: Composing Static and Dynamic Analysis to Validate Sanitization in
Web Applications. In 2008 IEEE Symposium on Security and
Privacy (2008), IEEE, pp. 387–401.
[5] BALZAROTTI , D., C OVA , M., F ELMETSGER , V., AND V IGNA ,
G. Multi-module Vulnerability Analysis of Web-based Applications. In Proceedings of the ACM conference on Computer and
Communications Security (CCS) (2007), pp. 25–35.
[6] BAU , J., B URSZTEIN , E., G UPTA , D., AND M ITCHELL , J. State
of the Art: Automated Black-Box Web Application Vulnerability
Testing. In Security and Privacy (SP), 2010 IEEE Symposium on
(2010), IEEE, pp. 332–345.
[7] B ERG , T., J ONSSON , B., AND R AFFELT, H. Regular Inference
for State Machines using Domains with Equality Tests. In Proceedings of the Theory and practice of software, 11th international conference on Fundamental approaches to software engineering (2008), Springer-Verlag, pp. 317–331.
[8] B YRNE , D.
com/.

Grendel-Scan.

[9] C HINOTEC T ECHNOLOGIES .
parosproxy.org/.

http://www.grendel-scan.
Paros.

http://www.

[10] C HONG , S., V IKRAM , K., AND M YERS , A. SIF: Enforcing confidentiality and integrity in web applications. In Proceedings of
16th USENIX Security Symposium on USENIX Security Symposium (2007), USENIX Association, p. 1.
[11] C OVA , M., BALZAROTTI , D., F ELMETSGER , V., AND V IGNA ,
G. Swaddler: An Approach for the Anomaly-based Detection of
State Violations in Web Applications. In Proceedings of the International Symposium on Recent Advances in Intrusion Detection
(RAID 2007) (2007), pp. 63–86.
[12] C SALLNER , C., S MARAGDAKIS , Y., AND X IE , T. DSDCrasher: A hybrid analysis tool for bug finding. ACM Transactions on Software Engineering and Methodology (TOSEM) 17,
2 (2008), 1–37.

[13] D I L UCCA , G., FASOLINO , A., PACE , F., T RAMONTANA , P.,
AND D E C ARLINI , U. WARE: a tool for the reverse engineering of Web applications. In Sixth European Conference
on Software Maintenance and Reengineering, 2002. Proceedings
(2002), pp. 241–250.
[14] D IJKSTRA , E. W. A Note on Two Problems in Connexion with
Graphs. Numerische Mathematik 1 (1959), 269–271.
[15] D OUP É , A., B OE , B., K RUEGEL , C., AND V IGNA , G. Fear
the EAR: Discovering and Mitigating Execution After Redirect
Vulnerabilities. In Proceedings of the 18th ACM Conference on
Computer and Communications Security (CCS 2011) (Chicago,
IL, October 2011).
[16] D OUP É , A., C OVA , M., AND V IGNA , G. Why Johnny Can’t
Pentest: An Analysis of Black-box Web Vulnerability Scanners.
In Detection of Intrusions and Malware, and Vulnerability Assessment (DIMVA 2010) (2010), Springer, pp. 111–131.
[17] F ELMETSGER , V., C AVEDON , L., K RUEGEL , C., AND V IGNA ,
G. Toward Automated Detection of Logic Vulnerabilities in Web
Applications. In Proceedings of the USENIX Security Symposium
(Washington, DC, August 2010).
[18] F OSSI , M. Symantec Global Internet Security Threat Report.
Tech. rep., Symantec, April 2009. Volume XIV.
[19] G ARGOYLE S OFTWARE I NC . HtmlUnit. http://htmlunit.
sourceforge.net/.
[20] G ARRETT, J. J. Ajax: A New Approach to Web Applications. http://www.adaptivepath.com/ideas/essays/
archives/000385.php, Feb. 2005.
[21] G ROSSMAN , J. Challenges of Automated Web Application Scanning. Blackhat Windows 2004, 2004.
[22] H ALFOND , W., C HOUDHARY, S., AND O RSO , A. Penetration
testing with improved input vector identification. In Software
Testing Verification and Validation, 2009. ICST’09. International
Conference on (2009), IEEE, pp. 346–355.

[31] L I , X., YAN , W., AND X UE , Y. SENTINEL: Securing Database
from Logic Flaws in Web Applications. In CODASPY (2012),
pp. 25–36.
[32] M C A LLISTER , S., K IRDA , E., AND K RUEGEL , C. Leveraging User Interactions for In-Depth Testing of Web Applications.
In Recent Advances in Intrusion Detection (2008), Springer,
pp. 191–210.
[33] M ESBAH , A., B OZDAG , E., AND VAN D EURSEN , A. Crawling
AJAX by Inferring User Interface State Changes. In Web Engineering, 2008. ICWE ’08. Eighth International Conference on
(july 2008), pp. 122 –134.
[34] N URMI , D., W OLSKI , R., G RZEGORCZYK , C., O BERTELLI ,
G., S OMAN , S., Y OUSEFF , L., AND Z AGORODNOV, D. The
Eucalyptus Open-Source Cloud-Computing System. In Cluster
Computing and the Grid, 2009. CCGRID ’09. 9th IEEE/ACM International Symposium on (may 2009), pp. 124 –131.
[35] P ORT S WIGGER. Burp Proxy. http://www.portswigger.
net/burp/.
[36] R AGHAVAN , S., AND G ARCIA -M OLINA , H. Crawling the hidden web. In Proceedings of the International Conference on Very
Large Data Bases (2001), Citeseer, pp. 129–138.
[37] R IANCHO , A. w3af – Web Application Attack and Audit Framework. http://w3af.sourceforge.net/.
[38] ROBERTSON , W., AND V IGNA , G. Static Enforcement of
Web Application Integrity Through Strong Typing. In Proceedings of the USENIX Security Symposium (Montreal, Quebec CA,
September 2009).
[39] S COTT, D., AND S HARP, R. Abstracting application-level web
security. In Proceedings of the 11th international conference on
World Wide Web (2002), ACM, pp. 396–407.
[40] SPI DYNAMICS . Complete Web Application Security: Phase
1 – Building Web Application Security into Your Development
Process. SPI Dynamics Whitepaper, 2002.

[23] HP. WebInspect. https://download.hpsmartupdate.com/
webinspect/.

[41] S TEVE , C., AND M ARTIN , R. Vulnerability Type Distributions
in CVE. Mitre report, May (2007).

[24] H UANG , Y.-W., H UANG , S.-K., L IN , T.-P., AND T SAI , C.-H.
Web application security assessment by fault injection and behavior monitoring. In Proceedings of the 12th international conference on World Wide Web (New York, NY, USA, 2003), WWW
’03, ACM, pp. 148–159.

[42] S UTO , L. Analyzing the Accuracy and Time Costs of Web Application Security Scanners, 2010.

[25] H UANG , Y.-W., Y U , F., H ANG , C., T SAI , C.-H., L EE , D.T., AND K UO , S.-Y. Securing web application code by static
analysis and runtime protection. In WWW ’04: Proceedings of
the 13th international conference on World Wide Web (New York,
NY, USA, 2004), ACM, pp. 40–52.
[26] IBM.
AppScan.
http://www-01.ibm.com/software/
awdtools/appscan/.
[27] J ENSEN , T. R., AND T OFT, B. Graph Coloring Problems.
Wiley-Interscience Series on Discrete Mathematics and Optimization. Wiley, 1994.
[28] J OVANOVIC , N., K RUEGEL , C., AND K IRDA , E. Static analysis for detecting taint-style vulnerabilities in web applications.
Journal of Computer Security 18, 5 (2010), 861–907.
[29] K ALS , S., K IRDA , E., K RUEGEL , C., AND J OVANOVIC , N.
Secubat: a Web Vulnerability Scanner. In Proceedings of the
15th international conference on World Wide Web (2006), ACM,
pp. 247–256.
[30] L I , X., AND X UE , Y. BLOCK: A Black-box Approach for Detection of State Violation Attacks Towards Web Applications. In
Proceedings of the Annual Computer Security Applications Conference (ACSAC 2011) (Orlando, FL, December 2011).

[43] V IEIRA , M., A NTUNES , N., AND M ADEIRA , H. Using Web
Security Scanners to Detect Vulnerabilities in Web Services. In
Dependable Systems & Networks, 2009. DSN’09. IEEE/IFIP International Conference on (2009), IEEE, pp. 566–571.

A

Web Applications

This section describes the web applications along with
the functionality against which we ran the black-box web
vulnerability scanner.
Gallery is an open-source photo hosting application.
The administrator can upload photos and organize them
into albums. Guests can then view and comment on
the uploaded photos. Gallery has AJAX functionality but gracefully degrades (is fully functional) without
JavaScript. No modifications were made to the application.
PhpBB v2 is an open-source forum software. It allows
registered users to perform many actions such as create new threads, comment on threads, and message other
users. Version 2 is notorious for the amount of security
vulnerabilities it contains [6], and we included it for this
reason. We modified it to remove the “recently online”
section on pages, because this section is based on time.
PhpBB v3 is the latest version of the popular opensource forum software. It is a complete rewrite from
Version 2, but retains much of the same functionality.
Similar to PhpBB v2, we removed the “recently online”
section, because it is time-based.
SCARF, the Stanford Conference And Research Forum, is an open-source conference management system. The administrator can upload papers, and registered users can comment on the uploaded papers. We
included this application because it was used by previous research [5, 11, 30, 31]. No modifications were made
to this application.
Vanilla Forums is an open-source forum software similar in functionality to PhpBB. Registered users can create new threads, comment on threads, bookmark interesting threads, and send a message to another user.
Vanilla Forums is unique in our test set in that it uses
the path to pass parameters in a URL, whereas all other
applications pass parameters using the query part of
the URL. For instance, a specific user’s profile is GET
/profile/scanner1, while a discussion thread is located at GET /discussion/1/how-to-scan. Vanilla
Forums also makes extensive use of AJAX, and does not
gracefully degrade without JavaScript. For instance, with
JavaScript disabled, posting a comment returns a JSON
object that contains the success or failure of the comment posting, instead of an HTML response. We modified Vanilla Forums by setting an XSRF token that it used
to a constant value.
WackoPicko v2 is an open-source intentionally vulnerable web application which was originally created to evaluate many black-box web vulnerability scanners [16]. A
registered user can upload pictures, comment on other
user’s pictures, and purchase another user’s picture. Ver-

sion 2 contains minor tweaks from the original paper, but
no additional functionality.
WordPress v2 is an open-source blogging platform. An
administrator can create blog posts, where guests can
leave comments. No changes were made to this application.
WordPress v3 is an up-to-date version of the opensource blogging platform. Just like the previous version,
administrators can create blog posts, while a guest can
comment on blog posts. No changes were made to this
application.

B Scanner Configuration
The following describes the exact settings that were used
to run each of the evaluated scanners.
• wget is run in the following way:
wget -rp -w 0 --waitretry=0 -nd
--delete-after --execute robots=off
• w3af settings:
misc-settings
set maxThreads 0
back
plugins
discovery webSpider
audit blindSqli, eval,
localFileInclude, osCommanding,
remoteFileInclude, sqli, xss
• skipfish is run in the following way:
skipfish -u -LV -W /dev/null -m 10

Why Johnny Can’t Pentest:
An Analysis of Black-box Web Vulnerability Scanners
Adam Doupé, Marco Cova, and Giovanni Vigna
University of California, Santa Barbara
{adoupe,marco,vigna}@cs.ucsb.edu

Abstract. Black-box web vulnerability scanners are a class of tools that can be
used to identify security issues in web applications. These tools are often marketed as “point-and-click pentesting” tools that automatically evaluate the security of web applications with little or no human support. These tools access a web
application in the same way users do, and, therefore, have the advantage of being
independent of the particular technology used to implement the web application.
However, these tools need to be able to access and test the application’s various
components, which are often hidden behind forms, JavaScript-generated links,
and Flash applications.
This paper presents an evaluation of eleven black-box web vulnerability scanners,
both commercial and open-source. The evaluation composes different types of
vulnerabilities with different challenges to the crawling capabilities of the tools.
These tests are integrated in a realistic web application. The results of the evaluation show that crawling is a task that is as critical and challenging to the overall
ability to detect vulnerabilities as the vulnerability detection techniques themselves, and that many classes of vulnerabilities are completely overlooked by
these tools, and thus research is required to improve the automated detection of
these flaws.

1 Introduction
Web application vulnerabilities, such as cross-site scripting and SQL injection, are one
of the most pressing security problems on the Internet today. In fact, web application
vulnerabilities are widespread, accounting for the majority of the vulnerabilities reported in the Common Vulnerabilities and Exposures database [4]; they are frequent
targets of automated attacks [20]; and, if exploited successfully, they enable serious attacks, such as data breaches [9] and drive-by-download attacks [17]. In this scenario,
security testing of web applications is clearly essential.
A common approach to the security testing of web applications consists of using
black-box web vulnerability scanners. These are tools that crawl a web application to
enumerate all the reachable pages and the associated input vectors (e.g., HTML form
fields and HTTP GET parameters), generate specially-crafted input values that are submitted to the application, and observe the application’s behavior (e.g., its HTTP responses) to determine if a vulnerability has been triggered.
Web application scanners have gained popularity, due to their independence from
the specific web application’s technology, ease of use, and high level of automation.
(In fact, web application scanners are often marketed as “point-and-click” pentesting

tools.) In the past few years, they have also become a requirement in several standards,
most notably, in the Payment Card Industry Data Security Standard [15].
Nevertheless, web application scanners have limitations. Primarily, as most testing
tools, they provide no guarantee of soundness. Indeed, in the last few years, several
reports have shown that state-of-the-art web application scanners fail to detect a significant number of vulnerabilities in test applications [1, 16, 21, 22, 24]. These reports
are valuable, as they warn against the naive use of web application scanners (and the
false sense of security that derives from it), enable more informed buying decisions,
and prompt to rethink security compliance standards.
However, knowing that web application scanners miss vulnerabilities (or that, conversely, they may raise false alerts) is only part of the question. Understanding why
these tools have poor detection performance is critical to gain insights into how current
tools work and to identify open problems that require further research. More concretely,
we seek to determine the root causes of the errors that web application scanners make,
by considering all the phases of their testing cycle, from crawling, to input selection,
to response analysis. For example, some of the questions that we want to answer are:
Do web application scanners correctly handle JavaScript code? Can they detect vulnerabilities that are “deep” in the application (e.g., that are reachable only after correctly
submitting complex forms)? Can they precisely keep track of the state of the application?
To do this, we built a realistic web application, called WackoPicko, and used it to
evaluate eleven web application scanners on their ability to crawl complex web applications and to identify the associated vulnerabilities. More precisely, the WackoPicko
application uses features that are commonly found in modern web applications and that
make their crawling difficult, such as complex HTML forms, extensive JavaScript and
Flash code, and dynamically-created pages. Furthermore, we introduced in the application’s source code a number of vulnerabilities that are representative of the bugs commonly found in real-world applications. The eleven web application scanners that we
tested include both commercial and open-source tools. We evaluated each of them under three different configuration settings, corresponding to increasing levels of manual
intervention. We then analyzed the results produced by the tools in order to understand
how the tools work, how effective they are, and what makes them fail. The ultimate
goal of this effort is to identify which tasks are the most challenging for black-box
vulnerability scanners and may require novel approaches to be tackled successfully.
The main contributions of this paper are the following:
– We performed the most extensive and thorough evaluation of black-box web application vulnerability scanners so far.
– We identify a number of challenges that scanners need to overcome to successfully test modern web applications both in terms of crawling and attack analysis
capabilities.
– We describe the design of a testing web site for web application scanners that composes crawling challenges with vulnerability instances. This site has been made
available to the public and can be used by other researchers in the field.
– We analyze in detail why the web application vulnerability scanners succeed or fail
and we identify areas that need further research.

2 Background
Before discussing the design of our tests, it is useful to briefly discuss the vulnerabilities
that web application scanners try to identify and to present an abstract model of a typical
scanner.
2.1 Web Application Vulnerabilities
Web applications contain a mix of traditional flaws (e.g., ineffective authentication and
authorization mechanisms) and web-specific vulnerabilities (e.g., using user-provided
inputs in SQL queries without proper sanitization). Here, we will briefly describe some
of the most common vulnerabilities in web applications (for further details, the interested reader can refer to the OWASP Top 10 List, which tracks the most critical vulnerabilities in web applications [13]):
– Cross-Site Scripting (XSS): XSS vulnerabilities allow an attacker to execute malicious JavaScript code as if the application sent that code to the user. This is the
first most serious vulnerability of the OWASP Top 10 List, and WackoPicko includes five different XSS vulnerabilities, both reflected and stored.
– SQL Injection: SQL injection vulnerabilities allow one to manipulate, create or
execute arbitrary SQL queries. This is the second most serious vulnerability on the
OWASP Top 10 List, and the WackoPicko web application contains both a reflected
and a stored SQL injection vulnerability.
– Code Injection: Code injection vulnerabilities allow an attacker to execute arbitrary commands or execute arbitrary code. This is the third most serious vulnerability on the OWASP Top 10 List, and WackoPicko includes both a command line
injection and a file inclusion vulnerability (which might result in the execution of
code).
– Broken Access Controls: A web application with broken access controls fails to
properly define or enforce access to some of its resources. This is the tenth most
serious vulnerability on the OWASP Top 10 List, and WackoPicko has an instance
of this kind of vulnerability.
2.2 Web Application Scanners
In abstract, web application scanners can be seen as consisting of three main modules: a
crawler module, an attacker module, and an analysis module. The crawling component
is seeded with a set of URLs, retrieves the corresponding pages, and follows links and
redirects to identify all the reachable pages in the application. In addition, the crawler
identifies all the input points to the application, such as the parameters of GET requests,
the input fields of HTML forms, and the controls that allow one to upload files.
The attacker module analyzes the URLs discovered by the crawler and the corresponding input points. Then, for each input and for each vulnerability type for which the
web application vulnerability scanner tests, the attacker module generates values that
are likely to trigger a vulnerability. For example, the attacker module would attempt
to inject JavaScript code when testing for XSS vulnerabilities, or strings that have a
special meaning in the SQL language, such as ticks and SQL operators, when testing

for SQL injection vulnerabilities. Input values are usually generated using heuristics or
using predefined values, such as those contained in one of the many available XSS and
SQL injection cheat-sheets [18, 19].
The analysis module analyzes the pages returned by the web application in response
to the attacks launched by the attacker module to detect possible vulnerabilities and to
provide feedback to the other modules. For example, if the page returned in response to
input testing for SQL injection contains a database error message, the analysis module
may infer the existence of a SQL injection vulnerability.

3 The WackoPicko Web Site
A preliminary step for assessing web application scanners consists of choosing a web
application to be tested. We have three requirements for such an application: it must
have clearly defined vulnerabilities (to assess the scanner’s detection performance), it
must be easily customizable (to add crawling challenges and experiment with different
types of vulnerabilities), and it must be representative of the web applications in use
today (in terms of functionality and of technologies used).
We found that existing applications did not satisfy our requirements. Applications
that deliberately contain vulnerabilities, such as HacmeBank [5] and WebGoat [11], are
often designed to be educational tools rather than realistic testbeds for scanners. Others,
such as SiteGenerator [10], are well-known, and certain scanners may be optimized to
perform well on them. An alternative then is to use an older version of an open-source
application that has known vulnerabilities. In this case, however, we would not be able
to control and test the crawling capabilities of the scanners, and there would be no way
to establish a false negative rate.
Therefore, we decided to create our own test application, called WackoPicko. It
is important to note that WackoPicko is a realistic, fully functional web application.
As opposed to a simple test application that contains just vulnerabilities, WackoPicko
tests the scanners under realistic conditions. To test the scanners’ support for clientside JavaScript code, we also used the open source Web Input Vector Extractor Teaser
(WIVET). WIVET is a synthetic benchmark that measures how well a crawler is able
to discover and follow links in a variety of formats, such as JavaScript, Flash, and form
submissions.
3.1 Design
WackoPicko is a photo sharing and photo-purchasing site. A typical user of WackoPicko
is able to upload photos, browse other user’s photos, comment on photos, and purchase
the rights to a high-quality version of a photo.
Authentication. WackoPicko provides personalized content to registered users. Despite
recent efforts for a unified login across web sites [14], most web applications require
a user to create an account in order to utilize the services offered. Thus, WackoPicko
has a user registration system. Once a user has created an account, he/she can log in to
access WackoPicko’s restricted features.
Upload Pictures. When a photo is uploaded to WackoPicko by a registered user, other
users can comment on it, as well as purchase the right to a high-quality version.

Comment On Pictures. Once a picture is uploaded into WackoPicko, all registered
users can comment on the photo by filling out a form. Once created, the comment
is displayed, along with the picture, with all the other comments associated with the
picture.
Purchase Pictures. A registered user on WackoPicko can purchase the high-quality
version of a picture. The purchase follows a multi-step process in which a shopping
cart is filled with the items to be purchased, similar to the process used in e-commerce
sites. After pictures are added to the cart, the total price of the cart is reviewed, discount
coupons may be applied, and the order is placed. Once the pictures are purchased, the
user is provided with links to the high-quality version of the pictures.
Search. To enable users to easily search for various pictures, WackoPicko provides a
search toolbar at the top of every page. The search functionality utilizes the tag field
that was filled out when the picture was uploaded. After a query is issued, the user is
presented with a list of all the pictures that have tags that match the query.
Guestbook. A guestbook page provides a way to receive feedback from all visitors to
the WackoPicko web site. The form used to submit feedback contains a “name” field
and a “comment” field.
Admin Area. WackoPicko has a special area for administrators only, which has a different login mechanism than regular users. Administrators can perform special actions,
such as deleting user accounts, or changing the tags of a picture.
3.2 Vulnerabilities
The WackoPicko web site contains sixteen vulnerabilities that are representative of vulnerabilities found in the wild, as reported by the OWASP Top 10 Project [13]. In the
following we provide a brief description of each vulnerability.
3.2.1 Publicly Accessible Vulnerabilities A number of vulnerabilities in WackoPicko
can be exploited without first logging into the web site.
Reflected XSS: There is a XSS vulnerability on the search page, which is accessible
without having to log into the application. In fact, the query parameter is not sanitized
before being echoed to the user. The presence of the vulnerability can be tested by
setting the query parameter to <script>alert(’xss’)</script>. When this
string is reflected to the user, it will cause the browser to display an alert message. (Of
course, an attacker would leverage the vulnerability to perform some malicious activity
rather than alerting the victim.)
Stored XSS: There is a stored XSS vulnerability in the guestbook page. The comment
field is not properly escaped, and therefore, an attacker can exploit this vulnerability by
creating a comment containing JavaScript code. Whenever a user visits the guestbook
page, the attack will be triggered and the (possibly malicious) JavaScript code executed.
Session ID: The session information associated with administrative accounts is handled differently than the information associated with the sessions of normal users. The
functionality associated with normal users uses PHP’s session handling capabilities,
which is assumed to be free of any session-related vulnerabilities (e.g., session fixation,
easily-guessable session IDs). However the admin section uses a custom session cookie

to keep track of sessions. The value used in the cookie is a non-random value that is
incremented when a new session is created. Therefore, an attacker can easily guess the
session id and access the application with administrative rights.
Weak password: The administrative account page has an easily-guessable username
and password combination: admin/admin.
Reflected SQL Injection: WackoPicko contains a reflected SQL injection in the username field of the login form. By introducing a tick into the username field it is possible to perform arbitrary queries in the database and obtain, for example, the usernames
and passwords of all the users in the system.
Command Line Injection: WackoPicko provides a simple service that checks to see
if a user’s password can be found in the dictionary. The password parameter of the
form used to request the check is used without sanitization in the shell command: grep
ˆ<password>$ /etc/dictionaries-common/words. This can be exploited
by providing as the password value a dollar sign (to close grep’s regular expression),
followed by a semicolon (to terminate the grep command), followed by extra commands.
File Inclusion: The admin interface is accessed through a main page, called index.php.
The index page acts as a portal; any value that is passed as its page parameter will be
concatenated with the string “.php”, and then the resulting PHP script will be run. For
instance, the URL for the admin login page is /admin/index.php?page=login.
On the server side, index.php will execute login.php which displays the form. This
design is inherently flawed, because it introduces a file inclusion vulnerability. An attacker can exploit this vulnerability and execute remote PHP code by supplying, for
example, http://hacker/blah.php%00 as the page parameter to index.php.
The %00 at the end of the string causes PHP to ignore the “.php” that is appended
to the page parameter. Thus index.php will download and execute the code at http:
//hacker/blah.php.
Unauthorized File Exposure: In addition to executing remote code, the file inclusion
vulnerability can also be exploited to expose local files. Passing /etc/passwd%00
as the “page” GET parameter to index.php of the admin section will cause the contents
of the /etc/passwd file to be disclosed.
Reflected XSS Behind JavaScript: On WackoPicko’s home page there is a form that
checks if a file is in the proper format for WackoPicko to process. This form has two
parameters, a file parameter and a name parameter. Upon a successful upload, the name
is echoed back to the user unsanitized, and therefore, this represents a reflected vulnerability. However, the form is dynamically generated using JavaScript, and the target of
the form is dynamically created by concatenating strings. This prevents a crawler from
using simple pattern matching to discover the URL used by the form.
Parameter Manipulation: The WackoPicko home page provides a link to a sample
profile page. The link uses the “userid” GET parameter to view the sample user (who
has id of 1). An attacker can manipulate this variable to view any profile page without
having a valid user account.
3.2.2 Vulnerabilities Requiring Authentication A second class of vulnerabilities in
WackoPicko can be exploited only after logging into the web site.

Stored SQL Injection: When users create an account, they are asked to supply their
first name. This supplied value is then used unsanitized on a page that shows other users
who have a similar first name. An attacker can exploit this vulnerability by creating a
user with the name “’ ; DROP users;#” then visiting the similar users page.
Directory Traversal: When uploading a picture, WackoPicko copies the file uploaded
by the user to a subdirectory of the upload directory. The name of the subdirectory
is the user-supplied tag of the uploaded picture. A malicious user can manipulate the
tag parameter to perform a directory traversal attack. More precisely, by pre-pending
“../../” to the tag parameter the attacker can reference files outside the upload directory and overwrite them.
Multi-Step Stored XSS: Similar to the stored XSS attack that exists on the guestbook,
comments on pictures are susceptible to a stored XSS attack. However, this vulnerability is more difficult to exploit because the user must be logged in and must confirm the
preview of the comment before the attack is actually triggered.
Forceful Browsing: One of the central ideas behind WackoPicko is the ability of users
to purchase the rights to high-quality versions of pictures. However, the access to the
links to the high-quality version of the picture is not checked, and an attacker who
acquires the URL of a high-quality picture can access it without creating an account,
thus bypassing the authentication logic.
Logic Flaw: The coupon system suffers from a logic flaw, as a coupon can be applied
multiple times to the same order reducing the final price of an order to zero.
Reflected XSS Behind Flash: On the user’s home page there is a Flash form that asks
the user for his/her favorite color. The resulting page is vulnerable to a reflected XSS
attack, where the “value” parameter is echoed back to the user without being sanitized.
3.3 Crawling Challenges
Crawling is arguably the most important part of a web application vulnerability scanner;
if the scanner’s attack engine is poor, it might miss a vulnerability, but if its crawling engine is poor and cannot reach the vulnerability, then it will surely miss the vulnerability.
Because of the critical nature of crawling, we have included several types of crawling
challenges in WackoPicko, some of which hide vulnerabilities.
HTML Parsing. Malformed HTML makes it difficult for web application scanners to
crawl web sites. For instance, a crawler must be able to navigate HTML frames and be
able to upload a file. Even though these tasks are straightforward for a human user with
a regular browser, they represent a challenge for crawlers.
Multi-Step Process. Even though most web sites are built on top of the stateless HTTP
protocol, a variety of techniques are utilized to introduce state into web applications.
In order to properly analyze a web site, web application vulnerability scanners must be
able to understand the state-based transactions that take place. In WackoPicko, there are
several state-based interactions.
Infinite Web Site. It is often the case that some dynamically-generated content will
create a very large (possibly infinite) crawling space. For example, WackoPicko has the
ability to display a daily calendar. Each page of the calendar displays the agenda for a
given day and links to the page for the following day. A crawler that naively followed
the links in the WackoPicko’s calendar would end up trying to visit an infinite sequence
of pages, all generated dynamically by the same component.

Name
Acunetix
AppScan

Version Used
License
6.1 Build 20090318
Commercial
7.8.0.0 iFix001 Build: 570 Security
Commercial
Rules Version 647
Burp
1.2
Commercial
Grendel-Scan 1.0
GPLv3
Hailstorm
5.7 Build 3926
Commercial
Milescan
1.4
Commercial
N-Stalker
2009 - Build 7.0.0.207
Commercial
NTOSpider 3.2.067
Commercial
Paros
3.2.13
Clarified Artistic License
w3af
1.0-rc2
GPLv2
Webinspect 7.7.869.0
Commercial

Type
Price
Standalone $4,995-$6,350
Standalone $17,550-$32,500
Proxy £125 ($190.82)
Standalone
N/A
Standalone
$10,000
Proxy
$495-$1,495
Standalone
$899-$6,299
Standalone
$10,000
Proxy
N/A
Standalone
N/A
Standalone $6,000-$30,000

Table 1: Characteristics of the scanners evaluated.

Authentication. One feature that is common to most web sites is an authentication
mechanism. Because this is so prevalent, scanners must properly handle authentication,
possibly by creating accounts, logging in with valid credentials, and recognizing actions
that log the crawler out. WackoPicko includes a registration and login system to test the
scanner’s crawlers ability to handle the authentication process correctly.
Client-side Code. Being able to parse and understand client-side technologies presents
a major challenge for web application vulnerability scanners. WackoPicko includes vulnerabilities behind a JavaScript-created form, as well as behind a Flash application.
Link Extraction. We also tested the scanners on WIVET, an open-source benchmark
for web link extractors [12]. WIVET contains 54 tests and assigns a final score to a
crawler based on the percent of tests that it passes. The tests require scanners to analyze
simple links, multi-page forms, links in comments and JavaScript actions on a variety
of HTML elements. There are also AJAX-based tests as well as Flash-based tests. In
our tests, we used WIVET version number 129.

4 Experimental Evaluation
We tested 11 web application scanners by running them on our WackoPicko web site.
The tested scanners included 8 proprietary tools and 3 open source programs. Their cost
ranges from free to tens of thousands of dollars. We used evaluation versions of each
software, however they were fully functional. A summary of the characteristics of the
scanners we evaluated is given in Table 1.
We ran the WackoPicko web application on a typical LAMP machine, with Apache
2.2.9, PHP 5.2.6, and MySQL 5.0.67. We enabled the allow url fopen PHP option and disabled the allow url include and magic quotes options. We ran
the scanners on a machine with a Pentium 4 3.6GHz CPU, 1024 MB of RAM, and
Microsoft Windows XP, Service Pack 2.
4.1 Setup
The WackoPicko server used in testing the web vulnerability scanners was run in a
virtual machine, so that before each test run the server could be put in an identical
initial state. This state included ten regular users, nine pictures, and five administrator
users.

Name

Reflected
XSS

Stored XSS Reflected
SQL
Injection
Acunetix
INITIAL
INITIAL
INITIAL
AppScan
INITIAL
INITIAL
INITIAL
Burp
INITIAL
MANUAL INITIAL
Grendel-Scan MANUAL
CONFIG
Hailstorm
INITIAL
CONFIG
CONFIG
Milescan
INITIAL
MANUAL CONFIG
N-Stalker
INITIAL
MANUAL MANUAL
NTOSpider INITIAL
INITIAL
INITIAL
Paros
INITIAL
INITIAL
CONFIG
w3af
INITIAL
MANUAL INITIAL
Webinspect INITIAL
INITIAL
INITIAL

Command- File
line
Inclusion
Injection
INITIAL
INITIAL
INITIAL

File
Exposure

XSS via
JavaScript

XSS via
Flash

INITIAL
INITIAL
INITIAL

INITIAL

INITIAL

INITIAL

MANUAL

INITIAL

MANUAL
MANUAL
MANUAL

MANUAL
MANUAL

INITIAL
INITIAL

Table 2: Detection results. For each scanner, the simplest configuration that detected a vulnerability is given. Empty cells indicate no detection in any mode.

Each scanner was run in three different configuration modes against WackoPicko,
with each configuration requiring more setup on the part of the user. In all configuration
styles, the default values for configuration parameters were used, and when choices
were required, sensible values were chosen. In the INITIAL configuration mode, the
scanner was directed to the initial page of WackoPicko and told to scan for all vulnerabilities. In the CONFIG setup, the scanner was given a valid username/password combination or login macro before scanning. MANUAL configuration required the most
work on the part of the user; each scanner was put into a “proxy” mode and then the
user browsed to each vulnerable page accessible without credentials; then, the user
logged in and visited each vulnerability that required a login. Additionally a picture
was uploaded, the rights to a high-quality version of a picture were purchased, and a
coupon was applied to the order. The scanner was then asked to scan the WackoPicko
web site.
4.2 Detection Results
The results of running the scanners against the WackoPicko site are shown in Table 2
and, graphically, in Figure 1. The values in the table correspond to the simplest configuration that discovered the vulnerability. An empty cell indicates that the given scanner
did not discover the vulnerability in any mode. The table only reports the vulnerabilities
that were detected by at least one scanner. Further analysis of why the scanners missed
certain vulnerabilities is contained in Sections 4.3 and 4.4.
The running time of the scanners is shown in Figure 3. These times range from 74
seconds for the fastest tool (Burp) to 6 hours (N-Stalker). The majority of the scanners
completed the scan within a half hour, which is acceptable for an automated tool.
4.2.1 False Negatives One of the benefits of developing the WackoPicko web application to test the scanners is the ability for us to measure the false negatives of the
scanners. An ideal scanner would be able to detect all vulnerabilities. In fact, we had
a group composed of students with average security skills analyze WackoPicko. The
students found all vulnerabilities except for the forceful browsing vulnerability. The

automated scanners did not do as well; there were a number of vulnerabilities that were
not detected by any scanner. These vulnerabilities are discussed hereinafter.
Session ID: No scanner was able to detect the session ID vulnerability on the admin
login page. The vulnerability was not detected because the scanners were not given a
valid username/password combination for the admin interface. This is consistent with
what would happen when scanning a typical application, as the administration interface
would include powerful functionality that the scanner should not invoke, like view,
create, edit or delete sensitive user data. The session ID was only set on a successful
login, which is why this vulnerability was not detected by any scanner.
Weak Password: Even though the scanners were not given a valid username/password
combination for the administrator web site, an administrator account with the combination of admin/admin was present on the system. NTOSpider was the only scanner that
successfully logged in with the admin/admin combination. However, it did not report
it as an error, which suggests that it was unable to detect that the login was successful,
even though the response that was returned for this request was different from every
other login attempt.
Parameter Manipulation: The parameter manipulation vulnerability was not discovered by any scanner. There were two causes for this: first, only three of the scanners
(AppScan, NTOSpider, and w3af) input a different number than the default value “1”
to the userid parameter. Of the three, only NTOSpider used a value that successfully
manipulated the userid parameter. The other reason was that in order to successfully
detect a parameter manipulation vulnerability, the scanner needs to determine which
pages require a valid username/password to access and which ones do not and it is clear
that none of the scanners make this determination.
Stored SQL Injection: The stored SQL injection was also not discovered by any
scanners, due to the fact that a scanner must create an account to discover the stored
SQL injection. The reasons for this are discussed in more detail in Section 4.4.4.
Directory Traversal: The directory traversal vulnerability was also not discovered
by any of the scanners. This failure is caused by the scanners being unable to upload
a picture. We discuss this issue in Section 4.4.2, when we analyze how each of the
scanners behaved when they had to upload a picture.
Multi-Step Stored XSS: The stored XSS vulnerability that required a confirmation
step was also missed by every scanner. In Section 4.4.5, we analyze how many of the
scanners were able to successfully create a comment on a picture.
Forceful Browsing: No scanner found the forceful browsing vulnerability, which is
not surprising since it is an application-specific vulnerability. These vulnerabilities are
difficult to identify without access to the source code of the application [2].
Logic Flaw: Another vulnerability that none of the scanners uncovered was the logic
flaw that existed in the coupon management functionality. Also in this case, some domain knowledge about the application is needed to find the vulnerability.

100%
80%

False negatives
Detection in MANUAL mode
Detection in CONFIG mode
Detection in INITIAL mode

60%
40%

Webinspect

w3af

Paros

NTOSpider

N−Stalker

Milescan

Hailstorm

Grendel−Scan

Burp

Appscan

0%

Acunetix

20%

Fig. 1: Detection performance (true positives and false negatives) of the evaluated scanners.
Name
INITIAL CONFIG MANUAL
Acunetix
1
7
4
AppScan
11
20
26
Burp
1
2
6
Grendel-Scan
15
16
16
Hailstorm
3
11
3
Milescan
0
0
0
N-Stalker
5
0
0
NTOSpider
3
1
3
Paros
1
1
1
w3af
1
1
9
Webinspect
215
317
297

Table 3: False positives.
4.2.2 False Positives The total number of false positives for each of the scanning
configurations are show in Table 3. The number of false positives that each scanner
generates is an important metric, because the greater the number of false positives, the
less useful the tool is to the end user, who has to figure out which of the vulnerabilities
reported are actual flaws and which are spurious results of the analysis.
The majority of the false positives across all scanners were due to a supposed
“Server Path Disclosure.” This is an information leakage vulnerability where the server
leaks the paths of local files, which might give an attacker hints about the structure of
the file system.
An analysis of the results identified two main reasons why these false positives
were generated. The first is that while testing the application for file traversal or file
injection vulnerabilities, some of the scanners passed parameters with values of file
names, which, on some pages (e.g., the guestbook page), caused the file name to be
included in that page’s contents. When the scanner then tested the page for a Server
Path Disclosure, it found the injected values in the page content, and generated a Server
Path Disclosure vulnerability report. The other reason for the generation of false positives is that WackoPicko uses absolute paths in the href attribute of anchors (e.g.,
/users/home.php), which the scanner mistook for the disclosure of paths in the

Webinspect

Acunetix

Burp

N-Stalker

INITIAL
CONFIG

27,103

Grendel-Scan

Fig. 2: Dominates graph.

4,000

w3af

Webinspect

Paros

NTOSpider

Milescan

0

N−Stalker

2,000

Hailstorm

Milescan

6,000

Burp

NTOSpider

8,000

Grendel−Scan

AppScan

Appscan

Hailstorm

w3af

Acunetix

Paros

Running Time (Seconds)

10,000

Fig. 3: Scanner Running Times

local system. Webinspect generated false positives because of both the above reasons,
which explains the large amount of false positives produced by the tool.
Some scanners reported genuine false positives: Hailstorm reported a false XSS vulnerability and two false PHP code injection vulnerabilities, NTOSpider reported three
false XSS vulnerabilities and w3af reported a false PHP eval() input injection vulnerability.

4.2.3 Measuring and Comparing Detection Capabilities Comparing the scanners
using a single benchmark like WackoPicko does not represent an exhaustive evaluation.
However, we believe that the results provide insights about the current state of blackbox web application vulnerability scanners.
One possible way of comparing the results of the scanners is arranging them in a
lattice. This lattice is ordered on the basis of strict dominance. Scanner A strictly dominates Scanner B if and only if for every vulnerability discovered by Scanner B, Scanner
A discovered that vulnerability with the same configuration level or simpler, and Scanner A either discovered a vulnerability that Scanner B did not discover or Scanner A
discovered a vulnerability that Scanner B discovered, but with a simpler configuration.
Strictly dominates has the property that any assignment of scores to vulnerabilities must
preserve the strictly dominates relationship.
Figure 2 shows the strictly dominates graph for the scanners, where a directed edge
from Scanner A to Scanner B means that Scanner A strictly dominates Scanner B.
Because strictly dominates is transitive, if one scanner strictly dominates another it also
strictly dominates all the scanners that the dominated scanner dominates, therefore, all
redundant edges are not included. Figure 2 is organized so that the scanners in the top
level are those that are not strictly dominated by any scanners. Those in the second level
are strictly dominated by only one scanner and so on, until the last level, which contains
those scanners that strictly dominate no other scanner.
Some interesting observations arise from Figure 2. N-Stalker does not strictly dominate any scanner and no scanner strictly dominates it. This is due to the unique combination of vulnerabilities that N-Stalker discovered and missed. Burp is also interesting

Name
XSS Reflected
XSS Stored
SessionID
SQL Injection Reflected
Commandline Injection
File Inclusion
File Exposure
XSS Reflected behind
JavaScript
Parameter Manipulation
Weak password
SQL Injection Stored Login
Directory Traversal Login
XSS Stored Login
Forceful Browsing Login
Logic Flaws - Coupon
XSS Reflected behind flash

Detection INITIAL
Reachability
1
0
2
0
4
0
1
0
4
0
3
0
3
0
1
3

CONFIG
Reachability
0
0
0
0
0
0
0
3

MANUAL
Reachability
0
0
0
0
0
0
0
0

8
3
7
8
2
8
9
1

0
0
3
6
7
6
8
7

0
0
3
4
6
3
6
1

0
0
7
8
8
7
9
9

Name
Score
Acunetix
14
Webinspect
13
Burp
13
N-Stalker
13
AppScan
10
w3af
9
Paros
6
Hailstorm
6
NTOSpider
4
Milescan
4
Grendel-Scan
3

Table 5: Final ranking.

Table 4: Vulnerability scores.

due to the fact that it only dominates two scanners but no scanner dominates Burp because it was the only scanner to discover the command-line injection vulnerability.
While Figure 2 is interesting, it does not give a way to compare two scanners where
one does not strictly dominate the other. In order to compare the scanners, we assigned
scores to each vulnerability present in WackoPicko. The scores are displayed in Table 4.
The “Detection” score column in Table 4 is how many points a scanner is awarded based
on how difficult it is for an automated tool to detect the existence of the vulnerability. In
addition to the “Detection” score, each vulnerability is assigned a “Reachability” score,
which indicates how difficult the vulnerability is to reach (i.e., it reflects the difficulty
of crawling to the page that contains the vulnerability). There are three “Reachability” scores for each vulnerability, corresponding to how difficult it is for a scanner to
reach the vulnerability when run in INITIAL, CONFIG, or MANUAL mode. Of course,
these vulnerability scores are subjective and depend on the specific characteristics of
our WackoPicko application. However, their values try to estimate the crawling and
detection difficulty of each vulnerability in this context.
The final score for each scanner is calculated by adding up the “Detection” score
for each vulnerability the scanner detected and the “Reachability” score for the configuration (INITIAL, CONFIG and MANUAL) used when running the scanner. In the
case of a tie, the scanners were ranked by how many vulnerabilities were discovered in
INITIAL mode, which was enough to break all ties. Table 5 shows the final ranking of
the scanners.
4.3 Attack and Analysis Capabilities
Analyzing how each scanner attempted to detect vulnerabilities gives us insight into
how these programs work and illuminates areas for further research. First, the scanner
would crawl the site looking for injection points, typically in the form of GET or POST
parameters. Once the scanner identifies all the inputs on a page, it then attempts to
inject values for each parameter and observes the response. When a page has more
than one input, each parameter is injected in turn, and generally no two parameters are

injected in the same request. However, scanners differ in what they supply as values of
the non-injected parameters: some have a default value like 1234 or Peter Wiener,
while others leave the fields blank. This has an impact on the results of the scanner, for
example the WackoPicko guestbook requires that both the name and comment fields
are present before making a comment, and thus the strategy employed by each scanner
can affect the effectiveness of the vulnerability scanning process.
When detecting XSS attacks, most scanners employed similar techniques, some
with a more sophisticated attempt to evade possible filters than others. One particularly
effective strategy employed was to first input random data with various combinations of
dangerous characters, such as / ,",’,<, and >, and then, if one of these combinations was found unchanged in the response, to attempt the injection of the full range of
XSS attacks. This technique speeds up the analysis significantly, because the full XSS
attack is not attempted against every input vector. Differently, some of the scanners took
an exhaustive approach, attempting the full gamut of attacks on every combination of
inputs.
When attempting a XSS attack, the thorough scanners would inject the typical
<script> alert(’xss’) </script> as well as a whole range of XSS attack
strings, such as JavaScript in a tag with the onmouseover attribute, in an img, div
or meta tag, or iframe. Other scanners attempted to evade filters by using a different
JavaScript function other than alert, or by using a different casing of script, such
as ScRiPt.
Unlike with XSS, scanners could not perform an easy test to exclude a parameter
from thorough testing for other Unsanitized Input vulnerabilities because the results of
a successful exploit might not be readily evident in the response. This is true for the
command-line injection on the WackoPicko site, because the output of the injectable
command was not used in the response. Burp, the only scanner that was able to successfully detect the command line injection vulnerability, did so by injecting ‘ping
-c 100 localhost‘ and noticing that the response time for the page was much
slower than when nothing was injected.
This pattern of measuring the difference in response times was also seen in detecting
SQL injections. In addition to injecting something with a SQL control character, such
as tick or quote and seeing if an error is generated, the scanners also used a time-delay
SQL injection, inputting waitfor delay ’0:0:20’ and seeing if the execution
was delayed. This is a variation of the technique of using time-delay SQL injection to
extract database information from a blind SQL vulnerability.
When testing for File Exposure, the scanners were typically the same; however one
aspect caused them to miss the WackoPicko vulnerability. Each scanner that was looking for this vulnerability input the name of a file that they knew existed on the system,
such as /etc/passwd on UNIX-like systems or C:\boot.ini for Windows. The
scanners then looked for known strings in the response. The difficulty in exploiting the
WackoPicko file exposure was including the null-terminating character (%00) at the
end of the string, which caused PHP to ignore anything added by the application after
the /etc/passwd part. The results show that only 4 scanners successfully discovered
this vulnerability.
The remote code execution vulnerability in WackoPicko is similar to the file exposure vulnerability. However, instead of injecting known files, the scanners injected

Name

Reflected XSS

Acunetix
AppScan
Burp
Grendel-Scan
Hailstorm
Milescan
N-Stalker
NTOSpider
Paros
w3af
Webinspect
Name

INITIAL CONFIG MANUAL
496
638
498
581
575
817
256
256
207
0
0
44
232
229
233
104
0
208
1738
1162
2689
856
679
692
68
68
58
157
157
563
108
108
105
Parameter Manipulation

Acunetix
AppScan
Burp
Grendel-Scan
Hailstorm
Milescan
N-Stalker
NTOSpider
Paros
w3af
Webinspect

2
221
192
3
3
105
1291
107
72
128
102

0
210
194
3
143
0
1270
115
72
128
102

2
222
124
6
146
103
1302
115
72
124
102

Stored XSS

613 779 724
381 352 492
192 192 262
1
1
3
10 205 209
50 0
170
2484 2100 3475
252 370 370
126 126 110
259 257 464
631 631 630
Directory
Traversal
35 1149 37
80 70 941
68 68 394
1
1
3
336 329 344
8
0
163
22 2079 4704
11 572 572
14 14 0
31 30 783
29 29 690

Reflected SQL Command-line
Injection
Injection

File Inclusion / XSS Reflected
File Exposure / - JavaScript
Weak password

544 709 546
274 933 628
68 222 221
14 34 44
45 224 231
75 272 1237
2764 1022 2110
184 5
5
151 299 97
1377 1411 2634
297 403 346
Logic Flaw

198 244 200
267 258 430
125 316 320
2
2
5
8
204 216
80 0
246
1437 2063 1824
243 614 614
146 146 185
263 262 470
239 237 234
XSS Reflected
behind flash
1
34 458
0
0
243
0
0
125
0
0
3
0
0
143
0
0
68
0
0
1315
0
11 11
0
0
60
0
0
119
0
0
97

0
0
0
0
131
0
0
0
0
0
0

0
0
0
0
132
0
0
11
0
0
8

5
329
314
6
5
1
3
11
114
235
3

495 637 497
189 191 288
68 68 200
1
1
3
180 160 162
0
0
131
2005 1894 1987
105 9
9
28 28 72
140 142 253
164 164 164
Forceful
Browsing
0
0
206
0
0
71
0
0
151
0
0
1
102 102 105
0
0
60
0
0
2
0
0
0
0
0
70
0
0
270
0
118 82

670 860 671
0
0
442
0
0
178
0
0
2
153 147 148
0
0
163
1409 1292 1335
11 13 13
0
0
56
0
0
34
909 909 0

Table 6: Number of accesses to vulnerable web pages in INITIAL, CONFIG, and MANUAL
modes.

known web site addresses. This was typically from a domain the scanner’s developers
owned, and thus when successfully exploited, the injected page appeared instead of the
regular page. The same difficulty in a successful exploitation existed in the File Exposure vulnerability, so a scanner had to add %00 after the injected web site. Only 3
scanners were able to successfully identify this vulnerability.
4.4 Crawling Capabilities
The number of URLs requested and accessed varies considerably among scanners, depending on the capability and strategies implemented in the crawler and attack components. Table 6 shows the number of times each scanner made a POST or GET request to
a vulnerable URL when the scanners were run in INITIAL, CONFIG, and MANUAL
mode. For instance, from Table 6 we can see that Hailstorm was able to access many
of the vulnerable pages that required a valid username/password when run in INITIAL
mode. It can also be seen that N-Stalker takes a shotgun-like approach to scanning; it
has over 1,000 accesses for each vulnerable URL, while in contrast Grendel-Scan never
had over 50 accesses to a vulnerable URL.
In the following, we discuss the main challenges that the crawler components of the
web application scanners under test faced.
4.4.1 HTML The results for the stored XSS attack reveal some interesting characteristics of the analysis performed by the various scanners. For instance, Burp, GrendelScan, Hailstorm, Milescan, N-Stalker, and w3af were unable to discover the stored XSS

vulnerability in INITIAL configuration mode. Burp and N-Stalker failed because of defective HTML parsing. Neither of the scanners correctly interpreted the <textarea>
tag as an input to the HTML form. This was evident because both scanners only sent the
name parameter when attempting to leave a comment on the guestbook. When run in
MANUAL mode, however, the scanners discovered the vulnerability, because the user
provided values for all these fields. Grendel-Scan and Milescan missed the stored XSS
vulnerability for the same reason: they did not attempt a POST request unless the user
used the proxy to make the request.
Hailstorm did not try to inject any values to the guestbook when in INITIAL mode,
and, instead, used testval as the name parameter and Default text as the
comment parameter. One explanation for this could be that Hailstorm was run in the
default “turbo” mode, which Cenzic claims catches 95% of vulnerabilities, and chose
not to fuzz the form to improve speed.
Finally, w3af missed the stored XSS vulnerability due to leaving one parameter
blank while attempting to inject the other parameter. It was unable to create a guestbook
entry, because both parameters are required.
4.4.2 Uploading a Picture Being able to upload a picture is critical to discover the
Directory Traversal vulnerability, as a properly crafted tag parameter can overwrite
any file the web server can access. It was very difficult for the scanners to successfully
upload a file: no scanner was able to upload a picture in INITIAL and CONFIG modes,
and only AppScan and Webinspect were able to upload a picture after being showed
how to do it in MANUAL configuration, with AppScan and Webinspect uploading 324
and 166 pictures respectively. Interestingly, Hailstorm, N-Stalker and NTOSpider never
successfully uploaded a picture, even in MANUAL configuration. This surprising result
is due to poor proxies or poor in-application browsers. For instance, Hailstorm includes
an embedded Mozilla browser for the user to browse the site when they want to do so
manually, and after repeated attempts the embedded browser was never able to upload a
file. The other scanners that failed, N-Stalker and NTOSpider, had faulty HTTP proxies
that did not know how to properly forward the file uploaded, thus the request never
completed successfully.
4.4.3 Client-side Code The results of the WIVET tests are shown in Figure 4. Analyzing the WIVET results gives a very good idea of the JavaScript capabilities of
each scanner. Of all the 54 WIVET tests, 24 required actually executing or understand
JavaScript code; that is, the test could not be passed simply by using a regular expression to extract the links on the page. Webinspect was the only scanner able to complete
all of the dynamic JavaScript challenges. Of the rest of the scanners, Acunetix and
NTOSpider only missed one of the dynamic JavaScript tests. Even though Hailstorm
missed 12 of the dynamic JavaScript tests, we believe that this is because of a bug in the
JavaScript analysis engine and not a general limitation of the tool. In fact, Hailstorm was
able to correctly handle JavaScript on the onmouseup and onclick parametrized
functions. These tests were on parametrized onmouseout and onmousedown functions, but since Hailstorm was able to correctly handle the onmouseup and onclick
parametrized functions, this can be considered a bug in Hailstorm’s JavaScript parsing.
From this, it can also be concluded that AppScan, Grendel-Scan, Milescan, and w3af

N−Stalker

Burp

Paros

Milescan

Grendel−Scan

w3af

Appscan

Hailstorm

Acunetix

NTOSpider

Webinspect

% of WIVET Tests Passed

perform no dynamic JavaScript parsing. Thus, Webinspect, Acunetix, NTOSpider, and
Hailstorm can be claimed to have the best JavaScript parsing. The fact that N-Stalker
found the reflected XSS vulnerability behind a JavaScript form in WackoPicko suggests
that it can execute JavaScript, however it failed the WIVET benchmark so we cannot
evaluate the extent of the parsing performed.
In looking at the WIVET results,
there was one benchmark that no scan100%
ner was able to reach, which was behind
90%
80%
a Flash application. The application had a
70%
link on a button’s onclick event, how60%
50%
ever this link was dynamically created at
40%
30%
run time. This failure shows that none
20%
of the current scanners processes Flash
10%
0%
content with the same level of sophistication as JavaScript. This conclusion is
supported by none of the scanners discovering the XSS vulnerability behind a
Fig. 4: WIVET results.
Flash application in WackoPicko when in
INITIAL or CONFIG mode.
4.4.4 Authentication Table 7 shows the attempts that were made to create an account on
Name
Successful Error
the WackoPicko site. The Name column is the
Acunetix
0 431
name of the scanner, “Successful” is the number
AppScan
1 297
Burp
0
0
of accounts successfully created, and “Error” is
Grendel-Scan
0
0
the number of account creation attempts that were
Hailstorm
107 276
Milescan
0
0
unsuccessful. Note that Table 7 reports the results
N-Stalker
74 1389
of the scanners when run in INITIAL mode only,
NTOSpider
74 330
Paros
0 176
because the results for the other configurations
w3af
0 538
were almost identical.
Webinspect
127 267
Table 7 shows the capability of the scanners to
handle user registration functionality. As can be
Table 7: Account creation.
seen from Table 7, only five of the scanners were
able to successfully create an account. Of these,
Hailstorm was the only one to leverage this ability to visit vulnerable URLs that required a login in its INITIAL run.
Creating an account is important in discovering the stored SQL injection that no
scanner successfully detected. It is fairly telling that even though five scanners were
able to create an account, none of them detected the vulnerability. It is entirely possible
that none of the scanners actively searched for stored SQL injections, which is much
harder to detect than stored XSS injections.
In addition to being critically important to the WackoPicko benchmark, being able
to create an account is an important skill for a scanner to have when analyzing any web
site, especially if that scanner wishes to be a point-and-click web application vulnerability scanner.

4.4.5 Multi-step Processes In the WackoPicko web site there is a vulnerability that
is triggered by going through a multi-step process. This vulnerability is the stored XSS
on pictures, which requires an attacker to confirm a comment posting for the attack to
be successful. Hailstorm and NTOSpider were the only scanners to successfully create
a comment on the INITIAL run (creating 25 and 1 comment, respectively). This is
important for two reasons: first, to be able to create a comment in the INITIAL run,
the scanner had to create an account and log in with that account, which is consistent
with Table 7. Also, all 25 of the comments successfully created by Hailstorm only
contained the text Default text, which means that Hailstorm was not able to create
a comment that exploited the vulnerability.
All scanners were able to create a comment when run in MANUAL configuration,
since they were shown by the user how to carry out this task. However, only AppScan,
Hailstorm, NTOSpider, and Webinspect (creating 6, 21, 7, and 2 comments respectively) were able to create a comment that was different than the one provided by the
user. Of these scanners only Webinspect was able to create a comment that exploited the
vulnerability, <iFrAmE sRc=hTtP://xSrFtEsT .sPi/> </iFrAmE>, however Webinspect failed to report this vulnerability. One plausible explanation for not
detecting would be the scanners’ XSS strategy discussed in Section 4.3. While testing the text parameter for a vulnerability, most of the scanners realized that it was
properly escaped on the preview page, and thus stopped trying to inject XSS attacks.
This would explain the directory traversal attack comment that AppScan successfully
created and why Hailstorm did not attempt any injection. This is an example where the
performance optimization of the vulnerability analysis can lead to false negatives.
4.4.6 Infinite Web Sites One of the scanners attempted to visit all of the pages of the
infinite calendar. When running Grendel-Scan, the calendar portion of WackoPicko had
to be removed because the scanner ran out of memory attempting to access every page.
Acunetix, Burp, N-Stalker and w3af had the largest accesses (474, 691, 1780 and 3094
respectively), due to their attempts to exploit the calendar page. The other scanners used
less accesses (between 27 and 243) because they were able to determine that no error
was present.

5 Lessons Learned
We found that the crawling of modern web applications can be a serious challenge for
today’s web vulnerability scanners. A first class of problems we encountered consisted
of implementation errors and the lack of support for commonly-used technologies. For
example, handling of multimedia data (image uploads) exposed bugs in certain proxybased scanners, which prevented the tools from delivering attacks to the application
under test. Incomplete or incorrect HTML parsers caused scanners to ignore input vectors that would have exposed vulnerabilities. The lack of support for JavaScript (and
Flash) prevented tools from reaching vulnerable pages altogether. Support for wellknown, pervasive technology should be improved.
The second class of problems that hindered crawling is related to the design of modern web applications. In particular, applications with complex forms and aggressive
checking of input values can effectively block a scanner, preventing it from crawling

the pages “deep” in the web site structure. Handling this problem could be done, for
example, by using heuristics to identify acceptable inputs or by reverse engineering the
input filters. Furthermore, the behavior of an application can be wildly different depending on its internal “state,” i.e., the values of internal variables that are not explicitly
exposed to the scanner. The classic example of application state is whether the current
user is logged in or not. A scanner that does not correctly model and track the state of an
application (e.g., it does not realize that it has been automatically logged out) will fail
to crawl all relevant parts of the application. More sophisticated algorithms are needed
to perform “deep” crawling and track the state of the application under test.
Current scanners fail to detect (or even check for) application-specific (or “logic”)
vulnerabilities. Unfortunately, as applications become more complex, this type of vulnerabilities will also become more prevalent. More research is warranted to automate
the detection of application logic vulnerabilities.
In conclusion, far from being point-and-click tools to be used by anybody, web
application black-box security scanners require a sophisticated understanding of the
application under test and of the limitations of the tool, in order to be effective.

6 Related Work
Our work is related to two main areas of research: the design of web applications for
assessing vulnerability analysis tools and the evaluation of web scanners.
Designing test web applications. Vulnerable test applications are required to assess
web vulnerability scanners. Unfortunately, no standard test suite is currently available
or accepted by the industry. HacmeBank [5] and WebGoat [11] are two well-known,
publicly-available, vulnerable web applications, but their design is focused more on
teaching web application security rather than testing automated scanners. SiteGenerator [10] is a tool to generate sites with certain characteristics (e.g., classes of vulnerabilities) according to its input configuration. While SiteGenerator is useful to automatically
produce different vulnerable sites, we found it easier to manually introduce in WackoPicko the vulnerabilities with the characteristics that we wanted to test.
Evaluating web vulnerability scanners. There exists a growing body of literature on
the evaluation of web vulnerability scanners. For example, Suto compared three scanners against three different applications and used code coverage, among other metrics,
as a measure of the effectiveness of each scanner [21]. In a recent follow-up study,
Suto [22] assessed seven scanners and compared their detection capabilities and the
time required to run them. Wiegenstein et al. ran five unnamed scanners against a custom benchmark [24]. Unfortunately, the authors do not discuss in detail the reasons for
detections or spidering failures. In their survey of web security assessment tools, Curphey and Araujo reported that black-box scanners perform poorly [3]. Peine examined
in depth the functionality and user interfaces of seven scanners (three commercial) that
were run against WebGoat and one real-world application [16]. Kals et al. developed a
new web vulnerability scanner and tested it on about 25,000 live web pages [7]. Since
no ground truth is available for these sites, the authors cannot discuss false negative
rate or failures of their tool. More recently, AnantaSec released an evaluation of three
scanners against 13 real-world applications, three web applications provided by the
scanners vendors, and a series of JavaScript tests [1]. While this experiment assesses a

large number of real-world applications, only a limited number of scanners are tested
and no explanation is given for the results. In addition, Vieira et al. tested four web
scanners on 300 web services [23]. They also report high rates of false positives and
false negatives.
In comparison, our work, to the best of our knowledge, performs the largest evaluation of web application scanners in terms of the number of tested tools (eleven, both
commercial and open-source), and the class of vulnerabilities analyzed. In addition, we
discuss the effectiveness of different configurations and levels of manual intervention,
and examine in detail the reasons for a scanner’s success or failure.
Furthermore, we provide a discussion of challenges (i.e., critical limitations) of current web vulnerability scanners. While some of these problem areas were discussed
before [6, 8], we provide quantitative evidence that these issues are actually limiting
the performance of today’s tools. We believe that this discussion will provide useful
insight into how to improve state-of-the-art of black-box web vulnerability scanners.

7 Conclusions
This paper presented the evaluation of eleven black-box web vulnerability scanners.
The results of the evaluation clearly show that the ability to crawl a web application and
reach “deep” into the application’s resources is as important as the ability to detect the
vulnerabilities themselves.
It is also clear that although techniques to detect certain kinds of vulnerabilities are
well-established and seem to work reliably, there are whole classes of vulnerabilities
that are not well-understood and cannot be detected by the state-of-the-art scanners. We
found that eight out of sixteen vulnerabilities were not detected by any of the scanners.
We have also found areas that require further research so that web application vulnerability scanners can improve their detection of vulnerabilities. Deep crawling is vital
to discover all vulnerabilities in an application. Improved reverse engineering is necessary to keep track of the state of the application, which can enable automated detection
of complex vulnerabilities.
Finally, we found that there is no strong correlation between cost of the scanner and
functionality provided as some of the free or very cost-effective scanners performed as
well as scanners that cost thousands of dollars.
Acknowledgments
This work has been supported by the National Science Foundation, under grants CCR0524853, CCR-0716095, CCR-0831408, CNS-0845559 and CNS-0905537, and by the
ONR under grant N000140911042.

References
1. AnantaSec:
Web
Vulnerability
Scanners
Evaluation
(January
2009),
http://anantasec.blogspot.com/2009/01/web-vulnerability-scanners-comparison.html

2. Balzarotti, D., Cova, M., Felmetsger, V., Vigna, G.: Multi-module Vulnerability Analysis of
Web-based Applications. In: Proceedings of the ACM conference on Computer and Communications Security (CCS). pp. 25–35 (2007)
3. Curphey, M., Araujo, R.: Web Application Security Assessment Tools. IEEE Security and
Privacy 4(4), 32–41 (2006)
4. CVE: Common Vulnerabilities and Exposures. http://www.cve.mitre.org
5. Foundstone: Hacme Bank v2.0. http://www.foundstone.com/us/resources/
proddesc/hacmebank.htm (May 2006)
6. Grossman, J.: Challenges of Automated Web Application Scanning. In: BlackHat Windows
Security Conference (2004)
7. Kals, S., Kirda, E., Kruegel, C., Jovanovic, N.: SecuBat: A Web Vulnerability Scanner. In:
Proceedings of the International World Wide Web Conference (2006)
8. McAllister, S., Kruegel, C., Kirda, E.: Leveraging User Interactions for In-Depth Testing
of Web Applications. In: Proceedings of the Symposium on Recent Advances in Intrusion
Detection (2008)
9. Open Security Foundation: OSF DataLossDB: Data Loss News, Statistics, and Research.
http://datalossdb.org/
10. Open Web Application Security Project (OWASP): OWASP SiteGenerator. http://www.
owasp.org/index.php/OWASP\_SiteGenerator
11. Open Web Application Security Project (OWASP): OWASP WebGoat Project.
http://www.owasp.org/index.php/Category:OWASP WebGoat Project
12. Open Web Application Security Project (OWASP): Web Input Vector Extractor Teaser.
http://code.google.com/p/wivet/
13. Open Web Application Security Project (OWASP): OWASP Top Ten Project. http://
www.owasp.org/index.php/Top_10 (2010)
14. OpenID Foundation: OpenID. http://openid.net/
15. PCI Security Standards Council: PCI DDS Requirements and Security Assessment Procedures, v1.2 (October 2008)
16. Peine, H.: Security Test Tools for Web Applications. Tech. Rep. 048.06, Fraunhofer IESE
(January 2006)
17. Provos, N., Mavrommatis, P., Rajab, M., Monrose, F.: All Your iFRAMEs Point to Us. In:
Proceedings of the USENIX Security Symposium. pp. 1–16 (2008)
18. RSnake: Sql injection cheat sheet. http://ha.ckers.org/sqlinjection/
19. RSnake: XSS (Cross Site Scripting) Cheat Sheet. http://ha.ckers.org/xss.html
20. Small, S., Mason, J., Monrose, F., Provos, N., Stubblefield, A.: To Catch a Predator: A Natural Language Approach for Eliciting Malicious Payloads. In: Proceedings of the USENIX
Security Symposium (2008)
21. Suto, L.: Analyzing the Effectiveness and Coverage of Web Application Security Scanners
(October 2007), case Study
22. Suto, L.: Analyzing the Accuracy and Time Costs of Web Application Security Scanners
(Feb 2010)
23. Vieira, M., Antunes, N., Madeira, H.: Using Web Security Scanners to Detect Vulnerabilities
in Web Services. In: Proceedings of the Conference on Dependable Systems and Networks
(2009)
24. Wiegenstein, A., Weidemann, F., Schumacher, M., Schinzel, S.: Web Application Vulnerability Scanners—a Benchmark. Tech. rep., Virtual Forge GmbH (October 2006)

Hit ’em Where it Hurts:
A Live Security Exercise on Cyber Situational Awareness
Adam Doupé, Manuel Egele, Benjamin Caillat, Gianluca Stringhini,
Gorkem Yakin, Ali Zand, Ludovico Cavedon, and Giovanni Vigna
University of California, Santa Barbara

{adoupe, maeg, benjamin, gianluca, gyakin, zand, cavedon, vigna}@cs.ucsb.edu
ABSTRACT

Security education can be performed at diﬀerent levels to
reach diﬀerent segments, from everyday Internet users, to
high school students, to undergraduate and graduate students. Recently, competition-based educational tools have
become popular in graduate and undergraduate education,
as competition among students fosters creativity, innovation,
and the desire to excel.
Previous work has described traditional “capture the ﬂag
competitions” [19, 20], and, more recently, new designs for
this type of competition [2]. The development of new designs improved the competition and forced the participants
to analyze and understand unfamiliar, complex sets of interdependent components, similar to those that are part of reallife networks and malware infrastructures.
Our novel insight is that these competitions, can, in addition to their educational value, provide interesting datasets
that can be used in research. To validate this idea we designed and developed a novel security competition based on
the concept of Cyber Situational Awareness (described in
Section 2). The competition is called the iCTF (international Capture the Flag) and was carried out on December
3rd, 2010, involving 72 teams and 900 students, making it
the largest live educational security exercise ever performed.
This paper presents the design of the competition, the
data that was collected, and the lessons learned. The data
is the ﬁrst publicly available dataset that explicitly supports
research in Cyber Situational Awareness.
In summary, this paper adds the following contributions:

Live security exercises are a powerful educational tool to
motivate students to excel and foster research and development of novel security solutions. Our insight is to design
a live security exercise to provide interesting datasets in a
speciﬁc area of security research. In this paper we validated
this insight, and we present the design of a novel kind of live
security competition centered on the concept of Cyber Situational Awareness. The competition was carried out in December 2010, and involved 72 teams (900 students) spread
across 16 countries, making it the largest educational live
security exercise ever performed. We present both the innovative design of this competition and the novel dataset we
collected. In addition, we deﬁne Cyber Situational Awareness metrics to characterize the toxicity and eﬀectiveness of
the attacks performed by the participants with respect to
the missions carried out by the targets of the attack.

1.

INTRODUCTION

In recent years, security attacks have become increasingly
wide-spread and sophisticated. These attacks are made possible by vulnerable software, poorly conﬁgured systems, and
a lack of security awareness and education of end users.
While a large portion of the security research eﬀorts are
focused on developing novel mechanisms and policies to detect, block, and/or prevent security attacks, there is also
the need for the development of novel approaches to educate those who create the computer infrastructure, as well
as those who use it everyday.
This is an often-overlooked aspect of computer security,
but a critical one. Almost all sophisticated, widely deployed,
security mechanisms can be made useless by luring an unsuspecting user (or a developer) into performing actions that,
eventually, will compromise the security of their environment. A clear example of the popularity of these attacks
is the proliferation of fake anti-virus scams, in which users
who are not technically savvy are conned into installing a
Trojan application [17].

• We describe the design and implementation of a novel
computer security competition, whose goal is to not
just foster computer security education, but to create
a Cyber Situational Awareness dataset.
• We analyze the collected dataset and discuss its use
in Cyber Situational Awareness research, introducing
a novel metric that characterizes the eﬀectiveness of
attacks with respect to a speciﬁc mission.
• We discuss the lessons learned from the competition,
and we provide suggestions to other educators that
might implement similar competitions.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ACSAC ’11 Dec. 5-9, 2011, Orlando, Florida USA
Copyright 2011 ACM 978-1-4503-0672-0/11/12 ...$10.00.

2.

BACKGROUND AND HISTORY

In this section, we provide background on two of the most
important aspects of this paper: the design and execution of
live security competitions, and the concepts associated with
Cyber Situational Awareness.



2.1

Live Security Competitions

and score them according to their ability to react to attacks.
This evaluation method is subjective and requires a human
judge for each team thus rendering it impractical in a largescale on-line security competition.
The 2010 iCTF diﬀered from the 2009 iCTF [2] in the
following way: we realized that a live security exercise could
be structured to create a dataset to enable security research.
We utilized this idea in the 2010 iCTF by creating a Cyber
Situational Awareness security competition that would generate a useful Cyber Situational Awareness dataset.

Security challenges have been a way to attract the interest of security researchers, practitioners, and students.
Live security challenges add a real-time factor that supports
deeper involvement and introduces the “crisis factor” associated with many real-life security problems: “something bad
is happening right now and has to be taken care of.”
There have been a number of live security challenges,
but the best-known competition is DefCon’s Capture The
Flag (CTF). This competition started with a simple design,
where a host with vulnerable services was made available to
the participants, who would attack the target concurrently.
Whoever was able to break a service and steal the ﬂag ﬁrst,
obtained the points associated with that service. The original design was changed in 2002. In this edition of DefCon’s
CTF, the participating teams received an identical copy of
a virtualized system containing a number of vulnerable services. Each team ran their virtual machine on a virtual
private network (VPN), with the goal of maintaining the
service’s availability and integrity whilst concurrently compromising the other teams’ services. Since each team had
exactly the same copy of the services, the participants had
to analyze the services, ﬁnd the vulnerabilities, patch their
own copies, and break into the other teams’ services and
steal the associated ﬂags. Every other DefCon CTF following 2002 used more or less the same design [4].
Even though DefCon’s CTF was designed to test the skills
of hackers and security professionals, it was clear that the
same type of competition could be used as an educational
tool. One of the major diﬀerences between the iCTF and
DefCon’s CTF is that the iCTF involves educational institutions spread out across the world, where the DefCon CTF
allows only locally-connected teams. Therefore, DefCon requires the physical co-location of the contestants thus constraining participation to a limited number of teams. By
providing remote access, the iCTF allows dozens of remotely
located teams to compete.
The iCTF editions from 2003 to 2007 were similar to
the DefCon CTF: the participants had to protect and attack a virtualized system containing vulnerable services [20].
In 2008 and 2009, two new designs were introduced: in
2008, the competition was designed as a “treasure hunt,”
where the participants had to sequentially break into a series of hosts; in 2009, the competition focused on drive-bydownload attacks, and the targets were a large pool of vulnerable browsers [2]. The iCTF inspired other educational
hacking competitions, e.g., CIPHER [12] and RuCTF [18].
Recently, a diﬀerent type of competition has received a
signiﬁcant amount of attention. In the Pwn2Own hacking
challenge [13] participants try to compromise the security
of various up-to-date computer devices such as laptops and
smart phones. Whoever successfully compromises a device,
wins the device itself as a prize. This competition is solely
focused on attack, does not have an educational focus, and
does not allow any real interaction amongst the participants
who attack a single target in parallel.
Another interesting competition is the Cyber Defense Exercise (CDX) [1,10,14], in which a number of military schools
compete in protecting their networks from external attackers. This competition diﬀers from the UCSB iCTF in a
number of ways. First, the competition’s sole focus is on
defense. Second, the competition is scored in person by human evaluators who observe the activity of the participants,

2.2

Cyber Situational Awareness

Cyber Situational Awareness (CSA) is an extension of
traditional Situational Awareness (SA) to computer networks. The idea behind SA is that by analyzing the surrounding environment and putting perceived events into the
context of the current mission, it is possible to improve
decision-making. In the cyber-world, the concept of Situational Awareness includes the concept of mission awareness,
which is the analysis of network events with respect to the
mission being carried out by a particular organization.
One of the most important ideas behind CSA is that not
all attacks have the same impact. The relevance of an attack
is determined by the importance of the target with respect
to a speciﬁc mission and a speciﬁc moment in time. For
example, an attack against an FTP server could be harmless
if the server is not a necessary component for the currently
executing mission(s) in the next, say, eight hours, because
within that time frame the server could be ﬁxed/cleaned
and it could be available when needed. Instead, consider
an attack against a VoIP router when a strategic meeting
must use that particular piece of infrastructure. The attack
will directly impact the mission being carried out, and might
impose delays or cause the mission to fail.
There are several challenges in CSA. First of all, it is difﬁcult to correctly model missions. In many cases, organizations and companies are not even aware of their cybermissions. Usually, identifying cyber-missions is easier in environments where repetitive tasks are performed cyclically.
For example, banks have well-deﬁned missions with tasks
that must be carried out in speciﬁc sequences (e.g., closing
balances, reconcile balance sheets) and must be performed
within a certain time limit (e.g., midnight of the current
day). Another example is military systems, where cycles
of observation/analysis/operation phases are carefully followed, with precise time frames and clear dependencies.
In all these cases, one must choose a particular format
to precisely describe a mission. A simple solution is to use
Gantt charts [3], which clearly represent the duration and
dependency of diﬀerent tasks. For the cyber-missions described in this paper, we used Missionary, a Petri net [11]
based formalism we created which extends the basic Petri
net model with timing and code fragments associated with
transitions and states. In this formalism, the tasks are represented by the states of the Petri net. A token in a state
characterizes an active instance of the task. A task terminates when a token is removed from the corresponding state
as a side-eﬀect of the ﬁring of a transition. Analogously, a
task starts when a token is created in a state as the sideeﬀect of the ﬁring of a transition. Peterson [11] has a detailed
description of Petri nets and their extensions.
Another challenge in CSA is to represent the dependency
between cyber-missions and both the human actors and as-



sets involved in the missions [5]. For the sake of simplicity
we do not address the former. For the latter, we used Missionary’s service composition formalism, which allows the
association of diﬀerent types of service compositions to a
task in a mission. In a nutshell, the formalism allowed us
to specify services that were associated with a state in the
Petri net, thus creating an association between a mission
task and the services necessary to carry out the task.

3.

ﬁrst by bribing Litya’s administrators, then by keeping their
VMware image connected to a “mothership.” If the teams
generated an intrusion detection system alert, they were
blocked from the network for a ﬁxed amount of time.

3.3.1

2010 iCTF

The iCTF competition was held on December 3rd, 2010,
and lasted from 09:00 until 17:00, PST.

3.1

Pre-competition setup

Registration for the iCTF competition began a month before the start date. Attempting to alleviate the VPN connection problems that can occur on the day of the competition,
we distributed a VMware [21] image along with VPN connection instructions to each team 11 days before the competition. The VMware image was meant as an example of the
type of VMware image that would be used for the competition. We took particular care in making sure that the teams
solved their connectivity problems well in advance, so that
they could focus on the competition.

3.2

3.3.2

Firewall and IDS

A substantial innovation introduced in the iCTF was creating an intrusion prevention system (IPS) by connecting the
Snort [16] intrusion detection system to the ﬁrewall. If Snort
detected an intrusion attempt (alert) from a team on trafﬁc directed towards Litya’s services, the oﬀending team was
shut oﬀ from the network for ten minutes. The team would
either have to wait until connectivity was allowed again or
spend money bribing Litya’s network administrators to gain
access to the network for a certain amount of time. The
teams had full knowledge of the Snort version and conﬁguration, thus, they could predict if their traﬃc would generate
an alert. Connecting Snort to the ﬁrewall forced the teams
to come up with novel ways to circumvent an IDS.

Story

The theme of the iCTF competition was “Mission awareness in state-sponsored cyberwar.” The following text was
given to the teams the day before the competition:

3.3.3

The country of Litya has become a major center
for illegal activities of all kinds. The country is
ruled by the ruthless dictator Lisvoy Bironulesk,
who has pioneered the use of large malware infrastructures in order to support Litya’s economy. Recently, he has claimed that Litya has “a
botnet in every country.”

Botnet

Bribing Litya’s network administrators for access opened
up the network for a limited amount of time (proportional to
the amount of money used to bribe). To remain connected
to the network, the teams needed to run a bot, which we
provided 2 hours before the competition. This bot would
connect to a mothership every 30 seconds and while the bot
was connected to the mothership it would drain money from
the team at a rate of 6 money per minute. As long as the bot
remained connected to the mothership, the team had money,
and the team didn’t generate any Snort alerts, they could
access the services. The two means of connecting to the
network (bot connection or bribing) forced teams to make
strategic decisions about when to connect, when to attack,
how to attack, and when to bribe (spend money). These
strategic decisions added another dimension to the iCTF
competition: Teams had to decide the proper allocation of
money (bot connection or bribing) to maximize their access
to the network and thus maximize points.
Like a real-world bot, these machines were “compromised”
and had 3vilSh3ll [15], a backdoor bind connect, running on
port 8000. This allowed anyone to connect on port 8000,
supply the password: hacked, and obtain a root shell. The
idea was to encourage teams to be careful about their ﬁrewall, and force them to defensively select what traﬃc they
allowed into their network.

His complete disregard for international laws,
his support of banking fraud and phishing scams,
together with his well-known taste for underage
girls has ﬁnally brought the attention of the international community into his shady dealings.
Enough is enough. Now, the aﬀected nations
have decided to strike back. Spies who inﬁltrated
Litya’s corrupt administration have leaked plans
of the most critical missions carried out in the
country. These plans appear to describe the various activities of each mission, their ordering and
timing, and their dependency on particular services.
In this scenario, each team represented a country with the
common goal of dismantling Litya’s infrastructure, thus ending Bironulesk’s reign. In addition to this text, the teams
were given a number of images that described the various
“missions” carried out by Litya. One of the missions is shown
in Figure 1.

3.3

Scoring

There were two types of scores: money and points. The
team with the highest points won the competition, thus
points were more important than money. Points were acquired by exploiting services at the correct time. However,
if a team did not have any money, they would be shut oﬀ
from the network and not be able to score any points. In
addition to starting the competition with 1000 in money and
zero points, each team earned money by solving challenges.

3.3.4

Competition Description

Challenges

To gain money to bribe the Litya network administrators,
as well as allow the mothership to steal money and remain
connected to the network, teams needed to solve challenges.
We created 33 challenges to provide multiple ways to earn
money, but also to oﬀer opportunities to test and improve

At a high level, the competition was designed to force the
teams to exploit services at speciﬁc times, when they are
most needed by Litya, thus emulating a Cyber Situational
Awareness scenario. The teams had to access the services,



Service
LityaBook
LityaHot
icbmd
StormLog
StolenCC
SecureJava
IdreamOfJeannie
WeirdTCP
MostWanted
OvertCovert

Vulnerability
Cross-Site Scripting
Session Fixation
Foam Rocket Firing
Oﬀ-By-One Overﬂow
Perl’s open abuse
Broken Crypto
Java JNI Oﬀ-By-One Error
TCP IP Spooﬁng
SQL-Injection
Format String

Table 1: Brief description of vulnerable services.
the network became stuck (no eligible transitions) then the
network was reset.
For example, running the CARGODSTR mission, Figure 1 and Figure 2a, would involve ﬁrst placing a token in
its “Start” position. As T1 is the only eligible transition to
ﬁre, it is chosen, and this information is leaked to the teams.
The token moves from “Start” to “Ship.” Because services
S8 and S3 are associated with the “Ship” state, and it has a
token, they are active. After a pause of one to two minutes,
the next time-step occurs. Once again, there is only one
eligible transition, T2. It is chosen, and the token on “Ship”
moves to “Validate Cargo.” Now, services S8 and S3 are no
longer active, but service S1 becomes active. After another
pause, the process repeats, but with two eligible transitions,
T4 and T5. One of these is randomly chosen, say T5, and the
token moves. This process repeats until the end of the competition. A visualization of the execution of all four missions
throughout the iCTF competition is available1 .
From this example of the execution of the CARGODSTR
mission, the teams received the sequence: T1 T2 T5. With
only this information, they had to reverse engineer the state
of the mission to ﬁnd out which services were active. The
teams would then attack only the active services. In the
CARGODSTR mission, this is simple because the transitions are unique, however this is not the case for all the
missions, as shown in the SEDAFER mission (Figure 2d).

Figure 1: CARGODSTR mission that was distributed to the
teams.
diﬀerent skills, from cryptanalysis to forensics, program and
network analysis.

3.3.5

Scoreboard

In a capture the ﬂag competition, a scoreboard showcasing the current status and ranking of each team is vital.
For the iCTF competition, we also needed to show the connection status of each team; if they were connected to the
network, and why they were disconnected: Either from an
IDS alert, lack of a bot connection, or lack of money. The
scoreboard also showed the history of each team’s money
and points. The scoreboard is a very important piece of
the infrastructure, because it provides immediate feedback
to the teams about the success (or failure) of their attacks.
Unfortunately, we had some glitches in our scoreboard that
we will discuss in Section 5.

3.3.6

Missions

3.3.7

The day before the competition each team received an
email containing a link to four pictures. Each picture contained a description of a Cyber Situational Awareness mission, in the form of a hand-drawn Petri net. Figure 1 shows
one of these missions, the CARGODSTR mission. In the
Petri net, all of the transitions were named (although not
unique across the missions and even within some missions),
as were most of the states. Some of the states were associated with one or more of the 10 services (S0-S9). For
example, the “Receive” state in the lower right of Figure 1 is
associated with services S7 and S9. The four Petri net missions given to the teams are graphically shown in Figure 2.
A service that we ran executed the Petri nets by inserting
a token in each of the “Start” states, and running each Petri
net separately. At each time-step, for each of the missions,
one of the eligible transitions (a transition where all inputs
had tokens) was randomly chosen to ﬁre. Then, the token
was consumed on all the inputs to the chosen transitions,
and a token was placed on all the outputs. The four chosen
transitions (one from each mission) were leaked to the teams
after each time-step. Then, after each mission was executed,
the service suspended for a random amount of time between
one and two minutes, and the process repeated until the
end of the competition. If a token was in an “End” state, or

Flags

A ﬂag was a sequence of hexadecimal values preﬁxed with
FLG and was speciﬁc to a service. Flags were not directly
accessible: a service must be compromised to access the associated ﬂag. Therefore, ﬂags are used by the participants
as proof that, at a certain time, they were able to compromise a speciﬁc service. On each step of the service that
executed the Petri nets, a new ﬂag speciﬁc to each service
was distributed to the corresponding service. Each ﬂag contained (cryptographically) the service that it belonged to,
the state of the service (active or not), and a timestamp signifying when the ﬂag was created. Thus, when a ﬂag was
submitted by a team, the ﬂag submission service had all the
necessary information to determine the ﬂag’s validity (ﬂags
were valid for 5 minutes).

3.3.8

Vulnerable Services

There were 10 services in the iCTF, each service could be
exploited only once per Petri net execution round; exploiting
a service when it was not active resulted in an equal amount
of negative points. Thus, to win the competition it was
essential to understand and follow the missions. Table 1
1



http://ictf.cs.ucsb.edu/data/ictf2010/final.gif





 % &'% (' #"



#%%# *







	

! '%#


$'%#

" &'% (' #"

 $





 )

 !!

"





 &'% (' #"


'%'

(a) CARGODSTR



# $$


%#%

%%
"&$% 


! #%
#% 








 # $$



	
$&$$ '&% 







$%#&% 



(b) COMSAT








%" '##
#$#"&'



$$ %!

$$

	

&

&"$$






$"$



$

 

"  #%$ '##


(c) DRIVEBY


 &#*+ *




 $,-




+- %*




&(+


&"(*( *

 %&!+)


&&$
)*( -


 $'*,

%



	

&( &)
	







&( ,

&#+*+% 



,( *

&*&)+(&#%



&("%


(&&(&)*



# * (


*(*

(d) SEDAFER
Figure 2: Graphical representation of the missions given to the teams. The teams were actually given formats similar to
Figure 1. Not shown here are the associations of the services to states in the Perti nets.



brieﬂy summarizes the services. We direct the interested
reader to Appendix A for an extended description of the
services.

4.

DATA ANALYSIS

In addition to being an excellent learning exercise for the
teams involved, a security competition, if properly designed,
can be a great source of data that is diﬃcult to obtain in
other contexts. In the iCTF competition, we created a game
scenario to generate a Cyber Situational Awareness dataset.
Traﬃc collected during a security competition can be easier to analyze than real-world traﬃc, because there is more
information about the network and participants in the competition. For example, all teams are identiﬁed, the vulnerable services are known, and there is no “noise traﬃc.”
Of course, a dataset collected in such controlled conditions
also suﬀers from a lack of realism and is limited in scope.
Nonetheless, the data collected during this competition is
the ﬁrst publicly available dataset that allows researchers to
correlate attacks with the missions being carried out.
The iCTF competition generated 37 gigabytes of network
traﬃc and complete information about services broken, challenges solved, ﬂags submitted, bribes paid, IDS alerts, and
bot connections. This data is made freely available2 .
As this is the ﬁrst Cyber Situational Awareness dataset,
many possibilities exist for its use in Situational Awareness
research. One example would be using the dataset to train
a host-based CSA intrusion detection system that could use
more restrictive rules for a rule-based system (or tighter
thresholds in an anomaly-based system) when a service is
critical to a mission. One can also think of extending a
host-based IDS to a network CSA intrusion detection system that understands not only the criticality of the services,
but also their dependencies and relationships. Another example is the visualization of a network’s activity with CSA
in mind that helps a system administrator know which services are currently critical and which will become critical
soon, helping them defend their network.
The ﬁrewall, bribing, bot, and money/points system can
be viewed in a game theory light. The teams had to decide
on the best way to allocate a scarce resource (money) to access the network and potentially win the game. The teams
could perform any combination of bot connection and/or
bribing to access the network. Further research could investigate how the choice of resource allocation aﬀected each
team’s ﬁnal result.

4.1

Total

Active

Inact.

% Inact.

Teams

MostWanted
OvertCovert
IdreamOf.
WeirdTCP
LityaBook
icbmd
StolenCC

680
97
49
24
16
5
1

562
82
37
23
12
3
0

118
15
12
1
4
2
1

17
15
24
4
25
40
100

38
6
6
2
3
1
1

Flags/Team

17.895
16.167
8.167
12
5.333
5
1

Table 2: Flags submitted per service.
service. Many of the 39 teams submitted multiple ﬂags, indicating that they understood the Petri net mission model.
At 17:00, “Plaid Parliament of Pwning” (PPP) of Carnegie
Mellon University, took ﬁrst place with 24,000 points. PPP
submitted a total of 93 ﬂags, with only 3 inactive ﬂags
(thus generating negative points), by compromising IdreamOfJeannie, MostWanted, and OvertCovert. Because PPP
was able to compromise three services as well as understand
the Petri net model (as evidenced by the submission of only
three negative ﬂags), they won ﬁrst place.
Overall the teams exploited 7 of the 10 services: icbmd,
IdreamOfJeannie, LityaBook, MostWanted, OvertCovert,
StolenCC, and WeirdTCP. We believe this is because we
underestimated the diﬃculty of the other 3 services. SecureJava and StormLog required a complex, multi-step process
that proved too diﬃcult for the teams to exploit. The teams
also had trouble understanding the steps involved to exploit
the session ﬁxation vulnerability in LityaHot.
Table 2 describes the number of ﬂags submitted for each
service. The “Total” column is the total number of ﬂags submitted for the service, “Active” and “Inact.” are the number
of ﬂags that were submitted when a service was active or
inactive. “% Inact.” is the percent of ﬂag submissions when
the service was inactive. “Teams” shows the number of teams
that submitted ﬂags for the service and “Flags/Team” shows
the average number of ﬂags submitted per team.
MostWanted was the most exploited service, with 680 total ﬂags submitted, followed by OvertCovert, with 97 ﬂags
submitted. It is clear that we did not estimate the diﬃculty
of the services correctly, and, as evidenced by the number of
teams that broke it, MostWanted was the easiest. Because
the teams did not know the diﬃculty of the services, some
luck is involved when teams decide which service to analyze
ﬁrst.
When we decided to create a complex competition, we
knew that not every team would have the skills, experience,
and luck to exploit a service and understand the Petri net
mission model. However, we included 33 challenges in the
competition of varying levels of diﬃculty and needing various skills to solve. We knew from past experience that even if
a team couldn’t exploit a service or understand the Petri net
model of the missions, they would at least learn from (and
enjoy) solving challenges. In fact, 69 out of 72 teams solved
at least one challenge. Thus, even if a team was unable to
exploit a service, they solved a challenge and hopefully had
fun or learned something while competing in the iCTF.

Description of Results

One problem with designing and implementing a novel
competition is that teams may not understand the rules.
This was a concern during the design of the iCTF competition. We worried that the novel aspects of the competition,
especially the Petri net mission model, would be too complex for the teams to understand. However, when the ﬁrst
ﬂags were submitted at 13:29, and subsequently when teams
started submitting ﬂags only for active services, it became
apparent that many teams understood the competition.
Of the 72 teams, 39 submitted a ﬂag, with 872 ﬂags submitted in total. 48% may seem like a low number, however
this means that almost half the teams broke at least one
2

Service

4.2

Network Analysis

A beneﬁt of designing a security competition is the ability
to create an environment that allows for the testing of models and theories. By focusing the iCTF on Cyber Situational
Awareness, we were able to create and evaluate Situational
Awareness metrics. These metrics are applicable to many as-

http://ictf.cs.ucsb.edu/data/ictf2010/



pects of CSA. We introduce toxicity and eﬀectiveness, which
are explained in the rest of this section.
First, we deﬁne three functions: C(s, t), A(a, s, t), and
D(s, t), each with a range of [0, 1]. Every function is speciﬁc
to a service, s, and A(a, s, t) represents an attacker, a.
C(s, t) represents how critical a service, s, is with respect
to time for a speciﬁc mission or set of missions. A value of
1 means that the service is very critical, while 0 means that
the service is not critical.
A(a, s, t) represents an attacker’s, a, activity with respect
to a service, s, throughout time. The value of the function is
the perceived risk to the mission associated with the service.
In most cases, the function has a value of 1 when an attack
occurs and a value of 0 when there is no malicious activity.
However, other, more complex models could be used (e.g.,
the type of attack could be taken into account).
D(s, t) represents the damage to any attacker for attempting an attack on a service, s, at a given time, t. This function models the fact that every time an attack is carried
out, there is a risk to the attacker, e.g., an intrusion detection system might discover the attack, the person using the
targeted machine/service might notice unusual activity, etc.
We wish to deﬁne a metric, called toxicity, that captures
how much damage an attacker has caused to a service over
a time frame. Intuitively, it is the total amount of havoc the
attacker has caused to the mission (or missions) associated
with a service. Toxicity is calculated by ﬁrst subtracting the
damage to an attacker, D(s, t), from the criticality of the
service, C(s, t). The resulting function, with a range of [-1,
1], describes at each point in time how much any attacker
can proﬁt by attacking at that moment. A negative value
indicates that the attacker should not attack at that time.
The previously calculated function is general and has no
bearing on a particular attacker. To calculate the damage
caused by a speciﬁc attacker over time, we take the previously calculated function, C(s, t) − D(s, t), and multiply
it by A(a, s, t). The resulting function, with a range of [1, 1], shows how much damage a speciﬁc attacker caused
to a given service. To calculate toxicity from this function,
for a given time interval, t1 to t2 , we take the integral of
A(a, s, t) ∗ (C(s, t) − D(s, t)) with respect to time. Equation (1) shows the calculation of the toxicity metric.
Toxicity is a measure for how much damage an attacker
has caused to a given service, and can compare two attackers
against the same service to see who did the most damage,
however, it is speciﬁc to one service, and thus is useless as
a comparison between a single attacker attacking multiple
services or two attackers attacking diﬀerent services. We
propose eﬀectiveness as a measure of how close an attacker
is to causing the maximum toxicity possible. Intuitively, it is
the ratio of the toxicity caused by an attacker to the toxicity
an optimal attacker would cause. We deﬁne an optimal attacker as an attacker who attacks whenever C(s, t) - D(s, t)
is positive, and this is shown in Equation (2). By substituting the optimal attacker in Equation (1) for A(a, s, t),
we obtain the formula for maximum toxicity, given in Equation (3). Taking the ratio of toxicity to maximum toxicity
gives eﬀectiveness, shown in Equation (4).
Toxicity, eﬀectiveness, and C(s, t), A(a, s, t), and D(s, t)
can be used in future Cyber Situational Awareness research.
By using the ideas presented here, an IDS could predict the
behavior of an optimal attacker. Other tools could enable
a network defender to perform “what-if” scenarios, seeing

what would happen by increasing the damage to an attacker
(e.g., by getting a new IDS), versus decreasing the criticality
of the service (e.g., by getting a new server to perform the
same function).
Toxicity(a, s, t1 , t2 ) =
 t2
A(a, s, t) ∗ (C(s, t) − D(s, t)) dt

(1)

t1

OptimalAttacker (s, t) =

1
if C(s, t) − D(s, t) > 0
0
otherwise

(2)

MaxToxicity(s, t1 , t2 ) =
 t2
OptimalAttacker (s, t) ∗ (C(s, t) − D(s, t)) dt (3)
t1

Eﬀectiveness(a, s, t1 , t2 ) =
Toxicity(a, s, t1 , t2 )
MaxToxicity(s, t1 , t2 )

(4)

The deﬁnitions of toxicity and eﬀectiveness are general
and apply to any arbitrary functions C(s, t), A(a, s, t), and
D(s, t). However, we constructed the iCTF competition so
that we could measure and observe these functions and ensure they are valid metrics. We expected the higher ranked
teams to show high toxicity and eﬀectiveness for the services
they broke.
The criticality, C(s, t), of each service was deﬁned in the
following way: the function takes the value 1 when the service is active, and 0 when the service is inactive. Figure 3
shows the criticality graph for the most exploited service:
MostWanted. When the function has a value of 1, one of
the missions is in a state associated with the MostWanted
service, otherwise the function has a value of 0. Note that
for these and all the rest of the graphs of the competition,
the X-axis is time, and starts at 13:30 PST, when the ﬁrst
ﬂag was submitted, and ends at 17:00 PST, which was the
end of the competition.
In our analysis, we deﬁne the damage to the attacker,
D(s, t), as the complement of the criticality graph, because
if an attacker attacked a service when it was not active, they
would get an equal amount of negative points. The damage
graph alternates between 0 and 1, becoming 1 when the criticality is 0 and 0 when the criticality is 1. In our analysis, the
criticality and damage functions are related as a byproduct
of our design; however our deﬁnitions of toxicity and eﬀectiveness do not depend on this; criticality and damage can
be arbitrary and independent functions.
In order to calculate the toxicity of Plaid Parliament of
Pwning against the various services, we must ﬁrst calculate
A(a, s, t) ∗ (C(s, t) − D(s, t)) (note that this function has a
range of [-1, 1]. Negative values in this context denote ﬂags
submitted when a service was inactive). This is shown in
Figure 4 for the service MostWanted, and Figure 5 for the
service OvertCovert. As can be seen in Figure 4, PPP did
not attack at the incorrect time for the MostWanted service, but submitted several incorrect ﬂags for OvertCovert,
as evidenced by the negative values in Figure 5.
Toxicity is calculated by taking the integral of this function between 13:30 and 17:00 PST. However, since the time
in-between each ﬂag change is a random value between 60
and 120 seconds, and a team is able to exploit the service
only once per ﬂag change, we simpliﬁed the time between



1

0
13:30

14:00

14:30

15:00

15:30

16:00

16:30

17:00

16:30

17:00

Time During Competition
Figure 3: C(s, t) of the service MostWanted.

1
0
-1
13:30

14:00

14:30

15:00

15:30

16:00

Time During Competition
Figure 4: A(a, s, t) ∗ (C(s, t) − D(s, t)) of team PPP against the service MostWanted.
ﬂags as 1, which returned a round number for the toxicity
metric. In the general case, however, the amount of time
a service is critical is very important for calculating toxicity and should not be oversimpliﬁed. Because the criticality
of our services changed at discrete intervals, we are able to
make this simpliﬁcation without adversely aﬀecting our results.
Table 3 shows the toxicity and eﬀectiveness of the top
5 teams for each of the services that were successfully exploited. The results are as we expected; many of the most
eﬀective teams placed high in the ﬁnal rankings. The ﬁrst
place team, PPP, team #113, was not only the most effective for three diﬀerent services: IdreamOfJeannie, MostWanted, and OvertCovert, but, also, with 65% eﬀectiveness on MostWanted, had the highest eﬀectiveness of any
team. PPP’s dominance is apparent because they did not
just break three services, but they were also highly eﬀective.
The second place team, 0ld Eur0pe (team #129), was the
second most eﬀective at IdreamOfJeannie and third most
eﬀective at MostWanted.

5.

In the past, having a complex competition frustrated many
teams and caused them to spend a substantial amount of
time trying to ﬁgure out the competition instead of actually
competing. To combat this, we released details about the
structure of the game, the Petri net models of the missions,
and the Snort conﬁguration in advance. We hoped that this
would give teams the opportunity to come to the competition well-prepared. Another advantage in giving advance
notice is that it rewards teams who put in extra time outside
of the eight hours of the competition. This is important, as
the larger part of the education process is actually associated
with the preparation phase, when students need to become
familiar with diﬀerent technologies and brainstorm possible
attack/defense scenarios.
Another positive feedback we received through informal
communication was that the theme of the competition was
clear and consistent. The iCTF competition has always had
a well-deﬁned background story, which supports understanding and provides hints on how to solve speciﬁc challenges.
People explicitly appreciated the eﬀort put into creating a
consistent competition environment and complained about
competitions that are simply a bundle of vulnerable services
to exploit.
From the comments of the players, it was clear that a
substantial amount of eﬀort was put into preparing and developing the right tools for the competition. This is one of
the most positive side-eﬀects of the participation in this kind
of live exercises. Having to deal with unknown, unforeseen
threats forces the teams to come up with general, conﬁgurable security tools that can be easily repurposed once the
focus of the competition is disclosed. The continuous change
in the iCTF design prevents the “overﬁtting” of such tools
to speciﬁc competition schemes.
In general, through the past three years we found that
radical changes in the competition’s design helped leveling
the playing ﬁeld. Although the winning teams in the 2008,

LESSONS LEARNED

For this edition of the iCTF competition, we tried to capitalize on our previous experience by learning from mistakes
of years past. However, we may hope to the contrary, we
are still human: we made some mistakes and learned new
lessons. We present them here so that future similar competitions can take advantage of what worked and avoid repeating the same mistakes.

5.1

What Worked

The pre-competition setup worked extremely well. Having
the teams connect to the VPN and host their own VMware
bot image was helpful in reducing the support burden on the
day of the competition, where the time is extremely limited.



1
0
-1
13:30

14:00

14:30

15:00

15:30

16:00

16:30

17:00

Time During Competition
Figure 5: A(a, s, t) ∗ (C(s, t) − D(s, t)) of team PPP against the service OvertCovert.
Service
icbmd
icbmd
IdreamOfJeannie
IdreamOfJeannie
IdreamOfJeannie
IdreamOfJeannie
IdreamOfJeannie
LityaBook
LityaBook
LityaBook
LityaBook
StolenCC
StolenCC
StolenCC

Team
126
124
113
129
123
111
128
149
166
150
137
123
105
152

Toxicity
3
1
14
12
10
2
-6
8
5
-1
-2
1
1
0

Eﬀectiveness
0.03896
0.01298
0.23728
0.20338
0.16949
0.03389
-0.10169
0.11428
0.07142
-0.01428
-0.02857
0.02040
0.02040
0.0

Service
MostWanted
MostWanted
MostWanted
MostWanted
MostWanted
OvertCovert
OvertCovert
OvertCovert
OvertCovert
OvertCovert
WeirdTCP
WeirdTCP

Team
113
114
129
105
152
113
131
123
117
127
156
105

Toxicity
42
40
36
34
30
36
16
10
9
2
13
6

Eﬀectiveness
0.65625
0.625
0.5625
0.53125
0.46875
0.48648
0.21621
0.13513
0.12162
0.02702
0.23214
0.10714

Table 3: Top 5 most eﬀective teams per service.
2009, and 2010 editions were still experienced groups, teams
of ﬁrst-time competitors placed quite high in the ranking.
This was possible because we intentionally did not disclose
in advance to the teams the nature of these new competitions. Many “veteran” teams expected a standard CTF and
were surprised to learn that this was not the case. Of course,
it is hard to keep surprising teams, as designing new competitions requires a substantial amount of work. However, it
is arguable that this type of competition is inherently easier
for novice teams to participate in.
Finally, the competition generated a unique, useful dataset
that can be used to evaluate cyber situation awareness approaches. This aspect of security competitions cannot be
overemphasized, as a well-designed data-capturing framework can provide a wealth of useful data to security researchers.

5.2

Once the change was made, at 13:30 PST, teams started
submitting ﬂags, and the rest of the competition went fairly
smoothly.
As the scoreboard is the only way for teams to understand the current state of the game, making the scoreboard
accurately reﬂect the status of the competition was essential. However, each piece of the competition’s infrastructure
was developed and tested independently. Knowing that getting the ﬁrewall, mothership, and Snort systems working
properly was very important, those parts of the functionality were heavily tested in isolation. However, the interaction
of these systems with the scoreboard was not tested before
the competition. Thus, during the competition we discovered that the reasons given to teams for being blocked on
the scoreboard were not correct, and in some instances the
connection status of some teams were incorrect. Due to one
of the developers being ill, it took us most of the competition
to completely resolve this issue. While we were ﬁxing the
issue, we communicated to teams that to test their network
connectivity, they could simply try connecting to one of the
services. In the future, we will be testing our infrastructure
as a whole, including important pieces like the scoreboard.
One issue with creating a complex and novel competition
is that some teams might not “get” the competition. This
can be on a number of levels, perhaps the team has never
heard of Petri nets or could not exploit any of the services.
This puts them at an extreme disadvantage in the rankings,
as they cannot score any points. This was the case for 33
teams. However, for the 39 teams that submitted ﬂags, a
novel competition challenged them to create new solutions
and tools, learning in the process. Ultimately, it is up to the
competition administrators to balance novelty, complexity,
and fairness.

What Did Not Work

LityaLeaks, the part of the infrastructure used to distribute the ﬁred transitions of the Petri nets, as well as
various hints and clues about services and challenges, was
an integral part of our design (and the name ﬁt in nicely
with the theme). However, using a base MediaWiki [9] installation on a virtual machine with 256 MB of RAM was
a mistake. As soon as the competition started, LityaLeaks
was brought to a crawl due to the amount of traﬃc created
by the teams.
Having LityaLeaks down was very problematic, because if
teams couldn’t see which transitions were ﬁring then they
couldn’t submit ﬂags. Eventually, a static mirror of LityaLeaks was brought up. Because of this, we had to change the
Petri net software on the ﬂy to update a publicly accessible
ﬁle with the transition ﬁrings instead of using LityaLeaks.



5.3

What Worked?

[3] W. Clark. The Gantt chart: A working tool of
management. New York: Ronald Press, 1922.
[4] C. Cowan, S. Arnold, S. Beattie, C. Wright, and
J. Viega. Defcon Capture the Flag: defending
vulnerable code from intense attack. In Proceedings of
the DARPA Information Survivability Conference and
Exposition, April 2003.
[5] A. D’Amico, L. Buchanan, J. Goodall, and
P. Walczak. Mission Impact of Cyber Events:
Scenarios and Ontology to Express the Relationships
between Cyber Assets, Missions and Users. In
Proceedings of the International Conference on
Information Warfare and Security, Dayton, Ohio,
April 2010.
[6] D. R. Hipp. Sqlite. http://www.sqlite.org/, 2010.
[7] Justin.tv. http://justin.tv/.
[8] S. Liang. Java Native Interface: Programmer’s Guide
and Reference. Addison-Wesley Longman Publishing
Co., Inc., Boston, MA, USA, 1st edition, 1999.
[9] Mediawiki. http://www.mediawiki.org/.
[10] B. Mullins, T. Lacey, R.Mills, J. Trechter, and
S. Bass. How the Cyber Defense Exercise Shaped an
Information-Assurance Curriculum. IEEE Security &
Privacy, 5(5), 2007.
[11] J. Peterson. Petri Nets. ACM Computing Surveys,
9(3), September 1977.
[12] L. Pimenidis. Cipher: capture the ﬂag.
http://www.cipher-ctf.org/, 2008.
[13] Pwn2own 2009 at cansecwest. http://dvlabs.
tippingpoint.com/blog/2009/02/25/pwn2own-2009,
March 2009.
[14] W. Schepens, D. Ragsdale, and J. Surdu. The Cyber
Defense Exercise: An Evaluation of the Eﬀectiveness
of Information Assurance Education. Black Hat
Federal, 2003.
[15] Simpp. 3vilsh3ll.c. http://packetstormsecurity.
org/files/view/64687/3vilSh3ll.c.
[16] Snort. http://www.snort.org/.
[17] B. Stone-Gross, R. Abman, R. Kemmerer, C. Kruegel,
D. Steigerwald, and G. Vigna. The Underground
Economy of Fake Antivirus Software.
[18] The HackerDom Group. The ructf challenge.
http://www.ructf.org, 2009.
[19] G. Vigna. Teaching Hands-On Network Security:
Testbeds and Live Exercises. Journal of Information
Warfare, 3(2):8–25, 2003.
[20] G. Vigna. Teaching Network Security Through Live
Exercises. In C. Irvine and H. Armstrong, editors,
Proceedings of the Third Annual World Conference on
Information Security Education (WISE 3), pages
3–18, Monterey, CA, June 2003. Kluwer Academic
Publishers.
[21] VMware. http://www.vmware.com/.

Putting a backdoor into the bot VM that we distributed
to the teams was something that we implemented ﬁve hours
before the distribution of the VM. Something that we saw
as funny turned out to have serious implications. One team
came to us and said that they had an exploit to reduce every team’s money to zero, eﬀectively removing everyone else
from the competition. Using the backdoor, they could bribe
the Litya administrators as the team’s bot, thus draining all
of the team’s money. We asked them not to do this, as it was
unsporting to completely shut oﬀ most team’s access to the
services, and ﬁxed this avenue of attack. We also alerted the
teams to the existence of a backdoor on their VMs. Later in
the competition, a team came to us complaining that their
points kept decreasing. Looking into it, a team was exploiting a service, and submitting all the inactive ﬂags (worth
negative points) through another team’s compromised bot.
The team that this happened to came in last place (with
-3300 points).
The backdoor provided some interesting (and funny) situations, however it came at a price. The last place team
felt that this was an unsporting thing to do and were rightly
upset over their last-place standing. We ruled that, since
we had given notice about the backdoor, and given the extremely easy ﬁx (ﬁlter the traﬃc from other teams), the outcome was acceptable. However, this situation did highlight
an issue that these kind of “easter eggs” can produce: while
it may be exciting and interesting for the teams who discover
them, the more inexperienced teams who are not looking for
them and/or can’t ﬁnd them are put at a disadvantage. This
just increases the gap between the experienced and inexperienced.

6.

CONCLUSIONS

Live cyber-security exercises are a powerful educational
tool. The main drawback of these exercises is that they
require substantial resources to be designed, implemented,
and executed. It is therefore desirable that these exercises
provide long-lasting byproducts for others to use for security research. In this paper, we presented a unique, novel
design for a live educational cyber-security exercise. This
design was implemented and a competition involving almost
a thousand world-wide students was carried out in December
2010. We discussed the lessons learned, and we presented
the dataset we collected, which we believe is the ﬁrst public
dataset focused on Cyber Situational Awareness. We hope
that this dataset will be useful to other researchers in this
increasingly popular ﬁeld and that future security exercises
will yield interesting datasets.

7.

REFERENCES

[1] T. Augustine and R. Dodge. Cyber Defense Exercise:
Meeting Learning Objectives thru Competition. In
Proceedings of the Colloquium for Information
Systems Security Education (CISSE), 2006.
[2] N. Childers, B. Boe, L. Cavallaro, L. Cavedon,
M. Cova, M. Egele, and G. Vigna. Organizing Large
Scale Hacking Competitions. In Proceedings of the
Conference on Detection of Intrusions and Malware
and Vulnerability Assessment (DIMVA), Bonn,
Germany, July 2010.



APPENDIX
A.

WeirdTCP was a C service that acted as a ﬁle server with
a trust relationship with a speciﬁc IP address. A blind
TCP spooﬁng attack against the service pretending to be
the trusted IP address was required to ﬁnd the key. However, due to the VPN technology we were using, packets
could not be spoofed. A custom IP protocol RFC was given
to the teams, which introduced an IP option that could be
used to overwrite the source address of an IP packet. Thus
an attacker had to use the IP option to spoof the trusted
IP address, and, in addition, perform a sequence number
guessing attack, in order to provide the correct acknowledgment number during the TCP handshake. Once the TCP
connection was established, the attacker received the ﬂag.
MostWanted was a Python service with a SQLite [6] backend. The service hosted mugshots of various wanted “criminals,” and allowed a user to create or view mugshots. MostWanted had a stored SQL-injection vulnerability, which an
attacker had to exploit to access the ﬂag.
OvertCovert was a C-based service that allowed a user
to store and access encrypted data. An attacker had to
ﬁrst exploit a printf vulnerability (which disallowed %n) to
extract the encryption key. Then, an oﬀ-by-one error was
used to access the encrypted ﬂag. Using the key previously
obtained, the attacker could decrypt the ﬂag and exploit the
service.

VULNERABLE SERVICES

A brief description of the 10 services in the iCTF and the
vulnerabilities associated with it follows.
LityaBook was a social networking website, similar to Facebook. By creating an underage girl proﬁle, the attacker
would cause President Bironulesk to visit their proﬁle. They
could then use a Cross-Site Scripting attack to steal President Bironulesk’s browser’s cookie, which contained the ﬂag.
LityaBook also had a session ﬁxation vulnerability. The
authentication cookie contained the MD5 of the session ID.
Therefore, an attacker could lure a victim to log in with
a speciﬁc session ID, allowing an attacker to impersonate
the victim. This vulnerability could have been exploited by
using another website, LityaHot.
LityaHot was a website where young models posted links
to their pictures, waiting for casting agents to contact them.
Periodically, a member of President Bironulesk’s staﬀ, Femily Edeo, visited this site, clicking on links people had posted.
If the link was a LityaBook page, he logged in to check the
pictures. Thus an attacker could post a link on LityaHot,
leveraging the session ﬁxation vulnerability to log into LityaBook as Edeo and obtain the ﬂag.
icbmd was the ﬁrst iCTF service with perceptible eﬀects on
the real world. A USB foam rocket launcher was connected
to a control program, pointing in the direction of a physical target. A time-sharing mechanism was used to share
the missile launcher amongst the teams. Each team had a
visual clue of where the launcher was aiming, via a webcam mounted on the missile launcher with a live streamed
video to the Justin.tv on-line video streaming service [7].
The team currently controlling the missile launcher could
exclusively connect to the control and move the launcher’s
turret. An encoded version of the launch code was leaked to
the teams. After deciphering the code, the teams were able
to launch a missile. Once a team successfully hit the target,
the ﬂag was sent to them.
StormLog was a web application that displayed log ﬁles
generated by a fake botnet called “Storm.” This service had
a directory traversal vulnerability which allowed an attacker
to download a copy of the cgi-bin program. An attacker had
to exploit an oﬀ-by-one overﬂow in the cgi-bin program to
execute arbitrary code and obtain the ﬂag.
StolenCC was a web service that displayed text ﬁles containing credit card numbers. The cgi-bin program was written in Perl and contained a directory traversal vulnerability.
By inserting a null character into the filename parameter,
an attacker could bypass the program’s sanity checking and
open any ﬁle. Then, an attacker could use additional functionality of Perl’s open to execute any command, ﬁnding and
displaying the ﬂag.
SecureJava was a web service that used a Java applet to
perform authentication. An attacker needed to get past the
authentication to ﬁnd the ﬂag. This involved reverse engineering the encryption algorithm. Once understood, the
attacker leveraged a ﬂaw in the encryption algorithm to steal
the ﬂag.
IdreamOfJeannie was a Java service that collected credit
card information. Even though the bulk of the service was
written in Java, JNI [8] was used to include a function written in C, which contained an oﬀ-by-one error. The attacker
could utilize the oﬀ-by-one error to obtain the ﬂag.



Fear the EAR: Discovering and Mitigating
Execution After Redirect Vulnerabilities
Adam Doupé, Bryce Boe, Christopher Kruegel, and Giovanni Vigna
University of California, Santa Barbara

{adoupe, bboe, chris, vigna}@cs.ucsb.edu

ABSTRACT

1. INTRODUCTION

The complexity of modern web applications makes it difficult for developers to fully understand the security implications of their code. Attackers exploit the resulting security
vulnerabilities to gain unauthorized access to the web application environment. Previous research into web application
vulnerabilities has mostly focused on input validation flaws,
such as cross-site scripting and SQL injection, while logic
flaws have received comparably less attention.
In this paper, we present a comprehensive study of a relatively unknown logic flaw in web applications, which we call
Execution After Redirect, or EAR. A web application developer can introduce an EAR by calling a redirect method
under the assumption that execution will halt. A vulnerability occurs when server-side execution continues after the
developer’s intended halting point, which can lead to broken/insufficient access controls and information leakage. We
start with an analysis of how susceptible applications written
in nine web frameworks are to EAR vulnerabilities. We then
discuss the results from the EAR challenge contained within
the 2010 International Capture the Flag Competition. Finally, we present an open-source, white-box, static analysis
tool to detect EARs in Ruby on Rails web applications. This
tool found 3,944 EAR instances in 18,127 open-source applications. Finally, we describe an approach to prevent EARs
in web frameworks.

An increasing number of services are being offered online. For example, banking, shopping, socializing, reading
the news, and enjoying entertainment are all available on the
web. The increasing amount of sensitive data stored by web
applications has attracted the attention of cyber-criminals,
who break into systems to steal valuable information such
as passwords, credit card numbers, social security numbers,
and bank account credentials.
Attackers use a variety of vulnerabilities to exploit web
applications. In 2008, Albert Gonzalez was accused and
later convicted of stealing 40 million credit and debit cards
from major corporate retailers, by writing SQL injection
attacks [20, 30]. Another common vulnerability, cross-site
scripting (XSS), is the second highest-ranked entry on the
OWASP top ten security risks for web applications, behind
injection attacks like SQL injection [29]. Thus, SQL injection and XSS have received a large amount of attention
by the security community. Other popular web application
vulnerabilities include cross site request forgery (XSRF) [5],
HTTP parameter pollution (HPP) [3, 12], HTTP response
splitting [27], and clickjacking [2, 21].

Categories and Subject Descriptors
D.2.5 [Testing and Debugging]

General Terms
Security

Keywords
static analysis, web applications, execution after redirect

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CCS’11, October 17–21, 2011, Chicago, Illinois, USA.
Copyright 2011 ACM 978-1-4503-0948-6/11/10 ...$10.00.

In this paper, we present an in-depth study of a little-known
real-world web application logic flaw; one we are calling Execution After Redirect (EAR). An EAR occurs because of
a developer’s misunderstanding of how the web application
framework operates. In the normal workflow of a web application, a user sends a request to the web application. The
web application receives this request, performs some serverside processing, and returns an HTTP response. Part of
the HTTP response can be a notification that the client (a
web browser) should look elsewhere for the requested resource. In this case, the web application sets the HTTP
response code to 301, 302, 303, or 307, and adds a Location header [32]. These response codes instruct the browser
to look for the resource originally requested at a new URL
specified by the web application in the HTTP Location
header [31]. This process is known as redirection1 ; the web
application redirects the user to another resource.
Intuitively, one assumes that a redirect should end execution of the server side code; the reason is that the browser
immediately sends a request for the new location as soon as
the redirection response is received, and it does not process
the rest of the web application’s output. Some web frame1

In this paper, we consider only HTTP server-side redirection. Other forms of redirection, executed on the client, exist
such as JavaScript redirect or HTML meta refresh.

works, however, do not halt execution on a redirect. This
can lead to EAR vulnerabilities.
Specifically, an EAR can be introduced when a web application developer writes code that issues an HTTP redirect
under the assumption that the redirect will automatically
halt execution of the web application. Depending on the
framework, execution can continue after the call to the redirect function, potentially violating the security properties of
the web application.
We define halt-on-redirect as a web framework behavior where server-side code execution halts on a redirect,
thus preventing EARs. Unfortunately, some languages make
halt-on-redirect difficult to implement, for instance, by not
supporting a goto-type statement. Therefore, web frameworks differ in supporting halt-on-redirect behavior. This
difference in redirect method semantics can increase the developer’s confusion when developing applications in different
frameworks.
In this paper, we present a comprehensive study of Execution After Redirect vulnerabilities: we provide an overview
of EARs and classify EARs into different types. We also
analyze nine web application frameworks’ susceptibility to
EARs, specifying their redirect semantics, as well as detailing what exactly makes them vulnerable to EARs. Moreover, we develop a novel static analysis algorithm to detect
EARs, which we implemented in an open-source tool to analyze Ruby on Rails web applications. Finally, we discovered
hundreds of vulnerabilities in open-source Ruby on Rails
web applications, with a very low false positive rate.
In summary, this paper provides the following contributions:
• We categorize EARs and provide an analysis of nine
frameworks’ susceptibility to various types of EARs.
• We discuss the results from the EAR challenge contained within our 2010 International Capture the Flag
Competition.
• We present an algorithm to statically detect EARs in
Ruby on Rails applications.
• We run our white-box tool on 18,127 open-source Ruby
on Rails applications, which found 3,944 EARs.

2.

OVERVIEW OF EARS

An Execution After Redirect vulnerability is a logic flaw
in web applications that results from a developer’s misunderstanding of the semantics of redirection. Very often this
misunderstanding is caused by the web framework used by
the developer2 . In particular, developers typically assume
that the web application will halt after calling a function
of the web framework that performs a redirect. Certain
web frameworks, however, do not halt execution on a redirect, and instead, execute all the code that follows the redirect operation. The web browser perpetuates this misunderstanding, as it obediently performs the redirect, thus falsely
indicating that the code is correct. As a result, when the
2

This misunderstanding was confirmed by a developer who
responded to us when we notified him of an EAR in his code,
who said, “I wasn’t aware at all of this problem because I
thought ruby on rails will always end any execution after a
redirect.” This example shows that developers do not always
understand how their web framework handles redirects.

1 class TopicsControl l er <
A p p l i c a t i o n C o n t r o l le r
2 def update
3
@topic = Topic . find ( params [: id ])
4
if not current_user . is_admin ?
5
redirect_to ( " / " )
6
end
7
@topic . update_attri bu t es ( params [: topic ])
8
flash [: notice ] = " Topic updated ! "
9 end
10 end
Listing 1: Example of an Execution After Redirect
vulnerability in Ruby on Rails.

developer tests the web application using the browser, the
observed behavior seems in line with the intentions of the
developer, and, consequently, the application is assumed to
be correct.
Note that an EAR is not a code injection vulnerability;
an attacker cannot execute arbitrary code, only code already
present after the redirect. An EAR is also different from XSS
and SQL injection vulnerabilities; it is not an input validation flaw, but rather a mismatch between the developer’s
intentions and the actual implementation.
As an example, consider the EAR vulnerability in the
Ruby on Rails code shown in Listing 1. The code appears to
redirect the current user to “/” if she is not an administrator (Line 5), and, if she is an administrator, @topic will be
updated with the parameters sent by the user in the params
variable (Line 7). The code does not execute in this way, because Ruby on Rails does not support halt-on-redirect behavior. Thus, any user, not only the administrator, can
update the topic, violating the intended authorization and
compromising the security of the web application.
The simple way to fix Listing 1 is to add a return after
the redirect_to call on Line 5. This will cause the update
method to terminate after the redirect, thus, no additional
code will be executed. Adding a return after all redirects is
a good best practice, however, it is insufficient to prevent all
EARs. Listing 2 depicts an example of an EAR that cannot
be prevented by adding a return after a redirect. Here, the
redirect_to on Line 4 is followed by a return, so there is no
EAR in the ensure_admin method. However, ensure_admin
is called by delete on Line 10, which calls redirect_to on
Line 4. The return call on Line 5 will return the control flow
back into the delete method, and execution will continue
on Line 11. Thus, the @user object will still be deleted
on Line 12, regardless of whether the current_user is an
administrator or not, introducing an EAR. Unfortunately
in some frameworks, the developer cannot simply use exit
instead of return to halt execution after a redirect because
the web application is expected to handle multiple requests.
Therefore, calling exit would kill the web application and
prevent further requests.

2.1 EAR History
Execution After Redirect vulnerabilities are not a new
occurrence; we found 17 Common Vulnerabilities and Exposures (CVE) EAR vulnerabilities dating back to 2007.
These CVE entries were difficult to find because EARs do
not have a separate vulnerability type; the EAR CVE vul-

1 class UsersControll er <
A p p l i c a t i o n C o n t r o l l er
2
def ensure_admin
3
if not current_user . is_admin ?
4
redirect_to ( " / " )
5
return
6
end
7
end
8
9
def delete
10
ensure_admin ()
11
@user = User . find ( params [: id ])
12
@user . delete ()
13
flash [: notice ] = " User Deleted "
14
end
15 end
Listing 2: Example of a complex Execution After Redirect
vulnerability in Ruby on Rails.

nerabilities we found3 were spread across different Common
Weakness Enumeration Specification (CWE) types: “Input
Validation,” “Authentication Issues,” “Design Error,” “Credentials Management,” “Code Injection,” and “Permissions,
Privileges, and Access Control.” These vulnerabilities types
vary greatly, and this indicates that EARs are not well understood by the security community.

2.2 EARs as Logic Flaws
While logic flaws are typically thought of as being unique
to a specific web application, we believe EARs are logic
flaws, even though they are systemic to many web applications. Because an EAR is the result of the developer’s
misunderstanding of the web application framework, there
is an error in her logic. The intuition is that the redirect is
an indication of the developer’s intent for ending server-side
processing. A redirect can be thought of as a goto - the
developer, in essence, wishes to tell the user to look somewhere else. However, it does not act as a goto, because the
server-side control flow of the application is not terminated,
even though that is how it appears from the perspective of
the client.
There are almost no valid reasons to have code executed
after a redirect method. The few exceptions are: performing cleanup actions, such as closing open files, and starting
long-running processes, such as encoding a video file. In
the former case, the cleanup code can be executed before a
redirect, and in the latter case, long-running processes can
be started asynchronously, alleviating the need to have code
executed after a redirect.
Because there is no reason to execute code after a redirect, we can infer that the presence of code executed after a
redirect is a logic flaw.

2.3 Types of EARs
Execution After Redirect logic flaws can be of two types:
benign or vulnerable. A benign EAR is one in which no security properties of the application are violated, even though
3

The interested reader is directed to the following EARs:
CVE-2009-2168, CVE-2009-1936, CVE-2008-6966, CVE2008-6965, CVE-2008-0350, CVE-2007-6652, CVE-20076550, CVE-2007-6414, CVE-2007-5578, CVE-2007-4932,
CVE-2007-4240, CVE-2007-2988, CVE-2007-2776, CVE2007-2775, CVE-2007-2713, CVE-2007-2372, and CVE2007-2003.

1
2
3
4
5
6

$current_user = get_current_ us er () ;
if (! $current_user - > is_admin () )
{
header ( " Location : / " ) ;
}
echo " Sensitive Information " ;
Listing 3: Example of an information leakage Execution
After Redirect vulnerability in PHP. If the current_user
is not an administrator, the PHP header function will be
called, redirecting the user to “/”. However, the sensitive
information will still be returned in the output, thus leaking
information. The fix is to call the exit function after the
header call.

additional, unintended, code is executed after a redirect. For
example, the code executed after the redirect could set a local variable to a static string, and the local variable is not
used or stored. Although no security properties are violated,
a benign EAR may indicate that a developer misunderstood
the redirect semantics of the web framework, posing the risk
that code will, in the future, be added after the redirect, elevating the EAR from benign to vulnerable.
A vulnerable EAR occurs when the code executed after
the redirect violates the security properties of the web application. More specifically, in a vulnerable EAR the code
executed after the redirect allows unauthorized modification
to the state of the web application (typically the database),
and/or causes leakage (reads and returns to the browser)
of data to an unauthorized user. In the former case (e.g.,
see Listing 1), the integrity of the web application is compromised, while in the latter case, the confidentiality of the
web application is violated (e.g., see Listing 3). Thus, every
vulnerable EAR is an instance of broken/insufficient access
controls, because the redirect call is an indication that the
user who made the request is not allowed to access the requested resource.
EAR vulnerabilities can be silent. In a silent EAR, the
execution of code does not produce any output. This lack of
information makes silent EARs difficult to detect via a blackbox approach, while information leakage EARs are easier to
detect with black-box tools. Listings 1 and 2 are examples of
silent EARs, and Listing 3 is an example of an information
leakage EAR.

2.4 Framework Analysis
Web application frameworks vary on supporting halt-onredirect behavior. Therefore, different frameworks provide
protection against different kinds of EAR vulnerabilities.
The differing semantics of redirects increases the confusion
of developers. A developer we contacted said, “I didn’t realize that [Ruby on Rails’] redirect to was like PHP’s header
redirect and continued to run code.” Thus, an understanding of the web framework’s redirect semantics is essential to
produce correct, EAR-free, code.
We analyzed nine of the most popular web frameworks
to see how they differ with respect to their built-in redirect
functions. The nine frameworks were chosen based on their
StackOverflow activity, and include one framework for each
of the Ruby, Groovy, and Python languages, three frameworks for the PHP language, one framework that can be
applied to both C# and Visual Basic, and two frameworks

for the Java language [7]. While the frameworks selected for
analysis are not exhaustive, we believe they are diverse and
popular enough to be representative of real-world usage.
To analyze the frameworks, we created nearly identical
copies of a simple web service in each of the nine web frameworks. This web service provided access to four pages within
the web application. The first was the root page, “/”, which
simply linked to the other three pages. The second was
the redirect page, “/redirect”, which was used to test proper
redirect behavior. The third was the EAR page, “/ear”,
which called the framework’s redirect function, appended a
message to a log file regarding the request, and finally attempted to return a rendered response to the browser. The
last page was the log page, “/log”, which simply displayed
the contents of the log file.
Using this design for the web application allowed us to
check for integrity violations, represented by the appended
log message, and confidentiality violations, represented by
output sent after the HTTP redirect response when requesting the EAR page. We approached the implementation of
this web application in each framework as many developers
new to that framework would. That is, whenever possible,
we followed the recommended tutorials and coding practices
required to build a web application in the framework.
A brief background on the model-view-controller (MVC)
software architecture is necessary to follow our analysis, as
each framework analyzed fits the MVC pattern. The MVC
architecture supports the separation of the persistent storage (model), the user interface (view), and the control flow
(controller) [33]. More precisely, the models interact with
the database, the views specify the output to return to the
client, and the controllers are the glue that puts everything
together. The controller must handle HTTP requests, fetch
or update models, and finally return a view as an HTTP
response. When following the MVC paradigm, a controller
is responsible for issuing a redirect call.
The following sections describe our analysis of each framework’s susceptibility to EAR vulnerabilities based on their
redirect functions’ use and documentation. We developed
the test application in the latest stable version of each framework available at the time. The version numbers are listed
adjacent to the framework name in the section headers.

2.4.1 Ruby on Rails 3.0.5
Ruby on Rails, commonly referred to as Rails, is a popular
web application framework. Unfortunately, Rails is susceptible to EAR vulnerabilities. Rails provides the redirect_to
function, which prepares the controller for sending the HTTP
redirect. However, the redirect is not actually sent at this
point, and code continues to execute following the call to
redirect_to. In Rails, there is no mechanism to ensure
that code halts following a redirect, thus if exit is called, a
developer must return from the controller’s entry function
without executing additional code.
As previously mentioned in Section 2, the Ruby exit command cannot be used to halt the execution of a controller
after a redirect. This is for two reasons: the first is that
redirect_to does not immediately send output when it is
called, thus if exit is called, the user will never see the
redirect. The second reason is that Rails web applications
are long-running processes that handle multiple incoming requests, unlike PHP, which typically spawns a new instance
for each request. Therefore, calling exit to halt execution

is not feasible, as it will terminate the Rails application,
preventing it from handling further requests.
On a positive note, information leakage EARs are impossible in Rails web applications because a controller can either
perform a redirect, or render a response (view) to the user.
Any call to render after a redirect will result in Rails throwing a DoubleRenderError. This exception is thrown in all
possible combinations: render after a redirect, render after a
render, redirect after a render, and redirect after a redirect.

2.4.2 Grails 1.3.7
Grails is a framework written in Groovy, which was modeled after the Ruby on Rails framework. Thus, Grails behaves in a manner nearly identical to Rails with respect
to redirects. Specifically, code will continue to execute following a call to the redirect function, and, therefore, the
developer must take precautions to avoid creating an EAR
vulnerability. Unfortunately, as of this writing, nowhere in
the Grails documentation on redirects does it mention that
code will continue to execute following a redirect [34].
Unlike Ruby on Rails, the behavior of Grails is somewhat
less predictable when it comes to the order of view rendering
and/or calls to redirect. To explain, we will say that to
“render” means to output a view, and to “redirect” means
to call the redirect function. As previously mentioned in
Section 2.4.1, in Rails, only one render or one redirect may
be called in a controller; a DoubleRenderError is thrown
in the case of multiple calls. In Grails, however, the only
redirect exception, CannotRedirectException, occurs when
a redirect is called following another redirect. In cases where
multiple calls to render are made, the final render is the only
one that is sent to the browser. More importantly, in cases
where both redirect and render are called, regardless of their
order, the redirect is actually sent to the browser and the
render call is simply ignored. Due to this behavior of Grails,
it is not vulnerable to an information leakage EAR. However,
like Rails, it is still vulnerable to silent EARs that violate
the integrity of the application.

2.4.3 Django 1.2.5
Django is a Python web application framework that differs in its handling of redirects compared to the other frameworks (save for ASP.NET MVC). Rather than calling functions to render or perform the redirect, Django requires the
developer to return an HttpResponse object from each controller. Django’s documentation makes it clear that calling Django’s redirect function merely returns a subclass
of the HttpResponse object. Thus, there is no reason for
the developer to expect the code to halt when calling redirect. The actual HTTP redirect is sent to the browser only
if this object is also returned from the controller’s entry
point, thereby removing the possibility of further code execution [15]. Because the controller’s entry point can only
return a single HttpResponse object, the developer can rely
completely on her browser for testing purposes. This behavior makes Django impervious to all EARs.

2.4.4 ASP.NET MVC 3.0
ASP.NET MVC is a web application framework developed
by Microsoft that adds a Model-View-Controller paradigm
on top of traditional ASP.NET, which includes the languages
C# and Visual Basic [1]. ASP.NET MVC is similar to
Django, in that all controllers must return an ActionRe-

sult object. In order to perform redirection, either a RedirectResult or RedirectToRouteResult object must be returned, which are both subclasses of ActionResult. Like
Django, this behavior makes ASP.NET MVC impervious to
all EARs.

2.4.5 Zend Framework 2.3
By default, the PHP based Zend Framework is not susceptible to EAR vulnerabilities because its redirect methods
immediately result in the termination of server-side code.
This default behavior is consistent in the two methods used
to perform a redirect in the Zend Framework. The simplest
method is by using the _redirect method of the controller,
however, the recommended method is to use the Redirector
helper object [36].
While the default behavior is not vulnerable to EARs,
the Zend Framework supports disabling halt-on-redirect for
both methods. The _redirect method will not halt when
the keyword argument exit=False is provided as part of the
call. Disabling halt-on-redirect when using the Redirector
helper requires calling SetExit(False) on the Redirector
helper object prior to making the redirect call. The latter
method is particularly interesting because any code executed
during the request has the ability to modify the behavior of
redirects called using the Redirector helper. Fortunately,
even when using the Redirector helper, the developer has
the option of using a set of functions suffixed with “AndExit”
that always halt-on-redirect.
When halt-on-redirect is disabled in Zend, it becomes vulnerable to integrity violation EARs. However, the default
view rendering behavior no longer occurs. Thus, even when
modifying the default behavior, information leakage EARs
will never occur in the Zend Framework.

2.4.6 CakePHP 1.3.7
Similar to the Zend Framework, the CakePHP framework
is also not susceptible to EAR vulnerabilities out of the box.
By default, CakePHP’s single redirect method immediately
results in the termination of the PHP script. In a manner
similar to the Zend Framework, this default behavior can
be modified by setting the third argument of redirect to
False, which in turn also disables the default mechanism for
view rendering [11]. Thus CakePHP is vulnerable to EARs
in exactly the same way as the Zend Framework.

2.4.7 CodeIgniter 2.0.0
Unlike the Zend Framework and CakePHP, CodeIgniter
is a very lightweight PHP framework, and thus, it does
not offer much out of the box. Nevertheless, the framework still provides a url helper class that contains a redirect method [16]. CodeIgniter’s redirect method always exits after setting the redirect header; a behavior that cannot be changed. Therefore CodeIgniter is impervious to
EARs when developers use only the provided redirect function. Unfortunately, the url helper class must be included
manually. As a result, there is the risk that developers will
not use the provided redirect function and instead introduce EARs by neglecting to call exit following a call to
header("Location:<path>").

2.4.8 J2EE 1.4
Java 2 Platform, Enterprise Edition (J2EE) defines a servlet paradigm for the development of web applications and

web application frameworks in Java. Thus, to perform a
redirect in J2EE, or a J2EE-based framework, the developer calls HttpServletResponse.sendRedirect. This redirect function will clear out everything previously in the output buffer, set the Location header to the redirect location,
set the response code to 302, and finally flushes the output
buffer to the browser. However, sendRedirect does not halt
execution of the servlet. Thus, only silent EARs are present
in J2EE web applications, or any framework that is based
on J2EE servlets.

2.4.9 Struts 2.2.3
Apache Struts is an MVC framework that is built on top
of the servlet model provided by J2EE. Thus, Struts inherits all the potential vulnerabilities of the J2EE framework,
specifically that silent EARs are possible but information
leakage EARs are not possible. This is because to perform a
redirect, the HttpServletResponse.sendRedirect method
of J2EE must be called.

2.5 EAR Security Challenge
Each year since 2003, we have organized and hosted a security competition called the International Capture the Flag
(iCTF). The competition pits dozens of teams from various
universities across the world against each other in a test of
their security prowess. While each iCTF has a primary objective, the competitions typically involve secondary security
challenges tangential to the primary objective [14].
For the 2010 edition of the iCTF, we constructed a security challenge to observe the familiarity of the teams to
Execution After Redirect vulnerabilities. The challenge involved a vulnerable EAR that violated both the confidentiality and the integrity of the web application. The confidentiality was violated when the web application’s administrator view was leaked to unauthorized users following a
redirect; the unauthorized users were “correctly” redirected
to an error page. The information contained in the leaked
view provided enough information to allow for an integrity
violation had the database not purposefully been in a readonly state. More importantly, the initial data leak provided
the means to leak further information, thus allowing teams
to successfully solve the challenge [6].
The crux of the EAR challenge relied on the automatic
redirecting of web browsers and other web clients, such as
wget and curl. To our surprise, many of the teams relied
only on the output produced by their web browser, and,
therefore, failed to notice the leaked information. It is important to note that the teams in this competition are primarily made up of graduate and undergraduate level students from various universities; many would not be considered security professionals. Nevertheless, we assumed that
the meticulous eye of a novice-to-intermediate level hacker
attempting to break into a web service would be more likely
to detect information leakage when compared to a web developer testing their application for “correct” page flow.
Of the 72 teams in the competition, 69 contacted the web
server at least once. 44 of these 69 teams advanced past
the first step, which required them to submit a file as per
the web application’s specifications. 34 of the 44 teams advanced past the second step, which required them to brute
force a two-digit password. It was at this point that the
EAR vulnerability was exposed to the teams, resulting in
both a redirect to the unauthorized error page and the leak-

Rails Application
1) Build CFG
CFG
2) Find Redirection Methods
CFG, interesting methods
3) Prune Infeasible Paths
CFG, interesting methods
4) Detect EARs
EARs
5) Classify as Vulnerable
Benign EARs, Vulnerable EARs

Figure 1: The logical flow of the white-box tool.

age of the administrator page as part of the HTTP redirect
response. Of the 34 teams who made it this far, only 12
successfully discovered and exploited the vulnerability. The
fact that only 12 out of 34 teams were successfully able to
discover the information leaked to their browser in a hacking
competition indicated that more research and exposure was
necessary for EAR vulnerabilities.

3.

EAR DETECTION

In this section, we discuss the design and implementation
of our system to detect EAR vulnerabilities. This system
uses static source code analysis to identify cases in which
code might be executed after the call to a redirect function.
We also introduce a heuristic to distinguish benign EARs
from vulnerable EARs.
Our tool targets the Ruby language, specifically the Ruby
on Rails web framework. We chose this framework for two
reasons. First, Ruby on Rails is a very popular web framework, thus, there is a large number of open-source Ruby
on Rails web applications available for inspection (e.g., on
GitHub [19]). Second, due to the characteristics discussed
in Section 2.4.1, all EARs present in Rails are silent. Thus,
it is necessary to use a white-box tool to detect EARs in
Ruby on Rails web applications. Again, it is important to
note that redirects originate within the controllers4 , thus,
our white-box tool operates specifically on controllers.

3.1 Detection Algorithm
The goal of our EAR detector is to find a path in the
controller’s Control Flow Graph (CFG) that contains both
a call to a redirect method and code following that redirect
method. An overview of our algorithm is given in Figure 1.
The algorithm operates in five steps: (i) generate the CFG
4

Redirects can also occur in Rails’ routing, before the request gets to the controller. However, EARs cannot occur
in this context, because control flow never reaches a controller. Thus, we are not concerned with these redirects.

of the controller; (ii) find redirection methods; (iii) prune
infeasible paths in the CFG to reduce false positives; (iv)
detect EARs by finding a path in the CFG where code is executed after a redirect method is called; (v) use a heuristic
to differentiate between benign and vulnerable EARs.
Step 1: Building the Control Flow Graph
We built our system on top of the Ruby parser presented by
Furr et al. [18]. This parser first compiles Ruby into a subset
of the Ruby language called Ruby Intermediate Language,
or RIL. The purpose of RIL is to simplify Ruby code into an
easier-to-analyze format. The simplification is performed by
removing ambiguities in expressions, reducing Ruby’s four
different branches to one canonical representation, making
method calls explicit, and adding explicit returns. At the
end of the transformation, every statement in RIL is either
a statement with one side effect or a branch. The parser
generates the CFG of RIL.
Due to Ruby’s dynamic nature, this CFG might be incomplete. In particular, strings containing Ruby code can
be evaluated at run-time using the eval function, object
methods can be dynamically called at run-time using the
send function, and methods can be added to objects at runtime. We do not address EAR vulnerabilities associated
with these language features. However, we have found that
these features are rarely used in practice (see Section 3.2).
Step 2: Finding Redirection
To detect EARs, we must first find all program paths (from
any program entry to any program exit point) in the CFG
that call the Ruby on Rails method redirect_to. The reason is that we need to check these paths for the presence
of code execution between the redirect call and the program
exit point. Note that intra-procedural analysis is not enough
to find all EARs. Consider the code in Listing 2. Simply
looking in ensure_admin for code execution after the call to
redirect_to and before the end of this method is not sufficient. Thus, we need to perform inter-procedural analysis to
find all possible ways in which code execution can continue
after a redirect_to call until the end of the program.
Our inter-procedural analysis proceeds as follows: we start
by finding all methods that directly call redirect_to. These
methods are added to a set called interesting methods. Then,
for each method in the interesting methods set, we add to
this set all methods that call it. This process is iterated
until a fixpoint is reached, and no new interesting methods
are found.
At this point, every element (method) in interesting methods can eventually lead to a redirect_to call. Whenever a
call to an interesting method returns, its execution will continue after the call site in the caller. Thus, all paths from
invocations of redirect_to until the end of the program are
captured by the paths from all invocations (call sites) of interesting methods to the end of the methods that contain
these calls. Now, to detect an EAR, we can simply look for
code that is executed on a path from the call site of an interesting method until the end of the method that contains
this call.
Step 3: Prune Infeasible Paths
Looking for all paths from the redirect_to method to the
program exit point might lead to false positives due to infeasible paths. Consider the example in Listing 4. There

1 class UsersControll er <
A p p l i c a t i o n C o n t r o l l er
2
def ensure_logge d_ in
3
if not current_user
4
redirect_to ( " / " ) and return false
5
end
6
@logged_in_u se rs += 1
7
return true
8
end
9
10
def delete_all
11
if not ensure_logge d_ in ()
12
return
13
User . delete (: all )
14
end
15 end

delete_all

_tmp_ = ensure_logged_in()

ensure_logged_in

current_user
false
redirect_to("/")
true

true
false (1)

return false

Listing 4: Example of potential false positive.

@logged_in_users += 1
(1)

are no EARs in this code. The redirect_to on Line 4 will
always return true, thus, return false (also on Line 4) will
execute as well. Because of this, ensure_logged_in will always return false after performing a redirect. As a result,
the call to ensure_logged_in on Line 11 will always return
false, and the return on Line 12 will always occur.
The CFG for the code in Listing 4 is shown in Figure 2.
With no additional processing, we would incorrectly report
the path from redirect_to on Line 4 to the statement in
Line 6. Moreover, we would also report an EAR because of
the path from the redirect to the User.delete on Line 13.
The first path is denoted as (1) in Figure 2, the second path
as (2).
To prune infeasible paths in the CFG, we explore all paths
that follow an interesting method. If all paths following an
interesting method call return the same Boolean value, we
propagate this Boolean constant to all the call sites of this
method. Then, we recursively continue constant value propagation at all the call sites, pruning infeasible paths everywhere after the interesting method is called. We iteratively
continue this process throughout the CFG; whenever we find
a constant return value, we propagate this return value to
all call sites.
Figure 2 shows the results of performing our pruning process on the CFG of Listing 4. Initially, all paths after the
redirect_to in ensure_logged_in do not return the same
Boolean, so we cannot conclude anything about the return
value of ensure_logged_in. However, redirect_to always
returns true. Therefore, we perform constant value propagation on the return value of redirect_to, which is used in
a branch. As a consequence, we can prune all of the paths
that result from the false branch. The edges of this path
are labeled with (1) in Figure 2. Now, all paths from redirect_to return false, which means that ensure_logged_in
will always return false after a redirect. We now perform constant value propagation at all the call sites of ensure_logged_in, removing all the paths labeled with (2).
At this point, there is nothing more to be pruned, so we
stop. It can be seen that there is no path from redirect_to
to state-changing code (defined in the next step) along the
solid lines.
Step 4: Detecting EARs
Once the CFG of the controller has been simplified and interesting method information has been extracted, we perform
EAR detection. This is a fairly simple process; we traverse

return true
(1)
_tmp_
true (2)
User.delete(:all)

false
return

(2)
return

Figure 2: Control Flow Graph for the code shown in Listing 4. The dotted lines are paths removed from the CFG by
Step 3 of the EAR detection algorithm.

the CFG of every method to see if potentially problematic
code can be executed after a call to an interesting method.
We conservatively define such code as any statement that
could possibly modify the program state, excluding statements that alter the control flow. This excludes return and
branches, but includes assignment and method calls. As a
special case, we also disregard all operations that set the
flash or session array variable. These arrays are used in
the former case to set a message to be displayed on the
destination page, and in the latter case to store some information in the user’s session. These calls are disregarded because they do no affect the state of the web application and
are frequently called after redirection. We report as a potential EAR each method that executes potentially problematic
code between the invocation of an interesting method and
its return statements.
Step 5: Distinguishing Between Benign and Vulnerable EARs
We also introduce a heuristic to identify vulnerable EARs.
This heuristic looks for paths from an interesting method
to a function that modifies the database. If one is found,
the EAR is marked as vulnerable. We used the Rails documentation to determine the 16 functions that modify the
database. Of course, this list can be easily extended. This
process is not sound, because we perform no type analysis,
and look only at the method names being called. Moreover,
we do not analyze the models, only looking for this specific
list. Despite these limitations, our results (Section 4.1) show

Type of EAR reported
Benign
Vulnerable
Total
Total Projects
Any EAR
Only Benign
At least one vulnerable EAR

Number reported
3,089
855
3,944
18,127
1,173
830
343

Table 1: Results of running the white-box detector against
Ruby on Rails applications, 6.5% of which contained an EAR
flaw. 2.9% of the projects had an EAR classified as vulnerable.
that this heuristic is still a good indicator of potentially vulnerable EARs that deserve the developer’s attention.

3.2 Limitations
The white-box EAR detector is limited to analyzing Ruby
on Rails applications, although the detection algorithm can
be extended to any programming language and web framework. Detection is neither sound nor complete. False negatives can occur when a Rails application uses Ruby’s dynamic features such as eval or send to execute a redirect.
While such dynamic features are used extensively in the
Ruby on Rails framework itself, they are rarely used by web
applications written in Rails. Of the 3,457,512 method calls
in controllers that we tested our tool on, there were 428
(0.012%) eval method calls and 2,426 (0.07%) send method
calls, which shows how infrequently these are used in Rails
web applications.
The white-box tool can report two types of false positives:
false EARs, that is, the tool reports an EAR although no
code can be executed after a redirect, or false vulnerable
EARs, where the tool mistakes a benign EAR as vulnerable.
False EARs can occur for several reasons. One reason is
that the path from the redirect function to the code execution that we found is infeasible. A typical example is when
the redirect call and the code execution occur in opposite
branches. The branch conditions for these are mutually exclusive, so there can never be a path from the redirect call to
the code execution. Examples of this type of false positive
are discussed in Section 4.1, and these could be mitigated
by introducing better path sensitivity.
False vulnerable EARs are a problem caused by the heuristic that we use. The biggest issue is that we simply look for
method calls that have the same name as method calls that
update/change the database. However, we do not perform
any type analysis to determine the object that the method
is called on. Thus, methods such as delete on a hash table
will trigger a false vulnerable EAR, since delete is also a
method of the database object. Improved heuristics could be
developed, for instance, that include the type of the object
the method is being invoked on.
Despite these limitations, our experiments demonstrate
that the tool works very well in practice. In addition, Ruby
on Rails controllers are typically very small, as most application logic is present in the models. Thus, our tool works
very well on these types of controllers. We provide5 our
tool to the community at large, so that others may use it to
detect EARs in their code.
5

https://github.com/adamdoupe/find_ear_rails

Classification after manual analysis
True Vulnerable EARs
Benign EARs
No EARs (False Positives)

Number
485
325
45

Table 2: Results of manually inspecting the 855 vulnerable
EARs reported by our white-box tool. 40.1% were benign,
and 5.3% were not EARs.

4. RESULTS
We used our EAR detection tool to find real-world EARs
in open-source Ruby on Rails web applications. First, we
downloaded 59,255 open-source projects from GitHub [19]
that were designated as Ruby projects and that were not a
fork of another project. We identified 18,127 of the downloaded Ruby projects that had an app/controllers folder,
indicating a Ruby on Rails application.
Table 1 summarizes the results. In total, we found 3,944
EAR instances in 1,173 projects. 855 of these EARs, present
in 343 projects, were classified as vulnerable by our system.
This means that 6.5% of Rails applications we tested contained at least one EAR, and 29.3% of the applications containing EARs had an EAR classified as vulnerable.
Of the 1,173 projects that contained at least one EAR, we
notified those project owners that had emails listed in their
GitHub profile, for a total of 624. Of these project owners,
107 responded to our email. Half of the respondents, 49,
confirmed the EARs we reported. 26 other respondents told
us that the GitHub project was no longer being maintained
or was a demo/toy. Three respondents pointed out false
positives, which we confirmed, while 6 of the project owners said that there were not going to fix the EAR because
there was no security compromise. The rest of the responses
thanked us for the report but did not offer a confirmation of
the reported EAR.

4.1 Detection Effectiveness
To determine the effectiveness of our tool, we manually
inspected all 855 vulnerable EARs. The results are shown
in Table 2. We manually verified that 485, or 59.9%, were
true positives. Many of these were caused by ad-hoc authorization checks, where the developer simply introduced
a redirect when the check failed. Some examples of security violations were allowing non-administrators access to
administrator functionality, allowing modifications to items
not belonging to the current user, and being able to sign up
for a conference even though it was full.
Listing 5 shows an interesting example adapted from a real
EAR where the redirect is followed by and return (Line 3),
however, due to Ruby’s semantics, this code contains an
EAR. In Ruby, a return with no arguments returns false6 ,
thus, redirect_to_login will always return false (because
of the “no argument” return call on Line 3). The result is
that the return on Line 8 will never be executed, because
redirect_to_login will always return false, and the shortcircuit logic of and will cause Line 10 to be executed. This
example shows that our tool discovers non-obvious EARs.
For vulnerable EARs, we consider two different types of
false positives: false vulnerable EARs, which are benign
6
Technically nil, but nil and false are equivalent for
Boolean comparisons.

1 class BanksControll er <
A p p l i c a t i o n C o n t r o l l er
2
def redirect_to_ l og in
3
redirect_to ( " / login " ) and return
4
end
5
6
def create
7
if not current_user . is_admin ?
8
redirect_to_ l og in () and return
9
end
10
@bank = Bank . create ( params [: bank ])
11
end
12 end
Listing 5:
True positive Execution After Redirect
vulnerability in Ruby on Rails.

EARs mistakenly reported as vulnerable, and false EARs
(false positives).
As shown in Table 2, the white-box tool generated 45 false
EARs, for a false positive rate of 5.3%. These false positives
came from two main categories. About half of the false positives were due to impossible paths from the redirect methods
to some code. An example of this is when a redirect method
was called at the end of a branch that checked that the request was an HTTP GET, while the code executed after a
redirect was in a branch that checked that the request was
an HTTP POST. These two conditions are mutually exclusive, thus, this path is impossible. The other half of false
positives were due to local variables that had the same name
as a redirect method. The parsing library, RIL, mistakenly
identified the local variable access as a method call to a redirect method. We are currently looking into fixing this issue
in RIL, which will almost halve our false positive rate.
While our false EAR rate was only 5.5%, our vulnerable
EAR detection heuristic had a higher false detection rate of
40.1%. The biggest culprit for false vulnerable EARs (72.9%
of the instances) was due to no feasible path from the redirect to the method that changed the state of the database.
For instance, the redirect method occurred in a branch that
was taken only when a certain object was nil7 . Later, the
database method was called on this object. Thus, when the
redirect happens, the object will be nil. Because of the presence of an EAR flaw, execution will continue and reach the
database access method. However, since the object is nil,
the database will not be affected. Because our heuristics
cannot detect the fact that, after the redirect, the database
function will always be called with a nil object, we report
a vulnerability. The other common false vulnerable EAR
were instances where the redirect method was called before
code was executed, however, it was clear that the developer
was fully aware of the redirect semantics and intended for
the code to be executed.
We also checked that the false EAR rate did not differ
significantly among the benign EARs by manually inspecting 200 random EARs reported as benign. We saw 13 false
EARs in the manual inspection, for a false positive rate of
6.5%. Thus, the total false positive rate among the instances
we manually inspected is 5.5%. We also did not see any vulnerable EARs among the benign EARs, thus, we did not see
any false negative vulnerable EARs in our experiments.
7

nil is Ruby’s null

From our results, we can conclude that we detect EARs
well. However, it is more difficult to distinguish between
benign and vulnerable EARs. Classification could be improved by using a better heuristic to detect intended redirects. However, even though certain EARs might not be
vulnerable at the moment, they are still programming errors that should be fixed. This is confirmed by the responses
that we received from developers who were grateful for error
reports even though they are not exploitable at the moment.
Also, our tool reports one true vulnerability for every benign
EAR mistakenly classified as vulnerable. This is well in line
with the precision of previous static analysis tools [24,25,28].

4.2 Performance
To evaluate the performance of our tool, we measured the
running time against the 18,127 Ruby on Rails applications.
We ran our experiments on an Intel Core i7 with 12 gigabytes
of RAM. Our algorithm scales linearly with the size of the
CFG and is fast; no project took longer than 2.5 seconds
even with the largest CFG size of 40,217 statements.

5. PREVENTION
The old adage “an ounce of prevention is worth a pound
of cure” is true in software. Boehm showed that the later
in an application’s life-cycle bugs are caught, the more expensive they are to fix [8]. Thus, preventing certain types
of bugs from even being introduced is attractive from both
an economic standpoint, and a security perspective. Our
recommendation to web frameworks, therefore, is to make
Execution After Redirect vulnerabilities impossible to occur, by having every invocation of the redirect method halt
execution, which we call halt-on-redirect behavior.
As we have shown in Section 2.4, some frameworks have
already either adopted the approach of making EARs impossible, or their approach to generating HTTP responses
makes EARs highly unlikely. For existing frameworks that
wish to decrease the chance of EARs being introduced, such
draconian measures may not be acceptable because they
break backward-compatibility. Our suggestion in these cases
is to make an application-wide setting to enable halt-onredirect behavior, along with an argument to the redirect
function to halt execution after the redirect. Of course, we
suggest making halt-on-redirect the default behavior, however each framework will have to properly balance security
and backward-compatibility.
To improve the security of Ruby on Rails, we are in discussions with the Rails development team about our proposed
change. The difficulty with implementing halt-on-redirect
behavior in Rails is that there are no gotos, and Rails applications run in a single-threaded context. This limits the two
obvious forms of implementing halt-on-redirect: we cannot
use a goto or language equivalent statement to jump from
the end of the redirect_to method to the code after the
controller is called. Moreover, we also cannot, at the end
of the redirect_to method, send the HTTP response and
cause the current thread to stop execution. PHP frameworks
can use the exit function to implement halt-on-redirect behavior, because each request spawns a new PHP process.
Our proposed solution is to throw a new type of exception,
RedirectOccuredException, at the end of the redirect_to
body. In the Ruby on Rails framework core, where the controller is called, there is a catch block for this exception.
While this will prevent almost all EARs, there is a possi-

bility for code to be executed in an ensure block, Ruby’s
equivalent of a “finally” block. Code in this block will be executed regardless of a redirect. However, we believe this is
semantically in line with the way the language should work:
ensure blocks will always be executed, no matter what happens, and this is clear to the programmer via the language’s
semantics.

6.

RELATED WORK

Specific instances of Execution After Redirect vulnerabilities have been previously identified. Hofstetter wrote a
blog post alerting people to not forget to exit after a redirect when using the PHP framework CakePHP [22]. This
discussion resulted in a bug being filed with the CakePHP
team [9]. This bug was resolved by updating the CakePHP
documentation to indicate the redirect method did not end
execution [10].
Felmetsger et al. presented a white-box static analysis tool
for J2EE servlets to automatically detect logic flaws in web
applications. The tool, Waler, found Execution After Redirect vulnerabilities in a web application called Global Internship Management System (GIMS) [17]. However, neither Felmetsger nor Hofstetter identified EARs as a systemic
flaw among web applications.
Wang et al. manually discovered logic flaws in the interaction of Cashier-as-a-Service (CaaS) APIs and the web applications that use them [35]. This is interesting because there
is a three-way interaction between the users, the CaaS, and
the web application. In our work, we consider one specific
type of logic flaw across many applications.
Our white-box EAR detection tool uses the Ruby Intermediate Language (RIL) developed by Furr et al. [18]. RIL
was used by An et al. to introduce static typing to Ruby on
Rails [23]. They use the resulting system, DRails, on eleven
Rails applications to statically discover type errors. DRails
parses Rails applications by compiling them to equivalent
Ruby code, making implicit Rails conventions explicit. This
differs from our tool, which operates directly on the Rails
application’s control flow graph. Moreover, we are looking at
a specific logic flaw, while DRails is looking for type errors.
Chaudhuri and Foster built a symbolic analysis engine on
top of DRails, called Rubyx [13]. They are able to analyze
the security properties of Rails applications using symbolic
execution. Rubyx detected XSS, XSRF, session manipulation, and unauthorized access in the seven applications
tested. Due to the symbolic execution and verifying of path
conditions, false positives are reduced. However, Rubyx requires the developer to manually specify an analysis script
that defines invariants on used objects, as well as the security requirements of the applications. Our tool, on the
other hand, operates on raw, unmodified Rails applications,
and does not require any developer input. This is due to
the different focus; we are focusing on one specific type of
flaw, while Rubyx is broader and can verify different types
of security violations.
Our work is also related to numerous white-box tools that
have previously been published. Huang et al. were one
of the first to propose a static analysis tool for a serverside scripting language, specifically PHP. They implemented
taint propagation to detect XSS, SQL injection, and general
injection [24]. Livshits and Lam proposed a static analysis
technique for Java web applications that used points-to analysis for improved precision [28]. Their tool detected 29 in-

stances of SQL injection, XSS, HTTP response splitting, and
command injection in nine open-source applications. Jovanovic et al. developed Pixy, an open-source static analysis
tool to discover XSS attacks by performing flow-sensitive,
inter-procedural, and context-sensitive data flow analysis
on PHP web applications [26]. They later improved Pixy,
adding precise alias analysis, to discover hundreds of XSS
vulnerabilities in three PHP applications, half of which were
false positives [25]. Balzarotti et al. used static and dynamic
analysis to develop MiMoSa, a tool that performs intermodule data flow analysis to discover attacks that leverage
several modules, such as stored XSS. They found 27 data
flow violations in five PHP web applications [4].
All of these static analysis tools differ from our white box
tool because we are not looking for unsanitized input vulnerabilities, but rather for unexpected execution that a developer did not intend. We also performed our analysis on
a large corpus of real-world applications, and found a correspondingly large number of true vulnerable EARs.

7. CONCLUSIONS
We have described a new type of vulnerability, Execution
After Redirect, and developed a novel static analysis tool to
effectively find EARs. We showed that EARs are difficult to
differentiate between benign and vulnerable. This difficulty
is due to vulnerable EARs violating the specific logic of the
web application. Better understanding of the application’s
logic should help differentiate vulnerable and benign EARs
and it will be the focus of future work.

Acknowledgments
This work was also partially supported by the ONR under
grant N000140911042 and by the National Science Foundation (NSF) under grants CNS-0820907, CNS-0905537, and
CNS-0716095.

8. REFERENCES
[1] ASP.NET MVC. http://www.asp.net/mvc.
[2] Balduzzi, M., Egele, M., Kirda, E., Balzarotti,
D., and Kruegel, C. A Solution for the Automated
Detection of Clickjacking Attacks. In Proceedings of
the ACM Symposium on Information, Computer and
Communications Security (AsiaCCS) (Beijing, China,
April 2010).
[3] Balduzzi, M., Gimenez, C., Balzarotti, D., and
Kirda, E. Automated discovery of parameter
pollution vulnerabilities in web applications. In
Proceedings of the 18th Network and Distributed
System Security Symposium (2011).
[4] Balzarotti, D., Cova, M., Felmetsger, V. V.,
and Vigna, G. Multi-module vulnerability analysis of
web-based applications. In Proceedings of the 14th
ACM conference on Computer and communications
security (New York, NY, USA, 2007), CCS ’07, ACM,
pp. 25–35.
[5] Barth, A., Jackson, C., and Mitchell, J. C.
Robust defenses for cross-site request forgery. In
Proceedings of the 15th ACM Conference on Computer
and Communications Security (CCS 2008) (2008).
[6] Boe, B. UCSB’s International Capture The Flag
Competition 2010 Challenge 6: Fear The EAR.

[7]

[8]

[9]

[10]

[11]

[12]
[13]

[14]

[15]

[16]

[17]

[18]

[19]
[20]

[21]
[22]

[23]

http://cs.ucsb.edu/~bboe/r/ictf10, December
2010.
Boe, B. Using StackOverflow’s API to Find the Top
Web Frameworks. http://cs.ucsb.edu/~bboe/r/
top-web-frameworks, February 2011.
Boehm, B. W. Software Engineering Economics,
1st ed. Prentice Hall PTR, Upper Saddle River, NJ,
USA, 1981.
Include exit with a redirect call. http://replay.web.
archive.org/20061011152124/https://trac.
cakephp.org/ticket/1076, August 2006.
docs should mention redirect does not ”exit” a script.
http://replay.web.archive.org/20061011180440/
https://trac.cakephp.org/ticket/1358, August
2006.
Cake Software Foundation, Inc. The CakePHP
1.3 Book. http://book.cakephp.org/view/982/
redirect, 2011.
Carettoni, L., and Di Paola, S. HTTP Parameter
Pollution. OWASP AppSec Europe 2009, May 2009.
Chaudhuri, A., and Foster, J. Symbolic security
analysis of ruby-on-rails web applications. In
Proceedings of the 17th ACM Conference on Computer
and Communications Security (CCS’10) (2010), ACM,
pp. 585–594.
Childers, N., Boe, B., Cavallaro, L., Cavedon,
L., Cova, M., Egele, M., and Vigna, G.
Organizing large scale hacking competitions. In
Proceedings of the 7th international conference on
Detection of intrusions and malware, and vulnerability
assessment (Berlin, Heidelberg, 2010), DIMVA’10,
Springer-Verlag, pp. 132–152.
Django Software Foundation. Django shortcut
functions. http://docs.djangoproject.com/en/dev/
topics/http/shortcuts/#django.shortcuts.
redirect, 2011.
EllisLab, Inc. CodeIgniter User Guide Version 2.0.2.
http://codeigniter.com/user_guide/helpers/url_
helper.html, 2011.
Felmetsger, V., Cavedon, L., Kruegel, C., and
Vigna, G. Toward Automated Detection of Logic
Vulnerabilities in Web Applications. In Proceedings of
the USENIX Security Symposium (Washington, DC,
August 2010).
Furr, M., hoon (David) An, J., Foster, J. S.,
and Hicks, M. The Ruby intermediate language. In
Proceedings of the ACM SIGPLAN Dynamic
Languages Symposium (DLS) (Oct. 2009).
GitHub. http://github.com.
Indictment in U.S. v. Albert Gonzalez. http://www.
justice.gov/usao/ma/news/IDTheft/Gonzalez,
%20Albert%20-%20Indictment%20080508.pdf, August
2008.
Hansen, R. Clickjacking. http://ha.ckers.org/
blog/20080915/clickjacking/, September 2008.
Hofstetter, D. Don’t forget to exit after a redirect.
http://cakebaker.wordpress.com/2006/08/28/
dont-forget-to-exit-after-a-redirect/, August
2006.
hoon An, J., Chaudhuri, A., and Foster, J.
Static typing for ruby on rails. In Proceedings of the

[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]
[34]
[35]

[36]

24th IEEE/ACM Conference on Automated Software
Engineering (ASE’09) (2009), IEEE, pp. 590–594.
Huang, Y.-W., Yu, F., Hang, C., Tsai, C.-H.,
Lee, D.-T., and Kuo, S.-Y. Securing web
application code by static analysis and runtime
protection. In Proceedings of the 13th international
conference on World Wide Web (New York, NY, USA,
2004), WWW ’04, ACM, pp. 40–52.
Jovanovic, N., Kruegel, C., and Kirda, E. Pixy:
A static analysis tool for detecting web application
vulnerabilities (short paper). In IN 2006 IEEE
SYMPOSIUM ON SECURITY AND PRIVACY
(2006), pp. 258–263.
Jovanovic, N., Kruegel, C., and Kirda, E.
Precise alias analysis for static detection of web
application vulnerabilities. In Proceedings of the 2006
workshop on Programming languages and analysis for
security (New York, NY, USA, 2006), PLAS ’06,
ACM, pp. 27–36.
Klein, A. Divide and conquer: HTTP response
splitting, Web cache poisoning attacks, and related
topics. http://www.packetstormsecurity.org/
papers/general/whitepaper/httpresponse.pdf,
2004.
Livshits, V. B., and Lam, M. S. Finding security
vulnerabilities in java applications with static analysis.
In Proceedings of the 14th conference on USENIX
Security Symposium - Volume 14 (Berkeley, CA, USA,
2005), USENIX Association, pp. 18–18.
Open Web Application Security Project
(OWASP). OWASP Top Ten Project. http://www.
owasp.org/index.php/Top_10, 2010.
Ortiz, C. Outcome of sentencing in U.S. v. Albert
Gonzalez. http://www.justice.gov/usao/ma/news/
IDTheft/09-CR-10382/GONZALEZ%20website%20info
%205-11-10.pdf, March 2010.
R. Fielding, J. Gettys, J. M. H. F. L. M. P. L.
T. B.-L. RFC 2616: Hypertext Transfer Protocol –
HTTP/1.1 Header Field Definitions. http://www.w3.
org/Protocols/rfc2616/rfc2616-sec14.html#
sec14.30, June 1999.
R. Fielding, J. Gettys, J. M. H. F. L. M. P. L.
T. B.-L. RFC 2616: Hypertext Transfer Protocol –
HTTP/1.1 Status Code Definitions. http://www.w3.
org/Protocols/rfc2616/rfc2616-sec10.html, June
1999.
Reenskaug, T. Models - views - controllers. Tech.
rep., Xerox Parc, 1979.
SpringSource. Contollers - Redirects. http://www.
grails.org/Controllers+-+Redirects, 2010.
Wang, R., Chen, S., Wang, X., and Qadeer, S.
How to shop for free online - security analysis of
cashier-as-a-service based web stores. In Proceedings of
the 32nd IEEE Symposium on Security and Privacy
(Oakland, CA, May 2011), IEEE.
Zend Technologies Ltd. Zend Framework:
Documentation: Action Helpers - Zend Framework
Manual. http://framework.zend.com/manual/en/
zend.controller.actionhelpers.html#zend.
controller.actionhelpers.redirector, 2011.

Toward a Moving Target Defense for Web Applications
(Invited Paper)
Marthony Taguinod, Adam Doupé, Ziming Zhao, and Gail-Joon Ahn
Arizona State University
{mtaguino, doupe, zmzhao, gahn}@asu.edu
Abstract—Web applications are a critical component of the
security ecosystem as they are often the “front door” for many
companies; as such, vulnerabilities in web applications allow
hackers access to companies’ private data, which contains
consumers’ private financial information. Web applications
are, by their nature, available to everyone, at anytime, from
anywhere, and this includes attackers. Therefore, attackers
have the opportunity to perform reconnaissance at their leisure,
acquiring information on the layout and technologies of the web
application, before launching an attack. However, the defender
must be prepared for all possible attacks and does not have
the luxury of performing reconnaissance on the attacker.
The idea behind Moving Target Defense (MTD) is to
reduce the information asymmetry between the attacker and
defender, ultimately rendering the reconnaissance information
misleading or useless. In this paper we take the first steps of
applying MTD concepts to web applications in order to create
effective defensive layers. We first analyze the web application
stack to understand where and how MTD can be applied.
The key issue here is that an MTD application must prevent
or disrupt a vulnerability or exploit, while still providing
identical functionality. Then, we discuss our implementation
of two MTD approaches, which can mitigate several classes of
web application vulnerabilities or exploits. We hope that our
discussion will help guide future research in applying the MTD
concepts to the web application stack.

I. I NTRODUCTION
Web applications remain the most popular way for businesses to provide services over the Internet. With more
web applications available, more sensitive business and
user data is managed and processed by web applications.
Consequently, vulnerabilities in web applications put both
businesses and end-users’ security and privacy at risk.
This is not an abstract risk, as the JPMorgan Chase breach
in 2014 affected 76 million US households [1]. Bloomberg
reported that the hackers “exploited an overlooked flaw in
one of the bank’s websites” [2]. Thus, web applications are
the “front door” for many companies, and therefore their
security is of paramount importance.
Many techniques and tools using static analysis (whitebox) or dynamic analysis (black-box) approaches have been
proposed and developed to discover the vulnerabilities of
web applications [3]–[7], so that the vulnerabilities can
be removed before attackers discover and exploit them.
However, the efforts of discovering and fixing vulnerabilities
are not enough to protect web applications for many reasons:
(1) the increasing complexity of modern web applications

brings inevitable risks that cannot be fully mitigated in the
process of web application development and deployment [8],
and (2) attackers can take their time, to understand the
web application’s functionality and technology stack, before
launching an attack.
We believe that a defense-in-depth approach is best to
securing web applications. Therefore, to complement the
aforementioned vulnerability analysis techniques, we propose to use the ideas of Moving Target Defense (MTD) to
create a novel and proactive approach that adds an additional
layer of defense to web applications. At a high level, a
moving target defense dynamically configures and shifts
systems over time to increase the uncertainty and complexity
for attackers to perform probing and attacking [9], [10].
While a system’s availability is preserved to legitimate users,
the system components are changed in unpredictable ways
to the attackers. Therefore, the attacker’s window of attack
opportunities decrease and the costs of attack increase. Even
if an attacker succeeds in finding a vulnerability at one point,
the vulnerability could be unavailable as the result of shifting
the underlying system, which makes the environment more
resilient against attacks.
To best apply the MTD ideas to protect web applications, there are two high-level decisions: (1) choose what
component to move in a web application, and (2) decide
the optimal time and how often to move components. To
assist in answering these questions, we dissect the modern
web application architecture, both client and server, and their
running environments to explore the possibilities of applying
MTD at different layers. We hope our analysis provides
insights into the trade-offs among the different places to
apply MTD to web applications.
We also discuss our first steps in applying MTD techniques to protect web applications. The first technique
changes the server-side language used in a web application by automatically translating server-side web application
code to another language in order to prevent Code Injection
exploits. The second technique shifts the database used in a
web application by transforming the backend SQL database
into different implementations that speak different dialects
in order to prevent SQL Injection exploits.
The main contributions of this paper are the following:
• We discuss the possibilities of applying moving target
defense to different layers of web applications.

•

We propose two novel approaches to changing the
implementation language of a web application and the
database implementation while keeping the functionality.
II. BACKGROUND

In order to properly understand how to apply the ideas
of moving target defense to web applications, we first
describe web application, then the ideas behind moving
target defense.
A. Web Applications
As shown in Figure 1, a web application follows a
distributed application structure, with components running
on the server and the client.
The client first uses the communication channels (typically the protocol HTTP and its derivative protocols, such
as HTTPS, SPDY, and HTTP/2) to issue a request to the
server-side component.
The server side typically includes the following layers
from top to bottom1 :
• The server-side logic layer implements the application business logic using high-level programming languages, such as Java, PHP, or Python.
• The web server layer receives the HTTP request from
then client, parses the HTTP request, and passes the request to the appropriate server-side program. Examples
include Apache web server, Windows IIS, or Nginx.
• The data storage layer that stores the web application state and user data. Popular data storage systems
are traditional SQL databases, which include MySQL,
PostgreSQL, or MSSQL.
• The operating system layer that provides the running
environment for the web server layer and database
storage layer.
• The infrastructure layer that runs the operating systems.
An infrastructure could be a physical machine or a
virtualization platform which manages multiple virtual
machines.
The client receives the HTTP response from the serverside code, and the job of the client is to convert the HTML
contained in the HTTP response into a graphical interface
for the user. The client includes:
• The client-side logic layer that is usually called the
presentation layer. This is written in a combination of
HTML, CSS, and JavaScript, with JavaScript providing
a way for the server-side code to execute application
logic on the client.
• The browser retrieves the presentation layer code from
the server, interprets it, and presents it as a graphical
interface to the user.
1 Of course, modern web application stacks can become increasingly
complex, with caches, external requests, or other services, however in this
paper we restrict our discussion to this abstracted model.

The storage layer that the presentation layer code
uses to store data. Available storage methods include
cookies, localStorage, IndexedDB, and File
APIs.
• The operating system layer, which the browser runs on.
If a layer in Figure 1 is compromised, its upper layers
are not trustworthy. For instance, if the server’s operating
system is compromised, then the data storage, web server,
and server-side logic are all compromised. Because the
presentation layer is created by the server and sent across
the communication channel, a compromise of the server or
the communication channel compromises the presentation
layer. Adversaries can attack a layer in Figure 1 through its
interfaces exposed to the upper layers.
For example, in a heap spraying attack on the client
browser layer [11], an attacker allocates malicious objects
using JavaScript in the presentation layer to coerce the
browser to spray objects in the heap, increasing the success
rate of an exploit that jumps to a location within the heap.
In this case, the attacker uses the presentation layer to
exploit a vulnerability in the browser layer that leads to
arbitrary code execution in the browser’s address space. The
injected arbitrary code can in turn exploit a vulnerability
in the client operating system to escalate its privilege and
further infect the client machine. Furthermore, the malicious
JavaScript code might be delivered by an attacker exploiting
a vulnerability in the server-side logic layer, using a reflected
or stored cross-site scripting (XSS) vulnerability.
•

B. Moving Target Defense
The basic idea of moving target defense (MTD) is to
continually shift and change system configurations over time
to increase complexity and cost for attackers. MTD does not
remove vulnerabilities directly but limits the exposure of
vulnerabilities, so opportunities for attack can be decreased.
In this way, MTD acts as part of a defense-in-depth strategy.
The effectiveness of an MTD approach depends on how
many components are moved (what-to-move) and the frequency of movement (when-to-move).
The widely adopted address space layout randomization
(ASLR) [12] in modern operating systems is an instance
of MTD. Existing ASLR mechanisms randomly arrange the
address space positions of key data areas (what-to-move)
of a process when it is launched (when-to-move), including
the base of the executable and the positions of the stack,
heap, and libraries. In this way, even if attackers are able
to exploit a memory corruption vulnerability in a binary
(such as the classic buffer overflow), it is difficult for them
to transfer control flow to their injected shellcode, as they
cannot predict the memory layout of the process.
MTD mechanisms for programs can be categorized into
two classes depending on if a program is running (dynamic)
or not (static) at the time when moving happens. For
example, existing ASLR approaches are static, because the

Client side

Server side

Presentation Layer

Logic Layer

Web Server Layer

Browser

Storage Layer
Local Storage

Communication
Channel

Cookies, IndexedDB,
localStorage, File API

Operating System

Application
Layer

Operating System

Infrastructure

HTTP/1.1, HTTP/2

Figure 1.

A Modern Web Application Architecture and Its Running Environments.

positions of code and data areas are only moved at the launch
of a program but not when a program is running. A dynamic
MTD offers more choices for when-to-move, but may be
more difficult to implement.
III. M OVING TARGET D EFENSE FOR W EB
A PPLICATIONS
The core idea of moving target defense (MTD) can be
applied to every layer of web applications and their running
environments. However, the key issue is that the “moving,”
when applied, must either prevent a vulnerability or exploit
while at the same time not alter the application functionality. In this section, we discuss what components that are
available for moving at the web application layers. We
specifically focus on the layers specific to web applications:
the logic layer, storage layer, and presentation layer. For
other layers that are common to other applications, which
include the operating system layer and the infrastructure
layer, we refer the interested reader to research in these
areas [12]–[21].
A. Logic Layer
There are at least two ways to apply MTD at the logic
layer by changing a web application’s implementation. The
first approach uses the idea of software diversity [14], which
is widely used in lower-level languages, to change and
modify the code at the statement, function, or object levels.
This type of diversity is used to combat memory corruption
vulnerabilities, specifically return-oriented programming exploits, which take advantage of previously known codelayouts. This automated diversity MTD technique can be

done statically or dynamically. However, many web applications are written in higher-level languages, such as
Java, Python, and Ruby, which are immune from memory
corruption vulnerabilities. In fact, most web application
vulnerabilities are inherent in the code itself, such as CrossSite Scripting (XSS), where the server-side web application
code creates HTML from unsanitized, untrusted attacker
input. Software diversity does not handle this case, as the
vulnerability is part of the web application’s logic.

Another MTD approach is to switch a web application’s
implementation from one language to another, which could
eliminate some language- or framework-specific vulnerabilities, as some vulnerability classes are specific to certain programming languages. For example, an application that is developed with Ruby on Rails 3.0.5 may introduce executionafter-redirect vulnerabilities, while its counterpart developed
with Python and Django 1.2.5 is impervious to this class of
vulnerabilities due to the different implementations of the
underlying framework [22]. Changing the web application’s
implementation language could be either static or dynamic.
In a static switch, the web server simply launches another
language implementation of the application. To make the
process automated, web application developers only need
to develop the application once using the language they
prefer, and a translator program translates their code into
functionally equivalent code in the other language. The
translation is difficult when the input language has some
features that the output language can not offer. In a dynamic
switch, the states of the running web application would
need to be maintained or transformed for the program in

another language to understand. In Section IV we discuss
our implementation of this idea.
B. Storage Layer
The biggest challenge the storage layer faces are SQL
injection attacks in which data from the logic layer is
interpreted as SQL statements by the database management
systems. In order to perform successful SQL injection attacks, attackers need to carefully craft their input by using
some reversed tokens in the targeted SQL syntax in order to
modify the logic layer developer’s intended SQL statement.
While SQL itself is standard, different SQL database
implementations use slightly different SQL syntaxes (also
called dialects). Taking advantage of the fact that different
databases use slightly different SQL syntaxes, switching the
database used in a web application may defeat some SQL
injection exploits that are targeted at a specific SQL dialect.
For example, both single ('') and double ("") quotes
are used for quoting values in MySQL—while PostgreSQL
uses only single quotes for values, instead reserving double
quotes for identifying field names, table names, etc.
Static MTD for the database can be realized by exporting the data from one database implementation and
then importing it into a different database implementation.
Dynamic MTD for storage layer can also be achieved if
multiple, yet different, database instances are running and
being continually synchronized. In Section V we discuss
our implementation of this idea.
C. Presentation Layer
The presentation layer contains technologies that are
most directly accessible to the user. For instance, clientside JavaScript code running some of the web application’s
logic, the HTML DOM containing form information, and
CSS that enables modification of the web layout. The most
direct threat of the presentation layer are Cross-Site Scripting
(XSS) attacks, where malicious scripts are injected into the
web application in order to steal information from users.
There are techniques related to MTD that have been
proposed to prevent against such attacks. One such technique
is to introduce a degree of randomness to the underlying
HTML form fields by adding tags to each field that hides
their real values [18]. Another approach, targeting a different
technology, is to introduce randomness to the JavaScript
code by mutating tokens in such a way that the attacker
cannot guess the correct token to inject in addition to running
multiple versions of the website that each utilizes varying
JavaScript versions [23].
D. Browsers
Modern web browsers have modularized architectures that
typically include rendering engines, JavaScript interpreters,
and XML parsers [24]. By moving and changing these
components, vulnerabilities in particular components can be

mitigated. In this way, the browser itself and the underlying
operating system can be protected. For example, the Cheetah
browser [25] and the 360 browser [26] can change their
rendering engines between WebKit and Trident.
Besides protecting browsers from exploits, changing
browser configurations can also protect the privacy of web
users. Every browser instance has its unique configurations, therefore web applications can uniquely fingerprint a
browser in order to track users [27]. Diversifying a browser’s
font, plugins, and other configurations can prevent it from
being fingerprinted, hence protecting the privacy of web
users [28]–[30].
IV. S OURCE C ODE L ANGUAGE D IVERSIFICATION
To apply MTD ideas in the logic layer of the serverside, we propose to change the underlying language implementation of the web application; taking care to retain the
main functionalities of the original application. In doing so,
we prevent certain categories of vulnerabilities from being
effectively exploited—remote code injection exploits would
cease to work as code an attacker manages to insert will
not match the web application’s language. In addition, any
unpatched or zero-day vulnerabilities present in the original
language would not be available for exploit during the time
frame of the randomization, as the language is completely
different from what the attacker original perceives it to be.
In this section, we describe our implementation of a
static MTD mechanism for the logic layer. As a first step,
we choose to convert between PHP, a web development
language used by approximately 82% of all web applications [31] and Python, which is used by popular companies
such as Google, YouTube, Pinterest, and Bing [32], [33]. Our
approach is to develop a translator to automatically convert a
Python web application to PHP. We first translate the source
code as-is, resulting in syntactically valid, but semantically
invalid output in the target language.
We approached the problem of translating from Python
to PHP by first exploring the available open source tools.
However, no such tool exists, and we believe that this is
due to the varying web application frameworks available for
Python. While PHP is a programming language that was
built for creating server-side web application logic, Python is
a general-purpose language that can be used to write serverside web application logic. Therefore, there are many different frameworks for building server-side web applications
in Python. For this reason, we focus on Python applications
that utilize cgi-lib, however the our translation concept
is general and can be implemented for other frameworks in
the future.
Our translator is essentially a compiler, and to speed
development we leverage existing functionality in Python
to initiate the first step in translating to PHP—specifically,
the use of Python’s ast module to build an Abstract Syntax
Tree (AST) of a Python program. Then, we use the Python

def

Print ( self , t ):
self . f i l l (” print ”)
do comma = F a l s e
if t . dest :
s e l f . w r i t e ( ”>>” )
self . dispatch ( t . dest )
do comma = T r u e
for e in t . values :
i f do comma : s e l f . w r i t e ( ” , ” )
e l s e : do comma= T r u e
self . dispatch ( e)
i f not t . n l :
self . write ( ” ,” )

Listing 1.

def

public function exit ( $status )
{
exit ( $status )
}
}

Listing 3.

sys Object in PHP

Original Print in unparse to generate Python code.

Print ( self , t ):
s e l f . f i l l ( ” echo ” )
do comma = F a l s e
if t . dest :
s e l f . w r i t e ( ”>>” )
self . dispatch ( t . dest )
do comma = T r u e
for e in t . values :
i f do comma : s e l f . w r i t e ( ” , ” )
e l s e : do comma= T r u e
self . dispatch ( e)
i f not t . n l :
self . write ( ” ,” )
self . write ( ” ; ” )

Listing 2.

c l a s s sys
{
<... c o n s t r u c t o r ... >
<... other f u n c t i o n s ... >

Modified Print in unparse to generate PHP code.

unparse module, which is a module that converts an
AST to Python code. We develop a new library based on
unparse that generates PHP code instead of Python code.
Specifically, we modify the unparse module code by
replacing the print-to-output function for a given
Python statement with the corresponding PHP-specific statement. For instance, when translating a simple print statement from Python to the PHP equivalent of echo, we
modify the _Print function in the unparse module as
shown in Listing 1.
We replace the print Python keyword with the echo
PHP keyword and ensure that the instruction is terminated
with a semicolon as shown in Listing 2.
Once this is done, we have a program that is syntactically
valid PHP, however it does not have the same semantics
as the original Python program. Therefore, the next step is
to make the translated program semantically equivalent to
the original program. This step is necessary because there
may not be a one-to-one translation for every feature in a
language to another. For example, a Python instruction to
terminate and exit the program is done using:
sys . e x i t (0)

After the translation is done and a PHP valid output is
generated:
s y s−>e x i t ( 0 )

However, PHP sees this as a new sys object with a
call to a function exit(0), which does not exist in PHP.

The instruction does however have an equivalent function
call—exit(status) in PHP. Therefore, we implement
Python built-in functions as shims in order to match the
new function calls. To this end, we create a PHP library
that contains an object called sys with a function call to
exit(status) as shown in Listing 3. This PHP library
shim can be included in the translated application in order
for the function call to remain semantically valid.
Using this approach, we recursively run the tool on the
Python functions that the original program calls, and convert
them as well. If, for instance, the original program is written
in C, then we create a function shim for it.
However, in order to achieve the MTD goal, we must
also decide on how frequently to move or randomize the
component in order to be effective while considering the
cost to legitimate users. Furthermore, there may be risk in
the translated application missing critical function calls that
we have not yet created shims. Finally, we anticipate this
approach to be resource and time intensive as it is essentially
creating two implementations of one web application.
V. DATABASE D IALECT D IVERSIFICATION
To enable movement in the server-side storage layer, we
implemented an approach to change the underlying database
implementation, while preserving data and retaining functionality. In doing so, we again protect against certain
categories of vulnerabilities and exploits—SQL injection
exploits would be rendered ineffective due to syntactical
differences between database implementations.
When performing the database translation, no alterations
must be made to the data content—that is, once the translation is completed, the users must see the same information regardless of the underlying database implementation.
Access to the database must be guaranteed and kept transparent to the user during and after the translation process.
Database translations may be costly as well, especially
regarding larger, more established databases—optimizations
in the original implementation may become invalid once
converted. Similar to our source to source approach, by
continuously changing database implementations, we expect
any database-specific exploits as well as unpatched or zeroday vulnerabilities will be ineffective.

As a first step, we choose to convert between MySQL and
PostgreSQL, which are ranked 2nd and 5th in db-engines.com
popularity ranking, respectively [34]. MySQL is used by
well known companies, such as Facebook, Google, Amazon
and Dropbox [35] and PostgreSQL is used by U.S. Dept of
labor, U.S. State Department, and Sun Microsystems [36].
Some differences between the MySQL and PostgreSQL
syntaxes include:
• The # or -- (A space after the -- is required) is
used to begin a comment in MySQL, while PostgreSQL
instead uses -- (the space is not required).
• Single ('') quotes or double ("") quotes are used in
quoting values in MySQL, while PostgreSQL uses only
single quotes for values, reserving double quotes in
identifying field names, table names, etc.
• MySQL is case-insensitive when doing string comparision while PostgreSQL is case-sensitive, i.e.
john != JOHN != John.
All of these differences affect the SQL injection exploits
written to take advantage of an SQL injection vulnerability.
If an attacker assumes that the web application is using a
particular database backend, specifically if the attacker is
searching the entire web for vulnerabilities, the exploit will
fail.
Similar to our source-to-source approach, we developed
a tool that can automate the conversion or migration between databases. In order to convert from PostgreSQL to
MySQL, we modifed an existing open-source tool created
by Lightbox that converts PostgreSQL to MySQL—although
we can simply create a database dump from PostgreSQL,
directly importing to MySQL will not work as there are
differences between syntax and data types, which must
be properly translated. In addition, certain flags need to
be enabled when creating a database dump to allow for
initial compatibility (PostgreSQL db dumps need to have
--inserts enabled to properly include the data stored;
MySQL needs to have --compatible=postgresql
flag to properly include PostgreSQL keywords). To remedy
this situation, our tool processes the original database dump
by parsing the file and replacing any PostgreSQL keywords
and data-types into corresponding MySQL keywords and
data types. Some considerations have to made regarding
conversions between data types, for instance PostgreSQL’s
BYTEA can be converted to any of the MySQL data types
shown in Table I.
The data type chosen needs to be generic enough that it
covers the possible data value that is in the original database,
while attempting to be as performant as possible. When
testing the original implementation of the converter, we observed that the output did not generate a database dump that
is supported by the latest version of MySQL. In addition,
it did not correctly convert the raw database dump from
PostgreSQL, as the final output still contained PostgreSQL
keywords and data-types. To handle conversion in the reverse

MySQL
BINARY(n)
VARBINARY(n)
TINYBLOB
BLOB
MEDIUMBLOB
LONGBLOB

PostgreSQL
BYTEA
BYTEA
BYTEA
BYTEA
BYTEA
BYTEA

Table I
C OMPARISON OF M Y SQL AND P OSTGRE SQL DATA T YPES .

direction, from MySQL to PostgreSQL, we re-purposed the
code by reversing the process—that is, we parse through
the dump file looking for MySQL keywords and data-types
converting them to the corresponding PostgreSQL keywords
and data-types.
VI. R ELATED W ORK
The idea and philosophy of moving target defense, which
is to increase uncertainty and complexity for attackers, has
been proposed and studied for decades [37]–[40]. Okhravi et
al. surveyed techniques that applied the philosophy of moving target defense in different cyber research domains [41].
According to them, existing techniques can be categorized
into five classes based on what-to-move: (1) changing runtime environment [12], [13], (2) changing application’s code
dynamically or diversifying software [14], [15], (3) changing
data representations [38], [42], (4) changing platforms [16],
[17], and (5) changing network configurations [43]–[45].
However, applying the moving target defense concept
to web applications is still new. Huang et al. proposed
to create and rotate among a set of virtual servers, each
of which is configured with a unique software mix, to
move attack surfaces for web surfaces [46]. Their work
also explored the various opportunities of diversification
in the web application software stack, providing a higherlevel overview of the attack surface. Our work builds on
this by further analyzing the components in each layer and
defining what randomization in each layer entails, in addition
to attempting an automated approach to diversification in
the logic and storage layer. Boyd et al. proposed to create
instances of unpredictable database query languages and
to translate them to standard SQL using an intermediary
proxy to prevent SQL injection attacks [47]. Although their
approach also aims to prevent SQL injections, we chose a
different approach in order to prevent a broader range of
vulnerabilities—specifically unpatched vulnerabilities, zero
day exploits, and mass-attacks targeting specific database
implementations. Portner et al. proposed to defend crosssite scripting by mutating the symbols in JavaScript so
that maliciously injected code can be identified [23]. Their
work aims to prevent a different class of vulnerabilities,
specifically located at the presentation layer on the client
side. Our proposed approaches are aimed at applying MTD
ideas on the server side of the web application architecture.

However, we envision techniques such as these, that are in
each layer, to cooperate together to provide a defense-indepth approach to defending web applications.
VII. F UTURE W ORK
As part of our future work, we plan to address the
remaining semantic issues after translating from a Python
application to a syntactically correct PHP application—one
such issue is the translation of Python data structures to their
equivalent in PHP; for instance, Python lists do not have a
direct equivalent in PHP. Another component that we plan
to explore is the possibility of automating the conversion
to other web development languages. Similarly, we plan
to explore the possibility of conversion to other database
implementations. In addition, we will further investigate the
other web application layers that can be moved.
At the heart of any MTD mechanism is the technique
to decide when to move the chosen components. As such,
we also plan to explore the various movement schemes
and its effect on web applications that use our MTD approach. Finally, we will create a modular framework that
can automatically apply our implemented MTD techniques
in each layer of the web application in order to create a vast
number of possible configurations. We will then evaluate
this framework using real web applications deployed on a
real-world network, in order to measure its effectiveness.
VIII. C ONCLUSION
In this paper we explored the feasibility of applying
MTD concepts to web applications in order to create
defensive layers. We analyzed the web application stack
to understand where and how MTD can be applied, as
well as current techniques implemented in each layer. In
addition, we also discussed our implementation of two
MTD approaches, which can mitigate several classes of web
application vulnerabilities or exploits, wherein we change
the language implementation of the web application and the
database implementation while retaining functionality. We
believe that MTD offers an exciting new research area in
defending web applications, and we believe that the future
of web application defense lies in reducing the information
asymmetry inherent in the current web application security
environment.
ACKNOWLEDGMENT
This work was partially supported by the grant from
National Science Foundation (NSF-SFS-1129561).
R EFERENCES
[1] J. Silver-Greenberg, M. Goldstein, and N. Perlroth, “JPMorgan Chase Hacking Affects 76 Million Households,” The New
York Times, Oct. 2014.
[2] J. Robertson and M. Riley, “JPMorgan Hack Said to Span
Months Via Multiple Flaws,” Aug. 2014.

[3] D. Balzarotti, M. Cova, V. Felmetsger, N. Jovanovic, E. Kirda,
C. Kruegel, and G. Vigna, “Saner: Composing static and
dynamic analysis to validate sanitization in web applications,”
in Security and Privacy, 2008. SP 2008. IEEE Symposium on.
IEEE, 2008, pp. 387–401.
[4] V. Felmetsger, L. Cavedon, C. Kruegel, and G. Vigna, “Toward automated detection of logic vulnerabilities in web
applications,” in USENIX Security Symposium, 2010, pp. 143–
160.
[5] N. Jovanovic, C. Kruegel, and E. Kirda, “Static analysis
for detecting taint-style vulnerabilities in web applications,”
Journal of Computer Security, vol. 18, no. 5, pp. 861–907,
2010.
[6] A. Doupé, L. Cavedon, C. Kruegel, and G. Vigna, “Enemy of the State: A State-Aware Black-Box Vulnerability
Scanner,” in Proceedings of the USENIX Security Symposium
(USENIX), Bellevue, WA, August 2012.
[7] A. Doupé, W. Cui, M. H. Jakubowski, M. Peinado,
C. Kruegel, and G. Vigna, “deDacota: Toward Preventing
Server-Side XSS via Automatic Code and Data Separation,”
in Proceedings of the ACM Conference on Computer and
Communications Security (CCS), Berlin, Germany, November
2013.
[8] D. Wichers, “Owasp top-10 2013,” OWASP Foundation,
February, 2013.
[9] A. Cui and S. J. Stolfo, “Symbiotes and defensive mutualism:
Moving target defense,” in Moving Target Defense. Springer,
2011, pp. 99–108.
[10] R. Zhuang, S. A. DeLoach, and X. Ou, “Towards a theory
of moving target defense,” in Proceedings of the First ACM
Workshop on Moving Target Defense, ser. MTD ’14. New
York, NY, USA: ACM, 2014, pp. 31–40. [Online]. Available:
http://doi.acm.org/10.1145/2663474.2663479
[11] P. Ratanaworabhan, V. B. Livshits, and B. G. Zorn, “Nozzle:
A defense against heap-spraying code injection attacks.” in
USENIX Security Symposium, 2009, pp. 169–186.
[12] P. Team, “Address space layout randomization,” Phrack, 2003.
[13] E. G. Barrantes, D. H. Ackley, T. S. Palmer, D. Stefanovic,
and D. D. Zovi, “Randomized instruction set emulation to
disrupt binary code injection attacks,” in Proceedings of the
10th ACM conference on Computer and communications
security. ACM, 2003, pp. 281–289.
[14] P. Larsen, A. Homescu, S. Brunthaler, and M. Franz, “Sok:
Automated software diversity,” in Security and Privacy (SP),
2014 IEEE Symposium on. IEEE, 2014, pp. 276–291.
[15] R. Wartell, V. Mohan, K. W. Hamlen, and Z. Lin, “Binary
stirring: Self-randomizing instruction addresses of legacy x86
binary code,” in Proceedings of the 2012 ACM conference on
Computer and communications security. ACM, 2012, pp.
157–168.
[16] D. Williams, W. Hu, J. W. Davidson, J. D. Hiser, J. C.
Knight, and A. Nguyen-Tuong, “Security through diversity:
Leveraging virtual machine technology,” Security & Privacy,
IEEE, vol. 7, no. 1, pp. 26–33, 2009.

[17] B. Salamat, T. Jackson, G. Wagner, C. Wimmer, and
M. Franz, “Runtime defense against code injection attacks
using replicated execution,” Dependable and Secure Computing, IEEE Transactions on, vol. 8, no. 4, pp. 588–601, 2011.
[18] S. Vikram, C. Yang, and G. Gu, “Nomad: Towards nonintrusive moving-target defense against web bots,” in Communications and Network Security (CNS), 2013 IEEE Conference on. IEEE, 2013, pp. 55–63.
[19] M. Dunlop, S. Groat, W. Urbanski, R. Marchany, and J. Tront,
“Mt6d: A moving target ipv6 defense,” in MILITARY COMMUNICATIONS CONFERENCE, 2011 - MILCOM 2011,
Nov 2011, pp. 1321–1326.
[20] M. Carvalho and R. Ford, “Moving-target defenses for computer networks,” Security Privacy, IEEE, vol. 12, no. 2, pp.
73–76, Mar 2014.
[21] Y. Li, R. Dai, and J. Zhang, “Morphing communications of
cyber-physical systems towards moving-target defense,” in
Communications (ICC), 2014 IEEE International Conference
on, June 2014, pp. 592–598.
[22] A. Doupé, B. Boe, C. Kruegel, and G. Vigna, “Fear the ear:
discovering and mitigating execution after redirect vulnerabilities,” in Proceedings of the 18th ACM conference on
Computer and communications security. ACM, 2011, pp.
251–262.
[23] J. Portner, J. Kerr, and B. Chu, “Moving target defense against
cross-site scripting attacks (position paper),” in Foundations
and Practice of Security. Springer, 2014, pp. 85–91.
[24] C. Reis, A. Barth, and C. Pizano, “Browser security: lessons
from google chrome,” Queue, vol. 7, no. 5, p. 3, 2009.
[25] Liebao, “Cheetah browser. http://www.liebao.cn/index.html.”
[26] Qihu, “360 browser. http://www.360safe.com/browser.html.”
[27] P. Eckersley, “How unique is your web browser?” in Privacy
Enhancing Technologies. Springer, 2010, pp. 1–18.
[28] P. Laperdrix, W. Rudametkin, and B. Baudry, “Mitigating
browser fingerprint tracking: multi-level reconfiguration and
diversification,” in Proceedings of the International Symposium on Software Engineering for Adaptive and SelfManaging Systems (SEAMS’15), 2015.
[29] N. Nikiforakis, W. Joosen, and B. Livshits, “PriVaricator: Deceiving fingerprinters with Little White Lies,” in Proceedings
of the International World Wide Web Conference (WWW),
2015.
[30] N. Nikiforakis, A. Kapravelos, W. Joosen, C. Kruegel,
F. Piessens, and G. Vigna, “Cookieless Monster: Exploring
the Ecosystem of Web-based Device Fingerprinting,” in Proceedings of the IEEE Symposium on Security and Privacy,
2013.
[31] Ide,
“Php
just
grows
&
grows.
http://news.netcraft.com/archives/2013/01/31/php-justgrows-grows.html.”

[32] Sangster,
“Organizations
using
python.
https://wiki.python.org/moin/organizationsusingpython.”
[33] Donohue,
“Top
15
sites
built
with
python.
http://coderfactory.com/posts/top-sites-built-with-python.”
[34] “Db-engines ranking. http://db-engines.com/en/ranking.”
[35] “Mysql users. https://www.mysql.com/customers/.”
[36] “Postgresql users. http://www.postgresql.org/about/users/.”
[37] A. Avizienis and L. Chen, “On the implementation of nversion programming for software fault tolerance during
execution,” in Proc. IEEE COMPSAC, vol. 77, 1977, pp. 149–
155.
[38] P. E. Ammann and J. C. Knight, “Data diversity: An approach
to software fault tolerance,” Computers, IEEE Transactions
on, vol. 37, no. 4, pp. 418–425, 1988.
[39] K. Pettis and R. C. Hansen, “Profile guided code positioning,”
in ACM SIGPLAN Notices, vol. 25, no. 6. ACM, 1990, pp.
16–27.
[40] S. Forrest, A. Somayaji, and D. H. Ackley, “Building diverse
computer systems,” in Operating Systems, 1997., The Sixth
Workshop on Hot Topics in. IEEE, 1997, pp. 67–72.
[41] H. Okhravi, M. Rabe, T. Mayberry, W. Leonard, T. Hobson,
D. Bigelow, and W. Streilein, “Survey of cyber moving target
techniques,” DTIC Document, Tech. Rep., 2013.
[42] A. Nguyen-Tuong, D. Evans, J. C. Knight, B. Cox, and
J. W. Davidson, “Security through redundant data diversity,”
in Dependable Systems and Networks With FTCS and DCC,
2008. DSN 2008. IEEE International Conference on. IEEE,
2008, pp. 187–196.
[43] R. Zhuang, S. Zhang, A. Bardas, S. DeLoach, X. Ou, and
A. Singhal, “Investigating the application of moving target
defenses to network security,” in Resilient Control Systems
(ISRCS), 2013 6th International Symposium on, Aug 2013,
pp. 162–169.
[44] L. Ge, W. Yu, D. Shen, G. Chen, K. Pham, E. Blasch,
and C. Lu, “Toward effectiveness and agility of network
security situational awareness using moving target defense
(mtd),” vol. 9085, 2014, pp. 90 850Q–90 850Q–9. [Online].
Available: http://dx.doi.org/10.1117/12.2050782
[45] J. H. Jafarian, E. Al-Shaer, and Q. Duan, “Openflow
random host mutation: Transparent moving target defense
using software defined networking,” in Proceedings
of the First Workshop on Hot Topics in Software
Defined Networks, ser. HotSDN ’12. New York, NY,
USA: ACM, 2012, pp. 127–132. [Online]. Available:
http://doi.acm.org/10.1145/2342441.2342467
[46] Y. Huang and A. K. Ghosh, “Introducing diversity and
uncertainty to create moving attack surfaces for web services,”
in Moving Target Defense. Springer, 2011, pp. 131–151.
[47] S. W. Boyd and A. D. Keromytis, “Sqlrand: Preventing sql
injection attacks,” in Applied Cryptography and Network
Security. Springer, 2004, pp. 292–302.

Checking Intent-based Communication in Android with
Intent Space Analysis
Yiming Jing† , Gail-Joon Ahn† , Adam Doupé† , and Jeong Hyun Yi‡
†

Arizona State University

‡

Soongsil University

{ymjing,gahn,doupe}@asu.edu, jhyi@ssu.ac.kr

ABSTRACT

A type of messaging objects called intents build a major
and sophisticated inter-application communication mechanism in Android [13]. Intents are flexible as they can carry
simple data and even inter-process communication primitives (e.g. Binder [2] and file descriptors [3]). Moreover, the
intent attributes are rich with Android middleware semantics, which naturally facilitate access control decisions [12,
29]. As a result, the security community proposed plenty
of security extensions that implement policy-driven mandatory access control (MAC) for intent-based communication
[7,10–12,21,26,27,29,32,38]. Indeed, intents are not the only
inter-application mechanism in Android. The recent MAC
implementations [7,12,32] adopt derivations of SELinux kernel MAC to cover the other Android mechanisms such as
files, sockets, and Binder. However, intents are out of the
scope of kernel MAC due to the incompatible semantics of
the kernel and middleware layers [12].
Defining and verifying the policy for each individual security extension that controls intent-based communication
is a complex task for a policy analyst. The recent emerging security requirements, such as “bring your own device”
(BYOD), call for fine-grained and precise policies. For example, a single mobile device may host a doctor’s personal apps
and the apps of several clinics. The doctor and the clinics
would require that the deployed security policies accurately
enforce the boundaries between the apps of the respective
stakeholders. Meanwhile, mitigating existing threats related
to intents such as communication hijacking [13], confused
deputy attacks [10, 19], and accidental data disclosure [26]
requires that policies are tailored to the peculiarities of each
threat and each vulnerable app.
Furthermore, the complexity significantly increases when
intent-based communication is mediated by multiple collaborating security extensions that enforce their respective security polices. First, the security extensions define incompatible schemes, logic, and semantics for their policies. Second, the policies that determine how intents are processed
and forwarded among apps are distributed across multiple
security extensions. Moreover, the policies are stored and
updated in a dynamic manner due to frequent app installs,
uninstalls, and upgrades. As a consequence, a policy analyst
must manually inspect every security extension’s policy, aggregate them into a holistic view, and search for violation of
security properties. Overall, policy verification becomes an
error-prone and tedious task that requires great sophistication from the policy analyst. This leads to slow adoption of
Android security extensions despite that quite a few modern
security extensions have been proposed recently.

Intent-based communication is an inter-application communication mechanism in Android. While its importance has
been proven by plenty of security extensions that protect it
with policy-driven mandatory access control, an overlooked
problem is the verification of the security policies. Checking one security extension’s policy is indeed complex. Furthermore, intent-based communication introduces even more
complexities because it is mediated by multiple security extensions that respectively enforce their own incompatible,
distributed, and dynamic policies.
This paper seeks a systematic approach to address the
complexities involved in checking intent-based communication. To this end, we propose intent space analysis. Intent
space analysis formulates the intent forwarding functionalities of security extensions as transformations on a geometric
intent space. We further introduce a policy checking framework called IntentScope that proactively and automatically
aggregates distributed policies into a holistic and verifiable
view. We evaluate our approach against customized Android OSs and commodity Android devices. In addition, we
further conduct experiments with four security extensions
to demonstrate how our approach helps identify potential
vulnerabilities in each extension.

1. INTRODUCTION
Modern mobile operating systems have shifted into a security architecture that is fundamentally different from those
of traditional desktop OSs. Mobile applications (commonly
referred to as apps) run as unique security principles; they
are isolated in their respective sandboxes and receive few
privileges. Despite that apps are isolated, they interoperate
through inter-application communication. As such, a few
apps, whose workflows are directed by a user, can accomplish complex and diversified tasks. For example, an email
client exports a picture file to a photo editor; the photo editor modifies the picture and posts it online through a social
network client.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

ASIA CCS ’16, May 30-June 03, 2016, Xi’an, China
© 2016 ACM. ISBN 978-1-4503-4233-9/16/05. . . $15.00
DOI: http://dx.doi.org/10.1145/2897845.2897904

1

2.

To effectively address the complexities of checking intentbased communication, we argue for the need of a general,
holistic, and proactive policy checking framework that analyzes the incompatible, distributed, and dynamic intentbased communication policies in Android. In particular, the
framework automatically aggregates the policies and generates a holistic view that lends itself well for formal verification. The tools that currently exist are dependent and
specialized to each security extension. For example, EASEAndroid [33] and SETools [4] are tailored to SEAndroid and
SELinux. To the best of our knowledge, we propose the first
tool to holistically check multiple security extensions.
This paper proposes intent space analysis to address the
complexities in checking intent-based communication. Intent space analysis is built upon a geometric intent space
model. In this model, we propose to represent intents with
a K-dimensional space of regular languages, in which each
dimension corresponds to an intent attribute. As such, an
intent maps to a point in the space, multiple intents map to
a subspace, and security extensions are modeled as transfer
functions that map one subspace to another. For example,
a security extension that denies any intent can be modeled
as a transfer function that maps all K attribute values (denoted as {.∗}K ) to an empty space regardless of source or
destination apps.
We further propose a policy checking framework, called
IntentScope. Given an Android device, IntentScope acquires and parses the live intent forwarding states of each security extension that controls intents. Afterwards, IntentScope automatically instantiates transfer functions from
the acquired states. By composing the transfer functions,
IntentScope constructs a snapshot of the holistic intent
forwarding state as a graph whose vertices correspond to
apps and whose edges correspond to system-allowed intents
(as intent spaces) between apps. The graph supports flexible
queries and facilitates novel security assessment tasks such
as checking domain isolation, enumerating UI workflows [26],
and discovering permission re-delegation paths [19].
This paper makes the following main contributions:

BACKGROUND

In this section, we first discuss the background of intentbased communication in Android. We then present the problem description of this work.

2.1

Intent-based Communication

Components are the basic building blocks of Android apps.
There are four types of components, and each type serves a
specific purpose:
• Activities: An activity represents the user interface.
• Services: A service has no user interface and runs in
the background for time-consuming operations.
• Content providers: A content provider exposes an
app’s data as tables and supports basic operations such
as insert, delete, and update.
• Broadcast receivers: A broadcast receiver is triggered upon system or application events.
A component can be exported to other apps. Each exported component of an app is an entry point for intents
through which the other apps or the Android system can
send intents. Typically, an app exports its components to
other apps by statically declaring the exports in the app’s
manifest1 . However, an app can also dynamically create and
export components in its code. Two system services, PackageManagerService (PMS) and ActivityManagerService (AMS),
maintain the information about each installed app’s components regardless of how the components are exported—either
statically or dynamically.
Intents can connect an app’s component to exported components. An app creates an intent and sets its embedded attributes. The intent is then processed by the Android system
and the security extensions, which automatically resolve an
intent’s recipients based on the following intent attributes:
• Component name: This attribute explicitly specifies
the expected recipient of the intent.
• Action: This attribute describes the general action
to be taken by a recipient component, such as PICK,
VIEW, EDIT, or SHARE.
• Scheme: This attribute describes the protocol that
serves the data, such as http, mailto, or tel.
• Authority: This attribute describes the location of
the data, such as www.google.com or paypal.
• Type: This attribute describes the MIME type of the
data, such as audio/ogg, video/*, or */*. Note that
wildcards are allowed.
• Category: This attribute provides additional information about the data. For example, a category BROWSABLE implies the data that can be opened in a web
browser, such as a link to an image.

• We propose an intent space model for modeling intentbased inter-application communication in Android. Our
intent space model is general and independent of specific security extensions.
• We propose IntentScope, a general, holistic, and proactive policy checking framework for intent-based communication. IntentScope reasons about a holistic
graph derived from the live intent forwarding states
maintained by multiple security extensions in an Android device.
• We implement a prototype of IntentScope and evaluate it against mainstream security extensions, commodity Android devices, and customized Android OSs.
We also showcase a series of novel analysis tasks that
help a policy analyst discover weak points in policies.

Two types of intents exist in Android. Explicit intents
specify the component name only. Android delivers an explicit intent directly to its specified component regardless of
the presence of any other attributes. Implicit intents specify the attributes other than component name. Thus, an
implicit intent’s recipients are implicit and must be resolved
at intent-sending time; Android must search the registered
components to resolve the recipient components.

The remainder of this paper proceeds as follows. Section 2 provides the background and problem description of
our work. Section 3 describes our intent space model. Section 4 introduces IntentScope and describes its system design followed by experimental results in Section 5. Section 6
discusses limitations and future work. Section 7 overviews
related work. Section 8 concludes this paper.

1
The manifest is a required XML file included in the app by
the developer.

2

T1()

2.2 Problem Description
Intent-based inter-application communication has received
much research attention. In general, two aspects are covered: previously unknown security limitations of intents [10,
13, 15, 19, 25] and generic policy-driven security extensions
that remedy the limitations [7, 10–12, 21, 26, 27, 29, 32, 38].
However, there is an overlooked gap between configuring
generic security extensions and securing a specific Android
device. Every app, every device, and every user are different.
A policy analyst needs insights about intent-based communication before she can accurately define how the apps in her
device communicate through intents in her intended ways.
To bridge the gap, we seek a systematic approach for a policy
analyst to conveniently acquire such insights.
Intent-based communication is mediated by multiple security extensions. While multiple security extensions promote the flexibility of controlling intent-based communication, they also introduce new challenges in definition and
verification of their policies.
C-1: Incompatible policies. The security extensions define
their own incompatible schemas and semantics. For example, FlaskDroid [12] inherits SELinux’s policy semantics of
type enforcement. Saint [29] uses an XACML-like schema
customized by the authors. IntentFirewall’s policy is unique
and unlike the other security extensions, however it specifies a critical set of tests on intent attributes. As far as we
know, no existing policy checker can work with every extension’s policy. Therefore, checking such incompatible policies
remain a manual process that requires a policy analyst to
master the details of every security extension.
C-2: Local policies. Each security extension manages a local
view of system-wide policies. For example, IntentFirewall
enforces its centralized policy specified by a policy administrator; intent filters manages policies specified by decentralized apps but enforce the policy in a centralized manner.
Each security extension makes its decision by itself and is
not aware of the other security extensions. No security extension possesses a holistic view of the reachability among
installed apps as controlled by all the security extensions.
Problem Statement. To address the challenges in checking intent-base communication, we seek to build: a) a general policy checker that easily adapts to the policy schema
of any security extension that controls intents; b) a proactive policy checker that keeps monitoring the live intent forwarding states of security extensions; and c) a holistic policy checker that aggregates the policies into a holistic and
verifiable view. With the policy checker, we attempt to systematically answer the following two questions regardless of
specific security extensions, apps, or devices: a) what intents can an app send to a specific app; and b) what intents
can an app receive from a specific app. Meanwhile, we expect the checker to be mostly automated so as to reduce the
burden on policy analysts.
Assumptions. In this work, we assume an Android device is loaded with multiple policy-driven security extensions
that mediate intent-based communication among installed
apps. The apps could be malicious, but they do not seek
to escape from the confinement of the security extensions.
In other words, the policies define apps’ capabilities to send
or receive intents. Threats that compromise the integrity of
the Android system, the security extensions, and the policies
are beyond the scope of our approach.

A

Security
Extension

T2()

Security
Extension

T3()

Security
Extension

B

;ĂͿ

T3(T2(T1()))
A

B
;ďͿ

Figure 1: (a) The intent space from App A to App
B shrinks as it passes security extensions, modeled
here by the T1 , T2 , T3 . (b) Composing transfer functions to model app-to-app transformation.

3.

INTENT SPACE ANALYSIS: MODEL

We believe that creating the right abstraction model is the
first step toward checking intent-based communication. In
this section, we elaborate the intent space model that lays
the foundation for intent space analysis.
Figure 1 demonstrates a motivating example where App A
sends intents to App B. For simplicity of the example, we
consider only actions and categories, and we represent the
actions on the x-axis and the categories on the y-axis. The
initial space of App A is full in both dimensions because
an app can create arbitrary intents before the intents are
processed by any security extension. And because the security extensions only forward the intents that match certain
actions or categories specified in their policies, the space
gradually shrinks as the transformations T1 , T2 , and T3 are
applied to the initial space (Figure 1 (a)). The remaining
space at App B indicates the intents that App A can send to
reach App B. And if no space remains, App A cannot communicate with App B through intents. One step further,
we combine the transfer functions into a composite transfer
function that describes app-to-app space transformation as
illustrated in Figure 1 (b). This composite function captures
all the security extensions. Thus, it describes the holistic intent forwarding state.

3.1

Intent Space

Formally, an intent space is a K-dimensional space of regular languages defined as I = {.∗}K , where “.*” is the regular language that describes all words. The K dimensions
correspond to K intent attributes, which are selected by the
policy analyst based on her requirements. A policy analyst
can set a smaller K if the security extensions to be analyzed
do not inspect every intent attribute. An intent i maps
to a point in the space, such as: {action: SEND,category:
DEFAULT}2 for K = 2. Multiple intents map to a subspace
defined as a hypercube or a union of multiple hypercubes. A
hypercube is represented with exactly K regular languages
at K dimensions, such as {action: SEND|SEND_MULTI, category: ε (the empty string language)}. Any hypercube with
fewer than K dimensions or undefined dimensions is invalid
and considered as an empty space ∅ in the subsequent computations.
2
For clarity in this example we annotate the dimensions with
the attributes.

3

3.2 Intent Space Algebra

3.3

Transfer Function

Algorithms that check intent-based communication between two apps must determine whether an app’s allowed
outgoing intents overlap with the other apps’ allowed incoming intents. To this end, we define the basic set operations
on I: intersection, union, complementation, and difference.
Note that a point in I can be considered as a special hypercube whose regular languages contain only one word; and
a subspace is a union of multiple hypercubes. We therefore
define set operations for hypercubes and carry over the operations to other intent space objects. Throughout the rest of
this paper, we overload the term intent space to refer to all
types of intent space objects including points, hypercubes,
subspaces, as well as the entire intent space.
Intersection. The intersection of two intent spaces is
computed by intersecting the regular languages at each dimension. Formally, given two intent spaces i, j ⊂ I and
their dimension set D = {d1 , d2 , . . . , dk }, their intersection
i ∩ j is {d1 : regexi1 ∩ regexj1 , . . . , dk : regexik ∩ regexjk }. For
example, {A[12], C1} ∩ {ε, C1} is equivalent to {A[12], C1}
and {A[12], C1} ∩ {A3, C1} is equivalent to {∅, C1}. Note
that {∅, C1} is missing a dimension and thus is considered
as an empty space ∅.
Union. A union of intent spaces may not be simplified to
a single intent space. For example, the union of two intent
spaces {A1|A2, C1} and {A3, .*} cannot be represented by
any single hypercube and we simply represent the union as
{A1|A2, C1} ∪ {A3, .*}. We can simplify the result if the
intent spaces are on the same hyperplane. For example,
{A1|A2, C1} ∪ {A3, C1} is equivalent to {A[1-3], C1}.
Complementation. The complement of an intent space
i is the union of all the intent spaces that do not intersect
with i. Recall that the intersection of two intent spaces is an
empty space if the intersection is missing any of the K dimensions. We compute i’s complement i with Algorithm 1,
which finds all non-intersecting intent spaces by replacing
the regular language at one dimension with its complement
if the language is not .* and setting .* at the other dimensions. For example, the complement of {ε} is {.*} and the
complement of {A1, C1} is {A1, .*} ∪ {.*, C1}.

For convenience of analysis, we assume that all security
extensions deny by default. For those security extensions
that accept by default, it is trivial to reduce them into denyby-default extensions with a least-priority rule that accepts
everything. Therefore, apps cannot communicate if the security extensions specify no rule. Conversely, the rules of
a security extension that allow/deny some intents from one
app to another app essentially specify how the security extension forwards or drops intents from the source app to the
destination app. As we represent intents as an intent space,
we model a security extension’s intent forwarding and dropping functionality as intent space transformation and represent a security extension with a transfer function. Given
that the space of all apps is A, a transfer function T is:
T : (a, i) → 2A×I , a ∈ A, i ⊂ I
To aggregate multiple transfer functions into a holistic view,
we iteratively apply each (a, i) tuple of the output of a transfer function to the input of the next transfer function and
build a composite transfer function.
A transfer function captures the transformation that a security extension performs on A, I, or both. Suppose we are
to model a simple security extension that works like a Layer2 network switch: it only supports coarse-grained control
over which app can send intents to another app regardless
of intent attributes. Such an extension can be modeled as a
transfer function that transforms only on A. IntentFirewall
denies an app from sending a specific intent regardless of
the intent’s destination apps. It therefore can be modeled
as a transfer function that only transforms on I. We elaborate more details about how we model security extensions
for intent space analysis in the subsequent section.

4.

INTENT SPACE ANALYSIS: SYSTEM

In this section, we describe our policy analysis framework IntentScope which supports intent space analysis.
To demonstrate its generality, we also discuss how IntentScope works with the security extensions for intents in Android Open Source Project (AOSP) and their policies. We
emphasize that IntentScope is not limited to only the discussed security extensions in this paper.
Figure 2 depicts the workflow of IntentScope. In general, IntentScope starts from acquiring the policies of security extensions, then creates transfer functions, and converts
the composite transfer function into a holistic reachability
graph for subsequent analysis.
Acquiring Policies. The policy of a security extension is
often referred to as a dedicated file stored in the filesystem.
In this work, we opt for a more general definition of policy
and propose to acquire all the states and configurations of
security extensions so long as they specify how the intents
are forwarded. To this end, we create a privileged watchdog app for IntentScope that proactively observes policy
changes and automatically takes a snapshot of the policies.
The implementation of the watchdog app is largely specific
to the analyzed security extensions. For example, intent
filters are registered by apps and maintained by AMS and
PMS. The watchdog app acquires the registered intents filters on an Android device by dumping the internal states of
AMS and PMS after an app registers/unregisters any intent
filter.

Algorithm 1: Computing an intent space’s complement
Data: i
Result: i
i′ ← ∅;
for dimension di ∈ D do
L ← regular language at di ;
if L 6= .∗ then
i′ ← i′ ∪ {d1 : .∗, . . . , di : L, . . . , dk : .∗};
return i′
Difference: The difference (or subtraction) is computed
with intersection and complementation, i.e., i−j = i∩j. For
example, {A1|A2, .*} - {A2, .*} is equivalent to {A1|A2, .*}
∩ {A2, .*}, which is {A1, .*}. A slightly more complicated
example which reuses the complement of {A1, C1} is:
{A1|A2, C1|C2} − {A1, C1}
= {A1|A2, C1|C2} ∩ {A1, C1}
= {A1|A2, C1|C2} ∩ ({A1, .*} ∪ {.*, C1})
= {A2, C1|C2} ∪ {A1|A2, C2}
4

Implicit Intents
Intent
Firewall

Intent
Filters

C

Permissions
Apps

Apps

Protected
Broadcasts

A

B

E

Explicit Intents
Intent
Firewall

D

Permissions

Policies

Transfer Functions

F

Holistic Reachability Graph

Figure 2: IntentScope System Workflow.
As shown on the left side of Figure 2, two chains of security
extensions control implicit and explicit intents. We define
two intent spaces: (1) II as a six-dimensional implicit intent
space over five intent attributes action, category, scheme,
authority, type and one additional attribute permission;
and (2) IE as a two-dimensional explicit intent space over
component name and permission. Note that the permission of an intent is inherited from the app that created the
intent. The chain for implicit intents consists of four security
extensions: protected broadcasts, IntentFirewall, intent filters, and permissions; and we define their transfer functions
I
I
I
over II as TPI B , TIF
W , TIF , and TP ERM . The chain for
explicit intents includes two security extensions: IntentFirewall and permissions; and we define their transfer functions
E
E
over IE as TIF
W and TP ERM .

Creating Transfer Functions. Next, we map the acquired policies onto transfer functions. Given that a security extension makes decisions based on its loaded policy and
implemented policy interpretation logic, a transfer function
that models the intent forwarding state must capture both.
While the policy can be automatically retrieved by IntentScope’s watchdog app, the policy interpretation logic still
requires manual effort to model. IntentScope requires the
security extension’s authors or policy analysts to define a
transfer function for its policy interpretation logic and to
create a policy parser that instantiates the corresponding
transfer function. Note that this logic construction overhead is only performed once as the defined transfer functions
can be reused and the parsers can automatically instantiate
transfer functions. We elaborate our transfer functions for
the AOSP security extensions in Section 4.1.
Building a Holistic Reachability Graph. To facilitate analysis and visualization, we propose to convert the
composite transfer function into a directed graph that represents inter-application reachability. Formally, a holistic
reachability graph is denoted as G = (V, E), where V is a
set of vertices that correspond to the installed apps and E
is a set of edges that correspond to the intent spaces that
an app can send to reach another app. Constructing such a
reachability graph is straightforward. Each app maps to a
vertex in the graph. For each app, we apply the composite
transfer function on its initial intent space (e.g., {.∗}K ) and
add a directed edge if any non-empty intent space remains at
the destination app. We assign the remaining intent spaces
on the edges as their weights, which allows IntentScope
to support flexible queries and graph pruning as a policy
analyst adds constraints on the graph.

4.1.1

I
Intent Filters: TIF

An intent filter specifies the implicit intents that it allows
to be forwarded to the next security extension. Therefore,
an intent filter’s output is the intersection of the input intent space and the intent filter’s corresponding intent space.
Suppose a component dst.c in an app dst has an intent filter f ilter that describes an intent space idst.c
f ilter . Then, an
intent filter transforms (src, i) to (dst, i ∩ idst.c
f ilter ). Note that
the transformation is performed on both A and I. Given
the installed apps on a device as a set A, we combine their
registered intent filters and define TIF as follows:
I
n.c
TIF
(m, i) = {(n, i ∩ in.c
f ilter )|i ∩ if ilter 6= ∅,
∀c is a component of n, ∀n ∈ A, n 6= m; i, in.c
f ilter ⊂ II }

Next we explain how we map an intent filter to its intent
space if ilter . In general, an intent filter accepts an intent
if the intent’s attributes pass a series of tests on the intent filter’s attributes. Therefore, we reduce the problem
of modeling an intent filter to constructing a set of regular
languages which consists of the words that pass each test.
Action Test: An intent passes the action test if the intent’s action matches any action in the intent filter. Therefore, we map the one or more actions of an intent filter onto
a regular expression that concatenates the escaped action
strings and separates them with the vertical bar character
|, such as VIEW|EDIT. There are two corner cases in this
test. First, zero action in a filter fails the test. Second,
zero action in an implicit intent also fails the test. We capture both cases with a regular expression [], which denotes
an empty language whose intersection with any language is
empty. Note that the Android documentation is incorrect
with respect to the second corner case: “if an intent does not
specify an action, it will pass the test as long as the filter contains at least one action”. The reason is that queryIntent()

4.1 Modeling AOSP Security Extensions
Intent filters, IntentFirewall, protected broadcasts, and
permissions are the integral parts of AOSP and therefore
widely deployed in COTS Android devices. They also serve
as reference implementations for other security extensions.
For example, Apex [27] and CRePe [14] extend the permissions; and SEAndroid controls intents with a slightly modified IntentFirewall [5]. Based on these observations, we believe that the AOSP security extensions are a good starting
point to demonstrate that IntentScope is general, because
it can effectively work with their policies. In the remainder
of this section, we share our experiences of modeling these
security extensions for intent space analysis. Although we
are not the first to formally model them, we provide the
most accurate models by covering a complete set of intent
attributes and undocumented logic in the security extensions. Unless stated otherwise, the contents in this section
are based on the kitkat-release branch in AOSP.
5

in the IntentResolver class eventually denies such intents
even though matchAction() in the IntentFilter class allows. Our experiments also confirm this behavior. Interested readers are referred to the source code3 .
Scheme Test: An intent passes the scheme test if the
intent’s scheme matches any scheme in the filter. Therefore,
the regular expression here is constructed in the same way as
the action test, e.g., http|gopher. This test also has unique
cases. First, an intent filter without any scheme still matches
three schemes: content, file, or an empty string. We represent them with a regular expression file|content|, where
the last | matches the empty string. Second, an intent without any scheme passes the scheme test only if the intent filter
does not specify any scheme. We consider such intents as
intent spaces whose scheme is an empty string.
Authority Test: This test is dependent on the scheme
test. If the intent filter does not specify any scheme, this
test automatically passes regardless of the authority. This
test also passes if the filter does not specify any authority.
Thus, we use .* to match any authority in these two cases.
An intent without any authority passes the test only if the
filter has no authority. We represent such intents with an
empty string at the authority dimension. Otherwise, an intent passes the authority test if its authority matches any
authority in the filter.
Type Test: An intent passes the type test if the intent’s
MIME type matches any type in the filter. The challenge
here is the wildcard character * in MIME type strings. For
example, * and */* match any type; and audio/* matches
any subtype of audio. To maintain the semantics of the
wildcard character, we convert * and */* to .*. The slash
character / is a special character in regular expressions so we
escape it as \/. For example, audio\/.*|video\/mp4 represents every audio subtype and a single video type. Moreover,
an intent filter that has no type accepts only the intents that
have no type. Therefore, zero type in either the intent or
the filter maps to an empty string.
Category Test: Unlike the other attributes, an intent
can include more than one category. An intent passes the
category test if every category in the intent matches a category in the filter, i.e., the intent’s category set is the subset
of the filter’s category set. To capture this logic, we construct a regular language for an intent filter’s categories with
three steps: (1) escape the category strings; (2) concatenate
the escaped strings and separate them with |; and (3) surround the concatenated string with ( and )*. For example, the subsets of an intent filter’s category set {DEFAULT,
LAUNCHER, BROWSABLE} are represented with a single regular expression (DEFAULT|LAUNCHER|BROWSABLE)*. This expression also matches zero category and duplicate categories
specified in an intent. The other corner cases are similar to
those of the type test. No specification of category in an
intent or a filter maps to an empty string. An intent filter
with no category accepts only the intents with no category.

4.1.2

subtracts the intent space of each fwfilter from the input
intent space. Suppose a fwfilter that blocks an app src is
I
E
represented with an intent space isrc
f wf ilter . TIF W and TIF W
are defined in the same way as follows:
[ a
[ a
I
TIF
if wf ilter )|i −
if wf ilter 6= ∅,
W (a, i) = {(a, i −
∀f wf ilter that blocks the sender app a; i, iafwf ilter ⊂ II }
[ a
[ a
E
TIF
if wf ilter )|i −
if wf ilter 6= ∅,
W (a, i) = {(a, i −

∀f wf ilter that blocks the sender app a; i, iafwf ilter ⊂ IE }
Next we explain how we construct the intent space if wf ilter
for a fwfilter over the implicit intent space II and the explicit intent space IE , respectively. In general, we construct
if wf ilter according to IntentFirewall’s two-phase intent attribute matching process.
If a fwfilter is for implicit intents, IntentFirewall first considers the fwfilter as an intent filter and tests the intent attributes with the same tests as we discussed in Section 4.1.1.
We skip modeling this phase for brevity. In the second
phase, IntentFirewall tests the intent attributes with common string tests, such as isEqual, isStartsWith, isContained, and matchRegex. Therefore, we model these tests
with their equivalent regular expressions. For example, isStartsWith=abc maps to a regular expression abc.*; isContained=def maps to a regular expression .*def.*. The tests
can be aggregated by computing the intersection of the regular expressions. For example, two tests isEqual=abc and
isStartsWith=ab map to a regular expression abc.
For a fwfilter that filters explicit intents, we also construct
its intent space in two phases. In the first phase, IntentFirewall checks if an explicit intent’s component name matches
the one specified in the fwfilter. Thus, we simply copy
the fwfilter’s escaped component name to the corresponding dimension in if wf ilter . There are two corner cases to
be handled. An explicit intent with no component name is
dropped immediately because it resolves to nowhere. A fwfilter with no component name does not block any explicit
intent. We model the former case with a regular expression [] and model the latter case with a regular expression
.*. In the second phase, Intent Firewall tests the intent’s
component name with the identical string tests so we do not
I
E
rephrase how we model them. Finally, both TIF
W and TIF W
do not transform an intent space at the permission dimension because IntentFirewall does not inspect permissions.
Note that IntentFirewall is a relatively new security extension in AOSP with no official documentation and limited
comments in the code. At first we referred to the unofficial documentation maintained by Yagemann [36] to define the transfer functions. However, we found unexplained
behaviors of IntentFirewall when we tested IntentFirewall’s
sample policies, which led us to the discovery of the overlooked second matching phase. In order to obtain an accurate and comprehensive model, we manually derived the
transfer functions presented in this section from IntentFirewall’s source code4 .

E
I
IntentFirewall: TIF
W and TIF W

IntentFirewall is a policy-driven MAC framework that
block apps from sending specific intents. The policy files,
located at /data/system/ifw/*.xml, specify a list of firewall filters (fwfilters for short) that describe the implicit or
explicit intents to be blocked for a specific sender app. We
model IntentFirewall as a transformation over II or IE that

Permissions constrain an app’s capability to receive intents from other apps. Suppose an app has a sensitive
component that only accepts the intents from authorized
apps. Then, the app can define a permission and assign it

3

4

4.1.3

https://goo.gl/A1auU5 and https://goo.gl/cdzxg8
6

Permissions: TPI ERM and TPEERM

https://goo.gl/e4zzxL

to the component, which requires the component’s callers
to hold the exact same permission. If we treat intents as
if they inherit the permissions of their creator/sender apps,
a permission’s role is to forward only the intents that have
matching permissions. Therefore, a permission’s output is
the intersection of the input intent space and the permission’s own intent space. Note that permissions do not transform on A because the other security extensions have already resolved the destination app/component. Suppose a
component dst.c is protected by a permission p described
by an intent space idst.c
c.p . The transformation is defined as
(dst.c, i) → (dst.c, i ∩ idst.c
c.p ).
We define TPI ERM and TPEERM as follows:

explicit intent space, respectively. To build each chain of
transfer functions, we start from integrating the transfer
functions of those security extensions that restrict an app
from sending intents. Then, the transfer functions of the security extensions that restrict an app from receiving intents
follow. For the transfer functions defined in this section,
their composite transfer function T is defined as:
 I
I
I
I
(TIF
if i ⊂ II
 TP ERM (TIF
W (TP B (a, i))))
T (a, i) =

E
TPEERM (TIF
if i ⊂ IE
W (a, i))

5.

a.c
TPI ERM (a, i) = {(a.c, i ∩ ia.c
c.p )|i ∩ ic.p 6= ∅,

∀c is a component of a;
c is protected by c.p; i, ia.c
c.p ⊂ II }
a.c
TPEERM (a, i) = {(a.c, i ∩ ia.c
c.p )|i ∩ ic.p 6= ∅,
∀c is a component of a;

c is protected by c.p; i, ia.c
c.p ⊂ IE }

5.1

Mapping a permission to an intent space ip is straightforward. The regular language at the permission dimension of
ip is the escaped permission string. A special case is that a
content provider may have separate permissions for reading
and writing. Similar to the action test in intent filters, we
model this case with a regular expression perm_r|perm_w,
based on the fact that an app with either the read or write
permission can access the content provider. The regular
languages at the other dimensions are .*, leaving the intent
space unchanged at these dimensions.

4.1.4

Implementation

IntentScope includes an implementation of the intent
space model, a watchdog app that monitors and incrementally acquires the policies of the AOSP security extensions,
a set of policy parsers that build and compose transfer functions, and a graph builder that converts the composite transfer function into the holistic reachability graph.
The intent space model is built on Augeas Libfa [1], a
native library that supports accurate and fast operations
on regular expressions. In particular, we opt for Hopcroft’s
DFA minimization algorithm [22] to minimize regular expressions. This algorithm runs in O(nlogn) time in the worst
case, where n is the number of states of a regular expression’s
equivalent DFA. The watchdog app runs as a privileged system app. It detects state changes in PMS/AMS triggered
by app installs/uninstalls and re-acquires the intent filters
and permissions, regardless of whether they are statically declared in apps’ manifest or dynamically registered in app’s
code. The watchdog app also fetches the relevant files where
IntentFirewall and protected broadcasts store their policies.
As the operations over intent spaces are both computation
and memory intensive, the parsers and graph builder run on
a dedicated server rather than on the mobile device where
the watchdog app runs.

Protected Broadcasts: TPI B

Protected broadcasts are a set of implicit intents with
special actions that only the apps whose UIDs are SYSTEM,
BLUETOOTH, PHONE, or SHELL can send. The other apps are
prevented from sending such intents. Similar to IntentFirewall, we model protected broadcasts as a space transformation that subtracts the intent spaces of protected broadcasts
from the input intent space if the input app is not a systemapp. Suppose each protected broadcast maps to an intent
space iprotected . Then, we define the transfer function for
protected broadcasts as follows:

(a, i)
if a is an allowed app
S
TPI B (a, i) =
(a, i − iprotected ) otherwise

5.2

i, iprotected ⊂ II

Experimental Setup

We evaluated IntentScope on two Android devices and
four Android-based OSs, as shown in Table 1. The Galaxy
Note ran Samsung’s deeply customized Android (4.4.2), which
pre-installed a large number of Samsung’s apps. The Nexus
4 ran three OSs, including stock Android (5.0), MIUI (4.4.2),
and CyanogenMod (4.4.4). We kept them as they were and
did not install additional apps. In particular, the first two
OSs pre-installed a few proprietary Google-branded apps.
MIUI and CyanogenMod did not include these apps due to
licensing restrictions.
For each OS, we started each installed app and kept it
in the foreground. After the apps were started and IntentScope’s watchdog app did not report any new policy updates
in the latest one minute, we applied IntentScope to generate a reachability graph G and two subgraphs GI and GE
that respectively represent the holistic forwarding state of

A list of actions used by protected broadcasts is available in the Android SDK5 . Thus, we build an intent space
iprotected for each action by assigning the escaped action
string into the action dimension of the space. The other
dimensions do not involve space transformation and remain
with a regular expression .*.

4.1.5

EVALUATION

In this section, we first discuss a prototype implementation of IntentScope. We then present the experiments in
which we apply IntentScope to check intent-based communication mediated by the AOSP security extensions installed
in commodity Android devices and customized Android OSs.
We conclude with an evaluation of the throughput of our
system.

Composite Transfer Function

As we have defined the transfer function for each individual security extension, we combine them together to build
the composite transfer function. The composite function
covers two chains of transfer functions for the implicit and
5
ANDROID SDK ROOT/platforms/android-19/data/
broadcast actions.txt

7

Table 1: Evaluated Android Devices/OSs and Generated Reachability Graphs

1

Device

OS

|V|

Samsung Galaxy Note II

Customized Android

311

Stock Android

108

MIUI v5

104

CyanogenMod 11 M12

85

2
3

LGE Nexus 4

4

|EI |
|EE |
880,456
979,993
155,369
138,651
99,170
118,707
38,606
47,458

Global Clustering
Coefficient
0.986
0.994
0.971
0.990
0.979
0.991
0.974
0.989

Standard
Deviation
0.007
0.006
0.014
0.009
0.013
0.009
0.015
0.011

Table 2: Apps Ranked by PageRank
1

2

3

4

Highest in GI
com.viber.voip
com.android.contacts
com.android.settings
com.google.android.apps.plus
com.android.settings
com.google.android.apps.gms
com.android.mms
com.android.contacts
com.android.settings
com.android.gallery3d
com.android.email
com.android.contacts

Lowest in GI
com.android.proxyhandler
com.monotype.android.font.cooljazz
com.sec.android.provider.badge
com.android.dreams.basic
com.android.providers.userdictionary
com.android.vpndialogs
com.android.pacprocessor
com.android.sharedstoragebackup
com.miui.providers.weather
com.android.nfc
com.android.backupconfirm
com.android.sharedstoragebackup

Highest in GE
com.android.contacts
com.android.phone
com.android.settings
com.google.android.setupwizard
com.google.android.apps.plus
com.android.settings
com.android.email
com.android.mms
com.android.settings
com.android.contacts
com.android.email
com.android.settings

5.3

implicit and explicit intents. Each vertex represents an app
identified by its package name rather than UID6 . Parallel
edges are allowed and prevalent in the graphs to capture the
multiple entry points of an app.
Table 1 lists the number of vertices, the number of edges
(including parallel edges), and the global clustering coefficient (measured without parallel edges) of each GI and GE .
A global clustering coefficient is a measure of the degree to
which vertices in a graph tend to cluster together. We opted
for this measure to get a general idea about how freely the
installed apps on a mobile OS are allowed to communicate
with one another. As the clustering coefficient of a clique is
1, the measured values of CG indicate that the vertices in
all the graphs are densely connected, which is in line with
our observation that most apps have at least one component (the main activity) exposed to other apps. The large
number of edges also implies the complexities of managing
fine-grained policies for intent-based communication.
Given the large number of apps/vertices and edges, prioritizing the apps that expose larger attack surfaces is critical for efficiently analyzing and resolving policy conflicts
and violations. Therefore, we propose to identify such apps
with PageRank [30]. The underlying intuition is that such
apps are more likely to be accessed by other apps and thus
have more incoming edges, and the apps that have direct
incoming edges from such apps are also likely to be attacked. Table 2 lists the apps in the four mobile OSs with
the highest and lowest rankings. Most of the listed apps are
in line with intuition, such as com.android.settings and
com.android.email. Here we discuss two apps which are
displayed in bold in Table 2. The app com.google.android.
setupwizard is highly ranked because it exports 69 components that can be accessed with explicit intents. The app
com.viber.voip is highly ranked because of its 94 intent
filters that expose the components to implicit intents.

Lowest in GE
com.sec.enterprise.permissions
com.samsung.android.mdm
com.samung.android.sdk.spenv10
com.android.dreams.basic
com.android.wallpaper
com.google.android.apps.docs.editors.slides
cm.android.printspooler
com.android.nfc
com.android.noisefield
com.android.nfc
com.android.incallui
com.android.printspooler

Experiments

With IntentScope, checking what intents an app can
send is equivalent to checking the vertex’s outgoing edges
as well as the intent spaces assigned on them. Conversely,
checking what intents an app can receive is equivalent to
checking the incoming edges. In addition, IntentScope
supports flexible queries backed by regular expressions. Next
we elaborate four experiments in which we leverage the insights provided by IntentScope to identify potential vulnerabilities due to errors in security policies of the AOSP
security extensions.

5.3.1

Zero Permission 6= Zero Privilege
Enforcing least privilege is a common practice in mobile
security. While recent work [14, 27, 35] attempts to control
and minimize the set of an app’s granted permissions, we
are interested in another question: what can an app do if
it has no permissions. In this experiment, we created and
installed such a zero-permission app. We then checked what
components this app can reach with its allowed intents. This
experiment helps a policy analyst reveal the exposed components that could possibly be exploited by even a zeropermission app. If any sensitive components are exposed,
the details of the allowed intents that reach these components provide the necessary knowledge for a policy analyst
to create precise policies that protect them. We find that
zero permission does not necessarily mean zero privilege as
users might expect. Table 3 shows the number of the zeropermission app’s reachable apps (i.e. out-neighbors) and its
local clustering coefficient.
The flexible queries supported by IntentScope also allow
a policy analyst to pinpoint the intents that have interesting semantics. In the Galaxy Note, we found that this zeropermission app can send implicit intents that contain an interesting scheme called android_secret_code. For example,
one of the reachable apps is com.sec.android.app.wlantest,
which accepts intents with an action android.provider.
Telephony.SECRET_CODE, an authority of 526, and a scheme

6
Apps with the same UID are considered as separate apps
but share the permissions of one another [9].

8

ministrator to create precise rules that can be enforced by
Aquifer and similar access control systems.
In this experiment, we applied IntentScope to enumerate the workflows in MIUI that match the aforementioned
example. Specifically, we started from an app com.android.
providers.downloads, which manages downloaded files. We
then performed a breath-first search on the reachability graph
for a sequence of implicit intents as follows:

Table 3: Reachability of a Zero-Permission App

1
2
3
4

# Outgoing
Edges
2,767
3,072
1,443
1,280
955
1,142
454
557

# Reachable
Apps
241
263
77
92
79
90
62
72

Local Clustering
Coefficient
0.943
0.968
0.905
0.960
0.927
0.968
0.914
0.961

1. action=android.intent.action.VIEW, scheme=content,
category=android.intent.category.BROWSABLE;
2. action=android.intent.action.EDIT, type=image/*;
3. action=android.intent.action.SEND, type=image/*.

of android_secret_code. Another reachable app com.
wssyncmldm is a sensitive app that can silently download
and install apps. Therefore, an app with no permissions
could exploit a vulnerability in this app in order to download and install apps, thus escalating the privilege of the
zero-permission app without exploiting the underlying OS.
We also found that a recent attack [31] is applicable here,
where a malformed intent sent from a zero-permission app
can exploit and take over the exposed sensitive app.

Figure 3(b) shows the matching workflows that start from
the cyan node. The grey nodes are the first hop; the purple
nodes in the middle are the second hop. Note that the purple
nodes also serve as the first hop because the photo editors
can also handle the VIEW action. The yellow nodes represent
the last hop where data may leave a mobile device via emails,
Bluetooth, or MMS messages.

5.3.4
5.3.2

Fine-grained Domain Isolation

Chin et al. [13] presents a limitation of intent-based communication. Suppose a malicious app Mallory attempts to
attack a legitimate and sensitive app Alice and existing policies prevent their direct communication. The limitation allows Mallory to eavesdrop the intents from Alice to Bob and
allows Mallory to send spoofed intents to Alice. This situation calls for a fine-grained domain isolation model that not
only considers apps but also includes intents. IntentScope
is useful as it provides insights about intents.
Specifically, two apps are not isolated with respect to
eavesdropping attacks if they share in-neighbors and incoming intents in the reachability graph. They are not isolated
with respect to spoofing attacks if they share out-neighbors
and outgoing intents. Thus, IntentScope guarantees intent isolation between two apps if: (1) the apps are not
neighbors of each other; and (2) the intent spaces of their
incoming edges from common in-neighbors do not intersect;
and (3) the intent spaces of their outgoing edges to common
out-neighbors do not intersect.
As a case study, we checked the intent isolation between
two apps in the Galaxy Note: com.android.externalstorage
and com.fmm.dm. The former is an Android system app. The
latter is believed to be bloatware as reported on several online forums. IntentScope reported that the intent spaces
do not intersect, which implies that no app steals any intent
from the other. However, these two apps share 242 common out-neighbors and the intersection of the intent spaces
is not empty (see Figure 3(a)). Therefore, these apps are
still susceptible to spoofing attacks.

5.3.3

Discovering Permission Re-Delegation Paths

A zero-permission app may send an intent to a privileged app, thus delegating the privileged app to perform
permission-protected tasks for it [19]. In other words, permission re-delegation happens when apps with respective
permission sets communicate with each other with intents.
Under this definition, existing work [10,19] detects and mitigates permission re-delegation attempts at runtime. One
step further, we expect to enable a policy analyst to get insights into potential permission re-delegation paths before
apps may execute. Meanwhile, the intents used along redelegation paths provide semantics for the policy analyst to
make informed decisions and take precise actions against the
privileged apps that could be abused.
We propose to use connected subgraphs to represent permission re-delegation paths in a reachability graph. A subgraph is connected if every pair of its vertices has a path
that consists of only the vertices in the subgraph. This is
analogous to the situation where multiple apps collude but
cannot relay their communication via other apps. We define
the problem of discovering re-delegation paths as follows:
given a set of critical permissions denoted as CP , find all
the connected subgraphs of k vertices that satisfy:
• Each app (vertex) holds at least one permission but
not all the permissions in CP ; and
• The union of the apps’ permissions is a superset of CP .
The best algorithm we found to generate connected subgraphs of k vertices is ConSubG(G, k) [24], whose worstcase time complexity is exponential in k. The performance
of this algorithm is generally acceptable because we rarely
encounter cases where more than five apps collude.
We targeted the third-party apps installed on the Galaxy
Note and set k = 3. We attempted to create a synthetic attack where apps collude to drain the battery with a critical
permission set of three permissions: BLUETOOTH_ADMIN, NFC,
and FLASHLIGHT. Our results show 6 groups of apps (triangles) that can possibly collude to cover the critical permissions. In particular, the two apps in the center respectively
hold FLASHLIGHT and NFC, while the surrounding six apps
hold BLUETOOTH_ADMIN (see Figure 3(c)). After the groups

Enumerating Multi-app Workflows

In modern mobile operating systems, it is common for a
user to orchestrate multiple apps for a large and user-defined
task. For example, a user may streamline a workflow of
downloading, viewing, editing, and sending a picture with
a chain of apps. Under the hood of Android, a multi-app
workflow is implemented as a calling sequence of intents.
While controlling such workflows has been well covered by
Aquifer [26], IntentScope provides clues for a policy ad9

Table 4: System Throughput
1
2
3
4
Average

|EI |
800,456
155,369
99,170
38,606

Avg. Time (s)
302.05
70.08
38.69
15.63

StdDev (s)
5.73
3.02
0.92
1.00

# edges/sec
2,915
2,217
2,563
2,469
2,541

com.sec.android.AutoPreconfig
com.samsung.sec.android.application.csc

com.android.stk
com.sec.dsm.phone

com.android.providers.telephony
com.sec.android.app.DataCreate

(a) Common In-neighbors of Two Target Apps

5.4

com.android.bluetooth

com.miui.notes

com.android.contacts
com.android.email

com.android.mms
com.tencent.mm

(b) Workflows for Processing a Picture

com.viber.voip
(5)
(2)
(2)
(3)

com.shazam.android
(3)

(*)

(1)

(1)
com.surpax.ledflashlight.panel
com.vlingo.midas
(4)
(4)

System Throughput

We performed the benchmark in a Xen VM running Ubuntu
14.04 with Intel Xeon E5620 2.4GHz and 8GB of RAM. Only
one core was used during the benchmark. Table 4 shows the
average results of 10 runs. It took approximately 5 minutes
to check the customized Android OS of the Galaxy Note
loaded with 311 apps, and less than 1 minute to check the
others. In general, the processing time is proportional to the
number of edges. As shown in Table 4, IntentScope processed 2,541 implicit intent spaces and 7,225 explicit intent
spaces in a second. While explicit intent spaces were almost
three times faster than implicit intent spaces, we note that
an explicit intent spaces has only two dimensions and an
implicit intent space has six dimensions.

com.pandora.android

me.pou.app

# edges/sec
8,454
6,422
7,014
7,013
7,225

• iI : action=android\.intent\.action\.EDIT,
category=android\.intent\.category\.DEFAULT,
scheme=http, authority=\d+, type=mpeg,
permission=.*;
• iE : component=com\.sec\..*, permission=.*.

com.jeejen.family

(5)

StdDev (s)
2.02
0.74
1.02
0.45

To understand the performance of IntentScope, we performed a microbenchmark to evaluate the number of edges
that IntentScope can check in a second. Given that checking an edge is done by testing whether the intersection of the
edge’s intent space and a given intent space is empty, this
benchmark also implies the throughput of IntentScope in
terms of processing intent spaces. In the benchmark, we
used the following two intent spaces to evaluate the throughput of implicit intents and explicit intents, respectively. Note
that the intersection of an implicit intent space and an explicit intent space is always empty and thus not evaluated.

com.android.fileexplorer

com.android.providers.downloads

Avg. Time (s)
115.57
21.59
16.92
6.77

are identified, a security analyst can further look into the
apps for colluding behaviors with static or dynamic analysis. On the contrary, a user can eliminate colluding attacks
by placing the apps into separate domains.
Even though the discovered eight apps are mostly downloaded and seem to be trusted by general users, they may
carry third-party libraries or vulnerable components that
are exploitable by other apps. In other words, they may not
deliberately collude, but could be exploited by other apps
to acquire privileges. The analysis discussed in this experiment can be combined with the other analyses (e.g. zeropermission apps) to further generate knowledge for a policy
analyst to take precautions before real exploits occur.

com.android.phone
com.sec.android.Preconfig

com.miui.player

|EE |
979,993
138,651
118,707
47,458

(6)
(6)

com.antivirus

6.

com.tencent.mm

DISCUSSION

Policy analysis and app analysis. In terms of providing insights for configuring security extensions, our intent
space based policy analysis complements existing static and
dynamic app analysis. We make this argument based on
the fact that an app’s runtime behaviors on a specific mobile device are shaped by (1) the app whose code specifies

(c) Potentially Colluding Apps (k=3)

Figure 3: Experimental Results

10

its executional semantics; and (2) the security extensions
whose policies specify how the app’s specific behaviors are
restricted. While we admit that app analysis is indispensable, we also note the alarming trend of malware thwarting
app analysis. For example, code obfuscation and encryption
hide an app’s true semantics from static analysis. “Split personalities” in apps [8, 23] make malware appear innocent by
detecting and evading dynamic analysis tools. To get an upper hand against adversaries, we would need policy analysis
to orchestrate security extensions.
Generality of intent space analysis. While we presented intent space analysis for checking intent-based communication, the underlying methodology is beyond the scope
of intents and generally applicable to other security extensions. A promising target is SE Android [32], which controls almost every inter-application communication mechanism other than intent-based communication. Specifically,
it checks an attribute called security context when an app requests to access files, sockets and so on. Given that security
contexts and intent attributes are essentially access control
labels [16], we foresee that our intent space analysis can be
extended to a “context space analysis” for SE Android. We
will extend our framework to reason about SE Android policies and further maximize the coverage of inter-application
communication. However, we also admit the limitation that
the current intent space analysis cannot directly work with
existing context-aware security extensions. As for future
work, we shall map contexts into dynamic policy and provide support for such extensions.
Usability of the holistic reachability graph. As
we focused on developing the intent space model and implementing a prototype of IntentScope, usability of the
reachability graph was not the primary goal. Indeed, policy verification is a complicated task because the number
of apps and the allowed intents among them can be quite
large. However, policy management is inevitable to validate
policy-driven security extensions. IntentScope attempts
to reduce the burden on policy analysts by helping them intuitively perform intent-based communication analysis and
utilize flexible queries. Moreover, we believe that the usability of the graph has a lot of space to improve and indeed this
is an important research challenge to explore. For example,
the more interactive visualization may assist a security analyst in understanding the inter-application communication
and in ultimately developing a robust security policy.

igate unauthorized privilege escalations. QUIRE [15] provides provenance of intents so that a callee can track down
the original caller. XManDroid [10] maintains a systemcentric call graph for the intents that have been sent and
received. TaintDroid [18] and VetDroid [37] track sensitive data shared among apps with dynamic taint analysis.
Along these lines, our intent space analysis assists policy analysts by systematically analyzing how security extensions
confine apps’ behaviors. Its analysis is based on a holistic
call graph and data-flow graph derived from the intent forwarding states of security extensions in an Android device.
Experimental security extensions for Android: Besides intent filters, permissions, IntentFirewall, and protected
broadcasts covered in this work, previous research has proposed a series of experimental security extensions for Android. Saint [29] and TISSA [38] support policy-driven access control for intents. CRePe [14] and APEX [27] enable
context-aware and fine-grained permissions. FlaskDroid [12]
and SE Android [32] are generic and flexible MAC systems
that provide comprehensive protection on both Android’s
middleware and kernel layers. Aquifer [26] enforces distributed information flow control over intent-based UI workflows. Android Security Module (ASM) [21] and Android
Security Framework (ASF) [7] provide programmable interfaces that promote the creation of customized security extensions. IntentScope facilitates defining and verifying security policies for these security extensions. It is especially
useful for ASM and ASF that may host security extensions
from multiple stakeholders.

8.

CONCLUSION

In this paper, we have presented intent space analysis for
intent-based communication. Intent space analysis is based
on an intent space model and a systematic policy checking
framework called IntentScope. The intent space model
maps a security extension’s functionality of forwarding intents as transformation on a geometric space. Based on the
intent space model, IntentScope acquires the live states
of multiple security extensions and further derives a holistic
view that supports formal verification. Also we have described a prototype implementation, along with extensive
evaluation results of our approach.

Acknowledgements
This work was partially supported by the grants from Global
Research Laboratory Project through National Research Foundation (NRF-2014K1A1A2043029) and the Center for Cybersecurity and Digital Forensics at Arizona State University. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors
and do not reflect the views of the funding agencies.

7. RELATED WORK
Static and dynamic app analysis. App-oriented analysis provides insights for a policy analyst to create appropriate security policies. ComDroid [13] is the first work
that discusses the intent-based attack surfaces and discovers vulnerable components mistakenly exported by apps.
CHEX [25] is also built on static analysis that comprehensively discovers vulnerable ICC entry points in addition to
just exported components. Epicc [28] checks ICC vulnerabilities based on a sound and detailed ICC model and scales
well. AmanDroid [34], FlowDroid [6], and DroidSafe [20]
statically discover information flows that potentially leak
sensitive data. Elish et al. [17] statically reconstruct intents among apps to detect collusion. Beyond static analysis, dynamic runtime solutions reveal how apps communicate
through intents in real time. IPC Inspection [19] automatically reduces an intent sender’s effective permissions to mit-

9.

REFERENCES

[1] Finite automata. http://augeas.net/libfa/, 2014. Accessed:
06/2015.
[2] Bound services - Android developers. http://developer.
android.com/guide/components/bound-services.html, 2015.
Accessed: 06/2015.
[3] Requesting a shared file - Android developers. http://
developer.android.com/training/secure-file-sharing/
request-file.html, 2015. Accessed: 06/2015.
[4] Selinux policy analysis tools. https://github.com/
TresysTechnology/setools, 2015. Accessed: 06/2015.

11

[5] Selinux wiki. http://selinuxproject.org/page/NB
SEforAndroid 1, 2015. Accessed: 06/2015.
[6] S. Arzt, S. Rasthofer, C. Fritz, E. Bodden, A. Bartel,
J. Klein, Y. Le Traon, D. Octeau, and P. McDaniel.
Flowdroid: Precise context, flow, field, object-sensitive and
lifecycle-aware taint analysis for Android apps. In ACM
SIGPLAN Notices, volume 49, pages 259–269, 2014.
[7] M. Backes, S. Bugiel, S. Gerling, and P. von
Styp-Rekowsky. Android Security Framework: Extensible
multi-layered access control on Android. In Proceedings of
the Annual Computer Security Applications Conference.
ACM, 2014.
[8] D. Balzarotti, M. Cova, C. Karlberger, C. Kruegel,
E. Kirda, and G. Vigna. Efficient detection of split
personalities in malware. In Proceedings of Network and
Distributed System Security Symposium, 2010.
[9] D. Barrera, J. Clark, D. McCarney, and P. C. van
Oorschot. Understanding and improving app installation
security mechanisms through empirical analysis of Android.
In Proceedings of the ACM Workshop on Security and
Privacy in Smartphones and Mobile Devices, pages 81–92.
ACM, 2012.
[10] S. Bugiel, L. Davi, A. Dmitrienko, T. Fischer, A. Sadeghi,
and B. Shastry. Towards taming privilege-escalation attacks
on Android. In Proceedings of the Symposium on Network
and Distributed System Security, 2012.
[11] S. Bugiel, L. Davi, A. Dmitrienko, S. Heuser, A. Sadeghi,
and B. Shastry. Practical and lightweight domain isolation
on Android. In Proceedings of the ACM Workshop on
Security and Privacy in Smartphones and Mobile Devices,
pages 51–62. ACM, 2011.
[12] S. Bugiel, S. Heuser, and A.-R. Sadeghi. Flexible and
fine-grained mandatory access control on Android for
diverse security and privacy policies. In Proceedings of the
USENIX Security Symposium. USENIX Association, 2013.
[13] E. Chin, A. P. Felt, K. Greenwood, and D. Wagner.
Analyzing inter-application communication in Android. In
Proceedings of the 9th International Conference on Mobile
Systems, Applications, and Services (MobiSys), pages
239–252. ACM, 2011.
[14] M. Conti, V. T. N. Nguyen, and B. Crispo. Crepe:
Context-related policy enforcement for Android. In
Information Security, pages 331–345. Springer, 2011.
[15] M. Dietz, S. Shekhar, Y. Pisetsky, A. Shu, and D. S.
Wallach. Quire: Lightweight provenance for smart phone
operating systems. In Proceedings of the USENIX Security
Symposium. USENIX Association, 2011.
[16] P. Efstathopoulos, M. Krohn, S. VanDeBogart, C. Frey,
D. Ziegler, E. Kohler, D. Mazieres, F. Kaashoek, and
R. Morris. Labels and event processes in the asbestos
operating system. In ACM SIGOPS Operating Systems
Review, volume 39, pages 17–30. ACM, 2005.
[17] K. O. Elish, D. D. Yao, and B. G. Ryder. On the need of
precise inter-app icc classification for detecting Android
malware collusions. In Proceedings of IEEE Mobile Security
Technologies (MoST), in conjunction with the IEEE
Symposium on Security and Privacy, 2015.
[18] W. Enck, P. Gilbert, S. Han, V. Tendulkar, B.-G. Chun,
L. P. Cox, J. Jung, P. McDaniel, and A. N. Sheth.
Taintdroid: an information-flow tracking system for
realtime privacy monitoring on smartphones. ACM
Transactions on Computer Systems, 32(2):5, 2014.
[19] A. P. Felt, H. J. Wang, A. Moshchuk, S. Hanna, and
E. Chin. Permission re-delegation: Attacks and defenses. In
Proceedings of the USENIX Security Symposium. USENIX
Association, 2011.
[20] M. I. Gordon, D. Kim, J. Perkins, L. Gilham, N. Nguyen,
and M. Rinard. Information-flow analysis of Android
applications in droidsafe. In Proceedings of the Symposium
on Network and Distributed System Security, 2015.
[21] S. Heuser, A. Nadkarni, W. Enck, and A.-R. Sadeghi. Asm:

[22]
[23]

[24]

[25]

[26]

[27]

[28]

[29]

[30]
[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

12

A programmable interface for extending Android security.
In Proceedings of the USENIX Security Symposium.
USENIX Association, 2014.
J. E. Hopcroft. Introduction to automata theory, languages,
and computation. Pearson Education, 1979.
Y. Jing, Z. Zhao, G.-J. Ahn, and H. Hu. Morpheus:
automatically generating heuristics to detect Android
emulators. In Proceedings of the Annual Computer Security
Applications Conference, pages 216–225. ACM, 2014.
S. Karakashian. An Implementation of An Algorithm for
Generating All Connected Subgraphs of a Fixed Size.
Software (Version Oct2010), Constraint Systems
Laboratory, University of Nebraska-Lincoln, Lincoln, NE,
2010.
L. Lu, Z. Li, Z. Wu, W. Lee, and G. Jiang. Chex: statically
vetting Android apps for component hijacking
vulnerabilities. In Proceedings of the ACM Conference on
Computer and Communications Security, pages 229–240.
ACM, 2012.
A. Nadkarni and W. Enck. Preventing accidental data
disclosure in modern operating systems. In Proceedings of
the ACM Conference on Computer and Communications
Security, pages 1029–1042. ACM, 2013.
M. Nauman, S. Khan, and X. Zhang. Apex: extending
Android permission model and enforcement with
user-defined runtime constraints. In Proceedings of the
ACM Symposium on Information, Computer and
Communications Security, pages 328–332. ACM, 2010.
D. Octeau, P. McDaniel, S. Jha, A. Bartel, E. Bodden,
J. Klein, and Y. Le Traon. Effective inter-component
communication mapping in Android with epicc: An
essential step towards holistic security analysis. In
Proceedings of the USENIX Security Symposium. USENIX
Association, 2013.
M. Ongtang, S. McLaughlin, W. Enck, and P. McDaniel.
Semantically rich application-centric security in Android.
Security and Communication Networks, 5(6):658–673, 2012.
L. Page, S. Brin, R. Motwani, and T. Winograd. The
pagerank citation ranking: Bringing order to the web. 1999.
O. Peles and R. Hay. One class to rule them all: 0-day
deserialization vulnerabilities in Android. In 9th USENIX
Workshop on Offensive Technologies (WOOT 15), 2015.
S. Smalley and R. Craig. Security enhanced (se) Android:
Bringing flexible mac to Android. In Proceedings of the
Symposium on Network and Distributed System Security,
2013.
R. Wang, W. Enck, D. Reeves, X. Zhang, P. Ning, D. Xu,
W. Zhou, and A. M. Azab. EaseAndroid: Automatic policy
analysis and refinement for security enhanced Android via
large-scale semi-supervised learning.
F. Wei, S. Roy, X. Ou, et al. AmAndroid: A precise and
general inter-component data flow analysis framework for
security vetting of Android apps. In Proceedings of the
ACM Conference on Computer and Communications
Security, pages 1329–1341. ACM, 2014.
P. Wijesekera, A. Baokar, A. Hosseini, S. Egelman,
D. Wagner, and K. Beznosov. Android permissions
remystified: A field study on contextual integrity. In
Proceedings of the USENIX Security Symposium. USENIX
Association, 2015.
C. Yagemann. Intent firewall. http://www.cis.syr.edu/
˜wedu/android/IntentFirewall/index.html, 2014. Accessed:
06/2015.
Y. Zhang, M. Yang, B. Xu, Z. Yang, G. Gu, P. Ning, X. S.
Wang, and B. Zang. Vetting undesirable behaviors in
Android apps with permission use analysis. In Proceedings
of the ACM Conference on Computer and
Communications Security, pages 611–622. ACM, 2013.
Y. Zhou, X. Zhang, X. Jiang, and V. Freeh. Taming
information-stealing smartphone applications (on Android).
Trust and Trustworthy Computing, pages 93–107, 2011.

EARs in the Wild: Large-Scale Analysis of
Execution After Redirect Vulnerabilities
Pierre Payet

Adam Doupé, Christopher Kruegel,
Giovanni Vigna

Ecole Superieure d’Informatique Electronique
Automatique, Paris

University of California, Santa Barbara

payet@et.esiea.fr
ABSTRACT
Execution After Redirect vulnerabilities—logic flaws in web applications where unintended code is executed after a redirect—have
received little attention from the research community. In fact, we
found a research paper that incorrectly modeled the redirect semantics, causing their static analysis to miss EAR vulnerabilities.
To understand the breadth and scope of EARs in the real world,
we performed a large-scale analysis to determine the prevalence of
EARs on the Internet. We crawled 8,097,283 URLs from 255,957
domains. We employ a black-box approach that finds EARs which
manifest themselves by information leakage in the HTTP redirect
response. For this type of EAR, we developed a classification system that discovered 2,173 security-critical EARs among 416 domains. This result shows that EARs are a serious and prevalent
problem on the Internet today and deserve future research attention.

1. INTRODUCTION
The Internet and the Web have become an integral part in the
lives of billions of people who routinely use online services to store
and manage sensitive information. Unfortunately, the popularity of
online services has attracted cybercriminals. An important class of
attacks targets web applications: For example, in 2010, malicious
activity targeting businesses’ web applications increased 93% compared to the previous year [13]. Ideally, web applications would be
impervious to leaks, attacks, or any other threat. However, due
to the complexity of modern web architectures and web technologies, this goal is elusive. Even vulnerabilities such as SQL injection or cross-site scripting (XSS), which are well-known, are still
frequently exploited and make up a significant portion of the vulnerabilities discovered every year [6, 27].
In this paper, we focus on a class of security flaws that is not as
well-known as XSS and SQL injection vulnerabilities, but equally
serious from a security point of view: Execution After Redirect [10].
Execution After Redirect, or EAR, is a type of security flaw that
occurs when unintended code is executed after a call to a redirect
function in a web application. The web application developer intends for the control flow of the web application to halt at the call to

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SAC’13 March 18-22, 2013, Coimbra, Portugal.
Copyright 2013 ACM 978-1-4503-1656-9/13/03 ...$10.00.

{adoupe, chris, vigna}@cs.ucsb.edu
the redirect function, but, depending on the web application framework, the control flow does not halt. This misunderstanding of the
web application framework semantics causes an EAR vulnerability
because unintended code is executed and the web application sends
an HTTP redirect response. As a result of the EAR, the unintended
code execution can cause information leakage through the HTTP
redirect response or can cause unauthorized database modification.
Previous research on EARs focused on detecting EARs in Ruby
on Rails web applications via static analysis of the source code [10].
This approach detects unauthorized database modifications, but is
limited to Ruby on Rails applications and requires access to the
application code. This paper attempts to find information leakage
EARs by looking at the HTTP redirect response.
An HTTP redirect response is used by a web application in the
following way. When a user attempts to access a resource on a web
application, the server can send an HTTP redirect response, indicating to the user to look elsewhere for the requested resource. Depending on the HTTP status code sent by the server1 , the browser
will make a new request to the redirect location [12].
To assess the prevalence of information leakage EARs on the
web at large, we performed a large-scale crawl of the web. During this crawl, we are looking for evidence of an information leakage EAR. Because information is leaked through the content of the
HTTP redirect response, we look at the HTTP redirect response as
external evidence of an information leakage EAR. From our crawl,
we developed a classification system that classifies content of the
HTTP redirect response, or simply redirect content, as benign or
security critical. This classification system works in a black-box
manner—with no access to the web application’s code, only the
HTTP redirect response content.
In summary, we provide the following contributions:
• A black-box approach to detecting a class of EARs (more
precisely, information-leakage EARs).
• A classification tool based on the proposed approach that can
automatically and efficiently identify EARs.
• A quantification of information-leakage EARs on the Internet based on a large-scale crawl of web applications. We
found 2,173 likely vulnerable EARs across 416 domains.

2.

EXECUTION AFTER REDIRECT

An Execution After Redirect is broadly defined as any unintended (from the perspective of the developer) server-side code that
is executed after a redirect call. In this way, an EAR is an unintentional control flow vulnerability: The developer does not intend for
1

301, 302, 303, 304, or 307 all indicate a redirect.

1
2
3
4
5
6
7

<?php
if (!$user->is_premium_member())
{
header("Location: /signup.php");
}
echo "Premium content that requires a subscription.";
?>

Listing 1: Example of PHP code where execution continues after
the redirection is triggered by the header function.
HTTP/1.1 302 Found
Server: Apache/2.2.3 (CentOS)
X-Powered-By: PHP/5.1.6
Set-Cookie: PHPSESSID=oj5intb9382pmevfm92pbm7bj7; path=/
Location: /login.php?auth=false
Content-Type: text/html; charset=ISO-8859-1
<html>
<head><title>FootPlus: Player Statistics</title></head>
<body>
<div id="main_container">
<b>Player Name: </b>Christopher Vigna<br/>
<b>Position: </b>TE<br/>
<b>Avg Yds: </b>XYZ<br/>
<b>Avg Points: </b>X<br/>
... More Content ...
</div>
</body>
</html>

Listing 2: Raw HTTP response adapted from a real EAR our
crawler found. The response code is 302, yet there is content in
the body. In this case, the content requires a paid subscription to
access, yet it is sent in the response body.
the code to be executed. However, because of his misunderstanding of the redirection semantics of the web application framework,
unintended code is executed anyway. Note that EARs defined in
this way are bugs but not necessarily security-critical flaws. An
EAR can become security-critical depending on what code is executed after a redirect. Therefore, the severity of an EAR is entirely
application-specific.
A security-critical EAR can compromise the environment of the
web application in two ways: (1) permanently change the state
of the web application or (2) leak sensitive information in the response. We call EARs that leak sensitive information in the response explicit, and those that do not leak any information silent.
Frameworks, depending on their architecture, design, and language,
may allow silent EARs, explicit EARs, or both [10].
Listing 1 shows an example of an EAR in server-side PHP code.
Line 2 checks if the user is a premium member, and if not, the
header function on Line 3 is invoked to redirect the user to a page
where she can purchase a premium account. However, because
header() does not halt execution, the control flow will continue
and the premium content will be sent on Line 4. While this example
is simplistic, we refer the interested reader to previous research for
more complex examples [10].
Listing 2 shows a raw HTTP response adapted from an EAR
we discovered in the wild. The EAR exists on a web application
that sells American-football statistics, however a request to a page
with the player’s statistics leaks the data the application is selling.
As shown in Listing 2, the response code is 302, and there is a
location header redirecting the user to the login page. However,
the full content (in this case the player’s statistics) is leaked after
the headers. Note that this is equivalent to leaking the proprietary
contents of the site’s database to unauthenticated users.
An Execution After Redirect vulnerability can violate the confidentiality, integrity, or both of a web application. Confidentiality

is violated through information leakage (as shown in Listing 2),
where private data requiring an access fee is sent in the response.
The integrity of the web application can be threatened when the
server-side code continues to execute after failing an access-control
check. In these attacks, an unauthorized user can change the state of
the web application, often by modifying the database. An example
of this type of vulnerability is a forum where a non-administrator
user can change the title of a thread [10].
EAR vulnerabilities have been studied only recently. We found
the first public EAR instance in the Common Vulnerabilities and
Exposures (CVE) database in 2007, as CVE-2007-2003. There
is also an entry2 in the Common Weakness Enumeration (CWE)
database (a community-developed list of software weaknesses) from
2008 called “Redirect Without Exit.” However, even with this classification, no CVE entry is associated with this CWE entry. This
lack of association shows that even if the community at large might
have been exposed to the concept of EARs, they do not use this
category to classify discovered EARs.
EARs have received little direct attention from the research community, nonetheless, there is evidence of EARs in some of the literature. For instance, Swaddler [8] found an EAR in the PHP BloggIt
web application, however the authors did not identify it as such. In
addition, as previously reported by Doupé et al. [10], Felmetsger et
al. discovered an EAR in a Java web application and did not identify the vulnerability as an EAR [11]. Chaudhuri and Foster present
a Ruby on Rails example, UsersController, that contains an
unmentioned EAR vulnerability [5]. Finally, EAR vulnerabilities
are subtle and can occur even in published peer-reviewed papers.
For instance, Sun, et al. incorrectly model the semantics of redirection in PHP, thus ignoring and missing EAR vulnerabilities [23].

3.

EAR DETECTION

We developed a black-box classification system to detect different types of EAR vulnerabilities. In the following, we describe
the types of vulnerabilities that we identify and our detection technique.

3.1

White-Box vs. Black-Box

For automated vulnerability discovery, one can follow two major
approaches: white-box and black-box testing.
White-box testing analyzes the application’s source code. In this
way, the testing tool obtains a view of the entire application, including all entry points. Typically, a control flow graph (CFG) is created, and data flow techniques are used to find paths from the points
where external information (e.g., user input) is read (sources) to
security-critical operations (sinks). If insufficient sanitization is
performed along one of these paths, the application contains a vulnerability (e.g., a SQL injection if the sink was a SQL query).
While a white-box testing approach derives a full view of an application, it can only analyze applications written in the specific
language that the tool targets.
Black-box analysis approaches operate without any knowledge
of the internal working of the application: The internal state of the
application and its source code are unknown. A black-box web vulnerability analysis tool sends requests to the web application and
observes the associated responses. The idea is to detect vulnerabilities in a way that is independent of the underlying language.
Because they are not language-dependent, black-box tools are
able to operate on a wide range of applications. In addition, blackbox tools usually suffer from fewer false positives when compared
to white-box tools. The lack of false positives is due to the black2

CWE-698

box tool actually exercising the application and exploiting the vulnerability. However, black-box tools suffer from a discoverability
problem: They cannot find a vulnerability in code that they fail to
execute.
The black-box approach we took to detect information leakage
EARs has the possibility of false positives. These false positives are
due to the black-box nature of our approach: We do not know for
certain if the information leaked is the result of an EAR or is from
a legitimate HTTP redirect response. Moreover, the content of the
leaked information along with the way the web application uses
that information determines the severity of the EAR—therefore automatically determining the severity of the information leakage will
have false positives.

3.2 Black-Box EAR Detection
The goal of our approach is to detect EARs in a black-box manner. EARs need not be directly visible externally: code executing
after a redirect could, for instance, change the server-side state. We
limit ourselves to analyzing the HTTP response of the web application. For an HTTP response to indicate an EAR vulnerability, two
things must be true. First, by definition, the response must be an
HTTP redirect response. Second, the HTTP redirect response content must divulge confidential information about the web application. Therefore, our system attempts to detect if the HTTP redirect
response content divulges confidential information, thus indicating
a potential EAR.

3.3 Classification of non-EARs
To develop this classification system, we manually analyzed the
initial results from our large-scale crawl of the Internet. From
this analysis, categories emerged. It is from these categories—
developed by looking at actual content of redirect response—that
we created the non-EAR and EAR classification categories.
In the following we describe the heuristics we use to make the
distinction between legitimate content (non-EAR) and EAR content. We first identify as legitimate responses that are empty. Then
we developed a number of string patterns that match text commonly
used in legitimate redirections3 . There is no standard format for
such messages, but we found common patterns because the HTTP
redirect responses are sent by a few well-known web frameworks.
We also consider as legitimate responses in which the body of the
HTTP redirect response is a near duplicate of the page the redirection leads to. This is because, obviously, no additional, sensitive information was revealed to the client. Then, we discard broken and
malformed HTML content which typically contains page headers
or navigation menus.
Framework Redirect
We examined a number of the most popular web frameworks and
the web pages they produced as part of the redirection process.
From these pages, we were able to extract a set of signatures (regular expressions) that help us identify such content.
Generic Redirect
In the general case, the content of a redirect message aims to warn
a user of a changed location. Therefore, we manually developed
regular expressions by analyzing a large corpus of data during our
experiments. Overall, we developed 110 such signatures.
Irrelevant or Broken Content
These are HTTP redirect responses whose content is not syntactically correct or contains very little information. To estimate the
3

An
example
regular
expression
pattern
<title>(30[123] )?document moved
((permanently)|(temporarily))?</title>

is:

value and extent of information in an HTTP redirect response, we
first remove all the HTML tags (so that only text content is left).
If less than n words are left, we consider the content to be irrelevant. With this irrelevant content, it is clear that server-side code
has been executed but, due to the lack of information, we cannot automatically determine the severity of the flaw; so we classify these
as non-security critical.
Near Duplicate
Near duplicate detection recognizes cases in which content of the
redirect page is similar or identical to the redirect location’s content. We utilize the well-known Normalized Compression Distance
(NCD) to rapidly compute an approximation of the similarity between the HTTP redirect response content and the redirect location’s content [7]. These are not EARs, because the information
contained in the HTTP redirect response is very similar to the target page that is presented to the user.

3.4

Classification of EARs

Once we are left with content of the redirect page that is not obviously legitimate, we divide the remaining redirects using heuristics
that attempt to identify different cases based on their severity. Note
that our classification system processes the content of the redirect
page in a precise order that will be explained in Section 3.5.
Error Message
Some of the content sent by the server contains error messages,
which we wish to capture in a category. So we put in this category
potential security-critical errors such as PHP errors and Java errors.
Content of the redirect page in this category discloses information
about paths of the server, details about the framework, the language
and often, their version number. Black-box vulnerability scanners
look for the same error messages when scanning a web application. We developed 11 regular expressions to classify the response
content as an Error Message.
HTTPS Redirect
This category consists of responses from the server in which the
HTTP redirect response contains a full web page and the redirect
leads to a page served through a secure channel (via HTTPS). We
consider this a security-critical EAR because, by redirecting to a
secure channel, the web application is signaling that the content is
sensitive. Yet, the web application sends some content in the clear.
Pre-Login Access
Responses from this category redirect to a login page. The intuition here is twofold: (1) when one tries to access a restricted resource the web application will redirect her to a login page, and
(2) the restricted content of the redirect page will contain links to
restricted content. Here, we attempt the capture the (applicationspecific) logic of a web application requesting authentication from
a user.
To detect this type of security-critical EAR, we select links inside
the response body, in addition to the one in the Location header,
that also cause an HTTP redirect response. If, among these links, a
significant amount (80%, explained in Section 3.6) leads to a login
page, we classify them in this category.
Transparent Barrier
This category classifies responses which contain links that, when
requested, redirect to the same page. Web application developers
sometimes want the user to visit a certain page prior to allowing
access to the full content of the web application. For example the
user is redirected to the same page until she has “signed” a Terms
of Use form. These cases are considered as a non-security–critical
EAR because the information they limit the access to is available
after visiting the special page.

Extracts the links

302

R1
200 OK

HTTP 302

L1
L2
L3

200 OK

200 OK

Distance
Matrix

302

R3
Download the pages
Figure 1: Heuristic’s diagram of the Transparent Barrier category.

To classify content of the redirect page in this category, we extract all the links on the HTTP redirect response (L1 , L2 , and L3 in
Figure 1) and request the corresponding pages. From those requests
that redirect, and contain content (R1 and R3 in Figure 1), we compute the NCD similarity of each content to every other content. If
a certain percentage (discussed in Section 3.6) is very similar, then
the original response is considered to be a Transparent Barrier. The
intuition here is that we wish to capture instances where content is
hidden behind a barrier.
Other
This category encompasses content of the redirect page that did not
match any of the previous categories.
The previously-discussed categories provide a way to assess the
potential severity of the content of the redirect page. Instances
matching the Framework, Generic Redirect, and Near Duplicate
categories, even though they send content after a redirect, are not
EARs. However, EARs in the other categories point to possible
logic flaws.

3.5 Classification Pipeline
The goal of the classification process is to separate the securitycritical EARs from the legitimate content. To do this, we applied
each categorization to the content of the redirect page in a precise
order to improve the accuracy of each category as well as the overall efficiency. A diagram of the classification pipeline is given in
Figure 2.
The order we classify EARs is critical. This order acts as a pipeline; the content of the redirect page is placed in the first category
that it matches. We apply Empty Redirect first because it is fast and
allows us to reduce the remaining HTTP redirect responses by 49%.
We then separate the Framework Redirect and after apply Generic
Redirect, which removes 45% of the remaining HTTP redirect reponse.
Prior to extracting the security-relevant EARs, we noticed instances of HTTP redirect response that were not leaking information. We needed at this stage of the pipeline to apply the filter of a
security-relevant category: Error Message. These potential errors
messages are often not HTML or are short, thus we apply the Error
Message category before the Irrelevant category.
Then, after the Irrelevant category, we filter the HTTPS responses.
Disclosing information on a clear channel through the redirection
can allow an eavesdropper to redirect the traffic from a login form
running over SSL to a clear channel. The latter category is applied before the Near Duplicate category because HTTPS redirect
response are frequently similar to the target body content, therefore
the classification must occur before the Near Duplicate category.
After Near Duplicate, there are two more categories that are similar but each attempts to capture a different aspect of the HTTP
redirect response content. The Pre-Login Access filter searches for
information leakage, characterized by links within the HTTP redirect response content that redirects the user to a login page. On
the other hand, the Transparent Barrier looks for the pattern when

most of the HTTP redirect response redirect to the same page. In
this way, Transparent Barrier is more general than Pre-Login Access, thus, we classify Pre-Login Access before Transparent Barrier. We make a distinction between the two categories because
Pre-Login Access indicates a lack of authorization and is thus more
severe than the Transparent Barrier category.

3.6

Classification Thresholds

Our EAR categorization requires setting thresholds for the Irrelevant, Near Duplicate, Pre-Login Access, and Transparent Barrier
categories. Changing any of these parameters alters the false positives and false negatives of the EAR categorization process. In fact,
the security-critical EARs are interspersed along the classification
pipeline. To determine appropriate values for these thresholds we
performed experiments—using a subset of the data we collected
by crawling the Web (described in Section 4.1)—in which we varied each threshold to quantify its influence on the classification of
its category. We first selected a random day of crawling for the
classification subset, and then we manually categorized every content of the redirect page in the subset. Finally, for each category’s
threshold, we varied the threshold to understand how it affected the
categorization.
Figure 3 shows a ROC curve for each of the four categories that
use a threshold. A cross on the graph denotes where we chose
the threshold. The rest of this section describes in more detail
the threshold selection process for each categorization component’s
threshold.

3.6.1

Irrelevant.

The Irrelevant categorization component analyzes how much information the HTTP redirect response content contains, based on
how much non-HTML content is present. The HTTP redirect response content will match this category if the content has less than
the threshold number of non-HTML–markup words. We ran ten
experiments, ranging the threshold from 10 to 100 words.
Figure 3(a) shows the ROC curve for the Irrelevant threshold.
Because this category comes before the EAR categories in the pipeline, false positives are EARs that would be caught by the EAR
categories; thus, false positives are costly. Therefore, we chose
a value of 60 words for the Irrelevant threshold. As seen on the
ROC curve, Figure 3(a), increasing this threshold will allow more
false positives, which we are trying to minimize, and decreasing
this threshold will reduce the true positive rate significantly.

3.6.2

Near Duplicate.

The Near Duplicate categorization component uses the Normalized Compression Distance as a threshold which ranges from 0 to
1. This threshold can be adjusted: Lower thresholds will match
results that are more similar than a higher threshold.
The Near Duplicate, like the Irrelevant category, is classified in
the pipeline before the EAR categories. Thus, any false positives
that end up in this category are potential EARs that would be categorized in an EAR category. Therefore, we wish minimize false
positives as much as possible.
Figure 3(b) shows the 10 experiments we ran, ranging the threshold from 0 to 1. As can be seen from Figure 3(b), our chosen
threshold, 0.5, reduces the false positives while still classifying a
good portion of similar content.

3.6.3

Pre-Login Access.

The threshold, in the Pre-Login Access categorization component, is the percentage of links on the page that also redirect to a

Empty

Framework

Generic

Error
Message

Irrelevant

Other

Transparent
Barrier

Pre-Login
Access

Near
Duplicate

HTTPS

1
0.8
0.6
0.4
0.2
0
0

0.2

0.4

0.6

0.8

1

True Positive Rate

True Positive Rate

Figure 2: Flow of the redirects throught the multi-step classification pipeline.
1
0.8
0.6
0.4
0.2
0
0

False Positive Rate

0.6

0.8

1

True Positive Rate

True Positive Rate

0.4

0.6

0.8

1

(b) Near Duplicate

1
0.8
0.6
0.4
0.2
0
0.2

0.4

False Positive Rate

(a) Irrelevant

0

0.2

1
0.8
0.6
0.4
0.2
0
0

False Positive Rate

0.2

0.4

0.6

0.8

1

False Positive Rate

(c) Pre-Login Access
(d) Transparent Barrier
Figure 3: ROC curves of threshold experiments. A cross denotes where we chose our threshold.
login page. To test this threshold, we performed 10 experiments
with different percentages.
For this category, we chose a threshold of 0.8, which is shown
in Figure 3(c) as a cross. However, it appears in Figure 3(c) that
there is a better threshold, which corresponds to 0.4. However,
this is an unfortunate quirk of our testing data; there were only 15
Pre-Login Access redirect content in the testing data. The lack of
data is a side-effect of the rarity of this category, in fact, our final
classification found only 0.053% of these out of the total content of
the redirect page. We chose 0.8 over 0.4 as the threshold because
it is a tighter threshold and will allow less false positives into the
Pre-Login Access category.

3.6.4

Transparent Barrier.

To determine the threshold for the Transparent Barrier categorization component, which controls how many links on an HTTP
redirect response content must redirect to the same page for it to be
classified as Transparent Barrier, we performed 10 experiments,
ranging the threshold from 0 to 1. The results of the experiments
are shown in Figure 3(d). We chose a value of 0.5 because increasing the threshold any further increases the false positive rate
without increasing the true positive rate. One can see that after
this point the ROC curve plateaus. This gives the category a false
positive rate of 26% and a true positive rate of 55%.

4. EXPERIMENTAL EVALUATION
To investigate the prevalence of EARs on the Web, we performed
a large-scale Web crawl looking for content sent after a redirection.
We then classified the content of the redirect page according to the
categories we developed.

Crawling Period
52 days
Average Speed
164,472 pages/day
Pages Downloaded
8,097,283
Total Domains
255,957
Total Redirections
1,708,771
Table 1: Statistics of a large-scale crawl of the Internet.

4.1

Crawling

We needed a web crawler to find a large number of content of
redirect page on which to apply our black-box EAR detection. We
required two properties: (1) the ability to store the content of the
redirect page and (2) to execute JavaScript. To find such a crawler,
we looked at available open-source projects, such as Nutch 4 and
Heritrix 5 . Unfortunately, both did not index the redirection body’s
content and did not execute JavaScript. To the best of our knowledge, there was no open-source large-scale crawler that also executed JavaScript.
Due to the limitations of available crawlers, we developed our
own large-scale, high-fidelity web crawler. We discover more URLs
while executing JavaScript: On average, we find about 5% more
links with JavaScript execution, which increases the search space
for EARs.
Table 1 shows the size of the dataset collected in 52 days of
crawling. We downloaded 8,097,283 pages from 255,957 distinct
domains. Of these pages, 1,708,771, or 21.11% were HTTP redirect response.
Table 2 shows the results of our classification system on the
1,708,771 redirect pages. The results are shown by category, sep4
5

http://nutch.apache.org/
http://sourceforge.net/projects/archive-crawler/

Code
301
302
303
Total

Empty
337,089
477,757
14,284
829,130

Framework
4,487
47,350
4
51,841

Generic
312,719
450,002
2,353
765,074

Irrelevant
22,014
30,290
367
52,671

N. Duplicate
1,524
798
1
2,323

Total Legitimate = 1,701,039

Error
HTTPS
Pre-Login Access
T. Barrier
Other
82
228
137
1,083
2,843
340
596
766
833
792
23
0
1
1
7
445
824
904
1,917
3,642
Security-Critical EARs = 2,173
Benign EARs = 5,559
Total EARs = 7,732

Table 2: Classification of 1,708,771 redirect content that was collected over a period of 52 days.
You have an error in your SQL syntax; check the manual
that corresponds to your
MySQL server version for the right syntax to use near ''
at line 4<br />
SELECT b.watch AS watch,b.friendtypeid AS friendtypeid,
minutesago(u.logdate) AS age, u.username AS username,
b.friendid AS userid, u.logdate AS logdate
FROM friends b
LEFT JOIN users u ON u.userid = b.friendid
WHERE b.userid =

Listing 3: 302 Found response body content leaking a SQL
request adapted from a real EAR that our crawler found.
arated by HTTP response code, followed by the totals. Of the
1,708,771 HTTP redirect response we discovered in the wild, 40%
are 301, 59% are 302, and 1% are 303. A significant portion
of the HTTP redirect response are concentrated in the Empty and
Generic categories, as expected. However, there is a large amount
of content of redirect page in the Irrelevant category with a total of 52,671 HTTP redirect response. Our classification system
found 2,173 security-critical EARs spread out over 416 distinct domains; this result demonstrates that EARs are a serious problem
that plagues real-world web applications.
To get a handle on how effective our heuristics are, we evaluate the false positives of our classification approach. To evaluate the false positives, we randomly sampled 100 HTTP redirect
responses from each security-critical EAR category and manually
verified them.
Table 3 shows the results from the manual evaluation of the random sampling. The HTTPS and Error Message categories are accurate with 3% and 0% of false positives, respectively. The false
positives for the Error Message category is so low because we
are searching for specific error messages that are unlikely to appear in legitimate pages. The HTTPS category still contains unfiltered generic or irrelevant content. Because our heuristic to detect
generic content is not complete, we have false positives that display
information for the user.
The Pre-Login Access category has a false positive rate of 13%.
The uniqueness of each web application remains the main cause of
these false positives. Irrelevant responses end up in the Pre-Login
Access category because they had sufficient non-HTML words to
bypass the Irrelevant threshold, but did not leak any information.
Finally, the Pre-Login Access category suffers from the problem of
a login form being present on every page in a web application.
In an effort to improve the security of the web sites in our crawl,
we contacted 50 web sites, via email or contact form, that contained security-critical EAR vulnerabilities that our crawl discovered. Five developers replied to our vulnerability notification. Of
these, two developers fixed the EAR vulnerability and three thanked
us for the notification and said that they would look into the vulnerability. We believe this shows that this is a vulnerability that is
misunderstood by developers yet they believe that it is a serious
enough vulnerability to fix.

4.2 Interpretation
As previously mentioned, the content of redirect page is diverse.
We saw a variety of content: non-sensitive content, page duplica-

Category
Percentage False Positive
HTTPS
3%
Error Message
0%
Pre-Login Access
13%
Table 3: Percentage of false positives calculated by manual analysis
of a random sampling of 100 redirects of each security-critical EAR
category.
tion, half-formed HTML pages, just JavaScript (no HTML) pages,
navigation bars with no content, and unidentifiable (junk) content.
A confusing type of content of redirect page we found were
empty pages that contain only white spaces and tabulations. We
cannot definitively determine what is causing this, but we believe
that some code execution happened on the server because the indentation appears to be source code. Here, without additional information from the developer we cannot attest if it is a bug.
Next, we discuss the content that ended up in each security-critical
EAR category.

4.2.1

HTTPS Redirect.

The web application developer wants to secure data transmission
when she sends information over SSL. However, due to an EAR,
this private information is sent in the clear for content of redirect
page that was classified as HTTPS Redirect. Here, the privacy and
confidentiality of the data is compromised if an attacker can eavesdrop on the conversation.
Two particular, security-critical, examples stand out from our
dataset. The first is an HTTP redirect response that we analyzed
from a web application selling financial trading services. This response gives access to a (private) information request form in the
clear. Although we did not try—due to ethical considerations—we
suspect that the web application would allow us, as non-authenticated
users, to submit the form and request the information.
The second example HTTP redirect response is similar to the
Pre-Login Access category: The content of the redirect page leaks
a film’s review form that is only available to registered users on a
popular film database. However, the application sends us this form
in the content of the redirect page.

4.2.2

Error Message.

The Error Message category outputs sensitive information about
the technology used by the web application. We found in this category Java stack traces, SQL errors, and PHP source code. Listing 3
shows a SQL error EAR that we adapted from one found in the
wild. These types of information leakage ease the work of an attacker who is attempting to exploit the web application thanks to
information about local paths or SQL query syntax.

4.2.3

Pre-Login Access.

Inside the Pre-Login Access category, the most common type of
EAR was the one where content of the page was displayed even
though we were not authenticated. The web application detects
our lack of access rights, and it redirects us to a login page. The
application is aware that we are not authenticated (as evidenced
by the redirection) but still outputs the content of the web page.
Listing 2 shows a canonical example from this category. This web

application sells sports statistics. Our crawler discovered an EAR
that lets anyone access the private content.
In a similar fashion, we found an EAR on a social networking
web application where anyone can access a user’s profile inside
the content of the redirect page, thus violating that user’s privacy.
A conclusion that can be drawn from this type of EAR is that it
indicates that the authentication mechanism of the web application
is flawed. The harm is specific to the web application and can affect
the web application’s owner or the user as well.

4.2.4

Transparent Barrier.

Even though Transparent Barrier is a category that contains benign EARs—that is, there is HTTP redirect response content and
server-side execution, however the information leaked is not securitycritical—the results of this category surprised us. Often, the body
of the HTTP redirect response is outdated versions of the web application. Instead of removing the content, developers set a redirection but keep the old web application version. It is clear that the
web developers do not want the content to be accessible, however,
the old content is leaked in the response.
Another example is a domain name that has been sold. The domain then redirects, via a 301 response code, to a different domain.
We believe this is done for SEO purposes, as the 301 is permanent
and transfers the so called “Google juice,” or search engine importance, to the new domain. However, the former domain still runs
the code of the old web application and sends the old web application’s output in the content the redirect page. We also saw instances where the barrier required a user to select a country, supply
the user’s age, or accept Terms of Use while still sending the data
to which the barrier prevents the access.

4.2.5

Other.

The Other category was the last stage of the pipeline; any content that did not match one of the other categories landed here.
This category contains diverse content, however we will focus on
two types: legitimate HTTP redirect responses and security-critical
EARs.
Legitimate content is, for example, an HTTP redirect response
that displays enough words to bypass the Irrelevant category’s threshold and does not match a regular expression of the Generic category. This misclassification is due to the fact that our regular expressions are focused on English language content, while the Internet is international. We could improve this by creating regular
expressions that target a diverse set of languages.
The security-critical EARs were not classified in the proper category for one of two reasons. They were either application-specific
error messages, where the error message was too specific to match
the more generic regular expressions of the Error Message category. The other security-critical EARs that were in Other are those
that should have been in Pre-Login Access, however, the HTTP
redirect responses did not have enough links leading to a login page
to match the Pre-Login Access threshold.
We do not consider the Other category to be security-critical
because we were unable to broadly categorize the security relevance of these EARs. However, because legitimate content was
already classified earlier in the pipeline by the Empty, Framework,
Generic, Irrelevant, and Near Duplicate categories, the content in
the Other category has a large percentage of potentially securitycritical EARs. In fact, a random sampling of the Other category
found 59% security-critical EARs. This result shows that the Other
category can be a valuable source of potentially vulnerable EARs.

4.3

Limitations

Due to the black-box approach we took to detecting Execution
After Redirect vulnerabilities, we are limited to discovering explicit
EARs only. Our approach does not attempt to infer the web application’s state, and, therefore, we only find EARs that leak information
in the content of the redirect page.
Our classification method uses heuristics that were built by manually analyzing the initial data collected, and, as a result, our approach is prone to both false positives and false negatives. The
main reason for false negatives is the diversity of content of redirect
page; however, we believe that our heuristics capture the essential
semantics of information leakage EARs. False positives will occur
because each web application is different.
False positives that appear in all EAR categories are the typical
redirect message whose body is in a non-English language. Even if
our regular expressions could still capture most of the generic content, every web application developer has the ability to customize
the content sent in the HTTP redirect body. Thus, these legitimate
HTTP redirects are misclassified and can land in HTTPS, Pre-Login
Access, Transparent Barrier, and Other, depending on the features
present in the web application. Nonetheless, they are more frequent
inside the default category Other.
Misclassified Error Message responses are found in Irrelevant
and Other categories because we are not exhaustive in the search
of these errors. False negatives of the Pre-Login Access category
can be found in HTTPS and Other. The HTTPS misclassification
is due to the order of our pipeline: HTTPS categorization is done
before Pre-Login Access. Pre-Login Access redirect content can
end up in Other due to the threshold.
Even though our approach has both false positives and false negatives, the categorization process significantly reduces the amount
of content of redirect page that an analyst must review in order to
discover security-critical EARs.
Because we built the heuristics after in-depth manual analysis
of a subset of the crawling data, our categories are biased towards
trends we saw in this subset. It is possible that the frequency of
these categories is not representative of the whole Web. However,
we constructed the categories to be broad enough to enclose essential features of the security-critical EARs.

5.

RELATED WORK

The work presented in this paper is at the intersection of multiple topics of security research: vulnerability discovery, black-box
vulnerability detection, and logic flaws.
There have been many approaches to automatically detect security vulnerabilities in web applications. Most approaches track
the information flow through the application, using static or dynamic analysis, and offer an overview of how secure the application is regarding a known vulnerability. Many approaches attempt
to discover vulnerabilities via white-box analysis of the web application’s source code [1, 3, 5, 10, 17–19, 22, 23]. Among the whitebox approaches, our work is closely related to Doupé et al. work
which detected EARs in Ruby on Rails web applications via static
analysis of the source code [10].
We use a black-box approach to detect EARs in web applications. Another approach to detecting vulnerabilities in web applications is using a black-box web vulnerability scanner, which actively
attempts to exploit the web application to discover vulnerabilities.
Huang et al. developed one of the first tools for black-box analysis of security vulnerabilities [16]. Other tools were developed to
improve black-box web vulnerability scanners [2, 15, 20], and attempts were made to evaluate the capabilities of open-source and

commercial black-box web vulnerability scanners [4, 9, 14, 24, 25].
Unlike black-box web vulnerability scanners, we did not fuzz the
web applications, instead, we passively analyzed web applications
to understand the prevalence of EARs on the Internet.
Because EARs are a logic flaw in a web application, our work
is related to research on logic flaws. Felmetsger et al. developed a
white-box tool to assess logic flaws in web applications [11]. They
were able to find an EAR in GIMS, an HR web application, through
dynamic analysis and symbolic model checking. Wang et al. analyzed logic flaws in how web applications use Cashier-as-a-Service
APIs [26]. Li and Xue used a gray-box approach to detect state violation attacks against web applications [21], while Sun et al. used
static analysis to detect the same type of vulnerability in PHP web
applications [23].
In comparison, our approach is focused on vulnerability discovery and is intended to find a specific logic flaw through a black-box
approach. However, to our knowledge, no other research has been
done to understand the prevalence of EARs on the Internet. Without being intrusive, from the point of view of the web application,
we managed to discover vulnerable EARs on the Internet.

6. CONCLUSION
We have shown, using a novel approach, that Execution After Redirect vulnerabilities, despite being relatively obscure, are
widespread on the Internet. These vulnerable web applications are
leaking sensitive information to anyone who asks for it. And, because we used a passive approach, it may be possible to leverage
deeper interaction with the web application while crawling to discover even more EARs. We hope that this work raises awareness
of this prevalent security vulnerability.

7. REFERENCES

[1] An, J., Chaudhuri, A., Foster, J.: Static Typing for Ruby on
Rails. In: Proceedings of the 24th IEEE/ACM Conference on
Automated Software Engineering (ASE’09). pp. 590–594.
IEEE (2009)
[2] Balduzzi, M., Gimenez, C., Balzarotti, D., Kirda, E.:
Automated Discovery of Parameter Pollution Vulnerabilities
in Web Applications. In: Proceedings of the 18th Network
and Distributed System Security Symposium (2011)
[3] Balzarotti, D., Cova, M., Felmetsger, V.V., Vigna, G.:
Multi-module vulnerability analysis of web-based
applications. In: Proceedings of the 14th ACM conference
on Computer and communications security. ACM (2007)
[4] Bau, J., Bursztein, E., Gupta, D., Mitchell, J.: State of the
Art: Automated Black-Box Web Application Vulnerability
Testing. In: Security and Privacy (SP), 2010 IEEE
Symposium on. pp. 332–345. IEEE (2010)
[5] Chaudhuri, A., Foster, J.: Symbolic Security Analysis of
Ruby-on-Rails Web Applications. In: Proceedings of the
17th ACM Conference on Computer and Communications
Security (CCS’10). pp. 585–594. ACM (2010)
[6] Christey, S., Martin, R.A.: Vulnerability Type Distribution in
CVE. Tech. rep., MITRE Corporation (May 2007)
[7] Cilibrasi, R., Vitanyi, P.: Clustering by Compression.
Information Theory, IEEE Transactions on 51(4), 1523 –
1545 (april 2005)
[8] Cova, M., Balzarotti, D., Felmetsger, V., Vigna, G.:
Swaddler: An Approach for the Anomaly-based Detection of
State Violations in Web Applications. In: Proceedings of the
10th international conference on Recent advances in
intrusion detection. vol. 4637. Springer (2007)
[9] Doupé, A., Cova, M., Vigna, G.: Why Johnny Can’t Pentest:
An Analysis of Black-box Web Vulnerability Scanners. In:
Proceedings of the Conference on Detection of Intrusions
and Malware and Vulnerability Assessment (DIMVA). Bonn,
Germany (July 2010)

[10] Doupé, A., Boe, B., Kruegel, C., Vigna, G.: Fear the EAR:
Discovering and Mitigating Execution After Redirect
Vulnerabilities. In: Proceedings of the 18th ACM
Conference on Computer and Communications Security
(CCS 2011). Chicago, IL (October 2011)
[11] Felmetsger, V., Cavedon, L., Kruegel, C., Vigna, G.: Toward
Automated Detection of Logic Vulnerabilities in Web
Applications. In: Proceedings of the USENIX Security
Symposium. Washington, DC (August 2010)
[12] Fielding, R., Gettys, J., Mogul, J., Frystyk, H., Masinter, L.,
Leach, P., Berners-Lee, T.: Hypertext Transfer Protocol –
HTTP/1.1 (June 1999), http://www.w3.org/
Protocols/rfc2616/rfc2616-sec10.html
[13] Fossi, M., Egan, G., Haley, K., Johnson, E., Mack, T.,
Adams, T., Blackbird, J., King Low, M., Mazurek, D.,
McKinney, D., Paul, W.: Internet security threat report. Tech.
rep., Symantec Corp (April 2011), vol. 16
[14] Grossman, J.: Challenges of Automated Web Application
Scanning. Blackhat Windows 2004 (2004)
[15] Halfond, W., Choudhary, S., Orso, A.: Penetration testing
with improved input vector identification. In: Software
Testing Verification and Validation, 2009. ICST’09.
International Conference on. pp. 346–355. IEEE (2009)
[16] Huang, Y.W., Huang, S.K., Lin, T.P., Tsai, C.H.: Web
Application Security Assessment by Fault Injection and
Behavior Monitoring. In: Proceedings of the 12th
international conference on World Wide Web. pp. 148–159.
WWW ’03, ACM, New York, NY, USA (2003)
[17] Huang, Y.W., Yu, F., Hang, C., Tsai, C.H., Lee, D.T., Kuo,
S.Y.: Securing Web Application Code by Static Analysis and
Runtime Protection. In: Proceedings of the 13th international
conference on World Wide Web. pp. 40–52. WWW ’04,
ACM, New York, NY, USA (2004)
[18] Jovanovic, N., Kruegel, C., Kirda, E.: Pixy: A Static
Analysis Tool for Detecting Web Application Vulnerabilities
(Short Paper). In: Proceedings of the 2006 IEEE Symposium
on Security and Privacy. pp. 258–263 (2006)
[19] Jovanovic, N., Kruegel, C., Kirda, E.: Precise Alias Analysis
for Static Detection of Web Application Vulnerabilities. In:
Proceedings of the 2006 workshop on Programming
languages and analysis for security. ACM (2006)
[20] Kals, S., Kirda, E., Kruegel, C., Jovanovic, N.: Secubat: a
Web Vulnerability Scanner. In: Proceedings of the 15th
international conference on World Wide Web. pp. 247–256.
ACM (2006)
[21] Li, X., Xue, Y.: BLOCK: A Black-box Approach for
Detection of State Violation Attacks Towards Web
Applications. In: Proceedings of the Annual Computer
Security Applications Conference (ACSAC 2011). Orlando,
FL
[22] Livshits, B., Lam, M.: Finding Security Vulnerabilities in
Java Applications with Static Analysis. In: Proceedings of
the 14th conference on USENIX Security Symposium Volume 14. USENIX Association (2005)
[23] Sun, F., Xu, L., Su, Z.: Static Detection of Access Control
Vulnerabilities in Web Applications. In: Proceedings of the
20th USENIX conference on Security. pp. 11–11. SEC’11,
USENIX Association, Berkeley, CA, USA (2011)
[24] Suto, L.: Analyzing the Accuracy and Time Costs of Web
Application Security Scanners (2010)
[25] Vieira, M., Antunes, N., Madeira, H.: Using Web Security
Scanners to Detect Vulnerabilities in Web Services. In:
Dependable Systems & Networks, 2009. DSN’09. IEEE/IFIP
International Conference on. pp. 566–571. IEEE (2009)
[26] Wang, R., Chen, S., Wang, X., Qadeer, S.: How to Shop for
Free Online: Security Analysis of Cashier-as-a-Service
Based Web Stores. In: Security and Privacy (SP), 2011 IEEE
Symposium on. pp. 465 –480 (may 2011)
[27] Williams, J., Wichers, D.: Owasp top 10 - 2010 the ten most
critical web application security risks. Tech. rep., The
OWASP Community (2010)

HoneyMix: Toward SDN-based Intelligent Honeynet
Wonkyu Han, Ziming Zhao, Adam Doupé, and Gail-Joon Ahn
Arizona State University
{whan7, zzhao30, doupe, gahn}@asu.edu

ABSTRACT

Kojoney [9], which also offers an SSH service, only needs
a slightly different detection method, since it returns the
timestamp when Kojoney was installed. Using such detection techniques, attackers can easily identify honeypots and
behave differently afterwards. Therefore, techniques that
prevent attackers from detecting the existence of emulated
system and services are imperative in building effective honeypots.
Honeynet [23, 30], which is a network of honeypots, inevitably poses the same problem of honeypots themselves.
Moreover, honeynets must control incoming and outgoing
data in the network and aggregate captured data from different honeypots. For example, the third generation (Gen-III)
honeynet [28, 13] employs a customized firewall called honeywall as the gateway of the network to realize better control on inbound/outbound traffic. Honeywall runs in layer-2
bridge mode to hide its existence and monitors all incoming
and outgoing traffic. It is also used to contain and limit the
large volume of outbound traffic generated by compromised
honeypots (e.g., when used in a DDoS attack). However,
Gen-III architecture cannot fully support today’s heterogeneous services in honeynet due to its coarse-grained data
control. For example, let us assume that honeypot A exposes XSS vulnerability while running a fake HTTP service,
and we want to deploy a new honeypot B that emulates
SQL injection vulnerability over HTTP. Since conventional
architecture only allows one service to interact with attackers at any given time, opportunity for collecting SQL injection attack (or XSS attack) is inevitably restricted. We can
also consider that the honeypot B emulates the HTTP service and XSS vulnerability in a different level of interaction.
Current architecture is still failing in combining both honeypots to attract as many attacks as possible. Existing data
control mechanisms in Gen-III architecture are not sufficient
to accommodate such cases.
To defeat honeypot fingerprinting techniques and to provide fine-grained data control for honeynet, we propose to
leverage the emerging software-defined networking (SDN)
architecture and techniques [24]. SDN provides a flexible
and programmable network environment along with enhanced
control of the network by separating the control plane from
the data plane. In SDN, a network administrator (or a program operating on their behalf) can centrally program data
control logic via specific APIs (i.e., OpenFlow [12]). Because
an SDN switch can dynamically control network traffic by
applying various actions, data control in honeynet can centrally be managed with the help of SDN.

Honeynet is a collection of honeypots that are set up to attract as many attackers as possible to learn about their patterns, tactics, and behaviors. However, existing honeypots
suffer from a variety of fingerprinting techniques, and the
current honeynet architecture does not fully utilize features
of residing honeypots due to its coarse-grained data control
mechanisms. To address these challenges, we propose an
SDN-based intelligent honeynet called HoneyMix. HoneyMix leverages the rich programmability of SDN to circumvent attackers’ detection mechanisms and enables finegrained data control for honeynet. To do this, HoneyMix
simultaneously establishes multiple connections with a set
of honeypots and selects the most desirable connection to
inspire attackers to remain connected. In this paper, we
present the HoneyMix architecture and a description of its
core components.

Keywords
Software-defined Networking; Network Function Virtualization; Honeynet; Honeypot

1.

INTRODUCTION

Honeypots [29], as a form of electronic baits, are built
to intentionally expose vulnerable resources to attackers so
as to encourage probing and exploiting. Because the key
objective of honeypots is to learn about attackers’ behaviors and capture new types of malware, honeypots do not
have to implement all functionalities of a production system. Consequently, honeypots usually emulate or simulate
certain systems and services to reduce the computational
and maintenance cost.
However, emulator-based honeypots, including operating
system and services, can be easily fingerprinted. For example, recent research efforts [1] revealed that an SSH honeypot called Kippo [7] always returns a hardcoded timestamp
when attackers access its system information using the simple Linux command uname -r. Another honeypot called
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SDN-NFVSec’16, March 11, 2016, New Orleans, LA, USA.
c 2016 ACM. ISBN 978-1-4503-4078-6/16/03. . . $15.00

DOI: http://dx.doi.org/10.1145/2876019.2876022

1

1
2
3

class c o m m a n d _ u n a m e( H o n e y P o t C o m m a n d) :
def call ( self ) :
if len ( self . args ) and self . args [0]. strip () in
( ‘ -a ’ , ‘-- all ’) :
self . writeln ( ‘ Linux % s 2.6.26 -2 -686 #1 SMP
Wed Nov 4 20:45:37 UTC 2009 i686 GNU /
Linux ’ % self . honeypot . hostname )
else :
self . writeln ( ‘ Linux ’)
commands [ ‘/ bin / uname ’] = c o m m a n d _ u n a m e

4
5
6
7

Table 1: Overlapping services of honeypots.
Provided Service
Honeypot Level of Interaction
HTTP SSH FTP
Dionaea [3]
Low-Interaction
X
X
Valhala [11]
Low-Interaction
X
X
Glastopf [4]
Low-Interaction
X
HIHIT [25]
High-Interaction
X
Kojoney [9]
Low-Interaction
X
Kippo [7]
Low-Interaction
X
Honssh [5]
High-Interaction
X

Figure 1: Hardcoded Linux Version in Kippo Honeypot Source Code.
1
2
3
4

this timestamp in (ii) is a strong indicator of Kippo. Another SSH honeypot named Kojoney [9] uses a timestamp
generated at installation time as shown in Figure 2. Even
though the timestamp is not hardcoded, attackers can easily compare the timestamp with the time of attack to detect
Kojoney-based SSH honeypots.
In addition, Holz et al. [18] introduced other techniques
that help attackers detect honeypots by checking for suspicious environments. Because the majority of honeypots run
in virtualized environments, attackers can infer the presence
of honeypots by checking their environmental variables. For
example, user-mode Linux and physical device information
(video card or network interface card) are used to identify
virtual environments of honeypots.

def p r o c e s s _ u n a m e( self ) :
self . t r a n s p o r t. write ( FAKE_OS + ‘\ r \n ’)
...
FQDN = " f q d n _ p l a c e h o l d e r" # fake domain name
( i . e . , www . example . com )
F A K E _ K E R N E L _ V E R S I O N = "2.6.9 -5. ELsmp #1 SMP "
T I M E S T A M P = datetime . now () . strftime ("% b % d % H
:% M :% S % Z % Y ")
FAKE_OS =" Linux "+ FQDN +" "+ F A K E _ K E R N E L _ V E R S I O N
+" "+ T I M E S T A M P+" i386 GNU / Linux "

5
6
7

Figure 2: Timestamp Generation in Kojoney Honeypot Source Code.
To take advantage of SDN in building such a honeynet, we
propose an SDN-enabled intelligent honeynet called HoneyMix. HoneyMix keeps a map of all available services in
the network and generates data control rules in a centralized manner. To maximize the use of every honeypot, HoneyMix adopts group communication methods (multicast) to
distribute incoming traffic to a set of associated honeypots.
Then, HoneyMix selects the most desirable connection that
might induce attackers further behaviors, and it replies back
to attackers while associated honeypots collecting malicious
data. To do this, HoneyMix has five core components: (1)
Response Scrubber module, (2) Forwarding Decision Engine
(FDE), (3) Connection Selection Engine (CSE), (4) Behavior Learner module, and (5) SDN switch (es).
This paper is organized as follows. We discuss the limitations of existing honeynet architecture in Section 2. To
solve the problems, We present the design of HoneyMix
along with its core components in Section 3. In Section 4,
we discuss related work. Section 5 concludes this paper.

2.

2.2 Coarse-grained Data Control in Honeynet
A honeynet is a network of honeypots that is intended
to attract as many attackers as possible to collect data and
learn about the patterns, behaviors, and tactics of attackers [2, 28]. The third generation (Gen-III) honeynet [16, 23]
adopts a customized firewall called honeywall to realize two
important honeynet functionalities: data control and data
capture.
• Data control: honeywall runs in layer-2 bridge mode
to enable transparent monitoring of network traffic
without revealing its presence. More importantly, it
is used to contain the attacker’s actions against external networks. Of particular worry is Denial of Service
attacks, so honeywall limits the outbound connections
that are generated from compromised honeypots.
• Data capture: to capture malicious payloads and
behaviors, Gen-III honeynet integrates built-in logging
tools and IDS utilities such as iptables [27], snort [10],
and sebek [8].

PROBLEM STATEMENT

In this section we overview two limitations of existing honeynets: (1) most honeypots can be easily fingerprinted and
(2) Gen-III honeynets only provide coarse-grained data control.

However, the layer-2 bridge in Gen-III honeynet is not sufficient to dynamically convey data to honeypots where it can
properly be handled. A set of honeypots offering the same
fake services in the network (e.g., Kippo and Kojoney) need
to receive the same copy of relevant packets to maximize the
use of them while existing honeywall cannot support those
architecture. As shown in Table 1, there exist various honeypots that emulate the same service in terms of SSH and
HTTP (web) services. In particular, when we put a lowinteraction honeypot and a high-interaction honeypot together, which is categorized by different level of interaction,
in the network, determining the flow path of data for distribution becomes more complex. Because low-interaction
honeypots are usually effective in only the early stage of
attacks, in-depth data collection are mostly performed by
high-interaction honeypots.

2.1 Honeypot Detection Techniques
A wide range of fingerprinting techniques have been devised to detect the existence of honeypots. Some efforts [1,
22] focused on finding a group of invariants that indicates the
operating system (or the service) is emulated. For example,
Dean et al. [1] revealed that the Kippo honeypot [7], when
emulating a fake SSH service, always returns the same string
when attackers access its system information. As shown in
Figure 1, the default system information emulated in Kippo
is hard-coded so that it prints out in two ways: (i) Linux
(line 6); or (ii) Linux hhostnamei 2.6.26-2-686 #1 SMP Wed
Nov 4 20:45:37 UTC 2009 i686 GNU/Linux (line 4). Thus,
2

There are few efforts to address this issue such as Honeybrid [6]. To facilitate the use of both honeypots, Honeybrid
forwards initial attack traffic to low-interaction honeypot
and migrates the connection to a high-interaction honeypot
if needed. However, this approach does not fully utilize both
honeypots since it allows only one connection at a time, and
it would not work if the pair of honeypots consists of lowinteraction (or high-interaction) honeypots.
Moreover, Gen-III honeynet is only concerned about containing the outbound traffic, however an attack is also likely
to be dangerous to internal network. If a compromised honeypot attempts to infect another honeypot in the same network, honeywall cannot provide protection because malicious traffic is not destined to the external network.

HoneyMix-enabled Controller
Forwarding
Decision
Engine

Response
Scrubber

Connection
Selection
Engine

Honeypot A
(ssh)

Behavior
Learner

 con selection
 service mapping
Honeywall

 request
 reply

Attacker

Honeypot B
(ssh&http)

Honeypot C
(http)

SDN switch(es)

...

HONEYMIX ARCHITECTURE
In this Section we illustrate the five core components of
HoneyMix in detail. In particular, we focus on how HoneyMix achieves better data control than existing Gen-III
honeynet using Software-defined Networking (SDN).
3.

Figure 3: HoneyMix Architecture.
and checksums is necessary. Behavior Learner is responsible for computing weights, which CSE uses to
select the connection.

3.1 Overview of HoneyMix

• Behavior Learner computes a weight for each connection between SDN switch(es) and honeypots. This
weight acts as a score that indicates the activity of a
specific attacker’s connection. Based on the duration
time (δt) of the active connection and the frequency
of modification (#n) counted by Response Scrubber,
this module assigns a weight to each connection. The
longer the connection continues and the fewer modifications are made, the higher the connection weight
has.

HoneyMix is based on traditional Gen-III architecture
that includes a honeywall for controlling network traffic and
capturing malicious data. Behind the honeywall, we construct an SDN-enabled network to accomplish fine-grained
data control. By doing this, we not only take advantage of
Gen-III architecture but also enhance security of honeynet
with the help of SDN.
HoneyMix architecture consists of five components:
• Response Scrubber takes known fingerprinting techniques into account to reduce the possibility of exposure by scrubbing the response. Recall the example in
Section 2.1, network operators manually define existing detection mechanisms with their countermeasures
to perform sanitization. Response Scrubber first inspects attackers’ requests and selectively scrubs corresponding response that reveals the existence of honeypots.

• SDN switch(es) connects with HoneyMix-enabled controller to receive an instruction for steering data flow
and modifying network traffic in flight. SDN switch
can dynamically quarantine a compromised honeypot
and establish a new data stream for the newly instantiated honeypot using Network Function Virtualization
(NFV [17, 32]). The switch is mainly controlled by
FDE.

• Forwarding Decision Engine (FDE) creates a “service
map” that represents the services offered by the honeynet (heterogeneity) and overlapping services (redundancy) across honeypots. Based on the service map,
FDE centrally determines where network traffic should
be forwarded and pings each service running on every honeypot to ensure consistent and up-to-date honeynet status. To maximize the use of the honeypots,
FDE leverages SDN switches to forward malicious requests to all associated honeypots. To contain the malicious traffic and deliver seamless service, FDE quarantines the compromised honeypot and instantiates an
identical honeypot using Network Function Virtualization (NFV) technique.

To illustrate how HoneyMix works, we walk through a
use case as shown in Figure 3. When an attacker initiates a
connection, HoneyMix inspects the IP addresses and port
numbers to decide which services are associated with the
connection. If the connection attempt is destined to an SSH
service (default port: 22), FDE searches valid honeypots in
the network using the service map and installs forwarding
rules into corresponding SDN switches. At the same time,
CSE establishes a connection with an attacker on behalf
of honeypots. Upon a successful handshake, CSE creates
multiple connections with relevant honeypots. HoneyMix
performs selective traffic distribution using group communication (multicast). The conveyed request will trigger honeypots to generate multiple responses. Behavior Learner returns the weight of each connection so that CSE will choose
one of them and pipeline it to the connection which is established by FDE. If Response Scrubber detects the attempt of
fingerprinting, it sanitizes the response to make sure there
exists no clear evidence that indicates the system (or the
service) is emulated.
HoneyMix architecture has several strengths in data control when compared to the traditional Gen-III architecture.
First, every honeypot that offers the same service can receive

• Connection Selection Engine (CSE) establishes an
end-to-end connection between an attacker and a honeypot. HoneyMix maintains one connection between
an attacker and SDN switch and a number of connections between SDN switch (es) and honeypots. CSE
selects one of the connections from the latter area and
pipes it to the connection of the former area. Because HoneyMix basically breaks end-to-end connection, rewriting several header fields such as SEQ/ACK
3

honeypot1: service1, service3
Host
A

honeypot2: service2, service4
honeypot3: service1, service4, service7

Host
B

honeypot4: service2, service3
honeypot5: service5, service6, service7

Host
C

honeypot8: service1, service2, service7
honeypot9: service3

honeypot6: service3, service5
honeypot7: service1

Figure 4: Heterogeneous and Redundant Service Distribution in Honeynet.
malicious traffic by means of multicast while only one honeypot can be accessed at any given time in the traditional
honeynet. This help us maximize the use of multiple honeypots in the network. Second, it dynamically selects the
connection to send back with the most desirable response.
This response, of course, is immune to known fingerprinting
techniques when it is delivered to attackers (thanks to Response Scrubber). Third, SDN-enabled network can realize
flexible incident response by isolating a compromised honeypot. With the combination of NFV and SDN, HoneyMix
also activates a new honeypot and dynamically enable data
communication to deliver seamless service. In addition to
that, network reconfiguration techniques such as Moving
Target Defense (MTD [20]) are also possible in HoneyMix
architecture. Section 4.3 discusses this issue in detail.

• SM is the ‘service map’ constructed in an (m × n)
~hp1 ,
matrix form in which each row corresponds to S
~hpm .
~hp2 , · · · , and S
S
~ h and SM , we first compute service distribution
With H
k
−→
SD hk on a host hk such that 1 ≤ k ≤ l.
−→
~ h · SM
SD hk = H
k

(1)

−→
SD hk is an n-dimensional vector that shows heterogeneity
and redundancy of services on hk . For example, the list of
~ h = (1, 1, 1, 0, 0, 0, 0, 0, 0).
honeypots on Host A in Figure 4 is H
A
SM would be as below:

1 0
0 1
1 0

0 1

SM = 0 0
0 0

1 0


1
0
0
1
0
1
0
1 1 0
0 0 1

3.2 Centralized Data Control
3.2.1 Network Rule Computation
Hosting honeypots in honeynet requires significant manual configuration (e.g., adding ACL and routing rules). In
particular, honeypots co-existing on the same host may offer
a set of redundant services. As shown in Figure 4, we consider a honeynet that consists of three hosts, running nine
honeypots with seven vulnerable services1 in total. Each
honeypot may emulate multiple services (heterogeneity). In
addition, several services are necessarily redundant (redundancy) because the number of services is less than the number of honeypots. For example, honeypot1 provides two services (service1 , service3 ) and service1 is provided by two
honeypots (honeypot1 , honeypot3 ) on Host A.
Due to the heterogeneity and redundancy of provided services, generating network rules needs to consider the relations among host, honeypot, and service. We formalize this
problem using aforementioned elements as follows:

0
1
1
0
0
0
0
0
0

0
0
0
0
1
1
0
0
0

0
0
0
0
1
0
0
0
0

0
0
1

0

1
0

0

1
0



−→
~ h · SM = (2, 1, 1, 2, 0, 0, 1) refers to
Therefore, SD hA = H
A
the distribution of redundant services
on
−−−
→Host A. We next
compute entire service distribution (SDH) in honeynet via
−→
−→
−→
the addition of SDh1 , SD h2 , · · · , and SD hl .
l

−−−→ X −→
SDH =
SD hk

(2)

k=1

Therefore, entire service distribution in honeynet (Figure 4)
−−−→
−−−→
is SDH = (4, 3, 4, 2, 2, 1, 3). Note that SDH can also be
computed by a sum of row vectors of SM .
Based on above observations, Forwarding Decision Engine
−−−→
(FDE) translates the entire service distribution (SDH) into
corresponding network rules. FDE generates network rules
for each service and assigns different port numbers to differentiate redundant services on the same host if necessary.

• HN = {h1 , h2 , · · · , hl } is a set of hosts in the honeynet;
• HP = {hp1 , hp2 , · · · , hpm } is a set of honeypots in the
honeynet;
• SV C = {svc1 , svc2 , · · · , svcn } is a set of provided services in the honeynet;
~ h = hrh1 , rh2 , · · · , rhm i is an m-dimensional vector
• H
l
that corresponds to running honeypots on a specific
host hl . rhm equals to ‘1’ when hpm is installed on
host hl , otherwise ‘0’;
~hpm = has1 , as2 , · · · , asn i is an n-dimensional vector
• S
that represents active services on a particular honeypot hpm . asn equals to ‘1’ when svcn is active on
honeypot hpm , otherwise ‘0’;

3.2.2 Incident Response
For the best use of the centralized architecture of HoneyMix, we detect abnormalities in honeypots and reactively
cope with incidents. HoneyMix takes advantage of Gen-III
architecture that only limits a large volume of outbound
traffic (rate limiting). In addition to this, we dynamically
re-configure network rules to quarantine a compromised honeypot by leveraging the programmability of SDN. Based on
the logs collected from honeywall, FDE in HoneyMix removes existing network rules associated with the compromised honeypot and installs a new rule to block outbound
traffic from it. However, existing services provided by the
compromised honeypot would remain damaged until we fin-

1

Here, a service roughly means any type of program which
is occupying a specific network port to provide a communication channel.
4

ish the investigation (e.g., forensic) and recover the honeypot.
To remedy this limitation, HoneyMix embraces network
function virtualization (NFV) technique. Some efforts [26,
13] to build existing honeypots into a set of virtual instances
would also help us realize this approach. HoneyMix periodically snapshots each honeypot, and dynamically activates it
when an infected honeypot is detected. In such a way, HoneyMix ensures that every service in the network is always
up and running.

the modification ratio which is the number of unsanitized
response (#N − #n) divided by the number of successful
responses (#N ), where #n is the number of modifications
performed by Response Scrubber. Based on these two criteria, the weight of a connection a (Wa ) is computed as below:

3.3 Dynamic Connection Selection

4. RELATED WORK AND DISCUSSION

Wa = ts · δta ·

#N − #n
#N

(3)

We add a threshold (ts ) to prioritize a connection for the
quality of service.

To realize the architecture of HoneyMix, the importance
of connection selection in SDN switch(es)–honeypots area
cannot be stressed enough. However there exist many kinds
of obstacles in enabling this. First, dynamically hopping one
connection to another essentially breaks end-to-end connection between an attacker and a particular service. Modification of several packet headers, especially in the TCP
protocol, must be considered (e.g., rewriting of SEQ/ACK
numbers). Moreover, this may bring new chances for attackers to fingerprint the existence of NAT functions by checking
RTT delays. Second, selecting the most desirable connection
that could encourage attackers to launch subsequent attacks
is not trivial. Because there exists no clue to judge about
the suitability of connections, we need to develop a set of criteria to evaluate them in a reasonable way. Third, choosing
the right time for connection selection is also challenging.
For example, we do not want to transfer the connection in
the middle of transmitting a big file.
To accommodate the aforementioned challenges, we devise Connection Selection Engine (CSE) as a core building
block for connection selection. To enable seamless connection between an attacker and an emulated service in honeypot, CSE leverages existing SDN features that allows us to
perform network address translation (NAT). For data transmission over TCP protocol, HoneyMix maintains the state
tracking table in CSE that keeps track of sequence (SEQ)
and acknowledgement (ACK) numbers of connections. In
this table, CSE also inserts additional information for the
higher layers in the OSI reference model to handle a lot of
nonce which are dynamically generated by a specific service.
For example, HTTP service independently keeps cookie, referer, and authorization information to maintain the state of
users. Moreover, CSE records a set of key-pairs that are
used for encrypting/decrypting messages for SSH/HTTPS
service. Some additional header fields such as TCP checksum field are dynamically updated by CSE.
As discussed in Section 2.1, HoneyMix might circumvent
existing fingerprinting techniques just by hopping one connection to another. To achieve this, selecting the most realistic and desirable connection from multiple connections is
critical in building HoneyMix. Note that Response Scrubber module is also provisioning partial remedy for this in an
ad-hoc manner by fixing apparent indicators, but we want
to even avoid unknown fingerprinting techniques by dynamically hopping connections.
Behavior Learner module in HoneyMix determines the
weight of each connection based on two criteria: (1) the
duration time (δt) of a connection and (2) the number of
modifications (#n) made by Response Scrubber. To account
for attackers’ duration of session, the module measures continuing time of an active connection (δta ). It next obtains

4.1 Software-defined Networking (SDN)
SDN is an emerging network paradigm that separates the
control plane from the data plane [24]. Legacy network devices embed complex control logics to process network traffic
whereas SDN switches only perform simple “match-action”
based processing. By simplifying the data plane, SDN abstracts the control plane and consolidates those control logic
into a centralized controller. Because SDN enables logically
centralized network environment, SDN supports significant
programmability and flexibility that could help improve the
security of honeynet.
As the prevalent and widely adopted SDN protocol, OpenFlow [12] realizes such an SDN paradigm. To dynamically
program network traffic, the OpenFlow protocol supports
“Set-Field” operation in the data plane that allows us to
rewrite packet headers. Therefore, OpenFlow-enabled network implements network address translation (NAT) feature without employing additional network box. HoneyMix
makes the best use of OpenFlow-enabled network for realizing connection selection, which helps build more flexible
and robust honeynet.

4.2 Honeynet Architecture
The first generation (Gen-I) of honeynet, which was devised in 1999 [28], employs a firewall that mainly performs
data control at OSI layer-3. Although Gen-I architecture
successfully proved its ability in collecting attacks, it can be
easily detected by attackers. It could not properly handle
outgoing traffic either. The cornerstone of the second generation (Gen-II) and the third generation (Gen-III) honeynets
is a layer-2 based firewall called honeywall. Honeywall has
been devised to enable transparent network monitoring by
provisioning layer-2 bridging, which is difficult for attackers
to detect. Gen-II and Gen-III have the same architecture
except several additional functionalities [13]. Having Gen-II
components as the basis, Gen-III utilizes honeypot monitoring tools (e.g., sebek [8]) to check abnormalities and implements easier deployment of the honeywall. As cloud infrastructure is widely adopted in today’s networks, deploying
Gen-III honeynet in a virtual environment becomes more
popular since it brings many benefits (e.g., maintenance)
that deployment in a physical machine cannot provide [26].

4.3 Discussion
HoneyMix uses some ideas that have been presented in
moving target defense (MTD) [15, 31]. To increase uncertainty, MTD adopts several randomization techniques to reconfigure a set of properties of operating systems or network
interfaces [14]. Jafarian et al. [20] proposed to use the SDN
controller to randomize host information (i.e., MAC and IP
5

addresses) in order not to allow attackers to obtain the real
host information. Panos et al. [21] also proposed other randomization mechanisms to hide service version and OSes by
utilizing an SDN application to generate traffic that resembles a fake service or OS.
To prevent honeypot fingerprinting HoneyMix currently
adopts a connection selection engine that transfers from one
connection to another. This scheme helps increase anonymity
of existing honeypots by changing active connections. Moreover, Response Scrubber is designed to sanitize specific payloads that reveal information of honeypots. HoneyMix
could adopt aforementioned MTD techniques to further minimize the possibility of exposing network infrastructure (OS,
service, and host). For example, random host mutation
techniques introduced in MTD can also be considered in
HoneyMix-enabled controller to hide honeypots. In addition, we may consider to generate virtual IP and MAC
addresses to dynamically create corresponding DNS information to hide our network configurations.

5.

[14]

[15]
[16]

[17]

[18]

[19]

CONCLUSION

[20]

In this paper, we introduced HoneyMix architecture along
with its five components to build SDN-enabled intelligent
honeynet. To defeat existing honeypot fingerprinting techniques, HoneyMix dynamically selects the most desirable
connection among multiple connections and reactively sanitizes the response if it contains a known indicator of honeypots. To fix coarse-grained data control in traditional
Gen-III honeynet architecture, HoneyMix centrally computes service distribution in the network to enable finegrained data control and deploys corresponding rules via
SDN switches. Moreover, HoneyMix conducts a security
incident response including quarantine and recovery using
SDN and NFV. We are currently implementing HoneyMix
and planning to evaluate the architecture in real-world deployments.

[21]

[22]
[23]

[24]

Acknowledgments
[25]

This work was partially supported by the grants from Center
for Cybersecurity and Digital Forensics at ASU.

6.

[26]

REFERENCES

[1] Black Hat USA 2015 - Breaking Honeypots For Fun And Profit.
https://www.youtube.com/watch?v=Pjvr25lMKSY.
[2] Blogs|The Honeynet Project. https://www.honeynet.org/.
[3] Dionaea - carnivore. https://github.com/rep/dionaea.
[4] Glastopf Honeypot Project Page. http://glastopf.org/.
[5] Honssh Honeypot. https://github.com/tnich/honssh.
[6] Hybrid Honeypot Framework.
http://honeybrid.sourceforge.net/.
[7] Kippo SSH Honeypot. https://github.com/desaster/kippo.
[8] Know Your Enemy: Sebek (A kernel based data capture tool).
http://old.honeynet.org/papers/sebek.pdf.
[9] Kojoney2 SSH Honeypot.
https://github.com/madirish/kojoney2.
[10] Snort.Org. https://www.snort.org/.
[11] Valhalahoneypot Honeypot.
http://sourceforge.net/projects/valhalahoneypot/.
[12] OpenFlow Switch Specification Version 1.5.1 (Protocol version
0x06), December, 2014. https://www.opennetworking.org/
images/stories/downloads/sdn-resources/onf-specifications/
openflow/openflow-switch-v1.5.1.pdf.
[13] F. H. Abbasi and R. Harris. Experiences with a generation iii
virtual honeynet. In Telecommunication Networks and

[27]
[28]
[29]

[30]

[31]

[32]

6

Applications Conference (ATNAC), 2009 Australasian, pages
1–6. IEEE, 2009.
E. Al-Shaer. Toward network configuration randomization for
moving target defense. In Moving Target Defense, pages
153–159. Springer, 2011.
M. Carvalho and R. Ford. Moving-target defenses for computer
networks. IEEE Security & Privacy, (2):73–76, 2014.
M. Dornseif, F. C. Freiling, N. Gedicke, and T. Holz. Design
and implementation of the honey-dvd. In Information
Assurance Workshop, 2006 IEEE, pages 231–238. IEEE, 2006.
R. Guerzoni et al. Network functions virtualisation: an
introduction, benefits, enablers, challenges and call for action,
introductory white paper. In SDN and OpenFlow World
Congress, 2012.
T. Holz and F. Raynal. Detecting honeypots and other
suspicious environments. In Information Assurance Workshop,
2005. IAW’05. Proceedings from the Sixth Annual IEEE
SMC, pages 29–36. IEEE, 2005.
J. H. Jafarian, E. Al-Shaer, and Q. Duan. Openflow random
host mutation: transparent moving target defense using
software defined networking. In Proceedings of the first
workshop on Hot topics in software defined networks, pages
127–132. ACM, 2012.
J. H. Jafarian, E. Al-Shaer, and Q. Duan. Openflow random
host mutation: transparent moving target defense using
software defined networking. In Proceedings of ACM
SIGCOMM Workshop on Hot Topics in Software Defined
Networking (HotSDN’12), pages 127–132. ACM, 2012.
P. Kampanakis, H. Perros, and T. Beyene. Sdn-based solutions
for moving target defense network protection. In A World of
Wireless, Mobile and Multimedia Networks (WoWMoM),
2014 IEEE 15th International Symposium on, pages 1–6.
IEEE, 2014.
N. Krawetz. Anti-honeypot technology. Security & Privacy,
IEEE, 2(1):76–79, 2004.
J. Levine, R. LaBella, H. Owen, D. Contis, and B. Culver. The
use of honeynets to detect exploited systems across large
enterprise networks. In Information Assurance Workshop,
2003. IEEE Systems, Man and Cybernetics Society, pages
92–99. IEEE, 2003.
N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar,
L. Peterson, J. Rexford, S. Shenker, and J. Turner. Openflow:
enabling innovation in campus networks. ACM SIGCOMM
Computer Communication Review, 38(2):69–74, 2008.
M. Mueter, F. Freiling, T. Holz, and J. Matthews. A generic
toolkit for converting web applications into high-interaction
honeypots. University of Mannheim, 280, 2008.
N. Provos et al. A virtual honeypot framework. In USENIX
Security Symposium, volume 173, 2004.
M. Rash. Linux Firewalls: Attack Detection and Response
with iptables, psad, and fwsnort. No Starch Press, 2007.
L. Spitzner. The honeynet project: Trapping the hackers. IEEE
Security & Privacy, (2):15–23, 2003.
L. Spitzner. Honeypots: Catching the insider threat. In
Computer Security Applications Conference, 2003.
Proceedings. 19th Annual, pages 170–179. IEEE, 2003.
D. Watson and J. Riden. The honeynet project: Data collection
tools, infrastructure, archives and analysis. In WOMBAT
Workshop on Information Security Threats Data Collection
and Sharing, pages 24–30. IEEE, 2008.
R. Zhuang, S. A. DeLoach, and X. Ou. Towards a theory of
moving target defense. In Proceedings of the First ACM
Workshop on Moving Target Defense, pages 31–40. ACM,
2014.
M. Zimmerman, D. Allan, M. Cohn, N. Damouny, C. Kolias,
J. Maguire, S. Manning, D. McDysan, E. Roch, and
M. Shirazipour. Openflow-enabled sdn and network functions
virtualization. Solution Brief, ONF, Solution Brief
sbsdn-nvf-solution. pdf, 2014.

SoK: Everyone Hates Robocalls: A Survey of Techniques against Telephone Spam
Huahong Tu, Adam Doupé, Ziming Zhao, and Gail-Joon Ahn
Arizona State University
{tu, doupe, zzhao30, gahn}@asu.edu

Abstract—Telephone spam costs United States consumers
$8.6 billion annually. In 2014, the Federal Trade Commission
has received over 22 million complaints of illegal and wanted
calls. Telephone spammers today are leveraging recent technical advances in the telephony ecosystem to distribute massive
automated spam calls known as robocalls. Given that anti-spam
techniques and approaches are effective in the email domain,
the question we address is: what are the effective defenses
against spam calls?
In this paper, we first describe the telephone spam ecosystem, specifically focusing on the differences between email
and telephone spam. Then, we survey the existing telephone
spam solutions and, by analyzing the failings of the current
techniques, derive evaluation criteria that are critical to an
acceptable solution. We believe that this work will help guide
the development of effective telephone spam defenses, as well
as provide a framework to evaluate future defenses.

I. I NTRODUCTION
The national and global telephony system is a critical
component of our modern infrastructure and economy. In
the United States (US), the mobile telephone subscribership
penetration rate has already surpassed 100% [1]. According
to the U.S. Bureau of Labor Statistics, each day more than
240 million hours are spent on telephone calls in the United
States, equating to more than 88 trillion hours each year [2].
However, with the pervasiveness of telephone service
subscribership, telephone spam has also become an increasingly prevalent issue in the US. Recent technical advances
in the telephony ecosystem are leveraged by spammers to
distribute massive automated spam calls, known as robocalls. The Federal Trade Commission’s (FTC) National Do
Not Call Registry’s cumulative number of complaints of
illegal calls in the US totaled more than 22 million in
2014 [3], with about 200,000 complaints each month about
robocalls alone [4]. Despite US laws prohibiting robocalling
and telephone spamming (with some exceptions), complaints
on illegal calls have reached record numbers year after
year, which indicates that the laws have not deterred the
spammers.
Spam calls are significant annoyances for telephone users.
Unlike email spam, which can be ignored, spam calls
demand immediate attention. When a phone rings, a call
recipient generally must decide whether to accept the call
and listen to the call. After realizing that the call contains
unwanted information and disconnects from the call, the
recipient has already lost time, money (phone bill), and

productivity. A study in 2014 by Kimball et al. [5] found
that 75% of people listened to over 19 seconds of a robocall
message and the vast majority of people, 97%, listen to
at least 6 seconds. Even when the recipient ignores or
declines the call, today spammers can send a prerecorded
audio message directly into the recipient’s voicemail inbox.
Deleting a junk voicemail wastes even more time, taking at
least 6 steps to complete in a typical voicemail system.
Telephone spam are not only significant annoyances,
they also result in significant financial loss in the economy, mostly due to scams and identity theft. According
to complaint data collected by the FTC, Americans lose
more than $8.6 billion due to fraud annually, and the vast
majority of them (and still increasing) are due to phone
communication [4]. This situation is surprising, given the
significant gains made in reducing the amount of email
spam. This raises the question: are there any simple and
effective solutions that could stop telephone spam? The unfortunate answer is no. We found that this issue is not easily
solved, and, in fact, the simple and effective techniques
against email spam cannot be applied to telephone systems.
There are significant differences and unique challenges in the
telephone ecosystem that require novel approaches. Many
existing solutions have failed to overcome these challenges
and, as a result, have yet to be widely implemented.
The objective of this paper is to survey the existing
solutions in combating telephone spam and, by analyzing the
failings of the current techniques, derive the requirements
that are critical to an acceptable solution. This work will
help guide the development of effective telephone spam
defenses, as well as provide a framework to help evaluate
the techniques against telephone spam.
The main contributions of this paper are the following:
• We describe the telephone spam ecosystem, focusing
on the players involved and the technical challenges
that make telephone spam distinct from email spam.
• We develop a taxonomy that classifies the existing antispam techniques into three categories, providing a highlevel view of the benefits and drawbacks of each type
of technique.
• We provide a systematization of assessment criteria
for evaluating telephone spam countermeasures, and
we evaluate existing techniques using these assessment
criteria.

•

Termination Network

Termination Carrier

Possibly further anonymized behind VPNs and
Tor networks

Long Distance Network

Trunk Line

Spammer

VoIP Carrier

While email spam is arguably the most well-known form
of spam, telephone spam is now more popular than ever.
The Public Switched Telephone Network (PSTN) is an
aggregate of various interconnected telephone networks that
adheres to the core standards created by the International
Telecommunication Union, allowing most telephones to
intercommunicate. We define telephone spam as the mass
distribution of unwanted content to modern telephones in the
PSTN, which includes voice spam that distributes unwanted
voice content to answered phones, and voicemail spam
that distributes unwanted voice content into the recipient’s
voicemail inbox.
Due to the much greater capacity of IP infrastructure
and the wide availability of IP-based equipment, telephony
service providers have shifted their network infrastructure to
IP-based solutions, and the operation cost of the telephone
network has dramatically decreased. While the core PSTN
infrastructure has evolved to be almost entirely IP-based,
the core signaling protocols have not changed. The entire
ecosystem still relies on the three-decade-old Signaling System No. 7 (SS7) [6] suite of protocols, allowing any phone to
reach any other phone through a worldwide interconnection
of switching centers.
A very common way of disseminating telephone spam
is robocalling, which uses an autodialer that automatically
dials and delivers a prerecorded message to a list of phone
numbers. An autodialer is a generic term for any computer program or device that can automatically initiate calls
to telephone recipients. Today, an autodialer is usually a
computer program with Voice over Internet Protocol (VoIP)
connectivity to a high volume VoIP-to-PSTN carrier, that
may include features such as voicemail and SMS delivery,
customizable caller ID, Call Progress Analysis, scheduled
broadcast, text-to-speech, Interactive Voice Response, etc.
The high reachability of telephone numbers has led to
telephony being an attractive spam distribution channel. Almost every adult in the US can be reached with a telephone
number, and the vast majority of telephone numbers are
mobile telephone subscribers. Although VoIP usage has been
growing rapidly, we found that it is more of an add-on
protocol (instead of a wholesale replacement) of existing
mobile wireless and landline services. Using 2013 statistics,
there are about 335 million mobile telephone subscribers [1],
136 million fixed-telephone subscribers [7], and 34 million
VoIP subscribers [8] in the US (population 318 million).
We believe the improved cost efficiency of telephone
spamming, advancement of spam distribution technology,
and high reachability of telephone numbers contributed to
the recent surge in telephone spam. Furthermore, we believe
that telephone spam has the potential to be more persuasive

Open Internet

II. BACKGROUND

Interexchange Carrier

We provide a discussion on what we believe to be the
future direction of solving the telephone spam problem.

Victims

Possibly routing through more layers than
depicted

Figure 1: Routing of a spam call.

Victim

Spammer

VoIP
Carrier

Interexchange
Carrier

Termination
Carrier

Leads Seller

Figure 2: The flow of money in the telephone spam ecosystem.

than email spam, particularly when spammers use techniques
such as caller ID spoofing.
A. Key Players of Telephone Spam
To understand the telephone spam ecosystem, we will first
identify and explain the roles of all players who take part in
the routing of a telephone spam. Figure 1 show a graphical depiction of the routing process: The spammer connects through the Internet to an Internet Telephony Service
Provider, then the call is routed through an Interexchange
Carrier, before finally being accepted by the Termination
Carrier, who then routes the call to the victim.
Another way to understand the ecosystem is to show
how money flows through the system, which we display in
Figure 2: the money flows from the victim to the spammer,
and the spammer uses this money to obtain leads (new phone
numbers to spam) and to pay for the spam calls, the Internet
Telephony Service Provider receives the money from the
spammer and pays the Interexchange Carrier, who then pays
the Termination Carrier. Next we examine each of these
roles in turn.
Spammer is the agent that carries out the spamming
operation. The spammer could be part of an organization,
or an independent contractor that offers spamming-as-aservice. The goal of the spammer is usually to extract
money from victims through sales and scams, or to launch
a campaign of harassment. For cost efficiency, spam calls
are typically initiated using an autodialer connected to an
Internet Telephony Service Provider to reach the PSTN
victims. Currently, spamming to VoIP victims are not as
common, mainly due to the limited pool of potential victims,
and some VoIP users, such as Skype, may not be reachable
most of the time. We will describe the spammer’s operation
in more detail in Section II-B.

Internet Telephony Service Provider (ITSP), also
known as a VoIP carrier, is a type of termination carrier that
offers telecommunications service over the TCP/IP network,
i.e. the Internet. The ITSP typically offers high volume
calling at a lower cost compared to traditional carriers, and
generates revenue based on the minutes of calls hosted.
Whenever the spammer makes an outbound call to a PSTN
number, the ITSP will convert the signaling protocol from
VoIP to SS7, and route the converted signal through an
interexchange carrier.
Interexchange Carrier (IXC), also known as a long
distance carrier, is a cross-regional carrier that carries call
traffic between telephone exchanges over long distances.
The IXC charges its subscribers (mainly termination carrier
such as the ITSPs and local mobile/landline carriers) for
handling long distance phone calls and compensates the
next-hop carrier (such as the recipient’s termination carrier) for access. Unlike the peering model between Internet
service providers [9], the IXC negotiates access rates with
other carriers, known as intercarrier compensation. In the
US, intercarrier compensation [10] is a complex system in
which the rates vary according to traffic origination, location,
carrier, and traffic type, and the rates are governed by
federal and state regulators. In general, when two carriers are
directly connected, the originating carrier compensates the
next-hop carrier for routing the call in the next-hop carrier’s
network.
Termination Carrier, also known as local exchange
carrier, is a carrier that provides call routing services within
a local network that terminates at its end users. The termination carrier may be operating a landline, mobile, or IPbased telephone network. Most consumers and businesses
rely on termination carriers for their telecommunications
services. The termination carrier typically bills the IXC for
the amount of incoming traffic, known as the access charge.
In the US and some other countries, the recipient subscriber
may also be partially billed for incoming calls.
B. Spammer Operation
Spamming (regardless of the medium) requires three basic
elements: a recipient list, content, and a mass distribution
channel. In addition, a more sophisticated spammer may
employ circumvention measures to defeat spam countermeasures, and to avoid being stopped by law enforcement
agencies.
1) Gathering Numbers: Spamming first requires a list of
potential victims to contact, and in the case of telephone
spam: a list of phone numbers. While there are many ways
a spammer could gather phone numbers, the simplest method
is to purchase the numbers from a leads seller. We did
a simple Google search (keyword “leads for sale”) and
found hundreds of websites that offers access to millions
of curated phone numbers for less than $100. There are
also other ways to harvest phone numbers, such as crawling

the web, collecting form submissions, downloading leak
databases, covertly gathering through smartphone apps, or
simply generating the numbers based on phone numbering
plans. However, we do not know for sure the most popular
means of obtaining a list of phone numbers for spamming,
due to the lack of existing studies. Once the spammer gathers
a list of phone numbers, the spammer can load it in an
autodialer for mass distribution of the content.
2) Voice Spam Content: The content of telephone spam is
typically a prerecorded audio stream made by either recording human voice or by using a text-to-speech synthesizer
program. Telephone spam can also deliver interactive voice
content, with the use of an Interactive Voice Response (IVR)
system. When the recipient answers a call from an autodialer
with interactive content, the recipient can interact with the
system through voice and keypad inputs, and an automated
voice message is played back based on the interaction.
There are a wide variety of spam types, such as telemarketing, impersonation scam, debt collection, political
campaigns, one-ring scam, and so on. In order to provide
insight into the telephone spam content, we collected 100
audio samples from various publicly available sources where
audio recordings of voice or voicemail spam are uploaded.
We perform this analysis to gain a general understanding of
voice and voicemail spam, and we emphasize that, due to
the biased method of data collection, these results do not
constitute measurements that reflect trends on the whole of
voice and voicemail spam. However, these results provide
needed background and insight into the actual voice and
voicemail spam. We will describe the following prevalent
types of spam: credit card verification scam, fake tax agent
scam, and political robocalls.
In the credit card verification scam samples, the called
recipients are informed that their credit card account was
deactivated, and they are asked to enter their credit card and
social security number over the phone to verify their identity
and get the account reactivated. While we only were able
to listen to the audio of the call, based on comments from
some of the uploaders, the scammers would spoof the caller
ID to make it look as if the call originated from the credit
card issuer. All of these scam calls used an Interactive Voice
Response system to interact with the recipients and collect
their credit card information. We found that the audio from
the scammer’s IVR system came from either a synthesized
voice or audio duplicated from the IVR system of the real
credit card issuer. From what we observed, the use of caller
ID spoofing and sound duplicated from the real credit card
issuer’s IVR system made it almost indistinguishable from
a real credit card verification call.
In the fake tax agent scam samples, the recipient receives
a call from the scammer identifying himself as a tax agent
of the Internal Revenue Service (IRS) and provides a fake
badge number. The scammer proceeds to tell the recipient
that he or she owes a specific amount of money to the

IRS. Often, the scammers demand immediate payment and
threaten jail, deportation, or loss of driver’s license if the
victim does not pay. Based on the comments from the
uploaders, the scammers would spoof their caller ID to make
it look as if the call originated from a government agency
by showing an area code from 202 (Washington, DC). These
scammers seem to target immigrants [11]. We found that the
majority used a live person to interact with the victim, and
the rest used a prerecorded synthesized voice without an IVR
system. One thing we noted was that all of the live person
scammers had a South Asian accent, and in our opinion, the
accent had made the call sound highly suspicious and easy
to recognize as a scam (which might explain why it was
posted online as a scam).
In the political robocall samples, the typical content is a
prerecorded message making a political advertisement, or a
poll asking the recipient about their political opinion. In the
United States, political robocalls are exempt from regulation
by the national Do-Not-Call Registry and the Telephone
Consumer Protection Act of 1991. Before a national or state
level election, they are distributed in high frequency using
voice and voicemail broadcasting autodialers. All of the
audio samples contained a prerecorded message, and most
polls used an IVR system to interact with the recipient.
3) Mass Distribution: Mass distribution is the next critical step to a successful spam operation. The goal is to
massively and cost-effectively deliver the spam content to
a list of telephone numbers.
Using VoIP service to distribute calls to PSTN numbers,
the content can be disseminated at a much higher volume,
and at a fraction of the cost compared to traditional telephony. To understand the distribution cost of spamming, we
researched the prices and found hundreds of VoIP service
providers offering pay-by-the-minute calling service to US
telephone numbers priced around $0.01 per minute. We also
found some fixed monthly-fee pricing model with unlimited
calling for about $150, however, these service providers tend
to target small businesses, and these plans usually come with
throttling, so high volume calling services are almost always
offered with a pay-by-the-minute model.
Some VoIP service providers (such as CallFire1 and CallEm-All2 ) even cater specifically to telemarketers, providing
features such as integrated autodialer and customizable caller
ID in their service.
4) Circumvention: Spamming is an adversarial game, as
spam defenses are widely introduced, the spammer has an
incentive to defeat them. According to a poll conducted by
Harris Poll on behalf of WhitePages in 2013, 22% of US
smartphone users used a call-blocking app or a feature to
block calls on their device [12]. Most mobile phones today
1 https://www.callfire.com/
2 https://www.call-em-all.com/

contain basic capability to automatically block calls from a
list of unwanted callers.
For the spammers today, two common ways to defeat them
is to use voicemail injection and caller ID spoofing.
Voicemail injection is a recent extension of the autodialer
which delivers prerecorded voice messages into the recipients’ voice mailbox (voicemail). Typically, when a phone
call is unanswered or declined, it gets forwarded to an
answering machine that lets the caller leave a voice message. A voicemail broadcasting autodialer uses Answering
Machine Detection (AMD) [13] technology to automatically complete the process of inserting a prerecorded voice
message into the recipient’s voicemail. A more recent type
of voicemail broadcaster can even deliberately trigger the
recipient’s voicemail, a technique known as Forced Busy
Channel [14], to directly inject a voice message into the
recipient’s voicemail without waiting for the call to be
unanswered or declined.
Caller ID spoofing is the practice of deliberately falsifying
the caller ID information sent to the recipient that identifies
the caller of a phone call. It is particularly effective for
defeating the call blockers and helps to further a variety of
scams. The caller ID service provides the caller’s telephone
number (and in some cases the caller’s name) to the recipient
before or during the ring of an incoming call. It allows the
recipient to decide whether to answer a call based on the
caller ID information, or to call back if the call could not be
answered. The caller ID number is also widely used in other
non-voice communication services, such as SMS, MMS, and
many smartphone apps. The caller ID number is typically
provided by the caller’s switch, which can control what
caller ID number is sent on a call-by-call basis. For general
consumers, a legally mandated privacy feature allows them
to hide the calling number [15]. However, malicious callers
can also take advantage of the declarative nature of the
caller ID mechanism to spoof or block the caller ID number,
in order to defeat spam filters and further a variety of
scams. The caller ID number can be easily spoofed because
there is no built-in authentication mechanism, and it is not
immediately verifiable by the recipient. The caller’s service
provider does not have any legal obligation to ensure that
the caller ID number in the call request header is indeed
owned by the caller before it is transmitted. In fact, some
ITSPs today advertise customizable caller ID as a service
feature.

III. K EY C HALLENGES
We identify several challenges in combating telephone
spam—that are significantly different from email spam—
some of which are technical and some of which are regulatory.

A. Immediacy Constraint
Unlike email, which can be queued for later analysis, a
voice call has an immediacy constraint. A telephone call
request is immediate and therefore must be analyzed as
soon as it appears, and the telephone anti-spam system must
complete analysis and take action within a short window of
time to reduce the delay. If a solution adds too much delay
to a call request, the legitimate caller may assume that the
recipient could not answer the phone and hang up.

With the rise of VoIP services that provide features such
as caller ID customization over the Internet, it is trivial for
any caller to cheaply and effectively spoof the caller ID.
Thus, any telephone spam defense technique that relies on
the caller ID is now vulnerable to caller ID spoofing.
F. Difficulty of Tracing Spam Calls

The bar for user acceptance of a telephone anti-spam
system is much higher compared to email. Consumers,
rightly, have a very low tolerance for false positives of
blocked calls. Phone calls tends to be more urgent and
important compared the email, and once a phone call is
wrongfully blocked it could have severe consequences.

One way to combat spam is to make it illegal and enforce
those laws. In the history of email spam, a small number of
players were responsible for the majority of the spam, hence
taking action against these big targets resulted in significant
drops of spam volume. For instance, shutting down the Rustock botnet reduced global spam levels by around 40% [17].
It is reasonable to assume a similar distribution of telephone
spammers. Unfortunately, identifying the actual distribution
of telephone spammers is difficult due to the technical and
regulatory challenges of monitoring PSTN traffic and the
prevalence of caller ID spoofing.
It is difficult to locate the true origin of a call after it
has been initiated. PSTN calls are designed to work on
the principle of forwarding tables and circuit switching.
Each time a call is placed, only the destination number is
used for routing. It works by establishing individual circuits
down a sequence of neighboring switches until it ends up
at the recipient’s terminal. The outbound switch(es) do not
necessarily need to know whether the optional caller ID
number in the call request header would route back to
the caller’s terminal. If the outbound switch also serves as
the caller’s inbound switch, then the TSP could perhaps
verify the true owner of the caller ID number from its own
records. However, the TSPs do not have a legal obligation to
perform any verification, or to share that information with
the recipient, thus, without the cooperation of the caller’s
TSP, tracing a spam call is almost impossible.
To make matters worse, as spam calls can now be initiated
over the Internet, a spammer can further hide behind proxies,
VPNs, or Tor networks, or even distribute outbound calls
using a botnet, adding even more difficulty in tracing the
exact whereabouts of a spammer.

E. Caller ID Spoofing

G. Entrenched Legacy Systems

The Caller ID service is an information service that
provides the recipient with information of the caller before
answering the phone, which could be useful for blocking
spam calls. However, caller ID fundamentally has no authentication mechanism and is easily spoofed. The only security
mechanism comes from having the TSP send the caller ID
on behalf of the caller. This security mechanism is eroded
when the spammer subscribes to a TSP service that allows
customization of caller IDs. It used to be prohibitively expensive for individuals and small businesses to purchase the
equipment necessary to enable the customization of caller
IDs (an ISDN-PRI trunk line costs $500 to more than $1,000
per month and a PBX system that costs thousands [16]).

The PSTN ecosystem has been around for several decades,
allowing any phone to reach any other phone through a
vast interconnection of switching centers. While the core
networks have evolved to be almost entirely carried by
an IP-based infrastructure, the signaling protocols have not
changed (to ensure legacy compatibility). Even though VoIP
is touted as a major revolution of voice communication,
the legacy of PSTN protocols will remain for many years
to come. Change is difficult when the entire ecosystem
must ensure that the majority of legacy systems will work,
and therefore wholesale replacement of the core telephony
system is a nonstarter. As a result, telephone spammers can
exploit the weaknesses in the legacy technology (such as the

B. Difficulty of Working with Audio Streams
The content of a voice call is difficult to parse and analyze:
the content is an audio stream as opposed to the text of an
email. To make matters worse, the content of a voice call is
only revealed when the call is answered, and both the caller
and the recipient will be affected if an anti-spam system
answers the call. Whereas an email anti-spam system can
easily analyze the content of an email, and neither the sender
nor the receiver is affected.
C. Lack of Useful Header Data
Voice calls lack the rich header data of email. When a
call arrives at the recipient, it contains little useful header
information. An example of a call header used in traditional
phone terminals is shown in Table III in the Appendix. An
email header, however, has well-defined and informationrich SMTP headers—before the content of the email. It is
also difficult to omit the sender’s IP address and domain
name of the email. This is in stark contrast to a call request
header, where the header data is easily omittable by a
spammer.
D. Hard to Gain User Acceptance

lack of caller ID verification) to run a successful spamming
operation.
H. Lack of Effective Regulations
Unfortunately, there is also a lack of incentive for the
industry to participate in the anti-spam effort. Unlike email
and Internet traffic where the peering model [9] incentivizes
the Internet service providers to reduce the load of spam
traffic on their systems, telephony service providers profit
from the spam-generated traffic and intercarrier compensation fees. Most players (phone number collectors, lead
sellers, telephony service providers, and backbone carriers)
in the PSTN ecosystem profit from telephone spam, except
the consumer. Although TSPs may benefit in other ways
by reducing telephone spam (for instance, in better public
relations or charging spam-filtering service as a fee), there
exists, at least, a minor monetary disincentive.
Further complicating matters, the current United States
law ensure that TSPs are immune from liability for servicing
spam calls [18] under the Telephone Consumer Protection
Act of 1991, which means that they cannot be held liable
for servicing spam calls. Classified as common carriers,
TSPs have an obligation to move all phone traffic with no
exceptions [19]. Therefore, it is difficult to implement antispam solutions at the most natural place: the TSP who has
a direct view of the telephony network.
I. Lack of Globalized Enforcement
In the United States, a number of laws and regulation
exist at both the federal and state levels, such as making
robocalling illegal (with some exemptions) [20], making
caller ID spoofing illegal (with some exemptions) [21], and
the establishment of a national Do-Not-Call Registry [22].
The FTC is also interested in stopping telephone spam, and
they have held numerous competitions to combat robocalling [23]. Despite resolute efforts by the US government,
robocalling and caller ID spoofing is still an unsolved
problem. Technology and globalization have resulted in
telephony networks shifting from a national ecosystem to a
global ecosystem. With the use of VoIP service, a telephone
spammer can cheaply distribute outbound calls from an
overseas location. Because the spammers lie beyond the
jurisdiction of US law enforcement authorities, it is hard for
law enforcement to prosecute those spammers for breaking
the law. Effective control of telephone spam would therefore
require cross-border enforcement. However, cross-border
jurisdiction of telephone spam has yet to catch up with
the present technology, and many countries would have no
incentive to cooperate with US regulatory and enforcement
agencies.
IV. BASIC T ECHNIQUES
To identify the state-of-the-art in preventing voice and
voicemail spam, we gathered existing techniques from academic, industry, SPam over Internet Telephony (SPIT), and

Internet domain, and systematically categorize them into the
following classes: (1) Call Request Header Analysis, (2)
Voice Interactive Screening, and (3) Caller Compliance.
A. Call Request Header Analysis
Call Request Header Analysis is a category of techniques
that filters calls based on the header information associated
with the call request. For instance, the caller ID is a popular
type of request header information that can be used to
analyze a call. The effectiveness of Call Request Header
Analysis depends on the accuracy of the information
collected, which could be severely impacted when spoofing
or omission is possible.
Caller ID Blacklisting rejects a call if the caller’s phone
number (captured from caller ID or Automatic Number
Identification service) appears on a blacklist, otherwise,
calls from all other phone numbers are accepted. This
can be used to block spam calls by blacklisting phone
numbers that are known to be spamming, and the recipient’s
terminal would silently block all phone calls from those
phone numbers without disturbing the recipient. Caller ID
Blacklisting only blocks phone numbers that are explicitly
added to a blacklist, hence it tends to be permissive to all
other callers. As caller ID service has become ubiquitous
in all telephone services, Caller ID Blacklisting does not
face compatibility issues. Caller ID Blacklisting is easy to
implement and requires very little computational resources,
and it is a common feature in modern smartphones [24],
[25]. However, a blacklist must be well populated to be
effective against spam, therefore compiling a comprehensive
list would not be scalable for the recipient. A spammer
could defeat Caller ID Blacklisting by spoofing any number
not known to be blacklisted, hence it is not effective against
most forms of call request header manipulation.
Caller ID Whitelisting only accepts calls from phone
numbers that appear on a whitelist, otherwise, calls from
all other phone numbers are rejected. This can be used to
block spam calls by whitelisting phone numbers that are
known to be trusted, and the recipient’s terminal would
silently block phone calls from all other phone numbers
without disturbing the recipient. Caller ID Whitelisting is
easy to implement and requires very little resources, and
it is easy to find implementations on modern smartphones
[26], [27]. Caller ID Whitelisting blocks all calls that are
not added to a whitelist, and does not need to be well
populated to be effective against spam, hence it is quite
scalable for the recipient when defending against spam. It
is usually quite easy to populate a whitelist, as the numbers
could be derived from the recipient’s contacts list. However,
unknown legitimate callers would always get blocked in
Caller ID Whitelisting. A spammer could defeat Caller ID
Whitelisting by spoofing the caller ID of a number known

to be trusted by the recipient, however this is more difficult
without prior knowledge about the recipient’s whitelist.
Caller Reputation System uses reputation or trust
associated with a caller’s phone number to determine if the
caller is a spammer. A Caller Reputation System maintains
and publishes reputation scores associated with individual
callers, in which the reputation scores are computed based
on various caller-related information such as recipient
black/white-lists [28]–[31], caller behavior [29], [32],
[33], recipient behavior [28], [34], [35], caller’s domain
reputation [30], [36], social connections [34], [37]–[40], and
recipient feedbacks [28], [29], [31], [36], [41], [42]. There
are also many opportunities to improve a Caller Reputation
System by developing better scoring algorithms. The Caller
Reputation System can be used to filter spam calls by
configuring the recipient’s terminal to block calls from
callers associated with poor reputation. A Caller Reputation
System generally requires a large amount of data, which are
usually crowdsourced from many recipients, and the data
would need to be curated by an administrative third party. It
would also require frequent maintenance to ensure quality
and freshness of data in order to be effective. However,
large scale collection of personal information could be at
risk of violating privacy. Caller Reputation System could be
vulnerable to Sybil attacks, where a malicious caller obtains
multiple identities to gain a large influence over its own (or
other caller’s) reputation. Because the reputation of a caller
is associated with the caller’s phone number, a spammer
could defeat the Caller Reputation System by spoofing the
caller ID to a number with a good reputation. A malicious
caller could also sabotage someone by deliberately making
junk calls while spoofing the caller ID number, such that
the victim gets a poor reputation.
Caller Behavior Analysis uses the call behavioral features
associated with a caller’s phone number to determine if
the caller is a spammer, using behavioral features such as
call count/velocity [29], [33], [39], [43]–[49], call duration
sum/mean/variance [29], [39], [44]–[46], [48]–[50], call
rejection count/ratio [35], [39], [44], [46], [47], [49],
[51], [52], recipient diversity count/ratio [44], [45], [49],
[52], invalid recipient count/ratio [39], [47]–[49], [51],
repeated call count/ratio [45], [52], outbound-to-inbound
ratio [33], [48], [51], [53], [54], simultaneous calls [46],
and caller’s domain behavior [32], [51]. There are also
many opportunities to improve the technique by developing
better classification algorithms. Acquiring the caller’s
behavioral information usually requires participation from
the caller’s telephony service provider or a honeypot of
telephones [33], [35]. If not required by regulation, it is
usually not in the TSP’s business interest to report on or
impose a call behavior restriction on their callers. The
callers’ behavioral information would need to be updated

frequently to ensure accuracy and freshness in order to be
effective. Large scale collection of callers’ call behavior
could also face privacy issues and numerous obstacles from
legal regulations. Because the call behavior of a caller is
associated with the caller’s phone number, a spammer could
defeat the Caller Reputation System by spoofing the caller
ID to a number with good calling behavior. Furthermore, a
spammer could hide its illegitimate call behaviors by using
multiple caller identities.
Device Fingerprinting collects a variety of metadata from
the call request header for the purpose of creating a device
fingerprint of a caller’s terminal. Device fingerprinting
improves the accuracy of determining the caller’s identity
by using only a set of information that meets the properties
of diversity and stability. Device Fingerprinting has been
proposed for SPIT prevention by blacklisting or whitelisting
the device fingerprints of SIP-based terminals [55].
However, in PSTN, device fingerprint information is a
scarce resource. This is due to the little amount of header
information in PSTN call requests (an example of which is
shown in Table III in the Appendix) compared to SIP or
email, resulting in having too little workable information
for device fingerprinting to work effectively.
Caller ID Anomaly Detection searches for anomalous
patterns in the caller ID, such as invalid format, invalid
number, unavailable number, toll-free number, area codes,
regular expression, to determine if the caller is a spammer.
Caller ID Anomaly Detection is quite easy to implement and
requires very little computational resources and, therefore,
is easy to find in several call blocking apps [56], [57].
Caller ID Anomaly Detection does not track information
associated with any individual caller, instead, it looks
for general patterns in the caller ID that can be used to
differentiate spammers and legitimate callers. As Caller
ID Anomaly Detection tend to find matches more broadly,
it tends to be easier to manage and maintain. However,
some patterns may be potentially prone to false negatives,
and therefore may restrict some legitimate callers, such as
VoIP users or privacy enabled callers. A spammer could
defeat Caller ID Anomaly Detection by carefully crafting
the caller ID to not trigger any known anomalous patterns.
ANI-CPN Matching checks whether the Calling Party
Number (CPN) captured by the caller ID service matches
with the Automatic Number Identification (ANI) number
captured by the ANI service [58]. Automatic Number
Identification service [59] is a separate type of calling line
identification service that can capture the calling number
information even when the caller ID is not presented. It
was originally designed to obtain the calling party’s billing
number from a local exchange carrier to any interconnecting
carrier for billing of long distance calls. In most cases,

the billing number is the same as the CPN, and usually
when a mismatch happens it is likely due to caller ID
spoofing, or the caller is calling from a private branch
exchange (PBX). ANI-CPN Matching assumes that a
legitimate caller’s CPN matches the ANI number whereas
a malicious caller would spoof the CPN which results in
a mismatch. However, ANI service are usually not made
available to regular consumers (usually only offered to
800 toll-free, 900 premium-rate, or 911 emergency service
lines), therefore, only some businesses would benefit from
this technique. ANI service is also not always reliable at
capturing the caller’s ANI number. Placing a legitimate call
using an outbound VoIP service or a calling card service
would result in a non-working or a generic ANI number
being captured. As a result, false positives may frequently
occur which hinders user acceptance.
ANI-II Filtering can be used to filter spam calls by
blocking certain types of origin service captured by the
ANI-II service. ANI-II [60] is an extension of the ANI
service that identifies the type of service associated with the
originating switch. Each type of service is represented by
a two-digit code. ANI-II Filtering assumes that legitimate
callers would have a valid (00 or 61) ANI-II code, whereas,
malicious callers would be making VoIP calls that would
have an invalid ANI Failure (02) code, and therefore
should be blocked. However, with the growing use of
VoIP service by regular consumers, this technique could
potentially result in too many false positives if all calls
with ANI Failure codes are blocked. Only some businesses
would benefit from an implementation of this technique, as
ANI-II service is usually offered only to premium-rate, tollfree, or emergency lines. Therefore, this technique would
not be accessible or cost effective for the regular consumers.
B. Voice Interactive Screening
Voice Interactive Screening is a category of techniques
that forces the caller to interact with a voice input-based
interactive system and decide if the call is spam after
analyzing the caller’s interaction. The system either requires
active or passive interaction from the caller. An active
interaction system relies on the caller providing a response
to a specific task which requires some effort from the
caller, whereas a passive interaction system silently gathers
the caller’s response without explicitly informing the caller.
Voice Interactive Screening techniques do not need to
rely on the caller ID or any other call request header
information, hence they are generally not vulnerable to
caller ID spoofing. However, Voice Interactive Screening
techniques generally require processing of audio signals,
which tends to be more complex to implement. Because
these techniques can only work after recording a length of
the caller’s voice, all Voice Interactive Screening techniques

have a screening period, therefore, would introduce
additional delay to the caller. Due to the recording of the
caller’s voice during the screening, in the US, some states
require explicit consent of recording the conversation,
which could hinder the screening process or invoke privacy
fears from some legitimate callers. As telephone audio
can be manipulated, and tends to contain artifacts such as
background noise, network dropouts, or compression losses,
Voice Interactive Screening techniques are generally more
prone to errors.
Audio Fingerprinting uses the voice recording of the
caller, or audio features extracted from the voice recording
of the caller, to analyze for similarity to a set of known
spam call profiles. If the voice recording is similar to an
audio stream of a known spam profile, then the call is
classified as spam. Audio Fingerprinting has been proposed
to combat replayed voice spam in several works [61]–[67].
However, the performance of Audio Fingerprinting depends
on the completeness of spam profiles, which is usually not
feasible for a recipient to collect. Audio Fingerprinting
would usually require a thirty-party to continuously collect
and maintain the known-spam audio profiles to ensure
effectiveness. However, a spammer could potentially defeat
the mechanism by dynamically creating variations of the
spam audio message (such as adding audio artifacts or
using personalized messages) to avoid identification.
Speech Content Analysis first records the caller’s voice,
then makes use of speech recognition technology to
transcribe the voice into text. The text is then analyzed with
text profiles of known spam calls to classify if the call is
spam. As opposed to managing audio recordings, a corpus
of text data is usually much easier to manage. As many spam
calls are simply variations of a call script, a keywords-based
classification model could be used against variations of a
same type of spam [68]. However, the effectiveness of this
technique depends on the accuracy of speech recognition,
and of course the effectiveness of the classification model.
In practice, automatic speech recognition of telephone voice
is an ongoing research problem [69], which tends to be
prone to errors, and still has several years to go to reach
human-level performance [70].
Acoustic Pattern Analysis extracts distinguishing acoustic
patterns from the caller’s audio stream, such as signal
losses [71], peak uniformity [71], noise uniformity [71],
voice activity [72], [73], and double talks [72]–[74], to
determine if the call is spam. Audio Fingerprinting looks
for general patterns in the audio signal that can broadly
distinguish spam calls from legitimate calls. Unlike Audio
Fingerprinting and Speech Content Analysis, Acoustic
Pattern Analysis does not require a large collection of
known-spam profiles, which could be difficult to gather and

maintain. However, some patterns may be prone to false
positives and could be easily defeated with manipulation of
the audio stream.
CAPTCHA/Turing Test is an interactive challengeresponse technique that requires the caller to complete a
reverse Turing test to determine whether the caller is a
human or robocaller. The tests are designed to be difficult
for a computer but easy for a human to complete. For
instance, the test could ask the caller to key in what they
hear from a distorted audio stream of random numbers [75]–
[77]. However, CAPTCHA/Turing Test would need to be
careful not to discriminate against certain groups of people,
such as people with poor English or disabilities, while
not giving too much leeway for abuse by “decaptcha"
systems [78]. On the other hand, CAPTCHA/Turing Test
would also need to be careful not to be illegible even
for users with no handicaps, as the legitimate caller may
become irritated by the obstacles of initiating a call with
the recipient. Because CAPTCHA/Turing Test is highly
interactive, it tends to require a high degree of effort, and
cause significant delays to the caller.
C. Caller Compliance
Caller Compliance is a category of techniques that require
the caller to first satisfy a compliance requirement prior
to or during a call request. If the caller is able to satisfy
the compliance requirement, then the caller is allowed to
communicate with the recipient. Satisfying the requirements
should be easy for a legitimate caller but difficult (or
costly) for a spammer. Some compliance measures require
special changes made to the call setup process or to the
communicating terminals. Some techniques require prior
instructions given to the caller.
Do Not Call Registry simply provides a registry of
phone numbers that spammers are legally prohibited from
calling in most circumstances. The spammer may be
subject to substantial fines if they fail to comply. The
registry is usually maintained by the national government,
in the US [22], the list is maintained by the Federal
Trade Commission. However, the recipients would need to
actively provide feedbacks for the government to legally act
on spammers violating the law. The Do Not Call Registry
can act as a good deterrence for domestic law-abiding
telemarketers, however it would have little effectiveness on
spoofed numbers and overseas spammers.
Graylisting [79] first rejects the initial call request from a
caller and then accepts the next call request from the same
caller made within a short period of time. This technique
defends against autodialers that simply call a list of phone
numbers and do not make repeated call attempts. The

technique also assumes that if an uninformed (about the
defense) caller is calling about legitimate business, the
caller will try again. The implementation is simple and
does not require changes to the infrastructure. However,
the legitimate caller must make two calls for every call
request, which introduces additional delay and calling cost.
A spammer could easily defeat the Graylisting mechanism
by configuring the autodialer to automatically call again
if a call goes unanswered, but at the cost of higher phone
bills and reduced efficiency.
Consent-based Communication first requires the caller to
send a consent request to the recipient before initiating a
call. For instance, the request could be a forwarded greeting
message where an answering machine first records the
name spoken with the caller’s voice and then plays it to
the recipient [80]–[82]. The recipient then decides whether
to accept the caller’s request to communicate. If the call
is spam, the recipient is only limited to being exposed
to an abridged recording (or the request message) of the
spam call. However, the recipient is still disturbed for every
unconsented caller, therefore it is not scalable, and the
recipient is not spared from the disturbance of a spam call.
It also adds delay to each call, as legitimate callers are
forced to wait for consent before each call.
Call Back Verification first rejects an initial call from
a caller, then forces the caller to wait for the recipient
to call back the caller. Call Back Verification is a good
defense against caller ID spoofing, as it forces the caller
to provide a genuine caller ID. The basic mechanism is
simple, and some implementations try to automate this
process [83], [84]. However, it requires the caller to first
own a reachable inbound number, which could restrict
communication from legitimate VoIP users and telephone
extension terminals. Call Back Verification also add delays
to each communication, as the legitimate caller must wait
for the recipient to call back. Calling back could also add
calling cost on both the caller and recipient in PSTN, which
can be especially significant for premium or international
numbers.
Weakly Secret Information requires the caller to
demonstrate knowledge of a weakly secret information
before allowing communication with the recipient. Weakly
secret information could be in various forms such as a
passcode, an extension code, a limited-use phone number,
or a message identifier [85]. However, the recipient would
first need to share the weakly secret information to all
trusted callers, hence it may not be scalable for a recipient
with a large contact list. Legitimate calls from unknown
callers would also be restricted from communicating with
the recipient.

Payment at Risk is a micropayment, cost-based, technique
where the caller is required to deposit a small amount
of money before making a call. If the recipient reports
that the call is spam, then the deposit is confiscated or
kept by the recipient, otherwise the money is refunded
to the caller. This was proposed as a method for SIP
spam prevention [38]. This technique prevents spamming
by making it prohibitively expensive to send out a
large amount of spam calls, while costing very little
for legitimate callers. However, the solution requires a
universal micropayment system that collects payment on
every call, which may require significant resources to create
and administer. There also are many questions regarding
the legality of this approach, for instance on the lawful
confiscation of payments and abuse of spam reporting. The
value amount of the deposit would also affect the number of
recipients needed to report on the spam caller to effectively
make spamming unprofitable.
Proof of Work is a computational, cost-based, technique
where the caller’s terminal is required to produce a proofof-work, such as hashcash [86], that is moderately hard to
compute (being computational or memory-bound) but easy
for the recipient to verify, before allowing communication
with the recipient. As the amount of work increases,
it would be prohibitively inefficient to distribute large
amounts of spam calls. A legitimate caller would not be
significantly affected for making a few number of calls. On
one hand, Proof of Work has an advantage over Payment
at Risk by not requiring a micropayment system, therefore
avoiding the administrative and legality issues. On the other
hand, Proof of Work faces a trade-off problem between
permissiveness and anti-spam effectiveness. In PSTN, due
to the significant share of low-end telephone terminals, the
difficulty of the work would need to be low enough to
ensure permissiveness. However, this may allow a spammer
using moderately powerful computerized terminals to easily
generate as much work as needed for spamming. Legitimate
callers with high outbound calls, such as a bank, may
also be obstructed from doing legitimate business if it
is prohibitively costly to generate the proof-of-works to
contact a large number of customers.
Proof of Identity requires the caller to send a verifiable
identity token that would authenticate the credentials of
the caller whenever making a call. This technique has
been proposed for SIP domain users [83], [87]–[89], due
to the availability of SSL/TLS certificates and maturity of
the underlying public key infrastructure. This technique
prevents spamming by ensuring that the caller could be held
responsible for making illegal calls, and prevents scams by
ensuring that the caller cannot impersonate as someone else.
Proof of Identity could also prevent a spammer from using
multiple identities when identity verification is required.

Proof of Identity has an advantage over Proof of Work by
not having the issue of deciding the right difficulty level
of proof-of-work which could either obstruct calls from
low-end telephone terminals or give too much leeway for
spamming. However, the scheme could be hard to deploy
in PSTN, as it would require establishment of a certificate
authority for issuing and verifying caller identities, and may
require significant changes to the call request protocols in
PSTN.

V. A SSESSMENT C RITERIA
It is clear that there is no shortage of techniques to combat
telephone spam, but what would an ideal telephone spam
defense entail? Therefore, we propose a set of assessment
criteria.
We separate the assessment criteria into three categories:
(1) Usability, which evaluates the ease-of-use from either
the caller or recipient’s perspective, (2) Deployability, which
evaluates the ease of installation, deployment, and operation, and (3) Robustness, which evaluates the technique’s
resilience against errors and effectiveness against a spammer
actively evading the defense. We define each of the identified
criteria and give a mnemonic name.
A. Usability Criteria
No-Disturbance-to-Recipient When a known-spam call
arrives, the technique does not disturb the recipient, such
as prompting for additional action from the recipient.
Scalable-for-Recipient The technique does not increase the
burden of work on the recipient with an increasing number
of spam calls. The technique can handle a large variety of
spam calls with minimal input from the recipient.
Effortless-for-Caller When initiating a call, the technique
requires minimal or zero effort from the caller.
Negligible-Changes-to-Call-Setups The technique requires
negligible changes to the existing call setups or
configurations in the callers’ terminals.
Negligible-Delays When initiating a call, the technique
adds negligible or unperceivable delay to the caller, other
than the typical time to connect and time waiting for the
recipient to answer the phone.
Permissive-for-VoIP-Callers The technique would not
restrict any legitimate calls that use VoIP service. For
instance, some outbound-only VoIP users (such as Skype)
tend to have a generic (or unavailable) caller ID number
and cannot receive incoming PSTN calls.

Permissive-for-Unknown-Callers The technique would not
restrict calls from a legitimate caller not known by the
recipient.
B. Deployability Criteria
Negligible-Changes-to-Infrastructure The technique requires
zero or negligible changes to existing PSTN protocols,
terminals, or infrastructure.
No-Third-Party-Involvement The technique does not require
a third-party. A compromise of the third-party would not
result in mishandled calls or in a breach of privacy.
Low-Resource-Requirement The technique is lightweight
and does not require a significant amount of resources (e.g.,
people, equipment, engineering, or funding) to initiate and
deploy.
Low-Maintenance The technique requires low maintenance,
in terms of administrative cost, time, or resources, to
maintain good working order.
Negligible-Cost-per-Call The technique adds negligible
cost to each call, taxed on the legitimate caller, recipient,
third-party, or carriers. The cost could also be indirect, such
as reduced efficiency or capacity.
C. Robustness Criteria
Effective-Against-Dynamic-Caller-ID-Spoofing
The
technique is robust even when the spammer spoofs
different caller IDs nondeterministically.
Effective-Against-Targeted-Caller-ID-Spoofing
The
technique is robust even when the spammer spoofs a
specific caller ID known to be trusted by the recipient.
Effective-Against-Unavailable-Caller-ID The technique
is robust even when the spammer makes the caller ID
unavailable or sends a faulty caller ID to cause errors.
Effective-Against-Multiple-Identities The technique is robust
even when the spammer initiate calls from multiple sources,
such as using multiple subscriber accounts or a telephone
botnet, to disseminate spam calls. This is different from
caller ID spoofing where the caller IDs are not necessarily
spoofed but are instead initiated from different sources.
Effective-Against-Answering-Machine-Detection
The
technique is robust even when the spammer uses Answering
Machine Detection technology, which is a feature in
autodialers that can distinguish human pick-ups from
answering machines. With AMD, an autodialer can be

configured to call again later if the call was not answered
by a human, or to deliver the audio message into the
recipient’s voicemail.
Effective-Against-Dynamic-Audio-Content The technique is
robust even when the spammer uses an autodialer capable
of personalizing or altering the audio messages for different
recipients. This is usually featured in autodialers that are
able to synthesize text to speech.
We evaluate each technique using the criteria proposed in
Section V, and Table I visually summarizes this evaluation.
Each technique is evaluated as either satisfying the criteria
(denoted as ), may satisfy the criteria (denoted as #
G), or
not satisfying the criteria (denoted as #). “May satisfy the
criteria” means that the technique can be made to satisfy the
criteria depending on the implementation or configuration,
while some implementations do not fully satisfy the criteria.
Of course, this analysis requires some opinion, and in each
case we evaluated each technique and criteria to the best
of our abilities. While others may disagree with the exact
assessment of each technique, we believe that the criteria
outlined in Section V will help to guide future telephone
spam defenses and to provide a framework to evaluate these
defenses.
VI. C OMBINING T ECHNIQUES
From analyzing all the standalone techniques, it is clear
that there is no single technique that can satisfy all the
criteria. Therefore, an improved anti-spam system would
look to combine different techniques, to leverage the
positives and compensate the negatives. We outline the
different ways in which a solution could use a combination
of standalone techniques.
Phased Decisions combine several techniques into a linear
sequence (i.e., a pipeline process) of decision stages. If
an earlier technique determines the call is spam, then it
may not be necessary to run the evaluation techniques
at later stages. This is suitable for combining techniques
that uses information that are obtained chronologically,
such as first using Call Request Header Analysis, followed
by Voice Interactive Screening. We found use of Phased
Decisions approach in related works by Niccolini and
Quitek et al. [96], [97], Schlegel et al. [98], Gritzalis and
Mallios [99], [100], and Azad and Morla [39].
Weighted Scoring combines several techniques by running
each technique individually and then combining the outputs
to produce a final score by applying a weighted scoring
method. The classification of whether the call is spam
is based on the final score. As Weighted Scoring need
to collect outputs from all standalone techniques, it is
suitable for combining techniques that can be performed

G
#
G
#
# G
#
# G
#
# G
#
#
G
#
#
G
#
G
G
#
G
#

#
#
#
G
G
#
G
#

# #
#
#
G #
#
G #
#
G #
#
G #
G
# #
G #
G
# #
G #
#
G
#
G
#
G
#
G
# #
#
#
G
#
G
#

#
#
#
#
#
G
#
G
#
G
#
G

Effective-Against-Dynamic-Audio-Content

Effective-Against-Multiple-Identities

Effective-Against-Answering-Machine-Detection

Effective-Against-Unavailable-Caller-ID

Effective-Against-Targeted-Caller-ID-Spoofing

Effective-Against-Dynamic-Caller-ID-Spoofing

Robustness

Negligible-Cost-per-Call

No-Third-Party-Involvement

References
Caller ID Blacklisting
[24], [25]
#
G
Caller ID Whitelisting
[26], [27]
#
Caller Reputation System
[28]–[42], [90]
#
G
G
#
#
G
Caller Behavior Analysis [29], [32], [33], [35], [39], [41], [43]–[54], [91], [92]
#
G
#
G
Call Request Header Analysis
Device Fingerprinting
[55]
#
G
#
G
Caller ID Anomaly Detection
[56], [57]
#
G
ANI-CPN Matching
[58]
G
#
ANI-II Filtering
[58]
G
#
Audio Fingerprinting
[61]–[67]
#
G #
#
G
Speech Content Analysis
[62], [68]
#
G #
#
G
Voice Interactive Screening
Acoustic Pattern Analysis
[71]–[74]
#
G #
#
G
CAPTCHA/Turing Test
[75]–[77]
# #
Do Not Call Registry
[22] #
G #
G #
G
#
Graylisting
[74], [79]
#
G #
G
#
#
G
Consent-based Communication
[80]–[82] #
# #
G #
G
#
#
G
Call Back Verification
[83], [84] #
G #
G #
G # #
G #
G
Caller Compliance
Weakly Secret Information
[85]
#
G #
G #
G
# #
G #
G
Payment at Risk
[38]
# #
G
# # # #
Proof of Work
[86], [93]–[95]
# #
G #
Proof of Identity
[83], [87]–[89]
# #
G # #
G
= satisfy the criteria #
G= may satisfy the criteria #= does not satisfy the criteria

Low-Maintenance
Low-Resource-Requirement

Negligible-Changes-to-Call-Setups

Negligible-Changes-to-Infrastructure

Permissive-for-VoIP-Callers

Deployability

Permissive-for-Unknown-Callers

Negligible-Delays

Effortless-for-Caller

No-Disturbance-to-Recipient

Scalable-for-Recipient

Usability

#
G

#
G
#
G
#
G
#
#
#

# #
G

G
# G
#
G
# G
#
G
# G
#

#
G
# # #
G
#
#
G

Table I: Evaluation of various standalone techniques against the criteria described in Section V.

simultaneously, such as the various standalone Call
Request Header Analysis techniques. We found use of
Weighted Scoring approach in related works by Dantu and
Kolan [101], Niccolini and Quitek et al. [96], [97], Schlegel
et al. [98], Hansen et al. [102], and Mathieu et al. [103].
Conditional Procedures combine several techniques based
on a predefined set of rules (i.e., a policy or an algorithm).
This allows for higher flexibility of combining the techniques, such as using a different sequence of standalone
techniques based on the preference of each recipient or
the reputation of each caller. We found use of Conditional
Procedures approach in related works by d’Heureuse et
al. [104], Dritsas et al. [105], Scata and La Corte [106],
and Soupionis and Gritzalis [47].
We evaluate existing solutions using a combined approach, and summarized which standalone techniques those
solutions incorporated in Table II. All of these works are
mainly focused on defense against SPIT, and some of these
may include SPIT-specific techniques that does not appear

in our table. Again, this analysis requires some opinion, and
we evaluated each solution to the best of our abilities. We
believe that the various strategies of combining techniques
outlined in Section VI will help to improve future telephone
spam defenses.
VII. R ELATED W ORK
While in this paper we have compared and analyzed the
state-of-the-art research in telephone spam defense, we will
now discuss related survey papers. Most of the papers focus
on spam in the Voice over IP (VoIP) domain, so-called SPam
over Internet Telephony (SPIT), rather than the larger PSTN
telephony network.
Keromytis [107], [108] presented two comprehensive surveys of VoIP security, which summarized previous works
related to VoIP security and organized them according to an
extended version of the VoIP Security Alliance (VoIPSA)
Threat Taxonomy. The papers reviewed many previous
works addressing every type VoIP threat in the VoIPSA
taxonomy, with the social threats of spamming as one of
the categories.

Phased Decisions
Weighted Scoring
Conditional Procedures
Caller ID Blacklisting
Caller ID Whitelisting
Caller Reputation System
Caller Behavior Analysis
Device Fingerprinting
Caller ID Anomaly Detection
ANI-CPN Matching
ANI-II Filtering
Audio Fingerprinting
Speech Content Analysis
Acoustic Pattern Analysis
CAPTCHA/Turing Test
Do Not Call Registry
Graylisting
Consent-based Communication
Call Back Verification
Weakly Secret Information
Payment at Risk
Proof of Work
Proof of Identity

[96], [97]



[98]



[99]


[100]


[101]

[102]

[103]

[104]




















































[105]





































[106]








Table II: Summary of various anti-spam solutions using a combination of standalone techniques.

Baumann et al. [109] presented a survey of potential
solutions to SPIT. The paper provided an overview and
classification of SPIT prevention methods based on detection using Signaling versus Voice and order-based Before
Call versus After/While Call. The paper also proposed a
Biometric Framework for SPIT Prevention as a way to bind
identities to each caller.
Phithakkitnukoon et al. [110] presented a survey focused
on five primary types of VoIP attacks, SPIT being one of
them. The authorized provided an introduction to the basic
knowledge of VoIP systems and its available security tools,
and summarized a list of proposed solutions for SPIT from
previous literature.
Quinten et al. [111] presented a survey evaluating the
techniques to prevent and reduce SPIT. The authors evaluated the effectiveness of techniques by dividing them into
four categories: unsuitable techniques, techniques with potential, suitable techniques, and combinations of techniques.
Dantu et al. [112] presented a survey discussing the
attacks and solutions in VoIP, with VoIP Spam and Phishing
being one of the attacks. The authors reviewed previous
work addressing all types of VoIP attacks and proposed a
high-level security architecture to make the VoIP infrastructure more secure and robust.
Dritsas et al. [113] presented a survey reviewing a list
of SPIT identification criteria that can be used by anti-SPIT
mechanisms and identified the different detection stages. The
authors propose two generic categories of SPIT identification

criteria: SIP Message criteria and SIP User Agent criteria.
They also proposed a two-fold evaluation framework for
discovering possible SPIT messages.
Marias et al. [114] presented a survey assessing the
threats and vulnerabilities that the SIP protocol introduces.
The authors also reviewed existing anti-SPIT mechanisms
and classified them into three classes: Prevent, Detect, and
Handle. The paper also proposes a list of qualitative and
quantitative criteria to assess the effectiveness of the antiSPIT countermeasures.
Khan et al. [115] presented a survey reviewing various
existing methods for preventing spam in IP telephony. The
paper also presented a discussion on the implementation
costs of different types of techniques, and commented that
no single technique is sufficient and therefore a framework
of multiple techniques is recommended.
Rosenberg et al. [116] presented an open memo reviewing
various solutions that might be possible to deal with SIP
spam. The author also presented some borrowed techniques
that have been employed to deal with email spam. In
conclusion, the author recommends using identity related
techniques, while also commented that identity techniques
may be vulnerable when a SIP request without an authenticated identity cannot know whether the request lacked such
an identity because the originating domain didn’t support it,
or because a man-in-the-middle removed it.
In general, most existing survey papers focus on techniques against SPIT or more specifically spam in the SIP

protocol. This paper is focused on techniques to address
spamming in the PSTN telephony network. Some techniques
for SPIT are not applicable to PSTN due to protocol differences. As far as we are aware, this is the first survey
paper specifically addressing spam calls directed to the
PSTN telephony network. In terms of evaluation differences,
we are the first to propose a taxonomy to classify the
existing standalone techniques into three categories, the first
to evaluate the standalone techniques based on three sets of
assessment criteria, and the first to outline the three strategies
of combining standalone techniques.
VIII. C ONCLUSION
From analyzing and evaluating the existing solutions that
attempt to address telephone spam, we reach the conclusion
that there is no universally acceptable solution to telephone
spam. Every approach thus far has different tradeoffs, specifically between usability, deployability, and robustness.
From our analysis of the telephone spam ecosystem and
defensive techniques, we believe that usability is the most
important criteria for evaluating a defense. Unlike email,
which can be delayed or possibly lost due to a false positive,
telephony solutions have a high bar for user acceptance,
and we believe that users will not adopt techniques that
impose excessive burden on both the caller and recipient.
Therefore, future research into this area must consider the
usability of the defense from both the caller and the recipient
perspective.
We believe that one promising avenue of research is
using a combination of techniques, which should improve
on the robustness of standalone techniques, and potentially
each technique could address the weaknesses of the others.
However, as the telephony system has real-time immediacy
constraints, care must be taken so that the combination of
techniques will not degrade the user experience due to higher
complexity. Our intuition leads us to recommend combining
no more than two standalone techniques, as we observed that
a good balance of usability, deployability, and robustness
could be achieved by using two standalone techniques.
One glaring issue that continually reoccurs when analyzing the telephone spam ecosystem is caller ID spoofing. We
believe that the key to combating telephone spam is to make
the caller ID trusted and verifiable, while making minimal
changes to existing infrastructure. For instance, from our
evaluation of Call Request Header Analysis techniques,
they provide the best overall usability and deployability,
however they suffer from robustness due to the spammer’s
ability to spoof the caller ID. If caller ID spoofing can
be effectively prevented, then we believe that Call Request
Header Analysis would satisfy all of our evaluation criteria.
Telephone spam is poised to increase significantly,
defrauding consumers of billions of dollars. Therefore, an
effective telephone spam defense is critical. However, the
techniques and approaches that were effective in combating

email spam are inappropriate when applied to telephone
spam. We attribute this to differences not only in the
technology used, but more fundamentally to the type of
communication. This is why a survey of the telephone
spam area is necessary: to highlight these differences and
to define the ideal criteria for telephone spam defenses. We
hope that this paper provides a framework to help guide
and shape future telephone spam defenses.
ACKNOWLEDGMENT
This work was partially supported by the grants from
Cisco Inc. and Center for Cybersecurity and Digital Forensics at ASU.
R EFERENCES
[1] CTIA,
“Annual
Wireless
Industry
Survey,”
http://www.ctia.org/your-wireless-life/how-wirelessworks/annual-wireless-industry-survey.
[2] U.S. Bureau of Labor Statistics, “American time use survey
fact sheet,” http://www.bls.gov/tus/atussummary.pdf, June
2015.
[3] Federal Trade Commission, “National do not call registry
data book fy 2014,” https://www.ftc.gov/system/files/
documents/reports/national-do-not-call-registry-data-bookfiscal-year-2014/dncdatabookfy2014.pdf, Federal Trade
Commission, Tech. Rep., 2015.
[4] Federal Trade Commission, “Consumer sentinel data book
for january - december cy 2014,” https://www.ftc.gov/
system/files/documents/reports/consumer-sentinel-networkdata-book-january-december-2014/sentinel-cy2014-1.pdf,
Federal Trade Commission, Tech. Rep., 2015.
[5] S. H. Kimball, T. Levy, H. Venturelli, and S. Miller, “Interactive Voice Recognition Communication in Electoral Politics: Exploratory Metadata Analysis,” American Behavioral
Scientist, 2014.
[6] A. R. Modarressi and R. Skoog, “Signaling System No. 7:
A Tutorial,” IEEE Communications Magazine, 1990.
[7] International Telecommunication Union, “Fixed-telephone
subscriptions,”
http://www.itu.int/en/ITU-D/Statistics/
Documents/statistics/2014/Fixed_tel_2000-2013.xls.
[8] Statista, “Countries by number of Voice over
Internet Protocol (VoIP) subscribers in 1Q 2013,”
http://www.statista.com/statistics/236824/number-of-voipsubscribers-by-leading-countries/.
[9] B. Woodcock and V. Adhikari, “Survey of Characteristics of
Internet Carrier Interconnection Agreements,” Packet Clearing House, Tech. Rep., 2011.
[10] Federal Communications Commission, “Intercarrier
compensation,”
https://www.fcc.gov/encyclopedia/
intercarrier-compensation, 2015.

[11] P. Casanova, R. Bandyopadhyay, and V. Balasubramaniyan,
“Largest IRS Phone Scam Likely Exceeded 450,000 Potential Victims in March,” http://www.pindropsecurity.com/irsphone-scam-live-call_analysis/, 2015.
[12] Marketwired, “From Stalkers to Spam, WhitePages
Study Breaks Down Reasons Americans Block
Calls,”
http://www.marketwired.com/press-release/fromstalkers-to-spam-whitepages-study-breaks-down-reasonsamericans-block-calls-1900134.htm.
[13] C. A. Hamilton, “Machine answer detection,” Dec. 6 1994,
uS Patent 5,371,787.
[14] T. Mobarak and A. Han, “Method and apparatus for forcing
a call to a carrier provided voice mail facility,” 2013, uS
Patent 8,605,869.
[15] Federal Communications Commission, “Calling Number
Identification Service–Caller ID,” 2015.
[16] C. Business, “PRI Trunk Plans,” http://business.comcast.
com/phone/pri-trunks/plans-pricing.
[17] E. Park, “Rustock Takedown’s Effect on Global Spam
Volume,” http://www.symantec.com/connect/blogs/rustocktakedown-s-effect-global-spam-volume, 2011.
[18] M. Carney, “Courts deem CallFire a common carrier,
setting a major precedent at intersection of telecom
and tech law,” http://pando.com/2015/02/27/courts-deemcallfire-a-common-carrier-setting-a-major-precedent-atintersection-of-telecom-and-tech-law/, Feb. 27, 2015.
[19] K. Cox, “FTC: Totally Fine By Us If Phone
Companies
Block
Robocalling
Numbers,”
http:
//consumerist.com/2015/01/27/ftc-totally-fine-by-us-ifphone-companies-block-robocalling-numbers/, Jan. 27,
2015.
[20] Federal Communications Commission, “Telephone consumer protection act 47 u.s.c. Âğ 227,” https://transition.fcc.
gov/cgb/policy/TCPA-Rules.pdf.
[21] Public Law 111âĂŞ331 111th Congress, “Truth in caller id
act of 2009,” https://www.congress.gov/111/plaws/publ331/
PLAW-111publ331.pdf.
[22] Federal Trade Commission, “National Do Not Call Registry,” https://www.donotcall.gov/.
[23] Federal Trade Commission, “Robocalls | consumer information,” https://www.consumer.ftc.gov/features/feature-0025robocalls, 2015.
[24] E. Montejo, “How to block phone calls on your
Android phone,” http://www.androidauthority.com/how-toblock-phone-calls-numbers-android-phone-246484/.
[25] Apple Inc., “Block calls and block or filter messages on your
iPhone, iPad, or iPod touch,” https://support.apple.com/enus/HT201229.
[26] T. Nimmerjahn, “Whitelist Call Blocker,” https:
//play.google.com/store/apps/details?id=de.tn_software.
callblocker.

[27] NQ Mobile Security, “NQ Mobile Call Blocker,” http://en.
nq.com/callblocker.
[28] P. Kolan and R. Dantu, “Socio-technical defense against
voice spamming,” ACM Transactions on Autonomous and
Adaptive Systems (TAAS), vol. 2, no. 1, p. 2, 2007.
[29] F. Wang, Y. Mo, and B. Huang, “P2p-avs: P2p based cooperative voip spam filtering,” in Wireless Communications and
Networking Conference, 2007. WCNC 2007. IEEE. IEEE,
2007, pp. 3547–3552.
[30] P. Patankar, G. Nam, G. Kesidis, and C. R. Das, “Exploring
anti-spam models in large scale voip systems,” in Distributed
Computing Systems, 2008. ICDCS’08. The 28th International Conference on. IEEE, 2008, pp. 85–92.
[31] R. Zhang and A. Gurtov, “Collaborative reputation-based
voice spam filtering,” in Database and Expert Systems
Application, 2009. DEXA’09. 20th International Workshop
on. IEEE, 2009, pp. 33–37.
[32] C. Sorge and J. Seedorf, “A provider-level reputation system
for assessing the quality of spit mitigation algorithms,” in
Communications, 2009. ICC’09. IEEE International Conference on. IEEE, 2009, pp. 1–6.
[33] V. B. Payas Gupta, Bharat Srinivasan and M. Ahamad,
“Phoneypot: Data-driven Understanding of Telephony
Threats,” in Proceedings of the Symposium on Network and
Distributed System Security (NDSS).
[34] P. Kolan, R. Dantu, and J. W. Cangussu, “Nuisance level of
a voice call,” ACM Transactions on Multimedia Computing,
Communications, and Applications (TOMM), vol. 5, no. 1,
p. 6, 2008.
[35] T. S. Corporation, “Nomorobo,” https://www.nomorobo.
com.
[36] K. Srivastava and H. G. Schulzrinne, “Preventing spam for
sip-based instant messages and sessions,” 2004.
[37] Y. Rebahi and D. Sisalem, “Sip service providers and the
spam problem,” in Proceedings of the 2nd VoIP Security
Workshop, 2005.
[38] Y. Rebahi, D. Sisalem, and T. Magedanz, “Sip spam detection,” in Digital Telecommunications„ 2006. ICDT’06.
International Conference on. IEEE, 2006, pp. 68–68.
[39] M. A. Azad and R. Morla, “Multistage spit detection in transit voip,” in Software, Telecommunications and Computer
Networks (SoftCOM), 2011 19th International Conference
on. IEEE, 2011, pp. 1–9.
[40] M. A. Azad, R. Morla, “Caller-rep: Detecting unwanted calls
with caller social strength,” Computers & Security, vol. 39,
pp. 219–236, 2013.
[41] Y.-S. Wu, S. Bagchi, N. Singh, and R. Wita, “Spam detection
in voice-over-ip calls through semi-supervised clustering,” in
Dependable Systems & Networks, 2009. DSN’09. IEEE/IFIP
International Conference on. IEEE, 2009, pp. 307–316.

[42] F. Wang, F. R. Wang, B. Huang, and L. T. Yang, “Advs:
a reputation-based model on filtering spit over p2p-voip
networks,” The Journal of Supercomputing, vol. 64, no. 3,
pp. 744–761, 2013.

[55] H. Yan, K. Sripanidkulchai, H. Zhang, Z.-Y. Shae, and
D. Saha, “Incorporating active fingerprinting into spit
prevention systems,” in Third annual security workshop
(VSWâĂŹ06). Citeseer, 2006.

[43] D. Shin, J. Ahn, and C. Shim, “Progressive Multi GrayLeveling: A Voice Spam Protection Algorithm,” IEEE Network, 2006.

[56] EveryCaller, “Call Control,” http://www.everycaller.com.

[44] H.-J. Kim, M. J. Kim, Y. Kim, and H. C. Jeong, “Devs-based
modeling of voip spam callersâĂŹ behavior for spit level
calculation,” Simulation Modelling Practice and Theory,
vol. 17, no. 4, pp. 569–584, 2009.
[45] M.-Y. Su and C.-H. Tsai, “A prevention system for spam
over internet telephony,” Appl. Math, vol. 6, no. 2S, pp.
579S–585S, 2012.
[46] M. Amanian, M. Moghaddam, and H. Roshkhari, “New
method for evaluating anti-SPIT in VoIP networks,” in
Computer and Knowledge Engineering (ICCKE), 2013 3th
International eConference on. IEEE, 2013, pp. 374–379.
[47] Y. Soupionis and D. Gritzalis, “Aspf: Adaptive anti-spit
policy-based framework,” in Availability, Reliability and
Security (ARES), 2011 Sixth International Conference on.
IEEE, 2011, pp. 153–160, http://dx.doi.org/10.1109/ARES.
2011.29.
[48] N. Chaisamran, T. Okuda, and S. Yamaguchi, “Trust-based
voip spam detection based on calling behaviors and human
relationships,” Information and Media Technologies, vol. 8,
no. 2, pp. 528–537, 2013.
[49] R. J. B. Chikha, T. Abbes, W. B. Chikha, and A. Bouhoula,
“Behavior-based approach to detect spam over IP telephony attacks,” International Journal of Information Security, 2015.

[57] Budaloop, “Regex Call Blocker,” https://play.google.com/
store/apps/details?id=com.budaloop.regexblocker.
[58] Pindrop Security, “Fraud Detection System,” http://www.
pindropsecurity.com/fraud-detection-system.
[59] S. J. Brolin and S. Colodner, “Automatic number identification in subscriber loop carrier systems,” Nov. 1 1977, uS
Patent 4,056,690.
[60] I. Neustar, “Nanpa : Ani ii digits - view assignments,” https://www.nationalnanpa.com/number_resource_
info/ani_ii_assignments.html, 2015.
[61] S. Horvath and T. Kasvand, “Voice identification prescreening and redirection system,” Sep. 6 2002, uS Patent
App. 10/236,810.
[62] D. Reich and R. Szabo, “Method and system of determining unsolicited callers,” Apr. 28 2004, uS Patent App.
10/833,515.
[63] C. Pörschmann and H. Knospe, “Analysis of Spectral Parameters of Audio Signals for the Identification of Spam
Over IP Telephony.” in CEAS, 2008.
[64] C. Pörschmann and H. Knospe, “Spectral Analysis of Audio
Signals for the Identification of Spam Over IP Telephony,”
in Proceedings of the NAG/DAGA International Conference
on Acoustics, 2009.

[50] H. Sengar, X. Wang, and A. Nichols, “Call Behavioral analysis to Thwart SPIT attacks on VoIP networks,” in Security
and Privacy in Communication Networks. Springer, 2012,
pp. 501–510.

[65] D. Lentzen, G. Grutzek, H. Knospe, and C. Porschmann,
“Content-based Detection and Prevention of Spam over
IP Telephony-System Design, Prototype and First Results,”
in Proceedings of the IEEE International Conference on
Communications (ICC), 2011.

[51] H. J. Kang, Z.-L. Zhang, S. Ranjan, and A. Nucci, “Sipbased voip traffic behavior profiling and its applications,” in
Proceedings of the 3rd annual ACM workshop on Mining
network data. ACM, 2007, pp. 39–44.

[66] J. Strobl, B. Mainka, G. Grutzek, and H. Knospe, “An Efficient Search Method for the Content-Based Identification of
Telephone-SPAM,” in Proceedings of the IEEE International
Conference on Communications (ICC), 2012.

[52] Y. Bai, X. Su, and B. Bhargava, “Adaptive Voice Spam
Control with User Behavior Analysis,” in Proceedings of
the IEEE International Conference on High Performance
Computing and Communications (HPCC), 2009.

[67] S. A. Iranmanesh, H. Sengar, and H. Wang, “A Voice Spam
Filter to Clean Subscribers’ Mailbox,” Security and Privacy
in Communication Networks, 2013.

[53] R. MacIntosh and D. Vinokurov, “Detection and mitigation
of spam in IP telephony networks using signaling protocol
analysis,” in Advances in Wired and Wireless Communication, 2005 IEEE/Sarnoff Symposium on. IEEE, 2005, pp.
49–52.
[54] S. Phithakkitnukoon, R. Dantu, R. Claxton, and N. Eagle,
“Behavior-based adaptive call predictor,” ACM Transactions
on Autonomous and Adaptive Systems (TAAS), vol. 6, no. 3,
p. 21, 2011.

[68] F. Maggi, “Are the Con Artists Back? A Preliminary Analysis of Modern Phone Frauds,” in Proceedings of the IEEE
International Conference on Computer and Information
Technology (CIT), 2010.
[69] L. R. Rabiner, “Applications of speech recognition in the
area of telecommunications,” in Automatic Speech Recognition and Understanding, 1997. Proceedings., 1997 IEEE
Workshop on. IEEE, 1997, pp. 501–510.
[70] David R. Wheeler, “Voice recognition will always be
stupid,” http://www.cnn.com/2013/08/20/opinion/wheelervoice-recognition/.

[71] V. A. Balasubramaniyan, A. Poonawalla, M. Ahamad,
M. T. Hunter, and P. Traynor, “Pindr0p: Using singleended audio features to determine call provenance,” in
Proceedings of the 17th ACM Conference on Computer
and Communications Security, ser. CCS ’10. New York,
NY, USA: ACM, 2010, pp. 109–120. [Online]. Available:
http://doi.acm.org/10.1145/1866307.1866320

[85] K. Ono and H. Schulzrinne, “Have i met you before?: using
cross-media relations to reduce spit,” in Proceedings of the
3rd International Conference on Principles, Systems and
Applications of IP Telecommunications. ACM, 2009, p. 3.

[72] H. Hai, Y. Hong-Tao, and F. Xiao-Lei, “A SPIT Detection
Method Using Voice Activity Analysis,” in Proceedings of
the International Conference on Multimedia Information
Networking and Security (MINES), 2009.

[87] H. Tschofenig, R. Falk, J. Peterson, J. Hodges, D. Sicker,
J. Polk, and A. Siemens, “Using saml to protect the session
initiation protocol (sip),” IEEE Network, vol. 20, no. 5, pp.
14–17, 2006.

[73] J. Quittek, S. Niccolini, S. Tartarelli, M. Stiemerling,
M. Brunner, and T. Ewald, “Detecting SPIT calls by checking human communication patterns,” in Proceedings of the
IEEE International Conference on Communications (ICC),
2007.

[88] S. Saklikar and S. Saha, “Identity federation for voip-based
services,” in Proceedings of the 2007 ACM workshop on
Digital identity management. ACM, 2007, pp. 62–71.

[74] J. Quittek, S. Niccolini, S. Tartarelli, and R. Schlegel,
“Prevention of Spam over IP Telephony (SPIT),” NEC, Tech.
Rep., 2006.
[75] J. Lindqvist and M. Komu, “Cure for spam over internet telephony,” in 4TH IEEE CONSUMER COMMUNICATIONS
AND NETWORKING CONFERENCE (CCNC 2007). ProceedingsâĂ˛e vol., n, 2007, pp. 896–900.
[76] A. Markkola and J. Lindqvist, “Accessible Voice
CAPTCHAs for Internet Telephony,” in Proceedings
of the Symposium on Accessible Privacy and Security
(SOAPS), 2008.
[77] Y. Soupionis, G. Tountas, and D. Gritzalis, “Audio
CAPTCHA for SIP-based VoIP,” in Proceedings of International Information Security Conference, 2009.
[78] E. Bursztein and S. Bethard, “Decaptcha: Breaking 75% of
eBay Audio CAPTCHAs,” in Proceedings of the USENIX
Workshop on Offensive Technologies (WOOT), 2009.

[86] A. Back, “Hashcash - A Denial of Service CounterMeasure,” http://www.hashcash.org/hashcash.pdf, 2002.

[89] L. Kong, V. A. Balasubramaniyan, and M. Ahamad, “A
lightweight scheme for securely and reliably locating sip
users,” in VoIP Management and Security, 2006. 1st IEEE
Workshop on. IEEE, 2006, pp. 9–17.
[90] V. Balasubramaniyan, M. Ahamad, and H. Park, “Callrank:
Combating spit using call duration, social networks and
global reputation.” in CEAS, 2007.
[91] R. Dantu and P. Kolan, “Preventing voice spamming,” in
Proceedings of the IEEE Global Telecommunications Conference (GLOBECOM), Workshop on VoIP Security Challenges and Solutions, 2004.
[92] B. Mathieu, Y. Gourhant, and Q. Loudier, “Spit mitigation
by a network level anti-spit entity,” in Proc. of the 3rd
Annual VoIP Security Workshop, 2006.
[93] C. Dwork and M. Naor, “Pricing via Processing or Combatting Junk Mail,” in Advances in Cryptology (CRYPTO),
1992.

[79] E. Harris, “The Next Step in the Spam Control
War: Greylisting,” http://projects.puremagic.com/greylisting/
whitepaper.html, 2003.

[94] N. Banerjee, S. Saklikar, and S. Saha, “Anti-vamming trust
enforcement in peer-to-peer voip networks,” in Proceedings
of the 2006 international conference on Wireless communications and mobile computing. ACM, 2006, pp. 201–206.

[80] Google Voice, “Screen calls,” https://support.google.com/
voice/answer/115083.

[95] C. Jennings, “Computational puzzles for spam reduction in
sip,” 2007.

[81] Verizon, “Call Intercept,” https://www.verizon.com/
support/residential/phone/homephone/calling+features/call+
intercept/130058.htm.

[96] S. Niccolini, “Spit prevention: state of the art and research
challenges,” in Proceedings of the 3rd Workshop on Securing
Voice over IP, 2006.

[82] Phone.com, “Phone.com university âĂŞ screening
calls
for
your
business
line
|
phone.com,”
https://www.phone.com/blog/tips-tricks/2014/02/24/phonecom-university-screening-calls-business-line/,
February
2014.

[97] J. Quittek, S. Niccolini, S. Tartarelli, and R. Schlegel, “On
Spam over Internet Telephony (SPIT) Prevention,” IEEE
Communications Magazine, 2008.

[83] N. Croft and M. Olivier, “A Model for Spam Prevention
in IP Telephony Networks using Anonymous Verifying Authorities,” in Proceedings of the Annual Information Security
South Africa Conference, 2005.
[84] H. Mustafa, W. Xu, A. R. Sadeghi, and S. Schulz, “You
Can Call but You Can’t Hide: Detecting Caller ID Spoofing
Attacks,” in Proceedings of the Conference on Dependable
Systems and Networks (DSN), 2014.

[98] R. Schlegel, S. Niccolini, S. Tartarelli, and M. Brunner,
“Ise03-2: Spam over internet telephony (spit) prevention
framework,” in Global Telecommunications Conference,
2006. GLOBECOM’06. IEEE. IEEE, 2006, pp. 1–6.
[99] D. Gritzalis and Y. Mallios, “A sip-oriented spit management
framework,” Computers & Security, vol. 27, no. 5, pp. 136–
153, 2008.

[100] D. Gritzalis, G. Marias, Y. Rebahi, Y. Soupionis, and
S. Ehlert, “Spider: A platform for managing sip-based spam
over internet telephony spit,” Journal of Computer Security,
vol. 19, no. 5, pp. 835–867, 2011.
[101] R. Dantu and P. Kolan, “Detecting spam in voip networks,”
in Proceedings of the steps to reducing unwanted traffic on
the internet on steps to reducing unwanted traffic on the
internet workshop. USENIX Association, 2005, pp. 5–5.
[102] M. Hansen, M. Hansen, J. Möller, T. Rohwer, C. Tolkmit,
and H. Waack, “Developing a legally compliant reachability management system as a countermeasure against spit,”
in Proceedings of Third Annual VoIP Security Workshop,
Berlin, Germany, 2006.
[103] B. Mathieu, S. Niccolini, and D. Sisalem, “SDRS: a voiceover-IP spam detection and reaction system,” Security &
Privacy, IEEE, vol. 6, no. 6, pp. 52–59, 2008.
[104] N. d’Heureuse, J. Seedorf, and S. Niccolini, “A policy
framework for personalized and role-based spit prevention,”
in Proceedings of the 3rd International Conference on Principles, Systems and Applications of IP Telecommunications.
ACM, 2009, p. 12.
[105] S. Dritsas, V. Dritsou, B. Tsoumas, P. Constantopoulos, and
D. Gritzalis, “Ontospit: Spit management through ontologies,” Computer Communications, vol. 32, no. 1, pp. 203–
212, 2009.
[106] M. Scatá and A. L. Corte, “Security analysis and countermeasures assessment against spit attacks on voip systems,”
in Internet Security (WorldCIS), 2011 World Congress on.
IEEE, 2011, pp. 177–183.
[107] A. D. Keromytis, “A survey of voice over ip security
research,” in Information Systems Security. Springer, 2009,
pp. 1–17.
[108] Keromytis, Angelos D, “A comprehensive survey of voice
over ip security research,” Communications Surveys & Tutorials, IEEE, vol. 14, no. 2, pp. 514–537, 2012.

[109] R. Baumann, S. Cavin, and S. Schmid, “Voice over ipsecurity and spit,” Swiss Army, FU Br, vol. 41, pp. 1–34,
2006.
[110] S. Phithakkitnukoon, R. Dantu, and E.-A. Baatarjav, “Voip
securityâĂŤattacks and solutions,” Information Security
Journal: A Global Perspective, vol. 17, no. 3, pp. 114–123,
2008.
[111] V. M. Quinten, R. Van De Meent, and A. Pras, “Analysis
of techniques for protection against spam over internet
telephony,” in Dependable and Adaptable Networks and
Services. Springer, 2007, pp. 70–77.
[112] R. Dantu, S. Fahmy, H. Schulzrinne, and J. Cangussu, “Issues and challenges in securing voip,” computers & security,
vol. 28, no. 8, pp. 743–753, 2009.
[113] S. Dritsas, Y. Soupionis, M. Theoharidou, Y. Mallios, and
D. Gritzalis, “Spit identification criteria implementation: Effectiveness and lessons learned,” in Proceedings of The Ifip
Tc 11 23rd International Information Security Conference.
Springer, 2008, pp. 381–395.
[114] G. F. Marias, S. Dritsas, M. Theoharidou, J. Mallios, and
D. Gritzalis, “SIP vulnerabilities and anti-SPIT mechanisms
assessment,” in Computer Communications and Networks,
2007. ICCCN 2007. Proceedings of 16th International Conference on. IEEE, 2007, pp. 597–604, http://dx.doi.org/10.
1109/ICCCN.2007.4317883.
[115] S. F. Khan, M. Portmann, and N. W. Bergmann, “A Review
of Methods for Preventing Spam in IP Telephony,” Modern
Applied Science, vol. 7, no. 7, p. p48, 2013.
[116] J. Rosenberg, C. Jennings, and J. Peterson, “The session
initiation protocol (SIP) and spam,” RFC 5039, January,
Tech. Rep., 2008.
[117] M. W. Slawson and C. I. O. C. Waiting, “Caller ID Basics,”
Intertek Testing Services/Testmark Laboratories, Lexington,
KY, 2001.

A PPENDIX
Description
Message Type (MDMF)
Message Length
Parameter Code (Date & Time)
Parameter Length
Month (November)
Day (28)
Hour (3pm)
Minutes (43)
Parameter Code (CPN)
Parameter Length (10)
From (6062241359)

Parameter Code (Name)
Parameter Length (9)
Name (Joe Smith)

Checksum

Decimal
128
33
1
8
49
49
50
56
49
53
52
51
2
10
54
48
54
50
50
52
49
51
53
57
7
9
74
111
101
32
83
109
105
116
104
88

ASCII

1
1
2
8
1
5
4
3

6
0
6
2
2
4
1
3
5
9

J
o
e
S
m
i
t
h

Hex
80
21
01
08
31
31
32
38
31
35
34
33
02
0A
36
30
36
32
32
34
31
33
35
39
07
09
4A
6F
65
20
53
6D
69
74
68
58

Table III: MDMF message sample in the existing POTS
protocol [117].

