arXiv:1408.4351v1 [physics.comp-ph] 19 Aug 2014

A Parallel Multi-Domain Solution Methodology Applied to Nonlinear Thermal Transport Problems in Nuclear Fuel Pins$
Bobby Philipa,, Mark A. Berrilla , Srikanth Allua , Steven P. Hamiltona , Rahul S. Sampatha , Kevin T. Clarnoa , Gary A. Diltsb
Oak Ridge National Laboratory One Bethel Valley Road, Oak Ridge, TN 37831 b Los Alamos National Laboratory P.O. Box 1663, Los Alamos NM 87545
a

Abstract This paper describes an efficient and nonlinearly consistent parallel solution methodology for solving coupled nonlinear thermal transport problems that occur in nuclear reactor applications over hundreds of individual 3D physical subdomains. Efficiency is obtained by leveraging knowledge of the physical domains, the physics on individual domains, and the couplings between them for preconditioning within a Jacobian Free Newton Krylov method. Details of the computational infrastructure that enabled this work, namely the open source Advanced Multi-Physics (AMP) package developed by the authors is described. Details of verification and validation experiments, and parallel performance analysis in weak and strong scaling studies demonstrating the achieved efficiency of the algorithm are presented. Furthermore, numerical experiments demonstrate that the preconditioner developed is independent of the number of fuel subdomains in a fuel rod, which is particularly important when simulating different types of fuel rods. Fi$ Notice: This manuscript has been authored by UT-Battelle, LLC, under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes.  Corresponding author: Bobby Philip (Email: philipb@ornl.gov)

Preprint submitted to Journal of Computational Physics

August 20, 2014

nally, we demonstrate the power of the coupling methodology by considering problems with couplings between surface and volume physics and coupling of nonlinear thermal transport in fuel rods to an external radiation transport code. Keywords: Inexact Newton, Jacobian Free Newton Krylov, Krylov Subspace Method, Domain Decomposition, Preconditioning, Iterative Method, Parallel Algorithm 2010 MSC: 49M15, 65F08, 65F10, 65N55, 65Y05, 68W10 1. Introduction Many real world engineering problems involve multiple coupled nonlinear physical processes that occur both within and across several interacting physical domains. Robust, accurate, and efficient three dimensional simulations for some of these complex problems pose significant challenges that require a combination of powerful numerical algorithms, efficient parallel implementations, and massive computing resources to tackle. These challenges include developing the numerical methods and the parallel software infrastructure for coupling physical phenomena that occur on the surface and within the interior of physical domains, coupling structured and unstructured mesh calculations, coupling models with different discretizations, and using tightly coupled solution methods to solve certain coupled physics problems and loosely coupled approaches for others. Developing such simulation capabilities is a nontrivial task. In this article, we will focus primarily on one such complex application where all of the features outlined above are present: thermal transport in nuclear fuel rods. However, we will also devote some effort to describing the parallel code infrastructure that was developed to provide the necessary meshing, discretization, linear algebra, linear and nonlinear solvers, physics modules (conservation laws and constitutive models), material property databases, and parallelization mechanisms for simulating this application in hopes that it will be beneficial to the broader scientific community. A nuclear fuel assembly consists of several hundred nuclear fuel rods (shown in Figure 1) bound together by spacer grids. While some of the rod locations are reserved for instrumentation and safety, most of the rods contain nuclear fuel. Each individual nuclear fuel rod in turn consists of several hundred nearly cylindrical nuclear fuel pellets (each with a height 2

Figure 1: Fuel assembly containing nuclear fuel rods that are filled with fuel pellets [1] to diameter ratio of approximately one) stacked one on top of another to form a long column enclosed within a metal tube called the clad. Heat is generated within the pellets by nuclear fission and is distributed within the pellets and clad via a diffusive process. There is thermal contact (modeled as a convective process) between neighboring pellets and between the pellets and the clad. Each fuel rod is cooled with water flowing axially up the outer surface of the clad. Modeling the heat transfer, along with other physics, leads to a very highaspect ratio problem with many inter-dependent domains. Traditional nuclear fuel simulation eliminates the computational challenge by approximating the heat transfer as entirely radial and neglecting the axial and azimuthal components, which are only coupled through the coolant temperature and other simplified physics [2, 3]. Recent efforts to develop advanced modeling and simulation tools for nuclear fuel rods [4, 5], which include simulating full 3

three-dimensional fuel rods with resolved pellets, have relied upon standard solution and preconditioning strategies that do not necessarily take advantage of the physics and geometry of the problem. However, this manuscript does not address the challenges of structural dynamics and the associated feedback on heat transfer. With respect to specific work related to heat transfer within nuclear fuel rods, there are several existing efforts to develop parallel codes that model three-dimensional heat transfer within nuclear fuel rods, including PLEIADES/ALCYONE [4, 6], MOOSE/Bison [4], and BACO [7]. These codes are all focused on the integration of the many physics required for modeling nuclear fuel performance in steady-state and transients to improve the underlying material science, including fracture/contact mechanics, fission gas generation and release, and corrosion chemistry. We have developed an efficient scalable parallel simulation framework for solving such multi-domain, multi-physics problems and have used it to solve the specific nuclear fuel problem described above. Within our particular application a nonlinearly consistent Jacobian Free Newton Krylov (JFNK) method is used (though the ability to use alternative solution methods also exists) across all the domains for each fuel rod. Physics-based preconditioning is used to accelerate the solution process and the multi-domain (pellets and clad) aspect of the problem is leveraged in developing methods that minimize communication as well as avoid the formation of full matrices over the whole domain. In the next section, we present a mathematical description of the problem under consideration. Section 3 will describe the finite element discretization of the models in Section 2. The algorithms used to solve the resulting nonlinear system of equations are described in Section 4. The computational framework that was used in this work is briefly described in Section 5. Section 6 reports on numerical experiments performed to verify and validate our code and test its parallel scalability. Section 7 provides details on coupling to reduced order flow models, coupling to oxide growth models on the exterior clad, and parallel full assembly simulations that couple thermal transport components on unstructured meshes with a structured mesh radiation transport code. The paper ends with a few concluding remarks.

4

2. Model A 3D fuel rod domain, , is modeled as consisting of the union of N pellet 3 C 3 subdomains, P i  R , i = 1, 2, . . . , N , and a clad subdomain   R , i.e, P C the global domain  = N i=1 i   . The number and geometric complexity of fuel pellets in fuel rods can vary significantly; from simple cylinders to the complex pellet geometries shown later in this manuscript and from a few pellets in an experimental rod to more than 400 pellets in a commerical nuclear fuel rod. In our numerical experiments N will be varied between 1 and 360 though there is no fundamental limitation on the number of pellet P domains. Here, P 1 will denote the domain of the lowest pellet and N will denote the topmost pellet domain. The surfaces of the pellet and clad C domains will be denoted by P i , i = 1, 2, . . . , N , and  , respectively. From now on, where there is no danger of confusion the superscript `P' will be dropped for the pellet subdomains. We will assume that the subdomains are disjoint. Only adjacent pellet subdomains are assumed to touch, i.e., i  i-1  i,i-1 =  for i = 2, . . . N , and i  j =  for j = i + 1, i - 1. In this paper we will only consider the case where no pellets are in mechanical contact with the clad or flow subdomains. We will first describe the models at the level of individual pellets, clad, and coolant domains before considering the full coupled multi-domain model. 2.1. Pellet Models: The temperature field, Ti , in each fuel pellet domain i is modeled by a nonlinear thermal diffusion equation, -  · ki (Ti , x)Ti (x) = fi (x), for x  i , (2.1)

where ki is a scalar nonlinear thermal conductivity and fi  L2 (i ) is a nonzero thermal source due to heat generated from nuclear fission in each pellet. In stand-alone applications, the radial shape of fi is usually approximated with a local exponential in the radial dimension and a globally low-order (quadratic or cosine) model in the axial dimension. ki and fi vary spatially within each pellet and can potentially vary in functional form from pellet to pellet to account for differences in materials and fission processes that do occur in practice. For example, insulator pellets with natural, rather than enriched, uranium are often introduced at the top and bottom of the pellet stack to reduce axial power peaking. 5

There is contact/gap resistance between two adjacent pellets and for pellet i this is modeled by Robin boundary conditions of the form
m ki (Ti )Ti · ni + hi,i-1 (Ti , Tim -1 )(Ti - Ti-1 ) = 0 on i,i-1 , m ki (Ti )Ti · ni + hi,i+1 (Ti , Tim +1 )(Ti - Ti+1 ) = 0 on i,i+1 .

(2.2) (2.3)

Here ni is the outward facing unit normal on the surface of pellet i and Tim -1 is the surface temperature field for pellet (i - 1) on i-1  i,i-1 interpolated to the surface i  i,i-1 of pellet i. The flux between adjacent pellets is assumed to be continuous while the temperature field is assumed to be discontinuous. Heat transfer between the pellets and clad is by radiative, conductive and convective processes through the gap region. For each pellet this is modeled using a Robin boundary condition ki (Ti )Ti · ni + hi,c (Ti , Tcm )(Ti - Tcm ) = 0 on i,c (2.4)

where Tcm is the surface temperature field for the cladding projected onto the surface, i,c , of pellet i and hi,c is an effective heat transfer coefficient that can be modeled to account for a wide variety of physical phenomena. In this manuscript, we limit discussion of the heat transfer coefficient to the nonlinearities associated with the dependence on the clad and fuel temperatures in the geometric orientation. This in effect results in a nonlinear coupling between all pellets and the clad through the boundary conditions. Zero Neumann boundary conditions ki (Ti )Ti · ni = 0 on i,n are imposed on all remaining surface boundary regions of each pellet. 2.2. Clad Model: The temperature field, Tc , for the clad domain c is also modeled by a nonlinear thermal diffusion equation, -  · kc (Tc , x)Tc (x) = 0 for x  c (2.6) (2.5)

with a scalar nonlinear thermal conductivity (kc ) and a zero right hand side since heat is not generated within the clad materials. Heat transfer from the clad outer surface to the coolant is modeled by a Robin boundary condition
m m ) = 0 on c,f kc (Tc )Tc · nf + hc,f (Tc , Tf )(Tc - Tf

(2.7)

6

where nf is an outward facing unit normal from the clad surface into the m flow region, Tf denotes the interpolated temperature from the fluid flow region, and hc,f is the effective heat transfer coefficient. The temperature of the flow can be described either by a constant fixed temperature or through an independent or coupled flow model. For most of this work we choose to couple a flow model that solves a form of the fluid equations as described in section 7 and Appendix A. Similarly, heat transfer between the clad inner surface and the gap region between the pellets and clad is modeled by a Robin boundary condition
m m kc (Tc )Tc · ng + hc,g (Tc , T1 , . . . , TN )(Tc - Tg ) = 0 on c,g

(2.8)

Here ng is the outward facing unit normal from the inner clad surface into the pellet-clad gap, Tim is the temperature on the outer surface of the pellet projected onto the inner surface of the cladding for each pellet i, and hc,g is the clad-side heat transfer coefficient that is corresponds to the hi,c to conserve energy across the gap. In addition zero Neumann boundary conditions are imposed on the top and bottom of the clad cylinder. 2.3. Weak Formulation: Given fi (x)  L2 (i ), for each pellet subproblem we seek a solution to Eqns (2.1)-(2.5) in the trial space of functions ¯ i , T  H 1 (i ), T satisfies Eqns (2.2)-(2.5) on i }. (2.9) Vi = {T (x)| x   The Galerkin weak formulation for each pellet subproblem is: Find Ti  Vi such that v  Vi -
i

v  · ki (Ti )Ti di =
i

fi v di .

(2.10)

Integration by parts and the divergence theorem yield ki (Ti )Ti · v di -
i i

vki (Ti )Ti · ni di =
i

fi v di

(2.11)

7

Using Eqns (2.2)-(2.5) the boundary integral on the left hand side may be simplified to give -
i

vki (Ti )Ti · ni di = vhi,i-1 (Ti - Tim -1 ) di + vhi,i+1 (Ti - Tim +1 ) di +
i,c m vhi,c (Ti - Tic ) di

i,i-1

i,i+1

Let u, v i =
i

uv di and (u, v ) =


(2.12) uv d denote the standard inner prod-

ucts over the domain i and a boundary segment  respectively. Then the Galerkin subproblems for the pellets, i = 1, 2, . . . , N , can be written as: Pellet Sub-Problems: Find Ti  Vi such that v  Vi ki (Ti )Ti , v i + (hi,i-1 Ti , v )i,i-1 + (hi,i+1 Ti , v )i,i+1 + (hi,c Ti , v )i,c (2.13) m m = fi , v i + (hi,i-1 Tim -1 , v )i,i-1 + (hi,i+1 Ti+1 , v )i,i+1 + (hi,c Tc , v )i,c Similarly, the Galerkin weak formulation for the clad subdomain may be stated as: Clad Sub-Problem: Find Tc  Vc such that v  Vc kc (Tc )Tc , v c + (hc,g Tc , v )c,g + (hc,f Tc , v )c,f m m , v )c,f = fc , v c + (hc,g Tg , v )c,g + (hc,f Tf 3. Discretization 3.1. Domain Discretization As can be seen from Figures 2 and 3, the clad subdomain c and the pellet subdomains i are bounded, connected volumes with piecewise smooth curved boundaries. For the purposes of this paper i is in general approximated by a polyhedral domain h i during the mesh generation process. i h h is partitioned into a set, P , of non-overlapping general hexahedral elei i h h h h ments, Pi = {Pi,1 , Pi,2 , . . . , Pi,M } which are geometrically conforming, i.e.:
h ·  Pi,j = h i j =1 h h · The intersection of two elements, Pi,l and Pi,m for l = m is either empty, a single vertex, an entire edge, or an entire face of both elements. M

(2.14)

8

Figure 2: A slice of a fuel rod mesh showing the outer clad mesh and three inner pellet meshes 3.2. Problem Discretization: A discrete approximation to Ti , the solution to Eqn (2.13) over the i-th pellet subdomain is obtained by solving the problem in a finite dimensional Ni subspace Vih of Vi . Let dim(Vih ) = Ni and {h i,k }k=1 be a suitable basis for Vih . Then, letting Tih =
Ni k=1 h h h Ti,k i,k where Ti,k are unknown coefficients, the

discrete Galerkin problem for each pellet is: Discrete Pellet Sub-Problems: Find Tih = v h  Vih ki Tih , v h i + (hi,i-1 Tih , v h )i,i-1 + (hi,i+1 Tih , v h )i,i+1 + (hi,c Tih , v h )i,c h m h m h = fih , v h i + (hi,i-1 Tim -1 , v )i,i-1 + (hi,i+1 Ti+1 , v )i,i+1 + (hi,c Tc , v )i,c (3.1) h h h Similarly, by a suitable choice of Vc  Vc with span{c,1 , . . . , c,Nc } = Vch and dim(Vch ) = Nc a discrete approximation to the solution of the clad problem (2.14) can be obtained by solving:
Ni k=1 h h Ti,k i,k  Vih such that

9

(a) IFA597 Pellet Mesh

(b) IFA432 Pellet Mesh

Figure 3: (a) pellet mesh with dish and chamfer (b) standard pellet mesh Discrete Clad Sub-Problem: Find Tch = v h  Vch kc Tch , v h
c Nc k=1 h h Tc,k c,k  Vch such that

+ (hc,g Tch , v h )c,g + (hc,f Tch , v h )c,f
c m h m h + (hc,g Tg , v )c,g + (hc,f Tf , v )c,f

h h = fc ,v

(3.2)

Basis functions: The use of general hexahedral or brick elements in discretizing the pellet and clad geometries leads us to use isoparametric finite elements for problem discretization. The basis functions over each geometric element Pi,k of a partition Pi are formed based on mappings of the standard trilinear Q1 basis functions defined over the reference cube [-1, 1]3 to Pi,k . This ensures continuity across element boundaries within a partition. All of the integral terms in Eqns (3.1) and (3.2) are evaluated using a second order Gaussian quadrature. For example, the term from Eqn (3.1) corresponding to the external source term, fih , v h i , is computed by evaluating fi (x) at the quadrature nodes used by the finite element bases and using the quadrature to evaluate the integrals, i.e.
M N

fih , v h i

=
j =1 =1

w fi (x ) v h (x ) ,

(3.3)

where the j index denotes the spatial elements within the domain h i, N 10

indicates the number of quadrature nodes per spatial element, and x and w are the quadrature nodes and weights, respectively. Evaluating the source term at quadrature points rather than mesh nodes allows strict conservation of the source term to be enforced automatically. Because the source term is due to nuclear fission in the fuel pellets, the corresponding term in Eqn (3.2) representing the source in the clad is treated as zero, i.e. fch , v h c = 0.
m m h h For fixed Tim -1 , Ti+1 , and Tc , by choosing v = i,j for j = 1, . . . , Ni in Eqn (3.1) we obtain a set of Ni equations for the unknown coefficients h , k = 1, . . . , Ni denoted by: Ti,k

Fi (Tih ) = 0,

(3.4)

t h h h where Tih = (Ti, 1 , Ti,2 , . . . , Ti,Ni ) with some abuse of notation. Since the conductivity ki and the effective heat coefficients hi,i-1 , hi,i+1 , and hi,c are nonlinear functions, Eqn (3.4) represents a coupled system of nonlinear algebraic equations over each pellet subdomain coupled through nonlinear boundary conditions to the adjacent pellets and clad. For the clad, a similar methodology leads to a set of Nc nonlinear algebraic equations for the unknown h coefficients Tc,k , k = 1, . . . , Nc which we denote by:

Fc (Tch ) = 0. The full set of nonlinear equations we wish to solve is given by F(T) = 0
t

(3.5)

(3.6)

h h h where F(T) = F1 (T1 ), F2 (T2 ), . . . , FN (TN ), Fc (Tch ) is the coupled set of block nonlinear equations across all pellet and clad domains with each block Fi specified by Eqn (3.4) and Fc given by Eqn (3.5). Here, T denotes the t h h h block unknowns across all domains, T  T1 , T2 , . . . , TN , Tch with each Ti a vector of unknowns itself.

4. Solution Strategy Several nonlinear solution strategies could be used to solve the nonlinear system of algebraic equations described in Section 3. The fact that our problems often involve several hundred subproblems each discretized over a

11

separate physical domain make solution methods that do not require formation of the full Jacobian matrix attractive. In particular we choose to use a Jacobian-Free Newton Krylov (JFNK) method [8] for our nonlinear solver. The efficiency of a JFNK method when applied to a particular problem depends heavily on the preconditioner used. We will describe a physics based preconditioner that exploits the natural subdomain decomposition present in our application. A brief overview of the JFNK method is given in Section 4.1 and the construction of the preconditioner is described in Section 4.2. 4.1. Jacobian-free Newton-Krylov Methods Let T denote the exact solution to Eqn (3.6). Classical Newton's method for solving Eqn (3.6) generates a sequence of approximations Tk to T , where Tk+1 = Tk + sk and the Newton step sk is the solution to the system of linear equations Jk sk = -F(Tk ), (4.1)

where Jk  F (Tk ) is the Jacobian of F evaluated at Tk . Newton's method is attractive because of its fast local convergence properties. For large-scale problems, Eqn (4.1) is typically solved using an iterative method because direct methods become impractical. Furthermore, it is often unnecessary to use a tight convergence tolerance for the iterative method when Tk is far from T , since the linearization that leads to (4.1) may be a poor approximation to F(T). Generally, it is much more efficient to employ inexact Newton methods [9], in which the convergence tolerance for (4.1) is selected adaptively by requiring that sk only satisfy: F(Tk ) + Jk sk  k F(Tk ) (4.2)

for some k  (0, 1) [9]. With an appropriate choice of the forcing term k superlinear and even quadratic convergence of the iteration can be achieved [10]. While any iterative method can be used to find an sk that satisfies (4.2), Krylov subspace methods are distinguished by the fact that they only require matrix-vector products to proceed. These matrix-vector products can be approximated by a finite-difference version of the directional (G^ ateaux) derivative as:

12

F(Tk + v) - F(Tk ) , (4.3)  which is especially advantageous when Jk is difficult to compute or expensive to store (as is the case in this application due to the presence of multiple meshes). While the selection of a suitable differencing parameter  may be non-trivial for some applications, it is generally well-understood [11]. For this application, we choose: Jk v  = 
mach

1 + Tk , v

where mach is machine precision and · refers to the l2 -norm. In our applications, which are performed in double precision,  is typically on the order of 10-10 . From the various Krylov methods available, GMRES was selected because it guarantees convergence with nonsymmetric, nonpositive definite systems [12] (the case in some of our examples), and because it provides normalized Krylov vectors v = 1, thus bounding the error introduced in the difference approximation of (4.3) (whose leading error term is proportional to  v 2 ) [13]. However, GMRES can be memory intensive (storage increases linearly with the number of GMRES iterations per Jacobian solve) and expensive (computational complexity of GMRES increases quadratically with the number of GMRES iterations per Jacobian solve). In principle, restarted GMRES can deal with these limitations; however, it lacks a theory of convergence, and stalling is frequently observed in real applications [14]. In our applications, we rely on performing inexact solves combined with efficient preconditioning to keep the number of GMRES iterations required to compute each inexact Newton step, sk , small. 4.2. Preconditioning Preconditioning is a numerical technique used within an iterative method to accelerate the process of finding a solution to a system of equations. The use of a preconditioner will increase the cost of each iteration but a good preconditioner will drastically reduce the total number of iterations required to solve the system of equations thus making the overall process significantly faster than the unpreconditioned case. While the idea can be applied to 13

both nonlinear and linear systems of equations, we only focus on applying to the linear systems at each Newton step within the JFNK procedure. In particular, we used a "right preconditioning" procedure to solve (4.1). In this approach, (4.1) is transformed to the equivalent system shown in (4.4) by using a preconditioner, M . JM-1 Ms = -F(T) (4.4)

The above system is solved in two steps: (1) solve Ay = -F(T) using the GMRES method where A = JM-1 and (2) compute s = M-1 y. The matrix-vector product, Av, is approximated as shown in (4.5). F(T + M-1 v) - F(T) , (4.5)  Ideally, M-1 should be a good approximation to J-1 , it should be easy and inexpensive to compute and apply, and the computation and application of M-1 should have good parallel scalability. It is difficult to meet all of these competing requirements and some trade-offs need to be made while designing M. The design of the preconditioner used for our problem stems from a careful observation of the structure of the true Jacobian matrix which is of the form:   J11 J12 0 0 ··· 0 J1C  J21 J22 J23 0 · · · 0 J2C     0 J32 J33 J34 · · ·  0 J 3 C    0 0 J43 J44 · · · 0 J4C  J= (4.6)   .  . . . . . . . . . . . . .  . . . . . . .     0 0 · · · · · · · · · JN N JN C  JC 1 JC 2 · · · · · · · · · JCN JCC Av  Here, each Jii , i = 1, . . . , N is a block matrix denoting the portion of the full Jacobian arising from interior-interior connections within the i-th pellet. Ji,i-1 and Ji,i+1 are block matrices corresponding to the boundary interactions between pellet i and pellets (i - 1) and (i + 1) respectively. JiC and JCi are block matrices corresponding to the boundary couplings between the i-th pellet and the clad. Under normal operating conditions within a reactor the heat transfer between adjacent pellet domains is considerably lower than between the pellets and clad. This prompts us to drop off diagonal terms corresponding to 14

pellet-pellet interactions when forming the preconditioning matrix M. This leaves off diagonal coupling terms between the pellets and clad. While a preconditioner that retains these terms is likely to yield better overall convergence it requires a block Gauss-Seidel type approach that limits the level of asynchrony within the preconditioner. By dropping off diagonal terms corresponding to pellet-clad interactions we choose to sacrifice potentially better convergence for the ability to asynchronously solve all domains at once within the preconditioner solve step. Finally, block diagonal terms of the Jacobian that involve the partial derivative of the thermal conductivity, k , with respect to the temperature, T are ignored to yield an approximate Jacobian of the form ~  J11 0 0 0 ··· 0 0 ~22 0  0 J 0 ··· 0 0     0 ~33 0 · · · 0 J 0 0     0  ~ 0 0 J · · · 0 0 M= (4.7) 44   .  . . . . . . . . . . . . .  . . . . . . .     0 ~ 0 · · · · · · · · · JN N 0  ~CC 0 0 ··· ··· ··· 0 J Inverting systems of the form Ms = y now correspond to solving (N + 1) independent subsystems ~ii si = yi J (4.8) At the k -th Newton step, systems 1 through N correspond to discretizing and solving N variable coefficient linear diffusion PDE systems, one for each pellet domain, of the form:
k -  · ki (Tik , x)Ti (x) = ri (x), for x  i ,

(4.9)

with boundary conditions ki (Tik )Ti · ni + hi,i-1 (Tik , 0)Ti = 0 on i,i-1 , ki (Tik )Ti · ni + hi,i+1 (Tik , 0)Ti = 0 on i,i+1 . (4.10) (4.11)

where Tik is the current approximation to the solution of the nonlinear system k (3.6) over the i-th pellet domain and ri is the nonlinear residual. System (N +1) of the preconditioner solve is over the clad domain and again involves

15

discretizing and solving a linear variable coefficient diffusion PDE system of the form: k -  · (kc (Tck , x)Tc ) = rc for x  c (4.12) with boundary conditions kc (Tck )Tc · ng + hc,g (Tck , 0)Tc = 0 on c,g kc (Tck )Tc · nf + hc,f (Tck , 0)Tc = 0 on c,f (4.13) (4.14)

with Tck being the current approximation to the solution of the nonlinear k system (3.6) over the clad domain and rc the nonlinear residual. Discretization of the linear variable coefficient diffusion PDE systems described above follows along the lines of the methodology described in section (3). Each application of the preconditioner involves (N + 1) algebraic multigrid solves to invert the systems (4.8). Since these are independent subsystems all of the (N + 1) solvers can operate simultaneously when distributed over different processor sets enabling us to obtain a high degree of parallelism irrespective of the number of pellet subdomains present. 5. Computational Infrastructure Several computational tools are necessary for performing simulations such as the one described in this paper; we developed the Advanced Multi-Physics (AMP) [15] package for this purpose. AMP is a complete system for simulating stationary and time dependent, multi-domain, coupled physics problems. AMP consists of several software components. Each component is designed to provide a uniform consistent interface which interacts with other components, and developers of other components are only exposed to these interfaces. This is despite the fact that AMP is designed to sit in between existing software frameworks to leverage their strengths and investments without over-dependence. The complexities of interfacing with different software frameworks are kept behind the standard interfaces that AMP provides. Fig. 4 illustrates the structure of the various components in AMP. A brief description of each of these components is given below. 5.1. Mesh and geometry The mesh and geometry interface (AMP::Mesh) allows AMP to interact with multiple mesh or geometry packages. AMP::Mesh already interfaces

16

External Codes

Time Integrators Solvers Operators Vectors/Matrices Discretization Mesh/Geometry

Figure 4: Structure of components in AMP

with the LibMesh [16] and STKMesh [17] packages, in addition to maintaining a native structured mesh capability. The native structured mesh capability includes automated mesh generation for simple geometries enabling users to create boxes, cylinders, and tubes without an external mesh generation tool. These internal mesh generators were used for most of the results presented in section 6. 5.2. Discretization Due to the close coupling between mesh and discretization, AMP::Mesh (using functionality contained in LibMesh), currently handles the discretization also. This will become a separate interface, if the need arises for us to interface different discretization packages with mesh packages. 5.3. Vectors and matrices AMP provides standard AMP::Vector and AMP::Matrix classes which serve two purposes. Firstly, they provide users with a standard interface to perform vector and matrix operations. At the same time, the classes hide the details of interfacing with various software packages that have their own definition of vectors and matrices. For example, Trilinos [18] and PETSc [19] both provide matrix operations and Trilinos, PETSc, and SUNDIALS [20] provide and/or use vector operations. AMP Vector and Matrix act as the interfaces to these packages through the Vector and Matrix classes and enable a user to combine components from all of these packages to build powerful AMP applications while not having to tackle the complexities of interacting with each of these packages. 17

The vector class also contains the ability to compose multiple parallel vectors into a single multi-vector that can be provided to a solver. For example, the global solution and source vectors are created as a composition of nodal vectors on each mesh and composed into a single multi-vector. A view to the multi-vector is provided to PETSc SNES nonlinear solver for the Newton iterations and a view of the vector and the diffusion matrix associated with a single domain is provided to Trilinos ML for the multi-grid preconditioning. 5.4. Operators Operators are the core of the AMP design and where all of the physics is contained. Operators encapsulate the details of the mapping operation L : X  Y where X and Y are appropriately defined spaces. Operators may represent discretized PDE operators, boundary operators, an operation to extract material properties from material databases or tables, linear or nonlinear algebraic operations, or compositions of the above. The ability to compose operators and to extract information from compositions is intended to facilitate the incremental construction of multi-physics and/or multidomain simulations as well as rapid prototyping and experimentation to understand couplings in multi-physics simulations. The nonlinear and linear FEM operators for diffusion, boundary conditions and interpolation maps between domains are encoded as operators. 5.5. Solvers Solvers in AMP refer to the nonlinear and linear solvers that represent the action of an approximate inverse map of a given operator if that inverse operation has some well defined meaning. In this sense solvers can also be considered as operators. An inverse operator can be easily constructed by wrapping a solver in an inverse operator class. The solver interface allows the user to utilize a standard interface to solvers from Trilinos, PETSc, native AMP solvers, and potentially other packages in the future. Again, the design emphasis has been to provide a standard interface to hide the complexity of particular software packages from a user and to avoid required dependence on a particular software package. The solver interface enables us to create the complex nonlinear solvers, linear solvers, and preconditioners across multiple domains without significant code rewrites.

18

5.6. Time integrators AMP time integrators provide a uniform interface to solving time-dependent systems which can include Differential Algebraic Equations (DAEs). This is necessary within the context of our broader target application class because of coupling between time dependent thermal and quasi-static mechanical systems being simulated. The design allows for explicit, semi-implicit, and fully implicit simulations of coupled multi-physics problems. In the case of semi-implicit and fully implicit calculations, the solver interfaces in AMP are used, and in all cases, the operator interfaces are used to allow composable multi-physics simulations allowing users to experiment with coupling different physics together. The time integrator interface is used to provide an interface to the SUNDIALS suite of time integrators and can be used in future to interface to other time integrator packages such as the Rhythmos package of Trilinos. 5.7. External packages AMP is designed to leverage existing software whenever possible including off-the-shelf leadership class computational packages that include the Trilinos, PETSc and SUNDIALS packages. In general the infrastructure design of AMP does not rely on any external software, but provides interfaces for using external software within AMP. For example, a user can create a vector using AMP's internal vector, a PETSc vector, or a Trilinos Epetra vector, but can then use the given vector within the solvers that may use PETSc or Trilinos solvers. AMP leverages capabilities within many software packages through a seamless application programming interface including MPI for parallel capabilities, PETSc for vectors, nonlinear and linear solvers, Trilinos for vectors, nonlinear solvers, and algebraic multigrid solvers, SUNDIALS for implicit time integrators, LibMesh and STKMesh for discretization and meshing and HDF5 and Silo for IO. 5.8. Parallel implementation AMP is primarily designed to be a parallel infrastructure based on MPI. An MPI-based utility class is provided that allows the user to utilize MPI. The interface enables AMP to be compiled without MPI for users who do not wish to leverage the parallel capabilities. The core design is independent of the parallelization, and the parallelization is based on parallel decomposition of the meshes. There is a two-level parallel decomposition used for the mesh 19

domains. First, the individual mesh domains are split onto separate communicators and independent processor groups. This minimizes the number of processors per mesh and ensures that independent meshes can utilize collective operations that do not include the processors of other meshes. This splitting is done internally within AMP and can be modified by the user. A second level of domain decomposition is then performed to divide each mesh domain among the processors in the MPI group for that domain. This level of decomposition is handled by the package responsible for the current mesh domain. For example, if the underlying mesh for a given domain is LibMesh, it will perform the decomposition, while a native AMP mesh will be controlled by AMP. Note that different mesh domains may be owned by different packages and this is fully supported. Once the domain-decomposition is performed, vectors and matrices may be created over a single mesh, an arbitrary combination of meshes, or a subset of a mesh (or multiple meshes). Each vector or matrix exists over a given communicator that does not need to match any mesh. Linear operations are then performed on this communicator reducing the need for global operations over the entire global communicator. Maps between multiple domains use a communicator that spans two or more existing communicators. The MPI utility class provides all routines for creating and managing the communicators with MPI, including their proper destruction when they are no longer needed. Section 6.4.1 includes the results of a scaling study conducted using the problem described in Section 6.1 and using the parallel load balancing strategies described here. 6. Numerical Experiments A suite of numerical experiments were defined to verify the accuracy of the thermal transport capability of AMP for a multi-domain problem that is based on the geometry and materials of nuclear fuel. Independent studies were performed to verify the accuracy of the solution using the method of manufactured solutions (Section 6.2), evaluate the accuracy of the code with respect to experimental data and a well characterized code used for regulatory analysis (Section 6.3), and evaluate the scalability of the parallel algorithm (Section 6.4). All of the studies were based on actual geometries and material properties for experimental nuclear fuel rods that are defined in Section 6.1. All numerical experiments were performed on the Titan (Cray XK7) and EOS (Cray XC30) supercomputers hosted at the Oak Ridge Leadership 20

Computing Facility with 8 and 16 MPI processes per compute node respectively. Load balancing is done automatically by the load balancer within AMP. The AMP nonlinear solver internally leveraged the JFNK implementation within the PETSc package with absolute and relative tolerances for the JFNK nonlinear solver being set to 1.0e-12 and 1.0e-10 respectively. Right preconditioned FGMRES with a maximum Krylov dimension of 40 was used within our simulations. The AMP preconditioner consists of a block Jacobi solver as described earlier with each block Jacobi solver component consisting of one or more iterations of an algebraic multigrid (Trilinos ML) V-cycle solver with 2 pre- and post-smoothing steps of a symmetric Gauss-Seidel smoother, a maximum of 10 multigrid levels and a coarse grid direct solver. All simulations were performed in double precision arithmetic. 6.1. Experimental Setup The materials and geometries used in the following numerical experiments are based on one of two well-characterized experiments from the International Fuel Performance Experiments (IFPE) database [21]. The Integrated Fuel Assembly (IFA) 432 [22], Rod 1, is a standard nuclear fuel rod (uranium-dioxide or, UO2 , fuel in Zircaloy-4 clad) that was irradiated in the Halden boiling water reactor from December 1975 to June 1982 with online temperature measurements at one axial location in the center of the fuel. The IFA 597 [23], Rod 2, contains weapons-grade mixed-oxide (MOX) fuel within Zircaloy-4 clad in a more modern geometry that includes a dish, chamfer, and central hole, as shown in Figure 5. IFA 597, Rod 2 was irradiated in the Halden boiling water reactor from from July 1997 to January 2002 with online temperature measurements at one axial location in the center fuel. Each of these rods is a short version of a full length commercial fuel rod. The geometric and material description of the experimental rods are provided in Table 1 for the IFA 432 and IFA 597 experiments. The clad height and number of pellets modeled were adjusted in Sections 6.2 and 6.4 to achieve the purpose of that section. However, these geometries were used exactly in Section 6.3. The thermal conductivity (k ) of Zircaloy-4 (Eq. 6.1) and fuel (Eq. 6.2), including both UO2 and MOX, depend on the temperature (T) and burnup (B), which is a measure of total heat generated locally. k [Zr] = 7.51 + 2.09 × 10-2 T - 1.45 × 10-5 T 2 + 7.67 × 10-9 T 3 21

(6.1)

Figure 5: Mid-plane slice of an annular nuclear fuel pellet with a dish and chamfer

22

Dimension Units Pellet ID mm Pellet OD mm Pellet Dish Depth mm Pellet Dish Diameter mm Pellet Chamfer Height mm Pellet Chamfer Diameter mm Pellet Density g/cc Pellet Height mm Number of Pellets Clad ID mm Clad OD mm Clad Height mm

IFA 432 0 10.67 0 0 0 0 10.42 13 44 10.9 12.78 622.8

IFA 597 1.8 8.05 0.26 2.15 0.15 5.3 10.5 10.5 21 8.22 9.5 252.

Table 1: Geometry and material specification for validation problems

k [Fuel] = 

 + T +  1.0 + e

 -T

-1

-1

+ T -2 e- T



(6.2)

where [UO2 ] = 1.00767;  [UO2 ] = 0.0452 + 0.00187B ;  [UO2 ] = 0.000246; [UO2 ] = 16361;  = =  =  =

[MOX] = 1.05353  [MOX] = 0.035 + 0.00187B  [MOX] = 0.000286 [MOX] = 13520

0.038 1.0 - 0.9e-0.04B B 0.28 396 6380 3.5 × 109

To define the heat source in the nuclear fuel, AMP allows the user to either define the power distribution, f (r, , z ), as a function of the radius (r) from the center of the pellet, height (z ), and azimuthal-angle () about the z axis or provide a power distribution in a coupled-physics calculation at every 23

quadrature-point in the problem. The user-defined power definition allows for a simple definition of the axial shape functions or complex nuclear-specific features, such as the radial rim effect or azimuthal variations guide tubes and control rods. The radial power shape includes the option to use a model that coincides with the empirically-derived TUBRNP model (Equation 6.4) from the Transuranus nuclear fuel performance code [24]. f (r, , z ) = 1 + a F (r) + b sin() +
k>0

ck Pk (z );

(6.3) (6.4)

F (r) = 1 + 3.45 exp[-3(R - r)0.45 ].

In equation 6.3, the user-defined coefficients (a, b , and ck ) define the magnitude of each component, Pk (z ) are Legendre polynomials, and F (r) is the TUBRNP model that is based on the radius and the outer radius of the fuel pellet (R). In the verification testing, a manufactured-source was utilized; the validation testing utilized the TUBRNP model. 6.2. Verification Studies Verification studies for modeling steady state thermal contact for nuclear fuels are presented here. The verification process uses the method of manufactured solutions. For this study, the pellet geometry is based on the IFA 432 experiment and the clad geometry is simplified as shown in Figure 6. Material properties of UO2 for the pellet and Zircaloy for the clad are based on the IFA 432 experiment as listed in Section 6.1. The pellet and clad domains are not in contact and have a (gap) distance between the surfaces. A total of four cases are studied using this configuration. Case 1: Three-dimensional single fuel pellet (with no clad) that exhibits strong gradients in all directions with different Robin boundary conditions between circular surface along the height and end surfaces. Case 2: Three fuel pellets stacked upon each other (with no clad). The surfaces are in contact and a manufactured solution is constructed using the fuel pellet contact conductance model. At the contact surfaces a Robin boundary condition is imposed and the results would make apparent any anomalies in the volume and boundary condition discretizations. Case 3: Single fuel pellet and clad with heat transfer across a gap between the surfaces. The manufactured solution for both the domains is constructed

24

Figure 6: Schematic of pellet and clad geometries used in verification studies

25

taking the nonlinear gap conductance between the internal surfaces into consideration. The results would make apparent any lack of energy conservation due to heat transfer across the gap. Case 4: Two pellets and clad including contact between pellets and heat transfer across a gap between pellet and clad. This verifies the implementation at corner points which intersect multiple domains. We begin by selecting the exact solution to be (x, y, z ) = 800z + 106 (0.00004 - 20x2 - 20y 2 ) (6.5)

which is qualitatively similar to the thermal solutions and in bounds to the material models. Substituting it for T in the differential equation (2.1) we obtain the corresponding analytic right hand side. These sources and corresponding boundary conditions are evaluated at each quadrature point of the entire finite element mesh in order to eliminate interpolation errors. The relative and absolute convergence tolerances within the nonlinear solver are set to 1.0e-10 to ensure solver errors are below discretization error bounds. The discretization error is evaluated using an L2 norm.
1/2 1/2

|| - h ||L2 =


( - h ) d

2


 qp

( - h ) J (xqp )wqp

2

(6.6) Convergence rates of these norms are reported on progressively refined meshes by increasing the number of elements in all three cylindrical coordinates to simplify computing the characteristic element length "h" of the unstructured mesh. Also, we make sure that the refinements are generated from the original geometry to ensure that the meshes are geometrically conforming. The rate of convergence is given by log p= log
||e2 || ||e1 || h2 2 h2 1

log 

||e2 || ||e1 ||

log (4)

(6.7)

26

Problem Case 1

Case 2

Case 3

Case 4

# Elements 1890 14944 119296 5670 44832 357888 3510 27904 222976 5400 42848 342272

|| - h ||L 2.1796 0.7426 0.2256 2.1845 0.7810 0.2644 2.9740 0.7053 0.1817 2.9851 0.7446 0.2207

|| - h ||L2 0.002411 0.000624 0.000155 0.004139 0.001073 0.000267 0.003132 0.000804 0.000200 0.004434 0.001139 0.000284

p 1.94 2.00 1.94 2.00 1.96 2.00 1.96 2.00

Table 2: Mesh refinement studies # Elements || - h ||L 10 Pellets 18900 2.1848 149440 0.7810 1192960 0.2644 50 Pellets 94500 2.1223 756000 0.7810 6048000 0.2644 || - h ||L2 0.007606 0.001971 0.000491 0.017045 0.004416 0.001100 p

1.94 2.00

1.94 2.00

Table 3: Many domain mesh refinement studies for Case 2

27

Elements 10 Pellets 35100 280800 2246400 50 Pellets 87750 702000 5616000

|| - h ||L 2.9899 0.7450 0.2210 2.9852 0.7450 0.2210

|| - h ||L2 0.009937 0.002549 0.000636 0.022190 0.005700 0.001421

p

1.96 2.00

1.96 2.00

Table 4: Many domain mesh refinement studies for Case 4 In addition to four cases mentioned, we also conducted the verification studies for generalized versions of case 2 and case 4 with 10 and 50 pellet domains to demonstrate the convergence rate of the solution procedure in parallel. These results are presented in Tables 3-4. 6.3. Validation Studies An extensive validation evaluation of AMP for nuclear fuel applications, which includes several experimental fuel rods for a variety of conditions, has been documented in [25­27]. This section includes an excerpt of that research to provide a basis for the accuracy of the material models with respect to the experimental results. Because of the extreme environment of nuclear fuel (high radiation, high temperature, and highly turbulent, multi-phase flow), it is difficult to precisely measure both the local temperature and the power in the fuel near the thermocouple. Therefore, nuclear fuel experiments generally assume a 5 to 10% experimental uncertainty in the measured data; for this report, we have incorporated a relatively tight expected tolerance of + 50K, which is generally less than the 10% error and approximately 3% at full power. Figure 7 provides the computational results of the AMP simulation and experimental measurements of the centerline fuel temperature in the IFA 597 experiment. The input power is relatively constant and the computational results are consistently within 50K and generally within 3% of the measured temperature. The results for the IFA 432 experiment are shown in Figure 8. Because the power distribution varies significantly more than in the IFA 597 experiment, the 50K error bars on the experimental results appear much

28

Figure 7: Validation of the temperature at the thermocouple in the IFA 597 experiment smaller. However, the AMP results generally fall within the experimental error. 6.4. Scaling Studies Scaling studies for the steady state thermal problem are presented here. The problem setup was described in Section 6.1, with the geometry based on the IFA-432 experiment, but now including 348 full pellets, which is representative of a full-scale commercial nuclear fuel rod. Several different aspects of the performance are studied using both strong and weak scaling. These include: · Solve: Total time spent within the JFNK solver for the entire coupled nonlinear thermal problem. · Nonlinear Residual: The time required to compute one consistent global nonlinear residual within the JFNK solver across all physical domains. Computing the nonlinear residual involves computing the nonlinear residual for each physical volume, applying boundary conditions, and mapping solution components across domains. The times 29

Figure 8: Validation of the temperature at the thermocouple in the IFA 432 experiment for these individual components will also be reported separately. Note that the number of calls will vary slightly depending on the number of iterations of the solver. This information is also included. · Diffusion Apply: Time required to compute the non-linear finite element residual at all interior degrees of freedom within all domains once. No communication between domains is required here. · Boundary Conditions: Time required to impose the non-linear finite element boundary conditions once within the computation of the global nonlinear residual. This requires traversing all surface elements on the finite element meshes for each domain. No communication between domains is required here. · Map Apply: Time per application of the maps for the solution transfer between the clad and pellets and the maps between the different pellet domains. The mapped values are required in imposition of the nonlinear Robin boundary conditions.

30

· Load Mesh: Time to create all of the meshes. This consists of creating the appropriate load balance for the different domains, creating the individual meshes, and initializing all mesh data. For the purposes of this problem the meshes used consist of internal, logically rectangular meshes. · Save Results: Time to write the results. This includes writing a separate SILO file for each core, and a single summary file that can be loaded into VisIt [28]. 6.4.1. Strong Scaling The strong scaling results are based on a standard resolution mesh, which contains approximately 1.6M unknowns. For high fidelity analysis of a single fuel rod, this is considered a relatively coarse mesh; a high resolution analysis would typically require an 8x increase in each dimension. On the other hand, for high-fidelity analyses of a full nuclear reactor containing tens of thousands of fuel rods, this resolution is sufficient because the accuracy requirements are typically lower. Demonstrating efficient strong scaling from serial to 100500 cores for this problem size (single rod with 1.6M unknowns) will be sufficiently indicative of good scaling results for a full reactor on millions of cores and will also provide a lower bound on expected scaling for fineresolution fuel rod calculations on tens of thousands of cores. The strong scaling studies were executed using 1-2048 cores on the Titan Cray XK7 and the EOS XC30 at Oak Ridge National Laboratory. Tables 5 and 6 show the scaling results on EOS and Titan respectively, with the different components of the solve as well as loading the meshes and saving the results. The first column is the number of cores, while all other columns are the wall-clock execution times for the problem in seconds. For the diffusion and map apply calls (Diffusion Apply and Map Apply, respectively), the execution time is the average accumulated time across all cores. This is necessary because some cores may have different execution times due to load imbalances and is particularly acute for small core counts (2-8) due to a load imbalance between the domains. For large core counts, each domain will exist on a non-overlapping set of cores. Figures 9 and 10 shows the plots of the scaling results compared to ideal scaling. For small core counts (1-8), the run time of the solve is limited by the maps between the different domains. The most time-consuming map is for the cladto-pellets heat transfer between the outer surface of the pellets and the inner 31

# of Processors 1 2 4 8 16 32 64 128 256 512 1024 2048

Nonlinear Diffusion Boundary Map Residual Apply Conditions Apply 3488.25 2690.93 751.84 200.66 1629.49 2265.47 1273.65 374.21 99.52 739.46 761.60 397.61 193.86 50.04 107.30 315.21 187.79 102.38 26.51 42.89 159.12 86.89 51.35 13.22 12.76 74.47 42.11 25.71 6.68 5.34 38.09 21.10 12.94 3.49 2.46 20.83 10.63 6.51 1.74 1.16 10.69 5.42 3.27 0.89 0.64 5.90 3.00 1.70 0.49 0.45 3.85 2.03 0.92 0.30 0.56 4.00 2.15 0.53 0.23 1.12 Solve

Mesh Loading 13.11 7.02 6.33 2.33 1.39 0.64 0.34 0.20 0.11 0.07 0.07 0.10

Save Results 483.12 306.11 30.83 12.48 2.90 1.05 0.42 0.32 0.11 0.11 0.15 0.22

Table 5: Strong scaling studies on EOS
# of Processors 2 4 8 16 32 64 128 256 512 1024 2048 Nonlinear Diffusion Boundary Map Residual Apply Conditions Apply 4908.14 2689.35 739.69 217.28 1609.45 1629.89 819.30 371.02 106.32 248.20 642.05 363.04 186.89 52.71 93.28 345.45 171.02 96.10 27.18 29.70 174.91 88.51 51.01 15.10 12.67 89.06 43.71 25.56 7.60 5.74 46.97 21.87 12.72 3.93 2.50 23.97 11.11 6.39 2.02 1.33 12.88 5.97 3.31 1.08 0.82 8.29 4.06 1.80 0.66 1.02 8.01 4.10 1.02 0.46 2.07 Solve Mesh Loading 13.83 13.78 4.76 2.81 1.42 0.72 0.40 0.23 0.15 0.14 0.22 Save Results 682.38 73.28 28.93 8.15 2.84 1.05 0.35 0.20 0.17 0.18 0.29

Table 6: Strong scaling studies on Titan surface of the clad. This map is constructed using a std::multimap with all points on the local surface, followed by a pair-wise all-all exchange of data between the two meshes. For the serial case, this results in a large number of points in the local map that must be managed. As the number of cores is increased to 128-512 cores, the number of points on the surface per processor is significantly reduced, which leads to an additional log(n) reduction in the wall-clock time that results in the super-linear speedup observed. The apparent lack of speedup from 1 to 2 cores is due to the load imbalance between the clad mesh located on one core and all of the pellet meshes on the

32

104 103 Time (s) 102 101 100 10-1 0 10 101 102 # of Processors 103

Ideal Solve Apply Diffusion Apply Robin Apply Maps Load Save

Figure 9: Strong scaling studies on EOS

104 103 Time (s) 102 101 100 10-1 0 10 101 102 # of Processors 103

Ideal Solve Apply Diffusion Apply Robin Apply Maps Load Save

Figure 10: Strong scaling studies on Titan

other. For very large core counts (1024-2048), the number of elements on a surface is sufficiently small that the communication time begins to dominate, which limits the scaling for this problem size. The behavior of the diffusion and the Robin boundary condition applies (Diffusion Apply and BC Apply, respectively) show nearly perfect scalability because they do not involve any communication. Generating the native structured meshes does not represent 33

2 Efficiency 1.5 1 0.5 0 0 10 101 102 # of Processors Figure 11: Scaling efficiency1 103

EOS Titan

a significant portion of the execution time, yet loading the meshes shows nearly perfect scaling up to 512 cores. At this relatively high core count, the load time is dominated by the load balance process, which is relatively independent of the number of cores (0.1 seconds). Finally, saving the results of the simulation (Save Results) has an acceptable scalability. The results are saved to multiple SILO files (one per core), with a single summary file. For small core counts, the Save Results is dominated by the time to write the data. For large core counts, Save Results is dominated by the time to open a file on the Lustre file system. This is approximately constant for all cores, but has a large variation that depends on the load of the computer. Several executions were made and the typical results are presented. The time to open a file varied between 0.02 and 1 second. Large executions are particularly sensitive to this effect because all cores must synchronize to write the summary file. Figure 11 shows the parallel efficiency on EOS and Titan. TS , where TS is the serial wall clock time, The efficiency is calculated as PN TP PN is the number of processors, and TP is the parallel wall clock time with PN processes. Note that for processor counts between 4 and 512, the speedup is greater than 1. This is due to the super-linear speedup discussed previously.
The serial wall clock time TS for Titan is not available due to maximum wall clock time restrictions. TS on Titan is estimated by assuming that the efficiency on 2 processors of Titan and EOS is similar. This assumption is justified by the data in Tables 5 and 6.
1

34

6.4.2. Weak Scaling Tables 7 and 8 shows the results of a weak scaling study performed on EOS and Titan. In the weak scaling study, the base mesh from the strong scaling study was used with 64 cores and the resolution was increased in each direction by a factor of 2x and 4x. This resulted in a set of executions of 1.6M unknowns on 64 cores, 12.8M unknowns on 512 cores, and 102M unknowns on 4096 cores. All times are in seconds and for functions that are called multiple times per solve the time per call is included in parentheses. Using weak scaling, varying the problem size and processor count by 64 times, the performance of the solve is approximately constant. The total number of non-linear iterations was independent of the resolution, and the total number of linear iterations varies slightly with resolution. This is most likely due to a slight degradation in the performance of the parallel smoother used within the algebraic multigrid solver. The solve time is approximately constant with some variation that is due to differences in the number of linear iterations. The contributions to the solve are specified in terms of the total time and the time per iteration in parentheses. Based on the time per iteration, the global apply, finite element diffusion operator apply, and resetting of Trilinos ML are all approximately constant. The apply call for the Robin boundary condition decreases slightly with problem size as the ratio of the total number of unknowns on the surface compared to the total number of unknowns decreases slowly with problem size. The variation in mapping between domains is primarily due to specific parallel decomposition and the variation in MPI performance on Titan and EOS that depend on the allocated nodes. Loading and saving the meshes show a slight increase for large problems due to increased demand on the Lustre parallel file system used but represents a small fraction of the total run time (2-3%). 6.4.3. Preconditioner Performance As mentioned in the introduction section the number of pellets within a fuel rod can vary dramatically. Hence it is important that the solver deliver good performance as the number of actual pellet domains is varied. This in turn is dependent primarily on the performance of the preconditioner employed. In order to study this we consider a series of numerical experiments where the total number of mesh elements across all pellet and clad domains is kept constant while the number of pellet domains is varied. We note that though the number of mesh elements is kept constant the number of degrees of freedom does rise slightly (3%) as the number of pellets is increased due 35

Core Count Degrees of Freedom Nonlinear iterations Linear iterations Solve Nonlinear Residual Diffusion Apply Boundary Conditions Map Apply Reset ML Mesh Loading Save Results

1x 64 1.6M 5 29 38.14 21.10 (0.73) 12.97 (0.45) 3.46 (0.12) 2.46 (0.08) 13.05 (0.45) 0.34 0.42

2x 512 12.8M 5 31 35.29 19.24 (0.62) 14.04 (0.45) 2.00 (0.06) 1.60 (0.05) 11.22 (0.26) 0.28 0.35

4x 4096 102M 5 36 49.56 26.91 (0.75) 17.12 (0.48) 1.81 (0.05) 5.97 (0.17) 12.74 (0.35) 0.42 1.11

Table 7: Weak scaling studies on EOS 1x 64 1.6M 5 29 88.99 43.66 (1.51) 25.55 (0.88) 7.57 (0.26) 5.74 (0.20) 29.27 (1.01) 0.72 1.06 2x 512 12.8M 5 31 77.94 38.10 (1.23) 27.51 (0.89) 4.30 (0.14) 3.05 (0.10) 25.09 (0.81) 0.59 0.72 4x 4096 102M 5 36 107.19 55.69 (1.55) 37.12 (1.03) 4.09 (0.11) 10.22 (0.28) 30.54 (0.85) 1.25 1.47

Core Count Degrees of Freedom Nonlinear iterations Linear iterations Solve Nonlinear Residual Diffusion Apply Boundary Conditions Map Apply Reset ML Mesh Loading Save Results

Table 8: Weak scaling studies on Titan to the introduction of more surface elements. In addition the number of solution transfer operations that require communication between domains also increases. Note that the additional communication incurred is point to point communication between pairs of processors and does not significantly affect the runtime. The dimensions of the pellets are chosen so that the height of the pellet stack matches that of the clad. Figure 12 plots the 36

Domains Solve

1 50.61

2 50.86

4 50.61

8 50.51

16 50.81

32 49.63

64 49.56

128 50.26

256 51.38

Table 9: Solution times as the number of pellet domains is varied number of nonlinear and linear iterations required for solving thermal fuel rod problems as the number of pellet domains is varied from 1 through 256. In addition the number of multigrid solves required within the preconditioner is varied from 1 to 3. As can be seen, the number of iterations remains largely constant. Figures 13(a) and 13(b) show the residual convergence history for the nonlinear solver as the number of domains is varied ranging from 1-256 domains. There is only a very slight effect on the residual due to increasing the number of domains. Finally, Table 9 shows that the required wall clock times for solution as the number of domains is varied does not change significantly.
20 Number of Iterations 15 10 5 0
Nonlinear (1 ML) Nonlinear (3 ML) Linear (1 ML) Linear (3 ML)

1

2

4

8 16 32 64 Number of Pellet Domains

128

256

Figure 12: Number of nonlinear and linear iterations as a function of the number of pellet domains and the number of algebraic multigrid (ML) iterations

7. Fuel Assembly Modeling In section 5 the components of the AMP multi-physics infrastructure that enabled the development of nonlinearly consistent multi-domain thermal transport calculations that form the main core of this paper were described. Here we illustrate further multi-physics capabilities of AMP by describing 37

102 Residual norm 100 10-2 10-4 10-6 10-8 1

Residual norm

NP=256 NP=128 NP=64 NP=32 NP=16 NP=8 NP=4 NP=2 NP=1

102 100 10-2 10-4 10-6 10-8

NP=256 NP=128 NP=64 NP=32 NP=16 NP=8 NP=4 NP=2 NP=1

2 3 4 Nonlinear iteration number

5

1

2 3 4 Nonlinear iteration number

5

(a) 1 ML iteration

(b) 3 ML iterations

Figure 13: Residual convergence history for fuel rods with NP=1 to NP=256 pellet subdomains with 1 and 3 multigrid iterations per subdomain within the preconditioner further extensions of the fuel rod modeling capability. Since our focus is on solution and coupling methodology and due to space limitations we will concentrate on the relevant coupling aspects with details on the models being provided in the appendices and provided references. Coupling to Coolant Models: The coolant liquid flowing on the outside of each fuel rod serves as a heat sink which flows axially along the length of the outside surface of the clad. The coolant model in the fluid domain and the thermal transport model in the clad domain are coupled nonlinearly through Robin boundary conditions (Eqn 2.7)
m m kc (Tc )Tc · nf + hc,f (Tc , Tf )(Tc - Tf ) = 0 on c,f m reproduced here for clarity. Here, Tf represents a mapped coolant temperature field over the clad surface. AMP enables us to couple different models interchangeably. The first model described in Appendix A.1 uses a reduced empirical model that solves a single axial equation using a simple finite difference scheme and is frequently sufficient for many calculations. This is the model used within the single fuel rod calculations presented in prior sections. The second model described in Appendix A.2 solves the two equations using a more complex model that is used when subchannel temperatures and densities are required. This model is used in the fuel assembly calculations

38

that we will describe further along. Let
h )=0 Ff (Tf

(7.1)

denote the nonlinear system of equations resulting from discretizing either h coolant model over the fluid domain with Tf a vector of fluid temperature unknowns. As described in Section 4.1, a JFNK solver only requires us to provide the ability to compute a nonlinear residual. Hence, augmenting the existing nonlinear system (3.6) of equations across the pellet and clad domains with eqns (7.1) for the flow domain enables us to perform coupled flow and thermal transport calculations. The augmented nonlinear system can be denoted by: ~ (T) = 0 F (7.2) t h h h h ~ (T) = F1 (T1 where F ), F2 (T2 ), . . . , FN (TN ), Fc (Tch ), Ff (Tf ) is the coupled set of block nonlinear equations across all pellet, clad, and flow domains with T denotes the block unknowns across all domains including flow. For preconditioning an augmented approximate Jacobian system of the form ~ = M 0 M ~f f 0 J (7.3)

~f f an approximate Jacobian for is used with M as defined in Eqn (4.7) and J the flow model. Since reduced order models are used for the flow, systems ~f f are inverted using a direct solver as opposed to the multigrid involving J solvers used to invert the other components. We note that the direct solver for the flow operates in parallel to the multigrid solves during each preconditioner solve step. Coupling to Oxide Growth Models: The formation of an oxide layer on the surface of a nuclear fuel rod can interfere with thermal conduction to the coolant and create additional stress within the clad. Typically the thickness of the oxide layer is much smaller than the other physical scales in the fuel rod. As a result the oxide growth is modeled at each point, x, on the surface of the clad by independent 1D models of the form: C (x, t)  C (x, t) = D ( x) t x x (7.4)

Solving for the oxygen concentration then reduces to solving equation 7.4 at each point on the surface of the clad subject to appropriate initial and boundary conditions which is described in detail in Appendix C. 39

For the purposes of this paper we consider a one way coupling of the full thermal transport model to the oxide models. As described previously a nonlinear solve is performed to convergence over the pellets, clad, and coolant domains. The resulting temperature field is used to initialize the oxide growth models at each point on an exterior clad surface mesh generated from the clad mesh using the mesh subset capability of AMP. Each point on the mesh contains an independent sub-grid oxide model which is distributed across the processors to match the clad load balance. For the results presented the clad runs on approximately half of the processors, with roughly the same number of surface nodes on each processor. Additionally since we have an independent model for every point the problem is embarrassingly parallel between the points and no additional communication is required. The results of the oxide model coupled to the thermal model is shown in Figure 14. The oxide layer thickness follows the surface temperature of the clad. This temperature is in turn affected by the power shape which can be seen by the fuel temperature and the flow temperature which creates a top-shifted peak to the temperature of the clad and oxide layer thickness. Coupled Radiation Transport and Fuel Assembly Thermal Modeling: In this section we describe the extension of the fuel rod modeling capability described to modeling a full nuclear assembly and coupling with a massively parallel radiation transport code, Denovo [29]. Here we focus on describing the coupling and scaling aspects and the interested reader is referred to [26, 27, 30­32] for further details including detailed verification and validation studies. Solving the fuel assembly problem consists of two parts: solving the neutronics equations to obtain the spatially varying source and solving the thermal diffusion with the appropriate heat sink. To accomplish this we solve the assembly level radiation transport equations using the methods described in section Appendix B, and an array of multiple fuel rod problems each of which is solved as described in this document. To accomplish the latter, we utilize a multi-mesh capability that can replicate a mesh to produce the appropriate array, and replicated column operators to produce a complete nonlinear system. The full nonlinear system is then solved using JFNK, with the preconditioner limited to the individual sub domains. The individual fuel rod physics are embarrassingly parallel between the fuel rods which we utilize through our load balance by generating a set of independent commu40

Figure 14: Oxide model results from left to right: Nuclear fuel pellet temperatures; Clad outer surface temperature; Coolant temperature; Resulting oxide layer thickness. nicators for each fuel rod, with the fuel rods distributed on independent sets of processors. We are not limited to this choice of parallel decomposition, but it yields the best performance for this problem. To demonstrate the ability to solve nuclear reactor analysis problems, we model a single fuel assembly of a pressurized water reactor with coupling between heat transfer, subchannel flow, and radiation transport. This corresponds to CASL AMA Progression Problem 6 [33]. The assembly consists of a 17 × 17 array of fuel rods on a square 1.26 cm pitch. Of the 289 pins, 264 are fuel rods containing 3.1% enriched UO2 , 24 are guide tube locations, and a central instrumentation tube. Zircaloy 4 clad surrounds all pins. The coolant surrounding the pins is water containing 1300 ppm soluble boron

41

and an inlet temperature of 569 K. The average power level in the assembly is 30,000 W/kg, approximately corresponding to an average power assembly from a full reactor core. Further details on the geometry and material specifications can be found in [33]. In the AMP computational model, the mesh for each fuel pellet contains 512 mesh cells each fuel rod contains 360 pellets. Additionally, each fuel rod is surrounded by a clad mesh containing 54,144 cells for a total of 238,464 cells per fuel rod. Over the 264 fuel pins in the full assembly, the total number of mesh cells in the AMP problem is approximately 63 million and the number of nodal degrees of freedom is slightly over 100 million. The Denovo computational model has approximately 4.6 million spatial cells, 23 energy groups, 32 angles, and P1 scattering (which uses four angular moments). The 23 group cross sections are collapsed from a 56 group library by the XSProc module of the SCALE package [34]. Power distributions computed by Denovo are mapped onto the AMP mesh using the polynomial smoothing process described in Appendix B. Temperatures and fluid densities computed by AMP are averaged over each of 49 axial levels within every fuel rod to be used for generating new cross sections. A simple Picard iteration is used to couple AMP and Denovo, with a damping (under-relaxation) factor of 0.4 applied to the temperature distribution, as described in Ref. [32]. The AMP thermal and subchannel problems are solved together using a JFNK approach as described in Appendix A and the Denovo k -eigenvalue problem is solved using a Krylov-Schur eigensolver [35]. A stopping criteria of 10-4 is applied to the Picard iterations and a tolerance of 10-5 for each of the AMP and Denovo subproblems. The problem was decomposed across 4624 computational cores, with both the AMP and Denovo problems utilizing the entire set of processors. The coupled problem converged in 12 Picard iterations, with an average of 1.6 Newton iterations per Picard iteration and 20 linear iterations per Newton step. The entire solution required 3976 seconds, of which 3182 seconds were spent in the Denovo transport solves and 680 seconds were spent in the AMP thermal solves. The temperature and power solution profiles throughout the assembly are shown in Fig. 15. The radial variation of the power distribution, both within individual fuel rods and across the assembly, is evident. Notably, the power level in pins neighboring guide tube locations is significantly higher than regions not near guide tubes (such as the assembly corners) due to increased neutron moderation. Although more difficult to visually discern, the fuel temperature distributions mirrors the same general trends as the 42

power, with higher temperatures corresponding to high power regions. Unlike the power, which attains its peak values at the outer radius of the fuel rods, the temperature distribution always peaks at or near the center of a fuel rod. The axial profiles clearly show the presence of the spacer grids as a series of local depressions due to increased absorption in those regions. 8. Conclusions Many real world engineering problems involve the complex interaction between many bodies, in a nonlinear manner. From mesh generation to predicting results, modeling these large complex systems presents significant computational challenges. An efficient parallel, multi-domain solution methodology has been developed and implemented to solve these systems by leveraging the natural decomposition of the problem associated with the individual domains. This methodology has been demonstrated for modeling heat transfer within nuclear fuel rods, which are composed of hundreds of individual pellets within a metal tube, and can be applied to a nuclear reactor, which is composed of tens of thousands of individual fuel pins. The model and discretization for the thermal transport in a nuclear fuel rod demonstration problem consists of nonlinear diffusion in each of the fuel pellets and clad, along with a Robin boundary condition on each surface, and maps between them, to account for the heat transfer between domains. Modeling the entire system in a single domain would create a significant challenge associated with mesh generation, but by modeling many individual domains, the generation of the mesh for the full problem becomes automatic and a negligible burden on the user. The Jacobian-Free Newton-Krylov (JFNK) method used to solve the nonlinear system of algebraic equations was described in detail, with a particular focus on the preconditioning strategy for the multi-domain problem. The Krylov solver has been shown to efficiently solve for the interaction between the individual domains, but an efficient preconditioner was required to account for the diffusion within individual domains. Therefore, the preconditioning algorithm leverages the natural decomposition in physical space through the use of a parallel block-diagonal structure in which each block is a single, physical domain that neglects the interaction between domains. To efficiently solve these multi-domain problems, the computational infrastructure of AMP was designed to support the problem specification, domain decomposition, and composition of mathematical constructs efficiently 43

(a) Radial Power Profile

(b) Radial Temperature Profile

(c) Axial Power Profile

(d) Axial Fuel Temperature Profile

(e) Clad and Coolant Axial Profiles

(f) 3D Temperature Profile

Figure 15: Assembly Solution Details 44

in parallel. The computational infrastructure was described in detail, with a particular focus on the parallel implementation. The infrastructure is designed to allow for the specification of nonlinear operators and linear preconditioners for individual domains and the composition of those operators into a single column operator that can be used by a solver, either internal or external, to compute the solution of the nonlinear problem. Similarly, the parallel vectors (solution and source) can be defined for individual domains and composed into a single vector that is used by the solvers and preconditioners. A "subsetting" approach has been developed to access individual components of the parallel vector that are associated with a single mesh (or surface of a mesh) and variable (such as temperature). Because there is a knowledge of the individual physical domains, parallel communicators are created for each domain and the interacting surfaces between domains. Therefore, the infrastructure can easily "subset" the parallel vector associated with an individual domain, determine a communicator specific to that domain, and access the preconditioned for that domain, which is provided to a solver for the compact preconditioning step on an individual domain. The domain decomposition strategy is designed with an awareness of the individual domains to minimize communication during the computationally-expensive preconditioning and apply processes. A series of numerical experiments were developed to verify, validate, and evaluate the parallel performance of the software and algorithm. Each of the numerical experiments were based on a single specification of material properties and geometry that was associated with a nuclear fuel performance experiment that is used as an international benchmark for fuel performance modeling. The numerical verification studies used the method of manufactured solutions and mesh refinement studies to verify the solution converged with the proper order to the manufactured solution. The validation study, specified in much greater detail in an associated manuscript, demonstrated that the full problem, from material model specification to discretization and solution, could accurately predict the temperature distribution within a nuclear fuel rod within the experimental uncertainty bounds. Both strong and weak scaling studies were performed with significant profiling, to understand the performance of the software, and algorithm. Excellent strong scaling was achieved as the processor count increased three orders of magnitude, until the number of elements per core was under 1000. This solution strategy, and the associated software, has been shown to accurately and efficiently solve large, complex, multi-domain, nonlinear prob45

lems by leveraging the natural structure of the physical domains. This has the potential to impact many computational engineering applications beyond nuclear fuel simulation. Acknowledgements and Access The AMP (Advanced Multi-Physics) code is distributed with a modified BSD license and accessible either by contacting the corresponding author or through the Radiation Safety Information Computational Center (RSICC) at Oak Ridge National Laboratory, with an RSICC license, as CCC793. The development of AMP, and the nuclear fuel performance application built upon it, was funded by the Nuclear Energy Advanced Modeling and Simulation (NEAMS) program of the U.S. Department of Energy Office of Nuclear Energy, Advanced Modeling and Simulation Office. This material is also based upon work supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program, Extreme Scale Solvers project (EASIR). Much appreciation is due to Aaron Phillippe, Jim Banfield, and Larry Ott for validation studies, to William Cochran, Srdjan Simunovic, Jay Billings, and Phani Nukala for their early contributions to the software, and to Mark Baird for computational support with the third party libraries. This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725. Mark Berrill acknowledges support from the Eugene P. Wigner Fellowship at Oak Ridge National Laboratory, managed by UTBattelle, LLC, for the U.S. Department of Energy under Contract DE-AC0500OR22725. References [1] http://energy.gov/sites/prod/files/styles/borealis photo gallery large respondxl2/public/fuel assembly for production of nuclear power.jpg. [2] G. A. Berna, C. E. Beyer, K. L. Davis, D. D. Lanning, "FRAPCON-3: A computer code for the calculation of steady-state, thermal-mechanical behavior of oxide fuel rods for high burnup", Tech. Rep. NUREG/CR6534, Pacific Northwest National Laboratory (1997). 46

[3] W. F. Lyon, M. N. Jahingir, R. O. Montgomery, "Fuel analysis and licensing code: FALCON MOD01: Volume 3: Verification and validation", Tech. Rep. 1011309, EPRI, Palo Alto, CA (2004). [4] C. Newman, G. Hansen, D. Gaston, "Three dimensional coupled simulation of thermomechanics, heat, and oxygen diffusion in UO2 nuclear fuel rods", Journal of Nuclear Materials. [5] G. Thouyenin, B. Michal, J. Sercombe, D. Planca, "Multidimensional modeling of a ramp test with the PWR fuel performance code ALCYONE", in: Proceedings of Top Fuel 2007, San Francisco, CA, 2007. [6] G. Thouyenin, J. M. Ricaud, D. Planca, P. Thevenin, "ALCYONE: the Pleiades fuel performance code dedicated to multidimensional PWR studies", in: Proceedings of Top Fuel 2006, Salamanaca, Spain, 2006. [7] A. Marino, G. Demarco, D. Brasnarof, F. Florido, "A 3D behavior modeling for design and performance analysis of LWR fuels", in: Proceedings of Top Fuel 2007, San Francisco, CA, 2007. [8] D. A. Knoll, D. E. Keyes, "Jacobian-free Newton-Krylov methods: a survey of approaches and applications", J. Comput. Phys. 193 (2004) 357­397. [9] R. S. Dembo, S. C. Eisenstat, T. Steihaug, "Inexact Newton methods", SIAM J. Numer. Anal. 19 (1982) 400­408. [10] S. C. Eisenstat, H. F. Walker, "Globally convergent inexact Newton methods", SIAM J. Optimization 4 (1994) 393­422. [11] C. T. Kelley, "Iterative methods for linear and nonlinear equations", SIAM, Philadelphia, 1995. [12] Y. Saad, M. Schultz, "GMRES: a generalized minimal residual algorithm for solving non-symetric linear systems", SIAM J. Sci. Stat. Comput. 7 (1986) 856­869. [13] P. McHugh, D. Knoll, "Comparison of standard and matrix-free implementations of several Newton-Krylov solvers", AIAA Journal 32 (12) (1994) 2394 ­ 2400.

47

[14] D. Knoll, P. McHugh, "Enhanced nonlinear iterative techniques applied to a nonequilibrium plasma flow", SIAM Journal on Scientific Computing 19 (1) (1998) 291 ­ 301. [15] K. T. Clarno, B. Philip, W. K. Cochran, R. S. Sampath, S. Allu, P. Barai, S. Simunovic, L. J. Ott, S. Pannala, P. Nukala, G. A. Dilts, B. Mihaila, C. Unal, G. Yesilyurt, J. H. Lee, J. E. Banfield, G. I. Maldonado, "The AMP (Advanced Multi-Physics) nuclear fuel performance code", Tech. Rep. ORNL/TM-2011/42, Oak Ridge National Laboratory (2011). [16] B. S. Kirk, J. W. Peterson, R. H. Stogner, G. F. Carey, libMesh: A C++ Library for Parallel Adaptive Mesh Refinement/Coarsening Simulations, Engineering with Computers 22 (3­4) (2006) 237­254, http://dx.doi. org/10.1007/s00366-006-0049-3. [17] http://trilinos.sandia.gov/packages/stk. [18] M. Heroux, R. Bartlett, V. H. R. Hoekstra, J. Hu, T. Kolda, R. Lehoucq, K. Long, R. Pawlowski, E. Phipps, A. Salinger, H. Thornquist, R. Tuminaro, J. Willenbring, A. Williams, An Overview of Trilinos, Tech. Rep. SAND2003-2927, Sandia National Laboratories (2003). [19] S. Balay, K. Buschelman, W. D. Gropp, D. Kaushik, M. G. Knepley, L. C. McInnes, B. F. Smith, H. Zhang, "PETSc home page", http: //www.mcs.anl.gov/petsc (2001). [20] https://computation.llnl.gov/casc/sundials/main.html. [21] http://www.oecd-nea.org/science/wprs/fuel/ifpelst.html. [22] http://www.oecd-nea.org/tools/abstract/detail/nea-1488. [23] http://www.oecd-nea.org/tools/abstract/detail/nea-1772. [24] A. Schubert, P. V. Uffelen, J. V. de Laar, C. Walker, W. Haeck, Extension of the transuranus burn-up model, Journal of Nuclear Materials 376 (2008) 1­10. [25] A. Phillippe, "A validation study of the AMP nuclear fuel performance code", Master's thesis, University of Tennessee-Knoxville (2012). 48

[26] A. M. Phillippe, K. T. Clarno, J. E. Banfield, L. J. Ott, B. Philip, M. A. Berrill, R. S. Sampath, S. Allu, S. P. Hamilton, A validation study of pin heat transfer for UO2 fuel based on the IFA-432 experiments, Nuclear Science and Engineering 177 (2014) 000­000. [27] A. M. Phillippe, K. T. Clarno, J. E. Banfield, L. J. Ott, B. Philip, M. A. Berrill, R. S. Sampath, S. Allu, S. P. Hamilton, A validation study of pin heat transfer for MOX fuel based on the IFA-597 experiments, Nuclear Science and Engineering 178 (2014) 171­200. [28] H. Childs, E. S. Brugger, K. S. Bonnell, J. S. Meredith, M. Miller, B. J. Whitlock, N. Max, A contract-based system for large data visualization, in: Proceedings of IEEE Visualization 2005, 2005, pp. 190­198. [29] T. Evans, A. Stafford, R. Slaybaugh, K. Clarno, DENOVO: A new three-dimensional parallel discrete ordinates code in SCALE, Nuclear Technology 171 (2010) 171­200. [30] K. T. Clarno, B. Philip, W. K. Cochran, R. S. Sampath, S. Allu, P. Barai, S. Simunovic, M. A. Berrill, L. J. Ott, S. Pannala, G. A. Dilts, B. Mihaila, G. Yesilyurt, J. H. Lee, J. E. Banfield, The AMP (Advanced MultiPhysics) nuclear fuel performance code, Nuclear Engineering and Design 252 (2012) 108­120. [31] S. Hamilton, K. Clarno, B. Philip, M. Berrill, R. Sampath, S. Allu, Integrated radiation transport and nuclear fuel performance for assemblylevel simulations, in: PHYSOR 2012: Advanced in Reactor Physics, Knoxville, TN, USA, 2012. [32] S. Hamilton, K. Clarno, M. Berrill, T. Evans, R. Sampath, Multiphysics simulations for LWR analysis, in: International Conference on Mathematics and Computational Methods Applied to Nuclear Science & Engineering (M&C 2013), Sun Valley, ID, USA, 2013. [33] S. Palmtag, Coupled single assembly solution with VERA (problem 6), Tech. Rep. CASL-U-2013-0150-000, Consortium for Advanced Simulation of LWR's (2013). [34] SCALE: A comprehensive modeling and simulation suite for nuclear safety analysis and design, Tech. Rep. ORNL/TM-2005/39, Version 6.1, Oak Ridge National Laboratory, Oak Ridge, TN (2011). 49

[35] G. W. Stewart, A Krylov­Schur algorithm for large eigenproblems, SIAM Journal on Matrix Analysis and Applications 23 (3) (2001) 601­ 614. [36] D. R. Rector, C. L. Wheeler, N. J. Lombardo, Cobra-sfs (spent fuel storage): A thermal-hydraulic analysis computer code: Volume 1, mathematical models and solution method, Tech. rep., PNL-6049-Vol.1 (1986). [37] E. E. Lewis, W. F. Miller, Jr., Computational Methods of Neutron Transport, American Nuclear Society, Inc., La Grange Park, Illinois, USA, 1993. [38] J. Jarrell, T. Evans, G. Davidson, A. Godfrey, Full core reactor analysis: Running Denovo on Jaguar, Nuclear Science and Engineering 175 (3) (2013) 283­291. [39] J. V. Cuthcart, et. al., Zirconium metal-water oxidation kinetics iv. reaction rate studies, Tech. rep., ORNL/NUREG-1 (1977). [40] R. Perkins, The diffusion of oxygen in oxygen stabilized alpha-zirconium and zircaloy-4, Journal of Nuclear Materials 73 (1978) 20­29. Appendix A. Coolant Modeling: The coolant liquid serves as a heat sink which flows axially along the length of the outside of the clad. Therefore simulating the coolant flow serves as a boundary condition for the thermal solve on the clad surface. Solving the coolant flow involves solving the fluid equations for conservation of mass, momentum, and energy equations:  +  · (v ) = 0 t vi = - · (vi v ) + (-p +  ·  ) - g t U +  · (U v ) = -p · v +  + k (T )T + q  t (A.1) (A.2) (A.3)

where  is the mass density, v is the velocity, p is the pressure, g is the force exerted by gravity,  is the viscosity tensor, U is the internal energy density,  is the dissipation function, k is the thermal conductivity, and q  is the thermal 50

source. The computational resources needed for a full 3D flow calculations are significant and are usually not necessary for an accurate calculation of the thermal solution within the pellets and clad. This allows us to use a two-equation approximation in which we assume that the coolant flow is only in the z-direction and neglect thermal diffusion between the channels. Assuming steady-state this reduces to: v =0 (A.4) z v 2 p + +g =0 (A.5) z z v  U v T = -p +  k (T ) +q  (A.6) z z z z We have two different models for solving these equations that can be used interchangeably. The first model described in Appendix A.1 uses a reduced empirical model that solves a single axial equation using a simple finite difference scheme and is frequently sufficient for many calculations within the nuclear engineering community. The second model described in Appendix A.2 is a more complex two equation model that is used when subchannel temperatures and densities are required. Appendix A.1. Single EquationFlow A standard reduced uniaxial model based on conservation of energy in the coolant (Equation A.7) with a given mass flux (G) and a specific heat capacity at constant pressure (Cp ) is employed. dTf + hf Tf = hf Tc , (A.7) GCp dz where · is the integral of · over the heated perimeter of the outer surface of the clad and Tf is the bulk coolant temperature. The conservation of coolant energy is solved on a 1D domain using a finite difference scheme. This 1D domain is divided into N equidistant grid points leading to a set of coupled equations with the i-th equation given by 4hf (z ) i- 1 i Tf - Tf + (T i - < Tci >)dz = 0 (A.8) i Cp (Tf )GDe f where hf is the Dittus-Boelter [2] film conductance given by hf = (0.023k/De )Re0.8 P r0.4 with Re the Reynolds number and P r the Prandtl number. 51 (A.9)

Appendix A.2. Subchannel Equations In this section we describe the model that approximates the distribution of flow, pressure, and temperature within a channel (the space between adjacent fuel rods) as uniaxial in the vertical direction and utilizes empirically-derived friction factors to account for the additional complexities. In our subchannel model we solve the 1D set of 2 equations described in section Appendix A. Solving these equations require two independent variables per grid point and we choose enthalpy and pressure. Note that the internal energy density is related to the enthalpy density through U = h - p. v p + v +g =0 z z hv p  T =v + k (T ) z z z z

(A.10) +q  (A.11)

At each axial layer, a simplified model where the crossflow terms are neglected thus eliminating the conservation of mass and lateral momentum equations is employed. This results in conservation of energy and axial momentum equations with specific enthalpy and pressure as variables using complex material models for the temperature, specific enthalpy, density, specific heat capacity, thermal conductivity, viscosity, and surface tension [36]. The following finite difference form of conservation of energy and axial momentum equation are given: m(hi,j + - hi,j - ) - zj + zj
k  K ( i)

(1 + i,r )Prheat i,r qc,j
r  R ( i)

t  wk,j (h i,j - hn,j ) + zj kK (i)

Ck,j sk (Ti,j - Tn,j ) = 0. (A.12)

m(ui,j + - ui,j - ) + Ai (pi,j + - pi,j - ) + + 1 2Ai

gAi zj cos   i,j
t wk,j (ui,j - un,j ) = 0. kK (i)

zj fi,j  + i,j |m| mi,j + C t zj Di

(A.13) 52

The heat flux qc,j is computed using a convective heat transfer coefficient hconv and the temperature difference between the clad surface and the i,j temperature of the flow in the center of the subchannel:
j qc,j = hconv i,j (< Tc > -Ti,j ).

(A.14)

The convective heat transfer coefficient is related to the Nusselt number N ui,j : hconv i,j Di N ui,j = (A.15) ki,j The Nusselt number will vary depending on the turbulence of the flow, so different models were developed for laminar and turbulent flow. To avoid convergence issues due to the discontinuity between the models, the effective heat transfer coefficient is taken as the maximum of the laminar heat transfer coefficient hi,j and the turbulent heat transfer coefficient ht i,j : hconv = max(hi,j , ht i,j i,j ), (A.16)

where the laminar heat transfer coefficient is evaluated with N ui,j = 8.0: hi,j = 8.0 ki,j , Di (A.17)

and the turbulent heat transfer coefficient is calculated as
0.8 0 .4 ht i,j = 0.023Rei,j P ri,j

ki,j Di

,

(A.18)

which uses the well-known Dittus-Boelter correlation for Nusselt number [A.9]: 0.8 0.4 N ui,j = 0.023Rei,j P ri,j , (A.19) where the Reynolds number and Prandtl number have their usual definitions: Rei,j cp i,j ui,j Di i,j µi,j = , P ri,j = µi,j ki,j (A.20)

As boundary conditions, axial inlet mass flow rates, inlet temperature and outlet pressure are selected.

53

Appendix B. Radiation Transport Model Nuclear fuel simulation requires a heat source and a heat sink. The heat sink is approximated with a closed-channel coolant flow model and the heat source is generated within an operating nuclear reactor primarily as a result of neutron-induced fission in fuel (primarily Uranium-235 in most reactors). The distribution of neutrons distribution requires the solution to the Boltzmann transport equation [37]. This is most often modeled using the k -eigenvalue form of the Boltzmann transport equation given by ^ ·  ( ^ , E ) +  (E ) ( ^ , E) 


=
0

dE
4 

^ s ( ^  ^ , E  E ) ( ^ ,E ) d dE
0 4

(B.1)

1 + (E ) k

^ f (E ) ( ^ ,E ) , d

^ is the direction of particle travel; E is the particle energy;  is the where  angular flux distribution; k is the multiplication factor of the system;  , s , and f are the total, scattering, and fission cross sections, respectively;  is the fission energy spectrum; and  is the number of neutrons produced per fission. For a given flux distribution, the power distribution can be written as  ^ f (E ) ( ^ ,E ) , P = dE d (B.2)
0 4

where  is the energy release per fission event. The Denovo radiation transport code [29] offers a variety of spatial discretizations solving the discrete ordinates [37] form of Eq. B.1 in parallel on Cartesian meshes. Denovo has demonstrated excellent scalability to high performance computing environments [38]. Nuclear data is generated using the XSProc module of the SCALE package [34]. Two approaches are available for transferring the power distribution from Denovo to AMP are possible. The first is a direct point-wise mapping of the Denovo solution onto the Gauss points of the AMP finite element basis functions. Interpolation to a point within Denovo can be piecewise constant, linear, or tri-linear depending on the spatial discretization used. The second approach uses a polynomial expansion of the power distribution within each cylindrical fuel rod (Zernike polynomials in the x - y plane and Legendre polynomials in the axial direction) to smooth out artifacts of the Denovo 54

Cartesian mesh, allowing a coarser spatial mesh to be used [31]. In both cases, conservation of the globally integrated power is enforced by normalizing the distribution before and after mapping the power onto the AMP mesh. Appendix C. Oxide Model: The formation of an oxide layer on the surface of a nuclear fuel rod can interfere with thermal conduction to the coolant and create additional stress within the clad. As shown in Figure C.16, the material regions of interest can be divided into 4 regions, a coolant region which is the source of the oxygen for oxide growth, the oxide layer itself which consists of Zirconium oxide, an oxygen rich alpha phase, and a normal beta phase region [39].

Coolant

Oxide

alpha

beta

Figure C.16: Sample layers for oxide growth

Within each layer the oxygen concentration is governed by thermal diffusion. Since the thickness of the oxide and alpha layers is much smaller than the other physical scales in the fuel rod, the oxide growth is modeled at each point, x, on the surface of the clad by independent 1D models:  C (x, t) C (x, t) = D ( x) t x x (C.1)

Solving for the oxygen concentration then reduces to solving in each region subject to the appropriate boundary conditions. Note that the boundaries between the different phases move as the different layers grow. Oxide layer growth: The oxide layers are moving domains in which the growth of each layer is given by the difference between the oxygen flux between the layers. Associated with each interface is the associated velocity of that interface. For example, the velocity of the oxide-alpha layer vox, is given by: vox, = Jox (xox, ) - J (xox, ) Cox, - C,ox 55 (C.2)

where Jox (xox, ) and J (xox, ) are the flux of oxygen in the oxide and  layers evaluated at the oxide interface and Cox, and C,ox are the oxygen concentrations at the boundaries of the oxide-alpha interface in the oxide and alpha layers. Boundary conditions: The boundary conditions between the different layers are relatively simple. At each interface, the oxygen concentration can be evaluated using the equilibrium value for the given phase. For example, at the oxide-coolant interface, the oxygen concentration is assumed to be 1.511 g/cm3 , derived from stoichiometry. The oxygen concentration in the oxide at the oxide-alpha interface is modeled by C = 1.517 - 7.5  10-5  T [39]. The oxygen concentration in the alpha layer at the alpha-oxide interface is a fixed 29% (atomic density) or 0.45 37 g/cm3 [40]. The oxygen concentration in the alpha layer at the alphabeta interface is calculated using the equilibrium concentration. This can be expressed as [40]: C = -0.2263 + 0.0649  C=0 T - 16.877 63.385 T >= 1123 K otherwise

Discretization: Since we are primarily conserned with the size of the oxide layers, we choose to solve the differential equation using finite difference in a moving frame.  dC (x, t) C (x, t) C (x, t) = (C.3) D ( x) +v· dt x x x We divide the space into N uniformly spaced regions of size h = (xN - x0 )/N . Since each zone is moving at a velocity vi , we can apply the convective derivative to get the diffusion equation in this moving frame. We can follow the boundaries of the oxide layer by choosing the velocity at the boundaries to match the oxide growth rate. For example consider the layers shown in Figure C.17. It's left boundary is moving at a velocity of v1 and it's right boundary is moving at a velocity of v2 . We want to use a conservative scheme with upwinding for the convective term. Assuming v1+1/2  0: 1 Di+1 Ci+ 3 - Ci+ 1 - Di Ci+ 1 - Ci- 1 2 2 2 2 dt h2 1 i+ 2 (vN - v0 ) vi+ 1 = v0 + 2 N
2

dCi+ 1

=

+

vi+ 1 h

2

C i+ 3 - C i + 1
2

2

56

Ci - 1
2

Ci + 1 vi

2

C i+ 3 vi+1

2

v0

vN

Figure C.17: Sample layers for numerical form

Rewriting: dC = F (C, v, h) dt 1 F (C, v, h) = 2 Di+1 Ci+ 3 - Ci+ 1 - Di Ci+ 1 - Ci- 1 2 2 2 2 h We can then apply the Crank-Nicholson method: C n+1 - C n 1 = F C n+1 , v n+1 , hn+1 + F (C n , v n , hn ) t 2 +1 +1 hn+1 = xn - xn /N 0 N
+1 n n+1 n xn = xn - v0 t2 0 0 + v0 t + 0.5 v0 +1 n+1 n n xn = xn - vN t2 N + vN t + 0.5 vN N

+

vi+ 1 h

2

C i+ 3 - C i+ 1
2 2

If we assume v n+1 is known (it is actually calculated from C n+1 ), then the system becomes a standard linear system of equations. The resulting matrix is banded and can be solved through direct solves using LAPACK. Time-Step Control: Careful control of the time step is necessary to produce an accurate answer. A first limitation of the time step is the calculation of v n+1 . While it is possible to create a non-linear system that solves for v n+1 , the other time step requirements will make this work unnecessary. Instead we will assume v n+1  v n . With this assumption we need a time step that will ensure the change in v is small. The second restriction comes from the convective term. To ensure the proper error, we need to limit the matrix h . norm (for this term) to  1. This gives us the condition t  v i 57

