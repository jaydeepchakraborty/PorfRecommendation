Washington University School of Medicine

Digital Commons@Becker
Open Access Publications

2015

WormBase 2016: Expanding to enable helminth genomic research
Tim Schedl
Washington University School of Medicine in St. Louis

et al

Follow this and additional works at: http://digitalcommons.wustl.edu/open_access_pubs Recommended Citation
Schedl, Tim and et al, ,"WormBase 2016: Expanding to enable helminth genomic research." Nucleic Acids Research.,. 1-7. (2015). http://digitalcommons.wustl.edu/open_access_pubs/4394

This Open Access Publication is brought to you for free and open access by Digital Commons@Becker. It has been accepted for inclusion in Open Access Publications by an authorized administrator of Digital Commons@Becker. For more information, please contact engeszer@wustl.edu.

Nucleic Acids Research Advance Access published November 17, 2015
Nucleic Acids Research, 2015 1 doi: 10.1093/nar/gkv1217

WormBase 2016: expanding to enable helminth genomic research
Kevin L. Howe1,* , Bruce J. Bolt1 , Scott Cain2 , Juancarlos Chan3 , Wen J. Chen3 , Paul Davis1 , James Done3 , Thomas Down1 , Sibyl Gao2 , Christian Grove3 , Todd W. Harris2 , Ranjana Kishore3 , Raymond Lee3 , Jane Lomax4 , Yuling Li3 , Hans-Michael Muller3 , Cecilia Nakamura3 , Paulo Nuin2 , Michael Paulini1 , Daniela Raciti3 , Gary Schindelman3 , Eleanor Stanley4 , Mary Ann Tuli3 , Kimberly Van Auken3 , Daniel Wang3 , Xiaodong Wang3 , Gary Williams1 , Adam Wright2 , Karen Yook3 , Matthew Berriman4 , Paul Kersey1 , Tim Schedl5 , Lincoln Stein2 and Paul W. Sternberg2,6
1

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

European Molecular Biology Laboratory, European Bioinformatics Institute, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK, 2 Informatics and Bio-computing Platform, Ontario Institute for Cancer Research, Toronto, ON M5G0A3, Canada, 3 Division of Biology and Biological Engineering 156­29, California Institute of Technology, Pasadena, CA 91125, USA, 4 Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SA, UK, 5 Department of Genetics, Washington University School of Medicine, St. Louis, MO 63110, USA and 6 Howard Hughes Medical Institute, California Institute of Technology, Pasadena, CA 91125, USA

Received September 30, 2015; Revised October 26, 2015; Accepted October 28, 2015

ABSTRACT WormBase (www.wormbase.org) is a central repository for research data on the biology, genetics and genomics of Caenorhabditis elegans and other nematodes. The project has evolved from its original remit to collect and integrate all data for a single species, and now extends to numerous nematodes, ranging from evolutionary comparators of C. elegans to parasitic species that threaten plant, animal and human health. Research activity using C. elegans as a model system is as vibrant as ever, and we have created new tools for community curation in response to the ever-increasing volume and complexity of data. To better allow users to navigate their way through these data, we have made a number of improvements to our main website, including new tools for browsing genomic features and ontology annotations. Finally, we have developed a new portal for parasitic worm genomes. WormBase ParaSite (parasite.wormbase.org) contains all publicly available nematode and platyhelminth annotated genome sequences, and is designed specifically to support helminth genomic research.

INTRODUCTION Nematodes are the most abundant animals on the planet (1), and over 25 000 species have been described (2), displaying a remarkable diversity in terms of size, form, lifestyle, habitat and reproductive mode (3). Caenorhabditis elegans is a free-living nematode that has been used for over 50 years as a model system in experimental biology (4). Its transparency, reproductive mode, small size, rapid generation time, simple nervous system and invariant cell lineage have made it ideally suited to the study of animal genetics [(5); www.wormbook.org]. The mission of WormBase is to facilitate and accelerate biological research using C. elegans by making the collected outputs of the research community accessible from a single resource, and to enable the transfer of this wealth of knowledge to the study of other metazoa, from nematodes to humans. The specific aims of WormBase are to: (i) place nematode data described in the research literature, deposited in the archives, or directly submitted to us into context via a combination of detailed manual curation and semi-automatic data integration; (ii) curate the reference genome sequence, gene structures and other genomic features for a small set of well-studied nematode species, thereby providing a high quality foundation for downstream studies and (iii) develop web displays and tools to allow users to easily visualize and query these data.

* To

whom correspondence should be addressed. Tel: +44 1223 494417; Fax: +44 1223 494 468; Email: kevin.howe@wormbase.org

C The Author(s) 2015. Published by Oxford University Press on behalf of Nucleic Acids Research. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

2 Nucleic Acids Research, 2015

CURATION STATUS Genomes and annotations for core species WormBase acts as the guardian of the reference genome and annotation for C. elegans and a small number of additional `core' species (www.wormbase.org/species/all), curating gene structures and other sequence features, assigning identifiers to primary annotation elements, and maintaining a complete historical record of all updates. In general, core status is reserved for species with a high-quality (chromosome-level) reference genome and identifiable research community that have expressed a desire for continued improvement of the genome sequence and annotation. We previously reported on the adoption of the filarial parasitic nematode Brugia malayi as a WormBase core species, and described a concerted effort to annotate its genome to high quality using a combination of modern automated methods and manual curation (6). Since then, we have added two key parasitic nematodes to the core set: Onchocerca volvulus, a causative agent of onchocerciasis (river blindness), and Strongyloides ratti, the rat laboratory analogue of the causative agent of strongyloidiasis (threadworm infection). Both genomes have undergone first pass automatic gene structure annotation similar to that performed for the B. malayi genome, and are currently undergoing targeted manual curation of gene structures. We encourage users to report issues they find with the annotation of these genomes, and are committed to addressing them as a priority. Enabling community curation of the research literature Comprehensive extraction and standardization of data from the C. elegans research literature remains one of the primary goals of the project. We target over 20 specific data types, and each presents its own challenge in terms of volume and ease of extraction. For some data types, such as gene expression patterns, curation keeps pace with the literature. For others, such as the phenotypes of mutant alleles and RNA-interference knockouts, the high volume of both historical publications and more recent high-throughput projects make the goal of keeping up-to-date unachievable with the resources we have. Help from the community is vital to address this shortfall, and we have recently worked on methods to allow researchers to contribute to curation for a small number of specific data types with the biggest backlog. The three data-types that we have targeted are: (i) phenotypes of mutant alleles; (ii) molecular details for mutant alleles and (iii) textual gene summaries. Custom web-forms for each of these types have been designed to make the barrier to participation as low as possible (www.wormbase.org/ about/userguide/submit data). Authors of newly published papers are invited to submit their data via the forms, and researchers are also invited to review and update existing annotations. Submissions received via this system are prioritised for curator review and inclusion in WormBase. The gene summaries referred to above are short paragraphs that provide an overview of the gene and its function, and include information about molecular identity, biological processes and pathways that the gene product partici-

pates in, temporal and spatial expression, and relationships with other genes via interaction or homology. Although WormBase users regard these summaries as highly valuable, the process of writing them is time-consuming. To address this, we have developed new software that automatically transforms structured data on gene function into natural language sentences, which are then collated to form a summary description. For example, the description created for the C. elegans gene tbc-8 (www.wormbase.org/species/ c elegans/gene/WBGene00008018) is: `tbc-8 is an ortholog of human SGSM2 (small G protein signaling modulator 2) and SGSM1 (small G protein signaling modulator 1); tbc-8 is involved in dense core granule maturation; tbc-8 exhibits Rab GTPase binding activity and is predicted to have GTPase activator activity, based on protein domain information; tbc-8 is localized to the Golgi trans cisterna, the early endosome, the Golgi medial cisterna and the cytosol.' The software has allowed us to create several thousand provisional gene summaries for C. elegans and other core species. As well as being useful in their own right, they also act as valuable and convenient starting points for manual revision. WORMBASE PARASITE Beyond C. elegans, the nematode species covered by WormBase fall into one of two categories: free-living relatives of C. elegans; and plant and animal parasitic nematodes. For the former, the research interest is mainly in the area of comparative genomics and evolution of the `reference' nematode C. elegans. The latter, however, have biomedical and agricultural importance and attract research interest in their own right. The primary research goal of parasitologists is to identify ways of controlling the parasite, and as such their desired entry points and common use-cases for WormBase are often distinct from those of scientists doing basic science using C. elegans as a model. Furthermore, the community of scientists working on parasitic worms (helminths) includes those working on platyhelminths (flatworms), which have historically been beyond the scope of WormBase. There have been recent concerted efforts to sequence the genomes of many helminths (e.g. the 50 Helminth Genomes initiative, www.sanger.ac.uk/science/collaboration/50HGP). Most of the genomes released so far are draft quality, and subject to high update frequency. In response to these challenges, we have embarked on a systematic data integration effort for parasitic worms. The results can be viewed in WormBase ParaSite (parasite.wormbase.org), a new sub-portal of WormBase aiming to focus on the use-cases of scientists engaged in helminth genomics. Genomes and annotation We have endeavoured to include all publicly available helminth genomes in WormBase ParaSite. Where multiple independent genome projects exists for the same species (e.g. for Haemonchus contortus (7,8)), and Ascaris suum (9,10)), we have included all. At time of writing we have genomes from 63 nematode species (71 genomes) and 27

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

Nucleic Acids Research, 2015 3

platyhelminth species (28 genomes), and we maintain an up-to-date list of all genomes available through the site (parasite.wormbase.org/species.html). These genome sequences have been collected from a variety of different sources, including the nucleotide archives, project-specific FTP sites, and direct engagement with genome project scientists. In all, 18 different groups were responsible for generating the data. In about half of the cases, we imported previously described or directly submitted gene structure annotations. For the remaining genomes, we used MAKER2 (11) to generate high-quality annotations by integrating evidence from multiple sources: ab initio gene predictions from AUGUSTUS (12), GeneMarkES (13) and SNAP (14); projected annotations from C. elegans and the taxonomically nearest previously-annotated helminth using GenBlastG (15) and RATT (16); and alignments of ESTs, mRNAs and proteins from related organisms. Comparative genomics We have used the Ensembl Compara system (17) to infer evolutionary histories for all helminth genes, supplemented by gene sets from 9 free-living species (including C. elegans) and 12 comparator species (including human and other model organisms). The result is the organisation of around 2.5 million genes into around 150 000 homology groups, each with a protein multiple alignment which is used to infer orthologous and paralogous relationships between the genes. In addition to gene-based comparative analysis, we have also a produced a whole-genome multiple alignment for a subset of the collection, using Progressive Cactus (18,19). This alignment is made available in the form of a Hierarchical Alignment (HAL) file (20), and included in the WormBase UCSC Assembly Hub (see below). More genomes will be added to the alignment in the future. Electronic annotation of gene function The research literature for most of the species in WormBase ParaSite is sparse, although we anticipate that the provision of the genomes will stimulate new research. In lieu of annotations based on experimental evidence, we have used established automated methods to predict the function of as many gene products as possible. First, to predict protein domains, assign terms from the Gene Ontology (GO), and classify the proteins into families, we have used InterProScan (21). Second, we have used the ortholog assignments from the Compara pipeline line to `project' experimentally derived GO annotations and product names from well-annotated species (for example C. elegans and other model organisms). Infrastructure and tools The Ensembl infrastructure (22) is the basis for much of the management, analysis and display of data in WormBase ParaSite. This has allowed us to provide a number of tools developed for the Ensembl project with little or no modification, for example the Variant Effect Predictor (23), and

REST application programming interface (24). Others we have customised for use in WormBase ParaSite. For example, we have modified Ensembl code to provide a sequence search service and BioMart data mining platform (25) that allow the interrogation of all species, or large sub-groups of species (e.g. all nematodes) at once in a single query (see Figure 1). All data for WormBase ParaSite can be downloaded from our FTP site in a new folder (ftp://ftp.wormbase.org/pub/ wormbase/parasite) that uses identical structure and naming conventions to the main FTP site (which now restricts to WormBase core species and genomic data sets from other free-living nematode species). WEBSITE AND TOOLS The main website (www.wormbase.org) continues to focus on the central mission of WormBase, which is to serve scientists using C. elegans as a model system. Since we launched a new version of the site in 2012, we have continued refining the organization and presentation of data with the aim of making more information available at a single glance. Not only does this minimize the amount of mouse clicking required to access common data, it can also provide new insights not possible when examining data on a piecemeal basis. New genome browser Browsing the genome is one of the most common tasks at WormBase, facilitating exploration of the genetic and physical maps, reagents, gene structures, candidate genes for forward genetic screens, and targets for reverse genetics. More recently, the genome browser has become the standard tool for the exploration of large-scale, genome-wide studies of gene expression and functional elements. The current software driving the WormBase genome browser, GBrowse (26), was originally developed for the WormBase site and has now become a mainstay of many model organism database projects (27­31). Designed before the advent of large-scale genomic datasets and the types of queries users expect to levy against them, GBrowse has reached end-of-life development status. After conducting a due-diligence survey of available replacements, we have selected JBrowse (32) from the Generic Model Organism Database project (GMOD, www.gmod.org) to replace GBrowse in WormBase. JBrowse represents a significant advancement over GBrowse, being much faster when browsing large regions, large datasets, or many tracks at concurrently. It also shares many of the user interface elements with GBrowse, providing a shallow learning curve for WormBase users. JBrowse provides a number of features not present in GBrowse. For example, it allows users to view their own data in the browser without requiring the files to be uploaded to the server. Instead, the browser can be pointed to local file or a URL specifying the location of a remote file, and JBrowse will render the data by processing it locally. This is made possible due to the `in browser' execution of JBrowse; all of the software to process the data and display it is included in the JavaScript that is downloaded

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

4 Nucleic Acids Research, 2015

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

Figure 1. Querying multiple species in the WormBase ParaSite BioMart. A taxonomy-tree-based widget allows the selection of whole clades of species for querying, with an auto-complete feature allowing rapid identification of specific species or clades within the tree. The selection of species belonging to the Filarioidea nematode superfamily is shown.

when the user first accesses the tool. Another new feature allows users to perform a degree of in-browser data analysis by combining data in tracks using arithmetic and set operations, for example finding the union, intersection or exclusive or (XOR) of two tracks. Combination tracks can be used as input to other combination tracks, allowing users to build up arbitrarily complex analysis tracks. We have created JBrowse browsers for all core species in WormBase, and these are currently available alongside GBrowse (accessible from the Tools menu, and also as an alternative view on the Location panel on each Gene page). This allows users to migrate to the new tool at their own pace. New ontology browser WormBase annotates genes with terms from established ontologies for anatomy (33) (for spatial expression), disease (34) (for human disease relevance), life stage (for temporal expression) and phenotype (35) (for perturbation outcome), as well as gene function using the Gene Ontology (36). We have added a graphical tool to make it easier for users to navigate between related terms in these ontologies, and to quickly retrieve genes annotated with specific terms. The WormBase Ontology Browser (WOBr) is adapted from

AmiGO 2, which uses Apache Solr to store and index the Gene Ontology and annotations made with it (36). We have generalised the build procedure to allow the loading of other ontologies and annotations, and created an index for each ontology used by WormBase. Building on these indexes, we have developed two interfaces for exploring ontology annotations: (i) a view for each ontology term, which shows the relevant sub-graph of related terms as an interactive tree and (ii) an interactive browser, which allows root-to-leaf navigation of the complete directed acyclic graph for an ontology (see Figure 2).

Data mining platforms The primary data mining platform in WormBase is WormMine, our custom deployment of the InterMine data warehousing software (37). We have recently migrated WormMine to the Amazon Elastic Compute Cloud (AWS EC2), facilitating more frequent updates and greater performance. All WormBase core species are queryable in WormMine. In forthcoming releases we are committed to deepening available data, first to RNA interference experiments and generic access to genomic sequence, and later to all data classes available through WormBase.

Nucleic Acids Research, 2015 5

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

Figure 2. The WormBase Ontology Browser. (A) view of the anatomy term `mechanosensory neuron'. The sub-graph of the term and its ancestors in the hierarchy are depicted graphically to the left. To the right, the inference tree view of the term is shown, with icons indicating relationships in reference to the focus term (P = Part-of, I = Is-a). Numbers to the right are counts of genes annotated with that term (either directly or by inheritance). A list of parallel (sibling) terms to the focus term can also be viewed (bottom right). (B) main entry page for the ontology browser, which allows top-down navigation from the root term for each WormBase ontology.

6 Nucleic Acids Research, 2015

We also provide a separate data mining platform for the WormBase ParaSite portal, using the BioMart technology (38). Whereas WormMine aims to provide access to a rich set of data for a small number of core species, ParaSite BioMart aims to provide simple access to a small set of basic data types (e.g. sequence, orthologs, cross references) for all nematode and platyhelminth genomes, and has been optimised to allow the querying of many species at once (Figure 1).

FUTURE PLANS As we reported previously (6), we are embarked on a major re-design of the back-end infrastructure and curation tools for WormBase. This is a long-term project, but we plan to roll out elements of this in the coming year in a way that will cause no disruption to users. Increasing the utility of WormBase for biomedical research continues to be a priority for the project. We annotate C. elegans genes that have relevance to the study of human disease (42), curating descriptions of the association and forming connections to terms from the Disease Ontology (34) and to disease and gene records in the Online Mendelian Inheritance in Man (OMIM) (43). We plan to take this further by associating mutant alleles in C. elegans with orthologous genomic variants in human, using a combination of programmatic data integration and manual curation. We envision that this will be particularly useful for the study of human variants of unknown significance. On the specific issue of human disease caused by parasitic worms, we plan to augment WormBase ParaSite with additional features for the identification of putative targets for anti-helminthic drugs, for example by linking gene products to the ChEMBL (44) database of medicinal chemistry. ACKNOWLEDGEMENTS After many years working closely with WormBase as the guardian and authority for C. elegans genetic nomenclature, Jonathan Hodgkin has now stepped down from this role. We would like to thank him for his guidance and leadership. FUNDING US National Human Genome Research Institute [U41-HG002223]; UK Medical Research Council [MR/L001220]; UK Biotechnology and Biological Sciences Research Council [BB/K020080 to WormBase]; P.W.S. is an investigator with the Howard Hughes Medical Institute. Funding for open access charge: US National Human Genome Research Institute [U41-HG002223]. Conflict of interest statement. None declared. REFERENCES
1. Platt,H.M. (1994) Foreword. In: Lorenezen,S (ed). The Phylogenetic Systematics of Free-living Nematodes. The Ray Society, London, pp. 1­2. 2. Zhang,Z.Q. (2013) Animal biodiversity: an outline of higher-level classification and survey of taxonomic richness (Addenda 2013). Zootaxa, 3703, 1­82. 3. De Ley,P. (2006) A quick tour of nematode diversity and the backbone of nematode phylogeny. WormBook, doi:10.1895/wormbook.1.41.1. 4. Brenner,S. (1974) The genetics of Caenorhabditis elegans. Genetics, 77, 71­94. 5. Riddle,D.L., Blumenthal,T., Meyer,B.J. and Priess,J.R. (1997) C. elegans II. 2nd edn. Cold Spring Harbor Laboratory Press, Cold Spring Harbor, NY. 6. Harris,T.W., Baran,J., Bieri,T., Cabunoc,A., Chan,J., Chen,W.J., Davis,P., Done,J., Grove,C., Howe,K. et al. (2014) WormBase 2014: new views of curated biology. Nucleic Acids Res., 42, D789­D793. 7. Schwarz,E.M., Korhonen,P.K., Campbell,B.E., Young,N.D., Jex,A.R., Jabbar,A., Hall,R.S., Mondal,A., Howe,A.C., Pell,J. et al. (2013) The genome and developmental transcriptome of the strongylid nematode Haemonchus contortus. Genome Biol., 14, R89.

OUTREACH WormBase fosters a close relationship with its primary users, the C. elegans research community, collecting information on every active nematode laboratory and researcher, and connecting data and research outputs to the scientists that reported them. We continue to implement modern and engaging ways to communicate with our users. Our community forum (forums.wormbase.org) for discussion on general issues of nematode biology now has nearly 2000 members. We also continue to draw attention to interesting events and data sets in WormBase with our blog (blog.wormbase.org) and Twitter feed (www.twitter.com/ wormbase). To expand outreach efforts and engage more closely with our users, we have recently added a new in-line real-time chat feature to our website. Once initiated, users are connected directly with a WormBase staff member. Chat transcripts are automatically posted to our help desk issue tracker to verify that questions and comments are addressed satisfactorily. If a chat attempt is made whilst no operator is online, questions are posted directly to our help desk queue (as always, it is WormBase policy to acknowledge all queries within a 24 h window). Whilst chatting, users can freely navigate to other areas of the website, and even share their screen with WormBase staff to discuss issues. This feature provides us with a rapid way to address common questions or concerns so that users can continue their research without waiting for a help desk response via email.

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

INTEGRATION WITH OTHER RESOURCES An important remit of WormBase is the dissemination of project outputs beyond our own websites. We submit sequence annotations to the International Nucleotide Sequence Database Collaboration (INSDC) (39), and have a formal partnership with the Ensembl (22) and Ensembl Genomes (40) projects, working to ensure that up-to-date data for nematode genomes is displayed in those resources. To further increase the range of resources in which WormBase data can be browsed, we have implemented a WormBase Assembly Hub. Assembly Hubs (41) are an emerging standard for the representation and remote hosting of data that can be displayed by genome browsers. The WormBase Assembly Hub (www.wormbase.org/about/ userguide/wormbasehubs) is updated with every WormBase release, and allows, for the first time, current WormBase genomes and annotations to be viewed in the UCSC genome browser (41).

Nucleic Acids Research, 2015 7

8. Laing,R., Kikuchi,T., Martinelli,A., Tsai,I.J., Beech,R.N., Redman,E., Holroyd,N., Bartley,D.J., Beasley,H., Britton,C. et al. (2013) The genome and transcriptome of Haemonchus contortus, a key model parasite for drug and vaccine discovery. Genome Biol., 14, R88. 9. Jex,A.R., Liu,S., Li,B., Young,N.D., Hall,R.S., Li,Y., Yang,L., Zeng,N., Xu,X., Xiong,Z. et al. (2011) Ascaris suum draft genome. Nature, 479, 529­533. 10. Wang,J., Mitreva,M., Berriman,M., Thorne,A., Magrini,V., Koutsovoulos,G., Kumar,S., Blaxter,M.L. and Davis,R.E. (2012) Silencing of germline-expressed genes by DNA elimination in somatic cells. Dev. Cell, 23, 1072­1080. 11. Holt,C. and Yandell,M. (2011) MAKER2: an annotation pipeline and genome-database management tool for second-generation genome projects. BMC Bioinformatics, 12, 491. 12. Stanke,M., Keller,O., Gunduz,I., Hayes,A., Waack,S. and Morgenstern,B. (2006) AUGUSTUS: ab initio prediction of alternative transcripts. Nucleic Acids Res., 34, W435­W439. 13. Ter-Hovhannisyan,V., Lomsadze,A., Chernoff,Y.O. and Borodovsky,M. (2008) Gene prediction in novel fungal genomes using an ab initio algorithm with unsupervised training. Genome Res., 18, 1979­1990. 14. Korf,I. (2004) Gene finding in novel genomes. BMC Bioinformatics, 5, 59. 15. She,R., Chu,J.S., Uyar,B., Wang,J., Wang,K. and Chen,N. (2011) genBlastG: using BLAST searches to build homologous gene models. Bioinformatics, 27, 2141­2143. 16. Otto,T.D., Dillon,G.P., Degrave,W.S. and Berriman,M. (2011) RATT: Rapid Annotation Transfer Tool. Nucleic Acids Res., 39, e57. 17. Vilella,A.J., Severin,J., Ureta-Vidal,A., Heng,L., Durbin,R. and Birney,E. (2009) EnsemblCompara GeneTrees: complete, duplication-aware phylogenetic trees in vertebrates. Genome Res., 19, 327­335. 18. Nguyen,N., Hickey,G., Raney,B.J., Armstrong,J., Clawson,H., Zweig,A., Karolchik,D., Kent,W.J., Haussler,D. and Paten,B. (2014) Comparative assembly hubs: web-accessible browsers for comparative genomics. Bioinformatics, 30, 3293­3301. 19. Paten,B., Earl,D., Nguyen,N., Diekhans,M., Zerbino,D. and Haussler,D. (2011) Cactus: algorithms for genome multiple sequence alignment. Genome Res., 21, 1512­1528. 20. Hickey,G., Paten,B., Earl,D., Zerbino,D. and Haussler,D. (2013) HAL: a hierarchical format for storing and analyzing multiple genome alignments. Bioinformatics, 29, 1341­1342. 21. Jones,P., Binns,D., Chang,H.Y., Fraser,M., Li,W., McAnulla,C., McWilliam,H., Maslen,J., Mitchell,A., Nuka,G. et al. (2014) InterProScan 5: genome-scale protein function classification. Bioinformatics, 30, 1236­1240. 22. Cunningham,F., Amode,M.R., Barrell,D., Beal,K., Billis,K., Brent,S., Carvalho-Silva,D., Clapham,P., Coates,G., Fitzgerald,S. et al. (2015) Ensembl 2015. Nucleic Acids Res., 43, D662­D669. 23. McLaren,W., Pritchard,B., Rios,D., Chen,Y., Flicek,P. and Cunningham,F. (2010) Deriving the consequences of genomic variants with the Ensembl API and SNP Effect Predictor. Bioinformatics, 26, 2069­2070. 24. Yates,A., Beal,K., Keenan,S., McLaren,W., Pignatelli,M., Ritchie,G.R., Ruffier,M., Taylor,K., Vullo,A. and Flicek,P. (2015) The Ensembl REST API: ensembl data for any language. Bioinformatics, 31, 143­145. 25. Kinsella,R.J., Kahari,A., Haider,S., Zamora,J., Proctor,G., Spudich,G., Almeida-King,J., Staines,D., Derwent,P., Kerhornou,A. et al. (2011) Ensembl BioMarts: a hub for data retrieval across taxonomic space. Database (Oxford), bar030. 26. Stein,L.D., Mungall,C., Shu,S., Caudy,M., Mangone,M., Day,A., Nickerson,E., Stajich,J.E., Harris,T.W., Arva,A. et al. (2002) The generic genome browser: a building block for a model organism system database. Genome Res., 12, 1599­1610. 27. dos Santos,G., Schroeder,A.J., Goodman,J.L., Strelets,V.B., Crosby,M.A., Thurmond,J., Emmert,D.B., Gelbart,W.M. and FlyBase,C. (2015) FlyBase: introduction of the Drosophila

28.

29.

30.

31.

32. 33. 34.

35.

36. 37.

38. 39.

40.

41.

42.

43.

44.

melanogaster Release 6 reference genome assembly and large-scale migration of genome annotations. Nucleic Acids Res., 43, D690­D697. Costanzo,M.C., Engel,S.R., Wong,E.D., Lloyd,P., Karra,K., Chan,E.T., Weng,S., Paskov,K.M., Roe,G.R., Binkley,G. et al. (2014) Saccharomyces genome database provides new regulation data. Nucleic Acids Res., 42, D717­D725. Ruzicka,L., Bradford,Y.M., Frazer,K., Howe,D.G., Paddock,H., Ramachandran,S., Singer,A., Toro,S., Van Slyke,C.E., Eagle,A.E. et al. (2015) ZFIN, The zebrafish model organism database: Updates and new directions. Genesis, 53, 498­509. Eppig,J.T., Blake,J.A., Bult,C.J., Kadin,J.A., Richardson,J.E. and Mouse Genome Database, G. (2015) The Mouse Genome Database (MGD): facilitating mouse as a model for human biology and disease. Nucleic Acids Res., 43, D726­D736. Berardini,T.Z., Reiser,L., Li,D., Mezheritsky,Y., Muller,R., Strait,E. and Huala,E. (2015) The arabidopsis information resource: Making and mining the `gold standard' annotated reference plant genome. Genesis, 53, 474­485. Skinner,M.E., Uzilov,A.V., Stein,L.D., Mungall,C.J. and Holmes,I.H. (2009) JBrowse: a next-generation genome browser. Genome Res., 19, 1630­1638. Lee,R.Y. and Sternberg,P.W. (2003) Building a cell and anatomy ontology of Caenorhabditis elegans. Comp. Funct. Genomics, 4, 121­126. Kibbe,W.A., Arze,C., Felix,V., Mitraka,E., Bolton,E., Fu,G., Mungall,C.J., Binder,J.X., Malone,J., Vasant,D. et al. (2015) Disease Ontology 2015 update: an expanded and updated database of human diseases for linking biomedical knowledge through disease data. Nucleic Acids Res., 43, D1071­D1078. Schindelman,G., Fernandes,J.S., Bastiani,C.A., Yook,K. and Sternberg,P.W. (2011) Worm Phenotype Ontology: integrating phenotype data within and beyond the C. elegans community. BMC Bioinformatics, 12, 32. Gene Ontology Consortium. (2015) Gene Ontology Consortium: going forward. Nucleic Acids Res., 43, D1049­D1056. Smith,R.N., Aleksic,J., Butano,D., Carr,A., Contrino,S., Hu,F., Lyne,M., Lyne,R., Kalderimis,A., Rutherford,K. et al. (2012) InterMine: a flexible data warehouse system for the integration and analysis of heterogeneous biological data. Bioinformatics, 28, 3163­3165. Smedley,D., Haider,S., Ballester,B., Holland,R., London,D., Thorisson,G. and Kasprzyk,A. (2009) BioMart­biological queries made easy. BMC Genomics, 10, 22. Nakamura,Y., Cochrane,G., Karsch-Mizrachi,I. and International Nucleotide Sequence Database Collaboration. (2013) The International Nucleotide Sequence Database Collaboration. Nucleic Acids Res., 41, D21­D24. Kersey,P.J., Allen,J.E., Christensen,M., Davis,P., Falin,L.J., Grabmueller,C., Hughes,D.S., Humphrey,J., Kerhornou,A., Khobova,J. et al. (2014) Ensembl Genomes 2013: scaling up access to genome-wide data. Nucleic Acids Res., 42, D546­D552. Rosenbloom,K.R., Armstrong,J., Barber,G.P., Casper,J., Clawson,H., Diekhans,M., Dreszer,T.R., Fujita,P.A., Guruvadoo,L., Haeussler,M. et al. (2015) The UCSC Genome Browser database: 2015 update. Nucleic Acids Res.43, D670­D681. Yook,K., Harris,T.W., Bieri,T., Cabunoc,A., Chan,J., Chen,W.J., Davis,P., Cruz,N., Duong,A., Fang,R. et al. (2012) WormBase 2012: more genomes, more data, new website. Nucleic Acids Res.40, D735­D741. Amberger,J.S., Bocchini,C.A., Schiettecatte,F., Scott,A.F. and Hamosh,A. (2015) OMIM.org: Online Mendelian Inheritance in Man (OMIM(R)), an online catalog of human genes and genetic disorders. Nucleic Acids Res., 43, D789­D798. Davies,M., Nowotka,M., Papadatos,G., Dedman,N., Gaulton,A., Atkinson,F., Bellis,L. and Overington,J.P. (2015) ChEMBL web services: streamlining access to drug discovery data and utilities. Nucleic Acids Res., 43, W612­W620.

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

Agile methods are the fastest rising software lifecycle process methods in software engineering. Educators are converting traditional and project-base courses to agile in response, but this is a daunting task with few structured teaching resources methods available to reduce the burden on the educator. In professional practice, agile methods have been particularly effective in empowering experienced software engineers through its focus on empirical process control and constant feedback loop. These process traits are difficult to simulate in an academic setting, as student developers are inexperienced, synchronous meeting times are few and far between, and obtaining meaningful constant feedback a laborious undertaking. This workshop will present a comprehensive approach to teaching Agile methods that is itself agile, employing a highly iterative, continuous feedback-driven process. Pedagogical and assessment strategies will be shared, and the presenter will facilitate a best practices interactive discussion to draw out lessons learned from workshop participants. Specific agile practices with supporting labs from the popular Scrum and eXtreme Programming (XP) process models will be presented. The workshop will also encourage interaction amongst participants to share best practices and lessons learned. Research directions related to the application of agile principles to teaching and learning will be discussed.This work explores the impact of teaching and learning if the rate of learner engagement outside the classroom is continuously measured and available to the instructor and students. We describe an ongoing implementation of a monitoring tool built within a software engineering continuous integration and testing (CI & Test) platform that integrates multiple streams of student activity and performance on yearlong junior software engineering projects. The CI & Test platform allows for continuous and instantaneous feedback, which we will use to inform student behavior change. In the work-in-progress we describe the technology, its impact on the teaching process for the instructor, and preliminary results observing impacts on student engagement behavior.2015 International Conference on Indoor Positioning and Indoor Navigation, 13 th-16th October 2015

CAPTURE ­ Extending the Scope of SelfLocalization in Indoor Positioning Systems
Gary Cullen, Kevin Curran, Jose Santos
Intelligent Systems Research Centre, University of Ulster, Magee College, Derry, United Kingdom Gary.Cullen@lyit.ie
Abstract--The concept of devices cooperatively assisting with the localization of other devices in either the indoor and outdoor arena is not a new phenomenon. The primary focus of research into such a theory is however limited to solving the problem of localization accuracy. This paper outlines our Cooperatively Applied Positioning Techniques Utilizing Range Extension (CAPTURE) system which aims to provide a solution to the current range limitations of an Indoor Position System (IPS). These range limitations are the culmination of well documented difficulties of localizing using wireless signals Non-Line of Sight (NLOS) environments. The coverage of a localization solution is still a challenging issue in the indoor environment. In this paper we implement a version of CAPTURE that uses Wi-Fi Direct and Bluetooth Low Energy (Bluetooth LE 4.0) that takes advantage of mobile devices at the outer limits of an IPS to help extend its reach into blind spots, where devices cannot be located. CAPTURE is evaluated using a live test environment, where range estimations are captured between cooperating devices. These range estimations are filtered before being placed into a trilateration algorithm to position lost devices. Finally the accuracy of CAPTURE is presented, demonstrating the achievable benefits of implementing CAPTURE as a solution to the problem of coverage in an Indoor environment. Keywords--Localization; Indoor Positioning Systems; Indoor Localization; Geographical Positioning; wireless; Wi-Fi; Bluetooth; Cooperative Localization; Collaborative Positioning; Self-Positioning

Gearoid Maguire, Denis Bourne
Letterkenny Institute of Technology, Co. Donegal, Ireland

the aforementioned to be locatable further exacerbates the need for an expansive solution to accurately locate in all areas of an indoor environment. Devices such as these were typically not designed with wireless network capability to merely, assist in locating other devices. Which although, is a great reuse of an existing technology, does however, throw up a secondary issue. If these devices are connected to the network, can we realistically conceive that they will disconnect from that network to connect in a Peer to Peer network to cooperatively assist in locating lost devices? Wi-Fi Direct offers the ability to be in both Ad-Hoc Mode and Infrastructure mode simultaneously [2], Bluetooth LE allows the Wi-Fi chip to remain connected to the network whilst transmitting. Therefore using CAPTURE to extend the range of an IPS using Wi-Fi Direct and Bluetooth LE capable devices allows the user to remain connected to their network whilst cooperatively assisting in locating other devices it can `see'. Generally, IPS implementations can be grouped as either exogenous or endogenous depending on the available infrastructure that can be employed to establish location information. An exogenous infrastructure implementation is typically oriented towards an IPS application. An endogenous solution however, is made up of infrastructure that has not been installed primarily for positioning reasons. Currently, one of the most popular techniques to locate devices in the indoor environment is to use the preinstalled Wireless Access Points (WAPs) which are used to provide wireless network access to mobile devices. Typically, good system implementations are those that achieve an appropriate balance between requirements, technological advances, and costs. Whilst utilizing an existing infrastructure, such as this offers many noble qualities, not least the reduced costs in procuring equipment to implement a solution, it does introduce some problems. The decision process behind the strategic positioning of such equipment to provide mobile network coverage does not fulfil the requirements of an Indoor Positioning System (IPS) to locate devices. Therefore it is inevitable that blind spots should exist in these ISP's. When deciding on the positioning of Wi-Fi equipment such as Wireless Access Points (WAPs) the typical focus of network designers was to provide the highest available throughput to the largest congregations of wireless network users at key areas within the building. The ability to locate devices within that environment was not necessarily to the fore in their decision process, leaving gaping holes in terms of coverage in some of the IPS's currently in-

I.

INTRODUCTION

The indoor location problem has been around for many years and has motivated a great deal of research into finding a solution. Cooperative solutions have made up a significant contribution of this research. Cooperation among devices to self-locate requires a key prerequisite - there must be an adequate number of devices willing to assist in locating a lost device. The proliferation of tablet devices and Smartphones, fully loaded with Wi-Fi, Bluetooth and gyroscopes, somewhat address this need. The advent of the Internet of Things (IoT's) however, providing access to 100's of billions of devices [1] offers an even more fertile community of wirelessly connected smart objects in a connectivity ecosystem. The pace of innovation of wearable computing coupled with tumbling costs, mirrored in the consumer interest of the iWatch offers no sense of a drop off in access to these collaborative devices. Indeed the requirement for nomadic wearable devices such as

2015 International Conference on Indoor Positioning and Indoor Navigation, 13 th-16th October 2015 place. This coupled with some of the architectural barriers to the positioning of WAPs within a buildings infrastructure doesn't suggest a solution to this issue in the near future. In the outdoor arena, apart from some natural obstacles such as overhead trees, cavernous areas and mountainous regions the satellites that make up the Global Positioning System (GPS) have a relatively clear, unobstructed view of the devices they need to locate. There are also some man made obstructions, such as the urban canyon phenomenon [3], that can obscure views in the outdoor world. These also provide barriers to accurate position estimation, but bear little resemblance to the many impediments that make up the indoor environment. The ability to locate in the outdoor world has more or less been solved as a problem through the many advances in GPS technology down through the years. The inability of a GPS signals to penetrate a building's infrastructure after propagating 22,000 Kilometers however, renders it more or less redundant as an indoor positioning solution. Considering we spend more time in the indoor arena, nearly 89% of the time according to a recent study [4] carried out in Canada, the need for a solution is evident. Usually, distance estimations from more than one WAP to a lost device are required so that a positioning algorithm can provide a reasonable level of accuracy. Indeed, sometimes up to four are required to provide positioning estimations on a 3D plane. We aim to show in this paper how CAPTURE can be used in this scenario to augment an existing ISP, assisting in the positioning of devices that would traditionally be unlocatable. Using devices bundled with Wi-Fi or Bluetooth antennae we will make up the required number of, or completely replace the WAPs required. CAPTURE utilizes Received Signal Strength Indicator (RSSI) readings from WiFi, Bluetooth or Wi-Fi Direct enabled devices to estimate the range between mobile devices. The measurement of signal attenuation of these RSSI values between cooperating devices is used to gauge a propagation distance. These distance estimations are filtered, to remove any outliers, before being used as input to a trilateration positioning algorithm. Previous iterations of CAPTURE [5, 6] have used both Wi-Fi and Bluetooth 2.0 to assist in the cooperative position of devices that are beyond the range of standard IPS's. In this paper we propose to use both Bluetooth 4.0 (Bluetooth LE (Low Energy)) and Wi-Fi Direct to find the location of remote devices. Convincing other devices to cooperatively assist in locating lost devices would be impossible if, as part of that cooperation the assisting devices had to sacrifice copious amounts of battery power. In this paper, we will also provide an evaluation of the typically battery consumption when a device is utilized in a CAPTURE implementation. This will provide further evidence of the use of CAPTURE as a solution to the ranging problem. Consider the following scenario, `Bob' is sitting in the far end of the airport lounge reading his newspaper on his tablet and is considering ordering food. He has network connectivity and can see online that his flight is due to leave on time. Bob has been to this airport before but is unfamiliar with the time it would take to get to his specific departure gate, or which area he has to navigate his way through security. The airports IPS could assist with this, but he only has visibility of one WAP. This provides a robust network connection but is incapable of positioning Bob within the airport. Sue is in the airport café some 45 meters to the west of Bob, Sue's phone can be `seen' by 3 different WAPs within the airports network and can be located to within 2 meters of her current position, via the inhouse IPS. Sue's phone can also `see' Bob's tablet. The drinks vending machine in the main hall is 25 meters to the north of Bob, because of its location in the main hall it has access to 7 WAPs that are utilized in the airports IPS. This smart device also has a wireless Network Interface Card (NIC) allowing it to connect to the airport inventory system providing minute by minute updates on its current stock levels. But more importantly it is positioned within the network IPS. The 25 meter distance to Bob's tablet is a simple hop, well within its read range. In a normal scenario Bob would be beyond the range of the airports IPS, but because CAPTURE can utilize the known positions of Sue's phone, the drinks vending machine and the WAP that Bob can currently `see', Bob can be positioned. CAPTURE takes these devices that know their position and estimates range distances from Bobs lost device to them. These range estimates are then placed into a trilateration algorithm to position Bob within the airport. CAPTURE provides a position estimate relative to the devices locating it, which can then be mapped onto a global overview of the airport IPS. Bob can now see that he is 15 minutes from the departure gate, he is advised to go via the security area just behind the lounge. Bob orders the duck, all is good. II. CAPTURE ­ SYSTEM MODEL

Here we describe the overall system model used to implement the CAPTURE solution. CAPTURE provides the ability to locate a lost device relative to devices that were used to assist in its localization. If these devices were originally positioned with an IPS this position can be translated to Cartesian coordinate values on a 2D plane. There is no need for a calibration stage with CAPTURE as would be required with the classical fingerprinting model. CAPTURE uses only real time RSSI values, providing a robust opportunistic solution in dynamic environments. Literature within the realm of Location Based Systems frequently use terms such as Anchor or Anchor Nodes to describe devices that help to determine the position of lost or unknown devices. The term anchor elicits a perception of a static or permanent device, which in a cooperative solution these devices most certainly are not. For this reason we will use the term reference device when describing devices that assist in the positioning of lost or unknown devices. Fundamental to any position estimation algorithm is the ranging technique that is employed to gauge the distance from the transmitting device(s) to the receiving device(s). This is determined using a given metric, for example the length of time it takes a signal to propagate the distance from the transmitter to the receiver. Issues relating to the small scale fading of signals due to reflection, refraction, absorption, diffraction and scattering on obstacles in the indoor environment are non-trivial and have been well documented in

2015 International Conference on Indoor Positioning and Indoor Navigation, 13 th-16th October 2015 the following literature [7-9]. A high percentage of location algorithms use range measurements or angle measurements to estimate distance between a lost device and a particular reference device. CAPTURE uses the Received Signal Strength Indicator (RSSI) to estimate range between devices. This range estimation is then used as input to a trilateration algorithm to ascertain the Cartesian coordinate values of the lost device. In order for CAPTURE to be able to cooperatively locate a lost device within a network, there must be at least 3 reference devices within sight of the lost device. Each of these must have `a prior' knowledge of their location within a pre existing localization solution. During the experiments, different configurations were modelled to mimic the different scenarios that could befall a lost device being positioned using CAPTURE. Initially 3 Smartphones were used to position the lost device, then two phones along with a WAP were used to provide range estimations as input. Finally RSSI readings from a Smart TV, equipped with a Wi-Fi card, a tablet and a Smartphone were used to represent the scenario described as Bob's scenario at the airport. MSM7227 chipset. 3 of the phones were used as reference devices, the other phone acted as the lost device. All equipment used during the experiments were the same make and model ruling out any issues with diverse RSSI reads with different antenna types. Issues relating to varied reads with diverse antenna makes have been documented in the literature [10, 11]. Lisheng et al., [10] describe the bias being as much as 11.2 dBm out with different antenna types over a 25 meter read range. Although these issues describe scenarios relating to WiFi radio signals, it is the opinion of the author that these would have a negative impact on Bluetooth LE transmissions also. Other issues relating to the orientation of devices described in the following literature [12] were also considered during the testing phase.

Figure 2: CAPTURE Client Interface A MySQL Server version: 5.0.96 hosted on a Linux platform was used to store all data collected by the devices. The server was online and the mobile devices wrote directly to it as they recorded RSSI values from each other. The data was then passed through a low level filter to remove any outliers, before an average RSSI reading was calculated for each required ranging measurement, to be used in the trilateration algorithm to estimate the position of the lost device. A Dell Latitude E6440 iCore3 running Windows 7 Professional was used to develop the app to gather the RSSI from the phones. An algorithm was designed to convert this RSSI reading into a ranging measurement before a trilateration algorithm converted the ranging measurements into Cartesian coordinate values. We used the Eclipse IDE and Android Software Development Kit (SDK) for Android development and debugging, to develop the app. IV. DATA COLLECTION AND PRESENTATION

Figure 1: Test Environment III. EXPERIMENTAL TEST BED

In this section, we will demonstrate the suitability of CAPTURE as a solution to the indoor ranging problem. We will back up this assertion with evidence based tests, carried out in a large campaign of measurements taken in a Sports Hall. The hall offers a 40m wide diagonal testing environment, providing Line of Sight measurements for all tests, as can be seen in the image depicted in Figure 1. Each device used in the test is given a name (BSSID) TestDevice1, TestDevice2 for example. CAPTURE takes the RSSI readings of all available reference points, i.e. all devices it can `see', but it filters out only the test devices selected by the user carrying out the tests. This is achieved via a lookup table mapping the MAC address of the device to the device name and allows us to work only with cooperating devices, allowing the use of only a specified device or a group of device during any given test. The experimental setup of CAPTURE was made up of 7 Samsung GT-S5310 Galaxy Pocket phones, running Google Android 2.2.1 on a 600 MHz ARMv6, Adreno 200 GPU, Qualcomm

In this section we provide an overview of some of the data collected during the experiments that were carried out during this implementation of CAPTURE. An initial test was carried out to provide an average 1 meter read range for the ranging algorithm. This test involved over 500, 1 meter RSSI readings recorded at different locations throughout the test area. Any

2015 International Conference on Indoor Positioning and Indoor Navigation, 13 th-16th October 2015 outliers were removed with a simple filter, this allowed for the accurate depiction of this reading to be used for the ranging algorithm. Additional tests were carried out to measure the accuracy of the RSSI readings and the computed range estimations achieved by the algorithm. The following table, Table, Table I. CAPTURE RSSI Readings, outlines some of the readings achieved during this initial test phase of the implementation.
TABLE I. CAPTURE RSSI READINGS RSSI Readings Distance Average Std. Dev Estimate Distance Average Std. Dev Estimate 0­5m -57.264 0.4996 4.517 0 ­ 25 m -68.38 0.6884 21.544 0 ­ 10 m -61.5652 0.4 8.269 0 ­ 30 m -70.75 0.9797 30.059 0 ­ 15 m -69.5263 0.85346 25.31 0 ­ 35 m -71.854 0.6803 35.104 0 ­ 20 m -67.5662 0.48992 19.216 0 ­ 40 m -73.681 0.7901 45.379 [1]

object of some description, and the final scenario were only mobile devices existed within range of the lost device. It is proposed that further experiments (in the coming months) will be implemented to replicate each scenario in detail. Each of these would be evaluated to establish the feasibility and accuracy of each which would further advocate CAPTURE as a solution to these problems.

REFERENCES
G. Kortuem, F. Kawsar, D. Fitton, and V. Sundramoorthy, "Smart objects as building blocks for the Internet of things," Internet Computing, IEEE, vol. 14, pp. 44-51, 2010. [2] W.-F. Alliance, "Wi-Fi Certified Wi-Fi Direct," White paper, October 2010. [3] M. Spangenberg, J. Y. Tourneret, V. Calmettes, and G. Duchateau, "Detection of variance changes and mean value jumps in measurement noise for multipath mitigation in urban navigation," in Signals, Systems and Computers, 2008 42nd Asilomar Conference on, 2008, pp. 11931197. [4] C. G. Richardson, J. Memetovic, P. A. Ratner, and J. L. Johnson, "Examining gender differences in emerging tobacco use using the adolescents' need for smoking scale," Addiction, vol. 106, pp. 18461854, 2011. [5] G. Cullen, K. Curran, J. Santos, G. Maguire, and D. Bourne, "CAPTURE - Cooperatively Applied Positioning Techniques Utilizing Range Extensions," in Indoor Positioning and Indoor Navigation (IPIN), 2014, 2014, pp. 248-254. [6] G. Cullen, K. Curran, J. Santos, G. Maguire, and D. Bourne, "To wireless fidelity and beyond & Beyond; CAPTURE, extending indoor positioning systems," in Ubiquitous Positioning Indoor Navigation and Location Based Service (UPINLBS), 2014, 2014, pp. 248-254. [7] T. S. Rappaport, Wireless communications : principles and practice, 2nd ed. ed. Upper Saddle River, N.J. ; [Great Britain]: Prentice Hall PTR, 2002. [8] S. S. Haykin and M. Moher, Modern wireless communications. Upper Saddle River, N.J.: Pearson/Prentice Hall, 2005. [9] A. F. Molisch, Wireless communications, 2nd ed. ed. Oxford: John Wiley & Sons, 2011. [10] X. Lisheng, Y. Feifei, J. Yuqi, Z. Lei, F. Cong, and B. Nan, "Variation of Received Signal Strength in Wireless Sensor Network," in Advanced Computer Control (ICACC), 2011 3rd International Conference on, 2011, pp. 151-154. [11] F. D. Rosa, X. Li, J. Nurmi, M. Pelosi, C. Laoudias, and A. Terrezza, "Hand-grip and body-loss impact on RSS measurements for localization of mass market devices," in Localization and GNSS (ICLGNSS), 2011 International Conference on, 2011, pp. 58-63. [12] K. Kaemarungsi and P. Krishnamurthy, "Properties of indoor received signal strength for WLAN location fingerprinting," in Mobile and Ubiquitous Systems: Networking and Services, 2004. MOBIQUITOUS 2004. The First Annual International Conference on, 2004, pp. 14-23.

V.

CONCLUSIONS

A comprehensive solution to localization in the indoor arena poses a uniquely complex set of problems. Problems that have fostered growing interest, but for which, a single or indeed swift answer does not currently exist. The success of GPS has further exacerbated the need for an indoor representation. This paper introduced a novel architecture for cooperative localization, CAPTURE, a solution to extend the range of an Indoor Positioning System. The proposed approach provides an efficient reliable mechanism to plug into an in-situ solution. An implementation of CAPTURE was evaluated and tested which demonstrated the ability of CAPTURE to augment an existing indoor localization solution to locate mobile devices. Battery consumption was highlighted earlier in the paper, this will be adequately analyzed in a larger version of this paper. Three different scenarios were also outlined describing specific situations where typically devices could not be located using a standard solution. The first of these scenarios was, were only one WAP was reachable by the lost device, the second were the lost device could `see' a WAP and a smart

Designing a Mobile Application to Support the Indicated Prevention and Early Intervention of Childhood Anxiety
Mandar Patwardhan
Arizona State University School of Computing, Informatics and Decision Systems Engineering Ira A. Fulton Schools of Engineering Tempe, AZ USA

Ryan Stoll
Arizona State University Department of Psychology Tempe, AZ USA

Derek B. Hamel
Arizona State University School of Computing, Informatics and Decision Systems Engineering Ira A. Fulton Schools of Engineering Tempe, AZ USA

Ryan.Stoll@asu.edu

mpatward@asu.edu Ashish Amresh Kevin A. Gary

dbhamel@asu.edu Armando Pina
Arizona State University Department of Psychology Tempe, AZ USA

Arizona State University Arizona State University School of Computing, Informatics and School of Computing, Informatics and Decision Systems Engineering Decision Systems Engineering Ira A. Fulton Schools of Engineering Ira A. Fulton Schools of Engineering Tempe, AZ USA Tempe, AZ USA

Armando.Pina@asu.edu

amresh@asu.edu ABSTRACT

kgary@asu.edu 1. INTRODUCTION
Mobile health applications (mHealth apps) span a wide spectrum of health-related issues and treatment approaches, such as health monitoring (physiological or self-reported), protocol adherence through reminder communications, and (psycho)education [15]. Interestingly, the ubiquitous and familiar nature of smartphone devices creates the potential for mobile health (mHealth) applications targeted to youth "at risk" for anxiety disorders or meeting criteria for anxiety disorder diagnoses. In fact, mHealth for anxiety disorders may be of unique importance because most parents do not seek help for their anxious youth, effect sizes from anxiety programs are generally modest and need to be potentiated, and there is a pressing need for sustainable and streamlined intervention efforts that have "real world" utility [2][3][13]. In addition, targeting anxiety disorders is of public health significance because these are among the most prevalent psychiatric problems in children with rates ranging from 5% to 10% and as high as 25% in adolescents. Anxiety disorders also cause significant impairment, typically fail to spontaneously remit, and are prospectively linked to clinical depression and problematic substance use for some youth [13]. Although the popularity of mHealth apps is exploding, few lessons have been shared regarding the user experience design for such innovations. Building on randomized control trial (RCTs) studies and theory, this research focuses on the design process for adapting aspects of an empirically informed child anxiety disorder intervention to a smartphone platform. Thus, this work is significant due to the domain (anxiety), the nature of the intervention (preventative-early intervention), the use of an app to increase protocol efficiency, and the integration of concepts from innovative design technology (gaming, notifications, user experience design) to improve outcomes. Focusing on the anxiety protocol, it is important to note that considerable strides have been made to develop evidence-based treatment and prevention armamentaria targeting youth anxiety with almost every protocol employing the same cognitive and behavioral procedures (Fisak et al., 2011; Silverman et al., 2008) .

This paper presents the design of an mHealth application for prevention and early intervention of childhood anxiety. The application is based on REACH, a preventative-early intervention protocol for childhood anxiety. This paper describes the multidisciplinary design process, sharing lessons learned in developing an effective mHealth application. This mHealth application is unique due to participant age, preventive-early intervention focus, and utilization of mobile technology in a situated manner. A design process inspired by user-centered leveraging key informant interviews was used to identify application features, including game based strategies and an animated motivational avatar. Validation was performed through external review and a usability study performed with target end users of the application. Results suggest overall satisfaction, ease of use, and increased motivation.

Categories and Subject Descriptors
D.2.2 [Software Engineering]: Design Tools and Techniques ­ Evolutionary prototyping, and user interfaces.

General Terms
Design, Human Factors, Verification

Keywords
Youth Anxiety Prevention, mHealth, User-Centered Design.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Wireless Health `15, October 14­16, 2015, Bethesda, MD, USA. Copyright 2015 ACM 978-1-4503-3160-9 ...$15.00. http://dx.doi.org/10.1145/2811780.2811954

REACH for Success (REACH hereafter) is a school-based cognitive-behavioral protocol designed for 4th and 5th graders for the indicated prevention and early intervention of childhood anxiety and related problems. REACH uses procedures found to be efficacious in RCTs, including in our own 3 RCT [8][9][12]; however, there are several features that set REACH apart. Most relevant to this paper is data suggesting that the classic design of evidence-based prevention programs (including programs like FRIENDS [1]) is simply not feasible or sustainable in schools (e.g., there are too many sessions, sessions are too long, manuals are too cumbersome and not organized for real worldimplementation, too much training is required, and preparation is too time consuming). In contrast, REACH was created from our evidence-based exposure-based cognitive-behavioral protocols as a practical intervention that can build a foundation for sustainable large-scale diffusion. That is, REACH was streamlined into 6 sessions (instead of the typical 12-15), each 20-30 minutes in length (rather than the typical 60 to 90 minutes), and uses an easyto-follow manual (each session is condensed into one page front and back while FRIENDS, for example, has an 89 page manual). One concern with REACH, however, is that such a streamlined protocol may result in a lower dosage of the active change ingredients and fewer opportunities for youth to practice coping skills because there are fewer sessions and less practitioner feedback time. This concern is justified as a recent child anxiety treatment study evaluating an 8 session adaptation of the 16 to 20 session Coping Cat program yielded lower youth response rates suggesting that difficulty practicing the skills was a major impediment to recovery [11]. A purpose of this research was to design an mHealth platform to accompany the REACH 6 session school-based preventative early intervention protocol. Specifically, the goal was to develop an mHealth app that: (a) provides on-demand opportunities for skill practice, (b) uses notifications relevant to skill practice to improve compliance, (c) offers tools for personalizing and tailoring the protocol, (d) increases opportunities for corrective feedback based on user data amenable to creating personalized reports of youth weekly practice and response, and (e) yields high user ratings along core validated usability dimensions relevant to technology innovation efforts. Herein, the REACH protocol, the app design process, and the app implementation are described. Results from an empirical study in a usability context are presented. To set domain context, the face-to-face protocol is described followed by a discussion on design, implementation, and usability.

game). S6: Learn to face situations and Engage in behavioral exposures to mild-moderate anxiety-provoking situations. Core skill acquisition and practice tools include the use of Daily Diaries, Guided Relaxation, STOP acronym, and STIC acronym. Relevant to the REACH app, Daily Diaries are used to facilitate self-evaluation of emotion expressiveness. Youth self-monitor and describe in writing the anxiety or fear provoking situations that occurred during the week. Youth also rate using a 0-8 feelings thermometer the severity of anxiety/fear associated with the situation. Lastly, youth describe in writing thoughts that occurred before/during/after the situation (e.g., worries) and actions that resulted (e.g., avoidance behaviors). In terms of Guided Relaxation, youth are provided with pre-recorded standardized step-by-step procedures designed to improve self-regulation of anxiety related physiological hyperarousal via breathing exercises, muscle tension/release exercises, and imagery. When it comes to cognitive self-control, a four-step coping plan is introduced via the "STOP" acronym where S = Scared? T = Thoughts, O= Other [thoughts], P = Praise. STOP is first practiced via the Worryheads game by using pre-written emotionally ambiguous and anxiety provoking scenarios along with an accompanying "worry thought". Youth are then asked to change the "worry thought" for a more realistic and alternative solution to the scenario provided. In the game, successful resolution of the worry thought results in advances toward a common goal for each player (reaching the end to win the game). Subsequently, with basic knowledge of STOP, youth engage in prospectively applying the technique to situations that emerge as anxiety or fear provoking for them during the course of each week. Lastly, behavioral exposures are introduced via STIC jobs (STIC = Show That I Can. STICs are provided in the form of a pre-written or prepopulated Fear Hierarchies based on modules from the Anxiety Disorders Interview Schedule for Children where each avoidance behavior has been pre-populated for the child as individual exposures. The REACH protocol has been implemented using a paper-andpencil approach. The protocol, while effective, encountered some common limitations in practice, notably protocol compliance. Specifically, subjects did not practice skills between sessions or were not diligent in recording practice activity and outcomes. Further, as noted in section 1, lower dosage in the related Coping Cat tool resulted in lower response rates. Data capture with paperand-pencil methods is also time consuming and subject to human coding errors or oversights. The psychology researchers believed mobile and gaming technologies could effectively address the limitations, improve compliance and data capture, thereby reducing dosage while increasing effectiveness. They teamed with software engineering researchers to conduct a multidisciplinary design and development process to construct the app.

2. THE REACH PROTOCOL
REACH for Personal and Academic Success is an indicated prevention and early intervention program targeting anxiety disorders and related problems in youth. The protocol is administered in a group format (five to seven children per group). Each session (S) in the manual is organized in terms of Overview, Content (didactic, games), Review/Closing, and After the Session (homework). Self-evaluation of emotion expressiveness is embedded in every session. The protocol focuses on broad-based exposure and problem solving skills, which have a wide reach for the range of anxiety disorders targeted. Unique session content is as follows. S1: Introduction (group name, rules, and confidentiality), Learn about emotions, and Relaxation. S2-3: Define worries, Learn cognitive self-control, and Practice cognitive self-control (Worryheads game). S4: Define social skills and Learn about conversation skills (starting and managing conversations). Practice conversations (make-believe game). S5: Learn about assertiveness and Practice assertiveness (stand-up!

3. DESIGN PROCESS
The multidisciplinary team embarked on a highly iterative design process focused on the capabilities and context of end users. The researchers aspired to use a user-centered design (UCD) approach, but in practice the designers did not have direct access to end users during the design process and as such relied on subject matter experts (SMEs) as proxies. The SMEs were the psychologists who developed the REACH protocol and had deployed it 56 times to youth over 6 months. Section 5 describes external validation via design review by a school advisory board and a usability study with independent youth end users (n=22).

3.1 Gap Analysis
REACH is a pre-existing protocol, so the first design activity was to review program materials and workflow, seeking opportunities to effectively translate existing steps, and later innovating on smartphone-specific solutions to achieve the domain objectives for increased dosage, engagement, and feedback (see Section 1). To better understand the domain of the app, the SMEs shared the provider manual of the REACH protocol to the designers and the materials for delivering the protocol (board games, handouts, MP3s). The manual describes how the sessions, each conducted consecutively over the course of six weeks, employ specific practice worksheets, information gathering forms, and interactive exercises designed to train youth in the preventive and coping skills. The main activities defined in the manual were Daily Diary, Relaxation, S.T.O.P, Worryheads board game, and STICs. Table 1 summarizes the protocol component steps and highlights challenges in porting these steps to the mobile environment. Table 1: REACH protocol components and gap analysis
REACH Daily Diary Relaxation S.T.I.C. S.T.O.P. Worryheads Component Description / Design Challenges Self-monitoring engagement; daily compliance; rich data entry Pre-recorded audio exercises media porting and translation Behavioral exposures with adult feedback preserving steps; rewards; feedback Self-application of cognitive self-control plan encouraging tool engagement through positive UX Learn and practice cognitive self-control plan with provided scenarios detailed alternatives; increasing dosage; feedback

Issues #2 and #5 were more significant. Issue #2 represents a "blind spot" in design, due to factors such as missing information implicitly understood by the SMEs but not apparent to the design team. Issue #5 was a recognition that the design team did not understand who would be using the app and in what context. At this point the design team realized a more patient-centric approach was required to overcome these design obstacles.

3.2 A Patient-centric Design Process
The design process described in the previous section focused on translating a field manual; it is not surprising that the translation had gaps derived from implicit knowledge assumed by the manual authors and not understood by the designers. The software engineering researchers suggested a more user-centric approach, where the needs of the end user, in this case the patients, is the focus of the design process. The gold standard for such a design process is User-Centered Design (UCD), originally credited to Norman and Draper [7]. UCD assumes a participatory design process with end users, but for this research we prefer the more inclusive definition of UCD as "the active involvement of users for a clear understanding of user and task requirements, iterative design and evaluation, and a multi-disciplinary approach." [14]. ISO 9241-210 [4] identifies 6 principles to UCD (quote): 1. The design is based upon an explicit understanding of users, tasks and environments. 2. Users are involved throughout design and development. 3. The design is driven and refined by user-centered evaluation. 4. The process is iterative. 5. The design addresses the whole user experience. 6. The design team includes multidisciplinary skills and perspectives. These principles were especially attractive to the design team due to the uniqueness of the domain and protocol, and identified issues understanding the end user context. The team realized the app would not be a direct translation of the paper-based REACH protocol, and needed to focus on context and end user experience. There is a wide range of practices supporting UCD; the design team utilized personas, prototyping with iterative feedback, participatory design, and end user validation. The SMEs served as participatory designers, eliminating the back-and-forth ad hoc aspects of the initial process. They also served as proxies for the end users during design as gaining access to youth (4th-5th grade users for an extended time for intense design activities was not possible). Access to end users would have certainly been preferable during the design process but was not possible at the time. However end user validation was emphasized before approving the app for protocol trial; these results are reported in section 5. Fortunately, prior domain research and SME interviews from the gap analysis proved useful in the context of the UCD.

A round of stakeholder interviews involving the SMEs followed the domain research of the REACH protocol. These included working sessions between the design team leads and the SMEs, visits by the SMEs to the design team's lab, and synchronous question-answer sessions over email and videoconferencing. This step of the process addressed difficulties relating to understanding the protocol and assumptions on both sides regarding implementation objectives. This step took longer than expected, with a result of inconsistent understanding of implementation outcomes. The design team conducted an internal review to identify root causes and come up with design process alternatives. The causes identified included: 1. New terminology. 2. Gaps in understanding by the design team with respect to the protocol. 3. Assumptions of the designers based on past implementations of mHealth apps in non-preventative domains. 4. Ad hoc communications patterns between SMEs and the design team, and within the design team itself. 5. A lack of understanding of the end user context. Together, these issues are not uncommon in design processes, and some were addressed (1, 3, 4) through simple awareness of the issue in the team review. For example, improving ad hoc communication patterns was improved through more frequent design team meetings, clarifying the lines of communication with SMEs, and reiterating design team understanding of requirements back to the SMEs for validation.

3.2.1 Personas
The design team started the UCD process by developing personas, or proxies for categories of end users, and inviting the SMEs to review them. The SMEs were not familiar with personas, and after overcoming initial confusion about the technique, gained enthusiasm and effectively provided useful feedback. The personas shared with the SMEs are presented in Table 2. Iterating over these personas led to several design insights that were previously not understood by the design team. For example, the design team came to understand subjects in this domain have a higher need for re-assurance; respond well to attention and approval, and are highly compliant (persona 2). Discussion of the

personas with the SMEs further revealed that in community samples girls are more likely identified as "anxious" than boys, and anxious youth fear the evaluative nature of social situations (personas 3 and 4). After capturing a clearer idea about end user context through discussing the personas created with the SME, the design team started a phase of rapid prototyping to ensure the SMEs provided frequent feedback on each design decision. Table 2: REACH protocol components and gap analysis Persona 1 Jacob is 10 years old, and is currently being raised by his single mother. He was held back for behavior problems as he tends to lash out when stressed. When confronted with even minor change he shuts down, and becomes irritable. His goal is to do as little as possible, or just enough so he doesn't get in trouble. Persona 2 Jessie is 9 years old and very shy. In larger groups of 10 or more people she panics, and is dangerously on edge. She has a strong recognition of her symptoms, and works very hard at overcoming them. Her goal is to be free from required effort as soon as possible. Persona 3 Mike is 12 years old. He finds it difficult interact in groups. He thinks that everyone has prying eyes on him and judging his every move. He loves to read books and is distracted by day dreaming. He gets very anxious and nervous in social situations. Persona 4 Elizabeth is 10 years old. She is relatively overweight and is embarrassed in evaluative situations. When her classmates tease her, she cries and withdraws from interacting with peers. This typically happens during physical education and school games.

Figure 1: S.T.O.P. Mockup in Pencil

3.2.2.2 Translating Protocol Components
As identified in the gap analysis (section 3.1), some protocol components are a fairly straightforward translation, or port, to the mobile app, while others are not. For example, the Relaxation audio components were a straightforward port of the media to the device wrapped with a simple consistent interaction metaphor. Of course this component also requires the least user interaction of any of the components. On the contrary, the Worryheads game is a multiplayer board game involving cards. The app required limiting the game experience to a single user compared to the multiplayer board game. The design team replaced the physical cards in the board game with preset "Situations" and "Thoughts" screens. The user was then presented with a choice of four of "Other Thoughts" options to choose from. Once the user selects a choice from possible options a praise message was showed on the screen to appreciate the correct answer. Screens depicting Worryheads are shown in section 4. A design concern in translating the protocol was the significant amount of text a child is asked to input during activities such as the Daily Diary and S.T.O.P. The mobile device is not suited for textual input that goes beyond instant messaging or social media apps, and further the end users are at an age where they are often mobile-aware, but not proficient mobile typists. The fear was that textual input would be skipped or significantly limited, or in the worst-case cause frustration of the app to the extent children would abandon it. The design team identified speech capture input as a means to facilitate better information capture.

3.2.2 Rapid Prototyping
Rapid prototyping is an iterative design technique refining the details of interaction models and overall user experience. Early prototypes, or storyboards, focus on task sequences, or the mapping of task workflows to interface screens. This leads to user interaction modeling; the identification of user input actions effecting transitions between screens or for the capture of critical information. Later iterations refine these models and also layer in thematic elements, until a final design is converged upon. Iterations are meant to be short, frequent, and focused on answering specific questions regarding the user experience.

3.2.2.1 Storyboarding and Clickthrough Prototypes
The design team used the freely available Pencil prototyping tool to construct screen and clickthrough mockups. Clickthroughs take simple screen mockups and overlay "hot regions" that advance the mock to a new screen, simulating a user interaction. One drawback is the tool runs its simulations in a web browser so tap and swipe gestures are not supported; however, the tool does support mobile UI "skins" to promote a look-and-feel consistent with the mobile user experience. Figure 2 shows an example of an early mockup created for S.T.O.P. activity. The team created mockups of different scenarios in the app. Each mockup was peer-reviewed within the design team, validated against the documented protocol, and then presented to the SMEs for feedback. The design was iteratively refined until the scenario interactions were adequately captured, and the design team felt comfortable moving to implementation on the Android platform.

3.2.3 Injecting Innovations in the Mobile Experience
A challenge in applying mHealth concepts to existing clinical protocols is the desire to innovate versus leveraging validated protocol steps. For this project, the mobile platform provided the means for increasing dosage by virtue of the device being everpresent. However, ubiquity is not enough, end users must be motivated to practice the protocol. Engagement was addressed through innovative features introduced in the mobile platform including thematic and age-appropriate media, game strategies (e.g. progressive reward incentives), and mobile notifications.

3.2.3.1 Designing an Appropriate Theme
A user interface theme refers to the consistent application of stylistic elements such as images, fonts, audio or video media, and user interface widgets (buttons, menus, taps, etc.). To gain acceptance of the app amongst users familiar with the paper protocol, the design team used the same theme used in the paper protocol. The team ensured that color codes and the fonts used in paper based protocol and the fonts used in the app are same. To

design the features of the app, the team studied the paper-based versions of the activities to be performed by youth to get a better idea of how to replicate the activities in the application. The team followed the same nomenclature of the existing activities in the screen designs reduce confusion and gain rapid acceptance. The user experience required a gender-neutral, age-appropriate proxy for the human guide who assists in the existing REACH protocol. This proxy personifies the guide, providing instruction and feedback to the end user through the mobile interface. Initial ideas focused on themes such as "feed your pet" or "grow your plant" but were rejected as being either too "babyish" for the target age range or gender-biased. The design team came up with the idea of an animated motivational character in the form of a blob. The design team referred to the character as "Bob the Blob" (Figure 3), but the male name is never used in the app itself. Based on game design concepts, "Bob" presents an age-appropriate, gender-neutral proxy for protocol guidance and feedback [6][8].

Fixed schedules are daily time-based notifications, such as for the Daily Diary, to complete a regular interval task. Adaptive notifications require tracking end user interactions with the app and dynamically determining whether to issue a notification to engage with Bob the Blob again. The designers are concerned with the notion of alarm fatigue through over-notification, though currently the mobile device is given to the end users as a locked down tool for practicing the protocol, and not as a generalpurpose smartphone for personal use.

3.2.3.4 Security and Privacy
Any mHealth app needs to be concerned with how user data is stored, transmitted, and identified. These concerns can become overbearing nonfunctional requirements on the app and down to the underlying mobile operating system providing the communication and storage services. At this stage of the app's development, it made more sense to de-identify data and work in a locked-down, disconnected mode. There were several simplifying assumptions the design team was able to make: 1. The emphasis on increased dosage over remote monitoring of compliance or personal health measurements puts this project in a different class of mHealth apps. Such apps push data to remote providers (often via a cloud-based service) and support human or automated communication reminders. 2. The relatively small number of participants in planned early studies meant the devices, with a specific chosen version of the mobile operating system, could be purchased and distributed to end users. The design team selected a Motorola phone running Android API version 19 (KitKat). 3. The relatively small number of participants makes it easier to de-identify the data and manage it external to the app. A secret user interaction combined with a password protects access to functionality that supports exporting user interaction and task completion data (see above). Of course these assumptions will have to change in future generations of the platform to facilitate broader adoption. But as a dosage augmentation platform, the design team leveraged the weekly visits with the psychologists combined with the computational sophistication of modern smartphone platforms to provide a self-contained solution.

3.2.3.2 Progressive Reward Incentives
While one of the goals of the REACH protocol is to empower youth to be intrinsically motivated to enact the protocol, at the training stage it is imperative to repeat the dosage faithfully in order to attain this intrinsic motivation. A common gamification technique is to employ leveled rewards as an extrinsic motivator for performing a targeted behavior. Therefore a simple progressive (leveled) set of rewards for extrinsic motivation included in the app design. When an end user completes a task from the REACH protocol they get a reward in the form of Bob's abilities/tricks. This way the user is motivated to follow the protocol and completing the tasks (dosage) so s/he can unlock more complicated tricks for Bob. One concern SMEs raised during the design process was the potential to inadvertently punish the child for not performing a task. Given the domain, a design invariant was specified to keep all interactions with the child positive; therefore, all language and emotive expressions of Bob throughout the app were scrubbed to ensure there were no negative connotations. For progressive rewards, a setting in the app was designed to unlock new tricks twice every week. The presence of these tricks also served as extrinsic motivation for engagement.

3.2.3.3 Smartphone Notifications
Mobile platforms offer an "always on" communications channel between service providers and end users. Most categories of mHealth apps emphasize the communications channel between clinicians and patients, or between patients and automated big data platforms on the cloud. This project is unique in that it does not leverage the mobile device as a communications channel. In this generation of the app, the focus is on leveraging the device as an information collector and dosage vehicle for the protocol. In this sense the device serves more as a Personal Digital Assistant (PDA) than as a connected mobile phone. In this modality it is still important to present to the end user a feeling of connectedness. The personification of Bob the Blob as a proxy guide is one way the design provides this connectedness. As a second design concept, the design team wanted to make use of mobile notifications, but without relying on cloud-based push notifications as these would require a persistent network connection. Therefore the design supports local notifications presented to the end user in both fixed and adaptive schedules.

4. APP IMPLEMENTATION
The Android platform was selected to support the app. The openness of the Android platform, the availability of low-cost devices, the ease of the Google Speech API, and the ability to deploy the app without the involvement of an app store were the deciding factors for the first generation of the app. This section briefly describes the implementation on the Android platform. The final user interaction model combined with scheduled interactions per protocol rules is shown in Figure 2. This timeline in Figure 2 is based on weeks one to six of the REACH training program. Daily Diary, as the name suggests needs to be made available daily for all the six weeks whereas the Worryheads needs to be made available only in third, fourth and fifth week of the training program.

When the user selects the app from the Android home screen, a landing page is shown allowing the user to select from 5 available activities (see Figure 3, upper left). At any time only activities that are available can be selected from the landing page. Further, activities that are overdue are highlighted by a soft gold pulsing glow around the button (not shown) to provide a further visual cue to the end user to perform an activity. The S.T.I.C activity is shown in the upper right in Figure 3. In this activity end users are encouraged to do a task they would normally avoid due to their anxiety. In the paper protocol, once a child completes the activity s/he receives a physical stamp from an adult (usually a teacher or parent). In the app this was implemented as a secret code entered by the adult, who could then provide an electronic stamp of approval. The S.T.O.P. activity (Figure 3, mid-left) asks the child to provide responses to a set of questions (see section 2). Each response is stored in a SQLite database on the device. Figure 3, mid-right shows the "O" (Other Thoughts) step of the Worryheads game. This is basically a variant of the S.T.O.P. activity with preselected "S" and "T"s. The child has to consider the given "S" and "T" and select an appropriate "O" and "P" to complete the simulation. At the conclusion of these activities Bob the Blob praises the child (Figure 3, bottom right). The Daily Diary (Figure 3, bottom left) is a scheduled activity available to the child each day. The activity is available during school hours but notifications are only given after school hours. As described in section 2, the Daily Diary asks the child to reflect on potentially anxiety-provoking events from her/his day, and inquires about thoughts that came to mind in that situation. Youth also rate how s/he handled and felt about the situation. This embedded diary is part of the organizational framework of REACH emphasizing the need to identify and confront anxietyprovoking situations that are threatening but manageable. In addition to the 5 protocol activities available from the landing page, the end user also can tap directly on Bob the Blob and be taken to a table-oriented layout of "tricks" Bob can perform. The tricks (animations) available at any time are based on the protocol schedule as described in section 3.2.3.2. Additional features were provided by the app to support research outcomes (section 2). An on-device database stores all end user responses, and tracks each user action. The latter will be used after trials to answer research questions such as whether alarm fatigue occurred, or end users were not sufficiently motivated to engage with the app. A data export feature provided only to interventionists allows data to be offloaded as csv files. S.T.O.P. Worryheads Finally, in the face-to-face protocol trial, interventionists can personalize dosage schedules or tailor training activities during weekly visits. To support this in the app, a hidden feature was embedded only for the interventionist role. A specific multi-tap sequence combined with a secret PIN unlocks this feature so interventionists can decide if a protocol component should be enabled/disabled or otherwise modify the planned dosage for that week. Additional settings include selecting the start date of the protocol, notification time windows and frequency, the schedule trick release, changing the teacher PIN, and exporting data.

Figure 2: REACH App intervention Timeline

Landing page

S.T.I.C.

5. VALIDATION
Daily Diary Positive Reinforcement The highly iterative participatory design process described in section 3 enabled continuous feedback during app evolution. After completing the initial candidate release version, the design team and psychologists conducted two types of external validation. The first was two feedback sessions with external SMEs from a school

Figure 3: REACH App Interaction Screens

advisory board (SAB). The second was a usability study conducted with actual youth end users in the schools.

5.1 Advisory Board Feedback
The SAB consisted of two school psychologists with experience delivering REACH, and two school district administrators who oversee student services and prevention efforts for 47 K-8 schools. Based on their experience with youth, the SAB considered the developmental appropriateness of the design and program tools included (e.g., during the face to face sessions, youth wanted to utilize Relaxation and play Worryheads ondemand, so those activities were selected for inclusion in the app). From the SAB feedback, three issues emerged: 1. Safety and security - would youth have access to texting and Internet on the devices? 2. Cost: would parents be responsible for the devices, if lost? 3. Flexibility - would versions of the app be available for the iPhone, smartboards, and tablets? The first issue was addressed by adding security software SureLock to every device. The second was addressed by applying procedures used by the school relevant to laptop computers where parents are financially responsible. For flexibility, it was determined that preliminary data is necessary prior to investing in additional versions of the technology for different devices.

at a university laboratory or at their school. At the beginning of the study, each youth was provided with an envelope that contained a device and a questionnaire. After receiving the study materials, three phases (1-Listen to the Relaxation; play Worryheads game; 2-Write a daily-dairy or S.T.O.P. entry; 3-Play with the Blob) were implemented by trained research assistants. For a phase, each prescribed interactions with the app was 2minutes and responding to the survey lasted about 5 minutes. At the end, youth were thanked for their participation in the study, which lasted a total of 20 to 30 minutes. Parents of participant youth were provided with $15.00 at the end of the study.

5.2.4 Results
Descriptive statistics and correlations for the focal variables are given in Table 3. There were no missing data and some variables exceeded conventional cutoffs of |2| for skewness and |7| for kurtosis [16]: System Ease of Use (-3.04 skewness, 10.39 kurtosis), System Ease of Learning (-2.15 skewness; 3.9 kurtosis), and System Satisfaction (-2.23 skewness; 4.53 kurtosis). Moreover, statistically significant Shapiro-Wilks test values were found for these indicators and thus subsequent tests were conducted via non-parametric approaches. Specifically, Wilcoxon-Mann-Whitney tests were conducted to estimate any sex (boys vs. girls) or ethnicity/race (Hispanic/Latino vs. NonHispanic/Latino) variations in terms of: system ease of use, quality of support information, system ease of learning, and system satisfaction. No statistically significant mean differences were found suggesting robustness across sex and ethnicity/race. Table 3. Usability Study Results
Mean Overall Usability 35.69 1. SYSUSE 2. INFOQUAL 3. SYSEASE 4. SYSSATIS 8.94 9.13 8.72 8.90 sd 19.84 1.48 1.28 2.03 1.70 Median 38.23 9.24 9.67 9.41 9.75 -- .61** -.92** .80** -.47* .53* .48* -1 2 3 4

5.2 Usability Study
5.2.1 Participants
With parental consent (and assent from child), 22 youth (Mean age = 9.67 years, 12 girls, 12 Hispanic/Latino, 5 White, 1 Black, 1 Asian, 3 "other") from public schools participated in the `system usefulness, satisfaction, and ease' aspect of this research. The median household income was about $39,000 and most youth were recruited from the same zip code and class grades. In addition, 77% reported knowing how to use an Android smartphone and 54.5% reported playing games using a smartphone "all the time".

5.2.2 Measures
System usefulness, satisfaction, and ease were assessed via 22items from the Usefulness, Satisfaction, and Ease of Use Questionnaire [4] modified for children and adolescents. Youth responded to each item using a 10-point rating scale (1= "not at all" to 10 = "very much"). System ease of use (SYSUSE) was measured via 11 items (e.g., it is easy to use; it is simple to use), quality of support information (INFOQUAL) was measured via 3 items (e.g., instructions and messages are easy to understand; messages to fix problems are clear), system ease of learning (SYSEASE) was measured via 4 items (e.g., I easily remember how to use it; I quickly became good at it), and system satisfaction (SYSSATIS) was measured via 4 items (e.g., I am happy with this app; I would tell a friend about this app). Consistent with the original measure, alpha reliabilities were excellent: system ease of use ( = 0.92), quality of support information ( = 0.83), system ease of learning ( = 0.92), system satisfaction ( = 0.88), and stigma ( = 0.81) scale scores, and overall usability score ( = 0.95).

Note: Ranges from 0 to 40 for Overall Usability, 0 to 10 for other variables; SYSUSE = system ease of use; INFOQUAL = quality of support information; SYSEASE = system ease of learning; SYSSATIS = system satisfaction; *p< .05; **p< .01

5.2.3 Procedures
Parents (primary caregivers, legal guardians) received a letter from the research team describing the nature of the study and the timeframe for participation (within the next 7 to 10 days). From those contacted, 26% provided child consent and every child provided assent (n=22). Youth with consent/assent provided data

Given these findings, mean estimates for the total sample were calculated and results showed that the REACH app system was highly and positively rated, for the most part, along the four dimensions of interest: system ease of use, quality of support information, system ease of learning, and system satisfaction with means ranging from 8.72 to 9.13. Also, as shown in Table 3, statistically significant correlations were found among the four dimensions with correlation coefficients ranging from .47 to .80 (p < .05). Lastly, transforming SUSE-Y overall total scores into a traditional "grade" scale, analyses showed that the REACH app system earned an "A" grade from 55% of youth, "A-" from 14%, "B+" from 9%, "B" from 9%, and failing grades of "C-" or less from 13% (or 3 youth). Focusing those youth who rated the system with a "C-" grade or less, data showed that all three youth reported no knowledge of Android operating system. One of the three youth did not know how to connect the earbuds to the phone, had trouble placing earbuds in his ears, asked what he is supposed to press during the Worryheads, asked what the word "respond" means, and did not know what to press during the STOP task. Another seemed "lost" during Worryheads and the third youth was distracted by SureLock pop-ups during testing.

6. DISCUSSION
Our multidisciplinary, collaborative efforts resulted in a smartphone app to potentiate the prevention and early intervention of childhood anxiety disorders and related problems. To our knowledge this is the first research-based child anxiety prevention and early intervention app with known usability ratings. The FRIENDS for Life Program released an app for Android, but there is no research relevant to the technology developed. In child anxiety treatment, SmartCAT is a promising mhealth platform for ecological momentary intervention, used as an adjunct to the Coping Cat treatment program [11]. The REACH prevention app appears to be more similar than different to SmartCAT whereas the FRIENDS app is mostly psychoeducational. Focusing on prevention, for example, REACH and FRIENDS provide ondemand opportunities for skill practice but REACH explicitly focuses on reducing problematic anxiety at the indicated and early intervention level as it includes focused and direct features relevant to engaging youths in self-monitoring, in-vivo exposures, and cognitive self-control. In addition, REACH is capable of deploying notifications relevant to skill practice, offers tools for personalizing and tailoring the protocol (e.g., increase notifications, activate new tools based on performance, activate tools parallel to the weekly focal module), and allows for opportunities for corrective feedback based on user data amenable to creating personalized reports of youth weekly practice and response. When it comes to contrasting the SmartCAT treatment app with the REACH prevention app, both yielded high "ease of use" ratings. Moreover, as found in this research, the REACH prevention app yielded overall high ratings along additional dimensions not examined for FRIENDS or SmartCAT. That is, REACH showed high ratings for quality of support information, system ease of learning, and system satisfaction. Also, this research found no significant differences between boys and girls or between Hispanic/Latino and non-Hispanic/Latino youth on any of the usability dimensions examined. The REACH app appears promising and has the potential to study questions not only relevant to potentiating program response and refining aspects of the technology, but about large scale diffusion, personalized care, and bridging the gap in health disparities when it comes to affective problems and its related disease outcomes. The version of the app described in this paper was designed and created through a multidisciplinary process that is user-centered in the broad interpretation of the process. Our subsequent plans for the REACH app include incorporating patients, caregivers, and interventionists directly into the design process, and broadening its applicability to minority populations, populations with sleep disorders, and studying the potential for positive remedies for negative outcomes of anxiety, notably drug abuse.

[4] International Organization for Standardization, 2008. Ergonomics of human system interaction-Part 210: Humancentred design for interactive systems (formerly known as 13407). [5] Lund, M. 2001. Measuring usability with the USE questionnaire. http://ww2.stcsig.org/usability/newsletter/0110_measuring_ with_use.html [6] Murray, T., Hardy, D., Spruijt-Metz, D., Hekler, E., and Raij, A. 2013. Avatar interfaces for biobehavioral feedback. Design, User Experience, and Usability. Health, Learning, Playing, Cultural, and Cross-Cultural User Experience Berlin Heidelberg: Springer [7] Norman, D. A., and Draper, S. W. 1986. User-Centered System Design: New Perspectives on Human Computer Interacti. Hillsdale N.J. : Lawrence Erlbaum Associates. [8] Pina, A. A., Silverman, W. K., Fuentes, R. M., Kurtines, W. M., and Weems, C. F. 2003. Exposure-based cognitivebehavioral treatment for phobic and anxiety disorders: Treatment effects and maintenance for Hispanic/Latino relative to European-American youths. Journal of the American Academy of Child & Adolescent Psychiatry, 42(10), 1179-1187. [9] Pina, A. A., Zerr, A. A., Villalta, I. K., and Gonzales, N. A. (2012). Indicated prevention and early intervention for childhood anxiety: A randomized trial with Caucasian and Hispanic/Latino youth. Journal of consulting and clinical psychology, 80(5), 940. [10] Pinto, M. D., Greenblatt, A. M., Hickman, R. L., Rice, H. M., Thomas, T. L., and Clochesy, J. M. 2015. Assessing the Critical Parameters of eSMART-MH: A Promising AvatarBased Digital Therapeutic Intervention to Reduce Depressive Symptoms. Perspectives in Psychiatric Care, n/a-n/a. [11] Pramana, G., Parmanto, B., Kendall, P. C., and Silk, J. S. 2014. The SmartCAT: An m- Health Platform for Ecological Momentary Intervention in Child Anxiety Treatment. Telemedicine and E-Health, 20(5), 419-427. [12] Silverman, W. K., Kurtines, W. M., Jaccard, J., and Pina, A. A. (2009). Directionality of change in youth anxiety treatment involving parents: an initial examination. Journal of Consulting and Clinical Psychology, 77(3), 474. [13] Silverman, W. K., Pina, A. A., and Viswesvaran, C. 2008. Evidence-based psychosocial treatments for phobic and anxiety disorders in children and adolescents. J Clin Child Adolesc Psychol, 37(1), 105-130. [14] Vredenburg, K., Mao, J., Smith, P. W., and Carey, T. 2002. A Survey of User-Centered design Practice. Paper presented at the Proceedings of the 2002 ACM Symposium on ComputerHuman Interaction (CHI 2002), Minneapolis, MN, April 2002., Minneapolis, MN. [15] Wang, J. T., Wang, Y. Y., Wei, C. L., Yao, N. L., Yuan, A., Shan, Y. Y., and Yuan, C. R. 2014. Smartphone Interventions for Long-Term Health Management of Chronic Diseases: An Integrative Review. Telemedicine and EHealth, 20(6), 570-583. [16] West, S. G., Finch, J. F., and Curran, P. J. 1995. Structural equation models with nonnormal variables: Problems and remedies.

7. REFERENCES
[1] Barrett, P., and Turner, C. 2001. Prevention of anxiety symptoms in primary school children: preliminary results from a universal school-based trial. Br J Clin Psychol, 40(Pt 4), 399-410. [2] Chavira, D. A., Stein, M. B., Bailey, K., and Stein, M. T. 2003. Parental opinions regarding treatment for social anxiety disorder in youth. J Dev Behav Pediatr, 24(5), 315322. [3] Fisak, B. J., Richard, D., and Mann, A. 2011. The prevention of child and adolescent anxiety: a meta-analytic review. Prev Sci, 12(3), 255-268.

Published online 4 November 2013

Nucleic Acids Research, 2014, Vol. 42, Database issue D789­D793 doi:10.1093/nar/gkt1063

WormBase 2014: new views of curated biology
Todd W. Harris1,*, Joachim Baran1, Tamberlyn Bieri2, Abigail Cabunoc1, Juancarlos Chan3, Wen J. Chen3, Paul Davis4, James Done3, Christian Grove3, Kevin Howe4, Ranjana Kishore3, Raymond Lee3, Yuling Li3, Hans-Michael Muller3, Cecilia Nakamura3, Philip Ozersky2, Michael Paulini4, Daniela Raciti3, Gary Schindelman3, Mary Ann Tuli4, Kimberly Van Auken3, Daniel Wang3, Xiaodong Wang3, Gary Williams4, J. D. Wong1, Karen Yook3, Tim Schedl5, Jonathan Hodgkin6, Matthew Berriman7, Paul Kersey4, John Spieth2, Lincoln Stein1 and Paul W. Sternberg3,8
1

Informatics and Bio-computing Platform, Ontario Institute for Cancer Research, Toronto, ON M5G0A3, Canada, 2Genome Sequencing Center, Washington University, School of Medicine, St Louis, MO 63108, USA, 3 Division of Biology and Biological Engineering 156-29, California Institute of Technology, Pasadena, CA 91125, USA, 4European Molecular Biology Laboratory, European Bioinformatics Institute, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK, 5Department of Genetics Campus, Washington University School of Medicine, St. Louis, MO 63110, USA, 6Genetics Unit, Department of Biochemistry, University of Oxford, Oxford OX1 3QU, UK, 7Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SA, UK and 8Howard Hughes Medical Institute, California Institute of Technology, Pasadena, CA 91125, USA
Received October 10, 2013; Accepted October 12, 2013

Downloaded from http://nar.oxfordjournals.org/ by guest on December 20, 2016

ABSTRACT WormBase (http://www.wormbase.org/) is a highly curated resource dedicated to supporting research using the model organism Caenorhabditis elegans. With an electronic history predating the World Wide Web, WormBase contains information ranging from the sequence and phenotype of individual alleles to genome-wide studies generated using nextgeneration sequencing technologies. In recent years, we have expanded the contents to include data on additional nematodes of agricultural and medical significance, bringing the knowledge of C. elegans to bear on these systems and providing support for underserved research communities. Manual curation of the primary literature remains a central focus of the WormBase project, providing users with reliable, up-to-date and highly crosslinked information. In this update, we describe efforts to organize the original atomized and highly contextualized curated data into integrated syntheses of discrete biological topics. Next, we discuss our experiences coping with the vast increase in available genome sequences made

possible through next-generation sequencing platforms. Finally, we describe some of the features and tools of the new WormBase Web site that help users better find and explore data of interest. INTRODUCTION Caenorhabditis elegans is a free-living soil nematode found throughout the world. Its small size (1 mm), rapid generation time (3 days), simple nervous system and invariant developmental program have made it a well-known system for studying a broad array of biological problems [(1,2); http://www.wormbook.org]. WormBase aims to facilitate and accelerate research using C. elegans through a process of deliberate and detailed curation of the primary literature. When launched, WormBase expanded prior community-driven curation to touch on virtually every aspect of classical and modern experimental biology, including next-generation sequence and high-throughput data. As these efforts continue, we are expanding our focus to create synthesized views of the scientific knowledge contained in WormBase. These `biological topics' represent large and complex problems not readily described through gene-by-gene curation and not always represented in the primary literature.

*To whom correspondence should be addressed. Tel: +1 406 222 2450; Fax: +1 801 784 8466; Email: todd@wormbase.org
ß The Author(s) 2013. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

D790 Nucleic Acids Research, 2014, Vol. 42, Database issue

Next-generation sequencing technology has had a tremendous impact on the direction of curatorial efforts at WormBase. These include an exploration of natural variation in C. elegans and a constant stream of whole-genome sequences and preliminary annotation from related species. We balance inclusion of these data sets based on potential value to our user community and resources required to adequately import data into WormBase. To support increased demand for WormBase, changing user expectations and constantly growing data requirements, we have redesigned the WormBase Web site from the ground up. Launched in March 2012, the new site offers users the option to customize the content and arrangement of pages to suit individual needs. WormMine, a new data mining tool using the Intermine data mining platform, was launched offering new options for querying and interacting with data at WormBase. BIOLOGICAL TOPICS CURATION For 13 years, WormBase curators have been collecting data of various types pertaining to the biology of C. elegans and related nematodes. These data types have included gene models, allelic variations, mutant phenotypes, anatomy function, expression patterns, gene interactions (physical, genetic and regulatory) and, more recently, human disease relevance. Although these various data types have existed predominantly in isolation, WormBase is now aiming to synthesize integrated views of these data in the form of `Biological Topics', big-picture perspectives that draw together all data relevant to a biological topic of interest. C. elegans has proven to be a tremendously useful model organism for the study of many topics, including cell death and differentiation, embryogenesis, organ development and aging. Much of this important research has been summarized in the online `WormBook' (http://www.wormbook.org/), a collection of review articles written by the nematode research community. The content of WormBook has inspired the creation of the first generation of WormBase Biological Topics, including behavioral topics such as locomotion, foraging and male mating; cellular topics such as cell fusion, cell migration and cell death; and signaling pathway topics such as RTK/Ras/ MAPK, EFG and Notch. Researchers who come to the WormBase Web site with a particular goal of understanding how nematode research has informed a particular field of study are now able to explore WormBase data from a perspective that most closely pertains to their field of inquiry. Whether researching a human disease or studying a molecular mechanism, users can search for their topic and review the relevant WormBase data in an intuitive manner. Each WormBase Biological Topic has a dedicated web page for displaying all relevant WormBase entities. In addition to a curatorgenerated text summary, the page lists relevant genes, phenotypes, anatomy terms, life stages, gene expression clusters, interactions, molecules (small molecules, chemicals, drugs), Gene Ontology (GO) terms, human diseases and publications. The connections of WormBase entities,

such as genes or phenotypes, to a particular Biological Topic are curator confirmed, ensuring high-quality annotations. A cytoscape-based interaction viewer allows users to see all genetic, physical and regulatory interactions that affect the topic of interest. These interaction network views can be filtered to allow closer inspection of certain types of interactions (regulatory versus genetic) or associated phenotypes (for genetic interactions). In addition, the Biological Topic page may include one or more depictions of relevant pathways, whether they be molecular signaling pathways, or more large-scale cell­cell interaction pathways, for example. WormBase works with WikiPathways (http://www.wikipathways.org) to generate pathway diagrams for C. elegans and related nematodes to be displayed on WormBase Biological Topic pages. The WikiPathways approach provides the benefit that many WormBase curators or experts in the field may simultaneously create, develop and maintain a common pathway, or depict alternate pathways, of a Biological Topic. WormBase curators will review nematode pathways on WikiPathways and provide official approval to pathways that meet certain quality criteria, such as proper citations and evidence. Once approved, these pathways will be incorporated into WormBase Biological Topic pages. WikiPathways has a specific WormBase `Portal' page (http://www.wikipathways.org/index.php/Portal: WormBase) that directly links users to nematode pathways of interest. WikiPathways currently houses >50 C. elegans pathways, nine of which are WormBase approved. In an effort to coordinate curation effort and most effectively synthesize the Biological Topic pages described earlier, the WormBase literature curation pipeline has undergone some changes. Previously, curators went through publications paper by paper to extract specific data types. Now, we concentrate on one Biological Topic at a time, extracting all relevant data in the literature. From this collection of information, we can then generate the most comprehensive and up-to-date view of the topic. GENOMES AND SPECIES The C. elegans reference genome and sequence annotation Careful manual curation of the C. elegans reference genome sequence and annotation continues to be a key activity for WormBase. We have recently released a new version of the reference genome (WBcel235) that includes 1402 corrections, drawn and reviewed from a number of independent projects that have re-sequenced the Bristol N2 reference strain (3­5). Active refinement of the canonical gold-standard set of structures for protein-coding genes, non-coding RNA genes, pseudogenes, transposons and operons also continues, using experimental data drawn from a wide variety of sources and tools developed within project (6,7). C. elegans natural variation The past 2 years have seen rapid growth in volume and diversity of nematode genomic variation data, in large part due to various projects engaged in whole-genome sequencing of hundreds of C. elegans wild-isolate strains

Downloaded from http://nar.oxfordjournals.org/ by guest on December 20, 2016

Nucleic Acids Research, 2014, Vol. 42, Database issue

D791

(8,9). We have responded to this challenge by making changes to the way in which we curate, store and display variation data. One significant change has been to clearly distinguish between naturally occurring polymorphisms and laboratory-induced mutations, at both the database and display levels. We have also consolidated redundant data from independent wild-isolate sequencing projects, creating reference variation records that collate all studies that have characterized a specific molecular variant, and all strains that carry it. This has increased the efficiency of our storage and computation, and has also allowed us to provide more meaningful and intuitive displays for the data. Other nematodes The manual curation of primary annotation for other nematode species is directed by user requests and perceived impact. Accordingly, we have begun to prioritize key parasitic species of direct relevance to human health. As a pilot project, we performed a first-pass annotation of the genome of Brugia malayi, a causative agent of lymphatic filariasis, manually reviewing nearly 3000 gene loci (21% of all genes) within a 6-month period. Working in collaboration with the filariasis community via FR3 (NIAID Filariasis Research Reagent Resource Center), targets for manual curation were identified based on their likely importance to the research community (e.g. putative drug targets; putative essential genes, based on C. elegans orthology; protein kinases; and transcription factors). WormBase now houses the reference genomic sequence annotation for >20 nematode species. A number of these data sets originate from third-party genome sequencing and annotation projects, and WormBase's role is primarily to add value via a number of computation analyses, display the data and make it available in standard formats. The basic workflow for integrating a genome into WormBase comprises (i) review and quality control of the primary submitted data, (ii) deployment of computational pipelines to provide additional first-pass functional annotation of the gene products and predictions of orthology and paralogy to other nematode genes and (iii) the provision of a genome browser, as well as data files in standard formats (e.g. FASTA, GFF v3) made available via our FTP site (ftp://ftp.wormbase.org). Species that we have recently brought into WormBase in this way are Heterorhabditis bacteriophora [a nematode used in horticulture (10)], Bursaphelenchus xylophilus [the pine wilt nematode (11)], Loa loa [a causative agent of Loa loa filariasis (12)], Panagrellus redivivus [the `sour paste' nematode (13)] and Dirofilaria immitis [the dog heartworm (14)]. Owing to diminishing costs of sequencing, it is now becoming more common to see the initiation of multiple independent reference genome projects for a single species. This is exemplified by the cases of two particular nematode species: Ascaris suum, for which two independent projects have each sequenced a different tissue (15,16); and Haemonchus contortus, for which two independent projects have each sequenced a different key strain (17,18). To distinguish between different genome

projects on our FTP site and Web site displays and services, we use the NCBI BioProject accession (http:// www.ncbi.nlm.nih.gov/bioproject), which is guaranteed to be a unique handle.

IMPROVEMENTS TO THE WEB SITE To address increased demand for the Web site and the need to store and present growing amounts of data, we rebuilt the WormBase Web site from the ground up. Released on 30 March 2012, this rewrite included not only a brand new user interface but also new searching tools and increased user support. A new back-end architecture provides support for the site and we have begun migration of hosting to the Amazon AWS cloud. User interface We designed the new user interface with the following objectives: (i) the interface should be as species-agnostic as possible, removing the emphasis on C. elegans when appropriate, (ii) the interface should be customizable and allow users to select which types of data they wish to see and (iii) the interface should be future-forward and permit facile changes to the content and display. As mentioned earlier, our primary user community remains researchers using C. elegans as a model system. Reworking the Web site to accommodate additional species serves two purposes. First, comparative approach against closely related species is a typical use case for studying gene function and genome architecture in C. elegans. Second, by de-emphasizing C. elegans, we have made it possible to more easily support underserved research communities studying nematodes of agricultural and medical significance. To accomplish this, we added a site-wide `Species' option on the main navigation bar. From here, users can toggle between species from any location on the site, see genome assembly and version information, jump directly to customized report pages and searches and so on. Precomputed homology and orthology relationships provide further means for moving quickly between species. As the number of species and extent of data housed at WormBase continue to grow, we wanted to both create data-rich reports and also allow users to pick and choose which data are most important to them, as well as control its presentation on the page. On report pages (say, for a given gene), a navigational sidebar lists the available `widgets' for that page. When a user clicks on a widget title, the corresponding widget opens. Widgets can be rearranged on the page by drag-and-drop, collapsed and dismissed as needed. A flexible single or multicolumn layout lets users build the perfect page report for the research question at hand. For users who have chosen to log in to the site, layout settings persist between sessions. Many other options for interactivity and customization have been built in to the new site. Users can log in using Google, Facebook or local WormBase credentials. Once logged in, they may save favorite pages (My Favorites) and papers (My Library).
Downloaded from http://nar.oxfordjournals.org/ by guest on December 20, 2016

D792 Nucleic Acids Research, 2014, Vol. 42, Database issue

We enhanced the ease of finding content at WormBase by building a custom search engine powered by Xapian (www. xapian.org). Users can conduct full-text searches on Worm Base, and retrieve faceted results broken down by data type (e.g. gene, molecule), paper type (e.g. review, journal, article) and species. The results can be further filtered by type or species, or downloaded for further analysis. The new interface also introduces elements created to help foster community engagement. Every page has a feedback tab prompting users to leave feedback, submit content corrections, report bugs or ask for help. Furthermore, each report page has a place for public comments, creating a low participation-barrier community annotation system. The Perl web framework Catalyst (www.catalystframe work.org) provides the core of the new site. A ModelView-Controller design implementation effectively separates concerns and allows us to create different presentations when accessing the same data. In this manner, the WormBase site can easily continue to evolve to meet user needs and expectations. Back-end architecture WormBase continues to rely on AceDB as the primary platform for data integration and distribution. This single-threaded database management system is >20 years old and built before the era of multi-species whole-genome sequences and annotation. We have encapsulated the role of AceDB in the new Web site architecture by building a RESTful Application Programming Interface (API) into our application that consumes data from AceDB and supplementary MySQL databases, returning data properly structured for presentation. This encapsulation effectively decouples the Web site from the back-end store, opening the door for us to migrate to a new system in the near future. One migration path that we have begun to explore uses the NoSQL document store CouchDB (couchdb.apache.org). In our current application, we precompute computationally intensive displays (using the RESTful API) and store the data in CouchDB as an efficient data cache. AceDB is only accessed when data do not already exist in CouchDB. We have extended this proof of concept by rewriting the Perl interface to AceDB (AcePerl) to use data loaded into a standalone CouchDB instance (Ace::Couch), completely removing the requirement for AceDB to drive the Web site. Improvements to the WormBase Web site have not been limited to software upgrades. Most significantly, we have begun to move the entirety of the WormBase Web site to hosting on Amazon's commercial cloud computing services. Services such as Elastic Cloud Compute (EC2) are well suited for hosting our non-sensitive information and simplify many aspects of managing the Web site. Administrative costs--both in time and money--of hosting and maintaining the Web site are greatly reduced from traditional on-site environments. Because pricing models use a `pay for what you use' scheme, the costs of hosting in the cloud are comparable to or cheaper than institutional hosting when factoring in overhead costs.

Moreover, additional storage and compute capacity can be added (and later removed) as needs arise without incurring capital expense. Cloud-based data are easily versioned and inexpensively archived through the use of snapshots. Finally, cloud resources can be launched in various geographical locations to provide better performance for users in different areas of the globe. New data visualization and mining tools The new Web site architecture allows us to easily maintain and add new tools to the Web site. For example, popular pre-existing tools such as GBrowse and BLAST/BLAT tools were retrofitted to work with the new site structure. We have expanded the options for data mining in two significant ways. First, we have launched an instance of the InterMine [www.intermine.org; (19)] data warehousing and mining platform called WormMine. WormMine gives users new ways to query data, save and manipulate lists of objects and download data en masse. WormMine also increases the interoperability of WormBase with other model organism databases that have built their own InterMine instances. Second, we have opened the same RESTful API that we use to build the Web site. Developers can consume this API to create their own presentations of the WormBase data. Researchers can use this to programmatically retrieve WormBase data in a variety of formats. Community and user support With the release of the new Web site, we have made it simpler for users to interact with WormBase developers and curators. A `Questions, Feedback & Help' tab is visible on every page on WormBase. Submitting a query here is integrated with our mail-based help desk. User forums, Twitter, a blog and webcasts augment the direct user support that WormBase provides. FUTURE DIRECTIONS The WormBase curation strategy, build process and Web site continue to evolve in response to user feedback and technical requirements. In the near future, we plan to finish relocating the Web site to the Amazon cloud. We are continuing to explore back-end replacement options for the two roles AceDB plays at WormBase: as the primary data integration platform and as a data source that drives the Web site. To accommodate increasing numbers of users accessing the WormBase Web site, we will shortly launch a version of the site optimized for mobile use to be followed by native applications for both Android and iOS. FUNDING The US National Human Genome Research Institute [U41-HG002223] to WormBase and British Medical Research Council [G070119] to WormBase; P.W.S. is an investigator with the Howard Hughes Medical Institute. Funding for open access charge: US National Human Genome Research Institute [U41-HG002223].

Downloaded from http://nar.oxfordjournals.org/ by guest on December 20, 2016

Nucleic Acids Research, 2014, Vol. 42, Database issue

D793

Conflict of interest statement. None declared. REFERENCES
1. Brenner,S. (1974) The genetics of Caenorhabditis elegans. Genetics, 77, 71­94. 2. Riddle,D.L., Blumenthal,T., Meyer,B.J. and Priess,J.R. (1997) C. elegans II. Cold Spring Harbor Laboratory Press, Cold Spring Harbor, NY. 3. McGrath,P.T., Xu,Y., Ailion,M., Garrison,J.L., Butcher,R.A. and Bargmann,C.I. (2011) Parallel evolution of domesticated Caenorhabditis species targets pheromone receptor genes. Nature, 477, 321­325. 4. Doitsidou,M., Poole,R.J., Sarin,S., Bigelow,H. and Hobert,O. (2010) C. elegans mutant identification with a one-step whole-genome-sequencing and SNP mapping strategy. PLoS One, 5, 15435. 5. Weber,K.P., De,S., Kozarewa,I., Turner,D.J., Babu,M.M. and de Bono,M. (2010) Whole genome sequencing highlights genetic changes associated with laboratory domestication of C. elegans. PLoS One, 5, e13922. 6. Howe,K., Davis,P., Paulini,M., Tuli,M.A., Williams,G., Yook,K., Durbin,R., Kersey,P. and Sternberg,P.W. (2012) WormBase: annotating many nematode genomes. Worm, 1, 15­21. 7. Williams,G.W., Davis,P.A., Rogers,A.S., Bieri,T., Ozersky,P. and Spieth,J. (2011) Methods and strategies for gene structure curation in WormBase. Database, 2011, baq039. 8. Andersen,E.C., Gerke,J.P., Shapiro,J.A., Crissman,J.R., Ghosh,R., ´ lix,M.A. and Kruglyak,L. (2012) Chromosome-scale Bloom,J.S., Fe selective sweeps shape Caenorhabditis elegans genomic diversity. Nat. Genet., 44, 285­290. 9. Thompson,O., Edgley,M., Strasbourger,P., Flibotte,S., Ewing,B., Adair,R., Au,V., Chaudhry,I., Fernando,L., Hutter,H. et al. (2013) The million mutation project: a new approach to genetics in Caenorhabditis elegans. Genome Res., 23, 1749­1762. 10. Bai,X., Adams,B.J., Ciche,T.A., Clifton,S., Gaugler,R., Kim,K.S., Spieth,J., Sternberg,P.W., Wilson,R.K. and Grewal,P.S. (2013) A lover and a fighter: the genome sequence of an entomopathogenic nematode Heterorhabditis bacteriophora. PLoS One, 8, e69618.

11. Kikuchi,T., Cotton,J.A., Dalzell,J.J., Hasegawa,K., Kanzaki,N., McVeigh,P., Takanashi,T., Tsai,I.J., Assefa,S.A., Cock,P.J. et al. (2011) Genomic insights into the origin of parasitism in the emerging plant pathogen Bursaphelenchus xylophilus. PLoS Pathog., 7, e1002219. 12. Desjardins,C.A., Cerqueira,G.C., Goldberg,J.M., Dunning Hotopp,J.C., Haas,B.J., Zucker,J., Ribeiro,J.M., Saif,S., Levin,J.Z., Fan,L. et al. (2013) Genomics of Loa loa, a Wolbachia-free filarial parasite of humans. Nat. Genet., 45, 495­500. 13. Srinivasan,J., Dillman,A.R., Macchietto,M.G., Heikkinen,L., Lakso,M., Fracchia,K.M., Antoshechkin,I., Mortazavi,A., Wong,G. and Sternberg,P.W. (2013) The draft genome and transcriptome of Panagrellus redivivus are shaped by the harsh demands of a free-living lifestyle. Genetics, 193, 1279­1295. 14. Godel,C., Kumar,S., Koutsovoulos,G., Ludin,P., Nilsson,D., Comandatore,F., Wrobel,N., Thompson,M., Schmid,C.D., Goto,S. et al. (2012) The genome of the heartworm, Dirofilaria immitis, reveals drug and vaccine targets. FASEB J., 26, 4650­4661. 15. Jex,A.R., Liu,S., Li,B., Young,N.D., Hall,R.S., Li,Y., Yang,L., Zeng,N., Xu,X., Xiong,Z. et al. (2011) Ascaris suum draft genome. Nature, 479, 529­533. 16. Wang,J., Mitreva,M., Berriman,M., Thorne,A., Magrini,V., Koutsovoulos,G., Kumar,S., Blaxter,M.L. and Davis,R.E. (2012) Silencing of germline-expressed genes by DNA elimination in somatic cells. Dev. Cell, 23, 1072­1080. 17. Laing,R., Kikuchi,T., Martinelli,A., Tsai,I.J., Beech,R.N., Redman,E., Holroyd,N., Bartley,D.J., Beasley,H., Britton,C. et al. (2013) The genome and transcriptome of Haemonchus contortus, a key model parasite for drug and vaccine discovery. Genome Biol., 14, R88. 18. Schwarz,E.M., Korhonen,P.K., Campbell,B.E., Young,N.D., Jex,A.R., Jabbar,A., Hall,R.S., Mondal,A., Howe,A.C., Pell,J. et al. (2013) The genome and developmental transcriptome of the strongylid nematode Haemonchus contortus. Genome Biol., 14, R89. 19. Smith,R.N., Aleksic,J., Butano,D., Carr,A., Contrino,S., Hu,F., Lyne,M., Lyne,R., Kalderimis,A., Rutherford,K. et al. (2012) InterMine: a flexible data warehouse system for the integration and analysis of heterogeneous biological data. Bioinformatics, 28, 3163­3165.

Downloaded from http://nar.oxfordjournals.org/ by guest on December 20, 2016

CAPTURE - Cooperatively Applied Positioning Techniques Utilizing Range Extensions

Item type Authors Publisher Downloaded Item License Link to item

Article Cullen, Gary IEEE 20-Dec-2016 14:11:14 http://creativecommons.org/licenses/by-nc-nd/4.0/ http://hdl.handle.net/10759/325964

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014

CAPTURE - Cooperatively Applied Positioning Techniques Utilizing Range Extensions
Gary Cullen, Kevin Curran, Jose Santos
Intelligent Systems Research Centre, University of Ulster, Magee College, Derry, United Kingdom Gary.Cullen@lyit.ie
Abstract-- The most commonly implemented Indoor Location Based Solution uses existing Wi-Fi network components to locate devices within its range. While this technique offers obvious economic rewards by utilizing a preinstalled infrastructure, these network topologies were typically designed to provide network coverage to mobile devices rather than deliver an Indoor Location Based Solution. Large areas without coverage are common in these networks because network designers were not typically concerned about providing 100% coverage for mobile data. Hallways, toilet areas or other general purpose areas that ordinarily would not require network coverage did not get dedicated WAPs installed. Transient users navigating these areas of the network were un-locatable using this infrastructure. Furthermore the indoor arena is an especially noisy atmosphere, being home to other wireless devices such as Bluetooth Headsets, Cordless Phones and Microwave Ovens. Considering users spend more time in an indoor environment, over 88%, the need for a solution is obvious. Therefore, we propose a solution to resolve the issue of restricted coverage of Indoor Location Based solutions, using a cooperative localization technique Cooperatively Applied Positioning Techniques Utilizing Range Extension (CAPTURE). CAPTURE offers a method of locating devices that are beyond the range of the current in-house location based solution. It presents a unique contribution to research in this field by offering the ability to utilize devices that know their location within a Location Based Solution (LBS), to evaluate the position of unknown devices beyond the range capacity of the LBS. This effectively extends the locating distances of an Indoor LBS by utilizing the existing mobile infrastructure without the need for any additional hardware. The proliferation of smart phones and the tablet form factor, bundled with Wi-Fi, Bluetooth and gyroscopes ­ technologies currently used to track position, provide a fertile community for CAPTURE to cooperatively deliver a location solution. Keywords -- Localisation; Indoor positioning; localisation; geographical positioning; wireless. Indoor

Gearoid Maguire, Denis Bourne
Letterkenny Institute of Technology, Co. Donegal, Ireland

everywhere you were at a given time. Googles manoeuvring into the indoor location mappings realm [1] opens up the opportunity to deliver this virtual reality, currently being able to provide door to door route planning. Being able to navigate your way from your office desk out through your company's building (taking the stairwell to avoid your boss in the lift) is eminently achievable albeit with a small number of locations on a modern smartphone using google maps. A level switcher allows you to onion slice through multiple floor level plans, before switching to GPS to offer possible transport alternatives through the outdoor environment. On reaching what `historically' would have been your destination, Google Indoor Maps and more importantly an Indoor Positioning System (IPS) picks up where GPS left off offering a point to point navigation solution. This can then take you through the complexities of an airport terminal for example, via specific waypoints such as security and check-in desks directly to your departure gate. One of the barriers to implementation of such a concept is the limitation in coverage and accuracy of currently implemented Indoor Position or Location Based Systems [2]. IPSs typically utilize pre-existing Wi-Fi network infrastructure taking ranging information from Wireless Access Points (WAP's) as inputs for a localization algorithm. Unfortunately the drivers behind the strategic decisions on the positioning of WAPs, in a Wi-Fi based solution, were typically to catch large congregations of users and primarily to provide the highest available throughput to those users. Coverage for IPSs is not necessarily to the forefront of network designer's minds when designing such networks, leaving large areas beyond the range of an IPS. GPS on the other hand, offers near global coverage, bar some issues with urban canyons and other high rise natural obstacles that prevent Line of Sight (LoS) to the just under 30 satellites required [3] to deliver such wide scope. The indoor environment does not afford such clear unobstructed views to and from tracking devices, the many doors, walls, floors, pillars and ceilings hinder the capacity of an IPS to locate devices. Furthermore the indoor arena is an especially noisy atmosphere, being home to other wireless devices such as Bluetooth Headsets, Cordless Phones and Microwave Ovens. All of these devices operate in the same frequency band as the Wi-Fi solution, namely 2.4 GHz and therefore can interfere with the reception of signals used to locate [2], making them behave in an unpredictable fashion.

I.

Introduction

On loosing something or forgetting where you last placed something, a common piece of advice is to retrace your steps back in your mind. This can be quite a formidable task given the multimodal transport available today coupled with the complexity and scale of buildings we interact with on a regular basis. The ability to place an avatar of yourself onto a map to graphically retrace your steps in real-time would dramatically reduce the brain power required to remember

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014 These environmental dynamics combine to dramatically affect the ability of an indoor solution to provide an acceptable level of coverage. Literature from Yang [4] and Rowe [5] reflect that Location Awareness is rapidly becoming a fundamental requirement for mobile application development. This highlights the challenges posed for ubiquitous localization of devices in the indoor arena. Considering users spend more time in an indoor environment, over 88.9% according to a recent Canadian study [6] , the need for a solution is obvious. We propose a solution to this issue of coverage limitations by using a cooperative localization technique, CAPTURE. CAPTURE can plug into an in situ solution irrespective of the technology or location technique that solution currently uses to locate. It provides a location relative to the devices locating it, which can then be mapped onto a global overview of the Location Based System (LBS), assisting in the aforementioned scenario to get you to the departure gate in a point to point navigation solution. Consider the following scenario where a user `Bob', is in his favorite seat in the library, unfortunately the seat is in the far corner of the library, which can only be `seen' by one Wireless Access Point. In this position Bob's tablet can gain Wi-Fi access through this Access Point to allow him access to online resources. However one Access Point is not enough for the in-house Location Based System to accurately locate Bob within the building using Trilateration positioning techniques. Sue is sitting near the front of the library and can be `seen' by 4 Wireless Access Points, and is thereby accurately located on the Location Based System. She is also 25 meters to the left of Bob and the Wireless Network Card on her Laptop can see Bob's tablet. The Librarian is stacking books on the shelves behind where Bob is sitting and her smartphone is currently located within the Location Based System also. The wireless NIC on her smartphone can also `see' Bob's tablet, therefore, in a normal scenario, Bob would be beyond the range of the Location Based System, but because CAPTURE can use the known positions of the Librarian and Sue and Bob's position relative to them it can accurately estimate Bob's position within the library. The rest of this paper is laid out as follows; Section II describes the system model used to implement CAPTURE. Section III provides an overview of the experimental test bed used to evaluate the solution and Section IV documents the data collected during test. In Section V we describe the findings of the experiments that were carried out validating the feasibility of the system, the penultimate section, Section VI outlines the proposed implementation of CAPTURE and the paper closes with a conclusion in Section VII, providing an insight into some projected future work with CAPTURE.

Figure 1: An Danlann Sports Hall LyIT

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014

II. CAPTURE - System Model
This section describes a system model that can be used in a localization solution to establish the Cartesian coordinate values of a lost device within a two dimensional plane. CAPTURE does not require a preceding calibration stage or a site survey, providing a robust opportunistic solution in dynamic environments, using only real time RSSI values without changing the IEEE 802.11 network. Literature within the realm of Location Based Systems frequently use terms such as Anchor or Anchor Nodes to describe devices that help to determine the position of lost or unknown devices. The term anchor elicits a perception of a static or permanent device, which in a cooperative solution these devices most certainly are not. For this reason we will use the term reference device when describing devices that assist in the positioning of lost or unknown devices. Two key components typically make up the estimation of the position of a lost device. First of all ranging techniques are used to estimate the distance from the transmitting device(s) to the receiving device(s). This is calculated using a metric for example the length of time it takes a signal to propagate the distance from the transmitter to the receiver. The second component is the position estimation technique, here the ranging variables are calculated using one or more ranging techniques and these are used as input for an estimation algorithm (mathematical formulae) to calculate the position of the lost device. A. RSSI ­ Received Signal Strength Indicator Possibly the most popular ranging technique used in Indoor Localization, Received Signal Strength Indicator (RSSI) is a measurement of the voltage that exists in a transmitted radio signal, which is an indication of the power being received by the antenna. When a signal first leaves a transmitting device, the power of the signal drops or attenuates, this is true of both wired and wireless transmissions. As a radio signal propagates through the air some of its power is absorbed and the signal loses a specific amount of its strength, therefore, the higher the RSSI value (or least negative in some devices), the stronger the signal. Knowing the amount of signal loss over a given distance provides a method to calculate the distance from a transmitting device, given a Received Signal Strength. At its most basic level this allows for the `coarse' localization or as referred to in other literature, `presence-based localization' [7] of a device relative to the transmitting device. This can be illustrated by the RSSI calculated distance being the radius of a circle and the `searching' device being at the center of that circle. The estimated position of the lost device is anywhere on the circumference of that circle. In an IEEE 802.11 network if the locations of the Access Points are already known, then the location of Mobile Devices traversing the network can be located relative to them, albeit only to the circumference of the radius of the calculated distance. Further localization algorithms and position estimation filtering techniques must be applied to provide a more precise level of localization.

In a cooperative paradigm, mobile devices can simulate the role carried out by Access Points providing a relative reference to a lost devices location. RSSI values can be extracted from beacons transmitted between devices within range. Correlation of these signal indicators and distance can be estimated using many of the methods already applied throughout literature in this arena [8-11]. RSSI based or more broadly speaking, Wi-Fi based Indoor Positioning Systems have had notoriously irregular environment variables such as reflection, refraction, diffraction and absorption of radio waves that can impact positioning estimated dramatically [12]. Although RSSI is a measure of signal loss, it is not a linear representation of how many dBm is actually reaching the card. If a signal indicator is reading -72, this means that it is 72 dBm less powerful by the time it gets to your device. Experimental test carried out at an early stage with CAPTURE further extoled this assumption. Results of these tests can be viewed in Table 1: 5 meter increments in Section V, Data Collection and Presentation. Crudely extracting the RSSI at given distance increments to attempt to derive a meter distance being equal to a given dBm increase in RSSI reading was not going to yield any value worth using in any further experiments. The authors in [13] advocate a solution utilizing a RSSI smoothing algorithm to minimize the dynamic fluctuation of the RSSI values. B. Trilateration Trilateration is a key component of GPS position estimation techniques. It is a process that can estimate the position of a mobile device given the positions of at least three other objects and the distance from those objects to the device to be located. In the scenario depicted below in Figure 2(a), illustrated using a cooperative localization example, the circle depicts the distance from a reference device to a lost device. This distance would have been derived using the RSSI value between the reference and lost devices. All we can say about the whereabouts of the lost device is that it resides somewhere on the circumference of the circle that is constructed using the radius of the estimated measurement between the two devices. A second reference device will allow the position of the lost device to be narrowed further as can be seen in Figure 2(b). Now the ranging estimates of the lost device have been calculated relative to the second reference device also. Therefore considering the lost device must be on the circumference of the circles created by the distance between it and the two reference devices there are only 2 possible positions where it might be, the intersections of these two circles.

Figure 2 (a) Single Distance

(b) With 2nd Reference Device

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014 To calculate the exact position of the lost device we need a third reference device. When we calculate the distance from this final reference device to the lost device and considering we already know the distance from the other reference devices. We can then determine that the lost device can only be at one specific position to match those three particular distance estimations ­ the intersections of the three circles (see Figure 3). The ranging estimates calculated from the RSSI values in the tests were used as the inputs for the trilateration algorithm on the CAPTURE, to provide an estimate on the position of the lost phones. Each phone used in the test is given a name (BSSID) TestPhone1, TestPhone2 for example. CAPTURE reads the RSSI of all available reference points, i.e. all devices it can `see', but it filters out only the test phones selected by the user carrying out the tests. This can be seen in the image in figure 5, and is achieved via a lookup table mapping the MAC address of the phone to the phone name. This allows the use of only a specified phone or a group of phones during any given test.

Figure 3: Trilateration Example II.

Figure 5: CAPTURE Client Interface A. System Components The experimental setup of the prototype consisted of the following system components:  Mobile Devices 5 Samsung GT-S5310 Galaxy Pocket phones, running Google Android 2.2.1 on a 600 MHz ARMv6, Adreno 200 GPU, Qualcomm MSM7227 chipset, were used to carry out the evaluation of the CAPTURE system. 4 of the phones were used as reference devices, the other phone acted as the lost device. All phones used during the test were of an exact make and model so as to rule out any issues with varied RSSI reads with different antenna types, some of these issues have been described in the literature [14, 15]. Lisheng et al., [15] go so far as to discribe the distortion being as much as 11.2 dBm out with different antenna types over a 25 meter read range. During the tests all phones were place at a distance of 80cm above floor level, to mimic as close to a real world example of a user holding them. The phones were placed on identical platforms during the tests to negate the impact of Hand-Grip body-loss effect which can also impact ranging measurements as documented in litrature by Rosa et al., [16]. Kaemarungsi and Krishnamurthy highlighted in their litrature [17] that device orientation can also introduce errors when calculating signal range estimates, so all phones had the same orientation when used in our tests.

Experimental Test Bed

In this section, we will provide evidence showing the suitability of CAPTURE as a solution to the indoor ranging problem. To do that we carried out a large campaign of measurements in the An Danlann Sports Hall in Letterkenny Institute of Technology illustrated in figure 1. The hall offers a 40m diagonal testing range, providing Line of Sight measurements for all tests, as can be seen in the picture depicted in figure 4. When readings were been recorded all users vacated the hall, this provided an optimal environment to use as a benchmark for future tests on CAPTURE.

Figure 4: Test Environment

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014  Database the algorithm, these were smoothed with a filter before the average was calculated.

A MySQL Server version: 5.0.96 hosted on a Linux platform was used to store all data collected by the devices. The server was online and the phones wrote directly to it as they recorded RSSI values from each other. The data was then passed through a low level filter to remove any outliers, before an average RSSI reading was calculated for each required ranging measurement, to be used in the trilateration algorithm to estimate the position of the lost device.  Laptop A Dell Latitude E6440 iCore3 running Windows 7 Professional was used to develop the app to gather the RSSI from the phones. An algorithm was designed to convert this RSSI reading into a ranging measurement before a trilateration algorithm converted the ranging measurements into Cartesian coordinate values. We used the Eclipse IDE and Android Software Development Kit (SDK) for Android development and debugging, to develop the app. B. Ranging Measurement Estimation The RSSI values captured from the beacons transmitted by devices within range of the `lost device' were used to estimate the relative distance between them. As explained earlier RSSI values do not provide a linear representation of distance. The authors in [13] advocate using the formula in "(1)," below to estimate RSSI, and thereby extrapolate distance given RSSI: Where: RSSI = - (10n Log10 (d) +A) Equation (1)

Figure 6: Meter RSSI values Further tests were then carried out to measure the accuracy of both the RSSI values received and the resulting range estimations from the algorithm. Table 1 below, depicts the results of tests to capture the RSSI values between two phones at 5 meter increments diagonally across the hall. It highlights the RSSI value beginning at -52.48 for the 0-5 meter range. A sample set of 200 readings was recorded per section, an average was then taken from this set. The standard deviation was also documented to illustrate any fluctuations in the received values, typically these were found to be low during our tests. Distance Average Std Dev Estimate Distance Average Std Dev Estimate 0-5m -57.264 0.4996 4.517 0 - 25 m -68.38 0.6884 21.544 0 - 10 m -61.5652 0.4 8.269 0 - 30 m -70.75 0.9797 30.059 0 - 15 m -69.5263 0.85346 25.31 0 - 35 m -71.854 0.6803 35.104 0 - 20 m -67.5662 0.48332 19.216 0 - 40 m -73.681 0.7901 45.379

n: Path Loss Exponent d: Distance from transmitting device A: Received signal strength at 1 meter distance The path loss exponent typically varies from 1.5 to 4, with 1.5 representing a free-space Line of Sight (LoS) value and 4 representing an environment that incorporates a high level of signal attenuation. Not having a good equation modeling the environment in which your experiments are to be deployed, will be reflected in horrible results. After initial pre-tests were evaluated, a Path Loss Exponent of 1.5 was determined for the test environment, because of the open plan design of the Hall offering LoS between all devices and the RSSI at 1 meter was measured at -43.6316. The results of the collected data are illustrated in the following section. III.

Table 1: 5 meter increments The average was then inputted into the algorithm to derive a range estimate based on the RSSI values received. As mentioned before RSSI values do not provide a linear representation of measurement, and therefore some of the increments do not initially seem like they could assist in finding a distance at a given measurement. The trilateration algorithm accounts for an error bounds of 2.5 meters in the range estimation of the RSSI value. One notable issue with the recorded RSSI values was the reading taken at the 0-1 meter distance however. It jumped dramatically at this distance, giving a RSSI value higher than the 0-20 and 0-25 meter tests. This test (0-10 meters) was carried out at different areas of the hall, to try and rule out signal interference. But irrespective of which location the reading were taken the RSSI value was

Data Collection and Presentation

Here we present all of the data collated throughout this work, the data sets are illustrated in the graphs and tables. During the recording of data the hall was emptied of people so as to provide a clean set of results. An initial test was run to establish the 1 meter range for input into the algorithm in equation 1, the results of this test can be seen in figure 6. Over 500 readings were recorded at various locations throughout the hall, to accurately obtain the meter value for

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014 always higher (or more negative) than the next 3 larger tests. No reason could be given at the time of writing for this anomaly within the set. IV. V.

CAPTURE ­ System Implementation

Experimental Results

Figure 7 depicts one of the tests were CAPTURE accurately locates a lost phone within 2.5 meters. TestPhone1, TestPhone2 and TesPhone3 know their location, via the inhouse IPS. They also know the distance between themselves (TestPhone1 - TestPhone2 = 15 meters, TestPhone1 ­ TestPhone3 = 13 meters and TestPhone2 ­ TestPhone3 = 17 meters), the RSSI readings from the Lost Phone to TestPhone1 is -61.5551dBm, from the Lost Phone to TestPhone2 is 65.34534 dBm and from the Lost Phone to TestPhone3 is 61.8952dBm. These RSSI readings translate to a ranging estimate of 13.345, 15.1221 and 9.349 meters respectively when put through the ranging algorithm. The actual distance between TestPhone1 and the lost phone is 11.5 meters, between TestPhone2 and the lost phone is 13.2 meters and TestPhone3 and the Lost Phone is 11.96 giving an approximate error rate of 2.5 meters.

In order for CAPTURE to be able to cooperatively locate a lost device within a network, there must be at least 3 reference devices within sight of the lost device. Each of these must have `a prior' knowledge of their location within a preexisting localization solution. The hypothesis of CAPTURE was to extend the range of in-house IPS's, and tests shown have proven that it can achieve exactly this. Existing IPS's have dramatically more powerful infrastructure than what CAPTURE would utilize though. For example 230 volt AC powered Access Points in a standard IPS versus 12 volt DC powered mobile reference devices (smart phones, tablets and\or laptops) in a cooperative solution. It would be naive to think that accuracy levels of an in-house IPS would also `extend' to a cooperative model, although this does not take away from the solution to the range issue that CAPTURE provides. The implementation of a more comprehensive filter would nonetheless assist with accuracy the Kalman or Extended Kalman Filters are recommended in the following literature [18, 19]. VI.

Conclusion

This paper introduces CAPTURE a cooperative localization system that provides a solution to the problem of devices being out of range of a hosted Indoor Positioning System. Experiments with the CAPTURE system have demonstrated that utilizing a cooperative framework of mobile devices can extend the range of an in situ Indoor Positioning System by at least the range of the outermost devices located within the system. Some issues arose during testing for example the 0-10 meter readings, and this necessitates further work. A more comprehensive algorithm would provide more accuracy for the system. An expansion of CAPTURE to avail of Bluetooth 4.0 would allow for the extension of an IPS incorporating some of the advantages of this technology. Bluetooth has been used as a cooperative solution to the accuracy issue in IPS's and can be seen in the following literature [20, 21]. Further investigation into the incorporation and evaluation of Wi-Fi Direct as a solution is also warranted.

Figure 7: Finding Lost Phone

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014 REFERENCES [1] M. Aly and J. Y. Bouguet, "Street view goes indoors: Automatic pose estimation from uncalibrated unordered spherical panoramas," in Applications of Computer Vision (WACV), 2012 IEEE Workshop on, 2012, pp. 1-8. G. Cullen, K. Curran, and J. Santos, "Cooperatively extending the range of Indoor Localisation," in Signals and Systems Conference (ISSC 2013), 24th IET Irish, 2013, pp. 1-8. G. M. Djuknic and R. E. Richton, "Geolocation and assisted GPS," Computer, vol. 34, pp. 123-125, 2001. Y. Fan and A. Dong, "A Solution of Ubiquitous Location Based on GPS and Wi-Fi ULGW," in Hybrid Intelligent Systems, 2009. HIS '09. Ninth International Conference on, 2009, pp. 260-263. A. Rowe, Z. Starr, and R. Rajkumar, "Using microclimate sensing to enhance RF localization in assisted living environments," in Systems, Man and Cybernetics, 2007. ISIC. IEEE International Conference on, 2007, pp. 3668-3675. C. J. Matz, D. M. Stieb, K. Davis, M. Egyed, A. Rose, B. Chou, et al., "Effects of Age, Season, Gender and Urban-Rural Status on Time-Activity: Canadian Human Activity Pattern Survey 2 (CHAPS 2)," International journal of environmental research and public health, vol. 11, pp. 2108-2124, 2014. A. E. Kosba, A. Saeed, and M. Youssef, "Robust WLAN Device-free Passive motion detection," in Wireless Communications and Networking Conference (WCNC), 2012 IEEE, 2012, pp. 32843289. X. Yaqian, L. Sian Lun, R. Kusber, and K. David, "An experimental investigation of indoor localization by unsupervised Wi-Fi signal clustering," in Future Network & Mobile Summit (FutureNetw), 2012, 2012, pp. 1-10. S. Shioda and K. Shimamura, "Anchor-free localization: Estimation of relative locations of sensors," in Personal Indoor and Mobile Radio Communications (PIMRC), 2013 IEEE 24th International Symposium on, 2013, pp. 2087-2092. M. O. Gani, C. Obrien, S. I. Ahamed, and R. O. Smith, "RSSI Based Indoor Localization for Smartphone Using Fixed and Mobile Wireless Node," in Computer Software and Applications Conference (COMPSAC), 2013 IEEE 37th Annual, 2013, pp. 110-117. D. Gualda, J. Urena, J. C. Garcia, E. Garcia, and D. Ruiz, "RSSI distance estimation based on Genetic Programming," in Indoor Positioning and Indoor Navigation (IPIN), 2013 International Conference on, 2013, pp. 1-8. [12] L. Erin-Ee-Lin and C. Wan-Young, "Enhanced RSSI-Based Real-Time User Location Tracking System for Indoor and Outdoor Environments," in Convergence Information Technology, 2007. International Conference on, 2007, pp. 1213-1218. J. Joonyoung, K. Dongoh, and B. Changseok, "Automatic WBAN area recognition using P2P signal strength in office environment," in Advanced Communication Technology (ICACT), 2014 16th International Conference on, 2014, pp. 282-285. X. Lisheng, Y. Feifei, J. Yuqi, Z. Lei, F. Cong, and B. Nan, "Variation of Received Signal Strength in Wireless Sensor Network," in Advanced Computer Control (ICACC), 2011 3rd International Conference on, 2011, pp. 151-154. F. D. Rosa, X. Li, J. Nurmi, M. Pelosi, C. Laoudias, and A. Terrezza, "Hand-grip and bodyloss impact on RSS measurements for localization of mass market devices," in Localization and GNSS (ICL-GNSS), 2011 International Conference on, 2011, pp. 58-63. K. Kaemarungsi and P. Krishnamurthy, "Properties of indoor received signal strength for WLAN location fingerprinting," in Mobile and Ubiquitous Systems: Networking and Services, 2004. MOBIQUITOUS 2004. The First Annual International Conference on, 2004, pp. 14-23. K. Alexis, C. Papachristos, G. Nikolakopoulos, and A. Tzes, "Model predictive quadrotor indoor position control," in Control & Automation (MED), 2011 19th Mediterranean Conference on, 2011, pp. 1247-1252. S. S. Saad and Z. S. Nakad, "A Standalone RFID Indoor Positioning System Using Passive Tags," Industrial Electronics, IEEE Transactions on, vol. 58, pp. 1961-1970, 2011. A. Baniukevic, D. Sabonis, C. S. Jensen, and L. Hua, "Improving Wi-Fi Based Indoor Positioning Using Bluetooth Add-Ons," in Mobile Data Management (MDM), 2011 12th IEEE International Conference on, 2011, pp. 246-255. Z. Zhichao and C. Guohong, "APPLAUS: A Privacy-Preserving Location Proof Updating System for location-based services," in INFOCOM, 2011 Proceedings IEEE, 2011, pp. 1889-1897.

[13]

[2]

[14]

[3] [4]

[15]

[5]

[16]

[6]

[17]

[7]

[18]

[8]

[19]

[9]

[20]

[10]

[11]

Operational Experiences with Disk Imaging in a Multi-Tenant Datacenter
Kevin Atkinson, Gary Wong, and Robert Ricci, University of Utah
https://www.usenix.org/conference/nsdi14/technical-sessions/presentation/atkinson

This paper is included in the Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI '14).
April 2­4, 2014 · Seattle, WA, USA
ISBN 978-1-931971-09-6

Open access to the Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI '14) is sponsored by USENIX

Operational Experiences with Disk Imaging in a Multi-Tenant Datacenter
USENIX Symposium on Networked Systems Design and Implementation--Operational Systems Track

Kevin Atkinson 

Gary Wong

Robert Ricci

University of Utah School of Computing {kevina, gtw, ricci}@cs.utah.edu www.emulab.net Abstract
Disk images play a critical role in multi-tenant datacenters. In this paper, the first study of its kind, we analyze operational data from the disk imaging system that forms part of the infrastructure of the Emulab facility. This dataset spans four years and more than a quarter-million disk image loads requested by Emulab's users. From our analysis, we draw observations about the nature of the images themselves (for example: how similar are they to each other?) and about usage patterns (what is the statistical distribution of image popularity?). Many of these observations have implications for the design and operation of disk imaging systems, including how images are stored, how caching is employed, the effectiveness of pre-loading, and strategies for network distribution. Large multi-tenant facilities have hundreds to hundreds of thousands of servers and thousands to millions of users [5]. A busy facility may have many thousands of user images and provision tens of thousands of servers per day. Disk images are commonly written to drives attached to the host; EC2, for example, calls this "instance storage" [1], and it is available on nearly all VM types. Disk imaging is on the critical path for provisioning servers, which cannot be booted until the requested image has been loaded. Images can consume significant resources on the facility, including the space used to store them and the network bandwidth required to distribute them to the hosts on which they are to be used. Thus, understanding disk images and their use is important to the design and operation of multi-tenant datacenters. In this paper, we study four years' worth of data from the operation of the Emulab testbed [16], a multi-tenant facility with approximately six hundred hosts and over five thousand user accounts. The data we examine covers 279,972 requests for disk images (Section 2) and is, to our knowledge, the only dataset currently available to the public that contains detailed traces of disk imaging in a multi-tenant datacenter. It allows us to study properties of the disk images themselves as well as how they are used by the facility's users, and we draw a number of conclusions that are applicable to the design and operation of imaging systems. Our key findings include: · Section 3: There is substantial block-level similarity between many images, suggesting that deduplicating storage is appropriate. The lifespan of images varies greatly, from days to years, and many images go unused for months at a time, making multi-tier data storage attractive. · Section 4: The working set of images is quite small (mean: 12 per day, 30 per week), making caching of frequently used images potentially effective. However, the makeup of this working set changes frequently, and there are no dominant images. The daily working set size grows linearly with the number of users, but the total number of facility and user

1

Introduction

Computers in datacenters are frequently re-allocated from one purpose to another, need to have their software upgraded, or need to be returned to a known "clean" state. This type of re-provisioning is particularly important in multi-tenant datacenters [4], which are shared by a large number of applications running on behalf of different clients. Notably, this is the model adopted by "Infrastructure as a Service" (IaaS) clouds such as Amazon EC2 [2], Rackspace [11], and datacenters managed with software such as OpenStack [15]. These facilities provide physical or virtual servers (infrastructure) on which users run their own operating systems and applications [9, 15]. The primary means for initializing user resources is to load them with an initial disk image, which is a block-level snapshot of a filesystem containing an installed operating system and set of applications. Typically, a cloud will provide a set of images that any user may install on servers that they provision (facility images). Users may also create their own images (user images): this is commonly accomplished by loading a facility image, customizing it, and taking a snapshot of the resulting disk.
 Work

done at the University of Utah; now at Rice University

USENIX Association 

11th USENIX Symposium on Networked Systems Design and Implementation 217

images follow different curves. · Section 5: The popularity of user images follows a heavy-tailed distribution, while the popularity of facility images does not. Most users skew heavily towards using either facility provided images or custom images, not both. While most users do not create their own images, those who do number among the facility's heaviest users. · Section 6: We consider the technique of pre-loading popular facility images, allowing some requests to be satisfied without waiting for the image to load. We find that two factors control the potential benefit from this strategy: (a) the ratio of the working set size to the number of idle disks available for preloading, and (b) the ratio of the rate at which the facility can load disks to the arrival rate of requests. · Section 7: Differential loading (pre-loading a base image, then transferring only differing blocks as required) shows potential. In order to be effective, it will require development of sophisticated prediction techniques that take into account both the popularity of images themselves and their block-level similarity to each other. We conclude in Section 8 with several concrete suggestions regarding the design and operation of disk imaging systems, and point to fertile areas for future work.

sion solely virtual machines, we believe that this difference does not have a significant impact on conclusions drawn from the dataset: in either case, the user is presented with the abstraction of an PC on which they may load and boot an operating system. While the details of operating systems that run within physical and virtual machines may vary, the quantity and diversity of users' desired images is unlikely to be affected. Emulab uses block-level disk images and distributes them using the Frisbee [6] disk imaging system. The format uses filesystem-aware compression, meaning that it does not store disk blocks that are not used by the filesytem, and compresses the allocated blocks with zlib [7] for efficient storage. Frisbee uses IP multicast to distribute images, and is highly optimized so that the bottleneck in image distribution and installation is the write speed of the target disk. The amount of time required to load a disk image depends on the number of used blocks in the filesystem that it contains, but is typically on the order of a few minutes. Facility images are visible to and may be requested by all users. User images are visible only to their creators unless the creator decides to make the image public, which few do.

2.1

Dataset Details

2

Dataset

Emulab is a network testbed widely used by the distributed systems and networking communities. An experimenter describes a network in terms of links and hosts. Included in this specification is the disk image to be loaded on each host. Emulab then provisions servers, physical or virtual, loading the requested disk image. This provisioning is done on demand as requests come in, and there is only limited support for ahead-of-time scheduling or batch jobs. The facility provides a number of standard images, including "default" images that are used if the user does not explicitly request an image. Many users create their own images by booting from a facility image, customizing it (for example, by installing software packages or modifying the operating system), and taking a snapshot. This user image can be referenced in future requests, saving the user the effort of re-installing the packages they use, or to scale out to much larger experiments. This basic model of image usage and creation is similar to that used in most IaaS clouds [14]. Emulab is capable of provisioning both physical and virtual machines; physical machines are the most commonly allocated resource. While many IaaS clouds provi-

The dataset that we study covers four years of disk image requests on Emulab, from March 2009 to March 2013. The dataset covers a total of 279,972 requests for 714 unique images. The requests were made by 368 users, at an average rate of 192 disk images loaded per day. The records cover the identity of the image, the user making the request, and the timestamp at which the request was made. Furthermore, the data indicates whether each image was a facility image or a user image, and whether it was requested explicitly by the user or was chosen as a default because the user did not specify an image. To preserve user anonymity, users and user images are assigned random integers as identifiers in this paper. We present the names of facility images using their Emulab-assigned names; user images are presented as user/image pairs. One of the things we studied was the block-level differences between images. Our primary interest in examining the contents of images is to determine the potential savings from loading a "base" image (usually a facility image), then transferring and writing only the disk blocks required to transform it into a particular "derived" image (usually a user image). We define the difference of two images A and B as: (A, B ) = |i  b : B [i] = A[i]| (1)

where b is the set the indices of allocated storage blocks in image B , and A[i] and B [i] are the contents of images A and B , respectively, at index i. This measure directly

218 11th USENIX Symposium on Networked Systems Design and Implementation 

USENIX Association

captures the numbers of blocks in image B that would need to be written to a disk that already contains image A. We define  (A, B ) as the fraction of blocks that would need to be written: that is,  (A, B ) = (A, B ) |b| (2)

that purchase time on a cloud such as EC2, or individual business units that share a company-wide datacenter. In the remainder of this paper, we consider all individuals who are part of the same project to be a singe "user" of the facility--when we refer to "users," we are referring to Emulab projects. The number of individuals who requested disk image loads over this time period was 1,301.

The Emulab dataset does not record the provenance of images (that is, which user images where based on which facility images). We assume that each user image U was based on the facility image F for which (F, U ) is minimized. For a particular image U , we refer to this base image as UB . Emulab allow users to delete their image files: only 37.4% (267) of the images found in the request traces were available for analysis of block-level similarities. Though large in number, the missing images were relatively unpopular, accounting for only 15.8% of all requests. Emulab also allows its users to modify images, so the image files that we analyzed represented a snapshot of image contents at a particular point in time.

2.4

Limitations of This Study

2.2

Removing Sources of Bias in the Dataset

We filtered the dataset to remove certain biases. First, we omit all uses of the facility by its operational staff: the maintenance, testing, etc. that they perform is likely to follow different patterns than users of the facility. Second, as a network testbed, Emulab supports a feature known as "delay nodes," [13, 12] which perform a trafficshaping role that does not represent a function present in most multi-tenant datacenters. Third, Emulab includes some resources that are not the standard PC servers used in clouds and datacenters: these include wireless nodes, programmable network hardware, and sensors. This filtering removed 183,824 of the original 463,796 requests (39.6%), 215 images (23.1%), and 30 users (7.5%), leaving us with the 279,972 requests, 714 images, and 368 users that we studied. It is worth making special note of Emulab's "default" images. If an Emulab experimenter does not specify a particular disk image in their experiment description, they get a default that is, for historical reasons, quite old. Due to their ages, the default images are not very popular. Most users select the facility image that best meets their needs; as a result, the presence of a default does not have a dominating effect on the way that users select images.

2.3

Users and Projects

For the purposes of this study, we consider users at the level of organizations. Emulab groups individual users into "projects." These loosely-defined groups represent research groups, classes, or cross-institution collaborations. Because of this, they are analogous to businesses

The Emulab dataset is, to our knowledge, the only one of its type currently publicly available. Therefore, we cannot quantitatively assess the degree to which it matches other multi-tenant facilities. We believe our analysis remains valuable nonetheless, for two reasons. First, it is the only analysis to date to apply such a large quantity of real-world data to the problem of improving disk imaging systems. Second, we conjecture that the most fundamental findings in our work remain applicable in other environments, even if specifics (such as the  parameter to the facility image popularity distribution) differ. Our dataset covers a large number of disk image loads, but comes from a mid-sized facility. We attempt to analyze the effects of facility size in Section 4.3, but application of our conclusions to larger facilities necessarily involves extrapolation. In addition, two features unique to Emulab affected our ability to run certain analyses. First, the nature of resource allocation in Emulab makes it difficult to study the inter-arrival times of image requests. Emulab's primary unit of resource allocation is the experiment: a collections of hosts that together make up a network experiment. In contrast, most IaaS clouds consider only individual servers or "instances," and the cloud has no semantic information about which instances are contributing to the same application. Thus, image requests in Emulab arrive in well-defined bursts that do not have a direct analog in many other datacenters. Deploying an application in a datacenter or cloud does often involve provisioning of multiple machines in a short timeframe; however, we have no data that would allow us to analyze whether experiment sizes in Emulab are representative of burst sizes in other environments. For this reason, we avoid analyzing this aspect of the dataset, and all of our analyses are with respect to individual loads of disk images rather than Emulab experiments. Second, we chose not to analyze the relative popularity of the operating systems contained in the images (eg. Linux vs. BSD, or the relative popularities of Linux distributions). Emulab's user base is overwhelmingly comprised of academic researchers and students, and their OS preferences may not be representative of a broader population. In particular, while Emulab supports Windows, it constitutes a small fraction of all Emulab use--almost certainly a smaller fraction than would be seen in other

USENIX Association 

11th USENIX Symposium on Networked Systems Design and Implementation 219

10000

Facility User

Requests

10

100

1000

u

u
0 100 200 300 Rank 400 500 600

Figure 1: Requests for facility and user images, sorted on the x axis by popularity. Note the log-scale y axis.

u u u

settings. We restrict our analysis to the popularity of disk images rather than the operating systems they contain, and it is possible that this distribution is affected by the operating system preferences of Emulab's user base.

u u

3

Storage of Disk Images

We begin our study by examining the basic properties of the images in our dataset, with an eye towards understanding how they should be stored. We pay special attention to the relationship between images that are provided by the facility and images that are created by users; as we will see through further analysis, these images have different characteristics that warrant different treatment.

u

u u u

Image name All facility images All user images RHL90-STD [D] FEDORA10-STD UBUNTU10-STD RHL90-STD FC4-UPDATE 715/10 FBSD410-STD FEDORA8-STD 237/69 296/35 787/24 UBUNTU70-STD UBUNTU12-64-STD 787/14 226/44 FEDORA10-UPDATE CENTOS55-64-STD FC6-STD 762/69 FC4-WIRELESS FC4-STD FEDORA10-STD [D] UBUNTU11-64-STD 624/89 238/50 226/51

Requests 155,617 124,355 21,993 18,042 14,402 13,182 12,097 11,156 8,916 8,153 7,512 7,179 6,243 6,021 5,834 5,231 5,198 4,861 4,710 4,455 4,213 3,700 3,615 3,604 3,383 3,277 3,113 2,899

% 55.6% 44.4% 7.9% 6.4% 5.1% 4.7% 4.3% 4.0% 3.2% 2.9% 2.7% 2.6% 2.2% 2.2% 2.1% 1.9% 1.9% 1.7% 1.7% 1.6% 1.5% 1.3% 1.3% 1.3% 1.2% 1.2% 1.1% 1.0%

3.1

1

Prevalence of User Images

During the 48 months covered by our dataset, there were a total of 368 users. Of these, nearly two thirds (231) used only facility images, and slightly over one third (137, or 37.2%) used at least one user image. This implies that optimizing the provisioning of facility images can improve the experience of a majority of users. For example, if a suitable set of facility images can be identified for preloading on to servers, this could take image loading out of the critical path for creation of those users' instances. We explore this issue further in Section 6. The number of users who request user images, however, is non-negligible, suggesting that an imaging system should also take their needs into account. In fact, we find that there are more user images in our dataset (619) than facility images (94), meaning that, on average, each user who creates at least one disk image creates 4.5 of them.

Table 1: Total requests for all user and facility images. Also shown are the number of requests for all images that account for more than 1% of all requests. User images are marked with a `u' in the left column, and images requested implicitly as defaults are marked with a `[D]'; explicit requests for default images are counted separately.

3.2

Popularity of User Imags

The top of Table 1 shows the relative popularity of facility and user images. We see that the percentage of requests for user images is over 44%; since only 37.2% of users

create their own images, this imples that this set of users are heavier users of the testbed by at least 18%. Table 1 also shows all images that made up at least 1% of the requests. Of these twenty four images, ten are user images. Note that RHL90-STD and FEDORA10-STD each appear twice, because they are both common explicitly requested images and also images loaded by default. The complete image popularity data is plotted in Figure 1. We can see that the number of user images is much larger than the number of facility images, but that the population of user images contains many images that are used few times. Together, the top 17 facility images are more popular than the top 17 user images (the 17th facility image had 1,772 requests, and the 17th user 1,330). From the 18th image onwards, the user images are more popular--the 18th user image had 1,260 requests and the 18th facility image had

220 11th USENIX Symposium on Networked Systems Design and Implementation 

USENIX Association

1,233. Both facility and user images have tails consisting of images that were requested fewer than ten times, but this tail is much more prevalent in the case of user images, where the tail represents nearly half of all user images. From this data, we can conclude that facility images dominate, but that there are a small number of user images that are as popular as some facility images.

Frequency

3.3

Image Lifespan

0 0

1

5 10

50

200

500

The Emulab dataset does not include explicit creation and deletion dates for images. Thus, we define the lifespan of an image to be the number of days between when the image was first seen in the request stream and when it was last seen. Note that this will tend to underestimate lifespan: some images were likely first used before our dataset begins, and some will continue to be used after the end of the dataset. A histogram of user image lifespans can be seen in Figure 2. While the majority of images have very short lifespans, there is a long tail: several were used throughout the entire four years covered by the dataset. The observed mean lifespan is 100.4 days. We found the number of images with short lifespans to be quite surprising, so we examined them in greater detail, and it became clear that a large majority of these shortlifespan images were requested only on a single day: 196 of the 619 user images (31%) fall into this usage pattern. This suggests that a number of users create images for the purposes of running a single experiment, a conclusion borne out by looking at the experiment metadata. Finally, we looked at how long user images "go idle". We found that it is common for user images to have gaps of months in between requests for them. During this time, there is no need to have the images constantly available; they could be moved to cheaper, but slower, storage. The distribution of the maximum idle periods for the 214 user images with a lifespan of at least 30 days is shown in Figure 3. In total, 162 of the images (76% of long-lived images, and 26% of images overall) had gaps in usage of one month or more. Two images even had gaps of over two years between successive uses.

100

300

500

700

900

1100

1300

1500

Image lifetime (days)

Figure 2: Histogram of the lifespans of user images. Note that the y axis is log-scale.
50 Frequency 0 0 10 20 30 40

90

180

270

360

450

540

630

720

810

Maximum interval between requests (days)

Figure 3: Histogram of usage gaps for user images.
40 Frequency 0 0 10 20 30

20

40 % similarity

60

80

100

3.4

Block-Level Differences Between Images

We next examined how much user images differ from the facility images they are based on. We use the definitions of (A, B ),  (A, B ), and "base" images given in Section 2.1. Figure 4 shows a histogram of similarities between user images and their associated base facility images. From this figure, it is clear that many user images do show significant similarities to their bases--most are more than 50% similar, with a significant peak in the 60%­80% range. This is in line with findings from

Figure 4: Histogram of similarity (1 -  (UB , U )) between user images and their associated base images. Higher percentages indicate more similarity.

smaller studies in the past [8]. There is also a significant tail of more than twenty images images with very low similarity (below 10%) to their base images. Overall, these numbers point to two potential strategies for improving disk imaging systems. First, they suggest that significant storage savings can be had by storing images in a deduplicating storage system [10], which would

USENIX Association 

11th USENIX Symposium on Networked Systems Design and Implementation 221





Frequency





                                                



0

50

100

150



Median = 12.00 Mean = 11.98 Std. dev. = 4.33

   

   

   

    

0

5

10

15

20

25

30

Images used during day

Figure 5: Variation of facility image popularity over time. The fifteen most popular facility images are shown.

Figure 6: Histogram showing the distribution of the working set size over one-day periods (midnight to midnight).
Median = 30.00 Mean = 30.17 Std. dev. = 6.43 15 Frequency 0 15 5 10

store only one copy of the blocks that the base and derived images have in common. Second, they suggest that the technique of differential disk loading, which transforms a base image into a derived image by writing only the blocks that differ, has a potential for reducing the time and bandwith for distributing the user images. We explore the latter in detail in Section 7.

4

Working Set Size and Caching Potential

Having looked at the images themselves, we turn our attention to trends of usage over time, paying particular attention to the working set; understanding the size and composition of the working set is critical to designing strategies for caching and pre-loading.

20

25

30

35

40

45

Images used during week

Figure 7: Histogram showing the distribution of the working set size over one week periods (Sunday to Saturday).

4.1

No Dominant Images

If a small set of facility images dominates the request stream, it would be possible to design the disk imaging system around that fact. In particular, it would make sense to pre-load most or all idle disks with popular images, allowing user requests to be satisfied without waiting for a disk to load. This is the policy adopted by Emulab: the images labeled `[D]' in Table 1 are loaded as part of the process of freeing machines for the next user. As we can see in Figure 5, there is no such dominant image. The popularity of all facility images fluctuates wildly from month to month, with new images becoming popular quickly, old images falling out of favor, and some images swinging between popular and unpopular. Even the default images, which remain active throughout the entire time period, sees large changes in popularity. Note that we do not distinguish between explicit and implicit requests for default images as we did in Table 1; for the purposes of disk loading, these two cases are equivalent. As a result, we conclude that the strategy of pre-loading a single default image is unhelpful. It is, in fact, counterproductive: servers must be taken out of circulation

while they are loaded with the default image, and most are re-loaded a second time when requested by a user. If pre-loading strategies are to be useful, they will require more sophisticated methods for predicting future requests.

4.2

Size and Variation of the Working Set

Figure 6 depicts the working set size (number of unique images requested) over one-day periods. The mean working set size is quite small, at a mean of 11.98 images per day--this represents only 1.7% of the total number of images. While there is some variation in the working set size, it is not large: it follows a normal distribution with a standard deviation of 4.33. This result is encouraging from the perspective of caching: it suggests that only a small fraction of images need to be available for quick loading at any point in time, and that others could be stored in cheaper, slower storage systems. Figure 7 shows the distribution over week-long periods. The average working set size is approximately two and a half times larger than the daily average, and again follows a normal distribution with a reasonably small standard deviation.

222 11th USENIX Symposium on Networked Systems Design and Implementation 

USENIX Association

100 200 300 400 500 600 700

Mean daily working set size

Number of images used

1

2

3

4

5

6

User Facility

7

User Facility 0 20 40 60 80 100

0

0

20

40

60

80

100

0

Percentage of user base

Percentage of user base

Figure 8: Total number of images used over four years when considering random subsamples of the Emulab userbase.

Figure 9: Mean daily working set size when considering random subsamples of the Emulab userbase.

4.3

Scaling of the Working Set

To get a feel for how the size of the working set might vary on facilities larger or smaller than Emulab, we subsampled our data to simulate differently sized userbases. Figure 8 shows the total number of images used over the 4-year period when considering only 10% of the userbase, 20%, etc. The set of facility images quickly reaches saturation (all images are used at least once) and stops growing with additional users. The set of user images, on the other hand, grows linearly with respect to the number of users. This is explained by simple intuition: the set of useful facility images is more a function of the facility than of the userbase, while more users mean more usercreated images. Thus, we can expect that a facility with many more users than Emulab will have a greater number of user images in proportion roughly to its greater userbase, but that its set of facility images will not be larger by the same proportion. Indeed, Amazon EC2, which has a userbase that is at least three orders of magnitude larger than Emulab, advertises less than thirty images provided directly by AWS [3] and less than a hundred public images provided by their business partners. In comparison, Emulab has 94 public facility-provided images. However, this does not quite tell the whole story. Figure 9 shows the same subsampings, but this time looks at the mean daily working set size. Here, we see that the number of images loaded in a typical day increases linearly with the userbase for both facility and user images. Thus, we can expect that facilities much larger than Emulab do exhibit larger working sets. The working set of facility images is capped by the total number of such images, so very large facilities are likely to include most or all of their facility images in the daily working set. The general trend we can expect, is that for small facilities, the daily image working set size is in direct proportion to the size of the userbase. For large facilities, the working set will contain a relatively small set of facility

images, and a very large set of user images; however, we find that the fraction of requests that are for user images stays fairly constant regardless of the size of the userbase, meaning that these requests must necessarily be diverse.

5

Users' Behavior

We now turn our attention to the behavior of individual users; a facility that understands how its users interact with images is in a better position to provide the interfaces and image management tools that they require.

5.1

Distribution of Image Popularity

In distributions with "light" tails, such as the normal distribution, a relatively small subset of the population accounts for most of the popularity. For "heavy tailed" distributions (defined as those whose tail is not bounded by the exponential [17]), this effect is less pronounced, and it takes more of the population to cover the same level of popularity. We compared the popularity distributions of facility and user images separately to exponential distributions chosen to match the sample means. We found that facility images are a reasonably good match for the corresponding exponential distribution (with Kolmogorov Smirnov statistic nD n = 1.13), but user images are not  ( nDn = 5.54). As can be seen in Figure 10, the tail for user images lies substantially above the exponential. This is a key finding: user-created images have a significant heavy tail, while facility-provided images do not. The primary consequence of this discrepancy is that strategies that depend on being able satisfy a large number of requests with a relatively small number of images (such as pre-loading, examined in detail in Section 6), will be more effective with facility images than with user images.

USENIX Association 

11th USENIX Symposium on Networked Systems Design and Implementation 223

Facility Exp(=0.143) User Exp(=0.026)

      

Requests

1

10

100

1000

10000

0

100

200

300 Rank

400

500

600

  

Figure 10: Distribution of image popularity compared to the exponential (shown as dashed lines); note the log-scale y axis.

Figure 12: Profile of users making at least 500 disk image requests. Requests for facility images are shown as bars above the axis, and user images are below the axis.

5.3

Behavior of Heavy Users

60

0

10

20

30

40

50

60

Images used

Figure 11: Histogram showing the number of users who use different quantities of images.

5.2

Users and Images

As we can see in Figure 11, most users use a relatively small set of images. There are, however, two surprising features of this data. Only 20% of users used a single image--a large majority used two or more. We believe that this is due to three factors. First, since our sample period covers four years, many users likely migrated to newer versions of images as operating systems were updated. Second, any user who creates a custom image will use at least two images: they will request the base facility image at least once, then move to the custom image they create. Third, users may have started off using the default images provided by Emulab, found them unsuitable for their needs, and switched to non-default images. Another surprising feature is that there are a small number of users who use a very large number of images. Twenty users use at least 20 images, and one outlier uses more than 60.

Because user images are created by customizing facility images, we can expect that all users will employ facility images at least once, and likely a few times. The question remains, however, whether users tend to use primarily facility images, primarily their own images, or some balanced mixture of the two. We are particularly interested in the answer to this question for heavy users of the facility. Figure 12 shows a profile of the heaviest users (those who made at least 500 image requests) from the Emulab dataset. Two important facts are evident. First, while a few users do mix facility and user images (i.e. have bars both above and below the axis in the figure), most tend to skew heavily towards one or the other. Second, among the twenty heaviest users, twelve employ primarily user images. Past this point, facility images dominate. This clearly establishes that custom user images are a "power user" feature: their dominant use is by a relatively small number of users, who use them heavily.

Frequency

0

20

40

6

Prediction and Pre-Loading

We now turn our attention to techniques that may allow the facility to service user requests more quickly. The first technique that we consider is pre-loading: if it is possible to predict which images will be requested in the near future, the facility can pre-load them onto idle disks. If the predictions are correct, users requests may be satisfied immediately; if not, the user will have to wait for their image to be loaded. Note that this strategy does not save bandwidth on the datacenter's image distribution network; it simply shifts the image distribution to before the user's request arrives. In fact, pre-loading may increase the bandwidth used for distributing images: in the case of mispredictions, a node pre-loaded with one disk image may need to be re-loaded with another.

224 11th USENIX Symposium on Networked Systems Design and Implementation 

USENIX Association

100

80

60

40

20

Figure 13: Percentage of requests for three classes of images.
1.0

Probability of satisfying request

0.0

0.2

0.4

0.6

0.8

Figure 14: Fraction of requests satisfied from pre-loaded images for varying ratios of free pool size to the working set size.

6.1

We begin with the observation from Section 3.2 that the popularity of user images has a much longer, heavier tail than the set of requests for facility images. Therefore, strategies targeting prediction of facility images are likely to bear more fruit. We also recall from Section 4.1 that there does not exist a consistently dominant image, though Section 4.2 showed us that the working set size over a day is fairly small. This small working set size is encouraging from a prediction standpoint. An illustration of the potential for prediction can be found in Figure 13, which shows three classes of image requests. Requests for default images can be satisfied by simply pre-loading default images without complicated prediction strategies. This strategy is clearly ineffective in Emulab, as few requests are for the defaults. On top of these are requests for non-default facility images, which represent attractive targets for pre-loading. Finally, we see that approximately 40% of requests are for user images, which are a poor target for prediction because of their long tail. Thus, we target the 60% of requests that are for the relatively predictable facility images. We first consider how the size of the free pool affects the potential for prediction, where the free pool is defined

2009-03 2009-04 2009-05 2009-06 2009-07 2009-08 2009-09 2009-10 2009-11 2009-12 2010-01 2010-02 2010-03 2010-04 2010-05 2010-06 2010-07 2010-08 2010-09 2010-10 2010-11 2010-12 2011-01 2011-02 2011-03 2011-04 2011-05 2011-06 2011-07 2011-08 2011-09 2011-10 2011-11 2011-12 2012-01 2012-02 2012-03 2012-04 2012-05 2012-06 2012-07 2012-08 2012-09 2012-10 2012-11 2012-12 2013-01 2013-02 TOTAL

0

Default

Other Facility

User Images

0.0

0.2

0.4

0.6

0.8

1.0

Ratio of free pool size to number of images

Free Pool vs. Working Set Size

as the set of idle nodes or disks that are not in use and are thus available for pre-loading. We consider a simple model in which we assume that the inter-arrival time of requests is greater than the time required to load an image. (We will relax this assumption below.) In this model, the determinant of prediction accuracy is the ratio between the size of the free pool and the working set size. In this scenario, the best prediction mechanism is to pre-load those N disks with the N most popular images. Figure 14 shows the percentage of requests for facility images satisfied under this model, using the empirical request and working set data from Emulab. Intuitively, if there are no disks available for pre-loading, it is not possible to satisfy any requests from pre-loaded machines, and if one can pre-load the entire working set of images (the ratio is 1.0 or greater), it is possible to satisfy all requests. Because the distribution of facility image popularity is roughly exponential, the ability to load the top 25% of images satisfies 95% of all facility requests. It is interesting to consider how this result applies to different sizes of facilities. In many cases, the size of the free pool will be a fraction of the physical resources, meaning that it is much larger, in absolute terms, for larger facilities. At the same time, we have seen that the working set size of facility images grows linearly with the userbase, but is capped at a relatively small size by the total number of facility images. The practical effect is that small facilities (tens of nodes) are likely to fall on the left side of the curve in Figure 14, meaning that pre-loading is not likely to be particularly effective. Large facilities (thousands of nodes), on the other hand, are likely to be on the far right, with free pool sizes that far exceed the number of facility images--for them, pre-loading is likely to be able to satisfy all requests for facility images. In between these extremes, a facility needs to carefully consider the free pool to working set ratio to determine whether pre-loading makes sense.

6.2

Reload Rate vs. Arrival Rate

Our previous experiment made the simplifying assumption that request inter-arrival time was smaller than the time required to re-load an image; this enables the facility to ensure that the N most popular facility images are loaded at all times, and that only one copy of each image needs to be kept pre-loaded. We now consider the relationship between the arrival rate of new requests and the rate at which the facility can pre-load images in response. If bursts of requests arrive at a faster rate the the facility can re-image, it is useful to have more than one pre-loaded copy of each image. It is also possible for bursts of requests to outpace the facility's ability to keep the image loaded, meaning that there can be mispredictions even for very popular images.

USENIX Association 

11th USENIX Symposium on Networked Systems Design and Implementation 225

7

Differential Disk Loading

1.0

0.0

0.5

1.0

1.5

Reload rate (normalised to mean arrival rate)

Figure 15: Fraction of requests satisfied against the rate at which images can be pre-loaded.

We model this scenario using standard tools from queuing theory: each image is modeled as a queue, with a number of queue slots equal to the number of disks onto which the image is pre-loaded. The distribution of pre-loaded images is taken directly from the observed distribution of requests; using our results from Section 5.1, we model this distribution as being exponential with  = 0.143. We make the standard queuing theory assumption that requests arrive according to a Poisson process [18]. We picked a facility size of 1,000 disks, with an average utilization rate of 90%, meaning that on average, 100 disks are available for pre-loading. Figure 15 shows the results of a Monte Carlo simulation using this model. We varied the ratio of reload rate to the mean request arrival rate, and find that this ratio is critical. If the facility can reload images at a faster rate than requests arrive (the area to the right of the 1.0 ratio), it can easily keep the proper set of facility images preloaded and can satisfy most requests for these images; this matches the case modeled in Figure 14. If the reload rate is lower (to the left of the 1.0 ratio), the value of preloading falls quickly, as bursts of requests overwhelm the facility's ability to keep a pre-loaded pool that contains the appropriate set of images. We conclude that pre-loading facility images can be an effective strategy for reducing user wait time, but that the critical determining factors for its success are: (1) the ratio betwen the size of the free pool and the working set size; and (2) the ratio between the facilities' reload rate and the mean arrival rate.

The second optimization we consider targets requests for user images: it may be possible to pre-load facility images, and when requests for user images arrive, load only the blocks that differ. This differential loading strategy is attractive for two reasons. As we saw in Section 5.1, the distribution of user image popularity has a heavy tail, making it difficult to pre-load enough of them to satisfy many requests. But, as we saw in Section 3.4, user images have high levels of similarity to the smaller set of facility images. Thus, we have the potential to reduce user wait times by picking a pre-loaded facility image and doing a fast load of just the blocks that differ. In this section, we develop metrics that quantify the potential benefits of differential disk loading and give us a general understanding of the potential effectiveness of this technique. In order to realize these benefits, additional methods for predicting future requests would need to be developed, which take into account not only image popularity, but also block-level similarity between the pre-loaded images and the images that may be layered on top of them. We consider only the problem of finding the differences between two disk images, and not the more general problem of taking the difference between a disk image and arbitrary disk state (i.e. the state in which the disk is left by the previous user). Earlier work [6] has shown that disk distribution and installation can run at the full write speed of the target disk, meaning that schemes that require reading disk contents before writing are likely to slow the process down, and are likely to be fruitful only in cases where users do not write much to the disk.

Probability of satisfying request

0.0

0.2

0.4

0.6

0.8

7.1

Limits to Savings

As we have seen, the set of facility images is smaller and more predictable than the set of user images. Thus, as with the last section, we continue to pre-load only facility images; when a user image U is requested, if its base image UB has been pre-loaded, we need to transfer only (UB , U ) blocks instead of the full |u| blocks belonging to the image. Clearly, this strategy relies on having the correct set of base images pre-loaded. To simplify, we start by assuming that we have an oracle that tells us what facility images to pre-load or sufficient capacity to preload all facility images; we relax this assumption below. We begin by defining the number of disk blocks loaded for user images when differential loading is not in use (i.e. the entire user image must be loaded). For an individual image U , this quantity is: |u| · UC (3)

Recall that u is the set of block addresses with defined values in image U , and therefore |u| represents the size

226 11th USENIX Symposium on Networked Systems Design and Implementation 

USENIX Association

of the image. We define UC to be the number of times the image is loaded. Intuitively, then, this quantity is simply the number of blocks in the image multiplied by the number of times the image is used. To obtain the total number of blocks loaded across the universe of all user images, U, we sum the total blocks loaded for each image U  U:
U U

User image traffic required

|u| · UC

0.0 0

(4)

0.2

0.4

0.6

0.8

1.0

20

40

60

80

100

To adapt these equations for differential loading, we substitute (UB , U ) for |u|, giving us the number of blocks that must be loaded assuming the base image has been pre-loaded. This gives us the total number of blocks: (UB , U ) · UC (5)

Percentage of facility images pre-loaded

Figure 16: Network traffic required to load user images, when various facility images may be pre-loaded.

U U

Differential Savings Potential (DSP): The maximum relative savings from differential loading (assuming the correct UB images are always loaded) is derived by combining Equation 4 and Equation 5: DSP =
U U

|u| - (UB , U ) UC |u|

(6)

In the Emulab dataset, the values for Equation 4 and Equation 5 are 174 TB and 78 TB, giving a DSP of 0.55. This indicates that, in the presence of an oracle, the Emulab facility could save over half of the blocks it transfers for user images at request time, potentially halving the average time users must wait for custom images to load. Adjusted Differential Savings (ADS): We next relax the assumption of an oracle. To do so, we use the notation P [I ] to indicate the probability that image I is pre-loaded on the facility. We adjust Equation 6 to indicate that with some probability, the user request can be fulfilled with differential loading because the requisite base image is loaded. If not, the entire image must be loaded (resulting in no savings): ADS =
U U

pre-loaded. The y axis of this graph represents the fraction of blocks that must be loaded at request time, with lower numbers being better, and the limit being 1 - DSP (0.45). Along the x axis, we show the fraction of facility images loaded--we rank facilitiy images by an adjusted popularity that is the sum of their own popularity and the popularity of all users images that use that facility image as a base, and then pre-load the x most popular. What we can see is that relatively few facility images act as bases for user images, so it is necessary to pre-load only a small subset of them (approximately 20%) in order to get most of the benefit of differential loading. This implies that this technique can be effective even on facilities that have low free pool to working set ratios. Also of interest in Figure 16 is that, for our dataset, the most popular facility images (the default images) are not commonly used as bases for user images--this accounts for the small plateau on the left of the graph. We hypothesize that this is due to the age of Emulab's defaults.

8

Recommendations and Future Work

P [ UB ]

|u| - (UB , U ) UC |u|

(7)

In our exploration of the Emulab disk image request dataset, we have uncovered a number of properties that can be used to guide the operation and design of disk image storage and installation systems. Based on our analysis, we make the following recommendations: · Storing images in a deduplicating image store is likely to result in substantial savings. Reads from deduplicating stores can be slow, but the working set size is small enough that it is possible to cache images in faster storage. · Focusing pre-loading strategies on facility images is likely to produce the best results. The tail of user images is much longer and heavier than the one for facility images, and only a few user images approach the popularity of the heaviest-used facility

Note that if P [UB ] = 1 for all images (perfect prediction), this gives us Equation 6. For smaller P [UB ] values (worse predictions), the adjusted savings are lower than the savings potential, which fits with the intuitive notion that sub-optimal pre-loading will reduce the value of differential loading.

7.2

Savings With Predictions

Figure 16 shows the effectiveness of differential loading as a function of the fraction of facility images that are

USENIX Association 

11th USENIX Symposium on Networked Systems Design and Implementation 227

images. For very large facilities, it is likely that most facility images appear in the daily working set, making prediction straightforward. · Pre-loading of a single default image is not a useful strategy, as the diversity of user requests means that no one image, even the default, is dominant on any time scale. · For small facilities (those where the number of idle disks is significantly smaller than the working set size), pre-loading is likely not a valuable strategy. For large facilities, the number of idle disks is likely to be much larger than the working set size, making simple pre-loading strategies highly effective. To accurately model the effectiveness of pre-loading for mid-sized facilities, additional study of request inter-arrival distributions is necessary. · Large facilities would do well to focus on techniques that allow them to sustain high reload rates. The only way for pre-loading to be effective is to keep this rate significantly above the request arrival rate, which is likely to be high for large facilities. Techniques of interest include distribution using multicast and image distribution servers spread throughout the datacenter. · Differential loading has the potential to be effective, especially on facilities with limited free pools. It shows the potential to halve the number of disk blocks transferred to satisfy user requests, but that potential depends on correct predictions when preloading the appropriate base images. This changes the criteria for pre-loading, since base images should be selected not only on their own popularity, but also on the popularity of images that may be laid down on top and their block-level similarly with the base image. This complex optimization problem presents an interesting area for future study. An anonymized version of the dataset used for thus study, plus all code used to analyze it and produce the figures for this paper, can be found at: http://aptlab.net/p/tbres/nsdi14

References
[1] Amazon Web Services. Amazon EC2 instance store: User guide. http://docs.aws.amazon.com/AWSEC2/ latest/UserGuide/InstanceStorage.html. [2] Amazon Web Services. Amazon Elastic Compute Cloud website. http://aws.amazon.com/ec2/. [3] Amazon Web Services. Amazon Machine Images (AMIs). https://aws.amazon.com/amis. [4] M. Armbrust, A. Fox, R. Griffith, A. D. Joseph, R. H. Katz, A. Konwinski, G. Lee, D. A. Patterson, A. Rabkin, I. Stoica, and M. Zaharia. Above the clouds: A Berkeley view of cloud computing. Technical Report UCB/EECS-200928, EECS Department, University of California, Berkeley, Feb 2009. [5] L. A. Barroso and U. Holzle. The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines, volume 6 of Synthesis Lectures on Computer Architecture. Morgan and Claypool, 2009. [6] M. Hibler, L. Stoller, J. Lepreau, R. Ricci, and C. Barb. Fast, scalable disk imaging with Frisbee. In Proc. of the USENIX Annual Technical Conference (ATC), pages 283­ 296, San Antonio, TX, June 2003. [7] Jean-loup Gailly and Mark Adler. zlib website. http: //www.zlib.org. [8] K. Jin and E. L. Miller. The effectiveness of deduplication on virtual machine disk images. In Proc. of SYSTOR, the Israeli Experimental Systems Conference, May 2009. [9] D. Nurmi, R. Wolski, C. Grzegorczyk, G. Obertelli, S. Soman, L. Youseff, and D. Zagorodnov. The Eucalyptus open-source cloud-computing system. In Proc. of the Workshop on Cloud Computing and its Applications (CCA), 2008. [10] S. Quinlan and S. Dorward. Venti: A new approach to archival storage. In Proc. of the USENIX Conference on File and Storage Technologies (FAST), pages 89­101, Jan. 2002. [11] Rackspace US, Inc. Rackspace hosting website. http: //www.rackspace.com/. [12] L. Rizzo. Dummynet: a simple approach to the evaluation of network protocols. Computer Communication Review, 27(1):31­41, Jan. 1997. [13] P. Sanaga, J. Duerig, R. Ricci, and J. Lepreau. Modeling and emulation of Internet paths. In Proc. of the USENIX Symposium on Networked Systems Design and Implementation (NSDI), Boston, MA, Apr. 2009. [14] The OpenStack Team. OpenStack user documentation. http://docs.openstack.org/user-guide/. [15] The OpenStack Team. OpenStack website. http://www. openstack.org. [16] The University of Utah. Emulab website. http://www. emulab.net/. [17] Wikipedia: Heavy-tailed Distribution. http: //en.wikipedia.org/wiki/Heavy-tailed_ distribution. [18] Wikipedia: Poisson Process. http://en.wikipedia. org/wiki/Poisson_process.

Acknowledgments
We would like to thank the administrators of Emulab for their assistance in collecting the data used for this study. We would also like to thank Dave Andersen, our shepherd Bruce Maggs, and the anonymous reviewers for their valuable comments. This work was supported by NSF under award CNS-0709427.

228 11th USENIX Symposium on Networked Systems Design and Implementation 

USENIX Association

The portfolio of capability of an entire defence force is a complex system of systems. Understanding and managing this portfolio has many challenges. This paper presents an account of the successful practical application of systems engineering tools and approaches to support portfolio level decision making. The concept of âDefence Capability Situation Awarenessâ is presented along with the prototype tool âProgram Viewerâ that demonstrates its utility. DODAF 2 and UPDM provide the standards underlying the modelling and management of data, which is stored in a systems engineering tool. The data to populate the model is collected from existing data sources from across the organisation, and this provides a critical mass of relevant and useful information. Custom fit-for-purpose visualisation and manipulation tools are provided to support understanding of the portfolio by decision-makers from many perspectives and present the consequences of proposed courses of action.The portfolio of capability of an entire defence force is a complex system of systems. Understanding and managing this portfolio has many challenges. This paper presents an account of the successful practical application of systems engineering tools and approaches to support portfolio level decision making. The concept of âDefence Capability Situation Awarenessâ is presented along with the prototype tool âProgram Viewerâ that demonstrates its utility. DODAF 2 and UPDM provide the standards underlying the modelling and management of data, which is stored in a systems engineering tool. The data to populate the model is collected from existing data sources from across the organisation, and this provides a critical mass of relevant and useful information. Custom fit-for-purpose visualisation and manipulation tools are provided to support understanding of the portfolio by decision-makers from many perspectives and present the consequences of proposed courses of action.To Wireless Fidelity and Beyond - CAPTURE, Extending Indoor Positioning Systems

Item type Authors Publisher Downloaded Item License Link to item

Article Cullen, Gary IEEE 20-Dec-2016 14:36:19 http://creativecommons.org/licenses/by-nc-nd/4.0/ http://hdl.handle.net/10759/325943

To Wireless Fidelity and Beyond - CAPTURE, Extending Indoor Positioning Systems
Gary Cullen, Kevin Curran, Jose Santos
Intelligent Systems Research Centre, University of Ulster, Magee College, Derry, United Kingdom Gary.Cullen@lyit.ie
Abstract--The benefits of context aware computing and specifically the context that location can provide to applications and systems has been heavily documented in recent times. Moreover the move from traditional outdoor localization solutions to the indoor arena has seen a dramatic increase in research into this area. Most of this research has surrounded the problem of positioning accuracy, with attempts to solve this using a myriad of technologies and algorithms. One of the problems that seems to be somewhat overlooked is the issue of coverage in an indoor localization solution. The mostly unobstructed views of the Global Positioning System (GPS) which requires a mere 30 satellites to provide global coverage never had these problems. The dearth of literature around this issue in the outdoor arena is testament to this fact. Unfortunately unobstructed views are not something that can be achieved in most indoor environments and economical as well as physical barriers can prevent the installation of an infrastructure to achieve total coverage. In this paper we propose a solution to this issue of indoor coverage by deploying a solution to extend the range of a positioning system Cooperatively Applied Positioning Techniques Utilizing Range Extension (CAPTURE). CAPTURE provides a system to locate devices that cannot be reached by an in-house location based system. It presents a unique contribution to research in this field by offering the ability to utilize devices that currently know their location within a Location Based Solution (LBS), to help evaluate the position of unknown devices beyond the range capacity of the LBS. Effectively extending the locating distances of an Indoor LBS by utilizing the existing mobile infrastructure without the requirement for additional hardware. CAPTURE uses the Bluetooth radios on mobile devices to estimate the distance between devices, before inserting these range estimates into a trilateration algorithm to ascertain position. CAPTURE has been tested through experiments carried out in a real world environment, proving the capacity to provide a solution to the ranging issue. Keywords--Localization; Indoor positioning; Indoor localization; geographical positioning; Bluetooth; Cooperative Positioning.)

Gearoid Maguire, Denis Bourne
Letterkenny Institute of Technology, Co. Donegal, Ireland

Bluetooth beacon. The test area and experiments were the same for both systems and the results were weighed to evaluate the best solution to solve the problem of range in Indoor Positioning Systems (IPS). On loosing something or forgetting where you last placed something, a common piece of advice is to retrace your steps back in your mind. This can be quite a formidable task given the multimodal transport available today coupled with the complexity and scale of buildings we interact with on a regular basis. The ability to place an avatar of yourself onto a map to graphically retrace your steps in real-time would dramatically reduce the brain power required to remember everywhere you were at a given time. Googles maneuverings into the indoor location mappings realm [2] opens up the opportunity to deliver this virtual reality, currently being able to provide door to door route planning. Being able to navigate your way from your office desk out through your company's building (taking the stairwell to avoid your boss in the lift) is eminently achievable albeit with a small number of locations on a modern smartphone using google maps. A level switcher allows you to onion slice through multiple floor level plans, before switching to GPS to offer possible transport alternatives through the outdoor environment. On reaching what `historically' would have been your destination, Google Indoor Maps and more importantly an IPS picks up where GPS left off offering a point to point navigation solution. This can then take you through the complexities of an airport terminal for example, via specific waypoints such as security and check-in desks directly to your departure gate. One of the barriers to implementation of such a concept is the limitation in coverage and accuracy of currently implemented Indoor Position or Location Based Systems [3]. IPSs typically utilize pre-existing Wi-Fi network infrastructure taking ranging information from Wireless Access Points (WAP's) as inputs for a localization algorithm. Unfortunately the drivers behind the strategic decisions on the positioning of WAPs, in a Wi-Fi based solution, were typically to catch large congregations of users and primarily to provide the highest available throughput to those users. Coverage for IPSs is not necessarily to the forefront of network designer's minds when designing such networks, leaving large areas beyond the range of an IPS. GPS on the other hand, offers near global coverage, bar some issues with urban canyons and other high rise natural obstacles that prevent Line of Sight (LoS) to the just under 30 satellites required [4] to deliver such wide scope.

I.

INTRODUCTION

The first iteration of CAPTURE described in the following literature [1], used the RSSI readings taken from the IEEE 802.11 radio on the mobile devices to gauge the range between the devices by measuring the signal loss to estimate distance. The version of CAPTURE implemented and evaluated in this paper uses the Bluetooth radio on the devices to estimate distances between devices based on the RSSI received from the

The indoor environment does not afford such clear unobstructed views to and from tracking devices, the many doors, walls, floors, pillars and ceilings hinder the capacity of an IPS to locate devices. Furthermore the indoor arena is an especially noisy atmosphere, being home to other wireless devices such as Bluetooth Headsets, Cordless Phones and Microwave Ovens. All of these devices operate in the same frequency band as the Wi-Fi solution, namely 2.4 GHz and therefore can interfere with the reception of signals used to locate [3], making them behave in an unpredictable fashion. These environmental dynamics combine to dramatically affect the ability of an indoor solution to provide an acceptable level of coverage. Literature from Yang [4] and Rowe [5] reflect that Location Awareness is rapidly becoming a fundamental requirement for mobile application development. This highlights the challenges posed for ubiquitous localization of devices in the indoor arena. Considering users spend more time in an indoor environment, over 88.9% according to a recent Canadian study [5], the need for a solution is obvious. We propose a solution to this issue of coverage limitations by using a cooperative localization technique, CAPTURE. CAPTURE can plug into an in situ solution irrespective of the technology or location technique that solution currently uses to locate.

Consider the following scenario where a user `Bob', is in his favorite seat in the library, unfortunately the seat is in the far corner of the library, which can only be `seen' by one Wireless Access Point. In this position Bob's tablet can gain Wi-Fi access through this Access Point to allow him access to online resources. However one Access Point is not enough for the inhouse Location Based System to accurately locate Bob within the building using Trilateration positioning techniques. Sue is sitting near the front of the library and can be `seen' by 4 Wireless Access Points, and is thereby accurately located on the Location Based System. She is also 25 meters to the left of Bob and the Wireless Network Card on her Laptop can see Bob's tablet. The Librarian is stacking books on the shelves behind where Bob is sitting and her smartphone is currently located within the Location Based System also. The wireless NIC on her smartphone can also `see' Bob's tablet, therefore, in a normal scenario, Bob would be beyond the range of the Location Based System, but because CAPTURE can use the known positions of the Librarian and Sue and Bob's position relative to them it can accurately estimate Bob's position within the library. It provides a location relative to the devices locating it, which can then be mapped onto a global overview of the Location Based System (LBS), assisting in the aforementioned scenario to get you to the departure gate in a point to point navigation solution.

Figure 1: Sports Hall LyIT

The rest of this paper is laid out as follows; Section II describes the system model used to implement CAPTURE. Section III provides an overview of the experimental test bed used to evaluate the solution and Section IV documents the data collected during testing. In Section V we describe the findings of the experiments that were carried out, validating the feasibility of the system, the penultimate section, Section VI outlines the proposed implementation of CAPTURE and the paper concludes Section VII, providing an insight into some projected future work with CAPTURE. II. CAPTURE - SYSTEM MODEL This section describes a system model that can be used in a localization solution to establish the Cartesian coordinate values of a lost device within a two dimensional plane. CAPTURE does not require a preceding calibration stage or a site survey, providing a robust opportunistic solution in dynamic environments, using only real time RSSI values. We use the term reference device to describe devices that cooperatively assist in the positioning of lost or unknown devices. Traditionally the term anchor node is used to describe these devices, but this seems to elicit a perception of static or permanent devices, which in a cooperative solution these devices most certainly are not. Two key components typically make up the estimation of the position of a lost device. First of all ranging techniques are used to estimate the distance from the transmitting device(s) to the receiving device(s). This is calculated using a metric for example the length of time it takes a signal to propagate the distance from the transmitter to the receiver. The second component is the position estimation technique, here the ranging variables are calculated using one or more ranging techniques and these are used as input for an estimation algorithm to calculate the position of the lost device. A. CAPTURE ­ Bluetooth CAPTURE was first implemented using RSSI measurements from the 802.11 radio on mobile device [1]. The implementation of CAPTURE described in this paper utilizes Bluetooth radio beacons to measure range. Bluetooth has been around for quite some time now, designed by phone manufacturer Ericsson in 1994, it was initially developed to replace the then ageing RS-232 and Infrared (IR) interfaces for connecting peripheral devices. It operates at the same 2.4GHz frequency as Wi-Fi and is specified in the IEEE 802.15.1 standard. The overriding benefit of using Bluetooth for Indoor Localization is its availability in nearly every mobile device in use today. Using Bluetooth in a cooperative framework also allows the user to remain connected to the 802.11 network while simultaneously assisting in the location of others with Bluetooth radio signals. Bluetooth transmits beacons similar to 802.11 radios and the strength of the signal received from these beacons can be captured and measured to provide a range estimate. Kloch et al [6] investigate effects in Collaborative Indoor Localization as an example of selforganizing in ubiquitous sensing systems, using Bluetooth to correct Pedestrian Dead Reckoning (PDR) drift. They analyze the collaborative approach as a solution to the indoor localization problem, and found that when using PDR in

isolation the variance grows bigger as people are walking. That is to say that the position estimation becomes less and less accurate the further the people being tracked travel. Implementing a hybrid solution incorporating Bluetooth RSSI readings to measure the distance between devices, dramatically improved positioning accuracy. Bluetooth has been further used as a cooperative solution to the accuracy issue in IPS's [7-11]. B. RSSI ­ Received Signal Strength Indicator Possibly the most popular ranging technique used in Indoor Localization, Received Signal Strength Indicator (RSSI) is a measurement of the voltage that exists in a transmitted radio signal, which is an indication of the power being received by the antenna. When a signal first leaves a transmitting device, the power of the signal drops or attenuates, this is true of both wired and wireless transmissions. As a radio signal propagates through the air some of its power is absorbed and the signal loses a specific amount of its strength, therefore, the higher the RSSI value (or least negative in some devices), the stronger the signal. Knowing the amount of signal loss over a given distance provides a method to calculate the distance from a transmitting device, given a Received Signal Strength. At its most basic level this allows for the `coarse' localization or as referred to in other literature, `presence-based localization' [12] of a device relative to the transmitting device. This can be illustrated by the RSSI calculated distance being the radius of a circle and the `searching' device being at the centre of that circle. The estimated position of the lost device is anywhere on the circumference of that circle. In an IEEE 802.11 network if the locations of the Access Points are already known, then the location of Mobile Devices traversing the network can be located relative to them, albeit only to the circumference of the radius of the calculated distance. Further localization algorithms and position estimation filtering techniques must be applied to provide a more precise level of localization. In a cooperative paradigm, mobile devices can simulate the role carried out by Access Points, providing a relative reference to a lost devices location. RSSI values can be extracted from beacons transmitted between devices within range. Correlation of these signal indicators and distance can be estimated using many of the methods already applied throughout literature in this arena [13-15]. RSSI based or more broadly speaking, radio based Indoor Positioning Systems have had notoriously irregular environment variables such as reflection, refraction, diffraction and absorption of radio waves that can impact positioning estimated dramatically [16]. Although RSSI is a measure of signal loss, it is not a linear representation of how many dBm is actually reaching the card. If a signal indicator is reading -72, this means that it is 72 dBm less powerful by the time it gets to your device. Experimental test carried out at an early stage with CAPTURE further extoled this assumption. Results of these tests can be viewed in Table 1: 5 meter increments in Section V, Data Collection and Presentation. Crudely extracting the RSSI at given distance increments to attempt to derive a meter distance

being equal to a given dBm increase in RSSI reading was not going to yield any value worth using in any further experiments. The authors in [17] advocate a solution utilizing a RSSI smoothing Low Pass Filter (LPF) to minimize the dynamic fluctuation of the RSSI values. C. Trilateration Trilateration is a key component of the GPS position estimation techniques. It is a process that can estimate the position of a mobile device given the positions of at least three other objects and the distance from those objects to the device to be located. In the scenario depicted below in Figure 2(a), illustrated using a cooperative localization example, the circle depicts the distance from a reference device to a lost device. This distance would have been derived using the RSSI value between the reference and lost devices. All we can say about the whereabouts of the lost device is that it resides somewhere on the circumference of the circle that is constructed using the radius of the estimated measurement between the two devices. A second reference device will allow the position of the lost device to be narrowed further as can be seen in Figure 2(b). Now the ranging estimates of the lost device have been calculated relative to the second reference device also. Therefore considering the lost device must be on the circumference of the circles created by the distance between it and the two reference devices there are only 2 possible positions where it might be, the intersections of these two circles.

To calculate the exact position of the lost device we need a third reference device. When we calculate the distance from this final reference device to the lost device and considering we already know the distance from the other reference devices. We can then determine that the lost device can only be at one specific position to match those three particular distance estimations ­ the intersections of the three circles (see Figure 3). The ranging estimates calculated from the RSSI values in the tests were used as the inputs for the trilateration algorithm on the CAPTURE, to provide an estimate on the position of the lost phones. III. EXPERIMENTAL TEST BED In this section, we will provide evidence showing the suitability of CAPTURE as a solution to the indoor ranging problem. To do that we carried out a large campaign of measurements in the Sports Hall in Letterkenny Institute of Technology illustrated in Figure 1. The hall offers a 40m diagonal testing range, providing Line of Sight measurements for all tests, as can be seen in the picture depicted in Figure 4. When readings were been recorded all users vacated the hall, this provided an optimal environment to use as a benchmark for future tests on CAPTURE.

Figure 2: (a) Single Distance

(b) With 2nd Reference Device

Each phone used in the test is given a name (BSSID) TestPhone1, TestPhone2 for example. CAPTURE reads the RSSI of all available reference points, i.e. all devices it can `see', but it filters out only the test phones selected by the user carrying out the tests. This can be seen in the image in figure 5, and is achieved via a lookup table mapping the MAC address of the phone to the phone name. This allows the use of only a specified phone or a group of phones during any given test. A. System Components Figure 4: Test Environment The experimental setup of the prototype consisted of 7 Samsung GT-S5310 Galaxy Pocket phones (Figure 5), running Google Android 2.2.1 on a 600 MHz ARMv6, Adreno 200 GPU, Qualcomm MSM7227 chipset, were used to carry out the evaluation of the CAPTURE system. 3 of the phones were used as reference devices, the other phone acted as the lost device. All phones used during the test were of an exact make and model so as to rule out any issues with varied RSSI reads with different antenna types. Some of these issues have been described in the literature [18, 19]. Lisheng et al., [19] go so far as to describe the distortion being as much as 11.2 dBm out with different antenna types over a 25 meter read range. Although these issues referenced above describe problems in the 802.11 realm, it is the author's opinion that these could have an impact on Bluetooth radio signals also.

Figure 3: Trilateration Example

Where: n: Path Loss Exponent d: Distance from transmitting device A: Received signal strength at 1 meter distance The path loss exponent typically varies from 1.5 to 4, with 1.5 representing a free-space Line of Sight (LoS) value and 4 representing an environment that incorporates a high level of signal attenuation. Not having a good equation modeling the environment in which your experiments are to be deployed, will be reflected in horrible results. After initial pre-tests were evaluated, a Path Loss Exponent of 1.5 was determined for the test environment, because of the open plan design of the Hall offering LoS between all devices and the RSSI at 1 meter was measured at -66.8194. The results of the collected data are described in the following section. IV. DATA COLLECTION AND PRESENTATION Here we present all of the data collated throughout this work, the data sets are illustrated in the graph and table. During the recording of data the hall was emptied of people so as to provide a clean set of results. An initial test was run to establish the 1 meter range for input into the algorithm in equation 1, the results of this test can be seen in Figure 6. The fluctuations in the meter range values was one of the notable differences between the tests recorded in the 802.11 version of CAPTURE versus the Bluetooth version. In the Wi-Fi version meter read values were captured from -42 to -45. Here, as can be seen in the graph readings ranged from -62 to -77, a difference of 3dBm was recorded in the Wi-Fi test, with a difference of 15dBm in the Bluetooth experiments.

Figure 5: Test Phones During the tests all phones were place at a distance of 80cm above floor level, to mimic as close to a real world example of a user holding them. The phones were placed on identical platforms during the tests to negate the impact of Hand-Grip body-loss effect which can also impact ranging measurements [18]. Device orientation can also introduce errors when calculating signal range estimates, so all phones had the same orientation when used in our tests [20].  Database A MySQL Server version: 5.0.96 hosted on a Linux platform was used to store all data collected by the devices. The server was online and the phones wrote directly to it as they recorded RSSI values from each other. The data was then passed through a low level filter to remove any outliers, before an average RSSI reading was calculated for each required ranging measurement, to be used in the trilateration algorithm to estimate the position of the lost device.  Laptop A Dell Latitude E6440 iCore3 running Windows 7 Professional was used to develop the app to gather the RSSI from the phones. An algorithm was designed to convert this RSSI reading into a ranging measurement before a trilateration algorithm converted the ranging measurements into Cartesian coordinate values. We used the Eclipse IDE and Android Software Development Kit (SDK) for Android development and debugging, to develop the app. B. Ranging Measurement Estimation The RSSI values captured from the beacons transmitted by devices within range of the `lost device' were used to estimate the relative distance between them. As explained earlier RSSI values do not provide a linear representation of distance. The authors in [17] advocate using the formula in "(1)," below to estimate RSSI, and thereby extrapolate distance given RSSI: RSSI = - (10n Log10 (d) +A) Equation (1)

1 Meter RSSI values
-80 -75 -70 -65 -60 Figure 6: 1 meter readings 500 readings were recorded at various locations throughout the hall, to accurately obtain the meter value for the algorithm, these were smoothed with a filter before the final average was calculated. Further tests were then carried out to measure the accuracy of both the RSSI values received and the resulting range estimations from the algorithm. Table 1 below, depicts the results of tests to capture the RSSI values between two phones at 5 meter increments diagonally across the hall. It highlights the RSSI value beginning at -72.3793 for the 0-5 meter range.

A sample set of 200 readings were recorded per section, an average was then taken from this set. The standard deviation was also documented to illustrate any fluctuations in the received values. In our previous experiments with CAPTURE using Wi-Fi [1] the standard deviation was typically low, in this case using Bluetooth as can be seen in the table below standard deviation ranges from 4.2 to 2.9, these are large fluctuations from the average. Distance Average Std Dev Estimate Distance Average Std Dev Estimate 0-5m -72.3793 4.1140 3.73 0 - 25 m -80.6205 4.1062 28.82 0 - 10 m -74.8966 3.6327 7.62 0 - 30 m -80.9657 3.3776 29.38 0 - 15 m -76.6333 3.9603 11.20 0 - 35 m -80.2759 4.2823 27.87 0 - 20 m -76.3103 3.9226 9.69 0 - 40 m -83.3103 2.9490 49.95

They also know the distance between themselves:    TestPhone1 to TestPhone2 is 15 meters TestPhone1 to TestPhone3 is 13 meters TestPhone2 to TestPhone3 is 17 meters

The RSSI readings from the:    Lost Phone to TestPhone1 is - 77.5351dBm Lost Phone to TestPhone2 is - 78.8457dBm Lost Phone to TestPhone3 is - 76.1021dBm

These RSSI readings translate to a ranging estimate of 13.345, 15.1221 and 9.349 meters respectively when put through the ranging algorithm. The actual distance between:    TestPhone1 and the Lost Phone is 11.5 meters TestPhone2 and the Lost Phone is 13.2 meters TestPhone3 and the Lost Phone is 11.9 meters

Table 1: 5 meter increments The average was then inputted into the algorithm to derive a range estimate based on the RSSI values received. As mentioned before RSSI values do not provide a linear representation of measurement, and therefore some of the increments do not initially seem like they could assist in finding a distance at a given measurement. The ranging estimates show an error high of 11.31 meters at the 0-20 meter range and low of .62 meters at the 0-30 meter range. V. EXPERIMENTAL RESULTS Figure 7 depicts one of the tests where CAPTURE accurately locates a lost phone within 2.5 meters. TestPhone1, TestPhone2 and TesPhone3 know their location, via the inhouse IPS.

Giving an approximate average error rate of 2.5 meters. From the schematic of the test pictured in Figure 7 CAPTURE's visualizer module (Figure 8) graphically depicts the positions of the cooperative reference devices on screen along with the actual and estimated positions of the lost device. The positions of the 3 reference devices are entered into the visualizer manually, which can be seen in blue on the screen. The position of the lost device is also entered, it is illustrated in red on the screen. The application then reads in the RSSI values before estimating the position of the lost device, shown in green here on the screen.

Figure 7: Finding Lost Phone

Figure 8: Visualizer module

VI.

CAPTURE ­ SYSTEM IMPLEMENTATION

[5]

In order for CAPTURE to be able to cooperatively locate a lost device within a network, there must be at least 3 reference devices within sight of the lost device. Each of these must have `a prior' knowledge of their location within a preexisting localization solution. The hypothesis of CAPTURE was to extend the range of in-house IPS's, and tests shown in both have proven that it can achieve exactly this. Existing IPS's have dramatically more powerful infrastructure than what CAPTURE would utilize though. For example 230 volt AC powered Access Points in a standard IPS versus 12 volt DC powered mobile reference devices (smart phones, tablets and\or laptops) in a cooperative solution. It would be naive to think that accuracy levels of an in-house IPS would also `extend' to a cooperative model, although this does not take away from the solution to the range issue that CAPTURE provides. The implementation of a more comprehensive filter would nonetheless assist with accuracy for example the Kalman or Extended Kalman Filters are recommended in the following literature [7, 21]. VII. CONCLUSION This paper introduces CAPTURE a cooperative localization system using Bluetooth, that provides a solution to the problem of devices being out of range of a hosted Indoor Positioning System. Although the earlier implementation of CAPTURE using 802.11 provided more accurate results, experiments with the Bluetooth version of CAPTURE still demonstrate that utilizing a cooperative framework of mobile devices can extend the range of an in situ Indoor Positioning System by at least the range of the outermost devices located within the system. While CAPTURE using 802.11 [1] provides a more accurate solution, CAPTURE Bluetooth can actively transmit and receive beacons while still connected to the Wi-Fi network, something the 802.11 version cannot currently achieve. Disconnecting a user from a network to allow them to assist in the localization of another device is not something that would lead to large scale adoption of a solution. Wi-Fi Direct proposes to solve the issue of peer-to-peer communication during network connectivity. The implementation of a Wi-Fi direct version of CAPTURE is something that the next iteration of CAPTURE would hope to include. REFERENCES
[1] G. Cullen, K. Curran, and J. Santos, "CAPTURE - Cooperatively Applied Positioning Techniques Utilizing Range Extensions," in 5th International Conference on Indoor Positioning and Indoor Navigation (IPIN 2014), IEEE, Busan, Korea, 2014, pp. 22-29. M. Aly and J. Y. Bouguet, "Street view goes indoors: Automatic pose estimation from uncalibrated unordered spherical panoramas," in Applications of Computer Vision (WACV), 2012 IEEE Workshop on, 2012, pp. 1-8. G. Cullen, K. Curran, and J. Santos, "Cooperatively extending the range of Indoor Localisation," in Signals and Systems Conference (ISSC 2013), 24th IET Irish, 2013, pp. 1-8. G. M. Djuknic and R. E. Richton, "Geolocation and assisted GPS," Computer, vol. 34, pp. 123-125, 2001.

[6]

[7]

[8]

[9] [10]

[11] [12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[2]

[20]

[3] [4]

[21]

C. J. Matz, D. M. Stieb, K. Davis, M. Egyed, A. Rose, B. Chou , et al., "Effects of Age, Season, Gender and Urban-Rural Status on Time-Activity: Canadian Human Activity Pattern Survey 2 (CHAPS 2)," International journal of environmental research and public health, vol. 11, pp. 2108-2124, 2014. K. Kloch, G. Pirkl, P. Lukowicz, and C. Fischer, "Emergent behaviour in collaborative indoor localisation: An example of selforganisation in ubiquitous sensing systems," in Architecture of Computing Systems-ARCS 2011, ed: Springer, 2011, pp. 207-218. A. Baniukevic, D. Sabonis, C. S. Jensen, and L. Hua, "Improving Wi-Fi Based Indoor Positioning Using Bluetooth Add-Ons," in Mobile Data Management (MDM), 2011 12th IEEE International Conference on, 2011, pp. 246-255. S. Aparicio, J. Perez, A. M. Bernardos, and J. R. Casar, "A fusion method based on bluetooth and WLAN technologies for indoor location," in Multisensor Fusion and Integration for Intelligent Systems, 2008. MFI 2008. IEEE International Conference on, 2008, pp. 487-491. F. J. Gonzalez-Castano and J. Garcia-Reinoso, "Bluetooth location networks," in Global Telecommunications Conference, 2002. GLOBECOM '02. IEEE, 2002, pp. 233-237 vol.1. C. Liang, H. Kuusniemi, C. Yuwei, P. Ling, T. Kroger, and C. Ruizhi, "Information filter with speed detection for indoor Bluetooth positioning," in Localization and GNSS (ICL-GNSS), 2011 International Conference on, 2011, pp. 47-52. Z. Sheng and J. K. Pollard, "Position measurement using Bluetooth," Consumer Electronics, IEEE Transactions on, vol. 52, pp. 555-558, 2006. A. E. Kosba, A. Saeed, and M. Youssef, "Robust WLAN Devicefree Passive motion detection," in Wireless Communications and Networking Conference (WCNC), 2012 IEEE, 2012, pp. 32843289. D. Gualda, J. Urena, J. C. Garcia, E. Garcia, and D. Ruiz, "RSSI distance estimation based on Genetic Programming," in Indoor Positioning and Indoor Navigation (IPIN), 2013 International Conference on, 2013, pp. 1-8. M. O. Gani, C. Obrien, S. I. Ahamed, and R. O. Smith, "RSSI Based Indoor Localization for Smartphone Using Fixed and Mobile Wireless Node," in Computer Software and Applications Conference (COMPSAC), 2013 IEEE 37th Annual, 2013, pp. 110117. S. Shioda and K. Shimamura, "Anchor-free localization: Estimation of relative locations of sensors," in Personal Indoor and Mobile Radio Communications (PIMRC), 2013 IEEE 24th International Symposium on, 2013, pp. 2087-2092. L. Erin-Ee-Lin and C. Wan-Young, "Enhanced RSSI-Based RealTime User Location Tracking System for Indoor and Outdoor Environments," in Convergence Information Technology, 2007. International Conference on, 2007, pp. 1213-1218. J. Joonyoung, K. Dongoh, and B. Changseok, "Automatic WBAN area recognition using P2P signal strength in office environment," in Advanced Communication Technology (ICACT), 2014 16th International Conference on, 2014, pp. 282-285. F. D. Rosa, X. Li, J. Nurmi, M. Pelosi, C. Laoudias, and A. Terrezza, "Hand-grip and body-loss impact on RSS measurements for localization of mass market devices," in Localization and GNSS (ICL-GNSS), 2011 International Conference on, 2011, pp. 58-63. X. Lisheng, Y. Feifei, J. Yuqi, Z. Lei, F. Cong, and B. Nan, "Variation of Received Signal Strength in Wireless Sensor Network," in Advanced Computer Control (ICACC), 2011 3rd International Conference on, 2011, pp. 151-154. K. Kaemarungsi and P. Krishnamurthy, "Properties of indoor received signal strength for WLAN location fingerprinting," in Mobile and Ubiquitous Systems: Networking and Services, 2004. MOBIQUITOUS 2004. The First Annual International Conference on, 2004, pp. 14-23. S. S. Saad and Z. S. Nakad, "A Standalone RFID Indoor Positioning System Using Passive Tags," Industrial Electronics, IEEE Transactions on, vol. 58, pp. 1961-1970, 2011.

Exposing Ourselves: Displaying our Cultural Assets for Public Consumption
Gary Munnelly
Adapt Centre O'Reilly Building Trinity College Dublin, Ireland

Kevin Koidl
Adapt Centre O'Reilly Building Trinity College Dublin, Ireland

Séamus Lawless
Adapt Centre O'Reilly Building Trinity College Dublin, Ireland

munnellg@tcd.ie ABSTRACT

Kevin.Koidl@scss.tcd.ie

Seamus.Lawless@scss.tcd.ie

This paper discusses an early stage project to develop a new, enhanced interface for Trinity College Dublin (TCD) Digital Collections website. We describe the current state of the portal and outline some of the unique issues observed when examining user engagement. A major factor in our development of enhanced search tools will be to leverage the entities present in the documents to establish more reliable connections between items in the collection. Not only do we expect that this will lead to better ranked search results, but we also wish to investigate how these entities may be used to encourage site visitors to explore the site beyond their initial research goal. The early stage of this project means that plans are still being finalised. Hence we speculate about other methods which may be applied to this corpus.

Figure 1: Graph of most popular search terms on the Digital Collections site

Keywords
Entity Search; Digital Libraries; Information Retrieval

1.

INTRODUCTION

In many ways, the vision of Digital Humanities with regards to cultural heritage is a noble one. It is one in which all people have free, unbridled access to primary sources from which they may learn about their heritage and the rich history of their origins. We are free to lose ourselves in the depths of a historical archive from the comfort of our computer screens and supported in our exploration by a host of intelligent information retrieval systems. In theory, after the arduous process of digitising the collection, providing such functionality ought to be a simple task. Building and deploying a website has become a trivial process and off the shelf tools such as Solr provide state-of-theart text retrieval functionality with minimal effort. Given a suitable portal and a search box which returns ranked results, what more could a user want?

1 st International Workshop on Accessing Cultural Heritage at Scale (ACHS'16), 22 nd June 2016, Newark, NJ, USA. Copyright © 2016 for this paper by its authors. Copying permitted for private and academic purposes.

As it happens, this approach to curating documents has been found wanting in many ways. The most immediate problem with the query-response paradigm is that in order to be able to use the search interface we must know exactly what we are looking for and the manner in which it is represented in the collection. The search engine retrieves documents that it judges to be pertinent to our query and returns them to us without any explanation as to why these might be relevant, nor any encouragement to continue our investigation in a particular direction. It is up to the user to interpret the results, it is up to the user to establish relationships within the collection and it is up to us as the user to identify worthwhile avenues of future research [7]. Given that their knowledge of the collection is probably quite limited to begin with, this is hardly helpful. As was aptly put by Mitchell Whitelaw [8], these interfaces are not "generous". This need for a more generous interface is the focus of a project currently being undertaken by Trinity College Dublin (TCD) Digital Collections. At present the website provides the simple search box that we have come to expect which is driven by a default deployment of Solr. After conducting a search, users can narrow their interests along a broad series of facets: genre, media type, Trinity department, date and subject area. This interface results in a limited search experience, particularly with regards to exploration. The effects of this are demonstrable simply by looking at where the majority of traffic flows through the site (Figure 2). The most famous text on the Digital Collections portal is the Book of Kells [1]. A huge percentage of hits on the site can be attributed to this single page and variants of

Figure 2: Graph of pages which site visitors first land on. Note the DRIS ID for the Book of Kells is MS58 003v which ranks above the home page

the query string "Book of Kells" are consistently among the most frequent searches conducted. Indeed, it is worth noting that many visitors to the site land directly on the page for the Book of Kells having been referred there from Google, Facebook, Twitter etc. They never even see the initial search box on the homepage. After viewing the book, most users then simply browse away from the portal, not realising that they have barely touched the tip of the iceberg with regards to the volume of information and material available to them. Hence our goal is twofold; to provide a better, more accurate, more supportive search experience to users who come to explore the TCD Digital Collections site and to foster a sense of curiosity in those who come to see one artifact, but may have an interest in so many more.

name cannot be found in NAF, then ULAN is used instead. Although the entries in these ontologies are not explicitly referenced by the meta-data (i.e. there are no URIs used in the XML file), the names of various fields have been selected so that they may be related back to their ontological equivalents. For example, the field denoting the subject of a document is named subjectlcsh indicating that the data stored here is relevant to the LCSH ontology. While this is not ideal, it does mean that semantically linking the collection is possible and has be made easier by this method of annotating the data. In addition to these rigidly defined attribute fields, there are also a number of free text fields, abstract and description being the two most verbose. These free text fields contain additional information about the artifact, much of which is not actually described in the more semantic attributes. These are human readable sections which describe the artifact in moderate detail, giving an explanation of its origins, who commissioned it, where was it commissioned, how it came to be in the library or any other information which was available to the transcriber. Often these fields reference entities which are not mentioned in any of the other document attributes, meaning there is much information hidden in these fields which could be extracted and harnessed to power a more meaningful search experience.

3.

METHOD

2.

CORPUS

The corpus is comprised of approximately 100,000 high resolution scans of various documents curated by the Digital Collections group. These range from manuscripts to illustrations, etchings, postcards, templates, graphs, musical scores and more, spanning more than 1,000 years of human history. Information extraction techniques such as optical character recognition (OCR) have not been applied to the renderings, but each image has meta-data associated with it describing important attributes of the artifact. This data is listed in a single XML file which has been provided to us and is the foundation upon which we must build a new search interface. As is typical in collections of this type, many of the XML fields denote information such as page number, document ID, catalogue number etc. However, there has also been some effort made to make the collection semantically inclined, although not fully semantically linked. The names of several fields are designed to reflect the structure of four well established library cataloguing ontologies: The Library of Congress Name Authority File (NAF), The Library of Congress Subject Headings (LCSH), Getty Vocabularies Art and Architecture Thesaurus (AAT) and Getty Vocabularies Union List of Artist Names (ULAN). The choice of ontology for a particular field is dependent on the nature of the content it represents and the availability of information within  Zs ´ the ontologies themselves. For example, if an artist^ aA

Fostering engagement and encouraging exploration means discerning what interests a user and presenting them with content which relates to that interest. It may also mean determining what is of interest to a community of people at large and using this group perspective to assist an individual whose exploration has stalled. While we could use traditional language modelling or probabilistic methods to determine which documents may be discussing the same subject and then make recommendations based on that, it is much better if we can establish what real world, tangible objects are influencing the user's search and then trace these figures through the collection. In order to do this, we must know what entities are present in the corpus to begin with. We are fortunate that many potentially useful entities have been manually extracted and stored in the XML file for us. However, much information is also hidden in the free text fields spread throughout the meta-data. This presents some interesting opportunities to perform automatic information extraction and analysis on the collection. Named Entity Recognition (NER) is a well established field in Natural Language Processing (NLP) for locating references to known entities in a body of text [6]. In general we search for specific patterns, parts of speech or words which appear in a gazetteer of terms. Much like anything involving natural language and computers, the results can be noisy. However, after the results of NER have been sanitised, they may then be disambiguated to a suitable knowledge source [5, 2]. Within the Digital Collections corpus, identifying mentions of entities in the free text fields and disambiguating them to a common knowledge base will allow us to establish which documents are related to which entities and, by extension, which documents are related to each other. Disambiguation involves more than just co-referencing these entities within the collection. It links the collection's enti-

Figure 3: A screenshot of the current home page of the digital collections website ties to a higher knowledge base which may connect them by proxy to external knowledge sources such as Wikipedia. These external sources may assist the user in understanding the primary source material making the content more accessible for those who are inexperienced with the collection. The challenge is to determine which entity in the knowledge base is being referred to by the mention found in the text. While this focus on entities may be useful, it may also be of benefit to attempt to establish the larger context in which a user's search is taking place. While the corpus is large in size (the abstracts alone totalling almost 21,000,000 words) the vocabulary is highly constrained (a little over 10,000 unique terms) suggesting that topic modelling may also be a viable option for structuring the corpus and influencing search. Accurate topic modelling is difficult to achieve. Determining exactly how much content is required in order for a topic model to stabilise can be hard [4] and even after the model has stabilised there is no guarantee that the topics will be of use. Nevertheless, it may still be a worthwhile investigation to perform topic analysis such as Latent Dirichlet Allocation [3] on the collection to see if new, useful patterns beyond the broad facets already in use may be found. of data also plays a role in the accuracy of automatic methods. However with the data extracted from the collection, we have more information at our disposal for assisting and engaging with the user as they search the collection. Of course, even the best search interface can be felled by poor user interface design. This too will be a factor in the final development of the new Digital Collections portal.

5.

ACKNOWLEDGMENTS

This research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) in the ADAPT Centre (adaptcentre.ie) at Trinity College, Dublin.

6.

REFERENCES

4.

CONCLUSIONS

As can been seen, there are several options for what can be done when given a collection such as TCD's Digital Collections corpus. The quality with which we can automatically extract information and relationships from the collection are greatly dependent on the quality of the data itself. Quantity

[1] Book of Kells. http://digitalcollections.tcd.ie/home/ index.php?DRIS ID=MS58 003v. [Online; accessed 30-May-2016]. [2] A. Alhelbawy and R. J. Gaizauskas. Graph ranking for collective named entity disambiguation. [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993­1022, 2003.  ZCallaghan, ´ [4] D. Greene, D. O^ aA and P. Cunningham. How many topics? stability analysis for topic models. In Machine Learning and Knowledge Discovery in Databases, pages 498­513. Springer, 2014. [5] Z. Guo and D. Barbosa. Robust entity linking via random walks. In Proceedings of the 23rd ACM International Conference on Conference on Information

and Knowledge Management, pages 499­508. ACM, 2014. [6] D. Nadeau and S. Sekine. A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):3­26, 2007. [7] R. W. White and R. A. Roth. Exploratory search: Beyond the query-response paradigm. Synthesis Lectures on Information Concepts, Retrieval, and Services, 1(1):1­98, 2009. [8] M. Whitelaw. Generous interfaces for digital cultural collections. Digital Humanities Quarterly, 9(1), 2015.

A framework is developed that supports the theoretical design of an organizational memory information system (OMIS). The framework provides guidance for managing the processing capabilities of an organization by matching knowledge location, flexibility, and processing requirements with data architecture. This framework is tested using three different sets of data attributes and data architectures from 147 business professionals that have experience in IS development. We find that trade-offs exist between the amount of knowledge embedded in the data architecture and the flexibility of data architectures. This trade-off is contingent on the characteristics of the set of tasks that the data architecture is being designed to support. Further, the match is important to consider in the design of OMIS database architecture.A Project Spine for Software Engineering Curricular Design
Kevin Gary, Timothy Lindquist, Srividya Bansal, Arbi Ghazarian Arizona State University Mesa, AZ 85212 USA {kgary*, tim, srividya.bansal, arbi.ghazarian} @ asu.edu Abstract
Software engineering education is a technologically challenging, rapidly evolving discipline. Like all STEM educators, software engineering educators are bombarded with a constant stream of new tools and techniques (MOOCs! Active learning! Inverted classrooms!) while under national pressure to produce outstanding STEM graduates. Software engineering educators are also pressured on the discipline side; a constant evolution of technology coupled with a still emerging engineering discipline. As a handson engineering discipline, where engineers not only design but also construct the technology, guidance on the adoption of project-centric curricula is needed. This paper focuses on vertical integration of project experiences in undergraduate software engineering degree programs or course sequences. The Software Enterprise, now in its 9 th year, has grown from an upper-division course sequence to a vertical integration program feature. The Software Enterprise is presented as an implementation of a project spine curricular pattern, and a plan for maturing this model is given.

1. Introduction
Hands-on, or applied learning is expected to produce learners more engaged than those in traditional lecture oriented classes [1]. Significant efforts [2][3][4] in software engineering and computer science education focus on content taxonomies and bodies of knowledge. This is not a bad thing, but taken in isolation may lead educators to believe content coverage is more important than applied learning experiences. There is literature on project-based learning within computing as a means to learn soft skills and complex technical competencies. However, project experiences tend to be disjoint [5]; there may be a freshman project or a capstone project or a semester project assigned by an individual instructor. Yearlong capstone projects are offered at most institutions as a synthesis activity, but to steal a line from Agile methods, if synthesis is good, why not do it all the time ? Project experiences, while pervasive in computing programs, are not a central integrating feature. Sheppard et al. [6] suggests that engineering curricular design should move away from a linear, deductive model and move instead toward a networked model: "The ideal learning trajectory is a spiral, with all components revisited at increasing levels of sophistication and interconnection" ([6] p. 191). The general engineering degree program at Arizona State University (ASU) was designed from its inception in 2005 [7] to be a flexible, project-centric curriculum that embodied such integration (even before [6]). Likewise, the Software Enterprise was started at ASU in 2004 as an upper-division course sequence to integrate contextualized project experiences with software engineering fundamental concepts. The computing and engineering programs at ASU's Polytechnic campus merged under the Department of Engineering in 2008, and in 2009 the Arizona

Board of Regents (ABOR) approved a new Bachelor's degree in software engineering (BS SE). The first graduating class will be in Spring 2014 and ASU plans to undergo accreditation review shortly thereafter. At the course level the Software Enterprise defines a delivery structure integrating established learning techniques around a project-based contextualized learning experience. At the degree program level, the Enterprise weaves project experiences throughout the BS SE degree program, integrating program outcomes at each year of the major. There are several publications on the manner in which the Software Enterprise is conducted within a project course (for example, [8][9]]), and we summarize this in-course integration pedagogy in section 2. The intent of this work-in-progress paper is to describe extending the Enterprise as a spiral curricular design feature we refer to as the project spine , and present our plans moving forward to mature and validate this approach.

2. The Software Enterprise Delivery Model
The Software Enterprise is an innovative pedagogical model for accelerating a student's competencies from understanding to comprehension to applied knowledge by co-locating preparation , discussion , practice , reflection , and contextualized learning activities in time. In this model, learners prepare for a module by doing readings, tutorials, or research before a class meeting time. The class discusses the module's concepts, in a lecture or seminar-style setting. The students then practice with a tool or technique that reinforces the concepts in the next class meeting. At this point students reflect to internalize the concepts and elicit student expectations, or hypotheses , for the utility of the concept. Then, students apply the concept in the context of a team-oriented, scalable project, and finally reflect again to (in)validate their earlier hypotheses. These activities take place in a single three-week sprint , resulting is a highly iterative methodology for rapidly evolving student competencies (Fig 1).

Figure 1. Software Enterprise Delivery (left) Kolb Learning Cycle (right) The Software Enterprise represents an innovation derived from existing scholarship in that it assembles best practices such as preparation, reflection, practice (labs), and project-centered learning in a rapid integration model that accelerates applied learning. Readers may recognize the similarity of the model (Figure 1 left) to the Learning Cycle [10] (Figure 1 right). Our work for the past 9 years in the Enterprise has focused on maturing the delivery process, creating new or packaging existing learning materials to fit the delivery model, and to explore ways to assess project-centered learning.

3. The Software Enterprise Project Spine
An innovation in the new BS in Software Engineering at ASU has been the vertical adoption of the Software Enterprise. Enterprise courses are now required from the sophomore to senior years. This innovation represents what [6] calls a professional spine , as the Enterprise serves as an integrator of learning outcomes for a given year in the major. We refer to our project-centered realization as a project spine , where foundational concepts are tied to project work throughout the undergraduate program . There is significant computing literature on projects (embedded, mobile, gaming, etc.) to achieve learning or retention outcomes. However, computing lacks a framework for integrating concepts in a project spine. The Enterprise is an implementation that moves students from basic comprehension to applied Figure 2. ASU Project Spine knowledge to critical analysis outcomes. In the BS SE at ASU, program outcomes are described at 4 levels: describe , apply , select , and internalize . Students must achieve level 3 ( select between alternatives) in at least 1 outcome and achieve level 2 ( apply ) in all others. The program outcomes for the BS SE include Design, Computing Practice, Critical Thinking, Professionalism, Perspective, Problem Solving, Communication, and Technical Competence . An example leveled outcome description for Perspective is given in Table 1. The Enterprise accelerates level 3 outcomes by providing contextualized integrated experiences fostering decision-making in the presence of soft (communication, teamwork, etc.) and hard (technical) outcomes. Table 1. Perspective learning outcome for the BS SE at ASU
Perspective. An understanding of the role and impact of engineering and computing technology in business, global, economic, environmental, and societal contexts. Level 1. Understands technological change and development have both positive & negative effects. Level 2. Identifies and evaluates the assumptions made by others in their description of the role and impact of engineering and computing on the world. Level 3. Selects from different scenarios for the future and appropriately adapts them to match current technical, social, economic and political concerns. Level 4 . Has formed a constructive model for the future of our society, and makes life and career decisions that are influenced by the model.

The project spine (center of Figure 2 in dark hexagons) integrates technical competencies by assigning projects inclusive of the technical material covered in the regular computing courses. So for example, junior projects (Software Enterprise III and IV) emphasize technical complexities in Networks, Distributed Computing, and Databases, while senior projects emphasize technical complexities in Web and Mobile computing. The technical "focus area" courses are chosen more based on faculty expertise and recruitment goals than software engineering outcomes; one can envision many different areas represented by upper division courses here. These do help address the concern that an accredited software engineering degree has an application area. A risk we have not yet addressed is if the technical area impacts the software engineering process, such as with a soon-to-be-introduced embedded systems focus area.

There are 2 additional aspects of integration to the project spine. As summarized in section 2, the Enterprise integrates software engineering concepts throughout the project experiences. Students in the sophomore year learn the Personal Software Process [11] as a means to build individual understanding of time management, defect management, and estimation skills. They then focus on Quality, including but not limited to testing. In the junior year Enterprise students focus on Design (human-centered and system design principles) followed by best practices in software construction, taken primarily from eXtreme Programming. In the senior year students focus on Requirements Engineering then Process and Project Management. The final aspect of integration is with soft-skill outcomes such as Communication , Teamwork , and Professionalism (see Table 1). Throughout the spine the project experiences are crafted to ensure variations on pedagogy to address these outcomes. For example, in the freshman year students receive explicit instruction in teamwork. In the senior year the emphasis is on formal documentation as a means of communication. In the junior year, students work on service learning projects of high social impact to address communication with sensitive populations in a humancentered context.

4. Literature Review and Next Steps
The newness of software engineering degree programs means there are few studies of program adoption. There are examples of program design and lessons learned [5][12][13], or reflections and recommendations on the software engineering education landscape [14][15][16][17][18]. These are worthwhile guides but do not offer examples on evaluation instruments for program adoption. The SE2004 report [4] contains a chapter on "Program Implementation and Assessment" which discusses a number of key factors in program adoption, but is geared toward accreditation and not evaluation instruments. A survey instrument is presented in [19] but is designed for comparison of a large number of programs and not for adoption challenges. Bagert & Chenoweth [22] survey graduate programs in software engineering but more as an aggregate counting exercise in knowledge areas than as an adoption evaluation approach. The 2009 Graduate Software Engineering project conducted a survey of graduate degree programs [20] and then produced a comparison report [21] of graduate programs to the GSwE2009 reference model, which includes data on program characteristics and in-depth profiles from 3 institutions. A recent study is Conry's [23] survey of accredited software engineering degree programs. Conry summarizes institutional, administrative, and curricular (knowledge area) aspects in describing the 19 accredited programs as of October 2009. Certainly program adoption measures from other engineering programs are also relevant, though software engineering programs are unique due to the forces discussed in section 1. Our next steps for the Enterprise-as-project-spine involve defining measures for adoption impact, and determining how this concept fits with established patterns for curricular maps in software engineering programs. We plan to use quantitative and qualitative instruments to evaluate adoption. Quantitative data, such as program size, institution type, faculty and student backgrounds, can be collected via available resources (departmental archives or online) and direct surveys. Qualitative data can be collected through survey instruments and interviews of all stakeholders (faculty participants, administrators, and advisors). Different instruments may be used at different times to evaluate "in-stream" attitudes versus post-adoption reflections. Defining and validating these instruments is a significant area of work going forward. The SEEK, SWEBOK, and CS2013 guides define taxonomies of knowledge in software engineering. Taxonomies are useful and the sign of an emerging discipline. We

have done initial mappings of Enterprise modules to SEEK and CS2013 content areas, and plan to elaborate on these mappings. Specifically, we intend to produce CS2013 course exemplars. Further, the SE2004 report includes a section on program curricular patterns, and we will propose new patterns based on the project spine concept, which we hope will be timely in light of the in-process revisions to the SE2004 [24].

References
[1] National Academy of Engineering. Educating the Engineer of 2020: Adapting Engineering Education to the New Century. The National Academies Press, Washington D.C., 2005. [2] Institute for Electrical and Electronic Engineers Computer Society, Guide to the Software Engineering Body of Knowledge (SWEBOK). Los Alamitos, CA, 2004. [3] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society Joint Task Force (2012). Computer Science Curricula 2013 (Ironman draft). Available at http://ai.stanford.edu/users/sahami/CS2013, last accessed January 9, 2013. [4] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society. Software Engineering 2004 Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering. Joint Task Force on Computing Curricula, 2004. [5] Shepard, T. "An Efficient Set of Software Degree Programs for One Domain." In Proceedings of the International Conference on Software Engineering (ICSE) 2001. [6] Sheppard, S.D., Macatangay, K., Colby, A., and Sullivan. W.M. Educating Engineers: Designing for the Future of the Field, Jossey-Bass, San Francisco, 2008. [7] Morrell, D., Roberts, C., Grondin, B. Kuo, C.Y., Hinks, R., Danielson, S., and Henderson, M. "A Flexible Curriculum for a Multi-disciplinary Undergraduate Engineering Degree." Proceedings of the Frontiers in Education Conference 2005. [8] Gary, K. "The Software Enterprise: Practicing Best Practices in Software Engineering Education", The International Journal of Engineering Education Special Issue on Trends in Software Engineering Education, Volume 24, Number 4, July 2008, pp. 705-716. [9] Gary, K., "The Software Enterprise: Preparing Industry-ready Software Engineers" Software Engineering: Effective Teaching and Learning Approaches, Ellis, H., Demurjian, S., and Naveda, J.F., (eds.), Idea Group Publishing. October 2008. [10] Kolb Experiential Learning: Experience as the Source of Learning and Development, Prentice-Hall, N.J. 1984. [11] Humphrey, W.S. Introduction to the Personal Software Process , Addison-Wesley, Boston, 1997. [12] Lutz, M. and Naveda, J.F. "The Road Less Traveled: A Baccalaureate Degree in Software Engineering." Proceedings of the ACM Conference Special Interest Group on Computer Science Education (SIGCSE), 1997. [13] Frezza, S.T., Tang, M., and Brinkman, B.J. Creating an Accreditable Software Engineering Bachelor's Program. IEEE Software November/December 2006. [14] Hilburn, T.B., Hislop, G., Bagert, D.J., Lutz, M., Mengel, S. and McCracken, M. "Guidance for the development of software engineering education programs." The Journal of Systems and Software, 49(1999):163-169. 1999. [15] Ghezzi, C. and Mandrioli. "The Challenges of Software Engineering Education." In Proceedings of the International Conference on Software Engineering (ICSE) 2006. [16] Lethbridge, T., Diaz-Herrera, J., LeBlanc, R.J., and Thompson, J.B. "Improving software practice through education: Challenges and future trends." Proceedings of the Future of Software Engineering Conference, 2007. [17] Shaw, M. (2000). Software Engineering Education: A Roadmap. Proceedings of the 2007 Conference on the Future of Software Engineering, Limerick Ireland, 2000. [18] Mead, N. (2009). Software Engineering Education: How far We've Come and How far We Have to Go. Proceedings of the 21st Conference on Software Engineering Education and Training (CSEET 2009). 2009. [19] Modesitt, K., Bagert, D.J., and Werth, L. "Academic Software Engineering: What is it and What Could it be? Results of the First International Survey for SE Programs." Proceedings of the International Conference on Software Engineering (ICSE) 2001. [20] Pyster, A., Turner, R., Devanandham, H., Lasfer, K., Bernstein, L. "The Current State of Software Engineering Masters Degree Programs", The Conference on Software Engineering Education & Training (CSEET), 2008. [21] Frailey, D., Ardis, M., Hutchison, N. (eds). Comparisons of GSwE2009 to Current Master's Programs in Software Engineering, v1.0. Available at http://gswe2009.org, last accessed January 24, 2013. [22] Bagert, D.J. & Chenoweth, S.V. "Future Growth of Software Engineering Baccalaureate Programs in the United States", Proceedings of the American Society for Engineering Education Conference. Portland, OR, 2005. [23] Conry, S. Software Engineering: Where do curricula stand today? Proceedings of the National Conference of the American Society for Engineering Education, Louisville, KY, 2010. [24] Hislop, G., Ardis, M., Budgen, D., Sebern, M.J., Offut, J., & Visser, W. (2013). "Revision of the SE2004 Curriculum Model." Panel at the ACM Conference of the Special Interest Group on Computer Science Education (SIGCSE), Denver, CO, 2013.

The Software Enterprise is a pedagogical model combining traditional lecture with project-based learning. The Enterprise model leads students through a modular series of lessons that combine foundational concepts with skills-based competencies. In this tutorial, software engineering educators in higher education or industry will learn the methodology, get exposed to assessment techniques such as e-portfolios and concept maps, and become familiarized with the open resources available to educators that adopt the pedagogy. This tutorial should be of interest to any educator interested in making their project-based courses more engaging and more relevant to students needing to be ready to practice the profession from the first day they exit the Enterprise environment.The maritime oil tanker routing and scheduling problem is known to the literature since before 1950. In the presented problem, oil tankers transport crude oil from supply points to demand locations around the globe. The objective is to find ship routes, load sizes, as well as port arrival and departure times, in a way that minimizes transportation costs. We introduce a path flow model where paths are ship routes. Continuous variables distribute the cargo between the different routes. Multiple products are transported by a heterogeneous fleet of tankers. Pickup and delivery requirements are not paired to cargos beforehand and arbitrary split of amounts is allowed. Small realistic test instances can be solved with route pre-generation for this model. The results indicate possible simplifications and stimulate further research.n this paper we present a detailed analysis of the design and implementation of an educational game targeting young women entrepreneurs by a predominantly male team. During the process, we arrived at assumptions based on intrinsic and extrinsic influences that effected the design of the game. After creating a prototype, the game was provided to the target audience during a usability test. Our observations reveal that even after following a rigorous and agile development model that included stakeholders at several time frames, we were not successful in delivering the desired experience to our target audience. We conclude by presenting a strategy for changing the agile development model to be inclusive of the target audience.An Empirical Study of Cycle Toggling Based Laplacian Solvers
Kevin Deweese UCSB kdeweese@cs.ucsb.edu Richard Peng Georgia Tech rpeng@cc.gatech.edu John R. Gilbert UCSB gilbert@cs.ucsb.edu Hao Ran Xu MIT haoranxu510@gmail.com September 13, 2016 Gary Miller CMU glmiller@cs.cmu.edu Shen Chen Xu CMU shenchex@cs.cmu.edu

arXiv:1609.02957v1 [cs.DS] 9 Sep 2016

Abstract We study the performance of linear solvers for graph Laplacians based on the combinatorial cycle adjustment methodology proposed by [KelnerOrecchia-Sidford-Zhu STOC-13]. The approach finds a dual flow solution to this linear system through a sequence of flow adjustments along cycles. We study both data structure oriented and recursive methods for handling these adjustments. The primary difficulty faced by this approach, updating and querying long cycles, motivated us to study an important special case: instances where all cycles are formed by fundamental cycles on a length n path. Our methods demonstrate significant speedups over previous implementations, and are competitive with standard numerical routines.

Figure 1: Performance profile of cycle-toggle time. The relative performance ratio of a method is its cycle-toggle time / best cycle toggle time for a single problem. This plot shows the fraction of problems that are within a distance from this relative performance ratio. The faster 1 Introduction a method converges to 1 on this plot, the better its Much progress has been made recently toward the performance relative to the others.

development of graph Laplacian linear solvers that run in linear times polylogarithmic time [16, 17, 18, 21, 24, 9, 15]. These methods use a combination of combinatorial, randomized, and numerical methods to obtain algorithms that provably solve any graph Laplacian linear system in time faster than sorting to constant precision.
 Partially supported by NSF Grants CCF-1637523, CCF1637564, and CCF-1637566 titled: AitF: Collaborative Research: High Performance Linear System Solvers with Focus on Graph Laplacians  Partially supported by Intel Corporation

Linear solvers for graph Laplacians have a wide range of applications. They can be used to solve problems such as image denoising, finding maximum flows in a graph, and more generally solving linear programs with an underlying graph, such as, minimum cost maximum flow and graph theoretic regression problems [5, 30, 8, 11, 20, 23, 22, 7, 19]. Many of these applications stem from the following connection through optimization: solving linear systems is equivalent to minimizing 2 norms over a suitable set. Many applications can in turn be

2 1 4 3

5

(a) Original Graph

nation, or partial Cholesky factorization steps from the ultra-sparsification routines [29]. Recursively dividing the cycle set yields a recurrence of the form: T (N ) = O(N ) + 2T (N/2),
2

1 4

3

5

(b) Subgraph 1,4

(c) Subgraph 2,3,5

2 1 3

4

5

(d) Contraction of(b)

(e) Contraction of(c)

Figure 3: Illustration of graph reduction and contraction in divide-and-conquer. 5 cycles are preselected in the original graph(a) and divided into two groups, cycles (1,4) and (2,3,5). These cycles induce subgraphs (b,c) which only include edges and vertices of the relevant cycles. These subgraphs are then path contracted (d,e) to further reduce size.

which solves to T (N ) = O(N log N ). If we set the size of our preselected cycle set to O(n), then updating the entire set takes O(n log n) work, leading to a cost of O(log n) per update. Unfortunately, the divide-and-conquer scheme does not parallelize naturally: the second recursive call still depends on the outcome of the first one. Furthermore, the bottleneck of this routine's performance is the restriction and prolongation steps, which unlike multigrid can not be reused when we resample another set. A large part of the expense is that vertices and edges must be relabeled as the graph is reduced. Doing this in random order leads to random access of vertex and edge labels. We try to optimize this by either compressing the memory of the graph storage, or by reordering the updates within each batch. In the case that the tree is just a path, much of the vertex and edge labeling can be done implicitly, reducing the overhead. 4 Heavy Path Graphs

can further reduce the size of the graph by path contraction, condensing two edges if they are only updated when the other is updated. An example of this reduction and contraction is shown in Figure 3. This process results in several smaller graphs, where the cycles are updated, before pushing the cycle update information back up the recursive subgraph hierarchy. As this process resembles the recursive subgraph hierarchy of multigrid methods, we borrow the terms restriction and prolongation to describe the transfer of flow information up and down the hierarchy. This process is more formally captured in the following lemma.

Here we introduce a class of model problems that we will use to test and analyze different cycletoggling approaches. These graphs are constructed by adding edges between vertices on a path graph. Edge resistances are selected so that the low-stretch spanning tree of the resulting graph is always the underlying path. As a consequence the edges on the path have larger edge weights than the off-path edges, so we refer to this class of graphs as heavy path graphs. An example of such a graph is shown in Figure 4.

Lemma 3.1. Given a tree on n vertices, and N cycle updates, we can form a tree on 3N vertices, perform the corresponding cycle updates on them, Figure 4: An example of a heavy path graph. The solid path edges are the low-stretch spanning tree of and transfer the state back to the original graph. the graph. Furthermore, both the reduction and prolongation Our interest in these problems does not come steps take O(n) time. from any real world application. Instead we believe This procedure is identical to the greedy elimi- these are natural models to consider when studying

KOSZ and other cycle-toggling algorithms. We believe that this model can be tuned to have various stretch properties along with spectral and graph separator properties, though we do not explore that in this paper. Furthermore they allow us to explore very fundamental questions about data structures and cycle-toggling implementations. This model simplifies many of the implementation issues associated with dynamic trees, as the paths are easier to handle than more general tree layouts. Specifically, we can use a static, perfectly balanced binary tree for the path. This likely has the least data structure overhead as the optimum separator of an interval is implicitly the middle. Furthermore, this allows us to store the tree in heap order, which means the tree paths can be mapped to a subinterval using bit operations, and the downward/upward propagations can be performed iteratively. 4.1 Example Models There are many possible subclasses that belong to the heavy path graph model. We introduce several subclasses here for experimentation. (1) Fixed Cycle Length-1k: These graphs are composed of a tree path with random resistances between 1 and 10,000, combined with off-tree edges between every pair (i, i + 1000), e.g. an edge between vertices 1 and 1000, between vertices 2 and 1001, and so on. (2) Fixed Cycle Length-2: These graphs are composed of a tree path with random resistances between 1 and 10,000, combined with off-tree edges between every pair (i, i + 2), e.g. an edge between vertices 1 and 3, between vertices 2 and 4, and so on. (3) Random Cycle Length: These graphs are composed of a tree path with random resistances between 1 and 1000, combined with n randomly selected off-tree edges, where n is the number of vertices. (4) 2D Mesh: These graphs embed a tree path in a 2D mesh. The tree path resistances are chosen randomly between 1 and 1000. (5) 3D Mesh, Uniform Stretch: These graphs are similar to (4) but with a 3D mesh.

We then consider two different ways of setting resistances on the off-tree edges on all of the models above. 1. Uniform Stretch Resistances of off-tree edges are chosen so that stretch is 1 for every cycle. 2. Exponential Stretch Resistances of off-tree edges are chosen so that cycles have stretch sampled from an exponential distribution. 5 Experiments

5.1 Experimental Design We now describe empirical evaluations of the cycle-toggling implementations from Section 3 on the class of graphs described in Section 4. As we only experiment on these path models, we can use cycle-toggling methods that will only work on a path, but we also employ their more general versions that will work on any graph. The four cycle-toggling implementations are as follows: 1. BST-based data structure for general graphs 2. Path-only BST decomposition 3. Recursive divide-and-conquer for general graphs 4. Path-only recursive divide-and-conquer Additionally we implement a preconditioned conjugate gradient with diagonal scaling to compare against the cycle-toggling methods. We implemented all of these in C++ and also have a Python/Cython implementation of the general recursive method. All algorithm implementations, graph generators, and test results for this paper can be found at https://github.com/sxu/cycleToggling. We also experimented with Hoske et al.'s [14] implementation of cycle-toggling. We use all of the generators described in Section 4.1 to create different heavy path graphs with a varying total stretch. We use vertex sizes of 5 × 104 , 105 , 5 × 105 , and 106 . For the fixed cycle length generators, we set hop = 1000, and for the random cycle length generators, we set the number of offtree edges to 2n. To get an idea for the various stretch properties of these graphs, we list the total stretch for size 106 in Table 1. We also generate right hand side vectors b in two different ways to obtain both local and global

Cycles log -1

Fixed Length-1k Fixed Length-2 Random Length 2D Mesh 3D Mesh

Uniform 1.01e6 2.00e6 2.00e6 2.00e6 3.82e6

Exponential 1.12e6 1.04e7 1.30e7 1.08e7 2.27e7

108 107 106 105 104 103 3 10

Table 1: Total stretch for all graph models of size 106 . For each of the model problems in 4.1, this table shows the total stretch of cycles formed by adding edges to the underlying path. The models were generated with weights to create cycles with uniform stretch (all cycles with stretch 1), and exponential stretch(cycles with stretch chosen from an exponential distribution).

104

Total Stretch

105

106

107

108

behaviors. 1. Random: Randomly select x and form b = Lx ,

Figure 5: KOSZ asymptotic dependence on tree stretch. The number of toggles required by KOSZ is shown as a function of tree stretch. The reasonable slope indicates a lack of large hidden constants in KOSZ complexity.

and every graph, the relative performance ratio 2. (-1,1): Pick b to route 1 unit of electrical flow is the method's average cycle-toggle time divided from the left endpoint of the path to the right by the lowest average cycle-toggle time over all endpoint. methods. Then to capture how a method fares Experiments were performed on Mirasol, a across the entire problem set, the performance shared memory machine at Georgia Tech, with 80 profile shows the fraction of test problems (on the Intel(R) Xeon(R) E7-8870 processors at 2.40GHz. y-axis) that are within a distance (on the x-axis) Problems were solved to a residual tolerance of from the relative performance ratio. This plot contains all the different model problems at every 10-5 . problem size tested. Weak scaling experiments, measuring cycle5.2 Experimental Results We first examine the asymptotic behavior of the cycle-toggling meth- toggle performance as graph size increases, are useods on all the test graphs. Figure 5 shows the num- ful for predicting performance on larger problems. ber of cycles required for convergence as a function The scaling behavior was relatively similar across of total stretch. This figure only involves solves the model problems so we only show one example using the 0-1 right hand side as this was always a in Figure 6 for the 3D Unweighted Mesh with exponential stretch. more difficult case. We examine how much time the recursive We omit results from the Hoske et al. implementation because we found its performance to be method spends restricting and prolonging flow in slower by a factor of 50 than our cycle-toggling im- the recursive hierarchy, and how much time is plementations. Their initialization costs are much spent doing cycle-toggles in Figure 7. Results are higher than solve costs, making it prohibitively ex- shown for the FixedLength-1k model with a slightly pensive to run on all of the test graphs in our set. wider range of problem size than the other experTo visualize the comparison of cycle-toggling iments. The solve time in this plot includes the implementations on all the different test graphs, we sum of the other operation timings, along with utilize a performance profile plot shown in Figure 1. memory allocation. We did this profiling with A performance profile [12] calculates, for some our Python/Cython implementation, but we beperformance metric, the relative performance ratio lieve the C++ performance is comparable. Figure 8 shows BST-based cycle-toggle timing between each solver and the best solver on every problem instance. In our case the metric of interest results relative to PCG results. Points below the is the average cycle-toggle time, so for each method line indicate cycle-toggling was faster, while points

Path-only BST Decomposition BST-Based

Path-only Recursive Recursive

Restrict Prolong

Update Solve

10-5

10-5

Average Toggle Time(s)

Average Toggle Time(s)

10-6

10-6

10-7

10-7

10-8 104

Path Length

105

106

103

104

Path Length

105

106

107

Figure 6: Weak scaling of cycle-toggle performance of all methods on unweighted 3D mesh model problems with exponential stretch. Average cycle-toggle time is shown as a function of problem size where an upward slope indicates decreased performance with larger problem size.

Figure 7: Weak scaling of cycle-toggle performance for the recursive solver on FixedLength-1k model problems. Average cycle-toggle time is shown along with its most expensive sub-components: restriction, solve, and prolongation. Upward slopes indicate decreasing performance with problem size.
Uniform Stretch FixedLength-2 2D Mesh Random Exp Stretch FixedLength-1k 3D Mesh

Toggle Time(s)

above the line are slower. This plot only includes size 106 problems using the 0-1 right hand side. A random right hand side plot is omitted for space as these problems were much easier for both solvers, though slightly relatively easier for PCG. 5.3 Experimental Analysis In Figure 5 the cycle-toggling methods' asymptotic dependence on tree stretch is near constant with a slope close to 1. Note that this plot would be linear even without the log axes. Concerning KOSZ practicality, it is highly important to see that there is not a large slope, which would indicate a large hidden constant in the KOSZ cost complexity. This plot tells us that with a combination of low-stretch trees and fast cycle update methods, dual space algorithms have potential. This figure also helps illustrate the range of problems we are using for these experiments. The stretch and resulting cycle cost both vary between four to five orders of magnitude. The performance profile in Figure 1 indicates that the data structure based cycle-toggling methods performed the best using our implementations. For the path-only BST decomposition, the fraction of problems is already at 1 for a relative performance distance of 1, meaning that this was always the fastest. The path-only recursive method was

105 104 103 102 101 100 0 10

101

102

PCG Time(s)

103

104

105

Figure 8: Comparison of BST-based data structure cycle-toggling to PCG by graph type. Points under the line indicate cycle-toggling method outperformed PCG.

slower, but still typically performed better than the general implementations, being half as fast as the path-only BST method on 60% of the problems. Comparing the two general implementations, the tree data structure is within a factor 4 of the best on 80% of the problems, whereas the recursive method is only within a factor of 4 on 40% of the problems. A distance of 10 indicates performance within the same order of magnitude, which the general recursive method achieved on 80% of

the problems, indicating that these methods are competitive with one another. The weak scaling experiments shown in Figure 6 do indicate a decrease in cycle-toggle performance as graph size increases. However, this plot is fairly optimistic, the largest performance decrease is about 2.5× as the graph size increases two orders of magnitude. The non steady plot for the general recursive solver probably indicates that the batch sizes were not scaled appropriately. Again, this plot is only for one of the graph models, but most of them looked very similar to this. Figure 7 helps identify the performance bottlenecks of the recursive method. The actual time spent updating cycles is less than the restriction and prolongation time. The restriction time is by far the most expensive, as it also includes time for relabeling edges and vertices. The scaling of this plot shows a stable update cost, with increasing restriction and prolongation costs. This method was designed to keep the update costs stable while increasing problem size, which seems to be case. Unfortunately the restriction and prolongation overhead costs are large and growing with problem size. Still, these operations are not highly optimized, and we wonder if we can borrow techniques from the multigrid community to speed them up. The PCG experiments in Figure 8 indicate that cycle-toggling can outperform PCG on these heavy path models, using the 0-1 right hand side. This class of problems had a wider performance gap for PCG than for the cycle-toggling routines, by about an order of magnitude. Furthermore, the graph property that causes difficulty for the solvers is different in each case; cycle-toggling has trouble on the graphs with exponential stretch, while PCG has difficulty with the fixed cycle length problems (FixedLength-2 with uniform stretch even failed). These results suggest that heavy path graphs are a good direction to explore while searching for problems which could benefit from cycle-toggling methods. 6 Discussion and Conclusion

model, we experimented on problems that are are conceptually simple, but provide a range of solve behavior through varying graph structure and stretch. The recursive cycle-toggling was not as fast as the data structure approach, but was still competitive, being in the same order of magnitude on most problems. method to general graphs, exhibited competitive behaviors. Also both methods scaled reasonably with problem size. While these experiments are a good start, there are several directions we hope to continue this work. The recursive update approach is outperformed by the BST-based data structure approach in timing experiments. We hope to complement these results with floating point operation measurements. We don't claim to have optimized the graph contraction, flow restriction/prolongation, or cycle updates. Measuring the number of operations the recursive solver spends on these would help indicate fundamental performance. The heavy path graphs are a great model problem for seeing the effect path resistances have on solver behavior. They also allow us set aside the issue of finding a low stretch spanning tree to focus instead on the cost per cycle update. We plan to continue modifying these path resistances and initial vertex demands to find interesting test cases. However, for these methods to be useful in practice we must extend them to more general classes of graphs. Dual cycle-toggling Laplacian solvers have until now been considered mainly in the realm of theory. Our comparisons of these methods to PCG indicate that there are problems for which the dual methods can be useful. In the future, we plan to combine primal and dual methods, trying to get the best of both worlds. References

We studied two approaches for implementing cycletoggling based solvers, data structures and recursive divide-and-conquer. Using the heavy path

[1] O. Axelsson, Iterative solution methods, Cambridge University Press, New York, NY, 1994. [2] M. A. Bender, E. D. Demaine, and M. FarachColton, Cache-oblivious B-trees, IEEE FOCS, Redondo Beach, CA, 2000, pp. 399­409. [3] E. G. Boman, K. Deweese, and J. R. Gilbert, Evaluating the dual randomized Kaczmarz Laplacian

[4]

[5]

[6] [7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

linear solver, Informatica, 40(1) (2016), pp. 95­ 107. E. G. Boman, K. Deweese, and J. R. Gilbert, An empirical comparison of graph Laplacian solvers, SIAM ALENEX, Arlington, VA, 2016, pp. 174­ 188. E. G. Boman, B. Hendrickson, and S. Vavasis, Solving elliptic finite element systems in nearlinear time with support preconditioners, SIAM J. on Numerical Anal., 46(6) (2008), pp. 3264­3284. W. L. Briggs, V. E. Henson, and S. F. McCormick, A multigrid tutorial, SIAM, 2000. M. B. Cohen, B. T. Fasy, G. L. Miller, A. Nayyeri, R. Peng, and N. Walkington, Solving 1-Laplacians of convex simplicial complexes in nearly linear time: collapsing and expanding a topological ball, SIAM SODA, Portland, OR, 2014, pp. 204­216. P. Christiano, J. A. Kelner, A. Madry, D. A. Spielman, and S.- H. Teng, Electrical flows, Laplacian systems, and faster approximation of maximum flow in undirected graphs, ACM STOC, San Jose, CA, 2011, pp. 273­282. M. B. Cohen, R. Kyng, G. L. Miller, J. W. Pachocki, R. Peng, A. Rao, and S. C. Xu, Solving SDD linear systems in nearly mlog1/2 n time, ACM STOC, San Jose, CA, 2011, pp. 343­352. T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to algorithms, MIT Press and McGraw-Hill, 2009. H. H. Chen, A. Madry, G. L. Miller, and R. Peng, Runtime guarantees for regression problems, ITCS, Berkeley, CA, 2013, pp. 269­282. E. D. Dolan and J. J. Mor´ e, Benchmarking optimization software with performance profiles, Mathematical Programming, 91(2) (2002), pp. 201­213. P. G. Doyle and J. L. Snell, Random walks and electric networks, Mathematical Association of America, 1984. D. Hoske, D. Lukarski, H. Meyerhenke, and M. Wegner, Is nearly-linear time the same in theory and practice? A case study with a combinatorial Laplacian solver, SEA, Paris, FRA, 2015, pp. 205­ 218. R. Kyng, Y. T. Lee, R. Peng, S. Sachdeva, and D. A. Spielman, Sparsified Cholesky and multigrid solvers for connection Laplacians, Computing Research Repository, 2015, http://arxiv.org/abs/1512.01892. I. Koutis, G. L. Miller, and R. Peng, Approaching optimality for solving SDD systems, SIAM J. on Comp., 43(3) (2014), pp. 337­354. I. Koutis, G. L. Miller, and R. Peng A Nearly-m

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26] [27]

[28]

[29]

[30]

log n time solver for SDD linear systems, IEEE FOCS, Palm Springs, CA, 2011, pp. 590­598. J. A. Kelner, L. Orecchia, A. Sidford, and Z. A. Zhu, A simple, combinatorial algorithm for solving SDD systems in nearly-linear time, ACM STOC, Palo Alto, CA, 2013, pp. 911­920. R. Kyng, A. Rao, and S. Sachdeva, Fast, provable algorithms for isotonic regression in all p -norms, NIPS, Montreal, QC, 2015, pp. 2701­2709. Y. T. Lee, S. Rao, and N. Srivastava, A new approach to computing maximum flows using electrical flows, ACM STOC, Palo Alta, CA, 2013, pp. 755­764. Y. T. Lee and A. Sidford, Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems, IEEE FOCS, Berkeley, CA, 2013, pp. 147­156. Y. T. Lee and A. Sidford, Path finding methods for linear  programming: solving linear programs ~ ( rank ) iterations and faster algorithms for in O maximum Flow, IEEE FOCS, Philadelphia, PA, USA, 2014, pp. 424­433. A. Madry, Navigating central path with electrical flows: from flows to matchings, and back, IEEE FOCS, Berkeley, CA, 2013. pp 253­262. R. Peng and D. A. Spielman, An efficient parallel solver for SDD linear systems, ACM STOC, New York, NY, USA, 2014, pp. 333­342. M. Reid-Miller, G. L. Miller, and F. Modugno, List ranking and parallel tree contraction in J. H. Reif Synthesis of parallel algorithms, Morgan Kaufmann, San Francisco, CA, 1993, pp. 115­194. Y. Saad, Iterative methods for sparse linear systems, SIAM, 2003. D. D. Sleator and R. E. Tarjan, A data structure for dynamic trees, J. Comp. Syst. Sci., 26(3) (1983), pp. 362­391. D. A. Spielman and N. Srivastava, Graph sparsification by effective resistances, SIAM J. on Comp., 40(6) 2011, pp. 1913­1926. D. A. Spielman and S.- H. Teng, Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems, SIAM J. on Matrix Anal. and Appl., 35(3) 2014, pp. 835­885. D. Tolliver and G. L. Miller, Graph Partitioning by Spectral Rounding: Applications in Image Segmentation and Clustering, IEEE CVPR, New York, NY, 2006, pp. 1053­1060.

arXiv:1408.4351v1 [physics.comp-ph] 19 Aug 2014

A Parallel Multi-Domain Solution Methodology Applied to Nonlinear Thermal Transport Problems in Nuclear Fuel Pins$
Bobby Philipa,, Mark A. Berrilla , Srikanth Allua , Steven P. Hamiltona , Rahul S. Sampatha , Kevin T. Clarnoa , Gary A. Diltsb
Oak Ridge National Laboratory One Bethel Valley Road, Oak Ridge, TN 37831 b Los Alamos National Laboratory P.O. Box 1663, Los Alamos NM 87545
a

Abstract This paper describes an efficient and nonlinearly consistent parallel solution methodology for solving coupled nonlinear thermal transport problems that occur in nuclear reactor applications over hundreds of individual 3D physical subdomains. Efficiency is obtained by leveraging knowledge of the physical domains, the physics on individual domains, and the couplings between them for preconditioning within a Jacobian Free Newton Krylov method. Details of the computational infrastructure that enabled this work, namely the open source Advanced Multi-Physics (AMP) package developed by the authors is described. Details of verification and validation experiments, and parallel performance analysis in weak and strong scaling studies demonstrating the achieved efficiency of the algorithm are presented. Furthermore, numerical experiments demonstrate that the preconditioner developed is independent of the number of fuel subdomains in a fuel rod, which is particularly important when simulating different types of fuel rods. Fi$ Notice: This manuscript has been authored by UT-Battelle, LLC, under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes.  Corresponding author: Bobby Philip (Email: philipb@ornl.gov)

Preprint submitted to Journal of Computational Physics

August 20, 2014

nally, we demonstrate the power of the coupling methodology by considering problems with couplings between surface and volume physics and coupling of nonlinear thermal transport in fuel rods to an external radiation transport code. Keywords: Inexact Newton, Jacobian Free Newton Krylov, Krylov Subspace Method, Domain Decomposition, Preconditioning, Iterative Method, Parallel Algorithm 2010 MSC: 49M15, 65F08, 65F10, 65N55, 65Y05, 68W10 1. Introduction Many real world engineering problems involve multiple coupled nonlinear physical processes that occur both within and across several interacting physical domains. Robust, accurate, and efficient three dimensional simulations for some of these complex problems pose significant challenges that require a combination of powerful numerical algorithms, efficient parallel implementations, and massive computing resources to tackle. These challenges include developing the numerical methods and the parallel software infrastructure for coupling physical phenomena that occur on the surface and within the interior of physical domains, coupling structured and unstructured mesh calculations, coupling models with different discretizations, and using tightly coupled solution methods to solve certain coupled physics problems and loosely coupled approaches for others. Developing such simulation capabilities is a nontrivial task. In this article, we will focus primarily on one such complex application where all of the features outlined above are present: thermal transport in nuclear fuel rods. However, we will also devote some effort to describing the parallel code infrastructure that was developed to provide the necessary meshing, discretization, linear algebra, linear and nonlinear solvers, physics modules (conservation laws and constitutive models), material property databases, and parallelization mechanisms for simulating this application in hopes that it will be beneficial to the broader scientific community. A nuclear fuel assembly consists of several hundred nuclear fuel rods (shown in Figure 1) bound together by spacer grids. While some of the rod locations are reserved for instrumentation and safety, most of the rods contain nuclear fuel. Each individual nuclear fuel rod in turn consists of several hundred nearly cylindrical nuclear fuel pellets (each with a height 2

Figure 1: Fuel assembly containing nuclear fuel rods that are filled with fuel pellets [1] to diameter ratio of approximately one) stacked one on top of another to form a long column enclosed within a metal tube called the clad. Heat is generated within the pellets by nuclear fission and is distributed within the pellets and clad via a diffusive process. There is thermal contact (modeled as a convective process) between neighboring pellets and between the pellets and the clad. Each fuel rod is cooled with water flowing axially up the outer surface of the clad. Modeling the heat transfer, along with other physics, leads to a very highaspect ratio problem with many inter-dependent domains. Traditional nuclear fuel simulation eliminates the computational challenge by approximating the heat transfer as entirely radial and neglecting the axial and azimuthal components, which are only coupled through the coolant temperature and other simplified physics [2, 3]. Recent efforts to develop advanced modeling and simulation tools for nuclear fuel rods [4, 5], which include simulating full 3

three-dimensional fuel rods with resolved pellets, have relied upon standard solution and preconditioning strategies that do not necessarily take advantage of the physics and geometry of the problem. However, this manuscript does not address the challenges of structural dynamics and the associated feedback on heat transfer. With respect to specific work related to heat transfer within nuclear fuel rods, there are several existing efforts to develop parallel codes that model three-dimensional heat transfer within nuclear fuel rods, including PLEIADES/ALCYONE [4, 6], MOOSE/Bison [4], and BACO [7]. These codes are all focused on the integration of the many physics required for modeling nuclear fuel performance in steady-state and transients to improve the underlying material science, including fracture/contact mechanics, fission gas generation and release, and corrosion chemistry. We have developed an efficient scalable parallel simulation framework for solving such multi-domain, multi-physics problems and have used it to solve the specific nuclear fuel problem described above. Within our particular application a nonlinearly consistent Jacobian Free Newton Krylov (JFNK) method is used (though the ability to use alternative solution methods also exists) across all the domains for each fuel rod. Physics-based preconditioning is used to accelerate the solution process and the multi-domain (pellets and clad) aspect of the problem is leveraged in developing methods that minimize communication as well as avoid the formation of full matrices over the whole domain. In the next section, we present a mathematical description of the problem under consideration. Section 3 will describe the finite element discretization of the models in Section 2. The algorithms used to solve the resulting nonlinear system of equations are described in Section 4. The computational framework that was used in this work is briefly described in Section 5. Section 6 reports on numerical experiments performed to verify and validate our code and test its parallel scalability. Section 7 provides details on coupling to reduced order flow models, coupling to oxide growth models on the exterior clad, and parallel full assembly simulations that couple thermal transport components on unstructured meshes with a structured mesh radiation transport code. The paper ends with a few concluding remarks.

4

2. Model A 3D fuel rod domain, , is modeled as consisting of the union of N pellet 3 C 3 subdomains, P i  R , i = 1, 2, . . . , N , and a clad subdomain   R , i.e, P C the global domain  = N i=1 i   . The number and geometric complexity of fuel pellets in fuel rods can vary significantly; from simple cylinders to the complex pellet geometries shown later in this manuscript and from a few pellets in an experimental rod to more than 400 pellets in a commerical nuclear fuel rod. In our numerical experiments N will be varied between 1 and 360 though there is no fundamental limitation on the number of pellet P domains. Here, P 1 will denote the domain of the lowest pellet and N will denote the topmost pellet domain. The surfaces of the pellet and clad C domains will be denoted by P i , i = 1, 2, . . . , N , and  , respectively. From now on, where there is no danger of confusion the superscript `P' will be dropped for the pellet subdomains. We will assume that the subdomains are disjoint. Only adjacent pellet subdomains are assumed to touch, i.e., i  i-1  i,i-1 =  for i = 2, . . . N , and i  j =  for j = i + 1, i - 1. In this paper we will only consider the case where no pellets are in mechanical contact with the clad or flow subdomains. We will first describe the models at the level of individual pellets, clad, and coolant domains before considering the full coupled multi-domain model. 2.1. Pellet Models: The temperature field, Ti , in each fuel pellet domain i is modeled by a nonlinear thermal diffusion equation, -  · ki (Ti , x)Ti (x) = fi (x), for x  i , (2.1)

where ki is a scalar nonlinear thermal conductivity and fi  L2 (i ) is a nonzero thermal source due to heat generated from nuclear fission in each pellet. In stand-alone applications, the radial shape of fi is usually approximated with a local exponential in the radial dimension and a globally low-order (quadratic or cosine) model in the axial dimension. ki and fi vary spatially within each pellet and can potentially vary in functional form from pellet to pellet to account for differences in materials and fission processes that do occur in practice. For example, insulator pellets with natural, rather than enriched, uranium are often introduced at the top and bottom of the pellet stack to reduce axial power peaking. 5

There is contact/gap resistance between two adjacent pellets and for pellet i this is modeled by Robin boundary conditions of the form
m ki (Ti )Ti · ni + hi,i-1 (Ti , Tim -1 )(Ti - Ti-1 ) = 0 on i,i-1 , m ki (Ti )Ti · ni + hi,i+1 (Ti , Tim +1 )(Ti - Ti+1 ) = 0 on i,i+1 .

(2.2) (2.3)

Here ni is the outward facing unit normal on the surface of pellet i and Tim -1 is the surface temperature field for pellet (i - 1) on i-1  i,i-1 interpolated to the surface i  i,i-1 of pellet i. The flux between adjacent pellets is assumed to be continuous while the temperature field is assumed to be discontinuous. Heat transfer between the pellets and clad is by radiative, conductive and convective processes through the gap region. For each pellet this is modeled using a Robin boundary condition ki (Ti )Ti · ni + hi,c (Ti , Tcm )(Ti - Tcm ) = 0 on i,c (2.4)

where Tcm is the surface temperature field for the cladding projected onto the surface, i,c , of pellet i and hi,c is an effective heat transfer coefficient that can be modeled to account for a wide variety of physical phenomena. In this manuscript, we limit discussion of the heat transfer coefficient to the nonlinearities associated with the dependence on the clad and fuel temperatures in the geometric orientation. This in effect results in a nonlinear coupling between all pellets and the clad through the boundary conditions. Zero Neumann boundary conditions ki (Ti )Ti · ni = 0 on i,n are imposed on all remaining surface boundary regions of each pellet. 2.2. Clad Model: The temperature field, Tc , for the clad domain c is also modeled by a nonlinear thermal diffusion equation, -  · kc (Tc , x)Tc (x) = 0 for x  c (2.6) (2.5)

with a scalar nonlinear thermal conductivity (kc ) and a zero right hand side since heat is not generated within the clad materials. Heat transfer from the clad outer surface to the coolant is modeled by a Robin boundary condition
m m ) = 0 on c,f kc (Tc )Tc · nf + hc,f (Tc , Tf )(Tc - Tf

(2.7)

6

where nf is an outward facing unit normal from the clad surface into the m flow region, Tf denotes the interpolated temperature from the fluid flow region, and hc,f is the effective heat transfer coefficient. The temperature of the flow can be described either by a constant fixed temperature or through an independent or coupled flow model. For most of this work we choose to couple a flow model that solves a form of the fluid equations as described in section 7 and Appendix A. Similarly, heat transfer between the clad inner surface and the gap region between the pellets and clad is modeled by a Robin boundary condition
m m kc (Tc )Tc · ng + hc,g (Tc , T1 , . . . , TN )(Tc - Tg ) = 0 on c,g

(2.8)

Here ng is the outward facing unit normal from the inner clad surface into the pellet-clad gap, Tim is the temperature on the outer surface of the pellet projected onto the inner surface of the cladding for each pellet i, and hc,g is the clad-side heat transfer coefficient that is corresponds to the hi,c to conserve energy across the gap. In addition zero Neumann boundary conditions are imposed on the top and bottom of the clad cylinder. 2.3. Weak Formulation: Given fi (x)  L2 (i ), for each pellet subproblem we seek a solution to Eqns (2.1)-(2.5) in the trial space of functions ¯ i , T  H 1 (i ), T satisfies Eqns (2.2)-(2.5) on i }. (2.9) Vi = {T (x)| x   The Galerkin weak formulation for each pellet subproblem is: Find Ti  Vi such that v  Vi -
i

v  · ki (Ti )Ti di =
i

fi v di .

(2.10)

Integration by parts and the divergence theorem yield ki (Ti )Ti · v di -
i i

vki (Ti )Ti · ni di =
i

fi v di

(2.11)

7

Using Eqns (2.2)-(2.5) the boundary integral on the left hand side may be simplified to give -
i

vki (Ti )Ti · ni di = vhi,i-1 (Ti - Tim -1 ) di + vhi,i+1 (Ti - Tim +1 ) di +
i,c m vhi,c (Ti - Tic ) di

i,i-1

i,i+1

Let u, v i =
i

uv di and (u, v ) =


(2.12) uv d denote the standard inner prod-

ucts over the domain i and a boundary segment  respectively. Then the Galerkin subproblems for the pellets, i = 1, 2, . . . , N , can be written as: Pellet Sub-Problems: Find Ti  Vi such that v  Vi ki (Ti )Ti , v i + (hi,i-1 Ti , v )i,i-1 + (hi,i+1 Ti , v )i,i+1 + (hi,c Ti , v )i,c (2.13) m m = fi , v i + (hi,i-1 Tim -1 , v )i,i-1 + (hi,i+1 Ti+1 , v )i,i+1 + (hi,c Tc , v )i,c Similarly, the Galerkin weak formulation for the clad subdomain may be stated as: Clad Sub-Problem: Find Tc  Vc such that v  Vc kc (Tc )Tc , v c + (hc,g Tc , v )c,g + (hc,f Tc , v )c,f m m , v )c,f = fc , v c + (hc,g Tg , v )c,g + (hc,f Tf 3. Discretization 3.1. Domain Discretization As can be seen from Figures 2 and 3, the clad subdomain c and the pellet subdomains i are bounded, connected volumes with piecewise smooth curved boundaries. For the purposes of this paper i is in general approximated by a polyhedral domain h i during the mesh generation process. i h h is partitioned into a set, P , of non-overlapping general hexahedral elei i h h h h ments, Pi = {Pi,1 , Pi,2 , . . . , Pi,M } which are geometrically conforming, i.e.:
h ·  Pi,j = h i j =1 h h · The intersection of two elements, Pi,l and Pi,m for l = m is either empty, a single vertex, an entire edge, or an entire face of both elements. M

(2.14)

8

Figure 2: A slice of a fuel rod mesh showing the outer clad mesh and three inner pellet meshes 3.2. Problem Discretization: A discrete approximation to Ti , the solution to Eqn (2.13) over the i-th pellet subdomain is obtained by solving the problem in a finite dimensional Ni subspace Vih of Vi . Let dim(Vih ) = Ni and {h i,k }k=1 be a suitable basis for Vih . Then, letting Tih =
Ni k=1 h h h Ti,k i,k where Ti,k are unknown coefficients, the

discrete Galerkin problem for each pellet is: Discrete Pellet Sub-Problems: Find Tih = v h  Vih ki Tih , v h i + (hi,i-1 Tih , v h )i,i-1 + (hi,i+1 Tih , v h )i,i+1 + (hi,c Tih , v h )i,c h m h m h = fih , v h i + (hi,i-1 Tim -1 , v )i,i-1 + (hi,i+1 Ti+1 , v )i,i+1 + (hi,c Tc , v )i,c (3.1) h h h Similarly, by a suitable choice of Vc  Vc with span{c,1 , . . . , c,Nc } = Vch and dim(Vch ) = Nc a discrete approximation to the solution of the clad problem (2.14) can be obtained by solving:
Ni k=1 h h Ti,k i,k  Vih such that

9

(a) IFA597 Pellet Mesh

(b) IFA432 Pellet Mesh

Figure 3: (a) pellet mesh with dish and chamfer (b) standard pellet mesh Discrete Clad Sub-Problem: Find Tch = v h  Vch kc Tch , v h
c Nc k=1 h h Tc,k c,k  Vch such that

+ (hc,g Tch , v h )c,g + (hc,f Tch , v h )c,f
c m h m h + (hc,g Tg , v )c,g + (hc,f Tf , v )c,f

h h = fc ,v

(3.2)

Basis functions: The use of general hexahedral or brick elements in discretizing the pellet and clad geometries leads us to use isoparametric finite elements for problem discretization. The basis functions over each geometric element Pi,k of a partition Pi are formed based on mappings of the standard trilinear Q1 basis functions defined over the reference cube [-1, 1]3 to Pi,k . This ensures continuity across element boundaries within a partition. All of the integral terms in Eqns (3.1) and (3.2) are evaluated using a second order Gaussian quadrature. For example, the term from Eqn (3.1) corresponding to the external source term, fih , v h i , is computed by evaluating fi (x) at the quadrature nodes used by the finite element bases and using the quadrature to evaluate the integrals, i.e.
M N

fih , v h i

=
j =1 =1

w fi (x ) v h (x ) ,

(3.3)

where the j index denotes the spatial elements within the domain h i, N 10

indicates the number of quadrature nodes per spatial element, and x and w are the quadrature nodes and weights, respectively. Evaluating the source term at quadrature points rather than mesh nodes allows strict conservation of the source term to be enforced automatically. Because the source term is due to nuclear fission in the fuel pellets, the corresponding term in Eqn (3.2) representing the source in the clad is treated as zero, i.e. fch , v h c = 0.
m m h h For fixed Tim -1 , Ti+1 , and Tc , by choosing v = i,j for j = 1, . . . , Ni in Eqn (3.1) we obtain a set of Ni equations for the unknown coefficients h , k = 1, . . . , Ni denoted by: Ti,k

Fi (Tih ) = 0,

(3.4)

t h h h where Tih = (Ti, 1 , Ti,2 , . . . , Ti,Ni ) with some abuse of notation. Since the conductivity ki and the effective heat coefficients hi,i-1 , hi,i+1 , and hi,c are nonlinear functions, Eqn (3.4) represents a coupled system of nonlinear algebraic equations over each pellet subdomain coupled through nonlinear boundary conditions to the adjacent pellets and clad. For the clad, a similar methodology leads to a set of Nc nonlinear algebraic equations for the unknown h coefficients Tc,k , k = 1, . . . , Nc which we denote by:

Fc (Tch ) = 0. The full set of nonlinear equations we wish to solve is given by F(T) = 0
t

(3.5)

(3.6)

h h h where F(T) = F1 (T1 ), F2 (T2 ), . . . , FN (TN ), Fc (Tch ) is the coupled set of block nonlinear equations across all pellet and clad domains with each block Fi specified by Eqn (3.4) and Fc given by Eqn (3.5). Here, T denotes the t h h h block unknowns across all domains, T  T1 , T2 , . . . , TN , Tch with each Ti a vector of unknowns itself.

4. Solution Strategy Several nonlinear solution strategies could be used to solve the nonlinear system of algebraic equations described in Section 3. The fact that our problems often involve several hundred subproblems each discretized over a

11

separate physical domain make solution methods that do not require formation of the full Jacobian matrix attractive. In particular we choose to use a Jacobian-Free Newton Krylov (JFNK) method [8] for our nonlinear solver. The efficiency of a JFNK method when applied to a particular problem depends heavily on the preconditioner used. We will describe a physics based preconditioner that exploits the natural subdomain decomposition present in our application. A brief overview of the JFNK method is given in Section 4.1 and the construction of the preconditioner is described in Section 4.2. 4.1. Jacobian-free Newton-Krylov Methods Let T denote the exact solution to Eqn (3.6). Classical Newton's method for solving Eqn (3.6) generates a sequence of approximations Tk to T , where Tk+1 = Tk + sk and the Newton step sk is the solution to the system of linear equations Jk sk = -F(Tk ), (4.1)

where Jk  F (Tk ) is the Jacobian of F evaluated at Tk . Newton's method is attractive because of its fast local convergence properties. For large-scale problems, Eqn (4.1) is typically solved using an iterative method because direct methods become impractical. Furthermore, it is often unnecessary to use a tight convergence tolerance for the iterative method when Tk is far from T , since the linearization that leads to (4.1) may be a poor approximation to F(T). Generally, it is much more efficient to employ inexact Newton methods [9], in which the convergence tolerance for (4.1) is selected adaptively by requiring that sk only satisfy: F(Tk ) + Jk sk  k F(Tk ) (4.2)

for some k  (0, 1) [9]. With an appropriate choice of the forcing term k superlinear and even quadratic convergence of the iteration can be achieved [10]. While any iterative method can be used to find an sk that satisfies (4.2), Krylov subspace methods are distinguished by the fact that they only require matrix-vector products to proceed. These matrix-vector products can be approximated by a finite-difference version of the directional (G^ ateaux) derivative as:

12

F(Tk + v) - F(Tk ) , (4.3)  which is especially advantageous when Jk is difficult to compute or expensive to store (as is the case in this application due to the presence of multiple meshes). While the selection of a suitable differencing parameter  may be non-trivial for some applications, it is generally well-understood [11]. For this application, we choose: Jk v  = 
mach

1 + Tk , v

where mach is machine precision and · refers to the l2 -norm. In our applications, which are performed in double precision,  is typically on the order of 10-10 . From the various Krylov methods available, GMRES was selected because it guarantees convergence with nonsymmetric, nonpositive definite systems [12] (the case in some of our examples), and because it provides normalized Krylov vectors v = 1, thus bounding the error introduced in the difference approximation of (4.3) (whose leading error term is proportional to  v 2 ) [13]. However, GMRES can be memory intensive (storage increases linearly with the number of GMRES iterations per Jacobian solve) and expensive (computational complexity of GMRES increases quadratically with the number of GMRES iterations per Jacobian solve). In principle, restarted GMRES can deal with these limitations; however, it lacks a theory of convergence, and stalling is frequently observed in real applications [14]. In our applications, we rely on performing inexact solves combined with efficient preconditioning to keep the number of GMRES iterations required to compute each inexact Newton step, sk , small. 4.2. Preconditioning Preconditioning is a numerical technique used within an iterative method to accelerate the process of finding a solution to a system of equations. The use of a preconditioner will increase the cost of each iteration but a good preconditioner will drastically reduce the total number of iterations required to solve the system of equations thus making the overall process significantly faster than the unpreconditioned case. While the idea can be applied to 13

both nonlinear and linear systems of equations, we only focus on applying to the linear systems at each Newton step within the JFNK procedure. In particular, we used a "right preconditioning" procedure to solve (4.1). In this approach, (4.1) is transformed to the equivalent system shown in (4.4) by using a preconditioner, M . JM-1 Ms = -F(T) (4.4)

The above system is solved in two steps: (1) solve Ay = -F(T) using the GMRES method where A = JM-1 and (2) compute s = M-1 y. The matrix-vector product, Av, is approximated as shown in (4.5). F(T + M-1 v) - F(T) , (4.5)  Ideally, M-1 should be a good approximation to J-1 , it should be easy and inexpensive to compute and apply, and the computation and application of M-1 should have good parallel scalability. It is difficult to meet all of these competing requirements and some trade-offs need to be made while designing M. The design of the preconditioner used for our problem stems from a careful observation of the structure of the true Jacobian matrix which is of the form:   J11 J12 0 0 ··· 0 J1C  J21 J22 J23 0 · · · 0 J2C     0 J32 J33 J34 · · ·  0 J 3 C    0 0 J43 J44 · · · 0 J4C  J= (4.6)   .  . . . . . . . . . . . . .  . . . . . . .     0 0 · · · · · · · · · JN N JN C  JC 1 JC 2 · · · · · · · · · JCN JCC Av  Here, each Jii , i = 1, . . . , N is a block matrix denoting the portion of the full Jacobian arising from interior-interior connections within the i-th pellet. Ji,i-1 and Ji,i+1 are block matrices corresponding to the boundary interactions between pellet i and pellets (i - 1) and (i + 1) respectively. JiC and JCi are block matrices corresponding to the boundary couplings between the i-th pellet and the clad. Under normal operating conditions within a reactor the heat transfer between adjacent pellet domains is considerably lower than between the pellets and clad. This prompts us to drop off diagonal terms corresponding to 14

pellet-pellet interactions when forming the preconditioning matrix M. This leaves off diagonal coupling terms between the pellets and clad. While a preconditioner that retains these terms is likely to yield better overall convergence it requires a block Gauss-Seidel type approach that limits the level of asynchrony within the preconditioner. By dropping off diagonal terms corresponding to pellet-clad interactions we choose to sacrifice potentially better convergence for the ability to asynchronously solve all domains at once within the preconditioner solve step. Finally, block diagonal terms of the Jacobian that involve the partial derivative of the thermal conductivity, k , with respect to the temperature, T are ignored to yield an approximate Jacobian of the form ~  J11 0 0 0 ··· 0 0 ~22 0  0 J 0 ··· 0 0     0 ~33 0 · · · 0 J 0 0     0  ~ 0 0 J · · · 0 0 M= (4.7) 44   .  . . . . . . . . . . . . .  . . . . . . .     0 ~ 0 · · · · · · · · · JN N 0  ~CC 0 0 ··· ··· ··· 0 J Inverting systems of the form Ms = y now correspond to solving (N + 1) independent subsystems ~ii si = yi J (4.8) At the k -th Newton step, systems 1 through N correspond to discretizing and solving N variable coefficient linear diffusion PDE systems, one for each pellet domain, of the form:
k -  · ki (Tik , x)Ti (x) = ri (x), for x  i ,

(4.9)

with boundary conditions ki (Tik )Ti · ni + hi,i-1 (Tik , 0)Ti = 0 on i,i-1 , ki (Tik )Ti · ni + hi,i+1 (Tik , 0)Ti = 0 on i,i+1 . (4.10) (4.11)

where Tik is the current approximation to the solution of the nonlinear system k (3.6) over the i-th pellet domain and ri is the nonlinear residual. System (N +1) of the preconditioner solve is over the clad domain and again involves

15

discretizing and solving a linear variable coefficient diffusion PDE system of the form: k -  · (kc (Tck , x)Tc ) = rc for x  c (4.12) with boundary conditions kc (Tck )Tc · ng + hc,g (Tck , 0)Tc = 0 on c,g kc (Tck )Tc · nf + hc,f (Tck , 0)Tc = 0 on c,f (4.13) (4.14)

with Tck being the current approximation to the solution of the nonlinear k system (3.6) over the clad domain and rc the nonlinear residual. Discretization of the linear variable coefficient diffusion PDE systems described above follows along the lines of the methodology described in section (3). Each application of the preconditioner involves (N + 1) algebraic multigrid solves to invert the systems (4.8). Since these are independent subsystems all of the (N + 1) solvers can operate simultaneously when distributed over different processor sets enabling us to obtain a high degree of parallelism irrespective of the number of pellet subdomains present. 5. Computational Infrastructure Several computational tools are necessary for performing simulations such as the one described in this paper; we developed the Advanced Multi-Physics (AMP) [15] package for this purpose. AMP is a complete system for simulating stationary and time dependent, multi-domain, coupled physics problems. AMP consists of several software components. Each component is designed to provide a uniform consistent interface which interacts with other components, and developers of other components are only exposed to these interfaces. This is despite the fact that AMP is designed to sit in between existing software frameworks to leverage their strengths and investments without over-dependence. The complexities of interfacing with different software frameworks are kept behind the standard interfaces that AMP provides. Fig. 4 illustrates the structure of the various components in AMP. A brief description of each of these components is given below. 5.1. Mesh and geometry The mesh and geometry interface (AMP::Mesh) allows AMP to interact with multiple mesh or geometry packages. AMP::Mesh already interfaces

16

External Codes

Time Integrators Solvers Operators Vectors/Matrices Discretization Mesh/Geometry

Figure 4: Structure of components in AMP

with the LibMesh [16] and STKMesh [17] packages, in addition to maintaining a native structured mesh capability. The native structured mesh capability includes automated mesh generation for simple geometries enabling users to create boxes, cylinders, and tubes without an external mesh generation tool. These internal mesh generators were used for most of the results presented in section 6. 5.2. Discretization Due to the close coupling between mesh and discretization, AMP::Mesh (using functionality contained in LibMesh), currently handles the discretization also. This will become a separate interface, if the need arises for us to interface different discretization packages with mesh packages. 5.3. Vectors and matrices AMP provides standard AMP::Vector and AMP::Matrix classes which serve two purposes. Firstly, they provide users with a standard interface to perform vector and matrix operations. At the same time, the classes hide the details of interfacing with various software packages that have their own definition of vectors and matrices. For example, Trilinos [18] and PETSc [19] both provide matrix operations and Trilinos, PETSc, and SUNDIALS [20] provide and/or use vector operations. AMP Vector and Matrix act as the interfaces to these packages through the Vector and Matrix classes and enable a user to combine components from all of these packages to build powerful AMP applications while not having to tackle the complexities of interacting with each of these packages. 17

The vector class also contains the ability to compose multiple parallel vectors into a single multi-vector that can be provided to a solver. For example, the global solution and source vectors are created as a composition of nodal vectors on each mesh and composed into a single multi-vector. A view to the multi-vector is provided to PETSc SNES nonlinear solver for the Newton iterations and a view of the vector and the diffusion matrix associated with a single domain is provided to Trilinos ML for the multi-grid preconditioning. 5.4. Operators Operators are the core of the AMP design and where all of the physics is contained. Operators encapsulate the details of the mapping operation L : X  Y where X and Y are appropriately defined spaces. Operators may represent discretized PDE operators, boundary operators, an operation to extract material properties from material databases or tables, linear or nonlinear algebraic operations, or compositions of the above. The ability to compose operators and to extract information from compositions is intended to facilitate the incremental construction of multi-physics and/or multidomain simulations as well as rapid prototyping and experimentation to understand couplings in multi-physics simulations. The nonlinear and linear FEM operators for diffusion, boundary conditions and interpolation maps between domains are encoded as operators. 5.5. Solvers Solvers in AMP refer to the nonlinear and linear solvers that represent the action of an approximate inverse map of a given operator if that inverse operation has some well defined meaning. In this sense solvers can also be considered as operators. An inverse operator can be easily constructed by wrapping a solver in an inverse operator class. The solver interface allows the user to utilize a standard interface to solvers from Trilinos, PETSc, native AMP solvers, and potentially other packages in the future. Again, the design emphasis has been to provide a standard interface to hide the complexity of particular software packages from a user and to avoid required dependence on a particular software package. The solver interface enables us to create the complex nonlinear solvers, linear solvers, and preconditioners across multiple domains without significant code rewrites.

18

5.6. Time integrators AMP time integrators provide a uniform interface to solving time-dependent systems which can include Differential Algebraic Equations (DAEs). This is necessary within the context of our broader target application class because of coupling between time dependent thermal and quasi-static mechanical systems being simulated. The design allows for explicit, semi-implicit, and fully implicit simulations of coupled multi-physics problems. In the case of semi-implicit and fully implicit calculations, the solver interfaces in AMP are used, and in all cases, the operator interfaces are used to allow composable multi-physics simulations allowing users to experiment with coupling different physics together. The time integrator interface is used to provide an interface to the SUNDIALS suite of time integrators and can be used in future to interface to other time integrator packages such as the Rhythmos package of Trilinos. 5.7. External packages AMP is designed to leverage existing software whenever possible including off-the-shelf leadership class computational packages that include the Trilinos, PETSc and SUNDIALS packages. In general the infrastructure design of AMP does not rely on any external software, but provides interfaces for using external software within AMP. For example, a user can create a vector using AMP's internal vector, a PETSc vector, or a Trilinos Epetra vector, but can then use the given vector within the solvers that may use PETSc or Trilinos solvers. AMP leverages capabilities within many software packages through a seamless application programming interface including MPI for parallel capabilities, PETSc for vectors, nonlinear and linear solvers, Trilinos for vectors, nonlinear solvers, and algebraic multigrid solvers, SUNDIALS for implicit time integrators, LibMesh and STKMesh for discretization and meshing and HDF5 and Silo for IO. 5.8. Parallel implementation AMP is primarily designed to be a parallel infrastructure based on MPI. An MPI-based utility class is provided that allows the user to utilize MPI. The interface enables AMP to be compiled without MPI for users who do not wish to leverage the parallel capabilities. The core design is independent of the parallelization, and the parallelization is based on parallel decomposition of the meshes. There is a two-level parallel decomposition used for the mesh 19

domains. First, the individual mesh domains are split onto separate communicators and independent processor groups. This minimizes the number of processors per mesh and ensures that independent meshes can utilize collective operations that do not include the processors of other meshes. This splitting is done internally within AMP and can be modified by the user. A second level of domain decomposition is then performed to divide each mesh domain among the processors in the MPI group for that domain. This level of decomposition is handled by the package responsible for the current mesh domain. For example, if the underlying mesh for a given domain is LibMesh, it will perform the decomposition, while a native AMP mesh will be controlled by AMP. Note that different mesh domains may be owned by different packages and this is fully supported. Once the domain-decomposition is performed, vectors and matrices may be created over a single mesh, an arbitrary combination of meshes, or a subset of a mesh (or multiple meshes). Each vector or matrix exists over a given communicator that does not need to match any mesh. Linear operations are then performed on this communicator reducing the need for global operations over the entire global communicator. Maps between multiple domains use a communicator that spans two or more existing communicators. The MPI utility class provides all routines for creating and managing the communicators with MPI, including their proper destruction when they are no longer needed. Section 6.4.1 includes the results of a scaling study conducted using the problem described in Section 6.1 and using the parallel load balancing strategies described here. 6. Numerical Experiments A suite of numerical experiments were defined to verify the accuracy of the thermal transport capability of AMP for a multi-domain problem that is based on the geometry and materials of nuclear fuel. Independent studies were performed to verify the accuracy of the solution using the method of manufactured solutions (Section 6.2), evaluate the accuracy of the code with respect to experimental data and a well characterized code used for regulatory analysis (Section 6.3), and evaluate the scalability of the parallel algorithm (Section 6.4). All of the studies were based on actual geometries and material properties for experimental nuclear fuel rods that are defined in Section 6.1. All numerical experiments were performed on the Titan (Cray XK7) and EOS (Cray XC30) supercomputers hosted at the Oak Ridge Leadership 20

Computing Facility with 8 and 16 MPI processes per compute node respectively. Load balancing is done automatically by the load balancer within AMP. The AMP nonlinear solver internally leveraged the JFNK implementation within the PETSc package with absolute and relative tolerances for the JFNK nonlinear solver being set to 1.0e-12 and 1.0e-10 respectively. Right preconditioned FGMRES with a maximum Krylov dimension of 40 was used within our simulations. The AMP preconditioner consists of a block Jacobi solver as described earlier with each block Jacobi solver component consisting of one or more iterations of an algebraic multigrid (Trilinos ML) V-cycle solver with 2 pre- and post-smoothing steps of a symmetric Gauss-Seidel smoother, a maximum of 10 multigrid levels and a coarse grid direct solver. All simulations were performed in double precision arithmetic. 6.1. Experimental Setup The materials and geometries used in the following numerical experiments are based on one of two well-characterized experiments from the International Fuel Performance Experiments (IFPE) database [21]. The Integrated Fuel Assembly (IFA) 432 [22], Rod 1, is a standard nuclear fuel rod (uranium-dioxide or, UO2 , fuel in Zircaloy-4 clad) that was irradiated in the Halden boiling water reactor from December 1975 to June 1982 with online temperature measurements at one axial location in the center of the fuel. The IFA 597 [23], Rod 2, contains weapons-grade mixed-oxide (MOX) fuel within Zircaloy-4 clad in a more modern geometry that includes a dish, chamfer, and central hole, as shown in Figure 5. IFA 597, Rod 2 was irradiated in the Halden boiling water reactor from from July 1997 to January 2002 with online temperature measurements at one axial location in the center fuel. Each of these rods is a short version of a full length commercial fuel rod. The geometric and material description of the experimental rods are provided in Table 1 for the IFA 432 and IFA 597 experiments. The clad height and number of pellets modeled were adjusted in Sections 6.2 and 6.4 to achieve the purpose of that section. However, these geometries were used exactly in Section 6.3. The thermal conductivity (k ) of Zircaloy-4 (Eq. 6.1) and fuel (Eq. 6.2), including both UO2 and MOX, depend on the temperature (T) and burnup (B), which is a measure of total heat generated locally. k [Zr] = 7.51 + 2.09 × 10-2 T - 1.45 × 10-5 T 2 + 7.67 × 10-9 T 3 21

(6.1)

Figure 5: Mid-plane slice of an annular nuclear fuel pellet with a dish and chamfer

22

Dimension Units Pellet ID mm Pellet OD mm Pellet Dish Depth mm Pellet Dish Diameter mm Pellet Chamfer Height mm Pellet Chamfer Diameter mm Pellet Density g/cc Pellet Height mm Number of Pellets Clad ID mm Clad OD mm Clad Height mm

IFA 432 0 10.67 0 0 0 0 10.42 13 44 10.9 12.78 622.8

IFA 597 1.8 8.05 0.26 2.15 0.15 5.3 10.5 10.5 21 8.22 9.5 252.

Table 1: Geometry and material specification for validation problems

k [Fuel] = 

 + T +  1.0 + e

 -T

-1

-1

+ T -2 e- T



(6.2)

where [UO2 ] = 1.00767;  [UO2 ] = 0.0452 + 0.00187B ;  [UO2 ] = 0.000246; [UO2 ] = 16361;  = =  =  =

[MOX] = 1.05353  [MOX] = 0.035 + 0.00187B  [MOX] = 0.000286 [MOX] = 13520

0.038 1.0 - 0.9e-0.04B B 0.28 396 6380 3.5 × 109

To define the heat source in the nuclear fuel, AMP allows the user to either define the power distribution, f (r, , z ), as a function of the radius (r) from the center of the pellet, height (z ), and azimuthal-angle () about the z axis or provide a power distribution in a coupled-physics calculation at every 23

quadrature-point in the problem. The user-defined power definition allows for a simple definition of the axial shape functions or complex nuclear-specific features, such as the radial rim effect or azimuthal variations guide tubes and control rods. The radial power shape includes the option to use a model that coincides with the empirically-derived TUBRNP model (Equation 6.4) from the Transuranus nuclear fuel performance code [24]. f (r, , z ) = 1 + a F (r) + b sin() +
k>0

ck Pk (z );

(6.3) (6.4)

F (r) = 1 + 3.45 exp[-3(R - r)0.45 ].

In equation 6.3, the user-defined coefficients (a, b , and ck ) define the magnitude of each component, Pk (z ) are Legendre polynomials, and F (r) is the TUBRNP model that is based on the radius and the outer radius of the fuel pellet (R). In the verification testing, a manufactured-source was utilized; the validation testing utilized the TUBRNP model. 6.2. Verification Studies Verification studies for modeling steady state thermal contact for nuclear fuels are presented here. The verification process uses the method of manufactured solutions. For this study, the pellet geometry is based on the IFA 432 experiment and the clad geometry is simplified as shown in Figure 6. Material properties of UO2 for the pellet and Zircaloy for the clad are based on the IFA 432 experiment as listed in Section 6.1. The pellet and clad domains are not in contact and have a (gap) distance between the surfaces. A total of four cases are studied using this configuration. Case 1: Three-dimensional single fuel pellet (with no clad) that exhibits strong gradients in all directions with different Robin boundary conditions between circular surface along the height and end surfaces. Case 2: Three fuel pellets stacked upon each other (with no clad). The surfaces are in contact and a manufactured solution is constructed using the fuel pellet contact conductance model. At the contact surfaces a Robin boundary condition is imposed and the results would make apparent any anomalies in the volume and boundary condition discretizations. Case 3: Single fuel pellet and clad with heat transfer across a gap between the surfaces. The manufactured solution for both the domains is constructed

24

Figure 6: Schematic of pellet and clad geometries used in verification studies

25

taking the nonlinear gap conductance between the internal surfaces into consideration. The results would make apparent any lack of energy conservation due to heat transfer across the gap. Case 4: Two pellets and clad including contact between pellets and heat transfer across a gap between pellet and clad. This verifies the implementation at corner points which intersect multiple domains. We begin by selecting the exact solution to be (x, y, z ) = 800z + 106 (0.00004 - 20x2 - 20y 2 ) (6.5)

which is qualitatively similar to the thermal solutions and in bounds to the material models. Substituting it for T in the differential equation (2.1) we obtain the corresponding analytic right hand side. These sources and corresponding boundary conditions are evaluated at each quadrature point of the entire finite element mesh in order to eliminate interpolation errors. The relative and absolute convergence tolerances within the nonlinear solver are set to 1.0e-10 to ensure solver errors are below discretization error bounds. The discretization error is evaluated using an L2 norm.
1/2 1/2

|| - h ||L2 =


( - h ) d

2


 qp

( - h ) J (xqp )wqp

2

(6.6) Convergence rates of these norms are reported on progressively refined meshes by increasing the number of elements in all three cylindrical coordinates to simplify computing the characteristic element length "h" of the unstructured mesh. Also, we make sure that the refinements are generated from the original geometry to ensure that the meshes are geometrically conforming. The rate of convergence is given by log p= log
||e2 || ||e1 || h2 2 h2 1

log 

||e2 || ||e1 ||

log (4)

(6.7)

26

Problem Case 1

Case 2

Case 3

Case 4

# Elements 1890 14944 119296 5670 44832 357888 3510 27904 222976 5400 42848 342272

|| - h ||L 2.1796 0.7426 0.2256 2.1845 0.7810 0.2644 2.9740 0.7053 0.1817 2.9851 0.7446 0.2207

|| - h ||L2 0.002411 0.000624 0.000155 0.004139 0.001073 0.000267 0.003132 0.000804 0.000200 0.004434 0.001139 0.000284

p 1.94 2.00 1.94 2.00 1.96 2.00 1.96 2.00

Table 2: Mesh refinement studies # Elements || - h ||L 10 Pellets 18900 2.1848 149440 0.7810 1192960 0.2644 50 Pellets 94500 2.1223 756000 0.7810 6048000 0.2644 || - h ||L2 0.007606 0.001971 0.000491 0.017045 0.004416 0.001100 p

1.94 2.00

1.94 2.00

Table 3: Many domain mesh refinement studies for Case 2

27

Elements 10 Pellets 35100 280800 2246400 50 Pellets 87750 702000 5616000

|| - h ||L 2.9899 0.7450 0.2210 2.9852 0.7450 0.2210

|| - h ||L2 0.009937 0.002549 0.000636 0.022190 0.005700 0.001421

p

1.96 2.00

1.96 2.00

Table 4: Many domain mesh refinement studies for Case 4 In addition to four cases mentioned, we also conducted the verification studies for generalized versions of case 2 and case 4 with 10 and 50 pellet domains to demonstrate the convergence rate of the solution procedure in parallel. These results are presented in Tables 3-4. 6.3. Validation Studies An extensive validation evaluation of AMP for nuclear fuel applications, which includes several experimental fuel rods for a variety of conditions, has been documented in [25­27]. This section includes an excerpt of that research to provide a basis for the accuracy of the material models with respect to the experimental results. Because of the extreme environment of nuclear fuel (high radiation, high temperature, and highly turbulent, multi-phase flow), it is difficult to precisely measure both the local temperature and the power in the fuel near the thermocouple. Therefore, nuclear fuel experiments generally assume a 5 to 10% experimental uncertainty in the measured data; for this report, we have incorporated a relatively tight expected tolerance of + 50K, which is generally less than the 10% error and approximately 3% at full power. Figure 7 provides the computational results of the AMP simulation and experimental measurements of the centerline fuel temperature in the IFA 597 experiment. The input power is relatively constant and the computational results are consistently within 50K and generally within 3% of the measured temperature. The results for the IFA 432 experiment are shown in Figure 8. Because the power distribution varies significantly more than in the IFA 597 experiment, the 50K error bars on the experimental results appear much

28

Figure 7: Validation of the temperature at the thermocouple in the IFA 597 experiment smaller. However, the AMP results generally fall within the experimental error. 6.4. Scaling Studies Scaling studies for the steady state thermal problem are presented here. The problem setup was described in Section 6.1, with the geometry based on the IFA-432 experiment, but now including 348 full pellets, which is representative of a full-scale commercial nuclear fuel rod. Several different aspects of the performance are studied using both strong and weak scaling. These include: · Solve: Total time spent within the JFNK solver for the entire coupled nonlinear thermal problem. · Nonlinear Residual: The time required to compute one consistent global nonlinear residual within the JFNK solver across all physical domains. Computing the nonlinear residual involves computing the nonlinear residual for each physical volume, applying boundary conditions, and mapping solution components across domains. The times 29

Figure 8: Validation of the temperature at the thermocouple in the IFA 432 experiment for these individual components will also be reported separately. Note that the number of calls will vary slightly depending on the number of iterations of the solver. This information is also included. · Diffusion Apply: Time required to compute the non-linear finite element residual at all interior degrees of freedom within all domains once. No communication between domains is required here. · Boundary Conditions: Time required to impose the non-linear finite element boundary conditions once within the computation of the global nonlinear residual. This requires traversing all surface elements on the finite element meshes for each domain. No communication between domains is required here. · Map Apply: Time per application of the maps for the solution transfer between the clad and pellets and the maps between the different pellet domains. The mapped values are required in imposition of the nonlinear Robin boundary conditions.

30

· Load Mesh: Time to create all of the meshes. This consists of creating the appropriate load balance for the different domains, creating the individual meshes, and initializing all mesh data. For the purposes of this problem the meshes used consist of internal, logically rectangular meshes. · Save Results: Time to write the results. This includes writing a separate SILO file for each core, and a single summary file that can be loaded into VisIt [28]. 6.4.1. Strong Scaling The strong scaling results are based on a standard resolution mesh, which contains approximately 1.6M unknowns. For high fidelity analysis of a single fuel rod, this is considered a relatively coarse mesh; a high resolution analysis would typically require an 8x increase in each dimension. On the other hand, for high-fidelity analyses of a full nuclear reactor containing tens of thousands of fuel rods, this resolution is sufficient because the accuracy requirements are typically lower. Demonstrating efficient strong scaling from serial to 100500 cores for this problem size (single rod with 1.6M unknowns) will be sufficiently indicative of good scaling results for a full reactor on millions of cores and will also provide a lower bound on expected scaling for fineresolution fuel rod calculations on tens of thousands of cores. The strong scaling studies were executed using 1-2048 cores on the Titan Cray XK7 and the EOS XC30 at Oak Ridge National Laboratory. Tables 5 and 6 show the scaling results on EOS and Titan respectively, with the different components of the solve as well as loading the meshes and saving the results. The first column is the number of cores, while all other columns are the wall-clock execution times for the problem in seconds. For the diffusion and map apply calls (Diffusion Apply and Map Apply, respectively), the execution time is the average accumulated time across all cores. This is necessary because some cores may have different execution times due to load imbalances and is particularly acute for small core counts (2-8) due to a load imbalance between the domains. For large core counts, each domain will exist on a non-overlapping set of cores. Figures 9 and 10 shows the plots of the scaling results compared to ideal scaling. For small core counts (1-8), the run time of the solve is limited by the maps between the different domains. The most time-consuming map is for the cladto-pellets heat transfer between the outer surface of the pellets and the inner 31

# of Processors 1 2 4 8 16 32 64 128 256 512 1024 2048

Nonlinear Diffusion Boundary Map Residual Apply Conditions Apply 3488.25 2690.93 751.84 200.66 1629.49 2265.47 1273.65 374.21 99.52 739.46 761.60 397.61 193.86 50.04 107.30 315.21 187.79 102.38 26.51 42.89 159.12 86.89 51.35 13.22 12.76 74.47 42.11 25.71 6.68 5.34 38.09 21.10 12.94 3.49 2.46 20.83 10.63 6.51 1.74 1.16 10.69 5.42 3.27 0.89 0.64 5.90 3.00 1.70 0.49 0.45 3.85 2.03 0.92 0.30 0.56 4.00 2.15 0.53 0.23 1.12 Solve

Mesh Loading 13.11 7.02 6.33 2.33 1.39 0.64 0.34 0.20 0.11 0.07 0.07 0.10

Save Results 483.12 306.11 30.83 12.48 2.90 1.05 0.42 0.32 0.11 0.11 0.15 0.22

Table 5: Strong scaling studies on EOS
# of Processors 2 4 8 16 32 64 128 256 512 1024 2048 Nonlinear Diffusion Boundary Map Residual Apply Conditions Apply 4908.14 2689.35 739.69 217.28 1609.45 1629.89 819.30 371.02 106.32 248.20 642.05 363.04 186.89 52.71 93.28 345.45 171.02 96.10 27.18 29.70 174.91 88.51 51.01 15.10 12.67 89.06 43.71 25.56 7.60 5.74 46.97 21.87 12.72 3.93 2.50 23.97 11.11 6.39 2.02 1.33 12.88 5.97 3.31 1.08 0.82 8.29 4.06 1.80 0.66 1.02 8.01 4.10 1.02 0.46 2.07 Solve Mesh Loading 13.83 13.78 4.76 2.81 1.42 0.72 0.40 0.23 0.15 0.14 0.22 Save Results 682.38 73.28 28.93 8.15 2.84 1.05 0.35 0.20 0.17 0.18 0.29

Table 6: Strong scaling studies on Titan surface of the clad. This map is constructed using a std::multimap with all points on the local surface, followed by a pair-wise all-all exchange of data between the two meshes. For the serial case, this results in a large number of points in the local map that must be managed. As the number of cores is increased to 128-512 cores, the number of points on the surface per processor is significantly reduced, which leads to an additional log(n) reduction in the wall-clock time that results in the super-linear speedup observed. The apparent lack of speedup from 1 to 2 cores is due to the load imbalance between the clad mesh located on one core and all of the pellet meshes on the

32

104 103 Time (s) 102 101 100 10-1 0 10 101 102 # of Processors 103

Ideal Solve Apply Diffusion Apply Robin Apply Maps Load Save

Figure 9: Strong scaling studies on EOS

104 103 Time (s) 102 101 100 10-1 0 10 101 102 # of Processors 103

Ideal Solve Apply Diffusion Apply Robin Apply Maps Load Save

Figure 10: Strong scaling studies on Titan

other. For very large core counts (1024-2048), the number of elements on a surface is sufficiently small that the communication time begins to dominate, which limits the scaling for this problem size. The behavior of the diffusion and the Robin boundary condition applies (Diffusion Apply and BC Apply, respectively) show nearly perfect scalability because they do not involve any communication. Generating the native structured meshes does not represent 33

2 Efficiency 1.5 1 0.5 0 0 10 101 102 # of Processors Figure 11: Scaling efficiency1 103

EOS Titan

a significant portion of the execution time, yet loading the meshes shows nearly perfect scaling up to 512 cores. At this relatively high core count, the load time is dominated by the load balance process, which is relatively independent of the number of cores (0.1 seconds). Finally, saving the results of the simulation (Save Results) has an acceptable scalability. The results are saved to multiple SILO files (one per core), with a single summary file. For small core counts, the Save Results is dominated by the time to write the data. For large core counts, Save Results is dominated by the time to open a file on the Lustre file system. This is approximately constant for all cores, but has a large variation that depends on the load of the computer. Several executions were made and the typical results are presented. The time to open a file varied between 0.02 and 1 second. Large executions are particularly sensitive to this effect because all cores must synchronize to write the summary file. Figure 11 shows the parallel efficiency on EOS and Titan. TS , where TS is the serial wall clock time, The efficiency is calculated as PN TP PN is the number of processors, and TP is the parallel wall clock time with PN processes. Note that for processor counts between 4 and 512, the speedup is greater than 1. This is due to the super-linear speedup discussed previously.
The serial wall clock time TS for Titan is not available due to maximum wall clock time restrictions. TS on Titan is estimated by assuming that the efficiency on 2 processors of Titan and EOS is similar. This assumption is justified by the data in Tables 5 and 6.
1

34

6.4.2. Weak Scaling Tables 7 and 8 shows the results of a weak scaling study performed on EOS and Titan. In the weak scaling study, the base mesh from the strong scaling study was used with 64 cores and the resolution was increased in each direction by a factor of 2x and 4x. This resulted in a set of executions of 1.6M unknowns on 64 cores, 12.8M unknowns on 512 cores, and 102M unknowns on 4096 cores. All times are in seconds and for functions that are called multiple times per solve the time per call is included in parentheses. Using weak scaling, varying the problem size and processor count by 64 times, the performance of the solve is approximately constant. The total number of non-linear iterations was independent of the resolution, and the total number of linear iterations varies slightly with resolution. This is most likely due to a slight degradation in the performance of the parallel smoother used within the algebraic multigrid solver. The solve time is approximately constant with some variation that is due to differences in the number of linear iterations. The contributions to the solve are specified in terms of the total time and the time per iteration in parentheses. Based on the time per iteration, the global apply, finite element diffusion operator apply, and resetting of Trilinos ML are all approximately constant. The apply call for the Robin boundary condition decreases slightly with problem size as the ratio of the total number of unknowns on the surface compared to the total number of unknowns decreases slowly with problem size. The variation in mapping between domains is primarily due to specific parallel decomposition and the variation in MPI performance on Titan and EOS that depend on the allocated nodes. Loading and saving the meshes show a slight increase for large problems due to increased demand on the Lustre parallel file system used but represents a small fraction of the total run time (2-3%). 6.4.3. Preconditioner Performance As mentioned in the introduction section the number of pellets within a fuel rod can vary dramatically. Hence it is important that the solver deliver good performance as the number of actual pellet domains is varied. This in turn is dependent primarily on the performance of the preconditioner employed. In order to study this we consider a series of numerical experiments where the total number of mesh elements across all pellet and clad domains is kept constant while the number of pellet domains is varied. We note that though the number of mesh elements is kept constant the number of degrees of freedom does rise slightly (3%) as the number of pellets is increased due 35

Core Count Degrees of Freedom Nonlinear iterations Linear iterations Solve Nonlinear Residual Diffusion Apply Boundary Conditions Map Apply Reset ML Mesh Loading Save Results

1x 64 1.6M 5 29 38.14 21.10 (0.73) 12.97 (0.45) 3.46 (0.12) 2.46 (0.08) 13.05 (0.45) 0.34 0.42

2x 512 12.8M 5 31 35.29 19.24 (0.62) 14.04 (0.45) 2.00 (0.06) 1.60 (0.05) 11.22 (0.26) 0.28 0.35

4x 4096 102M 5 36 49.56 26.91 (0.75) 17.12 (0.48) 1.81 (0.05) 5.97 (0.17) 12.74 (0.35) 0.42 1.11

Table 7: Weak scaling studies on EOS 1x 64 1.6M 5 29 88.99 43.66 (1.51) 25.55 (0.88) 7.57 (0.26) 5.74 (0.20) 29.27 (1.01) 0.72 1.06 2x 512 12.8M 5 31 77.94 38.10 (1.23) 27.51 (0.89) 4.30 (0.14) 3.05 (0.10) 25.09 (0.81) 0.59 0.72 4x 4096 102M 5 36 107.19 55.69 (1.55) 37.12 (1.03) 4.09 (0.11) 10.22 (0.28) 30.54 (0.85) 1.25 1.47

Core Count Degrees of Freedom Nonlinear iterations Linear iterations Solve Nonlinear Residual Diffusion Apply Boundary Conditions Map Apply Reset ML Mesh Loading Save Results

Table 8: Weak scaling studies on Titan to the introduction of more surface elements. In addition the number of solution transfer operations that require communication between domains also increases. Note that the additional communication incurred is point to point communication between pairs of processors and does not significantly affect the runtime. The dimensions of the pellets are chosen so that the height of the pellet stack matches that of the clad. Figure 12 plots the 36

Domains Solve

1 50.61

2 50.86

4 50.61

8 50.51

16 50.81

32 49.63

64 49.56

128 50.26

256 51.38

Table 9: Solution times as the number of pellet domains is varied number of nonlinear and linear iterations required for solving thermal fuel rod problems as the number of pellet domains is varied from 1 through 256. In addition the number of multigrid solves required within the preconditioner is varied from 1 to 3. As can be seen, the number of iterations remains largely constant. Figures 13(a) and 13(b) show the residual convergence history for the nonlinear solver as the number of domains is varied ranging from 1-256 domains. There is only a very slight effect on the residual due to increasing the number of domains. Finally, Table 9 shows that the required wall clock times for solution as the number of domains is varied does not change significantly.
20 Number of Iterations 15 10 5 0
Nonlinear (1 ML) Nonlinear (3 ML) Linear (1 ML) Linear (3 ML)

1

2

4

8 16 32 64 Number of Pellet Domains

128

256

Figure 12: Number of nonlinear and linear iterations as a function of the number of pellet domains and the number of algebraic multigrid (ML) iterations

7. Fuel Assembly Modeling In section 5 the components of the AMP multi-physics infrastructure that enabled the development of nonlinearly consistent multi-domain thermal transport calculations that form the main core of this paper were described. Here we illustrate further multi-physics capabilities of AMP by describing 37

102 Residual norm 100 10-2 10-4 10-6 10-8 1

Residual norm

NP=256 NP=128 NP=64 NP=32 NP=16 NP=8 NP=4 NP=2 NP=1

102 100 10-2 10-4 10-6 10-8

NP=256 NP=128 NP=64 NP=32 NP=16 NP=8 NP=4 NP=2 NP=1

2 3 4 Nonlinear iteration number

5

1

2 3 4 Nonlinear iteration number

5

(a) 1 ML iteration

(b) 3 ML iterations

Figure 13: Residual convergence history for fuel rods with NP=1 to NP=256 pellet subdomains with 1 and 3 multigrid iterations per subdomain within the preconditioner further extensions of the fuel rod modeling capability. Since our focus is on solution and coupling methodology and due to space limitations we will concentrate on the relevant coupling aspects with details on the models being provided in the appendices and provided references. Coupling to Coolant Models: The coolant liquid flowing on the outside of each fuel rod serves as a heat sink which flows axially along the length of the outside surface of the clad. The coolant model in the fluid domain and the thermal transport model in the clad domain are coupled nonlinearly through Robin boundary conditions (Eqn 2.7)
m m kc (Tc )Tc · nf + hc,f (Tc , Tf )(Tc - Tf ) = 0 on c,f m reproduced here for clarity. Here, Tf represents a mapped coolant temperature field over the clad surface. AMP enables us to couple different models interchangeably. The first model described in Appendix A.1 uses a reduced empirical model that solves a single axial equation using a simple finite difference scheme and is frequently sufficient for many calculations. This is the model used within the single fuel rod calculations presented in prior sections. The second model described in Appendix A.2 solves the two equations using a more complex model that is used when subchannel temperatures and densities are required. This model is used in the fuel assembly calculations

38

that we will describe further along. Let
h )=0 Ff (Tf

(7.1)

denote the nonlinear system of equations resulting from discretizing either h coolant model over the fluid domain with Tf a vector of fluid temperature unknowns. As described in Section 4.1, a JFNK solver only requires us to provide the ability to compute a nonlinear residual. Hence, augmenting the existing nonlinear system (3.6) of equations across the pellet and clad domains with eqns (7.1) for the flow domain enables us to perform coupled flow and thermal transport calculations. The augmented nonlinear system can be denoted by: ~ (T) = 0 F (7.2) t h h h h ~ (T) = F1 (T1 where F ), F2 (T2 ), . . . , FN (TN ), Fc (Tch ), Ff (Tf ) is the coupled set of block nonlinear equations across all pellet, clad, and flow domains with T denotes the block unknowns across all domains including flow. For preconditioning an augmented approximate Jacobian system of the form ~ = M 0 M ~f f 0 J (7.3)

~f f an approximate Jacobian for is used with M as defined in Eqn (4.7) and J the flow model. Since reduced order models are used for the flow, systems ~f f are inverted using a direct solver as opposed to the multigrid involving J solvers used to invert the other components. We note that the direct solver for the flow operates in parallel to the multigrid solves during each preconditioner solve step. Coupling to Oxide Growth Models: The formation of an oxide layer on the surface of a nuclear fuel rod can interfere with thermal conduction to the coolant and create additional stress within the clad. Typically the thickness of the oxide layer is much smaller than the other physical scales in the fuel rod. As a result the oxide growth is modeled at each point, x, on the surface of the clad by independent 1D models of the form: C (x, t)  C (x, t) = D ( x) t x x (7.4)

Solving for the oxygen concentration then reduces to solving equation 7.4 at each point on the surface of the clad subject to appropriate initial and boundary conditions which is described in detail in Appendix C. 39

For the purposes of this paper we consider a one way coupling of the full thermal transport model to the oxide models. As described previously a nonlinear solve is performed to convergence over the pellets, clad, and coolant domains. The resulting temperature field is used to initialize the oxide growth models at each point on an exterior clad surface mesh generated from the clad mesh using the mesh subset capability of AMP. Each point on the mesh contains an independent sub-grid oxide model which is distributed across the processors to match the clad load balance. For the results presented the clad runs on approximately half of the processors, with roughly the same number of surface nodes on each processor. Additionally since we have an independent model for every point the problem is embarrassingly parallel between the points and no additional communication is required. The results of the oxide model coupled to the thermal model is shown in Figure 14. The oxide layer thickness follows the surface temperature of the clad. This temperature is in turn affected by the power shape which can be seen by the fuel temperature and the flow temperature which creates a top-shifted peak to the temperature of the clad and oxide layer thickness. Coupled Radiation Transport and Fuel Assembly Thermal Modeling: In this section we describe the extension of the fuel rod modeling capability described to modeling a full nuclear assembly and coupling with a massively parallel radiation transport code, Denovo [29]. Here we focus on describing the coupling and scaling aspects and the interested reader is referred to [26, 27, 30­32] for further details including detailed verification and validation studies. Solving the fuel assembly problem consists of two parts: solving the neutronics equations to obtain the spatially varying source and solving the thermal diffusion with the appropriate heat sink. To accomplish this we solve the assembly level radiation transport equations using the methods described in section Appendix B, and an array of multiple fuel rod problems each of which is solved as described in this document. To accomplish the latter, we utilize a multi-mesh capability that can replicate a mesh to produce the appropriate array, and replicated column operators to produce a complete nonlinear system. The full nonlinear system is then solved using JFNK, with the preconditioner limited to the individual sub domains. The individual fuel rod physics are embarrassingly parallel between the fuel rods which we utilize through our load balance by generating a set of independent commu40

Figure 14: Oxide model results from left to right: Nuclear fuel pellet temperatures; Clad outer surface temperature; Coolant temperature; Resulting oxide layer thickness. nicators for each fuel rod, with the fuel rods distributed on independent sets of processors. We are not limited to this choice of parallel decomposition, but it yields the best performance for this problem. To demonstrate the ability to solve nuclear reactor analysis problems, we model a single fuel assembly of a pressurized water reactor with coupling between heat transfer, subchannel flow, and radiation transport. This corresponds to CASL AMA Progression Problem 6 [33]. The assembly consists of a 17 × 17 array of fuel rods on a square 1.26 cm pitch. Of the 289 pins, 264 are fuel rods containing 3.1% enriched UO2 , 24 are guide tube locations, and a central instrumentation tube. Zircaloy 4 clad surrounds all pins. The coolant surrounding the pins is water containing 1300 ppm soluble boron

41

and an inlet temperature of 569 K. The average power level in the assembly is 30,000 W/kg, approximately corresponding to an average power assembly from a full reactor core. Further details on the geometry and material specifications can be found in [33]. In the AMP computational model, the mesh for each fuel pellet contains 512 mesh cells each fuel rod contains 360 pellets. Additionally, each fuel rod is surrounded by a clad mesh containing 54,144 cells for a total of 238,464 cells per fuel rod. Over the 264 fuel pins in the full assembly, the total number of mesh cells in the AMP problem is approximately 63 million and the number of nodal degrees of freedom is slightly over 100 million. The Denovo computational model has approximately 4.6 million spatial cells, 23 energy groups, 32 angles, and P1 scattering (which uses four angular moments). The 23 group cross sections are collapsed from a 56 group library by the XSProc module of the SCALE package [34]. Power distributions computed by Denovo are mapped onto the AMP mesh using the polynomial smoothing process described in Appendix B. Temperatures and fluid densities computed by AMP are averaged over each of 49 axial levels within every fuel rod to be used for generating new cross sections. A simple Picard iteration is used to couple AMP and Denovo, with a damping (under-relaxation) factor of 0.4 applied to the temperature distribution, as described in Ref. [32]. The AMP thermal and subchannel problems are solved together using a JFNK approach as described in Appendix A and the Denovo k -eigenvalue problem is solved using a Krylov-Schur eigensolver [35]. A stopping criteria of 10-4 is applied to the Picard iterations and a tolerance of 10-5 for each of the AMP and Denovo subproblems. The problem was decomposed across 4624 computational cores, with both the AMP and Denovo problems utilizing the entire set of processors. The coupled problem converged in 12 Picard iterations, with an average of 1.6 Newton iterations per Picard iteration and 20 linear iterations per Newton step. The entire solution required 3976 seconds, of which 3182 seconds were spent in the Denovo transport solves and 680 seconds were spent in the AMP thermal solves. The temperature and power solution profiles throughout the assembly are shown in Fig. 15. The radial variation of the power distribution, both within individual fuel rods and across the assembly, is evident. Notably, the power level in pins neighboring guide tube locations is significantly higher than regions not near guide tubes (such as the assembly corners) due to increased neutron moderation. Although more difficult to visually discern, the fuel temperature distributions mirrors the same general trends as the 42

power, with higher temperatures corresponding to high power regions. Unlike the power, which attains its peak values at the outer radius of the fuel rods, the temperature distribution always peaks at or near the center of a fuel rod. The axial profiles clearly show the presence of the spacer grids as a series of local depressions due to increased absorption in those regions. 8. Conclusions Many real world engineering problems involve the complex interaction between many bodies, in a nonlinear manner. From mesh generation to predicting results, modeling these large complex systems presents significant computational challenges. An efficient parallel, multi-domain solution methodology has been developed and implemented to solve these systems by leveraging the natural decomposition of the problem associated with the individual domains. This methodology has been demonstrated for modeling heat transfer within nuclear fuel rods, which are composed of hundreds of individual pellets within a metal tube, and can be applied to a nuclear reactor, which is composed of tens of thousands of individual fuel pins. The model and discretization for the thermal transport in a nuclear fuel rod demonstration problem consists of nonlinear diffusion in each of the fuel pellets and clad, along with a Robin boundary condition on each surface, and maps between them, to account for the heat transfer between domains. Modeling the entire system in a single domain would create a significant challenge associated with mesh generation, but by modeling many individual domains, the generation of the mesh for the full problem becomes automatic and a negligible burden on the user. The Jacobian-Free Newton-Krylov (JFNK) method used to solve the nonlinear system of algebraic equations was described in detail, with a particular focus on the preconditioning strategy for the multi-domain problem. The Krylov solver has been shown to efficiently solve for the interaction between the individual domains, but an efficient preconditioner was required to account for the diffusion within individual domains. Therefore, the preconditioning algorithm leverages the natural decomposition in physical space through the use of a parallel block-diagonal structure in which each block is a single, physical domain that neglects the interaction between domains. To efficiently solve these multi-domain problems, the computational infrastructure of AMP was designed to support the problem specification, domain decomposition, and composition of mathematical constructs efficiently 43

(a) Radial Power Profile

(b) Radial Temperature Profile

(c) Axial Power Profile

(d) Axial Fuel Temperature Profile

(e) Clad and Coolant Axial Profiles

(f) 3D Temperature Profile

Figure 15: Assembly Solution Details 44

in parallel. The computational infrastructure was described in detail, with a particular focus on the parallel implementation. The infrastructure is designed to allow for the specification of nonlinear operators and linear preconditioners for individual domains and the composition of those operators into a single column operator that can be used by a solver, either internal or external, to compute the solution of the nonlinear problem. Similarly, the parallel vectors (solution and source) can be defined for individual domains and composed into a single vector that is used by the solvers and preconditioners. A "subsetting" approach has been developed to access individual components of the parallel vector that are associated with a single mesh (or surface of a mesh) and variable (such as temperature). Because there is a knowledge of the individual physical domains, parallel communicators are created for each domain and the interacting surfaces between domains. Therefore, the infrastructure can easily "subset" the parallel vector associated with an individual domain, determine a communicator specific to that domain, and access the preconditioned for that domain, which is provided to a solver for the compact preconditioning step on an individual domain. The domain decomposition strategy is designed with an awareness of the individual domains to minimize communication during the computationally-expensive preconditioning and apply processes. A series of numerical experiments were developed to verify, validate, and evaluate the parallel performance of the software and algorithm. Each of the numerical experiments were based on a single specification of material properties and geometry that was associated with a nuclear fuel performance experiment that is used as an international benchmark for fuel performance modeling. The numerical verification studies used the method of manufactured solutions and mesh refinement studies to verify the solution converged with the proper order to the manufactured solution. The validation study, specified in much greater detail in an associated manuscript, demonstrated that the full problem, from material model specification to discretization and solution, could accurately predict the temperature distribution within a nuclear fuel rod within the experimental uncertainty bounds. Both strong and weak scaling studies were performed with significant profiling, to understand the performance of the software, and algorithm. Excellent strong scaling was achieved as the processor count increased three orders of magnitude, until the number of elements per core was under 1000. This solution strategy, and the associated software, has been shown to accurately and efficiently solve large, complex, multi-domain, nonlinear prob45

lems by leveraging the natural structure of the physical domains. This has the potential to impact many computational engineering applications beyond nuclear fuel simulation. Acknowledgements and Access The AMP (Advanced Multi-Physics) code is distributed with a modified BSD license and accessible either by contacting the corresponding author or through the Radiation Safety Information Computational Center (RSICC) at Oak Ridge National Laboratory, with an RSICC license, as CCC793. The development of AMP, and the nuclear fuel performance application built upon it, was funded by the Nuclear Energy Advanced Modeling and Simulation (NEAMS) program of the U.S. Department of Energy Office of Nuclear Energy, Advanced Modeling and Simulation Office. This material is also based upon work supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program, Extreme Scale Solvers project (EASIR). Much appreciation is due to Aaron Phillippe, Jim Banfield, and Larry Ott for validation studies, to William Cochran, Srdjan Simunovic, Jay Billings, and Phani Nukala for their early contributions to the software, and to Mark Baird for computational support with the third party libraries. This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725. Mark Berrill acknowledges support from the Eugene P. Wigner Fellowship at Oak Ridge National Laboratory, managed by UTBattelle, LLC, for the U.S. Department of Energy under Contract DE-AC0500OR22725. References [1] http://energy.gov/sites/prod/files/styles/borealis photo gallery large respondxl2/public/fuel assembly for production of nuclear power.jpg. [2] G. A. Berna, C. E. Beyer, K. L. Davis, D. D. Lanning, "FRAPCON-3: A computer code for the calculation of steady-state, thermal-mechanical behavior of oxide fuel rods for high burnup", Tech. Rep. NUREG/CR6534, Pacific Northwest National Laboratory (1997). 46

[3] W. F. Lyon, M. N. Jahingir, R. O. Montgomery, "Fuel analysis and licensing code: FALCON MOD01: Volume 3: Verification and validation", Tech. Rep. 1011309, EPRI, Palo Alto, CA (2004). [4] C. Newman, G. Hansen, D. Gaston, "Three dimensional coupled simulation of thermomechanics, heat, and oxygen diffusion in UO2 nuclear fuel rods", Journal of Nuclear Materials. [5] G. Thouyenin, B. Michal, J. Sercombe, D. Planca, "Multidimensional modeling of a ramp test with the PWR fuel performance code ALCYONE", in: Proceedings of Top Fuel 2007, San Francisco, CA, 2007. [6] G. Thouyenin, J. M. Ricaud, D. Planca, P. Thevenin, "ALCYONE: the Pleiades fuel performance code dedicated to multidimensional PWR studies", in: Proceedings of Top Fuel 2006, Salamanaca, Spain, 2006. [7] A. Marino, G. Demarco, D. Brasnarof, F. Florido, "A 3D behavior modeling for design and performance analysis of LWR fuels", in: Proceedings of Top Fuel 2007, San Francisco, CA, 2007. [8] D. A. Knoll, D. E. Keyes, "Jacobian-free Newton-Krylov methods: a survey of approaches and applications", J. Comput. Phys. 193 (2004) 357­397. [9] R. S. Dembo, S. C. Eisenstat, T. Steihaug, "Inexact Newton methods", SIAM J. Numer. Anal. 19 (1982) 400­408. [10] S. C. Eisenstat, H. F. Walker, "Globally convergent inexact Newton methods", SIAM J. Optimization 4 (1994) 393­422. [11] C. T. Kelley, "Iterative methods for linear and nonlinear equations", SIAM, Philadelphia, 1995. [12] Y. Saad, M. Schultz, "GMRES: a generalized minimal residual algorithm for solving non-symetric linear systems", SIAM J. Sci. Stat. Comput. 7 (1986) 856­869. [13] P. McHugh, D. Knoll, "Comparison of standard and matrix-free implementations of several Newton-Krylov solvers", AIAA Journal 32 (12) (1994) 2394 ­ 2400.

47

[14] D. Knoll, P. McHugh, "Enhanced nonlinear iterative techniques applied to a nonequilibrium plasma flow", SIAM Journal on Scientific Computing 19 (1) (1998) 291 ­ 301. [15] K. T. Clarno, B. Philip, W. K. Cochran, R. S. Sampath, S. Allu, P. Barai, S. Simunovic, L. J. Ott, S. Pannala, P. Nukala, G. A. Dilts, B. Mihaila, C. Unal, G. Yesilyurt, J. H. Lee, J. E. Banfield, G. I. Maldonado, "The AMP (Advanced Multi-Physics) nuclear fuel performance code", Tech. Rep. ORNL/TM-2011/42, Oak Ridge National Laboratory (2011). [16] B. S. Kirk, J. W. Peterson, R. H. Stogner, G. F. Carey, libMesh: A C++ Library for Parallel Adaptive Mesh Refinement/Coarsening Simulations, Engineering with Computers 22 (3­4) (2006) 237­254, http://dx.doi. org/10.1007/s00366-006-0049-3. [17] http://trilinos.sandia.gov/packages/stk. [18] M. Heroux, R. Bartlett, V. H. R. Hoekstra, J. Hu, T. Kolda, R. Lehoucq, K. Long, R. Pawlowski, E. Phipps, A. Salinger, H. Thornquist, R. Tuminaro, J. Willenbring, A. Williams, An Overview of Trilinos, Tech. Rep. SAND2003-2927, Sandia National Laboratories (2003). [19] S. Balay, K. Buschelman, W. D. Gropp, D. Kaushik, M. G. Knepley, L. C. McInnes, B. F. Smith, H. Zhang, "PETSc home page", http: //www.mcs.anl.gov/petsc (2001). [20] https://computation.llnl.gov/casc/sundials/main.html. [21] http://www.oecd-nea.org/science/wprs/fuel/ifpelst.html. [22] http://www.oecd-nea.org/tools/abstract/detail/nea-1488. [23] http://www.oecd-nea.org/tools/abstract/detail/nea-1772. [24] A. Schubert, P. V. Uffelen, J. V. de Laar, C. Walker, W. Haeck, Extension of the transuranus burn-up model, Journal of Nuclear Materials 376 (2008) 1­10. [25] A. Phillippe, "A validation study of the AMP nuclear fuel performance code", Master's thesis, University of Tennessee-Knoxville (2012). 48

[26] A. M. Phillippe, K. T. Clarno, J. E. Banfield, L. J. Ott, B. Philip, M. A. Berrill, R. S. Sampath, S. Allu, S. P. Hamilton, A validation study of pin heat transfer for UO2 fuel based on the IFA-432 experiments, Nuclear Science and Engineering 177 (2014) 000­000. [27] A. M. Phillippe, K. T. Clarno, J. E. Banfield, L. J. Ott, B. Philip, M. A. Berrill, R. S. Sampath, S. Allu, S. P. Hamilton, A validation study of pin heat transfer for MOX fuel based on the IFA-597 experiments, Nuclear Science and Engineering 178 (2014) 171­200. [28] H. Childs, E. S. Brugger, K. S. Bonnell, J. S. Meredith, M. Miller, B. J. Whitlock, N. Max, A contract-based system for large data visualization, in: Proceedings of IEEE Visualization 2005, 2005, pp. 190­198. [29] T. Evans, A. Stafford, R. Slaybaugh, K. Clarno, DENOVO: A new three-dimensional parallel discrete ordinates code in SCALE, Nuclear Technology 171 (2010) 171­200. [30] K. T. Clarno, B. Philip, W. K. Cochran, R. S. Sampath, S. Allu, P. Barai, S. Simunovic, M. A. Berrill, L. J. Ott, S. Pannala, G. A. Dilts, B. Mihaila, G. Yesilyurt, J. H. Lee, J. E. Banfield, The AMP (Advanced MultiPhysics) nuclear fuel performance code, Nuclear Engineering and Design 252 (2012) 108­120. [31] S. Hamilton, K. Clarno, B. Philip, M. Berrill, R. Sampath, S. Allu, Integrated radiation transport and nuclear fuel performance for assemblylevel simulations, in: PHYSOR 2012: Advanced in Reactor Physics, Knoxville, TN, USA, 2012. [32] S. Hamilton, K. Clarno, M. Berrill, T. Evans, R. Sampath, Multiphysics simulations for LWR analysis, in: International Conference on Mathematics and Computational Methods Applied to Nuclear Science & Engineering (M&C 2013), Sun Valley, ID, USA, 2013. [33] S. Palmtag, Coupled single assembly solution with VERA (problem 6), Tech. Rep. CASL-U-2013-0150-000, Consortium for Advanced Simulation of LWR's (2013). [34] SCALE: A comprehensive modeling and simulation suite for nuclear safety analysis and design, Tech. Rep. ORNL/TM-2005/39, Version 6.1, Oak Ridge National Laboratory, Oak Ridge, TN (2011). 49

[35] G. W. Stewart, A Krylov­Schur algorithm for large eigenproblems, SIAM Journal on Matrix Analysis and Applications 23 (3) (2001) 601­ 614. [36] D. R. Rector, C. L. Wheeler, N. J. Lombardo, Cobra-sfs (spent fuel storage): A thermal-hydraulic analysis computer code: Volume 1, mathematical models and solution method, Tech. rep., PNL-6049-Vol.1 (1986). [37] E. E. Lewis, W. F. Miller, Jr., Computational Methods of Neutron Transport, American Nuclear Society, Inc., La Grange Park, Illinois, USA, 1993. [38] J. Jarrell, T. Evans, G. Davidson, A. Godfrey, Full core reactor analysis: Running Denovo on Jaguar, Nuclear Science and Engineering 175 (3) (2013) 283­291. [39] J. V. Cuthcart, et. al., Zirconium metal-water oxidation kinetics iv. reaction rate studies, Tech. rep., ORNL/NUREG-1 (1977). [40] R. Perkins, The diffusion of oxygen in oxygen stabilized alpha-zirconium and zircaloy-4, Journal of Nuclear Materials 73 (1978) 20­29. Appendix A. Coolant Modeling: The coolant liquid serves as a heat sink which flows axially along the length of the outside of the clad. Therefore simulating the coolant flow serves as a boundary condition for the thermal solve on the clad surface. Solving the coolant flow involves solving the fluid equations for conservation of mass, momentum, and energy equations:  +  · (v ) = 0 t vi = - · (vi v ) + (-p +  ·  ) - g t U +  · (U v ) = -p · v +  + k (T )T + q  t (A.1) (A.2) (A.3)

where  is the mass density, v is the velocity, p is the pressure, g is the force exerted by gravity,  is the viscosity tensor, U is the internal energy density,  is the dissipation function, k is the thermal conductivity, and q  is the thermal 50

source. The computational resources needed for a full 3D flow calculations are significant and are usually not necessary for an accurate calculation of the thermal solution within the pellets and clad. This allows us to use a two-equation approximation in which we assume that the coolant flow is only in the z-direction and neglect thermal diffusion between the channels. Assuming steady-state this reduces to: v =0 (A.4) z v 2 p + +g =0 (A.5) z z v  U v T = -p +  k (T ) +q  (A.6) z z z z We have two different models for solving these equations that can be used interchangeably. The first model described in Appendix A.1 uses a reduced empirical model that solves a single axial equation using a simple finite difference scheme and is frequently sufficient for many calculations within the nuclear engineering community. The second model described in Appendix A.2 is a more complex two equation model that is used when subchannel temperatures and densities are required. Appendix A.1. Single EquationFlow A standard reduced uniaxial model based on conservation of energy in the coolant (Equation A.7) with a given mass flux (G) and a specific heat capacity at constant pressure (Cp ) is employed. dTf + hf Tf = hf Tc , (A.7) GCp dz where · is the integral of · over the heated perimeter of the outer surface of the clad and Tf is the bulk coolant temperature. The conservation of coolant energy is solved on a 1D domain using a finite difference scheme. This 1D domain is divided into N equidistant grid points leading to a set of coupled equations with the i-th equation given by 4hf (z ) i- 1 i Tf - Tf + (T i - < Tci >)dz = 0 (A.8) i Cp (Tf )GDe f where hf is the Dittus-Boelter [2] film conductance given by hf = (0.023k/De )Re0.8 P r0.4 with Re the Reynolds number and P r the Prandtl number. 51 (A.9)

Appendix A.2. Subchannel Equations In this section we describe the model that approximates the distribution of flow, pressure, and temperature within a channel (the space between adjacent fuel rods) as uniaxial in the vertical direction and utilizes empirically-derived friction factors to account for the additional complexities. In our subchannel model we solve the 1D set of 2 equations described in section Appendix A. Solving these equations require two independent variables per grid point and we choose enthalpy and pressure. Note that the internal energy density is related to the enthalpy density through U = h - p. v p + v +g =0 z z hv p  T =v + k (T ) z z z z

(A.10) +q  (A.11)

At each axial layer, a simplified model where the crossflow terms are neglected thus eliminating the conservation of mass and lateral momentum equations is employed. This results in conservation of energy and axial momentum equations with specific enthalpy and pressure as variables using complex material models for the temperature, specific enthalpy, density, specific heat capacity, thermal conductivity, viscosity, and surface tension [36]. The following finite difference form of conservation of energy and axial momentum equation are given: m(hi,j + - hi,j - ) - zj + zj
k  K ( i)

(1 + i,r )Prheat i,r qc,j
r  R ( i)

t  wk,j (h i,j - hn,j ) + zj kK (i)

Ck,j sk (Ti,j - Tn,j ) = 0. (A.12)

m(ui,j + - ui,j - ) + Ai (pi,j + - pi,j - ) + + 1 2Ai

gAi zj cos   i,j
t wk,j (ui,j - un,j ) = 0. kK (i)

zj fi,j  + i,j |m| mi,j + C t zj Di

(A.13) 52

The heat flux qc,j is computed using a convective heat transfer coefficient hconv and the temperature difference between the clad surface and the i,j temperature of the flow in the center of the subchannel:
j qc,j = hconv i,j (< Tc > -Ti,j ).

(A.14)

The convective heat transfer coefficient is related to the Nusselt number N ui,j : hconv i,j Di N ui,j = (A.15) ki,j The Nusselt number will vary depending on the turbulence of the flow, so different models were developed for laminar and turbulent flow. To avoid convergence issues due to the discontinuity between the models, the effective heat transfer coefficient is taken as the maximum of the laminar heat transfer coefficient hi,j and the turbulent heat transfer coefficient ht i,j : hconv = max(hi,j , ht i,j i,j ), (A.16)

where the laminar heat transfer coefficient is evaluated with N ui,j = 8.0: hi,j = 8.0 ki,j , Di (A.17)

and the turbulent heat transfer coefficient is calculated as
0.8 0 .4 ht i,j = 0.023Rei,j P ri,j

ki,j Di

,

(A.18)

which uses the well-known Dittus-Boelter correlation for Nusselt number [A.9]: 0.8 0.4 N ui,j = 0.023Rei,j P ri,j , (A.19) where the Reynolds number and Prandtl number have their usual definitions: Rei,j cp i,j ui,j Di i,j µi,j = , P ri,j = µi,j ki,j (A.20)

As boundary conditions, axial inlet mass flow rates, inlet temperature and outlet pressure are selected.

53

Appendix B. Radiation Transport Model Nuclear fuel simulation requires a heat source and a heat sink. The heat sink is approximated with a closed-channel coolant flow model and the heat source is generated within an operating nuclear reactor primarily as a result of neutron-induced fission in fuel (primarily Uranium-235 in most reactors). The distribution of neutrons distribution requires the solution to the Boltzmann transport equation [37]. This is most often modeled using the k -eigenvalue form of the Boltzmann transport equation given by ^ ·  ( ^ , E ) +  (E ) ( ^ , E) 


=
0

dE
4 

^ s ( ^  ^ , E  E ) ( ^ ,E ) d dE
0 4

(B.1)

1 + (E ) k

^ f (E ) ( ^ ,E ) , d

^ is the direction of particle travel; E is the particle energy;  is the where  angular flux distribution; k is the multiplication factor of the system;  , s , and f are the total, scattering, and fission cross sections, respectively;  is the fission energy spectrum; and  is the number of neutrons produced per fission. For a given flux distribution, the power distribution can be written as  ^ f (E ) ( ^ ,E ) , P = dE d (B.2)
0 4

where  is the energy release per fission event. The Denovo radiation transport code [29] offers a variety of spatial discretizations solving the discrete ordinates [37] form of Eq. B.1 in parallel on Cartesian meshes. Denovo has demonstrated excellent scalability to high performance computing environments [38]. Nuclear data is generated using the XSProc module of the SCALE package [34]. Two approaches are available for transferring the power distribution from Denovo to AMP are possible. The first is a direct point-wise mapping of the Denovo solution onto the Gauss points of the AMP finite element basis functions. Interpolation to a point within Denovo can be piecewise constant, linear, or tri-linear depending on the spatial discretization used. The second approach uses a polynomial expansion of the power distribution within each cylindrical fuel rod (Zernike polynomials in the x - y plane and Legendre polynomials in the axial direction) to smooth out artifacts of the Denovo 54

Cartesian mesh, allowing a coarser spatial mesh to be used [31]. In both cases, conservation of the globally integrated power is enforced by normalizing the distribution before and after mapping the power onto the AMP mesh. Appendix C. Oxide Model: The formation of an oxide layer on the surface of a nuclear fuel rod can interfere with thermal conduction to the coolant and create additional stress within the clad. As shown in Figure C.16, the material regions of interest can be divided into 4 regions, a coolant region which is the source of the oxygen for oxide growth, the oxide layer itself which consists of Zirconium oxide, an oxygen rich alpha phase, and a normal beta phase region [39].

Coolant

Oxide

alpha

beta

Figure C.16: Sample layers for oxide growth

Within each layer the oxygen concentration is governed by thermal diffusion. Since the thickness of the oxide and alpha layers is much smaller than the other physical scales in the fuel rod, the oxide growth is modeled at each point, x, on the surface of the clad by independent 1D models:  C (x, t) C (x, t) = D ( x) t x x (C.1)

Solving for the oxygen concentration then reduces to solving in each region subject to the appropriate boundary conditions. Note that the boundaries between the different phases move as the different layers grow. Oxide layer growth: The oxide layers are moving domains in which the growth of each layer is given by the difference between the oxygen flux between the layers. Associated with each interface is the associated velocity of that interface. For example, the velocity of the oxide-alpha layer vox, is given by: vox, = Jox (xox, ) - J (xox, ) Cox, - C,ox 55 (C.2)

where Jox (xox, ) and J (xox, ) are the flux of oxygen in the oxide and  layers evaluated at the oxide interface and Cox, and C,ox are the oxygen concentrations at the boundaries of the oxide-alpha interface in the oxide and alpha layers. Boundary conditions: The boundary conditions between the different layers are relatively simple. At each interface, the oxygen concentration can be evaluated using the equilibrium value for the given phase. For example, at the oxide-coolant interface, the oxygen concentration is assumed to be 1.511 g/cm3 , derived from stoichiometry. The oxygen concentration in the oxide at the oxide-alpha interface is modeled by C = 1.517 - 7.5  10-5  T [39]. The oxygen concentration in the alpha layer at the alpha-oxide interface is a fixed 29% (atomic density) or 0.45 37 g/cm3 [40]. The oxygen concentration in the alpha layer at the alphabeta interface is calculated using the equilibrium concentration. This can be expressed as [40]: C = -0.2263 + 0.0649  C=0 T - 16.877 63.385 T >= 1123 K otherwise

Discretization: Since we are primarily conserned with the size of the oxide layers, we choose to solve the differential equation using finite difference in a moving frame.  dC (x, t) C (x, t) C (x, t) = (C.3) D ( x) +v· dt x x x We divide the space into N uniformly spaced regions of size h = (xN - x0 )/N . Since each zone is moving at a velocity vi , we can apply the convective derivative to get the diffusion equation in this moving frame. We can follow the boundaries of the oxide layer by choosing the velocity at the boundaries to match the oxide growth rate. For example consider the layers shown in Figure C.17. It's left boundary is moving at a velocity of v1 and it's right boundary is moving at a velocity of v2 . We want to use a conservative scheme with upwinding for the convective term. Assuming v1+1/2  0: 1 Di+1 Ci+ 3 - Ci+ 1 - Di Ci+ 1 - Ci- 1 2 2 2 2 dt h2 1 i+ 2 (vN - v0 ) vi+ 1 = v0 + 2 N
2

dCi+ 1

=

+

vi+ 1 h

2

C i+ 3 - C i + 1
2

2

56

Ci - 1
2

Ci + 1 vi

2

C i+ 3 vi+1

2

v0

vN

Figure C.17: Sample layers for numerical form

Rewriting: dC = F (C, v, h) dt 1 F (C, v, h) = 2 Di+1 Ci+ 3 - Ci+ 1 - Di Ci+ 1 - Ci- 1 2 2 2 2 h We can then apply the Crank-Nicholson method: C n+1 - C n 1 = F C n+1 , v n+1 , hn+1 + F (C n , v n , hn ) t 2 +1 +1 hn+1 = xn - xn /N 0 N
+1 n n+1 n xn = xn - v0 t2 0 0 + v0 t + 0.5 v0 +1 n+1 n n xn = xn - vN t2 N + vN t + 0.5 vN N

+

vi+ 1 h

2

C i+ 3 - C i+ 1
2 2

If we assume v n+1 is known (it is actually calculated from C n+1 ), then the system becomes a standard linear system of equations. The resulting matrix is banded and can be solved through direct solves using LAPACK. Time-Step Control: Careful control of the time step is necessary to produce an accurate answer. A first limitation of the time step is the calculation of v n+1 . While it is possible to create a non-linear system that solves for v n+1 , the other time step requirements will make this work unnecessary. Instead we will assume v n+1  v n . With this assumption we need a time step that will ensure the change in v is small. The second restriction comes from the convective term. To ensure the proper error, we need to limit the matrix h . norm (for this term) to  1. This gives us the condition t  v i 57

The IceProd Framework: Distributed Data Processing for the IceCube Neutrino Observatory
M. G. Aartsenb , R. Abbasiac , M. Ackermannat, J. Adamso , J. A. Aguilarw, M. Ahlersac , D. Altmannv , C. Arguellesac, J. Auffenbergac, X. Baiah,1 , M. Bakerac , S. W. Barwicky , V. Baumae , R. Bayg , J. J. Beattyq,r , J. Becker Tjusj , K.-H. Beckeras , S. BenZviac , P. Berghausat, D. Berleyp , E. Bernardiniat, A. Bernhardag, D. Z. Bessonaa , G. Binderh,g , D. Bindigas, M. Bissoka , E. Blaufussp , J. Blumenthala , D. J. Boersmaar , C. Bohmak , D. Boseam , S. B¨ oserk , ar m at o z e m ac O. Botner , L. Brayeur , H.-P. Bretz , A. M. Brown , R. Bruijn , J. Casey , M. Casier , D. Chirkin , A. Christovw , B. Christyp , K. Clarkan , L. Classenv , F. Clevermannt, S. Coendersa , S. Cohenz , D. F. Cowenaq,ap, A. H. Cruz Silvaat , M. Danningerak, J. Daughheteee, J. C. Davisq , M. Dayac , C. De Clercqm , S. De Ridderx , P. Desiatiac,, K. D. de Vriesm , M. de Withi , T. DeYoungaq, J. C. D´ iaz-V´ elezac,, M. Dunkmanaq, R. Eaganaq, B. Eberhardtae, j ac a ah B. Eichmann , J. Eisch , S. Euler , P. A. Evenson , O. Fadiranac,, A. R. Fazelyf , A. Fedynitchj, J. Feintzeigac, T. Feuselsx , K. Filimonovg, C. Finleyak , T. Fischer-Waselsas , S. Flisak , A. Franckowiakk, K. Frantzent , T. Fuchst , T. K. Gaisserah , J. Gallagherab, L. Gerhardth,g, L. Gladstoneac, T. Gl¨ usenkampat, A. Goldschmidth, G. Golupm , ah p v u J. G. Gonzalez , J. A. Goodman , D. G´ ora , D. T. Grandmont , D. Grantu , P. Gretskova, J. C. Grohaq, A. Großag , h,g x a C. Ha , A. Haj Ismail , P. Hallen , A. Hallgrenar, F. Halzenac , K. Hansonl , D. Hebeckerk, D. Heeremanl, D. Heinena , K. Helbingas , R. Hellauerp , S. Hickfordo, G. C. Hillb , K. D. Hoffmanp, R. Hoffmannas, A. Homeierk , K. Hoshinaac , F. Huangaq , W. Huelsnitzp , P. O. Hulthak , K. Hultqvistak , S. Hussainah , A. Ishiharan , E. Jacobiat , J. Jacobsenac , K. Jagielskia , G. S. Japaridzed , K. Jeroac , O. Jlelatix , B. Kaminskyat, A. Kappesv , T. Kargat, A. Karleac , M. Kauerac , J. L. Kelleyac, J. Kirylukal, J. Kl¨ asas , S. R. Kleinh,g , J.-H. K¨ ohnet , G. Kohnenaf, H. Kolanoskii , ae ac as s k L. K¨ opke , C. Kopper , S. Kopper , D. J. Koskinen , M. Kowalski , M. Krasbergac, A. Kriestena , K. Kringsa , G. Krollae , J. Kunnenm, N. Kurahashiac, T. Kuwabaraah, M. Labarex, H. Landsmanac, M. J. Larsonao, M. Lesiak-Bzdakal, M. Leuermanna, J. Leuteag , J. L¨ unemannae, O. Mac´ iaso , J. Madsenaj , G. Maggim , ac n h ac p R. Maruyama , K. Mase , H. S. Matis , F. McNally , K. Meagher , M. Merckac , G. Merinoac , T. Meuresl , S. Miareckih,g , E. Middellat , N. Milket , J. Millerm , L. Mohrmannat, T. Montaruliw,2, R. Morseac , R. Nahnhauerat, U. Naumannas, H. Niederhausenal, S. C. Nowickiu , D. R. Nygrenh , A. Obertackeas, S. Odrowskiu, A. Olivasp , A. Omairatas, A. O'Murchadhal, L. Paula , J. A. Pepperao, C. P´ erez de los Herosar , C. Pfendnerq, D. Pielotht , E. Pinatl , as g h aq J. Posselt , P. B. Price , G. T. Przybylski , M. Quinnan , L. R¨ adela , I. Raead,, M. Rameezw , K. Rawlinsc , P. Redlp , a ag t z R. Reimann , E. Resconi , W. Rhode , M. Ribordy , M. Richmanp , B. Riedelac , J. P. Rodriguesac, C. Rottam , T. Ruhet , B. Ruzybayevah, D. Ryckboschx, S. M. Sabaj , H.-G. Sanderae, M. Santanderac, S. Sarkars,ai, K. Schattoae, F. Scheriaut , T. Schmidtp , M. Schmitzt , S. Schoenena, S. Sch¨ onebergj, A. Sch¨ onwaldat, A. Schukrafta, L. Schultek , ac, ag ah ag aj D. Schultz , O. Schulz , D. Seckel , Y. Sestayo , S. Seunarine , R. Shanidzeat , C. Sherematau, M. W. E. Smithaq , D. Soldinas , G. M. Spiczakaj , C. Spieringat , M. Stamatikosq,3 , T. Stanevah , N. A. Stanishaaq , A. Stasikk , T. Stezelbergerh, R. G. Stokstadh , A. St¨ oßlat , E. A. Strahlerm , R. Str¨ omar , N. L. Strotjohannk, G. W. Sullivanp , ar e ah as H. Taavola , I. Taboada , A. Tamburro , A. Tepe , S. Ter-Antonyanf, G. Te si´ caq , S. Tilavah , P. A. Toaleao , ac ac v j k M. N. Tobin , S. Toscano , M. Tselengidou , E. Unger , M. Usner , S. Vallecorsaw , N. van Eijndhovenm, A. Van Overloopx, J. van Santenac , M. Vehringa, M. Vogek , M. Vraeghex , C. Walckak , T. Waldenmaieri , M. Wallraffa , Ch. Weaverac , M. Wellonsac , C. Wendtac , S. Westerhoffac , N. Whitehornac, K. Wiebeae , C. H. Wiebuscha , D. R. Williamsao , H. Wissingp , M. Wolfak , T. R. Woodu , K. Woschnaggg , D. L. Xuao , X. W. Xuf , J. P. Yanezat , G. Yodhy , S. Yoshidan , P. Zarzhitskyao, J. Ziemannt, S. Zierkea, M. Zollak
a III. b School

arXiv:1311.5904v3 [cs.DC] 22 Aug 2014

Physikalisches Institut, RWTH Aachen University, D-52056 Aachen, Germany of Chemistry & Physics, University of Adelaide, Adelaide SA, 5005 Australia

author corresponding author Email addresses: desiati@icecube.wisc.edu (P. Desiati), juancarlos@wipac.wisc.edu (J. C. D´ iaz-V´ elez), ofadiran@icecube.wisc.edu (O. Fadiran), ian@cs.wisc.edu (I. Rae), dschultz@icecube.wisc.edu (D. Schultz) 1 Physics Department, South Dakota School of Mines and Technology, Rapid City, SD 57701, USA 2 also Sezione INFN, Dipartimento di Fisica, I-70126, Bari, Italy 3 NASA Goddard Space Flight Center, Greenbelt, MD 20771, USA Preprint submitted to Journal of Parallel and Distributed Computing
 Principal

 Corresponding

August 26, 2014

of Physics and Astronomy, University of Alaska Anchorage, 3211 Providence Dr., Anchorage, AK 99508, USA d CTSPS, Clark-Atlanta University, Atlanta, GA 30314, USA e School of Physics and Center for Relativistic Astrophysics, Georgia Institute of Technology, Atlanta, GA 30332, USA f Dept. of Physics, Southern University, Baton Rouge, LA 70813, USA g Dept. of Physics, University of California, Berkeley, CA 94720, USA h Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA i Institut f¨ ur Physik, Humboldt-Universit¨ at zu Berlin, D-12489 Berlin, Germany j Fakult¨ at f¨ ur Physik & Astronomie, Ruhr-Universit¨ at Bochum, D-44780 Bochum, Germany k Physikalisches Institut, Universit¨ at Bonn, Nussallee 12, D-53115 Bonn, Germany l Universit´ e Libre de Bruxelles, Science Faculty CP230, B-1050 Brussels, Belgium m Vrije Universiteit Brussel, Dienst ELEM, B-1050 Brussels, Belgium n Dept. of Physics, Chiba University, Chiba 263-8522, Japan o Dept. of Physics and Astronomy, University of Canterbury, Private Bag 4800, Christchurch, New Zealand p Dept. of Physics, University of Maryland, College Park, MD 20742, USA q Dept. of Physics and Center for Cosmology and Astro-Particle Physics, Ohio State University, Columbus, OH 43210, USA r Dept. of Astronomy, Ohio State University, Columbus, OH 43210, USA s Niels Bohr Institute, University of Copenhagen, DK-2100 Copenhagen, Denmark t Dept. of Physics, TU Dortmund University, D-44221 Dortmund, Germany u Dept. of Physics, University of Alberta, Edmonton, Alberta, Canada T6G 2E1 v Erlangen Centre for Astroparticle Physics, Friedrich-Alexander-Universit¨ at Erlangen-N¨ urnberg, D-91058 Erlangen, Germany w D´ epartement de physique nucl´ eaire et corpusculaire, Universit´ e de Gen` eve, CH-1211 Gen` eve, Switzerland x Dept. of Physics and Astronomy, University of Gent, B-9000 Gent, Belgium y Dept. of Physics and Astronomy, University of California, Irvine, CA 92697, USA z Laboratory for High Energy Physics, Ecole ´ Polytechnique F´ ed´ erale, CH-1015 Lausanne, Switzerland aa Dept. of Physics and Astronomy, University of Kansas, Lawrence, KS 66045, USA ab Dept. of Astronomy, University of Wisconsin, Madison, WI 53706, USA ac Dept. of Physics and Wisconsin IceCube Particle Astrophysics Center, University of Wisconsin, Madison, WI 53706, USA ad Dept. of Computer Science, University of Wisconsin, Madison, WI 53706, USA ae Institute of Physics, University of Mainz, Staudinger Weg 7, D-55099 Mainz, Germany af Universit´ e de Mons, 7000 Mons, Belgium ag T.U. Munich, D-85748 Garching, Germany ah Bartol Research Institute and Dept. of Physics and Astronomy, University of Delaware, Newark, DE 19716, USA ai Dept. of Physics, University of Oxford, 1 Keble Road, Oxford OX1 3NP, UK aj Dept. of Physics, University of Wisconsin, River Falls, WI 54022, USA ak Oskar Klein Centre and Dept. of Physics, Stockholm University, SE-10691 Stockholm, Sweden al Dept. of Physics and Astronomy, Stony Brook University, Stony Brook, NY 11794-3800, USA am Dept. of Physics, Sungkyunkwan University, Suwon 440-746, Korea an Dept. of Physics, University of Toronto, Toronto, Ontario, Canada, M5S 1A7 ao Dept. of Physics and Astronomy, University of Alabama, Tuscaloosa, AL 35487, USA ap Dept. of Astronomy and Astrophysics, Pennsylvania State University, University Park, PA 16802, USA aq Dept. of Physics, Pennsylvania State University, University Park, PA 16802, USA ar Dept. of Physics and Astronomy, Uppsala University, Box 516, S-75120 Uppsala, Sweden as Dept. of Physics, University of Wuppertal, D-42119 Wuppertal, Germany at DESY, D-15735 Zeuthen, Germany

c Dept.

Abstract IceCube is a one-gigaton instrument located at the geographic South Pole, designed to detect cosmic neutrinos, identify the particle nature of dark matter, and study high-energy neutrinos themselves. Simulation of the IceCube detector and processing of data require a significant amount of computational resources. This paper presents the first detailed description of IceProd, a lightweight distributed management system designed to meet these requirements. It is driven by a central database in order to manage mass production of simulations and analysis of data produced by the IceCube detector. IceProd runs as a separate layer on top of other middleware and can take advantage of a variety of computing resources, including grids and batch systems such as CREAM, HTCondor, and PBS. This is accomplished by a set of dedicated daemons that process job submission in a coordinated fashion through the use of middleware plugins that serve to abstract the details of job submission and job management from the framework. Keywords: Data Management, Grid Computing, Monitoring, Distributed Computing 2

1. Introduction Large experimental collaborations often need to produce extensive volumes of computationally intensive Monte Carlo simulations and process vast amounts of data. These tasks are usually farmed out to large computing clusters or grids. For such large datasets, it is important to be able to document details associated with each task, such as software versions and parameters like the pseudo-random number generator seeds used for each dataset. Individual members of such collaborations might have access to modest computational resources that need to be coordinated for production. Such computational resources could also potentially be pooled in order to provide a single, more powerful, and more productive system that can be used by the entire collaboration. This article describes the design of a software package meant to address all of these concerns. It provides a simple way to coordinate processing and storage of large datasets by integrating grids and small clusters. 1.1. The IceCube Detector The IceCube detector shown in Figure 1 is located at the geographic South Pole and was completed at the end of 2010 [1, 2]. It consists of 5160 optical sensors buried between 1450 and 2450 meters below the surface of the South Pole ice sheet and is designed to detect interactions of neutrinos of astrophysical origin [1]. However, it is also sensitive to downward-going highly energetic muons and neutrinos produced in cosmic-rayinduced air showers. IceCube records 1010 cosmic-ray events per year. The cosmic-ray-induced muons outnumber neutrino-induced events (including ones from atmospheric origin) by about 500,000:1. They represent a background for most IceCube analyses and are filtered prior to transfer to the data processing center in the Northern Hemisphere. Filtering at the data collection source is required because of bandwidth limitations on the satellite connection between the detector and the processing location [3]. About 100 GB of data from the IceCube detector is transferred to the main data storage facility daily. In order to facilitate record keeping, the data is divided into runs, and each run is further subdivided into multiple files. The size of each file is dictated by what is considered optimal for storage and access. Each run typically consists of hundreds of files, resulting in 400,000 files for each year of detector operation. Once the data has been transferred, additional, more computationally-intensive event reconstructions are performed and the data is filtered to select events for various analyses. The computing requirements for 3

the various levels of data processing are shown in Table 1. In order to develop event reconstructions, perform analyses, and understand systematic uncertainties, physicists require statistics from Monte Carlo simulations that are comparable to the data collected by the detector. This requires thousands of years of CPU processing time as can be seen from Table 2. Table 1: Data processing demands. Data is filtered on 400 cores at the South Pole using loose selection criteria to reduce volume by a factor of 10 before satellite transfer to the Northern Hemisphere (Level1). Once in the North, more computationally intensive event reconstructions are performed in order to further reduce background contamination (Level2). Further event selections are made for each analysis channel (Level3). Each run is equivalent to approximately eight hours of detector livetime and the processing time is based on a 2.8 GHz core. Filter Level1 Level2 Level3 Processing time/run 2400 h 9500 h 15 h Total per year 2.6 × 106 h 1.0 × 107 h 1.6 × 104 h

Table 2: Runtime of various Monte Carlo simulations of background cosmic-ray shower events and neutrino signal with different energy distributions. The median energy is based on the distribution of events that trigger the detector. The number of events reflects the typical per-year requirements for IceCube analyses. Simulation Air showers Neutrinos Neutrinos Med. Energy1 1.2 × 104 GeV 3.9 × 106 GeV 8.1 × 101 GeV t/event 5 ms 316 ms 53 ms events 1014 108 109

1.2. IceCube Computing Resources The IceCube collaboration is comprised of 43 research institutions from Europe, North America, Japan, Australia, and New Zealand. Members of the collaboration have access to 25 different computing clusters and grids in Europe, Japan, Canada and the U.S. These range from small computer farms of 30 nodes to large grids, such as the European Grid Infrastructure (EGI), Swedish Grid Initiative (SweGrid), Canada's WestGrid and the Open Science Grid (OSG), that may each have
11

GeV = 109 electronvolts (unit of energy)

Figure 1: The IceCube detector: the dotted lines at the bottom represent the instrumented portion of the ice. The circles on the top surface represent IceTop, a surface air-shower subdetector.

4

thousands of computing nodes. The total number of nodes available to IceCube member institutions varies with time since much of our use is opportunistic and availability depends on the usage by other projects and experiments. In total, IceCube simulation has run on more than 11,000 distinct multicore computing nodes. On average, IceCube simulation production has run concurrently on 4, 000 cores at any given time since deployment, and it is anticipated to run on 5, 000 cores simultaneously during upcoming productions. 2. IceProd The IceProd framework is a software package developed for IceCube with the goal of managing productions across distributed systems and pooling together isolated computing resources that are scattered across member institutions of the Collaboration and beyond. It consists of a central database and a set of daemons that are responsible for the management of grid jobs and data handling through the use of existing grid technology and network protocols. IceProd makes job scripting easier and sharing productions more efficient. In many ways it is similar to PANDA Grid, the analysis framework for the PANDA experiment [4], in that both tools are distributed systems based on a central database and an interface to local batch systems. Unlike PANDA Grid which depends heavily on AliEn, the grid middleware for the ALICE experiment [5], and on the ROOT analysis framework [6], IceProd was built in-house with minimal software requirements and is not dependent on any particular middleware or analysis framework. It is designed to run completely in user space with no administrative access, allowing greater flexibility in installation. IceProd also includes a built-in monitoring system with no dependencies on any external tools for this purpose. These properties make IceProd a very lightweight yet powerful tool and give it a greater scope beyond IceCube-specific applications. The software package includes a set of libraries, executables and daemons that communicate with the central database and coordinate to share responsibility for the completion of tasks. The details of job submission and management in different grid environments are abstracted through the use of plugin modules that will be discussed in Section 3.2.1. IceProd can be used to integrate an arbitrary number of sites including clusters and grids. It is, however, not a replacement for other cluster and grid management tools or any other middleware. Instead, it runs on top of 5

these as a separate layer providing additional functionality. IceProd fills a gap between the user or production manager and the powerful middleware and batch system tools available on computing clusters and grids. Many of the existing middleware tools, including Condor-C, Globus and CREAM, make it possible to interface any number of computing clusters into a larger pool. However, most of these tools need to be installed and configured by system administrators and, in some cases, customization for general purpose applications is not feasible. In contrast to most of these applications, IceProd runs at the user level and does not require administrator privileges. This makes it possible for individual users to build large production systems by pooling small computational resources together. Security and data integrity are concerns in any software architecture that depends heavily on communication through the Internet. IceProd includes features aimed at minimizing security and data corruption risks. Security and data integrity are addressed in Section 3.8. The IceProd client provides a graphical user interface (GUI) for configuring simulations and submitting jobs through a "production server." It provides a method for recording all the software versions, physics parameters, system settings, and other steering parameters associated with a job in a central production database. IceProd also includes a web interface for visualization and live monitoring of datasets. Details about the GUI client and a text-based client are discussed in Section 3.5.

3. Design Elements of IceProd The IceProd software package can be logically divided into the following components or software libraries: · iceprod-core--a set of modules and libraries of common use throughout IceProd. · iceprod-server--a collection of daemons and libraries to manage and schedule job submission and monitoring. · iceprod-modules--a collection of predefined classes that provide an interface between IceProd and an arbitrary task to be performed on a computing node, as defined in Section 3.3. · iceprod-client--a client (both graphical and text) that can download, edit, and submit dataset steering files to be processed.

· A database that stores configured parameters, libraries (including version information), job information, and performance statistics. · A web application for monitoring and controlling dataset processing. These components are described in further detail in the following sections. 3.1. IceProd Core Package The iceprod-core package contains modules and libraries common to all other IceProd packages. These include classes and methods for writing and parsing XML files and transporting data. The classes that define job execution on a host are contained in this package. The iceprod-core also includes an interpreter (Section 3.1.3) for a simple scripting language that provides some flexibility for parsing XML steering files. 3.1.1. The JEP One of the complications of operating on heterogeneous systems is the diversity of architectures, operating systems, and compilers. IceProd uses HTCondor's NMI-Metronome build and test system [7] for building the IceCube software on a variety of platforms and storing the built packages on a server. As part of the management of each job, IceProd submits a Job Execution Pilot (JEP) to the cluster/grid queue. This script determines what platform a job is running on and, after contacting the monitoring server, which software package to download and execute. During runtime, the JEP performs status updates through the monitoring server via remote procedure calls using XML-RPC [8]. This information is updated on the database and is displayed on the monitoring web interface. Upon completion, the JEP removes temporary files and directories created for the job. Depending on the configuration, it will also cache a copy of the software used, making it available for future JEPs. When caching is enabled, an MD5 checksum is performed on the cached software and compared to what is stored on the server in order to avoid using corrupted or outdated software. Jobs can fail under many circumstances. These failures include failed submissions due to transient system problems and execution failures due to problems with the execution host. At a higher level, errors specific to IceProd include communication problems with the monitoring daemon or the data repository. In order to account for possible transient errors, the design of IceProd includes a set of states through which a job will transition in order to guarantee successful completion of 6

Submit Start True

WAITING

QUEUEING

ok?

QUEUED

PROCESSING

CLEANING

Max. time reached

False

False

RESET SUSPENDED
requeue

ERROR

ok?
True

False

True

OK

ok?

CLEANING

COPIED

Move data to disk

Figure 2: State diagram for the JEP. Each of the nonerror states through which a job passes includes a configurable timeout. The purpose of this timeout is to account for any communication errors that may have prevented a job from setting its status correctly.

a well-configured job. The state diagram for an IceProd job is depicted in Figure 2. 3.1.2. XML Job Description In the context of this document, a dataset is defined as a collection of jobs that share a basic set of scripts and software but whose input parameters depend on the ID of each individual job. A configuration or steering file describes the tasks to be executed for an entire dataset. IceProd steering files are XML documents with a defined schema. These steering files include information about the specific software versions used for each of the sections, known as trays (a term borrowed from IceTray, the C++ software framework used by the IceCube Collaboration [9]). An IceProd tray represents an instance of an environment corresponding to a set of libraries and executables and a chain of configurable modules with corresponding parameters and input files needed for the job. In addition, there is a header section for user-defined parameters and expressions that are globally accessible by different modules. 3.1.3. IceProd XML expressions A limited programming language was developed in order to allow more scripting flexibility that depends on runtime parameters such as job ID, run ID, and dataset ID. This lightweight, embedded, domain-specific language (DSL) allows for a single XML job description to be applied to an entire dataset following an SPMD (single process, multiple data) paradigm. It is powerful enough to give some flexibility but sufficiently re-

strictive to limit abuse. Examples of valid expressions include the following: · $args(<var>)--a command line argument passed to the job (such as job ID or dataset ID). · $steering(<var>)--a user defined variable. · $system(<var>)--a system-specific parameter defined by the server. · $eval(<expr>)--a mathematical or logical expression (in Python). · $sprintf(<format>,<list>)--string formatting. · $choice(<list>)--random choice of an element from the list. The evaluation of such expressions is recursive and allows for some complexity. However, there are limitations in place that prevent abuse of this feature. As an example, $eval() statements prohibit such things as loops and import statements that would allow the user to write an entire program within an expression. There is also a limit on the number of recursions in order to prevent closed loops in recursive statements. 3.2. IceProd Server

There are two modes of operation. The first is an unmonitored mode in which jobs are simply sent to the queue of a particular system. This mode provides a tool for scheduling jobs that don't need to be recorded and does not require a database. In the second mode, all parameters are stored in a database that also tracks the progress of each job. The soapqueue daemon running at each of the participating sites periodically queries the database to check if any tasks have been assigned to it. It then downloads the steering configuration and submits a given number of jobs to the cluster or grid where it is running. The number of jobs that IceProd maintains in the queue at each site can be configured individually according to the specifics of each cluster, including the size of the cluster and local queuing policies. Figure 3 is a graphical representation that describes the interrelation of these daemons. The state diagram in Figure 4 illustrates the role of the daemons in dataset submission while Figure 5 illustrates the flow of information through the various protocols.
False True

soaptray

monitor?

Job

monitor?

soapmon

True

Client

Database
True

soapqueue

jobs?

False

The iceprod-server package is comprised of four daemons and their respective libraries: 1. soaptray --an HTTP server that receives client XML-RPC requests for scheduling jobs and steering information which then uploaded to the database. 2. soapqueue--a daemon that queries the database for available tasks to be submitted to a particular cluster or grid. This daemon is also responsible for submitting jobs to the cluster or grid through a set of plugin classes. 3. soapmon--a monitoring HTTP server that receives XML-RPC updates from jobs during execution and performs status updates to the database. 4. soapdh--a data handling/garbage collection daemon that removes temporary files and performs any postprocessing tasks.
1 The prefix soap is used for historical reasons. The original implementation of IceProd relied on SOAP for remote procedure calls. This was replaced by XML-RPC which has better support in Python.

False

True ok?

requeue

Move data to data warehouse

1

Figure 4: State diagram of queuing algorithm. The iceprod-client sends requests to the soaptray server which then loads the information to the database (in production mode) or directly submits jobs to the cluster (in unmonitored mode). The soapqueue daemons periodically query the database for pending requests and handle job submission in the local cluster.

3.2.1. IceProd Server Plugins In order to abstract the process of job submission from the framework for the various types of systems, IceProd defines a Grid base class that provides an interface for queuing jobs. The Grid base class interface includes a set of methods for queuing and removing jobs, performing status checks, and setting attributes such as job priority and maximum allowed wall time and job requirements such as disk space and memory usage. The 7

Figure 3: Network diagram of IceProd system. The IceProd clients and JEPs communicate with iceprod-server modules via XML-RPC. Database calls are restricted to iceprod-server modules. Queueing daemons called soapqueue are installed at each site and periodically query the database for pending job requests. The soapmon server receives monitoring update from the jobs. An instance of soapdh handles garbage collection and any post processing tasks after job completion.
client soaptray soapqueue
batch system submit cmd* (unmonitored) submit jobs MySQL enqueue dataset batch system* submit cmd submit job

batch system

running job

soapmon

soapdh

XML-RPC submit

batch system protocol* schedule job on cluster XML-RPC status update MySQL MySQL status update

XML-RPC check status batch system protocol*/shell remove failed job and clean up files *Condor, PBS, SGE, CREAM, GLite

status update batch system protocol*/shell remove completed job and clean up files

Figure 5: Data flow for job submission, monitoring and removal. Communication between server instances (labeled "soap*") is handled through a database. Client/server communication and monitoring updates are handled via XMLRPC. Interaction with the grid or cluster is handled through a set of plugin modules and depends on the specifics of the system. set of methods defined by this base class include but are 8 not limited to:

· WriteConfig: write protocol-specific submission scripts (i.e., a JDL job description file in the case of CREAM or gLite or a shell script with the appropriate PBS/SGE headers). · Submit: submit jobs and record the job ID in the local queue. · CheckJobStatus: query job status from the queue. · Remove: cancel/abort a job. · CleanQ: remove any orphan jobs that might be left in the queue. The actual implementation of these methods is done by a set of plugin subclasses that launch the corresponding commands or library calls, as the case may be. In the case of PBS and SGE, most of these methods result in the appropriate system calls to qsub, qstat, qdel, etc. For other systems, these can be direct library calls through a Python API. IceProd contains a growing library of plugins, including classes for interfacing with batch systems such as HTCondor, PBS and SGE as well as grid systems like Globus, gLite, EDG, CREAM and ARC. In addition, one can easily implement user-defined plugins for any new type of system that is not included in this list. 3.3. IceProd Modules The iceprod-modules package is a collection of configurable modules with a common interface. These represent the atomic tasks to be performed as part of the job. They are derived from a base class IPModule and provide a standard interface that allows for an arbitrary set of parameters to be configured in the XML document and passed from the IceProd framework. In turn, the module returns a set of statistics in the form of a string-to-float dictionary back to the framework so that it can be recorded in the database and displayed on the monitoring web interface. By default, the base class will report the module's CPU usage, but the user can define any set of values to be reported, such as number of events that pass a given processing filter. IceProd also includes a library of predefined modules for performing common tasks such as file transfers through GridFTP, tarball manipulation, etc. 3.4. External IceProd Modules Included in the library of predefined modules is a special module that has two parameters: class and URL. The first is a string that defines the name of an external IceProd module and the second specifies a URL for a 9

(preferably version-controlled) repository where the external module code can be found. Any other parameters passed to this module are assumed to belong to the referred external module and will be ignored. This allows for the use of user-defined modules without the need to install them at each IceProd site. External modules share the same interface as any other IceProd module. External modules are retrieved and cached by the server at the time of submission. These modules are then included as file dependencies for the jobs, thus preventing the need for jobs to directly access the file code repository. Additional precautions, such as enforcing the use of secure protocols for URLs, must be taken to avoid security risks. 3.5. IceProd Client The iceprod-client package contains two applications for interacting with the server and submitting datasets. One is a PyGTK-based GUI (see Figure 6) and the other is a text-based application that can run as a commandline executable or as an interactive shell. Both of these applications allow the user to download, edit, and submit steering configuration files as well as control datasets running on the IceProd-controlled grid. The graphical interface includes drag and drop features for moving modules around and provides the user with a list of valid parameters for known modules. Information about parameters for external modules is not included since these are not known a priori. The interactive shell also allows the user to perform grid management tasks such as starting and stopping a remote server and adding and removing production sites participating in the processing of a dataset. The user can also perform job-specific actions such as suspension and resetting of jobs. 3.6. Database At the time of this writing, the current implementation of IceProd works exclusively with a MySQL database, but all database calls are handled by a database module that abstracts queries from the framework and could be easily replaced by a different relational database. This section describes the relational structure of the IceProd database. Each dataset is defined by a set of modules and parameters that operate on separate data (single process, multiple data). At the top level of the database structure is the dataset table. The dataset ID is the unique identifier for each dataset, though it is possible to assign a mnemonic string alias. The tables in the IceProd database are logically divided into two distinct classes

Figure 6: The iceprod-client uses pyGtk and provides a graphical user interface to IceProd. It is both a graphical editor of XML steering files and an XML-RPC client for dataset submission. that could in principle be entirely different databases. The first describes a steering file or dataset configuration (items 1­6 and 9 in the list below) and the second is a job-monitoring database (items 7 and 8). The most important tables are described below. 1. dataset: contains a unique identifier as well as attributes to describe and categorize the dataset, including a textual description. 2. steering-parameter: describes general global variables that can be referenced from any module. 3. meta-project: describes a software environment including libraries and executables. 4. tray: describes a grouping of modules that will execute given the same software environment or metaproject. 5. module: specifies an instance of an IceProd Module class. 6. cparameter: contains all the configured parameters associated with a module. 7. job: describes each job in the queue related to a dataset, including the state and host where the job is executed. 10 8. task: keeps track of the state of a task in a way similar to what is done in the jobs table. A task represents a subprocess for a job in a process workflow. More details on this will be provided in Section 4. 9. task-rel: describes the hierarchical relationship between tasks. 3.7. Monitoring The status updates and statistics are reported by the JEP via XML-RPC to soapmon and stored in the database, and provide useful information for monitoring the progress of processing datasets and for detecting errors. The updates include status changes and information about the execution host as well as job statistics. This is a multi-threaded server that can run as a standalone daemon or as a CGI script within a more robust web server. The data collected from each job are made available for analysis, and patterns can be detected with the aid of visualization tools as described in the following section. 3.7.1. Web Interface The current web interface for IceProd was designed to work independently of the IceProd framework but

Figure 7: A screen capture of the web interface that allows the monitoring of ongoing jobs and datasets. The monitoring web interface has a number of views with different levels of detail. The view shown displays the job progress for active jobs within a dataset. The web interface provides authenticated users with buttons to control datasets and individual jobs. utilizes the same database. It is written in PHP and makes use of the CodeIgniter framework [10]. Each of the simulation and data-processing web-monitoring tools provide different views, which include, from top level downward: · general view: displays all datasets filtered by status, type, grid, etc. · grid view: shows all datasets running on a particular site. · dataset view: displays all jobs and accompanying statistics for a given dataset, including every site that it is running on. · job view: shows each individual job, including the status, job statistics, execution host, and possible errors. There are some additional views that are applicable only to the processing of real IceCube detector data: · calendar view: displays a calendar with a color coding that indicates the status of jobs associated with data taken on a particular date. · day view: shows the status of jobs associated with a given calendar day of data taking. 11 · run view: displays the status of jobs associated with a particular detector run. The web interface also provides the functionality to control jobs and datasets by authenticated users. This is done by sending commands to the soaptray daemon using the XML-RPC protocol. Other features of the interface include graphs displaying completion rates, errors and number of jobs in various states. Figure 7 shows a screen capture of one of a number of views from the web interface.

3.7.2. Statistical Data One aspect of IceProd that is not found in most grid middleware is the built-in collection of user-defined statistical data. Each IPModule instance is passed a stringto-float dictionary to which the JEP can add entries or increment a given value. IceProd collects these data in the central database and displays them on the monitoring page. Statistics are reported individually for each job and collectively for the whole dataset as a sum, average and standard deviation. The typical types of information collected on IceCube jobs include CPU usage, number of events meeting predefined physics criteria, and number of calls to a particular module.

3.8. Security and Data Integrity When dealing with network applications, one must always be concerned with security and data integrity in order to avoid compromising privacy and the validity of scientific results. Some effort has been made to minimize security risks in the design and implementation of IceProd. This section will summarize the most significant of these. Figure 3 shows the various types of network communication between the client, server, and worker node. 3.8.1. Authentication Authentication in IceProd can be handled in two ways: IceProd can authenticate dataset submission against an LDAP server or, if one is not available, authentication is handled by means of direct database authentication. LDAP authentication allows the IceProd administrator to restrict usage to individual users that are responsible for job submissions and are accountable for improper use so direct database authentication should be disabled whenever LDAP is available. This setup also precludes the need to distribute database passwords and thus prevents users from being able to directly query the database via a MySQL client. When dealing with databases, one also needs to be concerned about allowing direct access to the database and passing login credentials to jobs running on remote sites. For this reason, all monitoring calls are done via XML-RPC, and the only direct queries are performed by the server, which typically operates behind a firewall on a trusted system. The current web interface does make direct queries to the database; a dedicated readonly account is used for this purpose. 3.8.2. Encryption Both soaptray and soapmon can be configured to use SSL certificates in order to encrypt all data communication between client and server. The encryption is done by the HTTPS server with either a self-signed certificate or, preferably, with a certificate signed by a trusted Certificate Authority (CA). This is recommended for clientserver communication for soaptray but is generally not considered necessary for monitoring information sent to soapmon by the JEP as this is not considered sensitive enough to justify the additional system CPU resources required for encryption. 3.8.3. Data Integrity In order to guarantee data integrity, an MD5 checksum or digest is generated for each file that is transmitted. This information is stored in the database and 12

is checked against the file after transfer. IceProd data transfers support several protocols, but the preference is to rely primarily on GridFTP, which makes use of GSI authentication [11, 12]. An additional security measure is the use of a temporary passkey that is assigned to each job at the time of submission. This passkey is used for authenticating communication between the job and the monitoring server and is only valid during the duration of the job. If the job is reset, this passkey will be changed before a new job is submitted. This prevents stale jobs that might be left running from making monitoring updates after the job has been reassigned. 4. Intrajob Parallelism As described in Section 3.1.2, a single IceProd job consists of a number of trays and modules that execute different parts of the job, for example, a simulation chain. These trays and modules describe a workflow with a set of interdependencies, where the output from some modules and trays is used as input to others. Initial versions of IceProd ran jobs solely as monolithic scripts that executed these modules serially on a single machine. This approach was not very efficient because it did not take advantage of the workflow structure implicit in the job description. To address this issue, IceProd includes a representation of a job as a directed, acyclic graph (DAG) of tasks. Jobs are recharacterized as groups of arbitrary tasks and modules that are defined by users in a job's XML steering file, and each task can depend on any number of other tasks in the job. This workflow is encoded in a DAG, where each vertex represents a single instance of a task to be executed on a computing node, and edges in the graph indicate dependencies between tasks (see Figures 8 and 9). DAG jobs on the cluster are executed by means of the HTCondor DAGMan which is a workflow manager developed by the HTCondor group at the University of Wisconsin­Madison and included with the HTCondor batch system [13]. For IceCube simulation production, IceProd has utilized the DAG support in two specific cases: improving task-level parallelism and running jobs that utilize graphics processing units (GPUs) for portions of their processing. 4.1. Task-level Parallelism In addition to problems caused by coarse-grained requirements specifications, monolithic jobs also underutilize cluster resources. As shown in Figure 8, portions

!"#$%&'()*

64%)"5

!"#$%&'()*

64%)"5

!"#$%&'()*

64%)"5

!"#$%&'()*

64%)"5

!"#$%&'()*

64%)"5

+,-

+,-

+,-

+,-

+,-

*./.#/'&01

*./.#/'&01

*./.#/'&01

*./.#/'&01

*./.#/'&02

*./.#/'&02

*./.#/'&02

*./.#/'&02

*./.#/'&01

*./.#/'&02

*"/"0345/.&0!

*"/"0345/.&0"

%"&!"%.0#'55.#/4')

Figure 9: A more complicated DAG in IceProd with multiple inputs and multiple outputs that are eventually merged into a single output. The vertices in the second level run on computing nodes equipped with GPUs. PBS and Sun Grid Engine plugins. This enables faster execution of individual jobs by utilizing more computing nodes; however, one limitation of this implementation is that DAG jobs are restricted to a specific type of cluster, and DAG jobs cannot distribute tasks across multiple sites. 4.2. DAGs Based on System Requirements
detector A detector B

background

signal

GPU

garbage collection

Figure 8: A simple DAG in IceProd. This DAG corresponds to a typical IceCube simulation. The two root vertices require standard computing hardware and produce different types of signal. Their output is then combined and processed on GPUs. The output is then used as input for two different detector simulations.

of the workflow within a job are independent; however, if a job is monolithic, these portions will be run serially instead of in parallel. Therefore, although the entire simulation can be parallelized by submitting multiple jobs to different machines, this opportunity for additional parallelism is not exploited by monolithic jobs. Support for breaking a job into discrete tasks is now included in the HTCondor IceProd plugin as described above, and similar features have been developed for the 13

Individual parts of a job may have different system hardware and software requirements. Breaking these up into tasks that run on separate nodes allows for better utilization of resources. The IceCube detector simulation chain is a good example of this scenario in which tasks are distributed across computing nodes with different hardware resources. Light propagation in the instrumented volume of ice at the South Pole is difficult to model, but recent developments in IceCube's simulation include a much faster approach for simulating direct propagation of photons in the optically complex Antarctic ice [14, 15] by using general-purpose GPUs. This new simulation module is much faster than a CPU-based implementation and more accurate than using parametrization tables [16], but the rest of the simulation requires standard CPUs. When executing an IceProd job monolithically, only one set of cluster requirements can be applied when it is submitted to the cluster. Accordingly, if any part of the job requires use of a GPU, the entire monolithic job must be scheduled on a cluster machine with the appropriate hardware. As of this writing, IceCube has the potential to access 20, 000 CPU cores distributed throughout the world, but only a small number of these nodes are equipped with GPU cards. Because the simulation is primarily

CPU bound, the pool of GPU-equipped nodes is not sufficient to run all simulation jobs in an acceptable amount of time. Additionally, this would be an inefficient use of resources, since executing the CPU-oriented portions of monolithic jobs would leave the GPU idle for periods of time. In order to solve this problem, the modular design of the IceCube simulation design is used to divide the CPU- and GPU-oriented portions of jobs into separate tasks in a DAG. Since each task in a DAG is submitted separately to the cluster, their requirements can be specified independently and CPU-oriented tasks can be executed on general-purpose grid nodes while photon propagation tasks can be executed on GPU-enabled machines, as depicted in Figure 9. 5. Applications IceProd's highly configurable nature lets it serve the needs of many different applications, both inside and beyond the IceCube Collaboration. 5.1. IceCube Simulation Production The IceCube simulations are based on a modular software framework called IceTray in which modules are executed in sequential order. Data is passed between modules in the form of a "frame" object. IceCube simulation modules represent different steps in the generation and propagation of particles, in-ice light propagation, signal detection, and simulation of the electronics and data acquisition hardware. These modules are "chained" together in a single IceTray instance but can also be broken into separate instances configured to write intermediate data files. This allows for breaking up the simulation chain into multiple IceProd tasks in order to optimize the use of resources as described in Section 4. For IceCube, Monte Carlo simulations are the most computationally intensive task, which is dominated by the production of background cosmic-ray showers (see Table 2). A typical Monte Carlo simulation lasts on the order of 8 hours but corresponds to only four seconds of detector livetime. In order to generate sufficient statistics, IceCube simulation production needs to make use of available computing resources which are distributed across the world. Table 3 lists all of the sites that have participated in Monte Carlo production. 5.2. Off-line Processing of the IceCube Detector Data IceProd was designed primarily for managing the production of Monte Carlo simulations for IceCube, 14

Table 3: Sites participating in IceCube Monte Carlo production by country.
Country Sweden Canada Germany Queue Type ARC PBS SGE PBS CREAM PBS HTCondor PBS SGE HTCondor No. of Sites 2 2 1 3 4 2 4 3 4 1

Belgium USA

Japan

but it has also been successfully adopted for managing the processing and reconstruction of experimental data collected by the detector. This data collected by IceCube and previously described in Section 1.1 must undergo multiple steps of processing, including calibration, multiple-event track reconstructions, and sorting into various analysis channels based on predefined criteria. IceProd has proved to be an ideal framework for processing this large volume of data. For off-line data processing, the existing features in IceProd are used for job submission, monitoring, data transfer, verification, and error handling. However, in contrast to a Monte Carlo production dataset where the number of jobs are defined a priori, a configuration for off-line processing of experimental data initiates with an empty dataset of zero jobs. A separate script is then run over the data in order to map a job to a particular file (or group of files) and to generate MD5 checksums for each input file. Additional minor modifications were needed in order to support the desired features in off-line processing. In addition to the tables described in section 3.6, a run table was created to keep records of runs and dates associated with each file and unique to the data storage structure. All data collected during a season (or a one year cycle) are processed as a single IceProd dataset. This is because, for each IceCube season, all the data collected is processed with the same set of scripts, thus following the SPMD model. A job for such a dataset consists of all the tasks needed to complete the processing of a single data file. Off-line processing takes advantage of the IceProd built-in system for collecting statistics in order to provide information through web interface about the number of events that pass different quality selection criteria from completed jobs. Troubleshooting and error cor-

rection of jobs during processing is also facilitated by IceProd's real-time feedback system accessible through the web interface. The data integrity checks discussed in Section 3.8.3 also provide a convenient way to validate data written to storage and to check for errors during the file transfer task. 5.3. Off-line Event Reconstruction for the HAWC Gamma-Ray Observatory IceProd's scope is not limited to IceCube. Its design is general enough to be used for other applications. The High-Altitude Water Cherenkov (HAWC) Observatory [17] has recently begun using IceProd for its own offline event reconstruction and data transfer [18]. HAWC has two main computing centers, one located at the University of Maryland and one at UNAM in Mexico City. Data is collected from the detector in Mexico and then replicated to UMD. The event reconstruction for HAWC is similar in nature to IceCube's data processing. Unlike IceCube's Monte Carlo production, it is I/O bound and better suited for a local cluster rather than a distributed grid environment. The HAWC Collaboration has made important contributions to the development of IceProd and maintained active collaboration with the development team. 5.4. Deploying an IceProd Site Deployment of an IceProd instance is relatively easy. Installation of the software packages is handled through Python's built-in Module Distribution Utilities package. If the intent is to create a stand-alone instance or to start a new grid, the software distribution also includes scripts that define the MySQL tables required for IceProd. After the software is installed, the server needs to be configured through an INI-style file. This configuration file contains three main sections: general queueing options, site-specific system parameters, and job environment. The queueing options are used by the server plugin to help configure submission (e.g. selecting a queue or passing custom directives to the queueing system). System parameters can be used to define the location of a download directory on a shared filesystem or a scratch directory to write temporary files. The job environment can be modified by the server configuration to modify paths appropriately or set other environment variables. If the type of grid/batch system for the new site is already supported, the IceProd instance can be configured to use an existing server plugin, with the appropriate local queuing options. Otherwise, the server plugin must be written, as described in Section 3.2.1. 15

5.5. Extending Functionality The ease of adaptation of the framework for the applications discussed in Sections 5.2 and 5.3 illustrates how IceProd can be ported to other projects with minimal customization, which is facilitated by its Python code base. There are a couple of simple ways in which functionality can be extended: One is through the implementation of additional IceProd Modules as described in Section 3.3. Another is by adding XML-RPC methods to the soapmon module in order to provide a way for jobs to communicate with the server. There are, of course, more intrusive ways of extending functionality, but those require a greater familiarity with the framework. 6. Performance Since its initial deployment in 2006, the IceProd framework has been instrumental in generating Monte Carlo simulations for the IceCube collaboration. The IceCube Monte Carlo production has utilized more than three thousand CPU-core hours distributed between collaborating institutions at an increasing rate and produced nearly two petabytes of data distributed between the two principal storage sites in the U.S. and Germany. Figure 10 shows the relative share of CPU resources contributed towards simulation production. The IceCube IceProd grid has grown from 8 sites to 25 over the years and incorporated new computing resources. Incorporating new sites is trivial since each set of daemons acts as a volunteer that operates opportunistically on a set of job/tasks independent of other sites. There is no central manager that needs to scale with the number of computing sites. The central database is the one component that does need to scale up and can also be a single point of failure. Plans to address this weakness will be discussed in Section 7. The IceProd framework has also been successfully used for the off-line processing of data collected from the IceCube detector over a 4-year period beginning in the Spring of 2010. This corresponds to 500 terabytes of data and over 3 × 1011 event reconstructions. Table 4 summarizes the resources utilized by IceProd for simulation production and off-line processing. 7. Future Work Development of IceProd is an ongoing effort. One important area of current development is the implementation of workflow management capabilities like HTCondor's DAGMan but in a way that is independent of

Figure 10: Share of CPU resources contributed by members of the IceCube Collaboration towards simulation production. The relative contributions are integrated over the lifetime of the experiment. The size of the sector reflects both the size of the pool and how long a site has participated in simulation production.

any batch system in order to optimize the use of specialized hardware and network topologies by running different job subtasks on different nodes. Work is also ongoing on a second generation of IceProd designed to be more robust and flexible. The database will be partially distributed to prevent it from being a single point of failure and to better handle higher loads. Caching of files will be more prevalent and easier to implement to optimize bandwidth usage. The JEP will be made more versatile by executing ordinary scripts in addition to modules. Tasks will become a fundamental part of the design rather than an added feature and will therefore be fully supported throughout the framework. Improvements in the new design are based on lessons learned from the first generation IceProd and provide a better foundation on which to continue development. 8. Conclusions IceProd has proven to be very successful for managing IceCube simulation production and data processing across a heterogeneous collection of individual grid sites and batch computing clusters. With few software dependencies, IceProd can be deployed and administered with little effort. It makes use of existing trusted grid technology and network protocols, which help to minimize security and data integrity concerns that are common to any software that depends heavily on communication through the Internet. Two important features in the design of this framework are the iceprod-modules and iceprod-server plugins, which allow users to easily extend the functionality of the code. The former provide an interface between the IceProd framework and user scripts and applications. The latter provide an interface that abstracts the details of job submission and management in different grid environments from the framework. IceProd contains a growing library of plugins that support most major grid and batch system protocols. Though it was originally developed for managing IceCube simulation production, IceProd is general enough for many types of grid applications and there are plans to make it generally available to the scientific community in the near future. Acknowledgements We acknowledge the support from the following agencies: U.S. National Science Foundation-Office of 16

Table 4: IceCube simulation production and off-line processing resource utilization. The production rate has steadily increased since initial deployment. The numbers reflect utilization of owned computing resources and opportunistic ones. Simulation 25  3000 yr  45000 2421 1.6 × 107 2.3 × 107 1.2 PB Off-line 1  160 yr 2000 5 1.5 × 106 1.5 × 106 0.5 PB

Computing centers CPU-core time CPU-cores No. of datasets No. of jobs No. of tasks Data volume

Polar Programs, U.S. National Science FoundationPhysics Division, University of Wisconsin Alumni Research Foundation, the Grid Laboratory Of Wisconsin (GLOW) grid infrastructure at the University of Wisconsin­Madison, the Open Science Grid (OSG) grid infrastructure; U.S. Department of Energy, and National Energy Research Scientific Computing Center, the Louisiana Optical Network Initiative (LONI) grid computing resources; Natural Sciences and Engineering Research Council of Canada, WestGrid and Compute/Calcul Canada; Swedish Research Council, Swedish Polar Research Secretariat, Swedish National Infrastructure for Computing (SNIC), and Knut and Alice Wallenberg Foundation, Sweden; German Ministry for Education and Research (BMBF), Deutsche Forschungsgemeinschaft (DFG), Helmholtz Alliance for Astroparticle Physics (HAP), Research Department of Plasmas with Complex Interactions (Bochum), Germany; Fund for Scientific Research (FNRS-FWO), FWO Odysseus programme, Flanders Institute to encourage scientific and technological research in industry (IWT), Belgian Federal Science Policy Office (Belspo); University of Oxford, United Kingdom; Marsden Fund, New Zealand; Australian Research Council; Japan Society for Promotion of Science (JSPS); the Swiss National Science Foundation (SNSF), Switzerland; National Research Foundation of Korea (NRF); Danish National Research Foundation, Denmark (DNRF) The authors would like to also thank T. Weisgarber from the HAWC collaboration for his contributions to IceProd development.

References
[1] F. Halzen, IceCube A Kilometer-Scale Neutrino Observatory at the South Pole, IAU XXV General Assembly, ASP Conference Series 13 (2003) 13­16. [2] M. G. Aartsen, et al., Search for Galactic PeV gamma rays with the IceCube Neutrino Observatory, Phys. Rev. D 87 (2013) 62002. [3] F. Halzen, S. R. Klein, IceCube: An Instrument for Neutrino Astronomy, Invited Review Article: Rev. Sci. Inst. 81 (2010) 081101. [4] D. Protopopescu, K. Schwarz, PANDA Grid­a Tool for Physics, J. Phys.: Conf. Ser. 331 (2011) 072028. [5] P. Buncic, A. Peters, P. Saiz, The AliEn system, status and perspectives, eConf C0303241 (2003) MOAT004. arXiv:cs/0306067 . [6] R. Brun, F. Rademakers, ROOT - An Object Oriented Data Analysis Framework, Nuclear Inst. and Meth. in Phys. Res., A 389 (1997) 81­86. [7] A. Pavlo, P. Couvares, R. Gietzel, A. Karp, I. D. Alderman, M. Livny, The NMI build and test laboratory: Continuous integration framework for distributed computing software, Proc. USENIX/SAGE Large Installation System Administration Conference (2006) 263­273. [8] D. Winer, XML/RPC Specification, http://www.xmlrpc.com/spec (1999). [9] T. DeYoung, IceTray: A software framework for IceCube, Int. Conf. on Comp. in High-Energy Phys. and Nucl. Phys. (CHEP2004) (2005) 463­466. [10] R. Ellis, the ExpressionEngine Development Team, CodeIgniter User Guide, http://codeigniter.com (online manual). [11] W. Allcock, et al., GridFTP: Protocol extensions to FTP for the Grid, http://www.ggf.org/documents/GWD-R/GFD-R.020.pdf (April 2003). [12] The Globus Security Team, Globus Toolkit Version 4 Grid Security Infrastructure: A Standards Perspective (2005). [13] P. Couvares, T. Kosar, A. Roy, J. Weber, K. Wenger, Workflow Management in Condor, In Workflows for e-Science part III (2007) 357­375. [14] M. G. Aartsen, et al., Measurement of South Pole ice transparency with the IceCube LED calibration system, Nuclear Inst. and Meth. in Phys. Res., A A711 (2013) 73­89. [15] D. Chirkin, Study of South Pole ice transparency with IceCube flashers, Proc. International Cosmic Ray Conference 4 (2011) 161. [16] D. Chirkin, Photon tracking with GPUs in IceCube, Nuclear Inst. and Meth. in Phys. Res., A 725 (2013) 141­143. [17] A. U. Abeysekara, et al., On the sensitivity of the HAWC observatory to gamma-ray bursts HAWC Collaboration, Astropart. Phys. 35 (2012) 641­650. [18] T. Weisgarber, Production Reconstruction, HAWC Collaboration Meeting, May, 2013 (Unpublished results).

Appendix

The following is a comprehensive list of sites participating in IceCube Monte Carlo production: Uppsala University (SweGrid), Stockholm University (SweGrid), University of Alberta (WestGrid), TU Dortmund (PHiDO, LIDO), Ruhr-Uni Bochum (LiDO), University of Mainz, Universit´ e Libre de Bruxelles/Vrije Universiteit Brussel, Universiteit Gent (Trillian) Southern University (LONI), Pennsylvania State University (LIONX), University of Wisconsin (CHTC, GLOW, NPX4), Open Science Grid, RWTH Aachen University (EGI), Universit¨ at Dortmund (EGI), Deutsches Elektronen-Synchrotron (EGI, DESY), Universit¨ at Wuppertal (EGI), University of Delaware, Lawrence Berkeley National Laboratory (PDSF, Dirac, Carver), University of Maryland. 17

A GPU-based Correlator X-engine Implemented on the CHIME Pathfinder
Nolan Denman, Mandana Amiri,§ Kevin Bandura, Jean-Franc ¸ ois Cliche, Liam Connor,¶  § § § Matt Dobbs, Mateus Fandino, Mark Halpern, Adam Hincks, Gary Hinshaw,§ Carolin H¨ ofer,§  §    Peter Klages, Kiyoshi Masui, Juan Mena Parra, Laura Newburgh, Andre Recnik, J. Richard Shaw,¶ Kris Sigurdson,§ Kendrick Smith, and Keith Vanderlinde

arXiv:1503.06202v2 [astro-ph.IM] 11 Jun 2015

Dunlap Institute, University of Toronto Department of Astronomy and Astrophysics, University of Toronto § Department of Physics and Astronomy, University of British Columbia  Department of Physics, McGill University ¶ Canadian Institute for Theoretical Astrophysics Canadian Institute for Advanced Research, CIFAR Program in Cosmology and Gravity  Perimeter Institute for Theoretical Physics Contact E-Mail: denman@astro.utoronto.ca




Abstract--We present the design and implementation of a custom GPU-based compute cluster that provides the correlation X-engine of the CHIME Pathfinder radio telescope. It is among the largest such systems in operation, correlating 32,896 baselines (256 inputs) over 400 MHz of radio bandwidth. Making heavy use of consumer-grade parts and a custom software stack, the system was developed at a small fraction of the cost of comparable installations. Unlike existing GPU backends, this system is built around OpenCL kernels running on consumer-level AMD GPUs, taking advantage of low-cost hardware and leveraging packed integer operations to double algorithmic efficiency. The system achieves the required 105 TOPS in a 10 kW power envelope, making it one of the most power-efficient X-engines in use today.

the system achieves high performance at a small fraction of the hardware cost of comparable installations. This paper focuses on the system architecture and implementation, while two companion papers describe the custom software stacks, one focusing on an innovative OpenCL-based X-engine GPU kernel [2], and one on the handling of the vast data volume flowing through the system [3]. The paper is structured as follows: design considerations and constraints are discussed in §II; the hardware components of the system are described in §III, and the software in §IV; the scaling of the X-engine to the full-size CHIME telescope is described in §V, and a summary and conclusion follow in §VI. II. D ESIGN C ONSIDERATIONS

I.

I NTRODUCTION While most components in CHIME scale linearly with number of inputs N , the computational cost of pairwise correlation scales as N 2 , making efficiency in the X-engine a primary concern. There are correlation techniques which rely on the redundancy of CHIME feed separations to scale as N log N , but the real-time calibrations these require for precision observations remain largely unproven in an astrophysical context. Design decisions were guided by the need to produce an inexpensive system capable of scaling to support full CHIME, and which would support rapid development and deployment of new data processing algorithms. These requirements of computational power and ease of development drove the decision to build the X-engine around GPUs rather than Application-Specific Integrated Circuits (ASICs) or FPGAs. The computational cost  of pairwise element correlation for N elements across a bandwidth of  is  =  · N · (N + 1)/2 (1)

The Canadian Hydrogen Intensity Mapping Experiment (CHIME) is an interferometric radio telescope, presently under construction at the Dominion Radio Astrophysical Observatory (DRAO) in British Columbia, Canada, which will map the northern sky over a radio band from 400 to 800 MHz. With over 2000 inputs and a 400 MHz bandwidth, the correlation task (measured as the bandwidth-baselines product) on CHIME will be an order-of-magnitude larger than on any currently existing telescope array. The correlator follows an FX split design, with a first stage Field Programmable Gate Array (FPGA)-based F-engine which digitizes, Fourier transforms (channelizes), and bundles the data into independent frequency bands, followed by a second-stage Graphics Processing Unit (GPU)-based X-engine which produces a spatial correlation matrix consisting of the integrated pairwise products of all the inputs at each frequency. The CHIME Pathfinder instrument [1] features 128 dualpolarization feeds and a reduced-scale prototype of the full CHIME correlator. This paper describes the X-engine of the Pathfinder's 256-input hybrid FPGA/GPU FX correlator, among the largest such systems in operation. Through extensive use of off-the-shelf consumer-grade hardware and heavy optimization of custom data handling and processing software,

measured in complex multiply-accumulate (cMAC) operations per second; for the CHIME Pathfinder,  = 13 TcMAC/s. For large N this dominates the cost of any other processing proposed for the X-engine. Top-end GPUs in 2014 provided of

Fig. 4. Screenshot showing the correlator status webpage. The monitoring system displays the status of each FPGA and GPU node, and of the correlator software; per-GPU temperatures and per-node power consumption are also available.

Fig. 3. A diagram showing the liquid-cooling structure in the CHIME Pathfinder, with red and blue indicating hot and cold coolant, respectively. The heat exchanger uses a large fan to cool the liquid using ambient outside air. The object marked `M' is a temperature-controlled mixing valve, which regulates the temperature of the `cold' sections of the loop.

kotekan4 software pipeline manages the data flow and processing within GPU nodes. Due to the high I/O demands (820 Gb/s in total), the system must make maximally efficient use of the available bandwidth at each stage. A packetized and loss-tolerant data handling system, similar to that in operation in the PAPER [6] correlator [7] ensures that momentary faults do not impede long-term data gathering. Recnik et. al. [3] discuss the data handling in detail; a brief description follows. Data arrive as UDP packets and are buffered by the host CPU in system memory for inspection and staging prior to transfer into the GPUs for processing. Packet loss, though rare, is tracked along with other flags from the F-engine from e.g. saturation of the ADCs or from FFT channelizing. The count of missing or saturated data is used to renormalize the postintegration correlation matrices. A series of OpenCL kernels are executed on the GPUs; these pre-condition the data, compute and integrate correlation matrices, and post-process the data if necessary (see [2] for more details). Computed correlation matrices are assembled by the CPU and forwarded to gamelan, which stitches the full 400 MHz band back together using data from all active nodes. This reassembly is robust against individual node failures or outages; they simply result in loss of data from the inactive nodes. Integrated correlation matrices are recorded onto an array of disks in gamelan, and these data are asynchronously copied to a remote archive server hosting a much larger array of drives, and then copied off-site for scientific analyses.

Fig. 5. Screenshot showing the live view webpage. The triangle is the full correlation matrix for a particular frequency, with colour indicating the complex value's phase; the website may be queried for any of the associated data.

B. Monitoring and Control The X-engine software pipeline (composed of the kotekan instances on each node, along with the collection server software) is launched and controlled through scripts run on gamelan. The nodes run CentOS Linux 6.5 and can be accessed by remote shell login, while the PDUs allow remote power cycling to aid in recovery of crashed systems. The status of each of the GPU nodes is tracked by the gamelan control node, and made available via a web interface; see Figure 4 for an example of the tracking display. In addition, the last few hours of data are streamed over TCP to a second server, where it is available for live analysis and monitoring. An example of the live-monitoring webpage is shown in Figure 5. C. GPU Data Processing Tasks The most computationally expensive operation performed on the GPU nodes is the mission-critical pairwise feed correlation. In parallel with this, the Pathfinder correlator will explore alternate correlation methods which leverage the redundant layout of the CHIME baselines. Supplemental tasks include beamforming, gating, time-shifting of inputs, and RFI excision. Brief descriptions of these tasks follow.

4 A style of playing fast interlocking parts in Balinese Gamelan music; see http://en.wikipedia.org/wiki/Kotekan

1) Full Correlation: The primary responsibility of the Xengine is to calculate and integrate the correlation matrix of all the spatial inputs. This involves accumulation of 32,896 pairwise products for each of 1024 frequency bands. The default integration period is 223 samples, corresponding to 21.47 s, much faster than the 2.5 minute beam-crossing time from sky rotation. The computational requirements of the full X-engine system are dominated by this correlation operation, such that all other processes constitute an insignificant additional burden. The current implementation achieves nearmaximum-theoretical throughput; for details, see Klages et. al. [2] 2) Alternate Correlation Techniques: Interferometric arrays with highly redundant baselines can take advantage of correlation techniques that are more efficient than the na¨ ive pairwise method. In the case of feeds which are evenly spaced, FFT-based transformations can be used to increase the efficiency of the correlation to N log N , at the cost of strict calibration requirements. [8][9][10] These correlation strategies will be tested in parallel with the pairwise N 2 correlation; additionally, they may be used in hybrid form with some N 2 and some N log N stages. 3) Discrete Beamforming: The CHIME Pathfinder is a stationary telescope that cannot physically point at a specific source or location on the sky. When observing localized sources, it is desirable to form one or more beams, `pointing' the telescope digitally to an arbitrary location within the main beam. This is accomplished in the GPUs by phase-shifting and summing the data from all antennas, so that signals originating in one region of the sky interfere constructively. This signal is then written out at very high cadence, allowing examination of a localized source with very fine time resolution. 4) Output Gating: The CHIME Pathfinder will observe periodic sources such as astronomical pulsars and injected calibration signals. These sources generally vary faster than the default 21 s integration period, but high-cadence gating may be used to observe sub-integration signal structure. Gating consists of partitioning the output into a set of sub-buffers based on the time relative to the period of the source, so that independent `on' and `off' signals may be constructed. 5) Time Shifting: Signals from outlying telescope stations can be fed into the correlator. Large spatial separations introduce decorrelation between inputs, which can be corrected for by time-shifting samples within the GPUs. The current implementation permits the correction of any input by up to 168 ms, and has been tested with the nearby John A. Galt 26m radio telescope at DRAO. 6) RFI Cleaning: Anthropogenic radio frequency interference (RFI) introduces a significant source of additional noise to the astronomical signal. These signals are generally narrow-band and intermittent, coming and going on timescales much shorter than the default 21 s integration period, but with relatively low duty cycles. High-cadence identification and excision of RFI can be performed within the GPUs, and a variety of algorithms are under development including robust outlier and higher-moment statistical tests. [11][12]

V.

X- ENGINE S CALABILITY

The X-engine described here was designed for the CHIME Pathfinder, and must be scaled up significantly for the full CHIME instrument. Given a scaled F-engine providing channelized data, the X-engine's design allows it to scale straightforwardly to a broader band or larger-N arrays. Additional radio bandwidth is trivially added through additional nodes; increasing the number of inputs adds to the computational demand on each node, and can be addressed through newer, more powerful GPUs. At the time of writing, the computational power per node could be roughly tripled by simply replacing the GPUs. To support larger N 2 requirements, the bandwidth handled in each node can be reduced, in exchange for proportionally more nodes. The bandwidth fed to each GPU can similarly be reduced, and for very large N , when even a single frequency band is beyond the capacity of a single processing node, data can be time-multiplexed across multiple GPUs. The expansion to full CHIME (N = 2048) yields an N 2 computational requirement  an order-of-magnitude greater than any system currently in existence. Using current technology, a straightforward scaling of the current system -- 256 nodes each containing 2 dual-chip R9 295X2 GPUs -- could handle the entire pairwise correlation task, without additional software development. This density of processors is easily achievable with the liquid cooling demonstrated, and would occupy a modest physical footprint, at a very low hardware cost of $1M. However, it is not expected that the full CHIME instrument will rely on a complete N 2 correlation, instead pursuing a fast alternate correlation technique as discussed in §IV-C2. VI. C ONCLUSION

We have implemented a low-cost, high-efficiency GPUbased correlator X-engine for the CHIME Pathfinder. Capable of correlating 32,896 baselines over 400 MHz of radio bandwidth, it makes efficient use of consumer-grade parts and executes a highly optimized software stack. Measured by the computational requirement of a na¨ ive N 2 correlation ­ the bandwidth-baseline product  defined by Equation 1 ­ the CHIME Pathfinder correlator is among the largest in the world. Aspects of the system such as the cooling systems have been substantially modified, optimizing the X-engine's efficiency and ensuring economical scaling to the full-size CHIME instrument. ACKNOWLEDGEMENTS We are very grateful for the warm reception and skillful help we have received from the staff of the Dominion Radio Astrophysical Observatory, which is operated by the National Research Council of Canada. We acknowledge support from the Canada Foundation for Innovation, the Natural Sciences and Engineering Research Council of Canada, the B.C. Knowledge Development Fund, le Cofinancement gouvernement du Qu´ ebec-FCI, the Ontario Research Fund, the CIFAR Cosmology and Gravity program, the Canada Research Chairs program, and the National Research Council of Canada. PK thanks IBM Canada for funding

his research and work through the Southern Ontario Smart Computing Innovation Platform (SOSCIP). We thank Xilinx University Programs for their generous support of the CHIME project, and AMD for donation of test units. R EFERENCES
[1] K. Bandura et al., "Canadian Hydrogen Intensity Mapping Experiment (CHIME) Pathfinder," in Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, ser. Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, vol. 9145, Jul. 2014, p. 22. P. Klages et al., "Data Packing for High-Speed 4-Bit GPU Correlators," 2015, In press; accepted to IEEE ASAP 2015. A. Recnik et al., "An Efficient Real-time Data Pipeline for the CHIME Pathfinder Radio Telescope X-Engine," 2015, In press; accepted to IEEE ASAP 2015. M. A. Clark, P. C. La Plante, and L. J. Greenhill, "Accelerating Radio Astronomy Cross-Correlation with Graphics Processing Units," ArXiv e-prints, Jul. 2011. Khronos Group: The OpenCL Specification. [Online]. Available: khronos.org/opencl A. R. Parsons et al., "New Limits on 21 cm Epoch of Reionization from PAPER-32 Consistent with an X-Ray Heated Intergalactic Medium at z = 7.7," The Astrophysical Journal, vol. 788, p. 106, Jun. 2014. A. Parsons et al., "A Scalable Correlator Architecture Based on Modular FPGA Hardware, Reuseable Gateware, and Data Packetization," Publications of the Astronomical Society of the Pacific, vol. 120, pp. 1207­1221, Nov. 2008. J. D. Bunton, "Antenna Array Geometries to Reduce the Compute Load in Radio Telescopes," IEEE Transactions on Antennas and Propagation, vol. 59, pp. 2041­2046, Jun. 2011. M. Tegmark and M. Zaldarriaga, "Fast Fourier transform telescope," Physical Review D, vol. 79, no. 8, p. 083530, Apr. 2009. ----, "Omniscopes: Large area telescope arrays with only NlogN computational cost," Physical Review D, vol. 82, no. 10, p. 103501, Nov. 2010. G. M. Nita, D. E. Gary, Z. Liu, G. J. Hurford, and S. M. White, "Radio Frequency Interference Excision Using Spectral-Domain Statistics," Publications of the Astronomical Society of the Pacific, vol. 119, pp. 805­827, Jul. 2007. G. M. Nita and D. E. Gary, "Statistics of the Spectral Kurtosis Estimator," Publications of the Astronomical Society of the Pacific, vol. 122, pp. 595­607, May 2010.

[2] [3]

[4]

[5] [6]

[7]

[8]

[9] [10]

[11]

[12]

An Efficient Real-time Data Pipeline for the CHIME Pathfinder Radio Telescope X-Engine
Andre Recnik , Kevin Bandura , Nolan Denman , Adam D. Hincks§ , Gary Hinshaw§ Peter Klages , Ue-Li Pen¶ , and Keith Vanderlinde
Institute for Astronomy & Astrophysics, University of Toronto of Astronomy & Astrophysics, University of Toronto  Department of Physics, McGill University § Department of Physics and Astronomy, University of British Columbia ¶ Canadian Institute for Theoretical Astrophysics, University of Toronto Contact Email: vanderlinde@dunlap.utoronto.ca
 Department  Dunlap

arXiv:1503.06189v2 [astro-ph.IM] 15 Jun 2015

Abstract--The CHIME Pathfinder is a new interferometric radio telescope that uses a hybrid FPGA/GPU FX correlator. The GPU-based X-engine of this correlator processes over 819 Gb/s of 4+4-bit complex astronomical data from N=256 inputs across a 400 MHz radio band. A software framework is presented to manage this real-time data flow, which allows each of 16 processing servers to handle 51.2 Gb/s of astronomical data, plus 8 Gb/s of ancillary data. Each server receives data in the form of UDP packets from an FPGA F-engine over the eight 10 GbE links, combines data from these packets into large (32MB256MB) buffered frames, and transfers them to multiple GPU co-processors for correlation. The results from the GPUs are combined and normalized, then transmitted to a collection server, where they are merged into a single file. Aggressive optimizations enable each server to handle this high rate of data; allowing the efficient correlation of 25 MHz of radio bandwidth per server. The solution scales well to larger values of N by adding additional servers.

I. I NTRODUCTION The increasing performance of commodity computer hardware has opened up new possibilities for building real-time radio correlators. Traditionally correlators have used custom ASICs or FPGAs for all calculations. The first systems built with off-the-shelf hardware used CPUs, for example the realtime software correlator designed for the Giant Metrewave Radio Telescope (GMRT) [1]. While early experiments using GPUs showed relatively poor performance [2], the development of new GPUs and the efficient xGPU code [3] for NVIDIA's CUDA platform has popularized the use of GPUs in FX style correlators. FX correlators operate in two stages: first the F-engine samples astronomical data from each radio input and channelizes it into frequency bands using a Fourier transform, then the X-engine correlates all of the inputs against one another within each frequency band. The low cost and flexibility of 10 Gigabit Ethernet (10 GbE) has enabled these two stages to be easily separated, as demonstrated by Parsons, et al. [4]; with separate FPGAs performing the F-engine and X-engine stages connected by a 10 GbE packet switched network. A recent trend has been to replace FPGAs with GPUs in the X-engine, leading to the so called hybrid correlator approach. Projects using hybrid correlators include: the Precision Array

for Probing the Epoch of Reionization (PAPER) [5], the Large Aperture Experiment to Detect the Dark Ages (LEDA) [6], and the Murchison Wide-field Array (MWA) [7]. The Canadian Hydrogen Intensity Mapping Experiment (CHIME) Pathfinder [8] is a cylindrical radio telescope with 128 dual-polarization receivers for N = 256 total inputs. It uses a newly developed hybrid FX correlator. The F-engine consists of 16 custom FPGA boards, each with 16 ADCs, which sample and channelize the data into 1024 frequency bins. The data are reduced to 4+4-bit complex numbers, then a custom backplane network shuffles this data, such that each FPGA has data for all inputs in a subset of frequency bands. The shuffled data is then transmitted over 128 x 10 GbE links in User Datagram Protocol (UDP) packets to a GPU based X-engine. This paper presents the software pipeline, called kotekan,1 which manages data flow in the X-engine. The data is received from the F-engine as UDP packets, is merged and sent to the GPUs for processing, then the output from the GPUs is sent out to the collection and aggregation server. The software is written in the C programming language. In the CHIME Pathfinder, this software runs on 16 servers, built mostly with low-cost consumer-grade hardware. Each server has one Intel i7 4-core CPU, two AMD R9 280X GPUs, one AMD R9 270X GPU, two Silicom 4x 10 GbE network interface cards (NICs), and 16 GB of DDR3 RAM. The GPUs and NICs are each connected to the CPU by 8x PCI Express (PCIe) 3.0 lanes. An abstract layout of each of these components is given in Figure 1. The operating system used is CentOS Linux 6.5. II. DATA F LOW The software design is based around generic buffer objects, which support multiple consumers and producers, and handle the majority of thread synchronization tasks using
1 Given the musical acronym of the experiment, CHIME, the collaboration uses musically inspired names for system components. Kotekan is a style of playing fast interlocking parts in Balinese Gamelan music using percussive instruments. http://en.wikipedia.org/wiki/Kotekan

31 Header
Stream ID ENC #Fra -mes Prot LEN cookie

0

#Freq Bins Ancillary data Timestamp

#Antennas

Data (1-4 Frames) 8 Frequencies per Frame 256 bytes per frequency 2048 Bytes per Frame 2048-8192 Bytes per packet

16 bytes

FREQ BIN 1 256ANT - 256 bits

FREQ BIN 0 256ANT - 256 bits

. . .
FREQ BIN 7 256ANT - 256 bits FREQ BIN 6 256ANT - 256 bits

Post FFT 4-bit Scaling Flags ADC Flags

1 bit flag per antenna per frequency 256 Bytes per Frame
ADC INPUT 256 ANT 512bits

1 flag per input

Fig. 3: The UDP packet format sent by the FPGAs.

entire array. There are a number of considerations to make when processing UDP packets at high rates: 1) Efficiently receiving packets from the network: The Linux kernel has a large overhead when processing high bandwidth network traffic. Early tests showed that even with large packets and optimizations to the Linux kernel network parameters, it would be unable to process the required data rates while simultaneously managing the GPU PCIe transfers. The solution was to bypass the Linux kernel's network stack entirely using modified network drivers and custom kernel modules. There are a number of pre-built solutions which enable kernel bypass, including: DPDK [9], netmap [10], and others. For this project, NTOP's PF RING/DNA [11], [12] framework was chosen for its support on the chosen network card vendor. However, the overall system is not tied to a particular bypass stack; changing the bypass stack would just involving replacing calls to the PF RING API with another bypass API. The most important aspect of these bypass drivers, aside from avoiding slow kernel packet processing code, is the use of a co-mapped ring buffer that is addressable in both user and kernel space. This allows the NIC driver to write directly to the co-mapped memory with a Direct Memory Access (DMA) operation, avoiding the traditional kernel-to-user-space copy. 2) Handling Losses/Errors: Since the network protocol is UDP, the system must handle lost, out of order, and duplicate packets. This is achieved by tracking the sequence number in each packet header. When the system detects packet loss, it writes zeros into the area of the GPU staging buffer where the missing packet would have been placed. This removes the requirement to zero the buffers between each use; the network thread guaranties the buffer will have good data or zeros in every location. In the case of out of order packets, the sequence number is used to copy the packet into the correct location. Duplicate packets are simply ignored. In early testing with the standard socket API, vectored

I/O was used to separate the packet into data and header components with a readv system call. The sequence number was inferred based on the pervious packet, and the data was copied directly to the GPU staging buffer. If the sequence number did not match the expected number, then the data was moved or overwritten. Given low packet loss, this saves a second copy for the vast majority of packets, and allowed maximum capture rates around 40 Gb/s per CPU. With the addition of co-mapped memory this became unnecessary, since the sequence number can be read before moving the data to the GPU staging buffer. 3) Efficient Memory Transfers: The standard memory copying functions like memcpy found in libc are inefficient for use in a high bandwidth environment like this one. This issue largely stems from the fact that compilers assume temporal locality, that data being copied will be used in the near future and should be added to the CPU cache. This causes the destination memory to be cached in the process of performing a copy. This puts huge pressure on the cache when moving large amounts of data, resulting in unnecessary memory controller usage. To mediate the issue caused by the standard memcpy, a set of custom memory copy functions that use Intel AVX intrinsics with non-temporal hints were created. The non-temporal hint prevents the memory copy destination from being added to the CPU's cache. These functions copy the data from the comapped packet buffer to the GPU staging frame. B. Kernel Invocation Process A set of GPU programs called kernels do the actual crosscorrelation and integration. The kernels are written in the Open Computing Language (OpenCL) framework2 , allowing them to be run on a number of different platforms. The kernels used achieve very high levels of efficiency using packed multiplyaccumulate operations and are detailed in [13]. The first step in this process is copying the data to the GPU's device memory, a process largely managed by the OpenCL drivers. In our tests we found that large (32 MB or greater) frames transferred most efficiently. The memory used for these frames is page-locked allowing the GPU drivers to preform an efficient DMA operation to copy the memory between the host and device. In contrast to some systems using xGPU [3] like the MWA [7] and LEDA correlators [6] which promote the data from 4-bit to 8-bit integers in their CPUs, this system does not promote the data before sending it to the GPUs. Avoiding this promotion reduced both CPU memory and PCIe bandwidth, and was a key part in limiting the number of servers and CPUs needed. The entire process is pipelined to allow concurrent operation of the memory transfers and kernel invocations, as shown in Figure 4. The "Copy to Host Memory" step is not run on every invocation, since the correlation kernels simply add to the output of the last run, extending the integration time of the output.
2 https://www.khronos.org/opencl/

Data Ready

Data Ready Copy to GPU Memory

Data Ready Copy to GPU Memory Kernels Copy to GPU Memory Kernels Copy to Host Memory Kernels Copy to Host Memory Host Callback

Host Events GPU Copy-in Queue

GPU Kernel Queue GPU Copy-out Queue Host Callbacks

Copy to Host Memory Host Callback Host Callback

Fig. 4: GPU Pipeline. The "Kernels" stage includes correlation, as well as extra data processing operations like time shifting or RFI detection. The dashed lines indicate optional steps or dependencies. The time taken by each step is not to scale.

When executing an OpenCL device operation, for example a memory copy or kernel invocation, there is a delay between the function call and the time the device starts the operation. To minimize this delay, we pre-load entire chains of OpenCL operations, the first of which is set to depend on the "Data Ready" event in Figure 4, which is triggered by the GPU thread when a buffer has been filled by a network thread. When a chain finishes, a call-back function ("Host Callback" in Figure 4) adds a new chain of events which replaces the one that just finished, and marks the associated output buffer as full, so the GPU post-processing thread can take the data. C. GPU Post-Processing This GPU post-processing thread combines all eight streams, and then normalizes the results to correct for lost input samples. Loss of input samples can result from packet loss, or numerical range and sampling limitations in the FPGAs and ADCs. When this happens the data points are zeroed, before they are transferred to the GPU. This causes some correlated data points to contain fewer input samples in each integration. The network threads track packet loss and F-engine error flags, which are used by the GPU post-processing thread to generate a normalization matrix as follows: · Packet loss is tracked as a single counter, then added to every point in the normalization matrix at the end of each integration. · The F-engine flags are extracted from bit-fields in the packet footers and stored in counters for each input and frequency per integration. The counters are used to populate the normalization matrix at the end of each integration. · If two or more inputs are flaged by the F-engine in the same time sample, the previous step will result in a double counting in the normalization matrix at their intersections.

To correct this, these intersections are recorded per sample, and subtracted from the normalization matrix. The fraction of lost data given by the normalization matrix is then applied to the correlation matrix. This process is optimized in two ways. First the bit-field of flags in the footer is read initially as 32-bit integers, and bitfield extraction is performed only if the 32-bit representation of the bit-field is non-zero. With a low number of flaged data points, this avoids checking each bit individually. Second, by only updating counts of errors and their associated intersections, the number of memory accesses is greatly reduced. The complexity of tracking the intersections per time sample is O(E 2 ), where E is the number of flaged data points in a given time sample. This can be processed by the CPU, provided E N. After the data has been combined and normalized, it is formatted for transmission over a TCP stream and placed in an output buffer. A final output thread then transmits this data to the collection server using a TCP socket connection. D. Data Collection At the collection server, the TCP streams from each kotekan instance are combined. The streamID is used to identify the frequency bands provided by each stream, allowing the collection server to correctly order the frequency bands regardless of how the 10 GbE links were connected to the individual processing servers. The sequence number is used to align the frames in time. The data is saved to disk using the Hierarchical Data Format Version 5 (HDF5) standard3 . Individual servers may be switched off and back on without interrupting the entire system. If one or more of the servers stops working, the collection server simply stops recording the frequency bands associated with that server. When the server
3 http://www.hdfgroup.org/HDF5/

TABLE I: Data Processing Rates
Data Type 4-bit Sky Data 4-bit Sky Data + Headers & Flags Packets per Second (PPS) Output Data (10s Cadence) Rate per Link 6.4 Gb/s 7.45 Gb/s 97,565 PPS 16.8 Mb/s Rate per Host 51.2 Gb/s 59.6 Gb/s 781,250 PPS 16.8 Mb/s Full System Rate 819.2 Gb/s 953.6 Gb/s 12.5 MPPS 269.5 Mb/s

of the CHIME Pathfinder, with N = 256 inputs, using only 16 servers. The system has been shown to scale well from N = 16 to N = 256, and will be used in a future system with N = 2048 and a total input bandwidth of close to 8 Tb/s. ACKNOWLEDGMENTS We are very grateful for the warm reception and skillful help we have received from the staff of the Dominion Radio Astrophysical Observatory, which is operated by the National Research Council of Canada. We acknowledge support from the Canada Foundation for Innovation, the Natural Sciences and Engineering Research Council of Canada, the B.C. Knowledge Development Fund, le Cofinancement gouvernement du Qu´ ebec-FCI, the Ontario Research Fund, the CIfAR Cosmology and Gravity program, the Canada Research Chairs program, and the National Research Council of Canada. We thank Xilinx University Programs for their generous support of the CHIME project, and AMD for donation of test units. Peter Klages thanks IBM Canada for funding his research and work through the Southern Ontario Smart Computing Innovation Platform (SOSCIP). R EFERENCES
[1] J. Roy, Y. Gupta, U.-L. Pen, J. Peterson, S. Kudale, and J. Kodilkar, "A real-time software backend for the gmrt," Experimental Astronomy, vol. 28, no. 1, pp. 25­60, 2010. [Online]. Available: http://dx.doi.org/ 10.1007/s10686-010-9187-0 [2] R. V. van Nieuwpoort and J. W. Romein, "Using many-core hardware to correlate radio astronomy signals," in Proceedings of the 23rd International Conference on Supercomputing, ser. ICS '09. New York, NY, USA: ACM, 2009, pp. 440­449. [Online]. Available: http://doi.acm.org/10.1145/1542275.1542337 [3] M. Clark, P. L. Plante, and L. Greenhill, "Accelerating radio astronomy cross-correlation with graphics processing units," Int. J. High Perform. Comput. Appl., vol. 27, no. 2, pp. 178­192, May 2013. [Online]. Available: http://dx.doi.org/10.1177/1094342012444794 [4] A. Parsons et al., "A scalable correlator architecture based on modular fpga hardware, reuseable gateware, and data packetization," Publications of the Astronomical Society of the Pacific, vol. 120, no. 873, pp. pp. 1207­1221, 2008. [Online]. Available: http://www.jstor.org/stable/10.1086/593053 [5] A. R. Parsons, A. Liu, J. E. Aguirre, Z. S. Ali, R. F. Bradley et al., "New Limits on 21cm EoR From PAPER-32 Consistent with an X-Ray Heated IGM at z=7.7," Astrophys.J., vol. 788, p. 106, 2014. [6] J. Kocz et al., "Digital Signal Processing Using Stream High Performance Computing: A 512-Input Broadband Correlator for Radio Astronomy," Journal of Astronomical Instrumentation, vol. 4, p. 50003, Mar. 2015. [7] S. M. Ord et al., "The Murchison Widefield Array Correlator," Publications of the Astronomical Society of Australia, vol. 32, p. 6, Mar. 2015. [8] K. Bandura et al., "Canadian Hydrogen Intensity Mapping Experiment (CHIME) pathfinder," in Society of Photo-Optical Instrumentation Engineers Conference Series, vol. 9145, Jul. 2014, p. 22. [9] Intel, Intel Data Plane Development Kit: Programmer's Guide, 2014. [10] L. Rizzo, "netmap: a novel framework for fast packet i/o," in USENIX Annual Technical Conference, 2012. [11] L. Deri, "Improving Passive Packet Capture:Beyond Device Polling," in 4th International System Administration and Network Engineering Conference, 2004. [12] NTOP. (2012) RFC-2544 performance test - PF Ring-DNA VS Standard network Driver. [Online]. Available: http://www.ntop.org/ wp-content/uploads/2012/04/DNA ip forward RFC2544.pdf [13] P. Klages, K. Bandura, N. Denman, A. Recnik, J. Sievers, and K. Vanderlinde, "Data Packing for High-Speed 4-Bit GPU Correlators," In Press. IEEE ASAP, 2015.

is repaired, it reconnects to the collection system, and those frequency bands resume recording. III. D ISCUSSION This software pipeline design was largely focused on maximizing input data bandwidth per CPU, in order to minimize the overall cost of the system. Table I shows the data rates achieved by the software in operation. There are ways in which the pipeline could be made even more efficient. The packets are written to a user/kernel space mapping buffer, then copied to a GPU staging buffer. This copy could be avoided if the data segment of each packet was sent directly to the GPU buffers via DMA from the NIC, with the headers/footers sent to another buffer for processing. Such a solution would require a tighter coupling with software and the NIC driver, so that the userspace application could direct which memory the NIC DMA transfered packet sections into. The OpenFabrics Enterprise Distribution4 (OFED) kernel bypass software can in principle provide this; it was not pursued it due to lack of driver support on the chosen hardware. As the number of inputs N increases, the computation cost of correlation increases as O(N 2 ), while data bandwidth scales only as O(N ). With more servers required to do the correlation, the bandwidth to each should go down. However, as co-processors become more powerful, it will be possible to correlate larger values of N with fewer servers, continuing to put pressure on the bandwidth requirements of each server. The generic design of this software allows it to be extended easily to large values of N . It has already been scaled from N = 16 in early tests of the CHIME Pathfinder using one server, to the current N = 256 mode using 16 servers, and is expected to scale to N = 2048 for the full CHIME experiment. IV. C ONCLUSION We have developed an optimized software pipeline to manage the data flow in the X-engine of a hybrid FX FPGA/GPU correlator. Using a combination of kernel bypass network drivers, efficient memory copy functions, memory pools, minimal memory copies, large packets and GPU frames, and GPU kernels that can process 4-bit data, the system achieves a very high input data bandwidth per server and per CPU. This allows input data processing rates of over 51.2 Gb/s per single socket server, and 819 Gb/s system wide. In terms of radio bandwidth, the systems process the full 400 MHz band
4 https://www.openfabrics.org/

Panelists will discuss how collective intelligence can be applied to large-scale problems through collaborative online systems. The features and affordances of several such systems will be described, inviting discussion about how such systems can be better designed by the CSCW community.Washington University School of Medicine

Digital Commons@Becker
Open Access Publications

2015

WormBase 2016: Expanding to enable helminth genomic research
Tim Schedl
Washington University School of Medicine in St. Louis

et al

Follow this and additional works at: http://digitalcommons.wustl.edu/open_access_pubs Recommended Citation
Schedl, Tim and et al, ,"WormBase 2016: Expanding to enable helminth genomic research." Nucleic Acids Research.,. 1-7. (2015). http://digitalcommons.wustl.edu/open_access_pubs/4394

This Open Access Publication is brought to you for free and open access by Digital Commons@Becker. It has been accepted for inclusion in Open Access Publications by an authorized administrator of Digital Commons@Becker. For more information, please contact engeszer@wustl.edu.

Nucleic Acids Research Advance Access published November 17, 2015
Nucleic Acids Research, 2015 1 doi: 10.1093/nar/gkv1217

WormBase 2016: expanding to enable helminth genomic research
Kevin L. Howe1,* , Bruce J. Bolt1 , Scott Cain2 , Juancarlos Chan3 , Wen J. Chen3 , Paul Davis1 , James Done3 , Thomas Down1 , Sibyl Gao2 , Christian Grove3 , Todd W. Harris2 , Ranjana Kishore3 , Raymond Lee3 , Jane Lomax4 , Yuling Li3 , Hans-Michael Muller3 , Cecilia Nakamura3 , Paulo Nuin2 , Michael Paulini1 , Daniela Raciti3 , Gary Schindelman3 , Eleanor Stanley4 , Mary Ann Tuli3 , Kimberly Van Auken3 , Daniel Wang3 , Xiaodong Wang3 , Gary Williams1 , Adam Wright2 , Karen Yook3 , Matthew Berriman4 , Paul Kersey1 , Tim Schedl5 , Lincoln Stein2 and Paul W. Sternberg2,6
1

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

European Molecular Biology Laboratory, European Bioinformatics Institute, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK, 2 Informatics and Bio-computing Platform, Ontario Institute for Cancer Research, Toronto, ON M5G0A3, Canada, 3 Division of Biology and Biological Engineering 156­29, California Institute of Technology, Pasadena, CA 91125, USA, 4 Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SA, UK, 5 Department of Genetics, Washington University School of Medicine, St. Louis, MO 63110, USA and 6 Howard Hughes Medical Institute, California Institute of Technology, Pasadena, CA 91125, USA

Received September 30, 2015; Revised October 26, 2015; Accepted October 28, 2015

ABSTRACT WormBase (www.wormbase.org) is a central repository for research data on the biology, genetics and genomics of Caenorhabditis elegans and other nematodes. The project has evolved from its original remit to collect and integrate all data for a single species, and now extends to numerous nematodes, ranging from evolutionary comparators of C. elegans to parasitic species that threaten plant, animal and human health. Research activity using C. elegans as a model system is as vibrant as ever, and we have created new tools for community curation in response to the ever-increasing volume and complexity of data. To better allow users to navigate their way through these data, we have made a number of improvements to our main website, including new tools for browsing genomic features and ontology annotations. Finally, we have developed a new portal for parasitic worm genomes. WormBase ParaSite (parasite.wormbase.org) contains all publicly available nematode and platyhelminth annotated genome sequences, and is designed specifically to support helminth genomic research.

INTRODUCTION Nematodes are the most abundant animals on the planet (1), and over 25 000 species have been described (2), displaying a remarkable diversity in terms of size, form, lifestyle, habitat and reproductive mode (3). Caenorhabditis elegans is a free-living nematode that has been used for over 50 years as a model system in experimental biology (4). Its transparency, reproductive mode, small size, rapid generation time, simple nervous system and invariant cell lineage have made it ideally suited to the study of animal genetics [(5); www.wormbook.org]. The mission of WormBase is to facilitate and accelerate biological research using C. elegans by making the collected outputs of the research community accessible from a single resource, and to enable the transfer of this wealth of knowledge to the study of other metazoa, from nematodes to humans. The specific aims of WormBase are to: (i) place nematode data described in the research literature, deposited in the archives, or directly submitted to us into context via a combination of detailed manual curation and semi-automatic data integration; (ii) curate the reference genome sequence, gene structures and other genomic features for a small set of well-studied nematode species, thereby providing a high quality foundation for downstream studies and (iii) develop web displays and tools to allow users to easily visualize and query these data.

* To

whom correspondence should be addressed. Tel: +44 1223 494417; Fax: +44 1223 494 468; Email: kevin.howe@wormbase.org

C The Author(s) 2015. Published by Oxford University Press on behalf of Nucleic Acids Research. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

2 Nucleic Acids Research, 2015

CURATION STATUS Genomes and annotations for core species WormBase acts as the guardian of the reference genome and annotation for C. elegans and a small number of additional `core' species (www.wormbase.org/species/all), curating gene structures and other sequence features, assigning identifiers to primary annotation elements, and maintaining a complete historical record of all updates. In general, core status is reserved for species with a high-quality (chromosome-level) reference genome and identifiable research community that have expressed a desire for continued improvement of the genome sequence and annotation. We previously reported on the adoption of the filarial parasitic nematode Brugia malayi as a WormBase core species, and described a concerted effort to annotate its genome to high quality using a combination of modern automated methods and manual curation (6). Since then, we have added two key parasitic nematodes to the core set: Onchocerca volvulus, a causative agent of onchocerciasis (river blindness), and Strongyloides ratti, the rat laboratory analogue of the causative agent of strongyloidiasis (threadworm infection). Both genomes have undergone first pass automatic gene structure annotation similar to that performed for the B. malayi genome, and are currently undergoing targeted manual curation of gene structures. We encourage users to report issues they find with the annotation of these genomes, and are committed to addressing them as a priority. Enabling community curation of the research literature Comprehensive extraction and standardization of data from the C. elegans research literature remains one of the primary goals of the project. We target over 20 specific data types, and each presents its own challenge in terms of volume and ease of extraction. For some data types, such as gene expression patterns, curation keeps pace with the literature. For others, such as the phenotypes of mutant alleles and RNA-interference knockouts, the high volume of both historical publications and more recent high-throughput projects make the goal of keeping up-to-date unachievable with the resources we have. Help from the community is vital to address this shortfall, and we have recently worked on methods to allow researchers to contribute to curation for a small number of specific data types with the biggest backlog. The three data-types that we have targeted are: (i) phenotypes of mutant alleles; (ii) molecular details for mutant alleles and (iii) textual gene summaries. Custom web-forms for each of these types have been designed to make the barrier to participation as low as possible (www.wormbase.org/ about/userguide/submit data). Authors of newly published papers are invited to submit their data via the forms, and researchers are also invited to review and update existing annotations. Submissions received via this system are prioritised for curator review and inclusion in WormBase. The gene summaries referred to above are short paragraphs that provide an overview of the gene and its function, and include information about molecular identity, biological processes and pathways that the gene product partici-

pates in, temporal and spatial expression, and relationships with other genes via interaction or homology. Although WormBase users regard these summaries as highly valuable, the process of writing them is time-consuming. To address this, we have developed new software that automatically transforms structured data on gene function into natural language sentences, which are then collated to form a summary description. For example, the description created for the C. elegans gene tbc-8 (www.wormbase.org/species/ c elegans/gene/WBGene00008018) is: `tbc-8 is an ortholog of human SGSM2 (small G protein signaling modulator 2) and SGSM1 (small G protein signaling modulator 1); tbc-8 is involved in dense core granule maturation; tbc-8 exhibits Rab GTPase binding activity and is predicted to have GTPase activator activity, based on protein domain information; tbc-8 is localized to the Golgi trans cisterna, the early endosome, the Golgi medial cisterna and the cytosol.' The software has allowed us to create several thousand provisional gene summaries for C. elegans and other core species. As well as being useful in their own right, they also act as valuable and convenient starting points for manual revision. WORMBASE PARASITE Beyond C. elegans, the nematode species covered by WormBase fall into one of two categories: free-living relatives of C. elegans; and plant and animal parasitic nematodes. For the former, the research interest is mainly in the area of comparative genomics and evolution of the `reference' nematode C. elegans. The latter, however, have biomedical and agricultural importance and attract research interest in their own right. The primary research goal of parasitologists is to identify ways of controlling the parasite, and as such their desired entry points and common use-cases for WormBase are often distinct from those of scientists doing basic science using C. elegans as a model. Furthermore, the community of scientists working on parasitic worms (helminths) includes those working on platyhelminths (flatworms), which have historically been beyond the scope of WormBase. There have been recent concerted efforts to sequence the genomes of many helminths (e.g. the 50 Helminth Genomes initiative, www.sanger.ac.uk/science/collaboration/50HGP). Most of the genomes released so far are draft quality, and subject to high update frequency. In response to these challenges, we have embarked on a systematic data integration effort for parasitic worms. The results can be viewed in WormBase ParaSite (parasite.wormbase.org), a new sub-portal of WormBase aiming to focus on the use-cases of scientists engaged in helminth genomics. Genomes and annotation We have endeavoured to include all publicly available helminth genomes in WormBase ParaSite. Where multiple independent genome projects exists for the same species (e.g. for Haemonchus contortus (7,8)), and Ascaris suum (9,10)), we have included all. At time of writing we have genomes from 63 nematode species (71 genomes) and 27

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

Nucleic Acids Research, 2015 3

platyhelminth species (28 genomes), and we maintain an up-to-date list of all genomes available through the site (parasite.wormbase.org/species.html). These genome sequences have been collected from a variety of different sources, including the nucleotide archives, project-specific FTP sites, and direct engagement with genome project scientists. In all, 18 different groups were responsible for generating the data. In about half of the cases, we imported previously described or directly submitted gene structure annotations. For the remaining genomes, we used MAKER2 (11) to generate high-quality annotations by integrating evidence from multiple sources: ab initio gene predictions from AUGUSTUS (12), GeneMarkES (13) and SNAP (14); projected annotations from C. elegans and the taxonomically nearest previously-annotated helminth using GenBlastG (15) and RATT (16); and alignments of ESTs, mRNAs and proteins from related organisms. Comparative genomics We have used the Ensembl Compara system (17) to infer evolutionary histories for all helminth genes, supplemented by gene sets from 9 free-living species (including C. elegans) and 12 comparator species (including human and other model organisms). The result is the organisation of around 2.5 million genes into around 150 000 homology groups, each with a protein multiple alignment which is used to infer orthologous and paralogous relationships between the genes. In addition to gene-based comparative analysis, we have also a produced a whole-genome multiple alignment for a subset of the collection, using Progressive Cactus (18,19). This alignment is made available in the form of a Hierarchical Alignment (HAL) file (20), and included in the WormBase UCSC Assembly Hub (see below). More genomes will be added to the alignment in the future. Electronic annotation of gene function The research literature for most of the species in WormBase ParaSite is sparse, although we anticipate that the provision of the genomes will stimulate new research. In lieu of annotations based on experimental evidence, we have used established automated methods to predict the function of as many gene products as possible. First, to predict protein domains, assign terms from the Gene Ontology (GO), and classify the proteins into families, we have used InterProScan (21). Second, we have used the ortholog assignments from the Compara pipeline line to `project' experimentally derived GO annotations and product names from well-annotated species (for example C. elegans and other model organisms). Infrastructure and tools The Ensembl infrastructure (22) is the basis for much of the management, analysis and display of data in WormBase ParaSite. This has allowed us to provide a number of tools developed for the Ensembl project with little or no modification, for example the Variant Effect Predictor (23), and

REST application programming interface (24). Others we have customised for use in WormBase ParaSite. For example, we have modified Ensembl code to provide a sequence search service and BioMart data mining platform (25) that allow the interrogation of all species, or large sub-groups of species (e.g. all nematodes) at once in a single query (see Figure 1). All data for WormBase ParaSite can be downloaded from our FTP site in a new folder (ftp://ftp.wormbase.org/pub/ wormbase/parasite) that uses identical structure and naming conventions to the main FTP site (which now restricts to WormBase core species and genomic data sets from other free-living nematode species). WEBSITE AND TOOLS The main website (www.wormbase.org) continues to focus on the central mission of WormBase, which is to serve scientists using C. elegans as a model system. Since we launched a new version of the site in 2012, we have continued refining the organization and presentation of data with the aim of making more information available at a single glance. Not only does this minimize the amount of mouse clicking required to access common data, it can also provide new insights not possible when examining data on a piecemeal basis. New genome browser Browsing the genome is one of the most common tasks at WormBase, facilitating exploration of the genetic and physical maps, reagents, gene structures, candidate genes for forward genetic screens, and targets for reverse genetics. More recently, the genome browser has become the standard tool for the exploration of large-scale, genome-wide studies of gene expression and functional elements. The current software driving the WormBase genome browser, GBrowse (26), was originally developed for the WormBase site and has now become a mainstay of many model organism database projects (27­31). Designed before the advent of large-scale genomic datasets and the types of queries users expect to levy against them, GBrowse has reached end-of-life development status. After conducting a due-diligence survey of available replacements, we have selected JBrowse (32) from the Generic Model Organism Database project (GMOD, www.gmod.org) to replace GBrowse in WormBase. JBrowse represents a significant advancement over GBrowse, being much faster when browsing large regions, large datasets, or many tracks at concurrently. It also shares many of the user interface elements with GBrowse, providing a shallow learning curve for WormBase users. JBrowse provides a number of features not present in GBrowse. For example, it allows users to view their own data in the browser without requiring the files to be uploaded to the server. Instead, the browser can be pointed to local file or a URL specifying the location of a remote file, and JBrowse will render the data by processing it locally. This is made possible due to the `in browser' execution of JBrowse; all of the software to process the data and display it is included in the JavaScript that is downloaded

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

4 Nucleic Acids Research, 2015

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

Figure 1. Querying multiple species in the WormBase ParaSite BioMart. A taxonomy-tree-based widget allows the selection of whole clades of species for querying, with an auto-complete feature allowing rapid identification of specific species or clades within the tree. The selection of species belonging to the Filarioidea nematode superfamily is shown.

when the user first accesses the tool. Another new feature allows users to perform a degree of in-browser data analysis by combining data in tracks using arithmetic and set operations, for example finding the union, intersection or exclusive or (XOR) of two tracks. Combination tracks can be used as input to other combination tracks, allowing users to build up arbitrarily complex analysis tracks. We have created JBrowse browsers for all core species in WormBase, and these are currently available alongside GBrowse (accessible from the Tools menu, and also as an alternative view on the Location panel on each Gene page). This allows users to migrate to the new tool at their own pace. New ontology browser WormBase annotates genes with terms from established ontologies for anatomy (33) (for spatial expression), disease (34) (for human disease relevance), life stage (for temporal expression) and phenotype (35) (for perturbation outcome), as well as gene function using the Gene Ontology (36). We have added a graphical tool to make it easier for users to navigate between related terms in these ontologies, and to quickly retrieve genes annotated with specific terms. The WormBase Ontology Browser (WOBr) is adapted from

AmiGO 2, which uses Apache Solr to store and index the Gene Ontology and annotations made with it (36). We have generalised the build procedure to allow the loading of other ontologies and annotations, and created an index for each ontology used by WormBase. Building on these indexes, we have developed two interfaces for exploring ontology annotations: (i) a view for each ontology term, which shows the relevant sub-graph of related terms as an interactive tree and (ii) an interactive browser, which allows root-to-leaf navigation of the complete directed acyclic graph for an ontology (see Figure 2).

Data mining platforms The primary data mining platform in WormBase is WormMine, our custom deployment of the InterMine data warehousing software (37). We have recently migrated WormMine to the Amazon Elastic Compute Cloud (AWS EC2), facilitating more frequent updates and greater performance. All WormBase core species are queryable in WormMine. In forthcoming releases we are committed to deepening available data, first to RNA interference experiments and generic access to genomic sequence, and later to all data classes available through WormBase.

Nucleic Acids Research, 2015 5

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

Figure 2. The WormBase Ontology Browser. (A) view of the anatomy term `mechanosensory neuron'. The sub-graph of the term and its ancestors in the hierarchy are depicted graphically to the left. To the right, the inference tree view of the term is shown, with icons indicating relationships in reference to the focus term (P = Part-of, I = Is-a). Numbers to the right are counts of genes annotated with that term (either directly or by inheritance). A list of parallel (sibling) terms to the focus term can also be viewed (bottom right). (B) main entry page for the ontology browser, which allows top-down navigation from the root term for each WormBase ontology.

6 Nucleic Acids Research, 2015

We also provide a separate data mining platform for the WormBase ParaSite portal, using the BioMart technology (38). Whereas WormMine aims to provide access to a rich set of data for a small number of core species, ParaSite BioMart aims to provide simple access to a small set of basic data types (e.g. sequence, orthologs, cross references) for all nematode and platyhelminth genomes, and has been optimised to allow the querying of many species at once (Figure 1).

FUTURE PLANS As we reported previously (6), we are embarked on a major re-design of the back-end infrastructure and curation tools for WormBase. This is a long-term project, but we plan to roll out elements of this in the coming year in a way that will cause no disruption to users. Increasing the utility of WormBase for biomedical research continues to be a priority for the project. We annotate C. elegans genes that have relevance to the study of human disease (42), curating descriptions of the association and forming connections to terms from the Disease Ontology (34) and to disease and gene records in the Online Mendelian Inheritance in Man (OMIM) (43). We plan to take this further by associating mutant alleles in C. elegans with orthologous genomic variants in human, using a combination of programmatic data integration and manual curation. We envision that this will be particularly useful for the study of human variants of unknown significance. On the specific issue of human disease caused by parasitic worms, we plan to augment WormBase ParaSite with additional features for the identification of putative targets for anti-helminthic drugs, for example by linking gene products to the ChEMBL (44) database of medicinal chemistry. ACKNOWLEDGEMENTS After many years working closely with WormBase as the guardian and authority for C. elegans genetic nomenclature, Jonathan Hodgkin has now stepped down from this role. We would like to thank him for his guidance and leadership. FUNDING US National Human Genome Research Institute [U41-HG002223]; UK Medical Research Council [MR/L001220]; UK Biotechnology and Biological Sciences Research Council [BB/K020080 to WormBase]; P.W.S. is an investigator with the Howard Hughes Medical Institute. Funding for open access charge: US National Human Genome Research Institute [U41-HG002223]. Conflict of interest statement. None declared. REFERENCES
1. Platt,H.M. (1994) Foreword. In: Lorenezen,S (ed). The Phylogenetic Systematics of Free-living Nematodes. The Ray Society, London, pp. 1­2. 2. Zhang,Z.Q. (2013) Animal biodiversity: an outline of higher-level classification and survey of taxonomic richness (Addenda 2013). Zootaxa, 3703, 1­82. 3. De Ley,P. (2006) A quick tour of nematode diversity and the backbone of nematode phylogeny. WormBook, doi:10.1895/wormbook.1.41.1. 4. Brenner,S. (1974) The genetics of Caenorhabditis elegans. Genetics, 77, 71­94. 5. Riddle,D.L., Blumenthal,T., Meyer,B.J. and Priess,J.R. (1997) C. elegans II. 2nd edn. Cold Spring Harbor Laboratory Press, Cold Spring Harbor, NY. 6. Harris,T.W., Baran,J., Bieri,T., Cabunoc,A., Chan,J., Chen,W.J., Davis,P., Done,J., Grove,C., Howe,K. et al. (2014) WormBase 2014: new views of curated biology. Nucleic Acids Res., 42, D789­D793. 7. Schwarz,E.M., Korhonen,P.K., Campbell,B.E., Young,N.D., Jex,A.R., Jabbar,A., Hall,R.S., Mondal,A., Howe,A.C., Pell,J. et al. (2013) The genome and developmental transcriptome of the strongylid nematode Haemonchus contortus. Genome Biol., 14, R89.

OUTREACH WormBase fosters a close relationship with its primary users, the C. elegans research community, collecting information on every active nematode laboratory and researcher, and connecting data and research outputs to the scientists that reported them. We continue to implement modern and engaging ways to communicate with our users. Our community forum (forums.wormbase.org) for discussion on general issues of nematode biology now has nearly 2000 members. We also continue to draw attention to interesting events and data sets in WormBase with our blog (blog.wormbase.org) and Twitter feed (www.twitter.com/ wormbase). To expand outreach efforts and engage more closely with our users, we have recently added a new in-line real-time chat feature to our website. Once initiated, users are connected directly with a WormBase staff member. Chat transcripts are automatically posted to our help desk issue tracker to verify that questions and comments are addressed satisfactorily. If a chat attempt is made whilst no operator is online, questions are posted directly to our help desk queue (as always, it is WormBase policy to acknowledge all queries within a 24 h window). Whilst chatting, users can freely navigate to other areas of the website, and even share their screen with WormBase staff to discuss issues. This feature provides us with a rapid way to address common questions or concerns so that users can continue their research without waiting for a help desk response via email.

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

INTEGRATION WITH OTHER RESOURCES An important remit of WormBase is the dissemination of project outputs beyond our own websites. We submit sequence annotations to the International Nucleotide Sequence Database Collaboration (INSDC) (39), and have a formal partnership with the Ensembl (22) and Ensembl Genomes (40) projects, working to ensure that up-to-date data for nematode genomes is displayed in those resources. To further increase the range of resources in which WormBase data can be browsed, we have implemented a WormBase Assembly Hub. Assembly Hubs (41) are an emerging standard for the representation and remote hosting of data that can be displayed by genome browsers. The WormBase Assembly Hub (www.wormbase.org/about/ userguide/wormbasehubs) is updated with every WormBase release, and allows, for the first time, current WormBase genomes and annotations to be viewed in the UCSC genome browser (41).

Nucleic Acids Research, 2015 7

8. Laing,R., Kikuchi,T., Martinelli,A., Tsai,I.J., Beech,R.N., Redman,E., Holroyd,N., Bartley,D.J., Beasley,H., Britton,C. et al. (2013) The genome and transcriptome of Haemonchus contortus, a key model parasite for drug and vaccine discovery. Genome Biol., 14, R88. 9. Jex,A.R., Liu,S., Li,B., Young,N.D., Hall,R.S., Li,Y., Yang,L., Zeng,N., Xu,X., Xiong,Z. et al. (2011) Ascaris suum draft genome. Nature, 479, 529­533. 10. Wang,J., Mitreva,M., Berriman,M., Thorne,A., Magrini,V., Koutsovoulos,G., Kumar,S., Blaxter,M.L. and Davis,R.E. (2012) Silencing of germline-expressed genes by DNA elimination in somatic cells. Dev. Cell, 23, 1072­1080. 11. Holt,C. and Yandell,M. (2011) MAKER2: an annotation pipeline and genome-database management tool for second-generation genome projects. BMC Bioinformatics, 12, 491. 12. Stanke,M., Keller,O., Gunduz,I., Hayes,A., Waack,S. and Morgenstern,B. (2006) AUGUSTUS: ab initio prediction of alternative transcripts. Nucleic Acids Res., 34, W435­W439. 13. Ter-Hovhannisyan,V., Lomsadze,A., Chernoff,Y.O. and Borodovsky,M. (2008) Gene prediction in novel fungal genomes using an ab initio algorithm with unsupervised training. Genome Res., 18, 1979­1990. 14. Korf,I. (2004) Gene finding in novel genomes. BMC Bioinformatics, 5, 59. 15. She,R., Chu,J.S., Uyar,B., Wang,J., Wang,K. and Chen,N. (2011) genBlastG: using BLAST searches to build homologous gene models. Bioinformatics, 27, 2141­2143. 16. Otto,T.D., Dillon,G.P., Degrave,W.S. and Berriman,M. (2011) RATT: Rapid Annotation Transfer Tool. Nucleic Acids Res., 39, e57. 17. Vilella,A.J., Severin,J., Ureta-Vidal,A., Heng,L., Durbin,R. and Birney,E. (2009) EnsemblCompara GeneTrees: complete, duplication-aware phylogenetic trees in vertebrates. Genome Res., 19, 327­335. 18. Nguyen,N., Hickey,G., Raney,B.J., Armstrong,J., Clawson,H., Zweig,A., Karolchik,D., Kent,W.J., Haussler,D. and Paten,B. (2014) Comparative assembly hubs: web-accessible browsers for comparative genomics. Bioinformatics, 30, 3293­3301. 19. Paten,B., Earl,D., Nguyen,N., Diekhans,M., Zerbino,D. and Haussler,D. (2011) Cactus: algorithms for genome multiple sequence alignment. Genome Res., 21, 1512­1528. 20. Hickey,G., Paten,B., Earl,D., Zerbino,D. and Haussler,D. (2013) HAL: a hierarchical format for storing and analyzing multiple genome alignments. Bioinformatics, 29, 1341­1342. 21. Jones,P., Binns,D., Chang,H.Y., Fraser,M., Li,W., McAnulla,C., McWilliam,H., Maslen,J., Mitchell,A., Nuka,G. et al. (2014) InterProScan 5: genome-scale protein function classification. Bioinformatics, 30, 1236­1240. 22. Cunningham,F., Amode,M.R., Barrell,D., Beal,K., Billis,K., Brent,S., Carvalho-Silva,D., Clapham,P., Coates,G., Fitzgerald,S. et al. (2015) Ensembl 2015. Nucleic Acids Res., 43, D662­D669. 23. McLaren,W., Pritchard,B., Rios,D., Chen,Y., Flicek,P. and Cunningham,F. (2010) Deriving the consequences of genomic variants with the Ensembl API and SNP Effect Predictor. Bioinformatics, 26, 2069­2070. 24. Yates,A., Beal,K., Keenan,S., McLaren,W., Pignatelli,M., Ritchie,G.R., Ruffier,M., Taylor,K., Vullo,A. and Flicek,P. (2015) The Ensembl REST API: ensembl data for any language. Bioinformatics, 31, 143­145. 25. Kinsella,R.J., Kahari,A., Haider,S., Zamora,J., Proctor,G., Spudich,G., Almeida-King,J., Staines,D., Derwent,P., Kerhornou,A. et al. (2011) Ensembl BioMarts: a hub for data retrieval across taxonomic space. Database (Oxford), bar030. 26. Stein,L.D., Mungall,C., Shu,S., Caudy,M., Mangone,M., Day,A., Nickerson,E., Stajich,J.E., Harris,T.W., Arva,A. et al. (2002) The generic genome browser: a building block for a model organism system database. Genome Res., 12, 1599­1610. 27. dos Santos,G., Schroeder,A.J., Goodman,J.L., Strelets,V.B., Crosby,M.A., Thurmond,J., Emmert,D.B., Gelbart,W.M. and FlyBase,C. (2015) FlyBase: introduction of the Drosophila

28.

29.

30.

31.

32. 33. 34.

35.

36. 37.

38. 39.

40.

41.

42.

43.

44.

melanogaster Release 6 reference genome assembly and large-scale migration of genome annotations. Nucleic Acids Res., 43, D690­D697. Costanzo,M.C., Engel,S.R., Wong,E.D., Lloyd,P., Karra,K., Chan,E.T., Weng,S., Paskov,K.M., Roe,G.R., Binkley,G. et al. (2014) Saccharomyces genome database provides new regulation data. Nucleic Acids Res., 42, D717­D725. Ruzicka,L., Bradford,Y.M., Frazer,K., Howe,D.G., Paddock,H., Ramachandran,S., Singer,A., Toro,S., Van Slyke,C.E., Eagle,A.E. et al. (2015) ZFIN, The zebrafish model organism database: Updates and new directions. Genesis, 53, 498­509. Eppig,J.T., Blake,J.A., Bult,C.J., Kadin,J.A., Richardson,J.E. and Mouse Genome Database, G. (2015) The Mouse Genome Database (MGD): facilitating mouse as a model for human biology and disease. Nucleic Acids Res., 43, D726­D736. Berardini,T.Z., Reiser,L., Li,D., Mezheritsky,Y., Muller,R., Strait,E. and Huala,E. (2015) The arabidopsis information resource: Making and mining the `gold standard' annotated reference plant genome. Genesis, 53, 474­485. Skinner,M.E., Uzilov,A.V., Stein,L.D., Mungall,C.J. and Holmes,I.H. (2009) JBrowse: a next-generation genome browser. Genome Res., 19, 1630­1638. Lee,R.Y. and Sternberg,P.W. (2003) Building a cell and anatomy ontology of Caenorhabditis elegans. Comp. Funct. Genomics, 4, 121­126. Kibbe,W.A., Arze,C., Felix,V., Mitraka,E., Bolton,E., Fu,G., Mungall,C.J., Binder,J.X., Malone,J., Vasant,D. et al. (2015) Disease Ontology 2015 update: an expanded and updated database of human diseases for linking biomedical knowledge through disease data. Nucleic Acids Res., 43, D1071­D1078. Schindelman,G., Fernandes,J.S., Bastiani,C.A., Yook,K. and Sternberg,P.W. (2011) Worm Phenotype Ontology: integrating phenotype data within and beyond the C. elegans community. BMC Bioinformatics, 12, 32. Gene Ontology Consortium. (2015) Gene Ontology Consortium: going forward. Nucleic Acids Res., 43, D1049­D1056. Smith,R.N., Aleksic,J., Butano,D., Carr,A., Contrino,S., Hu,F., Lyne,M., Lyne,R., Kalderimis,A., Rutherford,K. et al. (2012) InterMine: a flexible data warehouse system for the integration and analysis of heterogeneous biological data. Bioinformatics, 28, 3163­3165. Smedley,D., Haider,S., Ballester,B., Holland,R., London,D., Thorisson,G. and Kasprzyk,A. (2009) BioMart­biological queries made easy. BMC Genomics, 10, 22. Nakamura,Y., Cochrane,G., Karsch-Mizrachi,I. and International Nucleotide Sequence Database Collaboration. (2013) The International Nucleotide Sequence Database Collaboration. Nucleic Acids Res., 41, D21­D24. Kersey,P.J., Allen,J.E., Christensen,M., Davis,P., Falin,L.J., Grabmueller,C., Hughes,D.S., Humphrey,J., Kerhornou,A., Khobova,J. et al. (2014) Ensembl Genomes 2013: scaling up access to genome-wide data. Nucleic Acids Res., 42, D546­D552. Rosenbloom,K.R., Armstrong,J., Barber,G.P., Casper,J., Clawson,H., Diekhans,M., Dreszer,T.R., Fujita,P.A., Guruvadoo,L., Haeussler,M. et al. (2015) The UCSC Genome Browser database: 2015 update. Nucleic Acids Res.43, D670­D681. Yook,K., Harris,T.W., Bieri,T., Cabunoc,A., Chan,J., Chen,W.J., Davis,P., Cruz,N., Duong,A., Fang,R. et al. (2012) WormBase 2012: more genomes, more data, new website. Nucleic Acids Res.40, D735­D741. Amberger,J.S., Bocchini,C.A., Schiettecatte,F., Scott,A.F. and Hamosh,A. (2015) OMIM.org: Online Mendelian Inheritance in Man (OMIM(R)), an online catalog of human genes and genetic disorders. Nucleic Acids Res., 43, D789­D798. Davies,M., Nowotka,M., Papadatos,G., Dedman,N., Gaulton,A., Atkinson,F., Bellis,L. and Overington,J.P. (2015) ChEMBL web services: streamlining access to drug discovery data and utilities. Nucleic Acids Res., 43, W612­W620.

Downloaded from http://nar.oxfordjournals.org/ at Washington University, Law School Library on December 1, 2015

Agile methods are the fastest rising software lifecycle process methods in software engineering. Educators are converting traditional and project-base courses to agile in response, but this is a daunting task with few structured teaching resources methods available to reduce the burden on the educator. In professional practice, agile methods have been particularly effective in empowering experienced software engineers through its focus on empirical process control and constant feedback loop. These process traits are difficult to simulate in an academic setting, as student developers are inexperienced, synchronous meeting times are few and far between, and obtaining meaningful constant feedback a laborious undertaking. This workshop will present a comprehensive approach to teaching Agile methods that is itself agile, employing a highly iterative, continuous feedback-driven process. Pedagogical and assessment strategies will be shared, and the presenter will facilitate a best practices interactive discussion to draw out lessons learned from workshop participants. Specific agile practices with supporting labs from the popular Scrum and eXtreme Programming (XP) process models will be presented. The workshop will also encourage interaction amongst participants to share best practices and lessons learned. Research directions related to the application of agile principles to teaching and learning will be discussed.This work explores the impact of teaching and learning if the rate of learner engagement outside the classroom is continuously measured and available to the instructor and students. We describe an ongoing implementation of a monitoring tool built within a software engineering continuous integration and testing (CI & Test) platform that integrates multiple streams of student activity and performance on yearlong junior software engineering projects. The CI & Test platform allows for continuous and instantaneous feedback, which we will use to inform student behavior change. In the work-in-progress we describe the technology, its impact on the teaching process for the instructor, and preliminary results observing impacts on student engagement behavior.2015 International Conference on Indoor Positioning and Indoor Navigation, 13 th-16th October 2015

CAPTURE ­ Extending the Scope of SelfLocalization in Indoor Positioning Systems
Gary Cullen, Kevin Curran, Jose Santos
Intelligent Systems Research Centre, University of Ulster, Magee College, Derry, United Kingdom Gary.Cullen@lyit.ie
Abstract--The concept of devices cooperatively assisting with the localization of other devices in either the indoor and outdoor arena is not a new phenomenon. The primary focus of research into such a theory is however limited to solving the problem of localization accuracy. This paper outlines our Cooperatively Applied Positioning Techniques Utilizing Range Extension (CAPTURE) system which aims to provide a solution to the current range limitations of an Indoor Position System (IPS). These range limitations are the culmination of well documented difficulties of localizing using wireless signals Non-Line of Sight (NLOS) environments. The coverage of a localization solution is still a challenging issue in the indoor environment. In this paper we implement a version of CAPTURE that uses Wi-Fi Direct and Bluetooth Low Energy (Bluetooth LE 4.0) that takes advantage of mobile devices at the outer limits of an IPS to help extend its reach into blind spots, where devices cannot be located. CAPTURE is evaluated using a live test environment, where range estimations are captured between cooperating devices. These range estimations are filtered before being placed into a trilateration algorithm to position lost devices. Finally the accuracy of CAPTURE is presented, demonstrating the achievable benefits of implementing CAPTURE as a solution to the problem of coverage in an Indoor environment. Keywords--Localization; Indoor Positioning Systems; Indoor Localization; Geographical Positioning; wireless; Wi-Fi; Bluetooth; Cooperative Localization; Collaborative Positioning; Self-Positioning

Gearoid Maguire, Denis Bourne
Letterkenny Institute of Technology, Co. Donegal, Ireland

the aforementioned to be locatable further exacerbates the need for an expansive solution to accurately locate in all areas of an indoor environment. Devices such as these were typically not designed with wireless network capability to merely, assist in locating other devices. Which although, is a great reuse of an existing technology, does however, throw up a secondary issue. If these devices are connected to the network, can we realistically conceive that they will disconnect from that network to connect in a Peer to Peer network to cooperatively assist in locating lost devices? Wi-Fi Direct offers the ability to be in both Ad-Hoc Mode and Infrastructure mode simultaneously [2], Bluetooth LE allows the Wi-Fi chip to remain connected to the network whilst transmitting. Therefore using CAPTURE to extend the range of an IPS using Wi-Fi Direct and Bluetooth LE capable devices allows the user to remain connected to their network whilst cooperatively assisting in locating other devices it can `see'. Generally, IPS implementations can be grouped as either exogenous or endogenous depending on the available infrastructure that can be employed to establish location information. An exogenous infrastructure implementation is typically oriented towards an IPS application. An endogenous solution however, is made up of infrastructure that has not been installed primarily for positioning reasons. Currently, one of the most popular techniques to locate devices in the indoor environment is to use the preinstalled Wireless Access Points (WAPs) which are used to provide wireless network access to mobile devices. Typically, good system implementations are those that achieve an appropriate balance between requirements, technological advances, and costs. Whilst utilizing an existing infrastructure, such as this offers many noble qualities, not least the reduced costs in procuring equipment to implement a solution, it does introduce some problems. The decision process behind the strategic positioning of such equipment to provide mobile network coverage does not fulfil the requirements of an Indoor Positioning System (IPS) to locate devices. Therefore it is inevitable that blind spots should exist in these ISP's. When deciding on the positioning of Wi-Fi equipment such as Wireless Access Points (WAPs) the typical focus of network designers was to provide the highest available throughput to the largest congregations of wireless network users at key areas within the building. The ability to locate devices within that environment was not necessarily to the fore in their decision process, leaving gaping holes in terms of coverage in some of the IPS's currently in-

I.

INTRODUCTION

The indoor location problem has been around for many years and has motivated a great deal of research into finding a solution. Cooperative solutions have made up a significant contribution of this research. Cooperation among devices to self-locate requires a key prerequisite - there must be an adequate number of devices willing to assist in locating a lost device. The proliferation of tablet devices and Smartphones, fully loaded with Wi-Fi, Bluetooth and gyroscopes, somewhat address this need. The advent of the Internet of Things (IoT's) however, providing access to 100's of billions of devices [1] offers an even more fertile community of wirelessly connected smart objects in a connectivity ecosystem. The pace of innovation of wearable computing coupled with tumbling costs, mirrored in the consumer interest of the iWatch offers no sense of a drop off in access to these collaborative devices. Indeed the requirement for nomadic wearable devices such as

2015 International Conference on Indoor Positioning and Indoor Navigation, 13 th-16th October 2015 place. This coupled with some of the architectural barriers to the positioning of WAPs within a buildings infrastructure doesn't suggest a solution to this issue in the near future. In the outdoor arena, apart from some natural obstacles such as overhead trees, cavernous areas and mountainous regions the satellites that make up the Global Positioning System (GPS) have a relatively clear, unobstructed view of the devices they need to locate. There are also some man made obstructions, such as the urban canyon phenomenon [3], that can obscure views in the outdoor world. These also provide barriers to accurate position estimation, but bear little resemblance to the many impediments that make up the indoor environment. The ability to locate in the outdoor world has more or less been solved as a problem through the many advances in GPS technology down through the years. The inability of a GPS signals to penetrate a building's infrastructure after propagating 22,000 Kilometers however, renders it more or less redundant as an indoor positioning solution. Considering we spend more time in the indoor arena, nearly 89% of the time according to a recent study [4] carried out in Canada, the need for a solution is evident. Usually, distance estimations from more than one WAP to a lost device are required so that a positioning algorithm can provide a reasonable level of accuracy. Indeed, sometimes up to four are required to provide positioning estimations on a 3D plane. We aim to show in this paper how CAPTURE can be used in this scenario to augment an existing ISP, assisting in the positioning of devices that would traditionally be unlocatable. Using devices bundled with Wi-Fi or Bluetooth antennae we will make up the required number of, or completely replace the WAPs required. CAPTURE utilizes Received Signal Strength Indicator (RSSI) readings from WiFi, Bluetooth or Wi-Fi Direct enabled devices to estimate the range between mobile devices. The measurement of signal attenuation of these RSSI values between cooperating devices is used to gauge a propagation distance. These distance estimations are filtered, to remove any outliers, before being used as input to a trilateration positioning algorithm. Previous iterations of CAPTURE [5, 6] have used both Wi-Fi and Bluetooth 2.0 to assist in the cooperative position of devices that are beyond the range of standard IPS's. In this paper we propose to use both Bluetooth 4.0 (Bluetooth LE (Low Energy)) and Wi-Fi Direct to find the location of remote devices. Convincing other devices to cooperatively assist in locating lost devices would be impossible if, as part of that cooperation the assisting devices had to sacrifice copious amounts of battery power. In this paper, we will also provide an evaluation of the typically battery consumption when a device is utilized in a CAPTURE implementation. This will provide further evidence of the use of CAPTURE as a solution to the ranging problem. Consider the following scenario, `Bob' is sitting in the far end of the airport lounge reading his newspaper on his tablet and is considering ordering food. He has network connectivity and can see online that his flight is due to leave on time. Bob has been to this airport before but is unfamiliar with the time it would take to get to his specific departure gate, or which area he has to navigate his way through security. The airports IPS could assist with this, but he only has visibility of one WAP. This provides a robust network connection but is incapable of positioning Bob within the airport. Sue is in the airport café some 45 meters to the west of Bob, Sue's phone can be `seen' by 3 different WAPs within the airports network and can be located to within 2 meters of her current position, via the inhouse IPS. Sue's phone can also `see' Bob's tablet. The drinks vending machine in the main hall is 25 meters to the north of Bob, because of its location in the main hall it has access to 7 WAPs that are utilized in the airports IPS. This smart device also has a wireless Network Interface Card (NIC) allowing it to connect to the airport inventory system providing minute by minute updates on its current stock levels. But more importantly it is positioned within the network IPS. The 25 meter distance to Bob's tablet is a simple hop, well within its read range. In a normal scenario Bob would be beyond the range of the airports IPS, but because CAPTURE can utilize the known positions of Sue's phone, the drinks vending machine and the WAP that Bob can currently `see', Bob can be positioned. CAPTURE takes these devices that know their position and estimates range distances from Bobs lost device to them. These range estimates are then placed into a trilateration algorithm to position Bob within the airport. CAPTURE provides a position estimate relative to the devices locating it, which can then be mapped onto a global overview of the airport IPS. Bob can now see that he is 15 minutes from the departure gate, he is advised to go via the security area just behind the lounge. Bob orders the duck, all is good. II. CAPTURE ­ SYSTEM MODEL

Here we describe the overall system model used to implement the CAPTURE solution. CAPTURE provides the ability to locate a lost device relative to devices that were used to assist in its localization. If these devices were originally positioned with an IPS this position can be translated to Cartesian coordinate values on a 2D plane. There is no need for a calibration stage with CAPTURE as would be required with the classical fingerprinting model. CAPTURE uses only real time RSSI values, providing a robust opportunistic solution in dynamic environments. Literature within the realm of Location Based Systems frequently use terms such as Anchor or Anchor Nodes to describe devices that help to determine the position of lost or unknown devices. The term anchor elicits a perception of a static or permanent device, which in a cooperative solution these devices most certainly are not. For this reason we will use the term reference device when describing devices that assist in the positioning of lost or unknown devices. Fundamental to any position estimation algorithm is the ranging technique that is employed to gauge the distance from the transmitting device(s) to the receiving device(s). This is determined using a given metric, for example the length of time it takes a signal to propagate the distance from the transmitter to the receiver. Issues relating to the small scale fading of signals due to reflection, refraction, absorption, diffraction and scattering on obstacles in the indoor environment are non-trivial and have been well documented in

2015 International Conference on Indoor Positioning and Indoor Navigation, 13 th-16th October 2015 the following literature [7-9]. A high percentage of location algorithms use range measurements or angle measurements to estimate distance between a lost device and a particular reference device. CAPTURE uses the Received Signal Strength Indicator (RSSI) to estimate range between devices. This range estimation is then used as input to a trilateration algorithm to ascertain the Cartesian coordinate values of the lost device. In order for CAPTURE to be able to cooperatively locate a lost device within a network, there must be at least 3 reference devices within sight of the lost device. Each of these must have `a prior' knowledge of their location within a pre existing localization solution. During the experiments, different configurations were modelled to mimic the different scenarios that could befall a lost device being positioned using CAPTURE. Initially 3 Smartphones were used to position the lost device, then two phones along with a WAP were used to provide range estimations as input. Finally RSSI readings from a Smart TV, equipped with a Wi-Fi card, a tablet and a Smartphone were used to represent the scenario described as Bob's scenario at the airport. MSM7227 chipset. 3 of the phones were used as reference devices, the other phone acted as the lost device. All equipment used during the experiments were the same make and model ruling out any issues with diverse RSSI reads with different antenna types. Issues relating to varied reads with diverse antenna makes have been documented in the literature [10, 11]. Lisheng et al., [10] describe the bias being as much as 11.2 dBm out with different antenna types over a 25 meter read range. Although these issues describe scenarios relating to WiFi radio signals, it is the opinion of the author that these would have a negative impact on Bluetooth LE transmissions also. Other issues relating to the orientation of devices described in the following literature [12] were also considered during the testing phase.

Figure 2: CAPTURE Client Interface A MySQL Server version: 5.0.96 hosted on a Linux platform was used to store all data collected by the devices. The server was online and the mobile devices wrote directly to it as they recorded RSSI values from each other. The data was then passed through a low level filter to remove any outliers, before an average RSSI reading was calculated for each required ranging measurement, to be used in the trilateration algorithm to estimate the position of the lost device. A Dell Latitude E6440 iCore3 running Windows 7 Professional was used to develop the app to gather the RSSI from the phones. An algorithm was designed to convert this RSSI reading into a ranging measurement before a trilateration algorithm converted the ranging measurements into Cartesian coordinate values. We used the Eclipse IDE and Android Software Development Kit (SDK) for Android development and debugging, to develop the app. IV. DATA COLLECTION AND PRESENTATION

Figure 1: Test Environment III. EXPERIMENTAL TEST BED

In this section, we will demonstrate the suitability of CAPTURE as a solution to the indoor ranging problem. We will back up this assertion with evidence based tests, carried out in a large campaign of measurements taken in a Sports Hall. The hall offers a 40m wide diagonal testing environment, providing Line of Sight measurements for all tests, as can be seen in the image depicted in Figure 1. Each device used in the test is given a name (BSSID) TestDevice1, TestDevice2 for example. CAPTURE takes the RSSI readings of all available reference points, i.e. all devices it can `see', but it filters out only the test devices selected by the user carrying out the tests. This is achieved via a lookup table mapping the MAC address of the device to the device name and allows us to work only with cooperating devices, allowing the use of only a specified device or a group of device during any given test. The experimental setup of CAPTURE was made up of 7 Samsung GT-S5310 Galaxy Pocket phones, running Google Android 2.2.1 on a 600 MHz ARMv6, Adreno 200 GPU, Qualcomm

In this section we provide an overview of some of the data collected during the experiments that were carried out during this implementation of CAPTURE. An initial test was carried out to provide an average 1 meter read range for the ranging algorithm. This test involved over 500, 1 meter RSSI readings recorded at different locations throughout the test area. Any

2015 International Conference on Indoor Positioning and Indoor Navigation, 13 th-16th October 2015 outliers were removed with a simple filter, this allowed for the accurate depiction of this reading to be used for the ranging algorithm. Additional tests were carried out to measure the accuracy of the RSSI readings and the computed range estimations achieved by the algorithm. The following table, Table, Table I. CAPTURE RSSI Readings, outlines some of the readings achieved during this initial test phase of the implementation.
TABLE I. CAPTURE RSSI READINGS RSSI Readings Distance Average Std. Dev Estimate Distance Average Std. Dev Estimate 0­5m -57.264 0.4996 4.517 0 ­ 25 m -68.38 0.6884 21.544 0 ­ 10 m -61.5652 0.4 8.269 0 ­ 30 m -70.75 0.9797 30.059 0 ­ 15 m -69.5263 0.85346 25.31 0 ­ 35 m -71.854 0.6803 35.104 0 ­ 20 m -67.5662 0.48992 19.216 0 ­ 40 m -73.681 0.7901 45.379 [1]

object of some description, and the final scenario were only mobile devices existed within range of the lost device. It is proposed that further experiments (in the coming months) will be implemented to replicate each scenario in detail. Each of these would be evaluated to establish the feasibility and accuracy of each which would further advocate CAPTURE as a solution to these problems.

REFERENCES
G. Kortuem, F. Kawsar, D. Fitton, and V. Sundramoorthy, "Smart objects as building blocks for the Internet of things," Internet Computing, IEEE, vol. 14, pp. 44-51, 2010. [2] W.-F. Alliance, "Wi-Fi Certified Wi-Fi Direct," White paper, October 2010. [3] M. Spangenberg, J. Y. Tourneret, V. Calmettes, and G. Duchateau, "Detection of variance changes and mean value jumps in measurement noise for multipath mitigation in urban navigation," in Signals, Systems and Computers, 2008 42nd Asilomar Conference on, 2008, pp. 11931197. [4] C. G. Richardson, J. Memetovic, P. A. Ratner, and J. L. Johnson, "Examining gender differences in emerging tobacco use using the adolescents' need for smoking scale," Addiction, vol. 106, pp. 18461854, 2011. [5] G. Cullen, K. Curran, J. Santos, G. Maguire, and D. Bourne, "CAPTURE - Cooperatively Applied Positioning Techniques Utilizing Range Extensions," in Indoor Positioning and Indoor Navigation (IPIN), 2014, 2014, pp. 248-254. [6] G. Cullen, K. Curran, J. Santos, G. Maguire, and D. Bourne, "To wireless fidelity and beyond & Beyond; CAPTURE, extending indoor positioning systems," in Ubiquitous Positioning Indoor Navigation and Location Based Service (UPINLBS), 2014, 2014, pp. 248-254. [7] T. S. Rappaport, Wireless communications : principles and practice, 2nd ed. ed. Upper Saddle River, N.J. ; [Great Britain]: Prentice Hall PTR, 2002. [8] S. S. Haykin and M. Moher, Modern wireless communications. Upper Saddle River, N.J.: Pearson/Prentice Hall, 2005. [9] A. F. Molisch, Wireless communications, 2nd ed. ed. Oxford: John Wiley & Sons, 2011. [10] X. Lisheng, Y. Feifei, J. Yuqi, Z. Lei, F. Cong, and B. Nan, "Variation of Received Signal Strength in Wireless Sensor Network," in Advanced Computer Control (ICACC), 2011 3rd International Conference on, 2011, pp. 151-154. [11] F. D. Rosa, X. Li, J. Nurmi, M. Pelosi, C. Laoudias, and A. Terrezza, "Hand-grip and body-loss impact on RSS measurements for localization of mass market devices," in Localization and GNSS (ICLGNSS), 2011 International Conference on, 2011, pp. 58-63. [12] K. Kaemarungsi and P. Krishnamurthy, "Properties of indoor received signal strength for WLAN location fingerprinting," in Mobile and Ubiquitous Systems: Networking and Services, 2004. MOBIQUITOUS 2004. The First Annual International Conference on, 2004, pp. 14-23.

V.

CONCLUSIONS

A comprehensive solution to localization in the indoor arena poses a uniquely complex set of problems. Problems that have fostered growing interest, but for which, a single or indeed swift answer does not currently exist. The success of GPS has further exacerbated the need for an indoor representation. This paper introduced a novel architecture for cooperative localization, CAPTURE, a solution to extend the range of an Indoor Positioning System. The proposed approach provides an efficient reliable mechanism to plug into an in-situ solution. An implementation of CAPTURE was evaluated and tested which demonstrated the ability of CAPTURE to augment an existing indoor localization solution to locate mobile devices. Battery consumption was highlighted earlier in the paper, this will be adequately analyzed in a larger version of this paper. Three different scenarios were also outlined describing specific situations where typically devices could not be located using a standard solution. The first of these scenarios was, were only one WAP was reachable by the lost device, the second were the lost device could `see' a WAP and a smart

Designing a Mobile Application to Support the Indicated Prevention and Early Intervention of Childhood Anxiety
Mandar Patwardhan
Arizona State University School of Computing, Informatics and Decision Systems Engineering Ira A. Fulton Schools of Engineering Tempe, AZ USA

Ryan Stoll
Arizona State University Department of Psychology Tempe, AZ USA

Derek B. Hamel
Arizona State University School of Computing, Informatics and Decision Systems Engineering Ira A. Fulton Schools of Engineering Tempe, AZ USA

Ryan.Stoll@asu.edu

mpatward@asu.edu Ashish Amresh Kevin A. Gary

dbhamel@asu.edu Armando Pina
Arizona State University Department of Psychology Tempe, AZ USA

Arizona State University Arizona State University School of Computing, Informatics and School of Computing, Informatics and Decision Systems Engineering Decision Systems Engineering Ira A. Fulton Schools of Engineering Ira A. Fulton Schools of Engineering Tempe, AZ USA Tempe, AZ USA

Armando.Pina@asu.edu

amresh@asu.edu ABSTRACT

kgary@asu.edu 1. INTRODUCTION
Mobile health applications (mHealth apps) span a wide spectrum of health-related issues and treatment approaches, such as health monitoring (physiological or self-reported), protocol adherence through reminder communications, and (psycho)education [15]. Interestingly, the ubiquitous and familiar nature of smartphone devices creates the potential for mobile health (mHealth) applications targeted to youth "at risk" for anxiety disorders or meeting criteria for anxiety disorder diagnoses. In fact, mHealth for anxiety disorders may be of unique importance because most parents do not seek help for their anxious youth, effect sizes from anxiety programs are generally modest and need to be potentiated, and there is a pressing need for sustainable and streamlined intervention efforts that have "real world" utility [2][3][13]. In addition, targeting anxiety disorders is of public health significance because these are among the most prevalent psychiatric problems in children with rates ranging from 5% to 10% and as high as 25% in adolescents. Anxiety disorders also cause significant impairment, typically fail to spontaneously remit, and are prospectively linked to clinical depression and problematic substance use for some youth [13]. Although the popularity of mHealth apps is exploding, few lessons have been shared regarding the user experience design for such innovations. Building on randomized control trial (RCTs) studies and theory, this research focuses on the design process for adapting aspects of an empirically informed child anxiety disorder intervention to a smartphone platform. Thus, this work is significant due to the domain (anxiety), the nature of the intervention (preventative-early intervention), the use of an app to increase protocol efficiency, and the integration of concepts from innovative design technology (gaming, notifications, user experience design) to improve outcomes. Focusing on the anxiety protocol, it is important to note that considerable strides have been made to develop evidence-based treatment and prevention armamentaria targeting youth anxiety with almost every protocol employing the same cognitive and behavioral procedures (Fisak et al., 2011; Silverman et al., 2008) .

This paper presents the design of an mHealth application for prevention and early intervention of childhood anxiety. The application is based on REACH, a preventative-early intervention protocol for childhood anxiety. This paper describes the multidisciplinary design process, sharing lessons learned in developing an effective mHealth application. This mHealth application is unique due to participant age, preventive-early intervention focus, and utilization of mobile technology in a situated manner. A design process inspired by user-centered leveraging key informant interviews was used to identify application features, including game based strategies and an animated motivational avatar. Validation was performed through external review and a usability study performed with target end users of the application. Results suggest overall satisfaction, ease of use, and increased motivation.

Categories and Subject Descriptors
D.2.2 [Software Engineering]: Design Tools and Techniques ­ Evolutionary prototyping, and user interfaces.

General Terms
Design, Human Factors, Verification

Keywords
Youth Anxiety Prevention, mHealth, User-Centered Design.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Wireless Health `15, October 14­16, 2015, Bethesda, MD, USA. Copyright 2015 ACM 978-1-4503-3160-9 ...$15.00. http://dx.doi.org/10.1145/2811780.2811954

REACH for Success (REACH hereafter) is a school-based cognitive-behavioral protocol designed for 4th and 5th graders for the indicated prevention and early intervention of childhood anxiety and related problems. REACH uses procedures found to be efficacious in RCTs, including in our own 3 RCT [8][9][12]; however, there are several features that set REACH apart. Most relevant to this paper is data suggesting that the classic design of evidence-based prevention programs (including programs like FRIENDS [1]) is simply not feasible or sustainable in schools (e.g., there are too many sessions, sessions are too long, manuals are too cumbersome and not organized for real worldimplementation, too much training is required, and preparation is too time consuming). In contrast, REACH was created from our evidence-based exposure-based cognitive-behavioral protocols as a practical intervention that can build a foundation for sustainable large-scale diffusion. That is, REACH was streamlined into 6 sessions (instead of the typical 12-15), each 20-30 minutes in length (rather than the typical 60 to 90 minutes), and uses an easyto-follow manual (each session is condensed into one page front and back while FRIENDS, for example, has an 89 page manual). One concern with REACH, however, is that such a streamlined protocol may result in a lower dosage of the active change ingredients and fewer opportunities for youth to practice coping skills because there are fewer sessions and less practitioner feedback time. This concern is justified as a recent child anxiety treatment study evaluating an 8 session adaptation of the 16 to 20 session Coping Cat program yielded lower youth response rates suggesting that difficulty practicing the skills was a major impediment to recovery [11]. A purpose of this research was to design an mHealth platform to accompany the REACH 6 session school-based preventative early intervention protocol. Specifically, the goal was to develop an mHealth app that: (a) provides on-demand opportunities for skill practice, (b) uses notifications relevant to skill practice to improve compliance, (c) offers tools for personalizing and tailoring the protocol, (d) increases opportunities for corrective feedback based on user data amenable to creating personalized reports of youth weekly practice and response, and (e) yields high user ratings along core validated usability dimensions relevant to technology innovation efforts. Herein, the REACH protocol, the app design process, and the app implementation are described. Results from an empirical study in a usability context are presented. To set domain context, the face-to-face protocol is described followed by a discussion on design, implementation, and usability.

game). S6: Learn to face situations and Engage in behavioral exposures to mild-moderate anxiety-provoking situations. Core skill acquisition and practice tools include the use of Daily Diaries, Guided Relaxation, STOP acronym, and STIC acronym. Relevant to the REACH app, Daily Diaries are used to facilitate self-evaluation of emotion expressiveness. Youth self-monitor and describe in writing the anxiety or fear provoking situations that occurred during the week. Youth also rate using a 0-8 feelings thermometer the severity of anxiety/fear associated with the situation. Lastly, youth describe in writing thoughts that occurred before/during/after the situation (e.g., worries) and actions that resulted (e.g., avoidance behaviors). In terms of Guided Relaxation, youth are provided with pre-recorded standardized step-by-step procedures designed to improve self-regulation of anxiety related physiological hyperarousal via breathing exercises, muscle tension/release exercises, and imagery. When it comes to cognitive self-control, a four-step coping plan is introduced via the "STOP" acronym where S = Scared? T = Thoughts, O= Other [thoughts], P = Praise. STOP is first practiced via the Worryheads game by using pre-written emotionally ambiguous and anxiety provoking scenarios along with an accompanying "worry thought". Youth are then asked to change the "worry thought" for a more realistic and alternative solution to the scenario provided. In the game, successful resolution of the worry thought results in advances toward a common goal for each player (reaching the end to win the game). Subsequently, with basic knowledge of STOP, youth engage in prospectively applying the technique to situations that emerge as anxiety or fear provoking for them during the course of each week. Lastly, behavioral exposures are introduced via STIC jobs (STIC = Show That I Can. STICs are provided in the form of a pre-written or prepopulated Fear Hierarchies based on modules from the Anxiety Disorders Interview Schedule for Children where each avoidance behavior has been pre-populated for the child as individual exposures. The REACH protocol has been implemented using a paper-andpencil approach. The protocol, while effective, encountered some common limitations in practice, notably protocol compliance. Specifically, subjects did not practice skills between sessions or were not diligent in recording practice activity and outcomes. Further, as noted in section 1, lower dosage in the related Coping Cat tool resulted in lower response rates. Data capture with paperand-pencil methods is also time consuming and subject to human coding errors or oversights. The psychology researchers believed mobile and gaming technologies could effectively address the limitations, improve compliance and data capture, thereby reducing dosage while increasing effectiveness. They teamed with software engineering researchers to conduct a multidisciplinary design and development process to construct the app.

2. THE REACH PROTOCOL
REACH for Personal and Academic Success is an indicated prevention and early intervention program targeting anxiety disorders and related problems in youth. The protocol is administered in a group format (five to seven children per group). Each session (S) in the manual is organized in terms of Overview, Content (didactic, games), Review/Closing, and After the Session (homework). Self-evaluation of emotion expressiveness is embedded in every session. The protocol focuses on broad-based exposure and problem solving skills, which have a wide reach for the range of anxiety disorders targeted. Unique session content is as follows. S1: Introduction (group name, rules, and confidentiality), Learn about emotions, and Relaxation. S2-3: Define worries, Learn cognitive self-control, and Practice cognitive self-control (Worryheads game). S4: Define social skills and Learn about conversation skills (starting and managing conversations). Practice conversations (make-believe game). S5: Learn about assertiveness and Practice assertiveness (stand-up!

3. DESIGN PROCESS
The multidisciplinary team embarked on a highly iterative design process focused on the capabilities and context of end users. The researchers aspired to use a user-centered design (UCD) approach, but in practice the designers did not have direct access to end users during the design process and as such relied on subject matter experts (SMEs) as proxies. The SMEs were the psychologists who developed the REACH protocol and had deployed it 56 times to youth over 6 months. Section 5 describes external validation via design review by a school advisory board and a usability study with independent youth end users (n=22).

3.1 Gap Analysis
REACH is a pre-existing protocol, so the first design activity was to review program materials and workflow, seeking opportunities to effectively translate existing steps, and later innovating on smartphone-specific solutions to achieve the domain objectives for increased dosage, engagement, and feedback (see Section 1). To better understand the domain of the app, the SMEs shared the provider manual of the REACH protocol to the designers and the materials for delivering the protocol (board games, handouts, MP3s). The manual describes how the sessions, each conducted consecutively over the course of six weeks, employ specific practice worksheets, information gathering forms, and interactive exercises designed to train youth in the preventive and coping skills. The main activities defined in the manual were Daily Diary, Relaxation, S.T.O.P, Worryheads board game, and STICs. Table 1 summarizes the protocol component steps and highlights challenges in porting these steps to the mobile environment. Table 1: REACH protocol components and gap analysis
REACH Daily Diary Relaxation S.T.I.C. S.T.O.P. Worryheads Component Description / Design Challenges Self-monitoring engagement; daily compliance; rich data entry Pre-recorded audio exercises media porting and translation Behavioral exposures with adult feedback preserving steps; rewards; feedback Self-application of cognitive self-control plan encouraging tool engagement through positive UX Learn and practice cognitive self-control plan with provided scenarios detailed alternatives; increasing dosage; feedback

Issues #2 and #5 were more significant. Issue #2 represents a "blind spot" in design, due to factors such as missing information implicitly understood by the SMEs but not apparent to the design team. Issue #5 was a recognition that the design team did not understand who would be using the app and in what context. At this point the design team realized a more patient-centric approach was required to overcome these design obstacles.

3.2 A Patient-centric Design Process
The design process described in the previous section focused on translating a field manual; it is not surprising that the translation had gaps derived from implicit knowledge assumed by the manual authors and not understood by the designers. The software engineering researchers suggested a more user-centric approach, where the needs of the end user, in this case the patients, is the focus of the design process. The gold standard for such a design process is User-Centered Design (UCD), originally credited to Norman and Draper [7]. UCD assumes a participatory design process with end users, but for this research we prefer the more inclusive definition of UCD as "the active involvement of users for a clear understanding of user and task requirements, iterative design and evaluation, and a multi-disciplinary approach." [14]. ISO 9241-210 [4] identifies 6 principles to UCD (quote): 1. The design is based upon an explicit understanding of users, tasks and environments. 2. Users are involved throughout design and development. 3. The design is driven and refined by user-centered evaluation. 4. The process is iterative. 5. The design addresses the whole user experience. 6. The design team includes multidisciplinary skills and perspectives. These principles were especially attractive to the design team due to the uniqueness of the domain and protocol, and identified issues understanding the end user context. The team realized the app would not be a direct translation of the paper-based REACH protocol, and needed to focus on context and end user experience. There is a wide range of practices supporting UCD; the design team utilized personas, prototyping with iterative feedback, participatory design, and end user validation. The SMEs served as participatory designers, eliminating the back-and-forth ad hoc aspects of the initial process. They also served as proxies for the end users during design as gaining access to youth (4th-5th grade users for an extended time for intense design activities was not possible). Access to end users would have certainly been preferable during the design process but was not possible at the time. However end user validation was emphasized before approving the app for protocol trial; these results are reported in section 5. Fortunately, prior domain research and SME interviews from the gap analysis proved useful in the context of the UCD.

A round of stakeholder interviews involving the SMEs followed the domain research of the REACH protocol. These included working sessions between the design team leads and the SMEs, visits by the SMEs to the design team's lab, and synchronous question-answer sessions over email and videoconferencing. This step of the process addressed difficulties relating to understanding the protocol and assumptions on both sides regarding implementation objectives. This step took longer than expected, with a result of inconsistent understanding of implementation outcomes. The design team conducted an internal review to identify root causes and come up with design process alternatives. The causes identified included: 1. New terminology. 2. Gaps in understanding by the design team with respect to the protocol. 3. Assumptions of the designers based on past implementations of mHealth apps in non-preventative domains. 4. Ad hoc communications patterns between SMEs and the design team, and within the design team itself. 5. A lack of understanding of the end user context. Together, these issues are not uncommon in design processes, and some were addressed (1, 3, 4) through simple awareness of the issue in the team review. For example, improving ad hoc communication patterns was improved through more frequent design team meetings, clarifying the lines of communication with SMEs, and reiterating design team understanding of requirements back to the SMEs for validation.

3.2.1 Personas
The design team started the UCD process by developing personas, or proxies for categories of end users, and inviting the SMEs to review them. The SMEs were not familiar with personas, and after overcoming initial confusion about the technique, gained enthusiasm and effectively provided useful feedback. The personas shared with the SMEs are presented in Table 2. Iterating over these personas led to several design insights that were previously not understood by the design team. For example, the design team came to understand subjects in this domain have a higher need for re-assurance; respond well to attention and approval, and are highly compliant (persona 2). Discussion of the

personas with the SMEs further revealed that in community samples girls are more likely identified as "anxious" than boys, and anxious youth fear the evaluative nature of social situations (personas 3 and 4). After capturing a clearer idea about end user context through discussing the personas created with the SME, the design team started a phase of rapid prototyping to ensure the SMEs provided frequent feedback on each design decision. Table 2: REACH protocol components and gap analysis Persona 1 Jacob is 10 years old, and is currently being raised by his single mother. He was held back for behavior problems as he tends to lash out when stressed. When confronted with even minor change he shuts down, and becomes irritable. His goal is to do as little as possible, or just enough so he doesn't get in trouble. Persona 2 Jessie is 9 years old and very shy. In larger groups of 10 or more people she panics, and is dangerously on edge. She has a strong recognition of her symptoms, and works very hard at overcoming them. Her goal is to be free from required effort as soon as possible. Persona 3 Mike is 12 years old. He finds it difficult interact in groups. He thinks that everyone has prying eyes on him and judging his every move. He loves to read books and is distracted by day dreaming. He gets very anxious and nervous in social situations. Persona 4 Elizabeth is 10 years old. She is relatively overweight and is embarrassed in evaluative situations. When her classmates tease her, she cries and withdraws from interacting with peers. This typically happens during physical education and school games.

Figure 1: S.T.O.P. Mockup in Pencil

3.2.2.2 Translating Protocol Components
As identified in the gap analysis (section 3.1), some protocol components are a fairly straightforward translation, or port, to the mobile app, while others are not. For example, the Relaxation audio components were a straightforward port of the media to the device wrapped with a simple consistent interaction metaphor. Of course this component also requires the least user interaction of any of the components. On the contrary, the Worryheads game is a multiplayer board game involving cards. The app required limiting the game experience to a single user compared to the multiplayer board game. The design team replaced the physical cards in the board game with preset "Situations" and "Thoughts" screens. The user was then presented with a choice of four of "Other Thoughts" options to choose from. Once the user selects a choice from possible options a praise message was showed on the screen to appreciate the correct answer. Screens depicting Worryheads are shown in section 4. A design concern in translating the protocol was the significant amount of text a child is asked to input during activities such as the Daily Diary and S.T.O.P. The mobile device is not suited for textual input that goes beyond instant messaging or social media apps, and further the end users are at an age where they are often mobile-aware, but not proficient mobile typists. The fear was that textual input would be skipped or significantly limited, or in the worst-case cause frustration of the app to the extent children would abandon it. The design team identified speech capture input as a means to facilitate better information capture.

3.2.2 Rapid Prototyping
Rapid prototyping is an iterative design technique refining the details of interaction models and overall user experience. Early prototypes, or storyboards, focus on task sequences, or the mapping of task workflows to interface screens. This leads to user interaction modeling; the identification of user input actions effecting transitions between screens or for the capture of critical information. Later iterations refine these models and also layer in thematic elements, until a final design is converged upon. Iterations are meant to be short, frequent, and focused on answering specific questions regarding the user experience.

3.2.2.1 Storyboarding and Clickthrough Prototypes
The design team used the freely available Pencil prototyping tool to construct screen and clickthrough mockups. Clickthroughs take simple screen mockups and overlay "hot regions" that advance the mock to a new screen, simulating a user interaction. One drawback is the tool runs its simulations in a web browser so tap and swipe gestures are not supported; however, the tool does support mobile UI "skins" to promote a look-and-feel consistent with the mobile user experience. Figure 2 shows an example of an early mockup created for S.T.O.P. activity. The team created mockups of different scenarios in the app. Each mockup was peer-reviewed within the design team, validated against the documented protocol, and then presented to the SMEs for feedback. The design was iteratively refined until the scenario interactions were adequately captured, and the design team felt comfortable moving to implementation on the Android platform.

3.2.3 Injecting Innovations in the Mobile Experience
A challenge in applying mHealth concepts to existing clinical protocols is the desire to innovate versus leveraging validated protocol steps. For this project, the mobile platform provided the means for increasing dosage by virtue of the device being everpresent. However, ubiquity is not enough, end users must be motivated to practice the protocol. Engagement was addressed through innovative features introduced in the mobile platform including thematic and age-appropriate media, game strategies (e.g. progressive reward incentives), and mobile notifications.

3.2.3.1 Designing an Appropriate Theme
A user interface theme refers to the consistent application of stylistic elements such as images, fonts, audio or video media, and user interface widgets (buttons, menus, taps, etc.). To gain acceptance of the app amongst users familiar with the paper protocol, the design team used the same theme used in the paper protocol. The team ensured that color codes and the fonts used in paper based protocol and the fonts used in the app are same. To

design the features of the app, the team studied the paper-based versions of the activities to be performed by youth to get a better idea of how to replicate the activities in the application. The team followed the same nomenclature of the existing activities in the screen designs reduce confusion and gain rapid acceptance. The user experience required a gender-neutral, age-appropriate proxy for the human guide who assists in the existing REACH protocol. This proxy personifies the guide, providing instruction and feedback to the end user through the mobile interface. Initial ideas focused on themes such as "feed your pet" or "grow your plant" but were rejected as being either too "babyish" for the target age range or gender-biased. The design team came up with the idea of an animated motivational character in the form of a blob. The design team referred to the character as "Bob the Blob" (Figure 3), but the male name is never used in the app itself. Based on game design concepts, "Bob" presents an age-appropriate, gender-neutral proxy for protocol guidance and feedback [6][8].

Fixed schedules are daily time-based notifications, such as for the Daily Diary, to complete a regular interval task. Adaptive notifications require tracking end user interactions with the app and dynamically determining whether to issue a notification to engage with Bob the Blob again. The designers are concerned with the notion of alarm fatigue through over-notification, though currently the mobile device is given to the end users as a locked down tool for practicing the protocol, and not as a generalpurpose smartphone for personal use.

3.2.3.4 Security and Privacy
Any mHealth app needs to be concerned with how user data is stored, transmitted, and identified. These concerns can become overbearing nonfunctional requirements on the app and down to the underlying mobile operating system providing the communication and storage services. At this stage of the app's development, it made more sense to de-identify data and work in a locked-down, disconnected mode. There were several simplifying assumptions the design team was able to make: 1. The emphasis on increased dosage over remote monitoring of compliance or personal health measurements puts this project in a different class of mHealth apps. Such apps push data to remote providers (often via a cloud-based service) and support human or automated communication reminders. 2. The relatively small number of participants in planned early studies meant the devices, with a specific chosen version of the mobile operating system, could be purchased and distributed to end users. The design team selected a Motorola phone running Android API version 19 (KitKat). 3. The relatively small number of participants makes it easier to de-identify the data and manage it external to the app. A secret user interaction combined with a password protects access to functionality that supports exporting user interaction and task completion data (see above). Of course these assumptions will have to change in future generations of the platform to facilitate broader adoption. But as a dosage augmentation platform, the design team leveraged the weekly visits with the psychologists combined with the computational sophistication of modern smartphone platforms to provide a self-contained solution.

3.2.3.2 Progressive Reward Incentives
While one of the goals of the REACH protocol is to empower youth to be intrinsically motivated to enact the protocol, at the training stage it is imperative to repeat the dosage faithfully in order to attain this intrinsic motivation. A common gamification technique is to employ leveled rewards as an extrinsic motivator for performing a targeted behavior. Therefore a simple progressive (leveled) set of rewards for extrinsic motivation included in the app design. When an end user completes a task from the REACH protocol they get a reward in the form of Bob's abilities/tricks. This way the user is motivated to follow the protocol and completing the tasks (dosage) so s/he can unlock more complicated tricks for Bob. One concern SMEs raised during the design process was the potential to inadvertently punish the child for not performing a task. Given the domain, a design invariant was specified to keep all interactions with the child positive; therefore, all language and emotive expressions of Bob throughout the app were scrubbed to ensure there were no negative connotations. For progressive rewards, a setting in the app was designed to unlock new tricks twice every week. The presence of these tricks also served as extrinsic motivation for engagement.

3.2.3.3 Smartphone Notifications
Mobile platforms offer an "always on" communications channel between service providers and end users. Most categories of mHealth apps emphasize the communications channel between clinicians and patients, or between patients and automated big data platforms on the cloud. This project is unique in that it does not leverage the mobile device as a communications channel. In this generation of the app, the focus is on leveraging the device as an information collector and dosage vehicle for the protocol. In this sense the device serves more as a Personal Digital Assistant (PDA) than as a connected mobile phone. In this modality it is still important to present to the end user a feeling of connectedness. The personification of Bob the Blob as a proxy guide is one way the design provides this connectedness. As a second design concept, the design team wanted to make use of mobile notifications, but without relying on cloud-based push notifications as these would require a persistent network connection. Therefore the design supports local notifications presented to the end user in both fixed and adaptive schedules.

4. APP IMPLEMENTATION
The Android platform was selected to support the app. The openness of the Android platform, the availability of low-cost devices, the ease of the Google Speech API, and the ability to deploy the app without the involvement of an app store were the deciding factors for the first generation of the app. This section briefly describes the implementation on the Android platform. The final user interaction model combined with scheduled interactions per protocol rules is shown in Figure 2. This timeline in Figure 2 is based on weeks one to six of the REACH training program. Daily Diary, as the name suggests needs to be made available daily for all the six weeks whereas the Worryheads needs to be made available only in third, fourth and fifth week of the training program.

When the user selects the app from the Android home screen, a landing page is shown allowing the user to select from 5 available activities (see Figure 3, upper left). At any time only activities that are available can be selected from the landing page. Further, activities that are overdue are highlighted by a soft gold pulsing glow around the button (not shown) to provide a further visual cue to the end user to perform an activity. The S.T.I.C activity is shown in the upper right in Figure 3. In this activity end users are encouraged to do a task they would normally avoid due to their anxiety. In the paper protocol, once a child completes the activity s/he receives a physical stamp from an adult (usually a teacher or parent). In the app this was implemented as a secret code entered by the adult, who could then provide an electronic stamp of approval. The S.T.O.P. activity (Figure 3, mid-left) asks the child to provide responses to a set of questions (see section 2). Each response is stored in a SQLite database on the device. Figure 3, mid-right shows the "O" (Other Thoughts) step of the Worryheads game. This is basically a variant of the S.T.O.P. activity with preselected "S" and "T"s. The child has to consider the given "S" and "T" and select an appropriate "O" and "P" to complete the simulation. At the conclusion of these activities Bob the Blob praises the child (Figure 3, bottom right). The Daily Diary (Figure 3, bottom left) is a scheduled activity available to the child each day. The activity is available during school hours but notifications are only given after school hours. As described in section 2, the Daily Diary asks the child to reflect on potentially anxiety-provoking events from her/his day, and inquires about thoughts that came to mind in that situation. Youth also rate how s/he handled and felt about the situation. This embedded diary is part of the organizational framework of REACH emphasizing the need to identify and confront anxietyprovoking situations that are threatening but manageable. In addition to the 5 protocol activities available from the landing page, the end user also can tap directly on Bob the Blob and be taken to a table-oriented layout of "tricks" Bob can perform. The tricks (animations) available at any time are based on the protocol schedule as described in section 3.2.3.2. Additional features were provided by the app to support research outcomes (section 2). An on-device database stores all end user responses, and tracks each user action. The latter will be used after trials to answer research questions such as whether alarm fatigue occurred, or end users were not sufficiently motivated to engage with the app. A data export feature provided only to interventionists allows data to be offloaded as csv files. S.T.O.P. Worryheads Finally, in the face-to-face protocol trial, interventionists can personalize dosage schedules or tailor training activities during weekly visits. To support this in the app, a hidden feature was embedded only for the interventionist role. A specific multi-tap sequence combined with a secret PIN unlocks this feature so interventionists can decide if a protocol component should be enabled/disabled or otherwise modify the planned dosage for that week. Additional settings include selecting the start date of the protocol, notification time windows and frequency, the schedule trick release, changing the teacher PIN, and exporting data.

Figure 2: REACH App intervention Timeline

Landing page

S.T.I.C.

5. VALIDATION
Daily Diary Positive Reinforcement The highly iterative participatory design process described in section 3 enabled continuous feedback during app evolution. After completing the initial candidate release version, the design team and psychologists conducted two types of external validation. The first was two feedback sessions with external SMEs from a school

Figure 3: REACH App Interaction Screens

advisory board (SAB). The second was a usability study conducted with actual youth end users in the schools.

5.1 Advisory Board Feedback
The SAB consisted of two school psychologists with experience delivering REACH, and two school district administrators who oversee student services and prevention efforts for 47 K-8 schools. Based on their experience with youth, the SAB considered the developmental appropriateness of the design and program tools included (e.g., during the face to face sessions, youth wanted to utilize Relaxation and play Worryheads ondemand, so those activities were selected for inclusion in the app). From the SAB feedback, three issues emerged: 1. Safety and security - would youth have access to texting and Internet on the devices? 2. Cost: would parents be responsible for the devices, if lost? 3. Flexibility - would versions of the app be available for the iPhone, smartboards, and tablets? The first issue was addressed by adding security software SureLock to every device. The second was addressed by applying procedures used by the school relevant to laptop computers where parents are financially responsible. For flexibility, it was determined that preliminary data is necessary prior to investing in additional versions of the technology for different devices.

at a university laboratory or at their school. At the beginning of the study, each youth was provided with an envelope that contained a device and a questionnaire. After receiving the study materials, three phases (1-Listen to the Relaxation; play Worryheads game; 2-Write a daily-dairy or S.T.O.P. entry; 3-Play with the Blob) were implemented by trained research assistants. For a phase, each prescribed interactions with the app was 2minutes and responding to the survey lasted about 5 minutes. At the end, youth were thanked for their participation in the study, which lasted a total of 20 to 30 minutes. Parents of participant youth were provided with $15.00 at the end of the study.

5.2.4 Results
Descriptive statistics and correlations for the focal variables are given in Table 3. There were no missing data and some variables exceeded conventional cutoffs of |2| for skewness and |7| for kurtosis [16]: System Ease of Use (-3.04 skewness, 10.39 kurtosis), System Ease of Learning (-2.15 skewness; 3.9 kurtosis), and System Satisfaction (-2.23 skewness; 4.53 kurtosis). Moreover, statistically significant Shapiro-Wilks test values were found for these indicators and thus subsequent tests were conducted via non-parametric approaches. Specifically, Wilcoxon-Mann-Whitney tests were conducted to estimate any sex (boys vs. girls) or ethnicity/race (Hispanic/Latino vs. NonHispanic/Latino) variations in terms of: system ease of use, quality of support information, system ease of learning, and system satisfaction. No statistically significant mean differences were found suggesting robustness across sex and ethnicity/race. Table 3. Usability Study Results
Mean Overall Usability 35.69 1. SYSUSE 2. INFOQUAL 3. SYSEASE 4. SYSSATIS 8.94 9.13 8.72 8.90 sd 19.84 1.48 1.28 2.03 1.70 Median 38.23 9.24 9.67 9.41 9.75 -- .61** -.92** .80** -.47* .53* .48* -1 2 3 4

5.2 Usability Study
5.2.1 Participants
With parental consent (and assent from child), 22 youth (Mean age = 9.67 years, 12 girls, 12 Hispanic/Latino, 5 White, 1 Black, 1 Asian, 3 "other") from public schools participated in the `system usefulness, satisfaction, and ease' aspect of this research. The median household income was about $39,000 and most youth were recruited from the same zip code and class grades. In addition, 77% reported knowing how to use an Android smartphone and 54.5% reported playing games using a smartphone "all the time".

5.2.2 Measures
System usefulness, satisfaction, and ease were assessed via 22items from the Usefulness, Satisfaction, and Ease of Use Questionnaire [4] modified for children and adolescents. Youth responded to each item using a 10-point rating scale (1= "not at all" to 10 = "very much"). System ease of use (SYSUSE) was measured via 11 items (e.g., it is easy to use; it is simple to use), quality of support information (INFOQUAL) was measured via 3 items (e.g., instructions and messages are easy to understand; messages to fix problems are clear), system ease of learning (SYSEASE) was measured via 4 items (e.g., I easily remember how to use it; I quickly became good at it), and system satisfaction (SYSSATIS) was measured via 4 items (e.g., I am happy with this app; I would tell a friend about this app). Consistent with the original measure, alpha reliabilities were excellent: system ease of use ( = 0.92), quality of support information ( = 0.83), system ease of learning ( = 0.92), system satisfaction ( = 0.88), and stigma ( = 0.81) scale scores, and overall usability score ( = 0.95).

Note: Ranges from 0 to 40 for Overall Usability, 0 to 10 for other variables; SYSUSE = system ease of use; INFOQUAL = quality of support information; SYSEASE = system ease of learning; SYSSATIS = system satisfaction; *p< .05; **p< .01

5.2.3 Procedures
Parents (primary caregivers, legal guardians) received a letter from the research team describing the nature of the study and the timeframe for participation (within the next 7 to 10 days). From those contacted, 26% provided child consent and every child provided assent (n=22). Youth with consent/assent provided data

Given these findings, mean estimates for the total sample were calculated and results showed that the REACH app system was highly and positively rated, for the most part, along the four dimensions of interest: system ease of use, quality of support information, system ease of learning, and system satisfaction with means ranging from 8.72 to 9.13. Also, as shown in Table 3, statistically significant correlations were found among the four dimensions with correlation coefficients ranging from .47 to .80 (p < .05). Lastly, transforming SUSE-Y overall total scores into a traditional "grade" scale, analyses showed that the REACH app system earned an "A" grade from 55% of youth, "A-" from 14%, "B+" from 9%, "B" from 9%, and failing grades of "C-" or less from 13% (or 3 youth). Focusing those youth who rated the system with a "C-" grade or less, data showed that all three youth reported no knowledge of Android operating system. One of the three youth did not know how to connect the earbuds to the phone, had trouble placing earbuds in his ears, asked what he is supposed to press during the Worryheads, asked what the word "respond" means, and did not know what to press during the STOP task. Another seemed "lost" during Worryheads and the third youth was distracted by SureLock pop-ups during testing.

6. DISCUSSION
Our multidisciplinary, collaborative efforts resulted in a smartphone app to potentiate the prevention and early intervention of childhood anxiety disorders and related problems. To our knowledge this is the first research-based child anxiety prevention and early intervention app with known usability ratings. The FRIENDS for Life Program released an app for Android, but there is no research relevant to the technology developed. In child anxiety treatment, SmartCAT is a promising mhealth platform for ecological momentary intervention, used as an adjunct to the Coping Cat treatment program [11]. The REACH prevention app appears to be more similar than different to SmartCAT whereas the FRIENDS app is mostly psychoeducational. Focusing on prevention, for example, REACH and FRIENDS provide ondemand opportunities for skill practice but REACH explicitly focuses on reducing problematic anxiety at the indicated and early intervention level as it includes focused and direct features relevant to engaging youths in self-monitoring, in-vivo exposures, and cognitive self-control. In addition, REACH is capable of deploying notifications relevant to skill practice, offers tools for personalizing and tailoring the protocol (e.g., increase notifications, activate new tools based on performance, activate tools parallel to the weekly focal module), and allows for opportunities for corrective feedback based on user data amenable to creating personalized reports of youth weekly practice and response. When it comes to contrasting the SmartCAT treatment app with the REACH prevention app, both yielded high "ease of use" ratings. Moreover, as found in this research, the REACH prevention app yielded overall high ratings along additional dimensions not examined for FRIENDS or SmartCAT. That is, REACH showed high ratings for quality of support information, system ease of learning, and system satisfaction. Also, this research found no significant differences between boys and girls or between Hispanic/Latino and non-Hispanic/Latino youth on any of the usability dimensions examined. The REACH app appears promising and has the potential to study questions not only relevant to potentiating program response and refining aspects of the technology, but about large scale diffusion, personalized care, and bridging the gap in health disparities when it comes to affective problems and its related disease outcomes. The version of the app described in this paper was designed and created through a multidisciplinary process that is user-centered in the broad interpretation of the process. Our subsequent plans for the REACH app include incorporating patients, caregivers, and interventionists directly into the design process, and broadening its applicability to minority populations, populations with sleep disorders, and studying the potential for positive remedies for negative outcomes of anxiety, notably drug abuse.

[4] International Organization for Standardization, 2008. Ergonomics of human system interaction-Part 210: Humancentred design for interactive systems (formerly known as 13407). [5] Lund, M. 2001. Measuring usability with the USE questionnaire. http://ww2.stcsig.org/usability/newsletter/0110_measuring_ with_use.html [6] Murray, T., Hardy, D., Spruijt-Metz, D., Hekler, E., and Raij, A. 2013. Avatar interfaces for biobehavioral feedback. Design, User Experience, and Usability. Health, Learning, Playing, Cultural, and Cross-Cultural User Experience Berlin Heidelberg: Springer [7] Norman, D. A., and Draper, S. W. 1986. User-Centered System Design: New Perspectives on Human Computer Interacti. Hillsdale N.J. : Lawrence Erlbaum Associates. [8] Pina, A. A., Silverman, W. K., Fuentes, R. M., Kurtines, W. M., and Weems, C. F. 2003. Exposure-based cognitivebehavioral treatment for phobic and anxiety disorders: Treatment effects and maintenance for Hispanic/Latino relative to European-American youths. Journal of the American Academy of Child & Adolescent Psychiatry, 42(10), 1179-1187. [9] Pina, A. A., Zerr, A. A., Villalta, I. K., and Gonzales, N. A. (2012). Indicated prevention and early intervention for childhood anxiety: A randomized trial with Caucasian and Hispanic/Latino youth. Journal of consulting and clinical psychology, 80(5), 940. [10] Pinto, M. D., Greenblatt, A. M., Hickman, R. L., Rice, H. M., Thomas, T. L., and Clochesy, J. M. 2015. Assessing the Critical Parameters of eSMART-MH: A Promising AvatarBased Digital Therapeutic Intervention to Reduce Depressive Symptoms. Perspectives in Psychiatric Care, n/a-n/a. [11] Pramana, G., Parmanto, B., Kendall, P. C., and Silk, J. S. 2014. The SmartCAT: An m- Health Platform for Ecological Momentary Intervention in Child Anxiety Treatment. Telemedicine and E-Health, 20(5), 419-427. [12] Silverman, W. K., Kurtines, W. M., Jaccard, J., and Pina, A. A. (2009). Directionality of change in youth anxiety treatment involving parents: an initial examination. Journal of Consulting and Clinical Psychology, 77(3), 474. [13] Silverman, W. K., Pina, A. A., and Viswesvaran, C. 2008. Evidence-based psychosocial treatments for phobic and anxiety disorders in children and adolescents. J Clin Child Adolesc Psychol, 37(1), 105-130. [14] Vredenburg, K., Mao, J., Smith, P. W., and Carey, T. 2002. A Survey of User-Centered design Practice. Paper presented at the Proceedings of the 2002 ACM Symposium on ComputerHuman Interaction (CHI 2002), Minneapolis, MN, April 2002., Minneapolis, MN. [15] Wang, J. T., Wang, Y. Y., Wei, C. L., Yao, N. L., Yuan, A., Shan, Y. Y., and Yuan, C. R. 2014. Smartphone Interventions for Long-Term Health Management of Chronic Diseases: An Integrative Review. Telemedicine and EHealth, 20(6), 570-583. [16] West, S. G., Finch, J. F., and Curran, P. J. 1995. Structural equation models with nonnormal variables: Problems and remedies.

7. REFERENCES
[1] Barrett, P., and Turner, C. 2001. Prevention of anxiety symptoms in primary school children: preliminary results from a universal school-based trial. Br J Clin Psychol, 40(Pt 4), 399-410. [2] Chavira, D. A., Stein, M. B., Bailey, K., and Stein, M. T. 2003. Parental opinions regarding treatment for social anxiety disorder in youth. J Dev Behav Pediatr, 24(5), 315322. [3] Fisak, B. J., Richard, D., and Mann, A. 2011. The prevention of child and adolescent anxiety: a meta-analytic review. Prev Sci, 12(3), 255-268.

Published online 4 November 2013

Nucleic Acids Research, 2014, Vol. 42, Database issue D789­D793 doi:10.1093/nar/gkt1063

WormBase 2014: new views of curated biology
Todd W. Harris1,*, Joachim Baran1, Tamberlyn Bieri2, Abigail Cabunoc1, Juancarlos Chan3, Wen J. Chen3, Paul Davis4, James Done3, Christian Grove3, Kevin Howe4, Ranjana Kishore3, Raymond Lee3, Yuling Li3, Hans-Michael Muller3, Cecilia Nakamura3, Philip Ozersky2, Michael Paulini4, Daniela Raciti3, Gary Schindelman3, Mary Ann Tuli4, Kimberly Van Auken3, Daniel Wang3, Xiaodong Wang3, Gary Williams4, J. D. Wong1, Karen Yook3, Tim Schedl5, Jonathan Hodgkin6, Matthew Berriman7, Paul Kersey4, John Spieth2, Lincoln Stein1 and Paul W. Sternberg3,8
1

Informatics and Bio-computing Platform, Ontario Institute for Cancer Research, Toronto, ON M5G0A3, Canada, 2Genome Sequencing Center, Washington University, School of Medicine, St Louis, MO 63108, USA, 3 Division of Biology and Biological Engineering 156-29, California Institute of Technology, Pasadena, CA 91125, USA, 4European Molecular Biology Laboratory, European Bioinformatics Institute, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK, 5Department of Genetics Campus, Washington University School of Medicine, St. Louis, MO 63110, USA, 6Genetics Unit, Department of Biochemistry, University of Oxford, Oxford OX1 3QU, UK, 7Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SA, UK and 8Howard Hughes Medical Institute, California Institute of Technology, Pasadena, CA 91125, USA
Received October 10, 2013; Accepted October 12, 2013

Downloaded from http://nar.oxfordjournals.org/ by guest on December 20, 2016

ABSTRACT WormBase (http://www.wormbase.org/) is a highly curated resource dedicated to supporting research using the model organism Caenorhabditis elegans. With an electronic history predating the World Wide Web, WormBase contains information ranging from the sequence and phenotype of individual alleles to genome-wide studies generated using nextgeneration sequencing technologies. In recent years, we have expanded the contents to include data on additional nematodes of agricultural and medical significance, bringing the knowledge of C. elegans to bear on these systems and providing support for underserved research communities. Manual curation of the primary literature remains a central focus of the WormBase project, providing users with reliable, up-to-date and highly crosslinked information. In this update, we describe efforts to organize the original atomized and highly contextualized curated data into integrated syntheses of discrete biological topics. Next, we discuss our experiences coping with the vast increase in available genome sequences made

possible through next-generation sequencing platforms. Finally, we describe some of the features and tools of the new WormBase Web site that help users better find and explore data of interest. INTRODUCTION Caenorhabditis elegans is a free-living soil nematode found throughout the world. Its small size (1 mm), rapid generation time (3 days), simple nervous system and invariant developmental program have made it a well-known system for studying a broad array of biological problems [(1,2); http://www.wormbook.org]. WormBase aims to facilitate and accelerate research using C. elegans through a process of deliberate and detailed curation of the primary literature. When launched, WormBase expanded prior community-driven curation to touch on virtually every aspect of classical and modern experimental biology, including next-generation sequence and high-throughput data. As these efforts continue, we are expanding our focus to create synthesized views of the scientific knowledge contained in WormBase. These `biological topics' represent large and complex problems not readily described through gene-by-gene curation and not always represented in the primary literature.

*To whom correspondence should be addressed. Tel: +1 406 222 2450; Fax: +1 801 784 8466; Email: todd@wormbase.org
ß The Author(s) 2013. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

D790 Nucleic Acids Research, 2014, Vol. 42, Database issue

Next-generation sequencing technology has had a tremendous impact on the direction of curatorial efforts at WormBase. These include an exploration of natural variation in C. elegans and a constant stream of whole-genome sequences and preliminary annotation from related species. We balance inclusion of these data sets based on potential value to our user community and resources required to adequately import data into WormBase. To support increased demand for WormBase, changing user expectations and constantly growing data requirements, we have redesigned the WormBase Web site from the ground up. Launched in March 2012, the new site offers users the option to customize the content and arrangement of pages to suit individual needs. WormMine, a new data mining tool using the Intermine data mining platform, was launched offering new options for querying and interacting with data at WormBase. BIOLOGICAL TOPICS CURATION For 13 years, WormBase curators have been collecting data of various types pertaining to the biology of C. elegans and related nematodes. These data types have included gene models, allelic variations, mutant phenotypes, anatomy function, expression patterns, gene interactions (physical, genetic and regulatory) and, more recently, human disease relevance. Although these various data types have existed predominantly in isolation, WormBase is now aiming to synthesize integrated views of these data in the form of `Biological Topics', big-picture perspectives that draw together all data relevant to a biological topic of interest. C. elegans has proven to be a tremendously useful model organism for the study of many topics, including cell death and differentiation, embryogenesis, organ development and aging. Much of this important research has been summarized in the online `WormBook' (http://www.wormbook.org/), a collection of review articles written by the nematode research community. The content of WormBook has inspired the creation of the first generation of WormBase Biological Topics, including behavioral topics such as locomotion, foraging and male mating; cellular topics such as cell fusion, cell migration and cell death; and signaling pathway topics such as RTK/Ras/ MAPK, EFG and Notch. Researchers who come to the WormBase Web site with a particular goal of understanding how nematode research has informed a particular field of study are now able to explore WormBase data from a perspective that most closely pertains to their field of inquiry. Whether researching a human disease or studying a molecular mechanism, users can search for their topic and review the relevant WormBase data in an intuitive manner. Each WormBase Biological Topic has a dedicated web page for displaying all relevant WormBase entities. In addition to a curatorgenerated text summary, the page lists relevant genes, phenotypes, anatomy terms, life stages, gene expression clusters, interactions, molecules (small molecules, chemicals, drugs), Gene Ontology (GO) terms, human diseases and publications. The connections of WormBase entities,

such as genes or phenotypes, to a particular Biological Topic are curator confirmed, ensuring high-quality annotations. A cytoscape-based interaction viewer allows users to see all genetic, physical and regulatory interactions that affect the topic of interest. These interaction network views can be filtered to allow closer inspection of certain types of interactions (regulatory versus genetic) or associated phenotypes (for genetic interactions). In addition, the Biological Topic page may include one or more depictions of relevant pathways, whether they be molecular signaling pathways, or more large-scale cell­cell interaction pathways, for example. WormBase works with WikiPathways (http://www.wikipathways.org) to generate pathway diagrams for C. elegans and related nematodes to be displayed on WormBase Biological Topic pages. The WikiPathways approach provides the benefit that many WormBase curators or experts in the field may simultaneously create, develop and maintain a common pathway, or depict alternate pathways, of a Biological Topic. WormBase curators will review nematode pathways on WikiPathways and provide official approval to pathways that meet certain quality criteria, such as proper citations and evidence. Once approved, these pathways will be incorporated into WormBase Biological Topic pages. WikiPathways has a specific WormBase `Portal' page (http://www.wikipathways.org/index.php/Portal: WormBase) that directly links users to nematode pathways of interest. WikiPathways currently houses >50 C. elegans pathways, nine of which are WormBase approved. In an effort to coordinate curation effort and most effectively synthesize the Biological Topic pages described earlier, the WormBase literature curation pipeline has undergone some changes. Previously, curators went through publications paper by paper to extract specific data types. Now, we concentrate on one Biological Topic at a time, extracting all relevant data in the literature. From this collection of information, we can then generate the most comprehensive and up-to-date view of the topic. GENOMES AND SPECIES The C. elegans reference genome and sequence annotation Careful manual curation of the C. elegans reference genome sequence and annotation continues to be a key activity for WormBase. We have recently released a new version of the reference genome (WBcel235) that includes 1402 corrections, drawn and reviewed from a number of independent projects that have re-sequenced the Bristol N2 reference strain (3­5). Active refinement of the canonical gold-standard set of structures for protein-coding genes, non-coding RNA genes, pseudogenes, transposons and operons also continues, using experimental data drawn from a wide variety of sources and tools developed within project (6,7). C. elegans natural variation The past 2 years have seen rapid growth in volume and diversity of nematode genomic variation data, in large part due to various projects engaged in whole-genome sequencing of hundreds of C. elegans wild-isolate strains

Downloaded from http://nar.oxfordjournals.org/ by guest on December 20, 2016

Nucleic Acids Research, 2014, Vol. 42, Database issue

D791

(8,9). We have responded to this challenge by making changes to the way in which we curate, store and display variation data. One significant change has been to clearly distinguish between naturally occurring polymorphisms and laboratory-induced mutations, at both the database and display levels. We have also consolidated redundant data from independent wild-isolate sequencing projects, creating reference variation records that collate all studies that have characterized a specific molecular variant, and all strains that carry it. This has increased the efficiency of our storage and computation, and has also allowed us to provide more meaningful and intuitive displays for the data. Other nematodes The manual curation of primary annotation for other nematode species is directed by user requests and perceived impact. Accordingly, we have begun to prioritize key parasitic species of direct relevance to human health. As a pilot project, we performed a first-pass annotation of the genome of Brugia malayi, a causative agent of lymphatic filariasis, manually reviewing nearly 3000 gene loci (21% of all genes) within a 6-month period. Working in collaboration with the filariasis community via FR3 (NIAID Filariasis Research Reagent Resource Center), targets for manual curation were identified based on their likely importance to the research community (e.g. putative drug targets; putative essential genes, based on C. elegans orthology; protein kinases; and transcription factors). WormBase now houses the reference genomic sequence annotation for >20 nematode species. A number of these data sets originate from third-party genome sequencing and annotation projects, and WormBase's role is primarily to add value via a number of computation analyses, display the data and make it available in standard formats. The basic workflow for integrating a genome into WormBase comprises (i) review and quality control of the primary submitted data, (ii) deployment of computational pipelines to provide additional first-pass functional annotation of the gene products and predictions of orthology and paralogy to other nematode genes and (iii) the provision of a genome browser, as well as data files in standard formats (e.g. FASTA, GFF v3) made available via our FTP site (ftp://ftp.wormbase.org). Species that we have recently brought into WormBase in this way are Heterorhabditis bacteriophora [a nematode used in horticulture (10)], Bursaphelenchus xylophilus [the pine wilt nematode (11)], Loa loa [a causative agent of Loa loa filariasis (12)], Panagrellus redivivus [the `sour paste' nematode (13)] and Dirofilaria immitis [the dog heartworm (14)]. Owing to diminishing costs of sequencing, it is now becoming more common to see the initiation of multiple independent reference genome projects for a single species. This is exemplified by the cases of two particular nematode species: Ascaris suum, for which two independent projects have each sequenced a different tissue (15,16); and Haemonchus contortus, for which two independent projects have each sequenced a different key strain (17,18). To distinguish between different genome

projects on our FTP site and Web site displays and services, we use the NCBI BioProject accession (http:// www.ncbi.nlm.nih.gov/bioproject), which is guaranteed to be a unique handle.

IMPROVEMENTS TO THE WEB SITE To address increased demand for the Web site and the need to store and present growing amounts of data, we rebuilt the WormBase Web site from the ground up. Released on 30 March 2012, this rewrite included not only a brand new user interface but also new searching tools and increased user support. A new back-end architecture provides support for the site and we have begun migration of hosting to the Amazon AWS cloud. User interface We designed the new user interface with the following objectives: (i) the interface should be as species-agnostic as possible, removing the emphasis on C. elegans when appropriate, (ii) the interface should be customizable and allow users to select which types of data they wish to see and (iii) the interface should be future-forward and permit facile changes to the content and display. As mentioned earlier, our primary user community remains researchers using C. elegans as a model system. Reworking the Web site to accommodate additional species serves two purposes. First, comparative approach against closely related species is a typical use case for studying gene function and genome architecture in C. elegans. Second, by de-emphasizing C. elegans, we have made it possible to more easily support underserved research communities studying nematodes of agricultural and medical significance. To accomplish this, we added a site-wide `Species' option on the main navigation bar. From here, users can toggle between species from any location on the site, see genome assembly and version information, jump directly to customized report pages and searches and so on. Precomputed homology and orthology relationships provide further means for moving quickly between species. As the number of species and extent of data housed at WormBase continue to grow, we wanted to both create data-rich reports and also allow users to pick and choose which data are most important to them, as well as control its presentation on the page. On report pages (say, for a given gene), a navigational sidebar lists the available `widgets' for that page. When a user clicks on a widget title, the corresponding widget opens. Widgets can be rearranged on the page by drag-and-drop, collapsed and dismissed as needed. A flexible single or multicolumn layout lets users build the perfect page report for the research question at hand. For users who have chosen to log in to the site, layout settings persist between sessions. Many other options for interactivity and customization have been built in to the new site. Users can log in using Google, Facebook or local WormBase credentials. Once logged in, they may save favorite pages (My Favorites) and papers (My Library).
Downloaded from http://nar.oxfordjournals.org/ by guest on December 20, 2016

D792 Nucleic Acids Research, 2014, Vol. 42, Database issue

We enhanced the ease of finding content at WormBase by building a custom search engine powered by Xapian (www. xapian.org). Users can conduct full-text searches on Worm Base, and retrieve faceted results broken down by data type (e.g. gene, molecule), paper type (e.g. review, journal, article) and species. The results can be further filtered by type or species, or downloaded for further analysis. The new interface also introduces elements created to help foster community engagement. Every page has a feedback tab prompting users to leave feedback, submit content corrections, report bugs or ask for help. Furthermore, each report page has a place for public comments, creating a low participation-barrier community annotation system. The Perl web framework Catalyst (www.catalystframe work.org) provides the core of the new site. A ModelView-Controller design implementation effectively separates concerns and allows us to create different presentations when accessing the same data. In this manner, the WormBase site can easily continue to evolve to meet user needs and expectations. Back-end architecture WormBase continues to rely on AceDB as the primary platform for data integration and distribution. This single-threaded database management system is >20 years old and built before the era of multi-species whole-genome sequences and annotation. We have encapsulated the role of AceDB in the new Web site architecture by building a RESTful Application Programming Interface (API) into our application that consumes data from AceDB and supplementary MySQL databases, returning data properly structured for presentation. This encapsulation effectively decouples the Web site from the back-end store, opening the door for us to migrate to a new system in the near future. One migration path that we have begun to explore uses the NoSQL document store CouchDB (couchdb.apache.org). In our current application, we precompute computationally intensive displays (using the RESTful API) and store the data in CouchDB as an efficient data cache. AceDB is only accessed when data do not already exist in CouchDB. We have extended this proof of concept by rewriting the Perl interface to AceDB (AcePerl) to use data loaded into a standalone CouchDB instance (Ace::Couch), completely removing the requirement for AceDB to drive the Web site. Improvements to the WormBase Web site have not been limited to software upgrades. Most significantly, we have begun to move the entirety of the WormBase Web site to hosting on Amazon's commercial cloud computing services. Services such as Elastic Cloud Compute (EC2) are well suited for hosting our non-sensitive information and simplify many aspects of managing the Web site. Administrative costs--both in time and money--of hosting and maintaining the Web site are greatly reduced from traditional on-site environments. Because pricing models use a `pay for what you use' scheme, the costs of hosting in the cloud are comparable to or cheaper than institutional hosting when factoring in overhead costs.

Moreover, additional storage and compute capacity can be added (and later removed) as needs arise without incurring capital expense. Cloud-based data are easily versioned and inexpensively archived through the use of snapshots. Finally, cloud resources can be launched in various geographical locations to provide better performance for users in different areas of the globe. New data visualization and mining tools The new Web site architecture allows us to easily maintain and add new tools to the Web site. For example, popular pre-existing tools such as GBrowse and BLAST/BLAT tools were retrofitted to work with the new site structure. We have expanded the options for data mining in two significant ways. First, we have launched an instance of the InterMine [www.intermine.org; (19)] data warehousing and mining platform called WormMine. WormMine gives users new ways to query data, save and manipulate lists of objects and download data en masse. WormMine also increases the interoperability of WormBase with other model organism databases that have built their own InterMine instances. Second, we have opened the same RESTful API that we use to build the Web site. Developers can consume this API to create their own presentations of the WormBase data. Researchers can use this to programmatically retrieve WormBase data in a variety of formats. Community and user support With the release of the new Web site, we have made it simpler for users to interact with WormBase developers and curators. A `Questions, Feedback & Help' tab is visible on every page on WormBase. Submitting a query here is integrated with our mail-based help desk. User forums, Twitter, a blog and webcasts augment the direct user support that WormBase provides. FUTURE DIRECTIONS The WormBase curation strategy, build process and Web site continue to evolve in response to user feedback and technical requirements. In the near future, we plan to finish relocating the Web site to the Amazon cloud. We are continuing to explore back-end replacement options for the two roles AceDB plays at WormBase: as the primary data integration platform and as a data source that drives the Web site. To accommodate increasing numbers of users accessing the WormBase Web site, we will shortly launch a version of the site optimized for mobile use to be followed by native applications for both Android and iOS. FUNDING The US National Human Genome Research Institute [U41-HG002223] to WormBase and British Medical Research Council [G070119] to WormBase; P.W.S. is an investigator with the Howard Hughes Medical Institute. Funding for open access charge: US National Human Genome Research Institute [U41-HG002223].

Downloaded from http://nar.oxfordjournals.org/ by guest on December 20, 2016

Nucleic Acids Research, 2014, Vol. 42, Database issue

D793

Conflict of interest statement. None declared. REFERENCES
1. Brenner,S. (1974) The genetics of Caenorhabditis elegans. Genetics, 77, 71­94. 2. Riddle,D.L., Blumenthal,T., Meyer,B.J. and Priess,J.R. (1997) C. elegans II. Cold Spring Harbor Laboratory Press, Cold Spring Harbor, NY. 3. McGrath,P.T., Xu,Y., Ailion,M., Garrison,J.L., Butcher,R.A. and Bargmann,C.I. (2011) Parallel evolution of domesticated Caenorhabditis species targets pheromone receptor genes. Nature, 477, 321­325. 4. Doitsidou,M., Poole,R.J., Sarin,S., Bigelow,H. and Hobert,O. (2010) C. elegans mutant identification with a one-step whole-genome-sequencing and SNP mapping strategy. PLoS One, 5, 15435. 5. Weber,K.P., De,S., Kozarewa,I., Turner,D.J., Babu,M.M. and de Bono,M. (2010) Whole genome sequencing highlights genetic changes associated with laboratory domestication of C. elegans. PLoS One, 5, e13922. 6. Howe,K., Davis,P., Paulini,M., Tuli,M.A., Williams,G., Yook,K., Durbin,R., Kersey,P. and Sternberg,P.W. (2012) WormBase: annotating many nematode genomes. Worm, 1, 15­21. 7. Williams,G.W., Davis,P.A., Rogers,A.S., Bieri,T., Ozersky,P. and Spieth,J. (2011) Methods and strategies for gene structure curation in WormBase. Database, 2011, baq039. 8. Andersen,E.C., Gerke,J.P., Shapiro,J.A., Crissman,J.R., Ghosh,R., ´ lix,M.A. and Kruglyak,L. (2012) Chromosome-scale Bloom,J.S., Fe selective sweeps shape Caenorhabditis elegans genomic diversity. Nat. Genet., 44, 285­290. 9. Thompson,O., Edgley,M., Strasbourger,P., Flibotte,S., Ewing,B., Adair,R., Au,V., Chaudhry,I., Fernando,L., Hutter,H. et al. (2013) The million mutation project: a new approach to genetics in Caenorhabditis elegans. Genome Res., 23, 1749­1762. 10. Bai,X., Adams,B.J., Ciche,T.A., Clifton,S., Gaugler,R., Kim,K.S., Spieth,J., Sternberg,P.W., Wilson,R.K. and Grewal,P.S. (2013) A lover and a fighter: the genome sequence of an entomopathogenic nematode Heterorhabditis bacteriophora. PLoS One, 8, e69618.

11. Kikuchi,T., Cotton,J.A., Dalzell,J.J., Hasegawa,K., Kanzaki,N., McVeigh,P., Takanashi,T., Tsai,I.J., Assefa,S.A., Cock,P.J. et al. (2011) Genomic insights into the origin of parasitism in the emerging plant pathogen Bursaphelenchus xylophilus. PLoS Pathog., 7, e1002219. 12. Desjardins,C.A., Cerqueira,G.C., Goldberg,J.M., Dunning Hotopp,J.C., Haas,B.J., Zucker,J., Ribeiro,J.M., Saif,S., Levin,J.Z., Fan,L. et al. (2013) Genomics of Loa loa, a Wolbachia-free filarial parasite of humans. Nat. Genet., 45, 495­500. 13. Srinivasan,J., Dillman,A.R., Macchietto,M.G., Heikkinen,L., Lakso,M., Fracchia,K.M., Antoshechkin,I., Mortazavi,A., Wong,G. and Sternberg,P.W. (2013) The draft genome and transcriptome of Panagrellus redivivus are shaped by the harsh demands of a free-living lifestyle. Genetics, 193, 1279­1295. 14. Godel,C., Kumar,S., Koutsovoulos,G., Ludin,P., Nilsson,D., Comandatore,F., Wrobel,N., Thompson,M., Schmid,C.D., Goto,S. et al. (2012) The genome of the heartworm, Dirofilaria immitis, reveals drug and vaccine targets. FASEB J., 26, 4650­4661. 15. Jex,A.R., Liu,S., Li,B., Young,N.D., Hall,R.S., Li,Y., Yang,L., Zeng,N., Xu,X., Xiong,Z. et al. (2011) Ascaris suum draft genome. Nature, 479, 529­533. 16. Wang,J., Mitreva,M., Berriman,M., Thorne,A., Magrini,V., Koutsovoulos,G., Kumar,S., Blaxter,M.L. and Davis,R.E. (2012) Silencing of germline-expressed genes by DNA elimination in somatic cells. Dev. Cell, 23, 1072­1080. 17. Laing,R., Kikuchi,T., Martinelli,A., Tsai,I.J., Beech,R.N., Redman,E., Holroyd,N., Bartley,D.J., Beasley,H., Britton,C. et al. (2013) The genome and transcriptome of Haemonchus contortus, a key model parasite for drug and vaccine discovery. Genome Biol., 14, R88. 18. Schwarz,E.M., Korhonen,P.K., Campbell,B.E., Young,N.D., Jex,A.R., Jabbar,A., Hall,R.S., Mondal,A., Howe,A.C., Pell,J. et al. (2013) The genome and developmental transcriptome of the strongylid nematode Haemonchus contortus. Genome Biol., 14, R89. 19. Smith,R.N., Aleksic,J., Butano,D., Carr,A., Contrino,S., Hu,F., Lyne,M., Lyne,R., Kalderimis,A., Rutherford,K. et al. (2012) InterMine: a flexible data warehouse system for the integration and analysis of heterogeneous biological data. Bioinformatics, 28, 3163­3165.

Downloaded from http://nar.oxfordjournals.org/ by guest on December 20, 2016

CAPTURE - Cooperatively Applied Positioning Techniques Utilizing Range Extensions

Item type Authors Publisher Downloaded Item License Link to item

Article Cullen, Gary IEEE 20-Dec-2016 14:11:14 http://creativecommons.org/licenses/by-nc-nd/4.0/ http://hdl.handle.net/10759/325964

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014

CAPTURE - Cooperatively Applied Positioning Techniques Utilizing Range Extensions
Gary Cullen, Kevin Curran, Jose Santos
Intelligent Systems Research Centre, University of Ulster, Magee College, Derry, United Kingdom Gary.Cullen@lyit.ie
Abstract-- The most commonly implemented Indoor Location Based Solution uses existing Wi-Fi network components to locate devices within its range. While this technique offers obvious economic rewards by utilizing a preinstalled infrastructure, these network topologies were typically designed to provide network coverage to mobile devices rather than deliver an Indoor Location Based Solution. Large areas without coverage are common in these networks because network designers were not typically concerned about providing 100% coverage for mobile data. Hallways, toilet areas or other general purpose areas that ordinarily would not require network coverage did not get dedicated WAPs installed. Transient users navigating these areas of the network were un-locatable using this infrastructure. Furthermore the indoor arena is an especially noisy atmosphere, being home to other wireless devices such as Bluetooth Headsets, Cordless Phones and Microwave Ovens. Considering users spend more time in an indoor environment, over 88%, the need for a solution is obvious. Therefore, we propose a solution to resolve the issue of restricted coverage of Indoor Location Based solutions, using a cooperative localization technique Cooperatively Applied Positioning Techniques Utilizing Range Extension (CAPTURE). CAPTURE offers a method of locating devices that are beyond the range of the current in-house location based solution. It presents a unique contribution to research in this field by offering the ability to utilize devices that know their location within a Location Based Solution (LBS), to evaluate the position of unknown devices beyond the range capacity of the LBS. This effectively extends the locating distances of an Indoor LBS by utilizing the existing mobile infrastructure without the need for any additional hardware. The proliferation of smart phones and the tablet form factor, bundled with Wi-Fi, Bluetooth and gyroscopes ­ technologies currently used to track position, provide a fertile community for CAPTURE to cooperatively deliver a location solution. Keywords -- Localisation; Indoor positioning; localisation; geographical positioning; wireless. Indoor

Gearoid Maguire, Denis Bourne
Letterkenny Institute of Technology, Co. Donegal, Ireland

everywhere you were at a given time. Googles manoeuvring into the indoor location mappings realm [1] opens up the opportunity to deliver this virtual reality, currently being able to provide door to door route planning. Being able to navigate your way from your office desk out through your company's building (taking the stairwell to avoid your boss in the lift) is eminently achievable albeit with a small number of locations on a modern smartphone using google maps. A level switcher allows you to onion slice through multiple floor level plans, before switching to GPS to offer possible transport alternatives through the outdoor environment. On reaching what `historically' would have been your destination, Google Indoor Maps and more importantly an Indoor Positioning System (IPS) picks up where GPS left off offering a point to point navigation solution. This can then take you through the complexities of an airport terminal for example, via specific waypoints such as security and check-in desks directly to your departure gate. One of the barriers to implementation of such a concept is the limitation in coverage and accuracy of currently implemented Indoor Position or Location Based Systems [2]. IPSs typically utilize pre-existing Wi-Fi network infrastructure taking ranging information from Wireless Access Points (WAP's) as inputs for a localization algorithm. Unfortunately the drivers behind the strategic decisions on the positioning of WAPs, in a Wi-Fi based solution, were typically to catch large congregations of users and primarily to provide the highest available throughput to those users. Coverage for IPSs is not necessarily to the forefront of network designer's minds when designing such networks, leaving large areas beyond the range of an IPS. GPS on the other hand, offers near global coverage, bar some issues with urban canyons and other high rise natural obstacles that prevent Line of Sight (LoS) to the just under 30 satellites required [3] to deliver such wide scope. The indoor environment does not afford such clear unobstructed views to and from tracking devices, the many doors, walls, floors, pillars and ceilings hinder the capacity of an IPS to locate devices. Furthermore the indoor arena is an especially noisy atmosphere, being home to other wireless devices such as Bluetooth Headsets, Cordless Phones and Microwave Ovens. All of these devices operate in the same frequency band as the Wi-Fi solution, namely 2.4 GHz and therefore can interfere with the reception of signals used to locate [2], making them behave in an unpredictable fashion.

I.

Introduction

On loosing something or forgetting where you last placed something, a common piece of advice is to retrace your steps back in your mind. This can be quite a formidable task given the multimodal transport available today coupled with the complexity and scale of buildings we interact with on a regular basis. The ability to place an avatar of yourself onto a map to graphically retrace your steps in real-time would dramatically reduce the brain power required to remember

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014 These environmental dynamics combine to dramatically affect the ability of an indoor solution to provide an acceptable level of coverage. Literature from Yang [4] and Rowe [5] reflect that Location Awareness is rapidly becoming a fundamental requirement for mobile application development. This highlights the challenges posed for ubiquitous localization of devices in the indoor arena. Considering users spend more time in an indoor environment, over 88.9% according to a recent Canadian study [6] , the need for a solution is obvious. We propose a solution to this issue of coverage limitations by using a cooperative localization technique, CAPTURE. CAPTURE can plug into an in situ solution irrespective of the technology or location technique that solution currently uses to locate. It provides a location relative to the devices locating it, which can then be mapped onto a global overview of the Location Based System (LBS), assisting in the aforementioned scenario to get you to the departure gate in a point to point navigation solution. Consider the following scenario where a user `Bob', is in his favorite seat in the library, unfortunately the seat is in the far corner of the library, which can only be `seen' by one Wireless Access Point. In this position Bob's tablet can gain Wi-Fi access through this Access Point to allow him access to online resources. However one Access Point is not enough for the in-house Location Based System to accurately locate Bob within the building using Trilateration positioning techniques. Sue is sitting near the front of the library and can be `seen' by 4 Wireless Access Points, and is thereby accurately located on the Location Based System. She is also 25 meters to the left of Bob and the Wireless Network Card on her Laptop can see Bob's tablet. The Librarian is stacking books on the shelves behind where Bob is sitting and her smartphone is currently located within the Location Based System also. The wireless NIC on her smartphone can also `see' Bob's tablet, therefore, in a normal scenario, Bob would be beyond the range of the Location Based System, but because CAPTURE can use the known positions of the Librarian and Sue and Bob's position relative to them it can accurately estimate Bob's position within the library. The rest of this paper is laid out as follows; Section II describes the system model used to implement CAPTURE. Section III provides an overview of the experimental test bed used to evaluate the solution and Section IV documents the data collected during test. In Section V we describe the findings of the experiments that were carried out validating the feasibility of the system, the penultimate section, Section VI outlines the proposed implementation of CAPTURE and the paper closes with a conclusion in Section VII, providing an insight into some projected future work with CAPTURE.

Figure 1: An Danlann Sports Hall LyIT

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014

II. CAPTURE - System Model
This section describes a system model that can be used in a localization solution to establish the Cartesian coordinate values of a lost device within a two dimensional plane. CAPTURE does not require a preceding calibration stage or a site survey, providing a robust opportunistic solution in dynamic environments, using only real time RSSI values without changing the IEEE 802.11 network. Literature within the realm of Location Based Systems frequently use terms such as Anchor or Anchor Nodes to describe devices that help to determine the position of lost or unknown devices. The term anchor elicits a perception of a static or permanent device, which in a cooperative solution these devices most certainly are not. For this reason we will use the term reference device when describing devices that assist in the positioning of lost or unknown devices. Two key components typically make up the estimation of the position of a lost device. First of all ranging techniques are used to estimate the distance from the transmitting device(s) to the receiving device(s). This is calculated using a metric for example the length of time it takes a signal to propagate the distance from the transmitter to the receiver. The second component is the position estimation technique, here the ranging variables are calculated using one or more ranging techniques and these are used as input for an estimation algorithm (mathematical formulae) to calculate the position of the lost device. A. RSSI ­ Received Signal Strength Indicator Possibly the most popular ranging technique used in Indoor Localization, Received Signal Strength Indicator (RSSI) is a measurement of the voltage that exists in a transmitted radio signal, which is an indication of the power being received by the antenna. When a signal first leaves a transmitting device, the power of the signal drops or attenuates, this is true of both wired and wireless transmissions. As a radio signal propagates through the air some of its power is absorbed and the signal loses a specific amount of its strength, therefore, the higher the RSSI value (or least negative in some devices), the stronger the signal. Knowing the amount of signal loss over a given distance provides a method to calculate the distance from a transmitting device, given a Received Signal Strength. At its most basic level this allows for the `coarse' localization or as referred to in other literature, `presence-based localization' [7] of a device relative to the transmitting device. This can be illustrated by the RSSI calculated distance being the radius of a circle and the `searching' device being at the center of that circle. The estimated position of the lost device is anywhere on the circumference of that circle. In an IEEE 802.11 network if the locations of the Access Points are already known, then the location of Mobile Devices traversing the network can be located relative to them, albeit only to the circumference of the radius of the calculated distance. Further localization algorithms and position estimation filtering techniques must be applied to provide a more precise level of localization.

In a cooperative paradigm, mobile devices can simulate the role carried out by Access Points providing a relative reference to a lost devices location. RSSI values can be extracted from beacons transmitted between devices within range. Correlation of these signal indicators and distance can be estimated using many of the methods already applied throughout literature in this arena [8-11]. RSSI based or more broadly speaking, Wi-Fi based Indoor Positioning Systems have had notoriously irregular environment variables such as reflection, refraction, diffraction and absorption of radio waves that can impact positioning estimated dramatically [12]. Although RSSI is a measure of signal loss, it is not a linear representation of how many dBm is actually reaching the card. If a signal indicator is reading -72, this means that it is 72 dBm less powerful by the time it gets to your device. Experimental test carried out at an early stage with CAPTURE further extoled this assumption. Results of these tests can be viewed in Table 1: 5 meter increments in Section V, Data Collection and Presentation. Crudely extracting the RSSI at given distance increments to attempt to derive a meter distance being equal to a given dBm increase in RSSI reading was not going to yield any value worth using in any further experiments. The authors in [13] advocate a solution utilizing a RSSI smoothing algorithm to minimize the dynamic fluctuation of the RSSI values. B. Trilateration Trilateration is a key component of GPS position estimation techniques. It is a process that can estimate the position of a mobile device given the positions of at least three other objects and the distance from those objects to the device to be located. In the scenario depicted below in Figure 2(a), illustrated using a cooperative localization example, the circle depicts the distance from a reference device to a lost device. This distance would have been derived using the RSSI value between the reference and lost devices. All we can say about the whereabouts of the lost device is that it resides somewhere on the circumference of the circle that is constructed using the radius of the estimated measurement between the two devices. A second reference device will allow the position of the lost device to be narrowed further as can be seen in Figure 2(b). Now the ranging estimates of the lost device have been calculated relative to the second reference device also. Therefore considering the lost device must be on the circumference of the circles created by the distance between it and the two reference devices there are only 2 possible positions where it might be, the intersections of these two circles.

Figure 2 (a) Single Distance

(b) With 2nd Reference Device

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014 To calculate the exact position of the lost device we need a third reference device. When we calculate the distance from this final reference device to the lost device and considering we already know the distance from the other reference devices. We can then determine that the lost device can only be at one specific position to match those three particular distance estimations ­ the intersections of the three circles (see Figure 3). The ranging estimates calculated from the RSSI values in the tests were used as the inputs for the trilateration algorithm on the CAPTURE, to provide an estimate on the position of the lost phones. Each phone used in the test is given a name (BSSID) TestPhone1, TestPhone2 for example. CAPTURE reads the RSSI of all available reference points, i.e. all devices it can `see', but it filters out only the test phones selected by the user carrying out the tests. This can be seen in the image in figure 5, and is achieved via a lookup table mapping the MAC address of the phone to the phone name. This allows the use of only a specified phone or a group of phones during any given test.

Figure 3: Trilateration Example II.

Figure 5: CAPTURE Client Interface A. System Components The experimental setup of the prototype consisted of the following system components:  Mobile Devices 5 Samsung GT-S5310 Galaxy Pocket phones, running Google Android 2.2.1 on a 600 MHz ARMv6, Adreno 200 GPU, Qualcomm MSM7227 chipset, were used to carry out the evaluation of the CAPTURE system. 4 of the phones were used as reference devices, the other phone acted as the lost device. All phones used during the test were of an exact make and model so as to rule out any issues with varied RSSI reads with different antenna types, some of these issues have been described in the literature [14, 15]. Lisheng et al., [15] go so far as to discribe the distortion being as much as 11.2 dBm out with different antenna types over a 25 meter read range. During the tests all phones were place at a distance of 80cm above floor level, to mimic as close to a real world example of a user holding them. The phones were placed on identical platforms during the tests to negate the impact of Hand-Grip body-loss effect which can also impact ranging measurements as documented in litrature by Rosa et al., [16]. Kaemarungsi and Krishnamurthy highlighted in their litrature [17] that device orientation can also introduce errors when calculating signal range estimates, so all phones had the same orientation when used in our tests.

Experimental Test Bed

In this section, we will provide evidence showing the suitability of CAPTURE as a solution to the indoor ranging problem. To do that we carried out a large campaign of measurements in the An Danlann Sports Hall in Letterkenny Institute of Technology illustrated in figure 1. The hall offers a 40m diagonal testing range, providing Line of Sight measurements for all tests, as can be seen in the picture depicted in figure 4. When readings were been recorded all users vacated the hall, this provided an optimal environment to use as a benchmark for future tests on CAPTURE.

Figure 4: Test Environment

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014  Database the algorithm, these were smoothed with a filter before the average was calculated.

A MySQL Server version: 5.0.96 hosted on a Linux platform was used to store all data collected by the devices. The server was online and the phones wrote directly to it as they recorded RSSI values from each other. The data was then passed through a low level filter to remove any outliers, before an average RSSI reading was calculated for each required ranging measurement, to be used in the trilateration algorithm to estimate the position of the lost device.  Laptop A Dell Latitude E6440 iCore3 running Windows 7 Professional was used to develop the app to gather the RSSI from the phones. An algorithm was designed to convert this RSSI reading into a ranging measurement before a trilateration algorithm converted the ranging measurements into Cartesian coordinate values. We used the Eclipse IDE and Android Software Development Kit (SDK) for Android development and debugging, to develop the app. B. Ranging Measurement Estimation The RSSI values captured from the beacons transmitted by devices within range of the `lost device' were used to estimate the relative distance between them. As explained earlier RSSI values do not provide a linear representation of distance. The authors in [13] advocate using the formula in "(1)," below to estimate RSSI, and thereby extrapolate distance given RSSI: Where: RSSI = - (10n Log10 (d) +A) Equation (1)

Figure 6: Meter RSSI values Further tests were then carried out to measure the accuracy of both the RSSI values received and the resulting range estimations from the algorithm. Table 1 below, depicts the results of tests to capture the RSSI values between two phones at 5 meter increments diagonally across the hall. It highlights the RSSI value beginning at -52.48 for the 0-5 meter range. A sample set of 200 readings was recorded per section, an average was then taken from this set. The standard deviation was also documented to illustrate any fluctuations in the received values, typically these were found to be low during our tests. Distance Average Std Dev Estimate Distance Average Std Dev Estimate 0-5m -57.264 0.4996 4.517 0 - 25 m -68.38 0.6884 21.544 0 - 10 m -61.5652 0.4 8.269 0 - 30 m -70.75 0.9797 30.059 0 - 15 m -69.5263 0.85346 25.31 0 - 35 m -71.854 0.6803 35.104 0 - 20 m -67.5662 0.48332 19.216 0 - 40 m -73.681 0.7901 45.379

n: Path Loss Exponent d: Distance from transmitting device A: Received signal strength at 1 meter distance The path loss exponent typically varies from 1.5 to 4, with 1.5 representing a free-space Line of Sight (LoS) value and 4 representing an environment that incorporates a high level of signal attenuation. Not having a good equation modeling the environment in which your experiments are to be deployed, will be reflected in horrible results. After initial pre-tests were evaluated, a Path Loss Exponent of 1.5 was determined for the test environment, because of the open plan design of the Hall offering LoS between all devices and the RSSI at 1 meter was measured at -43.6316. The results of the collected data are illustrated in the following section. III.

Table 1: 5 meter increments The average was then inputted into the algorithm to derive a range estimate based on the RSSI values received. As mentioned before RSSI values do not provide a linear representation of measurement, and therefore some of the increments do not initially seem like they could assist in finding a distance at a given measurement. The trilateration algorithm accounts for an error bounds of 2.5 meters in the range estimation of the RSSI value. One notable issue with the recorded RSSI values was the reading taken at the 0-1 meter distance however. It jumped dramatically at this distance, giving a RSSI value higher than the 0-20 and 0-25 meter tests. This test (0-10 meters) was carried out at different areas of the hall, to try and rule out signal interference. But irrespective of which location the reading were taken the RSSI value was

Data Collection and Presentation

Here we present all of the data collated throughout this work, the data sets are illustrated in the graphs and tables. During the recording of data the hall was emptied of people so as to provide a clean set of results. An initial test was run to establish the 1 meter range for input into the algorithm in equation 1, the results of this test can be seen in figure 6. Over 500 readings were recorded at various locations throughout the hall, to accurately obtain the meter value for

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014 always higher (or more negative) than the next 3 larger tests. No reason could be given at the time of writing for this anomaly within the set. IV. V.

CAPTURE ­ System Implementation

Experimental Results

Figure 7 depicts one of the tests were CAPTURE accurately locates a lost phone within 2.5 meters. TestPhone1, TestPhone2 and TesPhone3 know their location, via the inhouse IPS. They also know the distance between themselves (TestPhone1 - TestPhone2 = 15 meters, TestPhone1 ­ TestPhone3 = 13 meters and TestPhone2 ­ TestPhone3 = 17 meters), the RSSI readings from the Lost Phone to TestPhone1 is -61.5551dBm, from the Lost Phone to TestPhone2 is 65.34534 dBm and from the Lost Phone to TestPhone3 is 61.8952dBm. These RSSI readings translate to a ranging estimate of 13.345, 15.1221 and 9.349 meters respectively when put through the ranging algorithm. The actual distance between TestPhone1 and the lost phone is 11.5 meters, between TestPhone2 and the lost phone is 13.2 meters and TestPhone3 and the Lost Phone is 11.96 giving an approximate error rate of 2.5 meters.

In order for CAPTURE to be able to cooperatively locate a lost device within a network, there must be at least 3 reference devices within sight of the lost device. Each of these must have `a prior' knowledge of their location within a preexisting localization solution. The hypothesis of CAPTURE was to extend the range of in-house IPS's, and tests shown have proven that it can achieve exactly this. Existing IPS's have dramatically more powerful infrastructure than what CAPTURE would utilize though. For example 230 volt AC powered Access Points in a standard IPS versus 12 volt DC powered mobile reference devices (smart phones, tablets and\or laptops) in a cooperative solution. It would be naive to think that accuracy levels of an in-house IPS would also `extend' to a cooperative model, although this does not take away from the solution to the range issue that CAPTURE provides. The implementation of a more comprehensive filter would nonetheless assist with accuracy the Kalman or Extended Kalman Filters are recommended in the following literature [18, 19]. VI.

Conclusion

This paper introduces CAPTURE a cooperative localization system that provides a solution to the problem of devices being out of range of a hosted Indoor Positioning System. Experiments with the CAPTURE system have demonstrated that utilizing a cooperative framework of mobile devices can extend the range of an in situ Indoor Positioning System by at least the range of the outermost devices located within the system. Some issues arose during testing for example the 0-10 meter readings, and this necessitates further work. A more comprehensive algorithm would provide more accuracy for the system. An expansion of CAPTURE to avail of Bluetooth 4.0 would allow for the extension of an IPS incorporating some of the advantages of this technology. Bluetooth has been used as a cooperative solution to the accuracy issue in IPS's and can be seen in the following literature [20, 21]. Further investigation into the incorporation and evaluation of Wi-Fi Direct as a solution is also warranted.

Figure 7: Finding Lost Phone

2014 International Conference on Indoor Positioning and Indoor Navigation, 27th-30th October 2014 REFERENCES [1] M. Aly and J. Y. Bouguet, "Street view goes indoors: Automatic pose estimation from uncalibrated unordered spherical panoramas," in Applications of Computer Vision (WACV), 2012 IEEE Workshop on, 2012, pp. 1-8. G. Cullen, K. Curran, and J. Santos, "Cooperatively extending the range of Indoor Localisation," in Signals and Systems Conference (ISSC 2013), 24th IET Irish, 2013, pp. 1-8. G. M. Djuknic and R. E. Richton, "Geolocation and assisted GPS," Computer, vol. 34, pp. 123-125, 2001. Y. Fan and A. Dong, "A Solution of Ubiquitous Location Based on GPS and Wi-Fi ULGW," in Hybrid Intelligent Systems, 2009. HIS '09. Ninth International Conference on, 2009, pp. 260-263. A. Rowe, Z. Starr, and R. Rajkumar, "Using microclimate sensing to enhance RF localization in assisted living environments," in Systems, Man and Cybernetics, 2007. ISIC. IEEE International Conference on, 2007, pp. 3668-3675. C. J. Matz, D. M. Stieb, K. Davis, M. Egyed, A. Rose, B. Chou, et al., "Effects of Age, Season, Gender and Urban-Rural Status on Time-Activity: Canadian Human Activity Pattern Survey 2 (CHAPS 2)," International journal of environmental research and public health, vol. 11, pp. 2108-2124, 2014. A. E. Kosba, A. Saeed, and M. Youssef, "Robust WLAN Device-free Passive motion detection," in Wireless Communications and Networking Conference (WCNC), 2012 IEEE, 2012, pp. 32843289. X. Yaqian, L. Sian Lun, R. Kusber, and K. David, "An experimental investigation of indoor localization by unsupervised Wi-Fi signal clustering," in Future Network & Mobile Summit (FutureNetw), 2012, 2012, pp. 1-10. S. Shioda and K. Shimamura, "Anchor-free localization: Estimation of relative locations of sensors," in Personal Indoor and Mobile Radio Communications (PIMRC), 2013 IEEE 24th International Symposium on, 2013, pp. 2087-2092. M. O. Gani, C. Obrien, S. I. Ahamed, and R. O. Smith, "RSSI Based Indoor Localization for Smartphone Using Fixed and Mobile Wireless Node," in Computer Software and Applications Conference (COMPSAC), 2013 IEEE 37th Annual, 2013, pp. 110-117. D. Gualda, J. Urena, J. C. Garcia, E. Garcia, and D. Ruiz, "RSSI distance estimation based on Genetic Programming," in Indoor Positioning and Indoor Navigation (IPIN), 2013 International Conference on, 2013, pp. 1-8. [12] L. Erin-Ee-Lin and C. Wan-Young, "Enhanced RSSI-Based Real-Time User Location Tracking System for Indoor and Outdoor Environments," in Convergence Information Technology, 2007. International Conference on, 2007, pp. 1213-1218. J. Joonyoung, K. Dongoh, and B. Changseok, "Automatic WBAN area recognition using P2P signal strength in office environment," in Advanced Communication Technology (ICACT), 2014 16th International Conference on, 2014, pp. 282-285. X. Lisheng, Y. Feifei, J. Yuqi, Z. Lei, F. Cong, and B. Nan, "Variation of Received Signal Strength in Wireless Sensor Network," in Advanced Computer Control (ICACC), 2011 3rd International Conference on, 2011, pp. 151-154. F. D. Rosa, X. Li, J. Nurmi, M. Pelosi, C. Laoudias, and A. Terrezza, "Hand-grip and bodyloss impact on RSS measurements for localization of mass market devices," in Localization and GNSS (ICL-GNSS), 2011 International Conference on, 2011, pp. 58-63. K. Kaemarungsi and P. Krishnamurthy, "Properties of indoor received signal strength for WLAN location fingerprinting," in Mobile and Ubiquitous Systems: Networking and Services, 2004. MOBIQUITOUS 2004. The First Annual International Conference on, 2004, pp. 14-23. K. Alexis, C. Papachristos, G. Nikolakopoulos, and A. Tzes, "Model predictive quadrotor indoor position control," in Control & Automation (MED), 2011 19th Mediterranean Conference on, 2011, pp. 1247-1252. S. S. Saad and Z. S. Nakad, "A Standalone RFID Indoor Positioning System Using Passive Tags," Industrial Electronics, IEEE Transactions on, vol. 58, pp. 1961-1970, 2011. A. Baniukevic, D. Sabonis, C. S. Jensen, and L. Hua, "Improving Wi-Fi Based Indoor Positioning Using Bluetooth Add-Ons," in Mobile Data Management (MDM), 2011 12th IEEE International Conference on, 2011, pp. 246-255. Z. Zhichao and C. Guohong, "APPLAUS: A Privacy-Preserving Location Proof Updating System for location-based services," in INFOCOM, 2011 Proceedings IEEE, 2011, pp. 1889-1897.

[13]

[2]

[14]

[3] [4]

[15]

[5]

[16]

[6]

[17]

[7]

[18]

[8]

[19]

[9]

[20]

[10]

[11]

Operational Experiences with Disk Imaging in a Multi-Tenant Datacenter
Kevin Atkinson, Gary Wong, and Robert Ricci, University of Utah
https://www.usenix.org/conference/nsdi14/technical-sessions/presentation/atkinson

This paper is included in the Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI '14).
April 2­4, 2014 · Seattle, WA, USA
ISBN 978-1-931971-09-6

Open access to the Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI '14) is sponsored by USENIX

Operational Experiences with Disk Imaging in a Multi-Tenant Datacenter
USENIX Symposium on Networked Systems Design and Implementation--Operational Systems Track

Kevin Atkinson 

Gary Wong

Robert Ricci

University of Utah School of Computing {kevina, gtw, ricci}@cs.utah.edu www.emulab.net Abstract
Disk images play a critical role in multi-tenant datacenters. In this paper, the first study of its kind, we analyze operational data from the disk imaging system that forms part of the infrastructure of the Emulab facility. This dataset spans four years and more than a quarter-million disk image loads requested by Emulab's users. From our analysis, we draw observations about the nature of the images themselves (for example: how similar are they to each other?) and about usage patterns (what is the statistical distribution of image popularity?). Many of these observations have implications for the design and operation of disk imaging systems, including how images are stored, how caching is employed, the effectiveness of pre-loading, and strategies for network distribution. Large multi-tenant facilities have hundreds to hundreds of thousands of servers and thousands to millions of users [5]. A busy facility may have many thousands of user images and provision tens of thousands of servers per day. Disk images are commonly written to drives attached to the host; EC2, for example, calls this "instance storage" [1], and it is available on nearly all VM types. Disk imaging is on the critical path for provisioning servers, which cannot be booted until the requested image has been loaded. Images can consume significant resources on the facility, including the space used to store them and the network bandwidth required to distribute them to the hosts on which they are to be used. Thus, understanding disk images and their use is important to the design and operation of multi-tenant datacenters. In this paper, we study four years' worth of data from the operation of the Emulab testbed [16], a multi-tenant facility with approximately six hundred hosts and over five thousand user accounts. The data we examine covers 279,972 requests for disk images (Section 2) and is, to our knowledge, the only dataset currently available to the public that contains detailed traces of disk imaging in a multi-tenant datacenter. It allows us to study properties of the disk images themselves as well as how they are used by the facility's users, and we draw a number of conclusions that are applicable to the design and operation of imaging systems. Our key findings include: · Section 3: There is substantial block-level similarity between many images, suggesting that deduplicating storage is appropriate. The lifespan of images varies greatly, from days to years, and many images go unused for months at a time, making multi-tier data storage attractive. · Section 4: The working set of images is quite small (mean: 12 per day, 30 per week), making caching of frequently used images potentially effective. However, the makeup of this working set changes frequently, and there are no dominant images. The daily working set size grows linearly with the number of users, but the total number of facility and user

1

Introduction

Computers in datacenters are frequently re-allocated from one purpose to another, need to have their software upgraded, or need to be returned to a known "clean" state. This type of re-provisioning is particularly important in multi-tenant datacenters [4], which are shared by a large number of applications running on behalf of different clients. Notably, this is the model adopted by "Infrastructure as a Service" (IaaS) clouds such as Amazon EC2 [2], Rackspace [11], and datacenters managed with software such as OpenStack [15]. These facilities provide physical or virtual servers (infrastructure) on which users run their own operating systems and applications [9, 15]. The primary means for initializing user resources is to load them with an initial disk image, which is a block-level snapshot of a filesystem containing an installed operating system and set of applications. Typically, a cloud will provide a set of images that any user may install on servers that they provision (facility images). Users may also create their own images (user images): this is commonly accomplished by loading a facility image, customizing it, and taking a snapshot of the resulting disk.
 Work

done at the University of Utah; now at Rice University

USENIX Association 

11th USENIX Symposium on Networked Systems Design and Implementation 217

images follow different curves. · Section 5: The popularity of user images follows a heavy-tailed distribution, while the popularity of facility images does not. Most users skew heavily towards using either facility provided images or custom images, not both. While most users do not create their own images, those who do number among the facility's heaviest users. · Section 6: We consider the technique of pre-loading popular facility images, allowing some requests to be satisfied without waiting for the image to load. We find that two factors control the potential benefit from this strategy: (a) the ratio of the working set size to the number of idle disks available for preloading, and (b) the ratio of the rate at which the facility can load disks to the arrival rate of requests. · Section 7: Differential loading (pre-loading a base image, then transferring only differing blocks as required) shows potential. In order to be effective, it will require development of sophisticated prediction techniques that take into account both the popularity of images themselves and their block-level similarity to each other. We conclude in Section 8 with several concrete suggestions regarding the design and operation of disk imaging systems, and point to fertile areas for future work.

sion solely virtual machines, we believe that this difference does not have a significant impact on conclusions drawn from the dataset: in either case, the user is presented with the abstraction of an PC on which they may load and boot an operating system. While the details of operating systems that run within physical and virtual machines may vary, the quantity and diversity of users' desired images is unlikely to be affected. Emulab uses block-level disk images and distributes them using the Frisbee [6] disk imaging system. The format uses filesystem-aware compression, meaning that it does not store disk blocks that are not used by the filesytem, and compresses the allocated blocks with zlib [7] for efficient storage. Frisbee uses IP multicast to distribute images, and is highly optimized so that the bottleneck in image distribution and installation is the write speed of the target disk. The amount of time required to load a disk image depends on the number of used blocks in the filesystem that it contains, but is typically on the order of a few minutes. Facility images are visible to and may be requested by all users. User images are visible only to their creators unless the creator decides to make the image public, which few do.

2.1

Dataset Details

2

Dataset

Emulab is a network testbed widely used by the distributed systems and networking communities. An experimenter describes a network in terms of links and hosts. Included in this specification is the disk image to be loaded on each host. Emulab then provisions servers, physical or virtual, loading the requested disk image. This provisioning is done on demand as requests come in, and there is only limited support for ahead-of-time scheduling or batch jobs. The facility provides a number of standard images, including "default" images that are used if the user does not explicitly request an image. Many users create their own images by booting from a facility image, customizing it (for example, by installing software packages or modifying the operating system), and taking a snapshot. This user image can be referenced in future requests, saving the user the effort of re-installing the packages they use, or to scale out to much larger experiments. This basic model of image usage and creation is similar to that used in most IaaS clouds [14]. Emulab is capable of provisioning both physical and virtual machines; physical machines are the most commonly allocated resource. While many IaaS clouds provi-

The dataset that we study covers four years of disk image requests on Emulab, from March 2009 to March 2013. The dataset covers a total of 279,972 requests for 714 unique images. The requests were made by 368 users, at an average rate of 192 disk images loaded per day. The records cover the identity of the image, the user making the request, and the timestamp at which the request was made. Furthermore, the data indicates whether each image was a facility image or a user image, and whether it was requested explicitly by the user or was chosen as a default because the user did not specify an image. To preserve user anonymity, users and user images are assigned random integers as identifiers in this paper. We present the names of facility images using their Emulab-assigned names; user images are presented as user/image pairs. One of the things we studied was the block-level differences between images. Our primary interest in examining the contents of images is to determine the potential savings from loading a "base" image (usually a facility image), then transferring and writing only the disk blocks required to transform it into a particular "derived" image (usually a user image). We define the difference of two images A and B as: (A, B ) = |i  b : B [i] = A[i]| (1)

where b is the set the indices of allocated storage blocks in image B , and A[i] and B [i] are the contents of images A and B , respectively, at index i. This measure directly

218 11th USENIX Symposium on Networked Systems Design and Implementation 

USENIX Association

captures the numbers of blocks in image B that would need to be written to a disk that already contains image A. We define  (A, B ) as the fraction of blocks that would need to be written: that is,  (A, B ) = (A, B ) |b| (2)

that purchase time on a cloud such as EC2, or individual business units that share a company-wide datacenter. In the remainder of this paper, we consider all individuals who are part of the same project to be a singe "user" of the facility--when we refer to "users," we are referring to Emulab projects. The number of individuals who requested disk image loads over this time period was 1,301.

The Emulab dataset does not record the provenance of images (that is, which user images where based on which facility images). We assume that each user image U was based on the facility image F for which (F, U ) is minimized. For a particular image U , we refer to this base image as UB . Emulab allow users to delete their image files: only 37.4% (267) of the images found in the request traces were available for analysis of block-level similarities. Though large in number, the missing images were relatively unpopular, accounting for only 15.8% of all requests. Emulab also allows its users to modify images, so the image files that we analyzed represented a snapshot of image contents at a particular point in time.

2.4

Limitations of This Study

2.2

Removing Sources of Bias in the Dataset

We filtered the dataset to remove certain biases. First, we omit all uses of the facility by its operational staff: the maintenance, testing, etc. that they perform is likely to follow different patterns than users of the facility. Second, as a network testbed, Emulab supports a feature known as "delay nodes," [13, 12] which perform a trafficshaping role that does not represent a function present in most multi-tenant datacenters. Third, Emulab includes some resources that are not the standard PC servers used in clouds and datacenters: these include wireless nodes, programmable network hardware, and sensors. This filtering removed 183,824 of the original 463,796 requests (39.6%), 215 images (23.1%), and 30 users (7.5%), leaving us with the 279,972 requests, 714 images, and 368 users that we studied. It is worth making special note of Emulab's "default" images. If an Emulab experimenter does not specify a particular disk image in their experiment description, they get a default that is, for historical reasons, quite old. Due to their ages, the default images are not very popular. Most users select the facility image that best meets their needs; as a result, the presence of a default does not have a dominating effect on the way that users select images.

2.3

Users and Projects

For the purposes of this study, we consider users at the level of organizations. Emulab groups individual users into "projects." These loosely-defined groups represent research groups, classes, or cross-institution collaborations. Because of this, they are analogous to businesses

The Emulab dataset is, to our knowledge, the only one of its type currently publicly available. Therefore, we cannot quantitatively assess the degree to which it matches other multi-tenant facilities. We believe our analysis remains valuable nonetheless, for two reasons. First, it is the only analysis to date to apply such a large quantity of real-world data to the problem of improving disk imaging systems. Second, we conjecture that the most fundamental findings in our work remain applicable in other environments, even if specifics (such as the  parameter to the facility image popularity distribution) differ. Our dataset covers a large number of disk image loads, but comes from a mid-sized facility. We attempt to analyze the effects of facility size in Section 4.3, but application of our conclusions to larger facilities necessarily involves extrapolation. In addition, two features unique to Emulab affected our ability to run certain analyses. First, the nature of resource allocation in Emulab makes it difficult to study the inter-arrival times of image requests. Emulab's primary unit of resource allocation is the experiment: a collections of hosts that together make up a network experiment. In contrast, most IaaS clouds consider only individual servers or "instances," and the cloud has no semantic information about which instances are contributing to the same application. Thus, image requests in Emulab arrive in well-defined bursts that do not have a direct analog in many other datacenters. Deploying an application in a datacenter or cloud does often involve provisioning of multiple machines in a short timeframe; however, we have no data that would allow us to analyze whether experiment sizes in Emulab are representative of burst sizes in other environments. For this reason, we avoid analyzing this aspect of the dataset, and all of our analyses are with respect to individual loads of disk images rather than Emulab experiments. Second, we chose not to analyze the relative popularity of the operating systems contained in the images (eg. Linux vs. BSD, or the relative popularities of Linux distributions). Emulab's user base is overwhelmingly comprised of academic researchers and students, and their OS preferences may not be representative of a broader population. In particular, while Emulab supports Windows, it constitutes a small fraction of all Emulab use--almost certainly a smaller fraction than would be seen in other

USENIX Association 

11th USENIX Symposium on Networked Systems Design and Implementation 219

10000

Facility User

Requests

10

100

1000

u

u
0 100 200 300 Rank 400 500 600

Figure 1: Requests for facility and user images, sorted on the x axis by popularity. Note the log-scale y axis.

u u u

settings. We restrict our analysis to the popularity of disk images rather than the operating systems they contain, and it is possible that this distribution is affected by the operating system preferences of Emulab's user base.

u u

3

Storage of Disk Images

We begin our study by examining the basic properties of the images in our dataset, with an eye towards understanding how they should be stored. We pay special attention to the relationship between images that are provided by the facility and images that are created by users; as we will see through further analysis, these images have different characteristics that warrant different treatment.

u

u u u

Image name All facility images All user images RHL90-STD [D] FEDORA10-STD UBUNTU10-STD RHL90-STD FC4-UPDATE 715/10 FBSD410-STD FEDORA8-STD 237/69 296/35 787/24 UBUNTU70-STD UBUNTU12-64-STD 787/14 226/44 FEDORA10-UPDATE CENTOS55-64-STD FC6-STD 762/69 FC4-WIRELESS FC4-STD FEDORA10-STD [D] UBUNTU11-64-STD 624/89 238/50 226/51

Requests 155,617 124,355 21,993 18,042 14,402 13,182 12,097 11,156 8,916 8,153 7,512 7,179 6,243 6,021 5,834 5,231 5,198 4,861 4,710 4,455 4,213 3,700 3,615 3,604 3,383 3,277 3,113 2,899

% 55.6% 44.4% 7.9% 6.4% 5.1% 4.7% 4.3% 4.0% 3.2% 2.9% 2.7% 2.6% 2.2% 2.2% 2.1% 1.9% 1.9% 1.7% 1.7% 1.6% 1.5% 1.3% 1.3% 1.3% 1.2% 1.2% 1.1% 1.0%

3.1

1

Prevalence of User Images

During the 48 months covered by our dataset, there were a total of 368 users. Of these, nearly two thirds (231) used only facility images, and slightly over one third (137, or 37.2%) used at least one user image. This implies that optimizing the provisioning of facility images can improve the experience of a majority of users. For example, if a suitable set of facility images can be identified for preloading on to servers, this could take image loading out of the critical path for creation of those users' instances. We explore this issue further in Section 6. The number of users who request user images, however, is non-negligible, suggesting that an imaging system should also take their needs into account. In fact, we find that there are more user images in our dataset (619) than facility images (94), meaning that, on average, each user who creates at least one disk image creates 4.5 of them.

Table 1: Total requests for all user and facility images. Also shown are the number of requests for all images that account for more than 1% of all requests. User images are marked with a `u' in the left column, and images requested implicitly as defaults are marked with a `[D]'; explicit requests for default images are counted separately.

3.2

Popularity of User Imags

The top of Table 1 shows the relative popularity of facility and user images. We see that the percentage of requests for user images is over 44%; since only 37.2% of users

create their own images, this imples that this set of users are heavier users of the testbed by at least 18%. Table 1 also shows all images that made up at least 1% of the requests. Of these twenty four images, ten are user images. Note that RHL90-STD and FEDORA10-STD each appear twice, because they are both common explicitly requested images and also images loaded by default. The complete image popularity data is plotted in Figure 1. We can see that the number of user images is much larger than the number of facility images, but that the population of user images contains many images that are used few times. Together, the top 17 facility images are more popular than the top 17 user images (the 17th facility image had 1,772 requests, and the 17th user 1,330). From the 18th image onwards, the user images are more popular--the 18th user image had 1,260 requests and the 18th facility image had

220 11th USENIX Symposium on Networked Systems Design and Implementation 

USENIX Association

1,233. Both facility and user images have tails consisting of images that were requested fewer than ten times, but this tail is much more prevalent in the case of user images, where the tail represents nearly half of all user images. From this data, we can conclude that facility images dominate, but that there are a small number of user images that are as popular as some facility images.

Frequency

3.3

Image Lifespan

0 0

1

5 10

50

200

500

The Emulab dataset does not include explicit creation and deletion dates for images. Thus, we define the lifespan of an image to be the number of days between when the image was first seen in the request stream and when it was last seen. Note that this will tend to underestimate lifespan: some images were likely first used before our dataset begins, and some will continue to be used after the end of the dataset. A histogram of user image lifespans can be seen in Figure 2. While the majority of images have very short lifespans, there is a long tail: several were used throughout the entire four years covered by the dataset. The observed mean lifespan is 100.4 days. We found the number of images with short lifespans to be quite surprising, so we examined them in greater detail, and it became clear that a large majority of these shortlifespan images were requested only on a single day: 196 of the 619 user images (31%) fall into this usage pattern. This suggests that a number of users create images for the purposes of running a single experiment, a conclusion borne out by looking at the experiment metadata. Finally, we looked at how long user images "go idle". We found that it is common for user images to have gaps of months in between requests for them. During this time, there is no need to have the images constantly available; they could be moved to cheaper, but slower, storage. The distribution of the maximum idle periods for the 214 user images with a lifespan of at least 30 days is shown in Figure 3. In total, 162 of the images (76% of long-lived images, and 26% of images overall) had gaps in usage of one month or more. Two images even had gaps of over two years between successive uses.

100

300

500

700

900

1100

1300

1500

Image lifetime (days)

Figure 2: Histogram of the lifespans of user images. Note that the y axis is log-scale.
50 Frequency 0 0 10 20 30 40

90

180

270

360

450

540

630

720

810

Maximum interval between requests (days)

Figure 3: Histogram of usage gaps for user images.
40 Frequency 0 0 10 20 30

20

40 % similarity

60

80

100

3.4

Block-Level Differences Between Images

We next examined how much user images differ from the facility images they are based on. We use the definitions of (A, B ),  (A, B ), and "base" images given in Section 2.1. Figure 4 shows a histogram of similarities between user images and their associated base facility images. From this figure, it is clear that many user images do show significant similarities to their bases--most are more than 50% similar, with a significant peak in the 60%­80% range. This is in line with findings from

Figure 4: Histogram of similarity (1 -  (UB , U )) between user images and their associated base images. Higher percentages indicate more similarity.

smaller studies in the past [8]. There is also a significant tail of more than twenty images images with very low similarity (below 10%) to their base images. Overall, these numbers point to two potential strategies for improving disk imaging systems. First, they suggest that significant storage savings can be had by storing images in a deduplicating storage system [10], which would

USENIX Association 

11th USENIX Symposium on Networked Systems Design and Implementation 221





Frequency





                                                



0

50

100

150



Median = 12.00 Mean = 11.98 Std. dev. = 4.33

   

   

   

    

0

5

10

15

20

25

30

Images used during day

Figure 5: Variation of facility image popularity over time. The fifteen most popular facility images are shown.

Figure 6: Histogram showing the distribution of the working set size over one-day periods (midnight to midnight).
Median = 30.00 Mean = 30.17 Std. dev. = 6.43 15 Frequency 0 15 5 10

store only one copy of the blocks that the base and derived images have in common. Second, they suggest that the technique of differential disk loading, which transforms a base image into a derived image by writing only the blocks that differ, has a potential for reducing the time and bandwith for distributing the user images. We explore the latter in detail in Section 7.

4

Working Set Size and Caching Potential

Having looked at the images themselves, we turn our attention to trends of usage over time, paying particular attention to the working set; understanding the size and composition of the working set is critical to designing strategies for caching and pre-loading.

20

25

30

35

40

45

Images used during week

Figure 7: Histogram showing the distribution of the working set size over one week periods (Sunday to Saturday).

4.1

No Dominant Images

If a small set of facility images dominates the request stream, it would be possible to design the disk imaging system around that fact. In particular, it would make sense to pre-load most or all idle disks with popular images, allowing user requests to be satisfied without waiting for a disk to load. This is the policy adopted by Emulab: the images labeled `[D]' in Table 1 are loaded as part of the process of freeing machines for the next user. As we can see in Figure 5, there is no such dominant image. The popularity of all facility images fluctuates wildly from month to month, with new images becoming popular quickly, old images falling out of favor, and some images swinging between popular and unpopular. Even the default images, which remain active throughout the entire time period, sees large changes in popularity. Note that we do not distinguish between explicit and implicit requests for default images as we did in Table 1; for the purposes of disk loading, these two cases are equivalent. As a result, we conclude that the strategy of pre-loading a single default image is unhelpful. It is, in fact, counterproductive: servers must be taken out of circulation

while they are loaded with the default image, and most are re-loaded a second time when requested by a user. If pre-loading strategies are to be useful, they will require more sophisticated methods for predicting future requests.

4.2

Size and Variation of the Working Set

Figure 6 depicts the working set size (number of unique images requested) over one-day periods. The mean working set size is quite small, at a mean of 11.98 images per day--this represents only 1.7% of the total number of images. While there is some variation in the working set size, it is not large: it follows a normal distribution with a standard deviation of 4.33. This result is encouraging from the perspective of caching: it suggests that only a small fraction of images need to be available for quick loading at any point in time, and that others could be stored in cheaper, slower storage systems. Figure 7 shows the distribution over week-long periods. The average working set size is approximately two and a half times larger than the daily average, and again follows a normal distribution with a reasonably small standard deviation.

222 11th USENIX Symposium on Networked Systems Design and Implementation 

USENIX Association

100 200 300 400 500 600 700

Mean daily working set size

Number of images used

1

2

3

4

5

6

User Facility

7

User Facility 0 20 40 60 80 100

0

0

20

40

60

80

100

0

Percentage of user base

Percentage of user base

Figure 8: Total number of images used over four years when considering random subsamples of the Emulab userbase.

Figure 9: Mean daily working set size when considering random subsamples of the Emulab userbase.

4.3

Scaling of the Working Set

To get a feel for how the size of the working set might vary on facilities larger or smaller than Emulab, we subsampled our data to simulate differently sized userbases. Figure 8 shows the total number of images used over the 4-year period when considering only 10% of the userbase, 20%, etc. The set of facility images quickly reaches saturation (all images are used at least once) and stops growing with additional users. The set of user images, on the other hand, grows linearly with respect to the number of users. This is explained by simple intuition: the set of useful facility images is more a function of the facility than of the userbase, while more users mean more usercreated images. Thus, we can expect that a facility with many more users than Emulab will have a greater number of user images in proportion roughly to its greater userbase, but that its set of facility images will not be larger by the same proportion. Indeed, Amazon EC2, which has a userbase that is at least three orders of magnitude larger than Emulab, advertises less than thirty images provided directly by AWS [3] and less than a hundred public images provided by their business partners. In comparison, Emulab has 94 public facility-provided images. However, this does not quite tell the whole story. Figure 9 shows the same subsampings, but this time looks at the mean daily working set size. Here, we see that the number of images loaded in a typical day increases linearly with the userbase for both facility and user images. Thus, we can expect that facilities much larger than Emulab do exhibit larger working sets. The working set of facility images is capped by the total number of such images, so very large facilities are likely to include most or all of their facility images in the daily working set. The general trend we can expect, is that for small facilities, the daily image working set size is in direct proportion to the size of the userbase. For large facilities, the working set will contain a relatively small set of facility

images, and a very large set of user images; however, we find that the fraction of requests that are for user images stays fairly constant regardless of the size of the userbase, meaning that these requests must necessarily be diverse.

5

Users' Behavior

We now turn our attention to the behavior of individual users; a facility that understands how its users interact with images is in a better position to provide the interfaces and image management tools that they require.

5.1

Distribution of Image Popularity

In distributions with "light" tails, such as the normal distribution, a relatively small subset of the population accounts for most of the popularity. For "heavy tailed" distributions (defined as those whose tail is not bounded by the exponential [17]), this effect is less pronounced, and it takes more of the population to cover the same level of popularity. We compared the popularity distributions of facility and user images separately to exponential distributions chosen to match the sample means. We found that facility images are a reasonably good match for the corresponding exponential distribution (with Kolmogorov Smirnov statistic nD n = 1.13), but user images are not  ( nDn = 5.54). As can be seen in Figure 10, the tail for user images lies substantially above the exponential. This is a key finding: user-created images have a significant heavy tail, while facility-provided images do not. The primary consequence of this discrepancy is that strategies that depend on being able satisfy a large number of requests with a relatively small number of images (such as pre-loading, examined in detail in Section 6), will be more effective with facility images than with user images.

USENIX Association 

11th USENIX Symposium on Networked Systems Design and Implementation 223

Facility Exp(=0.143) User Exp(=0.026)

      

Requests

1

10

100

1000

10000

0

100

200

300 Rank

400

500

600

  

Figure 10: Distribution of image popularity compared to the exponential (shown as dashed lines); note the log-scale y axis.

Figure 12: Profile of users making at least 500 disk image requests. Requests for facility images are shown as bars above the axis, and user images are below the axis.

5.3

Behavior of Heavy Users

60

0

10

20

30

40

50

60

Images used

Figure 11: Histogram showing the number of users who use different quantities of images.

5.2

Users and Images

As we can see in Figure 11, most users use a relatively small set of images. There are, however, two surprising features of this data. Only 20% of users used a single image--a large majority used two or more. We believe that this is due to three factors. First, since our sample period covers four years, many users likely migrated to newer versions of images as operating systems were updated. Second, any user who creates a custom image will use at least two images: they will request the base facility image at least once, then move to the custom image they create. Third, users may have started off using the default images provided by Emulab, found them unsuitable for their needs, and switched to non-default images. Another surprising feature is that there are a small number of users who use a very large number of images. Twenty users use at least 20 images, and one outlier uses more than 60.

Because user images are created by customizing facility images, we can expect that all users will employ facility images at least once, and likely a few times. The question remains, however, whether users tend to use primarily facility images, primarily their own images, or some balanced mixture of the two. We are particularly interested in the answer to this question for heavy users of the facility. Figure 12 shows a profile of the heaviest users (those who made at least 500 image requests) from the Emulab dataset. Two important facts are evident. First, while a few users do mix facility and user images (i.e. have bars both above and below the axis in the figure), most tend to skew heavily towards one or the other. Second, among the twenty heaviest users, twelve employ primarily user images. Past this point, facility images dominate. This clearly establishes that custom user images are a "power user" feature: their dominant use is by a relatively small number of users, who use them heavily.

Frequency

0

20

40

6

Prediction and Pre-Loading

We now turn our attention to techniques that may allow the facility to service user requests more quickly. The first technique that we consider is pre-loading: if it is possible to predict which images will be requested in the near future, the facility can pre-load them onto idle disks. If the predictions are correct, users requests may be satisfied immediately; if not, the user will have to wait for their image to be loaded. Note that this strategy does not save bandwidth on the datacenter's image distribution network; it simply shifts the image distribution to before the user's request arrives. In fact, pre-loading may increase the bandwidth used for distributing images: in the case of mispredictions, a node pre-loaded with one disk image may need to be re-loaded with another.

224 11th USENIX Symposium on Networked Systems Design and Implementation 

USENIX Association

100

80

60

40

20

Figure 13: Percentage of requests for three classes of images.
1.0

Probability of satisfying request

0.0

0.2

0.4

0.6

0.8

Figure 14: Fraction of requests satisfied from pre-loaded images for varying ratios of free pool size to the working set size.

6.1

We begin with the observation from Section 3.2 that the popularity of user images has a much longer, heavier tail than the set of requests for facility images. Therefore, strategies targeting prediction of facility images are likely to bear more fruit. We also recall from Section 4.1 that there does not exist a consistently dominant image, though Section 4.2 showed us that the working set size over a day is fairly small. This small working set size is encouraging from a prediction standpoint. An illustration of the potential for prediction can be found in Figure 13, which shows three classes of image requests. Requests for default images can be satisfied by simply pre-loading default images without complicated prediction strategies. This strategy is clearly ineffective in Emulab, as few requests are for the defaults. On top of these are requests for non-default facility images, which represent attractive targets for pre-loading. Finally, we see that approximately 40% of requests are for user images, which are a poor target for prediction because of their long tail. Thus, we target the 60% of requests that are for the relatively predictable facility images. We first consider how the size of the free pool affects the potential for prediction, where the free pool is defined

2009-03 2009-04 2009-05 2009-06 2009-07 2009-08 2009-09 2009-10 2009-11 2009-12 2010-01 2010-02 2010-03 2010-04 2010-05 2010-06 2010-07 2010-08 2010-09 2010-10 2010-11 2010-12 2011-01 2011-02 2011-03 2011-04 2011-05 2011-06 2011-07 2011-08 2011-09 2011-10 2011-11 2011-12 2012-01 2012-02 2012-03 2012-04 2012-05 2012-06 2012-07 2012-08 2012-09 2012-10 2012-11 2012-12 2013-01 2013-02 TOTAL

0

Default

Other Facility

User Images

0.0

0.2

0.4

0.6

0.8

1.0

Ratio of free pool size to number of images

Free Pool vs. Working Set Size

as the set of idle nodes or disks that are not in use and are thus available for pre-loading. We consider a simple model in which we assume that the inter-arrival time of requests is greater than the time required to load an image. (We will relax this assumption below.) In this model, the determinant of prediction accuracy is the ratio between the size of the free pool and the working set size. In this scenario, the best prediction mechanism is to pre-load those N disks with the N most popular images. Figure 14 shows the percentage of requests for facility images satisfied under this model, using the empirical request and working set data from Emulab. Intuitively, if there are no disks available for pre-loading, it is not possible to satisfy any requests from pre-loaded machines, and if one can pre-load the entire working set of images (the ratio is 1.0 or greater), it is possible to satisfy all requests. Because the distribution of facility image popularity is roughly exponential, the ability to load the top 25% of images satisfies 95% of all facility requests. It is interesting to consider how this result applies to different sizes of facilities. In many cases, the size of the free pool will be a fraction of the physical resources, meaning that it is much larger, in absolute terms, for larger facilities. At the same time, we have seen that the working set size of facility images grows linearly with the userbase, but is capped at a relatively small size by the total number of facility images. The practical effect is that small facilities (tens of nodes) are likely to fall on the left side of the curve in Figure 14, meaning that pre-loading is not likely to be particularly effective. Large facilities (thousands of nodes), on the other hand, are likely to be on the far right, with free pool sizes that far exceed the number of facility images--for them, pre-loading is likely to be able to satisfy all requests for facility images. In between these extremes, a facility needs to carefully consider the free pool to working set ratio to determine whether pre-loading makes sense.

6.2

Reload Rate vs. Arrival Rate

Our previous experiment made the simplifying assumption that request inter-arrival time was smaller than the time required to re-load an image; this enables the facility to ensure that the N most popular facility images are loaded at all times, and that only one copy of each image needs to be kept pre-loaded. We now consider the relationship between the arrival rate of new requests and the rate at which the facility can pre-load images in response. If bursts of requests arrive at a faster rate the the facility can re-image, it is useful to have more than one pre-loaded copy of each image. It is also possible for bursts of requests to outpace the facility's ability to keep the image loaded, meaning that there can be mispredictions even for very popular images.

USENIX Association 

11th USENIX Symposium on Networked Systems Design and Implementation 225

7

Differential Disk Loading

1.0

0.0

0.5

1.0

1.5

Reload rate (normalised to mean arrival rate)

Figure 15: Fraction of requests satisfied against the rate at which images can be pre-loaded.

We model this scenario using standard tools from queuing theory: each image is modeled as a queue, with a number of queue slots equal to the number of disks onto which the image is pre-loaded. The distribution of pre-loaded images is taken directly from the observed distribution of requests; using our results from Section 5.1, we model this distribution as being exponential with  = 0.143. We make the standard queuing theory assumption that requests arrive according to a Poisson process [18]. We picked a facility size of 1,000 disks, with an average utilization rate of 90%, meaning that on average, 100 disks are available for pre-loading. Figure 15 shows the results of a Monte Carlo simulation using this model. We varied the ratio of reload rate to the mean request arrival rate, and find that this ratio is critical. If the facility can reload images at a faster rate than requests arrive (the area to the right of the 1.0 ratio), it can easily keep the proper set of facility images preloaded and can satisfy most requests for these images; this matches the case modeled in Figure 14. If the reload rate is lower (to the left of the 1.0 ratio), the value of preloading falls quickly, as bursts of requests overwhelm the facility's ability to keep a pre-loaded pool that contains the appropriate set of images. We conclude that pre-loading facility images can be an effective strategy for reducing user wait time, but that the critical determining factors for its success are: (1) the ratio betwen the size of the free pool and the working set size; and (2) the ratio between the facilities' reload rate and the mean arrival rate.

The second optimization we consider targets requests for user images: it may be possible to pre-load facility images, and when requests for user images arrive, load only the blocks that differ. This differential loading strategy is attractive for two reasons. As we saw in Section 5.1, the distribution of user image popularity has a heavy tail, making it difficult to pre-load enough of them to satisfy many requests. But, as we saw in Section 3.4, user images have high levels of similarity to the smaller set of facility images. Thus, we have the potential to reduce user wait times by picking a pre-loaded facility image and doing a fast load of just the blocks that differ. In this section, we develop metrics that quantify the potential benefits of differential disk loading and give us a general understanding of the potential effectiveness of this technique. In order to realize these benefits, additional methods for predicting future requests would need to be developed, which take into account not only image popularity, but also block-level similarity between the pre-loaded images and the images that may be layered on top of them. We consider only the problem of finding the differences between two disk images, and not the more general problem of taking the difference between a disk image and arbitrary disk state (i.e. the state in which the disk is left by the previous user). Earlier work [6] has shown that disk distribution and installation can run at the full write speed of the target disk, meaning that schemes that require reading disk contents before writing are likely to slow the process down, and are likely to be fruitful only in cases where users do not write much to the disk.

Probability of satisfying request

0.0

0.2

0.4

0.6

0.8

7.1

Limits to Savings

As we have seen, the set of facility images is smaller and more predictable than the set of user images. Thus, as with the last section, we continue to pre-load only facility images; when a user image U is requested, if its base image UB has been pre-loaded, we need to transfer only (UB , U ) blocks instead of the full |u| blocks belonging to the image. Clearly, this strategy relies on having the correct set of base images pre-loaded. To simplify, we start by assuming that we have an oracle that tells us what facility images to pre-load or sufficient capacity to preload all facility images; we relax this assumption below. We begin by defining the number of disk blocks loaded for user images when differential loading is not in use (i.e. the entire user image must be loaded). For an individual image U , this quantity is: |u| · UC (3)

Recall that u is the set of block addresses with defined values in image U , and therefore |u| represents the size

226 11th USENIX Symposium on Networked Systems Design and Implementation 

USENIX Association

of the image. We define UC to be the number of times the image is loaded. Intuitively, then, this quantity is simply the number of blocks in the image multiplied by the number of times the image is used. To obtain the total number of blocks loaded across the universe of all user images, U, we sum the total blocks loaded for each image U  U:
U U

User image traffic required

|u| · UC

0.0 0

(4)

0.2

0.4

0.6

0.8

1.0

20

40

60

80

100

To adapt these equations for differential loading, we substitute (UB , U ) for |u|, giving us the number of blocks that must be loaded assuming the base image has been pre-loaded. This gives us the total number of blocks: (UB , U ) · UC (5)

Percentage of facility images pre-loaded

Figure 16: Network traffic required to load user images, when various facility images may be pre-loaded.

U U

Differential Savings Potential (DSP): The maximum relative savings from differential loading (assuming the correct UB images are always loaded) is derived by combining Equation 4 and Equation 5: DSP =
U U

|u| - (UB , U ) UC |u|

(6)

In the Emulab dataset, the values for Equation 4 and Equation 5 are 174 TB and 78 TB, giving a DSP of 0.55. This indicates that, in the presence of an oracle, the Emulab facility could save over half of the blocks it transfers for user images at request time, potentially halving the average time users must wait for custom images to load. Adjusted Differential Savings (ADS): We next relax the assumption of an oracle. To do so, we use the notation P [I ] to indicate the probability that image I is pre-loaded on the facility. We adjust Equation 6 to indicate that with some probability, the user request can be fulfilled with differential loading because the requisite base image is loaded. If not, the entire image must be loaded (resulting in no savings): ADS =
U U

pre-loaded. The y axis of this graph represents the fraction of blocks that must be loaded at request time, with lower numbers being better, and the limit being 1 - DSP (0.45). Along the x axis, we show the fraction of facility images loaded--we rank facilitiy images by an adjusted popularity that is the sum of their own popularity and the popularity of all users images that use that facility image as a base, and then pre-load the x most popular. What we can see is that relatively few facility images act as bases for user images, so it is necessary to pre-load only a small subset of them (approximately 20%) in order to get most of the benefit of differential loading. This implies that this technique can be effective even on facilities that have low free pool to working set ratios. Also of interest in Figure 16 is that, for our dataset, the most popular facility images (the default images) are not commonly used as bases for user images--this accounts for the small plateau on the left of the graph. We hypothesize that this is due to the age of Emulab's defaults.

8

Recommendations and Future Work

P [ UB ]

|u| - (UB , U ) UC |u|

(7)

In our exploration of the Emulab disk image request dataset, we have uncovered a number of properties that can be used to guide the operation and design of disk image storage and installation systems. Based on our analysis, we make the following recommendations: · Storing images in a deduplicating image store is likely to result in substantial savings. Reads from deduplicating stores can be slow, but the working set size is small enough that it is possible to cache images in faster storage. · Focusing pre-loading strategies on facility images is likely to produce the best results. The tail of user images is much longer and heavier than the one for facility images, and only a few user images approach the popularity of the heaviest-used facility

Note that if P [UB ] = 1 for all images (perfect prediction), this gives us Equation 6. For smaller P [UB ] values (worse predictions), the adjusted savings are lower than the savings potential, which fits with the intuitive notion that sub-optimal pre-loading will reduce the value of differential loading.

7.2

Savings With Predictions

Figure 16 shows the effectiveness of differential loading as a function of the fraction of facility images that are

USENIX Association 

11th USENIX Symposium on Networked Systems Design and Implementation 227

images. For very large facilities, it is likely that most facility images appear in the daily working set, making prediction straightforward. · Pre-loading of a single default image is not a useful strategy, as the diversity of user requests means that no one image, even the default, is dominant on any time scale. · For small facilities (those where the number of idle disks is significantly smaller than the working set size), pre-loading is likely not a valuable strategy. For large facilities, the number of idle disks is likely to be much larger than the working set size, making simple pre-loading strategies highly effective. To accurately model the effectiveness of pre-loading for mid-sized facilities, additional study of request inter-arrival distributions is necessary. · Large facilities would do well to focus on techniques that allow them to sustain high reload rates. The only way for pre-loading to be effective is to keep this rate significantly above the request arrival rate, which is likely to be high for large facilities. Techniques of interest include distribution using multicast and image distribution servers spread throughout the datacenter. · Differential loading has the potential to be effective, especially on facilities with limited free pools. It shows the potential to halve the number of disk blocks transferred to satisfy user requests, but that potential depends on correct predictions when preloading the appropriate base images. This changes the criteria for pre-loading, since base images should be selected not only on their own popularity, but also on the popularity of images that may be laid down on top and their block-level similarly with the base image. This complex optimization problem presents an interesting area for future study. An anonymized version of the dataset used for thus study, plus all code used to analyze it and produce the figures for this paper, can be found at: http://aptlab.net/p/tbres/nsdi14

References
[1] Amazon Web Services. Amazon EC2 instance store: User guide. http://docs.aws.amazon.com/AWSEC2/ latest/UserGuide/InstanceStorage.html. [2] Amazon Web Services. Amazon Elastic Compute Cloud website. http://aws.amazon.com/ec2/. [3] Amazon Web Services. Amazon Machine Images (AMIs). https://aws.amazon.com/amis. [4] M. Armbrust, A. Fox, R. Griffith, A. D. Joseph, R. H. Katz, A. Konwinski, G. Lee, D. A. Patterson, A. Rabkin, I. Stoica, and M. Zaharia. Above the clouds: A Berkeley view of cloud computing. Technical Report UCB/EECS-200928, EECS Department, University of California, Berkeley, Feb 2009. [5] L. A. Barroso and U. Holzle. The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines, volume 6 of Synthesis Lectures on Computer Architecture. Morgan and Claypool, 2009. [6] M. Hibler, L. Stoller, J. Lepreau, R. Ricci, and C. Barb. Fast, scalable disk imaging with Frisbee. In Proc. of the USENIX Annual Technical Conference (ATC), pages 283­ 296, San Antonio, TX, June 2003. [7] Jean-loup Gailly and Mark Adler. zlib website. http: //www.zlib.org. [8] K. Jin and E. L. Miller. The effectiveness of deduplication on virtual machine disk images. In Proc. of SYSTOR, the Israeli Experimental Systems Conference, May 2009. [9] D. Nurmi, R. Wolski, C. Grzegorczyk, G. Obertelli, S. Soman, L. Youseff, and D. Zagorodnov. The Eucalyptus open-source cloud-computing system. In Proc. of the Workshop on Cloud Computing and its Applications (CCA), 2008. [10] S. Quinlan and S. Dorward. Venti: A new approach to archival storage. In Proc. of the USENIX Conference on File and Storage Technologies (FAST), pages 89­101, Jan. 2002. [11] Rackspace US, Inc. Rackspace hosting website. http: //www.rackspace.com/. [12] L. Rizzo. Dummynet: a simple approach to the evaluation of network protocols. Computer Communication Review, 27(1):31­41, Jan. 1997. [13] P. Sanaga, J. Duerig, R. Ricci, and J. Lepreau. Modeling and emulation of Internet paths. In Proc. of the USENIX Symposium on Networked Systems Design and Implementation (NSDI), Boston, MA, Apr. 2009. [14] The OpenStack Team. OpenStack user documentation. http://docs.openstack.org/user-guide/. [15] The OpenStack Team. OpenStack website. http://www. openstack.org. [16] The University of Utah. Emulab website. http://www. emulab.net/. [17] Wikipedia: Heavy-tailed Distribution. http: //en.wikipedia.org/wiki/Heavy-tailed_ distribution. [18] Wikipedia: Poisson Process. http://en.wikipedia. org/wiki/Poisson_process.

Acknowledgments
We would like to thank the administrators of Emulab for their assistance in collecting the data used for this study. We would also like to thank Dave Andersen, our shepherd Bruce Maggs, and the anonymous reviewers for their valuable comments. This work was supported by NSF under award CNS-0709427.

228 11th USENIX Symposium on Networked Systems Design and Implementation 

USENIX Association

The portfolio of capability of an entire defence force is a complex system of systems. Understanding and managing this portfolio has many challenges. This paper presents an account of the successful practical application of systems engineering tools and approaches to support portfolio level decision making. The concept of âDefence Capability Situation Awarenessâ is presented along with the prototype tool âProgram Viewerâ that demonstrates its utility. DODAF 2 and UPDM provide the standards underlying the modelling and management of data, which is stored in a systems engineering tool. The data to populate the model is collected from existing data sources from across the organisation, and this provides a critical mass of relevant and useful information. Custom fit-for-purpose visualisation and manipulation tools are provided to support understanding of the portfolio by decision-makers from many perspectives and present the consequences of proposed courses of action.The portfolio of capability of an entire defence force is a complex system of systems. Understanding and managing this portfolio has many challenges. This paper presents an account of the successful practical application of systems engineering tools and approaches to support portfolio level decision making. The concept of âDefence Capability Situation Awarenessâ is presented along with the prototype tool âProgram Viewerâ that demonstrates its utility. DODAF 2 and UPDM provide the standards underlying the modelling and management of data, which is stored in a systems engineering tool. The data to populate the model is collected from existing data sources from across the organisation, and this provides a critical mass of relevant and useful information. Custom fit-for-purpose visualisation and manipulation tools are provided to support understanding of the portfolio by decision-makers from many perspectives and present the consequences of proposed courses of action.To Wireless Fidelity and Beyond - CAPTURE, Extending Indoor Positioning Systems

Item type Authors Publisher Downloaded Item License Link to item

Article Cullen, Gary IEEE 20-Dec-2016 14:36:19 http://creativecommons.org/licenses/by-nc-nd/4.0/ http://hdl.handle.net/10759/325943

To Wireless Fidelity and Beyond - CAPTURE, Extending Indoor Positioning Systems
Gary Cullen, Kevin Curran, Jose Santos
Intelligent Systems Research Centre, University of Ulster, Magee College, Derry, United Kingdom Gary.Cullen@lyit.ie
Abstract--The benefits of context aware computing and specifically the context that location can provide to applications and systems has been heavily documented in recent times. Moreover the move from traditional outdoor localization solutions to the indoor arena has seen a dramatic increase in research into this area. Most of this research has surrounded the problem of positioning accuracy, with attempts to solve this using a myriad of technologies and algorithms. One of the problems that seems to be somewhat overlooked is the issue of coverage in an indoor localization solution. The mostly unobstructed views of the Global Positioning System (GPS) which requires a mere 30 satellites to provide global coverage never had these problems. The dearth of literature around this issue in the outdoor arena is testament to this fact. Unfortunately unobstructed views are not something that can be achieved in most indoor environments and economical as well as physical barriers can prevent the installation of an infrastructure to achieve total coverage. In this paper we propose a solution to this issue of indoor coverage by deploying a solution to extend the range of a positioning system Cooperatively Applied Positioning Techniques Utilizing Range Extension (CAPTURE). CAPTURE provides a system to locate devices that cannot be reached by an in-house location based system. It presents a unique contribution to research in this field by offering the ability to utilize devices that currently know their location within a Location Based Solution (LBS), to help evaluate the position of unknown devices beyond the range capacity of the LBS. Effectively extending the locating distances of an Indoor LBS by utilizing the existing mobile infrastructure without the requirement for additional hardware. CAPTURE uses the Bluetooth radios on mobile devices to estimate the distance between devices, before inserting these range estimates into a trilateration algorithm to ascertain position. CAPTURE has been tested through experiments carried out in a real world environment, proving the capacity to provide a solution to the ranging issue. Keywords--Localization; Indoor positioning; Indoor localization; geographical positioning; Bluetooth; Cooperative Positioning.)

Gearoid Maguire, Denis Bourne
Letterkenny Institute of Technology, Co. Donegal, Ireland

Bluetooth beacon. The test area and experiments were the same for both systems and the results were weighed to evaluate the best solution to solve the problem of range in Indoor Positioning Systems (IPS). On loosing something or forgetting where you last placed something, a common piece of advice is to retrace your steps back in your mind. This can be quite a formidable task given the multimodal transport available today coupled with the complexity and scale of buildings we interact with on a regular basis. The ability to place an avatar of yourself onto a map to graphically retrace your steps in real-time would dramatically reduce the brain power required to remember everywhere you were at a given time. Googles maneuverings into the indoor location mappings realm [2] opens up the opportunity to deliver this virtual reality, currently being able to provide door to door route planning. Being able to navigate your way from your office desk out through your company's building (taking the stairwell to avoid your boss in the lift) is eminently achievable albeit with a small number of locations on a modern smartphone using google maps. A level switcher allows you to onion slice through multiple floor level plans, before switching to GPS to offer possible transport alternatives through the outdoor environment. On reaching what `historically' would have been your destination, Google Indoor Maps and more importantly an IPS picks up where GPS left off offering a point to point navigation solution. This can then take you through the complexities of an airport terminal for example, via specific waypoints such as security and check-in desks directly to your departure gate. One of the barriers to implementation of such a concept is the limitation in coverage and accuracy of currently implemented Indoor Position or Location Based Systems [3]. IPSs typically utilize pre-existing Wi-Fi network infrastructure taking ranging information from Wireless Access Points (WAP's) as inputs for a localization algorithm. Unfortunately the drivers behind the strategic decisions on the positioning of WAPs, in a Wi-Fi based solution, were typically to catch large congregations of users and primarily to provide the highest available throughput to those users. Coverage for IPSs is not necessarily to the forefront of network designer's minds when designing such networks, leaving large areas beyond the range of an IPS. GPS on the other hand, offers near global coverage, bar some issues with urban canyons and other high rise natural obstacles that prevent Line of Sight (LoS) to the just under 30 satellites required [4] to deliver such wide scope.

I.

INTRODUCTION

The first iteration of CAPTURE described in the following literature [1], used the RSSI readings taken from the IEEE 802.11 radio on the mobile devices to gauge the range between the devices by measuring the signal loss to estimate distance. The version of CAPTURE implemented and evaluated in this paper uses the Bluetooth radio on the devices to estimate distances between devices based on the RSSI received from the

The indoor environment does not afford such clear unobstructed views to and from tracking devices, the many doors, walls, floors, pillars and ceilings hinder the capacity of an IPS to locate devices. Furthermore the indoor arena is an especially noisy atmosphere, being home to other wireless devices such as Bluetooth Headsets, Cordless Phones and Microwave Ovens. All of these devices operate in the same frequency band as the Wi-Fi solution, namely 2.4 GHz and therefore can interfere with the reception of signals used to locate [3], making them behave in an unpredictable fashion. These environmental dynamics combine to dramatically affect the ability of an indoor solution to provide an acceptable level of coverage. Literature from Yang [4] and Rowe [5] reflect that Location Awareness is rapidly becoming a fundamental requirement for mobile application development. This highlights the challenges posed for ubiquitous localization of devices in the indoor arena. Considering users spend more time in an indoor environment, over 88.9% according to a recent Canadian study [5], the need for a solution is obvious. We propose a solution to this issue of coverage limitations by using a cooperative localization technique, CAPTURE. CAPTURE can plug into an in situ solution irrespective of the technology or location technique that solution currently uses to locate.

Consider the following scenario where a user `Bob', is in his favorite seat in the library, unfortunately the seat is in the far corner of the library, which can only be `seen' by one Wireless Access Point. In this position Bob's tablet can gain Wi-Fi access through this Access Point to allow him access to online resources. However one Access Point is not enough for the inhouse Location Based System to accurately locate Bob within the building using Trilateration positioning techniques. Sue is sitting near the front of the library and can be `seen' by 4 Wireless Access Points, and is thereby accurately located on the Location Based System. She is also 25 meters to the left of Bob and the Wireless Network Card on her Laptop can see Bob's tablet. The Librarian is stacking books on the shelves behind where Bob is sitting and her smartphone is currently located within the Location Based System also. The wireless NIC on her smartphone can also `see' Bob's tablet, therefore, in a normal scenario, Bob would be beyond the range of the Location Based System, but because CAPTURE can use the known positions of the Librarian and Sue and Bob's position relative to them it can accurately estimate Bob's position within the library. It provides a location relative to the devices locating it, which can then be mapped onto a global overview of the Location Based System (LBS), assisting in the aforementioned scenario to get you to the departure gate in a point to point navigation solution.

Figure 1: Sports Hall LyIT

The rest of this paper is laid out as follows; Section II describes the system model used to implement CAPTURE. Section III provides an overview of the experimental test bed used to evaluate the solution and Section IV documents the data collected during testing. In Section V we describe the findings of the experiments that were carried out, validating the feasibility of the system, the penultimate section, Section VI outlines the proposed implementation of CAPTURE and the paper concludes Section VII, providing an insight into some projected future work with CAPTURE. II. CAPTURE - SYSTEM MODEL This section describes a system model that can be used in a localization solution to establish the Cartesian coordinate values of a lost device within a two dimensional plane. CAPTURE does not require a preceding calibration stage or a site survey, providing a robust opportunistic solution in dynamic environments, using only real time RSSI values. We use the term reference device to describe devices that cooperatively assist in the positioning of lost or unknown devices. Traditionally the term anchor node is used to describe these devices, but this seems to elicit a perception of static or permanent devices, which in a cooperative solution these devices most certainly are not. Two key components typically make up the estimation of the position of a lost device. First of all ranging techniques are used to estimate the distance from the transmitting device(s) to the receiving device(s). This is calculated using a metric for example the length of time it takes a signal to propagate the distance from the transmitter to the receiver. The second component is the position estimation technique, here the ranging variables are calculated using one or more ranging techniques and these are used as input for an estimation algorithm to calculate the position of the lost device. A. CAPTURE ­ Bluetooth CAPTURE was first implemented using RSSI measurements from the 802.11 radio on mobile device [1]. The implementation of CAPTURE described in this paper utilizes Bluetooth radio beacons to measure range. Bluetooth has been around for quite some time now, designed by phone manufacturer Ericsson in 1994, it was initially developed to replace the then ageing RS-232 and Infrared (IR) interfaces for connecting peripheral devices. It operates at the same 2.4GHz frequency as Wi-Fi and is specified in the IEEE 802.15.1 standard. The overriding benefit of using Bluetooth for Indoor Localization is its availability in nearly every mobile device in use today. Using Bluetooth in a cooperative framework also allows the user to remain connected to the 802.11 network while simultaneously assisting in the location of others with Bluetooth radio signals. Bluetooth transmits beacons similar to 802.11 radios and the strength of the signal received from these beacons can be captured and measured to provide a range estimate. Kloch et al [6] investigate effects in Collaborative Indoor Localization as an example of selforganizing in ubiquitous sensing systems, using Bluetooth to correct Pedestrian Dead Reckoning (PDR) drift. They analyze the collaborative approach as a solution to the indoor localization problem, and found that when using PDR in

isolation the variance grows bigger as people are walking. That is to say that the position estimation becomes less and less accurate the further the people being tracked travel. Implementing a hybrid solution incorporating Bluetooth RSSI readings to measure the distance between devices, dramatically improved positioning accuracy. Bluetooth has been further used as a cooperative solution to the accuracy issue in IPS's [7-11]. B. RSSI ­ Received Signal Strength Indicator Possibly the most popular ranging technique used in Indoor Localization, Received Signal Strength Indicator (RSSI) is a measurement of the voltage that exists in a transmitted radio signal, which is an indication of the power being received by the antenna. When a signal first leaves a transmitting device, the power of the signal drops or attenuates, this is true of both wired and wireless transmissions. As a radio signal propagates through the air some of its power is absorbed and the signal loses a specific amount of its strength, therefore, the higher the RSSI value (or least negative in some devices), the stronger the signal. Knowing the amount of signal loss over a given distance provides a method to calculate the distance from a transmitting device, given a Received Signal Strength. At its most basic level this allows for the `coarse' localization or as referred to in other literature, `presence-based localization' [12] of a device relative to the transmitting device. This can be illustrated by the RSSI calculated distance being the radius of a circle and the `searching' device being at the centre of that circle. The estimated position of the lost device is anywhere on the circumference of that circle. In an IEEE 802.11 network if the locations of the Access Points are already known, then the location of Mobile Devices traversing the network can be located relative to them, albeit only to the circumference of the radius of the calculated distance. Further localization algorithms and position estimation filtering techniques must be applied to provide a more precise level of localization. In a cooperative paradigm, mobile devices can simulate the role carried out by Access Points, providing a relative reference to a lost devices location. RSSI values can be extracted from beacons transmitted between devices within range. Correlation of these signal indicators and distance can be estimated using many of the methods already applied throughout literature in this arena [13-15]. RSSI based or more broadly speaking, radio based Indoor Positioning Systems have had notoriously irregular environment variables such as reflection, refraction, diffraction and absorption of radio waves that can impact positioning estimated dramatically [16]. Although RSSI is a measure of signal loss, it is not a linear representation of how many dBm is actually reaching the card. If a signal indicator is reading -72, this means that it is 72 dBm less powerful by the time it gets to your device. Experimental test carried out at an early stage with CAPTURE further extoled this assumption. Results of these tests can be viewed in Table 1: 5 meter increments in Section V, Data Collection and Presentation. Crudely extracting the RSSI at given distance increments to attempt to derive a meter distance

being equal to a given dBm increase in RSSI reading was not going to yield any value worth using in any further experiments. The authors in [17] advocate a solution utilizing a RSSI smoothing Low Pass Filter (LPF) to minimize the dynamic fluctuation of the RSSI values. C. Trilateration Trilateration is a key component of the GPS position estimation techniques. It is a process that can estimate the position of a mobile device given the positions of at least three other objects and the distance from those objects to the device to be located. In the scenario depicted below in Figure 2(a), illustrated using a cooperative localization example, the circle depicts the distance from a reference device to a lost device. This distance would have been derived using the RSSI value between the reference and lost devices. All we can say about the whereabouts of the lost device is that it resides somewhere on the circumference of the circle that is constructed using the radius of the estimated measurement between the two devices. A second reference device will allow the position of the lost device to be narrowed further as can be seen in Figure 2(b). Now the ranging estimates of the lost device have been calculated relative to the second reference device also. Therefore considering the lost device must be on the circumference of the circles created by the distance between it and the two reference devices there are only 2 possible positions where it might be, the intersections of these two circles.

To calculate the exact position of the lost device we need a third reference device. When we calculate the distance from this final reference device to the lost device and considering we already know the distance from the other reference devices. We can then determine that the lost device can only be at one specific position to match those three particular distance estimations ­ the intersections of the three circles (see Figure 3). The ranging estimates calculated from the RSSI values in the tests were used as the inputs for the trilateration algorithm on the CAPTURE, to provide an estimate on the position of the lost phones. III. EXPERIMENTAL TEST BED In this section, we will provide evidence showing the suitability of CAPTURE as a solution to the indoor ranging problem. To do that we carried out a large campaign of measurements in the Sports Hall in Letterkenny Institute of Technology illustrated in Figure 1. The hall offers a 40m diagonal testing range, providing Line of Sight measurements for all tests, as can be seen in the picture depicted in Figure 4. When readings were been recorded all users vacated the hall, this provided an optimal environment to use as a benchmark for future tests on CAPTURE.

Figure 2: (a) Single Distance

(b) With 2nd Reference Device

Each phone used in the test is given a name (BSSID) TestPhone1, TestPhone2 for example. CAPTURE reads the RSSI of all available reference points, i.e. all devices it can `see', but it filters out only the test phones selected by the user carrying out the tests. This can be seen in the image in figure 5, and is achieved via a lookup table mapping the MAC address of the phone to the phone name. This allows the use of only a specified phone or a group of phones during any given test. A. System Components Figure 4: Test Environment The experimental setup of the prototype consisted of 7 Samsung GT-S5310 Galaxy Pocket phones (Figure 5), running Google Android 2.2.1 on a 600 MHz ARMv6, Adreno 200 GPU, Qualcomm MSM7227 chipset, were used to carry out the evaluation of the CAPTURE system. 3 of the phones were used as reference devices, the other phone acted as the lost device. All phones used during the test were of an exact make and model so as to rule out any issues with varied RSSI reads with different antenna types. Some of these issues have been described in the literature [18, 19]. Lisheng et al., [19] go so far as to describe the distortion being as much as 11.2 dBm out with different antenna types over a 25 meter read range. Although these issues referenced above describe problems in the 802.11 realm, it is the author's opinion that these could have an impact on Bluetooth radio signals also.

Figure 3: Trilateration Example

Where: n: Path Loss Exponent d: Distance from transmitting device A: Received signal strength at 1 meter distance The path loss exponent typically varies from 1.5 to 4, with 1.5 representing a free-space Line of Sight (LoS) value and 4 representing an environment that incorporates a high level of signal attenuation. Not having a good equation modeling the environment in which your experiments are to be deployed, will be reflected in horrible results. After initial pre-tests were evaluated, a Path Loss Exponent of 1.5 was determined for the test environment, because of the open plan design of the Hall offering LoS between all devices and the RSSI at 1 meter was measured at -66.8194. The results of the collected data are described in the following section. IV. DATA COLLECTION AND PRESENTATION Here we present all of the data collated throughout this work, the data sets are illustrated in the graph and table. During the recording of data the hall was emptied of people so as to provide a clean set of results. An initial test was run to establish the 1 meter range for input into the algorithm in equation 1, the results of this test can be seen in Figure 6. The fluctuations in the meter range values was one of the notable differences between the tests recorded in the 802.11 version of CAPTURE versus the Bluetooth version. In the Wi-Fi version meter read values were captured from -42 to -45. Here, as can be seen in the graph readings ranged from -62 to -77, a difference of 3dBm was recorded in the Wi-Fi test, with a difference of 15dBm in the Bluetooth experiments.

Figure 5: Test Phones During the tests all phones were place at a distance of 80cm above floor level, to mimic as close to a real world example of a user holding them. The phones were placed on identical platforms during the tests to negate the impact of Hand-Grip body-loss effect which can also impact ranging measurements [18]. Device orientation can also introduce errors when calculating signal range estimates, so all phones had the same orientation when used in our tests [20].  Database A MySQL Server version: 5.0.96 hosted on a Linux platform was used to store all data collected by the devices. The server was online and the phones wrote directly to it as they recorded RSSI values from each other. The data was then passed through a low level filter to remove any outliers, before an average RSSI reading was calculated for each required ranging measurement, to be used in the trilateration algorithm to estimate the position of the lost device.  Laptop A Dell Latitude E6440 iCore3 running Windows 7 Professional was used to develop the app to gather the RSSI from the phones. An algorithm was designed to convert this RSSI reading into a ranging measurement before a trilateration algorithm converted the ranging measurements into Cartesian coordinate values. We used the Eclipse IDE and Android Software Development Kit (SDK) for Android development and debugging, to develop the app. B. Ranging Measurement Estimation The RSSI values captured from the beacons transmitted by devices within range of the `lost device' were used to estimate the relative distance between them. As explained earlier RSSI values do not provide a linear representation of distance. The authors in [17] advocate using the formula in "(1)," below to estimate RSSI, and thereby extrapolate distance given RSSI: RSSI = - (10n Log10 (d) +A) Equation (1)

1 Meter RSSI values
-80 -75 -70 -65 -60 Figure 6: 1 meter readings 500 readings were recorded at various locations throughout the hall, to accurately obtain the meter value for the algorithm, these were smoothed with a filter before the final average was calculated. Further tests were then carried out to measure the accuracy of both the RSSI values received and the resulting range estimations from the algorithm. Table 1 below, depicts the results of tests to capture the RSSI values between two phones at 5 meter increments diagonally across the hall. It highlights the RSSI value beginning at -72.3793 for the 0-5 meter range.

A sample set of 200 readings were recorded per section, an average was then taken from this set. The standard deviation was also documented to illustrate any fluctuations in the received values. In our previous experiments with CAPTURE using Wi-Fi [1] the standard deviation was typically low, in this case using Bluetooth as can be seen in the table below standard deviation ranges from 4.2 to 2.9, these are large fluctuations from the average. Distance Average Std Dev Estimate Distance Average Std Dev Estimate 0-5m -72.3793 4.1140 3.73 0 - 25 m -80.6205 4.1062 28.82 0 - 10 m -74.8966 3.6327 7.62 0 - 30 m -80.9657 3.3776 29.38 0 - 15 m -76.6333 3.9603 11.20 0 - 35 m -80.2759 4.2823 27.87 0 - 20 m -76.3103 3.9226 9.69 0 - 40 m -83.3103 2.9490 49.95

They also know the distance between themselves:    TestPhone1 to TestPhone2 is 15 meters TestPhone1 to TestPhone3 is 13 meters TestPhone2 to TestPhone3 is 17 meters

The RSSI readings from the:    Lost Phone to TestPhone1 is - 77.5351dBm Lost Phone to TestPhone2 is - 78.8457dBm Lost Phone to TestPhone3 is - 76.1021dBm

These RSSI readings translate to a ranging estimate of 13.345, 15.1221 and 9.349 meters respectively when put through the ranging algorithm. The actual distance between:    TestPhone1 and the Lost Phone is 11.5 meters TestPhone2 and the Lost Phone is 13.2 meters TestPhone3 and the Lost Phone is 11.9 meters

Table 1: 5 meter increments The average was then inputted into the algorithm to derive a range estimate based on the RSSI values received. As mentioned before RSSI values do not provide a linear representation of measurement, and therefore some of the increments do not initially seem like they could assist in finding a distance at a given measurement. The ranging estimates show an error high of 11.31 meters at the 0-20 meter range and low of .62 meters at the 0-30 meter range. V. EXPERIMENTAL RESULTS Figure 7 depicts one of the tests where CAPTURE accurately locates a lost phone within 2.5 meters. TestPhone1, TestPhone2 and TesPhone3 know their location, via the inhouse IPS.

Giving an approximate average error rate of 2.5 meters. From the schematic of the test pictured in Figure 7 CAPTURE's visualizer module (Figure 8) graphically depicts the positions of the cooperative reference devices on screen along with the actual and estimated positions of the lost device. The positions of the 3 reference devices are entered into the visualizer manually, which can be seen in blue on the screen. The position of the lost device is also entered, it is illustrated in red on the screen. The application then reads in the RSSI values before estimating the position of the lost device, shown in green here on the screen.

Figure 7: Finding Lost Phone

Figure 8: Visualizer module

VI.

CAPTURE ­ SYSTEM IMPLEMENTATION

[5]

In order for CAPTURE to be able to cooperatively locate a lost device within a network, there must be at least 3 reference devices within sight of the lost device. Each of these must have `a prior' knowledge of their location within a preexisting localization solution. The hypothesis of CAPTURE was to extend the range of in-house IPS's, and tests shown in both have proven that it can achieve exactly this. Existing IPS's have dramatically more powerful infrastructure than what CAPTURE would utilize though. For example 230 volt AC powered Access Points in a standard IPS versus 12 volt DC powered mobile reference devices (smart phones, tablets and\or laptops) in a cooperative solution. It would be naive to think that accuracy levels of an in-house IPS would also `extend' to a cooperative model, although this does not take away from the solution to the range issue that CAPTURE provides. The implementation of a more comprehensive filter would nonetheless assist with accuracy for example the Kalman or Extended Kalman Filters are recommended in the following literature [7, 21]. VII. CONCLUSION This paper introduces CAPTURE a cooperative localization system using Bluetooth, that provides a solution to the problem of devices being out of range of a hosted Indoor Positioning System. Although the earlier implementation of CAPTURE using 802.11 provided more accurate results, experiments with the Bluetooth version of CAPTURE still demonstrate that utilizing a cooperative framework of mobile devices can extend the range of an in situ Indoor Positioning System by at least the range of the outermost devices located within the system. While CAPTURE using 802.11 [1] provides a more accurate solution, CAPTURE Bluetooth can actively transmit and receive beacons while still connected to the Wi-Fi network, something the 802.11 version cannot currently achieve. Disconnecting a user from a network to allow them to assist in the localization of another device is not something that would lead to large scale adoption of a solution. Wi-Fi Direct proposes to solve the issue of peer-to-peer communication during network connectivity. The implementation of a Wi-Fi direct version of CAPTURE is something that the next iteration of CAPTURE would hope to include. REFERENCES
[1] G. Cullen, K. Curran, and J. Santos, "CAPTURE - Cooperatively Applied Positioning Techniques Utilizing Range Extensions," in 5th International Conference on Indoor Positioning and Indoor Navigation (IPIN 2014), IEEE, Busan, Korea, 2014, pp. 22-29. M. Aly and J. Y. Bouguet, "Street view goes indoors: Automatic pose estimation from uncalibrated unordered spherical panoramas," in Applications of Computer Vision (WACV), 2012 IEEE Workshop on, 2012, pp. 1-8. G. Cullen, K. Curran, and J. Santos, "Cooperatively extending the range of Indoor Localisation," in Signals and Systems Conference (ISSC 2013), 24th IET Irish, 2013, pp. 1-8. G. M. Djuknic and R. E. Richton, "Geolocation and assisted GPS," Computer, vol. 34, pp. 123-125, 2001.

[6]

[7]

[8]

[9] [10]

[11] [12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[2]

[20]

[3] [4]

[21]

C. J. Matz, D. M. Stieb, K. Davis, M. Egyed, A. Rose, B. Chou , et al., "Effects of Age, Season, Gender and Urban-Rural Status on Time-Activity: Canadian Human Activity Pattern Survey 2 (CHAPS 2)," International journal of environmental research and public health, vol. 11, pp. 2108-2124, 2014. K. Kloch, G. Pirkl, P. Lukowicz, and C. Fischer, "Emergent behaviour in collaborative indoor localisation: An example of selforganisation in ubiquitous sensing systems," in Architecture of Computing Systems-ARCS 2011, ed: Springer, 2011, pp. 207-218. A. Baniukevic, D. Sabonis, C. S. Jensen, and L. Hua, "Improving Wi-Fi Based Indoor Positioning Using Bluetooth Add-Ons," in Mobile Data Management (MDM), 2011 12th IEEE International Conference on, 2011, pp. 246-255. S. Aparicio, J. Perez, A. M. Bernardos, and J. R. Casar, "A fusion method based on bluetooth and WLAN technologies for indoor location," in Multisensor Fusion and Integration for Intelligent Systems, 2008. MFI 2008. IEEE International Conference on, 2008, pp. 487-491. F. J. Gonzalez-Castano and J. Garcia-Reinoso, "Bluetooth location networks," in Global Telecommunications Conference, 2002. GLOBECOM '02. IEEE, 2002, pp. 233-237 vol.1. C. Liang, H. Kuusniemi, C. Yuwei, P. Ling, T. Kroger, and C. Ruizhi, "Information filter with speed detection for indoor Bluetooth positioning," in Localization and GNSS (ICL-GNSS), 2011 International Conference on, 2011, pp. 47-52. Z. Sheng and J. K. Pollard, "Position measurement using Bluetooth," Consumer Electronics, IEEE Transactions on, vol. 52, pp. 555-558, 2006. A. E. Kosba, A. Saeed, and M. Youssef, "Robust WLAN Devicefree Passive motion detection," in Wireless Communications and Networking Conference (WCNC), 2012 IEEE, 2012, pp. 32843289. D. Gualda, J. Urena, J. C. Garcia, E. Garcia, and D. Ruiz, "RSSI distance estimation based on Genetic Programming," in Indoor Positioning and Indoor Navigation (IPIN), 2013 International Conference on, 2013, pp. 1-8. M. O. Gani, C. Obrien, S. I. Ahamed, and R. O. Smith, "RSSI Based Indoor Localization for Smartphone Using Fixed and Mobile Wireless Node," in Computer Software and Applications Conference (COMPSAC), 2013 IEEE 37th Annual, 2013, pp. 110117. S. Shioda and K. Shimamura, "Anchor-free localization: Estimation of relative locations of sensors," in Personal Indoor and Mobile Radio Communications (PIMRC), 2013 IEEE 24th International Symposium on, 2013, pp. 2087-2092. L. Erin-Ee-Lin and C. Wan-Young, "Enhanced RSSI-Based RealTime User Location Tracking System for Indoor and Outdoor Environments," in Convergence Information Technology, 2007. International Conference on, 2007, pp. 1213-1218. J. Joonyoung, K. Dongoh, and B. Changseok, "Automatic WBAN area recognition using P2P signal strength in office environment," in Advanced Communication Technology (ICACT), 2014 16th International Conference on, 2014, pp. 282-285. F. D. Rosa, X. Li, J. Nurmi, M. Pelosi, C. Laoudias, and A. Terrezza, "Hand-grip and body-loss impact on RSS measurements for localization of mass market devices," in Localization and GNSS (ICL-GNSS), 2011 International Conference on, 2011, pp. 58-63. X. Lisheng, Y. Feifei, J. Yuqi, Z. Lei, F. Cong, and B. Nan, "Variation of Received Signal Strength in Wireless Sensor Network," in Advanced Computer Control (ICACC), 2011 3rd International Conference on, 2011, pp. 151-154. K. Kaemarungsi and P. Krishnamurthy, "Properties of indoor received signal strength for WLAN location fingerprinting," in Mobile and Ubiquitous Systems: Networking and Services, 2004. MOBIQUITOUS 2004. The First Annual International Conference on, 2004, pp. 14-23. S. S. Saad and Z. S. Nakad, "A Standalone RFID Indoor Positioning System Using Passive Tags," Industrial Electronics, IEEE Transactions on, vol. 58, pp. 1961-1970, 2011.

Exposing Ourselves: Displaying our Cultural Assets for Public Consumption
Gary Munnelly
Adapt Centre O'Reilly Building Trinity College Dublin, Ireland

Kevin Koidl
Adapt Centre O'Reilly Building Trinity College Dublin, Ireland

Séamus Lawless
Adapt Centre O'Reilly Building Trinity College Dublin, Ireland

munnellg@tcd.ie ABSTRACT

Kevin.Koidl@scss.tcd.ie

Seamus.Lawless@scss.tcd.ie

This paper discusses an early stage project to develop a new, enhanced interface for Trinity College Dublin (TCD) Digital Collections website. We describe the current state of the portal and outline some of the unique issues observed when examining user engagement. A major factor in our development of enhanced search tools will be to leverage the entities present in the documents to establish more reliable connections between items in the collection. Not only do we expect that this will lead to better ranked search results, but we also wish to investigate how these entities may be used to encourage site visitors to explore the site beyond their initial research goal. The early stage of this project means that plans are still being finalised. Hence we speculate about other methods which may be applied to this corpus.

Figure 1: Graph of most popular search terms on the Digital Collections site

Keywords
Entity Search; Digital Libraries; Information Retrieval

1.

INTRODUCTION

In many ways, the vision of Digital Humanities with regards to cultural heritage is a noble one. It is one in which all people have free, unbridled access to primary sources from which they may learn about their heritage and the rich history of their origins. We are free to lose ourselves in the depths of a historical archive from the comfort of our computer screens and supported in our exploration by a host of intelligent information retrieval systems. In theory, after the arduous process of digitising the collection, providing such functionality ought to be a simple task. Building and deploying a website has become a trivial process and off the shelf tools such as Solr provide state-of-theart text retrieval functionality with minimal effort. Given a suitable portal and a search box which returns ranked results, what more could a user want?

1 st International Workshop on Accessing Cultural Heritage at Scale (ACHS'16), 22 nd June 2016, Newark, NJ, USA. Copyright © 2016 for this paper by its authors. Copying permitted for private and academic purposes.

As it happens, this approach to curating documents has been found wanting in many ways. The most immediate problem with the query-response paradigm is that in order to be able to use the search interface we must know exactly what we are looking for and the manner in which it is represented in the collection. The search engine retrieves documents that it judges to be pertinent to our query and returns them to us without any explanation as to why these might be relevant, nor any encouragement to continue our investigation in a particular direction. It is up to the user to interpret the results, it is up to the user to establish relationships within the collection and it is up to us as the user to identify worthwhile avenues of future research [7]. Given that their knowledge of the collection is probably quite limited to begin with, this is hardly helpful. As was aptly put by Mitchell Whitelaw [8], these interfaces are not "generous". This need for a more generous interface is the focus of a project currently being undertaken by Trinity College Dublin (TCD) Digital Collections. At present the website provides the simple search box that we have come to expect which is driven by a default deployment of Solr. After conducting a search, users can narrow their interests along a broad series of facets: genre, media type, Trinity department, date and subject area. This interface results in a limited search experience, particularly with regards to exploration. The effects of this are demonstrable simply by looking at where the majority of traffic flows through the site (Figure 2). The most famous text on the Digital Collections portal is the Book of Kells [1]. A huge percentage of hits on the site can be attributed to this single page and variants of

Figure 2: Graph of pages which site visitors first land on. Note the DRIS ID for the Book of Kells is MS58 003v which ranks above the home page

the query string "Book of Kells" are consistently among the most frequent searches conducted. Indeed, it is worth noting that many visitors to the site land directly on the page for the Book of Kells having been referred there from Google, Facebook, Twitter etc. They never even see the initial search box on the homepage. After viewing the book, most users then simply browse away from the portal, not realising that they have barely touched the tip of the iceberg with regards to the volume of information and material available to them. Hence our goal is twofold; to provide a better, more accurate, more supportive search experience to users who come to explore the TCD Digital Collections site and to foster a sense of curiosity in those who come to see one artifact, but may have an interest in so many more.

name cannot be found in NAF, then ULAN is used instead. Although the entries in these ontologies are not explicitly referenced by the meta-data (i.e. there are no URIs used in the XML file), the names of various fields have been selected so that they may be related back to their ontological equivalents. For example, the field denoting the subject of a document is named subjectlcsh indicating that the data stored here is relevant to the LCSH ontology. While this is not ideal, it does mean that semantically linking the collection is possible and has be made easier by this method of annotating the data. In addition to these rigidly defined attribute fields, there are also a number of free text fields, abstract and description being the two most verbose. These free text fields contain additional information about the artifact, much of which is not actually described in the more semantic attributes. These are human readable sections which describe the artifact in moderate detail, giving an explanation of its origins, who commissioned it, where was it commissioned, how it came to be in the library or any other information which was available to the transcriber. Often these fields reference entities which are not mentioned in any of the other document attributes, meaning there is much information hidden in these fields which could be extracted and harnessed to power a more meaningful search experience.

3.

METHOD

2.

CORPUS

The corpus is comprised of approximately 100,000 high resolution scans of various documents curated by the Digital Collections group. These range from manuscripts to illustrations, etchings, postcards, templates, graphs, musical scores and more, spanning more than 1,000 years of human history. Information extraction techniques such as optical character recognition (OCR) have not been applied to the renderings, but each image has meta-data associated with it describing important attributes of the artifact. This data is listed in a single XML file which has been provided to us and is the foundation upon which we must build a new search interface. As is typical in collections of this type, many of the XML fields denote information such as page number, document ID, catalogue number etc. However, there has also been some effort made to make the collection semantically inclined, although not fully semantically linked. The names of several fields are designed to reflect the structure of four well established library cataloguing ontologies: The Library of Congress Name Authority File (NAF), The Library of Congress Subject Headings (LCSH), Getty Vocabularies Art and Architecture Thesaurus (AAT) and Getty Vocabularies Union List of Artist Names (ULAN). The choice of ontology for a particular field is dependent on the nature of the content it represents and the availability of information within  Zs ´ the ontologies themselves. For example, if an artist^ aA

Fostering engagement and encouraging exploration means discerning what interests a user and presenting them with content which relates to that interest. It may also mean determining what is of interest to a community of people at large and using this group perspective to assist an individual whose exploration has stalled. While we could use traditional language modelling or probabilistic methods to determine which documents may be discussing the same subject and then make recommendations based on that, it is much better if we can establish what real world, tangible objects are influencing the user's search and then trace these figures through the collection. In order to do this, we must know what entities are present in the corpus to begin with. We are fortunate that many potentially useful entities have been manually extracted and stored in the XML file for us. However, much information is also hidden in the free text fields spread throughout the meta-data. This presents some interesting opportunities to perform automatic information extraction and analysis on the collection. Named Entity Recognition (NER) is a well established field in Natural Language Processing (NLP) for locating references to known entities in a body of text [6]. In general we search for specific patterns, parts of speech or words which appear in a gazetteer of terms. Much like anything involving natural language and computers, the results can be noisy. However, after the results of NER have been sanitised, they may then be disambiguated to a suitable knowledge source [5, 2]. Within the Digital Collections corpus, identifying mentions of entities in the free text fields and disambiguating them to a common knowledge base will allow us to establish which documents are related to which entities and, by extension, which documents are related to each other. Disambiguation involves more than just co-referencing these entities within the collection. It links the collection's enti-

Figure 3: A screenshot of the current home page of the digital collections website ties to a higher knowledge base which may connect them by proxy to external knowledge sources such as Wikipedia. These external sources may assist the user in understanding the primary source material making the content more accessible for those who are inexperienced with the collection. The challenge is to determine which entity in the knowledge base is being referred to by the mention found in the text. While this focus on entities may be useful, it may also be of benefit to attempt to establish the larger context in which a user's search is taking place. While the corpus is large in size (the abstracts alone totalling almost 21,000,000 words) the vocabulary is highly constrained (a little over 10,000 unique terms) suggesting that topic modelling may also be a viable option for structuring the corpus and influencing search. Accurate topic modelling is difficult to achieve. Determining exactly how much content is required in order for a topic model to stabilise can be hard [4] and even after the model has stabilised there is no guarantee that the topics will be of use. Nevertheless, it may still be a worthwhile investigation to perform topic analysis such as Latent Dirichlet Allocation [3] on the collection to see if new, useful patterns beyond the broad facets already in use may be found. of data also plays a role in the accuracy of automatic methods. However with the data extracted from the collection, we have more information at our disposal for assisting and engaging with the user as they search the collection. Of course, even the best search interface can be felled by poor user interface design. This too will be a factor in the final development of the new Digital Collections portal.

5.

ACKNOWLEDGMENTS

This research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) in the ADAPT Centre (adaptcentre.ie) at Trinity College, Dublin.

6.

REFERENCES

4.

CONCLUSIONS

As can been seen, there are several options for what can be done when given a collection such as TCD's Digital Collections corpus. The quality with which we can automatically extract information and relationships from the collection are greatly dependent on the quality of the data itself. Quantity

[1] Book of Kells. http://digitalcollections.tcd.ie/home/ index.php?DRIS ID=MS58 003v. [Online; accessed 30-May-2016]. [2] A. Alhelbawy and R. J. Gaizauskas. Graph ranking for collective named entity disambiguation. [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993­1022, 2003.  ZCallaghan, ´ [4] D. Greene, D. O^ aA and P. Cunningham. How many topics? stability analysis for topic models. In Machine Learning and Knowledge Discovery in Databases, pages 498­513. Springer, 2014. [5] Z. Guo and D. Barbosa. Robust entity linking via random walks. In Proceedings of the 23rd ACM International Conference on Conference on Information

and Knowledge Management, pages 499­508. ACM, 2014. [6] D. Nadeau and S. Sekine. A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):3­26, 2007. [7] R. W. White and R. A. Roth. Exploratory search: Beyond the query-response paradigm. Synthesis Lectures on Information Concepts, Retrieval, and Services, 1(1):1­98, 2009. [8] M. Whitelaw. Generous interfaces for digital cultural collections. Digital Humanities Quarterly, 9(1), 2015.

A framework is developed that supports the theoretical design of an organizational memory information system (OMIS). The framework provides guidance for managing the processing capabilities of an organization by matching knowledge location, flexibility, and processing requirements with data architecture. This framework is tested using three different sets of data attributes and data architectures from 147 business professionals that have experience in IS development. We find that trade-offs exist between the amount of knowledge embedded in the data architecture and the flexibility of data architectures. This trade-off is contingent on the characteristics of the set of tasks that the data architecture is being designed to support. Further, the match is important to consider in the design of OMIS database architecture.A Project Spine for Software Engineering Curricular Design
Kevin Gary, Timothy Lindquist, Srividya Bansal, Arbi Ghazarian Arizona State University Mesa, AZ 85212 USA {kgary*, tim, srividya.bansal, arbi.ghazarian} @ asu.edu Abstract
Software engineering education is a technologically challenging, rapidly evolving discipline. Like all STEM educators, software engineering educators are bombarded with a constant stream of new tools and techniques (MOOCs! Active learning! Inverted classrooms!) while under national pressure to produce outstanding STEM graduates. Software engineering educators are also pressured on the discipline side; a constant evolution of technology coupled with a still emerging engineering discipline. As a handson engineering discipline, where engineers not only design but also construct the technology, guidance on the adoption of project-centric curricula is needed. This paper focuses on vertical integration of project experiences in undergraduate software engineering degree programs or course sequences. The Software Enterprise, now in its 9 th year, has grown from an upper-division course sequence to a vertical integration program feature. The Software Enterprise is presented as an implementation of a project spine curricular pattern, and a plan for maturing this model is given.

1. Introduction
Hands-on, or applied learning is expected to produce learners more engaged than those in traditional lecture oriented classes [1]. Significant efforts [2][3][4] in software engineering and computer science education focus on content taxonomies and bodies of knowledge. This is not a bad thing, but taken in isolation may lead educators to believe content coverage is more important than applied learning experiences. There is literature on project-based learning within computing as a means to learn soft skills and complex technical competencies. However, project experiences tend to be disjoint [5]; there may be a freshman project or a capstone project or a semester project assigned by an individual instructor. Yearlong capstone projects are offered at most institutions as a synthesis activity, but to steal a line from Agile methods, if synthesis is good, why not do it all the time ? Project experiences, while pervasive in computing programs, are not a central integrating feature. Sheppard et al. [6] suggests that engineering curricular design should move away from a linear, deductive model and move instead toward a networked model: "The ideal learning trajectory is a spiral, with all components revisited at increasing levels of sophistication and interconnection" ([6] p. 191). The general engineering degree program at Arizona State University (ASU) was designed from its inception in 2005 [7] to be a flexible, project-centric curriculum that embodied such integration (even before [6]). Likewise, the Software Enterprise was started at ASU in 2004 as an upper-division course sequence to integrate contextualized project experiences with software engineering fundamental concepts. The computing and engineering programs at ASU's Polytechnic campus merged under the Department of Engineering in 2008, and in 2009 the Arizona

Board of Regents (ABOR) approved a new Bachelor's degree in software engineering (BS SE). The first graduating class will be in Spring 2014 and ASU plans to undergo accreditation review shortly thereafter. At the course level the Software Enterprise defines a delivery structure integrating established learning techniques around a project-based contextualized learning experience. At the degree program level, the Enterprise weaves project experiences throughout the BS SE degree program, integrating program outcomes at each year of the major. There are several publications on the manner in which the Software Enterprise is conducted within a project course (for example, [8][9]]), and we summarize this in-course integration pedagogy in section 2. The intent of this work-in-progress paper is to describe extending the Enterprise as a spiral curricular design feature we refer to as the project spine , and present our plans moving forward to mature and validate this approach.

2. The Software Enterprise Delivery Model
The Software Enterprise is an innovative pedagogical model for accelerating a student's competencies from understanding to comprehension to applied knowledge by co-locating preparation , discussion , practice , reflection , and contextualized learning activities in time. In this model, learners prepare for a module by doing readings, tutorials, or research before a class meeting time. The class discusses the module's concepts, in a lecture or seminar-style setting. The students then practice with a tool or technique that reinforces the concepts in the next class meeting. At this point students reflect to internalize the concepts and elicit student expectations, or hypotheses , for the utility of the concept. Then, students apply the concept in the context of a team-oriented, scalable project, and finally reflect again to (in)validate their earlier hypotheses. These activities take place in a single three-week sprint , resulting is a highly iterative methodology for rapidly evolving student competencies (Fig 1).

Figure 1. Software Enterprise Delivery (left) Kolb Learning Cycle (right) The Software Enterprise represents an innovation derived from existing scholarship in that it assembles best practices such as preparation, reflection, practice (labs), and project-centered learning in a rapid integration model that accelerates applied learning. Readers may recognize the similarity of the model (Figure 1 left) to the Learning Cycle [10] (Figure 1 right). Our work for the past 9 years in the Enterprise has focused on maturing the delivery process, creating new or packaging existing learning materials to fit the delivery model, and to explore ways to assess project-centered learning.

3. The Software Enterprise Project Spine
An innovation in the new BS in Software Engineering at ASU has been the vertical adoption of the Software Enterprise. Enterprise courses are now required from the sophomore to senior years. This innovation represents what [6] calls a professional spine , as the Enterprise serves as an integrator of learning outcomes for a given year in the major. We refer to our project-centered realization as a project spine , where foundational concepts are tied to project work throughout the undergraduate program . There is significant computing literature on projects (embedded, mobile, gaming, etc.) to achieve learning or retention outcomes. However, computing lacks a framework for integrating concepts in a project spine. The Enterprise is an implementation that moves students from basic comprehension to applied Figure 2. ASU Project Spine knowledge to critical analysis outcomes. In the BS SE at ASU, program outcomes are described at 4 levels: describe , apply , select , and internalize . Students must achieve level 3 ( select between alternatives) in at least 1 outcome and achieve level 2 ( apply ) in all others. The program outcomes for the BS SE include Design, Computing Practice, Critical Thinking, Professionalism, Perspective, Problem Solving, Communication, and Technical Competence . An example leveled outcome description for Perspective is given in Table 1. The Enterprise accelerates level 3 outcomes by providing contextualized integrated experiences fostering decision-making in the presence of soft (communication, teamwork, etc.) and hard (technical) outcomes. Table 1. Perspective learning outcome for the BS SE at ASU
Perspective. An understanding of the role and impact of engineering and computing technology in business, global, economic, environmental, and societal contexts. Level 1. Understands technological change and development have both positive & negative effects. Level 2. Identifies and evaluates the assumptions made by others in their description of the role and impact of engineering and computing on the world. Level 3. Selects from different scenarios for the future and appropriately adapts them to match current technical, social, economic and political concerns. Level 4 . Has formed a constructive model for the future of our society, and makes life and career decisions that are influenced by the model.

The project spine (center of Figure 2 in dark hexagons) integrates technical competencies by assigning projects inclusive of the technical material covered in the regular computing courses. So for example, junior projects (Software Enterprise III and IV) emphasize technical complexities in Networks, Distributed Computing, and Databases, while senior projects emphasize technical complexities in Web and Mobile computing. The technical "focus area" courses are chosen more based on faculty expertise and recruitment goals than software engineering outcomes; one can envision many different areas represented by upper division courses here. These do help address the concern that an accredited software engineering degree has an application area. A risk we have not yet addressed is if the technical area impacts the software engineering process, such as with a soon-to-be-introduced embedded systems focus area.

There are 2 additional aspects of integration to the project spine. As summarized in section 2, the Enterprise integrates software engineering concepts throughout the project experiences. Students in the sophomore year learn the Personal Software Process [11] as a means to build individual understanding of time management, defect management, and estimation skills. They then focus on Quality, including but not limited to testing. In the junior year Enterprise students focus on Design (human-centered and system design principles) followed by best practices in software construction, taken primarily from eXtreme Programming. In the senior year students focus on Requirements Engineering then Process and Project Management. The final aspect of integration is with soft-skill outcomes such as Communication , Teamwork , and Professionalism (see Table 1). Throughout the spine the project experiences are crafted to ensure variations on pedagogy to address these outcomes. For example, in the freshman year students receive explicit instruction in teamwork. In the senior year the emphasis is on formal documentation as a means of communication. In the junior year, students work on service learning projects of high social impact to address communication with sensitive populations in a humancentered context.

4. Literature Review and Next Steps
The newness of software engineering degree programs means there are few studies of program adoption. There are examples of program design and lessons learned [5][12][13], or reflections and recommendations on the software engineering education landscape [14][15][16][17][18]. These are worthwhile guides but do not offer examples on evaluation instruments for program adoption. The SE2004 report [4] contains a chapter on "Program Implementation and Assessment" which discusses a number of key factors in program adoption, but is geared toward accreditation and not evaluation instruments. A survey instrument is presented in [19] but is designed for comparison of a large number of programs and not for adoption challenges. Bagert & Chenoweth [22] survey graduate programs in software engineering but more as an aggregate counting exercise in knowledge areas than as an adoption evaluation approach. The 2009 Graduate Software Engineering project conducted a survey of graduate degree programs [20] and then produced a comparison report [21] of graduate programs to the GSwE2009 reference model, which includes data on program characteristics and in-depth profiles from 3 institutions. A recent study is Conry's [23] survey of accredited software engineering degree programs. Conry summarizes institutional, administrative, and curricular (knowledge area) aspects in describing the 19 accredited programs as of October 2009. Certainly program adoption measures from other engineering programs are also relevant, though software engineering programs are unique due to the forces discussed in section 1. Our next steps for the Enterprise-as-project-spine involve defining measures for adoption impact, and determining how this concept fits with established patterns for curricular maps in software engineering programs. We plan to use quantitative and qualitative instruments to evaluate adoption. Quantitative data, such as program size, institution type, faculty and student backgrounds, can be collected via available resources (departmental archives or online) and direct surveys. Qualitative data can be collected through survey instruments and interviews of all stakeholders (faculty participants, administrators, and advisors). Different instruments may be used at different times to evaluate "in-stream" attitudes versus post-adoption reflections. Defining and validating these instruments is a significant area of work going forward. The SEEK, SWEBOK, and CS2013 guides define taxonomies of knowledge in software engineering. Taxonomies are useful and the sign of an emerging discipline. We

have done initial mappings of Enterprise modules to SEEK and CS2013 content areas, and plan to elaborate on these mappings. Specifically, we intend to produce CS2013 course exemplars. Further, the SE2004 report includes a section on program curricular patterns, and we will propose new patterns based on the project spine concept, which we hope will be timely in light of the in-process revisions to the SE2004 [24].

References
[1] National Academy of Engineering. Educating the Engineer of 2020: Adapting Engineering Education to the New Century. The National Academies Press, Washington D.C., 2005. [2] Institute for Electrical and Electronic Engineers Computer Society, Guide to the Software Engineering Body of Knowledge (SWEBOK). Los Alamitos, CA, 2004. [3] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society Joint Task Force (2012). Computer Science Curricula 2013 (Ironman draft). Available at http://ai.stanford.edu/users/sahami/CS2013, last accessed January 9, 2013. [4] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society. Software Engineering 2004 Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering. Joint Task Force on Computing Curricula, 2004. [5] Shepard, T. "An Efficient Set of Software Degree Programs for One Domain." In Proceedings of the International Conference on Software Engineering (ICSE) 2001. [6] Sheppard, S.D., Macatangay, K., Colby, A., and Sullivan. W.M. Educating Engineers: Designing for the Future of the Field, Jossey-Bass, San Francisco, 2008. [7] Morrell, D., Roberts, C., Grondin, B. Kuo, C.Y., Hinks, R., Danielson, S., and Henderson, M. "A Flexible Curriculum for a Multi-disciplinary Undergraduate Engineering Degree." Proceedings of the Frontiers in Education Conference 2005. [8] Gary, K. "The Software Enterprise: Practicing Best Practices in Software Engineering Education", The International Journal of Engineering Education Special Issue on Trends in Software Engineering Education, Volume 24, Number 4, July 2008, pp. 705-716. [9] Gary, K., "The Software Enterprise: Preparing Industry-ready Software Engineers" Software Engineering: Effective Teaching and Learning Approaches, Ellis, H., Demurjian, S., and Naveda, J.F., (eds.), Idea Group Publishing. October 2008. [10] Kolb Experiential Learning: Experience as the Source of Learning and Development, Prentice-Hall, N.J. 1984. [11] Humphrey, W.S. Introduction to the Personal Software Process , Addison-Wesley, Boston, 1997. [12] Lutz, M. and Naveda, J.F. "The Road Less Traveled: A Baccalaureate Degree in Software Engineering." Proceedings of the ACM Conference Special Interest Group on Computer Science Education (SIGCSE), 1997. [13] Frezza, S.T., Tang, M., and Brinkman, B.J. Creating an Accreditable Software Engineering Bachelor's Program. IEEE Software November/December 2006. [14] Hilburn, T.B., Hislop, G., Bagert, D.J., Lutz, M., Mengel, S. and McCracken, M. "Guidance for the development of software engineering education programs." The Journal of Systems and Software, 49(1999):163-169. 1999. [15] Ghezzi, C. and Mandrioli. "The Challenges of Software Engineering Education." In Proceedings of the International Conference on Software Engineering (ICSE) 2006. [16] Lethbridge, T., Diaz-Herrera, J., LeBlanc, R.J., and Thompson, J.B. "Improving software practice through education: Challenges and future trends." Proceedings of the Future of Software Engineering Conference, 2007. [17] Shaw, M. (2000). Software Engineering Education: A Roadmap. Proceedings of the 2007 Conference on the Future of Software Engineering, Limerick Ireland, 2000. [18] Mead, N. (2009). Software Engineering Education: How far We've Come and How far We Have to Go. Proceedings of the 21st Conference on Software Engineering Education and Training (CSEET 2009). 2009. [19] Modesitt, K., Bagert, D.J., and Werth, L. "Academic Software Engineering: What is it and What Could it be? Results of the First International Survey for SE Programs." Proceedings of the International Conference on Software Engineering (ICSE) 2001. [20] Pyster, A., Turner, R., Devanandham, H., Lasfer, K., Bernstein, L. "The Current State of Software Engineering Masters Degree Programs", The Conference on Software Engineering Education & Training (CSEET), 2008. [21] Frailey, D., Ardis, M., Hutchison, N. (eds). Comparisons of GSwE2009 to Current Master's Programs in Software Engineering, v1.0. Available at http://gswe2009.org, last accessed January 24, 2013. [22] Bagert, D.J. & Chenoweth, S.V. "Future Growth of Software Engineering Baccalaureate Programs in the United States", Proceedings of the American Society for Engineering Education Conference. Portland, OR, 2005. [23] Conry, S. Software Engineering: Where do curricula stand today? Proceedings of the National Conference of the American Society for Engineering Education, Louisville, KY, 2010. [24] Hislop, G., Ardis, M., Budgen, D., Sebern, M.J., Offut, J., & Visser, W. (2013). "Revision of the SE2004 Curriculum Model." Panel at the ACM Conference of the Special Interest Group on Computer Science Education (SIGCSE), Denver, CO, 2013.

The Software Enterprise is a pedagogical model combining traditional lecture with project-based learning. The Enterprise model leads students through a modular series of lessons that combine foundational concepts with skills-based competencies. In this tutorial, software engineering educators in higher education or industry will learn the methodology, get exposed to assessment techniques such as e-portfolios and concept maps, and become familiarized with the open resources available to educators that adopt the pedagogy. This tutorial should be of interest to any educator interested in making their project-based courses more engaging and more relevant to students needing to be ready to practice the profession from the first day they exit the Enterprise environment.The maritime oil tanker routing and scheduling problem is known to the literature since before 1950. In the presented problem, oil tankers transport crude oil from supply points to demand locations around the globe. The objective is to find ship routes, load sizes, as well as port arrival and departure times, in a way that minimizes transportation costs. We introduce a path flow model where paths are ship routes. Continuous variables distribute the cargo between the different routes. Multiple products are transported by a heterogeneous fleet of tankers. Pickup and delivery requirements are not paired to cargos beforehand and arbitrary split of amounts is allowed. Small realistic test instances can be solved with route pre-generation for this model. The results indicate possible simplifications and stimulate further research.n this paper we present a detailed analysis of the design and implementation of an educational game targeting young women entrepreneurs by a predominantly male team. During the process, we arrived at assumptions based on intrinsic and extrinsic influences that effected the design of the game. After creating a prototype, the game was provided to the target audience during a usability test. Our observations reveal that even after following a rigorous and agile development model that included stakeholders at several time frames, we were not successful in delivering the desired experience to our target audience. We conclude by presenting a strategy for changing the agile development model to be inclusive of the target audience.An Empirical Study of Cycle Toggling Based Laplacian Solvers
Kevin Deweese UCSB kdeweese@cs.ucsb.edu Richard Peng Georgia Tech rpeng@cc.gatech.edu John R. Gilbert UCSB gilbert@cs.ucsb.edu Hao Ran Xu MIT haoranxu510@gmail.com September 13, 2016 Gary Miller CMU glmiller@cs.cmu.edu Shen Chen Xu CMU shenchex@cs.cmu.edu

arXiv:1609.02957v1 [cs.DS] 9 Sep 2016

Abstract We study the performance of linear solvers for graph Laplacians based on the combinatorial cycle adjustment methodology proposed by [KelnerOrecchia-Sidford-Zhu STOC-13]. The approach finds a dual flow solution to this linear system through a sequence of flow adjustments along cycles. We study both data structure oriented and recursive methods for handling these adjustments. The primary difficulty faced by this approach, updating and querying long cycles, motivated us to study an important special case: instances where all cycles are formed by fundamental cycles on a length n path. Our methods demonstrate significant speedups over previous implementations, and are competitive with standard numerical routines.

Figure 1: Performance profile of cycle-toggle time. The relative performance ratio of a method is its cycle-toggle time / best cycle toggle time for a single problem. This plot shows the fraction of problems that are within a distance from this relative performance ratio. The faster 1 Introduction a method converges to 1 on this plot, the better its Much progress has been made recently toward the performance relative to the others.

development of graph Laplacian linear solvers that run in linear times polylogarithmic time [16, 17, 18, 21, 24, 9, 15]. These methods use a combination of combinatorial, randomized, and numerical methods to obtain algorithms that provably solve any graph Laplacian linear system in time faster than sorting to constant precision.
 Partially supported by NSF Grants CCF-1637523, CCF1637564, and CCF-1637566 titled: AitF: Collaborative Research: High Performance Linear System Solvers with Focus on Graph Laplacians  Partially supported by Intel Corporation

Linear solvers for graph Laplacians have a wide range of applications. They can be used to solve problems such as image denoising, finding maximum flows in a graph, and more generally solving linear programs with an underlying graph, such as, minimum cost maximum flow and graph theoretic regression problems [5, 30, 8, 11, 20, 23, 22, 7, 19]. Many of these applications stem from the following connection through optimization: solving linear systems is equivalent to minimizing 2 norms over a suitable set. Many applications can in turn be

2 1 4 3

5

(a) Original Graph

nation, or partial Cholesky factorization steps from the ultra-sparsification routines [29]. Recursively dividing the cycle set yields a recurrence of the form: T (N ) = O(N ) + 2T (N/2),
2

1 4

3

5

(b) Subgraph 1,4

(c) Subgraph 2,3,5

2 1 3

4

5

(d) Contraction of(b)

(e) Contraction of(c)

Figure 3: Illustration of graph reduction and contraction in divide-and-conquer. 5 cycles are preselected in the original graph(a) and divided into two groups, cycles (1,4) and (2,3,5). These cycles induce subgraphs (b,c) which only include edges and vertices of the relevant cycles. These subgraphs are then path contracted (d,e) to further reduce size.

which solves to T (N ) = O(N log N ). If we set the size of our preselected cycle set to O(n), then updating the entire set takes O(n log n) work, leading to a cost of O(log n) per update. Unfortunately, the divide-and-conquer scheme does not parallelize naturally: the second recursive call still depends on the outcome of the first one. Furthermore, the bottleneck of this routine's performance is the restriction and prolongation steps, which unlike multigrid can not be reused when we resample another set. A large part of the expense is that vertices and edges must be relabeled as the graph is reduced. Doing this in random order leads to random access of vertex and edge labels. We try to optimize this by either compressing the memory of the graph storage, or by reordering the updates within each batch. In the case that the tree is just a path, much of the vertex and edge labeling can be done implicitly, reducing the overhead. 4 Heavy Path Graphs

can further reduce the size of the graph by path contraction, condensing two edges if they are only updated when the other is updated. An example of this reduction and contraction is shown in Figure 3. This process results in several smaller graphs, where the cycles are updated, before pushing the cycle update information back up the recursive subgraph hierarchy. As this process resembles the recursive subgraph hierarchy of multigrid methods, we borrow the terms restriction and prolongation to describe the transfer of flow information up and down the hierarchy. This process is more formally captured in the following lemma.

Here we introduce a class of model problems that we will use to test and analyze different cycletoggling approaches. These graphs are constructed by adding edges between vertices on a path graph. Edge resistances are selected so that the low-stretch spanning tree of the resulting graph is always the underlying path. As a consequence the edges on the path have larger edge weights than the off-path edges, so we refer to this class of graphs as heavy path graphs. An example of such a graph is shown in Figure 4.

Lemma 3.1. Given a tree on n vertices, and N cycle updates, we can form a tree on 3N vertices, perform the corresponding cycle updates on them, Figure 4: An example of a heavy path graph. The solid path edges are the low-stretch spanning tree of and transfer the state back to the original graph. the graph. Furthermore, both the reduction and prolongation Our interest in these problems does not come steps take O(n) time. from any real world application. Instead we believe This procedure is identical to the greedy elimi- these are natural models to consider when studying

KOSZ and other cycle-toggling algorithms. We believe that this model can be tuned to have various stretch properties along with spectral and graph separator properties, though we do not explore that in this paper. Furthermore they allow us to explore very fundamental questions about data structures and cycle-toggling implementations. This model simplifies many of the implementation issues associated with dynamic trees, as the paths are easier to handle than more general tree layouts. Specifically, we can use a static, perfectly balanced binary tree for the path. This likely has the least data structure overhead as the optimum separator of an interval is implicitly the middle. Furthermore, this allows us to store the tree in heap order, which means the tree paths can be mapped to a subinterval using bit operations, and the downward/upward propagations can be performed iteratively. 4.1 Example Models There are many possible subclasses that belong to the heavy path graph model. We introduce several subclasses here for experimentation. (1) Fixed Cycle Length-1k: These graphs are composed of a tree path with random resistances between 1 and 10,000, combined with off-tree edges between every pair (i, i + 1000), e.g. an edge between vertices 1 and 1000, between vertices 2 and 1001, and so on. (2) Fixed Cycle Length-2: These graphs are composed of a tree path with random resistances between 1 and 10,000, combined with off-tree edges between every pair (i, i + 2), e.g. an edge between vertices 1 and 3, between vertices 2 and 4, and so on. (3) Random Cycle Length: These graphs are composed of a tree path with random resistances between 1 and 1000, combined with n randomly selected off-tree edges, where n is the number of vertices. (4) 2D Mesh: These graphs embed a tree path in a 2D mesh. The tree path resistances are chosen randomly between 1 and 1000. (5) 3D Mesh, Uniform Stretch: These graphs are similar to (4) but with a 3D mesh.

We then consider two different ways of setting resistances on the off-tree edges on all of the models above. 1. Uniform Stretch Resistances of off-tree edges are chosen so that stretch is 1 for every cycle. 2. Exponential Stretch Resistances of off-tree edges are chosen so that cycles have stretch sampled from an exponential distribution. 5 Experiments

5.1 Experimental Design We now describe empirical evaluations of the cycle-toggling implementations from Section 3 on the class of graphs described in Section 4. As we only experiment on these path models, we can use cycle-toggling methods that will only work on a path, but we also employ their more general versions that will work on any graph. The four cycle-toggling implementations are as follows: 1. BST-based data structure for general graphs 2. Path-only BST decomposition 3. Recursive divide-and-conquer for general graphs 4. Path-only recursive divide-and-conquer Additionally we implement a preconditioned conjugate gradient with diagonal scaling to compare against the cycle-toggling methods. We implemented all of these in C++ and also have a Python/Cython implementation of the general recursive method. All algorithm implementations, graph generators, and test results for this paper can be found at https://github.com/sxu/cycleToggling. We also experimented with Hoske et al.'s [14] implementation of cycle-toggling. We use all of the generators described in Section 4.1 to create different heavy path graphs with a varying total stretch. We use vertex sizes of 5 × 104 , 105 , 5 × 105 , and 106 . For the fixed cycle length generators, we set hop = 1000, and for the random cycle length generators, we set the number of offtree edges to 2n. To get an idea for the various stretch properties of these graphs, we list the total stretch for size 106 in Table 1. We also generate right hand side vectors b in two different ways to obtain both local and global

Cycles log -1

Fixed Length-1k Fixed Length-2 Random Length 2D Mesh 3D Mesh

Uniform 1.01e6 2.00e6 2.00e6 2.00e6 3.82e6

Exponential 1.12e6 1.04e7 1.30e7 1.08e7 2.27e7

108 107 106 105 104 103 3 10

Table 1: Total stretch for all graph models of size 106 . For each of the model problems in 4.1, this table shows the total stretch of cycles formed by adding edges to the underlying path. The models were generated with weights to create cycles with uniform stretch (all cycles with stretch 1), and exponential stretch(cycles with stretch chosen from an exponential distribution).

104

Total Stretch

105

106

107

108

behaviors. 1. Random: Randomly select x and form b = Lx ,

Figure 5: KOSZ asymptotic dependence on tree stretch. The number of toggles required by KOSZ is shown as a function of tree stretch. The reasonable slope indicates a lack of large hidden constants in KOSZ complexity.

and every graph, the relative performance ratio 2. (-1,1): Pick b to route 1 unit of electrical flow is the method's average cycle-toggle time divided from the left endpoint of the path to the right by the lowest average cycle-toggle time over all endpoint. methods. Then to capture how a method fares Experiments were performed on Mirasol, a across the entire problem set, the performance shared memory machine at Georgia Tech, with 80 profile shows the fraction of test problems (on the Intel(R) Xeon(R) E7-8870 processors at 2.40GHz. y-axis) that are within a distance (on the x-axis) Problems were solved to a residual tolerance of from the relative performance ratio. This plot contains all the different model problems at every 10-5 . problem size tested. Weak scaling experiments, measuring cycle5.2 Experimental Results We first examine the asymptotic behavior of the cycle-toggling meth- toggle performance as graph size increases, are useods on all the test graphs. Figure 5 shows the num- ful for predicting performance on larger problems. ber of cycles required for convergence as a function The scaling behavior was relatively similar across of total stretch. This figure only involves solves the model problems so we only show one example using the 0-1 right hand side as this was always a in Figure 6 for the 3D Unweighted Mesh with exponential stretch. more difficult case. We examine how much time the recursive We omit results from the Hoske et al. implementation because we found its performance to be method spends restricting and prolonging flow in slower by a factor of 50 than our cycle-toggling im- the recursive hierarchy, and how much time is plementations. Their initialization costs are much spent doing cycle-toggles in Figure 7. Results are higher than solve costs, making it prohibitively ex- shown for the FixedLength-1k model with a slightly pensive to run on all of the test graphs in our set. wider range of problem size than the other experTo visualize the comparison of cycle-toggling iments. The solve time in this plot includes the implementations on all the different test graphs, we sum of the other operation timings, along with utilize a performance profile plot shown in Figure 1. memory allocation. We did this profiling with A performance profile [12] calculates, for some our Python/Cython implementation, but we beperformance metric, the relative performance ratio lieve the C++ performance is comparable. Figure 8 shows BST-based cycle-toggle timing between each solver and the best solver on every problem instance. In our case the metric of interest results relative to PCG results. Points below the is the average cycle-toggle time, so for each method line indicate cycle-toggling was faster, while points

Path-only BST Decomposition BST-Based

Path-only Recursive Recursive

Restrict Prolong

Update Solve

10-5

10-5

Average Toggle Time(s)

Average Toggle Time(s)

10-6

10-6

10-7

10-7

10-8 104

Path Length

105

106

103

104

Path Length

105

106

107

Figure 6: Weak scaling of cycle-toggle performance of all methods on unweighted 3D mesh model problems with exponential stretch. Average cycle-toggle time is shown as a function of problem size where an upward slope indicates decreased performance with larger problem size.

Figure 7: Weak scaling of cycle-toggle performance for the recursive solver on FixedLength-1k model problems. Average cycle-toggle time is shown along with its most expensive sub-components: restriction, solve, and prolongation. Upward slopes indicate decreasing performance with problem size.
Uniform Stretch FixedLength-2 2D Mesh Random Exp Stretch FixedLength-1k 3D Mesh

Toggle Time(s)

above the line are slower. This plot only includes size 106 problems using the 0-1 right hand side. A random right hand side plot is omitted for space as these problems were much easier for both solvers, though slightly relatively easier for PCG. 5.3 Experimental Analysis In Figure 5 the cycle-toggling methods' asymptotic dependence on tree stretch is near constant with a slope close to 1. Note that this plot would be linear even without the log axes. Concerning KOSZ practicality, it is highly important to see that there is not a large slope, which would indicate a large hidden constant in the KOSZ cost complexity. This plot tells us that with a combination of low-stretch trees and fast cycle update methods, dual space algorithms have potential. This figure also helps illustrate the range of problems we are using for these experiments. The stretch and resulting cycle cost both vary between four to five orders of magnitude. The performance profile in Figure 1 indicates that the data structure based cycle-toggling methods performed the best using our implementations. For the path-only BST decomposition, the fraction of problems is already at 1 for a relative performance distance of 1, meaning that this was always the fastest. The path-only recursive method was

105 104 103 102 101 100 0 10

101

102

PCG Time(s)

103

104

105

Figure 8: Comparison of BST-based data structure cycle-toggling to PCG by graph type. Points under the line indicate cycle-toggling method outperformed PCG.

slower, but still typically performed better than the general implementations, being half as fast as the path-only BST method on 60% of the problems. Comparing the two general implementations, the tree data structure is within a factor 4 of the best on 80% of the problems, whereas the recursive method is only within a factor of 4 on 40% of the problems. A distance of 10 indicates performance within the same order of magnitude, which the general recursive method achieved on 80% of

the problems, indicating that these methods are competitive with one another. The weak scaling experiments shown in Figure 6 do indicate a decrease in cycle-toggle performance as graph size increases. However, this plot is fairly optimistic, the largest performance decrease is about 2.5× as the graph size increases two orders of magnitude. The non steady plot for the general recursive solver probably indicates that the batch sizes were not scaled appropriately. Again, this plot is only for one of the graph models, but most of them looked very similar to this. Figure 7 helps identify the performance bottlenecks of the recursive method. The actual time spent updating cycles is less than the restriction and prolongation time. The restriction time is by far the most expensive, as it also includes time for relabeling edges and vertices. The scaling of this plot shows a stable update cost, with increasing restriction and prolongation costs. This method was designed to keep the update costs stable while increasing problem size, which seems to be case. Unfortunately the restriction and prolongation overhead costs are large and growing with problem size. Still, these operations are not highly optimized, and we wonder if we can borrow techniques from the multigrid community to speed them up. The PCG experiments in Figure 8 indicate that cycle-toggling can outperform PCG on these heavy path models, using the 0-1 right hand side. This class of problems had a wider performance gap for PCG than for the cycle-toggling routines, by about an order of magnitude. Furthermore, the graph property that causes difficulty for the solvers is different in each case; cycle-toggling has trouble on the graphs with exponential stretch, while PCG has difficulty with the fixed cycle length problems (FixedLength-2 with uniform stretch even failed). These results suggest that heavy path graphs are a good direction to explore while searching for problems which could benefit from cycle-toggling methods. 6 Discussion and Conclusion

model, we experimented on problems that are are conceptually simple, but provide a range of solve behavior through varying graph structure and stretch. The recursive cycle-toggling was not as fast as the data structure approach, but was still competitive, being in the same order of magnitude on most problems. method to general graphs, exhibited competitive behaviors. Also both methods scaled reasonably with problem size. While these experiments are a good start, there are several directions we hope to continue this work. The recursive update approach is outperformed by the BST-based data structure approach in timing experiments. We hope to complement these results with floating point operation measurements. We don't claim to have optimized the graph contraction, flow restriction/prolongation, or cycle updates. Measuring the number of operations the recursive solver spends on these would help indicate fundamental performance. The heavy path graphs are a great model problem for seeing the effect path resistances have on solver behavior. They also allow us set aside the issue of finding a low stretch spanning tree to focus instead on the cost per cycle update. We plan to continue modifying these path resistances and initial vertex demands to find interesting test cases. However, for these methods to be useful in practice we must extend them to more general classes of graphs. Dual cycle-toggling Laplacian solvers have until now been considered mainly in the realm of theory. Our comparisons of these methods to PCG indicate that there are problems for which the dual methods can be useful. In the future, we plan to combine primal and dual methods, trying to get the best of both worlds. References

We studied two approaches for implementing cycletoggling based solvers, data structures and recursive divide-and-conquer. Using the heavy path

[1] O. Axelsson, Iterative solution methods, Cambridge University Press, New York, NY, 1994. [2] M. A. Bender, E. D. Demaine, and M. FarachColton, Cache-oblivious B-trees, IEEE FOCS, Redondo Beach, CA, 2000, pp. 399­409. [3] E. G. Boman, K. Deweese, and J. R. Gilbert, Evaluating the dual randomized Kaczmarz Laplacian

[4]

[5]

[6] [7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

linear solver, Informatica, 40(1) (2016), pp. 95­ 107. E. G. Boman, K. Deweese, and J. R. Gilbert, An empirical comparison of graph Laplacian solvers, SIAM ALENEX, Arlington, VA, 2016, pp. 174­ 188. E. G. Boman, B. Hendrickson, and S. Vavasis, Solving elliptic finite element systems in nearlinear time with support preconditioners, SIAM J. on Numerical Anal., 46(6) (2008), pp. 3264­3284. W. L. Briggs, V. E. Henson, and S. F. McCormick, A multigrid tutorial, SIAM, 2000. M. B. Cohen, B. T. Fasy, G. L. Miller, A. Nayyeri, R. Peng, and N. Walkington, Solving 1-Laplacians of convex simplicial complexes in nearly linear time: collapsing and expanding a topological ball, SIAM SODA, Portland, OR, 2014, pp. 204­216. P. Christiano, J. A. Kelner, A. Madry, D. A. Spielman, and S.- H. Teng, Electrical flows, Laplacian systems, and faster approximation of maximum flow in undirected graphs, ACM STOC, San Jose, CA, 2011, pp. 273­282. M. B. Cohen, R. Kyng, G. L. Miller, J. W. Pachocki, R. Peng, A. Rao, and S. C. Xu, Solving SDD linear systems in nearly mlog1/2 n time, ACM STOC, San Jose, CA, 2011, pp. 343­352. T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to algorithms, MIT Press and McGraw-Hill, 2009. H. H. Chen, A. Madry, G. L. Miller, and R. Peng, Runtime guarantees for regression problems, ITCS, Berkeley, CA, 2013, pp. 269­282. E. D. Dolan and J. J. Mor´ e, Benchmarking optimization software with performance profiles, Mathematical Programming, 91(2) (2002), pp. 201­213. P. G. Doyle and J. L. Snell, Random walks and electric networks, Mathematical Association of America, 1984. D. Hoske, D. Lukarski, H. Meyerhenke, and M. Wegner, Is nearly-linear time the same in theory and practice? A case study with a combinatorial Laplacian solver, SEA, Paris, FRA, 2015, pp. 205­ 218. R. Kyng, Y. T. Lee, R. Peng, S. Sachdeva, and D. A. Spielman, Sparsified Cholesky and multigrid solvers for connection Laplacians, Computing Research Repository, 2015, http://arxiv.org/abs/1512.01892. I. Koutis, G. L. Miller, and R. Peng, Approaching optimality for solving SDD systems, SIAM J. on Comp., 43(3) (2014), pp. 337­354. I. Koutis, G. L. Miller, and R. Peng A Nearly-m

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26] [27]

[28]

[29]

[30]

log n time solver for SDD linear systems, IEEE FOCS, Palm Springs, CA, 2011, pp. 590­598. J. A. Kelner, L. Orecchia, A. Sidford, and Z. A. Zhu, A simple, combinatorial algorithm for solving SDD systems in nearly-linear time, ACM STOC, Palo Alto, CA, 2013, pp. 911­920. R. Kyng, A. Rao, and S. Sachdeva, Fast, provable algorithms for isotonic regression in all p -norms, NIPS, Montreal, QC, 2015, pp. 2701­2709. Y. T. Lee, S. Rao, and N. Srivastava, A new approach to computing maximum flows using electrical flows, ACM STOC, Palo Alta, CA, 2013, pp. 755­764. Y. T. Lee and A. Sidford, Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems, IEEE FOCS, Berkeley, CA, 2013, pp. 147­156. Y. T. Lee and A. Sidford, Path finding methods for linear  programming: solving linear programs ~ ( rank ) iterations and faster algorithms for in O maximum Flow, IEEE FOCS, Philadelphia, PA, USA, 2014, pp. 424­433. A. Madry, Navigating central path with electrical flows: from flows to matchings, and back, IEEE FOCS, Berkeley, CA, 2013. pp 253­262. R. Peng and D. A. Spielman, An efficient parallel solver for SDD linear systems, ACM STOC, New York, NY, USA, 2014, pp. 333­342. M. Reid-Miller, G. L. Miller, and F. Modugno, List ranking and parallel tree contraction in J. H. Reif Synthesis of parallel algorithms, Morgan Kaufmann, San Francisco, CA, 1993, pp. 115­194. Y. Saad, Iterative methods for sparse linear systems, SIAM, 2003. D. D. Sleator and R. E. Tarjan, A data structure for dynamic trees, J. Comp. Syst. Sci., 26(3) (1983), pp. 362­391. D. A. Spielman and N. Srivastava, Graph sparsification by effective resistances, SIAM J. on Comp., 40(6) 2011, pp. 1913­1926. D. A. Spielman and S.- H. Teng, Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems, SIAM J. on Matrix Anal. and Appl., 35(3) 2014, pp. 835­885. D. Tolliver and G. L. Miller, Graph Partitioning by Spectral Rounding: Applications in Image Segmentation and Clustering, IEEE CVPR, New York, NY, 2006, pp. 1053­1060.

arXiv:1408.4351v1 [physics.comp-ph] 19 Aug 2014

A Parallel Multi-Domain Solution Methodology Applied to Nonlinear Thermal Transport Problems in Nuclear Fuel Pins$
Bobby Philipa,, Mark A. Berrilla , Srikanth Allua , Steven P. Hamiltona , Rahul S. Sampatha , Kevin T. Clarnoa , Gary A. Diltsb
Oak Ridge National Laboratory One Bethel Valley Road, Oak Ridge, TN 37831 b Los Alamos National Laboratory P.O. Box 1663, Los Alamos NM 87545
a

Abstract This paper describes an efficient and nonlinearly consistent parallel solution methodology for solving coupled nonlinear thermal transport problems that occur in nuclear reactor applications over hundreds of individual 3D physical subdomains. Efficiency is obtained by leveraging knowledge of the physical domains, the physics on individual domains, and the couplings between them for preconditioning within a Jacobian Free Newton Krylov method. Details of the computational infrastructure that enabled this work, namely the open source Advanced Multi-Physics (AMP) package developed by the authors is described. Details of verification and validation experiments, and parallel performance analysis in weak and strong scaling studies demonstrating the achieved efficiency of the algorithm are presented. Furthermore, numerical experiments demonstrate that the preconditioner developed is independent of the number of fuel subdomains in a fuel rod, which is particularly important when simulating different types of fuel rods. Fi$ Notice: This manuscript has been authored by UT-Battelle, LLC, under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes.  Corresponding author: Bobby Philip (Email: philipb@ornl.gov)

Preprint submitted to Journal of Computational Physics

August 20, 2014

nally, we demonstrate the power of the coupling methodology by considering problems with couplings between surface and volume physics and coupling of nonlinear thermal transport in fuel rods to an external radiation transport code. Keywords: Inexact Newton, Jacobian Free Newton Krylov, Krylov Subspace Method, Domain Decomposition, Preconditioning, Iterative Method, Parallel Algorithm 2010 MSC: 49M15, 65F08, 65F10, 65N55, 65Y05, 68W10 1. Introduction Many real world engineering problems involve multiple coupled nonlinear physical processes that occur both within and across several interacting physical domains. Robust, accurate, and efficient three dimensional simulations for some of these complex problems pose significant challenges that require a combination of powerful numerical algorithms, efficient parallel implementations, and massive computing resources to tackle. These challenges include developing the numerical methods and the parallel software infrastructure for coupling physical phenomena that occur on the surface and within the interior of physical domains, coupling structured and unstructured mesh calculations, coupling models with different discretizations, and using tightly coupled solution methods to solve certain coupled physics problems and loosely coupled approaches for others. Developing such simulation capabilities is a nontrivial task. In this article, we will focus primarily on one such complex application where all of the features outlined above are present: thermal transport in nuclear fuel rods. However, we will also devote some effort to describing the parallel code infrastructure that was developed to provide the necessary meshing, discretization, linear algebra, linear and nonlinear solvers, physics modules (conservation laws and constitutive models), material property databases, and parallelization mechanisms for simulating this application in hopes that it will be beneficial to the broader scientific community. A nuclear fuel assembly consists of several hundred nuclear fuel rods (shown in Figure 1) bound together by spacer grids. While some of the rod locations are reserved for instrumentation and safety, most of the rods contain nuclear fuel. Each individual nuclear fuel rod in turn consists of several hundred nearly cylindrical nuclear fuel pellets (each with a height 2

Figure 1: Fuel assembly containing nuclear fuel rods that are filled with fuel pellets [1] to diameter ratio of approximately one) stacked one on top of another to form a long column enclosed within a metal tube called the clad. Heat is generated within the pellets by nuclear fission and is distributed within the pellets and clad via a diffusive process. There is thermal contact (modeled as a convective process) between neighboring pellets and between the pellets and the clad. Each fuel rod is cooled with water flowing axially up the outer surface of the clad. Modeling the heat transfer, along with other physics, leads to a very highaspect ratio problem with many inter-dependent domains. Traditional nuclear fuel simulation eliminates the computational challenge by approximating the heat transfer as entirely radial and neglecting the axial and azimuthal components, which are only coupled through the coolant temperature and other simplified physics [2, 3]. Recent efforts to develop advanced modeling and simulation tools for nuclear fuel rods [4, 5], which include simulating full 3

three-dimensional fuel rods with resolved pellets, have relied upon standard solution and preconditioning strategies that do not necessarily take advantage of the physics and geometry of the problem. However, this manuscript does not address the challenges of structural dynamics and the associated feedback on heat transfer. With respect to specific work related to heat transfer within nuclear fuel rods, there are several existing efforts to develop parallel codes that model three-dimensional heat transfer within nuclear fuel rods, including PLEIADES/ALCYONE [4, 6], MOOSE/Bison [4], and BACO [7]. These codes are all focused on the integration of the many physics required for modeling nuclear fuel performance in steady-state and transients to improve the underlying material science, including fracture/contact mechanics, fission gas generation and release, and corrosion chemistry. We have developed an efficient scalable parallel simulation framework for solving such multi-domain, multi-physics problems and have used it to solve the specific nuclear fuel problem described above. Within our particular application a nonlinearly consistent Jacobian Free Newton Krylov (JFNK) method is used (though the ability to use alternative solution methods also exists) across all the domains for each fuel rod. Physics-based preconditioning is used to accelerate the solution process and the multi-domain (pellets and clad) aspect of the problem is leveraged in developing methods that minimize communication as well as avoid the formation of full matrices over the whole domain. In the next section, we present a mathematical description of the problem under consideration. Section 3 will describe the finite element discretization of the models in Section 2. The algorithms used to solve the resulting nonlinear system of equations are described in Section 4. The computational framework that was used in this work is briefly described in Section 5. Section 6 reports on numerical experiments performed to verify and validate our code and test its parallel scalability. Section 7 provides details on coupling to reduced order flow models, coupling to oxide growth models on the exterior clad, and parallel full assembly simulations that couple thermal transport components on unstructured meshes with a structured mesh radiation transport code. The paper ends with a few concluding remarks.

4

2. Model A 3D fuel rod domain, , is modeled as consisting of the union of N pellet 3 C 3 subdomains, P i  R , i = 1, 2, . . . , N , and a clad subdomain   R , i.e, P C the global domain  = N i=1 i   . The number and geometric complexity of fuel pellets in fuel rods can vary significantly; from simple cylinders to the complex pellet geometries shown later in this manuscript and from a few pellets in an experimental rod to more than 400 pellets in a commerical nuclear fuel rod. In our numerical experiments N will be varied between 1 and 360 though there is no fundamental limitation on the number of pellet P domains. Here, P 1 will denote the domain of the lowest pellet and N will denote the topmost pellet domain. The surfaces of the pellet and clad C domains will be denoted by P i , i = 1, 2, . . . , N , and  , respectively. From now on, where there is no danger of confusion the superscript `P' will be dropped for the pellet subdomains. We will assume that the subdomains are disjoint. Only adjacent pellet subdomains are assumed to touch, i.e., i  i-1  i,i-1 =  for i = 2, . . . N , and i  j =  for j = i + 1, i - 1. In this paper we will only consider the case where no pellets are in mechanical contact with the clad or flow subdomains. We will first describe the models at the level of individual pellets, clad, and coolant domains before considering the full coupled multi-domain model. 2.1. Pellet Models: The temperature field, Ti , in each fuel pellet domain i is modeled by a nonlinear thermal diffusion equation, -  · ki (Ti , x)Ti (x) = fi (x), for x  i , (2.1)

where ki is a scalar nonlinear thermal conductivity and fi  L2 (i ) is a nonzero thermal source due to heat generated from nuclear fission in each pellet. In stand-alone applications, the radial shape of fi is usually approximated with a local exponential in the radial dimension and a globally low-order (quadratic or cosine) model in the axial dimension. ki and fi vary spatially within each pellet and can potentially vary in functional form from pellet to pellet to account for differences in materials and fission processes that do occur in practice. For example, insulator pellets with natural, rather than enriched, uranium are often introduced at the top and bottom of the pellet stack to reduce axial power peaking. 5

There is contact/gap resistance between two adjacent pellets and for pellet i this is modeled by Robin boundary conditions of the form
m ki (Ti )Ti · ni + hi,i-1 (Ti , Tim -1 )(Ti - Ti-1 ) = 0 on i,i-1 , m ki (Ti )Ti · ni + hi,i+1 (Ti , Tim +1 )(Ti - Ti+1 ) = 0 on i,i+1 .

(2.2) (2.3)

Here ni is the outward facing unit normal on the surface of pellet i and Tim -1 is the surface temperature field for pellet (i - 1) on i-1  i,i-1 interpolated to the surface i  i,i-1 of pellet i. The flux between adjacent pellets is assumed to be continuous while the temperature field is assumed to be discontinuous. Heat transfer between the pellets and clad is by radiative, conductive and convective processes through the gap region. For each pellet this is modeled using a Robin boundary condition ki (Ti )Ti · ni + hi,c (Ti , Tcm )(Ti - Tcm ) = 0 on i,c (2.4)

where Tcm is the surface temperature field for the cladding projected onto the surface, i,c , of pellet i and hi,c is an effective heat transfer coefficient that can be modeled to account for a wide variety of physical phenomena. In this manuscript, we limit discussion of the heat transfer coefficient to the nonlinearities associated with the dependence on the clad and fuel temperatures in the geometric orientation. This in effect results in a nonlinear coupling between all pellets and the clad through the boundary conditions. Zero Neumann boundary conditions ki (Ti )Ti · ni = 0 on i,n are imposed on all remaining surface boundary regions of each pellet. 2.2. Clad Model: The temperature field, Tc , for the clad domain c is also modeled by a nonlinear thermal diffusion equation, -  · kc (Tc , x)Tc (x) = 0 for x  c (2.6) (2.5)

with a scalar nonlinear thermal conductivity (kc ) and a zero right hand side since heat is not generated within the clad materials. Heat transfer from the clad outer surface to the coolant is modeled by a Robin boundary condition
m m ) = 0 on c,f kc (Tc )Tc · nf + hc,f (Tc , Tf )(Tc - Tf

(2.7)

6

where nf is an outward facing unit normal from the clad surface into the m flow region, Tf denotes the interpolated temperature from the fluid flow region, and hc,f is the effective heat transfer coefficient. The temperature of the flow can be described either by a constant fixed temperature or through an independent or coupled flow model. For most of this work we choose to couple a flow model that solves a form of the fluid equations as described in section 7 and Appendix A. Similarly, heat transfer between the clad inner surface and the gap region between the pellets and clad is modeled by a Robin boundary condition
m m kc (Tc )Tc · ng + hc,g (Tc , T1 , . . . , TN )(Tc - Tg ) = 0 on c,g

(2.8)

Here ng is the outward facing unit normal from the inner clad surface into the pellet-clad gap, Tim is the temperature on the outer surface of the pellet projected onto the inner surface of the cladding for each pellet i, and hc,g is the clad-side heat transfer coefficient that is corresponds to the hi,c to conserve energy across the gap. In addition zero Neumann boundary conditions are imposed on the top and bottom of the clad cylinder. 2.3. Weak Formulation: Given fi (x)  L2 (i ), for each pellet subproblem we seek a solution to Eqns (2.1)-(2.5) in the trial space of functions ¯ i , T  H 1 (i ), T satisfies Eqns (2.2)-(2.5) on i }. (2.9) Vi = {T (x)| x   The Galerkin weak formulation for each pellet subproblem is: Find Ti  Vi such that v  Vi -
i

v  · ki (Ti )Ti di =
i

fi v di .

(2.10)

Integration by parts and the divergence theorem yield ki (Ti )Ti · v di -
i i

vki (Ti )Ti · ni di =
i

fi v di

(2.11)

7

Using Eqns (2.2)-(2.5) the boundary integral on the left hand side may be simplified to give -
i

vki (Ti )Ti · ni di = vhi,i-1 (Ti - Tim -1 ) di + vhi,i+1 (Ti - Tim +1 ) di +
i,c m vhi,c (Ti - Tic ) di

i,i-1

i,i+1

Let u, v i =
i

uv di and (u, v ) =


(2.12) uv d denote the standard inner prod-

ucts over the domain i and a boundary segment  respectively. Then the Galerkin subproblems for the pellets, i = 1, 2, . . . , N , can be written as: Pellet Sub-Problems: Find Ti  Vi such that v  Vi ki (Ti )Ti , v i + (hi,i-1 Ti , v )i,i-1 + (hi,i+1 Ti , v )i,i+1 + (hi,c Ti , v )i,c (2.13) m m = fi , v i + (hi,i-1 Tim -1 , v )i,i-1 + (hi,i+1 Ti+1 , v )i,i+1 + (hi,c Tc , v )i,c Similarly, the Galerkin weak formulation for the clad subdomain may be stated as: Clad Sub-Problem: Find Tc  Vc such that v  Vc kc (Tc )Tc , v c + (hc,g Tc , v )c,g + (hc,f Tc , v )c,f m m , v )c,f = fc , v c + (hc,g Tg , v )c,g + (hc,f Tf 3. Discretization 3.1. Domain Discretization As can be seen from Figures 2 and 3, the clad subdomain c and the pellet subdomains i are bounded, connected volumes with piecewise smooth curved boundaries. For the purposes of this paper i is in general approximated by a polyhedral domain h i during the mesh generation process. i h h is partitioned into a set, P , of non-overlapping general hexahedral elei i h h h h ments, Pi = {Pi,1 , Pi,2 , . . . , Pi,M } which are geometrically conforming, i.e.:
h ·  Pi,j = h i j =1 h h · The intersection of two elements, Pi,l and Pi,m for l = m is either empty, a single vertex, an entire edge, or an entire face of both elements. M

(2.14)

8

Figure 2: A slice of a fuel rod mesh showing the outer clad mesh and three inner pellet meshes 3.2. Problem Discretization: A discrete approximation to Ti , the solution to Eqn (2.13) over the i-th pellet subdomain is obtained by solving the problem in a finite dimensional Ni subspace Vih of Vi . Let dim(Vih ) = Ni and {h i,k }k=1 be a suitable basis for Vih . Then, letting Tih =
Ni k=1 h h h Ti,k i,k where Ti,k are unknown coefficients, the

discrete Galerkin problem for each pellet is: Discrete Pellet Sub-Problems: Find Tih = v h  Vih ki Tih , v h i + (hi,i-1 Tih , v h )i,i-1 + (hi,i+1 Tih , v h )i,i+1 + (hi,c Tih , v h )i,c h m h m h = fih , v h i + (hi,i-1 Tim -1 , v )i,i-1 + (hi,i+1 Ti+1 , v )i,i+1 + (hi,c Tc , v )i,c (3.1) h h h Similarly, by a suitable choice of Vc  Vc with span{c,1 , . . . , c,Nc } = Vch and dim(Vch ) = Nc a discrete approximation to the solution of the clad problem (2.14) can be obtained by solving:
Ni k=1 h h Ti,k i,k  Vih such that

9

(a) IFA597 Pellet Mesh

(b) IFA432 Pellet Mesh

Figure 3: (a) pellet mesh with dish and chamfer (b) standard pellet mesh Discrete Clad Sub-Problem: Find Tch = v h  Vch kc Tch , v h
c Nc k=1 h h Tc,k c,k  Vch such that

+ (hc,g Tch , v h )c,g + (hc,f Tch , v h )c,f
c m h m h + (hc,g Tg , v )c,g + (hc,f Tf , v )c,f

h h = fc ,v

(3.2)

Basis functions: The use of general hexahedral or brick elements in discretizing the pellet and clad geometries leads us to use isoparametric finite elements for problem discretization. The basis functions over each geometric element Pi,k of a partition Pi are formed based on mappings of the standard trilinear Q1 basis functions defined over the reference cube [-1, 1]3 to Pi,k . This ensures continuity across element boundaries within a partition. All of the integral terms in Eqns (3.1) and (3.2) are evaluated using a second order Gaussian quadrature. For example, the term from Eqn (3.1) corresponding to the external source term, fih , v h i , is computed by evaluating fi (x) at the quadrature nodes used by the finite element bases and using the quadrature to evaluate the integrals, i.e.
M N

fih , v h i

=
j =1 =1

w fi (x ) v h (x ) ,

(3.3)

where the j index denotes the spatial elements within the domain h i, N 10

indicates the number of quadrature nodes per spatial element, and x and w are the quadrature nodes and weights, respectively. Evaluating the source term at quadrature points rather than mesh nodes allows strict conservation of the source term to be enforced automatically. Because the source term is due to nuclear fission in the fuel pellets, the corresponding term in Eqn (3.2) representing the source in the clad is treated as zero, i.e. fch , v h c = 0.
m m h h For fixed Tim -1 , Ti+1 , and Tc , by choosing v = i,j for j = 1, . . . , Ni in Eqn (3.1) we obtain a set of Ni equations for the unknown coefficients h , k = 1, . . . , Ni denoted by: Ti,k

Fi (Tih ) = 0,

(3.4)

t h h h where Tih = (Ti, 1 , Ti,2 , . . . , Ti,Ni ) with some abuse of notation. Since the conductivity ki and the effective heat coefficients hi,i-1 , hi,i+1 , and hi,c are nonlinear functions, Eqn (3.4) represents a coupled system of nonlinear algebraic equations over each pellet subdomain coupled through nonlinear boundary conditions to the adjacent pellets and clad. For the clad, a similar methodology leads to a set of Nc nonlinear algebraic equations for the unknown h coefficients Tc,k , k = 1, . . . , Nc which we denote by:

Fc (Tch ) = 0. The full set of nonlinear equations we wish to solve is given by F(T) = 0
t

(3.5)

(3.6)

h h h where F(T) = F1 (T1 ), F2 (T2 ), . . . , FN (TN ), Fc (Tch ) is the coupled set of block nonlinear equations across all pellet and clad domains with each block Fi specified by Eqn (3.4) and Fc given by Eqn (3.5). Here, T denotes the t h h h block unknowns across all domains, T  T1 , T2 , . . . , TN , Tch with each Ti a vector of unknowns itself.

4. Solution Strategy Several nonlinear solution strategies could be used to solve the nonlinear system of algebraic equations described in Section 3. The fact that our problems often involve several hundred subproblems each discretized over a

11

separate physical domain make solution methods that do not require formation of the full Jacobian matrix attractive. In particular we choose to use a Jacobian-Free Newton Krylov (JFNK) method [8] for our nonlinear solver. The efficiency of a JFNK method when applied to a particular problem depends heavily on the preconditioner used. We will describe a physics based preconditioner that exploits the natural subdomain decomposition present in our application. A brief overview of the JFNK method is given in Section 4.1 and the construction of the preconditioner is described in Section 4.2. 4.1. Jacobian-free Newton-Krylov Methods Let T denote the exact solution to Eqn (3.6). Classical Newton's method for solving Eqn (3.6) generates a sequence of approximations Tk to T , where Tk+1 = Tk + sk and the Newton step sk is the solution to the system of linear equations Jk sk = -F(Tk ), (4.1)

where Jk  F (Tk ) is the Jacobian of F evaluated at Tk . Newton's method is attractive because of its fast local convergence properties. For large-scale problems, Eqn (4.1) is typically solved using an iterative method because direct methods become impractical. Furthermore, it is often unnecessary to use a tight convergence tolerance for the iterative method when Tk is far from T , since the linearization that leads to (4.1) may be a poor approximation to F(T). Generally, it is much more efficient to employ inexact Newton methods [9], in which the convergence tolerance for (4.1) is selected adaptively by requiring that sk only satisfy: F(Tk ) + Jk sk  k F(Tk ) (4.2)

for some k  (0, 1) [9]. With an appropriate choice of the forcing term k superlinear and even quadratic convergence of the iteration can be achieved [10]. While any iterative method can be used to find an sk that satisfies (4.2), Krylov subspace methods are distinguished by the fact that they only require matrix-vector products to proceed. These matrix-vector products can be approximated by a finite-difference version of the directional (G^ ateaux) derivative as:

12

F(Tk + v) - F(Tk ) , (4.3)  which is especially advantageous when Jk is difficult to compute or expensive to store (as is the case in this application due to the presence of multiple meshes). While the selection of a suitable differencing parameter  may be non-trivial for some applications, it is generally well-understood [11]. For this application, we choose: Jk v  = 
mach

1 + Tk , v

where mach is machine precision and · refers to the l2 -norm. In our applications, which are performed in double precision,  is typically on the order of 10-10 . From the various Krylov methods available, GMRES was selected because it guarantees convergence with nonsymmetric, nonpositive definite systems [12] (the case in some of our examples), and because it provides normalized Krylov vectors v = 1, thus bounding the error introduced in the difference approximation of (4.3) (whose leading error term is proportional to  v 2 ) [13]. However, GMRES can be memory intensive (storage increases linearly with the number of GMRES iterations per Jacobian solve) and expensive (computational complexity of GMRES increases quadratically with the number of GMRES iterations per Jacobian solve). In principle, restarted GMRES can deal with these limitations; however, it lacks a theory of convergence, and stalling is frequently observed in real applications [14]. In our applications, we rely on performing inexact solves combined with efficient preconditioning to keep the number of GMRES iterations required to compute each inexact Newton step, sk , small. 4.2. Preconditioning Preconditioning is a numerical technique used within an iterative method to accelerate the process of finding a solution to a system of equations. The use of a preconditioner will increase the cost of each iteration but a good preconditioner will drastically reduce the total number of iterations required to solve the system of equations thus making the overall process significantly faster than the unpreconditioned case. While the idea can be applied to 13

both nonlinear and linear systems of equations, we only focus on applying to the linear systems at each Newton step within the JFNK procedure. In particular, we used a "right preconditioning" procedure to solve (4.1). In this approach, (4.1) is transformed to the equivalent system shown in (4.4) by using a preconditioner, M . JM-1 Ms = -F(T) (4.4)

The above system is solved in two steps: (1) solve Ay = -F(T) using the GMRES method where A = JM-1 and (2) compute s = M-1 y. The matrix-vector product, Av, is approximated as shown in (4.5). F(T + M-1 v) - F(T) , (4.5)  Ideally, M-1 should be a good approximation to J-1 , it should be easy and inexpensive to compute and apply, and the computation and application of M-1 should have good parallel scalability. It is difficult to meet all of these competing requirements and some trade-offs need to be made while designing M. The design of the preconditioner used for our problem stems from a careful observation of the structure of the true Jacobian matrix which is of the form:   J11 J12 0 0 ··· 0 J1C  J21 J22 J23 0 · · · 0 J2C     0 J32 J33 J34 · · ·  0 J 3 C    0 0 J43 J44 · · · 0 J4C  J= (4.6)   .  . . . . . . . . . . . . .  . . . . . . .     0 0 · · · · · · · · · JN N JN C  JC 1 JC 2 · · · · · · · · · JCN JCC Av  Here, each Jii , i = 1, . . . , N is a block matrix denoting the portion of the full Jacobian arising from interior-interior connections within the i-th pellet. Ji,i-1 and Ji,i+1 are block matrices corresponding to the boundary interactions between pellet i and pellets (i - 1) and (i + 1) respectively. JiC and JCi are block matrices corresponding to the boundary couplings between the i-th pellet and the clad. Under normal operating conditions within a reactor the heat transfer between adjacent pellet domains is considerably lower than between the pellets and clad. This prompts us to drop off diagonal terms corresponding to 14

pellet-pellet interactions when forming the preconditioning matrix M. This leaves off diagonal coupling terms between the pellets and clad. While a preconditioner that retains these terms is likely to yield better overall convergence it requires a block Gauss-Seidel type approach that limits the level of asynchrony within the preconditioner. By dropping off diagonal terms corresponding to pellet-clad interactions we choose to sacrifice potentially better convergence for the ability to asynchronously solve all domains at once within the preconditioner solve step. Finally, block diagonal terms of the Jacobian that involve the partial derivative of the thermal conductivity, k , with respect to the temperature, T are ignored to yield an approximate Jacobian of the form ~  J11 0 0 0 ··· 0 0 ~22 0  0 J 0 ··· 0 0     0 ~33 0 · · · 0 J 0 0     0  ~ 0 0 J · · · 0 0 M= (4.7) 44   .  . . . . . . . . . . . . .  . . . . . . .     0 ~ 0 · · · · · · · · · JN N 0  ~CC 0 0 ··· ··· ··· 0 J Inverting systems of the form Ms = y now correspond to solving (N + 1) independent subsystems ~ii si = yi J (4.8) At the k -th Newton step, systems 1 through N correspond to discretizing and solving N variable coefficient linear diffusion PDE systems, one for each pellet domain, of the form:
k -  · ki (Tik , x)Ti (x) = ri (x), for x  i ,

(4.9)

with boundary conditions ki (Tik )Ti · ni + hi,i-1 (Tik , 0)Ti = 0 on i,i-1 , ki (Tik )Ti · ni + hi,i+1 (Tik , 0)Ti = 0 on i,i+1 . (4.10) (4.11)

where Tik is the current approximation to the solution of the nonlinear system k (3.6) over the i-th pellet domain and ri is the nonlinear residual. System (N +1) of the preconditioner solve is over the clad domain and again involves

15

discretizing and solving a linear variable coefficient diffusion PDE system of the form: k -  · (kc (Tck , x)Tc ) = rc for x  c (4.12) with boundary conditions kc (Tck )Tc · ng + hc,g (Tck , 0)Tc = 0 on c,g kc (Tck )Tc · nf + hc,f (Tck , 0)Tc = 0 on c,f (4.13) (4.14)

with Tck being the current approximation to the solution of the nonlinear k system (3.6) over the clad domain and rc the nonlinear residual. Discretization of the linear variable coefficient diffusion PDE systems described above follows along the lines of the methodology described in section (3). Each application of the preconditioner involves (N + 1) algebraic multigrid solves to invert the systems (4.8). Since these are independent subsystems all of the (N + 1) solvers can operate simultaneously when distributed over different processor sets enabling us to obtain a high degree of parallelism irrespective of the number of pellet subdomains present. 5. Computational Infrastructure Several computational tools are necessary for performing simulations such as the one described in this paper; we developed the Advanced Multi-Physics (AMP) [15] package for this purpose. AMP is a complete system for simulating stationary and time dependent, multi-domain, coupled physics problems. AMP consists of several software components. Each component is designed to provide a uniform consistent interface which interacts with other components, and developers of other components are only exposed to these interfaces. This is despite the fact that AMP is designed to sit in between existing software frameworks to leverage their strengths and investments without over-dependence. The complexities of interfacing with different software frameworks are kept behind the standard interfaces that AMP provides. Fig. 4 illustrates the structure of the various components in AMP. A brief description of each of these components is given below. 5.1. Mesh and geometry The mesh and geometry interface (AMP::Mesh) allows AMP to interact with multiple mesh or geometry packages. AMP::Mesh already interfaces

16

External Codes

Time Integrators Solvers Operators Vectors/Matrices Discretization Mesh/Geometry

Figure 4: Structure of components in AMP

with the LibMesh [16] and STKMesh [17] packages, in addition to maintaining a native structured mesh capability. The native structured mesh capability includes automated mesh generation for simple geometries enabling users to create boxes, cylinders, and tubes without an external mesh generation tool. These internal mesh generators were used for most of the results presented in section 6. 5.2. Discretization Due to the close coupling between mesh and discretization, AMP::Mesh (using functionality contained in LibMesh), currently handles the discretization also. This will become a separate interface, if the need arises for us to interface different discretization packages with mesh packages. 5.3. Vectors and matrices AMP provides standard AMP::Vector and AMP::Matrix classes which serve two purposes. Firstly, they provide users with a standard interface to perform vector and matrix operations. At the same time, the classes hide the details of interfacing with various software packages that have their own definition of vectors and matrices. For example, Trilinos [18] and PETSc [19] both provide matrix operations and Trilinos, PETSc, and SUNDIALS [20] provide and/or use vector operations. AMP Vector and Matrix act as the interfaces to these packages through the Vector and Matrix classes and enable a user to combine components from all of these packages to build powerful AMP applications while not having to tackle the complexities of interacting with each of these packages. 17

The vector class also contains the ability to compose multiple parallel vectors into a single multi-vector that can be provided to a solver. For example, the global solution and source vectors are created as a composition of nodal vectors on each mesh and composed into a single multi-vector. A view to the multi-vector is provided to PETSc SNES nonlinear solver for the Newton iterations and a view of the vector and the diffusion matrix associated with a single domain is provided to Trilinos ML for the multi-grid preconditioning. 5.4. Operators Operators are the core of the AMP design and where all of the physics is contained. Operators encapsulate the details of the mapping operation L : X  Y where X and Y are appropriately defined spaces. Operators may represent discretized PDE operators, boundary operators, an operation to extract material properties from material databases or tables, linear or nonlinear algebraic operations, or compositions of the above. The ability to compose operators and to extract information from compositions is intended to facilitate the incremental construction of multi-physics and/or multidomain simulations as well as rapid prototyping and experimentation to understand couplings in multi-physics simulations. The nonlinear and linear FEM operators for diffusion, boundary conditions and interpolation maps between domains are encoded as operators. 5.5. Solvers Solvers in AMP refer to the nonlinear and linear solvers that represent the action of an approximate inverse map of a given operator if that inverse operation has some well defined meaning. In this sense solvers can also be considered as operators. An inverse operator can be easily constructed by wrapping a solver in an inverse operator class. The solver interface allows the user to utilize a standard interface to solvers from Trilinos, PETSc, native AMP solvers, and potentially other packages in the future. Again, the design emphasis has been to provide a standard interface to hide the complexity of particular software packages from a user and to avoid required dependence on a particular software package. The solver interface enables us to create the complex nonlinear solvers, linear solvers, and preconditioners across multiple domains without significant code rewrites.

18

5.6. Time integrators AMP time integrators provide a uniform interface to solving time-dependent systems which can include Differential Algebraic Equations (DAEs). This is necessary within the context of our broader target application class because of coupling between time dependent thermal and quasi-static mechanical systems being simulated. The design allows for explicit, semi-implicit, and fully implicit simulations of coupled multi-physics problems. In the case of semi-implicit and fully implicit calculations, the solver interfaces in AMP are used, and in all cases, the operator interfaces are used to allow composable multi-physics simulations allowing users to experiment with coupling different physics together. The time integrator interface is used to provide an interface to the SUNDIALS suite of time integrators and can be used in future to interface to other time integrator packages such as the Rhythmos package of Trilinos. 5.7. External packages AMP is designed to leverage existing software whenever possible including off-the-shelf leadership class computational packages that include the Trilinos, PETSc and SUNDIALS packages. In general the infrastructure design of AMP does not rely on any external software, but provides interfaces for using external software within AMP. For example, a user can create a vector using AMP's internal vector, a PETSc vector, or a Trilinos Epetra vector, but can then use the given vector within the solvers that may use PETSc or Trilinos solvers. AMP leverages capabilities within many software packages through a seamless application programming interface including MPI for parallel capabilities, PETSc for vectors, nonlinear and linear solvers, Trilinos for vectors, nonlinear solvers, and algebraic multigrid solvers, SUNDIALS for implicit time integrators, LibMesh and STKMesh for discretization and meshing and HDF5 and Silo for IO. 5.8. Parallel implementation AMP is primarily designed to be a parallel infrastructure based on MPI. An MPI-based utility class is provided that allows the user to utilize MPI. The interface enables AMP to be compiled without MPI for users who do not wish to leverage the parallel capabilities. The core design is independent of the parallelization, and the parallelization is based on parallel decomposition of the meshes. There is a two-level parallel decomposition used for the mesh 19

domains. First, the individual mesh domains are split onto separate communicators and independent processor groups. This minimizes the number of processors per mesh and ensures that independent meshes can utilize collective operations that do not include the processors of other meshes. This splitting is done internally within AMP and can be modified by the user. A second level of domain decomposition is then performed to divide each mesh domain among the processors in the MPI group for that domain. This level of decomposition is handled by the package responsible for the current mesh domain. For example, if the underlying mesh for a given domain is LibMesh, it will perform the decomposition, while a native AMP mesh will be controlled by AMP. Note that different mesh domains may be owned by different packages and this is fully supported. Once the domain-decomposition is performed, vectors and matrices may be created over a single mesh, an arbitrary combination of meshes, or a subset of a mesh (or multiple meshes). Each vector or matrix exists over a given communicator that does not need to match any mesh. Linear operations are then performed on this communicator reducing the need for global operations over the entire global communicator. Maps between multiple domains use a communicator that spans two or more existing communicators. The MPI utility class provides all routines for creating and managing the communicators with MPI, including their proper destruction when they are no longer needed. Section 6.4.1 includes the results of a scaling study conducted using the problem described in Section 6.1 and using the parallel load balancing strategies described here. 6. Numerical Experiments A suite of numerical experiments were defined to verify the accuracy of the thermal transport capability of AMP for a multi-domain problem that is based on the geometry and materials of nuclear fuel. Independent studies were performed to verify the accuracy of the solution using the method of manufactured solutions (Section 6.2), evaluate the accuracy of the code with respect to experimental data and a well characterized code used for regulatory analysis (Section 6.3), and evaluate the scalability of the parallel algorithm (Section 6.4). All of the studies were based on actual geometries and material properties for experimental nuclear fuel rods that are defined in Section 6.1. All numerical experiments were performed on the Titan (Cray XK7) and EOS (Cray XC30) supercomputers hosted at the Oak Ridge Leadership 20

Computing Facility with 8 and 16 MPI processes per compute node respectively. Load balancing is done automatically by the load balancer within AMP. The AMP nonlinear solver internally leveraged the JFNK implementation within the PETSc package with absolute and relative tolerances for the JFNK nonlinear solver being set to 1.0e-12 and 1.0e-10 respectively. Right preconditioned FGMRES with a maximum Krylov dimension of 40 was used within our simulations. The AMP preconditioner consists of a block Jacobi solver as described earlier with each block Jacobi solver component consisting of one or more iterations of an algebraic multigrid (Trilinos ML) V-cycle solver with 2 pre- and post-smoothing steps of a symmetric Gauss-Seidel smoother, a maximum of 10 multigrid levels and a coarse grid direct solver. All simulations were performed in double precision arithmetic. 6.1. Experimental Setup The materials and geometries used in the following numerical experiments are based on one of two well-characterized experiments from the International Fuel Performance Experiments (IFPE) database [21]. The Integrated Fuel Assembly (IFA) 432 [22], Rod 1, is a standard nuclear fuel rod (uranium-dioxide or, UO2 , fuel in Zircaloy-4 clad) that was irradiated in the Halden boiling water reactor from December 1975 to June 1982 with online temperature measurements at one axial location in the center of the fuel. The IFA 597 [23], Rod 2, contains weapons-grade mixed-oxide (MOX) fuel within Zircaloy-4 clad in a more modern geometry that includes a dish, chamfer, and central hole, as shown in Figure 5. IFA 597, Rod 2 was irradiated in the Halden boiling water reactor from from July 1997 to January 2002 with online temperature measurements at one axial location in the center fuel. Each of these rods is a short version of a full length commercial fuel rod. The geometric and material description of the experimental rods are provided in Table 1 for the IFA 432 and IFA 597 experiments. The clad height and number of pellets modeled were adjusted in Sections 6.2 and 6.4 to achieve the purpose of that section. However, these geometries were used exactly in Section 6.3. The thermal conductivity (k ) of Zircaloy-4 (Eq. 6.1) and fuel (Eq. 6.2), including both UO2 and MOX, depend on the temperature (T) and burnup (B), which is a measure of total heat generated locally. k [Zr] = 7.51 + 2.09 × 10-2 T - 1.45 × 10-5 T 2 + 7.67 × 10-9 T 3 21

(6.1)

Figure 5: Mid-plane slice of an annular nuclear fuel pellet with a dish and chamfer

22

Dimension Units Pellet ID mm Pellet OD mm Pellet Dish Depth mm Pellet Dish Diameter mm Pellet Chamfer Height mm Pellet Chamfer Diameter mm Pellet Density g/cc Pellet Height mm Number of Pellets Clad ID mm Clad OD mm Clad Height mm

IFA 432 0 10.67 0 0 0 0 10.42 13 44 10.9 12.78 622.8

IFA 597 1.8 8.05 0.26 2.15 0.15 5.3 10.5 10.5 21 8.22 9.5 252.

Table 1: Geometry and material specification for validation problems

k [Fuel] = 

 + T +  1.0 + e

 -T

-1

-1

+ T -2 e- T



(6.2)

where [UO2 ] = 1.00767;  [UO2 ] = 0.0452 + 0.00187B ;  [UO2 ] = 0.000246; [UO2 ] = 16361;  = =  =  =

[MOX] = 1.05353  [MOX] = 0.035 + 0.00187B  [MOX] = 0.000286 [MOX] = 13520

0.038 1.0 - 0.9e-0.04B B 0.28 396 6380 3.5 × 109

To define the heat source in the nuclear fuel, AMP allows the user to either define the power distribution, f (r, , z ), as a function of the radius (r) from the center of the pellet, height (z ), and azimuthal-angle () about the z axis or provide a power distribution in a coupled-physics calculation at every 23

quadrature-point in the problem. The user-defined power definition allows for a simple definition of the axial shape functions or complex nuclear-specific features, such as the radial rim effect or azimuthal variations guide tubes and control rods. The radial power shape includes the option to use a model that coincides with the empirically-derived TUBRNP model (Equation 6.4) from the Transuranus nuclear fuel performance code [24]. f (r, , z ) = 1 + a F (r) + b sin() +
k>0

ck Pk (z );

(6.3) (6.4)

F (r) = 1 + 3.45 exp[-3(R - r)0.45 ].

In equation 6.3, the user-defined coefficients (a, b , and ck ) define the magnitude of each component, Pk (z ) are Legendre polynomials, and F (r) is the TUBRNP model that is based on the radius and the outer radius of the fuel pellet (R). In the verification testing, a manufactured-source was utilized; the validation testing utilized the TUBRNP model. 6.2. Verification Studies Verification studies for modeling steady state thermal contact for nuclear fuels are presented here. The verification process uses the method of manufactured solutions. For this study, the pellet geometry is based on the IFA 432 experiment and the clad geometry is simplified as shown in Figure 6. Material properties of UO2 for the pellet and Zircaloy for the clad are based on the IFA 432 experiment as listed in Section 6.1. The pellet and clad domains are not in contact and have a (gap) distance between the surfaces. A total of four cases are studied using this configuration. Case 1: Three-dimensional single fuel pellet (with no clad) that exhibits strong gradients in all directions with different Robin boundary conditions between circular surface along the height and end surfaces. Case 2: Three fuel pellets stacked upon each other (with no clad). The surfaces are in contact and a manufactured solution is constructed using the fuel pellet contact conductance model. At the contact surfaces a Robin boundary condition is imposed and the results would make apparent any anomalies in the volume and boundary condition discretizations. Case 3: Single fuel pellet and clad with heat transfer across a gap between the surfaces. The manufactured solution for both the domains is constructed

24

Figure 6: Schematic of pellet and clad geometries used in verification studies

25

taking the nonlinear gap conductance between the internal surfaces into consideration. The results would make apparent any lack of energy conservation due to heat transfer across the gap. Case 4: Two pellets and clad including contact between pellets and heat transfer across a gap between pellet and clad. This verifies the implementation at corner points which intersect multiple domains. We begin by selecting the exact solution to be (x, y, z ) = 800z + 106 (0.00004 - 20x2 - 20y 2 ) (6.5)

which is qualitatively similar to the thermal solutions and in bounds to the material models. Substituting it for T in the differential equation (2.1) we obtain the corresponding analytic right hand side. These sources and corresponding boundary conditions are evaluated at each quadrature point of the entire finite element mesh in order to eliminate interpolation errors. The relative and absolute convergence tolerances within the nonlinear solver are set to 1.0e-10 to ensure solver errors are below discretization error bounds. The discretization error is evaluated using an L2 norm.
1/2 1/2

|| - h ||L2 =


( - h ) d

2


 qp

( - h ) J (xqp )wqp

2

(6.6) Convergence rates of these norms are reported on progressively refined meshes by increasing the number of elements in all three cylindrical coordinates to simplify computing the characteristic element length "h" of the unstructured mesh. Also, we make sure that the refinements are generated from the original geometry to ensure that the meshes are geometrically conforming. The rate of convergence is given by log p= log
||e2 || ||e1 || h2 2 h2 1

log 

||e2 || ||e1 ||

log (4)

(6.7)

26

Problem Case 1

Case 2

Case 3

Case 4

# Elements 1890 14944 119296 5670 44832 357888 3510 27904 222976 5400 42848 342272

|| - h ||L 2.1796 0.7426 0.2256 2.1845 0.7810 0.2644 2.9740 0.7053 0.1817 2.9851 0.7446 0.2207

|| - h ||L2 0.002411 0.000624 0.000155 0.004139 0.001073 0.000267 0.003132 0.000804 0.000200 0.004434 0.001139 0.000284

p 1.94 2.00 1.94 2.00 1.96 2.00 1.96 2.00

Table 2: Mesh refinement studies # Elements || - h ||L 10 Pellets 18900 2.1848 149440 0.7810 1192960 0.2644 50 Pellets 94500 2.1223 756000 0.7810 6048000 0.2644 || - h ||L2 0.007606 0.001971 0.000491 0.017045 0.004416 0.001100 p

1.94 2.00

1.94 2.00

Table 3: Many domain mesh refinement studies for Case 2

27

Elements 10 Pellets 35100 280800 2246400 50 Pellets 87750 702000 5616000

|| - h ||L 2.9899 0.7450 0.2210 2.9852 0.7450 0.2210

|| - h ||L2 0.009937 0.002549 0.000636 0.022190 0.005700 0.001421

p

1.96 2.00

1.96 2.00

Table 4: Many domain mesh refinement studies for Case 4 In addition to four cases mentioned, we also conducted the verification studies for generalized versions of case 2 and case 4 with 10 and 50 pellet domains to demonstrate the convergence rate of the solution procedure in parallel. These results are presented in Tables 3-4. 6.3. Validation Studies An extensive validation evaluation of AMP for nuclear fuel applications, which includes several experimental fuel rods for a variety of conditions, has been documented in [25­27]. This section includes an excerpt of that research to provide a basis for the accuracy of the material models with respect to the experimental results. Because of the extreme environment of nuclear fuel (high radiation, high temperature, and highly turbulent, multi-phase flow), it is difficult to precisely measure both the local temperature and the power in the fuel near the thermocouple. Therefore, nuclear fuel experiments generally assume a 5 to 10% experimental uncertainty in the measured data; for this report, we have incorporated a relatively tight expected tolerance of + 50K, which is generally less than the 10% error and approximately 3% at full power. Figure 7 provides the computational results of the AMP simulation and experimental measurements of the centerline fuel temperature in the IFA 597 experiment. The input power is relatively constant and the computational results are consistently within 50K and generally within 3% of the measured temperature. The results for the IFA 432 experiment are shown in Figure 8. Because the power distribution varies significantly more than in the IFA 597 experiment, the 50K error bars on the experimental results appear much

28

Figure 7: Validation of the temperature at the thermocouple in the IFA 597 experiment smaller. However, the AMP results generally fall within the experimental error. 6.4. Scaling Studies Scaling studies for the steady state thermal problem are presented here. The problem setup was described in Section 6.1, with the geometry based on the IFA-432 experiment, but now including 348 full pellets, which is representative of a full-scale commercial nuclear fuel rod. Several different aspects of the performance are studied using both strong and weak scaling. These include: · Solve: Total time spent within the JFNK solver for the entire coupled nonlinear thermal problem. · Nonlinear Residual: The time required to compute one consistent global nonlinear residual within the JFNK solver across all physical domains. Computing the nonlinear residual involves computing the nonlinear residual for each physical volume, applying boundary conditions, and mapping solution components across domains. The times 29

Figure 8: Validation of the temperature at the thermocouple in the IFA 432 experiment for these individual components will also be reported separately. Note that the number of calls will vary slightly depending on the number of iterations of the solver. This information is also included. · Diffusion Apply: Time required to compute the non-linear finite element residual at all interior degrees of freedom within all domains once. No communication between domains is required here. · Boundary Conditions: Time required to impose the non-linear finite element boundary conditions once within the computation of the global nonlinear residual. This requires traversing all surface elements on the finite element meshes for each domain. No communication between domains is required here. · Map Apply: Time per application of the maps for the solution transfer between the clad and pellets and the maps between the different pellet domains. The mapped values are required in imposition of the nonlinear Robin boundary conditions.

30

· Load Mesh: Time to create all of the meshes. This consists of creating the appropriate load balance for the different domains, creating the individual meshes, and initializing all mesh data. For the purposes of this problem the meshes used consist of internal, logically rectangular meshes. · Save Results: Time to write the results. This includes writing a separate SILO file for each core, and a single summary file that can be loaded into VisIt [28]. 6.4.1. Strong Scaling The strong scaling results are based on a standard resolution mesh, which contains approximately 1.6M unknowns. For high fidelity analysis of a single fuel rod, this is considered a relatively coarse mesh; a high resolution analysis would typically require an 8x increase in each dimension. On the other hand, for high-fidelity analyses of a full nuclear reactor containing tens of thousands of fuel rods, this resolution is sufficient because the accuracy requirements are typically lower. Demonstrating efficient strong scaling from serial to 100500 cores for this problem size (single rod with 1.6M unknowns) will be sufficiently indicative of good scaling results for a full reactor on millions of cores and will also provide a lower bound on expected scaling for fineresolution fuel rod calculations on tens of thousands of cores. The strong scaling studies were executed using 1-2048 cores on the Titan Cray XK7 and the EOS XC30 at Oak Ridge National Laboratory. Tables 5 and 6 show the scaling results on EOS and Titan respectively, with the different components of the solve as well as loading the meshes and saving the results. The first column is the number of cores, while all other columns are the wall-clock execution times for the problem in seconds. For the diffusion and map apply calls (Diffusion Apply and Map Apply, respectively), the execution time is the average accumulated time across all cores. This is necessary because some cores may have different execution times due to load imbalances and is particularly acute for small core counts (2-8) due to a load imbalance between the domains. For large core counts, each domain will exist on a non-overlapping set of cores. Figures 9 and 10 shows the plots of the scaling results compared to ideal scaling. For small core counts (1-8), the run time of the solve is limited by the maps between the different domains. The most time-consuming map is for the cladto-pellets heat transfer between the outer surface of the pellets and the inner 31

# of Processors 1 2 4 8 16 32 64 128 256 512 1024 2048

Nonlinear Diffusion Boundary Map Residual Apply Conditions Apply 3488.25 2690.93 751.84 200.66 1629.49 2265.47 1273.65 374.21 99.52 739.46 761.60 397.61 193.86 50.04 107.30 315.21 187.79 102.38 26.51 42.89 159.12 86.89 51.35 13.22 12.76 74.47 42.11 25.71 6.68 5.34 38.09 21.10 12.94 3.49 2.46 20.83 10.63 6.51 1.74 1.16 10.69 5.42 3.27 0.89 0.64 5.90 3.00 1.70 0.49 0.45 3.85 2.03 0.92 0.30 0.56 4.00 2.15 0.53 0.23 1.12 Solve

Mesh Loading 13.11 7.02 6.33 2.33 1.39 0.64 0.34 0.20 0.11 0.07 0.07 0.10

Save Results 483.12 306.11 30.83 12.48 2.90 1.05 0.42 0.32 0.11 0.11 0.15 0.22

Table 5: Strong scaling studies on EOS
# of Processors 2 4 8 16 32 64 128 256 512 1024 2048 Nonlinear Diffusion Boundary Map Residual Apply Conditions Apply 4908.14 2689.35 739.69 217.28 1609.45 1629.89 819.30 371.02 106.32 248.20 642.05 363.04 186.89 52.71 93.28 345.45 171.02 96.10 27.18 29.70 174.91 88.51 51.01 15.10 12.67 89.06 43.71 25.56 7.60 5.74 46.97 21.87 12.72 3.93 2.50 23.97 11.11 6.39 2.02 1.33 12.88 5.97 3.31 1.08 0.82 8.29 4.06 1.80 0.66 1.02 8.01 4.10 1.02 0.46 2.07 Solve Mesh Loading 13.83 13.78 4.76 2.81 1.42 0.72 0.40 0.23 0.15 0.14 0.22 Save Results 682.38 73.28 28.93 8.15 2.84 1.05 0.35 0.20 0.17 0.18 0.29

Table 6: Strong scaling studies on Titan surface of the clad. This map is constructed using a std::multimap with all points on the local surface, followed by a pair-wise all-all exchange of data between the two meshes. For the serial case, this results in a large number of points in the local map that must be managed. As the number of cores is increased to 128-512 cores, the number of points on the surface per processor is significantly reduced, which leads to an additional log(n) reduction in the wall-clock time that results in the super-linear speedup observed. The apparent lack of speedup from 1 to 2 cores is due to the load imbalance between the clad mesh located on one core and all of the pellet meshes on the

32

104 103 Time (s) 102 101 100 10-1 0 10 101 102 # of Processors 103

Ideal Solve Apply Diffusion Apply Robin Apply Maps Load Save

Figure 9: Strong scaling studies on EOS

104 103 Time (s) 102 101 100 10-1 0 10 101 102 # of Processors 103

Ideal Solve Apply Diffusion Apply Robin Apply Maps Load Save

Figure 10: Strong scaling studies on Titan

other. For very large core counts (1024-2048), the number of elements on a surface is sufficiently small that the communication time begins to dominate, which limits the scaling for this problem size. The behavior of the diffusion and the Robin boundary condition applies (Diffusion Apply and BC Apply, respectively) show nearly perfect scalability because they do not involve any communication. Generating the native structured meshes does not represent 33

2 Efficiency 1.5 1 0.5 0 0 10 101 102 # of Processors Figure 11: Scaling efficiency1 103

EOS Titan

a significant portion of the execution time, yet loading the meshes shows nearly perfect scaling up to 512 cores. At this relatively high core count, the load time is dominated by the load balance process, which is relatively independent of the number of cores (0.1 seconds). Finally, saving the results of the simulation (Save Results) has an acceptable scalability. The results are saved to multiple SILO files (one per core), with a single summary file. For small core counts, the Save Results is dominated by the time to write the data. For large core counts, Save Results is dominated by the time to open a file on the Lustre file system. This is approximately constant for all cores, but has a large variation that depends on the load of the computer. Several executions were made and the typical results are presented. The time to open a file varied between 0.02 and 1 second. Large executions are particularly sensitive to this effect because all cores must synchronize to write the summary file. Figure 11 shows the parallel efficiency on EOS and Titan. TS , where TS is the serial wall clock time, The efficiency is calculated as PN TP PN is the number of processors, and TP is the parallel wall clock time with PN processes. Note that for processor counts between 4 and 512, the speedup is greater than 1. This is due to the super-linear speedup discussed previously.
The serial wall clock time TS for Titan is not available due to maximum wall clock time restrictions. TS on Titan is estimated by assuming that the efficiency on 2 processors of Titan and EOS is similar. This assumption is justified by the data in Tables 5 and 6.
1

34

6.4.2. Weak Scaling Tables 7 and 8 shows the results of a weak scaling study performed on EOS and Titan. In the weak scaling study, the base mesh from the strong scaling study was used with 64 cores and the resolution was increased in each direction by a factor of 2x and 4x. This resulted in a set of executions of 1.6M unknowns on 64 cores, 12.8M unknowns on 512 cores, and 102M unknowns on 4096 cores. All times are in seconds and for functions that are called multiple times per solve the time per call is included in parentheses. Using weak scaling, varying the problem size and processor count by 64 times, the performance of the solve is approximately constant. The total number of non-linear iterations was independent of the resolution, and the total number of linear iterations varies slightly with resolution. This is most likely due to a slight degradation in the performance of the parallel smoother used within the algebraic multigrid solver. The solve time is approximately constant with some variation that is due to differences in the number of linear iterations. The contributions to the solve are specified in terms of the total time and the time per iteration in parentheses. Based on the time per iteration, the global apply, finite element diffusion operator apply, and resetting of Trilinos ML are all approximately constant. The apply call for the Robin boundary condition decreases slightly with problem size as the ratio of the total number of unknowns on the surface compared to the total number of unknowns decreases slowly with problem size. The variation in mapping between domains is primarily due to specific parallel decomposition and the variation in MPI performance on Titan and EOS that depend on the allocated nodes. Loading and saving the meshes show a slight increase for large problems due to increased demand on the Lustre parallel file system used but represents a small fraction of the total run time (2-3%). 6.4.3. Preconditioner Performance As mentioned in the introduction section the number of pellets within a fuel rod can vary dramatically. Hence it is important that the solver deliver good performance as the number of actual pellet domains is varied. This in turn is dependent primarily on the performance of the preconditioner employed. In order to study this we consider a series of numerical experiments where the total number of mesh elements across all pellet and clad domains is kept constant while the number of pellet domains is varied. We note that though the number of mesh elements is kept constant the number of degrees of freedom does rise slightly (3%) as the number of pellets is increased due 35

Core Count Degrees of Freedom Nonlinear iterations Linear iterations Solve Nonlinear Residual Diffusion Apply Boundary Conditions Map Apply Reset ML Mesh Loading Save Results

1x 64 1.6M 5 29 38.14 21.10 (0.73) 12.97 (0.45) 3.46 (0.12) 2.46 (0.08) 13.05 (0.45) 0.34 0.42

2x 512 12.8M 5 31 35.29 19.24 (0.62) 14.04 (0.45) 2.00 (0.06) 1.60 (0.05) 11.22 (0.26) 0.28 0.35

4x 4096 102M 5 36 49.56 26.91 (0.75) 17.12 (0.48) 1.81 (0.05) 5.97 (0.17) 12.74 (0.35) 0.42 1.11

Table 7: Weak scaling studies on EOS 1x 64 1.6M 5 29 88.99 43.66 (1.51) 25.55 (0.88) 7.57 (0.26) 5.74 (0.20) 29.27 (1.01) 0.72 1.06 2x 512 12.8M 5 31 77.94 38.10 (1.23) 27.51 (0.89) 4.30 (0.14) 3.05 (0.10) 25.09 (0.81) 0.59 0.72 4x 4096 102M 5 36 107.19 55.69 (1.55) 37.12 (1.03) 4.09 (0.11) 10.22 (0.28) 30.54 (0.85) 1.25 1.47

Core Count Degrees of Freedom Nonlinear iterations Linear iterations Solve Nonlinear Residual Diffusion Apply Boundary Conditions Map Apply Reset ML Mesh Loading Save Results

Table 8: Weak scaling studies on Titan to the introduction of more surface elements. In addition the number of solution transfer operations that require communication between domains also increases. Note that the additional communication incurred is point to point communication between pairs of processors and does not significantly affect the runtime. The dimensions of the pellets are chosen so that the height of the pellet stack matches that of the clad. Figure 12 plots the 36

Domains Solve

1 50.61

2 50.86

4 50.61

8 50.51

16 50.81

32 49.63

64 49.56

128 50.26

256 51.38

Table 9: Solution times as the number of pellet domains is varied number of nonlinear and linear iterations required for solving thermal fuel rod problems as the number of pellet domains is varied from 1 through 256. In addition the number of multigrid solves required within the preconditioner is varied from 1 to 3. As can be seen, the number of iterations remains largely constant. Figures 13(a) and 13(b) show the residual convergence history for the nonlinear solver as the number of domains is varied ranging from 1-256 domains. There is only a very slight effect on the residual due to increasing the number of domains. Finally, Table 9 shows that the required wall clock times for solution as the number of domains is varied does not change significantly.
20 Number of Iterations 15 10 5 0
Nonlinear (1 ML) Nonlinear (3 ML) Linear (1 ML) Linear (3 ML)

1

2

4

8 16 32 64 Number of Pellet Domains

128

256

Figure 12: Number of nonlinear and linear iterations as a function of the number of pellet domains and the number of algebraic multigrid (ML) iterations

7. Fuel Assembly Modeling In section 5 the components of the AMP multi-physics infrastructure that enabled the development of nonlinearly consistent multi-domain thermal transport calculations that form the main core of this paper were described. Here we illustrate further multi-physics capabilities of AMP by describing 37

102 Residual norm 100 10-2 10-4 10-6 10-8 1

Residual norm

NP=256 NP=128 NP=64 NP=32 NP=16 NP=8 NP=4 NP=2 NP=1

102 100 10-2 10-4 10-6 10-8

NP=256 NP=128 NP=64 NP=32 NP=16 NP=8 NP=4 NP=2 NP=1

2 3 4 Nonlinear iteration number

5

1

2 3 4 Nonlinear iteration number

5

(a) 1 ML iteration

(b) 3 ML iterations

Figure 13: Residual convergence history for fuel rods with NP=1 to NP=256 pellet subdomains with 1 and 3 multigrid iterations per subdomain within the preconditioner further extensions of the fuel rod modeling capability. Since our focus is on solution and coupling methodology and due to space limitations we will concentrate on the relevant coupling aspects with details on the models being provided in the appendices and provided references. Coupling to Coolant Models: The coolant liquid flowing on the outside of each fuel rod serves as a heat sink which flows axially along the length of the outside surface of the clad. The coolant model in the fluid domain and the thermal transport model in the clad domain are coupled nonlinearly through Robin boundary conditions (Eqn 2.7)
m m kc (Tc )Tc · nf + hc,f (Tc , Tf )(Tc - Tf ) = 0 on c,f m reproduced here for clarity. Here, Tf represents a mapped coolant temperature field over the clad surface. AMP enables us to couple different models interchangeably. The first model described in Appendix A.1 uses a reduced empirical model that solves a single axial equation using a simple finite difference scheme and is frequently sufficient for many calculations. This is the model used within the single fuel rod calculations presented in prior sections. The second model described in Appendix A.2 solves the two equations using a more complex model that is used when subchannel temperatures and densities are required. This model is used in the fuel assembly calculations

38

that we will describe further along. Let
h )=0 Ff (Tf

(7.1)

denote the nonlinear system of equations resulting from discretizing either h coolant model over the fluid domain with Tf a vector of fluid temperature unknowns. As described in Section 4.1, a JFNK solver only requires us to provide the ability to compute a nonlinear residual. Hence, augmenting the existing nonlinear system (3.6) of equations across the pellet and clad domains with eqns (7.1) for the flow domain enables us to perform coupled flow and thermal transport calculations. The augmented nonlinear system can be denoted by: ~ (T) = 0 F (7.2) t h h h h ~ (T) = F1 (T1 where F ), F2 (T2 ), . . . , FN (TN ), Fc (Tch ), Ff (Tf ) is the coupled set of block nonlinear equations across all pellet, clad, and flow domains with T denotes the block unknowns across all domains including flow. For preconditioning an augmented approximate Jacobian system of the form ~ = M 0 M ~f f 0 J (7.3)

~f f an approximate Jacobian for is used with M as defined in Eqn (4.7) and J the flow model. Since reduced order models are used for the flow, systems ~f f are inverted using a direct solver as opposed to the multigrid involving J solvers used to invert the other components. We note that the direct solver for the flow operates in parallel to the multigrid solves during each preconditioner solve step. Coupling to Oxide Growth Models: The formation of an oxide layer on the surface of a nuclear fuel rod can interfere with thermal conduction to the coolant and create additional stress within the clad. Typically the thickness of the oxide layer is much smaller than the other physical scales in the fuel rod. As a result the oxide growth is modeled at each point, x, on the surface of the clad by independent 1D models of the form: C (x, t)  C (x, t) = D ( x) t x x (7.4)

Solving for the oxygen concentration then reduces to solving equation 7.4 at each point on the surface of the clad subject to appropriate initial and boundary conditions which is described in detail in Appendix C. 39

For the purposes of this paper we consider a one way coupling of the full thermal transport model to the oxide models. As described previously a nonlinear solve is performed to convergence over the pellets, clad, and coolant domains. The resulting temperature field is used to initialize the oxide growth models at each point on an exterior clad surface mesh generated from the clad mesh using the mesh subset capability of AMP. Each point on the mesh contains an independent sub-grid oxide model which is distributed across the processors to match the clad load balance. For the results presented the clad runs on approximately half of the processors, with roughly the same number of surface nodes on each processor. Additionally since we have an independent model for every point the problem is embarrassingly parallel between the points and no additional communication is required. The results of the oxide model coupled to the thermal model is shown in Figure 14. The oxide layer thickness follows the surface temperature of the clad. This temperature is in turn affected by the power shape which can be seen by the fuel temperature and the flow temperature which creates a top-shifted peak to the temperature of the clad and oxide layer thickness. Coupled Radiation Transport and Fuel Assembly Thermal Modeling: In this section we describe the extension of the fuel rod modeling capability described to modeling a full nuclear assembly and coupling with a massively parallel radiation transport code, Denovo [29]. Here we focus on describing the coupling and scaling aspects and the interested reader is referred to [26, 27, 30­32] for further details including detailed verification and validation studies. Solving the fuel assembly problem consists of two parts: solving the neutronics equations to obtain the spatially varying source and solving the thermal diffusion with the appropriate heat sink. To accomplish this we solve the assembly level radiation transport equations using the methods described in section Appendix B, and an array of multiple fuel rod problems each of which is solved as described in this document. To accomplish the latter, we utilize a multi-mesh capability that can replicate a mesh to produce the appropriate array, and replicated column operators to produce a complete nonlinear system. The full nonlinear system is then solved using JFNK, with the preconditioner limited to the individual sub domains. The individual fuel rod physics are embarrassingly parallel between the fuel rods which we utilize through our load balance by generating a set of independent commu40

Figure 14: Oxide model results from left to right: Nuclear fuel pellet temperatures; Clad outer surface temperature; Coolant temperature; Resulting oxide layer thickness. nicators for each fuel rod, with the fuel rods distributed on independent sets of processors. We are not limited to this choice of parallel decomposition, but it yields the best performance for this problem. To demonstrate the ability to solve nuclear reactor analysis problems, we model a single fuel assembly of a pressurized water reactor with coupling between heat transfer, subchannel flow, and radiation transport. This corresponds to CASL AMA Progression Problem 6 [33]. The assembly consists of a 17 × 17 array of fuel rods on a square 1.26 cm pitch. Of the 289 pins, 264 are fuel rods containing 3.1% enriched UO2 , 24 are guide tube locations, and a central instrumentation tube. Zircaloy 4 clad surrounds all pins. The coolant surrounding the pins is water containing 1300 ppm soluble boron

41

and an inlet temperature of 569 K. The average power level in the assembly is 30,000 W/kg, approximately corresponding to an average power assembly from a full reactor core. Further details on the geometry and material specifications can be found in [33]. In the AMP computational model, the mesh for each fuel pellet contains 512 mesh cells each fuel rod contains 360 pellets. Additionally, each fuel rod is surrounded by a clad mesh containing 54,144 cells for a total of 238,464 cells per fuel rod. Over the 264 fuel pins in the full assembly, the total number of mesh cells in the AMP problem is approximately 63 million and the number of nodal degrees of freedom is slightly over 100 million. The Denovo computational model has approximately 4.6 million spatial cells, 23 energy groups, 32 angles, and P1 scattering (which uses four angular moments). The 23 group cross sections are collapsed from a 56 group library by the XSProc module of the SCALE package [34]. Power distributions computed by Denovo are mapped onto the AMP mesh using the polynomial smoothing process described in Appendix B. Temperatures and fluid densities computed by AMP are averaged over each of 49 axial levels within every fuel rod to be used for generating new cross sections. A simple Picard iteration is used to couple AMP and Denovo, with a damping (under-relaxation) factor of 0.4 applied to the temperature distribution, as described in Ref. [32]. The AMP thermal and subchannel problems are solved together using a JFNK approach as described in Appendix A and the Denovo k -eigenvalue problem is solved using a Krylov-Schur eigensolver [35]. A stopping criteria of 10-4 is applied to the Picard iterations and a tolerance of 10-5 for each of the AMP and Denovo subproblems. The problem was decomposed across 4624 computational cores, with both the AMP and Denovo problems utilizing the entire set of processors. The coupled problem converged in 12 Picard iterations, with an average of 1.6 Newton iterations per Picard iteration and 20 linear iterations per Newton step. The entire solution required 3976 seconds, of which 3182 seconds were spent in the Denovo transport solves and 680 seconds were spent in the AMP thermal solves. The temperature and power solution profiles throughout the assembly are shown in Fig. 15. The radial variation of the power distribution, both within individual fuel rods and across the assembly, is evident. Notably, the power level in pins neighboring guide tube locations is significantly higher than regions not near guide tubes (such as the assembly corners) due to increased neutron moderation. Although more difficult to visually discern, the fuel temperature distributions mirrors the same general trends as the 42

power, with higher temperatures corresponding to high power regions. Unlike the power, which attains its peak values at the outer radius of the fuel rods, the temperature distribution always peaks at or near the center of a fuel rod. The axial profiles clearly show the presence of the spacer grids as a series of local depressions due to increased absorption in those regions. 8. Conclusions Many real world engineering problems involve the complex interaction between many bodies, in a nonlinear manner. From mesh generation to predicting results, modeling these large complex systems presents significant computational challenges. An efficient parallel, multi-domain solution methodology has been developed and implemented to solve these systems by leveraging the natural decomposition of the problem associated with the individual domains. This methodology has been demonstrated for modeling heat transfer within nuclear fuel rods, which are composed of hundreds of individual pellets within a metal tube, and can be applied to a nuclear reactor, which is composed of tens of thousands of individual fuel pins. The model and discretization for the thermal transport in a nuclear fuel rod demonstration problem consists of nonlinear diffusion in each of the fuel pellets and clad, along with a Robin boundary condition on each surface, and maps between them, to account for the heat transfer between domains. Modeling the entire system in a single domain would create a significant challenge associated with mesh generation, but by modeling many individual domains, the generation of the mesh for the full problem becomes automatic and a negligible burden on the user. The Jacobian-Free Newton-Krylov (JFNK) method used to solve the nonlinear system of algebraic equations was described in detail, with a particular focus on the preconditioning strategy for the multi-domain problem. The Krylov solver has been shown to efficiently solve for the interaction between the individual domains, but an efficient preconditioner was required to account for the diffusion within individual domains. Therefore, the preconditioning algorithm leverages the natural decomposition in physical space through the use of a parallel block-diagonal structure in which each block is a single, physical domain that neglects the interaction between domains. To efficiently solve these multi-domain problems, the computational infrastructure of AMP was designed to support the problem specification, domain decomposition, and composition of mathematical constructs efficiently 43

(a) Radial Power Profile

(b) Radial Temperature Profile

(c) Axial Power Profile

(d) Axial Fuel Temperature Profile

(e) Clad and Coolant Axial Profiles

(f) 3D Temperature Profile

Figure 15: Assembly Solution Details 44

in parallel. The computational infrastructure was described in detail, with a particular focus on the parallel implementation. The infrastructure is designed to allow for the specification of nonlinear operators and linear preconditioners for individual domains and the composition of those operators into a single column operator that can be used by a solver, either internal or external, to compute the solution of the nonlinear problem. Similarly, the parallel vectors (solution and source) can be defined for individual domains and composed into a single vector that is used by the solvers and preconditioners. A "subsetting" approach has been developed to access individual components of the parallel vector that are associated with a single mesh (or surface of a mesh) and variable (such as temperature). Because there is a knowledge of the individual physical domains, parallel communicators are created for each domain and the interacting surfaces between domains. Therefore, the infrastructure can easily "subset" the parallel vector associated with an individual domain, determine a communicator specific to that domain, and access the preconditioned for that domain, which is provided to a solver for the compact preconditioning step on an individual domain. The domain decomposition strategy is designed with an awareness of the individual domains to minimize communication during the computationally-expensive preconditioning and apply processes. A series of numerical experiments were developed to verify, validate, and evaluate the parallel performance of the software and algorithm. Each of the numerical experiments were based on a single specification of material properties and geometry that was associated with a nuclear fuel performance experiment that is used as an international benchmark for fuel performance modeling. The numerical verification studies used the method of manufactured solutions and mesh refinement studies to verify the solution converged with the proper order to the manufactured solution. The validation study, specified in much greater detail in an associated manuscript, demonstrated that the full problem, from material model specification to discretization and solution, could accurately predict the temperature distribution within a nuclear fuel rod within the experimental uncertainty bounds. Both strong and weak scaling studies were performed with significant profiling, to understand the performance of the software, and algorithm. Excellent strong scaling was achieved as the processor count increased three orders of magnitude, until the number of elements per core was under 1000. This solution strategy, and the associated software, has been shown to accurately and efficiently solve large, complex, multi-domain, nonlinear prob45

lems by leveraging the natural structure of the physical domains. This has the potential to impact many computational engineering applications beyond nuclear fuel simulation. Acknowledgements and Access The AMP (Advanced Multi-Physics) code is distributed with a modified BSD license and accessible either by contacting the corresponding author or through the Radiation Safety Information Computational Center (RSICC) at Oak Ridge National Laboratory, with an RSICC license, as CCC793. The development of AMP, and the nuclear fuel performance application built upon it, was funded by the Nuclear Energy Advanced Modeling and Simulation (NEAMS) program of the U.S. Department of Energy Office of Nuclear Energy, Advanced Modeling and Simulation Office. This material is also based upon work supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program, Extreme Scale Solvers project (EASIR). Much appreciation is due to Aaron Phillippe, Jim Banfield, and Larry Ott for validation studies, to William Cochran, Srdjan Simunovic, Jay Billings, and Phani Nukala for their early contributions to the software, and to Mark Baird for computational support with the third party libraries. This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725. Mark Berrill acknowledges support from the Eugene P. Wigner Fellowship at Oak Ridge National Laboratory, managed by UTBattelle, LLC, for the U.S. Department of Energy under Contract DE-AC0500OR22725. References [1] http://energy.gov/sites/prod/files/styles/borealis photo gallery large respondxl2/public/fuel assembly for production of nuclear power.jpg. [2] G. A. Berna, C. E. Beyer, K. L. Davis, D. D. Lanning, "FRAPCON-3: A computer code for the calculation of steady-state, thermal-mechanical behavior of oxide fuel rods for high burnup", Tech. Rep. NUREG/CR6534, Pacific Northwest National Laboratory (1997). 46

[3] W. F. Lyon, M. N. Jahingir, R. O. Montgomery, "Fuel analysis and licensing code: FALCON MOD01: Volume 3: Verification and validation", Tech. Rep. 1011309, EPRI, Palo Alto, CA (2004). [4] C. Newman, G. Hansen, D. Gaston, "Three dimensional coupled simulation of thermomechanics, heat, and oxygen diffusion in UO2 nuclear fuel rods", Journal of Nuclear Materials. [5] G. Thouyenin, B. Michal, J. Sercombe, D. Planca, "Multidimensional modeling of a ramp test with the PWR fuel performance code ALCYONE", in: Proceedings of Top Fuel 2007, San Francisco, CA, 2007. [6] G. Thouyenin, J. M. Ricaud, D. Planca, P. Thevenin, "ALCYONE: the Pleiades fuel performance code dedicated to multidimensional PWR studies", in: Proceedings of Top Fuel 2006, Salamanaca, Spain, 2006. [7] A. Marino, G. Demarco, D. Brasnarof, F. Florido, "A 3D behavior modeling for design and performance analysis of LWR fuels", in: Proceedings of Top Fuel 2007, San Francisco, CA, 2007. [8] D. A. Knoll, D. E. Keyes, "Jacobian-free Newton-Krylov methods: a survey of approaches and applications", J. Comput. Phys. 193 (2004) 357­397. [9] R. S. Dembo, S. C. Eisenstat, T. Steihaug, "Inexact Newton methods", SIAM J. Numer. Anal. 19 (1982) 400­408. [10] S. C. Eisenstat, H. F. Walker, "Globally convergent inexact Newton methods", SIAM J. Optimization 4 (1994) 393­422. [11] C. T. Kelley, "Iterative methods for linear and nonlinear equations", SIAM, Philadelphia, 1995. [12] Y. Saad, M. Schultz, "GMRES: a generalized minimal residual algorithm for solving non-symetric linear systems", SIAM J. Sci. Stat. Comput. 7 (1986) 856­869. [13] P. McHugh, D. Knoll, "Comparison of standard and matrix-free implementations of several Newton-Krylov solvers", AIAA Journal 32 (12) (1994) 2394 ­ 2400.

47

[14] D. Knoll, P. McHugh, "Enhanced nonlinear iterative techniques applied to a nonequilibrium plasma flow", SIAM Journal on Scientific Computing 19 (1) (1998) 291 ­ 301. [15] K. T. Clarno, B. Philip, W. K. Cochran, R. S. Sampath, S. Allu, P. Barai, S. Simunovic, L. J. Ott, S. Pannala, P. Nukala, G. A. Dilts, B. Mihaila, C. Unal, G. Yesilyurt, J. H. Lee, J. E. Banfield, G. I. Maldonado, "The AMP (Advanced Multi-Physics) nuclear fuel performance code", Tech. Rep. ORNL/TM-2011/42, Oak Ridge National Laboratory (2011). [16] B. S. Kirk, J. W. Peterson, R. H. Stogner, G. F. Carey, libMesh: A C++ Library for Parallel Adaptive Mesh Refinement/Coarsening Simulations, Engineering with Computers 22 (3­4) (2006) 237­254, http://dx.doi. org/10.1007/s00366-006-0049-3. [17] http://trilinos.sandia.gov/packages/stk. [18] M. Heroux, R. Bartlett, V. H. R. Hoekstra, J. Hu, T. Kolda, R. Lehoucq, K. Long, R. Pawlowski, E. Phipps, A. Salinger, H. Thornquist, R. Tuminaro, J. Willenbring, A. Williams, An Overview of Trilinos, Tech. Rep. SAND2003-2927, Sandia National Laboratories (2003). [19] S. Balay, K. Buschelman, W. D. Gropp, D. Kaushik, M. G. Knepley, L. C. McInnes, B. F. Smith, H. Zhang, "PETSc home page", http: //www.mcs.anl.gov/petsc (2001). [20] https://computation.llnl.gov/casc/sundials/main.html. [21] http://www.oecd-nea.org/science/wprs/fuel/ifpelst.html. [22] http://www.oecd-nea.org/tools/abstract/detail/nea-1488. [23] http://www.oecd-nea.org/tools/abstract/detail/nea-1772. [24] A. Schubert, P. V. Uffelen, J. V. de Laar, C. Walker, W. Haeck, Extension of the transuranus burn-up model, Journal of Nuclear Materials 376 (2008) 1­10. [25] A. Phillippe, "A validation study of the AMP nuclear fuel performance code", Master's thesis, University of Tennessee-Knoxville (2012). 48

[26] A. M. Phillippe, K. T. Clarno, J. E. Banfield, L. J. Ott, B. Philip, M. A. Berrill, R. S. Sampath, S. Allu, S. P. Hamilton, A validation study of pin heat transfer for UO2 fuel based on the IFA-432 experiments, Nuclear Science and Engineering 177 (2014) 000­000. [27] A. M. Phillippe, K. T. Clarno, J. E. Banfield, L. J. Ott, B. Philip, M. A. Berrill, R. S. Sampath, S. Allu, S. P. Hamilton, A validation study of pin heat transfer for MOX fuel based on the IFA-597 experiments, Nuclear Science and Engineering 178 (2014) 171­200. [28] H. Childs, E. S. Brugger, K. S. Bonnell, J. S. Meredith, M. Miller, B. J. Whitlock, N. Max, A contract-based system for large data visualization, in: Proceedings of IEEE Visualization 2005, 2005, pp. 190­198. [29] T. Evans, A. Stafford, R. Slaybaugh, K. Clarno, DENOVO: A new three-dimensional parallel discrete ordinates code in SCALE, Nuclear Technology 171 (2010) 171­200. [30] K. T. Clarno, B. Philip, W. K. Cochran, R. S. Sampath, S. Allu, P. Barai, S. Simunovic, M. A. Berrill, L. J. Ott, S. Pannala, G. A. Dilts, B. Mihaila, G. Yesilyurt, J. H. Lee, J. E. Banfield, The AMP (Advanced MultiPhysics) nuclear fuel performance code, Nuclear Engineering and Design 252 (2012) 108­120. [31] S. Hamilton, K. Clarno, B. Philip, M. Berrill, R. Sampath, S. Allu, Integrated radiation transport and nuclear fuel performance for assemblylevel simulations, in: PHYSOR 2012: Advanced in Reactor Physics, Knoxville, TN, USA, 2012. [32] S. Hamilton, K. Clarno, M. Berrill, T. Evans, R. Sampath, Multiphysics simulations for LWR analysis, in: International Conference on Mathematics and Computational Methods Applied to Nuclear Science & Engineering (M&C 2013), Sun Valley, ID, USA, 2013. [33] S. Palmtag, Coupled single assembly solution with VERA (problem 6), Tech. Rep. CASL-U-2013-0150-000, Consortium for Advanced Simulation of LWR's (2013). [34] SCALE: A comprehensive modeling and simulation suite for nuclear safety analysis and design, Tech. Rep. ORNL/TM-2005/39, Version 6.1, Oak Ridge National Laboratory, Oak Ridge, TN (2011). 49

[35] G. W. Stewart, A Krylov­Schur algorithm for large eigenproblems, SIAM Journal on Matrix Analysis and Applications 23 (3) (2001) 601­ 614. [36] D. R. Rector, C. L. Wheeler, N. J. Lombardo, Cobra-sfs (spent fuel storage): A thermal-hydraulic analysis computer code: Volume 1, mathematical models and solution method, Tech. rep., PNL-6049-Vol.1 (1986). [37] E. E. Lewis, W. F. Miller, Jr., Computational Methods of Neutron Transport, American Nuclear Society, Inc., La Grange Park, Illinois, USA, 1993. [38] J. Jarrell, T. Evans, G. Davidson, A. Godfrey, Full core reactor analysis: Running Denovo on Jaguar, Nuclear Science and Engineering 175 (3) (2013) 283­291. [39] J. V. Cuthcart, et. al., Zirconium metal-water oxidation kinetics iv. reaction rate studies, Tech. rep., ORNL/NUREG-1 (1977). [40] R. Perkins, The diffusion of oxygen in oxygen stabilized alpha-zirconium and zircaloy-4, Journal of Nuclear Materials 73 (1978) 20­29. Appendix A. Coolant Modeling: The coolant liquid serves as a heat sink which flows axially along the length of the outside of the clad. Therefore simulating the coolant flow serves as a boundary condition for the thermal solve on the clad surface. Solving the coolant flow involves solving the fluid equations for conservation of mass, momentum, and energy equations:  +  · (v ) = 0 t vi = - · (vi v ) + (-p +  ·  ) - g t U +  · (U v ) = -p · v +  + k (T )T + q  t (A.1) (A.2) (A.3)

where  is the mass density, v is the velocity, p is the pressure, g is the force exerted by gravity,  is the viscosity tensor, U is the internal energy density,  is the dissipation function, k is the thermal conductivity, and q  is the thermal 50

source. The computational resources needed for a full 3D flow calculations are significant and are usually not necessary for an accurate calculation of the thermal solution within the pellets and clad. This allows us to use a two-equation approximation in which we assume that the coolant flow is only in the z-direction and neglect thermal diffusion between the channels. Assuming steady-state this reduces to: v =0 (A.4) z v 2 p + +g =0 (A.5) z z v  U v T = -p +  k (T ) +q  (A.6) z z z z We have two different models for solving these equations that can be used interchangeably. The first model described in Appendix A.1 uses a reduced empirical model that solves a single axial equation using a simple finite difference scheme and is frequently sufficient for many calculations within the nuclear engineering community. The second model described in Appendix A.2 is a more complex two equation model that is used when subchannel temperatures and densities are required. Appendix A.1. Single EquationFlow A standard reduced uniaxial model based on conservation of energy in the coolant (Equation A.7) with a given mass flux (G) and a specific heat capacity at constant pressure (Cp ) is employed. dTf + hf Tf = hf Tc , (A.7) GCp dz where · is the integral of · over the heated perimeter of the outer surface of the clad and Tf is the bulk coolant temperature. The conservation of coolant energy is solved on a 1D domain using a finite difference scheme. This 1D domain is divided into N equidistant grid points leading to a set of coupled equations with the i-th equation given by 4hf (z ) i- 1 i Tf - Tf + (T i - < Tci >)dz = 0 (A.8) i Cp (Tf )GDe f where hf is the Dittus-Boelter [2] film conductance given by hf = (0.023k/De )Re0.8 P r0.4 with Re the Reynolds number and P r the Prandtl number. 51 (A.9)

Appendix A.2. Subchannel Equations In this section we describe the model that approximates the distribution of flow, pressure, and temperature within a channel (the space between adjacent fuel rods) as uniaxial in the vertical direction and utilizes empirically-derived friction factors to account for the additional complexities. In our subchannel model we solve the 1D set of 2 equations described in section Appendix A. Solving these equations require two independent variables per grid point and we choose enthalpy and pressure. Note that the internal energy density is related to the enthalpy density through U = h - p. v p + v +g =0 z z hv p  T =v + k (T ) z z z z

(A.10) +q  (A.11)

At each axial layer, a simplified model where the crossflow terms are neglected thus eliminating the conservation of mass and lateral momentum equations is employed. This results in conservation of energy and axial momentum equations with specific enthalpy and pressure as variables using complex material models for the temperature, specific enthalpy, density, specific heat capacity, thermal conductivity, viscosity, and surface tension [36]. The following finite difference form of conservation of energy and axial momentum equation are given: m(hi,j + - hi,j - ) - zj + zj
k  K ( i)

(1 + i,r )Prheat i,r qc,j
r  R ( i)

t  wk,j (h i,j - hn,j ) + zj kK (i)

Ck,j sk (Ti,j - Tn,j ) = 0. (A.12)

m(ui,j + - ui,j - ) + Ai (pi,j + - pi,j - ) + + 1 2Ai

gAi zj cos   i,j
t wk,j (ui,j - un,j ) = 0. kK (i)

zj fi,j  + i,j |m| mi,j + C t zj Di

(A.13) 52

The heat flux qc,j is computed using a convective heat transfer coefficient hconv and the temperature difference between the clad surface and the i,j temperature of the flow in the center of the subchannel:
j qc,j = hconv i,j (< Tc > -Ti,j ).

(A.14)

The convective heat transfer coefficient is related to the Nusselt number N ui,j : hconv i,j Di N ui,j = (A.15) ki,j The Nusselt number will vary depending on the turbulence of the flow, so different models were developed for laminar and turbulent flow. To avoid convergence issues due to the discontinuity between the models, the effective heat transfer coefficient is taken as the maximum of the laminar heat transfer coefficient hi,j and the turbulent heat transfer coefficient ht i,j : hconv = max(hi,j , ht i,j i,j ), (A.16)

where the laminar heat transfer coefficient is evaluated with N ui,j = 8.0: hi,j = 8.0 ki,j , Di (A.17)

and the turbulent heat transfer coefficient is calculated as
0.8 0 .4 ht i,j = 0.023Rei,j P ri,j

ki,j Di

,

(A.18)

which uses the well-known Dittus-Boelter correlation for Nusselt number [A.9]: 0.8 0.4 N ui,j = 0.023Rei,j P ri,j , (A.19) where the Reynolds number and Prandtl number have their usual definitions: Rei,j cp i,j ui,j Di i,j µi,j = , P ri,j = µi,j ki,j (A.20)

As boundary conditions, axial inlet mass flow rates, inlet temperature and outlet pressure are selected.

53

Appendix B. Radiation Transport Model Nuclear fuel simulation requires a heat source and a heat sink. The heat sink is approximated with a closed-channel coolant flow model and the heat source is generated within an operating nuclear reactor primarily as a result of neutron-induced fission in fuel (primarily Uranium-235 in most reactors). The distribution of neutrons distribution requires the solution to the Boltzmann transport equation [37]. This is most often modeled using the k -eigenvalue form of the Boltzmann transport equation given by ^ ·  ( ^ , E ) +  (E ) ( ^ , E) 


=
0

dE
4 

^ s ( ^  ^ , E  E ) ( ^ ,E ) d dE
0 4

(B.1)

1 + (E ) k

^ f (E ) ( ^ ,E ) , d

^ is the direction of particle travel; E is the particle energy;  is the where  angular flux distribution; k is the multiplication factor of the system;  , s , and f are the total, scattering, and fission cross sections, respectively;  is the fission energy spectrum; and  is the number of neutrons produced per fission. For a given flux distribution, the power distribution can be written as  ^ f (E ) ( ^ ,E ) , P = dE d (B.2)
0 4

where  is the energy release per fission event. The Denovo radiation transport code [29] offers a variety of spatial discretizations solving the discrete ordinates [37] form of Eq. B.1 in parallel on Cartesian meshes. Denovo has demonstrated excellent scalability to high performance computing environments [38]. Nuclear data is generated using the XSProc module of the SCALE package [34]. Two approaches are available for transferring the power distribution from Denovo to AMP are possible. The first is a direct point-wise mapping of the Denovo solution onto the Gauss points of the AMP finite element basis functions. Interpolation to a point within Denovo can be piecewise constant, linear, or tri-linear depending on the spatial discretization used. The second approach uses a polynomial expansion of the power distribution within each cylindrical fuel rod (Zernike polynomials in the x - y plane and Legendre polynomials in the axial direction) to smooth out artifacts of the Denovo 54

Cartesian mesh, allowing a coarser spatial mesh to be used [31]. In both cases, conservation of the globally integrated power is enforced by normalizing the distribution before and after mapping the power onto the AMP mesh. Appendix C. Oxide Model: The formation of an oxide layer on the surface of a nuclear fuel rod can interfere with thermal conduction to the coolant and create additional stress within the clad. As shown in Figure C.16, the material regions of interest can be divided into 4 regions, a coolant region which is the source of the oxygen for oxide growth, the oxide layer itself which consists of Zirconium oxide, an oxygen rich alpha phase, and a normal beta phase region [39].

Coolant

Oxide

alpha

beta

Figure C.16: Sample layers for oxide growth

Within each layer the oxygen concentration is governed by thermal diffusion. Since the thickness of the oxide and alpha layers is much smaller than the other physical scales in the fuel rod, the oxide growth is modeled at each point, x, on the surface of the clad by independent 1D models:  C (x, t) C (x, t) = D ( x) t x x (C.1)

Solving for the oxygen concentration then reduces to solving in each region subject to the appropriate boundary conditions. Note that the boundaries between the different phases move as the different layers grow. Oxide layer growth: The oxide layers are moving domains in which the growth of each layer is given by the difference between the oxygen flux between the layers. Associated with each interface is the associated velocity of that interface. For example, the velocity of the oxide-alpha layer vox, is given by: vox, = Jox (xox, ) - J (xox, ) Cox, - C,ox 55 (C.2)

where Jox (xox, ) and J (xox, ) are the flux of oxygen in the oxide and  layers evaluated at the oxide interface and Cox, and C,ox are the oxygen concentrations at the boundaries of the oxide-alpha interface in the oxide and alpha layers. Boundary conditions: The boundary conditions between the different layers are relatively simple. At each interface, the oxygen concentration can be evaluated using the equilibrium value for the given phase. For example, at the oxide-coolant interface, the oxygen concentration is assumed to be 1.511 g/cm3 , derived from stoichiometry. The oxygen concentration in the oxide at the oxide-alpha interface is modeled by C = 1.517 - 7.5  10-5  T [39]. The oxygen concentration in the alpha layer at the alpha-oxide interface is a fixed 29% (atomic density) or 0.45 37 g/cm3 [40]. The oxygen concentration in the alpha layer at the alphabeta interface is calculated using the equilibrium concentration. This can be expressed as [40]: C = -0.2263 + 0.0649  C=0 T - 16.877 63.385 T >= 1123 K otherwise

Discretization: Since we are primarily conserned with the size of the oxide layers, we choose to solve the differential equation using finite difference in a moving frame.  dC (x, t) C (x, t) C (x, t) = (C.3) D ( x) +v· dt x x x We divide the space into N uniformly spaced regions of size h = (xN - x0 )/N . Since each zone is moving at a velocity vi , we can apply the convective derivative to get the diffusion equation in this moving frame. We can follow the boundaries of the oxide layer by choosing the velocity at the boundaries to match the oxide growth rate. For example consider the layers shown in Figure C.17. It's left boundary is moving at a velocity of v1 and it's right boundary is moving at a velocity of v2 . We want to use a conservative scheme with upwinding for the convective term. Assuming v1+1/2  0: 1 Di+1 Ci+ 3 - Ci+ 1 - Di Ci+ 1 - Ci- 1 2 2 2 2 dt h2 1 i+ 2 (vN - v0 ) vi+ 1 = v0 + 2 N
2

dCi+ 1

=

+

vi+ 1 h

2

C i+ 3 - C i + 1
2

2

56

Ci - 1
2

Ci + 1 vi

2

C i+ 3 vi+1

2

v0

vN

Figure C.17: Sample layers for numerical form

Rewriting: dC = F (C, v, h) dt 1 F (C, v, h) = 2 Di+1 Ci+ 3 - Ci+ 1 - Di Ci+ 1 - Ci- 1 2 2 2 2 h We can then apply the Crank-Nicholson method: C n+1 - C n 1 = F C n+1 , v n+1 , hn+1 + F (C n , v n , hn ) t 2 +1 +1 hn+1 = xn - xn /N 0 N
+1 n n+1 n xn = xn - v0 t2 0 0 + v0 t + 0.5 v0 +1 n+1 n n xn = xn - vN t2 N + vN t + 0.5 vN N

+

vi+ 1 h

2

C i+ 3 - C i+ 1
2 2

If we assume v n+1 is known (it is actually calculated from C n+1 ), then the system becomes a standard linear system of equations. The resulting matrix is banded and can be solved through direct solves using LAPACK. Time-Step Control: Careful control of the time step is necessary to produce an accurate answer. A first limitation of the time step is the calculation of v n+1 . While it is possible to create a non-linear system that solves for v n+1 , the other time step requirements will make this work unnecessary. Instead we will assume v n+1  v n . With this assumption we need a time step that will ensure the change in v is small. The second restriction comes from the convective term. To ensure the proper error, we need to limit the matrix h . norm (for this term) to  1. This gives us the condition t  v i 57

The IceProd Framework: Distributed Data Processing for the IceCube Neutrino Observatory
M. G. Aartsenb , R. Abbasiac , M. Ackermannat, J. Adamso , J. A. Aguilarw, M. Ahlersac , D. Altmannv , C. Arguellesac, J. Auffenbergac, X. Baiah,1 , M. Bakerac , S. W. Barwicky , V. Baumae , R. Bayg , J. J. Beattyq,r , J. Becker Tjusj , K.-H. Beckeras , S. BenZviac , P. Berghausat, D. Berleyp , E. Bernardiniat, A. Bernhardag, D. Z. Bessonaa , G. Binderh,g , D. Bindigas, M. Bissoka , E. Blaufussp , J. Blumenthala , D. J. Boersmaar , C. Bohmak , D. Boseam , S. B¨ oserk , ar m at o z e m ac O. Botner , L. Brayeur , H.-P. Bretz , A. M. Brown , R. Bruijn , J. Casey , M. Casier , D. Chirkin , A. Christovw , B. Christyp , K. Clarkan , L. Classenv , F. Clevermannt, S. Coendersa , S. Cohenz , D. F. Cowenaq,ap, A. H. Cruz Silvaat , M. Danningerak, J. Daughheteee, J. C. Davisq , M. Dayac , C. De Clercqm , S. De Ridderx , P. Desiatiac,, K. D. de Vriesm , M. de Withi , T. DeYoungaq, J. C. D´ iaz-V´ elezac,, M. Dunkmanaq, R. Eaganaq, B. Eberhardtae, j ac a ah B. Eichmann , J. Eisch , S. Euler , P. A. Evenson , O. Fadiranac,, A. R. Fazelyf , A. Fedynitchj, J. Feintzeigac, T. Feuselsx , K. Filimonovg, C. Finleyak , T. Fischer-Waselsas , S. Flisak , A. Franckowiakk, K. Frantzent , T. Fuchst , T. K. Gaisserah , J. Gallagherab, L. Gerhardth,g, L. Gladstoneac, T. Gl¨ usenkampat, A. Goldschmidth, G. Golupm , ah p v u J. G. Gonzalez , J. A. Goodman , D. G´ ora , D. T. Grandmont , D. Grantu , P. Gretskova, J. C. Grohaq, A. Großag , h,g x a C. Ha , A. Haj Ismail , P. Hallen , A. Hallgrenar, F. Halzenac , K. Hansonl , D. Hebeckerk, D. Heeremanl, D. Heinena , K. Helbingas , R. Hellauerp , S. Hickfordo, G. C. Hillb , K. D. Hoffmanp, R. Hoffmannas, A. Homeierk , K. Hoshinaac , F. Huangaq , W. Huelsnitzp , P. O. Hulthak , K. Hultqvistak , S. Hussainah , A. Ishiharan , E. Jacobiat , J. Jacobsenac , K. Jagielskia , G. S. Japaridzed , K. Jeroac , O. Jlelatix , B. Kaminskyat, A. Kappesv , T. Kargat, A. Karleac , M. Kauerac , J. L. Kelleyac, J. Kirylukal, J. Kl¨ asas , S. R. Kleinh,g , J.-H. K¨ ohnet , G. Kohnenaf, H. Kolanoskii , ae ac as s k L. K¨ opke , C. Kopper , S. Kopper , D. J. Koskinen , M. Kowalski , M. Krasbergac, A. Kriestena , K. Kringsa , G. Krollae , J. Kunnenm, N. Kurahashiac, T. Kuwabaraah, M. Labarex, H. Landsmanac, M. J. Larsonao, M. Lesiak-Bzdakal, M. Leuermanna, J. Leuteag , J. L¨ unemannae, O. Mac´ iaso , J. Madsenaj , G. Maggim , ac n h ac p R. Maruyama , K. Mase , H. S. Matis , F. McNally , K. Meagher , M. Merckac , G. Merinoac , T. Meuresl , S. Miareckih,g , E. Middellat , N. Milket , J. Millerm , L. Mohrmannat, T. Montaruliw,2, R. Morseac , R. Nahnhauerat, U. Naumannas, H. Niederhausenal, S. C. Nowickiu , D. R. Nygrenh , A. Obertackeas, S. Odrowskiu, A. Olivasp , A. Omairatas, A. O'Murchadhal, L. Paula , J. A. Pepperao, C. P´ erez de los Herosar , C. Pfendnerq, D. Pielotht , E. Pinatl , as g h aq J. Posselt , P. B. Price , G. T. Przybylski , M. Quinnan , L. R¨ adela , I. Raead,, M. Rameezw , K. Rawlinsc , P. Redlp , a ag t z R. Reimann , E. Resconi , W. Rhode , M. Ribordy , M. Richmanp , B. Riedelac , J. P. Rodriguesac, C. Rottam , T. Ruhet , B. Ruzybayevah, D. Ryckboschx, S. M. Sabaj , H.-G. Sanderae, M. Santanderac, S. Sarkars,ai, K. Schattoae, F. Scheriaut , T. Schmidtp , M. Schmitzt , S. Schoenena, S. Sch¨ onebergj, A. Sch¨ onwaldat, A. Schukrafta, L. Schultek , ac, ag ah ag aj D. Schultz , O. Schulz , D. Seckel , Y. Sestayo , S. Seunarine , R. Shanidzeat , C. Sherematau, M. W. E. Smithaq , D. Soldinas , G. M. Spiczakaj , C. Spieringat , M. Stamatikosq,3 , T. Stanevah , N. A. Stanishaaq , A. Stasikk , T. Stezelbergerh, R. G. Stokstadh , A. St¨ oßlat , E. A. Strahlerm , R. Str¨ omar , N. L. Strotjohannk, G. W. Sullivanp , ar e ah as H. Taavola , I. Taboada , A. Tamburro , A. Tepe , S. Ter-Antonyanf, G. Te si´ caq , S. Tilavah , P. A. Toaleao , ac ac v j k M. N. Tobin , S. Toscano , M. Tselengidou , E. Unger , M. Usner , S. Vallecorsaw , N. van Eijndhovenm, A. Van Overloopx, J. van Santenac , M. Vehringa, M. Vogek , M. Vraeghex , C. Walckak , T. Waldenmaieri , M. Wallraffa , Ch. Weaverac , M. Wellonsac , C. Wendtac , S. Westerhoffac , N. Whitehornac, K. Wiebeae , C. H. Wiebuscha , D. R. Williamsao , H. Wissingp , M. Wolfak , T. R. Woodu , K. Woschnaggg , D. L. Xuao , X. W. Xuf , J. P. Yanezat , G. Yodhy , S. Yoshidan , P. Zarzhitskyao, J. Ziemannt, S. Zierkea, M. Zollak
a III. b School

arXiv:1311.5904v3 [cs.DC] 22 Aug 2014

Physikalisches Institut, RWTH Aachen University, D-52056 Aachen, Germany of Chemistry & Physics, University of Adelaide, Adelaide SA, 5005 Australia

author corresponding author Email addresses: desiati@icecube.wisc.edu (P. Desiati), juancarlos@wipac.wisc.edu (J. C. D´ iaz-V´ elez), ofadiran@icecube.wisc.edu (O. Fadiran), ian@cs.wisc.edu (I. Rae), dschultz@icecube.wisc.edu (D. Schultz) 1 Physics Department, South Dakota School of Mines and Technology, Rapid City, SD 57701, USA 2 also Sezione INFN, Dipartimento di Fisica, I-70126, Bari, Italy 3 NASA Goddard Space Flight Center, Greenbelt, MD 20771, USA Preprint submitted to Journal of Parallel and Distributed Computing
 Principal

 Corresponding

August 26, 2014

of Physics and Astronomy, University of Alaska Anchorage, 3211 Providence Dr., Anchorage, AK 99508, USA d CTSPS, Clark-Atlanta University, Atlanta, GA 30314, USA e School of Physics and Center for Relativistic Astrophysics, Georgia Institute of Technology, Atlanta, GA 30332, USA f Dept. of Physics, Southern University, Baton Rouge, LA 70813, USA g Dept. of Physics, University of California, Berkeley, CA 94720, USA h Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA i Institut f¨ ur Physik, Humboldt-Universit¨ at zu Berlin, D-12489 Berlin, Germany j Fakult¨ at f¨ ur Physik & Astronomie, Ruhr-Universit¨ at Bochum, D-44780 Bochum, Germany k Physikalisches Institut, Universit¨ at Bonn, Nussallee 12, D-53115 Bonn, Germany l Universit´ e Libre de Bruxelles, Science Faculty CP230, B-1050 Brussels, Belgium m Vrije Universiteit Brussel, Dienst ELEM, B-1050 Brussels, Belgium n Dept. of Physics, Chiba University, Chiba 263-8522, Japan o Dept. of Physics and Astronomy, University of Canterbury, Private Bag 4800, Christchurch, New Zealand p Dept. of Physics, University of Maryland, College Park, MD 20742, USA q Dept. of Physics and Center for Cosmology and Astro-Particle Physics, Ohio State University, Columbus, OH 43210, USA r Dept. of Astronomy, Ohio State University, Columbus, OH 43210, USA s Niels Bohr Institute, University of Copenhagen, DK-2100 Copenhagen, Denmark t Dept. of Physics, TU Dortmund University, D-44221 Dortmund, Germany u Dept. of Physics, University of Alberta, Edmonton, Alberta, Canada T6G 2E1 v Erlangen Centre for Astroparticle Physics, Friedrich-Alexander-Universit¨ at Erlangen-N¨ urnberg, D-91058 Erlangen, Germany w D´ epartement de physique nucl´ eaire et corpusculaire, Universit´ e de Gen` eve, CH-1211 Gen` eve, Switzerland x Dept. of Physics and Astronomy, University of Gent, B-9000 Gent, Belgium y Dept. of Physics and Astronomy, University of California, Irvine, CA 92697, USA z Laboratory for High Energy Physics, Ecole ´ Polytechnique F´ ed´ erale, CH-1015 Lausanne, Switzerland aa Dept. of Physics and Astronomy, University of Kansas, Lawrence, KS 66045, USA ab Dept. of Astronomy, University of Wisconsin, Madison, WI 53706, USA ac Dept. of Physics and Wisconsin IceCube Particle Astrophysics Center, University of Wisconsin, Madison, WI 53706, USA ad Dept. of Computer Science, University of Wisconsin, Madison, WI 53706, USA ae Institute of Physics, University of Mainz, Staudinger Weg 7, D-55099 Mainz, Germany af Universit´ e de Mons, 7000 Mons, Belgium ag T.U. Munich, D-85748 Garching, Germany ah Bartol Research Institute and Dept. of Physics and Astronomy, University of Delaware, Newark, DE 19716, USA ai Dept. of Physics, University of Oxford, 1 Keble Road, Oxford OX1 3NP, UK aj Dept. of Physics, University of Wisconsin, River Falls, WI 54022, USA ak Oskar Klein Centre and Dept. of Physics, Stockholm University, SE-10691 Stockholm, Sweden al Dept. of Physics and Astronomy, Stony Brook University, Stony Brook, NY 11794-3800, USA am Dept. of Physics, Sungkyunkwan University, Suwon 440-746, Korea an Dept. of Physics, University of Toronto, Toronto, Ontario, Canada, M5S 1A7 ao Dept. of Physics and Astronomy, University of Alabama, Tuscaloosa, AL 35487, USA ap Dept. of Astronomy and Astrophysics, Pennsylvania State University, University Park, PA 16802, USA aq Dept. of Physics, Pennsylvania State University, University Park, PA 16802, USA ar Dept. of Physics and Astronomy, Uppsala University, Box 516, S-75120 Uppsala, Sweden as Dept. of Physics, University of Wuppertal, D-42119 Wuppertal, Germany at DESY, D-15735 Zeuthen, Germany

c Dept.

Abstract IceCube is a one-gigaton instrument located at the geographic South Pole, designed to detect cosmic neutrinos, identify the particle nature of dark matter, and study high-energy neutrinos themselves. Simulation of the IceCube detector and processing of data require a significant amount of computational resources. This paper presents the first detailed description of IceProd, a lightweight distributed management system designed to meet these requirements. It is driven by a central database in order to manage mass production of simulations and analysis of data produced by the IceCube detector. IceProd runs as a separate layer on top of other middleware and can take advantage of a variety of computing resources, including grids and batch systems such as CREAM, HTCondor, and PBS. This is accomplished by a set of dedicated daemons that process job submission in a coordinated fashion through the use of middleware plugins that serve to abstract the details of job submission and job management from the framework. Keywords: Data Management, Grid Computing, Monitoring, Distributed Computing 2

1. Introduction Large experimental collaborations often need to produce extensive volumes of computationally intensive Monte Carlo simulations and process vast amounts of data. These tasks are usually farmed out to large computing clusters or grids. For such large datasets, it is important to be able to document details associated with each task, such as software versions and parameters like the pseudo-random number generator seeds used for each dataset. Individual members of such collaborations might have access to modest computational resources that need to be coordinated for production. Such computational resources could also potentially be pooled in order to provide a single, more powerful, and more productive system that can be used by the entire collaboration. This article describes the design of a software package meant to address all of these concerns. It provides a simple way to coordinate processing and storage of large datasets by integrating grids and small clusters. 1.1. The IceCube Detector The IceCube detector shown in Figure 1 is located at the geographic South Pole and was completed at the end of 2010 [1, 2]. It consists of 5160 optical sensors buried between 1450 and 2450 meters below the surface of the South Pole ice sheet and is designed to detect interactions of neutrinos of astrophysical origin [1]. However, it is also sensitive to downward-going highly energetic muons and neutrinos produced in cosmic-rayinduced air showers. IceCube records 1010 cosmic-ray events per year. The cosmic-ray-induced muons outnumber neutrino-induced events (including ones from atmospheric origin) by about 500,000:1. They represent a background for most IceCube analyses and are filtered prior to transfer to the data processing center in the Northern Hemisphere. Filtering at the data collection source is required because of bandwidth limitations on the satellite connection between the detector and the processing location [3]. About 100 GB of data from the IceCube detector is transferred to the main data storage facility daily. In order to facilitate record keeping, the data is divided into runs, and each run is further subdivided into multiple files. The size of each file is dictated by what is considered optimal for storage and access. Each run typically consists of hundreds of files, resulting in 400,000 files for each year of detector operation. Once the data has been transferred, additional, more computationally-intensive event reconstructions are performed and the data is filtered to select events for various analyses. The computing requirements for 3

the various levels of data processing are shown in Table 1. In order to develop event reconstructions, perform analyses, and understand systematic uncertainties, physicists require statistics from Monte Carlo simulations that are comparable to the data collected by the detector. This requires thousands of years of CPU processing time as can be seen from Table 2. Table 1: Data processing demands. Data is filtered on 400 cores at the South Pole using loose selection criteria to reduce volume by a factor of 10 before satellite transfer to the Northern Hemisphere (Level1). Once in the North, more computationally intensive event reconstructions are performed in order to further reduce background contamination (Level2). Further event selections are made for each analysis channel (Level3). Each run is equivalent to approximately eight hours of detector livetime and the processing time is based on a 2.8 GHz core. Filter Level1 Level2 Level3 Processing time/run 2400 h 9500 h 15 h Total per year 2.6 × 106 h 1.0 × 107 h 1.6 × 104 h

Table 2: Runtime of various Monte Carlo simulations of background cosmic-ray shower events and neutrino signal with different energy distributions. The median energy is based on the distribution of events that trigger the detector. The number of events reflects the typical per-year requirements for IceCube analyses. Simulation Air showers Neutrinos Neutrinos Med. Energy1 1.2 × 104 GeV 3.9 × 106 GeV 8.1 × 101 GeV t/event 5 ms 316 ms 53 ms events 1014 108 109

1.2. IceCube Computing Resources The IceCube collaboration is comprised of 43 research institutions from Europe, North America, Japan, Australia, and New Zealand. Members of the collaboration have access to 25 different computing clusters and grids in Europe, Japan, Canada and the U.S. These range from small computer farms of 30 nodes to large grids, such as the European Grid Infrastructure (EGI), Swedish Grid Initiative (SweGrid), Canada's WestGrid and the Open Science Grid (OSG), that may each have
11

GeV = 109 electronvolts (unit of energy)

Figure 1: The IceCube detector: the dotted lines at the bottom represent the instrumented portion of the ice. The circles on the top surface represent IceTop, a surface air-shower subdetector.

4

thousands of computing nodes. The total number of nodes available to IceCube member institutions varies with time since much of our use is opportunistic and availability depends on the usage by other projects and experiments. In total, IceCube simulation has run on more than 11,000 distinct multicore computing nodes. On average, IceCube simulation production has run concurrently on 4, 000 cores at any given time since deployment, and it is anticipated to run on 5, 000 cores simultaneously during upcoming productions. 2. IceProd The IceProd framework is a software package developed for IceCube with the goal of managing productions across distributed systems and pooling together isolated computing resources that are scattered across member institutions of the Collaboration and beyond. It consists of a central database and a set of daemons that are responsible for the management of grid jobs and data handling through the use of existing grid technology and network protocols. IceProd makes job scripting easier and sharing productions more efficient. In many ways it is similar to PANDA Grid, the analysis framework for the PANDA experiment [4], in that both tools are distributed systems based on a central database and an interface to local batch systems. Unlike PANDA Grid which depends heavily on AliEn, the grid middleware for the ALICE experiment [5], and on the ROOT analysis framework [6], IceProd was built in-house with minimal software requirements and is not dependent on any particular middleware or analysis framework. It is designed to run completely in user space with no administrative access, allowing greater flexibility in installation. IceProd also includes a built-in monitoring system with no dependencies on any external tools for this purpose. These properties make IceProd a very lightweight yet powerful tool and give it a greater scope beyond IceCube-specific applications. The software package includes a set of libraries, executables and daemons that communicate with the central database and coordinate to share responsibility for the completion of tasks. The details of job submission and management in different grid environments are abstracted through the use of plugin modules that will be discussed in Section 3.2.1. IceProd can be used to integrate an arbitrary number of sites including clusters and grids. It is, however, not a replacement for other cluster and grid management tools or any other middleware. Instead, it runs on top of 5

these as a separate layer providing additional functionality. IceProd fills a gap between the user or production manager and the powerful middleware and batch system tools available on computing clusters and grids. Many of the existing middleware tools, including Condor-C, Globus and CREAM, make it possible to interface any number of computing clusters into a larger pool. However, most of these tools need to be installed and configured by system administrators and, in some cases, customization for general purpose applications is not feasible. In contrast to most of these applications, IceProd runs at the user level and does not require administrator privileges. This makes it possible for individual users to build large production systems by pooling small computational resources together. Security and data integrity are concerns in any software architecture that depends heavily on communication through the Internet. IceProd includes features aimed at minimizing security and data corruption risks. Security and data integrity are addressed in Section 3.8. The IceProd client provides a graphical user interface (GUI) for configuring simulations and submitting jobs through a "production server." It provides a method for recording all the software versions, physics parameters, system settings, and other steering parameters associated with a job in a central production database. IceProd also includes a web interface for visualization and live monitoring of datasets. Details about the GUI client and a text-based client are discussed in Section 3.5.

3. Design Elements of IceProd The IceProd software package can be logically divided into the following components or software libraries: · iceprod-core--a set of modules and libraries of common use throughout IceProd. · iceprod-server--a collection of daemons and libraries to manage and schedule job submission and monitoring. · iceprod-modules--a collection of predefined classes that provide an interface between IceProd and an arbitrary task to be performed on a computing node, as defined in Section 3.3. · iceprod-client--a client (both graphical and text) that can download, edit, and submit dataset steering files to be processed.

· A database that stores configured parameters, libraries (including version information), job information, and performance statistics. · A web application for monitoring and controlling dataset processing. These components are described in further detail in the following sections. 3.1. IceProd Core Package The iceprod-core package contains modules and libraries common to all other IceProd packages. These include classes and methods for writing and parsing XML files and transporting data. The classes that define job execution on a host are contained in this package. The iceprod-core also includes an interpreter (Section 3.1.3) for a simple scripting language that provides some flexibility for parsing XML steering files. 3.1.1. The JEP One of the complications of operating on heterogeneous systems is the diversity of architectures, operating systems, and compilers. IceProd uses HTCondor's NMI-Metronome build and test system [7] for building the IceCube software on a variety of platforms and storing the built packages on a server. As part of the management of each job, IceProd submits a Job Execution Pilot (JEP) to the cluster/grid queue. This script determines what platform a job is running on and, after contacting the monitoring server, which software package to download and execute. During runtime, the JEP performs status updates through the monitoring server via remote procedure calls using XML-RPC [8]. This information is updated on the database and is displayed on the monitoring web interface. Upon completion, the JEP removes temporary files and directories created for the job. Depending on the configuration, it will also cache a copy of the software used, making it available for future JEPs. When caching is enabled, an MD5 checksum is performed on the cached software and compared to what is stored on the server in order to avoid using corrupted or outdated software. Jobs can fail under many circumstances. These failures include failed submissions due to transient system problems and execution failures due to problems with the execution host. At a higher level, errors specific to IceProd include communication problems with the monitoring daemon or the data repository. In order to account for possible transient errors, the design of IceProd includes a set of states through which a job will transition in order to guarantee successful completion of 6

Submit Start True

WAITING

QUEUEING

ok?

QUEUED

PROCESSING

CLEANING

Max. time reached

False

False

RESET SUSPENDED
requeue

ERROR

ok?
True

False

True

OK

ok?

CLEANING

COPIED

Move data to disk

Figure 2: State diagram for the JEP. Each of the nonerror states through which a job passes includes a configurable timeout. The purpose of this timeout is to account for any communication errors that may have prevented a job from setting its status correctly.

a well-configured job. The state diagram for an IceProd job is depicted in Figure 2. 3.1.2. XML Job Description In the context of this document, a dataset is defined as a collection of jobs that share a basic set of scripts and software but whose input parameters depend on the ID of each individual job. A configuration or steering file describes the tasks to be executed for an entire dataset. IceProd steering files are XML documents with a defined schema. These steering files include information about the specific software versions used for each of the sections, known as trays (a term borrowed from IceTray, the C++ software framework used by the IceCube Collaboration [9]). An IceProd tray represents an instance of an environment corresponding to a set of libraries and executables and a chain of configurable modules with corresponding parameters and input files needed for the job. In addition, there is a header section for user-defined parameters and expressions that are globally accessible by different modules. 3.1.3. IceProd XML expressions A limited programming language was developed in order to allow more scripting flexibility that depends on runtime parameters such as job ID, run ID, and dataset ID. This lightweight, embedded, domain-specific language (DSL) allows for a single XML job description to be applied to an entire dataset following an SPMD (single process, multiple data) paradigm. It is powerful enough to give some flexibility but sufficiently re-

strictive to limit abuse. Examples of valid expressions include the following: · $args(<var>)--a command line argument passed to the job (such as job ID or dataset ID). · $steering(<var>)--a user defined variable. · $system(<var>)--a system-specific parameter defined by the server. · $eval(<expr>)--a mathematical or logical expression (in Python). · $sprintf(<format>,<list>)--string formatting. · $choice(<list>)--random choice of an element from the list. The evaluation of such expressions is recursive and allows for some complexity. However, there are limitations in place that prevent abuse of this feature. As an example, $eval() statements prohibit such things as loops and import statements that would allow the user to write an entire program within an expression. There is also a limit on the number of recursions in order to prevent closed loops in recursive statements. 3.2. IceProd Server

There are two modes of operation. The first is an unmonitored mode in which jobs are simply sent to the queue of a particular system. This mode provides a tool for scheduling jobs that don't need to be recorded and does not require a database. In the second mode, all parameters are stored in a database that also tracks the progress of each job. The soapqueue daemon running at each of the participating sites periodically queries the database to check if any tasks have been assigned to it. It then downloads the steering configuration and submits a given number of jobs to the cluster or grid where it is running. The number of jobs that IceProd maintains in the queue at each site can be configured individually according to the specifics of each cluster, including the size of the cluster and local queuing policies. Figure 3 is a graphical representation that describes the interrelation of these daemons. The state diagram in Figure 4 illustrates the role of the daemons in dataset submission while Figure 5 illustrates the flow of information through the various protocols.
False True

soaptray

monitor?

Job

monitor?

soapmon

True

Client

Database
True

soapqueue

jobs?

False

The iceprod-server package is comprised of four daemons and their respective libraries: 1. soaptray --an HTTP server that receives client XML-RPC requests for scheduling jobs and steering information which then uploaded to the database. 2. soapqueue--a daemon that queries the database for available tasks to be submitted to a particular cluster or grid. This daemon is also responsible for submitting jobs to the cluster or grid through a set of plugin classes. 3. soapmon--a monitoring HTTP server that receives XML-RPC updates from jobs during execution and performs status updates to the database. 4. soapdh--a data handling/garbage collection daemon that removes temporary files and performs any postprocessing tasks.
1 The prefix soap is used for historical reasons. The original implementation of IceProd relied on SOAP for remote procedure calls. This was replaced by XML-RPC which has better support in Python.

False

True ok?

requeue

Move data to data warehouse

1

Figure 4: State diagram of queuing algorithm. The iceprod-client sends requests to the soaptray server which then loads the information to the database (in production mode) or directly submits jobs to the cluster (in unmonitored mode). The soapqueue daemons periodically query the database for pending requests and handle job submission in the local cluster.

3.2.1. IceProd Server Plugins In order to abstract the process of job submission from the framework for the various types of systems, IceProd defines a Grid base class that provides an interface for queuing jobs. The Grid base class interface includes a set of methods for queuing and removing jobs, performing status checks, and setting attributes such as job priority and maximum allowed wall time and job requirements such as disk space and memory usage. The 7

Figure 3: Network diagram of IceProd system. The IceProd clients and JEPs communicate with iceprod-server modules via XML-RPC. Database calls are restricted to iceprod-server modules. Queueing daemons called soapqueue are installed at each site and periodically query the database for pending job requests. The soapmon server receives monitoring update from the jobs. An instance of soapdh handles garbage collection and any post processing tasks after job completion.
client soaptray soapqueue
batch system submit cmd* (unmonitored) submit jobs MySQL enqueue dataset batch system* submit cmd submit job

batch system

running job

soapmon

soapdh

XML-RPC submit

batch system protocol* schedule job on cluster XML-RPC status update MySQL MySQL status update

XML-RPC check status batch system protocol*/shell remove failed job and clean up files *Condor, PBS, SGE, CREAM, GLite

status update batch system protocol*/shell remove completed job and clean up files

Figure 5: Data flow for job submission, monitoring and removal. Communication between server instances (labeled "soap*") is handled through a database. Client/server communication and monitoring updates are handled via XMLRPC. Interaction with the grid or cluster is handled through a set of plugin modules and depends on the specifics of the system. set of methods defined by this base class include but are 8 not limited to:

· WriteConfig: write protocol-specific submission scripts (i.e., a JDL job description file in the case of CREAM or gLite or a shell script with the appropriate PBS/SGE headers). · Submit: submit jobs and record the job ID in the local queue. · CheckJobStatus: query job status from the queue. · Remove: cancel/abort a job. · CleanQ: remove any orphan jobs that might be left in the queue. The actual implementation of these methods is done by a set of plugin subclasses that launch the corresponding commands or library calls, as the case may be. In the case of PBS and SGE, most of these methods result in the appropriate system calls to qsub, qstat, qdel, etc. For other systems, these can be direct library calls through a Python API. IceProd contains a growing library of plugins, including classes for interfacing with batch systems such as HTCondor, PBS and SGE as well as grid systems like Globus, gLite, EDG, CREAM and ARC. In addition, one can easily implement user-defined plugins for any new type of system that is not included in this list. 3.3. IceProd Modules The iceprod-modules package is a collection of configurable modules with a common interface. These represent the atomic tasks to be performed as part of the job. They are derived from a base class IPModule and provide a standard interface that allows for an arbitrary set of parameters to be configured in the XML document and passed from the IceProd framework. In turn, the module returns a set of statistics in the form of a string-to-float dictionary back to the framework so that it can be recorded in the database and displayed on the monitoring web interface. By default, the base class will report the module's CPU usage, but the user can define any set of values to be reported, such as number of events that pass a given processing filter. IceProd also includes a library of predefined modules for performing common tasks such as file transfers through GridFTP, tarball manipulation, etc. 3.4. External IceProd Modules Included in the library of predefined modules is a special module that has two parameters: class and URL. The first is a string that defines the name of an external IceProd module and the second specifies a URL for a 9

(preferably version-controlled) repository where the external module code can be found. Any other parameters passed to this module are assumed to belong to the referred external module and will be ignored. This allows for the use of user-defined modules without the need to install them at each IceProd site. External modules share the same interface as any other IceProd module. External modules are retrieved and cached by the server at the time of submission. These modules are then included as file dependencies for the jobs, thus preventing the need for jobs to directly access the file code repository. Additional precautions, such as enforcing the use of secure protocols for URLs, must be taken to avoid security risks. 3.5. IceProd Client The iceprod-client package contains two applications for interacting with the server and submitting datasets. One is a PyGTK-based GUI (see Figure 6) and the other is a text-based application that can run as a commandline executable or as an interactive shell. Both of these applications allow the user to download, edit, and submit steering configuration files as well as control datasets running on the IceProd-controlled grid. The graphical interface includes drag and drop features for moving modules around and provides the user with a list of valid parameters for known modules. Information about parameters for external modules is not included since these are not known a priori. The interactive shell also allows the user to perform grid management tasks such as starting and stopping a remote server and adding and removing production sites participating in the processing of a dataset. The user can also perform job-specific actions such as suspension and resetting of jobs. 3.6. Database At the time of this writing, the current implementation of IceProd works exclusively with a MySQL database, but all database calls are handled by a database module that abstracts queries from the framework and could be easily replaced by a different relational database. This section describes the relational structure of the IceProd database. Each dataset is defined by a set of modules and parameters that operate on separate data (single process, multiple data). At the top level of the database structure is the dataset table. The dataset ID is the unique identifier for each dataset, though it is possible to assign a mnemonic string alias. The tables in the IceProd database are logically divided into two distinct classes

Figure 6: The iceprod-client uses pyGtk and provides a graphical user interface to IceProd. It is both a graphical editor of XML steering files and an XML-RPC client for dataset submission. that could in principle be entirely different databases. The first describes a steering file or dataset configuration (items 1­6 and 9 in the list below) and the second is a job-monitoring database (items 7 and 8). The most important tables are described below. 1. dataset: contains a unique identifier as well as attributes to describe and categorize the dataset, including a textual description. 2. steering-parameter: describes general global variables that can be referenced from any module. 3. meta-project: describes a software environment including libraries and executables. 4. tray: describes a grouping of modules that will execute given the same software environment or metaproject. 5. module: specifies an instance of an IceProd Module class. 6. cparameter: contains all the configured parameters associated with a module. 7. job: describes each job in the queue related to a dataset, including the state and host where the job is executed. 10 8. task: keeps track of the state of a task in a way similar to what is done in the jobs table. A task represents a subprocess for a job in a process workflow. More details on this will be provided in Section 4. 9. task-rel: describes the hierarchical relationship between tasks. 3.7. Monitoring The status updates and statistics are reported by the JEP via XML-RPC to soapmon and stored in the database, and provide useful information for monitoring the progress of processing datasets and for detecting errors. The updates include status changes and information about the execution host as well as job statistics. This is a multi-threaded server that can run as a standalone daemon or as a CGI script within a more robust web server. The data collected from each job are made available for analysis, and patterns can be detected with the aid of visualization tools as described in the following section. 3.7.1. Web Interface The current web interface for IceProd was designed to work independently of the IceProd framework but

Figure 7: A screen capture of the web interface that allows the monitoring of ongoing jobs and datasets. The monitoring web interface has a number of views with different levels of detail. The view shown displays the job progress for active jobs within a dataset. The web interface provides authenticated users with buttons to control datasets and individual jobs. utilizes the same database. It is written in PHP and makes use of the CodeIgniter framework [10]. Each of the simulation and data-processing web-monitoring tools provide different views, which include, from top level downward: · general view: displays all datasets filtered by status, type, grid, etc. · grid view: shows all datasets running on a particular site. · dataset view: displays all jobs and accompanying statistics for a given dataset, including every site that it is running on. · job view: shows each individual job, including the status, job statistics, execution host, and possible errors. There are some additional views that are applicable only to the processing of real IceCube detector data: · calendar view: displays a calendar with a color coding that indicates the status of jobs associated with data taken on a particular date. · day view: shows the status of jobs associated with a given calendar day of data taking. 11 · run view: displays the status of jobs associated with a particular detector run. The web interface also provides the functionality to control jobs and datasets by authenticated users. This is done by sending commands to the soaptray daemon using the XML-RPC protocol. Other features of the interface include graphs displaying completion rates, errors and number of jobs in various states. Figure 7 shows a screen capture of one of a number of views from the web interface.

3.7.2. Statistical Data One aspect of IceProd that is not found in most grid middleware is the built-in collection of user-defined statistical data. Each IPModule instance is passed a stringto-float dictionary to which the JEP can add entries or increment a given value. IceProd collects these data in the central database and displays them on the monitoring page. Statistics are reported individually for each job and collectively for the whole dataset as a sum, average and standard deviation. The typical types of information collected on IceCube jobs include CPU usage, number of events meeting predefined physics criteria, and number of calls to a particular module.

3.8. Security and Data Integrity When dealing with network applications, one must always be concerned with security and data integrity in order to avoid compromising privacy and the validity of scientific results. Some effort has been made to minimize security risks in the design and implementation of IceProd. This section will summarize the most significant of these. Figure 3 shows the various types of network communication between the client, server, and worker node. 3.8.1. Authentication Authentication in IceProd can be handled in two ways: IceProd can authenticate dataset submission against an LDAP server or, if one is not available, authentication is handled by means of direct database authentication. LDAP authentication allows the IceProd administrator to restrict usage to individual users that are responsible for job submissions and are accountable for improper use so direct database authentication should be disabled whenever LDAP is available. This setup also precludes the need to distribute database passwords and thus prevents users from being able to directly query the database via a MySQL client. When dealing with databases, one also needs to be concerned about allowing direct access to the database and passing login credentials to jobs running on remote sites. For this reason, all monitoring calls are done via XML-RPC, and the only direct queries are performed by the server, which typically operates behind a firewall on a trusted system. The current web interface does make direct queries to the database; a dedicated readonly account is used for this purpose. 3.8.2. Encryption Both soaptray and soapmon can be configured to use SSL certificates in order to encrypt all data communication between client and server. The encryption is done by the HTTPS server with either a self-signed certificate or, preferably, with a certificate signed by a trusted Certificate Authority (CA). This is recommended for clientserver communication for soaptray but is generally not considered necessary for monitoring information sent to soapmon by the JEP as this is not considered sensitive enough to justify the additional system CPU resources required for encryption. 3.8.3. Data Integrity In order to guarantee data integrity, an MD5 checksum or digest is generated for each file that is transmitted. This information is stored in the database and 12

is checked against the file after transfer. IceProd data transfers support several protocols, but the preference is to rely primarily on GridFTP, which makes use of GSI authentication [11, 12]. An additional security measure is the use of a temporary passkey that is assigned to each job at the time of submission. This passkey is used for authenticating communication between the job and the monitoring server and is only valid during the duration of the job. If the job is reset, this passkey will be changed before a new job is submitted. This prevents stale jobs that might be left running from making monitoring updates after the job has been reassigned. 4. Intrajob Parallelism As described in Section 3.1.2, a single IceProd job consists of a number of trays and modules that execute different parts of the job, for example, a simulation chain. These trays and modules describe a workflow with a set of interdependencies, where the output from some modules and trays is used as input to others. Initial versions of IceProd ran jobs solely as monolithic scripts that executed these modules serially on a single machine. This approach was not very efficient because it did not take advantage of the workflow structure implicit in the job description. To address this issue, IceProd includes a representation of a job as a directed, acyclic graph (DAG) of tasks. Jobs are recharacterized as groups of arbitrary tasks and modules that are defined by users in a job's XML steering file, and each task can depend on any number of other tasks in the job. This workflow is encoded in a DAG, where each vertex represents a single instance of a task to be executed on a computing node, and edges in the graph indicate dependencies between tasks (see Figures 8 and 9). DAG jobs on the cluster are executed by means of the HTCondor DAGMan which is a workflow manager developed by the HTCondor group at the University of Wisconsin­Madison and included with the HTCondor batch system [13]. For IceCube simulation production, IceProd has utilized the DAG support in two specific cases: improving task-level parallelism and running jobs that utilize graphics processing units (GPUs) for portions of their processing. 4.1. Task-level Parallelism In addition to problems caused by coarse-grained requirements specifications, monolithic jobs also underutilize cluster resources. As shown in Figure 8, portions

!"#$%&'()*

64%)"5

!"#$%&'()*

64%)"5

!"#$%&'()*

64%)"5

!"#$%&'()*

64%)"5

!"#$%&'()*

64%)"5

+,-

+,-

+,-

+,-

+,-

*./.#/'&01

*./.#/'&01

*./.#/'&01

*./.#/'&01

*./.#/'&02

*./.#/'&02

*./.#/'&02

*./.#/'&02

*./.#/'&01

*./.#/'&02

*"/"0345/.&0!

*"/"0345/.&0"

%"&!"%.0#'55.#/4')

Figure 9: A more complicated DAG in IceProd with multiple inputs and multiple outputs that are eventually merged into a single output. The vertices in the second level run on computing nodes equipped with GPUs. PBS and Sun Grid Engine plugins. This enables faster execution of individual jobs by utilizing more computing nodes; however, one limitation of this implementation is that DAG jobs are restricted to a specific type of cluster, and DAG jobs cannot distribute tasks across multiple sites. 4.2. DAGs Based on System Requirements
detector A detector B

background

signal

GPU

garbage collection

Figure 8: A simple DAG in IceProd. This DAG corresponds to a typical IceCube simulation. The two root vertices require standard computing hardware and produce different types of signal. Their output is then combined and processed on GPUs. The output is then used as input for two different detector simulations.

of the workflow within a job are independent; however, if a job is monolithic, these portions will be run serially instead of in parallel. Therefore, although the entire simulation can be parallelized by submitting multiple jobs to different machines, this opportunity for additional parallelism is not exploited by monolithic jobs. Support for breaking a job into discrete tasks is now included in the HTCondor IceProd plugin as described above, and similar features have been developed for the 13

Individual parts of a job may have different system hardware and software requirements. Breaking these up into tasks that run on separate nodes allows for better utilization of resources. The IceCube detector simulation chain is a good example of this scenario in which tasks are distributed across computing nodes with different hardware resources. Light propagation in the instrumented volume of ice at the South Pole is difficult to model, but recent developments in IceCube's simulation include a much faster approach for simulating direct propagation of photons in the optically complex Antarctic ice [14, 15] by using general-purpose GPUs. This new simulation module is much faster than a CPU-based implementation and more accurate than using parametrization tables [16], but the rest of the simulation requires standard CPUs. When executing an IceProd job monolithically, only one set of cluster requirements can be applied when it is submitted to the cluster. Accordingly, if any part of the job requires use of a GPU, the entire monolithic job must be scheduled on a cluster machine with the appropriate hardware. As of this writing, IceCube has the potential to access 20, 000 CPU cores distributed throughout the world, but only a small number of these nodes are equipped with GPU cards. Because the simulation is primarily

CPU bound, the pool of GPU-equipped nodes is not sufficient to run all simulation jobs in an acceptable amount of time. Additionally, this would be an inefficient use of resources, since executing the CPU-oriented portions of monolithic jobs would leave the GPU idle for periods of time. In order to solve this problem, the modular design of the IceCube simulation design is used to divide the CPU- and GPU-oriented portions of jobs into separate tasks in a DAG. Since each task in a DAG is submitted separately to the cluster, their requirements can be specified independently and CPU-oriented tasks can be executed on general-purpose grid nodes while photon propagation tasks can be executed on GPU-enabled machines, as depicted in Figure 9. 5. Applications IceProd's highly configurable nature lets it serve the needs of many different applications, both inside and beyond the IceCube Collaboration. 5.1. IceCube Simulation Production The IceCube simulations are based on a modular software framework called IceTray in which modules are executed in sequential order. Data is passed between modules in the form of a "frame" object. IceCube simulation modules represent different steps in the generation and propagation of particles, in-ice light propagation, signal detection, and simulation of the electronics and data acquisition hardware. These modules are "chained" together in a single IceTray instance but can also be broken into separate instances configured to write intermediate data files. This allows for breaking up the simulation chain into multiple IceProd tasks in order to optimize the use of resources as described in Section 4. For IceCube, Monte Carlo simulations are the most computationally intensive task, which is dominated by the production of background cosmic-ray showers (see Table 2). A typical Monte Carlo simulation lasts on the order of 8 hours but corresponds to only four seconds of detector livetime. In order to generate sufficient statistics, IceCube simulation production needs to make use of available computing resources which are distributed across the world. Table 3 lists all of the sites that have participated in Monte Carlo production. 5.2. Off-line Processing of the IceCube Detector Data IceProd was designed primarily for managing the production of Monte Carlo simulations for IceCube, 14

Table 3: Sites participating in IceCube Monte Carlo production by country.
Country Sweden Canada Germany Queue Type ARC PBS SGE PBS CREAM PBS HTCondor PBS SGE HTCondor No. of Sites 2 2 1 3 4 2 4 3 4 1

Belgium USA

Japan

but it has also been successfully adopted for managing the processing and reconstruction of experimental data collected by the detector. This data collected by IceCube and previously described in Section 1.1 must undergo multiple steps of processing, including calibration, multiple-event track reconstructions, and sorting into various analysis channels based on predefined criteria. IceProd has proved to be an ideal framework for processing this large volume of data. For off-line data processing, the existing features in IceProd are used for job submission, monitoring, data transfer, verification, and error handling. However, in contrast to a Monte Carlo production dataset where the number of jobs are defined a priori, a configuration for off-line processing of experimental data initiates with an empty dataset of zero jobs. A separate script is then run over the data in order to map a job to a particular file (or group of files) and to generate MD5 checksums for each input file. Additional minor modifications were needed in order to support the desired features in off-line processing. In addition to the tables described in section 3.6, a run table was created to keep records of runs and dates associated with each file and unique to the data storage structure. All data collected during a season (or a one year cycle) are processed as a single IceProd dataset. This is because, for each IceCube season, all the data collected is processed with the same set of scripts, thus following the SPMD model. A job for such a dataset consists of all the tasks needed to complete the processing of a single data file. Off-line processing takes advantage of the IceProd built-in system for collecting statistics in order to provide information through web interface about the number of events that pass different quality selection criteria from completed jobs. Troubleshooting and error cor-

rection of jobs during processing is also facilitated by IceProd's real-time feedback system accessible through the web interface. The data integrity checks discussed in Section 3.8.3 also provide a convenient way to validate data written to storage and to check for errors during the file transfer task. 5.3. Off-line Event Reconstruction for the HAWC Gamma-Ray Observatory IceProd's scope is not limited to IceCube. Its design is general enough to be used for other applications. The High-Altitude Water Cherenkov (HAWC) Observatory [17] has recently begun using IceProd for its own offline event reconstruction and data transfer [18]. HAWC has two main computing centers, one located at the University of Maryland and one at UNAM in Mexico City. Data is collected from the detector in Mexico and then replicated to UMD. The event reconstruction for HAWC is similar in nature to IceCube's data processing. Unlike IceCube's Monte Carlo production, it is I/O bound and better suited for a local cluster rather than a distributed grid environment. The HAWC Collaboration has made important contributions to the development of IceProd and maintained active collaboration with the development team. 5.4. Deploying an IceProd Site Deployment of an IceProd instance is relatively easy. Installation of the software packages is handled through Python's built-in Module Distribution Utilities package. If the intent is to create a stand-alone instance or to start a new grid, the software distribution also includes scripts that define the MySQL tables required for IceProd. After the software is installed, the server needs to be configured through an INI-style file. This configuration file contains three main sections: general queueing options, site-specific system parameters, and job environment. The queueing options are used by the server plugin to help configure submission (e.g. selecting a queue or passing custom directives to the queueing system). System parameters can be used to define the location of a download directory on a shared filesystem or a scratch directory to write temporary files. The job environment can be modified by the server configuration to modify paths appropriately or set other environment variables. If the type of grid/batch system for the new site is already supported, the IceProd instance can be configured to use an existing server plugin, with the appropriate local queuing options. Otherwise, the server plugin must be written, as described in Section 3.2.1. 15

5.5. Extending Functionality The ease of adaptation of the framework for the applications discussed in Sections 5.2 and 5.3 illustrates how IceProd can be ported to other projects with minimal customization, which is facilitated by its Python code base. There are a couple of simple ways in which functionality can be extended: One is through the implementation of additional IceProd Modules as described in Section 3.3. Another is by adding XML-RPC methods to the soapmon module in order to provide a way for jobs to communicate with the server. There are, of course, more intrusive ways of extending functionality, but those require a greater familiarity with the framework. 6. Performance Since its initial deployment in 2006, the IceProd framework has been instrumental in generating Monte Carlo simulations for the IceCube collaboration. The IceCube Monte Carlo production has utilized more than three thousand CPU-core hours distributed between collaborating institutions at an increasing rate and produced nearly two petabytes of data distributed between the two principal storage sites in the U.S. and Germany. Figure 10 shows the relative share of CPU resources contributed towards simulation production. The IceCube IceProd grid has grown from 8 sites to 25 over the years and incorporated new computing resources. Incorporating new sites is trivial since each set of daemons acts as a volunteer that operates opportunistically on a set of job/tasks independent of other sites. There is no central manager that needs to scale with the number of computing sites. The central database is the one component that does need to scale up and can also be a single point of failure. Plans to address this weakness will be discussed in Section 7. The IceProd framework has also been successfully used for the off-line processing of data collected from the IceCube detector over a 4-year period beginning in the Spring of 2010. This corresponds to 500 terabytes of data and over 3 × 1011 event reconstructions. Table 4 summarizes the resources utilized by IceProd for simulation production and off-line processing. 7. Future Work Development of IceProd is an ongoing effort. One important area of current development is the implementation of workflow management capabilities like HTCondor's DAGMan but in a way that is independent of

Figure 10: Share of CPU resources contributed by members of the IceCube Collaboration towards simulation production. The relative contributions are integrated over the lifetime of the experiment. The size of the sector reflects both the size of the pool and how long a site has participated in simulation production.

any batch system in order to optimize the use of specialized hardware and network topologies by running different job subtasks on different nodes. Work is also ongoing on a second generation of IceProd designed to be more robust and flexible. The database will be partially distributed to prevent it from being a single point of failure and to better handle higher loads. Caching of files will be more prevalent and easier to implement to optimize bandwidth usage. The JEP will be made more versatile by executing ordinary scripts in addition to modules. Tasks will become a fundamental part of the design rather than an added feature and will therefore be fully supported throughout the framework. Improvements in the new design are based on lessons learned from the first generation IceProd and provide a better foundation on which to continue development. 8. Conclusions IceProd has proven to be very successful for managing IceCube simulation production and data processing across a heterogeneous collection of individual grid sites and batch computing clusters. With few software dependencies, IceProd can be deployed and administered with little effort. It makes use of existing trusted grid technology and network protocols, which help to minimize security and data integrity concerns that are common to any software that depends heavily on communication through the Internet. Two important features in the design of this framework are the iceprod-modules and iceprod-server plugins, which allow users to easily extend the functionality of the code. The former provide an interface between the IceProd framework and user scripts and applications. The latter provide an interface that abstracts the details of job submission and management in different grid environments from the framework. IceProd contains a growing library of plugins that support most major grid and batch system protocols. Though it was originally developed for managing IceCube simulation production, IceProd is general enough for many types of grid applications and there are plans to make it generally available to the scientific community in the near future. Acknowledgements We acknowledge the support from the following agencies: U.S. National Science Foundation-Office of 16

Table 4: IceCube simulation production and off-line processing resource utilization. The production rate has steadily increased since initial deployment. The numbers reflect utilization of owned computing resources and opportunistic ones. Simulation 25  3000 yr  45000 2421 1.6 × 107 2.3 × 107 1.2 PB Off-line 1  160 yr 2000 5 1.5 × 106 1.5 × 106 0.5 PB

Computing centers CPU-core time CPU-cores No. of datasets No. of jobs No. of tasks Data volume

Polar Programs, U.S. National Science FoundationPhysics Division, University of Wisconsin Alumni Research Foundation, the Grid Laboratory Of Wisconsin (GLOW) grid infrastructure at the University of Wisconsin­Madison, the Open Science Grid (OSG) grid infrastructure; U.S. Department of Energy, and National Energy Research Scientific Computing Center, the Louisiana Optical Network Initiative (LONI) grid computing resources; Natural Sciences and Engineering Research Council of Canada, WestGrid and Compute/Calcul Canada; Swedish Research Council, Swedish Polar Research Secretariat, Swedish National Infrastructure for Computing (SNIC), and Knut and Alice Wallenberg Foundation, Sweden; German Ministry for Education and Research (BMBF), Deutsche Forschungsgemeinschaft (DFG), Helmholtz Alliance for Astroparticle Physics (HAP), Research Department of Plasmas with Complex Interactions (Bochum), Germany; Fund for Scientific Research (FNRS-FWO), FWO Odysseus programme, Flanders Institute to encourage scientific and technological research in industry (IWT), Belgian Federal Science Policy Office (Belspo); University of Oxford, United Kingdom; Marsden Fund, New Zealand; Australian Research Council; Japan Society for Promotion of Science (JSPS); the Swiss National Science Foundation (SNSF), Switzerland; National Research Foundation of Korea (NRF); Danish National Research Foundation, Denmark (DNRF) The authors would like to also thank T. Weisgarber from the HAWC collaboration for his contributions to IceProd development.

References
[1] F. Halzen, IceCube A Kilometer-Scale Neutrino Observatory at the South Pole, IAU XXV General Assembly, ASP Conference Series 13 (2003) 13­16. [2] M. G. Aartsen, et al., Search for Galactic PeV gamma rays with the IceCube Neutrino Observatory, Phys. Rev. D 87 (2013) 62002. [3] F. Halzen, S. R. Klein, IceCube: An Instrument for Neutrino Astronomy, Invited Review Article: Rev. Sci. Inst. 81 (2010) 081101. [4] D. Protopopescu, K. Schwarz, PANDA Grid­a Tool for Physics, J. Phys.: Conf. Ser. 331 (2011) 072028. [5] P. Buncic, A. Peters, P. Saiz, The AliEn system, status and perspectives, eConf C0303241 (2003) MOAT004. arXiv:cs/0306067 . [6] R. Brun, F. Rademakers, ROOT - An Object Oriented Data Analysis Framework, Nuclear Inst. and Meth. in Phys. Res., A 389 (1997) 81­86. [7] A. Pavlo, P. Couvares, R. Gietzel, A. Karp, I. D. Alderman, M. Livny, The NMI build and test laboratory: Continuous integration framework for distributed computing software, Proc. USENIX/SAGE Large Installation System Administration Conference (2006) 263­273. [8] D. Winer, XML/RPC Specification, http://www.xmlrpc.com/spec (1999). [9] T. DeYoung, IceTray: A software framework for IceCube, Int. Conf. on Comp. in High-Energy Phys. and Nucl. Phys. (CHEP2004) (2005) 463­466. [10] R. Ellis, the ExpressionEngine Development Team, CodeIgniter User Guide, http://codeigniter.com (online manual). [11] W. Allcock, et al., GridFTP: Protocol extensions to FTP for the Grid, http://www.ggf.org/documents/GWD-R/GFD-R.020.pdf (April 2003). [12] The Globus Security Team, Globus Toolkit Version 4 Grid Security Infrastructure: A Standards Perspective (2005). [13] P. Couvares, T. Kosar, A. Roy, J. Weber, K. Wenger, Workflow Management in Condor, In Workflows for e-Science part III (2007) 357­375. [14] M. G. Aartsen, et al., Measurement of South Pole ice transparency with the IceCube LED calibration system, Nuclear Inst. and Meth. in Phys. Res., A A711 (2013) 73­89. [15] D. Chirkin, Study of South Pole ice transparency with IceCube flashers, Proc. International Cosmic Ray Conference 4 (2011) 161. [16] D. Chirkin, Photon tracking with GPUs in IceCube, Nuclear Inst. and Meth. in Phys. Res., A 725 (2013) 141­143. [17] A. U. Abeysekara, et al., On the sensitivity of the HAWC observatory to gamma-ray bursts HAWC Collaboration, Astropart. Phys. 35 (2012) 641­650. [18] T. Weisgarber, Production Reconstruction, HAWC Collaboration Meeting, May, 2013 (Unpublished results).

Appendix

The following is a comprehensive list of sites participating in IceCube Monte Carlo production: Uppsala University (SweGrid), Stockholm University (SweGrid), University of Alberta (WestGrid), TU Dortmund (PHiDO, LIDO), Ruhr-Uni Bochum (LiDO), University of Mainz, Universit´ e Libre de Bruxelles/Vrije Universiteit Brussel, Universiteit Gent (Trillian) Southern University (LONI), Pennsylvania State University (LIONX), University of Wisconsin (CHTC, GLOW, NPX4), Open Science Grid, RWTH Aachen University (EGI), Universit¨ at Dortmund (EGI), Deutsches Elektronen-Synchrotron (EGI, DESY), Universit¨ at Wuppertal (EGI), University of Delaware, Lawrence Berkeley National Laboratory (PDSF, Dirac, Carver), University of Maryland. 17

A GPU-based Correlator X-engine Implemented on the CHIME Pathfinder
Nolan Denman, Mandana Amiri,§ Kevin Bandura, Jean-Franc ¸ ois Cliche, Liam Connor,¶  § § § Matt Dobbs, Mateus Fandino, Mark Halpern, Adam Hincks, Gary Hinshaw,§ Carolin H¨ ofer,§  §    Peter Klages, Kiyoshi Masui, Juan Mena Parra, Laura Newburgh, Andre Recnik, J. Richard Shaw,¶ Kris Sigurdson,§ Kendrick Smith, and Keith Vanderlinde

arXiv:1503.06202v2 [astro-ph.IM] 11 Jun 2015

Dunlap Institute, University of Toronto Department of Astronomy and Astrophysics, University of Toronto § Department of Physics and Astronomy, University of British Columbia  Department of Physics, McGill University ¶ Canadian Institute for Theoretical Astrophysics Canadian Institute for Advanced Research, CIFAR Program in Cosmology and Gravity  Perimeter Institute for Theoretical Physics Contact E-Mail: denman@astro.utoronto.ca




Abstract--We present the design and implementation of a custom GPU-based compute cluster that provides the correlation X-engine of the CHIME Pathfinder radio telescope. It is among the largest such systems in operation, correlating 32,896 baselines (256 inputs) over 400 MHz of radio bandwidth. Making heavy use of consumer-grade parts and a custom software stack, the system was developed at a small fraction of the cost of comparable installations. Unlike existing GPU backends, this system is built around OpenCL kernels running on consumer-level AMD GPUs, taking advantage of low-cost hardware and leveraging packed integer operations to double algorithmic efficiency. The system achieves the required 105 TOPS in a 10 kW power envelope, making it one of the most power-efficient X-engines in use today.

the system achieves high performance at a small fraction of the hardware cost of comparable installations. This paper focuses on the system architecture and implementation, while two companion papers describe the custom software stacks, one focusing on an innovative OpenCL-based X-engine GPU kernel [2], and one on the handling of the vast data volume flowing through the system [3]. The paper is structured as follows: design considerations and constraints are discussed in §II; the hardware components of the system are described in §III, and the software in §IV; the scaling of the X-engine to the full-size CHIME telescope is described in §V, and a summary and conclusion follow in §VI. II. D ESIGN C ONSIDERATIONS

I.

I NTRODUCTION While most components in CHIME scale linearly with number of inputs N , the computational cost of pairwise correlation scales as N 2 , making efficiency in the X-engine a primary concern. There are correlation techniques which rely on the redundancy of CHIME feed separations to scale as N log N , but the real-time calibrations these require for precision observations remain largely unproven in an astrophysical context. Design decisions were guided by the need to produce an inexpensive system capable of scaling to support full CHIME, and which would support rapid development and deployment of new data processing algorithms. These requirements of computational power and ease of development drove the decision to build the X-engine around GPUs rather than Application-Specific Integrated Circuits (ASICs) or FPGAs. The computational cost  of pairwise element correlation for N elements across a bandwidth of  is  =  · N · (N + 1)/2 (1)

The Canadian Hydrogen Intensity Mapping Experiment (CHIME) is an interferometric radio telescope, presently under construction at the Dominion Radio Astrophysical Observatory (DRAO) in British Columbia, Canada, which will map the northern sky over a radio band from 400 to 800 MHz. With over 2000 inputs and a 400 MHz bandwidth, the correlation task (measured as the bandwidth-baselines product) on CHIME will be an order-of-magnitude larger than on any currently existing telescope array. The correlator follows an FX split design, with a first stage Field Programmable Gate Array (FPGA)-based F-engine which digitizes, Fourier transforms (channelizes), and bundles the data into independent frequency bands, followed by a second-stage Graphics Processing Unit (GPU)-based X-engine which produces a spatial correlation matrix consisting of the integrated pairwise products of all the inputs at each frequency. The CHIME Pathfinder instrument [1] features 128 dualpolarization feeds and a reduced-scale prototype of the full CHIME correlator. This paper describes the X-engine of the Pathfinder's 256-input hybrid FPGA/GPU FX correlator, among the largest such systems in operation. Through extensive use of off-the-shelf consumer-grade hardware and heavy optimization of custom data handling and processing software,

measured in complex multiply-accumulate (cMAC) operations per second; for the CHIME Pathfinder,  = 13 TcMAC/s. For large N this dominates the cost of any other processing proposed for the X-engine. Top-end GPUs in 2014 provided of

Fig. 4. Screenshot showing the correlator status webpage. The monitoring system displays the status of each FPGA and GPU node, and of the correlator software; per-GPU temperatures and per-node power consumption are also available.

Fig. 3. A diagram showing the liquid-cooling structure in the CHIME Pathfinder, with red and blue indicating hot and cold coolant, respectively. The heat exchanger uses a large fan to cool the liquid using ambient outside air. The object marked `M' is a temperature-controlled mixing valve, which regulates the temperature of the `cold' sections of the loop.

kotekan4 software pipeline manages the data flow and processing within GPU nodes. Due to the high I/O demands (820 Gb/s in total), the system must make maximally efficient use of the available bandwidth at each stage. A packetized and loss-tolerant data handling system, similar to that in operation in the PAPER [6] correlator [7] ensures that momentary faults do not impede long-term data gathering. Recnik et. al. [3] discuss the data handling in detail; a brief description follows. Data arrive as UDP packets and are buffered by the host CPU in system memory for inspection and staging prior to transfer into the GPUs for processing. Packet loss, though rare, is tracked along with other flags from the F-engine from e.g. saturation of the ADCs or from FFT channelizing. The count of missing or saturated data is used to renormalize the postintegration correlation matrices. A series of OpenCL kernels are executed on the GPUs; these pre-condition the data, compute and integrate correlation matrices, and post-process the data if necessary (see [2] for more details). Computed correlation matrices are assembled by the CPU and forwarded to gamelan, which stitches the full 400 MHz band back together using data from all active nodes. This reassembly is robust against individual node failures or outages; they simply result in loss of data from the inactive nodes. Integrated correlation matrices are recorded onto an array of disks in gamelan, and these data are asynchronously copied to a remote archive server hosting a much larger array of drives, and then copied off-site for scientific analyses.

Fig. 5. Screenshot showing the live view webpage. The triangle is the full correlation matrix for a particular frequency, with colour indicating the complex value's phase; the website may be queried for any of the associated data.

B. Monitoring and Control The X-engine software pipeline (composed of the kotekan instances on each node, along with the collection server software) is launched and controlled through scripts run on gamelan. The nodes run CentOS Linux 6.5 and can be accessed by remote shell login, while the PDUs allow remote power cycling to aid in recovery of crashed systems. The status of each of the GPU nodes is tracked by the gamelan control node, and made available via a web interface; see Figure 4 for an example of the tracking display. In addition, the last few hours of data are streamed over TCP to a second server, where it is available for live analysis and monitoring. An example of the live-monitoring webpage is shown in Figure 5. C. GPU Data Processing Tasks The most computationally expensive operation performed on the GPU nodes is the mission-critical pairwise feed correlation. In parallel with this, the Pathfinder correlator will explore alternate correlation methods which leverage the redundant layout of the CHIME baselines. Supplemental tasks include beamforming, gating, time-shifting of inputs, and RFI excision. Brief descriptions of these tasks follow.

4 A style of playing fast interlocking parts in Balinese Gamelan music; see http://en.wikipedia.org/wiki/Kotekan

1) Full Correlation: The primary responsibility of the Xengine is to calculate and integrate the correlation matrix of all the spatial inputs. This involves accumulation of 32,896 pairwise products for each of 1024 frequency bands. The default integration period is 223 samples, corresponding to 21.47 s, much faster than the 2.5 minute beam-crossing time from sky rotation. The computational requirements of the full X-engine system are dominated by this correlation operation, such that all other processes constitute an insignificant additional burden. The current implementation achieves nearmaximum-theoretical throughput; for details, see Klages et. al. [2] 2) Alternate Correlation Techniques: Interferometric arrays with highly redundant baselines can take advantage of correlation techniques that are more efficient than the na¨ ive pairwise method. In the case of feeds which are evenly spaced, FFT-based transformations can be used to increase the efficiency of the correlation to N log N , at the cost of strict calibration requirements. [8][9][10] These correlation strategies will be tested in parallel with the pairwise N 2 correlation; additionally, they may be used in hybrid form with some N 2 and some N log N stages. 3) Discrete Beamforming: The CHIME Pathfinder is a stationary telescope that cannot physically point at a specific source or location on the sky. When observing localized sources, it is desirable to form one or more beams, `pointing' the telescope digitally to an arbitrary location within the main beam. This is accomplished in the GPUs by phase-shifting and summing the data from all antennas, so that signals originating in one region of the sky interfere constructively. This signal is then written out at very high cadence, allowing examination of a localized source with very fine time resolution. 4) Output Gating: The CHIME Pathfinder will observe periodic sources such as astronomical pulsars and injected calibration signals. These sources generally vary faster than the default 21 s integration period, but high-cadence gating may be used to observe sub-integration signal structure. Gating consists of partitioning the output into a set of sub-buffers based on the time relative to the period of the source, so that independent `on' and `off' signals may be constructed. 5) Time Shifting: Signals from outlying telescope stations can be fed into the correlator. Large spatial separations introduce decorrelation between inputs, which can be corrected for by time-shifting samples within the GPUs. The current implementation permits the correction of any input by up to 168 ms, and has been tested with the nearby John A. Galt 26m radio telescope at DRAO. 6) RFI Cleaning: Anthropogenic radio frequency interference (RFI) introduces a significant source of additional noise to the astronomical signal. These signals are generally narrow-band and intermittent, coming and going on timescales much shorter than the default 21 s integration period, but with relatively low duty cycles. High-cadence identification and excision of RFI can be performed within the GPUs, and a variety of algorithms are under development including robust outlier and higher-moment statistical tests. [11][12]

V.

X- ENGINE S CALABILITY

The X-engine described here was designed for the CHIME Pathfinder, and must be scaled up significantly for the full CHIME instrument. Given a scaled F-engine providing channelized data, the X-engine's design allows it to scale straightforwardly to a broader band or larger-N arrays. Additional radio bandwidth is trivially added through additional nodes; increasing the number of inputs adds to the computational demand on each node, and can be addressed through newer, more powerful GPUs. At the time of writing, the computational power per node could be roughly tripled by simply replacing the GPUs. To support larger N 2 requirements, the bandwidth handled in each node can be reduced, in exchange for proportionally more nodes. The bandwidth fed to each GPU can similarly be reduced, and for very large N , when even a single frequency band is beyond the capacity of a single processing node, data can be time-multiplexed across multiple GPUs. The expansion to full CHIME (N = 2048) yields an N 2 computational requirement  an order-of-magnitude greater than any system currently in existence. Using current technology, a straightforward scaling of the current system -- 256 nodes each containing 2 dual-chip R9 295X2 GPUs -- could handle the entire pairwise correlation task, without additional software development. This density of processors is easily achievable with the liquid cooling demonstrated, and would occupy a modest physical footprint, at a very low hardware cost of $1M. However, it is not expected that the full CHIME instrument will rely on a complete N 2 correlation, instead pursuing a fast alternate correlation technique as discussed in §IV-C2. VI. C ONCLUSION

We have implemented a low-cost, high-efficiency GPUbased correlator X-engine for the CHIME Pathfinder. Capable of correlating 32,896 baselines over 400 MHz of radio bandwidth, it makes efficient use of consumer-grade parts and executes a highly optimized software stack. Measured by the computational requirement of a na¨ ive N 2 correlation ­ the bandwidth-baseline product  defined by Equation 1 ­ the CHIME Pathfinder correlator is among the largest in the world. Aspects of the system such as the cooling systems have been substantially modified, optimizing the X-engine's efficiency and ensuring economical scaling to the full-size CHIME instrument. ACKNOWLEDGEMENTS We are very grateful for the warm reception and skillful help we have received from the staff of the Dominion Radio Astrophysical Observatory, which is operated by the National Research Council of Canada. We acknowledge support from the Canada Foundation for Innovation, the Natural Sciences and Engineering Research Council of Canada, the B.C. Knowledge Development Fund, le Cofinancement gouvernement du Qu´ ebec-FCI, the Ontario Research Fund, the CIFAR Cosmology and Gravity program, the Canada Research Chairs program, and the National Research Council of Canada. PK thanks IBM Canada for funding

his research and work through the Southern Ontario Smart Computing Innovation Platform (SOSCIP). We thank Xilinx University Programs for their generous support of the CHIME project, and AMD for donation of test units. R EFERENCES
[1] K. Bandura et al., "Canadian Hydrogen Intensity Mapping Experiment (CHIME) Pathfinder," in Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, ser. Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, vol. 9145, Jul. 2014, p. 22. P. Klages et al., "Data Packing for High-Speed 4-Bit GPU Correlators," 2015, In press; accepted to IEEE ASAP 2015. A. Recnik et al., "An Efficient Real-time Data Pipeline for the CHIME Pathfinder Radio Telescope X-Engine," 2015, In press; accepted to IEEE ASAP 2015. M. A. Clark, P. C. La Plante, and L. J. Greenhill, "Accelerating Radio Astronomy Cross-Correlation with Graphics Processing Units," ArXiv e-prints, Jul. 2011. Khronos Group: The OpenCL Specification. [Online]. Available: khronos.org/opencl A. R. Parsons et al., "New Limits on 21 cm Epoch of Reionization from PAPER-32 Consistent with an X-Ray Heated Intergalactic Medium at z = 7.7," The Astrophysical Journal, vol. 788, p. 106, Jun. 2014. A. Parsons et al., "A Scalable Correlator Architecture Based on Modular FPGA Hardware, Reuseable Gateware, and Data Packetization," Publications of the Astronomical Society of the Pacific, vol. 120, pp. 1207­1221, Nov. 2008. J. D. Bunton, "Antenna Array Geometries to Reduce the Compute Load in Radio Telescopes," IEEE Transactions on Antennas and Propagation, vol. 59, pp. 2041­2046, Jun. 2011. M. Tegmark and M. Zaldarriaga, "Fast Fourier transform telescope," Physical Review D, vol. 79, no. 8, p. 083530, Apr. 2009. ----, "Omniscopes: Large area telescope arrays with only NlogN computational cost," Physical Review D, vol. 82, no. 10, p. 103501, Nov. 2010. G. M. Nita, D. E. Gary, Z. Liu, G. J. Hurford, and S. M. White, "Radio Frequency Interference Excision Using Spectral-Domain Statistics," Publications of the Astronomical Society of the Pacific, vol. 119, pp. 805­827, Jul. 2007. G. M. Nita and D. E. Gary, "Statistics of the Spectral Kurtosis Estimator," Publications of the Astronomical Society of the Pacific, vol. 122, pp. 595­607, May 2010.

[2] [3]

[4]

[5] [6]

[7]

[8]

[9] [10]

[11]

[12]

An Efficient Real-time Data Pipeline for the CHIME Pathfinder Radio Telescope X-Engine
Andre Recnik , Kevin Bandura , Nolan Denman , Adam D. Hincks§ , Gary Hinshaw§ Peter Klages , Ue-Li Pen¶ , and Keith Vanderlinde
Institute for Astronomy & Astrophysics, University of Toronto of Astronomy & Astrophysics, University of Toronto  Department of Physics, McGill University § Department of Physics and Astronomy, University of British Columbia ¶ Canadian Institute for Theoretical Astrophysics, University of Toronto Contact Email: vanderlinde@dunlap.utoronto.ca
 Department  Dunlap

arXiv:1503.06189v2 [astro-ph.IM] 15 Jun 2015

Abstract--The CHIME Pathfinder is a new interferometric radio telescope that uses a hybrid FPGA/GPU FX correlator. The GPU-based X-engine of this correlator processes over 819 Gb/s of 4+4-bit complex astronomical data from N=256 inputs across a 400 MHz radio band. A software framework is presented to manage this real-time data flow, which allows each of 16 processing servers to handle 51.2 Gb/s of astronomical data, plus 8 Gb/s of ancillary data. Each server receives data in the form of UDP packets from an FPGA F-engine over the eight 10 GbE links, combines data from these packets into large (32MB256MB) buffered frames, and transfers them to multiple GPU co-processors for correlation. The results from the GPUs are combined and normalized, then transmitted to a collection server, where they are merged into a single file. Aggressive optimizations enable each server to handle this high rate of data; allowing the efficient correlation of 25 MHz of radio bandwidth per server. The solution scales well to larger values of N by adding additional servers.

I. I NTRODUCTION The increasing performance of commodity computer hardware has opened up new possibilities for building real-time radio correlators. Traditionally correlators have used custom ASICs or FPGAs for all calculations. The first systems built with off-the-shelf hardware used CPUs, for example the realtime software correlator designed for the Giant Metrewave Radio Telescope (GMRT) [1]. While early experiments using GPUs showed relatively poor performance [2], the development of new GPUs and the efficient xGPU code [3] for NVIDIA's CUDA platform has popularized the use of GPUs in FX style correlators. FX correlators operate in two stages: first the F-engine samples astronomical data from each radio input and channelizes it into frequency bands using a Fourier transform, then the X-engine correlates all of the inputs against one another within each frequency band. The low cost and flexibility of 10 Gigabit Ethernet (10 GbE) has enabled these two stages to be easily separated, as demonstrated by Parsons, et al. [4]; with separate FPGAs performing the F-engine and X-engine stages connected by a 10 GbE packet switched network. A recent trend has been to replace FPGAs with GPUs in the X-engine, leading to the so called hybrid correlator approach. Projects using hybrid correlators include: the Precision Array

for Probing the Epoch of Reionization (PAPER) [5], the Large Aperture Experiment to Detect the Dark Ages (LEDA) [6], and the Murchison Wide-field Array (MWA) [7]. The Canadian Hydrogen Intensity Mapping Experiment (CHIME) Pathfinder [8] is a cylindrical radio telescope with 128 dual-polarization receivers for N = 256 total inputs. It uses a newly developed hybrid FX correlator. The F-engine consists of 16 custom FPGA boards, each with 16 ADCs, which sample and channelize the data into 1024 frequency bins. The data are reduced to 4+4-bit complex numbers, then a custom backplane network shuffles this data, such that each FPGA has data for all inputs in a subset of frequency bands. The shuffled data is then transmitted over 128 x 10 GbE links in User Datagram Protocol (UDP) packets to a GPU based X-engine. This paper presents the software pipeline, called kotekan,1 which manages data flow in the X-engine. The data is received from the F-engine as UDP packets, is merged and sent to the GPUs for processing, then the output from the GPUs is sent out to the collection and aggregation server. The software is written in the C programming language. In the CHIME Pathfinder, this software runs on 16 servers, built mostly with low-cost consumer-grade hardware. Each server has one Intel i7 4-core CPU, two AMD R9 280X GPUs, one AMD R9 270X GPU, two Silicom 4x 10 GbE network interface cards (NICs), and 16 GB of DDR3 RAM. The GPUs and NICs are each connected to the CPU by 8x PCI Express (PCIe) 3.0 lanes. An abstract layout of each of these components is given in Figure 1. The operating system used is CentOS Linux 6.5. II. DATA F LOW The software design is based around generic buffer objects, which support multiple consumers and producers, and handle the majority of thread synchronization tasks using
1 Given the musical acronym of the experiment, CHIME, the collaboration uses musically inspired names for system components. Kotekan is a style of playing fast interlocking parts in Balinese Gamelan music using percussive instruments. http://en.wikipedia.org/wiki/Kotekan

31 Header
Stream ID ENC #Fra -mes Prot LEN cookie

0

#Freq Bins Ancillary data Timestamp

#Antennas

Data (1-4 Frames) 8 Frequencies per Frame 256 bytes per frequency 2048 Bytes per Frame 2048-8192 Bytes per packet

16 bytes

FREQ BIN 1 256ANT - 256 bits

FREQ BIN 0 256ANT - 256 bits

. . .
FREQ BIN 7 256ANT - 256 bits FREQ BIN 6 256ANT - 256 bits

Post FFT 4-bit Scaling Flags ADC Flags

1 bit flag per antenna per frequency 256 Bytes per Frame
ADC INPUT 256 ANT 512bits

1 flag per input

Fig. 3: The UDP packet format sent by the FPGAs.

entire array. There are a number of considerations to make when processing UDP packets at high rates: 1) Efficiently receiving packets from the network: The Linux kernel has a large overhead when processing high bandwidth network traffic. Early tests showed that even with large packets and optimizations to the Linux kernel network parameters, it would be unable to process the required data rates while simultaneously managing the GPU PCIe transfers. The solution was to bypass the Linux kernel's network stack entirely using modified network drivers and custom kernel modules. There are a number of pre-built solutions which enable kernel bypass, including: DPDK [9], netmap [10], and others. For this project, NTOP's PF RING/DNA [11], [12] framework was chosen for its support on the chosen network card vendor. However, the overall system is not tied to a particular bypass stack; changing the bypass stack would just involving replacing calls to the PF RING API with another bypass API. The most important aspect of these bypass drivers, aside from avoiding slow kernel packet processing code, is the use of a co-mapped ring buffer that is addressable in both user and kernel space. This allows the NIC driver to write directly to the co-mapped memory with a Direct Memory Access (DMA) operation, avoiding the traditional kernel-to-user-space copy. 2) Handling Losses/Errors: Since the network protocol is UDP, the system must handle lost, out of order, and duplicate packets. This is achieved by tracking the sequence number in each packet header. When the system detects packet loss, it writes zeros into the area of the GPU staging buffer where the missing packet would have been placed. This removes the requirement to zero the buffers between each use; the network thread guaranties the buffer will have good data or zeros in every location. In the case of out of order packets, the sequence number is used to copy the packet into the correct location. Duplicate packets are simply ignored. In early testing with the standard socket API, vectored

I/O was used to separate the packet into data and header components with a readv system call. The sequence number was inferred based on the pervious packet, and the data was copied directly to the GPU staging buffer. If the sequence number did not match the expected number, then the data was moved or overwritten. Given low packet loss, this saves a second copy for the vast majority of packets, and allowed maximum capture rates around 40 Gb/s per CPU. With the addition of co-mapped memory this became unnecessary, since the sequence number can be read before moving the data to the GPU staging buffer. 3) Efficient Memory Transfers: The standard memory copying functions like memcpy found in libc are inefficient for use in a high bandwidth environment like this one. This issue largely stems from the fact that compilers assume temporal locality, that data being copied will be used in the near future and should be added to the CPU cache. This causes the destination memory to be cached in the process of performing a copy. This puts huge pressure on the cache when moving large amounts of data, resulting in unnecessary memory controller usage. To mediate the issue caused by the standard memcpy, a set of custom memory copy functions that use Intel AVX intrinsics with non-temporal hints were created. The non-temporal hint prevents the memory copy destination from being added to the CPU's cache. These functions copy the data from the comapped packet buffer to the GPU staging frame. B. Kernel Invocation Process A set of GPU programs called kernels do the actual crosscorrelation and integration. The kernels are written in the Open Computing Language (OpenCL) framework2 , allowing them to be run on a number of different platforms. The kernels used achieve very high levels of efficiency using packed multiplyaccumulate operations and are detailed in [13]. The first step in this process is copying the data to the GPU's device memory, a process largely managed by the OpenCL drivers. In our tests we found that large (32 MB or greater) frames transferred most efficiently. The memory used for these frames is page-locked allowing the GPU drivers to preform an efficient DMA operation to copy the memory between the host and device. In contrast to some systems using xGPU [3] like the MWA [7] and LEDA correlators [6] which promote the data from 4-bit to 8-bit integers in their CPUs, this system does not promote the data before sending it to the GPUs. Avoiding this promotion reduced both CPU memory and PCIe bandwidth, and was a key part in limiting the number of servers and CPUs needed. The entire process is pipelined to allow concurrent operation of the memory transfers and kernel invocations, as shown in Figure 4. The "Copy to Host Memory" step is not run on every invocation, since the correlation kernels simply add to the output of the last run, extending the integration time of the output.
2 https://www.khronos.org/opencl/

Data Ready

Data Ready Copy to GPU Memory

Data Ready Copy to GPU Memory Kernels Copy to GPU Memory Kernels Copy to Host Memory Kernels Copy to Host Memory Host Callback

Host Events GPU Copy-in Queue

GPU Kernel Queue GPU Copy-out Queue Host Callbacks

Copy to Host Memory Host Callback Host Callback

Fig. 4: GPU Pipeline. The "Kernels" stage includes correlation, as well as extra data processing operations like time shifting or RFI detection. The dashed lines indicate optional steps or dependencies. The time taken by each step is not to scale.

When executing an OpenCL device operation, for example a memory copy or kernel invocation, there is a delay between the function call and the time the device starts the operation. To minimize this delay, we pre-load entire chains of OpenCL operations, the first of which is set to depend on the "Data Ready" event in Figure 4, which is triggered by the GPU thread when a buffer has been filled by a network thread. When a chain finishes, a call-back function ("Host Callback" in Figure 4) adds a new chain of events which replaces the one that just finished, and marks the associated output buffer as full, so the GPU post-processing thread can take the data. C. GPU Post-Processing This GPU post-processing thread combines all eight streams, and then normalizes the results to correct for lost input samples. Loss of input samples can result from packet loss, or numerical range and sampling limitations in the FPGAs and ADCs. When this happens the data points are zeroed, before they are transferred to the GPU. This causes some correlated data points to contain fewer input samples in each integration. The network threads track packet loss and F-engine error flags, which are used by the GPU post-processing thread to generate a normalization matrix as follows: · Packet loss is tracked as a single counter, then added to every point in the normalization matrix at the end of each integration. · The F-engine flags are extracted from bit-fields in the packet footers and stored in counters for each input and frequency per integration. The counters are used to populate the normalization matrix at the end of each integration. · If two or more inputs are flaged by the F-engine in the same time sample, the previous step will result in a double counting in the normalization matrix at their intersections.

To correct this, these intersections are recorded per sample, and subtracted from the normalization matrix. The fraction of lost data given by the normalization matrix is then applied to the correlation matrix. This process is optimized in two ways. First the bit-field of flags in the footer is read initially as 32-bit integers, and bitfield extraction is performed only if the 32-bit representation of the bit-field is non-zero. With a low number of flaged data points, this avoids checking each bit individually. Second, by only updating counts of errors and their associated intersections, the number of memory accesses is greatly reduced. The complexity of tracking the intersections per time sample is O(E 2 ), where E is the number of flaged data points in a given time sample. This can be processed by the CPU, provided E N. After the data has been combined and normalized, it is formatted for transmission over a TCP stream and placed in an output buffer. A final output thread then transmits this data to the collection server using a TCP socket connection. D. Data Collection At the collection server, the TCP streams from each kotekan instance are combined. The streamID is used to identify the frequency bands provided by each stream, allowing the collection server to correctly order the frequency bands regardless of how the 10 GbE links were connected to the individual processing servers. The sequence number is used to align the frames in time. The data is saved to disk using the Hierarchical Data Format Version 5 (HDF5) standard3 . Individual servers may be switched off and back on without interrupting the entire system. If one or more of the servers stops working, the collection server simply stops recording the frequency bands associated with that server. When the server
3 http://www.hdfgroup.org/HDF5/

TABLE I: Data Processing Rates
Data Type 4-bit Sky Data 4-bit Sky Data + Headers & Flags Packets per Second (PPS) Output Data (10s Cadence) Rate per Link 6.4 Gb/s 7.45 Gb/s 97,565 PPS 16.8 Mb/s Rate per Host 51.2 Gb/s 59.6 Gb/s 781,250 PPS 16.8 Mb/s Full System Rate 819.2 Gb/s 953.6 Gb/s 12.5 MPPS 269.5 Mb/s

of the CHIME Pathfinder, with N = 256 inputs, using only 16 servers. The system has been shown to scale well from N = 16 to N = 256, and will be used in a future system with N = 2048 and a total input bandwidth of close to 8 Tb/s. ACKNOWLEDGMENTS We are very grateful for the warm reception and skillful help we have received from the staff of the Dominion Radio Astrophysical Observatory, which is operated by the National Research Council of Canada. We acknowledge support from the Canada Foundation for Innovation, the Natural Sciences and Engineering Research Council of Canada, the B.C. Knowledge Development Fund, le Cofinancement gouvernement du Qu´ ebec-FCI, the Ontario Research Fund, the CIfAR Cosmology and Gravity program, the Canada Research Chairs program, and the National Research Council of Canada. We thank Xilinx University Programs for their generous support of the CHIME project, and AMD for donation of test units. Peter Klages thanks IBM Canada for funding his research and work through the Southern Ontario Smart Computing Innovation Platform (SOSCIP). R EFERENCES
[1] J. Roy, Y. Gupta, U.-L. Pen, J. Peterson, S. Kudale, and J. Kodilkar, "A real-time software backend for the gmrt," Experimental Astronomy, vol. 28, no. 1, pp. 25­60, 2010. [Online]. Available: http://dx.doi.org/ 10.1007/s10686-010-9187-0 [2] R. V. van Nieuwpoort and J. W. Romein, "Using many-core hardware to correlate radio astronomy signals," in Proceedings of the 23rd International Conference on Supercomputing, ser. ICS '09. New York, NY, USA: ACM, 2009, pp. 440­449. [Online]. Available: http://doi.acm.org/10.1145/1542275.1542337 [3] M. Clark, P. L. Plante, and L. Greenhill, "Accelerating radio astronomy cross-correlation with graphics processing units," Int. J. High Perform. Comput. Appl., vol. 27, no. 2, pp. 178­192, May 2013. [Online]. Available: http://dx.doi.org/10.1177/1094342012444794 [4] A. Parsons et al., "A scalable correlator architecture based on modular fpga hardware, reuseable gateware, and data packetization," Publications of the Astronomical Society of the Pacific, vol. 120, no. 873, pp. pp. 1207­1221, 2008. [Online]. Available: http://www.jstor.org/stable/10.1086/593053 [5] A. R. Parsons, A. Liu, J. E. Aguirre, Z. S. Ali, R. F. Bradley et al., "New Limits on 21cm EoR From PAPER-32 Consistent with an X-Ray Heated IGM at z=7.7," Astrophys.J., vol. 788, p. 106, 2014. [6] J. Kocz et al., "Digital Signal Processing Using Stream High Performance Computing: A 512-Input Broadband Correlator for Radio Astronomy," Journal of Astronomical Instrumentation, vol. 4, p. 50003, Mar. 2015. [7] S. M. Ord et al., "The Murchison Widefield Array Correlator," Publications of the Astronomical Society of Australia, vol. 32, p. 6, Mar. 2015. [8] K. Bandura et al., "Canadian Hydrogen Intensity Mapping Experiment (CHIME) pathfinder," in Society of Photo-Optical Instrumentation Engineers Conference Series, vol. 9145, Jul. 2014, p. 22. [9] Intel, Intel Data Plane Development Kit: Programmer's Guide, 2014. [10] L. Rizzo, "netmap: a novel framework for fast packet i/o," in USENIX Annual Technical Conference, 2012. [11] L. Deri, "Improving Passive Packet Capture:Beyond Device Polling," in 4th International System Administration and Network Engineering Conference, 2004. [12] NTOP. (2012) RFC-2544 performance test - PF Ring-DNA VS Standard network Driver. [Online]. Available: http://www.ntop.org/ wp-content/uploads/2012/04/DNA ip forward RFC2544.pdf [13] P. Klages, K. Bandura, N. Denman, A. Recnik, J. Sievers, and K. Vanderlinde, "Data Packing for High-Speed 4-Bit GPU Correlators," In Press. IEEE ASAP, 2015.

is repaired, it reconnects to the collection system, and those frequency bands resume recording. III. D ISCUSSION This software pipeline design was largely focused on maximizing input data bandwidth per CPU, in order to minimize the overall cost of the system. Table I shows the data rates achieved by the software in operation. There are ways in which the pipeline could be made even more efficient. The packets are written to a user/kernel space mapping buffer, then copied to a GPU staging buffer. This copy could be avoided if the data segment of each packet was sent directly to the GPU buffers via DMA from the NIC, with the headers/footers sent to another buffer for processing. Such a solution would require a tighter coupling with software and the NIC driver, so that the userspace application could direct which memory the NIC DMA transfered packet sections into. The OpenFabrics Enterprise Distribution4 (OFED) kernel bypass software can in principle provide this; it was not pursued it due to lack of driver support on the chosen hardware. As the number of inputs N increases, the computation cost of correlation increases as O(N 2 ), while data bandwidth scales only as O(N ). With more servers required to do the correlation, the bandwidth to each should go down. However, as co-processors become more powerful, it will be possible to correlate larger values of N with fewer servers, continuing to put pressure on the bandwidth requirements of each server. The generic design of this software allows it to be extended easily to large values of N . It has already been scaled from N = 16 in early tests of the CHIME Pathfinder using one server, to the current N = 256 mode using 16 servers, and is expected to scale to N = 2048 for the full CHIME experiment. IV. C ONCLUSION We have developed an optimized software pipeline to manage the data flow in the X-engine of a hybrid FX FPGA/GPU correlator. Using a combination of kernel bypass network drivers, efficient memory copy functions, memory pools, minimal memory copies, large packets and GPU frames, and GPU kernels that can process 4-bit data, the system achieves a very high input data bandwidth per server and per CPU. This allows input data processing rates of over 51.2 Gb/s per single socket server, and 819 Gb/s system wide. In terms of radio bandwidth, the systems process the full 400 MHz band
4 https://www.openfabrics.org/

